Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 841?848
Manchester, August 2008
Modeling Latent-Dynamic in Shallow Parsing:
A Latent Conditional Model with Improved Inference
Xu Sun? Louis-Philippe Morency? Daisuke Okanohara? Jun?ichi Tsujii??
?Department of Computer Science, The University of Tokyo, Hongo 7-3-1, Tokyo, Japan
?USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, USA
?School of Computer Science, The University of Manchester, 131 Princess St, Manchester, UK
?{sunxu, hillbig, tsujii}@is.s.u-tokyo.ac.jp ?morency@ict.usc.edu
Abstract
Shallow parsing is one of many NLP tasks
that can be reduced to a sequence la-
beling problem. In this paper we show
that the latent-dynamics (i.e., hidden sub-
structure of shallow phrases) constitutes a
problem in shallow parsing, and we show
that modeling this intermediate structure
is useful. By analyzing the automatically
learned hidden states, we show how the
latent conditional model explicitly learn
latent-dynamics. We propose in this paper
the Best Label Path (BLP) inference algo-
rithm, which is able to produce the most
probable label sequence on latent condi-
tional models. It outperforms two existing
inference algorithms. With the BLP infer-
ence, the LDCRF model significantly out-
performs CRF models on word features,
and achieves comparable performance of
the most successful shallow parsers on the
CoNLL data when further using part-of-
speech features.
1 Introduction
Shallow parsing identifies the non-recursive cores
of various phrase types in text. The paradigmatic
shallow parsing problem is noun phrase chunking,
in which the non-recursive cores of noun phrases,
called base NPs, are identified. As the represen-
tative problem in shallow parsing, noun phrase
chunking has received much attention, with the de-
velopment of standard evaluation datasets and with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
extensive comparisons among methods (McDon-
ald 2005; Sha & Pereira 2003; Kudo & Matsumoto
2001).
Syntactic contexts often have a complex under-
lying structure. Chunk labels are usually far too
general to fully encapsulate the syntactic behavior
of word sequences. In practice, and given the lim-
ited data, the relationship between specific words
and their syntactic contexts may be best modeled
at a level finer than chunk tags but coarser than
lexical identities. For example, in the noun phrase
(NP) chunking task, suppose that there are two lex-
ical sequences, ?He is her ?? and ?He gave her
? ?. The observed sequences, ?He is her? and
?He gave her?, would both be conventionally la-
beled by ?BOB?, where B signifies the ?beginning
NP?, and O the ?outside NP?. However, this label-
ing may be too general to encapsulate their respec-
tive syntactic dynamics. In actuality, they have dif-
ferent latent-structures, crucial in labeling the next
word. For ?He is her ??, the NP started by ?her? is
still incomplete, so the label for ? is likely to be I,
which conveys the continuation of the phrase, e.g.,
?[He] is [her brother]?. In contrast, for ?He gave
her ??, the phrase started by ?her? is normally self-
complete, and makes the next label more likely to
be B, e.g., ?[He] gave [her] [flowers]?.
In other words, latent-dynamics is an interme-
diate representation between input features and la-
bels, and explicitly modeling this can simplify the
problem. In particular, in many real-world cases,
when the part-of-speech tags are not available, the
modeling on latent-dynamics would be particu-
larly important.
In this paper, we model latent-dynamics in
shallow parsing by extending the Latent-Dynamic
Conditional Random Fields (LDCRFs) (Morency
et al 2007), which offer advantages over previ-
841
y y y1 2 m h h h1 2 m
y y y1 2 m
CRF LDCRF
x x x1 2 mx x x1 2 m
Figure 1: Comparison between CRF and LDCRF.
In these graphical models, x represents the obser-
vation sequence, y represents labels and h repre-
sents hidden states assigned to labels. Note that
only gray circles are observed variables. Also,
only the links with the current observation are
shown, but for both models, long range dependen-
cies are possible.
ous learning methods by explicitly modeling hid-
den state variables (see Figure 1). We expect LD-
CRFs to be particularly useful in those cases with-
out POS tags, though this paper is not limited to
this.
The inference technique is one of the most im-
portant components for a structured classification
model. In conventional models like CRFs, the op-
timal label path can be directly searched by using
dynamic programming. However, for latent condi-
tional models like LDCRFs, the inference is kind
of tricky, because of hidden state variables. In this
paper, we propose an exact inference algorithm,
the Best Label Path inference, to efficiently pro-
duce the optimal label sequence on LDCRFs.
The following section describes the related
work. We then review LDCRFs, and propose the
BLP inference. We further present a statistical
interpretation on learned hidden states. Finally,
we show that LDCRF-BLP is particularly effective
when pure word features are used, and when POS
tags are added, as existing systems did, it achieves
comparable results to the best reported systems.
2 Related Work
There is a wide range of related work on shallow
parsing. Shallow parsing is frequently reduced to
sequence labeling problems, and a large part of
previous work uses machine learning approaches.
Some approaches rely on k-order generative proba-
bilistic models of paired input sequences and label
sequences, such as HMMs (Freitag & McCallum
2000; Kupiec 1992) or multilevel Markov mod-
els (Bikel et al 1999). The generative model
provides well-understood training and inference
but requires stringent conditional independence as-
sumptions.
To accommodate multiple overlapping features
on observations, some other approaches view the
sequence labeling problem as a sequence of clas-
sification problems, including support vector ma-
chines (SVMs) (Kudo & Matsumoto 2001) and a
variety of other classifiers (Punyakanok & Roth
2001; Abney et al 1999; Ratnaparkhi 1996).
Since these classifiers cannot trade off decisions at
different positions against each other (Lafferty et
al. 2001), the best classifier based shallow parsers
are forced to resort to heuristic combinations of
multiple classifiers.
A significant amount of recent work has shown
the power of CRFs for sequence labeling tasks.
CRFs use an exponential distribution to model the
entire sequence, allowing for non-local dependen-
cies between states and observations (Lafferty et
al. 2001). Lafferty et al (2001) showed that CRFs
outperform classification models as well as HMMs
on synthetic data and on POS tagging tasks. As for
the task of shallow parsing, CRFs also outperform
many other state-of-the-art models (Sha & Pereira
2003; McDonald et al 2005).
When the data has distinct sub-structures, mod-
els that exploit hidden state variables are advanta-
geous in learning (Matsuzaki et al 2005; Petrov
et al 2007). Sutton et al (2004) presented an
extension to CRF called dynamic conditional ran-
dom field (DCRF) model. As stated by the authors,
training a DCRF model with unobserved nodes
(hidden variables) makes their approach difficult
to optimize. In the vision community, the LD-
CRF model was recently proposed by Morency et
al. (2007), and shown to outperform CRFs, SVMs,
and HMMs for visual sequence labeling.
In this paper, we introduce the concept of latent-
dynamics for shallow parsing, showing how hid-
den states automatically learned by the model
present similar characteristics. We will also pro-
pose an improved inference technique, the BLP,
for producing the most probable label sequence in
LDCRFs.
3 Latent-Dynamic Conditional Random
Fields
The task is to learn a mapping between a sequence
of observations x = x
1
, x
2
, . . . , x
m
and a sequence
of labels y = y
1
, y
2
, . . . , y
m
. Each y
j
is a class la-
842
bel for the j?th token of a word sequence and is a
member of a set Y of possible class labels. For
each sequence, the model also assumes a vector of
hidden state variables h = {h
1
, h
2
, . . . , h
m
}, which
are not observable in the training examples.
Given the above definitions, we define a latent
conditional model as follows:
P(y|x,?) =
?
h
P(y|h, x,?)P(h|x,?), (1)
where ? are the parameters of the model. The LD-
CRF model can seem as a natural extension of the
CRF model, and the CRF model can seem as a spe-
cial case of LDCRFs employing one hidden state
for each label.
To keep training and inference efficient, we re-
strict the model to have disjointed sets of hidden
states associated with each class label. Each h
j
is
a member of a set H
y
j
of possible hidden states for
the class label y
j
. We define H, the set of all pos-
sible hidden states to be the union of all H
y
j
sets.
Since sequences which have any h
j
< H
y
j
will by
definition have P(y|x,?) = 0, we can express our
model as:
P(y|x,?) =
?
h?H
y
1
?...?H
y
m
P(h|x,?), (2)
where P(h|x,?) is defined using the usual con-
ditional random field formulation: P(h|x,?) =
exp ??f(h|x)/
?
?h
exp ??f(h|x), in which f(h|x) is
the feature vector. Given a training set consisting
of n labeled sequences (x
i
, y
i
) for i = 1 . . . n, train-
ing is performed by optimizing the objective func-
tion to learn the parameter ?
?
:
L(?) =
n
?
i=1
log P(y
i
|x
i
,?) ? R(?). (3)
The first term of this equation is the conditional
log-likelihood of the training data. The second
term is the regularizer.
4 BLP Inference on Latent Conditional
Models
For testing, given a new test sequence x, we want
to estimate the most probable label sequence (Best
Label Path), y
?
, that maximizes our conditional
model:
y
?
= argmax
y
P(y|x,?
?
). (4)
In the CRF model, y
?
can be simply searched by
using the Viterbi algorithm. However, for latent
conditional models like LDCRF, the Best Label
Path y
?
cannot directly be produced by the Viterbi
algorithm because of the incorporation of hidden
states.
In this paper, we propose an exact inference al-
gorithm, the Best Label Path inference (BLP), for
producing the most probable label sequence y
?
on
LDCRF. In the BLP schema, top-n hidden paths
HP
n
= {h
1
,h
2
. . . h
n
} over hidden states are effi-
ciently produced by using A
?
search (Hart et al,
1968), and the corresponding probabilities of hid-
den paths P(h
i
|x,?) are gained. Thereafter, based
on HP
n
, the estimated probabilities of various la-
bel paths, P(y|x,?), can be computed by summing
the probabilities of hidden paths, P(h|x,?), con-
cerning the association between hidden states and
each class label:
P(y|x,?) =
?
h: h?H
y
1
?...?H
y
m
?h?HP
n
P(h|x,?). (5)
By using the A
?
search, HP
n
can be extended in-
crementally in an efficient manner, until the algo-
rithm finds that the Best Label Path is ready, and
then the search stops and ends the BLP inference
with success. The algorithm judges that y
?
is ready
when the following condition is achieved:
P(y
1
|x,?) ? P(y
2
|x,?) +
?
h<H
y
1
?...?H
y
m
P(h|x,?), (6)
where y
1
is the most probable label sequence, and
y
2
is the second ranked label sequence estimated
by using HP
n
. It would be straightforward to prove
that y
?
= y
1
, and further search is unnecessary, be-
cause in this case, the unknown probability mass
can not change the optimal label path. The un-
known probability mass can be computed by using
?
h<H
y
1
?...?H
y
m
P(h|x,?) = 1 ?
?
h?H
y
1
?...?H
y
m
P(h|x,?). (7)
The top-n hidden paths of HP
n
produced by the
A
?
-search are exact, and the BLP inference is ex-
act. To guarantee HP
n
is exact in our BLP in-
ference, an admissible heuristic function should
be used in A
?
search (Hart et al, 1968). We use
a backward Viterbi algorithm (Viterbi, 1967) to
compute the heuristic function of the forward A
?
search:
Heu
i
(h
j
) = max
h
?
i
=h
j
?h
?
i
?HP
|h|
i
P
?
(h
?
|x,?
?
), (8)
843
where h
?
i
= h
j
represents a partial hidden path
started from the hidden state h
j
, and HP
|h|
i
rep-
resents all possible partial hidden paths from the
position i to the ending position |h| . Heu
i
(h
j
) is
an admissible heuristic function for the A
?
search
over hidden paths, therefore HP
n
is exact and BLP
inference is exact.
The BLP inference is efficient when the prob-
ability distribution among the hidden paths is in-
tensive. By combining the forward A
?
with the
backward Viterbi algorithm, the time complexity
of producing HP
n
is roughly a linear complexity
concerning its size. In practice, on the CoNLL test
data containing 2,012 sentences, the BLP infer-
ence finished in five minutes when using the fea-
ture set based on both word and POS information
(see Table 3). The memory consumption is also
relatively small, because it is an online style algo-
rithm and it is not necessary to preserve HP
n
.
In this paper, to make a comparison, we also
study the Best Hidden Path inference (BHP):
y
BHP
= argmax
y
P(h
y
|x,?
?
), (9)
where h
y
? H
y
1
? . . . ?H
y
m
. In other words, the
Best Hidden Path is the label sequence that is di-
rectly projected from the most probable hidden
path h
?
.
In (Morency et al 2007), y
?
is estimated by us-
ing the Best Point-wise Marginal Path (BMP). To
estimate the label y
j
of token j, the marginal prob-
abilities P(h
j
= a|x,?) are computed for possible
hidden states a ? H. Then, the marginal probabili-
ties are summed and the optimal label is estimated
by using the marginal probabilities.
The BLP produces y
?
while the BHP and the
BMP perform an estimation on y
?
. We will make
an experimental comparison in Section 6.
5 Analyzing Latent-Dynamics
The chunks in shallow parsing are represented with
the three labels shown in Table 1, and shallow pars-
ing is treated as a sequence labeling task with those
three labels. A challenge for most shallow parsing
approaches is to determine the concepts learned by
the model. In this section, we show how we can
analyze the latent-dynamics.
5.1 Analyzing Latent-Dynamics
In this section, we show how to analyze the charac-
teristics of the hidden states. Our goal is to find the
words characterizing a specific hidden state, and
B words beginning a chunk
I words continuing a chunk
O words being outside a chunk
Table 1: Shallow parsing labels.
then look at the selected words with their associ-
ated POS tags to determine if the LDCRF model
has learned meaningful latent-dynamics.
In the experiments reported in this section, we
did not use the features on POS tags in order to
isolate the model?s capability of learning latent dy-
namics. In other words, the model could simply
learn the dynamics of POS tags as the latent dy-
namics if the model is given the information about
POS tags. The features used in the experiments are
listed on the left side (Word Features) in Table 3.
The main idea is to look at the marginal proba-
bilities P(h
j
= a|x,?) for each word j, and select
the hidden state a
?
with the highest probability. By
counting how often a specific word selected a as
the optimal hidden state, i.e., ?(w, a), we can cre-
ate statistics about the relationship between hidden
states and words. We define relative frequency as
the number of times a specific word selected a hid-
den state while normalized by the global frequency
of this word:
RltFreq(w, h
j
) =
Freq( ?(w, h
j
) )
Freq(w)
. (10)
5.2 Learned Latent-Dynamics from CoNLL
In this subsection, we show the latent-dynamics
learned automatically from the CoNLL dataset.
The details of these experiments are presented in
the following section.
The most frequent three words corresponding to
the individual hidden states of the labels, B and O,
are shown in Table 2. As shown, the automati-
cally learned hidden states demonstrate prominent
characteristics. The extrinsic label B, which begins
a noun phrase, is automatically split into 4 sub-
categories: wh-determiners (WDT, such as ?that?)
together with wh-pronouns (WP, such as ?who?),
the determiners (DT, such as ?any, an, a?), the per-
sonal pronouns (PRP, such as ?they, we, he?), and
the singular proper nouns (NNP, such as ?Nasdaq,
Florida?) together with the plural nouns (NNS,
such as ?cities?). The results of B1 suggests that
the wh-determiners represented by ?that?, and the
wh-pronouns represented by ?who?, perform simi-
844
Labels HidStat Words POS RltFreq
B
That WDT 0.85
B1 who WP 0.49
Who WP 0.33
any DT 1.00
B2 an DT 1.00
a DT 0.98
They PRP 1.00
B3 we PRP 1.00
he PRP 1.00
Nasdaq NNP 1.00
B4 Florida NNP 0.99
cities NNS 0.99
O
But CC 0.88
O1 by IN 0.73
or IN 0.67
4.6 CD 1.00
O2 1 CD 1.00
11 CD 0.62
were VBD 0.94
O3 rose VBD 0.93
have VBP 0.92
been VBN 0.97
O4 be VB 0.94
to TO 0.92
Table 2: Latent-dynamics learned automatically by
the LDCRF model. This table shows the top three
words and their gold-standard POS tags for each
hidden states.
lar roles in modeling the dynamics in shallow pars-
ing. Further, the singular proper nouns and the
plural nouns are grouped together, suggesting that
they may perform similar roles. Moreover, we can
notice that B2 and B3 are highly consistent.
The label O is automatically split into the coordi-
nating conjunctions (CC) together with the prepo-
sitions (IN) indexed by O1, the cardinal numbers
(CD) indexed by O2, the past tense verbs (VBD)
together with the personal verbs (VBP) indexed by
O3, and another sub-category, O4. From the results
we can find that gold-standard POS tags may not
be adequate in modeling latent-dynamics in shal-
low parsing, as we can notice that three hidden
states out of four (O1, O3 and O4) contains relat-
ing but different gold-standard POS tags.
6 Experiments
Following previous studies on shallow parsing, our
experiments are performed on the CoNLL 2000
Word Features:
{w
i?2
, w
i?1
, w
i
, w
i+1
, w
i+2
, w
i?1
w
i
, w
i
w
i+1
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
POS Features:
{t
i?1
, t
i
, t
i+1
, t
i?2
t
i?1
, t
i?1
t
i
, t
i
t
i+1
, t
i+1
t
i+2
,
t
i?2
t
i?1
t
i
, t
i?1
t
i
t
i+1
, t
i
t
i+1
t
i+2
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
Table 3: Feature templates used in the experi-
ments. w
i
is the current word; t
i
is current POS
tag; and h
i
is the current hidden state (for the case
of latent models) or the current label (for the case
of conventional models).
data set (Sang & Buchholz 2000; Ramshow &
Marcus 1995). The training set consists of 8,936
sentences, and the test set consists of 2,012 sen-
tences. The standard evaluation metrics for this
task are precision p (the fraction of output chunks
matching the reference chunks), recall r (the frac-
tion of reference chunks returned), and the F-
measure given by F = 2pr/(p + r).
6.1 LDCRF for Shallow Parsing
We implemented LDCRFs in C++, and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions.
We employ similar predicate sets defined in Sha
& Pereira (2003). We follow them in using predi-
cates that depend on words as well as POS tags in
the neighborhood of a given position, taking into
account only those 417,835 features which occur
at least once in the training data. The features are
listed in Table 3.
As for numerical optimization (Malouf 2002;
Wallach 2002), we performed gradient decent with
the Limited-Memory BFGS (L-BFGS) optimiza-
tion technique (Nocedal & Wright 1999). L-BFGS
is a second-order Quasi-Newton method that nu-
merically estimates the curvature from previous
gradients and updates. With no requirement on
specialized Hessian approximation, L-BFGS can
handle large-scale problems in an efficient manner.
We implemented an L-BFGS optimizer in C++ by
modifying the OWLQN package (Andrew & Gao
2007) developed by Galen Andrew. In our exper-
iments, storing 10 pairs of previous gradients for
the approximation of the function?s inverse Hes-
sian worked well, making the amount of the ex-
tra memory required modest. Using more pre-
vious gradients will probably decrease the num-
845
ber of iterations required to reach convergence,
but would increase memory requirements signifi-
cantly. To make a comparison, we also employed
the Conjugate-Gradient (CG) optimization algo-
rithm. For details of CG, see Shewchuk (1994).
Since the objective function of the LDCRF
model is non-convex, it is suggested to use the ran-
dom initialization of parameters for the training.
To reduce overfitting, we employed an L
2
Gaus-
sian weight prior (Chen & Rosenfeld 1999). Dur-
ing training and validation, we varied the number
of hidden states per label (from 2 to 6 states per
label), and also varied the L
2
-regularization term
(with values 10
k
, k from -3 to 3). Our experiments
suggested that using 4 or 5 hidden states per label
for the shallow parser is a viable compromise be-
tween accuracy and efficiency.
7 Results and Discussion
7.1 Performance on Word Features
As discussed in Section 4, it is preferred to not
use the features on POS tags in order to isolate
the model?s capability of learning latent dynam-
ics. In this sub-section, we use pure word fea-
tures with their counts above 10 in the training data
to perform experimental comparisons among dif-
ferent inference algorithms on LDCRFs, including
BLP, BHP, and existing BMP.
Since the CRF model is one of the success-
ful models in sequential labeling tasks (Lafferty et
al. 2001; Sha & Pereira 2003; McDonald et al
2005), in this section, we also compare LDCRFs
with CRFs. We tried to make experimental results
more comparable between LDCRF and CRF mod-
els, and have therefore employed the same fea-
tures set, optimizer and fine-tuning strategy be-
tween LDCRF and CRF models.
The experimental results are shown in Table 4.
In the table, Acc. signifies ?label accuracy?, which
is useful for the significance test in the follow-
ing sub-section. As shown, LDCRF-BLP outper-
forms LDCRF-BHP and LDCRF-BMP, suggesting
that BLP inference
1
is superior. The superiority
of BLP is statistically significant, which will be
shown in next sub-section. On the other side, all
the LDCRF models outperform the CRF model. In
particular, the gap between LDCRF-BLP and CRF
is 1.53 percent.
1
In practice, for efficiency, we approximated the BLP on a
few sentences by limiting the number of search steps.
Models: WF Acc. Pre. Rec. F
1
LDCRF-BLP 97.01 90.33 88.91 89.61
LDCRF-BHP 96.52 90.26 88.21 89.22
LDCRF-BMP 97.26 89.83 89.06 89.44
CRF 96.11 88.12 88.03 88.08
Table 4: Experimental comparisons among differ-
ent inference algorithms on LDCRFs, and the per-
formance of CRFs using the same feature set on
pure word features. The BLP inference outper-
forms the BHP and BMP inference. LDCRFs out-
perform CRFs.
Models F
1
Gap Acc. Gap Sig.
BLP vs. BHP 0.39 0.49 1e-10
BLP vs. CRF 1.53 0.90 5e-13
Table 5: The significance tests. LDCRF-BLP is
significantly more accurate than LDCRF-BHP and
CRFs.
7.2 Labeling Accuracy and Significance Test
As shown in Table 4, the accuracy rate for individ-
ual labeling decisions is over-optimistic as a mea-
sure for shallow parsing. Nevertheless, since test-
ing the significance of shallow parsers? F-measures
is tricky, individual labeling accuracy provides a
more convenient basis for statistical significance
tests (Sha & Pereira 2003). One such test is the
McNemar test on paired observations (Gillick &
Cox 1989). As shown in Table 5, for the LD-
CRF model, the BLP inference schema is sta-
tistically more accurate than the BHP inference
schema. Also, Evaluations show that the McNe-
mar?s value on labeling disagreement between the
LDCRF-BLP and CRF models is 5e-13, suggest-
ing that LDCRF-BLP is significantly more accu-
rate than CRFs.
On the other hand, the accuracy rate of BMP in-
ference is a special case. Since the BMP inference
is essentially an accuracy-first inference schema,
the accuracy rate and the F-measure have a differ-
ent relation in BMP. As we can see, the individual
labeling accuracy achieved by the LDCRF-BMP
model is as high as 97.26%, but its F-measure is
still lower than LDCRF-BLP.
7.3 Convergence Speed
It would be interesting to compare the convergence
speed between the objective loss function of LD-
CRFs and CRFs. We apply the L-BFGS optimiza-
846
 150
 200
 250
 300
 350
 400
 450
 500
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
LDCRF
CRF
Figure 2: The value of the penalized loss based on
the number of iterations: LDCRFs vs. CRFs.
 160
 180
 200
 220
 240
 260
 280
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
L-BFGS
CG
Figure 3: Training the LDCRF model: L-BFGS
vs. CG.
tion algorithm to optimize the loss function of LD-
CRF and CRF models, making a comparison be-
tween them. We find that the iterations required
for the convergence of LDCRFs is less than for
CRFs (see Figure 2). Normally, the LDCRF model
arrives at the plateau of convergence in 120-150
iterations, while CRFs require 210-240 iterations.
When we replace the L-BFGS optimizer by the CG
optimization algorithm, we observed as well that
LDCRF converges faster on iteration numbers than
CRF does.
On the contrary, however, the time cost of the
LDCRF model in each iteration is higher than the
CRF model, because of the incorporation of hid-
den states. The time cost of the LDCRF model
in each iteration is roughly a quadratic increase
concerning the increase of the number of hidden
states. Therefore, though the LDCRF model re-
quires less passes for the convergence, it is practi-
cally slower than the CRF model. Improving the
scalability of the LDCRF model would be a inter-
esting topic in the future.
Furthermore, we make a comparison between
Models: WF+POS Pre. Rec. F
1
LDCRF-BLP 94.65 94.03 94.34
CRF
N/A N/A 93.6
(Vishwanathan et al 06)
CRF
94.57 94.00 94.29
(McDonald et al 05)
Voted perceptron
N/A N/A 93.53
(Collins 02)
Generalized Winnow
93.80 93.99 93.89
(Zhang et al 02)
SVM combination
94.15 94.29 94.22
(Kudo & Matsumoto 01)
Memo. classifier
93.63 92.89 93.26
(Sang 00)
Table 6: Performance of the LDCRF-BLP model,
and the comparison with CRFs and other success-
ful approaches. In this table, all the systems have
employed POS features.
the L-BFGS and the CG optimizer for LDCRFs.
We observe that the L-BFGS optimizer is slightly
faster than CG on LDCRFs (see Figure 3), which
echoes the comparison between the L-BFGS and
the CG optimizing technique on the CRF model
(Sha & Pereira 2003).
7.4 Comparisons to Other Systems with POS
Features
Performance of the LDCRF-BLP model and some
of the best results reported previously are summa-
rized in Table 6. Our LDCRF model achieved
comparable performance to those best reported
systems in terms of the F-measure.
McDonald et al (2005) achieved an F-measure
of 94.29% by using a CRF model. By employing a
multi-model combination approach, Kudo & Mat-
sumoto (2001) also achieved a good performance.
They use a combination of 8 kernel SVMs with
a heuristic voting strategy. An advantage of LD-
CRFs over max-margin based approaches is that
LDCRFs can output N-best label sequences and
their probabilities using efficient marginalization
operations, which can be used for other compo-
nents in an information extraction system.
8 Conclusions and Future Work
In this paper, we have shown that automatic model-
ing on ?latent-dynamics? can be useful in shallow
parsing. By analyzing the automatically learned
847
hidden states, we showed how LDCRFs can natu-
rally learn latent-dynamics in shallow parsing.
We proposed an improved inference algorithm,
the BLP, for LDCRFs. We performed experiments
using the CoNLL data, and showed how the BLP
inference outperforms existing inference engines.
When further employing POS features as other
systems did, the performance of the LDCRF-BLP
model is comparable to those best reported results.
The LDCRF model demonstrates a significant ad-
vantage over other models on pure word features
in this paper. We expect it to be particularly useful
in the real-world tasks without rich features.
The latent conditional model handles latent-
dynamics naturally, and can be easily extended to
other labeling tasks. Also, the BLP inference algo-
rithm can be extended to other latent conditional
models for producing optimal label sequences. As
a future work, we plan to further speed up the BLP
algorithm.
Acknowledgments
Many thanks to Yoshimasa Tsuruoka for helpful
discussions on the experiments and paper-writing.
This research was partially supported by Grant-
in-Aid for Specially Promoted Research 18002007
(MEXT, Japan). The work at the USC Institute for
Creative Technology was sponsored by the U.S.
Army Research, Development, and Engineering
Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
References
Abney, S. 1991. Parsing by chunks. In R. Berwick, S. Ab-
ney, and C. Tenny, editors, Principle-based Parsing. Kluwer
Academic Publishers.
Abney, S.; Schapire, R. E. and Singer, Y. 1999. Boosting
applied to tagging and PP attachment. In Proc. EMNLP/VLC-
99.
Andrew, G. and Gao, J. 2007. Scalable training of L1-
regularized log-linear models. In Proc. ICML-07.
Bikel, D. M.; Schwartz, R. L. and Weischedel, R. M. 1999.
An algorithm that learns what?s in a name. Machine Learning,
34: 211-231.
Chen, S. F. and Rosenfeld, R. 1999. A Gaussian prior
for smooth-ing maximum entropy models. Technical Report
CMU-CS-99-108, CMU.
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with perceptron
algorithms. In Proc. EMNLP-02.
Freitag, D. and McCallum, A. 2000. Information extrac-
tion with HMM structures learned by stochastic optimization.
In Proc. AAAI-00.
Gillick, L. and Cox, S. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Interna-
tional Conference on Acoustics Speech and Signal Process-
ing, v1, pages 532-535.
Hart, P.E.; Nilsson, N.J.; and Raphael, B. 1968. A formal
basis for the heuristic determination of minimum cost path.
IEEE Trans. On System Science and Cybernetics, SSC-4(2):
100-107.
Kudo, T. and Matsumoto, Y. 2001. Chunking with support
vector machines. In Proc. NAACL-01.
Kupiec, J. 1992. Robust part-of-speech tagging using a
hidden Markov model. Computer Speech and Language.
6:225-242.
Lafferty, J.; McCallum, A. and Pereira, F. 2001. Condi-
tional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML-01, pages 282-289.
Malouf, R. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proc. CoNLL-02.
Matsuzaki, T.; Miyao Y. and Tsujii, J. 2005. Probabilistic
CFG with Latent Annotations. In Proc. ACL-05.
McDonald, R.; Crammer, K. and Pereira, F. 2005. Flexible
Text Segmentation with Structured Multilabel Classification.
In Proc. HLT/EMNLP-05, pages 987- 994.
Morency, L.P.; Quattoni, A. and Darrell, T. 2007. Latent-
Dynamic Discriminative Models for Continuous Gesture
Recognition. In Proc. CVPR-07, pages 1- 8.
Nocedal, J. and Wright, S. J. 1999. Numerical Optimiza-
tion. Springer.
Petrov, S.; Pauls, A.; and Klein, D. 2007. Discriminative
log-linear grammars with latent variables. In Proc. NIPS-07.
Punyakanok, V. and Roth, D. 2001. The use of classifiers
in sequential inference. In Proc. NIPS-01, pages 995-1001.
MIT Press.
Ramshaw, L. A. and Marcus, M. P. 1995. Text chunking
using transformation-based learning. In Proc. Third Work-
shop on Very Large Corpora. In Proc. ACL-95.
Ratnaparkhi, A. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP-96.
Sang, E.F.T.K. 2000. Noun Phrase Representation by Sys-
tem Combination. In Proc. ANLP/NAACL-00.
Sang, E.F.T.K and Buchholz, S. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proc. CoNLL-00,
pages 127-132.
Sha, F. and Pereira, F. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proc. HLT/NAACL-03.
Shewchuk, J. R. 1994. An introduction to the
conjugate gradient method without the agonizing pain.
http://www.2.cs.cmu.edu/jrs/jrspapers.html/#cg.
Sutton, C.; Rohanimanesh, K. and McCallum, A. 2004.
Dynamic conditional random fields: Factorized probabilistic
models for labeling and segmenting sequence data. In Proc.
ICML-04.
Viterbi, A.J. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. IEEE
Transactions on Information Theory. 13(2):260-269.
Vishwanathan, S.; Schraudolph, N. N.; Schmidt, M.W. and
Murphy, K. 2006. Accelerated training of conditional random
fields with stochastic meta-descent. In Proc. ICML-06.
Wallach, H. 2002. Efficient training of conditional random
fields. In Proc. 6th Annual CLUK Research Colloquium.
Zhang, T.; Damerau, F. and Johnson, D. 2002. Text chunk-
ing based on a generalization of winnow. Journal of Machine
Learning Research, 2:615-637.
848
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 772?780,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Sequential Labeling with Latent Variables:
An Exact Inference Algorithm and Its Efficient Approximation
Xu Sun? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Centre for Text Mining, Manchester, UK
{sunxu, tsujii}@is.s.u-tokyo.ac.jp
Abstract
Latent conditional models have become
popular recently in both natural language
processing and vision processing commu-
nities. However, establishing an effective
and efficient inference method on latent
conditional models remains a question. In
this paper, we describe the latent-dynamic
inference (LDI), which is able to produce
the optimal label sequence on latent con-
ditional models by using efficient search
strategy and dynamic programming. Fur-
thermore, we describe a straightforward
solution on approximating the LDI, and
show that the approximated LDI performs
as well as the exact LDI, while the speed is
much faster. Our experiments demonstrate
that the proposed inference algorithm out-
performs existing inference methods on
a variety of natural language processing
tasks.
1 Introduction
When data have distinct sub-structures, mod-
els exploiting latent variables are advantageous
in learning (Matsuzaki et al, 2005; Petrov and
Klein, 2007; Blunsom et al, 2008). Actu-
ally, discriminative probabilistic latent variable
models (DPLVMs) have recently become popu-
lar choices for performing a variety of tasks with
sub-structures, e.g., vision recognition (Morency
et al, 2007), syntactic parsing (Petrov and Klein,
2008), and syntactic chunking (Sun et al, 2008).
Morency et al (2007) demonstrated that DPLVM
models could efficiently learn sub-structures of
natural problems, and outperform several widely-
used conventional models, e.g., support vector ma-
chines (SVMs), conditional random fields (CRFs)
and hidden Markov models (HMMs). Petrov and
Klein (2008) reported on a syntactic parsing task
that DPLVM models can learn more compact and
accurate grammars than the conventional tech-
niques without latent variables. The effectiveness
of DPLVMs was also shown on a syntactic chunk-
ing task by Sun et al (2008).
DPLVMs outperform conventional learning
models, as described in the aforementioned pub-
lications. However, inferences on the latent condi-
tional models are remaining problems. In conven-
tional models such as CRFs, the optimal label path
can be efficiently obtained by the dynamic pro-
gramming. However, for latent conditional mod-
els such as DPLVMs, the inference is not straight-
forward because of the inclusion of latent vari-
ables.
In this paper, we propose a new inference al-
gorithm, latent dynamic inference (LDI), by sys-
tematically combining an efficient search strategy
with the dynamic programming. The LDI is an
exact inference method producing the most prob-
able label sequence. In addition, we also propose
an approximated LDI algorithm for faster speed.
We show that the approximated LDI performs as
well as the exact one. We will also discuss a
post-processing method for the LDI algorithm: the
minimum bayesian risk reranking.
The subsequent section describes an overview
of DPLVM models. We discuss the probability
distribution of DPLVM models, and present the
LDI inference in Section 3. Finally, we report
experimental results and begin our discussions in
Section 4 and Section 5.
772
y1 y2 ym
xmx2x1
h1 h2 hm
xmx2x1
ymy2y1
CRF DPLVM
Figure 1: Comparison between CRF models and
DPLVM models on the training stage. x represents
the observation sequence, y represents labels and
h represents the latent variables assigned to the la-
bels. Note that only the white circles are observed
variables. Also, only the links with the current ob-
servations are shown, but for both models, long
range dependencies are possible.
2 Discriminative Probabilistic Latent
Variable Models
Given the training data, the task is to learn a map-
ping between a sequence of observations x =
x1, x2, . . . , xm and a sequence of labels y =
y1, y2, . . . , ym. Each yj is a class label for the j?th
token of a word sequence, and is a member of a
set Y of possible class labels. For each sequence,
the model also assumes a sequence of latent vari-
ables h = h1, h2, . . . , hm, which is unobservable
in training examples.
The DPLVM model is defined as follows
(Morency et al, 2007):
P (y|x,?) =
?
h
P (y|h,x,?)P (h|x,?), (1)
where ? represents the parameter vector of the
model. DPLVM models can be seen as a natural
extension of CRF models, and CRF models can
be seen as a special case of DPLVMs that employ
only one latent variable for each label.
To make the training and inference efficient, the
model is restricted to have disjointed sets of latent
variables associated with each class label. Each
hj is a member in a set Hyj of possible latent vari-
ables for the class label yj . H is defined as the set
of all possible latent variables, i.e., the union of all
Hyj sets. Since sequences which have any hj /?
Hyj will by definition have P (y|hj ,x,?) = 0,
the model can be further defined as:
P (y|x,?) =
?
h?Hy1?...?Hym
P (h|x,?), (2)
where P (h|x,?) is defined by the usual condi-
tional random field formulation:
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
in which f(h,x) is a feature vector. Given a train-
ing set consisting of n labeled sequences, (xi,yi),
for i = 1 . . . n, parameter estimation is performed
by optimizing the objective function,
L(?) =
n?
i=1
logP (yi|xi,?)?R(?). (4)
The first term of this equation represents a condi-
tional log-likelihood of a training data. The sec-
ond term is a regularizer that is used for reducing
overfitting in parameter estimation.
3 Latent-Dynamic Inference
On latent conditional models, marginalizing la-
tent paths exactly for producing the optimal la-
bel path is a computationally expensive prob-
lem. Nevertheless, we had an interesting observa-
tion on DPLVM models that they normally had a
highly concentrated probability mass, i.e., the ma-
jor probability are distributed on top-n ranked la-
tent paths.
Figure 2 shows the probability distribution of
a DPLVM model using a L2 regularizer with the
variance ?2 = 1.0. As can be seen, the probabil-
ity distribution is highly concentrated, e.g., 90%
of the probability is distributed on top-800 latent
paths.
Based on this observation, we propose an infer-
ence algorithm for DPLVMs by efficiently com-
bining search and dynamic programming.
3.1 LDI Inference
In the inference stage, given a test sequence x, we
want to find the most probable label sequence, y?:
y? = argmaxyP (y|x,??). (5)
For latent conditional models like DPLVMs, the
y? cannot directly be produced by the Viterbi
algorithm because of the incorporation of latent
variables.
In this section, we describe an exact inference
algorithm, the latent-dynamic inference (LDI),
for producing the optimal label sequence y? on
DPLVMs (see Figure 3). In short, the algorithm
773
 0
 20
 40
 60
 80
 100
0.4K 0.8K 1.2K 1.6K 2K
To
p-
n 
Pr
ob
ab
ilit
y 
M
as
s 
(%
)
n
Figure 2: The probability mass distribution of la-
tent conditional models on a NP-chunking task.
The horizontal line represents the n of top-n latent
paths. The vertical line represents the probability
mass of the top-n latent paths.
generates the best latent paths in the order of their
probabilities. Then it maps each of these to its as-
sociated label paths and uses a method to compute
their exact probabilities. It can continue to gener-
ate the next best latent path and the associated la-
bel path until there is not enough probability mass
left to beat the best label path.
In detail, an A? search algorithm1 (Hart et al,
1968) with a Viterbi heuristic function is adopted
to produce top-n latent paths, h1,h2, . . .hn. In
addition, a forward-backward-style algorithm is
used to compute the exact probabilities of their
corresponding label paths, y1,y2, . . .yn. The
model then tries to determine the optimal label
path based on the top-n statistics, without enumer-
ating the remaining low-probability paths, which
could be exponentially enormous.
The optimal label path y? is ready when the fol-
lowing ?exact-condition? is achieved:
P (y1|x,?)?(1?
?
yk?LPn
P (yk|x,?)) ? 0, (6)
where y1 is the most probable label sequence
in current stage. It is straightforward to prove
that y? = y1, and further search is unnecessary.
This is because the remaining probability mass,
1??yk?LPn P (yk|x,?), cannot beat the currentoptimal label path in this case.
1A? search and its variants, like beam-search, are widely
used in statistical machine translation. Compared to other
search techniques, an interesting point of A? search is that it
can produce top-n results one-by-one in an efficient manner.
Definition:
Proj(h) = y ?? hj ? Hyj for j = 1 . . .m;
P (h) = P (h|x,?);
P (y) = P (y|x,?).
Input:
weight vector ?, and feature vector F (h,x).
Initialization:
Gap = ?1; n = 0; P (y?) = 0; LP0 = ?.
Algorithm:
while Gap < 0 do
n = n+ 1
hn = HeapPop[?, F (h,x)]
yn = Proj(hn)
if yn /? LPn?1 then
P (yn) = DynamicProg
?
h:Proj(h)=yn P (h)
LPn = LPn?1 ? {yn}
if P (yn) > P (y?) then
y? = yn
Gap = P (y?)?(1??yk?LPn P (yk))else
LPn = LPn?1
Output:
the most probable label sequence y?.
Figure 3: The exact LDI inference for latent condi-
tional models. In the algorithm, HeapPop means
popping the next hypothesis from the A? heap; By
the definition of the A? search, this hypothesis (on
the top of the heap) should be the latent path with
maximum probability in current stage.
3.2 Implementation Issues
We have presented the framework of the LDI in-
ference. Here, we describe the details on imple-
menting its two important components: designing
the heuristic function, and an efficient method to
compute the probabilities of label path.
As described, the A? search can produce top-n
results one-by-one using a heuristic function (the
backward term). In the implementation, we use
the Viterbi algorithm (Viterbi, 1967) to compute
the admissible heuristic function for the forward-
style A? search:
Heui(hj) = max
h?i=hj?h??HP
|h|
i
P ?(h? |x,??), (7)
where h?i = hj represents a partial latent path
started from the latent variable hj . HP|h|i rep-
resents all possible partial latent paths from the
774
position i to the ending position, |h|. As de-
scribed in the Viterbi algorithm, the backward
term, Heui(hj), can be efficiently computed by
using dynamic programming to reuse the terms
(e.g., Heui+1(hj)) in previous steps. Because this
Viterbi heuristic is quite good in practice, this way
we can produce the exact top-n latent paths effi-
ciently (see efficiency comparisons in Section 5),
even though the original problem is NP-hard.
The probability of a label path, P (yn) in Fig-
ure 3, can be efficiently computed by a forward-
backward algorithm with a restriction on the target
label path:
P (y|x,?) =
?
h?Hy1?...?Hym
P (h|x,?). (8)
3.3 An Approximated Version of the LDI
By simply setting a threshold value on the search
step, n, we can approximate the LDI, i.e., LDI-
Approximation (LDI-A). This is a quite straight-
forward method for approximating the LDI. In
fact, we have also tried other methods for approx-
imation. Intuitively, one alternative method is to
design an approximated ?exact condition? by us-
ing a factor, ?, to estimate the distribution of the
remaining probability:
P (y1|x,?)??(1?
?
yk?LPn
P (yk|x,?)) ? 0. (9)
For example, if we believe that at most 50% of the
unknown probability, 1 ??yk?LPn P (yk|x,?),can be distributed on a single label path, we can
set ? = 0.5 to make a loose condition to stop the
inference. At first glance, this seems to be quite
natural. However, when we compared this alter-
native method with the aforementioned approxi-
mation on search steps, we found that it worked
worse than the latter, in terms of performance and
speed. Therefore, we focus on the approximation
on search steps in this paper.
3.4 Comparison with Existing Inference
Methods
In Matsuzaki et al (2005), the Best Hidden Path
inference (BHP) was used:
yBHP = argmax
y
P (hy|x,??), (10)
where hy ? Hy1 ? . . .?Hym . In other words,
the Best Hidden Path is the label sequence
which is directly projected from the optimal la-
tent path h?. The BHP inference can be seen
as a special case of the LDI, which replaces the
marginalization-operation over latent paths with
the max-operation.
In Morency et al (2007), y? is estimated by the
Best Point-wise Marginal Path (BMP) inference.
To estimate the label yj of token j, the marginal
probabilities P (hj = a|x,?) are computed for
all possible latent variables a ? H. Then the
marginal probabilities are summed up according
to the disjoint sets of latent variables Hyj and the
optimal label is estimated by the marginal proba-
bilities at each position i:
yBMP (i) = argmax
yi?Y
P (yi|x,??), (11)
where
P (yi = a|x,?) =
?
h?Ha P (h|x,?)?
h P (h|x,?)
. (12)
Although the motivation is similar, the exact
LDI (LDI-E) inference described in this paper is a
different algorithm compared to the BLP inference
(Sun et al, 2008). For example, during the search,
the LDI-E is able to compute the exact probability
of a label path by using a restricted version of the
forward-backward algorithm, also, the exact con-
dition is different accordingly. Moreover, in this
paper, we more focus on how to approximate the
LDI inference with high performance.
The LDI-E produces y? while the LDI-A, the
BHP and the BMP perform estimation on y?. We
will compare them via experiments in Section 4.
4 Experiments
In this section, we choose Bio-NER and NP-
chunking tasks for experiments. First, we describe
the implementations and settings.
We implemented DPLVMs by extending the
HCRF library developed by Morency et al (2007).
We added a Limited-Memory BFGS optimizer
(L-BFGS) (Nocedal and Wright, 1999), and re-
implemented the code on training and inference
for higher efficiency. To reduce overfitting, we
employed a Gaussian prior (Chen and Rosenfeld,
1999). We varied the the variance of the Gaussian
prior (with values 10k, k from -3 to 3), and we
found that ?2 = 1.0 is optimal for DPLVMs on
the development data, and used it throughout the
experiments in this section.
775
The training stage was kept the same as
Morency et al (2007). In other words, there
is no need to change the conventional parameter
estimation method on DPLVM models for adapt-
ing the various inference algorithms in this paper.
For more information on training DPLVMs, refer
to Morency et al (2007) and Petrov and Klein
(2008).
Since the CRF model is one of the most success-
ful models in sequential labeling tasks (Lafferty et
al., 2001; Sha and Pereira, 2003), in this paper, we
choosed CRFs as a baseline model for the compar-
ison. Note that the feature sets were kept the same
in DPLVMs and CRFs. Also, the optimizer and
fine tuning strategy were kept the same.
4.1 BioNLP/NLPBA-2004 Shared Task
(Bio-NER)
Our first experiment used the data from the
BioNLP/NLPBA-2004 shared task. It is a biomed-
ical named-entity recognition task on the GENIA
corpus (Kim et al, 2004). Named entity recogni-
tion aims to identify and classify technical terms
in a given domain (here, molecular biology) that
refer to concepts of interest to domain experts.
The training set consists of 2,000 abstracts from
MEDLINE; and the evaluation set consists of 404
abstracts from MEDLINE. We divided the origi-
nal training set into 1,800 abstracts for the training
data and 200 abstracts for the development data.
The task adopts the BIO encoding scheme, i.e.,
B-x for words beginning an entity x, I-x for
words continuing an entity x, and O for words be-
ing outside of all entities. The Bio-NER task con-
tains 5 different named entities with 11 BIO en-
coding labels.
The standard evaluation metrics for this task are
precision p (the fraction of output entities match-
ing the reference entities), recall r (the fraction
of reference entities returned), and the F-measure
given by F = 2pr/(p+ r).
Following Okanohara et al (2006), we used
word features, POS features and orthography fea-
tures (prefix, postfix, uppercase/lowercase, etc.),
as listed in Table 1. However, their globally depen-
dent features, like preceding-entity features, were
not used in our system. Also, to speed up the
training, features that appeared rarely in the train-
ing data were removed. For DPLVM models, we
tuned the number of latent variables per label from
2 to 5 on preliminary experiments, and used the
Word Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi,
wiwi+1}
?{hi, hi?1hi}
POS Features:
{ti?2, ti?1, ti, ti+1, ti+2, ti?2ti?1, ti?1ti,
titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1,
titi+1ti+2}
?{hi, hi?1hi}
Orth. Features:
{oi?2, oi?1, oi, oi+1, oi+2, oi?2oi?1, oi?1oi,
oioi+1, oi+1oi+2}
?{hi, hi?1hi}
Table 1: Feature templates used in the Bio-NER
experiments. wi is the current word, ti is the cur-
rent POS tag, oi is the orthography mode of the
current word, and hi is the current latent variable
(for the case of latent models) or the current label
(for the case of conventional models). No globally
dependent features were used; also, no external re-
sources were used.
Word Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi,
wiwi+1}
?{hi, hi?1hi}
Table 2: Feature templates used in the NP-
chunking experiments. wi and hi are defined fol-
lowing Table 1.
number 4.
Two sets of experiments were performed. First,
on the development data, the value of n (the search
step, see Figure 3 for its definition) was varied in
the LDI inference; the corresponding F-measure,
exactitude (the fraction of sentences that achieved
the exact condition, Eq. 6), #latent-path (num-
ber of latent paths that have been searched), and
inference-time were measured. Second, the n
tuned on the development data was employed for
the LDI on the test data, and experimental com-
parisons with the existing inference methods, the
BHP and the BMP, were made.
4.2 NP-Chunking Task
On the Bio-NER task, we have studied the LDI
on a relatively rich feature-set, including word
features, POS features and orthographic features.
However, in practice, there are many tasks with
776
Models S.A. Pre. Rec. F1 Time
LDI-A 40.64 68.34 66.50 67.41 0.4K s
LDI-E 40.76 68.36 66.45 67.39 4K s
BMP 39.10 65.85 66.49 66.16 0.3K s
BHP 39.93 67.60 65.46 66.51 0.1K s
CRF 37.44 63.69 64.66 64.17 0.1K s
Table 3: On the test data of the Bio-NER task, ex-
perimental comparisons among various inference
algorithms on DPLVMs, and the performance of
CRFs. S.A. signifies sentence accuracy. As can
be seen, at a much lower cost, the LDI-A (A signi-
fies approximation) performed slightly better than
the LDI-E (E signifies exact).
only poor features available. For example, in POS-
tagging task and Chinese/Japanese word segmen-
tation task, there are only word features available.
For this reason, it is necessary to check the perfor-
mance of the LDI on poor feature-set. We chose
another popular task, the NP-chunking, for this
study. Here, we used only poor feature-set, i.e.,
feature templates that depend only on words (see
Table 2 for details), taking into account 200K fea-
tures. No external resources were used.
The NP-chunking data was extracted from the
training/test data of the CoNLL-2000 shallow-
parsing shared task (Sang and Buchholz, 2000). In
this task, the non-recursive cores of noun phrases
called base NPs are identified. The training set
consists of 8,936 sentences, and the test set con-
sists of 2,012 sentences. Our preliminary exper-
iments in this task suggested the use of 5 latent
variables for each label on latent models.
5 Results and Discussions
5.1 Bio-NER
Figure 4 shows the F-measure, exactitude, #latent-
path and inference inference time of the DPLVM-
LDI model, against the parameter n (the search
step, see Table 3), on the development dataset. As
can be seen, there was a dramatic climbing curve
on the F-measure, from 68.78% to 69.73%, when
we increased the number of the search step from
1 to 30. When n = 30, the F-measure has al-
ready reached its plateau, with the exactitude of
83.0%, and the inference time of 80 seconds. In
other words, the F-measure approached its plateau
when n went to 30, with a high exactitude and a
low inference time.
68
69
70
0K 2K 4K 6K 8K 10K
F-
m
ea
su
re
(%
)
65
70
75
80
85
90
95
0K 2K 4K 6K 8K 10K
Ex
ac
tit
ud
e(%
)
0
100
200
300
400
500
600
700
0K 2K 4K 6K 8K 10K
#l
at
en
t-p
at
h
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0K 2K 4K 6K 8K 10K
Ti
m
e(K
s)
n
68
69
70
0 50 100 150 200 250
65
70
75
80
85
90
95
0 50 100 150 200 250
0
100
200
300
400
500
600
0 50 100 150 200 250
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0 50 100 150 200 250
n
Figure 4: (Left) F-measure, exactitude, #latent-
path (averaged number of latent paths being
searched), and inference time of the DPLVM-LDI
model, against the parameter n, on the develop-
ment dataset of the Bio-NER task. (Right) En-
largement of the beginning portion of the left fig-
ures. As can be seen, the curve of the F-measure
approached its plateau when n went to 30, with a
high exactitude and a low inference time.
Our significance test based on McNemar?s test
(Gillick and Cox, 1989) shows that the LDI with
n = 30 was significantly more accurate (P <
0.01) than the BHP inference, while the inference
time was at a comparable level. Further growth
of n after the beginning point of the plateau in-
creases the inference time linearly (roughly), but
achieved only very marginal improvement on F-
measure. This suggests that the LDI inference can
be approximated aggressively by stopping the in-
ference within a small number of search steps, n.
This can achieve high efficiency, without an obvi-
ous degradation on the performance.
Table 3 shows the experimental comparisons
among the LDI-Approximation, the LDI-Exact
(here, exact means the n is big enough, e.g., n =
10K), the BMP, and the BHP on DPLVM mod-
777
Models S.A. Pre. Rec. F1 Time
LDI-A 60.98 91.76 90.59 91.17 42 s
LDI-E 60.88 91.72 90.61 91.16 1K s
BHP 59.34 91.54 90.30 90.91 25 s
CRF 58.37 90.92 90.33 90.63 18 s
Table 4: Experimental comparisons among differ-
ent inference algorithms on DPLVMs, and the per-
formance of CRFs using the same feature set on
the word features.
els. The baseline was the CRF model with the
same feature set. On the LDI-A, the parameter n
tuned on the development data was employed, i.e.,
n = 30.
To our surprise, the LDI-A performed slightly
better than the LDI-E even though the perfor-
mance difference was marginal. We expected that
LDI-A would perform worse than the LDI-E be-
cause LDI-A uses the aggressive approximation
for faster speed. We have not found the exact
cause of this interesting phenomenon, but remov-
ing latent paths with low probabilities may resem-
ble the strategy of pruning features with low fre-
quency in the training phase. Further analysis is
required in the future.
The LDI-A significantly outperformed the BHP
and the BMP, with a comparable inference time.
Also, all models of DPLVMs significantly outper-
formed CRFs.
5.2 NP-Chunking
As can be seen in Figure 5, compared to Figure 4
of the Bio-NER task, very similar curves were ob-
served in the NP-chunking task. It is interesting
because the tasks are different, and their feature
sets are very different.
The F-measure reached its plateau when n was
around 30, with a fast inference speed. This
echoes the experimental results on the Bio-NER
task. Moreover, as can be seen in Table 4, at a
much lower cost on inference time, the LDI-A per-
formed as well as the LDI-E. The LDI-A outper-
forms the BHP inference. All the DPLVM mod-
els outperformed CRFs. The experimental results
demonstrate that the LDI also works well on poor
feature-set.
89
89.2
89.4
89.6
89.8
0K 2K 4K 6K 8K 10K
F-
m
ea
su
re
(%
)
65
70
75
80
85
90
95
0K 2K 4K 6K 8K 10K
Ex
ac
tit
ud
e(%
)
0
200
400
600
800
0K 2K 4K 6K 8K 10K
#l
at
en
t-p
at
h
0
0.2
0.4
0.6
0.8
0K 2K 4K 6K 8K 10K
Ti
m
e(K
s)
n
89
89.2
89.4
89.6
89.8
0 50 100 150 200 250
65
70
75
80
85
90
95
0 50 100 150 200 250
0
200
400
600
800
0 50 100 150 200 250
0
0.2
0.4
0.6
0.8
0 50 100 150 200 250
n
Figure 5: (Left) F-measure, exactitude, #latent-
path, and inference time of the DPLVM-LDI
model against the parameter n on the NP-
chunking development dataset. (Right) Enlarge-
ment of the beginning portion of the left figures.
The curves echo the results on the Bio-NER task.
5.3 Post-Processing of the LDI: Minimum
Bayesian Risk Reranking
Although the label sequence produced by the LDI
inference is indeed the optimal label sequence by
means of probability, in practice, it may be benefi-
cial to use some post-processing methods to adapt
the LDI towards factual evaluation metrics. For
example, in practice, many natural language pro-
cessing tasks are evaluated by F-measures based
on chunks (e.g., named entities).
We further describe in this section the MBR
reranking method for the LDI. Here MBR rerank-
ing can be seen as a natural extension of the LDI
for adapting it to various evaluation criterions,
EVAL:
yMBR=argmax
y
?
y??LPn
P (y?)fEVAL(y|y?). (13)
The intuition behind our MBR reranking is the
778
Models Pre. Rec. F1 Time
LDI-A 91.76 90.59 91.17 42 s
LDI-A + MBR 92.22 90.40 91.30 61 s
Table 5: The effect of MBR reranking on the NP-
chunking task. As can be seen, MBR-reranking
improved the performance of the LDI.
?voting? by those results (label paths) produced by
the LDI inference. Each label path is a voter, and
it gives another one a ?score? (the score depend-
ing on the reference y? and the evaluation met-
ric EVAL, i.e., fEVAL(y|y?)) with a ?confidence?
(the probability of this voter, i.e., P (y?)). Finally,
the label path with the highest value, combining
scores and confidences, will be the optimal result.
For more details of the MBR technique, refer to
Goel & Byrne (2000) and Kumar & Byrne (2002).
An advantage of the LDI over the BHP and the
BMP is that the LDI can efficiently produce the
probabilities of the label sequences in LPn. Such
probabilities can be used directly for performing
the MBR reranking. We will show that it is easy
to employ the MBR reranking for the LDI, be-
cause the necessary statistics (e.g., the probabili-
ties of the label paths, y1,y2, . . .yn) are already
produced. In other words, by using LDI infer-
ence, a set of possible label sequences has been
collected with associated probabilities. Although
the cardinality of the set may be small, it accounts
for most of the probability mass by the definition
of the LDI. Eq.13 can be directly applied on this
set to perform reranking.
In contrast, the BHP and the BMP inference are
unable to provide such information for the rerank-
ing. For this reason, we can only report the results
of the reranking for the LDI.
As can be seen in Table 5, MBR-reranking im-
proved the performance of the LDI on the NP-
chunking task with a poor feature set. The pre-
sented MBR reranking algorithm is a general so-
lution for various evaluation criterions. We can
see that the different evaluation criterion, EVAL,
shares the common framework in Eq. 13. In prac-
tice, it is only necessary to re-implement the com-
ponent of fEVAL(y,y?) for a different evaluation
criterion. In this paper, the evaluation criterion is
the F-measure.
6 Conclusions and Future Work
In this paper, we propose an inference method, the
LDI, which is able to decode the optimal label se-
quence on latent conditional models. We study
the properties of the LDI, and showed that it can
be approximated aggressively for high efficiency,
with no loss in the performance. On the two NLP
tasks, the LDI-A outperformed the existing infer-
ence methods on latent conditional models, and its
inference time was comparable to that of the exist-
ing inference methods.
We also briefly present a post-processing
method, i.e., MBR reranking, upon the LDI
algorithm for various evaluation purposes. It
demonstrates encouraging improvement on the
NP-chunking tasks. In the future, we plan to per-
form further experiments to make a more detailed
study on combining the LDI inference and the
MBR reranking.
The LDI inference algorithm is not necessarily
limited in linear-chain structure. It could be ex-
tended to other latent conditional models with tree
structure (e.g., syntactic parsing with latent vari-
ables), as long as it allows efficient combination
of search and dynamic-programming. This could
also be a future work.
Acknowledgments
We thank Xia Zhou, Yusuke Miyao, Takuya Mat-
suzaki, Naoaki Okazaki and Galen Andrew for en-
lightening discussions, as well as the anonymous
reviewers who gave very helpful comments. The
first author was partially supported by University
of Tokyo Fellowship (UT-Fellowship). This work
was partially supported by Grant-in-Aid for Spe-
cially Promoted Research (MEXT, Japan).
References
Phillip Blunsom, Trevor Cohn, and Miles Osborne.
2008. A discriminative latent variable model for sta-
tistical machine translation. Proceedings of ACL?08.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, CMU.
L. Gillick and S. Cox. 1989. Some statistical issues
in the comparison of speech recognition algorithms.
International Conference on Acoustics Speech and
Signal Processing, v1:532?535.
V. Goel and W. Byrne. 2000. Minimum bayes-risk au-
tomatic speech recognition. Computer Speech and
Language, 14(2):115?135.
779
P.E. Hart, N.J. Nilsson, and B. Raphael. 1968. A
formal basis for the heuristic determination of mini-
mum cost path. IEEE Trans. On System Science and
Cybernetics, SSC-4(2):100?107.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
and Yuka Tateisi. 2004. Introduction to the bio-
entity recognition task at JNLPBA. Proceedings of
JNLPBA?04, pages 70?75.
S. Kumar and W. Byrne. 2002. Minimum bayes-
risk alignment of bilingual texts. Proceedings of
EMNLP?02.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. Proceedings of ICML?01, pages 282?
289.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Probabilistic CFG with latent annotations.
Proceedings of ACL?05.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings
of CVPR?07, pages 1?8.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal optimization. Springer.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?chi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. Proceedings of ACL?06.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (HLT-NAACL?07), pages 404?
411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In J.C.
Platt, D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing Systems
20 (NIPS), pages 1153?1160, Cambridge, MA. MIT
Press.
Erik Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the CoNLL-2000 shared task: Chunk-
ing. Proceedings of CoNLL?00, pages 127?132.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. Proceedings of
HLT/NAACL?03.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling latent-dynamic
in shallow parsing: A latent conditional model with
improved inference. Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING?08), pages 841?848.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, 13(2):260?269.
780
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56?64,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Discriminative Latent Variable Chinese Segmenter
with Hybrid Word/Character Information
Xu Sun
Department of Computer Science
University of Tokyo
sunxu@is.s.u-tokyo.ac.jp
Yaozhong Zhang
Department of Computer Science
University of Tokyo
yaozhong.zhang@is.s.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
School of Computer Science
University of Manchester
yoshimasa.tsuruoka@manchester.ac.uk
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo, Japan
School of Computer Science, University of Manchester, UK
National Centre for Text Mining, UK
tsujii@is.s.u-tokyo.ac.jp
Abstract
Conventional approaches to Chinese word
segmentation treat the problem as a character-
based tagging task. Recently, semi-Markov
models have been applied to the problem, in-
corporating features based on complete words.
In this paper, we propose an alternative, a
latent variable model, which uses hybrid in-
formation based on both word sequences and
character sequences. We argue that the use of
latent variables can help capture long range
dependencies and improve the recall on seg-
menting long words, e.g., named-entities. Ex-
perimental results show that this is indeed the
case. With this improvement, evaluations on
the data of the second SIGHAN CWS bakeoff
show that our system is competitive with the
best ones in the literature.
1 Introduction
For most natural language processing tasks, words
are the basic units to process. Since Chinese sen-
tences are written as continuous sequences of char-
acters, segmenting a character sequence into a word
sequence is the first step for most Chinese process-
ing applications. In this paper, we study the prob-
lem of Chinese word segmentation (CWS), which
aims to find these basic units (words1) for a given
sentence in Chinese.
Chinese character sequences are normally am-
biguous, and out-of-vocabulary (OOV) words are a
major source of the ambiguity. Typical examples
of OOV words include named entities (e.g., orga-
nization names, person names, and location names).
Those named entities may be very long, and a dif-
ficult case occurs when a long word W (|W | ? 4)
consists of some words which can be separate words
on their own; in such cases an automatic segmenter
may split the OOV word into individual words. For
example,
(Computer Committee of International Federation of
Automatic Control) is one of the organization names
in the Microsoft Research corpus. Its length is 13
and it contains more than 6 individual words, but it
should be treated as a single word. Proper recogni-
tion of long OOV words are meaningful not only for
word segmentation, but also for a variety of other
purposes, e.g., full-text indexing. However, as is il-
lustrated, recognizing long words (without sacrific-
ing the performance on short words) is challenging.
Conventional approaches to Chinese word seg-
mentation treat the problem as a character-based la-
1Following previous work, in this paper, words can also refer
to multi-word expressions, including proper names, long named
entities, idioms, etc.
56
beling task (Xue, 2003). Labels are assigned to each
character in the sentence, indicating whether the
character xi is the start (Labeli = B), middle or end
of a multi-character word (Labeli = C). A popu-
lar discriminative model that have been used for this
task is the conditional random fields (CRFs) (Laf-
ferty et al, 2001), starting with the model of Peng
et al (2004). In the Second International Chinese
Word Segmentation Bakeoff (the second SIGHAN
CWS bakeoff) (Emerson, 2005), two of the highest
scoring systems in the closed track competition were
based on a CRF model (Tseng et al, 2005; Asahara
et al, 2005).
While the CRF model is quite effective compared
with other models designed for CWS, it may be lim-
ited by its restrictive independence assumptions on
non-adjacent labels. Although the window can in
principle be widened by increasing the Markov or-
der, this may not be a practical solution, because
the complexity of training and decoding a linear-
chain CRF grows exponentially with the Markov or-
der (Andrew, 2006).
To address this difficulty, a choice is to relax the
Markov assumption by using the semi-Markov con-
ditional random field model (semi-CRF) (Sarawagi
and Cohen, 2004). Despite the theoretical advan-
tage of semi-CRFs over CRFs, however, some pre-
vious studies (Andrew, 2006; Liang, 2005) explor-
ing the use of a semi-CRF for Chinese word seg-
mentation did not find significant gains over the
CRF ones. As discussed in Andrew (2006), the rea-
son may be that despite the greater representational
power of the semi-CRF, there are some valuable fea-
tures that could be more naturally expressed in a
character-based labeling model. For example, on
a CRF model, one might use the feature ?the cur-
rent character xi is X and the current label Labeli
is C?. This feature may be helpful in CWS for gen-
eralizing to new words. For example, it may rule
out certain word boundaries if X were a character
that normally occurs only as a suffix but that com-
bines freely with some other basic forms to create
new words. This type of features is slightly less nat-
ural in a semi-CRF, since in that case local features
?(yi, yi+1, x) are defined on pairs of adjacent words.
That is to say, information about which characters
are not on boundaries is only implicit. Notably, ex-
cept the hybrid Markov/semi-Markov system in An-
drew (2006)2, no other studies using the semi-CRF
(Sarawagi and Cohen, 2004; Liang, 2005; Daume?
III and Marcu, 2005) experimented with features of
segmenting non-boundaries.
In this paper, instead of using semi-Markov mod-
els, we describe an alternative, a latent variable
model, to learn long range dependencies in Chi-
nese word segmentation. We use the discrimina-
tive probabilistic latent variable models (DPLVMs)
(Morency et al, 2007; Petrov and Klein, 2008),
which use latent variables to carry additional infor-
mation that may not be expressed by those original
labels, and therefore try to build more complicated
or longer dependencies. This is especially meaning-
ful in CWS, because the used labels are quite coarse:
Label(y) ? {B,C}, where B signifies beginning a
word and C signifies the continuation of a word.3
For example, by using DPLVM, the aforementioned
feature may turn to ?the current character xi is X ,
Labeli = C, and LatentV ariablei = LV ?. The
current latent variable LV may strongly depend on
the previous one or many latent variables, and there-
fore we can model the long range dependencies
which may not be captured by those very coarse la-
bels. Also, since character and word information
have their different advantages in CWS, in our latent
variable model, we use hybrid information based on
both character and word sequences.
2 A Latent Variable Segmenter
2.1 Discriminative Probabilistic Latent
Variable Model
Given data with latent structures, the task is to
learn a mapping between a sequence of observa-
tions x = x1, x2, . . . , xm and a sequence of labels
y = y1, y2, . . . , ym. Each yj is a class label for the
j?th character of an input sequence, and is a mem-
ber of a set Y of possible class labels. For each se-
quence, the model also assumes a sequence of latent
variables h = h1, h2, . . . , hm, which is unobserv-
able in training examples.
The DPLVM is defined as follows (Morency et al,
2The system was also used in Gao et al (2007), with an
improved performance in CWS.
3In practice, one may add a few extra labels based on lin-
guistic intuitions (Xue, 2003).
57
2007):
P (y|x,?) =?
h
P (y|h,x,?)P (h|x,?), (1)
where ? are the parameters of the model. DPLVMs
can be seen as a natural extension of CRF models,
and CRF models can be seen as a special case of
DPLVMs that have only one latent variable for each
label.
To make the training and inference efficient, the
model is restricted to have disjoint sets of latent vari-
ables associated with each class label. Each hj is a
member in a set Hyj of possible latent variables for
the class label yj . H is defined as the set of all pos-
sible latent variables, i.e., the union of all Hyj sets.
Since sequences which have any hj /? Hyj will by
definition have P (y|x,?) = 0, the model can be
further defined4 as:
P (y|x,?) = ?
h?Hy1?...?Hym
P (h|x,?), (2)
where P (h|x,?) is defined by the usual conditional
random field formulation:
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
in which f(h,x) is a feature vector. Given a training
set consisting of n labeled sequences, (xi,yi), for
i = 1 . . . n, parameter estimation is performed by
optimizing the objective function,
L(?) =
n?
i=1
log P (yi|xi,?) ? R(?). (4)
The first term of this equation is the conditional log-
likelihood of the training data. The second term is
a regularizer that is used for reducing overfitting in
parameter estimation.
For decoding in the test stage, given a test se-
quence x, we want to find the most probable label
sequence, y?:
y? = argmaxyP (y|x,??). (5)
For latent conditional models like DPLVMs, the best
label path y? cannot directly be produced by the
4It means that Eq. 2 is from Eq. 1 with additional definition.
Viterbi algorithm because of the incorporation of
hidden states. In this paper, we use a technique
based on A? search and dynamic programming de-
scribed in Sun and Tsujii (2009), for producing the
most probable label sequence y? on DPLVM.
In detail, an A? search algorithm5 (Hart et al,
1968) with a Viterbi heuristic function is adopted to
produce top-n latent paths, h1,h2, . . .hn. In addi-
tion, a forward-backward-style algorithm is used to
compute the exact probabilities of their correspond-
ing label paths, y1,y2, . . .yn. The model then tries
to determine the optimal label path based on the
top-n statistics, without enumerating the remaining
low-probability paths, which could be exponentially
enormous.
The optimal label path y? is ready when the fol-
lowing ?exact-condition? is achieved:
P (y1|x,?) ? (1 ?
?
yk?LPn
P (yk|x,?)) ? 0, (6)
where y1 is the most probable label sequence in
current stage. It is straightforward to prove that
y? = y1, and further search is unnecessary. This
is because the remaining probability mass, 1 ??
yk?LPn P (yk|x,?), cannot beat the current op-timal label path in this case. For more details of the
inference, refer to Sun and Tsujii (2009).
2.2 Hybrid Word/Character Information
We divide our main features into two types:
character-based features and word-based features.
The character-based features are indicator functions
that fire when the latent variable label takes some
value and some predicate of the input (at a certain
position) corresponding to the label is satisfied. For
each latent variable label hi (the latent variable la-
bel at position i), we use the predicate templates as
follows:
? Input characters/numbers/letters locating at po-
sitions i ? 2, i ? 1, i, i + 1 and i + 2
? The character/number/letter bigrams locating
at positions i ? 2, i ? 1, i and i + 1
5A? search and its variants, like beam-search, are widely
used in statistical machine translation. Compared to other
search techniques, an interesting point of A? search is that it
can produce top-n results one-by-one in an efficient manner.
58
? Whether xj and xj+1 are identical, for j = (i?
2) . . . (i + 1)
? Whether xj and xj+2 are identical, for j = (i?
3) . . . (i + 1)
The latter two feature templates are designed to de-
tect character or word reduplication, a morphologi-
cal phenomenon that can influence word segmenta-
tion in Chinese.
The word-based features are indicator functions
that fire when the local character sequence matches
a word or a word bigram. A dictionary containing
word and bigram information was collected from the
training data. For each latent variable label unigram
hi, we use the set of predicate template checking for
word-based features:
? The identity of the string xj . . . xi, if it matches
a word A from the word-dictionary of training
data, with the constraint i?6 < j < i; multiple
features will be generated if there are multiple
strings satisfying the condition.
? The identity of the string xi . . . xk, if it matches
a word A from the word-dictionary of training
data, with the constraint i < k < i+6; multiple
features could be generated.
? The identity of the word bigram (xj . . . xi?1,
xi . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
? The identity of the word bigram (xj . . . xi,
xi+1 . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
All feature templates were instantiated with val-
ues that occur in positive training examples. We
found that using low-frequency features that occur
only a few times in the training set improves perfor-
mance on the development set. We hence do not do
any thresholding of the DPLVM features: we simply
use all those generated features.
The aforementioned word based features can in-
corporate word information naturally. In addition,
following Wang et al (2006), we found using a
very simple heuristic can further improve the seg-
mentation quality slightly. More specifically, two
operations, merge and split, are performed on the
DPLVM/CRF outputs: if a bigram A B was not ob-
served in the training data, but the merged one AB
was, then A B will be simply merged into AB; on
the other hand, if AB was not observed but A B ap-
peared, then it will be split into A B. We found this
simple heuristic on word information slightly im-
proved the performance (e.g., for the PKU corpus,
+0.2% on the F-score).
3 Experiments
We used the data provided by the second Inter-
national Chinese Word Segmentation Bakeoff to
test our approaches described in the previous sec-
tions. The data contains three corpora from different
sources: Microsoft Research Asia (MSR), City Uni-
versity of Hong Kong (CU), and Peking University
(PKU).
Since the purpose of this work is to evaluate the
proposed latent variable model, we did not use ex-
tra resources such as common surnames, lexicons,
parts-of-speech, and semantics. For the generation
of word-based features, we extracted a word list
from the training data as the vocabulary.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the de-
coder), precision (P , the percentage of words in the
decoder output that are segmented correctly), bal-
anced F-score (F ) defined by 2PR/(P + R), recall
of OOV words (R-oov). For more detailed informa-
tion on the corpora and these metrics, refer to Emer-
son (2005).
3.1 Training the DPLVM Segmenter
We implemented DPLVMs in C++ and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions. We
employ the feature templates defined in Section 2.2,
taking into account those 3,069,861 features for the
MSR data, 2,634,384 features for the CU data, and
1,989,561 features for the PKU data.
As for numerical optimization, we performed
gradient decent with the Limited-Memory BFGS
59
(L-BFGS)6 optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order Quasi-
Newton method that numerically estimates the cur-
vature from previous gradients and updates. With
no requirement on specialized Hessian approxima-
tion, L-BFGS can handle large-scale problems in an
efficient manner.
Since the objective function of the DPLVM model
is non-convex, we randomly initialized parameters
for the training.7 To reduce overfitting, we employed
an L2 Gaussian weight prior8 (Chen and Rosen-
feld, 1999). During training, we varied the L2-
regularization term (with values 10k, k from -3 to
3), and finally set the value to 1. We use 4 hidden
variables per label for this task, compromising be-
tween accuracy and efficiency.
3.2 Comparison on Convergence Speed
First, we show a comparison of the convergence
speed between the objective function of DPLVMs
and CRFs. We apply the L-BFGS optimization algo-
rithm to optimize the objective function of DPLVM
and CRF models, making a comparison between
them. We find that the number of iterations required
for the convergence of DPLVMs are fewer than for
CRFs. Figure 1 illustrates the convergence-speed
comparison on the MSR data. The DPLVM model
arrives at the plateau of convergence in around 300
iterations, with the penalized loss of 95K when
#passes = 300; while CRFs require 900 iterations,
with the penalized loss of 98K when #passes =
900.
However, we should note that the time cost of the
DPLVM model in each iteration is around four times
higher than the CRF model, because of the incorpo-
ration of hidden variables. In order to speed up the
6For numerical optimization on latent variable models, we
also experimented the conjugate-gradient (CG) optimization al-
gorithm and stochastic gradient decent algorithm (SGD). We
found the L-BFGS with L2 Gaussian regularization performs
slightly better than the CG and the SGD. Therefore, we adopt
the L-BFGS optimizer in this study.
7For a non-convex objective function, different parame-
ter initializations normally bring different optimization results.
Therefore, to approach closer to the global optimal point, it
is recommended to perform multiple experiments on DPLVMs
with random initialization and then select a good start point.
8We also tested the L-BFGS with L1 regularization, and we
found the L-BFGS with L2 regularization performs better in
this task.
0
300K
600K
900K
1200K
1500K
1800K
 100  200  300  400  500  600  700  800  900
O
bj.
 Fu
nc
. V
alu
e
Forward-Backward Passes
DPLVM
CRF
Figure 1: The value of the penalized loss based on the
number of iterations: DPLVMs vs. CRFs on the MSR
data.
Style #W.T. #Word #C.T. #Char
MSR S.C. 88K 2,368K 5K 4,050K
CU T.C. 69K 1,455K 5K 2,403K
PKU S.C. 55K 1,109K 5K 1,826K
Table 1: Details of the corpora. W.T. represents word
types; C.T. represents character types; S.C. represents
simplified Chinese; T.C. represents traditional Chinese.
training speed of the DPLVM model in the future,
one solution is to use the stochastic learning tech-
nique9. Another solution is to use a distributed ver-
sion of L-BFGS to parallelize the batch training.
4 Results and Discussion
Since the CRF model is one of the most successful
models in Chinese word segmentation, we compared
DPLVMs with CRFs. We tried to make experimen-
tal results comparable between DPLVMs and CRF
models, and have therefore employed the same fea-
ture set, optimizer and fine-tuning strategy between
the two. We also compared DPLVMs with semi-
CRFs and other successful systems reported in pre-
vious work.
4.1 Evaluation Results
Three training and test corpora were used in the test,
including the MSR Corpus, the CU Corpus, and the
9We have tried stochastic gradient decent, as described pre-
viously. It is possible to try other stochastic learning methods,
e.g., stochastic meta decent (Vishwanathan et al, 2006).
60
MSR data P R F R-oov
DPLVM (*) 97.3 97.3 97.3 72.2
CRF (*) 97.1 96.8 97.0 72.0
semi-CRF (A06) N/A N/A 96.8 N/A
semi-CRF (G07) N/A N/A 97.2 N/A
CRF (Z06-a) 96.5 96.3 96.4 71.4
Z06-b 97.2 96.9 97.1 71.2
ZC07 N/A N/A 97.2 N/A
Best05 (T05) 96.2 96.6 96.4 71.7
CU data P R F R-oov
DPLVM (*) 94.7 94.4 94.6 68.8
CRF (*) 94.3 93.9 94.1 65.8
CRF (Z06-a) 95.0 94.2 94.6 73.6
Z06-b 95.2 94.9 95.1 74.1
ZC07 N/A N/A 95.1 N/A
Best05 (T05) 94.1 94.6 94.3 69.8
PKU data P R F R-oov
DPLVM (*) 95.6 94.8 95.2 77.8
CRF (*) 95.2 94.2 94.7 76.8
CRF (Z06-a) 94.3 94.6 94.5 75.4
Z06-b 94.7 95.5 95.1 74.8
ZC07 N/A N/A 94.5 N/A
Best05 (C05) 95.3 94.6 95.0 63.6
Table 2: Results from DPLVMs, CRFs, semi-CRFs, and
other systems.
PKU Corpus (see Table 1 for details). The results
are shown in Table 2. The results are grouped into
three sub-tables according to different corpora. Each
row represents a CWS model. For each group, the
rows marked by ? represent our models with hy-
brid word/character information. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; A06 represents the semi-CRF model in An-
drew (2006)10, which was also used in Gao et al
(2007) (denoted as G07) with an improved perfor-
mance; Z06-a and Z06-b represents the pure sub-
word CRF model and the confidence-based com-
bination of CRF and rule-based models, respec-
tively (Zhang et al, 2006); ZC07 represents the
word-based perceptron model in Zhang and Clark
(2007); T05 represents the CRF model in Tseng et
al. (2005); C05 represents the system in Chen et al
10It is a hybrid Markov/semi-Markov CRF model which
outperforms conventional semi-CRF models (Andrew, 2006).
However, in general, as discussed in Andrew (2006), it is essen-
tially still a semi-CRF model.
(2005). The best F-score and recall of OOV words
of each group is shown in bold.
As is shown in the table, we achieved the best
F-score in two out of the three corpora. We also
achieved the best recall rate of OOV words on those
two corpora. Both of the MSR and PKU Corpus use
simplified Chinese, while the CU Corpus uses the
traditional Chinese.
On the MSR Corpus, the DPLVM model reduced
more than 10% error rate over the CRF model us-
ing exactly the same feature set. We also compared
our DPLVM model with the semi-CRF models in
Andrew (2006) and Gao et al (2007), and demon-
strate that the DPLVM model achieved slightly bet-
ter performance than the semi-CRF models. Andrew
(2006) and Gao et al (2007) only reported the re-
sults on the MSR Corpus.
In summary, tests for the Second International
Chinese Word Segmentation Bakeoff showed com-
petitive results for our method compared with the
best results in the literature. Our discriminative la-
tent variable models achieved the best F-scores on
the MSR Corpus (97.3%) and PKU Corpus (95.2%);
the latent variable models also achieved the best re-
calls of OOV words over those two corpora. We will
analyze the results by varying the word-length in the
following subsection.
4.2 Effect on Long Words
One motivation of using a latent variable model for
CWS is to use latent variables to more adequately
learn long range dependencies, as we argued in Sec-
tion 1. In the test data of the MSR Corpus, 19% of
the words are longer than 3 characters; there are also
8% in the CU Corpus and 11% in the PKU Corpus,
respectively. In the MSR Corpus, there are some ex-
tremely long words (Length > 10), while the CU
and PKU corpus do not contain such extreme cases.
Figure 2 shows the recall rate on different groups
of words categorized by their lengths (the number
of characters). As we expected, the DPLVM model
performs much better on long words (Length ? 4)
than the CRF model, which used exactly the same
feature set. Compared with the CRF model, the
DPLVM model exhibited almost the same level of
performance on short words. Both models have
the best performance on segmenting the words with
the length of two. The performance of the CRF
61
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-M
SR
 (%
)
Length of Word (MSR)
DPLVM
CRF
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-C
U 
(%
)
Length of Word (CU)
DPLVM
CRF
 40
 50
 60
 70
 80
 90
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-P
KU
 (%
)
Length of Word (PKU)
DPLVM
CRF
Figure 2: The recall rate on words grouped by the length.
model deteriorates rapidly as the word length in-
creases, which demonstrated the difficulty on mod-
eling long range dependencies in CWS. Compared
with the CRF model, the DPLVM model performed
quite well in dealing with long words, without sacri-
ficing the performance on short words. All in all, we
conclude that the improvement of using the DPLVM
model came from the improvement on modeling
long range dependencies in CWS.
4.3 Error Analysis
Table 3 lists the major errors collected from the la-
tent variable segmenter. We examined the collected
errors and found that many of them can be grouped
into four types: over-generalization (the top row),
errors on named entities (the following three rows),
errors on idioms (the following three rows) and er-
rors from inconsistency (the two rows at the bottom).
Our system performed reasonably well on very
complex OOV words, such as
(Agricultural Bank of China,
Gold Segmentation Segmenter Output
//
Co-allocated org. names
(Chen Yao) //
(Chen Fei) //
(Vasillis) //
//
//
// //
Idioms
// (propagandist)
(desertification) //
Table 3: Error analysis on the latent variable seg-
menter. The errors are grouped into four types: over-
generalization, errors on named entities, errors on idioms
and errors from data-inconsistency.
Shijiazhuang-city Branch, the second sales depart-
ment) and (Science
and Technology Commission of China, National In-
stitution on Scientific Information Analysis). How-
ever, it sometimes over-generalized to long words.
For example, as shown in the top row,
(National Department of Environmental Protection)
and (The Central Propaganda Department)
are two organization names, but they are incorrectly
merged into a single word.
As for the following three rows, (Chen Yao)
and (Chen Fei) are person names. They are
wrongly segmented because we lack the features to
capture the information of person names (such use-
ful knowledge, e.g., common surname list, are cur-
rently not used in our system). In the future, such
errors may be solved by integrating open resources
into our system. (Vasillis) is a transliter-
ated foreign location name and is also wrongly seg-
mented.
For the corpora that considered 4 character idioms
as a word, our system successfully combined most
of new idioms together. This differs greatly from the
results of CRFs. However, there are still a number
of new idioms that failed to be correctly segmented,
as listed from the fifth row to the seventh row.
Finally, some errors are due to inconsistencies in
the gold segmentation. For example, // (pro-
pagandist) is two words, but a word with similar
62
structure, (theorist), is one word.
(desertification) is one word, but its synonym,
// (desertification), is two words in the gold seg-
mentation.
5 Conclusion and Future Work
We presented a latent variable model for Chinese
word segmentation, which used hybrid information
based on both word and character sequences. We
discussed that word and character information have
different advantages, and could be complementary
to each other. Our model is an alternative to the ex-
isting word based models and character based mod-
els.
We argued that using latent variables can better
capture long range dependencies. We performed
experiments and demonstrated that our model can
indeed improve the segmentation accuracy on long
words. With this improvement, tests on the data
of the Second International Chinese Word Segmen-
tation Bakeoff show that our system is competitive
with the best in the literature.
Since the latent variable model allows a wide
range of features, so the future work will consider
how to integrate open resources into our system. The
latent variable model handles latent-dependencies
naturally, and can be easily extended to other label-
ing tasks.
Acknowledgments
We thank Kun Yu, Galen Andrew and Xiaojun Lin
for the enlightening discussions. We also thank the
anonymous reviewers who gave very helpful com-
ments. This work was partially supported by Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan).
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
Proceedings of EMNLP?06, pages 465?472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of machine
learning methods for optimum chinese word segmen-
tation. Proceedings of the fourth SIGHAN workshop,
pages 134?137.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word seg-
mentation. Proceedings of the fourth SIGHAN work-
shop.
Hal Daume? III and Daniel Marcu. 2005. Learn-
ing as search optimization: approximate large mar-
gin methods for structured prediction. Proceedings of
ICML?05.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of the
fourth SIGHAN workshop, pages 123?133.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics (ACL?07), pages 824?831.
P.E. Hart, N.J. Nilsson, and B. Raphael. 1968. A formal
basis for the heuristic determination of minimum cost
path. IEEE Trans. On System Science and Cybernet-
ics, SSC-4(2):100?107.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. Proceed-
ings of ICML?01, pages 282?289.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings of
CVPR?07, pages 1?8.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
F. Peng and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random
fields. Proceedings of COLING?04.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings of
NIPS?08.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Proceedings of ICML?04.
Xu Sun and Jun?ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm and
its efficient approximation. Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL?09).
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bakeoff
63
2005. Proceedings of the fourth SIGHAN workshop,
pages 168?171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. Proceedings of ICML?06, pages 969?
976.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop, pages
138?141, July.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Computa-
tional Linguistics and Chinese Language Processing,
8(1).
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. Pro-
ceedings of ACL?07.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. Proceedings of
HLT/NAACL?06 companion volume short papers.
64
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905?913,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Robust Approach to Abbreviating Terms:
A Discriminative Latent Variable Model with Global Information
Xu Sun?, Naoaki Okazaki?, Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo,
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
?School of Computer Science, University of Manchester, UK
?National Centre for Text Mining, UK
{sunxu, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
The present paper describes a robust ap-
proach for abbreviating terms. First, in
order to incorporate non-local informa-
tion into abbreviation generation tasks, we
present both implicit and explicit solu-
tions: the latent variable model, or alter-
natively, the label encoding approach with
global information. Although the two ap-
proaches compete with one another, we
demonstrate that these approaches are also
complementary. By combining these two
approaches, experiments revealed that the
proposed abbreviation generator achieved
the best results for both the Chinese and
English languages. Moreover, we directly
apply our generator to perform a very dif-
ferent task from tradition, the abbreviation
recognition. Experiments revealed that the
proposed model worked robustly, and out-
performed five out of six state-of-the-art
abbreviation recognizers.
1 Introduction
Abbreviations represent fully expanded forms
(e.g., hidden markov model) through the use of
shortened forms (e.g., HMM). At the same time,
abbreviations increase the ambiguity in a text.
For example, in computational linguistics, the
acronym HMM stands for hidden markov model,
whereas, in the field of biochemistry, HMM is gen-
erally an abbreviation for heavy meromyosin. As-
sociating abbreviations with their fully expanded
forms is of great importance in various NLP ap-
plications (Pakhomov, 2002; Yu et al, 2006;
HaCohen-Kerner et al, 2008).
The core technology for abbreviation disam-
biguation is to recognize the abbreviation defini-
tions in the actual text. Chang and Schu?tze (2006)
reported that 64,242 new abbreviations were intro-
duced into the biomedical literatures in 2004. As
such, it is important to maintain sense inventories
(lists of abbreviation definitions) that are updated
with the neologisms. In addition, based on the
one-sense-per-discourse assumption, the recogni-
tion of abbreviation definitions assumes senses of
abbreviations that are locally defined in a docu-
ment. Therefore, a number of studies have at-
tempted to model the generation processes of ab-
breviations: e.g., inferring the abbreviating mech-
anism of the hidden markov model into HMM.
An obvious approach is to manually design
rules for abbreviations. Early studies attempted
to determine the generic rules that humans use
to intuitively abbreviate given words (Barrett and
Grems, 1960; Bourne and Ford, 1961). Since
the late 1990s, researchers have presented var-
ious methods by which to extract abbreviation
definitions that appear in actual texts (Taghva
and Gilbreth, 1999; Park and Byrd, 2001; Wren
and Garner, 2002; Schwartz and Hearst, 2003;
Adar, 2004; Ao and Takagi, 2005). For example,
Schwartz and Hearst (2003) implemented a simple
algorithm that mapped all alpha-numerical letters
in an abbreviation to its expanded form, starting
from the end of both the abbreviation and its ex-
panded forms, and moving from right to left.
These studies performed highly, especially for
English abbreviations. However, a more extensive
investigation of abbreviations is needed in order to
further improve definition extraction. In addition,
we cannot simply transfer the knowledge of the
hand-crafted rules from one language to another.
For instance, in English, abbreviation characters
are preferably chosen from the initial and/or cap-
ital characters in their full forms, whereas some
905
p o l y g l y c o l i c a c i dP S S S P S S S S S S S S P S S S [PGA]
??? ? ???S P P S S S P [???]Institute of History and Philology at Academia Sinica(b): Chinese Abbreviation Generation
(a): English Abbreviation Generation    
Figure 1: English (a) and Chinese (b) abbreviation
generation as a sequential labeling problem.
other languages, including Chinese and Japanese,
do not have word boundaries or case sensitivity.
A number of recent studies have investigated
the use of machine learning techniques. Tsuruoka
et al (2005) formalized the processes of abbrevia-
tion generation as a sequence labeling problem. In
the present study, each character in the expanded
form is tagged with a label, y ? {P,S}1, where
the label P produces the current character and
the label S skips the current character. In Fig-
ure 1 (a), the abbreviation PGA is generated from
the full form polyglycolic acid because the under-
lined characters are tagged with P labels. In Fig-
ure 1 (b), the abbreviation is generated using the
2nd and 3rd characters, skipping the subsequent
three characters, and then using the 7th character.
In order to formalize this task as a sequential
labeling problem, we have assumed that the la-
bel of a character is determined by the local in-
formation of the character and its previous label.
However, this assumption is not ideal for model-
ing abbreviations. For example, the model can-
not make use of the number of words in a full
form to determine and generate a suitable num-
ber of letters for the abbreviation. In addition, the
model would be able to recognize the abbreviat-
ing process in Figure 1 (a) more reasonably if it
were able to segment the word polyglycolic into
smaller regions, e.g., poly-glycolic. Even though
humans may use global or non-local information
to abbreviate words, previous studies have not in-
corporated this information into a sequential label-
ing model.
In the present paper, we propose implicit and
explicit solutions for incorporating non-local in-
formation. The implicit solution is based on the
1Although the original paper of Tsuruoka et al (2005) at-
tached case sensitivity information to the P label, for simplic-
ity, we herein omit this information.
y1 y2 ym
xmx2x1
h1 h2 hm
xmx2x1
ymy2y1
CRF DPLVM
Figure 2: CRF vs. DPLVM. Variables x, y, and h
represent observation, label, and latent variables,
respectively.
discriminative probabilistic latent variable model
(DPLVM) in which non-local information is mod-
eled by latent variables. We manually encode non-
local information into the labels in order to provide
an explicit solution. We evaluate the models on the
task of abbreviation generation, in which a model
produces an abbreviation for a given full form. Ex-
perimental results indicate that the proposed mod-
els significantly outperform previous abbreviation
generation studies. In addition, we apply the pro-
posed models to the task of abbreviation recogni-
tion, in which a model extracts the abbreviation
definitions in a given text. To the extent of our
knowledge, this is the first model that can per-
form both abbreviation generation and recognition
at the state-of-the-art level, across different lan-
guages and with a simple feature set.
2 Abbreviator with Non-local
Information
2.1 A Latent Variable Abbreviator
To implicitly incorporate non-local information,
we propose discriminative probabilistic latent
variable models (DPLVMs) (Morency et al, 2007;
Petrov and Klein, 2008) for abbreviating terms.
The DPLVM is a natural extension of the CRF
model (see Figure 2), which is a special case of the
DPLVM, with only one latent variable assigned for
each label. The DPLVM uses latent variables to
capture additional information that may not be ex-
pressed by the observable labels. For example, us-
ing the DPLVM, a possible feature could be ?the
current character xi = X, the label yi = P, and
the latent variable hi = LV.? The non-local infor-
mation can be effectively modeled in the DPLVM,
and the additional information at the previous po-
sition or many of the other positions in the past
could be transferred via the latent variables (see
Figure 2).
906
Using the label set Y = {P,S}, abbreviation
generation is formalized as the task of assigning
a sequence of labels y = y1, y2, . . . , ym for a
given sequence of characters x = x1, x2, . . . , xm
in an expanded form. Each label, yj , is a mem-
ber of the possible labels Y . For each sequence,
we also assume a sequence of latent variables
h = h1, h2, . . . , hm, which are unobservable in
training examples.
We model the conditional probability of the la-
bel sequence P (y|x) using the DPLVM,
P (y|x,?) =
?
h
P (y|h,x,?)P (h|x,?). (1)
Here, ? represents the parameters of the model.
To ensure that the training and inference are ef-
ficient, the model is often restricted to have dis-
jointed sets of latent variables associated with each
label (Morency et al, 2007). Each hj is a member
in a set Hyj of possible latent variables for the la-
bel yj . Here, H is defined as the set of all possi-
ble latent variables, i.e., H is the union of all Hyj
sets. Since the sequences having hj /? Hyj will,
by definition, yield P (y|x,?) = 0, the model is
rewritten as follows (Morency et al, 2007; Petrov
and Klein, 2008):
P (y|x,?) =
?
h?Hy1?...?Hym
P (h|x,?). (2)
Here, P (h|x,?) is defined by the usual formula-
tion of the conditional random field,
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
where f(h,x) represents a feature vector.
Given a training set consisting of n instances,
(xi,yi) (for i = 1 . . . n), we estimate the pa-
rameters ? by maximizing the regularized log-
likelihood,
L(?) =
n?
i=1
logP (yi|xi,?)?R(?). (4)
The first term expresses the conditional log-
likelihood of the training data, and the second term
represents a regularizer that reduces the overfitting
problem in parameter estimation.
2.2 Label Encoding with Global Information
Alternatively, we can design the labels such that
they explicitly incorporate non-local information.
? ? ? ? ? ? ? ? ? ? ? ? ? ?S S P S S S S S S P S P S SS0 S0 P1 S1 S1 S1 S1 S1 S1 P2 S2 P3 S3 S3
Management office of the imports and exports of endangered speciesOrig.GI
Figure 3: Comparison of the proposed label en-
coding method with global information (GI) and
the conventional label encoding method.
In this approach, the label yi at position i at-
taches the information of the abbreviation length
generated by its previous labels, y1, y2, . . . , yi?1.
Figure 3 shows an example of a Chinese abbre-
viation. In this encoding, a label not only con-
tains the produce or skip information, but also the
abbreviation-length information, i.e., the label in-
cludes the number of all P labels preceding the
current position. We refer to this method as label
encoding with global information (hereinafter GI).
The concept of using label encoding to incorporate
non-local information was originally proposed by
Peshkin and Pfeffer (2003).
Note that the model-complexity is increased
only by the increase in the number of labels. Since
the length of the abbreviations is usually quite
short (less than five for Chinese abbreviations and
less than 10 for English abbreviations), the model
is still tractable even when using the GI encoding.
The implicit (DPLVM) and explicit (GI) solu-
tions address the same issue concerning the in-
corporation of non-local information, and there
are advantages to combining these two solutions.
Therefore, we will combine the implicit and ex-
plicit solutions by employing the GI encoding in
the DPLVM (DPLVM+GI). The effects of this
combination will be demonstrated through experi-
ments.
2.3 Feature Design
Next, we design two types of features: language-
independent features and language-specific fea-
tures. Language-independent features can be used
for abbreviating terms in English and Chinese. We
use the features from #1 to #3 listed in Table 1.
Feature templates #4 to #7 in Table 1 are used
for Chinese abbreviations. Templates #4 and #5
express the Pinyin reading of the characters, which
represents a Romanization of the sound. Tem-
plates #6 and #7 are designed to detect character
duplication, because identical characters will nor-
mally be skipped in the abbreviation process. On
907
#1 The input char. xi?1 and xi
#2 Whether xj is a numeral, for j = (i? 3) . . . i
#3 The char. bigrams starting at (i? 2) . . . i
#4 The Pinyin of char. xi?1 and xi
#5 The Pinyin bigrams starting at (i? 2) . . . i
#6 Whether xj = xj+1, for j = (i? 2) . . . i
#7 Whether xj = xj+2, for j = (i? 3) . . . i
#8 Whether xj is uppercase, for j = (i? 3) . . . i
#9 Whether xj is lowercase, for j = (i? 3) . . . i
#10 The char. 3-grams starting at (i? 3) . . . i
#11 The char. 4-grams starting at (i? 4) . . . i
Table 1: Language-independent features (#1 to
#3), Chinese-specific features (#4 through #7), and
English-specific features (#8 through #11).
the other hand, such duplication detection features
are not so useful for English abbreviations.
Feature templates #8?#11 are designed for En-
glish abbreviations. Features #8 and #9 encode the
orthographic information of expanded forms. Fea-
tures #10 and #11 represent a contextual n-gram
with a large window size. Since the number of
letters in Chinese (more than 10K characters) is
much larger than the number of letters in English
(26 letters), in order to avoid a possible overfitting
problem, we did not apply these feature templates
to Chinese abbreviations.
Feature templates are instantiated with values
that occur in positive training examples. We used
all of the instantiated features because we found
that the low-frequency features also improved the
performance.
3 Experiments
For Chinese abbreviation generation, we used the
corpus of Sun et al (2008), which contains 2,914
abbreviation definitions for training, and 729 pairs
for testing. This corpus consists primarily of noun
phrases (38%), organization names (32%), and
verb phrases (21%). For English abbreviation gen-
eration, we evaluated the corpus of Tsuruoka et
al. (2005). This corpus contains 1,200 aligned
pairs extracted from MEDLINE biomedical ab-
stracts (published in 2001). For both tasks, we
converted the aligned pairs of the corpora into la-
beled full forms and used the labeled full forms as
the training/evaluation data.
The evaluation metrics used in the abbreviation
generation are exact-match accuracy (hereinafter
accuracy), including top-1 accuracy, top-2 accu-
racy, and top-3 accuracy. The top-N accuracy rep-
resents the percentage of correct abbreviations that
are covered, if we take the top N candidates from
the ranked labelings of an abbreviation generator.
We implemented the DPLVM in C++ and op-
timized the system to cope with large-scale prob-
lems. We employ the feature templates defined in
Section 2.3, taking into account these 81,827 fea-
tures for the Chinese abbreviation generation task,
and the 50,149 features for the English abbrevia-
tion generation task.
For numerical optimization, we performed a
gradient descent with the Limited-Memory BFGS
(L-BFGS) optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order
Quasi-Newton method that numerically estimates
the curvature from previous gradients and up-
dates. With no requirement on specialized Hes-
sian approximation, L-BFGS can handle large-
scale problems efficiently. Since the objective
function of the DPLVM model is non-convex,
different parameter initializations normally bring
different optimization results. Therefore, to ap-
proach closer to the global optimal point, it is
recommended to perform multiple experiments on
DPLVMs with random initialization and then se-
lect a good start point. To reduce overfitting,
we employed a L2 Gaussian weight prior (Chen
and Rosenfeld, 1999), with the objective function:
L(?) = ?ni=1 logP (yi|xi,?)?||?||2/?2. Dur-
ing training and validation, we set ? = 1 for the
DPLVM generators. We also set four latent vari-
ables for each label, in order to make a compro-
mise between accuracy and efficiency.
Note that, for the label encoding with
global information, many label transitions (e.g.,
P2S3) are actually impossible: the label tran-
sitions are strictly constrained, i.e., yiyi+1 ?
{PjSj,PjPj+1,SjPj+1,SjSj}. These con-
straints on the model topology (forward-backward
lattice) are enforced by giving appropriate features
a weight of ??, thereby forcing all forbidden la-
belings to have zero probability. Sha and Pereira
(2003) originally proposed this concept of imple-
menting transition restrictions.
4 Results and Discussion
4.1 Chinese Abbreviation Generation
First, we present the results of the Chinese abbre-
viation generation task, as listed in Table 2. To
evaluate the impact of using latent variables, we
chose the baseline system as the DPLVM, in which
each label has only one latent variable. Since this
908
Model T1A T2A T3A Time
Heu (S08) 41.6 N/A N/A N/A
HMM (S08) 46.1 N/A N/A N/A
SVM (S08) 62.7 80.4 87.7 1.3 h
CRF 64.5 81.1 88.7 0.2 h
CRF+GI 66.8 82.5 90.0 0.5 h
DPLVM 67.6 83.8 91.3 0.4 h
DPLVM+GI (*) 72.3 87.6 94.9 1.1 h
Table 2: Results of Chinese abbreviation gener-
ation. T1A, T2A, and T3A represent top-1, top-
2, and top-3 accuracy, respectively. The system
marked with the * symbol is the recommended
system.
special case of the DPLVM is exactly the CRF
(see Section 2.1), this case is hereinafter denoted
as the CRF. We compared the performance of the
DPLVM with the CRFs and other baseline sys-
tems, including the heuristic system (Heu), the
HMM model, and the SVM model described in
S08, i.e., Sun et al (2008). The heuristic method
is a simple rule that produces the initial character
of each word to generate the corresponding abbre-
viation. The SVM method described by Sun et al
(2008) is formalized as a regression problem, in
which the abbreviation candidates are scored and
ranked.
The results revealed that the latent variable
model significantly improved the performance
over the CRF model. All of its top-1, top-2,
and top-3 accuracies were consistently better than
those of the CRF model. Therefore, this demon-
strated the effectiveness of using the latent vari-
ables in Chinese abbreviation generation.
As the case for the two alternative approaches
for incorporating non-local information, the la-
tent variable method and the label encoding
method competed with one another (see DPLVM
vs. CRF+GI). The results showed that the la-
tent variable method outperformed the GI encod-
ing method by +0.8% on the top-1 accuracy. The
reason for this could be that the label encoding ap-
proach is a solution without the adaptivity on dif-
ferent instances. We will present a detailed discus-
sion comparing DPLVM and CRF+GI for the En-
glish abbreviation generation task in the next sub-
section, where the difference is more significant.
In contrast, to a larger extent, the results demon-
strate that these two alternative approaches are
complementary. Using the GI encoding further
improved the performance of the DPLVM (with
+4.7% on top-1 accuracy). We found that major
? ? ? ? ? ? ?P S P S P S PP1 S1 P2 S2 S2 S2 P3
State Tobacco Monopoly Administration DPLVM DPLVM+GI ???? [Wrong]??? [Correct]
Figure 4: An example of the results.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1  2  3  4  5  6
P
er
ce
nt
ag
e 
(%
)
Length of Produced Abbr.
Gold Train
Gold Test
DPLVM
DPLVM+GI
Figure 5: Percentage distribution of Chinese
abbreviations/Viterbi-labelings grouped by length.
improvements were achieved through the more ex-
act control of the output length. An example is
shown in Figure 4. The DPLVM made correct de-
cisions at three positions, but failed to control the
abbreviation length.2 The DPLVM+GI succeeded
on this example. To perform a detailed analysis,
we collected the statistics of the length distribution
(see Figure 5) and determined that the GI encod-
ing improved the abbreviation length distribution
of the DPLVM.
In general, the results indicate that all of the se-
quential labeling models outperformed the SVM
regression model with less training time.3 In the
SVM regression approach, a large number of neg-
ative examples are explicitly generated for the
training, which slowed the process.
The proposed method, the latent variable model
with GI encoding, is 9.6% better with respect to
the top-1 accuracy compared to the best system on
this corpus, namely, the SVM regression method.
Furthermore, the top-3 accuracy of the latent vari-
able model with GI encoding is as high as 94.9%,
which is quite encouraging for practical usage.
4.2 English Abbreviation Generation
In the English abbreviation generation task, we
randomly selected 1,481 instances from the gen-
2The Chinese abbreviation with length = 4 should have
a very low probability, e.g., only 0.6% of abbreviations with
length = 4 in this corpus.
3On Intel Dual-Core Xeon 5160/3 GHz CPU, excluding
the time for feature generation and data input/output.
909
Model T1A T2A T3A Time
CRF 55.8 65.1 70.8 0.3 h
CRF+GI 52.7 63.2 68.7 1.3 h
CRF+GIB 56.8 66.1 71.7 1.3 h
DPLVM 57.6 67.4 73.4 0.6 h
DPLVM+GI 53.6 63.2 69.2 2.5 h
DPLVM+GIB (*) 58.3 N/A N/A 3.0 h
Table 3: Results of English abbreviation genera-
tion.
somatosensory evoked potentials
(a) P1P2 P3 P4 P5 SMEPS
(b) P P P P SEPS
(a): CRF+GI with p=0.001 [Wrong]
(b): DPLVM with p=0.191 [Correct]
Figure 6: A result of ?CRF+GI vs. DPLVM?. For
simplicity, the S labels are masked.
eration corpus for training, and 370 instances for
testing. Table 3 shows the experimental results.
We compared the performance of the DPLVM
with the performance of the CRFs. Whereas the
use of the latent variables still significantly im-
proves the generation performance, using the GI
encoding undermined the performance in this task.
In comparing the implicit and explicit solutions
for incorporating non-local information, we can
see that the implicit approach (the DPLVM) per-
forms much better than the explicit approach (the
GI encoding). An example is shown in Figure 6.
The CRF+GI produced a Viterbi labeling with a
low probability, which is an incorrect abbrevia-
tion. The DPLVM produced the correct labeling.
To perform a systematic analysis of the
superior-performance of DPLVM compare to
CRF+GI, we collected the probability distribu-
tions (see Figure 7) of the Viterbi labelings from
these models (?DPLVM vs. CRF+GI? is high-
lighted). The curves suggest that the data sparse-
ness problem could be the reason for the differ-
ences in performance. A large percentage (37.9%)
of the Viterbi labelings from the CRF+GI (ENG)
have very small probability values (p < 0.1).
For the DPLVM (ENG), there were only a few
(0.5%) Viterbi labelings with small probabilities.
Since English abbreviations are often longer than
Chinese abbreviations (length < 10 in English,
whereas length < 5 in Chinese4), using the GI
encoding resulted in a larger label set in English.
4See the curve DPLVM+GI (CHN) in Figure 7, which
could explain the good results of GI encoding for the Chi-
nese task.
 0
 10
 20
 30
 40
 50
 0  0.2  0.4  0.6  0.8  1
P
er
ce
nt
ag
e 
(%
)
Probability of Viterbi labeling
CRF (ENG)
CRF+GI (ENG)
DPLVM (ENG)
DPLVM+GI (ENG)
DPLVM+GI (CHN)
Figure 7: For various models, the probability dis-
tributions of the produced abbreviations on the test
data of the English abbreviation generation task.
mitomycin C
DPLVM P P MC [Wrong]
DPLVM+GI P1 P2 P3 MMC [Correct]
Figure 8: Example of abbreviations composed
of non-initials generated by the DPLVM and the
DPLVM+GI.
Hence, the features become more sparse than in
the Chinese case.5 Therefore, a significant number
of features could have been inadequately trained,
resulting in Viterbi labelings with low probabili-
ties. For the latent variable approach, its curve
demonstrates that it did not cause a severe data
sparseness problem.
The aforementioned analysis also explains the
poor performance of the DPLVM+GI. However,
the DPLVM+GI can actually produce correct ab-
breviations with ?believable? probabilities (high
probabilities) in some ?difficult? instances. In
Figure 8, the DPLVM produced an incorrect la-
beling for the difficult long form, whereas the
DPLVM+GI produced the correct labeling con-
taining non-initials.
Hence, we present a simple voting method to
better combine the latent variable approach with
the GI encoding method. We refer to this new
combination as GI encoding with ?back-off? (here-
inafter GIB): when the abbreviation generated by
the DPLVM+GI has a ?believable? probability
(p > 0.3 in the present case), the DPLVM+GI
then outputs it. Otherwise, the system ?backs-off?
5In addition, the training data of the English task is much
smaller than for the Chinese task, which could make the mod-
els more sensitive to data sparseness.
910
Model T1A Time
CRF+GIB 67.2 0.6 h
DPLVM+GIB (*) 72.5 1.4 h
Table 4: Re-evaluating Chinese abbreviation gen-
eration with GIB.
Model T1A
Heu (T05) 47.3
MEMM (T05) 55.2
DPLVM (*) 57.5
Table 5: Results of English abbreviation genera-
tion with five-fold cross validation.
to the parameters trained without the GI encoding
(i.e., the DPLVM).
The results in Table 3 demonstrate that the
DPVLM+GIB model significantly outperformed
the other models because the DPLVM+GI model
improved the performance in some ?difficult? in-
stances. The DPVLM+GIB model was robust
even when the data sparseness problem was se-
vere.
By re-evaluating the DPLVM+GIB model for
the previous Chinese abbreviation generation task,
we demonstrate that the back-off method also im-
proved the performance of the Chinese abbrevia-
tion generators (+0.2% from DPLVM+GI; see Ta-
ble 4).
Furthermore, for interests, like Tsuruoka et al
(2005), we performed a five-fold cross-validation
on the corpus. Concerning the training time in
the cross validation, we simply chose the DPLVM
for comparison. Table 5 shows the results of the
DPLVM, the heuristic system (Heu), and the max-
imum entropy Markov model (MEMM) described
by Tsuruoka et al (2005).
5 Recognition as a Generation Task
We directly migrate this model to the abbrevia-
tion recognition task. We simplify the abbrevia-
tion recognition to a restricted generation problem
(see Figure 9). When a context expression (CE)
with a parenthetical expression (PE) is met, the
recognizer generates the Viterbi labeling for the
CE, which leads to the PE or NULL. Then, if the
Viterbi labeling leads to the PE, we can, at the
same time, use the labeling to decide the full form
within the CE. Otherwise, NULL indicates that the
PE is not an abbreviation.
For example, in Figure 9, the recognition is re-
stricted to a generation task with five possible la-
... cannulate for arterial pressure (AP)...
(1) P P AP
(2) P P AP
(3) P P AP
(4) P P AP
(5) SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS NULL
Figure 9: Abbreviation recognition as a restricted
generation problem. In some labelings, the S la-
bels are masked for simplicity.
Model P R F
Schwartz & Hearst (SH) 97.8 94.0 95.9
SaRAD 89.1 91.9 90.5
ALICE 96.1 92.0 94.0
Chang & Schu?tze (CS) 94.2 90.0 92.1
Nadeau & Turney (NT) 95.4 87.1 91.0
Okazaki et al (OZ) 97.3 96.9 97.1
CRF 89.8 94.8 92.1
CRF+GI 93.9 97.8 95.9
DPLVM 92.5 97.7 95.1
DPLVM+GI (*) 94.2 98.1 96.1
Table 6: Results of English abbreviation recogni-
tion.
belings. Other labelings are impossible, because
they will generate an abbreviation that is not AP.
If the first or second labeling is generated, AP is
selected as an abbreviation of arterial pressure. If
the third or fourth labeling is generated, then AP
is selected as an abbreviation of cannulate for ar-
terial pressure. Finally, the fifth labeling (NULL)
indicates that AP is not an abbreviation.
To evaluate the recognizer, we use the corpus6
of Okazaki et al (2008), which contains 864 ab-
breviation definitions collected from 1,000 MED-
LINE scientific abstracts. In implementing the
recognizer, we simply use the model from the ab-
breviation generator, with the same feature tem-
plates (31,868 features) and training method; the
major difference is in the restriction (according to
the PE) of the decoding stage and penalizing the
probability values of the NULL labelings7.
For the evaluation metrics, following Okazaki
et al (2008), we use precision (P = k/m), re-
call (R = k/n), and the F-score defined by
6The previous abbreviation generation corpus is improper
for evaluating recognizers, and there is no related research on
this corpus. In addition, there has been no report of Chinese
abbreviation recognition because there is no data available.
The previous generation corpus (Sun et al, 2008) is improper
because it lacks local contexts.
7Due to the data imbalance of the training corpus, we
found the probability values of the NULL labelings are ab-
normally high. To deal with this imbalance problem, we sim-
ply penalize all NULL labelings by using p = p? 0.7.
911
Model P R F
CRF+GIB 94.0 98.9 96.4
DPLVM+GIB 94.5 99.1 96.7
Table 7: English abbreviation recognition with
back-off.
2PR/(P + R), where k represents #instances in
which the system extracts correct full forms, m
represents #instances in which the system extracts
the full forms regardless of correctness, and n rep-
resents #instances that have annotated full forms.
Following Okazaki et al (2008), we perform 10-
fold cross validation.
We prepared six state-of-the-art abbreviation
recognizers as baselines: Schwartz and Hearst?s
method (SH) (2003), SaRAD (Adar, 2004), AL-
ICE (Ao and Takagi, 2005), Chang and Schu?tze?s
method (CS) (Chang and Schu?tze, 2006), Nadeau
and Turney?s method (NT) (Nadeau and Turney,
2005), and Okazaki et al?s method (OZ) (Okazaki
et al, 2008). Some methods use implementations
on the web, including SH8, CS9, and ALICE10.
The results of other methods, such as SaRAD, NT,
and OZ, are reproduced for this corpus based on
their papers (Okazaki et al, 2008).
As can be seen in Table 6, using the latent vari-
ables significantly improved the performance (see
DPLVM vs. CRF), and using the GI encoding
improved the performance of both the DPLVM
and the CRF. With the F-score of 96.1%, the
DPLVM+GI model outperformed five of six state-
of-the-art abbreviation recognizers. Note that all
of the six systems were specifically designed and
optimized for this recognition task, whereas the
proposed model is directly transported from the
generation task. Compared with the generation
task, we find that the F-measure of the abbrevia-
tion recognition task is much higher. The major
reason for this is that there are far fewer classifi-
cation candidates of the abbreviation recognition
problem, as compared to the generation problem.
For interests, we also tested the effect of the
GIB approach. Table 7 shows that the back-off
method further improved the performance of both
the DPLVM and the CRF model.
8http://biotext.berkeley.edu/software.html
9http://abbreviation.stanford.edu/
10http://uvdb3.hgc.jp/ALICE/ALICE index.html
6 Conclusions and Future Research
We have presented the DPLVM and GI encod-
ing by which to incorporate non-local information
in abbreviating terms. They were competing and
generally the performance of the DPLVM was su-
perior. On the other hand, we showed that the two
approaches were complementary. By combining
these approaches, we were able to achieve state-
of-the-art performance in abbreviation generation
and recognition in the same model, across differ-
ent languages, and with a simple feature set. As
discussed earlier herein, the training data is rela-
tively small. Since there are numerous unlabeled
full forms on the web, it is possible to use a semi-
supervised approach in order to make use of such
raw data. This is an area for future research.
Acknowledgments
We thank Yoshimasa Tsuruoka for providing the
English abbreviation generation corpus. We also
thank the anonymous reviewers who gave help-
ful comments. This work was partially supported
by Grant-in-Aid for Specially Promoted Research
(MEXT, Japan).
References
Eytan Adar. 2004. SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics, 20(4):527?
533.
Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
June A. Barrett and Mandalay Grems. 1960. Abbrevi-
ating words systematically. Communications of the
ACM, 3(5):323?324.
Charles P. Bourne and Donald F. Ford. 1961. A study
of methods for systematically abbreviating english
words and names. Journal of the ACM, 8(4):538?
552.
Jeffrey T. Chang and Hinrich Schu?tze. 2006. Abbre-
viations in biomedical text. In Sophia Ananiadou
and John McNaught, editors, Text Mining for Biol-
ogy and Biomedicine, pages 99?119. Artech House,
Inc.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, CMU.
Yaakov HaCohen-Kerner, Ariel Kass, and Ariel Peretz.
2008. Combined one sense disambiguation of ab-
breviations. In Proceedings of ACL?08: HLT, Short
Papers, pages 61?64, June.
912
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings
of CVPR?07, pages 1?8.
David Nadeau and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
In the 8th Canadian Conference on Artificial Intelli-
gence (AI?2005) (LNAI 3501), page 10 pages.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal optimization. Springer.
Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2008. A discriminative alignment model for
abbreviation recognition. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING?08), pages 657?664, Manch-
ester, UK.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
ACL?02, pages 160?167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of EMNLP?01, pages 126?133.
Leonid Peshkin and Avi Pfeffer. 2003. Bayesian in-
formation extraction network. In Proceedings of IJ-
CAI?03, pages 421?426.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings
of NIPS?08.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In the 8th Pacific Symposium on
Biocomputing (PSB?03), pages 451?462.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. Proceedings of
HLT/NAACL?03.
Xu Sun, Houfeng Wang, and Bo Wang. 2008. Pre-
dicting chinese abbreviations from definitions: An
empirical learning approach using support vector re-
gression. Journal of Computer Science and Tech-
nology, 23(4):602?611.
Kazem Taghva and Jeff Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition (IJ-
DAR), 1(4):191?198.
Yoshimasa Tsuruoka, Sophia Ananiadou, and Jun?ichi
Tsujii. 2005. A machine learning approach to
acronym generation. In Proceedings of the ACL-
ISMB Workshop, pages 25?31.
Jonathan D. Wren and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated con-
struction of comprehensive acronym-definition dic-
tionaries. Methods of Information in Medicine,
41(5):426?434.
Hong Yu, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based ap-
proach for automatically disambiguating biomedical
abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380?404.
913
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358?366,
Beijing, August 2010
A Large Scale Ranker-Based System  
for Search Query Spelling Correction 
 
Jianfeng Gao 
Microsoft Research, Redmond 
jfgao@microsoft.com 
Xiaolong Li 
Microsoft Corporation 
xiaolong.li@microsoft.com 
Daniel Micol 
Microsoft Corporation 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research, Redmond 
chrisq@microsoft.com 
Xu Sun 
University of Tokyo 
xusun@mist.i.u-tokyo.ac.jp 
 
 
Abstract 
This paper makes three significant extensions to a 
noisy channel speller designed for standard writ-
ten text to target the challenging domain of search 
queries. First, the noisy channel model is sub-
sumed by a more general ranker, which allows a 
variety of features to be easily incorporated. Se-
cond, a distributed infrastructure is proposed for 
training and applying Web scale n-gram language 
models. Third, a new phrase-based error model is 
presented. This model places a probability distri-
bution over transformations between multi-word 
phrases, and is estimated using large amounts of 
query-correction pairs derived from search logs. 
Experiments show that each of these extensions 
leads to significant improvements over the state-
of-the-art baseline methods. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods. New 
search queries emerge constantly. As a result, 
many queries contain valid search terms, such as 
proper nouns and names, which are not well es-
tablished in the language. Therefore, recent re-
search has focused on the use of Web corpora 
and search logs, rather than human-compiled lex-
icons, to infer knowledge about spellings and 
word usages in search queries (e.g., Whitelaw et 
al., 2009; Cucerzan and Brill, 2004).  
The spelling correction problem is typically 
formulated under the framework of the noisy 
channel model. Given an input query   
       , we want to find the best spelling correc-
tion           among all candidates: 
         
 
       (1) 
Applying Bayes' Rule, we have 
         
 
           (2) 
where the error model        models the trans-
formation probability from C to Q, and the lan-
guage model (LM)      models the likelihood 
that C is a correctly spelled query. 
This paper extends a noisy channel speller de-
signed for regular text to search queries in three 
ways: using a ranker (Section 3), using Web scale 
LMs (Section 4), and using phrase-based error 
models (Section 5). 
First of all, we propose a ranker-based speller 
that covers the noisy channel model as a special 
case. Given an input query, the system first gen-
erates a short list of candidate corrections using 
the noisy channel model. Then a feature vector is 
computed for each query and candidate correc-
tion pair. Finally, a ranker maps the feature vec-
tor to a real-valued score, indicating the likeli-
hood that this candidate is a desirable correction. 
We will demonstrate that ranking provides a flex-
ible modeling framework for incorporating a 
wide variety of features that would be difficult to 
model under the noisy channel framework. 
Second, we explore the use of Web scale LMs 
for query spelling correction. While traditional 
LM research focuses on how to make the model 
?smarter? via how to better estimate the probabil-
ity of unseen words (Chen and Goodman, 1999); 
and how to model the grammatical structure of 
language (e.g., Charniak, 2001), recent studies 
show that significant improvements can be 
achieved using ?stupid? n-gram models trained 
on very large corpora (e.g., Brants et al, 2007). 
We adopt the latter strategy in this study. We pre-
sent a distributed infrastructure to efficiently train 
and apply Web scale LMs. In addition, we ob-
serve that search queries are composed in a lan-
guage style different from that of regular text. We 
thus train multiple LMs using different texts as-
sociated with Web corpora and search queries. 
Third, we propose a phrase-based error model 
that captures the probability of transforming one 
358
multi-term phrase into another multi-term phrase. 
Compared to traditional error models that account 
for transformation probabilities between single 
characters or substrings (e.g., Kernighan et al, 
1990; Brill and Moore, 2000), the phrase-based 
error model is more effective in that it captures 
inter-term dependencies crucial for correcting 
real-word errors, prevalent in search queries. We 
also present a novel method of extracting large 
amounts of query-correction pairs from search 
logs. These pairs, implicitly judged by millions of 
users, are used for training the error models. 
Experiments show that each of the extensions 
leads to significant improvements over its base-
line methods that were state-of-the-art until this 
work, and that the combined method yields a sys-
tem which outperforms the noisy channel speller 
by a large margin: a 6.3% increase in accuracy on 
a human-labeled query set. 
2 Related Work 
Prior research on spelling correction for regular 
text can be grouped into two categories: correct-
ing non-word errors and real-word errors. The 
former focuses on the development of error mod-
els based on different edit distance functions (e.g., 
Kucich, 1992; Kernighan et al, 1990; Brill and 
Moore, 2000; Toutanova and Moore, 2002). Brill 
and Moore?s substring-based error model, con-
sidered to be state-of-the-art among these models, 
acts as the baseline against which we compare 
our models. On the other hand, real-word spelling 
correction tries to detect incorrect usages of a 
valid word based on its context, such as "peace" 
and "piece" in the context "a _ of cake". N-gram 
LMs and na?ve Bayes classifiers are commonly 
used models (e.g., Golding and Roth, 1996; 
Mangu and Brill, 1997; Church et al, 2007). 
While almost all of the spellers mentioned 
above are based on a pre-defined dictionary (ei-
ther a lexicon against which the edit distance is 
computed, or a set of real-word confusion pairs), 
recent research on query spelling correction fo-
cuses on exploiting noisy Web corpora and query 
logs to infer knowledge about spellings and word 
usag in queries (Cucerzan and Brill 2004; Ahmad 
and Kondrak, 2005; Li et al, 2006; Whitelaw et 
al., 2009).  Like those spellers designed for regu-
lar text, most of these query spelling systems are 
also based on the noisy channel framework. 
3 A Ranker-Based Speller 
The noisy channel model of Equation (2) does 
not have the flexibility to incorporate a wide va-
riety of features useful for spelling correction, 
e.g., whether a candidate appears as a Wikipedia 
document title. We thus generalize the speller to 
a ranker-based system. Let f be a feature vector 
of a query and candidate correction pair (Q, C). 
The ranker maps f to a real value y that indicates 
how likely C is a desired correction. For example, 
a linear ranker maps f to y with a weight vector w 
such as      , where w is optimized for accu-
racy on human-labeled       pairs. Since the 
logarithms of the LM and error model probabili-
ties can be included as features, the ranker covers 
the noisy channel model as a special case. 
For efficiency, our speller operates in two dis-
tinct stages: candidate generation and re-ranking. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. For each term 
q, we consult a lexicon to identify a list of 
spelling suggestions c whose edit distance from q 
is lower than some threshold. Our lexicon con-
tains around 430,000 high frequency query uni-
gram and bigrams collected from 1 year of query 
logs. These suggestions are stored in a lattice.  
We then use a decoder to identify the 20-best 
candidates from the lattice according to Equation 
(2), where the LM is a backoff bigram model 
trained on 1 year of query logs, and the error 
model is approximated by weighted edit distance:  
                         (3) 
The decoder uses a standard two-pass algorithm. 
The first pass uses the Viterbi algorithm to find 
the best C according to the model of Equations 
(2) and (3).  The second pass uses the A-star al-
gorithm to find the 20-best corrections, using the 
Viterbi scores computed at each state in the first 
pass as heuristics. 
The core component in the second stage is a 
ranker, which re-ranks the 20-best candidate cor-
rections using a set of features extracted from 
     . If the top C after re-ranking is different 
from Q, C is proposed as the correction. We use 
96 features in this study. In addition to the two 
features derived from the noisy channel model, 
the rest of the features can be grouped into the 
following 5 categories. 
1. Surface-form similarity features, which 
check whether C and Q differ in certain patterns, 
359
e.g., whether C is transformed from Q by adding 
an apostrophe, or by adding a stop word at the 
beginning or end of Q. 
2. Phonetic-form similarity features, which 
check whether the edit distance between the met-
aphones (Philips, 1990) of a query term and its 
correction candidate is below some thresholds. 
3. Entity features, which check whether the 
original query is likely to be a proper noun based 
on an in-house named entity recognizer. 
4. Dictionary features, which check whether 
a query term or a candidate correction are in one 
or more human-compiled dictionaries, such as the 
extracted Wiki, MSDN, and ODP dictionaries. 
5. Frequency features, which check whether 
the frequency of a query term or a candidate cor-
rection is above certain thresholds in different 
datasets, such as query logs and Web documents. 
4 Web Scale Language Models 
An n-gram LM assigns a probability to a word 
string   
            according to  
    
   ? (  |  
   )
 
   
 ? (  |      
   )
 
   
 (4) 
where the approximation is based on a Markov 
assumption that each word depends only upon the 
immediately preceding n-1 words. In a speller, 
the log of n-gram LM probabilities of an original 
query and its candidate corrections are used as 
features in the ranker.  
While recent research reports the benefits of 
large LMs trained on Web corpora on a variety of 
applications (e.g. Zhang et al, 2006; Brants et al, 
2007), it is also clear that search queries are com-
posed in a language style different from that of 
the body or title of a Web document. Thus, in this 
study we developed a set of large LMs from dif-
ferent text streams of Web documents and query 
logs. Below, we first describe the n-gram LM 
collection used in this study, and then present a 
distributed n-gram LM platform based on which 
these LMs are built and served for the speller. 
4.1 Web Scale Language Models 
Table 1 summarizes the data sets and Web scale 
n-gram LMs used in this study. The collection is 
built from high quality English Web documents 
containing trillions of tokens, served by a popular 
commercial search engine. The collection con-
sists of several data sets built from different Web 
sources, including the different text fields from 
the Web documents (i.e., body, title, and anchor 
texts) and search query logs. The raw texts ex-
tracted from these different sources were pre- 
processed in the following manner: texts are to-
kenized based on white-space and upper case let-
ters are converted to lower case. Numbers are 
retained, and no stemming/inflection is per-
formed. The n-gram LMs are word-based backoff 
models, where the n-gram probabilities are esti-
mated using Maximum Likelihood Estimation 
with smoothing. Specifically, for a trigram mod-
el, the smoothed probability is computed as 
                (5) 
{
               (             )
           
                   
                              
 
where      is the count of the n-gram in the train-
ing corpus and   is a normalization factor.      
is a discount function for smoothing. We use 
modified absolute discounting (Gao et al, 2001), 
whose parameters can be efficiently estimated 
and performance converges to that of more elabo-
rate state-of-the-art techniques like Kneser-Ney 
smoothing in large data (Nguyen et al 2007).  
4.2 Distributed N-gram LM Platform 
The platform is developed on a distributed com-
puting system designed for storing and analyzing 
massive data sets, running on large clusters con-
sisting of hundreds of commodity servers con-
nected via high-bandwidth network.  
We use the SCOPE (Structured Computations 
Optimized for Parallel Execution) programming 
model (Chaiken et al, 2008) to train the Web 
scale n-gram LMs shown in Table 1. The SCOPE 
scripting language resembles SQL which many 
programmers are familiar with. It also supports 
Dataset Body Anchor Title Query 
Total tokens 1.3T 11.0B 257.2B 28.1B 
Unigrams 1.2B 60.3M 150M 251.5M 
Bigrams 11.7B 464.1M 1.1B 1.3B 
Trigrams 60.0B 1.4B 3.1B 3.1B 
4-grams 148.5B 2.3B 5.1B 4.6B 
Size on disk# 12.8TB 183GB 395GB 393GB 
# N-gram entries as well as other model parameters are 
stored. 
Table 1: Statistics of the Web n-gram LMs collection (count 
cutoff = 0 for all models). These models will be accessible at 
Microsoft (2010). 
360
C# expressions so that users can easily plug-in 
customized C# classes. SCOPE supports writing 
a program using a series of simple data transfor-
mations so that users can simply write a script to 
process data in a serial manner without wonder-
ing how to achieve parallelism while the SCOPE 
compiler and optimizer are responsible for trans-
lating the script into an efficient, parallel execu-
tion plan. We illustrate the usage of SCOPE for 
building LMs using the following example of 
counting 5-grams from the body text of English 
Web pages. The flowchart is shown in Figure 1.  
The program is written in SCOPE as a step-
by- step of computation, where a command takes 
the output of the previous command as its input. 
ParsedDoc=SELECT docId, TokenizedDoc 
FROM @?/shares/?/EN_Body.txt? 
USING DefaultTextExtractor; 
NGram=PROCESS ParsedDoc 
PRODUCE NGram, NGcount 
USING NGramCountProcessor(-stream       
TokenizedDoc -order 5 ?bufferSize 
20000000); 
NGramCount=REDUCE NGram 
ON NGram 
PRODUCE NGram, NGcount 
USING NGramCountReducer; 
 
OUTPUT TO @?Body-5-gram-count.txt?; 
The first SCOPE command is a SELECT 
statement that extracts parsed Wed body text. The 
second command uses a build-in Processor 
(NGramCountProcessor) to map the parsed doc-
uments into separate n-grams together with their 
counts. It generates a local hash at each node 
(i.e., a core in a multi-core server) to store the (n-
gram, count) pairs. The third command (RE-
DUCE) aggregates counts from different nodes 
according to the key (n-gram string). The final 
command (OUTPUT) writes out the resulting to a 
data file. 
The smoothing method can be implemented 
similarly by the customized smoothing Proces-
sor/Reducer. They can be imported from the ex-
isting C# codes (e.g., developed for building LMs 
in a single machine) with minor changes.  
It is straightforward to apply the built LMs for 
the ranker in the speller. The n-gram platform 
provides a DLL for n-gram batch lookup. In the 
server, an n-gram LM is stored in the form of 
multiple lists of key-value pairs, where the key is 
the hash of an n-gram string and the value is ei-
ther the n-gram probability or backoff parameter.  
5 Phrase-Based Error Models 
The goal of an error model is to transform a cor-
rectly spelled query C into a misspelled query Q. 
Rather than replacing single words in isolation, 
the phrase-based error model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. The training pro-
cedure closely follows Sun et al (2010). For in-
stance, we might learn that ?theme part? can be 
replaced by ?theme park? with relatively high 
probability, even though ?part? is not a mis-
spelled word. We use this generative story: first 
the correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence 
q1, ?, qk, finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S de-
note the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as bi-
phrases. Finally, let M denote a permutation of K 
elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution 
over rewrite pairs. Let B(C, Q) denote the set of S, 
T, M triples that transform C into Q. Assuming a 
uniform probability over segmentations, the 
phrase-based probability can be defined as: 
Recursive 
Reducer
Node 1 Node 2 Node N?...
?...
Output
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
 
Figure 1. Distributed 5-gram counting. 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative procedure 
behind the phrase-based error model. 
361
       ?                    
            
 (6) 
As is common practice in SMT, we use the max-
imum approximation to the sum:  
          
            
                    (7) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs that will act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be identi-
fied with little ambiguity. Thus we restrict our 
attention to those phrase transformations con-
sistent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1?aJ  be a hidden variable representing 
the word alignment between them. Each ai takes 
on a value ranging from 1 to L indicating its cor-
responding word position in C, or 0 if the ith 
word in Q is unaligned. The cost of assigning k 
to ai is equal to the Levenshtein edit distance 
(Levenshtein, 1966) between the ith word in Q 
and the kth word in C, and the cost of assigning 0 
to ai is equal to the length of the i
th word in Q. 
The least cost alignment A* between Q and C is 
computed efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, 
which we denote as B(C, Q, A*). Here, consisten-
cy requires that if two words are aligned in A*, 
then they must appear in the same bi-phrase (ci, 
qi). Once the word alignment is fixed, the final 
permutation is uniquely determined, so we can 
safely discard that factor. Thus we have: 
          
       
       
         (8) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
         ?         
 
   , (9) 
where          is a phrase transformation prob-
ability, the estimation of which will be described 
in Section 5.2.  
To find the maximum probability assignment 
efficiently, we use a dynamic programming ap-
proach, similar to the monotone decoding algo-
rithm described in Och (2002).  
5.2 Training the Error Model  
Given a set of (Q, C) pairs as training data, we 
follow a method commonly used in SMT (Och 
and Ney, 2004) to extract bi- phrases and esti-
mate their replacement probabilities. A detailed 
description is discussed in Sun et al (2010). 
We now describe how (Q, C) pairs are gener-
ated automatically from massive query reformu-
lation sessions of a commercial Web browser. 
A query reformulation session contains a list 
of URLs that record user behaviors that relate to 
the query reformulation functions, provided by a 
Web search engine. For example, most commer-
cial search engines offer the "did you mean" 
function, suggesting a possible alternate interpre-
tation or spelling of a user-issued query. Figure 3 
shows a sample of the query reformulation ses-
sions that record the "did you mean" sessions 
from three of the most popular search engines. 
These sessions encode the same user behavior: A 
user first queries for "harrypotter sheme part", 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+part&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+part& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+part&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 3.  A sample of query reformulation sessions from 3 
popular search engines. These sessions show that a user first 
issues the query "harrypotter sheme part", and then clicks on 
the resulting spell suggestion "harry potter theme park". 
362
and then clicks on the resulting spelling sugges-
tion "harry potter theme park". We can "reverse-
engineer" the parameters from the URLs of these 
sessions, and deduce how each search engine en-
codes both a query and the fact that a user arrived 
at a URL by clicking on the spelling suggestion 
of the query ? an strong indication that the 
spelling suggestion is desired. In this study, from 
1 year of sessions, we extracted ~120 million 
pairs. We found the data set very clean because 
these spelling corrections are actually clicked, 
and thus judged implicitly, by many users. 
In addition to the "did you mean" functionali-
ty, recently some search engines have introduced 
two new spelling suggestion functions. One is the 
"auto-correction" function, where the search en-
gine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results. The other is the "split 
pane" result page, where one half portion of the 
search results are produced using the original 
query, while the other half, usually visually sepa-
rate portion of results, are produced using the 
auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the session data, is already able to 
correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of un-
derestimating the self-transformation probability 
of a query P(Q2=Q1|Q1), because we only includ-
ed in the training data the pairs where the query is 
different from the correction. To deal with this 
problem, we augmented the training data by in-
cluding correctly spelled queries, i.e., the pairs 
(Q1, Q2) where Q1 = Q2.  First, we extracted a set 
of queries from the sessions where no spell sug-
gestion is presented or clicked on. Second, we 
removed from the set those queries that were rec-
ognized as being auto-corrected by a search en-
gine. We do so by running a sanity check of the 
queries against our baseline noisy channel 
speller, which will be described in Section 6. If 
the system consider a query misspelled, we as-
sumed it an obvious misspelling, and removed it. 
The remaining queries were assumed to be cor-
rectly spelled and were added to the training data. 
6 Experiments 
We perform the evaluation using a manually an-
notated data set containing 24,172 queries sam-
pled from one year?s query logs from a commer-
cial search engine. The spelling of each query is 
manually corrected by four independent annota-
tors. The average length of queries in the data 
sets is 2.7 words. We divided the data set into 
non-overlapped training and test data sets. The 
training data contain 8,515       pairs, among 
which 1,743 queries are misspelled (i.e.    ). 
The test data contain 15,657       pairs, among 
which 2,960 queries are misspelled.  
The speller systems we developed in this 
study are evaluated using the following metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, a t-test 
with a significance level of 0.05. 
In our experiments, all the speller systems are 
ranker-based. Unless otherwise stated, the ranker 
is a two-layer neural net with 5 hidden nodes. 
The free parameters of the neural net are trained 
to optimize accuracy on the training data using 
the back propagation algorithm (Burges et al, 
2005) .  
6.1 System Results 
Table 1 summarizes the main results of different 
spelling systems. Row 1 is the baseline speller 
where the noisy channel model of Equations (2) 
363
and (3) is used. The error model is based on the 
weighted edit distance function and the LM is a 
backoff bigram model trained on 1 year of query 
logs, with count cutoff 30. Row 2 is the speller 
using a linear ranker to incorporate all ranking 
features described in Section 3. The weights of 
the linear ranker are optimized using the Aver-
aged Perceptron algorithm (Freund and Schapire, 
1999). Row 3 is the speller where a nonlinear 
ranker (i.e., 2-layer neural net) is trained atop the 
features. Rows 4, 5 and 6 are systems that incor-
porate the additional features derived from the 
phrase-based error model (PBEM) described in 
Section 5 and the four Web scale LMs (WLMs) 
listed in Table 1. 
The results show that (1) the ranker is a very 
flexible modeling framework where a variety of 
fine-grained features can be easily incorporated, 
and a ranker-based speller outperforms signifi-
cantly (p < 0.01) the traditional system based on 
the noisy channel model (Row 2 vs. Row 1); (2) 
the speller accuracy can be further improved by 
using more sophisticated rankers and learning 
algorithms (Row 3 vs. Row 2); (3) both WLMs 
and PBEM bring significant improvements 
(Rows 4 and 5 vs. Row 3); and (4) interestingly, 
the gains from WLMs and PBEM are additive 
and the combined leads to a significantly better 
speller (Row 6 vs. Rows 4 and 5) than that of 
using either of them individually. 
In what follows, we investigate in detail how 
the WLMs and PBEM trained on massive Web 
content and search logs improve the accuracy of 
the speller system. We will compare our models 
with the state-of-the-art models proposed previ-
ously. From now on, the system listed in Row 3 
of Table 1 will be used as baseline. 
6.2 Language Models 
The quality of n-gram LMs depends on the order 
of the model, the size of the training data, and 
how well the training data match the test data. 
Figure 4 illustrates the perplexity results of the 
four LMs trained on different data sources tested 
on a random sample of 733,147 queries. The re-
sults show that (1) higher order LMs produce 
lower perplexities, especially when moving be-
yond unigram models; (2) as expected, the query 
LMs are most predictive for the test queries, 
though they are from independent query log 
snapshots; (3) although the body LMs are trained 
on much larger amounts of data than the title and 
anchor LMs, the former lead to much higher per-
plexity values, indicating that both title and an-
chor texts are quantitatively much more similar to 
queries than body texts. 
Table 2 summarizes the spelling results using 
different LMs. For comparison, we also built a 4-
gram LM using the Google 1T web 5-gram cor-
pus (Brants and Franz, 2006). This model is re-
ferred to as the G1T model, and is trained using 
the ?stupid backoff? smoothing method (Brants et 
al., 2007). Due to the high count cutoff applied 
by the Google corpus (i.e., n-grams must appear 
at least 40 times to be included in the corpus), we 
found the G1T model results to a higher OOV 
rate (i.e., 6.5%) on our test data than that of the 4 
Web scale LMs (i.e., less than 1%). 
The results in Table 2 are more or less con-
sistent with the perplexity results: the query LM 
is the best performer; there is no significant dif-
ference among the body, title and anchor LMs 
though the body LM is trained on a much larger 
amount of data; and all the 4 Web scale LMs out-
perform the G1T model substantially due to the 
significantly lower OOV rates. 
6.3 Error Models 
This section compares the phrase-based error 
model (PBEM) described in Section 5, with one 
of the state-of-the-art error models, proposed by 
Brill and Moore (2000), henceforth referred to as 
# System Accuracy Precision Recall 
1 Noisy channel 85.3 72.1 35.9 
2 Linear ranker 88.0 74.0 42.8 
3 Nonlinear ranker 89.0 74.1 49.6 
4 3 + PBEM 90.7 78.7 58.2 
5 3 + WLMs 90.4 75.1 58.7 
6 3 + PBEM + WLMs  91.6 79.1 63.9 
Table 1. Summary of spelling correction results. 
 
Figure 4. Perplexity results on test queries, using n-
gram LMs with different orders, derived from differ-
ent data sources. 
 
364
the B&M model. B&M is a substring error mod-
el. It estimates        as 
          
    
           
?        
   
   
  (10) 
where R is a partitioning of correction term c into 
adjacent substrings, and T is a partitioning of 
query term q, such that |T|=|R|. The partitions are 
thus in one-to-one alignment. To train the B&M 
model, we extracted 1 billion term-correction 
pairs       from the set of 120 million query-
correction pairs      , derived from the search 
logs as described in Section 5.2.  
Table 3 summarizes the comparison results. 
Rows 1 and 2 are our ranker-based baseline sys-
tems with and without the error model (EM) fea-
ture. The error model is based on weighted edit 
distance of Eq. (3), where the weights are learned 
on some manually annotated word-correction 
pairs (which is not used in this study). Rows 3 
and 4 are the B&M models using different maxi-
mum substring lengths, specified by L. L=1 re-
duces B&M to the weighted edit distance model 
in Row 2. Rows 5 and 6 are PBEMs with differ-
ent maximum phrase lengths. L=1 reduces PBEM 
to a word-based error model. The results show 
the benefits of capturing context information in 
error models. In particular, the significant im-
provements resulting from PBEM demonstrate 
that the dependencies between words are far 
more effective than that between characters 
(within a word) for spelling correction. This is 
largely due to the fact that there are many real-
word spelling errors in search queries. We also 
notice that PBEM is a more powerful model  than   
# # of word pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 1M 89.15 73.71 50.74 
3 10M 89.22 74.11 50.92 
4 100M 89.20 73.60 51.06 
5 1B 89.21 73.72 50.99 
Table 4. The performance of B&M error model (L=3) as a 
function of the size of training data (# of word pairs). 
# # of (Q, C) pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 5M 89.59 77.01 52.34 
3 15M 90.23 77.87 56.67 
4 45M 90.45 78.56 57.02 
5 120M 90.70 78.49 58.12 
Table 5. The performance of PBEM (L=3) as a function of 
the size of training data (# of (Q, C) pairs). 
B&M in that it can benefit more from increasing-
ly larger training data. As shown in Tables 4 and 
5, whilst the performance of B&M saturates 
quickly with the increase of training data, the per-
formance of PBEM does not appear to have 
peaked ? further improvements are likely given a 
larger data set. 
7 Conclusions and Future Work 
This paper explores the use of massive Web cor-
pora and search logs for improving a ranker- 
based search query speller. We show significant 
improvements over a noisy channel speller using 
fine-grained features, Web scale LMs, and a 
phrase-based error model that captures intern- 
word dependencies. There are several techniques 
we are exploring to make further improvements. 
First, since a query speller is developed for im-
proving the Web search results, it is natural to use 
features from search results in ranking, as studied 
in Chen et al (2007). The challenge is efficiency. 
Second, in addition to query reformulation ses-
sions, we are exploring other search logs from 
which we might extract more       pairs for er-
ror model training. One promising data source is 
clickthrough data (e.g., Agichtein et al 2006; 
Gao et al, 2009). For instance, we might try to 
learn a transformation from the title or anchor 
text of a document to the query that led to a click 
on that document. Finally, the phrase-based error 
model is inspired by phrase-based SMT systems. 
We are introducing more SMT techniques such 
as alignment and translation rule exaction. In a 
broad sense, spelling correction can be viewed as 
a monolingual MT problem where we translate 
bad English queries into good ones. 
# System Accuracy Precision Recall 
1 Baseline 89.0 74.1 49.6 
2 1+ query 4-gram 90.1 75.6 56.3 
3 1 + body 4-gram 89.9 75.7 54.4 
4 1 + title 4-gram 89.8 75.4 54.7 
5 1 + anchor 4-gram 89.9 75.1 55.6 
6 1 + G1T 4-gram 89.4 75.1 51.5 
Table 2. Spelling correction results using different LMs 
trained on different data sources. 
# System Accuracy Precision Recall 
1 Baseline w/o EM 88.6 72.0 47.0 
2 Baseline 89.0 74.1 49.6 
3 1 + B&M, L=1 89.0 73.3 50.1 
4 1 + B&M, L=3 89.2 73.7 51.0 
5 1 + PBEM, L=1 90.1 76.7 55.6 
6 1 + PBEM, L=3 90.7 78.5 58.1 
Table 3. Spelling correction results using different error 
models. 
365
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Kuansan Wang for the 
very helpful discussions and collaboration. The 
work was done when Xu Sun was visiting Mi-
crosoft Research Redmond. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Improv-
ing web search ranking by incorporating user be-
havior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a spelling 
error model from search query logs. In HLT-
EMNLP, pp. 955-962. 
Brants, T., and Franz, A. 2006. Web 1T 5-gram corpus 
version 1.1. Technical report, Google Research. 
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. 
2007. Large language models in machine translation. 
In EMNLP-CoNLL, pp. 858 - 867. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In ACL, 
pp. 286-293. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In ICML, 
pp. 89-96.  
 Chaiken, R., Jenkins, B., Larson, P., Ramsey, B., 
Shakib, D., Weaver, S., and Zhou, J. 2008. SCOPE: 
easy and efficient parallel processing f massive data 
sets. In Proceedings of the VLDB Endowment, pp. 
1265-1276. 
Charniak, E. 2001. Immediate-head parsing for lan-
guage models. In ACL/EACL, pp. 124-131. 
Chen, S. F., and Goodman, J. 1999. An empirical 
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13(10):359-
394. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving que-
ry spelling correction using web search results. In 
EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compressing 
trigram language models with Golomb coding. In 
EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction as 
an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Freund, Y. and Schapire, R. E. 1999. Large margin 
classification using the perceptron algorithm. In 
Machine Learning, 37(3): 277-296. 
Gao, J., Goodman, J., and Miao, J. 2001. The use of 
clustering techniques for language modeling -
application to Asian languages. Computational Lin-
guistics and Chinese Language Processing, 
6(1):27?60, 2001.  
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web search 
ranking. In SIGIR, pp. 355-362.  
Golding, A. R., and Roth, D. 1996. Applying winnow 
to context-sensitive spelling correction. In ICML, pp. 
182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 127-
133. 
Kucich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Surveys, 
24(4):377-439. 
Levenshtein, V. I. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet 
Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. Ex-
ploring distributional similarity based models for 
query spelling correction. In ACL, pp. 1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule acquisi-
tion for spelling correction. In ICML, pp. 187-194. 
Microsoft Microsoft web n-gram services. 2010. 
http://research.microsoft.com/web-ngram 
Nguyen, P., Gao, J., and Mahajan, M. 2007. MSRLM: 
a scalable language modeling toolkit. Technical re-
port TR-2007-144, Microsoft Research. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Philips, L. 1990. Hanging on the metaphone. Comput-
er Language Magazine, 7(12):38-44. 
Sun, X., Gao, J., Micol, D., and Quirk, C. 2010. 
Learning phrase-based spelling error models from 
clickthrough data. In ACL.  
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In ACL, 
pp. 144-151.  
Whitelaw, C., Hutchinson, B., Chung, G. Y., and Ellis, 
G. 2009. Using the web for language independent 
spellchecking and autocorrection. In EMNLP, pp. 
890-899. 
Zhang, Y., Hildebrand, Al. S., and Vogel, S. 2006. 
Distributed language modeling for n-best list re-
ranking. In EMNLP, pp. 216-233. 
366
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 311?321,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring Representations from Unlabeled Data with Co-training
for Chinese Word Segmentation
Longkai Zhang Houfeng Wang? Xu Sun Mairgup Mansur
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cn, mairgup@gmail.com,
Abstract
Nowadays supervised sequence labeling
models can reach competitive performance
on the task of Chinese word segmenta-
tion. However, the ability of these mod-
els is restricted by the availability of an-
notated data and the design of features.
We propose a scalable semi-supervised fea-
ture engineering approach. In contrast
to previous works using pre-defined task-
specific features with fixed values, we dy-
namically extract representations of label
distributions from both an in-domain cor-
pus and an out-of-domain corpus. We
update the representation values with a
semi-supervised approach. Experiments
on the benchmark datasets show that our
approach achieve good results and reach
an f-score of 0.961. The feature engineer-
ing approach proposed here is a general
iterative semi-supervised method and not
limited to the word segmentation task.
1 Introduction
Chinese is a language without natural word
delimiters. Therefore, Chinese Word Segmen-
tation (CWS) is an essential task required by
further language processing. Previous research
shows that sequence labeling models trained on
labeled data can reach competitive accuracy on
the CWS task, and supervised models are more
accurate than unsupervised models (Xue, 2003;
Low et al, 2005). However, the resource of man-
ually labeled training corpora is limited. There-
fore, semi-supervised learning has become one
?Corresponding author
of the most natural forms of training for CWS.
Traditional semi-supervised methods focus on
adding new unlabeled instances to the training
set by a given criterion. The possible mislabeled
instances, which are introduced from the auto-
matically labeled raw data, can hurt the per-
formance and not easy to exclude by setting a
sound selecting criterion.
In this paper, we propose a simple and scal-
able semi-supervised strategy that works by pro-
viding semi-supervision at the level of represen-
tation. Previous works mainly assume that con-
text features are helpful to decide the potential
label of a character. However, when some of the
context features do not appear in the training
corpus, this assumption may fail. An example is
shown in table 1. Although the context of ???
and ??? is totally different, they share a homo-
geneous structure as ?verb-noun?. Therefore. A
much better way is to map the context informa-
tion to a kind of representation. More precisely,
the mapping should let the similar contexts map
to similar representations, while let the distinct
contexts map to distinct representations.
??? ???
Label B B
Character ? ? ? ? ? ?
Context C-1= ? C-1= ?
Features C0= ? C0= ?
C1= ? C1= ?
Table 1: Example of the context of ??? in ???
? (Eat fruits)? and the context of ??? in ????
(Play basketball)?
We use the label distribution information that
311
is extracted from the unlabeled corpus as this
representation to enhance the supervised model.
We add ?pseudo-labels? by tagging the unla-
beled data with the trained model on the train-
ing corpus. These ?pseudo-labels? are not accu-
rate enough. Therefore, we use the label distri-
bution, which is much more accurate.
To accurately calculate the precise label dis-
tribution, we use a framework similar to the co-
training algorithm to adjust the feature values
iteratively. Generally speaking, unlabeled data
can be classified as in-domain data and out-of-
domain data. In previous works these two kinds
of unlabeled data are used separately for differ-
ent purposes. In-domain data is mainly used to
solve the problem of data sparseness (Sun and
Xu, 2011). On the other hand, out-of domain
data is used for domain adaptation (Chang and
Han, 2010). In our work, we use in-domain and
out-of-domain data together to adjust the labels
of the unlabeled corpus.
We evaluate the performance of CWS on the
benchmark dataset of Peking University in the
second International Chinese Word Segmenta-
tion Bakeoff. Experiment results show that our
approach yields improvements compared with
the state-of-art systems. Even when the la-
beled data is insufficient, our methods can still
work better than traditional methods. Com-
pared to the baseline CWS model, which has
already achieved an f-score above 0.95, we fur-
ther reduce the error rate by 15%.
Our method is not limited to word segmen-
tation. It is also applicable to other problems
which can be solved by sequence labeling mod-
els. We also applied our method to the Chi-
nese Named Entity Recognition task, and also
achieved better results compared to traditional
methods.
The main contributions of our work are as fol-
lows:
? We proposed a general method to utilize
the label distribution given text contexts as
representations in a semi-supervised frame-
work. We let the co-training process ad-
just the representation values from label
distribution instead of using manually pre-
defined feature templates.
? Compared with previous work, our method
achieved a new state-of-art accuracy on the
CWS task as well as on the NER task.
The remaining part of this paper is organized
as follows. Section 2 describes the details of the
problem and our algorithm. Section 3 describes
the experiment and presents the results. Section
4 reviews the related work. Section 5 concludes
this paper.
2 System Architecture
2.1 Sequence Labeling
Nowadays the character-based sequence label-
ing approach is widely used for the Chinese word
segmentation problem. It was first proposed in
Xue (2003), which assigns each character a label
to indicate its position in the word. The most
prevalent tag set is the BMES tag set, which
uses 4 tags to carry word boundary information.
This tag set uses B, M, E and S to represent the
Beginning, the Middle, the End of a word and
a Single character forming a word respectively.
We use this tag set in our method. An example
of the ?BMES? representation is shown in table
2.
Character: ? ? ? ? ? ? ?
Tag: S S B E B M E
Table 2: An example for the ?BMES? representa-
tion. The sentence is ????????? (I love Bei-
jing Tian-an-men square), which consists of 4 Chi-
nese words: ??? (I), ??? (love), ???? (Beijing),
and ????? (Tian-an-men square).
2.2 Unlabeled Data
Unlabeled data can be divided into in-domain
data and out-of-domain data. In previous works,
these two kinds of unlabeled data are used sep-
arately for different purposes. In-domain data
only solves the problem of data sparseness (Sun
and Xu, 2011). Out-of domain data is used
only for domain adaptation (Chang and Han,
2010). These two functionalities are not contra-
dictory but complementary. Our study shows
312
that by correctly designing features and algo-
rithms, both in-domain unlabeled data and out-
of-domain unlabeled data can work together to
help enhancing the segmentation model. In our
algorithm, the dynamic features learned from
one corpus can be adjusted incrementally with
the dynamic features learned from the other cor-
pus.
As for the out-of-domain data, it will be even
better if the corpus is not limited to a specific
domain. We choose a Chinese encyclopedia cor-
pus which meets exactly this requirement. We
use the corpus to learn a large set of informative
features. In our experiment, two different views
of features on unlabeled data are considered:
Static Statistical Features (SSFs): These
features capture statistical information of char-
acters and character n-grams from the unlabeled
corpus. The values of these features are fixed
during the training process once the unlabeled
corpus is given.
Dynamic Statistical Features (DSFs):
These features capture label distribution infor-
mation from the unlabeled corpus given fixed
text contexts. As the training process proceeds,
the value of these features will change, since the
trained tagger at each training iteration may as-
sign different labels to the unlabeled data.
2.3 Framework
Suppose we have labeled data L, two unla-
beled corpora Ua and Ub (one is an in-domain
corpus and the other is an out-of-domain cor-
pus). Our algorithm is shown in Table 3.
During each iteration, we tag the unlabeled
corpus Ua using Tb to get pseudo-labels. Then
we extract features from the pseudo-labels. We
use the label distribution information as dy-
namic features. We add these features to the
training data to train a new tagger Ta. To adjust
the feature values, we extract features from one
corpus and then apply the statistics to the other
corpus. This is similar to the principle of co-
training (Yarowsky, 1995; Blum and Mitchell,
1998; Dasgupta et al, 2002). The difference is
that there are not different views of features, but
different kinds of unlabeled data. Detailed de-
scription of features is given in the next section.
Algorithm
Init:
Using baseline features only:
Train an initial tagger T0 based on L ()
Label Ua and Ub individually using T0
BEGIN LOOP:
Generate DSFs from tagged Ua
Augment L with DSFs to get La
Generate DSFs from tagged Ub
Augment L with DSFs to get Lb
Using baseline features, SSFs and DSFs:
Train new tagger Ta using La
Train new tagger Tb using Lb
Label Ua using Tb
Label Ub using Ta
LOOP until performance does not improve
RETURN the tagger which is trained with
in-domain features.
Table 3: Algorithm description
2.4 Features
2.4.1 Baseline Features
Our baseline feature templates include the
features described in previous works (Sun and
Xu, 2011; Sun et al, 2012). These features are
widely used in the CWS task. To be convenient,
for a character ci with context . . . ci?1cici+1 . . .,
its baseline features are listed below:
? Character uni-grams: ck (i? 3 < k < i+3)
? Character bi-grams: ckck+1 (i ? 3 < k <
i+ 2)
? Whether ck and ck+1 are identical (i? 2 <
k < i + 2)
? Whether ck and ck+2 are identical (i? 4 <
k < i + 2)
The last two feature templates are designed to
detect character reduplication, which is a mor-
phological phenomenon in Chinese language.
An example is ?????? (Perfect), which is
a Chinese idiom with structure ?ABAC?.
313
2.4.2 Static statistical features
Statistical features are statistics that distilled
from the large unlabeled corpus. They are
proved useful in the Chinese word segmenta-
tion task. We define Static Statistical Features
(SSFs) as features whose value do not change
during the training process. The SSFs in our
approach includes Mutual information, Punctu-
ation information and Accessor variety. Previ-
ous works have already explored the functions
of the three static statistics in the Chinese word
segmentation task, e.g. Feng et al (2004); Sun
and Xu (2011). We mainly follow their defini-
tions while considering more details and giving
some modification.
Mutual information
Mutual information (MI) is a quantity that
measures the mutual dependence of two random
variables. Previous works showed that larger MI
of two strings claims higher probability that the
two strings should be combined. Therefore, MI
can show the tendency of two strings forming
one word. However, previous works mainly fo-
cused on the balanced case, i.e., the MI of strings
with the same length. In our study we find that,
in Chinese, there remains large amount of imbal-
anced cases, like a string with length 1 followed
by a string with length 2, and vice versa. We
further considered the MI of these string pairs
to capture more information.
Punctuation information
Punctuations can provide implicit labels for
the characters before and after them. The char-
acter after punctuations must be the first char-
acter of a word. The character before punctua-
tions must be the last character of a word. When
a string appears frequently after punctuations,
it tends to be the beginning of a word. The situ-
ation is similar when a string appears frequently
preceding punctuations. Besides, the probabil-
ity of a string appears in the corpus also affects
this tendency. Considering all these factors,
we propose ?punctuation rate? (PR) to capture
this information. For a string with length len
and probability p in the corpus, we define the
left punctuation rate LPRlen as the number of
times the string appears after punctuations, di-
vided by p. Similarly, the right punctuation
rate RPRlen is defines as the number of times
it appears preceding punctuations divided by its
probability p. The length of string we consider
is from 1 to 4.
Accessor variety
Accessor variety (AV) is also known as letter
successor variety (LSV) (Harris, 1955; Hafer and
Weiss, 1974). If a string appears after or pre-
ceding many different characters, this may pro-
vide some information of the string itself. Pre-
vious work of Feng et al (2004), Sun and Xu
(2011) used AV to represent this statistic. Sim-
ilar to punctuation rate, we also consider both
left AV and right AV. For a string s with length
l, we define the left accessor variety (LAV) as
the types of distinct characters preceding s in
the corpus, and the right accessor variety (RAV)
as the types of distinct characters after s in the
corpus. The length of string we consider is also
from 1 to 4.
2.4.3 Dynamic statistical features
The unlabeled corpus lacks precise labels. We
can use the trained tagger to give the unla-
beled data ?pseudo-labels?. These labels can-
not guarantee an acceptable precision. How-
ever, the label distribution will not be largely
affected by small mistakes. Using the label dis-
tribution information is more accurate than us-
ing the pseudo-labels directly.
Based on this assumption, we propose ?dy-
namic statistical features? (DSFs). The DSFs
are intended to capture label distribution infor-
mation given a text context. The word ?Dy-
namic? is in accordance with the fact that these
feature values will change during the training
process.
We give a formal description of DSFs. Sup-
pose there are K labels in our task. For example,
K = 4 if we take BMES labeling method. We
define the whole character sequence with length
n as X = (x1, x2 ? ? ?xj ? ? ?xn). Given a text con-
text Ci, where i is current character position,
the DSFs can be represented as a list,
DSF (Ci) = (DSF (Ci)1, ? ? ? , DSF (Ci)K)
314
Each element in the list represents the proba-
bility of the corresponding label in the distribu-
tion.
For convenience, we further define function
?count(condition)? as the total number of times
a ?condition? is true in the unlabeled corpus.
For example, count (current=?a?) represents the
times the current character equals ?a?, which is
exactly the number of times character ?a? ap-
pears in the unlabeled corpus.
According to different types of text context
Ci, we can divide DSFs into 3 types:
1.Basic DSF
For Basic DSF of Ci, we define D(Ci):
D(Ci) = (D(Ci)1, . . . , D(Ci)K)
We define Basic DSF with current character po-
sition i, text context Ci and label l (the lth di-
mension in the list) as:
D(Ci)l = P (y = l|Ci = xi)
= count(Ci = xi ? y = l)count(Ci = xi)
In this equation, the numerator counts the num-
ber of times current character is xi with label l.
The denominator counts the number of times
current character is xi.
We use the term ?Basic? because this kind of
DSFs only considers the character of position i
as its context. The text context refers to the cur-
rent character itself. This feature captures the
label distribution information given the charac-
ter itself.
2.BigramDSF
Basic DSF is simple and very easy to imple-
ment. The weakness is that it is less power-
ful to describe word-building features. Although
characters convey context information, charac-
ters themselves in Chinese is sometimes mean-
ingless. Character bi-grams can carry more con-
text information than uni-grams. We modify
Basic DSFs to bi-gram level and propose Bigram
DSFs.
For Bigram DSF of Ci, we define B(Ci):
B(Ci) = (B(Ci)1, . . . , B(Ci)K)
We define Bigram DSF with current character
position i, text context Ci and label l (the lth
dimension in the list) as:
B(Ci)l = P (y = l|Ci = xi?jxi?j+1)
= count(Ci = xi?jxi?j+1 ? y = l)count(Ci = xi?jxi?j+1)
j can take value 0 and 1.
In this equation, the numerator counts the
number of times current context is xi?jxi?j+1
with label l. The denominator counts the num-
ber of times current context is xi?jxi?j+1.
3.WindowDSF
Considering Basic DSF and Bigram DSF only
might cause the over-fitting problem, therefore
we introduce another kind of DSF. We call it
Window DSF, which considers the surrounding
context of a character and omits the character
itself.
For Window DSF, we define W (Ci):
W (Ci) = (W (Ci)1, . . . ,W (Ci)K)
We define Window DSF with current character
position i, text context Ci and label l (the lth
dimension in the list) as:
W (Ci)l = P (y = l|Ci = xi?1xi+1)
= count(Ci = xi?1xi+1 ? y = l)count(Ci = xi?1xi+1)
In this equation, the numerator counts the
number of times current context is xi?1xi+1
with label l. The denominator counts the num-
ber of times current context is xi?1xi+1.
2.4.4 Discrete features VS. Continuous
features
The statistical features may be expressed as
real values. A more natural way is to use dis-
crete values to incorporate them into the se-
quence labeling models . Previous works like
Sun and Xu (2011) solve this problem by set-
ting thresholds and converting the real value
into boolean values. We use a different method
to solve this, which does not need to consider
tuning thresholds. In our method, we process
static and dynamic statistical features using dif-
ferent strategies.
315
For static statistical value:
For mutual information, we round the real
value to their nearest integer. For punctuation
rate and accessor variety, as the values tend to
be large, we first get the log value of the feature
and then use the nearest integer as the corre-
sponding discrete value.
For dynamic statistical value:
Dynamic statistical features are distributions
of a label. The values of DSFs are all percentage
values. We can solve this by multiply the proba-
bility by an integer N and then take the integer
part as the final feature value. We set the value
of N by cross-validation..
2.5 Conditional Random Fields
Our algorithm is not necessarily limited to
a specific baseline tagger. For simplicity and
reliability, we use a simple Conditional Ran-
dom Field (CRF) tagger, although other se-
quence labeling models like Semi-Markov CRF
Gao et al (2007) and Latent-variable CRF Sun
et al (2009) may provide better results than
a single CRF. Detailed definition of CRF can
be found in Lafferty et al (2001); McCallum
(2002); Pinto et al (2003).
3 Experiment
3.1 Data and metrics
We used the benchmark datasets provided by
the second International Chinese Word Segmen-
tation Bakeoff1 to test our approach. We chose
the Peking University (PKU) data in our exper-
iment. Although the benchmark provides an-
other three data sets, two of them are data of
traditional Chinese, which is quite different from
simplified Chinese. Another is the data from Mi-
crosoft Research (MSR). We experimented on
this data and got 97.45% in f-score compared
to the state-of-art 97.4% reported in Sun et al
(2012). However, this corpus is much larger
than the PKU corpus. Using the labeled data
alone can get a relatively good tagger and the
unlabeled data contributes little to the perfor-
mance. For simplicity and efficiency, our further
1http://www.sighan.org/bakeoff2005/
experiments are all conducted on the PKU data.
Details of the PKU data are listed in table 4.
We also used two un-segmented corpora as
unlabeled data. The first one is Chinese Giga-
word2 corpus. It is a comprehensive archive of
newswire data. The second one is articles from
Baike3 of baidu.com. It is a Chinese encyclope-
dia similar to Wikipedia but contains more Chi-
nese items and their descriptions. In the exper-
iment we used about 5 million characters from
each corpus for efficiency. Details of unlabeled
data can be found in table 5.
In our experiment, we did not use any ex-
tra resources such as common surnames, part-
of-speech or other dictionaries.
F-score is used as the accuracy measure. We
define precision P as the percentage of words
in the output that are segmented correctly. We
define recall R as the percentage of the words
in reference that are correctly segmented. Then
F-score is as follows:
F = 2 ? P ?RP +R
The recall of out-of-vocabulary is also taken into
consideration, which measures the ability of the
model to correctly segment out of vocabulary
words.
3.2 Main Results
Table 6 summarizes the segmentation results
on test data with different feature combinations.
We performed incremental evaluation. In this
table, we first present the results of the tagger
only using baseline features. Then we show the
results of adding SSF and DSF individually. In
the end we compare the results of combining
SSF and DSF with baseline features.
Because the baseline features is strong to
reach a relative good result, it is not easy to
largely enhance the performance. Neverthe-
less, there are significant increases in f-score and
OOV-Recall when adding these features. From
table 6 we can see that by adding SSF and DSF
individually, the F-score is improved by +1.1%
2http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T09
3http://baike.baidu.com/
316
Identical words Total word Identical Character Total character
5.5 ? 104 1.1 ? 106 5 ? 103 1.8 ? 106
Table 4: Details of the PKU data
Corpus Character used
Gigaword 5000193
Baike 5000147
Table 5: Details of the unlabeled data.
P R F OOV
Baseline 0.950 0.943 0.946 0.676
+SSF 0.961 0.953 0.957 0.728
+DSF 0.958 0.953 0.955 0.678
+SSF+DSF 0.965 0.958 0.961 0.731
Table 6: Segmentation results on test data with
different feature combinations. The symbol ?+?
means this feature configuration contains features set
containing the baseline features and all features after
?+?. The size of unlabeled data is fixed as 5 million
characters.
and +0.9%. The OOV-Recall is also improved,
especially after adding SSFs. When considering
SSF and DSF together, the f-score is improved
by +1.5% while the OOV-Recall is improved by
+5.5%.
To compare the contribution of unlabeled
data, we conduct experiments of using differ-
ent sizes of unlabeled data. Note that the SSFs
are still calculated using all the unlabeled data.
However, each iteration in the algorithm uses
unlabeled data with different sizes.
Table 7 shows the results when changing the
size of unlabeled data. We experimented on
three different sizes: 0.5 million, 1 million and 5
million characters.
P R F OOV
DSF(0.5M) 0.962 0.954 0.958 0.727
DSF(1M) 0.963 0.955 0.959 0.728
DSF(5M) 0.965 0.958 0.961 0.731
Table 7: Comparison of results when changing the
size of unlabeled data. (0.5 million, 1 million and 5
million characters).
We further experimented on unlabeled corpus
with larger size (up to 100 million characters).
However the performance did not change signif-
icantly. Besides, because the number of features
in our method is very large, using too large un-
labeled corpus is intractable in real applications
due to the limitation of memory.
Our method can keep working well even when
the labeled data are insufficient. Table 8 shows
the comparison of f-scores when changing the
size of labeled data. We compared the results
of using all labeled data with 3 different situa-
tions: using 1/10, 1/2 and 1/4 of all the labeled
data. In fact, the best system on the Second In-
ternational Chinese Word Segmentation bakeoff
reached 0.95 in f-score by using all labeled data.
From table 8 we can see that our algorithm only
needs 1/4 of all labeled data to achieve the same
f-score.
Baseline +SSF+DSF Improve
1/10 0.934 0.943 +0.96%
1/4 0.946 0.951 +0.53%
1/2 0.952 0.956 +0.42%
All 0.957 0.961 +0.42%
Table 8: Comparison of f-scores when changing the
size of labeled data. (1/10, 1/4, 1/2 and all labeled
data. The size of unlabeled data is fixed as 5 million
characters.)
We also explored how the performance
changes as iteration increases. Figure 1 shows
the change of F-score during the first 10 itera-
tions. From figure 1 we find that f-score has a
fast improvement in the first few iterations, and
then stables at a fixed point. Besides, as the size
of labeled data increases, it converges faster.
Using an in-domain corpus and an out-of-
domain corpus is better than use one corpus
alone. We compared our approach with the
method which uses only one unlabeled corpus.
To use only one corpus, we modify our algorithm
to extract DSFs from the Chinese Giga word
corpus and apply the learned features to itself.
317
Figure 1: Learning curve of using different size of
labeled data
Table 9 shows the result. We can see that our
method outperforms by +0.2% in f-score and
+0.7% in OOV-Recall.
Finally, we compared our method with the
state-of-art systems reported in the previous pa-
pers. Table 10 listed the results. Best05 repre-
sents the best system reported on the Second In-
ternational Chinese Word Segmentation Bake-
off. CRF + Rule system represents a combina-
tion of CRF model and rule based model pre-
sented in Zhang et al (2006). Other three sys-
tems all represent the methods using their cor-
responding model in the corresponding papers.
Note that these state-of-art systems are either
using complicated models with semi-Markov re-
laxations or latent variables, or modifying mod-
els to fit special conditions. Our system uses a
single CRF model. As we can see in table 10,
our method achieved higher F-scores than the
previous best systems.
3.3 Results on NER task
Our method is not limited to the CWS prob-
lem. It is applicable to all sequence labeling
problems. We applied our method on the Chi-
nese NER task. We used the MSR corpus of
the sixth SIGHAN Workshop on Chinese Lan-
guage Processing. It is the only NER corpus
using simplified Chinese in that workshop. We
compared our method with the pure sequence la-
beling approach in He and Wang (2008). We re-
implemented their method to eliminate the dif-
ference of various CRFs implementations. Ex-
periment results are shown in table 11. We can
see that our methods works better, especially
when handling the out-of-vocabulary named en-
tities;
4 Related work
Recent studies show that character sequence
labeling is an effective method of Chinese word
segmentation for machine learning (Xue, 2003;
Low et al, 2005; Zhao et al, 2006a,b). These su-
pervised methods show good results. Unsuper-
vised word segmentation (Maosong et al, 1998;
Peng and Schuurmans, 2001; Feng et al, 2004;
Goldwater et al, 2006; Jin and Tanaka-Ishii,
2006) takes advantage of the huge amount of raw
text to solve Chinese word segmentation prob-
lems. These methods need no annotated corpus,
and most of them use statistics to help model
the problem. However, they usually are less ac-
curate than supervised ones.
Currently ?feature-engineering? methods
have been successfully applied into NLP ap-
plications. Miller et al (2004) applied this
method to named entity recognition. Koo et al
(2008) applied this method to dependency pars-
ing. Turian et al (2010) applied this method to
both named entity recognition and text chunk-
ing. These papers shared the same concept of
word clustering. However, we cannot simply
equal Chinese character to English word because
characters in Chinese carry much less informa-
tion than words in English and the clustering
results is less meaningful.
Features extracted from large unlabeled cor-
pus in previous works mainly focus on statisti-
cal information of characters. Feng et al (2004)
used the accessor variety criterion to extract
word types. Li and Sun (2009) used punctua-
tion information in Chinese word segmentation
by introducing extra labels ?L? and ?R?. Chang
and Han (2010), Sun and Xu (2011) used rich
statistical information as discrete features in
a sequence labeling framework. All these ap-
proaches can be viewed as using static statistics
features in a supervised approach. Our method
is different from theirs. For the static statistics
features in our approach, we not only consider
richer string pairs with the different lengths, but
also consider term frequency when processing
318
P R F OOV
Using one corpus 0.963 0.955 0.959 0.724
Our method 0.965 0.958 0.961 0.731
Table 9: Comparison of our approach with using only the Gigaword corpus
Method P R F-score
Best05 (Chen et al (2005)) 0.953 0.946 0.950
CRF + rule-system (Zhang et al (2006)) 0.947 0.955 0.951
Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945
Latent-variable CRF (Sun et al (2009)) 0.956 0.948 0.952
ADF-CRF (Sun et al (2012)) 0.958 0.949 0.954
Our method 0.965 0.958 0.961
Table 10: Comparison of our approach with the state-of-art systems
P R F OOV
Traditional 0.925 0.872 0.898 0.712
Our method 0.916 0.887 0.902 0.737
Table 11: Comparison of our approach with tradi-
tional NER systems
punctuation features.
There are previous works using features ex-
tracted from label distribution of unlabeled cor-
pus in NLP tasks. Schapire et al (2002) use a
set of features annotated with majority labels
to boost a logistic regression model. We are
different from their approach because there is
no pseudo-example labeling process in our ap-
proach. Qi et al (2009) investigated on large
set of distribution features and used these fea-
tures in a self-training way. They applied the
method on three tasks: named entity recogni-
tion, POS tagging and gene name recognition
and got relatively good results. Our approach is
different from theirs. Although we all consider
label distribution, the way we use features are
different. Besides, our approach uses two unla-
beled corpora which can mutually enhancing to
get better result.
5 Conclusion and Perspectives
In this paper, we presented a semi-supervised
method for Chinese word segmentation. Two
kinds of new features are used for the itera-
tive modeling: static statistical features and dy-
namic statistical features. The dynamic statis-
tical features use label distribution information
for text contexts, and can be adjusted automat-
ically during the co-training process. Experi-
mental results show that the new features can
improve the performance on the Chinese word
segmentation task. We further conducted exper-
iments to show that the performance is largely
improved, especially when the labeled data is
insufficient.
The proposed iterative semi-supervised
method is not limited to the Chinese word
segmentation task. It can be easily extended
to any sequence labeling task. For example, it
works well on the NER task as well. As our
future work, we plan to apply our method to
other natural language processing tasks, such
as text chunking.
Acknowledgments
This research was partly supported by Ma-
jor National Social Science Fund of China(No.
12&ZD227),National High Technology Research
and Development Program of China (863 Pro-
gram) (No. 2012AA011101) and National Natu-
ral Science Foundation of China (No.91024009).
We also thank Xu Sun and Qiuye Zhao for proof-
reading the paper.
319
References
Blum, A. and Mitchell, T. (1998). Combining
labeled and unlabeled data with co-training.
In Proceedings of the eleventh annual confer-
ence on Computational learning theory, pages
92?100. ACM.
Chang, B. and Han, D. (2010). Enhancing
domain portability of chinese segmentation
model using chi-square statistics and boot-
strapping. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 789?798. Association
for Computational Linguistics.
Chen, A., Zhou, Y., Zhang, A., and Sun, G.
(2005). Unigram language model for chinese
word segmentation. In Proceedings of the
4th SIGHAN Workshop on Chinese Language
Processing, pages 138?141. Association for
Computational Linguistics Jeju Island, Korea.
Dasgupta, S., Littman, M. L., and McAllester,
D. (2002). Pac generalization bounds for co-
training. Advances in neural information pro-
cessing systems, 1:375?382.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75?93.
Gao, J., Andrew, G., Johnson, M., and
Toutanova, K. (2007). A comparative study
of parameter estimation methods for statisti-
cal natural language processing. In ANNUAL
MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page
824.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
the 21st International Conference on Compu-
tational Linguistics and the 44th annual meet-
ing of the Association for Computational Lin-
guistics, pages 673?680. Association for Com-
putational Linguistics.
Hafer, M. A. and Weiss, S. F. (1974). Word seg-
mentation by letter successor varieties. Infor-
mation storage and retrieval, 10(11):371?385.
Harris, Z. S. (1955). From phoneme to mor-
pheme. Language, 31(2):190?222.
He, J. and Wang, H. (2008). Chinese named en-
tity recognition and word segmentation based
on character. In Sixth SIGHAN Workshop on
Chinese Language Processing, page 128.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsu-
pervised segmentation of chinese text by use
of branching entropy. In Proceedings of the
COLING/ACL on Main conference poster
sessions, pages 428?435. Association for Com-
putational Linguistics.
Koo, T., Carreras, X., and Collins, M. (2008).
Simple semi-supervised dependency parsing.
Lafferty, J., McCallum, A., and Pereira, F.
(2001). Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data.
Li, Z. and Sun, M. (2009). Punctuation
as implicit annotations for chinese word
segmentation. Computational Linguistics,
35(4):505?512.
Low, J., Ng, H., and Guo, W. (2005). A
maximum entropy approach to chinese word
segmentation. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language
Processing, volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265?1271. Association for Computa-
tional Linguistics.
McCallum, A. (2002). Efficiently inducing fea-
tures of conditional random fields. In Proceed-
ings of the Nineteenth Conference on Uncer-
tainty in Artificial Intelligence, pages 403?410.
Morgan Kaufmann Publishers Inc.
Miller, S., Guinness, J., and Zamanian, A.
(2004). Name tagging with word clusters
and discriminative training. In Proceedings of
HLT-NAACL, volume 4.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
320
vances in Intelligent Data Analysis, pages
238?247.
Pinto, D., McCallum, A., Wei, X., and Croft,
W. (2003). Table extraction using conditional
random fields. In Proceedings of the 26th an-
nual international ACM SIGIR conference on
Research and development in informaion re-
trieval, pages 235?242. ACM.
Qi, Y., Kuksa, P., Collobert, R., Sadamasa,
K., Kavukcuoglu, K., and Weston, J. (2009).
Semi-supervised sequence labeling with self-
learned features. In Data Mining, 2009.
ICDM?09. Ninth IEEE International Confer-
ence on, pages 428?437. IEEE.
Schapire, R., Rochery, M., Rahim, M., and
Gupta, N. (2002). Incorporating prior
knowledge into boosting. In MACHINE
LEARNING-INTERNATIONAL WORK-
SHOP THEN CONFERENCE-, pages
538?545.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970?979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers),
pages 253?262, Jeju Island, Korea. Associa-
tion for Computational Linguistics.
Sun, X., Zhang, Y., Matsuzaki, T., Tsuruoka,
Y., and Tsujii, J. (2009). A discriminative
latent variable chinese segmenter with hybrid
word/character information. In Proceedings of
Human Language Technologies: The 2009 An-
nual Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics, pages 56?64. Association for Compu-
tational Linguistics.
Turian, J., Ratinov, L., and Bengio, Y. (2010).
Word representations: a simple and gen-
eral method for semi-supervised learning. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguis-
tics, pages 384?394. Association for Compu-
tational Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29?48.
Yarowsky, D. (1995). Unsupervised word sense
disambiguation rivaling supervised methods.
In Proceedings of the 33rd annual meeting
on Association for Computational Linguistics,
pages 189?196. Association for Computational
Linguistics.
Zhang, R., Kikui, G., and Sumita, E. (2006).
Subword-based tagging by conditional ran-
dom fields for chinese word segmentation. In
Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion
Volume: Short Papers, pages 193?196. Asso-
ciation for Computational Linguistics.
Zhang, Y. and Clark, S. (2007). Chi-
nese segmentation with a word-based percep-
tron algorithm. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL
LINGUISTICS, volume 45, page 840.
Zhao, H., Huang, C., and Li, M. (2006a). An
improved chinese word segmentation system
with conditional random field. In Proceed-
ings of the Fifth SIGHAN Workshop on Chi-
nese Language Processing, volume 117. Syd-
ney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field mod-
eling. In Proceedings of PACLIC, volume 20,
pages 87?94.
321
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1405?1414,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Predicting Chinese Abbreviations with Minimum Semantic Unit and
Global Constraints
Longkai Zhang Li Li Houfeng Wang Xu Sun
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
zhlongk@qq.com, {li.l,wanghf,xusun}@pku.edu.cn
Abstract
We propose a new Chinese abbreviation
prediction method which can incorporate
rich local information while generating the
abbreviation globally. Different to previ-
ous character tagging methods, we intro-
duce the minimum semantic unit, which is
more fine-grained than character but more
coarse-grained than word, to capture word
level information in the sequence labeling
framework. To solve the ?character dupli-
cation? problem in Chinese abbreviation
prediction, we also use a substring tagging
strategy to generate local substring tagging
candidates. We use an integer linear pro-
gramming (ILP) formulation with various
constraints to globally decode the final ab-
breviation from the generated candidates.
Experiments show that our method outper-
forms the state-of-the-art systems, without
using any extra resource.
1 Introduction
Abbreviation is defined as a shortened description
of the original fully expanded form. For example,
?NLP? is the abbreviation for the corresponding
full form ?Natural Language Processing?. The ex-
istence of abbreviations makes it difficult to iden-
tify the terms conveying the same concept in the
information retrieval (IR) systems and machine
translation (MT) systems. Therefore, it is impor-
tant to maintain a dictionary of the prevalent orig-
inal full forms and the corresponding abbrevia-
tions.
Previous works on Chinese abbreviation gen-
eration focus on the sequence labeling method,
which give each character in the full form an extra
label to indicate whether it is kept in the abbre-
viation. One drawback of the character tagging
strategy is that Chinese characters only contain
limited amount of information. Using character-
based method alone is not enough for Chinese ab-
breviation generation. Intuitively we can think of a
word as the basic tagging unit to incorporate more
information. However, if the basic tagging unit
is word, we need to design lots of tags to repre-
sent which characters are kept for each unit. For a
word with n characters, we should design at least
2
n
labels to cover all possible situations. This re-
duces the generalization ability of the proposed
model. Besides, the Chinese word segmentation
errors may also hurt the performance. Therefore
we propose the idea of ?Minimum Semantic Unit?
(MSU) which is the minimum semantic unit in
Chinese language. Some of the MSUs are words,
while others are more fine-grained than words.
The task of selecting representative characters in
the full form can be further broken down into se-
lecting representative characters in the MSUs. We
model this using the MSU-based tagging method,
which can both utilize semantic information while
keeping the tag set small.
Meanwhile, the sequence labeling method per-
forms badly when the ?character duplication? phe-
nomenon exists. Many Chinese long phrases con-
tain duplicated characters, which we refer to as
the ?character duplication? phenomenon. There is
no sound criterion for the character tagging mod-
els to decide which of the duplicated character
should be kept in the abbreviation and which one
to be skipped. An example is ????????
??(Beijing University of Aeronautics and Astro-
nautics) whose abbreviation is ????. The char-
acter ??? appears twice in the full form and only
one is kept in the abbreviation. In these cases, we
can break the long phase into local substrings. We
can find the representative characters in the sub-
strings instead of the long full form and let the de-
coding phase to integrate useful information glob-
ally. We utilize this sub-string based approach and
obtain this local tagging information by labeling
1405
on the sub-string of the full character sequence.
Given the MSU-based and substring-based
methods mentioned above, we can get a list of
potential abbreviation candidates. Some of these
candidates may not agree on keeping or skipping
of some specific characters. To integrate their ad-
vantages while considering the consistency, we
further propose a global decoding strategy using
Integer Linear Programming(ILP). The constraints
in ILP can naturally incorporate ?non-local? infor-
mation in contrast to probabilistic constraints that
are estimated from training examples. We can also
use linguistic constraints like ?adjacent identical
characters is not allowed? to decode the correct
abbreviation in examples like the previous ????
example.
Experiments show that our Chinese abbrevia-
tion prediction system outperforms the state-of-
the-art systems. In order to reduce the size of
the search space, we further propose pruning con-
straints that are learnt from the training corpus.
Experiment shows that the average number of con-
straints is reduced by about 30%, while the top-1
accuracy is not affected.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
our method, including the MSUs, the substring-
based tagging strategy and the ILP decoding pro-
cess. Experiments are described in section 3. We
also give a detailed analysis of the results in sec-
tion 3. In section 4 related works are introduced,
and the paper is concluded in the last section.
2 System Architecture
2.1 Chinese Abbreviation Prediction
Chinese abbreviations are generated by selecting
representative characters from the full forms. For
example, the abbreviation of ?????? (Peking
University) is ???? which is generated by se-
lecting the first and third characters, see TABLE
1. This can be tackled from the sequence labeling
point of view.
Full form ? ? ? ?
Status Keep Skip Keep Skip
Result ? ?
Table 1: The abbreviation ???? of the full form
?????? (Peking University)
From TABLE 1 we can see that Chinese abbre-
viation prediction is a problem of selecting repre-
sentative characters from the original full form
1
.
Based on this assumption, previous works mainly
focus on this character tagging schema. In these
methods, the basic tagging unit is the Chinese
character. Each character in the full form is la-
beled as ?K? or ?S?, where ?K? means the current
character should be kept in abbreviation and ?S?
means the current character should be skipped.
However, a Chinese character can only contain
limited amount of information. Using character-
based method alone is not enough for Chinese
abbreviation generation. We introduce an MSU-
based method, which models the process of se-
lecting representative characters given local MSU
information.
2.2 MSU Based Tagging
2.2.1 Minimum Semantic Unit
Because using the character-based method is not
enough for Chinese abbreviation generation, we
may think of word as the basic tagging unit to in-
corporate more information intuitively. In English,
the abbreviations (similar to acronyms) are usually
formed by concatenating initial letters or parts of a
series of words. In other words, English abbrevia-
tion generation is based on words in the full form.
However, in Chinese, word is not the most suit-
able abbreviating unit. Firstly, there is no natural
boundary between Chinese words. Errors from the
Chinese word segmentation tools will accumulate
to harm the performance of abbreviation predic-
tion. Second, it is hard to design a reasonable tag
set when the length of a possible Chinese word is
very long. The second column of TABLE 2 shows
different ways of selecting representative charac-
ters of Chinese words with length 3. For a Chi-
nese compound word with 3 characters, there are 6
possible ways to select characters. In this case we
should have at least 6 kinds of tags to cover all pos-
sible situations. The case is even worse for words
with more complicated structures. A suitable ab-
breviating unit should be smaller than word.
We propose the ?Minimum Semantic Unit
(MSU)? as the basic tagging unit. We define MSU
as follows:
1. A word whose length is less or equal to 2 is
an MSU.
1
A small portion of Chinese abbreviations are not gener-
ated from the full form. For example, the abbreviation of ??
??(Shan Dong Province) is ???. However, we can use a
look-up table to get this kind of abbreviations.
1406
Full form SK Label MSUs
???(nursery) ?/K?/S?/S ??+?
???(allowance) ?/S?/K?/S ??+?
???(Credit card) ?/S?/S?/K ??+?
???(Hydropower Station) ?/K?/K?/S ?+?+?
???(Senate) ?/K?/S?/K ?+?+?
???(Music group) ?/S?/K?/K ??+?
Table 2: Representing characters of Chinese words with length 3 (K for keep and S for skip) and the
corresponding MSUs
2. A word whose length is larger than 2, but
does not contain any MSUs with length equal
to 2. For example, ?????(Railway Sta-
tion) is not an MSU because the first two
characters ????(Train) can form an MSU.
By this definition, all 6 strings in TABLE 2 are
often thought as a word, but they are not MSUs
in our view. Their corresponding MSU forms are
shown in TABLE 2.
We collect all the MSUs from the benchmark
datasets provided by the second International Chi-
nese Word Segmentation Bakeoff
2
. We choose the
Peking University (PKU) data because it is more
fine-grained than all other corpora. Suppose we
represent the segmented data as L (In our case L
is the PKU word segmentation data), the MSU se-
lecting algorithm is shown in TABLE 3.
For a given full form, we first segment it us-
ing a standard word segmenter to get a coarse-
grained segmentation result. Here we use the Stan-
ford Chinese Word Segmenter
3
. Then we use the
MSU set to segment each word using the strategy
of ?Maximum Forward Matching?
4
to get the fine-
grained MSU segmentation result.
2.2.2 Labeling strategy
For MSU-based tagging, we use a labeling method
which uses four tags, ?KSFL?. ?K? stands for
?Keep the whole unit?, ?S? stands for ?Skip the
whole unit?, ?F? stands for ?keep the First charac-
ter of the unit?, and Label ?L? stands for ?keep the
Last character of the unit?. An example is shown
in TABLE 4.
The ?KSFL? tag set is also applicable for MSUs
whose length is greater than 2 (an example is ??
??/chocolate?). By examining the corpus we
find that such MSUs are either kept of skipped in
2
http://www.sighan.org/bakeoff2005/
3
http://nlp.stanford.edu/software/
segmenter.shtml
4
In Chinese, ?Forward? means from left to right.
????????????? (The ab-
breviation is ??????)
KSFL ??/K ??/F ??/S ??/S ?
?/F?/S
Table 4: The abbreviation ?????? of ????
??????? (National Linguistics Work Com-
mittee) based on MSU tagging.
the final abbreviations. Therefore, the labels of
these long MSUs are either ?K? or ?S?. Empirically,
this assumption holds for MSUs, but does not hold
for words
5
.
2.2.3 Feature templates
The feature templates we use are as follows. See
TABLE 5.
1. Word X
i
(?2 ? i ? 2)
2. POS tag of word X
i
(?2 ? i ? 2)
3. Word Bigrams (X
i
, X
i+1
) (?2 ? i ? 1)
4. Type of word X
i
(?2 ? i ? 2)
5. Length of word X
i
(?2 ? i ? 2)
Table 5: Feature templates for unit tagging. X
represents the MSU sequence of the full form. X
i
represents the ith MSU in the sequence.
Templates 1, 2 and 3 express word uni-grams
and bi-grams. In MSU-based tagging, we can uti-
lize the POS information, which we get from the
Stanford Chinese POS Tagger
6
. In template 4, the
type of word refers to whether it is a number, an
English word or a Chinese word. Because the ba-
sic tagging unit is MSU, which carries word infor-
mation, we can use many features that are infeasi-
ble in character-based tagging.
5
In table 2, all examples are partly kept.
6
http://nlp.stanford.edu/software/
tagger.shtml
1407
Init:
Let MSUSet = empty set
For each word w in L:
If Length(w) ? 2
Add w to MSUSet
End if
End for
For each word w in L:
If Length(w) > 2 and no word x in MSUSet is a substring of w
Add w to MSUSet
End if
End for
Return MSUSet
Table 3: Algorithm for collecting MSUs from the PKU corpus
2.2.4 Sequence Labeling Model
The MSU-based method gives each MSU an ex-
tra indicative label. Therefore any sequence label-
ing model is appropriate for the method. Previous
works showed that Conditional Random Fields
(CRFs) can outperform other sequence labeling
models like MEMMs in abbreviation generation
tasks (Sun et al., 2009; Tsuruoka et al., 2005). For
this reason we choose CRFs model in our system.
For a given full form?s MSU list, many can-
didate abbreviations are generated by choosing
the k-best results of the CRFs. We can use the
forward-backward algorithm to calculate the prob-
ability of a specified tagging result. To reduce the
searching complexity in the ILP decoding process,
we delete those candidate tagged sequences with
low probability.
2.3 Substring Based Tagging
As mentioned in the introduction, the sequence
labeling method, no matter character-based or
MSU-based, perform badly when the ?character
duplication? phenomenon exists. When the full
form contains duplicated characters, there is no
sound criterion for the sequence tagging strategy
to decide which of the duplicated character should
be kept in the abbreviation and which one to be
skipped. On the other hand, we can tag the sub-
strings of the full form to find the local represen-
tative characters in the substrings of the long full
form. Therefore, we propose the sub-string based
approach to given labeling results on sub-strings.
These results can be integrated into a more accu-
rate result using ILP constraints, which we will de-
scribe in the next section.
Another reason for using the sub-string based
methods is that long full forms contain more char-
acters and are much easier to make mistakes dur-
ing the sequence labeling phase. Zhang et al.
(2012) shows that if the full form contains less
than 5 characters, a simple tagger can reach an ac-
curacy of 70%. Zhang et al. (2012) also shows that
if the full form is longer than 10 characters, the
average accuracy is less than 30%. The numerous
potential candidates make it hard for the tagger to
choose the correct one. For the long full forms,
although the whole sequence is not correctly la-
beled, we find that if we only consider its short
substrings, we may find the correct representative
characters. This information can be integrated into
the decoding model to adjust the final result.
We use the MSU-based tagging method in the
sub-string tagging. The labeling strategy and fea-
ture templates are the same to the MSU-based tag-
ging method. In practice, enumerating all sub-
sequences of a given full form is infeasible if the
full form is very long. For a given full form,
we use the boundary MSUs to reduce the pos-
sible sub-sequence set. For example, ????
???(Chinese Academy of Science) has 5 sub-
sequences: ????, ??????, ????, ???
?? and ???.
2.4 ILP Formulation of Decoding
Given the MSU-based and sub-sequence-based
methods mentioned above as well as the preva-
lent character-based methods, we can get a list
of potential abbreviation candidates and abbrevi-
ated substrings. We should integrate their advan-
tages while keeping the consistency between each
1408
candidate. Therefore we further propose a global
decoding strategy using Integer Linear Program-
ming(ILP). The constraints in ILP can naturally
incorporate ?non-local? information in contrast to
probabilistic constraints that are estimated from
training examples. We can also use linguistic con-
straints like ?adjacent identical characters is not
allowed? to decode the correct abbreviation in ex-
amples like the ???? example in section 1.
Formally, given the character sequence of the
full form c = c
1
...c
l
, we keep Q top-ranked
MSU-based tagging results T=(T
1
, ..., T
Q
) and M
tagged substrings S=(S
1
, ..., S
M
) using the meth-
ods described in previous sections. We also
use N top-ranked character-based tagging results
R=(R
1
, ..., R
N
) based on the previous character-
based works. We also define the setU = S?R?T
as the union of all candidate sequences. Our goal
is to find an optimal binary variable vector solution
~v = ~x~y~z = (x
1
, ..., x
M
, y
1
, ..., y
N
, z
1
, ..., z
Q
) that
maximizes the object function:
?
1
M
?
i=1
score(S
i
) ? x
i
+ ?
2
N
?
i=1
score(R
i
) ? y
i
+?
3
Q
?
i=1
score(T
i
) ? z
i
subject to constrains in TABLE 6. The parame-
ters ?
1
, ?
2
, ?
3
controls the preference of the three
parts, and can be decided using cross-validation.
Constraint 1 indicates that x
i
, y
i
, z
i
are all
boolean variables. They are used as indicator vari-
ables to show whether the corresponding tagged
sequence is in accordance with the final result.
Constraint 2 is used to guarantee that at most
one candidate from the character-based tagging is
preserved. We relax the constraint to allow the
sum to be zero in case that none of the top-ranked
candidate is suitable to be the final result. If the
sum equals zero, then the sub-sequence based tag-
ging method will generate a more suitable result.
Constrain 3 has the same utility for the MSU-
based tagging.
Constraint 4, 5, 6 are inter-method constraints.
We use them to guarantee that the labels of the
preserved sequences of different tagging methods
do not conflict with each other. Constraint 7 is
used to guarantee that the labels of the preserved
sub-strings do not conflict with each other.
Constraint 8 is used to solve the ?character du-
plicate? problem. When two identical characters
are kept adjacently, only one of them will be kept.
Which one will be kept depends on the global de-
coding score. This is the advantage of ILP against
traditional sequence labeling methods.
2.5 Pruning Constraints
The efficiency of solving the ILP decoding prob-
lem depends on the number of candidate tagging
sequences N and Q, as well as the number of sub-
sequences M. Usually, N and Q is less than 10 in
our experiment. Therefore, M influences the time
complexity the most. Because we use the bound-
ary of MSUs instead of enumerating all possible
subsequences, the value of M can be largely re-
duced.
Some characters are always labeled as ?S? or
?K? once the context is given. We can use this
phenomenon to reduce the search space of decod-
ing. Let c
i
denote the i
th
character relative to the
current character c
0
and t
i
denote the tag of c
i
. The
context templates we use are listed in TABLE 7.
Uni-gram Contexts c
0
, c
?1
, c
1
Bi-gram Contexts c
?1
c0, c
?1
c
1
, c
0
c
1
Table 7: Context templates used in pruning
With respect to a training corpus, if a context
C relative to c
0
always assigns a certain tag t to
c
0
, then we can use this constraint in pruning. We
judge the degree of ?always? by checking whether
count(C?t
0
=t)
count(C)
> threshold. The threshold is a
non-negative real number under 1.0.
3 Experiments
3.1 Data and Evaluation Metric
We use the abbreviation corpus provided by Insti-
tute of Computational Linguistics (ICL) of Peking
University in our experiments. The corpus is sim-
ilar to the corpus used in Sun et al. (2008, 2009);
Zhang et al. (2012). It contains 8, 015 Chinese ab-
breviations, including noun phrases, organization
names and some other types. Some examples are
presented in TABLE 8. We use 80% abbreviations
as training data and the rest as testing data. In
some cases, a long phrase may contain more than
one abbreviation. For these cases, the corpus just
keeps their most commonly used abbreviation for
each full form.
The evaluation metric used in our experiment
is the top-K accuracy, which is also used by
Tsuruoka et al. (2005), Sun et al. (2009) and
1409
1. x
i
? {0, 1}, y
i
? {0, 1}, z
i
? {0, 1}
2.
?
N
i=1
y
i
? 1
3.
?
Q
i=1
z
i
? 1
4. ?R
i
? R, S
j
? S, if R
i
and S
j
have a same position but the position gets different labels,
then y
i
+ x
j
? 1
5. ?T
i
? T , S
j
? S, if T
i
and S
j
have a same position but the position gets different labels,
then z
i
+ x
j
? 1
6. ?R
i
? R, T
j
? T , if R
i
and T
j
have a same position but the position gets different labels,
then x
i
+ z
j
? 1
7. ?S
i
, S
j
? S if S
i
and S
j
have a same position but the position gets different labels, then
z
i
+ z
j
? 1
8. ?S
i
, S
j
? S if the last character S
i
keeps is the same as the first character S
j
keeps, then
z
i
+ z
j
? 1
Table 6: Constraints for ILP
Type Full form Abbreviation
Noun Phrase ????(Excellent articles) ??
Organization ????(Writers? Association) ??
Coordinate phrase ????(Injuries and deaths) ??
Proper noun ????(Media) ??
Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun)
Zhang et al. (2012). The top-K accuracy measures
what percentage of the reference abbreviations are
found if we take the top N candidate abbreviations
from all the results. In our experiment, top-10 can-
didates are considered in re-ranking phrase and the
measurement used is top-1 accuracy (which is the
accuracy we usually refer to) because the final aim
of the algorithm is to detect the exact abbreviation.
CRF++
7
, an open source linear chain CRF tool,
is used in the sequence labeling part. For ILP part,
we use lpsolve
8
, which is also an open source tool.
The parameters of these tools are tuned through
cross-validation on the training data.
3.2 Results
TABLE 9 shows the top-K accuracy of the
character-based and MSU-based method. We can
see that the MSU-based tagging method can uti-
lize word information, which can get better perfor-
mance than the character-based method. We can
also figure out that the top-5 candidates include the
reference abbreviation for most full forms. There-
fore reasonable decoding by considering all possi-
ble labeling of sequences may improve the perfor-
mance. Although the MSU-based methods only
outperforms character-based methods by 0.75%
7
http://crfpp.sourceforge.net/
8
http://lpsolve.sourceforge.net/5.5/
for top-1 accuracy, it is much better when consid-
ering top-2 to top-5 accuracy (+2.5%). We further
select the top-ranked candidates for ILP decod-
ing. Therefore the MSU-based method can further
improve the performance in the global decoding
phase.
K char-based MSU-based
1 0.5714 0.5789
2 0.6879 0.7155
3 0.7681 0.7819
4 0.8070 0.8283
5 0.8333 0.8583
Table 9: Top-K (K ? 5) results of character-based
tagging and MSU-based tagging
We then use the top-5 candidates of character-
based method and MSU-based method, as well
as the top-2 results of sub-sequence labeling in
the ILP decoding phase. Then we select the top-
ranked candidate as the final abbreviation of each
instance. TABLE 10 shows the results. We can see
that the accuracy of our method is 61.0%, which
improved by +3.89% compared to the character-
based method, and +3.14% compared to the MSU-
based method.
We find that the ILP decoding phase do play
an important role in generating the right an-
1410
Method Top-1 Accuracy
Char-based 0.5714
MSU-based 0.5789
ILP Result 0.6103
Table 10: Top-1 Accuracy after ILP decoding
swer. Some reference abbreviations which are not
picked out by either tagging method can be found
out after decoding. TABLE 11 shows the exam-
ple of the organization name ?????????
???? (Higher Education Admissions Office).
Neither the character-based method nor the MSU-
based method finds the correct answer ?????,
while after ILP decoding, ????? becomes the
final result. TABLE 12 and TABLE 13 give two
more examples.
True Result ???
Char-based ??
MSU-based ???
ILP Decoding ???
Table 11: Top-1 result of ??????????
??? (Higher Education Admissions Office)
True Result ??
Char-based ??
MSU-based ???
ILP Decoding ??
Table 12: Top-1 result of ?????? (Articles
exceed the value)
True Result ????
Char-based ???
MSU-based ???
ILP Decoding ????
Table 13: Top-1 result of ??????????
(Visual effects of sound and lights)
3.3 Improvements Considering Length
Full forms that are longer than five characters are
long terms. Long terms contain more characters,
which is much easier to make mistakes. Figure
1 shows the top-1 accuracy respect to the term
length using different tagging methods and using
ILP decoding. The x-axis represents the length of
the full form. The y-axis represents top-1 accu-
racy. We find that our method works especially
better than pure character-based or MSU-based
approach when the full form is long. By decod-
ing using ILP, both local and global information
are incorporated. Therefore many of these errors
can be eliminated.
Figure 1: Top-1 accuracy of different methods
considering length
3.4 Effect of pruning
As discussed in previous sections, if we are able
to pre-determine that some characters in a certain
context should be kept or skipped, then the num-
ber of possible boolean variable x can be reduced.
TABLE 14 shows the differences. To guarantee
a high accuracy, we set the threshold to be 0.99.
When the original full form is partially tagged by
the pruning constraints, the number of boolean
variables per full form is reduced from 34.4 to
25.5. By doing this, we can improve the predic-
tion speed over taking the raw input.
From TABLE 14 we can also see that the top-
1 accuracy is not affected by these pruning con-
straints. This is obvious, because CRF itself has
a strong modeling ability. The pruning constraints
cannot improve the model accuracy. But they can
help eliminate those false candidates to make the
ILP decoding faster.
Accuracy Average length Time(s)
raw 0.6103 34.4 12.5
pruned 0.6103 25.5 7.1
Table 14: Comparison of testing time of raw input
and pruned input
3.5 Compare with the State-of-the-art
Systems
We also compare our method with previous meth-
ods, including Sun et al. (2009) and Zhang et al.
(2012). Because we use a different corpus, we
re-implement the system Sun et al. (2009), Zhang
1411
et al. (2012) and Sun et al. (2013), and experi-
ment on our corpus. The first two are CRF+GI
and DPLVM+GI in Sun et al. (2009), which are
reported to outperform the methods in Tsuruoka
et al. (2005) and Sun et al. (2008). For DPLVM
we use the same model in Sun et al. (2009) and
experiment on our own data. We also compare
our approach with the method in Zhang et al.
(2012). However, Zhang et al. (2012) uses dif-
ferent sources of search engine result information
to re-rank the original candidates. We do not use
any extra web resources. Because Zhang et al.
(2012) uses web information only in its second
stage, we use ?BIEP?(the tag set used by Zhang
et al. (2012)) to denote the first stage of Zhang
et al. (2012), which also uses no web information.
TABLE 15 shows the results of the comparisons.
We can see that our method outperforms all other
methods which use no extra resource. Because
Zhang et al. (2012) uses extra web resource, the
top-1 accuracy of Zhang et al. (2012) is slightly
better than ours.
Method Top-1 Accuracy
CRF+GI 0.5850
DPLVM+GI 0.5990
BIEP 0.5812
Zhang et al. (2012) 0.6205
Our Result 0.6103
Table 15: Comparison with the state-of-the-art
systems
4 Related Work
Previous research mainly focuses on ?abbrevia-
tion disambiguation?, and machine learning ap-
proaches are commonly used (Park and Byrd,
2001; HaCohen-Kerner et al., 2008; Yu et al.,
2006; Ao and Takagi, 2005). These ways of link-
ing abbreviation pairs are effective, however, they
cannot solve our problem directly. In many cases
the full form is definite while we don?t know the
corresponding abbreviation.
To solve this problem, some approaches main-
tain a database of abbreviations and their corre-
sponding ?full form? pairs. The major problem
of pure database-building approach is obvious. It
is impossible to cover all abbreviations, and the
building process is quit laborious. To find these
pairs automatically, a powerful approach is to find
the reference for a full form given the context,
which is referred to as ?abbreviation generation?.
There is research on heuristic rules for gen-
erating abbreviations Barrett and Grems (1960);
Bourne and Ford (1961); Taghva and Gilbreth
(1999); Park and Byrd (2001); Wren et al. (2002);
Hearst (2003). Most of them achieved high per-
formance. However, hand-crafted rules are time
consuming to create, and it is not easy to transfer
the knowledge of rules from one language to an-
other.
Recent studies of abbreviation generation have
focused on the use of machine learning tech-
niques. Sun et al. (2008) proposed a supervised
learning approach by using SVM model. Tsu-
ruoka et al. (2005); Sun et al. (2009) formal-
ized the process of abbreviation generation as a
sequence labeling problem. In Tsuruoka et al.
(2005) each character in the full form is associated
with a binary value label y, which takes the value
S (Skip) if the character is not in the abbreviation,
and value P (Preserve) if the character is in the ab-
breviation. Then a MEMM model is used to model
the generating process. Sun et al. (2009) followed
this schema but used DPLVM model to incor-
porate both local and global information, which
yields better results. Sun et al. (2013) also uses
machine learning based methods, but focuses on
the negative full form problem, which is a little
different from our work.
Besides these pure statistical approaches, there
are also many approaches using Web as a corpus
in machine learning approaches for generating ab-
breviations.Adar (2004) proposed methods to de-
tect such pairs from biomedical documents. Jain
et al. (2007) used web search results as well as
search logs to find and rank abbreviates full pairs,
which show good result. The disadvantage is that
search log data is only available in a search en-
gine backend. The ordinary approaches do not
have access to search engine internals. Zhang et al.
(2012) used web search engine information to re-
rank the candidate abbreviations generated by sta-
tistical approaches. Compared to their approaches,
our method uses no extra resource, but reaches
comparable results.
ILP shows good results in many NLP tasks.
Punyakanok et al. (2004); Roth and Yih (2005)
used it in semantic role labeling (SRL). Martins
et al. (2009) used it in dependency parsing. (Zhao
and Marcus, 2012) used it in Chinese word seg-
mentation. (Riedel and Clarke, 2006) used ILP
1412
in dependency parsing. However, previous works
mainly focus on the constraints of avoiding bound-
ary confliction. For example, in SRL, two argu-
ment of cannot overlap. In CWS, two Chinese
words cannot share a same character. Different to
their methods, we investigate on the conflict of la-
bels of character sub-sequences.
5 Conclusion and Future work
We propose a new Chinese abbreviation predic-
tion method which can incorporate rich local in-
formation while generating the abbreviation glob-
ally. We propose the MSU, which is more coarse-
grained than character but more fine-grained than
word, to capture word information in the se-
quence labeling framework. Besides the MSU-
based method, we use a substring tagging strategy
to generate local substring tagging candidates. We
use an ILP formulation with various constraints
to globally decode the final abbreviation from the
generated candidates. Experiments show that our
method outperforms the state-of-the-art systems,
without using any extra resource. This method
is not limited to Chinese abbreviation generation,
it can also be applied to similar languages like
Japanese.
The results are promising and outperform the
baseline methods. The accuracy can still be im-
proved. Potential future works may include using
semi-supervised methods to incorporate unlabeled
data and design reasonable features from large cor-
pora. We are going to study on these issues in the
future.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018,61300063),Major
National Social Science Fund of
China(No.12&ZD227), National High Tech-
nology Research and Development Program of
China (863 Program) (No. 2012AA011101), and
Doctoral Fund of Ministry of Education of China
(No. 20130001120004). The contact author of
this paper, according to the meaning given to
this role by Key Laboratory of Computational
Linguistics, Ministry of Education, School of
Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang. We thank
Ke Wu for part of our work is inspired by his
previous work at KLCL.
References
Adar, E. (2004). Sarad: A simple and ro-
bust abbreviation dictionary. Bioinformatics,
20(4):527?533.
Ao, H. and Takagi, T. (2005). Alice: an algorithm
to extract abbreviations from medline. Journal
of the American Medical Informatics Associa-
tion, 12(5):576?586.
Barrett, J. and Grems, M. (1960). Abbreviating
words systematically. Communications of the
ACM, 3(5):323?324.
Bourne, C. and Ford, D. (1961). A study of
methods for systematically abbreviating english
words and names. Journal of the ACM (JACM),
8(4):538?552.
HaCohen-Kerner, Y., Kass, A., and Peretz, A.
(2008). Combined one sense disambiguation
of abbreviations. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Tech-
nologies: Short Papers, pages 61?64. Associa-
tion for Computational Linguistics.
Hearst, M. S. (2003). A simple algorithm for
identifying abbreviation definitions in biomed-
ical text.
Jain, A., Cucerzan, S., and Azzam, S. (2007).
Acronym-expansion recognition and ranking on
the web. In Information Reuse and Integration,
2007. IRI 2007. IEEE International Conference
on, pages 209?214. IEEE.
Martins, A. F., Smith, N. A., and Xing, E. P.
(2009). Concise integer linear programming
formulations for dependency parsing. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1,
pages 342?350. Association for Computational
Linguistics.
Park, Y. and Byrd, R. (2001). Hybrid text mining
for finding abbreviations and their definitions.
In Proceedings of the 2001 conference on em-
pirical methods in natural language processing,
pages 126?133.
Punyakanok, V., Roth, D., Yih, W.-t., and Zimak,
D. (2004). Semantic role labeling via integer
linear programming inference. In Proceedings
of the 20th international conference on Compu-
1413
tational Linguistics, page 1346. Association for
Computational Linguistics.
Riedel, S. and Clarke, J. (2006). Incremental in-
teger linear programming for non-projective de-
pendency parsing. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 129?137. Associ-
ation for Computational Linguistics.
Roth, D. and Yih, W.-t. (2005). Integer linear
programming inference for conditional random
fields. In Proceedings of the 22nd international
conference on Machine learning, pages 736?
743. ACM.
Sun, X., Li, W., Meng, F., and Wang, H. (2013).
Generalized abbreviation prediction with nega-
tive full forms and its application on improv-
ing chinese web search. In Proceedings of the
Sixth International Joint Conference on Natural
Language Processing, pages 641?647, Nagoya,
Japan. Asian Federation of Natural Language
Processing.
Sun, X., Okazaki, N., and Tsujii, J. (2009). Ro-
bust approach to abbreviating terms: A discrim-
inative latent variable model with global infor-
mation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 905?913. Association
for Computational Linguistics.
Sun, X., Wang, H., and Wang, B. (2008). Pre-
dicting chinese abbreviations from definitions:
An empirical learning approach using support
vector regression. Journal of Computer Science
and Technology, 23(4):602?611.
Taghva, K. and Gilbreth, J. (1999). Recognizing
acronyms and their definitions. International
Journal on Document Analysis and Recogni-
tion, 1(4):191?198.
Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005).
A machine learning approach to acronym gen-
eration. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontolo-
gies and Databases: Mining Biological Seman-
tics, pages 25?31. Association for Computa-
tional Linguistics.
Wren, J., Garner, H., et al. (2002). Heuristics
for identification of acronym-definition patterns
within text: towards an automated construc-
tion of comprehensive acronym-definition dic-
tionaries. Methods of information in medicine,
41(5):426?434.
Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,
J. (2006). A large scale, corpus-based approach
for automatically disambiguating biomedical
abbreviations. ACM Transactions on Informa-
tion Systems (TOIS), 24(3):380?404.
Zhang, L., Li, S., Wang, H., Sun, N., and Meng,
X. (2012). Constructing Chinese abbreviation
dictionary: A stacked approach. In Proceedings
of COLING 2012, pages 3055?3070, Mumbai,
India. The COLING 2012 Organizing Commit-
tee.
Zhao, Q. and Marcus, M. (2012). Exploring deter-
ministic constraints: from a constrained english
pos tagger to an efficient ilp solution to chinese
word segmentation. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1054?1062, Jeju Island, Korea. Associa-
tion for Computational Linguistics.
1414
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1881?1890,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Coarse-grained Candidate Generation and Fine-grained Re-ranking for
Chinese Abbreviation Prediction
Longkai Zhang Houfeng Wang Xu Sun
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
zhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cn
Abstract
Correctly predicting abbreviations given
the full forms is important in many natu-
ral language processing systems. In this
paper we propose a two-stage method to
find the corresponding abbreviation given
its full form. We first use the contextual
information given a large corpus to get ab-
breviation candidates for each full form
and get a coarse-grained ranking through
graph random walk. This coarse-grained
rank list fixes the search space inside the
top-ranked candidates. Then we use a sim-
ilarity sensitive re-ranking strategy which
can utilize the features of the candidates
to give a fine-grained re-ranking and se-
lect the final result. Our method achieves
good results and outperforms the state-of-
the-art systems. One advantage of our
method is that it only needs weak super-
vision and can get competitive results with
fewer training data. The candidate genera-
tion and coarse-grained ranking is totally
unsupervised. The re-ranking phase can
use a very small amount of training data
to get a reasonably good result.
1 Introduction
Abbreviation Prediction is defined as finding the
meaningful short subsequence of characters given
the original fully expanded form. As an example,
?HMM? is the abbreviation for the correspond-
ing full form ?Hidden Markov Model?. While
the existence of abbreviations is a common lin-
guistic phenomenon, it causes many problems like
spelling variation (Nenadi?c et al., 2002). The dif-
ferent writing manners make it difficult to identify
the terms conveying the same concept, which will
hurt the performance of many applications, such
as information retrieval (IR) systems and machine
translation (MT) systems.
Previous works mainly treat the Chinese ab-
breviation generation task as a sequence labeling
problem, which gives each character a label to in-
dicate whether the given character in the full form
should be kept in the abbreviation or not. These
methods show acceptable results. However they
rely heavily on the character-based features, which
means it needs lots of training data to learn the
weights of these context features. The perfor-
mance is good on some test sets that are similar to
the training data, however, when it moves to an un-
seen context, this method may fail. This is always
true in real application contexts like the social me-
dia where there are tremendous new abbreviations
burst out every day.
A more intuitive way is to find the full-
abbreviation pairs directly from a large text cor-
pus. A good source of texts is the news texts. In
a news text, the full forms are often mentioned
first. Then in the rest of the news its corresponding
abbreviation is mentioned as an alternative. The
co-occurrence of the full form and the abbrevia-
tion makes it easier for us to mine the abbreviation
pairs from the large amount of news texts. There-
fore, given a long full form, we can generate its
abbreviation candidates from the given corpus, in-
stead of doing the character tagging job.
For the abbreviation prediction task, the candi-
date abbreviation must be a sub-sequence of the
given full form. An intuitive way is to select
all the sub-sequences in the corpus as the can-
didates. This will generate large numbers of ir-
relevant candidates. Instead, we use a contextual
graph random walk method, which can utilize the
contextual information through the graph, to select
a coarse grained list of candidates given the full
form. We only select the top-ranked candidates to
reduce the search space. On the other hand, the
candidate generation process can only use limited
contextual information to give a coarse-grained
ranked list of candidates. During generation, can-
1881
didate level features cannot be included. There-
fore we propose a similarity sensitive re-ranking
method to give a fine-grained ranked list. We then
select the final result based on the rank of each
candidate.
The contribution of our work is two folds.
Firstly we propose an improved method for abbre-
viation generation. Compared to previous work,
our method can perform well with less training
data. This is an advantage in the context of so-
cial media. Secondly, we build a new abbreviation
corpus and make it publicly available for future re-
search on this topic.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
the abbreviation task. In section 3 we describe
the candidate generation part and in section 4 we
describe the re-ranking part. Experiments are de-
scribed in section 5. We also give a detailed anal-
ysis of the results in section 5. In section 6 related
works are introduced, and the paper is concluded
in the last section.
2 Chinese Abbreviation Prediction
System
Chinese Abbreviation Prediction is the task of
selecting representative characters from the long
full form
1
. Previous works mainly use the se-
quence labeling strategies, which views the full
form as a character sequence and give each char-
acter an extra label ?Keep? or ?Skip? to indicate
whether the current character should be kept in
the abbreviation. An example is shown in Table
1. The sequence labeling method assumes that
the character context information is crucial to de-
cide the keep or skip of a character. However,
we can give many counterexamples. An exam-
ple is ??????(Peking University) and ??
????(Tsinghua University), whose abbrevia-
tions correspond to ???? and ???? respec-
tively. Although sharing a similar character con-
text, the third character ??? is kept in the first case
and is skipped in the second case.
We believe that a better way is to extract these
abbreviation-full pairs from a natural text corpus
where the full form and its abbreviation co-exist.
Therefore we propose a two stage method. The
first stage generates a list of candidates given a
large corpus. To reduce the search space, we adopt
1
Details of the difference between English and Chinese
abbreviation prediction can be found in Zhang et al. (2012).
Full form ? ? ? ?
Status Skip Keep Keep Skip
Result ? ?
Table 1: The abbreviation ???? of the full form
?????? (Hong Kong University)
graph random walk to give a coarse-grained rank-
ing and select the top-ranked ones as the can-
didates. Then we use a similarity sensitive re-
ranking method to decide the final result. Detailed
description of the two parts is shown in the follow-
ing sections.
3 Candidate Generation through Graph
Random Walk
3.1 Candidate Generation and Graph
Representation
Chinese abbreviations are sub-sequences of the
full form. We use a brute force method to select
all strings in a given news article that is the sub-
sequence of the full form. The brute force method
is not time consuming compared to using more
complex data structures like trie tree, because in
a given news article there are a limited number of
sub-strings which meet the sub-sequence criteria
for abbreviations. When generating abbreviation
candidates for a given full form, we require the
full form should appear in the given news article
at least once. This is a coarse filter to indicate that
the given news article is related to the full form and
therefore the candidates generated are potentially
meaningful.
The main motivation of the candidate genera-
tion stage in our approach is that the full form and
its abbreviation tend to share similar context in a
given corpus. To be more detailed, given a word
context window w, the words that appear in the
context window of the full form tend to be sim-
ilar to those words in the context window of the
abbreviations.
We use a bipartite graph G(V
word
, V
context
, E)
to represent this phenomena. We build bipartite
graphs for each full form individually. For a given
full form v
full
, we first extract all its candidate
abbreviations V
C
. We have two kinds of nodes
in the bipartite graph: the word nodes and the
context nodes. We construct the word nodes as
V
word
= V
C
? {v
full
}, which is the node set of
the full form and all the candidates. We construct
the context nodes V
context
as the words that appear
1882
in a fixed window of V
word
. To reduce the size of
the graph, we make two extra assumptions: 1) We
only consider the nouns and verbs in the context
and 2) context words should appear in the vocab-
ulary for more than a predefined threshold (i.e. 5
times). Because G is bipartite graph, the edges E
only connect word node and context nodes. We
use the number of co-occurrence of the candidate
and the context word as the weight of each edge
and then form the weight matrix W . Details of the
bipartite graph construction algorithm are shown
in Table 2. An example bipartite graph is shown
in figure 1.
Figure 1: An example of the bipartite graph rep-
resentation. The full form is ??????(Hong
Kong University), which is the first node on the
left. The three candidates are ????, ????,
????, which are the nodes on the left. The
context words in this example are ?????(Tsui
Lap-chee, the headmaster of Hong Kong Uni-
versity), ????(Enrollment), ????(Hold), ??
??(Enact), ????(Subway), which are the nodes
on the right. The edge weight is the co-occurrence
of the left word and the right word.
3.2 Coarse-grained Ranking Using Random
Walks
We perform Markov Random Walk on the con-
structed bipartite graph to give a coarse-grained
ranked list of all candidates. In random walk, a
walker starts from the full form source node S
(in later steps, v
i
) and randomly walks to another
node v
j
with a transition probability p
ij
. In ran-
dom walk we assume the walker do the walking n
times and finally stops at a final node. When the
walking is done, we can get the probability of each
node that the walker stops in the end. Because
the destination of each step is selected based on
transition probabilities, the word node that shares
more similar contexts are more likely to be the fi-
nal stop. The random walk method we use is sim-
ilar to those defined in Norris (1998); Zhu et al.
(2003); Sproat et al. (2006); Hassan and Menezes
(2013); Li et al. (2013).
The transition probability p
ij
is calculated us-
ing the weights in the weight matrix W and then
normalized with respect to the source node v
i
with
the formula p
ij
=
w
ij?
l
w
il
. When the graph ran-
dom walk is done, we get a list of coarse-ranked
candidates, each with a confidence score derived
from the context information. By performing the
graph random walk, we reduce the search space
from exponential to the top-ranked ones. Now we
only need to select the final result from the candi-
dates, which we will describe in the next section.
4 Candidate Re-ranking
Although the coarse-grained ranked list can serve
as a basic reference, it can only use limited in-
formation like co-occurrence. We still need a re-
ranking process to decide the final result. The rea-
son is that we cannot get any candidate-specific
features when the candidate is not fully gener-
ated. Features such as the length of a candidate are
proved to be useful to rank the candidates by pre-
vious work. In this section we describe our second
stage for abbreviation generation, which we use a
similarity sensitive re-ranking method to find the
final result.
4.1 Similarity Sensitive Re-ranking
The basic idea behind our similarity sensitive re-
ranking model is that we penalize the mistakes
based on the similarity of the candidate and the
reference. If the model wrongly selects a less sim-
ilar candidate as the result, then we will attach a
large penalty to this mistake. If the model wrongly
chooses a candidate but the candidate is similar to
the reference, we slightly penalize this mistake.
The similarity between a candidate and the ref-
erence is measured through character similarity,
which we will describe later.
1883
Input: the full form v
full
, news corpus U
Output: bipartite graph G(V
word
, V
context
, E)
Candidate vector V
c
= ?, V
context
= ?
for each document d in U
if d contains v
full
add all words w in the window of v
full
into V
context
for each n-gram s in d
if s is a sub-sequence of v
full
add s into V
c
add all word w in the window of s into V
context
end if
end for
end if
end for
V
word
= V
c
? {v
full
}
for each word v
i
in V
word
for each word v
j
in V
context
calculate edge weight in E based on co-occurrence
end for
end for
Return G(V
word
, V
context
, E)
Table 2: Algorithm for constructing bipartite graphs
We first give some notation of the re-ranking
phase.
1. f(x, y) is a scoring function for a given com-
bination of x and y, where x is the original full
form and y is an abbreviation candidate. For a
given full form x
i
with K candidates, we assume
its corresponding K candidates are y
1
i
,y
2
i
,...,y
K
i
.
2. evaluation function s(x, y) is used to mea-
sure the similarity of the candidate to the refer-
ence, where x is the original full form and y is one
abbreviation candidate. We require that s(x, y)
should be in [0, 1] and s(x, y) = 1 if and only if y
is the reference.
One choice for s(x, y) may be the indicator
function. However, indicator function returns zero
for all false candidates. In the abbreviation predic-
tion task, some false candidates are much closer to
the reference than the rest. Considering this, we
use a Longest Common Subsequence(LCS) based
criterion to calculate s(x, y). Suppose the length
of a candidate is a, the length of the reference is b
and the length of their LCS is c, then we can define
precision P and recall R as:
P =
c
a
,
R =
c
b
,
F =
2 ? P ?R
P +R
(1)
It is easy to see that F is a suitable s(x, y).
Therefore we can use the F-score as the value for
s(x, y).
3. ?(x, y) is a feature function which returns a
m dimension feature vector. m is the number of
features in the re-ranking.
4. ~w is a weight vector with dimension m.
~w
T
?(x, y) is the score after re-ranking. The candi-
date with the highest score will be our final result.
Given these notations, we can now describe our
re-ranking algorithm. Suppose we have the train-
ing set X = {x
1
, x
2
, ..., x
n
}. We should find the
weight vector ~w that can minimize the loss func-
tion:
Loss(~w) =
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
? I(~w
T
?(x
i
, y
j
i
) ? ~w
T
?(x
i
, y
1
i
)))
(2)
1884
I(x) is the indicator function. It equals to 1
if and only if x ? 0. I(j) = 1 means that the
candidate which is less ?similar? to the reference
is ranked higher than the reference. Intuitively,
Loss(~w) is the weighted sum of all the wrongly
ranked candidates.
It is difficult to optimize Loss(~w) because
Loss(~w) is discontinuous. We make a relaxation
here
2
:
L(~w) =
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
?
1
1 + e
?~w
T
(?(x
i
,y
j
i
)??(x
i
,y
1
i
))
)
?
1
2
n
?
i=1
k
?
j=1
((s(x
i
, y
1
i
)? s(x
i
, y
j
i
))
? I(~w
T
?(x
i
, y
j
i
) ? ~w
T
?(x
i
, y
1
i
)))
=
1
2
Loss(~w)
(3)
From the equations above we can see that
2L(~w) is the upper bound of our loss function
Loss(~w). Therefore we can optimize L(~w) to ap-
proximate Loss(~w).
We can use optimization methods like gradient
descent to get the ~w that minimize the loss func-
tion. Because L is not convex, it may go into a lo-
cal minimum. In our experiment we held out 10%
data as the develop set and try random initializa-
tion to decide the initial ~w.
4.2 Features for Re-ranking
One advantage of the re-ranking phase is that it
can now use features related to candidates. There-
fore, we can use a variety of features. We list them
as follows.
1. The coarse-grained ranking score from the
graph random walk phase. From the de-
scription of the previous section we know that
this score is the probability a ?walker? ?walk?
from the full form node to the current candi-
date. This is a coarse-grained score because
it can only use the information of words in-
side the window. However, it is still informa-
tive because in the re-ranking phase we can-
not collect this information directly.
2
To prove this we need the following two inequalities: 1)
when x ? 0, I(x) ?
2
1+e
?x
and 2) s(x
i
, y
1
i
) ? s(x
i
, y
j
i
) ?
0.
2. The character uni-grams and bi-grams in
the candidate. This kind of feature cannot
be used in the traditional character tagging
methods.
3. The language model score of the candi-
date. In our experiment, we train a bi-gram
language model using Laplace smoothing on
the Chinese Gigaword Data
3
.
4. The length of the candidate. Intuitively,
abbreviations tend to be short. Therefore
length can be an important feature for the re-
ranking.
5. The degree of ambiguity of the candidate.
We first define the degree of ambiguity d
i
of a
character c
i
as the number of identical words
that contain the character. We then define the
degree of ambiguity of the candidate as the
sum of all d
i
in the candidates. We need a dic-
tionary to extract this feature. We collect all
words in the PKU data of the second Interna-
tional Chinese Word Segmentation Bakeoff
4
.
6. Whether the candidate is in a word dictio-
nary. We use the PKU dictionary in feature
5.
7. Whether all bi-grams are in a word dictio-
nary. We use the PKU dictionary in feature
5.
8. Adjacent Variety(AV) of the candidate. We
define the left AV of the candidate as the
probability that in a corpus the character in
front of the candidate is a character in the
full form. For example if we consider the full
form ??????(Peking University) and the
candidate ????, then the left AV of ????
is the probability that the character preced-
ing ???? is ??? or ??? or ??? or ??? in
a corpus. We can similarly define the right
AV, with respect to characters follow the can-
didate.
The AV feature is very useful because in some
cases a substring of the full form may have a con-
fusingly high frequency. In the example of ???
???(Peking University), an article in the corpus
may mention ??????(Peking University) and
3
http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T09
4
http://www.sighan.org/bakeoff2005/
1885
??????(Tokyo University) at the same time.
Then the substring ????? may be included in
the candidate generation phase for ??????
with a high frequency. Because the left AV of ??
??? is high, the re-ranker can easily detect this
false candidate.
In practice, all the features need to be scaled in
order to speed up training. There are many ways
to scale features. We use the most intuitive scal-
ing method. For a feature value x, we scale it as
(x?mean)/(max?min). Note that for language
model score and the score of random walk phase,
we scale based on their log value.
5 Experiments
5.1 Dataset and Evaluation metrics
For the dataset, we collect 3210 abbreviation pairs
from the Chinese Gigaword corpus. The abbre-
viation pairs include noun phrases, organization
names and some other types. The Chinese Gi-
gaword corpus contains news texts from the year
1992 to 2007. We only collect those pairs whose
full form and corresponding abbreviation appear
in the same article for at least one time. For full
forms with more than one reasonable reference,
we keep the most frequently used one as its refer-
ence. We use 80% abbreviation pairs as the train-
ing data and the rest as the testing data.
We use the top-K accuracy as the evaluation
metrics. The top-K accuracy is widely used as the
measurement in previous work (Tsuruoka et al.,
2005; Sun et al., 2008, 2009; Zhang et al., 2012). It
measures what percentage of the reference abbre-
viations are found if we take the top k candidate
abbreviations from all the results. In our experi-
ment, we compare the top-5 accuracy with base-
lines. We choose the top-10 candidates from the
graph random walk are considered in re-ranking
phase and the measurement used is top-1 accuracy
because the final aim of the algorithm is to detect
the exact abbreviation, rather than a list of candi-
dates.
5.2 Candidate List
Table 3 shows examples of the candidates. In our
algorithm we further reduce the search space to
only incorporate 10 candidates from the candidate
generation phase.
K Top-K Accuracy
1 6.84%
2 19.35%
3 49.01%
4 63.70%
5 73.60%
Table 4: Top-5 accuracy of the candidate genera-
tion phase
5.3 Comparison with baselines
We first show the top-5 accuracy of the candidate
generation phase Table 4. We can see that, just
like the case of using other feature alone, using
the score of random walk alone is far from enough.
However, the first 5 candidates contain most of the
correct answers. We use the top-5 candidates plus
another 5 candidates in the re-ranking phase.
We choose the character tagging method as the
baseline method. The character tagging strategy
is widely used in the abbreviation generation task
(Tsuruoka et al., 2005; Sun et al., 2008, 2009;
Zhang et al., 2012). We choose the ?SK? labeling
strategy which is used in Sun et al. (2009); Zhang
et al. (2012). The ?SK? labeling strategy gives each
character a label in the character sequence, with
?S? represents ?Skip? and ?K? represents ?Keep?.
Same with Zhang et al. (2012), we use the Con-
ditional Random Fields (CRFs) model in the se-
quence labeling process.
The baseline method mainly uses the charac-
ter context information to generate the candidate
abbreviation. To be fair we use the same fea-
ture set in Sun et al. (2009); Zhang et al. (2012).
One drawback of the sequence labeling method is
that it relies heavily on the character context in
the full form. With the number of new abbrevi-
ations grows rapidly (especially in social media
like Facebook or twitter), it is impossible to let the
model ?remember? all the character contexts. Our
method is different from theirs, we use a more in-
tuitive way which finds the list of candidates di-
rectly from a natural corpus.
Table 5 shows the comparison of the top-5 accu-
racy. We can see that our method outperforms the
baseline methods. The baseline model performs
well when using character features (Column 3).
However, it performs poorly without the charac-
ter features (Column 2). In contrast, without the
character features, our method (Column 4) works
much better than the sequence labeling method.
1886
Full form Reference Generated Candidates #Enum #Now
????? (Depart-
ment of International
Politics)
??? ???,???,????,??
?,??,??,??
30 7
?????? (Non-
nuclear Countries)
??? ??,??,??,???,??
?,???,????,???
?,????,?????,??
?,??,??
62 13
???? (Drug traf-
ficking)
?? ???,???,??,??,?? 14 5
?????????
????? (Yangtze
Joint River Economic
Development Inc.)
???? ??,??,????,???
?,????,????,???
?,????,??????,?
?????,??????,??
????,????????,?
?????,????,??,?
?,??,??,??
16382 20
Table 3: Generated Candidates. #Enum is the number of candidates generated by enumerating all possi-
ble candidates. #Now is the number of candidates generated by our method.
When we add character features, our method (Col-
umn 5) still outperforms the sequence labeling
method.
K CRF-char Our-char CRF Our
1 38.00% 48.60% 53.27% 55.61%
2 38.16% 70.87% 65.89% 73.10%
3 39.41% 81.78% 72.43% 81.96%
4 55.30% 87.54% 78.97% 87.57%
5 62.31% 89.25% 81.78% 89.27%
Table 5: Comparison of the baseline method and
our method. CRF-char (?-? means minus) is the
baseline method without character features. CRF
is the baseline method. Our-char is our method
without character features. We define character
features as the features that consider the charac-
ters from the original full form as their parts.
5.4 Performance with less training data
One advantage of our method is that it only
requires weak supervision. The baseline
method needs plenty of manually collected
full-abbreviation pairs to learn a good model.
In our method, the candidate generation and
coarse-grained ranking is totally unsupervised.
The re-ranking phase needs training instances
to decide the parameters. However we can use
a very small amount of training data to get a
reasonably good model. Figure 2 shows the result
of using different size of training data. We can
see that the performance of the baseline methods
drops rapidly when there are less training data.
In contrast, when using less training data, our
method does not suffer that much.
Figure 2: Top-1 accuracy when changing the size
of training data. For example, ?50%? means ?us-
ing 50% of all the training data?.
5.5 Comparison with previous work
We compare our method with the method in the
previous work DPLVM+GI in Sun et al. (2009),
which outperforms Tsuruoka et al. (2005); Sun
et al. (2008). We also compare our method with
the web-based method CRF+WEB in Zhang et al.
(2012). Because the comparison is performed on
different corpora, we run the two methods on our
data. Table 6 shows the top-1 accuracy. We
can see that our method outperforms the previous
1887
methods.
System Top-K Accuracy
DPLVM+GI 53.29%
CRF+WEB 54.02%
Our method 55.61%
Table 6: Comparison with previous work. The
search results of CRF+WEB is based on March 9,
2014 version of the Baidu search engine.
5.6 Error Analysis
We perform cross-validation to find the errors and
list the two major errors below:
1. Some full forms may correspond to more
than one acceptable abbreviation. In this
case, our method may choose the one that is
indeed used as the full form?s abbreviation in
news texts, but not the same as the standard
reference abbreviations. The reason for this
phenomenon may lie in the fact that the veri-
fication data we use is news text, which tends
to be formal. Therefore when a reference is
often used colloquially, our method may miss
it. We can relieve this by changing the corpus
we use.
2. Our method may provide biased information
when handling location sensitive phrases.
Not only our system, the system of Sun et al.
(2009); Zhang et al. (2012) also shows this
phenomenon. An example is the case of ??
?????? (Democracy League of Hong
Kong). Because most of the news is about
news in mainland China, it is hard for the
model to tell the difference between the ref-
erence ????? and a false candidate ??
??(Democracy League of China).
Another ambiguity is ??????(Tsinghua
University), which has two abbreviations ??
?? and ????. This happens because the
full form itself is ambiguous. Word sense dis-
ambiguation can be performed first to handle
this kind of problem.
6 Related Work
Abbreviation generation has been studied during
recent years. At first, some approaches maintain
a database of abbreviations and their correspond-
ing ?full form? pairs. The major problem of pure
database-building approach is obvious. It is im-
possible to cover all abbreviations, and the build-
ing process is quite laborious. To find these pairs
automatically, a powerful approach is to find the
reference for a full form given the context, which
is referred to as ?abbreviation generation?.
There is research on using heuristic rules
for generating abbreviations Barrett and Grems
(1960); Bourne and Ford (1961); Taghva and
Gilbreth (1999); Park and Byrd (2001); Wren et al.
(2002); Hearst (2003). Most of them achieved
high performance. However, hand-crafted rules
are time consuming to create, and it is not easy to
transfer the knowledge of rules from one language
to another.
Recent studies of abbreviation generation have
focused on the use of machine learning tech-
niques. Sun et al. (2008) proposed an SVM ap-
proach. Tsuruoka et al. (2005); Sun et al. (2009)
formalized the process of abbreviation generation
as a sequence labeling problem. The drawback of
the sequence labeling strategies is that they rely
heavily on the character features. This kind of
method cannot fit the need for abbreviation gen-
eration in social media texts where the amount of
abbreviations grows fast.
Besides these pure statistical approaches, there
are also many approaches using Web as a corpus
in machine learning approaches for generating ab-
breviations. Adar (2004) proposed methods to de-
tect such pairs from biomedical documents. Jain
et al. (2007) used web search results as well as
search logs to find and rank abbreviates full pairs,
which show good result. The disadvantage is that
search log data is only available in a search en-
gine backend. The ordinary approaches do not
have access to search engine internals. Zhang et al.
(2012) used web search engine information to re-
rank the candidate abbreviations generated by sta-
tistical approaches. Compared to their approaches,
our method only uses a fixed corpus, instead of us-
ing collective information, which varies from time
to time.
Some of the previous work that relate to ab-
breviations focuses on the task of ?abbreviation
disambiguation?, which aims to find the correct
abbreviation-full pairs. In these works, machine
learning approaches are commonly used (Park and
Byrd, 2001; HaCohen-Kerner et al., 2008; Yu
et al., 2006; Ao and Takagi, 2005). We focus on
another aspect. We want to find the abbreviation
1888
given the full form. Besides, Sun et al. (2013) also
works on abbreviation prediction but focuses on
the negative full form problem, which is a little
different from our work.
One related research field is text normalization,
with many outstanding works (Sproat et al., 2001;
Aw et al., 2006; Hassan and Menezes, 2013; Ling
et al., 2013; Yang and Eisenstein, 2013). While
the two tasks share similarities, abbreviation pre-
diction has its identical characteristics, like the
sub-sequence assumption. This results in different
methods to tackle the two different problems.
7 Conclusion
In this paper, we propose a unified framework for
Chinese abbreviation generation. Our approach
contains two stages: candidate generation and
re-ranking. Given a long term, we first gener-
ate a list of abbreviation candidates using the co-
occurrence information. We give a coarse-grained
rank using graph random walk to reduce the search
space. After we get the candidate lists, we can use
the features related to the candidates. We use a
similarity sensitive re-rank method to get the final
abbreviation. Experiments show that our method
outperforms the previous systems.
Acknowledgments
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018,61300063),Major
National Social Science Fund of
China(No.12&ZD227), National High Tech-
nology Research and Development Program of
China (863 Program) (No. 2012AA011101), and
Doctoral Fund of Ministry of Education of China
(No. 20130001120004). The contact author of
this paper, according to the meaning given to
this role by Key Laboratory of Computational
Linguistics, Ministry of Education, School of
Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang. We thank
Ke Wu for part of our work is inspired by his
previous work at KLCL.
References
Adar, E. (2004). Sarad: A simple and ro-
bust abbreviation dictionary. Bioinformatics,
20(4):527?533.
Ao, H. and Takagi, T. (2005). Alice: an algorithm
to extract abbreviations from medline. Journal
of the American Medical Informatics Associa-
tion, 12(5):576?586.
Aw, A., Zhang, M., Xiao, J., and Su, J. (2006). A
phrase-based statistical model for sms text nor-
malization. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 33?
40. Association for Computational Linguistics.
Barrett, J. and Grems, M. (1960). Abbreviating
words systematically. Communications of the
ACM, 3(5):323?324.
Bourne, C. and Ford, D. (1961). A study of
methods for systematically abbreviating english
words and names. Journal of the ACM (JACM),
8(4):538?552.
HaCohen-Kerner, Y., Kass, A., and Peretz, A.
(2008). Combined one sense disambiguation
of abbreviations. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Tech-
nologies: Short Papers, pages 61?64. Associa-
tion for Computational Linguistics.
Hassan, H. and Menezes, A. (2013). Social text
normalization using contextual graph random
walks. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1577?
1586, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Hearst, M. S. (2003). A simple algorithm for
identifying abbreviation definitions in biomed-
ical text.
Jain, A., Cucerzan, S., and Azzam, S. (2007).
Acronym-expansion recognition and ranking on
the web. In Information Reuse and Integration,
2007. IRI 2007. IEEE International Conference
on, pages 209?214. IEEE.
Li, J., Ott, M., and Cardie, C. (2013). Identify-
ing manipulated offerings on review portals. In
EMNLP, pages 1933?1942.
Ling, W., Dyer, C., Black, A. W., and Trancoso, I.
(2013). Paraphrasing 4 microblog normaliza-
tion. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language
Processing, pages 73?84, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Nenadi?c, G., Spasi?c, I., and Ananiadou, S. (2002).
Automatic acronym acquisition and term varia-
tion management within domain-specific texts.
1889
In Third International Conference on Language
Resources and Evaluation (LREC2002), pages
2155?2162.
Norris, J. R. (1998). Markov chains. Number
2008. Cambridge university press.
Park, Y. and Byrd, R. (2001). Hybrid text mining
for finding abbreviations and their definitions.
In Proceedings of the 2001 conference on em-
pirical methods in natural language processing,
pages 126?133.
Sproat, R., Black, A. W., Chen, S., Kumar, S.,
Ostendorf, M., and Richards, C. (2001). Nor-
malization of non-standard words. Computer
Speech & Language, 15(3):287?333.
Sproat, R., Tao, T., and Zhai, C. (2006). Named
entity transliteration with comparable corpora.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 73?80. Association for
Computational Linguistics.
Sun, X., Li, W., Meng, F., and Wang, H. (2013).
Generalized abbreviation prediction with nega-
tive full forms and its application on improv-
ing chinese web search. In Proceedings of the
Sixth International Joint Conference on Natural
Language Processing, pages 641?647, Nagoya,
Japan. Asian Federation of Natural Language
Processing.
Sun, X., Okazaki, N., and Tsujii, J. (2009). Ro-
bust approach to abbreviating terms: A discrim-
inative latent variable model with global infor-
mation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 905?913. Association
for Computational Linguistics.
Sun, X., Wang, H., and Wang, B. (2008). Pre-
dicting chinese abbreviations from definitions:
An empirical learning approach using support
vector regression. Journal of Computer Science
and Technology, 23(4):602?611.
Taghva, K. and Gilbreth, J. (1999). Recognizing
acronyms and their definitions. International
Journal on Document Analysis and Recogni-
tion, 1(4):191?198.
Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005).
A machine learning approach to acronym gen-
eration. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontolo-
gies and Databases: Mining Biological Seman-
tics, pages 25?31. Association for Computa-
tional Linguistics.
Wren, J., Garner, H., et al. (2002). Heuristics
for identification of acronym-definition patterns
within text: towards an automated construc-
tion of comprehensive acronym-definition dic-
tionaries. Methods of information in medicine,
41(5):426?434.
Yang, Y. and Eisenstein, J. (2013). A log-linear
model for unsupervised text normalization. In
Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing,
pages 61?72, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.
Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,
J. (2006). A large scale, corpus-based approach
for automatically disambiguating biomedical
abbreviations. ACM Transactions on Informa-
tion Systems (TOIS), 24(3):380?404.
Zhang, L., Li, S., Wang, H., Sun, N., and Meng,
X. (2012). Constructing Chinese abbreviation
dictionary: A stacked approach. In Proceedings
of COLING 2012, pages 3055?3070, Mumbai,
India. The COLING 2012 Organizing Commit-
tee.
Zhu, X., Ghahramani, Z., Lafferty, J., et al. (2003).
Semi-supervised learning using gaussian fields
and harmonic functions. In ICML, volume 3,
pages 912?919.
1890
Feature-Frequency?Adaptive On-line
Training for Fast and Accurate Natural
Language Processing
Xu Sun?
Peking University
Wenjie Li??
Hong Kong Polytechnic University
Houfeng Wang?
Peking University
Qin Lu?
Hong Kong Polytechnic University
Training speed and accuracy are two major concerns of large-scale natural language processing
systems. Typically, we need to make a tradeoff between speed and accuracy. It is trivial to improve
the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed.
Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time,
which is the target of this work. To reach this target, we present a new training method, feature-
frequency?adaptive on-line training, for fast and accurate training of natural language process-
ing systems. It is based on the core idea that higher frequency features should have a learning rate
that decays faster. Theoretical analysis shows that the proposed method is convergent with a fast
convergence rate. Experiments are conducted based on well-known benchmark tasks, including
named entity recognition, word segmentation, phrase chunking, and sentiment analysis. These
tasks consist of three structured classification tasks and one non-structured classification task,
with binary features and real-valued features, respectively. Experimental results demonstrate
that the proposed method is faster and at the same time more accurate than existing methods,
achieving state-of-the-art scores on the tasks with different characteristics.
? Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,
and School of EECS, Peking University, Beijing, China. E-mail: xusun@pku.edu.cn.
?? Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, Hong
Kong. E-mail: cswjli@comp.polyu.edu.hk.
? Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,
and School of EECS, Peking University, Beijing, China. E-mail: wanghf@pku.edu.cn.
? Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, Hong
Kong. E-mail: csluqin@comp.polyu.edu.hk.
Submission received: 27 December 2012; revised version received: 30 May 2013; accepted for publication:
16 September 2013.
doi:10.1162/COLI a 00193
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Training speed is an important concern of natural language processing (NLP) systems.
Large-scale NLP systems are computationally expensive. In many real-world applica-
tions, we further need to optimize high-dimensional model parameters. For example,
the state-of-the-art word segmentation system uses more than 40 million features (Sun,
Wang, and Li 2012). The heavyNLPmodels together with high-dimensional parameters
lead to a challenging problem onmodel training, whichmay require week-level training
time even with fast computing machines.
Accuracy is another very important concern of NLP systems. Nevertheless, usually
it is quite difficult to build a system that has fast training speed and at the same time
has high accuracy. Typically we need to make a tradeoff between speed and accuracy,
to trade training speed for higher accuracy or vice versa. In this work, we have tried
to overcome this problem: to improve the training speed and the model accuracy at the
same time.
There are twomajor approaches for parameter training: batch and on-line. Standard
gradient descent methods are normally batch training methods, in which the gradient
computed by using all training instances is used to update the parameters of the model.
The batch training methods include, for example, steepest gradient descent, conjugate
gradient descent (CG), and quasi-Newtonmethods like limited-memory BFGS (Nocedal
and Wright 1999). The true gradient is usually the sum of the gradients from each
individual training instance. Therefore, batch gradient descent requires the training
method to go through the entire training set before updating parameters. This is why
batch training methods are typically slow.
On-line learning methods can significantly accelerate the training speed compared
with batch training methods. A representative on-line training method is the stochastic
gradient descent method (SGD) and its extensions (e.g., stochastic meta descent) (Bottou
1998; Vishwanathan et al. 2006). The model parameters are updated more frequently
compared with batch training, and fewer passes are needed before convergence. For
large-scale data sets, on-line training methods can be much faster than batch training
methods.
However, we find that the existing on-line training methods are still not good
enough for training large-scale NLP systems?probably because those methods are
not well-tailored for NLP systems that have massive features. First, the convergence
speed of the existing on-line training methods is not fast enough. Our studies show that
the existing on-line training methods typically require more than 50 training passes
before empirical convergence, which is still slow. For large-scale NLP systems, the
training time per pass is typically long and fast convergence speed is crucial. Second,
the accuracy of the existing on-line training methods is not good enough. We want to
further improve the training accuracy. We try to deal with the two challenges at the
same time. Our goal is to develop a new training method for faster and at the same time
more accurate natural language processing.
In this article, we present a new on-line training method, adaptive on-line gradient
descent based on feature frequency information (ADF),1 for very accurate and fast
on-line training of NLP systems. Other than the high training accuracy and fast train-
ing speed, we further expect that the proposed training method has good theoretical
1 ADF source code and tools can be obtained from http://klcl.pku.edu.cn/member/sunxu/index.htm.
564
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
properties. We want to prove that the proposed method is convergent and has a fast
convergence rate.
In the proposed ADF training method, we use a learning rate vector in the on-line
updating. This learning rate vector is automatically adapted based on feature frequency
information in the training data set. Each model parameter has its own learning rate
adapted on feature frequency information. This proposal is based on the simple intu-
ition that a feature with higher frequency in the training process should have a learning
rate that decays faster. This is because a higher frequency feature is expected to be
well optimized with higher confidence. Thus, a higher frequency feature is expected to
have a lower learning rate. We systematically formalize this intuition into a theoretically
sound training algorithm, ADF.
The main contributions of this work are as follows:
r On the methodology side, we propose a general purpose on-line training
method, ADF. The ADF method is significantly more accurate than
existing on-line and batch training methods, and has faster training speed.
Moreover, theoretical analysis demonstrates that the ADF method is
convergent with a fast convergence rate.
r On the application side, for the three well-known tasks, including named
entity recognition, word segmentation, and phrase chunking, the proposed
simple method achieves equal or even better accuracy than the existing
gold-standard systems, which are complicated and use extra resources.
2. Related Work
Our main focus is on structured classification models with high dimensional features.
For structured classification, the conditional random fields model is widely used. To
illustrate that the proposed method is a general-purpose training method not limited to
a specific classification task or model, we also evaluate the proposal for non-structured
classification tasks like binary classification. For non-structured classification, the max-
imum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996)
is widely used. Here, we review the conditional random fields model and the related
work of on-line training methods.
2.1 Conditional Random Fields
The conditional random field (CRF) model is a representative structured classification
model and it is well known for its high accuracy in real-world applications. The CRF
model is proposed for structured classification by solving ?the label bias problem?
(Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of
observation sequence x and label sequence y to a global feature vector f, the probability
of a label sequence y conditioned on the observation sequence x is modeled as follows
(Lafferty, McCallum, and Pereira 2001):
P(y|x,w) =
exp {w>f (y,x)}
?
?y? exp {w>f (y? ,x)}
(1)
wherew is a parameter vector.
565
Computational Linguistics Volume 40, Number 3
Given a training set consisting of n labeled sequences, zi = (xi,yi), for i = 1 . . .n,
parameter estimation is performed by maximizing the objective function,
L(w) =
n
?
i=1
logP(yi|xi,w)? R(w) (2)
The first term of this equation represents a conditional log-likelihood of training
data. The second term is a regularizer for reducing overfitting. We use an L2 prior,
R(w) = ||w||
2
2?2 . In what follows, we denote the conditional log-likelihood of each sample
as logP(yi|xi,w) as `(zi,w). The final objective function is as follows:
L(w) =
n
?
i=1
`(zi,w)?
||w||2
2?2
(3)
2.2 On-line Training
The most representative on-line training method is the SGD method (Bottou 1998;
Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). The SGD method uses a
randomly selected small subset of the training sample to approximate the gradient of
an objective function. The number of training samples used for this approximation is
called the batch size. By using a smaller batch size, one can update the parameters
more frequently and speed up the convergence. The extreme case is a batch size of 1,
and it gives the maximum frequency of updates, which we adopt in this work. In this
case, the model parameters are updated as follows:
wt+1 = wt + ?t?wtLstoch(zi,wt) (4)
where t is the update counter, ?t is the learning rate or so-called decaying rate, and
Lstoch(zi,wt) is the stochastic loss function based on a training sample zi. (More details
of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and
Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying
rate works the best for natural language processing tasks, and it is adopted in our
implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013).
Other well-known on-line training methods include perceptron training (Freund
and Schapire 1999), averaged perceptron training (Collins 2002), more recent devel-
opment/extensions of stochastic gradient descent (e.g., the second-order stochastic
gradient descent training methods like stochastic meta descent) (Vishwanathan et al.
2006; Hsu et al. 2009), and so on. However, the second-order stochastic gradient descent
method requires the computation or approximation of the inverse of the Hessian matrix
of the objective function, which is typically slow, especially for heavily structured classi-
fication models. Usually the convergence speed based on number of training iterations
is moderately faster, but the time cost per iteration is slower. Thus the overall time cost
is still large.
Compared with the related work on batch and on-line training (Jacobs 1988;
Sperduti and Starita 1993; Dredze, Crammer, and Pereira 2008; Duchi, Hazan, and
Singer 2010; McMahan and Streeter 2010), our work is fundamentally different. The
proposedADF trainingmethod is based on feature frequency adaptation, and to the best
of our knowledge there is no prior work on direct feature-frequency?adaptive on-line
566
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
training. Compared with the confidence-weighted (CW) classification method and its
variation AROW (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza, and Dredze
2009), the proposed method is substantially different. While the feature frequency
information is implicitly modeled via a complicated Gaussian distribution framework
in Dredze, Crammer, and Pereira (2008) and Crammer, Kulesza, and Dredze (2009),
the frequency information is explicitly modeled in our proposal via simple learning
rate adaptation. Our proposal is more straightforward in capturing feature frequency
information, and it has no need to use Gaussian distributions and KL divergence,
which are important in the CW and AROW methods. In addition, our proposal is a
probabilistic learning method for training probabilistic models such as CRFs, whereas
the CW and AROW methods (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza,
and Dredze 2009) are non-probabilistic learning methods extended from perceptron-
style approaches. Thus, the framework is different. This work is a substantial extension
of the conference version (Sun, Wang, and Li 2012). Sun, Wang, and Li (2012) focus on
the specific task of word segmentation, whereas this article focuses on the proposed
training algorithm.
3. Feature-Frequency?Adaptive On-line Learning
In traditional on-line optimization methods such as SGD, no distinction is made for
different parameters in terms of the learning rate, and this may result in slow conver-
gence of the model training. For example, in the on-line training process, suppose the
high frequency feature f1 and the low frequency feature f2 are observed in a training
sample and their corresponding parameters w1 and w2 are to be updated via the same
learning rate ?t. Suppose the high frequency feature f1 has been updated 100 times
and the low frequency feature f2 has only been updated once. Then, it is possible that
the weight w1 is already well optimized and the learning rate ?t is too aggressive for
updating w1. Updating the weight w1 with the learning rate ?t may make w1 be far
from the well-optimized value, and it will require corrections in the future updates. This
causes fluctuations in the on-line training and results in slow convergence speed. On
the other hand, it is possible that the weight w2 is poorly optimized and the same learn-
ing rate ?t is too conservative for updating w2. This also results in slow convergence
speed.
To solve this problem, we propose ADF. In spite of the high accuracy and fast
convergence speed, the proposed method is easy to implement. The proposed method
with feature-frequency?adaptive learning rates can be seen as a learning method with
specific diagonal approximation of the Hessian information based on assumptions of
feature frequency information. In this approximation, the diagonal elements of the
diagonal matrix correspond to the feature-frequency?adaptive learning rates. Accord-
ing to the aforementioned example and analysis, it assumes that a feature with higher
frequency in the training process should have a learning rate that decays faster.
3.1 Algorithm
In the proposed ADF method, we try to use more refined learning rates than traditional
SGD training. Instead of using a single learning rate (a scalar) for all weights, we extend
the learning rate scalar to a learning rate vector, which has the same dimension as the
weight vector w. The learning rate vector is automatically adapted based on feature
567
Computational Linguistics Volume 40, Number 3
frequency information. By doing so, each weight has its own learning rate, and we will
show that this can significantly improve the convergence speed of on-line learning.
In the ADF learning method, the update formula is:
wt+1 = wt +?t ? gt (5)
The update term gt is the gradient term of a randomly sampled instance:
gt = ?wtLstoch(zi,wt) = ?wt
{
`(zi,wt)?
||wt||2
2n?2
}
In addition, ?t ? R
f
+ is a positive vector-valued learning rate and ? denotes the
component-wise (Hadamard) product of two vectors.
The learning rate vector ?t is automatically adapted based on feature frequency
information in the updating process. Intuitively, a feature with higher frequency in the
training process has a learning rate that decays faster. This is because a weight with
higher frequency is expected to be more adequately trained, hence a lower learning
rate is preferable for fast convergence. We assume that a high frequency feature should
have a lower learning rate, and a low frequency feature should have a relatively higher
learning rate in the training process.We systematically formalize this idea into a theoret-
ically sound training algorithm. The proposedmethodwith feature-frequency?adaptive
learning rates can be seen as a learning method with specific diagonal approximation
of the inverse of the Hessian matrix based on feature frequency information.
Given awindow size q (number of samples in awindow), we use a vector v to record
the feature frequency. The kth entry vk corresponds to the frequency of the feature k in
this window. Given a feature k, we use u to record the normalized frequency:
u = vk/q
For each feature, an adaptation factor ? is calculated based on the normalized frequency
information, as follows:
? = ?? u(?? ?)
where ? and ? are the upper and lower bounds of a scalar, with 0 < ? < ? < 1. Intu-
itively, the upper bound ? corresponds to the adaptation factor of the lowest frequency
features, and the lower bound ? corresponds to the adaptation factor of the highest
frequency features. The optimal values of ? and ? can be tuned based on specific real-
world tasks, for example, via cross-validation on the training data or using held-out
data. In practice, via cross-validation on the training data of different tasks, we found
that the following setting is sufficient to produce adequate performance for most of the
real-world natural language processing tasks: ? around 0.995, and ? around 0.6. This
indicates that the feature frequency information has similar characteristics across many
different natural language processing tasks.
As we can see, a feature with higher frequency corresponds to a smaller scalar via
linear approximation. Finally, the learning rate is updated as follows:
?k ? ??k
568
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
ADF learning algorithm
1: procedure ADF(Z,w, q, c, ?, ?)
2: w ? 0, t? 0, v ? 0, ? ? c
3: repeat until convergence
4: . Draw a sample zi at random from the data set Z
5: . v ? UPDATEFEATUREFREQ(v, zi)
6: . if t > 0 and t mod q = 0
7: . . ? ? UPDATELEARNRATE(?, v)
8: . . v ? 0
9: . g ??wLstoch(zi,w)
10: . w ? w +? ? g
11: . t? t+ 1
12: returnw
13:
14: procedure UPDATEFEATUREFREQ(v, zi)
15: for k ? features used in sample zi
16: . vk ? vk + 1
17: return v
18:
19: procedure UPDATELEARNRATE(?, v)
20: for k ? all features
21: . u? vk/q
22: . ?? ?? u(?? ?)
23: . ?k ? ??k
24: return ?
Figure 1
The proposed ADF on-line learning algorithm. In the algorithm, Z is the training data set; q, c, ?,
and ? are hyper-parameters; q is an integer representing window size; c is for initializing the
learning rates; and ? and ? are the upper and lower bounds of a scalar, with 0 < ? < ? < 1.
With this setting, different features correspond to different adaptation factors based
on feature frequency information. Our ADF algorithm is summarized in Figure 1.
The ADF training method is efficient because the only additional computation
(compared with traditional SGD) is the derivation of the learning rates, which is simple
and efficient. As we know, the regularization of SGD can perform efficiently via the opti-
mization based on sparse features (Shalev-Shwartz, Singer, and Srebro 2007). Similarly,
the derivation of ?t can also perform efficiently via the optimization based on sparse
features. Note that although binary features are common in natural language processing
tasks, the ADF algorithm is not limited to binary features and it can be applied to real-
valued features.
3.2 Convergence Analysis
We want to show that the proposed ADF learning algorithm has good convergence
properties. There are two steps in the convergence analysis. First, we show that the
ADF update rule is a contraction mapping. Then, we show that the ADF training is
asymptotically convergent, and with a fast convergence rate.
To simplify the discussion, our convergence analysis is based on the convex loss
function of traditional classification or regression problems:
L(w) =
n
?
i=1
`(xi, yi,w ? f i)?
||w||2
2?2
569
Computational Linguistics Volume 40, Number 3
where f i is the feature vector generated from the training sample (xi, yi). L(w) is a func-
tion in w ? f i, such as 12 (yi ?w ? f i)2 for regression or log[1+ exp(?yiw ? f i)] for binary
classification.
To make convergence analysis of the proposed ADF training algorithm, we need to
introduce several mathematical definitions. First, we introduce Lipschitz continuity:
Definition 1 (Lipschitz continuity)
A function F : X ? R is Lipschitz continuous with the degree of D if |F(x)? F(y)| ?
D|x? y| for ?x, y ? X . X can be multi-dimensional space, and |x? y| is the distance
between the points x and y.
Based on the definition of Lipschitz continuity, we give the definition of the
Lipschitz constant ||F||Lip as follows:
Definition 2 (Lipschitz constant)
||F||Lip := inf{D where |F(x)? F(y)| ? D|x? y| for ?x, y}
In other words, the Lipschitz constant ||F||Lip is the lower bound of the continuity degree
that makes the function F Lipschitz continuous.
Further, based on the definition of Lipschitz constant, we give the definition of
contraction mapping as follows:
Definition 3 (Contraction mapping)
A function F : X ? X is a contraction mapping if its Lipschitz constant is smaller than
1: ||F||Lip < 1.
Then, we can show that the traditional SGD update is a contraction mapping.
Lemma 1 (SGD update rule is contraction mapping)
Let ? be a fixed low learning rate in SGD updating. If ? ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1,
the SGD update rule is a contraction mapping in Euclidean space with Lipschitz con-
tinuity degree 1? ?/?2.
The proof can be extended from the relatedwork on convergence analysis of parallel
SGD training (Zinkevich et al. 2010). The stochastic training process is a one-following-
one dynamic update process. In this dynamic process, if we use the same update rule F,
we havewt+1 = F(wt) andwt+2 = F(wt+1). It is only necessary to prove that the dynamic
update is a contraction mapping restricted by this one-following-one dynamic process.
That is, for the proposed ADF update rule, it is only necessary to prove it is a dynamic
contraction mapping. We formally define dynamic contraction mapping as follows.
Definition 4 (Dynamic contraction mapping)
Given a function F : X ? X , suppose the function is used in a dynamic one-following-
one process: xt+1 = F(xt) and xt+2 = F(xt+1) for ?xt ? X . Then, the function F is a
dynamic contraction mapping if ?D < 1, |xt+2 ? xt+1| ? D|xt+1 ? xt| for ?xt ? X .
We can see that a contraction mapping is also a dynamic contraction mapping, but
a dynamic contraction mapping is not necessarily a contraction mapping. We first show
570
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
that the ADF update rule with a fixed learning rate vector of different learning rates is
a dynamic contraction mapping.
Theorem 1 (ADF update rule with fixed learning rates)
Let ? be a fixed learning rate vector with different learning rates. Let ?max be the max-
imum learning rate in the learning rate vector ?: ?max := sup{?i where ?i ? ?}. Then
if ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1, the ADF update rule is a dynamic contraction
mapping in Euclidean space with Lipschitz continuity degree 1? ?max/?2.
The proof is sketched in Section 5.
Further, we need to prove that the ADF update rule with a decaying learning
rate vector is a dynamic contraction mapping, because the real ADF algorithm has a
decaying learning rate vector. In the decaying case, the condition that ?max ? (||x2i || ?
||?y? `(xi, yi, y?)||Lip)?1 can be easily achieved, because ? continues to decay with an
exponential decaying rate. Even if the ? is initialized with high values of learning rates,
after a number of training passes (denoted as T) ?T is guaranteed to be small enough so
that ?max := sup{?i where ?i ? ?T} and ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1. Without
losing generality, our convergence analysis starts from the pass T and we take ?T as ?0
in the following analysis. Thus, we can show that the ADF update rule with a decaying
learning rate vector is a dynamic contraction mapping:
Theorem 2 (ADF update rule with decaying learning rates)
Let ?t be a learning rate vector in the ADF learning algorithm, which is decaying
over the time t and with different decaying rates based on feature frequency infor-
mation. Let ?t start from a low enough learning rate vector ?0 such that ?max ?
(||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1, where ?max is the maximum element in?0. Then, the ADF
update rule with decaying learning rate vector is a dynamic contraction mapping in
Euclidean space with Lipschitz continuity degree 1? ?max/?2.
The proof is sketched in Section 5.
Based on the connections between ADF training and contraction mapping, we
demonstrate the convergence properties of the ADF training method. First, we prove
the convergence of the ADF training.
Theorem 3 (ADF convergence)
ADF training is asymptotically convergent.
The proof is sketched in Section 5.
Further, we analyze the convergence rate of the ADF training. When we have the
lowest learning rate?t+1 = ??t, the expectation of the obtainedwt is as follows (Murata
1998; Hsu et al. 2009):
E(wt) = w? +
t
?
m=1
(I ??0?mH(w?))(w0 ?w?)
where w? is the optimal weight vector, and H is the Hessian matrix of the objective
function. The rate of convergence is governed by the largest eigenvalue of the function
Ct =
?t
m=1(I ??0?mH(w?)). Following Murata (1998) and Hsu et al. (2009), we can
derive a bound of rate of convergence, as follows.
571
Computational Linguistics Volume 40, Number 3
Theorem 4 (ADF convergence rate)
Assume ? is the largest eigenvalue of the function Ct =
?t
m=1(I ??0?mH(w?)). For the
proposed ADF training, its convergence rate is bounded by ?, and we have
? ? exp {?0???? 1}
where ? is the minimum eigenvalue ofH(w?).
The proof is sketched in Section 5.
The convergence analysis demonstrates that the proposed method with feature-
frequency-adaptive learning rates is convergent and the bound of convergence rate
is analyzed. It demonstrates that increasing the values of ?0 and ? leads to a lower
bound of the convergence rate. Because the bound of the convergence rate is just an
up-bound rather than the actual convergence rate, we still need to conduct automatic
tuning of the hyper-parameters, including ?0 and ?, for optimal convergence rate in
practice. The ADF training method has a fast convergence rate because the feature-
frequency-adaptive schema can avoid the fluctuations on updating the weights of high
frequency features, and it can avoid the insufficient training on updating the weights of
low frequency features. In the following sections, we perform experiments to confirm
the fast convergence rate of the proposed method.
4. Evaluation
Our main focus is on training heavily structured classification models. We evaluate the
proposal on three NLP structured classification tasks: biomedical named entity recogni-
tion (Bio-NER), Chinese word segmentation, and noun phrase (NP) chunking. For the
structured classification tasks, the ADF training is based on the CRF model (Lafferty,
McCallum, and Pereira 2001). Further, to demonstrate that the proposed method is
not limited to structured classification tasks, we also perform experiments on a non-
structured binary classification task: sentiment-based text classification. For the non-
structured classification task, the ADF training is based on themaximum entropymodel
(Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996).
4.1 Biomedical Named Entity Recognition (Structured Classification)
The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004
shared task. The task is to recognize five kinds of biomedical named entities, including
DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining
corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential
labeling task with the BIO encoding.
This data set consists of 20,546 training samples (from 2,000 MEDLINE article
abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data
are summarized in Table 1. State-of-the-art systems for this task include Settles (2004),
Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al.
(2009), and Tsuruoka, Tsujii, and Ananiadou (2009).
Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki,
et al. 2009), we use word token?based features, part-of-speech (POS) based features,
and orthography pattern?based features (prefix, uppercase/lowercase, etc.), as listed in
Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package),
the edges features usually contain only the information of yi?1 and yi, and ignore the
572
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Table 1
Summary of the Bio-NER data set.
#Abstracts #Sentences #Words
Train 2,000 20,546 (10/abs) 472,006 (23/sen)
Test 404 4,260 (11/abs) 96,780 (23/sen)
Table 2
Feature templates used for the Bio-NER task. wi is the current word token on position i. ti is the
POS tag on position i. oi is the orthography mode on position i. yi is the classification label on
position i. yi?1yi represents label transition. A? B represents a Cartesian product between
two sets.
Word Token?based Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}
?{yi, yi?1yi}
Part-of-Speech (POS)?based Features:
{ti?2, ti?1, ti, ti+1, ti+2, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}
?{yi, yi?1yi}
Orthography Pattern?based Features:
{oi?2, oi?1, oi, oi+1, oi+2, oi?2oi?1, oi?1oi, oioi+1, oi+1oi+2}
?{yi, yi?1yi}
information of the observation sequence (i.e., x). The major reason for this simple real-
ization of edge features in traditional CRF implementation is to reduce the dimension
of features. To improve the model accuracy, we utilize rich edge features following Sun,
Wang, and Li (2012), in which local observation information of x is combined in edge
features just like the implementation of node features. A detailed introduction to rich
edge features can be found in Sun, Wang, and Li (2012). Using the feature templates,
we extract a high dimensional feature set, which contains 5.3? 107 features in total.
Following prior studies, the evaluation metric for this task is the balanced F-score
defined as 2PR/(P+ R), where P is precision and R is recall.
4.2 Chinese Word Segmentation (Structured Classification)
Chinese word segmentation aims to automatically segment character sequences into
word sequences. Chinese word segmentation is important because it is the first step
for most Chinese language information processing systems. Our experiments are based
on the Microsoft Research data provided by The Second International Chinese Word
Segmentation Bakeoff. In this data set, there are 8.8? 104 word-types, 2.4? 106 word-
tokens, 5? 103 character-types, and 4.1? 106 character-tokens. State-of-the-art systems
for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and
Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010),
and Zhao and Kit (2011).
The feature engineering follows previous work on word segmentation (Sun, Wang,
and Li 2012). Rich edge features are used. For the classification label yi and the label
transition yi?1yi on position i, we use the feature templates as follows (Sun, Wang, and
Li 2012):
r Character unigrams located at positions i? 2, i? 1, i, i+ 1, and i+ 2.
573
Computational Linguistics Volume 40, Number 3
r Character bigrams located at positions i? 2, i? 1, i and i+ 1.
r Whether xj and xj+1 are identical, for j = i? 2, . . . , i+ 1.
r Whether xj and xj+2 are identical, for j = i? 3, . . . , i+ 1.
r The character sequence xj,i if it matches a word w ? U, with the constraint
i? 6 < j < i. The item xj,i represents the character sequence xj . . . xi.
U represents the unigram-dictionary collected from the training data.
r The character sequence xi,k if it matches a word w ? U, with the constraint
i < k < i+ 6.
r The word bigram candidate [xj,i?1, xi,k] if it hits a word bigram
[wi, wj] ? B, and satisfies the aforementioned constraints on j and k.
B represents the word bigram dictionary collected from the training data.
r The word bigram candidate [xj,i, xi+1,k] if it hits a word bigram
[wi, wj] ? B, and satisfies the aforementioned constraints on j and k.
All feature templates are instantiated with values that occurred in training samples.
The extracted feature set is large, and there are 2.4? 107 features in total. Our evaluation
is based on a closed test, and we do not use extra resources. Following prior studies, the
evaluation metric for this task is the balanced F-score.
4.3 Phrase Chunking (Structured Classification)
In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs,
are identified. The phrase chunking data is extracted from the data of the CoNLL-2000
shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936
sentences, and the test set consists of 2,012 sentences. We use the feature templates
based on word n-grams and part-of-speech n-grams, and feature templates are shown
in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8? 105
features in total. State-of-the-art systems for this task include Kudo and Matsumoto
(2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al.
(2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior
studies, the evaluation metric for this task is the balanced F-score.
4.4 Sentiment Classification (Non-Structured Classification)
To demonstrate that the proposed method is not limited to structured classification, we
select a well-known sentiment classification task for evaluating the proposed method
on non-structured classification.
Table 3
Feature templates used for the phrase chunking task. wi, ti, and yi are defined as before.
Word-Token?based Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}
?{yi, yi?1yi}
Part-of-Speech (POS)?based Features:
{ti?1, ti, ti+1, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}
?{yi, yi?1yi}
574
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Generally, sentiment classification classifies user review text as a positive or neg-
ative opinion. This task (Blitzer, Dredze, and Pereira 2007) consists of four subtasks
based on user reviews from Amazon.com. Each subtask is a binary sentiment clas-
sification task based on a specific topic. We use the maximum entropy model for
classification. We use the same lexical features as those used in Blitzer, Dredze, and
Pereira (2007), and the total number of features is 9.4? 105. Following prior work, the
evaluation metric is binary classification accuracy.
4.5 Experimental Setting
As for training, we perform gradient descent with the proposed ADF training method.
To compare with existing literature, we choose four popular training methods, a rep-
resentative batch training method, and three representative on-line training methods.
The batch training method is the limited-memory BFGS (LBFGS) method (Nocedal and
Wright 1999), which is considered to be one of the best optimizers for log-linear models
like CRFs. The on-line training methods include the SGD training method, which we
introduced in Section 2.2, the structured perceptron (Perc) training method (Freund
and Schapire 1999; Collins 2002), and the averaged perceptron (Avg-Perc) training
method (Collins 2002). The structured perceptron method and averaged perceptron
method are non-probabilistic training methods that have very fast training speed due
to the avoidance of the computation on gradients (Sun, Matsuzaki, and Li 2013). All
training methods, including ADF, SGD, Perc, Avg-Perc, and LBFGS, use the same set
of features.
We also compared the ADF method with the CW method (Dredze, Crammer, and
Pereira 2008) and the AROW method (Crammer, Kulesza, and Dredze 2009). The CW
and AROW methods are implemented based on the Confidence Weighted Learning
Library.2 Because the current implementation of the CW and AROW methods do not
utilize rich edge features, we removed the rich edge features in our systems to make
more fair comparisons. That is, we removed rich edge features in the CRF-ADF setting,
and this simplified method is denoted as ADF-noRich. The second-order stochastic
gradient descent training methods, including the SMD method (Vishwanathan et al.
2006) and the PSA method (Hsu et al. 2009), are not considered in our experiments
because we find those methods are quite slow when running on our data sets with high
dimensional features.
We find that the settings of q, ?, and ? in the ADF training method are not sensitive
among specific tasks and can be generally set. We simply set q = n/10 (n is the number
of training samples). It means that feature frequency information is updated 10 times
per iteration. Via cross-validation only on the training data of different tasks, we find
that the following setting is sufficient to produce adequate performance for most of
the real-world natural language processing tasks: ? around 0.995 and ? around 0.6.
This indicates that the feature frequency information has similar characteristics across
many different natural language processing tasks.
Thus, we simply use the following setting for all tasks: q = n/10, ? = 0.995, and
? = 0.6. This leaves c (the initial value of the learning rates) as the only hyper-parameter
that requires careful tuning. We perform automatic tuning for c based on the training
data via 4-fold cross-validation, testing with c = 0.005, 0.01, 0.05, 0.1, respectively, and
the optimal c is chosen based on the best accuracy of cross-validation. Via this automatic
2 http://webee.technion.ac.il/people/koby/code-index.html.
575
Computational Linguistics Volume 40, Number 3
tuning, we find it is proper to set c = 0.005, 0.1, 0.05, 0.005, for the Bio-NER, word
segmentation, phrase chunking, and sentiment classification tasks, respectively.
To reduce overfitting, we use an L2 Gaussian weight prior (Chen and Rosenfeld
1999) for the ADF, LBFGS, and SGD training methods. We vary the ? with different
values (e.g., 1.0, 2.0, and 5.0) for 4-fold cross validation on the training data of different
tasks, and finally set ? = 5.0 for all training methods in the Bio-NER task; ? = 5.0 for
all training methods in the word segmentation task; ? = 5.0, 1.0, 1.0 for ADF, SGD,
and LBFGS in the phrase chunking task; and ? = 1.0 for all training methods in the
sentiment classification task. Experiments are performed on a computer with an Intel(R)
Xeon(R) 2.0-GHz CPU.
4.6 Structured Classification Results
4.6.1 Comparisons Based on Empirical Convergence. First, we check the experimental re-
sults of different methods on their empirical convergence state. Because the perceptron
training method (Perc) does not achieve empirical convergence even with a very large
number of training passes, we simply report its results based on a large enough number
of training passes (e.g., 200 passes). Experimental results are shown in Table 4.
As we can see, the proposed ADF method is more accurate than other training
methods, either the on-line ones or the batch one. It is a bit surprising that the ADF
method performs even more accurately than the batch training method (LBFGS). We
notice that some previous work also found that on-line training methods could have
Table 4
Results for the Bio-NER, word segmentation, and phrase chunking tasks. The results and the
number of passes are decided based on empirical convergence (with score deviation of adjacent
five passes less than 0.01). For the non-convergent case, we simply report the results based on a
large enough number of training passes. As we can see, the ADF method achieves the best
accuracy with the fastest convergence speed.
Bio-NER Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 67.69 70.20 68.92 400 152,811.34
SGD (on-line) 70.91 72.69 71.79 91 76,549.21
Perc (on-line) 65.37 66.95 66.15 200 20,436.69
Avg-Perc (on-line) 68.76 72.56 70.61 37 3,928.01
ADF (proposal) 71.71 72.80 72.25 35 27,490.24
Segmentation Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 97.46 96.86 97.16 102 13,550.68
SGD (on-line) 97.58 97.11 97.34 27 6,811.15
Perc (on-line) 96.99 96.03 96.50 200 8,382.606
Avg-Perc (on-line) 97.56 97.05 97.30 16 716.87
ADF (proposal) 97.67 97.31 97.49 15 4,260.08
Chunking Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 94.57 94.09 94.33 105 797.04
SGD (on-line) 94.48 94.04 94.26 56 903.88
Perc (on-line) 93.66 93.31 93.48 200 543.51
Avg-Perc (on-line) 94.34 94.04 94.19 12 33.45
ADF (proposal) 94.66 94.38 94.52 17 282.17
576
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
better performance than batch training methods such as LBFGS (Tsuruoka, Tsujii, and
Ananiadou 2009; Schaul, Zhang, and LeCun 2012). The ADF training method can
achieve better results probably because the feature-frequency?adaptive training schema
can produce more balanced training of features with diversified frequencies. Traditional
SGD training may over-train high frequency features and at the same time may have
insufficient training of low frequency features. The ADF training method can avoid
such problems. It will be interesting to perform further analysis in future work.
We also performed significance tests based on t-tests with a significance level of
0.05. Significance tests demonstrate that the ADF method is significantly more accurate
than the existing training methods in most of the comparisons, whether on-line or
batch. For the Bio-NER task, the differences between ADF and LBFGS, SGD, Perc,
and Avg-Perc are significant. For the word segmentation task, the differences between
ADF and LBFGS, SGD, Perc, and Avg-Perc are significant. For the phrase chunking
task, the differences between ADF and Perc and Avg-Perc are significant; the differences
between ADF and LBFGS and SGD are non-significant.
Moreover, as we can see, the proposed method achieves a convergence state with
the least number of training passes, and with the least wall-clock time. In general, the
ADF method is about one order of magnitude faster than the LBFGS batch training
method and several times faster than the existing on-line training methods.
4.6.2 Comparisons with State-of-the-Art Systems. The three tasks are well-known bench-
mark tasks with standard data sets. There is a large amount of published research on
those three tasks. We compare the proposed method with the state-of-the-art systems.
The comparisons are shown in Table 5.
As we can see, our system is competitive with the best systems for the Bio-NER,
word segmentation, and NP-chunking tasks. Many of the state-of-the-art systems use
extra resources (e.g., linguistic knowledge) or complicated systems (e.g., voting over
Table 5
Comparing our results with some representative state-of-the-art systems.
Bio-NER Method F-score
(Okanohara et al. 2006) Semi-Markov CRF + global features 71.5
(Hsu et al. 2009) CRF + PSA(1) training 69.4
(Tsuruoka, Tsujii, and Ananiadou 2009) CRF + SGD-L1 training 71.6
Our Method CRF + ADF training 72.3
Segmentation Method F-score
(Gao et al. 2007) Semi-Markov CRF 97.2
(Sun, Zhang, et al. 2009) Latent-variable CRF 97.3
(Sun 2010) Multiple segmenters + voting 96.9
Our Method CRF + ADF training 97.5
Chunking Method F-score
(Kudo and Matsumoto 2001) Combination of multiple SVM 94.2
(Vishwanathan et al. 2006) CRF + SMD training 93.6
(Sun et al. 2008) Latent-variable CRF 94.3
Our Method CRF + ADF training 94.5
577
Computational Linguistics Volume 40, Number 3
multiple models). Thus, it is impressive that our single model?based system without
extra resources achieves good performance. This indicates that the proposed ADF
training method can train model parameters with good generality on the test data.
4.6.3 Training Curves. To study the detailed training process and convergence speed, we
show the training curves in Figures 2?4. Figure 2 focuses on the comparisons between
the ADF method and the existing on-line training methods. As we can see, the ADF
method converges faster than other on-line training methods in terms of both training
passes and wall-clock time. The ADF method has roughly the same training speed per
pass compared with traditional SGD training.
Figure 3 (Top Row) focuses on comparing the ADF method with the CW method
(Dredze, Crammer, and Pereira 2008) and the AROW method (Crammer, Kulesza, and
Dredze 2009). Comparisons are based on similar features. As discussed before, the ADF-
noRich method is a simplified system, with rich edge features removed from the CRF-
ADF system. As we can see, the proposed ADF method, whether with or without rich
edge features, outperforms the CWandAROWmethods. Figure 3 (BottomRow) focuses
on the comparisons with different mini-batch (the training samples in each stochastic
update) sizes. Representative results with a mini-batch size of 10 are shown. In general,
we find larger mini-batch sizes will slow down the convergence speed. Results demon-
strate that, compared with the SGD training method, the ADF training method is less
sensitive to mini-batch sizes.
Figure 4 focuses on the comparisons between the ADF method and the batch
training method LBFGS. As we can see, the ADF method converges at least one order
0 20 40 60 80 10066
67
68
69
70
71
72
73
Number of Passes
)
 
 
ADFSGDPerc
0 20 40 60 80 10096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. on-line)
Number of Passes
)
 
 
ADFSGDPerc
0 20 40 60 80 10092.5
93
93.5
94
94.5 Chunking (ADF vs. on-line)
Number of Passes
)
 
 
ADFSGDPerc
0 1 2 3 4 5 6x 104
66
67
68
69
70
71
72
73
Training Time (sec)
)
 
 
ADFSGDPerc
0 2,000 4,000 6,000 8,000 10,00096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. on-line)
Training Time (sec)
)
 
 
ADFSGDPerc
0 200 400 600 80092.5
93
93.5
94
94.5 Chunking (ADF vs. on-line)
Training Time (sec)
)
 
 
ADFSGDPerc
Figure 2
Comparisons among the ADF method and other on-line training methods. (Top Row)
Comparisons based on training passes. As we can see, the ADF method has the best accuracy
and with the fastest convergence speed based on training passes. (Bottom Row) Comparisons
based on wall-clock time.
578
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
0 20 40 60 80 10060
62
64
66
68
70
72
Bio?NER (ADF vs. CW/AROW)
Number of Passes
F?sco
re (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10092
93
94
95
96
97
98Segmentation (ADF vs. CW/AROW)
Number of Passes
F?sco
re  (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10092.5
93
93.5
94
94.5
95 Chunking (ADF vs. CW/AROW)
Number of Passes
F?sco
re (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10068
69
70
71
72
73 Bio?NER (MiniBatch=10)
Number of Passes
F?sco
re (%)
 
 
ADFSGD
0 20 40 60 80 10096.4
96.6
96.8
97
97.2
97.4
97.6
Segmentation (MiniBatch=10)
Number of Passes
F?sco
re  (%)
 
 
ADFSGD
0 20 40 60 80 10093.2
93.4
93.6
93.8
94
94.2
94.4
94.6
Chunking (MiniBatch=10)
Number of Passes
F?sco
re (%)
 
 
ADFSGD
Figure 3
(Top Row) Comparing ADF and ADF-noRich with CW and AROW methods. As we can see,
both the ADF and ADF-noRich methods work better than the CW and AROW methods.
(Bottom Row) Comparing different methods with mini-batch = 10 in the stochastic learning
setting.
magnitude faster than the LBFGS training in terms of both training passes and wall-
clock time. For the LBFGS training, we need to determine the LBFGSmemory parameter
m, which controls the number of prior gradients used to approximate the Hessian
information. A larger value of m will potentially lead to more accurate estimation
of the Hessian information, but at the same time will consume significantly more
memory. Roughly, the LBFGS training consumes m times more memory than the ADF
on-line training method. For most tasks, the default setting of m = 10 is reasonable. We
set m = 10 for the word segmentation and phrase chunking tasks, and m = 6 for the
Bio-NER task due to the shortage of memory for m > 6 cases in this task.
4.6.4 One-Pass Learning Results. Many real-world data sets can only observe the training
data in one pass. For example, some Web-based on-line data streams can only appear
once so that the model parameter learning should be finished in one-pass learning (see
Zinkevich et al. 2010). Hence, it is important to test the performance in the one-pass
learning scenario.
In the one-pass learning scenario, the feature frequency information is computed
?on the fly? during on-line training. As shown in Section 3.1, we only need to have
a real-valued vector v to record the cumulative feature frequency information, which
is updated when observing training instances one by one. Then, the learning rate
vector ? is updated based on the v only and there is no need to observe the training
instances again. This is the same algorithm introduced in Section 3.1 and no change is
required for the one-pass learning scenario. Figure 5 shows the comparisons between
the ADF method and baselines on one-pass learning. As we can see, the ADF method
579
Computational Linguistics Volume 40, Number 3
0 100 200 300 40066
67
68
69
70
71
72
73 Bio?NER (ADF vs. batch)
Number of Passes
F?sco
re (%)
 
 
ADFLBFGS
0 100 200 300 40096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. batch)
Number of Passes
F?sco
re  (%)
 
 
ADFLBFGS
0 100 200 300 400
93.6
93.8
94
94.2
94.4
Chunking (ADF vs. batch)
Number of Passes
F?sco
re (%)
 
 
ADFLBFGS
0 5 10 15x 104
66
67
68
69
70
71
72
73 Bio?NER (ADF vs. batch)
Training Time (sec)
F?sco
re (%)
 
 
ADFLBFGS
0 1 2 3 4 5x 104
9696.2
96.496.6
96.897
97.297.4
97.6
Segmentation (ADF vs. batch)
Training Time (sec)
F?sco
re  (%)
 
 
ADFLBFGS
0 1,000 2,000 3,00092.5
93
93.5
94
94.5 Chunking (ADF vs. batch)
Training Time (sec)
)
 
 
ADFLBFGS
Figure 4
Comparisons between the ADF method and the batch training method LBFGS. (Top Row)
Comparisons based on training passes. As we can see, the ADF method converges much faster
than the LBFGS method, and with better accuracy on the convergence state. (Bottom Row)
Comparisons based on wall-clock time.
consistently outperforms the baselines. This also reflects the fast convergence speed of
the ADF training method.
4.7 Non-Structured Classification Results
In previous experiments, we showed that the proposed method outperforms existing
baselines on structured classification. Nevertheless, we want to show that the ADF
method also has good performance on non-structured classification. In addition, this
task is based on real-valued features instead of binary features.
Figure 5
Comparisons among different methods based on one-pass learning. As we can see, the ADF
method has the best accuracy on one-pass learning.
580
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Table 6
Results on sentiment classification (non-structured binary classification).
Accuracy Passes Train-Time (sec)
LBFGS (batch) 87.00 86 72.20
SGD (on-line) 87.13 44 55.88
Perc (on-line) 84.55 25 5.82
Avg-Perc (on-line) 85.04 46 12.22
ADF (proposal) 87.89 30 57.12
Experimental results of different training methods on the convergence state are
shown in Table 6. As we can see, the proposed method outperforms all of the on-line
and batch baselines in terms of binary classification accuracy. Here again we observe
that the ADF and SGD methods outperform the LBFGS baseline.
The training curves are shown in Figure 6. As we can see, the ADF method con-
verges quickly. Because this data set is relatively small and the feature dimension is
much smaller than previous tasks, we find the baseline training methods also have
fast convergence speed. The comparisons on one-pass learning are shown in Fig-
ure 7. Just as for the experiments for structured classification tasks, the ADF method
0 20 40 60 80 10082
83
84
85
86
87
88
89 Sentiment (ADF vs. on-line)
Number of Passes
Accur
acy (%
)
 
 
ADFSGDPerc
0 20 40 60 8082
83
84
85
86
87
88
89 Sentiment (ADF vs. on-line)
Training Time (sec)
Accur
acy (%
)
 
 
ADFSGDPerc
0 50 100 15082
83
84
85
86
87
88
89 Sentiment (ADF vs. batch)
Number of Passes
Accur
acy (%
)
 
 
ADFLBFGS
0 50 100 15082
83
84
85
86
87
88
89 Sentiment (ADF vs. batch)
Training Time (sec)
Accu
racy (%
)
 
 
ADFLBFGS
Figure 6
F-score curves on sentiment classification. (Top Row) Comparisons among the ADF method and
on-line training baselines, based on training passes and wall-clock time, respectively. (Bottom
Row) Comparisons between the ADF method and the batch training method LBFGS, based on
training passes and wall-clock time, respectively. As we can see, the ADF method outperforms
both the on-line training baselines and the batch training baseline, with better accuracy and
faster convergence speed.
581
Computational Linguistics Volume 40, Number 3
Figure 7
One-pass learning results on sentiment classification.
outperforms the baseline methods on one-pass learning, with more than 12.7% error
rate reduction.
5. Proofs
This section gives proofs of Theorems 1?4.
Proof of Theorem 1 Following Equation (5), the ADF update rule is F(wt) := wt+1 =
wt +? ? gt. For ?wt ? X ,
|F(wt+1)? F(wt)|
= |F(wt+1)?wt+1|
= |wt+1 +? ? gt+1 ?wt+1|
= |? ? gt+1|
= [(a1b1)2 + (a2b2)2 + ? ? ?+ (af bf )2]1/2
? [(?maxb1)2 + (?maxb2)2 + ? ? ?+ (?maxbf )2]1/2
= |?maxgt+1|
= |FSGD(wt+1)? FSGD(wt)|
(6)
where ai and bi are the ith elements of the vector ? and gt+1, respectively. FSGD is the
SGD update rule with the fixed learning rate ?max such that ?max := sup{?i where ?i ?
?}. In other words, for the SGD update rule FSGD, the fixed learning rate ?max is derived
from the ADF update rule. According to Lemma 1, the SGD update rule FSGD is a
contraction mapping in Euclidean space with Lipschitz continuity degree 1? ?max/?2,
given the condition that ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1. Hence, it goes to
|FSGD(wt+1)? FSGD(wt)| ? (1? ?max/?2)|wt+1 ?wt| (7)
Combining Equations (6) and (7), it goes to
|F(wt+1)? F(wt)| ? (1? ?max/?2)|wt+1 ?wt|
Thus, according to the definition of dynamic contraction mapping, the ADF update rule
is a dynamic contraction mapping in Euclidean space with Lipschitz continuity degree
1? ?max/?2. ut
582
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Proof of Theorem 2 As presented in Equation (5), the ADF update rule is F(wt) :=
wt+1 = wt +?t ? gt. For ?wt ? X ,
|F(wt+1)? F(wt)|
= |?t+1 ? gt+1|
= [(a1b1)2 + (a2b2)2 + ? ? ?+ (af bf )2]1/2
? [(?maxb1)2 + (?maxb2)2 + ? ? ?+ (?maxbf )2]1/2
= |FSGD(wt+1)? FSGD(wt)|
(8)
where ai is the ith element of the vector ?t+1. bi and FSGD are the same as before.
Similar to the analysis of Theorem 1, the third step of Equation (8) is valid because ?max
is the maximum learning rate at the beginning and all learning rates are decreasing
when t is increasing. The proof can be easily derived following the same steps in the
proof of Theorem 1. To avoid redundancy, we do not repeat the derivation. ut
Proof of Theorem 3 Let M be the accumulative change of the ADF weight vectorwt:
Mt :=
?
t?=1,2,...,t
|wt?+1 ?wt? |
To prove the convergence of the ADF, we need to prove the sequence Mt converges as
t??. Following Theorem 2, we have the following formula for the ADF training:
|F(wt+1)? F(wt)| ? (1? ?max/?2)|wt+1 ?wt|
where ?max is the maximum learning rate at the beginning. Let d0 := |w2 ?w1| and
q := 1? ?max/?2, then we have:
Mt =
?
t?=1,2,...,t
|wt?+1 ?wt? |
? d0 + d0q+ d0q2 + ? ? ?+ d0qt?1
= d0(1? qt)/(1? q)
(9)
When t??, d0(1? qt)/(1? q) goes to d0/(1? q) because q < 1. Hence, we have:
Mt ? d0/(1? q)
Thus, Mt is upper-bounded. Because we know that Mt is a monotonically increasing
function when t??, it follows that Mt converges when t??. This completes the
proof. ut
583
Computational Linguistics Volume 40, Number 3
Proof of Theorem 4 First, we have
eigen(Ct) =
t
?
m=1
(1??0?m?)
? exp
{
??0?
t
?
m=1
?m
}
Then, we have
0 ?
n
?
j=1
(1? aj) ?
n
?
j=1
e?aj = e?
?n
j=1 aj
This is because 1? aj ? e?aj given 0 ? aj < 1. Finally, because
?t
m=1 ?m ?
?
1?? when
t??, we have
eigen(Ct) ? exp
{
??0?
t
?
m=1
?m
}
? exp
{
??0??
1? ?
}
This completes the proof. ut
6. Conclusions
In this work we tried to simultaneously improve the training speed andmodel accuracy
of natural language processing systems. We proposed the ADF on-line training method,
based on the core idea that high frequency features should result in a learning rate that
decays faster. We demonstrated that the ADF on-line training method is convergent
and has good theoretical properties. Based on empirical experiments, we can state the
following conclusions. First, the ADF method achieved the major target of this work:
faster training speed and higher accuracy at the same time. Second, the ADF method
was robust: It had good performance on several structured and non-structured classifi-
cation tasks with very different characteristics. Third, the ADF method worked well on
both binary features and real-valued features. Fourth, the ADF method outperformed
existing methods in a one-pass learning setting. Finally, our method achieved state-
of-the-art performance on several well-known benchmark tasks. To the best of our
knowledge, our simple method achieved a much better F-score than the existing best
reports on the biomedical named entity recognition task.
Acknowledgments
This work was supported by the National
Natural Science Foundation of China
(no. 61300063, no. 61370117), the Doctoral
Fund of Ministry of Education of China
(no. 20130001120004), a Hong Kong
Polytechnic University internal grant
(4-ZZD5), a Hong Kong RGC Project
(no. PolyU 5230/08E), the National High
Technology Research and Development
Program of China (863 Program,
no. 2012AA011101), and the Major National
Social Science Fund of China (no. 12&ZD227).
This work is a substantial extension of the
conference version presented at ACL 2012
(Sun, Wang, and Li 2012).
584
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
References
Berger, Adam L., Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics,
pages 440?447, Prague.
Bottou, Le?on. 1998. Online algorithms
and stochastic approximations. In
D. Saad, editor. Online Learning and Neural
Networks. Cambridge University Press,
pages 9?42.
Chen, Stanley F. and Ronald Rosenfeld.
1999. A Gaussian prior for smoothing
maximum entropy models. Technical
Report CMU-CS-99-108, Carnegie
Mellon University.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of
EMNLP?02, pages 1?8, Philadelphia, PA.
Crammer, Koby, Alex Kulesza, and Mark
Dredze. 2009. Adaptive regularization of
weight vectors. In NIPS?09, pages 414?422,
Vancouver.
Dredze, Mark, Koby Crammer, and
Fernando Pereira. 2008. Confidence-
weighted linear classification. In
Proceedings of ICML?08, pages 264?271,
Helsinki.
Duchi, John, Elad Hazan, and Yoram Singer.
2010. Adaptive subgradient methods
for online learning and stochastic
optimization. Journal of Machine
Learning Research, 12:2,121?2,159.
Finkel, Jenny, Shipra Dingare, Huy Nguyen,
Malvina Nissim, Christopher Manning,
and Gail Sinclair. 2004. Exploiting context
for biomedical entity recognition: From
syntax to the Web. In Proceedings of
BioNLP?04, pages 91?94, Geneva.
Freund, Yoav and Robert Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277?296.
Gao, Jianfeng, Galen Andrew, Mark Johnson,
and Kristina Toutanova. 2007. A comparative
study of parameter estimation methods for
statistical natural language processing. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics
(ACL?07), pages 824?831, Prague.
Hsu, Chun-Nan, Han-Shen Huang, Yu-Ming
Chang, and Yuh-Jye Lee. 2009. Periodic
step-size adaptation in second-order
gradient descent for single-pass on-line
structured learning. Machine Learning,
77(2-3):195?224.
Jacobs, Robert A. 1988. Increased rates of
convergence through learning rate
adaptation. Neural Networks, 1(4):295?307.
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa
Tsuruoka, and Yuka Tateisi. 2004.
Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of
BioNLP?04, pages 70?75, Geneva.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Proceedings of NAACL?01, pages 1?8,
Pittsburgh, PA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence
data. In ICML?01, pages 282?289,
Williamstown, MA.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Flexible text
segmentation with structured multilabel
classification. In Proceedings of HLT/
EMNLP?05, pages 987?994, Vancouver.
McMahan, H. Brendan and Matthew J.
Streeter. 2010. Adaptive bound
optimization for online convex
optimization. In Proceedings of COLT?10,
pages 244?256, Haifa.
Murata, Noboru. 1998. A statistical study
of on-line learning. In D. Saad, editor.
Online Learning in Neural Networks.
Cambridge University Press, pages 63?92.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical optimization. Springer.
Okanohara, Daisuke, Yusuke Miyao,
Yoshimasa Tsuruoka, and Jun?ichi Tsujii.
2006. Improving the scalability of
semi-Markov conditional random
fields for named entity recognition.
In Proceedings of COLING-ACL?06,
pages 465?472, Sydney.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
1996, pages 133?142, Pennsylvania.
Sang, Erik Tjong Kim and Sabine Buchholz.
2000. Introduction to the CoNLL-2000
shared task: Chunking. In Proceedings
of CoNLL?00, pages 127?132, Lisbon.
Schaul, Tom, Sixin Zhang, and Yann LeCun.
2012. No more pesky learning rates. CoRR,
abs/1206.1106.
585
Computational Linguistics Volume 40, Number 3
Settles, Burr. 2004. Biomedical named entity
recognition using conditional random
fields and rich feature sets. In Proceedings
of BioNLP?04, pages 104?107, Geneva.
Shalev-Shwartz, Shai, Yoram Singer, and
Nathan Srebro. 2007. Pegasos: Primal
estimated sub-gradient solver for SVM.
In Proceedings of ICML?07, pages 807?814,
Corvallis, OR.
Sperduti, Alessandro and Antonina Starita.
1993. Speed up learning and network
optimization with extended back
propagation. Neural Networks, 6(3):365?383.
Sun, Weiwei. 2010. Word-based and
character-based word segmentation
models: Comparison and combination.
In COLING?10 (Posters), pages 1,211?1,219,
Beijing.
Sun, Xu, Takuya Matsuzaki, and Wenjie Li.
2013. Latent structured perceptrons
for large-scale learning with hidden
information. IEEE Transactions on
Knowledge and Data Engineering,
25(9):2,063?2,075.
Sun, Xu, Takuya Matsuzaki, Daisuke
Okanohara, and Jun?ichi Tsujii. 2009.
Latent variable perceptron algorithm for
structured classification. In Proceedings
of the 21st International Joint Conference
on Artificial Intelligence (IJCAI 2009),
pages 1,236?1,242, Pasadena, CA.
Sun, Xu, Louis-Philippe Morency, Daisuke
Okanohara, and Jun?ichi Tsujii. 2008.
Modeling latent-dynamic in shallow
parsing: A latent conditional model with
improved inference. In Proceedings of
COLING?08, pages 841?848, Manchester.
Sun, Xu, Houfeng Wang, and Wenjie Li. 2012.
Fast online training with frequency-
adaptive learning rates for Chinese word
segmentation and new word detection.
In Proceedings of ACL?12, pages 253?262,
Jeju Island.
Sun, Xu, Yaozhong Zhang, Takuya
Matsuzaki, Yoshimasa Tsuruoka, and
Jun?ichi Tsujii. 2009. A discriminative
latent variable Chinese segmenter with
hybrid word/character information.
In Proceedings of NAACL-HLT?09,
pages 56?64, Boulder, CO.
Sun, Xu, Yao Zhong Zhang, Takuya
Matsuzaki, Yoshimasa Tsuruoka, and
Jun?ichi Tsujii. 2013. Probabilistic Chinese
word segmentation with non-local
information and stochastic training.
Information Processing & Management,
49(3):626?636.
Tseng, Huihsin, Pichuan Chang, Galen
Andrew, Daniel Jurafsky, and Christopher
Manning. 2005. A conditional random
field word segmenter for SIGHAN bakeoff
2005. In Proceedings of the Fourth SIGHAN
Workshop, pages 168?171, Jeju Island.
Tsuruoka, Yoshimasa, Jun?ichi Tsujii, and
Sophia Ananiadou. 2009. Stochastic
gradient descent training for
l1-regularized log-linear models with
cumulative penalty. In Proceedings of
ACL?09, pages 477?485, Suntec.
Vishwanathan, S. V. N., Nicol N.
Schraudolph, Mark W. Schmidt, and
Kevin P. Murphy. 2006. Accelerated
training of conditional random
fields with stochastic meta-descent.
In Proceedings of ICML?06, pages 969?976,
Pittsburgh, PA.
Zhang, Ruiqiang, Genichiro Kikui, and
Eiichiro Sumita. 2006. Subword-based
tagging by conditional random fields for
Chinese word segmentation. In Proceedings
of the Human Language Technology
Conference of the NAACL, Companion
Volume: Short Papers, pages 193?196,
New York City.
Zhang, Yue and Stephen Clark. 2007.
Chinese segmentation with a word-based
perceptron algorithm. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 840?847,
Prague.
Zhao, Hai, Changning Huang, Mu Li,
and Bao-Liang Lu. 2010. A unified
character-based tagging framework for
Chinese word segmentation. ACM
Transactions on Asian Language Information
Processing, 9(2): Article 5.
Zhao, Hai and Chunyu Kit. 2011. Integrating
unsupervised and supervised word
segmentation: The role of goodness
measures. Information Sciences,
181(1):163?183.
Zinkevich, Martin, Markus Weimer,
Alexander J. Smola, and Lihong Li.
2010. Parallelized stochastic gradient
descent. In Proceedings of NIPS?10,
pages 2,595?2,603, Vancouver.
586
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 266?274,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
  Learning Phrase-Based Spelling Error Models  
from Clickthrough Data 
 
 
 
Xu Sun? 
Dept. of Mathematical Informatics 
University of Tokyo, Tokyo, Japan 
xusun@mist.i.u-tokyo.ac.jp
Jianfeng Gao 
Microsoft Research 
Redmond, WA, USA 
jfgao@microsoft.com 
 
Daniel Micol 
Microsoft Corporation 
Munich, Germany 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research 
Redmond, WA, USA 
chrisq@microsoft.com 
 
                                                     
? The work was done when Xu Sun was visiting Microsoft Research Redmond. 
Abstract 
This paper explores the use of clickthrough data 
for query spelling correction. First, large amounts 
of query-correction pairs are derived by analyzing 
users' query reformulation behavior encoded in 
the clickthrough data. Then, a phrase-based error 
model that accounts for the transformation 
probability between multi-term phrases is trained 
and integrated into a query speller system. Expe-
riments are carried out on a human-labeled data 
set. Results show that the system using the 
phrase-based error model outperforms signifi-
cantly its baseline systems. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods for three 
main reasons (Ahmad and Kondrak, 2004).  First, 
spelling errors are more common in search queries 
than in regular written text: roughly 10-15% of 
queries contain misspelled terms (Cucerzan and 
Brill, 2004). Second, most search queries consist 
of a few key words rather than grammatical sen-
tences, making a grammar-based approach inap-
propriate. Most importantly, many queries con-
tain search terms, such as proper nouns and names, 
which are not well established in the language. 
For example, Chen et al (2007) reported that 
16.5% of valid search terms do not occur in their 
200K-entry spelling lexicon. 
Therefore, recent research has focused on the 
use of Web corpora and query logs, rather than 
human-compiled lexicons, to infer knowledge 
about misspellings and word usage in search 
queries (e.g., Whitelaw et al, 2009). Another 
important data source that would be useful for this 
purpose is clickthrough data. Although it is 
well-known that clickthrough data contain rich 
information about users' search behavior, e.g., 
how a user (re-) formulates a query in order to 
find the relevant document, there has been little 
research on exploiting the data for the develop-
ment of a query speller system. 
In this paper we present a novel method of 
extracting large amounts of query-correction pairs 
from the clickthrough data.  These pairs, impli-
citly judged by millions of users, are used to train 
a set of spelling error models. Among these 
models, the most effective one is a phrase-based 
error model that captures the probability of 
transforming one multi-term phrase into another 
multi-term phrase. Comparing to traditional error 
models that account for transformation probabili-
ties between single characters (Kernighan et al, 
1990) or sub-word strings (Brill and Moore, 
2000), the phrase-based model is more powerful 
in that it captures some contextual information by 
retaining inter-term dependencies. We show that 
this information is crucial to detect the correction 
of a query term, because unlike in regular written 
text, any query word can be a valid search term 
and in many cases the only way for a speller 
system to make the judgment is to explore its 
usage according to the contextual information. 
We conduct a set of experiments on a large 
data set, consisting of human-labeled 
266
query-correction pairs. Results show that the error 
models learned from clickthrough data lead to 
significant improvements on the task of query 
spelling correction. In particular, the speller sys-
tem incorporating a phrase-based error model 
significantly outperforms its baseline systems. 
To the best of our knowledge, this is the first 
extensive study of learning phase-based error 
models from clickthrough data for query spelling 
correction. The rest of the paper is structured as 
follows. Section 2 reviews related work. Section 3 
presents the way query-correction pairs are ex-
tracted from the clickthrough data. Section 4 
presents the baseline speller system used in this 
study. Section 5 describes in detail the phrase- 
based error model. Section 6 presents the expe-
riments. Section 7 concludes the paper. 
2 Related Work 
Spelling correction for regular written text is a 
long standing research topic. Previous researches 
can be roughly grouped into two categories: 
correcting non-word errors and real-word errors. 
In non-word error spelling correction, any 
word that is not found in a pre-compiled lexicon is 
considered to be misspelled.  Then, a list of lexical 
words that are similar to the misspelled word are 
proposed as candidate spelling corrections. Most 
traditional systems use a manually tuned similar-
ity function (e.g., edit distance function) to rank 
the candidates, as reviewed by Kukich (1992). 
During the last two decades, statistical error 
models learned on training data (i.e., 
query-correction pairs) have become increasingly 
popular, and have proven more effective (Ker-
nighan et al, 1990; Brill and Moore, 2000; Tou-
tanova and Moore, 2002; Okazaki et al, 2008).  
Real-word spelling correction is also referred 
to as context sensitive spelling correction (CSSC). 
It tries to detect incorrect usages of a valid word 
based on its context, such as "peace" and "piece" 
in the context "a _ of cake". A common strategy in 
CSSC is as follows. First, a pre-defined confusion 
set is used to generate candidate corrections, then 
a  scoring model, such as a trigram language 
model or na?ve Bayes classifier, is used to rank the 
candidates according to their context (e.g., 
Golding and Roth, 1996; Mangu and Brill, 1997; 
Church et al, 2007). 
When designed to handle regular written text, 
both CSSC and non-word error speller systems 
rely on a pre-defined vocabulary (i.e., either a 
lexicon or a confusion set). However, in query 
spelling correction, it is impossible to compile 
such a vocabulary, and the boundary between the 
non-word and real-word errors is quite vague. 
Therefore, recent research on query spelling 
correction has focused on exploiting noisy Web 
data and query logs to infer knowledge about 
misspellings and word usage in search queries. 
Cucerzan and Brill (2004) discuss in detail the 
challenges of query spelling correction, and 
suggest the use of query logs. Ahmad and Kon-
drak (2005) propose a method of estimating an 
error model from query logs using the EM algo-
rithm. Li et al (2006) extend the error model by 
capturing word-level similarities learned from 
query logs. Chen et al (2007) suggest using web 
search results to improve spelling correction. 
Whitelaw et al (2009) present a query speller 
system in which both the error model and the 
language model are trained using Web data. 
Compared to Web corpora and query logs, 
clickthrough data contain much richer informa-
tion about users? search behavior.  Although there 
has been a lot of research on using clickthrough 
data to improve Web document retrieval (e.g., 
Joachims, 2002; Agichtein et al, 2006; Gao et al, 
2009), the data have not been fully explored for 
query spelling correction. This study tries to learn 
error models from clickthrough data. To our 
knowledge, this is the first such attempt using 
clickthrough data. 
Most of the speller systems reviewed above are 
based on the framework of the source channel 
model. Typically, a language model (source 
model) is used to capture contextual information, 
while an error model (channel model) is consi-
dered to be context free in that it does not take into 
account any contextual information in modeling 
word transformation probabilities. In this study 
we argue that it is beneficial to capture contextual 
information in the error model. To this end, in-
spired by the phrase-based statistical machine 
translation (SMT) systems (Koehn et al, 2003; 
Och and Ney, 2004), we propose a phrase-based 
error model where we assume that query spelling 
correction is performed at the phrase level. 
In what follows, before presenting the phrase- 
based error model, we will first describe the 
clickthrough data and the query speller system we 
used in this study. 
3 Clickthrough Data and Spelling Cor-
rection 
This section describes the way the 
query-correction pairs are extracted from click-
267
through data. Two types of clickthrough data are 
explored in our experiment. 
The clickthrough data of the first type has been 
widely used in previous research and proved to be 
useful for Web search (Joachims, 2002; Agichtein 
et al, 2006; Gao et al, 2009) and query refor-
mulation (Wang and Zhai, 2008; Suzuki et al, 
2009). We start with this same data with the hope 
of achieving similar improvements in our task. 
The data consist of a set of query sessions that 
were extracted from one year of log files from a 
commercial Web search engine. A query session 
contains a query issued by a user and a ranked list 
of links (i.e., URLs) returned to that same user 
along with records of which URLs were clicked. 
Following Suzuki et al (2009), we extract 
query-correction pairs as follows. First, we extract 
pairs of queries Q1 and Q2 such that (1) they are 
issued by the same user; (2) Q2 was issued within 
3 minutes of Q1; and (3) Q2 contained at least one 
clicked URL in the result page while Q1 did not 
result in any clicks.  We then scored each query 
pair (Q1, Q2) using the edit distance between Q1 
and Q2, and retained those with an edit distance 
score lower than a pre-set threshold as query 
correction pairs. 
Unfortunately, we found in our experiments 
that the pairs extracted using the method are too 
noisy for reliable error model training, even with a 
very tight threshold, and we did not see any sig-
nificant improvement. Therefore, in Section 6 we 
will not report results using this dataset. 
The clickthrough data of the second type con-
sists of a set of query reformulation sessions 
extracted from 3 months of log files from a 
commercial Web browser.  A query reformulation 
session contains a list of URLs that record user 
behaviors that relate to the query reformulation 
functions, provided by a Web search engine. For 
example, almost all commercial search engines 
offer the "did you mean" function, suggesting a 
possible alternate interpretation or spelling of a 
user-issued query. Figure 1 shows a sample of the 
query reformulation sessions that record the "did 
you mean" sessions from three of the most pop-
ular search engines. These sessions encode the 
same user behavior: A user first queries for 
"harrypotter sheme park", and then clicks on the 
resulting spelling suggestion "harry potter theme 
park". In our experiments, we "reverse-engineer" 
the parameters from the URLs of these sessions, 
and deduce how each search engine encodes both 
a query and the fact that a user arrived at a URL 
by clicking on the spelling suggestion of the query 
? an important indication that the spelling sug-
gestion is desired. From these three months of 
query reformulation sessions, we extracted about 
3 million query-correction pairs. Compared to the 
pairs extracted from the clickthrough data of the 
first type (query sessions), this data set is much 
cleaner because all these spelling corrections are 
actually clicked, and thus judged implicitly, by 
many users. 
In addition to the "did you mean" function, 
recently some search engines have introduced two 
new spelling suggestion functions. One is the 
"auto-correction" function, where the search 
engine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results for the user.  The other is 
the "split pane" result page, where one half por-
tion of the search results are produced using the 
original query, while the other half, usually vi-
sually separate portion of results are produced 
using the auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+park&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+park& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+park&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 1.  A sample of query reformulation sessions 
from three popular search engines. These sessions 
show that a user first issues the query "harrypotter 
sheme park", and then clicks on the resulting spell 
suggestion "harry potter theme park". 
268
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the clickthrough data, is already able 
to correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of 
underestimating the self-transformation probabil-
ity of a query P(Q2=Q1|Q1), because we only 
included in the training data the pairs where the 
query is different from the correction. To deal 
with this problem, we augmented the training data 
by including correctly spelled queries, i.e., the 
pairs (Q1, Q2) where Q1 = Q2.  First, we extracted a 
set of queries from the sessions where no spell 
suggestion is presented or clicked on. Second, we 
removed from the set those queries that were 
recognized as being auto-corrected by a search 
engine. We do so by running a sanity check of the 
queries against our baseline spelling correction 
system, which will be described in Section 6. If 
the system thinks an input query is misspelled, we 
assumed it was an obvious misspelling, and re-
moved it. The remaining queries were assumed to 
be correctly spelled and were added to the training 
data. 
4 The Baseline Speller System 
The spelling correction problem is typically 
formulated under the framework of the source 
channel model. Given an input query ? ?
??. . . ??, we want to find the best spelling correc-
tion ? ? ??. . . ??  among all candidate spelling 
corrections: 
?? ? argmax
?
???|?? (1) 
Applying Bayes' Rule and dropping the constant 
denominator, we have 
?? ? argmax
?
???|?????? (2) 
where the error model ???|?? models the trans-
formation probability from C to Q, and the lan-
guage model ????  models how likely C is a 
correctly spelled query. 
The speller system used in our experiments is 
based on a ranking model (or ranker), which can 
be viewed as a generalization of the source 
channel model. The system consists of two 
components: (1) a candidate generator, and (2) a 
ranker. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. Then we scan 
the query from left to right, and each query term q 
is looked up in lexicon to generate a list of spel-
ling suggestions c whose edit distance from q is 
lower than a preset threshold. The lexicon we 
used contains around 430,000 entries; these are 
high frequency query terms collected from one 
year of search query logs. The lexicon is stored 
using a trie-based data structure that allows effi-
cient search for all terms within a maximum edit 
distance. 
The set of all the generated spelling sugges-
tions is stored using a lattice data structure, which 
is a compact representation of exponentially many 
possible candidate spelling corrections. We then 
use a decoder to identify the top twenty candi-
dates from the lattice according to the source 
channel model of Equation (2).  The language 
model (the second factor) is a backoff bigram 
model trained on the tokenized form of one year 
of query logs, using maximum likelihood estima-
tion with absolute discounting smoothing.  The 
error model (the first factor) is approximated by 
the edit distance function as 
?log???|?? ? EditDist??, ?? (3) 
The decoder uses a standard two-pass algorithm 
to generate 20-best candidates. The first pass uses 
the Viterbi algorithm to find the best C according 
to the model of Equations (2) and (3).  In the 
second pass, the A-Star algorithm is used to find 
the 20-best corrections, using the Viterbi scores 
computed at each state in the first pass as heuris-
tics. Notice that we always include the input query 
Q in the 20-best candidate list. 
The core of the second component of the 
speller system is a ranker, which re-ranks the 
20-best candidate spelling corrections. If the top 
C after re-ranking is different than the original 
query Q, the system returns C as the correction.   
Let f be a feature vector extracted from a query 
and candidate spelling correction pair (Q, C). The 
ranker maps f to a real value y that indicates how 
likely C is a desired correction of Q.  For example, 
a linear ranker simply maps f to y with a learned 
weight vector w such as ? ? ? ? ?, where w is 
optimized w.r.t. accuracy on a set of hu-
269
man-labeled (Q, C) pairs. The features in f are 
arbitrary functions that map (Q, C) to a real value. 
Since we define the logarithm of the probabilities 
of the language model and the error model (i.e., 
the edit distance function) as features, the ranker 
can be viewed as a more general framework, 
subsuming the source channel model as a special 
case. In our experiments we used 96 features and a 
non-linear model, implemented as a two-layer 
neural net, though the details of the ranker and the 
features are beyond the scope of this paper. 
5 A Phrase-Based Error Model 
The goal of the phrase-based error model is to 
transform a correctly spelled query C into a 
misspelled query Q. Rather than replacing single 
words in isolation, this model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. For instance, we 
might learn that ?theme part? can be replaced by 
?theme park? with relatively high probability, 
even though ?part? is not a misspelled word. We 
assume the following generative story: first the 
correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence q1, 
?, qk, and finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S 
denote the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as 
bi-phrases. Finally, let M denote a permutation of 
K elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution over 
rewrite pairs. Let B(C, Q) denote the set of S, T, M 
triples that transform C into Q. If we assume a 
uniform probability over segmentations, then the 
phrase-based probability can be defined as: 
???|?? ? ? ???|?, ?? ? ???|?, ?, ??
??,?,???
???,??
 (4) 
As is common practice in SMT, we use the 
maximum approximation to the sum:  
???|?? ? max
??,?,???
???,??
???|?, ?? ? ???|?, ?, ?? (5) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs which act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be iden-
tified with little ambiguity. Thus we restrict our 
attention to those phrase transformations consis-
tent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1, ?, aJ be a hidden variable 
representing the word alignment. Each ai takes on 
a value ranging from 1 to L indicating its corres-
ponding word position in C, or 0 if the ith word in 
Q is unaligned. The cost of assigning k to ai is 
equal to the Levenshtein edit distance (Levensh-
tein, 1966) between the ith word in Q and the kth 
word in C, and the cost of assigning 0 to ai is equal 
to the length of the ith word in Q. We can deter-
mine the least cost alignment A* between Q and C 
efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, which 
we denote as B(C, Q, A*). Here, consistency re-
quires that if two words are aligned in A*, then 
they must appear in the same bi-phrase (ci, qi). 
Once the word alignment is fixed, the final per-
mutation is uniquely determined, so we can safely 
discard that factor. Thus we have: 
???|?? ? max
??,?,???
???,?,???
???|?, ?? (6) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative 
procedure behind the phrase-based error model. 
 
270
???|?, ?? ? ? ????|???
?
??? , (7) 
where ????|???  is a phrase transformation 
probability, the estimation of which will be de-
scribed in Section 5.2.  
To find the maximum probability assignment 
efficiently, we can use a dynamic programming 
approach, somewhat similar to the monotone 
decoding algorithm described in Och (2002). 
Here, though, both the input and the output word 
sequences are specified as the input to the algo-
rithm, as is the word alignment. We define the 
quantity ?? to be the probability of the most likely 
sequence of bi-phrases that produce the first j 
terms of Q and are consistent with the word 
alignment and C. It can be calculated using the 
following algorithm: 
1. Initialization:  
 ?? ? 1 (8) 
2. Induction:  
 ?? ? max
????,???
????
???
??
?????????? (9) 
3. Total:   
 ???|?? ? ?? (10) 
The pseudo-code of the above algorithm is 
shown in Figure 3. After generating Q from left to 
right according to Equations (8) to (10), we record 
at each possible bi-phrase boundary its maximum 
probability, and we obtain the total probability at 
the end-position of Q. Then, by back-tracking the 
most probable bi-phrase boundaries, we obtain B*.  
The algorithm takes a complexity of O(KL2), 
where K is the total number of word alignments in 
A* which does not contain empty words, and L is 
the maximum length of a bi-phrase, which is a 
hyper-parameter of the algorithm. Notice that 
when we set L=1, the phrase-based error model is 
reduced to a word-based error model which as-
sumes that words are transformed independently 
from C to Q, without taking into account any 
contextual information. 
 
5.2 Model Estimation 
We follow a method commonly used in SMT 
(Koehn et al, 2003) to extract bi-phrases and 
estimate their replacement probabilities.  From 
each query-correction pair with its word align-
ment (Q, C, A*), all bi-phrases consistent with the 
word alignment are identified. Consistency here 
implies two things. First, there must be at least 
one aligned word pair in the bi-phrase. Second, 
there must not be any word alignments from 
words inside the bi-phrase to words outside the 
bi-phrase. That is, we do not extract a phrase pair 
if there is an alignment from within the phrase 
pair to outside the phrase pair. The toy example 
shown in Figure 4 illustrates the bilingual phrases 
we can generate by this process. 
 After gathering all such bi-phrases from the 
full training data, we can estimate conditional 
relative frequency estimates without smoothing. 
For example, the phrase transformation probabil-
ity ???|?? in Equation (7) can be estimated ap-
proximately as 
Input: biPhraseLattice ?PL? with length = K & height 
= L;  
Initialization: biPhrase.maxProb = 0; 
for (x = 0; x <= K ? 1; x++) 
      for (y = 1; y <= L; y++) 
            for (yPre = 1; yPre <= L; yPre++) 
            { 
                  xPre = x ? y;  
                  biPhrasePre = PL.get(xPre, yPre); 
                  biPhrase = PL.get(x, y); 
                  if (!biPhrasePre || !biPhrase) 
                         continue; 
                  probIncrs = PL.getProbIncrease(biPhrasePre,  
                                                                      biPhrase); 
                  maxProbPre = biPhrasePre.maxProb;  
                  totalProb = probIncrs + maxProbPre; 
                  if  (totalProb > biPhrase.maxProb)  
                  { 
                        biPhrase.maxProb = totalProb;  
                        biPhrase.yPre = yPre; 
                   } 
             } 
Result: record at each bi-phrase boundary its maxi-
mum probability (biPhrase.maxProb) and optimal 
back-tracking biPhrases (biPhrase.yPre). 
 
Figure 3: The dynamic programming algorithm for 
Viterbi bi-phrase segmentation. 
 A B C D E F  a A 
a #       adc ABCD 
d    #    d D 
c   #     dc CD 
f      #  dcf CDEF 
        c C 
        f F 
 
Figure 4: Toy example of (left) a word alignment 
between two strings "adcf" and "ABCDEF"; and (right) 
the bi-phrases containing up to four words that are 
consistent with the word alignment. 
 
 
271
???|?? ?
???, ??
? ???, ?????
 (11) 
where ???, ?? is the number of times that c is 
aligned to q in training data. These estimates are 
useful for contextual lexical selection with suffi-
cient training data, but can be subject to data 
sparsity issues. 
An alternate translation probability estimate 
not subject to data sparsity issues is the so-called 
lexical weight estimate (Koehn et al, 2003). 
Assume we have a word translation distribution 
???|??  (defined over individual words, not 
phrases), and a word alignment A between q and c; 
here, the word alignment contains (i, j) pairs, 
where  ? ? 1. . |?| and ? ? 0. . |?|, with 0 indicat-
ing an inserted word.  Then we can use the fol-
lowing estimate: 
????|?, ?? ??
1
|??|??, ?? ? ??|
? ????| ???
???,????
|?|
???
 (12) 
We assume that for every position in q, there is 
either a single alignment to 0, or multiple align-
ments to non-zero positions in c. In effect, this 
computes a product of per-word translation scores; 
the per-word scores are averages of all the trans-
lations for the alignment links of that word. We 
estimate the word translation probabilities using 
counts from the word aligned corpus: ???|?? ?
???,??
? ???,?????
. Here ???, ?? is the number of times that 
the words (not phrases as in Equation 11) c and q 
are aligned in the training data. These word based 
scores of bi-phrases, though not as effective in 
contextual selection, are more robust to noise and 
sparsity. 
Throughout this section, we have approached 
this model in a noisy channel approach, finding 
probabilities of the misspelled query given the 
corrected query. However, the method can be run 
in both directions, and in practice SMT systems 
benefit from also including the direct probability 
of the corrected query given this misspelled query 
(Och, 2002). 
5.3 Phrase-Based Error Model Features 
To use the phrase-based error model for spelling 
correction, we derive five features and integrate 
them into the ranker-based query speller system, 
described in Section 4. These features are as 
follows. 
? Two phrase transformation features: 
These are the phrase transformation scores 
based on relative frequency estimates in two 
directions. In the correction-to-query direc-
tion, we define the feature as  ?????, ?, ?? ?
log ???|?? , where ???|??  is computed by 
Equations (8) to (10), and ??????? is the rel-
ative frequency estimate of Equation (11).   
? Two lexical weight features: These are the 
phrase transformation scores based on the 
lexical weighting models in two directions. 
For example, in the correction-to-query di-
rection, we define the feature 
as ?????, ?, ?? ? log ???|??, where ???|?? 
is computed by Equations (8) to (10), and the 
phrase transformation probability is the 
computed as lexical weight according to Eq-
uation (12). 
? Unaligned word penalty feature: the feature 
is defined as the ratio between the number of 
unaligned query words and the total number 
of query words. 
6 Experiments 
We evaluate the spelling error models on a large 
scale real world data set containing 24,172 queries 
sampled from one year?s worth of query logs from 
a commercial search engine. The spelling of each 
query is judged and corrected by four annotators.  
We divided the data set into training and test 
data sets. The two data sets do not overlap. The 
training data contains 8,515 query-correction 
pairs, among which 1,743 queries are misspelled 
(i.e., in these pairs, the corrections are different 
from the queries). The test data contains 15,657 
query-correction pairs, among which 2,960 que-
ries are misspelled. The average length of queries 
in the training and test data is 2.7 words.  
The speller systems we developed in this study 
are evaluated using the following three metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, i.e., a t-test 
with a significance level of 0.05. A significant 
difference should be read as significant at the 95% 
level. 
272
In our experiments, all the speller systems are 
ranker-based. In most cases, other than the base-
line system (a linear neural net), the ranker is a 
two-layer neural net with 5 hidden nodes. The free 
parameters of the neural net are trained to optim-
ize accuracy on the training data using the back 
propagation algorithm, running for 500 iterations 
with a very small learning rate (0.1) to avoid 
overfitting. We did not adjust the neural net 
structure (e.g., the number of hidden nodes) or 
any training parameters for different speller sys-
tems. Neither did we try to seek the best tradeoff 
between precision and recall. Since all the sys-
tems are optimized for accuracy, we use accuracy 
as the primary metric for comparison. 
Table 1 summarizes the main spelling correc-
tion results.  Row 1 is the baseline speller system 
where the source-channel model of Equations (2) 
and (3) is used. In our implementation, we use a 
linear ranker with only two features, derived 
respectively from the language model and the 
error model models. The error model is based on 
the edit distance function. Row 2 is the rank-
er-based spelling system that uses all 96 ranking 
features, as described in Section 4. Note that the 
system uses the features derived from two error 
models.  One is the edit distance model used for 
candidate generation. The other is a phonetic 
model that measures the edit distance between the 
metaphones (Philips, 1990) of a query word and 
its aligned correction word. Row 3 is the same 
system as Row 2, with an additional set of features 
derived from a word-based error model. This 
model is a special case of the phrase-based error 
model described in Section 5 with the maximum 
phrase length set to one.  Row 4 is the system that 
uses the additional 5 features derived from the 
phrase-based error models with a maximum 
bi-phrase length of 3. 
In phrase based error model, L is the maxi-
mum length of a bi-phrase (Figure 3). This value 
is important for the spelling performance. We 
perform experiments to study the impact of L; 
the results are displayed in Table 2. Moreover, 
since we proposed to use clickthrough data for 
spelling correction, it is interesting to study the 
impact on spelling performance from the size of 
clickthrough data used for training. We varied 
the size of clickthrough data and the experi-
mental results are presented in Table 3. 
The results show first and foremost that the 
ranker-based system significantly outperforms 
the spelling system based solely on the 
source-channel model, largely due to the richer 
set of features used (Row 1 vs. Row 2).  Second, 
the error model learned from clickthrough data 
leads to significant improvements (Rows 3 and 4 
vs. Row 2).  The phrase-based error model, due to 
its capability of capturing contextual information, 
outperforms the word-based model with a small 
but statistically significant margin (Row 4 vs. 
Row 3), though using phrases longer (L > 3) does 
not lead to further significant improvement (Rows 
6 and 7 vs. Rows 8 and 9). Finally, using more 
clickthrough data leads to significant improve-
ment (Row 13 vs. Rows 10 to 12). The benefit 
does not appear to have peaked ? further im-
provements are likely given a larger data set. 
7 Conclusions 
Unlike conventional textual documents, most 
search queries consist of a sequence of key words, 
many of which are valid search terms but are not 
stored in any compiled lexicon. This presents a 
challenge to any speller system that is based on a 
dictionary.  
This paper extends the recent research on using 
Web data and query logs for query spelling cor-
rection in two aspects. First, we show that a large 
amount of training data (i.e. query-correction 
pairs) can be extracted from clickthrough data, 
focusing on query reformulation sessions. The 
resulting data are very clean and effective for 
error model training. Second, we argue that it is 
critical to capture contextual information for 
query spelling correction. To this end, we propose 
# System Accuracy Precision Recall 
1 Source-channel 0.8526 0.7213 0.3586 
2 Ranker-based 0.8904 0.7414 0.4964 
3 Word model 0.8994 0.7709 0.5413 
4 Phrase model (L=3) 0.9043 0.7814 0.5732 
Table 1. Summary of spelling correction results. 
# System Accuracy Precision Recall 
5 Phrase model (L=1) 0.8994 0.7709 0.5413 
6 Phrase model (L=2) 0.9014 0.7795 0.5605 
7 Phrase model (L=3) 0.9043 0.7814 0.5732 
8 Phrase model (L=5) 0.9035 0.7834 0.5698 
9 Phrase model (L=8) 0.9033 0.7821 0.5713 
Table 2. Variations of spelling performance as a func-
tion of phrase length. 
 
# System Accuracy Precision Recall 
10 L=3; 0 month data 0.8904 0.7414 0.4964 
11 L=3; 0.5 month data 0.8959 0.7701 0.5234 
12 L=3; 1.5 month data 0.9023 0.7787 0.5667 
13 L=3; 3 month data 0.9043 0.7814 0.5732 
Table 3. Variations of spelling performance as a func-
tion of the size of clickthrough data used for training. 
 
 
273
a new phrase-based error model, which leads to 
significant improvement in our spelling correc-
tion experiments.  
There is additional potentially useful informa-
tion that can be exploited in this type of model. 
For example, in future work we plan to investigate 
the combination of the clickthrough data collected 
from a Web browser with the noisy but large 
query sessions collected from a commercial 
search engine. 
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Galen Andrew for the 
very helpful discussions and collaboration. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Im-
proving web search ranking by incorporating user 
behavior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a 
spelling error model from search query logs. In 
HLT-EMNLP, pp 955-962. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In 
ACL, pp. 286-293. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving 
query spelling correction using web search results. 
In EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compress-
ing trigram language models with Golomb cod-
ing. In EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction 
as an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web 
search ranking. In SIGIR.  
Golding, A. R., and Roth, D. 1996. Applying win-
now to context-sensitive spelling correction. In 
ICML, pp. 182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 
127-133. 
Kukich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Sur-
veys. 24(4): 377-439. 
Levenshtein, V. I. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. 
Exploring distributional similarity based models 
for query spelling correction. In ACL, pp. 
1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule ac-
quisition for spelling correction. In ICML, pp. 
187-194. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Okazaki, N., Tsuruoka, Y., Ananiadou, S., and 
Tsujii, J. 2008. A discriminative candidate gene-
rator for string transformations. In EMNLP, pp. 
447-456. 
Philips, L. 1990. Hanging on the metaphone. 
Computer Language Magazine, 7(12):38-44. 
Suzuki, H., Li, X., and Gao, J. 2009. Discovery of 
term variation in Japanese web search queries. In 
EMNLP. 
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In 
ACL, pp. 144-151. 
Wang, X., and Zhai, C. 2008. Mining term associa-
tion patterns from search logs for effective query 
reformulation. In CIKM, pp. 479-488. 
Whitelaw, C., Hutchinson, B., Chung, G. Y., and 
Ellis, G. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In 
EMNLP, pp. 890-899.  
274
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?262,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast Online Training with Frequency-Adaptive Learning Rates for Chinese
Word Segmentation and New Word Detection
Xu Sun?, Houfeng Wang?, Wenjie Li?
?Department of Computing, The Hong Kong Polytechnic University
?Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, China
{csxsun, cswjli}@comp.polyu.edu.hk wanghf@pku.edu.cn
Abstract
We present a joint model for Chinese
word segmentation and new word detection.
We present high dimensional new features,
including word-based features and enriched
edge (label-transition) features, for the joint
modeling. As we know, training a word
segmentation system on large-scale datasets
is already costly. In our case, adding high
dimensional new features will further slow
down the training speed. To solve this
problem, we propose a new training method,
adaptive online gradient descent based on
feature frequency information, for very fast
online training of the parameters, even given
large-scale datasets with high dimensional
features. Compared with existing training
methods, our training method is an order
magnitude faster in terms of training time, and
can achieve equal or even higher accuracies.
The proposed fast training method is a general
purpose optimization method, and it is not
limited in the specific task discussed in this
paper.
1 Introduction
Since Chinese sentences are written as continuous
sequences of characters, segmenting a character
sequence into words is normally the first step
in the pipeline of Chinese text processing. The
major problem of Chinese word segmentation
is the ambiguity. Chinese character sequences
are normally ambiguous, and new words (out-
of-vocabulary words) are a major source of the
ambiguity. A typical category of new words
is named entities, including organization names,
person names, location names, and so on.
In this paper, we present high dimensional
new features, including word-based features and
enriched edge (label-transition) features, for the
joint modeling of Chinese word segmentation
(CWS) and new word detection (NWD). While most
of the state-of-the-art CWS systems used semi-
Markov conditional random fields or latent variable
conditional random fields, we simply use a single
first-order conditional random fields (CRFs) for
the joint modeling. The semi-Markov CRFs and
latent variable CRFs relax the Markov assumption
of CRFs to express more complicated dependencies,
and therefore to achieve higher disambiguation
power. Alternatively, our plan is not to relax
Markov assumption of CRFs, but to exploit more
complicated dependencies via using refined high-
dimensional features. The advantage of our choice
is the simplicity of our model. As a result, our
CWS model can be more efficient compared with
the heavier systems, and with similar or even higher
accuracy because of using refined features.
As we know, training a word segmentation system
on large-scale datasets is already costly. In our
case, adding high dimensional new features will
further slow down the training speed. To solve this
challenging problem, we propose a new training
method, adaptive online gradient descent based on
feature frequency information (ADF), for very fast
word segmentation with new word detection, even
given large-scale datasets with high dimensional
features. In the proposed training method, we try
to use more refined learning rates. Instead of using
a single learning rate (a scalar) for all weights,
we extend the learning rate scalar to a learning
rate vector based on feature frequency information
in the updating. By doing so, each weight has
253
its own learning rate adapted on feature frequency
information. We will show that this can significantly
improve the convergence speed of online learning.
We approximate the learning rate vector based
on feature frequency information in the updating
process. Our proposal is based on the intuition
that a feature with higher frequency in the training
process should be with a learning rate that is decayed
faster. Based on this intuition, we will show the
formalized training algorithm later. We will show in
experiments that our solution is an order magnitude
faster compared with exiting learning methods, and
can achieve equal or even higher accuracies.
The contribution of this work is as follows:
? We propose a general purpose fast online
training method, ADF. The proposed training
method requires only a few passes to complete
the training.
? We propose a joint model for Chinese word
segmentation and new word detection.
? Compared with prior work, our system
achieves better accuracies on both word
segmentation and new word detection.
2 Related Work
First, we review related work on word segmentation
and new word detection. Then, we review popular
online training methods, in particular stochastic
gradient descent (SGD).
2.1 Word Segmentation and New Word
Detection
Conventional approaches to Chinese word
segmentation treat the problem as a sequential
labeling task (Xue, 2003; Peng et al, 2004; Tseng
et al, 2005; Asahara et al, 2005; Zhao et al,
2010). To achieve high accuracy, most of the state-
of-the-art systems are heavy probabilistic systems
using semi-Markov assumptions or latent variables
(Andrew, 2006; Sun et al, 2009b). For example,
one of the state-of-the-art CWS system is the latent
variable conditional random field (Sun et al, 2008;
Sun and Tsujii, 2009) system presented in Sun et al
(2009b). It is a heavy probabilistic model and it is
slow in training. A few other state-of-the-art CWS
systems are using semi-Markov perceptron methods
or voting systems based on multiple semi-Markov
perceptron segmenters (Zhang and Clark, 2007;
Sun, 2010). Those semi-Markov perceptron systems
are moderately faster than the heavy probabilistic
systems using semi-Markov conditional random
fields or latent variable conditional random fields.
However, a disadvantage of the perceptron style
systems is that they can not provide probabilistic
information.
On the other hand, new word detection is also one
of the important problems in Chinese information
processing. Many statistical approaches have been
proposed (J. Nie and Jin, 1995; Chen and Bai, 1998;
Wu and Jiang, 2000; Peng et al, 2004; Chen and
Ma, 2002; Zhou, 2005; Goh et al, 2003; Fu and
Luke, 2004; Wu et al, 2011). New word detection
is normally considered as a separate process from
segmentation. There were studies trying to solve this
problem jointly with CWS. However, the current
studies are limited. Integrating the two tasks would
benefit both segmentation and new word detection.
Our method provides a convenient framework for
doing this. Our new word detection is not a stand-
alone process, but an integral part of segmentation.
2.2 Online Training
The most representative online training method
is the SGD method. The SGD uses a small
randomly-selected subset of the training samples to
approximate the gradient of an objective function.
The number of training samples used for this
approximation is called the batch size. By using a
smaller batch size, one can update the parameters
more frequently and speed up the convergence. The
extreme case is a batch size of 1, and it gives the
maximum frequency of updates, which we adopt in
this work. Then, the model parameters are updated
in such a way:
wt+1 = wt + ?t?wtLstoch(z i,wt), (1)
where t is the update counter, ?t is the learning rate,
and Lstoch(z i,wt) is the stochastic loss function
based on a training sample z i.
There were accelerated versions of SGD,
including stochastic meta descent (Vishwanathan
et al, 2006) and periodic step-size adaptation
online learning (Hsu et al, 2009). Compared with
those two methods, our proposal is fundamentally
254
different. Those two methods are using 2nd-order
gradient (Hessian) information for accelerated
training, while our accelerated training method
does not need such 2nd-order gradient information,
which is costly and complicated. Our ADF training
method is based on feature frequency adaptation,
and there is no prior work on using feature frequency
information for accelerating online training.
Other online training methods includes averaged
SGD with feedback (Sun et al, 2010; Sun et al,
2011), latent variable perceptron training (Sun et al,
2009a), and so on. Those methods are less related to
this paper.
3 System Architecture
3.1 A Joint Model Based on CRFs
First, we briefly review CRFs. CRFs are proposed
as a method for structured classification by solving
?the label bias problem? (Lafferty et al, 2001).
Assuming a feature function that maps a pair of
observation sequence x and label sequence y to a
global feature vector f , the probability of a label
sequence y conditioned on the observation sequence
x is modeled as follows (Lafferty et al, 2001):
P (y|x,w) =
exp
{
w?f (y,x)
}
?
?y? exp
{
w?f (y? ,x)
} , (2)
wherew is a parameter vector.
Given a training set consisting of n labeled
sequences, z i = (xi, y i), for i = 1 . . . n, parameter
estimation is performed by maximizing the objective
function,
L(w) =
n
?
i=1
logP (y i|xi,w)?R(w). (3)
The first term of this equation represents a
conditional log-likelihood of a training data. The
second term is a regularizer for reducing overfitting.
We employed an L2 prior, R(w) = ||w||
2
2?2 . In what
follows, we denote the conditional log-likelihood of
each sample logP (y i|xi,w) as ?(z i,w). The final
objective function is as follows:
L(w) =
n
?
i=1
?(z i,w)?
||w||2
2?2 . (4)
Since no word list can be complete, new word
identification is an important task in Chinese NLP.
New words in input text are often incorrectly
segmented into single-character or other very short
words (Chen and Bai, 1998). This phenomenon
will also undermine the performance of Chinese
word segmentation. We consider here new word
detection as an integral part of segmentation,
aiming to improve both segmentation and new word
detection: detected new words are added to the
word list lexicon in order to improve segmentation.
Based on our CRF word segmentation system,
we can compute a probability for each segment.
When we find some word segments are of reliable
probabilities yet they are not in the existing word
list, we then treat those ?confident? word segments
as new words and add them into the existing word
list. Based on preliminary experiments, we treat
a word segment as a new word if its probability
is larger than 0.5. Newly detected words are re-
incorporated into word segmentation for improving
segmentation accuracies.
3.2 New Features
Here, we will describe high dimensional new
features for the system.
3.2.1 Word-based Features
There are two ideas in deriving the refined
features. The first idea is to exploit word features
for node features of CRFs. Note that, although our
model is a Markov CRFmodel, we can still use word
features to learn word information in the training
data. To derive word features, first of all, our system
automatically collect a list of word unigrams and
bigrams from the training data. To avoid overfitting,
we only collect the word unigrams and bigrams
whose frequency is larger than 2 in the training set.
This list of word unigrams and bigrams are then used
as a unigram-dictionary and a bigram-dictionary to
generate word-based unigram and bigram features.
The word-based features are indicator functions that
fire when the local character sequence matches a
word unigram or bigram occurred in the training
data. The word-based feature templates derived for
the label yi are as follows:
? unigram1(x, yi) ? [xj,i, yi], if the
character sequence xj,i matches a word w ? U,
255
with the constraint i ? 6 < j < i. The item
xj,i represents the character sequence xj . . . xi.
U represents the unigram-dictionary collected
from the training data.
? unigram2(x, yi) ? [xi,k, yi], if the
character sequence xi,k matches a wordw ? U,
with the constraint i < k < i + 6.
? bigram1(x, yi) ? [xj,i?1, xi,k, yi], if
the word bigram candidate [xj,i?1, xi,k] hits
a word bigram [wi, wj ] ? B, and satisfies
the aforementioned constraints on j and k. B
represents the word bigram dictionary collected
from the training data.
? bigram2(x, yi) ? [xj,i, xi+1,k, yi], if
the word bigram candidate [xj,i, xi+1,k] hits a
word bigram [wi, wj ] ? B, and satisfies the
aforementioned constraints on j and k.
We also employ the traditional character-based
features. For each label yi, we use the feature
templates as follows:
? Character unigrams locating at positions i? 2,
i? 1, i, i + 1 and i + 2
? Character bigrams locating at positions i ?
2, i? 1, i and i + 1
? Whether xj and xj+1 are identical, for j = i?
2, . . . , i + 1
? Whether xj and xj+2 are identical, for j = i?
3, . . . , i + 1
The latter two feature templates are designed
to detect character or word reduplication, a
morphological phenomenon that can influence word
segmentation in Chinese.
3.2.2 High Dimensional Edge Features
The node features discussed above are based on
a single label yi. CRFs also have edge features
that are based on label transitions. The second idea
is to incorporate local observation information of
x in edge features. For traditional implementation
of CRF systems (e.g., the HCRF package), usually
the edges features contain only the information
of yi?1 and yi, and without the information of
the observation sequence (i.e., x). The major
reason for this simple realization of edge features
in traditional CRF implementation is for reducing
the dimension of features. Otherwise, there can
be an explosion of edge features in some tasks.
For example, in part-of-speech tagging tasks, there
can be more than 40 labels and more than 1,600
types of label transitions. Therefore, incorporating
local observation information into the edge feature
will result in an explosion of edge features, which
is 1,600 times larger than the number of feature
templates.
Fortunately, for our task, the label set is quite
small, Y = {B,I,E}1. There are only nine possible
label transitions: T = Y ? Y and |T| = 9.2 As
a result, the feature dimension will have nine times
increase over the feature templates, if we incorporate
local observation information of x into the edge
features. In this way, we can effectively combine
observation information of x with label transitions
yi?1yi. We simply used the same templates of
node features for deriving the new edge features.
We found adding new edge features significantly
improves the disambiguation power of our model.
4 Adaptive Online Gradient Descent based
on Feature Frequency Information
As we will show in experiments, the training of the
CRF model with high-dimensional new features is
quite expensive, and the existing training method is
not good enough. To solve this issue, we propose a
fast online training method: adaptive online gradient
descent based on feature frequency information
(ADF). The proposed method is easy to implement.
For high convergence speed of online learning, we
try to use more refined learning rates than the SGD
training. Instead of using a single learning rate (a
scalar) for all weights, we extend the learning rate
scalar to a learning rate vector, which has the same
dimension of the weight vector w. The learning
rate vector is automatically adapted based on feature
frequency information. By doing so, each weight
1B means beginning of a word, I means inside a word, and
E means end of a word. The B,I,E labels have been widely
used in previous work of Chinese word segmentation (Sun et
al., 2009b).
2The operator ? means a Cartesian product between two
sets.
256
ADF learning algorithm
1: procedure ADF(q, c, ?, ?)
2: w ? 0, t? 0, v ? 0, ? ? c
3: repeat until convergence
4: . Draw a sample z i at random
5: . v ? UPDATE(v , z i)
6: . if t > 0 and t mod q = 0
7: . . ? ? UPDATE(? , v)
8: . . v ? 0
9: . g ? ?wLstoch(z i,w)
10: . w ? w + ? ? g
11: . t? t + 1
12: returnw
13:
14: procedure UPDATE(v , z i)
15: for k ? features used in sample z i
16: . vk ? vk + 1
17: return v
18:
19: procedure UPDATE(? , v)
20: for k ? all features
21: . u? vk/q
22: . ? ? ?? u(?? ?)
23: . ?k ? ??k
24: return ?
Figure 1: The proposed ADF online learning algorithm.
q, c, ?, and ? are hyper-parameters. q is an integer
representing window size. c is for initializing the learning
rates. ? and ? are the upper and lower bounds of a scalar,
with 0 < ? < ? < 1.
has its own learning rate, and we will show that this
can significantly improve the convergence speed of
online learning.
In our proposed online learning method, the
update formula is as follows:
wt+1 = wt + ? t ? gt. (5)
The update term gt is the gradient term of a
randomly sampled instance:
gt = ?wtLstoch(z i,wt) = ?wt
{
?(z i,wt)?
||wt||2
2n?2
}
.
In addition, ? t ? Rf+ is a positive vector-
valued learning rate and ? denotes component-wise
(Hadamard) product of two vectors.
We learn the learning rate vector ? t based
on feature frequency information in the updating
process. Our proposal is based on the intuition that a
feature with higher frequency in the training process
should be with a learning rate that decays faster. In
other words, we assume a high frequency feature
observed in the training process should have a small
learning rate, and a low frequency feature should
have a relatively larger learning rate in the training.
Our assumption is based on the intuition that a
weight with higher frequency is more adequately
trained, hence smaller learning rate is preferable for
fast convergence.
Given a window size q (number of samples in a
window), we use a vector v to record the feature
frequency. The k?th entry vk corresponds to the
frequency of the feature k in this window. Given
a feature k, we use u to record the normalized
frequency:
u = vk/q.
For each feature, an adaptation factor ? is calculated
based on the normalized frequency information, as
follows:
? = ?? u(?? ?),
where ? and ? are the upper and lower bounds of
a scalar, with 0 < ? < ? < 1. As we can see,
a feature with higher frequency corresponds to a
smaller scalar via linear approximation. Finally, the
learning rate is updated as follows:
?k ? ??k.
With this setting, different features will correspond
to different adaptation factors based on feature
frequency information. Our ADF algorithm is
summarized in Figure 1.
The ADF training method is efficient, because
the additional computation (compared with SGD) is
only the derivation of the learning rates, which is
simple and efficient. As we know, the regularization
of SGD can perform efficiently via the optimization
based on sparse features (Shalev-Shwartz et al,
2007). Similarly, the derivation of ? t can also
perform efficiently via the optimization based on
sparse features.
4.1 Convergence Analysis
Prior work on convergence analysis of existing
online learning algorithms (Murata, 1998; Hsu et
257
Data Method Passes Train-Time (sec) NWD Rec Pre Rec CWS F-score
MSR Baseline 50 4.7e3 72.6 96.3 95.9 96.1
+ New features 50 1.2e4 75.3 97.2 97.0 97.1
+ New word detection 50 1.2e4 78.2 97.5 96.9 97.2
+ ADF training 10 2.3e3 77.5 97.6 97.2 97.4
CU Baseline 50 2.9e3 68.5 94.0 93.9 93.9
+ New features 50 7.5e3 68.0 94.4 94.5 94.4
+ New word detection 50 7.5e3 68.8 94.8 94.5 94.7
+ ADF training 10 1.5e3 68.8 94.8 94.7 94.8
PKU Baseline 50 2.2e3 77.2 95.0 94.0 94.5
+ New features 50 5.2e3 78.4 95.5 94.9 95.2
+ New word detection 50 5.2e3 79.1 95.8 94.9 95.3
+ ADF training 10 1.2e3 78.4 95.8 94.9 95.4
Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge
features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is
decided by empirical convergence of the training methods.
#W.T. #Word #C.T. #Char
MSR 8.8? 104 2.4? 106 5? 103 4.1? 106
CU 6.9? 104 1.5? 106 5? 103 2.4? 106
PKU 5.5? 104 1.1? 106 5? 103 1.8? 106
Table 1: Details of the datasets. W.T. represents word
types; C.T. represents character types.
al., 2009) can be extended to the proposed ADF
training method. We can show that the proposed
ADF learning algorithm has reasonable convergence
properties.
When we have the smallest learning rate ? t+1 =
?? t, the expectation of the obtainedwt is
E(wt) = w? +
t
?
m=1
(I ? ?0?mH (w?))(w0 ?w?),
wherew? is the optimal weight vector, andH is the
Hessian matrix of the objective function. The rate of
convergence is governed by the largest eigenvalue of
the functionC t =
?t
m=1(I ? ?0?mH (w?)). Then,
we can derive a bound of rate of convergence.
Theorem 1 Assume ? is the largest eigenvalue of
the function C t =
?t
m=1(I ? ?0?mH (w?)). For
the proposed ADF training, its convergence rate is
bounded by ?, and we have
? ? exp
{?0??
? ? 1
}
,
where ? is the minimum eigenvalue ofH (w?).
5 Experiments
5.1 Data and Metrics
We used benchmark datasets provided by the second
International Chinese Word Segmentation Bakeoff
to test our proposals. The datasets are from
Microsoft Research Asia (MSR), City University
of Hongkong (CU), and Peking University (PKU).
Details of the corpora are listed in Table 1. We
did not use any extra resources such as common
surnames, parts-of-speech, and semantics.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the
decoder), precision (P , the percentage of words in
the decoder output that are segmented correctly),
balanced F-score defined by 2PR/(P + R), and
recall of new word detection (NWD recall). For
more detailed information on the corpora, refer to
Emerson (2005).
5.2 Features, Training, and Tuning
We employed the feature templates defined in
Section 3.2. The feature sets are huge. There are
2.4 ? 107 features for the MSR data, 4.1 ? 107
features for the CU data, and 4.7 ? 107 features for
the PKU data. To generate word-based features, we
extracted high-frequency word-based unigram and
bigram lists from the training data.
As for training, we performed gradient descent
258
0 10 20 30 40 5095
95.5
96
96.5
97
97.5
MSR
Number of Passes
F?
sco
re
 
 
ADF
SGD
LBFGS (batch)
0 10 20 30 40 5092
92.5
93
93.5
94
94.5
95
CU
Number of Passes
F?
sco
re
0 10 20 30 40 5094
94.5
95
95.5
PKU
Number of Passes
F?
sco
re
0 2000 4000 600095
95.5
96
96.5
97
97.5
MSR
Training time (sec)
F?
sco
re
 
 
ADF
SGD
LBFGS (batch)
0 1000 2000 3000 400092
92.5
93
93.5
94
94.5
95
CU
Training time (sec)
F?
sco
re
0 1000 2000 3000 400094
94.5
95
95.5
PKU
Training time (sec)
F?
sco
re
Figure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods.
with our proposed training method. To compare
with existing methods, we chose two popular
training methods, a batch training one and an
online training one. The batch training method
is the Limited-Memory BFGS (LBFGS) method
(Nocedal and Wright, 1999). The online baseline
training method is the SGD method, which we have
introduced in Section 2.2.
For the ADF training method, we need to tune the
hyper-parameters q, c, ?, and ?. Based on automatic
tuning within the training data (validation in the
training data), we found it is proper to set q = n/10
(n is the number of training samples), c = 0.1,
? = 0.995, and ? = 0.6. To reduce overfitting,
we employed an L2 Gaussian weight prior (Chen
and Rosenfeld, 1999) for all training methods. We
varied the ? with different values (e.g., 1.0, 2.0, and
5.0), and finally set the value to 1.0 for all training
methods.
5.3 Results and Discussion
First, we performed incremental evaluation in this
order: Baseline (word segmentation model with
SGD training); Baseline + New features; Baseline
+ New features + New word detection; Baseline +
New features + New word detection + ADF training
(replacing SGD training). The results are shown in
Table 2.
As we can see, the new features improved
performance on both word segmentation and new
word detection. However, we also noticed that
the training cost became more expensive via
adding high dimensional new features. Adding
new word detection function further improved the
segmentation quality and the new word recognition
recall. Finally, by using the ADF training method,
the training speed is much faster than the SGD
training method. The ADF method can achieve
empirical optimum in only a few passes, yet
with better segmentation accuracies than the SGD
training with 50 passes.
To get more details of the proposed training
method, we compared it with SGD and LBFGS
training methods based on an identical platform,
by varying the number of passes. The comparison
was based on the same platform: Baseline + New
features + New word detection. The F-score curves
of the training methods are shown in Figure 2.
Impressively, the ADF training method reached
empirical convergence in only a few passes, while
the SGD and LBFGS training converged much
slower, requiring more than 50 passes. The ADF
training is about an order magnitude faster than
the SGD online training and more than an order
magnitude faster than the LBFGS batch training.
Finally, we compared our method with the state-
259
Data Method Prob. Pre Rec F-score
MSR Best05 (Tseng et al, 2005)
?
96.2 96.6 96.4
CRF + rule-system (Zhang et al, 2006)
?
97.2 96.9 97.1
Semi-Markov perceptron (Zhang and Clark, 2007) ? N/A N/A 97.2
Semi-Markov CRF (Gao et al, 2007)
?
N/A N/A 97.2
Latent-variable CRF (Sun et al, 2009b)
?
97.3 97.3 97.3
Our method (A Single CRF)
?
97.6 97.2 97.4
CU Best05 (Tseng et al, 2005)
?
94.1 94.6 94.3
CRF + rule-system (Zhang et al, 2006)
?
95.2 94.9 95.1
Semi-perceptron (Zhang and Clark, 2007) ? N/A N/A 95.1
Latent-variable CRF (Sun et al, 2009b)
?
94.7 94.4 94.6
Our method (A Single CRF)
?
94.8 94.7 94.8
PKU Best05 (Chen et al, 2005) N/A 95.3 94.6 95.0
CRF + rule-system (Zhang et al, 2006)
?
94.7 95.5 95.1
semi-perceptron (Zhang and Clark, 2007) ? N/A N/A 94.5
Latent-variable CRF (Sun et al, 2009b)
?
95.6 94.8 95.2
Our method (A Single CRF)
?
95.8 94.9 95.4
Table 3: Comparing our method with the state-of-the-art CWS systems.
of-the-art systems reported in the previous papers.
The statistics are listed in Table 3. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; CRF + rule-system represents confidence-
based combination of CRF and rule-based models,
presented in Zhang et al (2006). Prob. indicates
whether or not the system can provide probabilistic
information. As we can see, our method achieved
similar or even higher F-scores, compared with the
best systems reported in previous papers. Note that,
our system is a single Markov model, while most of
the state-of-the-art systems are complicated heavy
systems, with model-combinations (e.g., voting of
multiple segmenters), semi-Markov relaxations, or
latent-variables.
6 Conclusions and Future Work
In this paper, we presented a joint model for
Chinese word segmentation and newword detection.
We presented new features, including word-based
features and enriched edge features, for the joint
modeling. We showed that the new features can
improve the performance on the two tasks.
On the other hand, the training of the model,
especially with high-dimensional new features,
became quite expensive. To solve this problem,
we proposed a new training method, ADF training,
for very fast training of CRFs, even given large-
scale datasets with high dimensional features. We
performed experiments and showed that our new
training method is an order magnitude faster than
existing optimization methods. Our final system can
learn highly accurate models with only a few passes
in training. The proposed fast learning method
is a general algorithm that is not limited in this
specific task. As future work, we plan to apply
this fast learning method on other large-scale natural
language processing tasks.
Acknowledgments
We thank Yaozhong Zhang and Weiwei Sun
for helpful discussions on word segmentation
techniques. The work described in this paper was
supported by a Hong Kong RGC Project (No. PolyU
5230/08E), National High Technology Research and
Development Program of China (863 Program) (No.
2012AA011101), and National Natural Science
Foundation of China (No.91024009, No.60973053).
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
260
In Proceedings of EMNLP?06, pages 465?472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of
machine learning methods for optimum chinese word
segmentation. In Proceedings of The Fourth SIGHAN
Workshop, pages 134?137.
K.J. Chen and M.H. Bai. 1998. Unknown word
detection for chinese by a corpus-based learning
method. Computational Linguistics and Chinese
Language Processing, 3(1):27?44.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word
extraction for chinese documents. In Proceedings of
COLING?02.
Stanley F. Chen and Ronald Rosenfeld. 1999. A
gaussian prior for smoothing maximum entropy
models. Technical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word
segmentation. In Proceedings of the fourth SIGHAN
workshop, pages 138?141.
Thomas Emerson. 2005. The second international
chinese word segmentation bakeoff. In Proceedings
of the fourth SIGHAN workshop, pages 123?133.
Guohong Fu and Kang-Kwong Luke. 2004. Chinese
unknown word identification using class-based lm. In
Proceedings of IJCNLP?04, volume 3248 of Lecture
Notes in Computer Science, pages 704?713. Springer.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL?07), pages 824?831.
Chooi-Ling Goh, Masayuki Asahara, and Yuji
Matsumoto. 2003. Chinese unknown word
identification using character-based tagging and
chunking. In Kotaro Funakoshi, Sandra Kbler, and
Jahna Otterbacher, editors, Proceedings of ACL
(Companion)?03, pages 197?200.
Chun-Nan Hsu, Han-Shen Huang, Yu-Ming Chang, and
Yuh-Jye Lee. 2009. Periodic step-size adaptation in
second-order gradient descent for single-pass on-line
structured learning. Machine Learning, 77(2-3):195?
224.
M. Hannan J. Nie and W. Jin. 1995. Unknown
word detection and segmentation of chinese using
statistical and heuristic knowledge. Communications
of the Chinese and Oriental Languages Information
Processing Society, 5:47C57.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proceedings of the 18th International Conference on
Machine Learning (ICML?01), pages 282?289.
Noboru Murata. 1998. A statistical study of on-line
learning. In On-line learning in neural networks,
Cambridge University Press, pages 63?92.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
Coling 2004, pages 562?568, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of ICML?07.
Xu Sun and Jun?ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm
and its efficient approximation. In Proceedings of
EACL?09, pages 772?780, Athens, Greece, March.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling latent-dynamic
in shallow parsing: A latent conditional model with
improved inference. In Proceedings of COLING?08,
pages 841?848, Manchester, UK.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun?ichi Tsujii. 2009a. Latent variable perceptron
algorithm for structured classification. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence (IJCAI 2009), pages 1236?1242.
Xu Sun, Yaozhong Zhang, TakuyaMatsuzaki, Yoshimasa
Tsuruoka, and Jun?ichi Tsujii. 2009b. A
discriminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of NAACL-HLT?09, pages 56?64, Boulder, Colorado,
June.
Xu Sun, Hisashi Kashima, Takuya Matsuzaki, and
Naonori Ueda. 2010. Averaged stochastic gradient
descent with feedback: An accurate, robust, and
fast training method. In Proceedings of the 10th
International Conference on Data Mining (ICDM?10),
pages 1067?1072.
Xu Sun, Hisashi Kashima, Ryota Tomioka, and Naonori
Ueda. 2011. Large scale real-life action recognition
using conditional random fields with stochastic
training. In Proceedings of the 15th Pacific-Asia
Conf. on Knowledge Discovery and Data Mining
(PAKDD?11).
Weiwei Sun. 2010. Word-based and character-
based word segmentation models: Comparison and
combination. In Chu-Ren Huang and Dan Jurafsky,
editors, COLING?10 (Posters), pages 1211?1219.
Chinese Information Processing Society of China.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A
261
conditional random field word segmenter for sighan
bakeoff 2005. In Proceedings of The Fourth SIGHAN
Workshop, pages 168?171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. In Proceedings of ICML?06, pages 969?
976.
A. Wu and Z. Jiang. 2000. Statistically-enhanced new
word identification in a rule-based chinese system.
In Proceedings of the Second Chinese Language
Processing Workshop, page 46C51, Hong Kong,
China.
Yi-Lun Wu, Chaio-Wen Hsieh, Wei-Hsuan Lin, Chun-
Yi Liu, and Liang-Chih Yu. 2011. Unknown
word extraction from multilingual code-switching
sentences (in chinese). In Proceedings of ROCLING
(Posters)?11, pages 349?360.
Nianwen Xue. 2003. Chinese word segmentation
as character tagging. International Journal of
Computational Linguistics and Chinese Language
Processing, 8(1):29?48.
Yue Zhang and Stephen Clark. 2007. Chinese
segmentation with a word-based perceptron algorithm.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 840?
847, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Companion Volume: Short Papers, pages
193?196, New York City, USA, June. Association for
Computational Linguistics.
Hai Zhao, Changning Huang, Mu Li, and Bao-Liang Lu.
2010. A unified character-based tagging framework
for chinese word segmentation. ACM Trans. Asian
Lang. Inf. Process., 9(2).
Guodong Zhou. 2005. A chunking strategy
towards unknown word detection in chinese word
segmentation. In Robert Dale, Kam-Fai Wong,
Jian Su, and Oi Yee Kwong, editors, Proceedings
of IJCNLP?05, volume 3651 of Lecture Notes in
Computer Science, pages 530?541. Springer.
262
