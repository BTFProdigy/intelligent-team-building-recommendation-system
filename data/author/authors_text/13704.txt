Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 326?332,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
A Comparison between Dialog Corpora Acquired
with Real and Simulated Users
David Griol
Departamento de Informa?tica
Universidad Carlos III de Madrid
dgriol@inf.uc3m.es
Zoraida Callejas, Ramo?n Lo?pez-Co?zar
Dpto. Lenguajes y Sistemas Informa?ticos
Universidad de Granada
{zoraida, rlopezc}@ugr.es
Abstract
In this paper, we test the applicability
of a stochastic user simulation technique
to generate dialogs which are similar to
real human-machine spoken interactions.
To do so, we present a comparison be-
tween two corpora employing a compre-
hensive set of evaluation measures. The
first corpus was acquired from real inter-
actions of users with a spoken dialog sys-
tem, whereas the second was generated by
means of the simulation technique, which
decides the next user answer taking into
account the previous user turns, the last
system answer and the objective of the di-
alog.
1 Introduction
During the last decade, there has been a grow-
ing interest in learning corpus-based approaches
for the different components of spoken dialog sys-
tems (Minker, 1999), (Young, 2002), (Esteve et al,
2003), (He and Young, 2003), (Torres et al, 2005),
(Georgila et al, 2006), (Williams and Young,
2007). One of the most relevant areas of study
has been the automatic generation of dialogs be-
tween the dialog manager and an additional mod-
ule, called the user simulator, which generates au-
tomatic interactions with the dialog system.
A considerable effort is necessary to acquire
and label a corpus with the data necessary to train
good models. User simulators make it possible to
generate a large number of dialogs in a very simple
way, reducing the time and effort needed for the
evaluation of a dialog system each time the sys-
tem is modified.
The construction of user models based on sta-
tistical methods has provided interesting and well-
founded results in recent years and is currently a
growing research area. A probabilistic user model
can be trained from a corpus of human-computer
dialogs to simulate user answers. Therefore, it can
be used to learn a dialog strategy by means of its
interaction with the dialog manager. In the liter-
ature, there are several corpus-based approaches
for developing user simulators, learning optimal
management strategies, and evaluating the dialog
system (Scheffler and Young, 2001) (Pietquin and
Dutoit, 2005) (Georgila et al, 2006) (Cuaya?huitl
et al, 2006) (Lo?pez-Co?zar et al, 2006). A sum-
mary of user simulation techniques for reinforce-
ment learning of the dialog strategy can be found
in (Schatzmann et al, 2006). In this paper, we
propose a statistical approach to acquire a labeled
dialog corpus from the interaction of a user simu-
lator and a dialog manager. In our methodology,
the new user turn is selected using the probabil-
ity distribution provided by a neural network. By
means of the interaction of the dialog manager and
the user simulator, an initial dialog corpus can be
extended by increasing its variability and detect-
ing dialog situations in which the dialog manager
does not provide an appropriate answer. We pro-
pose the use of this corpus for evaluating both our
user simulation technique and our dialog system
performance.
Different studies have been carried out to com-
pare corpora acquired by means of different tech-
niques and to define the most suitable measures to
carry out this evaluation (Schatzmann et al, 2005),
(Turunen et al, 2006), (Ai et al, 2007b), (Ai and
Litman, 2006), (Ai and Litman, 2007), (Ai et al,
2007a). In this work, we have applied our dia-
log simulation technique to acquire a corpus in the
academic domain, and compared it with a corpus
recorded from real users interactions with a spo-
326
ken dialog system
The results of this comparison show that the
simulated corpus obtained is very similar to the
corpus recorded from real user interactions in
terms of number of turns, confirmations and dia-
log acts among other evaluation measures.
The rest of the paper is organized as follows.
Section 2 summarizes the main characteristics of
the UAH system. Section 3 describes our statis-
tical methodology for user simulation. Section 4
describes the set of measures used to compare the
corpus acquired with real users and the simulated
corpus. Section 5 presents the results of this eval-
uation, and finally, the conclusions are presented
in Section 6.
2 The UAH System
Universidad Al Habla (UAH - University on the
Line) is a spoken dialog system developed to pro-
vide spoken access to academic information about
our department (Callejas and Lo?pez-Co?zar, 2005)
(Callejas and Lo?pez-Co?zar, 2008). The system is
comprised of the five typical modules of current
spoken dialog systems, concerned with automatic
speech recognition (ASR), dialog management
(DM), database access (DB Access), data storage
(DB) and oral response generation (RG). In ad-
dition, we implemented a module called GAG to
automatically create ASR grammars (Callejas and
Lo?pez-Co?zar, 2007).
The dialog manager of the UAH system was im-
plemented using VoiceXML documents that are
dynamically created using PHP. This way, it can
adapt the system responses to the context of the
conversation and the dialog state, which improves
the naturalness of the interaction. For example, the
help messages provided by the system take into
account the topic that the user and the system are
addressing at a particular moment. The context is
used as well to decide the confirmation strategy to
use.
The information that the system provides can be
classified in four main groups: subjects, profes-
sors, doctoral studies and registration, as shown in
Table 1. As can be observed, the system must ask
the user for different pieces of information before
producing a response.
A set of 100 dialogs was acquired with the UAH
system by means of its interaction with real users.
The acquisition process resulted in a spontaneous
Spanish speech dialog corpus with 60 different
speakers. The total number of user turns was 422
and the recorded material has a duration of 150
minutes. Figure 1 shows an example of a dialog
extracted from this corpus. Turns tagged with an
?S? refer to system turns, and turns tagged with ?U?
refer to user turns.
S1: Welcome to the UAH system. How can I
help you?
U1: I want to know information about sub-
jects in Computer Science
S2: Which subject do you want to consult?
U2: Language Processors I
S3: What would you like to know?
U3: Can you inform me about the schedules
of Group D?
S4: The schedule of Group D is: Mondays
from 9 to 10 and Wednesdays from 9 to 11
and from 11 to 13. Anything else?
U4: Can you tell me the name of the lecturer?
S5: The name of the lecturer of the Group
D of Language Processors I is David Griol.
Anything else?
U5: No, thank you.
S6: Thank you for using the UAH system.
Figure 1: An example of a dialog acquired with
the interaction of the UAH system with real users
3 The Statistical User Simulation
Technique
In the UAH spoken dialogue system, the user sim-
ulator developed replaces the functions performed
by the ASR and the NLU modules.
The methodology that we have developed for
user simulation extends our work for developing
a statistical methodology for dialog management
(Griol et al, 2008). The user answers are gener-
ated taking into account the information provided
by the simulator throughout the history of the dia-
log, the last system turn, and the objective(s) pre-
defined for the dialog. A labeled corpus of dialogs
is used to estimate the user model. The formal de-
scription of the proposed model is as follows:
Let Ai be the output of the dialog system (the
system answer) at time i, expressed in terms of di-
alog acts. Let Ui be the semantic representation of
the user turn. We represent a dialog as a sequence
of pairs (system-turn, user-turn):
327
Category Information provided by the user (including examples) Information provided by the
system
Subject
Name Compilers Degree, lecturers, responsible
lecturer, semester, credits, web
page
Degree, in case that there are
several subjects with the same
name
Computer science
Group name and optionally type,
in case he asks for information
about a specific group
A
Theory A
Timetable, lecturer
Lecturers Any combination of name andsurnames
Zoraida
Zoraida Callejas
Ms. Callejas
Office location, contact infor-
mation (phone, fax, email),
groups and subjects, doctoral
courses
Optionally semester, in case he
asks for the tutoring hours
First semester
Second semester
Tutoring timetable
Doctoral studies Name of a doctoral program Software development Department, responsibleName of a course if he asks
for information about a specific
course
Object-oriented program-
ming
Type, credits
Registration Name of the deadline Provisional registration
confirmation
Initial time, final time, de-
scription
Table 1: Information provided by the UAH system
(A1, U1), ? ? ? , (Ai, Ui), ? ? ? , (An, Un)
where A1 is the greeting turn of the system (the
first turn of the dialog), and Un is the last user turn.
We refer to a pair (Ai, Ui) as Si, the state of the
dialog sequence at time i.
Given this representation, the objective of the
user simulator at time i is to find an appropriate
user answer Ui. This selection, which is a local
process for each time i, takes into account the se-
quence of dialog states that precede time i, the sys-
tem answer at time i, and the objective of the di-
alog O. If the most probable user answer Ui is
selected at each time i, the selection is made using
the following maximization:
U?i = argmax
Ui?U
P (Ui|S1, ? ? ? , Si?1, Ai,O)
where set U contains all the possible user answers.
As the number of possible sequences of states
is very large, we establish a partition in this space
(i.e., in the history of the dialog preceding time i).
Let URi be the user register at time i. The user
register is defined as a data structure that contains
the information provided by the user throughout
the previous history of the dialog.The partition
that we establish in this space is based on the as-
sumption that two different sequences of states are
equivalent if they lead to the same UR. After ap-
plying the above considerations and establishing
the equivalence relations in the histories of the di-
alogs, the selection of the best Ui is given by:
U?i = argmax
Ui?U
P (Ui|URi?1, Ai,O) (1)
We propose the use of a multilayer percep-
tron (MLP) to make the assignation of a user
turn. The input layer receives the current situa-
tion of the dialog, which is represented by the term
(URi?1, Ai,O) in Equation 1. The values of the
output layer can be viewed as the a posteriori prob-
ability of selecting the different user answers de-
fined for the simulator given the current situation
of the dialog. The choice of the most probable
user answer of this probability distribution leads
to Equation 1. In this case, the user simulator will
always generate the same answer for the same sit-
uation of the dialog. Since we want to provide the
user simulator with a richer variability of behav-
iors, we base our choice on the probability distri-
bution supplied by the MLP on all the feasible user
answers.
For the UAH task, the variable O is modeled
taking into account the different types of scenarios
defined for the acquisition of the original corpus
with real users (33).
The corpus acquired with real users includes in-
formation about the errors that were introduced by
328
the ASR and the NLU modules during this acqui-
sition. This information also includes confidence
measures, which are used by the DM to evaluate
the reliability of the concepts and attributes gener-
ated by the NLU module.
An error simulator module has been designed
to perform error generation. The error simulator
modifies the frames generated by the user simula-
tor once the UR is updated. In addition, the error
simulator adds a confidence score to each concept
and attribute in the frames. Experimentally, we
have detected 2.3 errors per dialog in our initial
corpus. This value can be modified to adapt the er-
ror simulator module to the operation of any ASR
and NLU modules.
A maximum number of twelve user turns per di-
alog was defined for acquiring a corpus using our
user simulator. A user request for closing the di-
alog is selected once the system has provided the
information defined in the objective(s) of the dia-
log. The dialogs that fulfill this condition before
the maximum number of turns are considered suc-
cessful. The dialog manager considers that the di-
alog is unsuccessful and decides to abort it when
the following conditions take place:
? The dialog exceeds the maximum number of
user turns.
? The answer selected by the dialog manager
corresponds with a query not required by the
user simulator.
? The database query module provides an error
warning because the user simulator has not
provided the mandatory information needed
to carry out the query.
? The answer generator provides a error warn-
ing when the selected answer involves the use
of a data not provided by the user simulator.
The semantics selected for the dialog manager
is represented through the 27 possible system an-
swers defined for the UAH task. The selection of
the possible user answers is carried out using the
semantics defined for the user in the NLU module.
A dialog corpus of 3000 successful dialogs was
acquired using the user simulation technique fol-
lowing a set of scenarios covering all the possible
types of queries regarding the information sought.
Figure 2 shows an example of a dialog cor-
responding to a scenario with two objectives (to
know timetables of a given lecturer and subject,
and information about the registration in its spe-
cific degree). The confidence scores generated by
the error simulator are shown between brackets.
Sentences in natural language, which are equiv-
alent to the labeled system and user turns, have
been incorporated to clarify the explanation.
S1: Opening
Welcome to the UAH system. How may I help
you?
U1: Lecturers [0.1]
I want to know information about lecturers.
S2: (Confirmation:Lecturers)
Do you want to know information about lec-
turers?
U2: Affirmation [0.9]
Yes.
S3: (Question:Lecturer Name)
Tell me the name of the lecturer.
U3: Lecturer Name: Ramo?n Lo?pez-Co?zar
[0.8]
Ramo?n Lo?pez-Co?zar.
S4: (Question:Lecturer Information)
What would you like to know about Mr.
Lo?pez-Co?zar?
U4: (Question:Office) [0.9]
The location of his office please.
S5: (Answer:Office) (New-Query)
{The office of Ramo?n Lo?pez-Co?zar is located
in the third floor of ...} Anything else?
U5: (Question:Registration) [0.9]
Degree: Computer Science[0.9]
How to register in Computer Science.
S6: (Answer:Registration) (New-Query)
{Registration information} Anything else?
U6: Negation [0.9] No.
S7: (Closing)
Thank you for using the UAH system.
Figure 2: An example of a dialog acquired by
means of the simulation technique
4 Evaluation of the Corpora
We used a set of measures to carry out the evalu-
ation of the acquired corpora based on prior work
in the dialog literature. (Schatzmann et al, 2005)
proposed a comprehensive set of quantitative eval-
uation measures to compare two dialog corpora.
These measures were adapted for our purpose and
can be divided into three types:
329
High-level dialog features
Average number of turns per dialog
Percentage of different dialogs
Number of repetitions of the most seen dialog
Number of turns of the most seen dialog
Number of turns of the shortest dialog
Number of turns of the longest dialog
Dialog style/cooperativeness measures
System dialog acts: Confirmation of concepts and attributes, Questions to require information, and
Answers generated after a database query.
User dialog acts: Request to the system, Provide information, Confirmation, Yes/No answers, and
Other answers.
Figure 3: Evaluation measures used to compare the acquired corpora
? High-level dialog features: These features
evaluate the duration of the dialogs, the
amount of information transmitted in the in-
dividual turns, and how active the dialog par-
ticipants are.
? Dialog style/cooperativeness measures:
These measures analyze the frequency of
the different speech acts and study, for
example, the proportion of actions which are
goal-directed vs. dialog formalities.
? Task success/efficiency measures: These are
computations of the goal achievement rates
and goal completion times.
We have defined six high-level dialog features
for the evaluation of the dialogs: the average num-
ber of turns per dialog, the percentage of differ-
ent dialogs without considering the attribute val-
ues, the number of repetitions of the most seen di-
alog, the number of turns of the most seen dialog,
the number of turns of the shortest dialog, and the
number of turns of the longest dialog. Using these
measures, we tried to evaluate the success of the
simulated dialogs as well as their efficiency and
variability with regard to the different objectives.
For dialog style features, we have defined a set
of system/user dialog acts. On the system side,
we have measured the frequency of confirmations,
questions that require information, and system an-
swers generated after a database query. We have
not taken into account the opening and closing sys-
tem turns. On the user side, we have measured the
percentage of turns in which the user carries out
a request to the system, provide information, con-
firms a concept or attribute, Yes/No answers, and
other answers not included in the previous cate-
gories.
We have not considered task success/efficiency
measures in our evaluation, since only the dialogs
that fulfill the objectives predefined in the scenar-
ios have been incorporated into our corpora. We
have considered successful dialogs those that ful-
fill the complete list of objectives defined in the
corresponding scenario. Figure 3 summarizes the
complete set of measures used in the evaluation.
5 Evaluation Results
To compare the two corpora, we have computed
the mean value for each corpus with respect to
each of the evaluation measures shown in the pre-
vious section. Then two-tailed t-tests have been
employed to compare the means across the two
corpora as described in (Ai et al, 2007a). All dif-
ferences reported as statistically significant have
p-values less than 0.05 after Bonferroni correc-
tions.
5.1 High-level Dialog Features
As stated in the previous section, the first group of
experiments covers the following statistical prop-
erties: i) Dialog length in terms of the average
number of turns per dialog, number of turns of the
shortest dialog, number of turns of the longest di-
alog, and number of turns of the most seen dialog;
ii) Number of different dialogs in each corpus in
terms of the percentage of different dialogs and the
number of repetitions of the most seen dialog; iii)
Turn length in terms of actions per turn; iv) Partic-
ipant activity as a ratio of system and user actions
per dialog.
330
Initial Corpus Simulated Corpus
Average number of user turns per dialog 4.99 3.75
Percentage of different dialogs 85.71% 77.42%
Number of repetitions of the most seen dialog 5 27
Number of turns of the most seen dialog 2 2
Number of turns of the shortest dialog 2 2
Number of turns of the longest dialog 14 12
Table 2: Results of the high-level dialog features defined for the comparison of the three corpora
Table 2 shows the results of the comparison of
the high-level dialog features. It can be observed
that all measures have similar values in both cor-
pora. The more significant difference is the aver-
age number of user turns. In the four types of sce-
narios, the dialogs acquired using the simulation
technique were shorter than the dialogs acquired
with real users. This can be explained by the fact
that there was a number of dialogs acquired with
real users in which the user asked for additional
information not included in the definition of the
corresponding scenario once the dialog objectives
had been achieved.
5.2 Dialog Style and Cooperativeness
Tables 3 and 4 respectively show the frequency of
the most dominant user and system dialog acts.
Table 3 shows the results of this comparison for
the system dialog acts. It can be observed that
there are also only slight differences between the
values obtained for both corpora. There is a higher
percentage of confirmations and questions in the
corpus acquired with real users due to its higher
average number of turns per dialog.
Table 4 shows the results of this comparison for
the user dialog acts. The most significant differ-
ence between both corpora is the percentage of
turns in which the user makes a request to the sys-
tem, which is lower in the corpus acquired with
real users. This is possibly because it is less prob-
able that simulated users provide useless informa-
tion, as it is shown in the lower percentage of the
users turns classified as Other answers.
6 Conclusions
In this paper, we have presented a comparison be-
tween two corpora acquired using two different
techniques. Firstly, we gathered an initial dialog
corpus from real user-system interactions. Sec-
ondly, we have employed a statistical user simu-
lation technique based on a classification process
to automatically obtain a corpus of simulated di-
alogs. Our results show that it is feasible to acquire
a realistic corpus by means of the simulation tech-
nique. The experimental results reported indicate
that the simulated and real interactions corpora are
very similar in terms of number of user turns, user
and system dialog style and cooperativeness, and
most frequent dialogs statistics. As future work,
we plan to employ the simulated dialogs for eval-
uation purposes and for extracting valuable infor-
mation to optimize the current dialog strategy.
References
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken
Dialogue Corpora. In Procs. of AAAI Workshop Sta-
tistical and Empirical Approaches for Spoken Dia-
logue Systems, Boston, USA.
H. Ai and D.J. Litman. 2007. Knowledge Consistent
User Simulations for Dialog Systems. In Proc. of In-
terspeech?07, pages 2697?2700, Antwerp, Belgium.
H. Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Litman.
2007a. Comparing Spoken Dialog Corpora Col-
lected with Recruited Subjects versus Real Users. In
Proc. of the SIGdial?07, pages 124?131, Antwerp,
Belgium.
H. Ai, J.R. Tetreault, and D.J. Litman. 2007b. Com-
paring User Simulation Models For Dialog Strategy
Learning. In Proc. of NAACL HLT?07, pages 1?4,
Rochester, NY, USA.
Z. Callejas and R. Lo?pez-Co?zar. 2005. Implementing
modular dialogue systems: a case study. In Proc. of
Applied Spoken Language Interaction in Distributed
Environments (ASIDE?05), Aalborg, Denmark.
Z. Callejas and R. Lo?pez-Co?zar. 2007. Automatic
creation of ASR grammar rules for unknown vo-
cabulary applications. In Proc. of the 8th Interna-
tional workshop on Electronics, Control, Modelling,
Measurement and Signals (ECMS?07), pages 50?55,
Liberec, Czech Republic.
Z. Callejas and R. Lo?pez-Co?zar. 2008. Relations be-
tween de-facto criteria in the evaluation of a spoken
331
Initial Corpus Simulated Corpus
Confirmation of concepts and attributes 13.51% 12.23%
Questions to require information 18.44% 16.57%
Answers generated after a database query 68.05% 71.20%
Table 3: Percentages of the different types of system dialog acts in both corpora
Initial Corpus Simulated Corpus
Request to the system 31.74% 35.43%
Provide information 21.72% 20.98%
Confirmation 10.81% 9.34%
Yes/No answers 33.47% 32.77%
Other answers 2.26% 1.48%
Table 4: Percentages of the different types of user dialog acts in both corpora
dialogue system. Speech Communication, 50(8?
9):646?665.
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2006. Learning Multi-Goal Dialogue
Strategies Using Reinforcement Learning with Re-
duced State-Action Spaces. In Proc. of the 9th Inter-
national Conference on Spoken Language Process-
ing (Interspeech/ICSLP), pages 469?472, Pittsburgh
(USA).
Y. Esteve, C. Raymond, F. Bechet, and R. De Mori.
2003. Conceptual Decoding for Spoken Dialog sys-
tems. In Proc. of European Conference on Speech
Communications and Technology (Eurospeech?03),
volume 1, pages 617?620, Geneva (Switzerland).
K. Georgila, J. Henderson, and O. Lemon. 2006. User
Simulation for Spoken Dialogue Systems: Learn-
ing and Evaluation. In Proc. of the 9th Interna-
tional Conference on Spoken Language Processing
(Interspeech/ICSLP), pages 1065?1068, Pittsburgh
(USA).
D. Griol, L.F. Hurtado, E. Segarra, and E. Sanchis.
2008. A Statistical Approach to Spoken Dialog Sys-
tems Design and Evaluation. Speech Communica-
tion, 50(8?9):666?682.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In Proc. of IEEE Auto-
matic Speech Recognition and Understanding Work-
shop (ASRU?03), pages 583?588, St. Thomas (U.S.
Virgin Islands).
R. Lo?pez-Co?zar, Z. Callejas, and M. McTear. 2006.
Testing the performance of spoken dialogue systems
by means of an artificially simulated user. Artificial
Intelligence Review, 26:291?323.
W. Minker. 1999. Stocastically-based semantic analy-
sis. In Kluwer Academic Publishers, Boston (USA).
O. Pietquin and T. Dutoit. 2005. A probabilistic
framework for dialog simulation and optimal strat-
egy learning. In IEEE Transactions on Speech and
Audio Processing, Special Issue on Data Mining of
Speech, Audio and Dialog, volume 14, pages 589?
599.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. of
SIGdial?05, pages 45?54, Lisbon (Portugal).
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. In Knowledge
Engineering Review, volume 21(2), pages 97?126.
K. Scheffler and S. Young. 2001. Automatic learning
of dialogue strategy using dialogue simulation and
reinforcement learning. In Proc. of HLT?02, pages
12?18, San Diego (USA).
F. Torres, L.F. Hurtado, F. Garc??a, E. Sanchis, and
E. Segarra. 2005. Error handling in a stochastic dia-
log system through confidence measures. In Speech
Communication, pages (45):211?229.
M. Turunen, J. Hakulinen, and A. Kainulainen. 2006.
Evaluation of a Spoken Dialogue System with Us-
ability Tests and Long-term Pilot Studies: Similar-
ities and Differences. In Proc. of the 9th Interna-
tional Conference on Spoken Language Processing
(Interspeech/ICSLP), pages 1057?1060, Pittsburgh,
USA.
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. In Computer Speech and Language, volume
21(2), pages 393?422.
S. Young. 2002. The Statistical Approach to the De-
sign of Spoken Dialogue Systems. Technical re-
port, CUED/F-INFENG/TR.433, Cambridge Uni-
versity Engineering Department, Cambridge (UK).
332
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 269?272,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Statistical Dialog Management Methodologies for Real Applications
David Griol
Dept. of Computer Science
Carlos III University of Madrid
Av. Universidad, 30, 28911, Legane?s
dgriol@inf.uc3m.es
Zoraida Callejas, Ramo?n Lo?pez-Co?zar
Dept. of Languages and Computer Systems, CITIC-UGR
University of Granada
C/ Pdta. Daniel Saucedo Aranda, 18071, Granada
{zoraida, rlopezc}@ugr.es
Abstract
In this paper we present a proposal for the
development of dialog systems that, on the
one hand, takes into account the benefits of
using standards like VoiceXML, whilst on
the other, includes a statistical dialog mod-
ule to avoid the effort of manually defin-
ing the dialog strategy. This module is
trained using a labeled dialog corpus, and
selects the next system response consider-
ing a classification process that takes into
account the dialog history. Thus, system
developers only need to define a set of
VoiceXML files, each including a system
prompt and the associated grammar to rec-
ognize the users responses to the prompt.
We have applied this technique to develop
a dialog system in VoiceXML that pro-
vides railway information in Spanish.
1 Introduction
When designing a spoken dialog system, develop-
ers need to specify the system actions in response
to user utterances and environmental states that,
for example, can be based on observed or inferred
events or beliefs. In addition, the dialog manager
needs a dialog strategy that defines the conversa-
tional behavior of the system. This is the funda-
mental task of dialog management (Paek and Pier-
accini, 2008), as the performance of the system is
highly dependent on the quality of this strategy.
Thus, a great effort is employed to empirically de-
sign dialog strategies for commercial systems. In
fact, the design of a good strategy is far from be-
ing a trivial task since there is no clear definition
of what constitutes a good strategy (Schatzmann
et al, 2006). Once the strategy has been designed,
the implementation of the system is leveraged by
programming languages such as VoiceXML, for
which different programming environments and
tools have been created to help developers.
As an alternative of the previously described
rule-based approaches, the application of statis-
tical approaches to dialog management makes it
possible to consider a wider space of dialog strate-
gies (Georgila et al, 2006; Williams and Young,
2007; Griol et al, 2009). The main reason is that
statistical models can be trained from real dialogs,
modeling the variability in user behaviors. The fi-
nal objective is to develop dialog systems that have
a more robust behavior and are easier to adapt to
different user profiles or tasks.
(Pieraccini et al, 2009) highlights the imprac-
ticality of applying statistical learning approaches
to develop commercial applications, in the sense
that it is difficult to consider the expert knowl-
edge of human designers. From his perspective,
a hybrid approach, combining statistical and rule-
based approaches, could be a good solution. The
reason is that statistical approaches can offer a
wider range of alternatives at each dialog state,
whereas rule based approaches may offer knowl-
edge on best practices.
For example, (Williams, 2008) proposes taking
advantage of POMDPs and rule-based approaches
by using POMDPs to foster robustness and at the
same time being able to incorporate handcrafted
constraints which cover expert knowledge in the
application domain. Also (Lee et al, 2010) have
recently proposed a different hybrid approach to
dialog modeling in which n-best recognition hy-
potheses are weighted using a mixture of expert
knowledge and data-driven measures by using an
agenda and an example-based machine translation
approach respectively. In both approaches, the hy-
brid method achieved significant improvements.
Additionally, speech recognition grammars for
commercial systems have been usually built on
the basis of handcrafted rules that are tested re-
cursively, which in complex applications is very
costly (McTear, 2004). However, as stated by
(Pieraccini et al, 2009), many sophisticated com-
269
mercial systems already available receive a large
volume of interactions. Therefore, industry is be-
coming more interested in substituting rule based
grammars with statistical approaches based on the
large amounts of data available.
As an attempt to improve the current technol-
ogy, we propose to merge statistical approaches
with VoiceXML. Our goal is to combine the flex-
ibility of statistical dialog management with the
facilities that VoiceXML offers, which would help
to introduce statistical approaches for the develop-
ment of commercial (and not strictly academic) di-
alog systems. To this end, our technique employs
a statistical dialog manager that takes into account
the history of the dialog up to the current dialog
state in order to decide the next system prompt.
In addition, the system prompts and the gram-
mars for ASR are implemented in VoiceXML-
compliant formats, for example, JSGF or SRGS.
As it is often difficult to find or gather a human-
machine corpus which cover an identical domain
as the system which is to be implemented, our ap-
proach is also based on the compilation of cor-
pora of interactions of simulated users, which is
a common practice when using machine learning
approaches for system development.
In contrast with other hybrid approaches, our
main aim is not to incorporate knowledge about
best strategies in statistical dialog management,
but rather to take advantage of an implementa-
tion language which has been traditionally used
to build rule-based systems (such as VoiceXML),
for the development of statistical dialog strate-
gies. Expert knowledge about deployment of
VoiceXML applications, development environ-
ments and tools can still be exploited using our
technique. The only change is in the transition be-
tween states, which is carried out on a data-driven
basis (i.e., is not deterministic). We have applied
our technique to develop a dialog system that pro-
vides railway information, for which we have de-
veloped a statistical dialog management technique
in a previous study.
2 Our Proposal to Introduce Statistical
Methodologies in Commercial
Applications
As stated in the introduction, our approach to inte-
grate statistical methodologies in commercial ap-
plications is based on the automatic learning of the
dialog strategy using a statistical dialog manage-
ment methodology. In most dialog systems, the
dialog manager makes decisions based only on the
information provided by the user in the previous
turns and its own dialog model. For example, this
is the case with most dialog systems for slot-filling
tasks. The methodology that we propose for the
selection of the next system response for this kind
of task is detailed in (Griol et al, 2008). It is based
on the definition of a data structure that we call
Dialog Register (DR), which contains the infor-
mation provided by the user throughout the dialog
history. In brief, it is as follows: for each time i,
the selection of the next system prompt Ai is car-
ried out by means of the following maximization:
A?i = argmax
Ai?A
P (Ai|DRi?1, Si?1)
where the set A contains all the possible system
responses and Si?1 is the state of the dialog se-
quence (system-turn, user-turn) at time i.
Each user turn supplies the system with infor-
mation about the task; that is, he/she asks for a
specific concept and/or provides specific values
for certain attributes. However, a user turn could
also provide other kinds of information, such as
task-independent information. This is the case of
turns corresponding to Affirmation, Negation and
Not-Understood dialog acts. This kind of infor-
mation implies some decisions which are different
from simply updating the DRi?1. Hence, for the
selection of the best system response Ai, we take
into account the DR that results from turn 1 to
turn i? 1, and we explicitly consider the last state
Si?1. Our model can be extended by incorporating
additional information to the DR, such as some
chronological information (e.g. number of turns
up to the current turn) or user profiles (e.g. user
experience or preferences).
The selection of the system response is car-
ried out through a classification process, for which
a multilayer perceptron (MLP) is used. The in-
put layer receives the codification of the pair
(DRi?1, Si?1). The output generated by the MLP
can be seen as the probability of selecting each of
the different system answers defined for a specific
task.
To learn the dialog model we use dialog sim-
ulation techniques. Our approach for acquiring a
dialog corpus is based on the interaction of a user
simulator and a dialog manager simulator (Griol et
al., 2007). The user simulation replaces the user
intention level, that is, it provides concepts and
270
attributes that represent the intention of the user.
This way, the user simulator carries out the func-
tions of the ASR and NLU modules. Errors and
confidence scores are simulated by a specific mod-
ule in the simulator. The acquired dialogs are em-
ployed to automatically generate VoiceXML code
for each system prompt and create the grammar
needed to recognize the possible user utterances
after each one of the system prompts.
3 Development of a railway information
system using the proposed technique
To test our proposal, we have used the defini-
tions taken to develop the DIHANA dialog system,
which was developed in a previous study to pro-
vide information about train services, schedules
and fares in Spanish (Griol et al, 2009; Griol et
al., 2008). The DR defined for the our railway in-
formation system is a sequence of 15 fields, corre-
sponding to the five concepts (Hour, Price, Train-
Type, Trip-Time, Services) and ten attributes (Ori-
gin, Destination, Departure-Date, Arrival-Date,
Departure-Hour, Arrival-Hour, Class, Train-Type,
Order-Number, Services). The system generates a
total of 51 different prompts.
Three levels of labeling are defined for the la-
beling of the system dialog acts. The first level
describes general acts which are task independent.
The second level is used to represent concepts and
attributes involved in dialog turns that are task-
dependent. The third level represents values of at-
tributes given in the turns. The following labels
are defined for the first level: Opening, Closing,
Undefined, Not-Understood, Waiting, New-Query,
Acceptance, Rejection, Question, Confirmation,
and Answer. The labels defined for the second and
third level were the following: Departure-Hour,
Arrival-Hour, Price, Train-Type, Origin, Destina-
tion, Date, Order-Number, Number-Trains, Ser-
vices, Class, Trip-Type, Trip-Time, and Nil. There
are dialog turns which are labeled with several di-
alog acts.
Having this kind of labeling and the values of
attributes obtained during a dialog, it is straightfor-
ward to construct a sentence in natural language.
Some examples of the dialog act labeling of the
system turns are shown in Figure 1.
Two million dialogs were simulated using a set
of two types of scenarios. Type S1 defines one
objective for the dialog, whereas Type S2 defines
two. Table 1 summarizes the statistics of the ac-
[SPANISH] Bienvenido al servicio de informacio?n de
trenes. ?En que? puedo ayudarle?
[ENGLISH] Welcome to the railway information sys-
tem. How can I help you?
(Opening:Nil:Nil)
[SPANISH] El u?nico tren es un Euromed que sale a las
0:27. ?Desea algo ma?s?
[ENGLISH] There is only one train, which is a Eu-
romed, that leaves at 0:27. Anything else?
(Answer:Departure-Hour:Departure-Hour:Departure-
Hour[0.27],Number-Trains[1],Train-Type[Euromed])
(New-Query:Nil:Nil)
Figure 1: Labeling examples of system turns from
the DIHANA corpus
quisition for the two types of scenarios.
Type S1 Type S2
Simulated dialogs 106 106
Successful dialogs 15,383 1,010
Different dialogs 14,921 998
Number of user turns per dialog 4.9 6.2
Table 1: Statistics of the new corpus acquisition
The 51 different system prompts have been au-
tomatically generated in VoiceXML using the pro-
posed technique. For example, Figure 2 shows the
VXML document to prompt the user for the origin
city, whereas Figure 3 shows the obtained gram-
mar for ASR.
<?xml version="1.0" encoding="UTF-8"?>
<vxml xmlns="http://www.w3.org/2001/vxml"
xmlns:xsi="http://www.w3.org/2001/
XMLSchema-instance"
xsi:schemaLocation="http://www.w3.org/2001/vxml
http://www.w3.org/TR/voicexml20/vxml.xsd"
version="2.0" application="app-dihana.vxml">
<form id="origin_form">
<field name="origin">
<grammar type="application/srgs+xml"
src="/grammars/origin.grxml"/>
<prompt>Tell me the origin city.</prompt>
<filled>
<return namelist="origin"/>
</filled>
</field>
</form>
</vxml>
Figure 2: VXML document to require the origin
city
4 Conclusions
In this paper, we have described a technique for
developing dialog systems using a well known
271
#JSGF V1.0;
grammar origin;
public <origin> = [<desire>]
[<travel> <city> {this.destination=$city}]
[<proceed> <city> {this.origin=$city}];
<desire> = I want [to know] | I would like
[to know] | I would like | I want | I need
| I have to;
<travel> = go to | travel to | to go to
| to travel to;
<city> = Jae?n | Co?rdoba | Sevilla | Huelva |
Ca?diz | Ma?laga | Granada | Almer??a |
Valencia | Alicante | Castello?n | Barcelona
| Madrid;
<proceed> = from | going from | go from;
Figure 3: Grammar defined to capture the origin
city
standard like VoiceXML, and considering a statis-
tical dialog model that is automatically learnt from
a dialog corpus.
The main objective of our work is to reduce the
gap between academic and commercial systems
by reducing the effort required to define optimal
dialog strategies and implement the system. Our
proposal works on the benefits of statistical meth-
ods for dialog management and VoiceXML, re-
spectively. The former provide an efficient means
to exploring a wider range of dialog strategies,
whereas the latter makes it possible to benefit from
the advantages of using the different tools and
platforms that are already available to simplify
system development. We have applied our tech-
nique to develop a dialog system that provides rail-
way information, and have shown that it enables
creating automatically VoiceXML documents to
prompt the user for data, as well as the necessary
grammars for ASR. As a future work, we plan to
study ways for adapting the proposed dialog man-
agement technique to more complex domains.
Additionally, we are interested in investigating
possible ways for easing the adoption of our tech-
nique in industry, and the main challenges that
might arise in using it to develop commercial sys-
tems.
Acknowledgments
This research has been funded by the Spanish
Ministry of Science and Technology, under project
HADA TIN2007-64718.
References
K. Georgila, J. Henderson, and O. Lemon. 2006. User
Simulation for Spoken Dialogue Systems: Learn-
ing and Evaluation. In Proc. of the 9th Inter-
speech/ICSLP, pages 1065?1068, Pittsburgh (USA).
D. Griol, L.F. Hurtado, E. Sanchis, and E. Segarra.
2007. Acquiring and Evaluating a Dialog Corpus
through a Dialog Simulation Technique. In Proc.
of the 8th SIGdial Workshop on Discourse and Dia-
logue, pages 39?42, Antwerp (Belgium).
D. Griol, L.F. Hurtado, E. Segarra, and E. Sanchis.
2008. A Statistical Approach to Spoken Dialog Sys-
tems Design and Evaluation. Speech Communica-
tion, 50(8?9):666?682.
D. Griol, G. Riccardi, and Emilio Sanchis. 2009. A
Statistical Dialog Manager for the LUNA project.
In Proc. of Interspeech/ICSLP?09, pages 272?275,
Brighton (UK).
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and
Gary Geunbae Lee. 2010. Hybrid approach to
robust dialog management using agenda and dia-
log examples. Computer Speech and Language,
24(4):609?631.
Michael F. McTear, 2004. Spoken Dialogue Technol-
ogy: Towards the Conversational User Interface.
Springer.
T. Paek and R. Pieraccini. 2008. Automating spoken
dialogue management design using machine learn-
ing: An industry perspective . Speech Communica-
tion, 50(8?9):716?729.
Roberto Pieraccini, David Suendermann, Krishna
Dayanidhi, and Jackson Liscombe. 2009. Are We
There Yet? Research in Commercial Spoken Dia-
log Systems. Lecture Notes in Computer Science,
5729:3?13.
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. In Knowledge
Engineering Review, volume 21(2), pages 97?126.
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. In Computer Speech and Language, volume
21(2), pages 393?422.
Jason D. Williams. 2008. The best of both
worlds: Unifying conventional dialog systems and
POMDPs. In Proceedings of the International Con-
ference on Spoken Language Processing.
272
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 281?288,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
F2 ? New Technique for Recognition of User Emotional States in    
Spoken Dialogue Systems 
Ram?n L?pez-C?zar 
Dept. of Languages and 
Computer Systems, CTIC-
UGR, University of Granada, 
Spain 
rlopezc@ugr.es 
Jan Silovsky 
Institute of Information 
Technology and Electronics, 
Technical University of 
Liberec, Czech Republic 
jan.silovsky@tul.cz 
David Griol 
Dept. of Computer Science 
Carlos III University of     
Madrid, Spain 
dgriol@inf.uc3m.es 
  
 
Abstract 
In this paper we propose a new technique to 
enhance emotion recognition by combining 
in different ways what we call emotion pre-
dictions. The technique is called F2 as the 
combination is based on a double fusion 
process. The input to the first fusion phase is 
the output of a number of classifiers which 
deal with different types of information re-
garding each sentence uttered by the user. 
The output of this process is the input to the 
second fusion stage, which provides as out-
put the most likely emotional category. Ex-
periments have been carried out using a pre-
viously-developed spoken dialogue system 
designed for the fast food domain. Results 
obtained considering three and two emo-
tional categories show that our technique 
outperforms the standard single fusion tech-
nique by 2.25% and 3.35% absolute, respec-
tively. 
1 Introduction 
Automatic recognition of user emotional states 
is a very challenging task that has attracted the 
attention of the research community for several 
decades. The goal is to design methods to 
make computers interact more naturally with 
human beings. This is a very complex task due 
to a variety of reasons. One is the absence of a 
generally agreed definition of emotion and of 
qualitatively different types of emotion. An-
other is that we still have an incomplete under-
standing of how humans process emotions, as 
even people have difficulty in distinguishing 
between them. Thus, in many cases a given 
emotion is perceived differently by different 
people. 
Studies in emotion recognition made by the 
research community have been applied to en-
hance the quality or efficiency of several ser-
vices provided by computers. For example, 
these have been applied to spoken dialogue 
systems (SDSs) used in automated call-centres, 
where the goal is to detect problems in the in-
teraction and, if appropriate, transfer the call 
automatically to a human operator. 
The remainder of the paper is organised as 
follows. Section  2 addresses related work on 
the application of emotion recognition to 
SDSs. Section  3 focuses on the proposed tech-
nique, describing the classifiers and fusion 
methods employed in the current implementa-
tion. Section  4 discusses our speech database 
and its emotional annotation. Section  5 pre-
sents the experiments, comparing results ob-
tained using the standard single fusion tech-
nique with the proposed double fusion. Finally, 
Section  6 presents the conclusions and outlines 
possibilities for future work. 
2 Related work 
Many studies can be found in the literature 
addressing potential improvements to SDSs by 
recognising user emotional states. A diversity 
of speech databases, features used for training 
and recognition, number of emotional catego-
ries, and recognition methods have been pro-
posed. For example, Batliner et al (2003) em-
ployed three different databases to detect trou-
bles in communication. One was collected 
from a single experienced actor who was told 
to express anger because of system malfunc-
tions. Other was collected from naive speakers 
who were asked to read neutral and emotional 
sentences. The third database was collected 
using a WOZ scenario designed to deliberately 
provoke user reactions to system malfunctions. 
The study focused on detecting two emotion 
categories: emotional (e.g. anger) and neutral, 
employing classifiers that dealt with prosodic, 
linguistic, and discourse information. 
281
Liscombe et al (2005) made experiments 
with a corpus of 5,690 dialogues collected with 
the ?How May I Help You? system, and con-
sidered seven emotional categories: posi-
tive/neutral, somewhat frustrated, very frus-
trated, somewhat angry, very angry, somewhat 
other negative, and very other negative. They 
employed standard lexical, prosodic and con-
textual features. 
Devillers and Vidrascu (2006) employed 
human-to-human dialogues on a financial task, 
and considered four emotional categories: an-
ger, fear, relief and sadness. Emotion classifi-
cation was carried out considering linguistic 
information and paralinguistic cues. 
Ai et al (2006) used a database collected 
from 100 dialogues between 20 students and a 
spoken dialogue tutor, and for classification 
employed lexical items, prosody, user gender, 
beginning and ending time of turns, user turns 
in the dialogue, and system/user performance 
features. Four emotional categories were con-
sidered: uncertain, certain, mixed and neutral. 
Morrison et al (2007) compared two emo-
tional speech data sources The former was col-
lected from a call-centre in which customers 
talked directly to a customer service represen-
tative. The second database was collected from 
12 non-professional actors and actresses who 
simulated six emotional categories: anger, dis-
gust, fear, happiness, sadness and surprise. 
3 The proposed technique 
The technique that we propose in this paper to 
enhance emotion recognition in SDSs consid-
ers that a set of classifiers ? = {C1, C2, ?, Cm} 
receive as input feature vectors f related to 
each sentence uttered by the user. As a result, 
each classifier generates one emotion predic-
tion, which is a vector of pairs (hi, pi), i = 1?S, 
where hi is an emotional category (e.g. Angry), 
pi is the probability of the utterance belonging 
to hi in accordance with the classifier, and S is 
the number of emotional categories considered, 
which forms the set E = {e1, e2, ?, eS}. 
The emotion predictions generated by the 
classifiers make up the input to the first fusion 
stage, which we call Fusion-0. This stage em-
ploys n fusion methods called F0i, i = 1?n, to 
generate other predictions: vectors of pairs 
(h0j,k , p0j,k), j = 1?n, k = 1?S, where h0j,k is an 
emotional category, and p0j,k is the probability 
of the utterance belonging to h0j,k in accordance 
with the fusion method F0j. 
The second fusion stage, called Fusion-1, 
receives the predictions provided by Fusion-0 
and generates the pair (h11,1 , p11,1), where h11,1 
is the emotional category with highest prob-
ability, p11,1. This emotional category is deter-
mined employing a fusion method called F11, 
and represents the user?s emotional state de-
duced by the technique. The best combination 
of fusion methods to be used in Fusion-0 (F01, 
F02,...,F0j, 1 ? j ? n) and the best fusion method 
to be used in Fusion-1 (F11) must be experi-
mentally determined.  
3.1 Classifiers 
In the current implementation our technique 
employs four classifiers, which deal with pros-
ody, acoustics, lexical items and dialogue acts 
regarding each utterance. 
3.1.1 Prosodic classifier 
The input to our prosodic classifier is an n-
dimensional feature vector obtained from 
global statistics of pitch and energy, and fea-
tures derived from the duration of 
voiced/unvoiced segments in each utterance. 
After carrying out experiments to find the ap-
propriate feature set for the classifier, we de-
cided to use the following 11 features: pitch 
mean, minimum and maximum, pitch deriva-
tives mean, mean and variance of absolute val-
ues of pitch derivatives, energy maximum, 
mean of absolute value of energy derivatives, 
correlation of pitch and energy derivatives, 
average length of voiced segments, and dura-
tion of longest monotonous segment. 
The classifier employs gender-dependent 
Gaussian Mixture Models (GMMs) to repre-
sent emotional categories. The likelihood for 
the n-dimensional feature vector (x), given an 
emotional category ?, is defined as: 
 
( ) ( )?
=
=
Q
l
ll xPwxP
1
?  
 
i.e., a weighted linear combination of Q uni-
modal Gaussian densities Pl(x). The density 
function Pl(x) is defined as: 
 
( ) ( ) ( ) ( )??
???
? ?????
?
= ? lll
l
n
l xxxP ???
1
2
1
exp
det2
1  
 
where the ?l?s are mean vectors and the ?l?s 
covariance matrices. The emotional category 
282
deduced by the classifier, h, is decided accord-
ing to the following expression: 
 ( )S
S
xPh ?maxarg=  (1) 
 
where ?S represents the models for the emo-
tional categories considered, and the max func-
tion is computed employing the EM (Expecta-
tion-Maximization) algorithm. To compute the 
probabilities pi for the emotion prediction of 
the classifier we use the following expression: 
 
?
=
=
S
k
kiip
1
/ ??  (2) 
 
where ?i is the log-likelihood of hi, S is the 
number of emotional categories considered, 
and the ?k?s are the log-likelihoods of these 
emotional categories. 
3.1.2 Acoustic classifier 
Prosodic features are nowadays among the 
most popular features for emotion recognition 
(Dellaert et al 1996; Luengo et al 2005). 
However, several authors have evaluated other 
features. For example, Nwe et al (2003) em-
ployed several short-term spectral features and 
observed that Logarithmic Frequency Power 
Coefficients (LFPCs) provide better perform-
ance than Mel-Frequency Cepstral Coefficient 
(MFCCs) or Linear Prediction Cepstral Coeffi-
cients (LPCCs). Experiments carried out with 
our speech database (which will be discussed 
in Section  4) have confirmed this observation. 
However, we have also noted that when we 
used the first and second derivatives, the best 
results were obtained for MFCCs. Hence, we 
decided to use 39-feature MFCCs (13 MFCCs, 
delta and delta-delta) for classification. 
The emotion patterns of the input utterances 
are modelled by gender-dependent GMMs, as 
with the prosodic classifier, but each input ut-
terance is represented employing a sequence of 
feature vectors x = {x1,?,xT} instead of one n-
dimensional vector. We assume mutual inde-
pendence of the feature vectors in x, and com-
pute the log-likelihood for an emotional cate-
gory ? as follows: 
 
( ) ( )?
=
=
T
t
txPxP
1
log ??  
 
The emotional category deduced by the classi-
fier, h, is decided employing Eq. (1), whereas 
Eq. (2) is used to compute the probabilities for 
the prediction, i.e. for the vector of pairs (hi, 
pi). 
3.1.3 Lexical classifier 
A number of previous studies on emotion rec-
ognition take into account information about 
the kinds of word uttered by the users, assum-
ing that there is a relationship between words 
and emotion categories. For example, swear 
words and insults can be considered as convey-
ing a negative emotion (Lee and Narayanan, 
2005). Analysis of our dialogue corpus (which 
will be discussed in Section  4) has shown that 
users did not utter swear words or insults dur-
ing the interaction with the Saplen system. 
Nevertheless, there were particular moments in 
the interaction at which their emotional state 
changed from Neutral to Tired or Angry. These 
moments correspond to dialogue states where 
the system had problems in recognising the 
sentences uttered by the users. 
The reasons for these problems are basically 
two. On the one hand, most users spoke with 
strong southern Spanish accents, characterised 
by the deletion of the final s of plural words, 
and an exchange of the phonemes s and c in 
many words. On the other hand, there are 
words in the system?s vocabulary that are very 
similar acoustically. 
Hence, our goal has been to automatically 
find these words by means of a study of the 
speech recognition results, and deduce the 
emotional category for each input utterance 
from the emotional information associated 
with the words in the recognition result. To do 
this we have followed the study of Lee and 
Narayanan (2005), which employs the infor-
mation-theoretic concept of emotional sali-
ence. The emotional salience of a word for a 
given emotional category can be defined as the 
mutual information between the word and the 
emotional category. Let W be a sentence 
(speech recognition result) comprised of a se-
quence of n words: W = w1 w2 ?wn, and E a 
set of emotional categories, E = {e1, e2, ? ,eS}. 
The mutual information between the word wi 
and an emotional category ej is defined as fol-
lows: 
 ( )
)(
|
log),(_
j
ij
ji eP
weP
ewnInformatiomutual =  
283
 
where P(ej | wi) is the posterior probability that 
a sentence containing the word wi implies the 
emotional category ej, and P(ej) represents the 
prior probability of the emotional category. 
Taking into account the previous definitions, 
we have defined the emotional salience of the 
word wi for an emotional category ej as fol-
lows: 
 
),(_)|(
),(
jiij
ji
ewnInformatiomutualweP
ewsalience
?
=
 
 
After the salient words for each emotional 
category have been identified employing a 
training corpus, we can carry out emotion rec-
ognition at the sentence level, considering that 
each word in the sentence is independent of the 
rest. The goal is to map the sentence W to any 
of the emotional categories in E. To do this, we 
compute an activation value ak for each emo-
tional category as follows: 
 
?
=
+=
n
m
kmkmk wwIa
1
 
 
where k = 1?S, n is the number of words in 
W, Im represents an indicator that has the value 
1 if wk is a salient word for the emotional cate-
gory (i.e. salience(wi,ej) ? 0) and the value 0 
otherwise; wmk is the connection weight be-
tween the word and the emotional category, 
and wk represents bias. We define the connec-
tion weight wmk as:  
 
),(_ kmmk ewnInformatiomutualw =  
 
whereas the bias is computed as: 
)(log kk ePw = . Finally, the emotional cate-
gory deduced by the classifier, h, is the one 
with highest activation value ak: 
 
)max(arg k
k
ah =  
 
To compute the probabilities pi?s for the emo-
tion prediction, we use the following expres-
sion: 
?
=
=
S
j
jii aap
1
/  
 
where ai represents the activation value of hi, 
and the aj?s are the activation values of the S 
emotional categories considered. 
3.1.4 Dialogue acts classifier 
A dialogue act can be defined as the function 
performed by an utterance within the context 
of a dialogue, for example, greeting, closing, 
suggestion, rejection, repeat, rephrase, confir-
mation, specification, disambiguation, or help 
(Batliner et al 2003; Lee and Narayanan, 
2005; Liscombe et al 2005). 
Our dialogue acts classifier is inspired by 
the study of Liscombe et al (2005), where the 
sequential structure of each dialogue is mod-
elled by a sequence of dialogue acts. A differ-
ence is that they assigned one or more labels 
related to dialogue acts to each user utterance, 
and did not assign labels to system prompts, 
whereas we assign just one label to each sys-
tem prompt and none to user utterances. This 
decision is made from the examination of our 
dialogue corpus. We have observed that users 
got tired or angry if the system generated the 
same prompt repeatedly (i.e. repeated the same 
dialogue act) to try to get a particular data 
item. For example, if it had difficulty in obtain-
ing a telephone number then it employed sev-
eral dialogue turns to obtain the number and 
confirm it, which annoyed the users, especially 
if they had employed other turns previously to 
correct misunderstandings. Hence, our dia-
logue act classifier aims to predict these nega-
tive emotional states by detecting successive 
repetitions of the same system?s prompt types 
(e.g. prompts to get the telephone number). 
In accordance with our approach, the emo-
tional category of a user?s dialogue turn, En, is 
that which maximises the posterior probability 
given a sequence of the most recent system 
prompts: 
 
),...,,|(maxarg 13)12*( ????= nnLnk
k
n DADADAEPE
 
where the prompt sequence is represented by a 
sequence of dialogue acts (DAi?s) and L is the 
length of the sequence, i.e. the number of sys-
tem?s dialogue turns in the sequence. Note that 
if L = 1 then the decision about En depends 
only on the previous system prompt. In other 
words, the emotional category obtained is that 
with the greatest probability given just the pre-
vious system turn in the dialogue. The prob-
ability of the considered emotional categories 
284
given a sequence of dialogue acts is obtained 
by employing a training dialogue corpus. 
By means of this equation, we decide the 
most likely emotional category for the input 
utterance, selecting the category with the high-
est probability given the sequence of dialogue 
acts of length L. This probability is used to 
create the pair (hi, pi) to be included in the 
emotion prediction. 
3.2 Fusion methods 
In the current implementation our technique 
employs the three fusion methods discussed in 
this section. When used in Fusion-0, these 
methods are employed to combine the predic-
tions provided by the classifiers. When used in 
Fusion-1, they are used to combine the predic-
tions generated by Fusion-0. 
3.2.1 Average of probabilities (AP) 
This method combines the predictions by aver-
aging their probabilities. To do this we con-
sider that each input utterance is represented 
by feature vectors x1,?,xm from feature spaces 
X1,?,Xm, where m is the number of classifiers. 
We also assume that each input utterance be-
longs to one of S emotional categories hi, i = 
1?S. In each of the m feature spaces a classi-
fier can be created that approximates the poste-
rior probability P(hi | x
k) as follows: 
 
)()|()( kki
k
i
kk
i xxhPxf ?+=  
 
where )( kki x?  is the error made by classifier 
k. We estimate P(hi | x
k) by )( kki xf  and as-
suming a zero-mean error for )( kki x? , we av-
erage all the )( kki xf ?s to obtain a less error-
sensitive estimation. In this way we obtain the 
following mean combination rule to decide the 
most likely emotional category: 
 
?
=
=
m
k
kk
i
m
i xfm
xxhP
1
1 )(
1
),...,|(  
3.2.2 Multiplication of probabilities (MP) 
Assuming that the feature spaces X1,?,Xm are 
different and independent, the probabilities can 
be written as follows: 
 
)|(...)|()|(
)|,...,(
21
1
i
m
ii
i
m
hxPhxPhxP
hxxP
???
=
 
 
Using Bayes rule we can obtain the following 
equation, which we use to decide the most 
likely emotional category for each input utter-
ance (represented as feature vectors x1,?,xm): 
 
? ?
?
??
?
??
?= ?
?
'
1
'
'
'
'
1
1
)(/)|(
)(/)|(
),...,|(
i
m
i
k
k
i
k
m
i
k
i
m
i
hPxhP
hPxhP
xxhP
 
3.2.3 Unweighted vote (UV) 
This method combines the emotion predictions 
by counting the number of classifiers (if used 
in Fusion-0) or fusion methods (if used in Fu-
sion-1) that consider an emotional category hi 
as the most likely for the input utterance. If we 
consider three emotional categories X, Y and Z, 
hi is decided as follows: 
 
?
?
?
?
?
?
?
?
?
??
??
??
=
? ? ? ?
? ? ??
? ? ??
= = = =
= = ==
= = ==
m
j
m
j
m
j
m
j
jjij
m
j
m
j
m
j
jj
m
j
jj
m
j
m
j
m
j
jj
m
j
jj
i
YZandXZifZ
ZYandXYifY
ZXandYXifX
h
1 1 1 1
1 1 11
1 1 11
 
 
where m is the number of classifiers or fusion 
methods employed (e.g., in our experiments, X 
= Neutral, Y = Tired and Z = Angry). The 
probability pi for hi to be included in the emo-
tion prediction is computed as follows: 
 
?
=
=
3
1
/),,|(
j
jii VhVhZYXhP  
 
where Vhi is the number of votes for hi, and the 
Vhj?s are the number of votes for the 3 emo-
tional categories. If we consider two emotional 
categories X and Y, the most likely emotional 
category hi and its probability pi are analo-
gously computed (e.g., in our experiments, X = 
Non-negative and Y = Negative). 
4 Emotional speech database 
Our emotional speech database has been con-
structed from a corpus of 440 telephone-based 
dialogues between students of the University 
of Granada and the Saplen system, which was 
285
previously developed in our lab for the fast 
food domain (L?pez-C?zar et al 1997; L?pez-
C?zar and Callejas, 2006). Each dialogue was 
stored in a log file in text format that includes 
each system prompt (e.g. Would you like to 
drink anything?), the type of prompt (e.g. Any-
FoodOrDrinkToOrder?), the name of the voice 
samples file (utterance) that stores the user re-
sponse to the prompt, and the speech recogni-
tion result for the utterance. The dialogue cor-
pus contains 7,923 utterances, 50.3% of which 
were recorded by male users and the remaining 
by female users. 
The utterances have been annotated by 4 la-
bellers (2 male and 2 female). The order of the 
utterances has been randomly chosen to avoid 
influencing the labellers by the situation in the 
dialogues, thus minimising the effect of dis-
course context. The labellers have initially as-
signed one label to each utterance, either 
<NEUTRAL>, <TIRED> or <ANGRY> according 
to the perceived emotional state of the user. 
One of these labels has been finally assigned to 
each utterance according to the majority opin-
ion of the labellers, so that 81% of the utter-
ances are annotated as ?Neutral?, 9.5% as 
?Tired? and 9.4% as ?Angry?. This shows that 
the database is clearly unbalanced in terms of 
emotional categories. 
To measure the amount of agreement be-
tween the labellers we employed the Kappa 
statistic (K), which is computed as follows 
(Cohen, 1960): 
 
)(1
)()(
EP
EPAP
K ?
?=  
 
where P(A) is the proportion of times that the 
labellers agree, and P(E) is the proportion of 
times we would expect the labellers to agree by 
chance. We obtained that K = 0.48 and K = 
0.45 for male and female labellers, respec-
tively, which according to Landis and Koch 
(1977) represents moderate agreement. 
5 Experiments 
The main goal of the experiments has been to 
test the proposed technique using our emo-
tional speech database, and employing: 
 
? Three emotional categories (Neutral, An-
gry and Tired) on the one hand, and two 
emotional categories (Non-negative and 
Negative) on the other. The experiments 
employing the former category set will be 
called 3-emotion experiments, whereas 
those employing the latter category will be 
called 2-emotion experiments. 
? The four classifiers described in Section 
 3.1, and the three fusion methods dis-
cussed in Section  3.2. 
 
In the 3-emotion experiments we consider that 
an input utterance is correctly classified if the 
emotional category deduced by the technique 
matches the label assigned to the utterance. In 
the 2-emotion experiments, the utterance is 
considered to be correctly classified if either 
the deduced emotional category is Non-
negative and the label is Neutral, or the cate-
gory is Negative and the label is Tired or An-
gry. 
To carry out training and testing we have 
used a script that takes as its input a set of la-
belled dialogues in a corpus, and processes 
each dialogue by locating within it, from the 
beginning to the end, each prompt of the 
Saplen system, the voice samples file that con-
tains the user?s response to the prompt, and the 
result provided by the system?s speech recog-
niser (sentence in text format). The type of 
each prompt is used to create a sequence of 
dialogue acts of length L, which is the input to 
the dialogue acts classifier. The voice samples 
file is the input to the prosodic and acoustic 
classifiers, and the speech recognition result is 
the input to the lexical classifier. This proce-
dure is repeated for all the dialogues in the cor-
pus. 
Experimental results have been obtained us-
ing 5-fold cross-validation, with each partition 
containing the utterances corresponding to 88 
different dialogues in the corpus. 
5.1 Performance of Fusion-0 
Table 1 sets out the average results obtained 
for Fusion-0 considering several combinations 
of the classifiers and employing the three fu-
sion methods. As can be observed, MP is the 
best fusion method, with average classification 
rates of 89.08% and 87.43% for the 2 and 3 
emotion experiments, respectively. The best 
classification rates (92.23% and 90.67%) are 
obtained by employing the four classifiers, 
which  means that the four types of informa-
tion considered (acoustic, prosodic, lexical and 
related to dialogue acts) are really useful to 
enhance classification rates. 
 
286
Fusion 
Method 
Classifiers 2 emot. 3 emot. 
Aco, Pro 84.15 82.46 
Lex, Pro 85.04 82.71 
DA, Pro 90.49 87.48 
Aco, Lex, Pro 89.20 86.17 
Aco, DA, Pro 90.24 88.56 
DA, Lex, Pro 90.02 88.02 
Aco, DA, Lex, Pro 90.49 88.32 
AP 
Average 88.66 86.25 
Aco, Pro 84.15 82.86 
Lex, Pro 85.16 83.71 
DA, Pro 91.49 89.78 
Aco, Lex, Pro 89.17 87.91 
Aco, DA, Pro 91.33 89.23 
DA, Lex, Pro 90.06 87.82 
Aco, DA, Lex, Pro 92.23 90.67 
MP 
Average 89.08 87.43 
Aco, Pro 88.64 85.19 
Lex, Pro 86.40 83.01 
DA, Pro 88.20 84.92 
Aco, Lex, Pro 88.76 85.54 
Aco, DA, Pro 88.91 85.89 
DA, Lex, Pro 88.47 85.61 
Aco, DA, Lex, Pro 89.04 87.56 
UV 
Average 88.35 85.39 
 
Table 1: Performance of Fusion-0 (results in %). 
5.2 Performance of Fusion-1 
Table 2 shows the average results obtained 
when Fusion-1 is used to combine the predic-
tions of Fusion-0. The three fusion methods are 
tested in Fusion-1, with Fusion-0 employing 
four combinations of these methods: AP,MP; 
AP,UV; MP,UV; and AP,MP,UV. In all cases 
Fusion-0 uses the four classifiers as this is the 
configuration that provides the highest classifi-
cation accuracy according to the previous sec-
tion. 
Comparison of both tables shows that Fu-
sion-1 clearly outperforms Fusion-0. The best 
results are attained for MP, which means that 
this method is preferable when the data contain 
small errors (emotion predictions generated by 
Fusion-0 with accuracy rates around 90%). 
To find the reasons for these enhancements 
we have analysed the confusion matrix of Fu-
sion-1 using MP. The study reveals that for the 
2-emotion experiments this fusion stage works 
very well in predicting the Non-negative cate-
gory, very slightly enhancing the classification 
rate of Fusion-0 (96.58% vs. 95.93%), whereas 
the classification rate of the Negative category 
is the same as that obtained by Fusion-0 
(88.91%). Overall, the best performance of 
Fusion-1 employing MP (94.48%) outdoes that 
of Fusion-0 employing AP (90.49%) and MP 
(92.23%). 
Regarding the 3-emotion experiments, our 
analysis shows that using MP, Fusion-1 
slightly lowers the classification rate of the 
Neutral category obtained by Fusion-0 
(97.79% vs. 97.9%), but slightly raises the rate 
of the Tired category (93.62% vs. 93.26%), 
and the Angry category (77.49% vs. 76.81%). 
Overall, the performance of Fusion-1 employ-
ing MP (94.02%) outdoes that of Fusion-0 em-
ploying AP (88.32%) and MP (90.67%). 
 
 
Fusion methods 
used in Fusion-0 
Fusion method 
used in Fusion-1 
(2 emotions) 
Fusion method 
used in Fusion-1 
(3 emotions) 
 AP MP UV AP MP UV 
AP,MP 93.68 94.48 93.53 91.77 94.02 90.96 
AP,UV 93.20 93.23 93.20 91.65 93.13 90.10 
MP,UV 93.34 94.38 93.20 91.27 93.98 89.48 
AP,MP,UV 93.23 94.36 93.17 91.57 93.97 89.06 
Average 93.40 94.11 93.28 91.57 93.78 89.90 
 
Table 2: Performance of Fusion-1 (results in %). 
 
6 Conclusions and future work 
Our experimental results show that the pro-
posed technique is useful to improve the classi-
fication rates of the standard fusion technique, 
which employs just one fusion stage. Compar-
ing results in Table 1 and Table 2 we can ob-
serve that for the 2-emotion experiments, Fu-
sion-1 enhances Fusion-0 by 2.25% absolute 
(from 92.23% to 94.48%), while for the 3-
emotion experiments, the improvement is 
3.35% absolute (from 90.67% to 94.02%). 
These improvements are obtained by employ-
ing AP and MP in Fusion-0 to combine the 
emotion predictions of the four classifiers, and 
using MP in Fusion-1 to combine the outputs 
of Fusion-0. 
The reason for these improvements is that 
the double fusion process (Fusion-0 and Fu-
sion-1) allows us to benefit from the advan-
287
tages of using different methods to combine 
information. According to our results, the best 
methods are AP and MP. The former allows 
gaining maximally from the independent data 
representation available, which are the input to 
Fusion-0 (in our study, prosody, acoustics, 
speech recognition errors, and dialogue acts). 
The latter provides better results when the data 
contain small errors, which occurs when the 
predictions provided by Fusion-0 are the input 
to Fusion-1. 
Future work will include testing the tech-
nique employing information sources not con-
sidered in this study. The sources we have 
dealt with in the experiments (prosodic, acous-
tic, lexical, and dialogue acts) are those most 
commonly employed in previous studies. 
However, there are also studies that suggest 
using other information sources, such as speak-
ing style, subject and problem identification, 
and non-verbal cues. 
Another future work is to test the technique 
employing other methods for classification and 
information fusion. For example, it is known 
that people are usually confused when they try 
to determine the emotional state of a speaker, 
given that the difference between some emo-
tions is not always clear. Hence, it would be 
interesting to investigate the performance of 
the technique employing classification algo-
rithms that deal with this vague boundary, such 
as fuzzy inference methods, and using boosting 
methods for improving the accuracy of the 
classifiers. 
Finally, in terms of application of the tech-
nique to improve the system-user interaction, 
we will evaluate different dialogue manage-
ment strategies to enable the system?s adapta-
tion to negative emotional states of users (Uni-
versity students). For example, a dialogue 
management strategy could be as follows: i) if 
the emotional state is Tired begin the following 
prompt apologising, and transfer the call to a 
human operator if this state is recognised twice 
consecutively, and ii) if the emotional state is 
Angry apologise and transfer the call to a hu-
man operator immediately. 
 
Acknowledgments 
This research has been funded by Spanish pro-
ject HADA TIN2007-64718, and the Czech 
Grant Agency project no. 102/08/0707. 
 
References  
Ai, H., Litman, D. J., Forbes-Riley, K., Rotaru, M., 
Tetreault, J., Purandare, A. 2006. Using system 
and user performance features to improve emo-
tion detection in spoken tutoring systems. Proc. 
of Interspeech, pp. 797-800. 
Batliner, A., Fischer, K., Huber, R., Spilker, J., 
N?th, E. 2003. How to find trouble in communi-
cation. Speech Communication, vol. 40, pp. 117-
143. 
Cohen, J. 1960. A coefficient of agreement for 
nominal scales. Educat. Psychol. Measurement, 
vol. 20, pp. 37-46. 
Dellaert, F., Polzin, T., Waibel, A. 1996. Recogniz-
ing emotion in speech. Proc. of ICSLP, pp. 
1970-1973. 
Devillers, L., Vidrascu, L. 2006. Real-life emotions 
detection with lexical and paralinguistic cues on 
human-human call center dialogs. Proc. of Inter-
speech, pp. 801-804. 
Landis, J. R., Koch, G. G. 1977. The measurement 
of observer agreement for categorical data. Bio-
metrics, vol. 33, pp. 159-174. 
Lee, C. M., Narayanan, S. S. 2005. Toward detect-
ing emotions in spoken dialogs. IEEE Transac-
tions on Speech and Audio Processing, vol. 
13(2), pp. 293-303. 
Liscombe, J., Riccardi, G., Hakkani-T?r, D. 2005. 
Using context to improve emotion detection in 
spoken dialogue systems. Proc. of Interspeech, 
pp. 1845-1848. 
L?pez-C?zar, R., Garc?a, P., D?az, J., Rubio, A. J. 
1997. A voice activated dialog system for fast-
food restaurant applications. Proc. of Eu-
rospeech, pp. 1783-1786. 
L?pez-C?zar, R., Callejas, Z. 2006. Combining 
Language Models in the Input Interface of a 
Spoken Dialogue System. Computer Speech and 
Language, 20, pp. 420-440. 
Luengo, I., Navas, E., Hern?ez, I, Sanchez, J. 2005. 
Automatic emotion recognition using prosodic 
parameters. Proc. of Interspeech, pp.493-496. 
Morrison, D., Wang, R., De Silva, L. C. 2007. En-
semble methods for spoken emotion recognition 
in call-centres. Speech Communication, vol. 
49(2) pp. 98-112. 
Nwe, T. L., Foo, S. V., De Silva, L. C. 2003. 
Speech emotion recognition using hidden 
Markov models. Speech Communication, vol. 
41(4), pp. 603-623. 
288
