Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 884?894, Dublin, Ireland, August 23-29 2014.
A Step Towards Usable Privacy Policy:
Automatic Alignment of Privacy Statements
Fei Liu Rohan Ramanath Norman Sadeh Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{feiliu, rrohan, sadeh, nasmith}@cs.cmu.edu
Abstract
With the rapid development of web-based services, concerns about user privacy have height-
ened. The privacy policies of online websites, which serve as a legal agreement between service
providers and users, are not easy for people to understand and therefore offer an opportunity for
natural language processing. In this paper, we consider a corpus of these policies, and tackle the
problem of aligning or grouping segments of policies based on the privacy issues they address.
A dataset of pairwise judgments from humans is used to evaluate two methods, one based on
clustering and another based on a hidden Markov model. Our analysis suggests a five-point gap
between system and median-human levels of agreement with a consensus annotation, of which
half can be closed with bag of words representations and half requires more sophistication.
1 Introduction
Privacy policies are legal documents, authored by privacy lawyers to protect the interests of companies
offering services through the web. According to a study conducted by McDonald and Cranor (2008), if
every internet user in the U.S. read the privacy notice of each new website she visited, it would take the
nation 54 billion hours annually to read privacy policies. It is not surprising that they often go unread
(Federal Trade Commission, 2012).
Users, nonetheless, might do well to understand the implications of agreeing to a privacy policy, and
might make different choices if they did. Researchers in the fields of internet privacy and security have
made various attempts to standardize the format of privacy notices, so that they are easier to understand
and to allow the general public to have better control of their personal information. An early effort is the
Platform for Privacy Preferences Project (P3P), which defines a machine-readable language that enables
the websites to explicitly declare their intended use of personal information (Cranor, 2002). Many other
studies primarily focus on the qualitative perspective of policies and use tens of carefully selected privacy
notices. For example, Kelley et al. (2010) proposed a ?nutrition label? approach that formalizes the
privacy policy into a standardized table format. Breaux et al. (2014) map privacy requirements encoded
in text to a formal logic, in order to detect conflicts in requirements and trace data flows (e.g., what data
might be collected, to whom the data will be transferred and for what purposes).
The need for automatically or semi-automatically generating simple, easy-to-digest privacy summaries
is further exacerbated by the emergence of the mobile Web and the Internet of Things, with early efforts
in this area including the use of static analysis to identify sensitive data flows in mobile apps (Lin et al.,
2012) and the development of mobile app privacy profiles (Liu et al., 2014).
Increased automation for such efforts motivates our interest in privacy policies as a text genre for NLP,
with the general goal of supporting both user-oriented tools that interpret policies and studies of the
contents of policies by legal scholars.
In this paper, we start with a corpus of 1,010 policies collected from widely-used websites (Ramanath
et al., 2014),
1
and seek to automatically align segments of policies. We believe this is a worthwhile first
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.usableprivacy.org/data
884
Amazon.com Privacy Notice
...
What About Cookies?
Cookies are unique identifiers that we transfer to your device to enable
our systems to recognize your device and to provide features such as
1-Click purchasing, Recommended for You, personalized advertisements
on other Web sites...
...Because cookies allow you to take advantage of some of Amazon.com?s
essential features, we recommend that you leave them turned on. For
instance, if you block or otherwise reject our cookies, you will not be
able to add items to your Shopping Cart, proceed to Checkout, or use any
Amazon.com products and services that require you to Sign in...
Walmart Privacy Policy
...
Information We Collect
...We use ?cookies? to recognize you as you use or return to our sites.
This is done so that we can provide a continuous and more personalized
shopping experience for you. A cookie is a small text file that a website
or email may save to your browser and store on your hard drive...
Your Choices
...You may exercise choices related to our online operations and adver-
tising. For instance, you can choose to browse our websites without ac-
cepting cookies. Please know that cookies allow us to recognize you from
page to page, and they support your transactions with us. Without cookies
enabled, you will still be able to browse our websites, but will not be able
to complete a purchase or take advantage of certain website features...
Table 1: Example privacy statements from Amazon.com (left) and Walmart.com (right). The statements
are concerned with the websites? cookie policy. The top-most level section subtitles are shown in bold.
step toward interpretation of the documents of direct interest here, and also that automatic alignment of
a large set of similarly-constructed documents might find application elsewhere.
Consider the example in Table 1, where we show privacy statements from Amazon.com
2
and Wal-
mart.com.
3
These statements are concerned with the usage of cookies?small data files transferred by a
website to the user?s computer hard drive?often used for tracking a user?s browsing behavior. Cookies
are one issue among many that are addressed by privacy policies; by aligning segments by issue, across
policies, we can begin to understand the range of policy approaches for each issue.
We contribute pairwise annotations of segment pairs drawn from different policies, for use in evalu-
ating the quality of alignments, an analysis of the inter-annotator reliability, and an experimental assess-
ment of three alignment methods, one based on clustering and two based on a hidden Markov model.
This paper?s results refine the findings of Ramanath et al. (2014). Our key finding is that these unsuper-
vised methods reach far better agreement with the consensus of crowdworkers than originally estimated,
and that the gap between these methods and the ?median? crowdworker is about half due to the greedy
nature of such methods and about half due to the bag of words representation.
2 Privacy Dataset and Annotations
For completeness, we review the corpus of privacy policies presented by Ramanath et al. (2014), and
then present the new annotations created for evaluation of alignment.
2.1 Corpus
We collected 1,010 unique privacy policy documents from the top websites ranked by Alexa.com.
4
These
policies were collected during a period of six weeks during December 2013 and January 2014. They are a
snapshot of privacy policies of mainstream websites covering fifteen of Alexa.com?s seventeen categories
(Table 2).
5
Finding a website?s policy is not trivial. Though many well-regulated commercial websites provide a
?privacy? link on their homepages, not all do. We found university websites to be exceptionally unlikely
to provide such a link. Even once the policy?s URL is identified, extracting the text presents the usual
challenges associated with scraping documents from the web. Since every site is different in its place-
ment of the document (e.g., buried deep within the website, distributed across several pages, or mingled
together with Terms of Service) and format (e.g., HTML, PDF, etc.), and since we wish to preserve as
much document structure as possible (e.g., section labels), full automation was not a viable solution.
2
https://www.amazon.com/gp/help/customer/display.html?nodeId=468496
3
http://corporate.walmart.com/privacy-security/walmart-privacy-policy
4
http://www.alexa.com
5
The ?Adult? category was excluded; the ?World? category was excluded since it contains mainly popular websites in
different languages, and we opted to focus on policies in English in this first stage of research, though multilingual policy
analysis presents interesting challenges for future work.
885
Sections Paragraphs Sections Paragraphs
Category Count Length Count Length Category Count Length Count Length
Arts 11.1 254.8 39.2 72.1 Recreation 11.9 218.8 38.5 67.4
Business 10.0 244.2 37.6 65.1 Reference 9.7 179.4 26.2 66.3
Computers 10.5 213.4 34.4 65.4 Regional 10.2 207.7 36.0 59.1
Games 10.0 244.1 34.9 70.1 Science 8.7 155.0 22.1 61.0
Health 9.9 228.2 32.4 69.4 Shopping 11.9 213.9 39.3 64.8
Home 11.6 201.5 32.4 72.0 Society 9.8 230.8 32.6 69.3
Kids and Teens 9.6 231.5 32.3 68.6 Sports 10.1 217.1 29.1 75.6
News 10.3 248.4 35.5 72.4 Average 10.4 221.9 34.1 68.0
Table 2: Fifteen website categories, average number of sections and paragraphs per document in that
category, and average length in word tokens.
We therefore crowdsourced the privacy policy document collection using Amazon Mechanical Turk.
For each website, we created a HIT in which a worker was asked to copy and paste the following privacy
policy-related information into text boxes: (i) privacy policy URL; (ii) last updated date (or effective
date) of the current privacy policy; (iii) privacy policy full text; and (iv) the section subtitles in the
top-most layer of the privacy policy. To identify the privacy policy URL, workers were encouraged to
go to the website and search for the privacy link. Alternatively, they could form a search query using
the website name and ?privacy policy? (e.g., ?Amazon.com privacy policy?) and search in the returned
results for the most appropriate privacy policy URL. Each HIT was completed by three workers, paid
$0.05, for a total cost of $380 (including Amazon?s surcharge). After excluding dupliates, the dataset
contains 1,010 unique documents.
6
Given the privacy policy full text and the section subtitles, we partition the full privacy document into
different sections, delimited by the section subtitles. To generate paragraphs, we break the sections by
lines, and consider each line as a paragraph. We require a paragraph to end with a period, if not, it will
be concatenated with the next paragraph. Using this partition scheme, sections contain 12 sentences on
average; and paragraphs contain 4 sentences on average. More statistics are presented in Table 2.
2.2 Pairwise Annotations
Ramanath et al. (2014) described an evaluation method in which pairs of privacy policy sections were
annotated by crowdworkers.
7
A sample of section pairs from different policies was drawn, stratified
by cosine similarity of unigram tfidf vectors. In a single task, a crowdworker was asked whether two
sections broadly discussed the same topic. The question was presented alongside three answer options,
essentially a strong yes, a yes, and a no. In that initial exploration, each item was annotated at least three
times, and up to fifteen, until an absolute majority was reached.
The annotations conducted for this study were done somewhat differently. Our motivations were to
enable a more careful exploration of inter-annotator agreement, which was complicated in the earlier
work by the variable number of annotations per pair, from three to fifteen. We also sought to explore a
more fine-grained problem at the paragraph level.
We sampled 1,000 document pairs from each of the 15 categories, then generated pairs (separately of
sections and of paragraphs) by choosing one at random from each document. In total, 1,278,204 section
pairs and 7,968,487 paragraph pairs were produced. These pairs were stratified by cosine similarity
intervals: [0, 0.25], (0.25, 0.5], (0.5, 0.75], (0.75, 1], as in Ramanath et al. (2014). We sampled 250 pairs
from each interval, resulting in 1,000 pairs each of sections and paragraphs.
These pairs were annotated on Amazon Mechanical Turk. The crowdworkers were instructed to care-
fully read the privacy statements and answer a ?yes/no? question, indicating whether the two texts are
discussing the same privacy issue or not. Several key privacy issues are provided as examples, including
6
Note that different websites may be covered by the same privacy policy provided by the parent company. For example,
espn.go.com, abc.go.com, and marvel.com are all covered under the Walt Disney privacy policy.
7
Another evaluation, based on text selected by humans in a separate, unrelated task, was also explored. Because such an
evaluation seems less broadly applicable, we did not pursue it here.
886
Sections Paragraphs
Cosine similarity: [0, .25] (.25, .5] (.5, .75] (.75, 1] All [0, .25] (.25, .5] (.5, .75] (.75, 1] All
5 workers agree 36.4 12.4 28.0 85.2 40.5 42.4 12.0 32.8 77.6 41.2
4 workers agree 42.8 42.4 42.0 13.6 35.2 39.6 36.8 35.6 17.6 32.4
3 workers agree 20.8 45.2 30.0 1.2 24.3 18.0 51.2 31.6 4.8 26.4
Consensus-yes 4.4 45.2 87.2 99.2 59.0 9.2 66.0 88.8 98.0 65.5
Consensus-no 95.6 54.8 12.8 0.8 41.0 90.8 34.0 11.2 2.0 34.5
Table 3: Inter-annotator agreement of section and paragraph pairs.
collection of personal information, sharing of information with third parties, cookies and other tracking
techniques, data security, children policies, and contact of the websites. To encourage the crowdwork-
ers to carefully read the privacy statements, we also asked them to copy and paste 1?3 keywords from
each section/paragraph, before answering the question.
8
Each section/paragraph pair was judged by five
crowdworkers and was rewarded $0.05. In total, $550 was spent on the annotations.
On average, it took a crowdworker 2.15 minutes to complete a section pair and 1.67 minutes for a
paragraph pair. Interestingly, although a section is roughly three times the length of a paragraph (see
Table 2), the time spent on annotation is not proportional to the text length.
In Table 3, we present the inter-annotator agreement results for section and paragraph pairs, broken
down by cosine-similarity bin and by the majority answer. 75.7% (73.6%) of section (paragraph) pairs
were agreed upon by four or more out of five annotators. Unsurprisingly, disagreement is greatest in
the (.25, .5] similarity bin. Cosine similarity is a very strong predictor of the consensus answer (Pearson
correlation 0.72 for section pairs, 0.67 for paragraphs, on this stratified sample).
Ramanath et al. (2014) considered only sections. A different method was used to obtain consensus
annotations; we simply kept adding annotators to a pair until consensus was reached. For a fair compar-
ison with the new data, we calculated pairwise agreement among three annotators per item, randomly
selected if there were more than three to choose from. On the old section-level data, this was 60.5%; on
the new data, it was 71.3% (using five annotators). Although a controlled experiment in the task setup
was not conducted, we take this as a sign that our binary question with keywords led to a higher quality
set of annotations than the three-way question in the older data. Our experiments in this paper use only
the new data.
2.3 Discussion
We had expected higher agreement at the paragraph level, since paragraphs are shorter, presumably easier
to read and compare, and presumably more focused on a smaller number of issues. This was not borne
out empirically, though a slightly different analysis presented in ?4.2 suggests that, among crowdworkers
who completed ten or more tasks, paragraphs were easier to agree on.
Privacy policies are generally written by attorneys with expertise in privacy law, though there are
automatic generation solutions available that allow a non-expert to quickly fill in a template to create a
policy document.
9
Example 1 in Table 4 shows a case of very high text overlap (five out of five annotators
agreed on a ?yes? answer for this pair). While this kind of localized alignment is not our aim here, we
believe that such ?boilerplate? text, to the extent that it occurs in large numbers of policies, will make
automatic alignment easier.
A case where annotators seem not to have understood, or not taken care to read carefully, is illustrated
by Example 2 in Table 4. Both sections describe ?opt-out? options for unsubscribing from mailing lists
that send promotional messages, though the first is more generally about ?communications? and the
second only addresses email. Three out of five crowdworkers labeled this example with?no.? Achieving
better consensus might require more careful training of annotators about a predefined set of concepts at
the right granularity.
8
We have not used these keywords for any other purpose.
9
For example: http://www.rendervisionsconsulting.com/blog/wp-content/uploads/2011/09/
Privacy-policy-solutions-list_rvc.pdf
887
Example 1 Example 2
Policy excerpt from Urban Outfitters website:
To serve you better, we may combine information you give us online, in
our stores or through our catalogs. We may also combine that information
with publicly available information and information we receive from or
cross-reference with our Select Partners and others. We use that com-
bined information to enhance and personalize the shopping experience of
you and others with us, to communicate with you about our products and
events that may be of interest to you, and for other promotional purposes.
Policy excerpt from Williams-Sonoma website:
To serve you better and improve our performance, we may combine
information you give us online, in our stores or through our catalogs. We
may also combine that information with publicly available information
and information we receive from or cross-reference with select partners
and others. By combining this information we are better able to com-
municate with you about our products, special events and promotional
purposes and to personalize your shopping experience.
Policy excerpt from IKEA website:
What if I prefer not to receive communications from IKEA? If you prefer
not to receive product information or promotions from us by U.S. Mail,
please click here. To unsubscribe from our email list, please follow the
opt-out instructions at the bottom of the email you received, or click here
and update your profile by deselecting ?Please send me: Inspirational
emails and updates.?
Policy excerpt from Neiman Marcus website:
Emails. You will receive promotional emails from us only if you have
asked to receive them. If you do not want to receive email from Neiman
Marcus or its affiliates you can click on the ?Manage Your Email
Preferences? link at the bottom of any email communication sent by us.
Choose ?Unsubscribe? at the bottom of the page that opens. Please allow
us 3 business days from when the request was received to complete the
removal, as some of our promotion s may already have been in process
before you submitted your request.
Table 4: Privacy policy excerpts. Example 1 (a pair of paragraphs) illustrates the likely use of boilerplate;
identical text is marked in gray. Example 2 shows a pair of sections where our intuitions disagree with
the annotations.
3 Problem Formulation
Given a collection of privacy policy documents and assuming each document consists of a sequence
of naturally-occurring text segments (e.g., sections or paragraphs), our goal is to automatically group
the text segments that address the same privacy issue, without pre-specifying the set of such issues.
We believe this exemplifies many scenarios where a collection of documents follow a similar content
paradigm, such as legal documents and, in some cases, scientific literature. Our interest in algorithms
that characterize each individual document?s parts in the context of the corpus is inspired by biological
sequence alignment in computational biology (Durbin et al., 1998).
In our experiments, we consider a hidden Markov model (HMM) that captures local transitions be-
tween topics. The motivation for the HMM is that privacy policies might tend to order issues similarly,
e.g., the discussion on ?sharing information to third parties? appears to often follow the discussion of
?personal information collection.? If each of these corresponds to an HMM state, then the regularity in
ordering is captured by the transition distribution, and each state is characterized by its emission dis-
tribution over words. In this section, we discuss the HMM and two estimation procedures based on
Expectation-Maximization (EM) and variational Bayesian (VB) inference.
3.1 Hidden Markov Model
Assume we have a sequence of observed text segments
10
O = [O
1
, O
2
, ..., O
T
], and each O
t
represents
a text segment in a privacy document (t ? {1, 2, ..., T}). We denote O
t
= [O
1
t
, O
2
t
, ..., O
N
t
t
], where each
O
j
t
corresponds to a word token in the tth text segment; N
t
is the total number of word tokens in the
segment; T represents the total number of segments in the observation sequence. Each text segment O
t
is associated with a hidden state S
t
(S
t
? {1, 2, . . . ,K}, where K is the total number of states). Given
an observation sequence O, our goal is to decode the corresponding hidden state sequence S.
We employ a first-order hidden Markov model where the next state depends only on the previous state.
A notable difference from the familiar HMM used in NLP (e.g., as used for part-of-speech tagging) is that
we allow multiple observation symbols to be emitted from each hidden state. Each symbol corresponds
to a word token in the text segment. Hence the likelihood for a single document can be written as:
L(?, ?) =
?
S?{1,...,K}
T
p(O,S | ?, ?) =
?
S?{1,...,K}
T
T+1
?
t=1
?
S
t
|S
t?1
N
t
?
j=1
?
O
j
t
|S
t
(1)
10
We use segments to refer abstractly to either sections or paragraphs. In any given instantiation, one or the other is used,
never a blend.
888
E-step:
Forward pass: ?
1
(?) = 1; ?
t
(k) =
?
K
k
?
=1
?
t?1
(k
?
) ? ?
k|k
?
?
?
N
t
j=1
?
O
j
t
|k
, ?t ? {2, . . . , T}, ?k ? {1, . . . ,K}(2)
Backward pass: ?
T+1
(?) = 1; ?
t
(k) =
?
K
k
?
=1
?
k
?
|k
?
?
N
t
j=1
?
O
j
t
|k
?
? ?
t+1
(k
?
), ?t ? {T, . . . , 1}, ?k ? {1, . . . ,K}
(3)
Likelihood: p(O | ?, ?) = p(O
1,
O
2
, ..., O
T
| ?, ?) =
?
K
k=1
?
t
(k) ? ?
t
(k) (for any t) (4)
Posteriors: ?
t
(k) = p(S
t
= k | O, ?, ?) =
?
t
(k) ? ?
t
(k)
p(O | ?, ?)
(5)
Pair posteriors: ?
t
(k, k
?
) = p(S
t
= k, S
t+1
= k
?
| O, ?, ?) =
?
t
(k) ? ?
k
?
|k
?
(
?
N
t+1
j=1
?
O
j
t+1
|k
?
)
? ?
t+1
(k
?
)
p(O | ?, ?)
(6)
M-step (in EM):
Transitions: ?
k
?
|k
=
?
T
t=1
?
t
(k, k
?
)
?
T
t=1
?
K
k
??
=1
?
t
(k, k
??
)
; Emissions: ?
v|k
=
?
T
t=1
?
t
(k) ?
?
N
t
j=1
1{O
j
t
= v}
?
T
t=1
?
t
(k) ?N
t
(7)
Variational update (in VB):
?
k
?
|k
=
exp ?
(
?
T
t=1
?
t
(k, k
?
) + ?
)
exp ?
(
?
T
t=1
?
K
k
??
=1
?
t
(k, k
??
) + ? ?K
)
; ?
v|k
=
exp ?
(
?
T
t=1
?
t
(k) ?
?
N
t
j=1
1{O
j
t
= v}+ ?
?
)
exp ?
(
?
T
t=1
?
t
(k) ?N
t
+ ?
?
? V
)
(8)
Table 5: Equations for parameter estimation of the HMM with multiple emissions at each state and a
single sequence. K is the number of states, V is the emission vocabulary size, and T is the length of the
sequence in sections. ?(?) is the digamma function.
?
k
?
|k
denotes the probability of transitioning to state k
?
given that the preceding state is k. ?
v|k
denotes
the probability that a particular symbol emitted during a visit to state k is the word v. As in standard
treatments, we assume an extra final state at the end of the sequence that emits a stop symbol.
Ramanath et al. (2014) considered three variants of the HMM, with different constraints on the tran-
sitions, such as a ?strict forward? variant that orders the states and only allows transition to ?later? states
than the current one. In the evaluation against direct human judgments, they found a slight benefit from
such constraints, but they increased performance variance considerably. Here we only consider an un-
constrained HMM.
3.2 EM and VB
We consider two estimation methods, neither novel. Both are greedy hillclimbing methods that locally
optimize functions based on likelihood under the HMM.
The first method is EM, adapted for the multiple emission case; the equations for the E-step (forward-
backward algorithm and subsequent posterior calculations) and the M-step are shown in Table 5.
We also consider Bayesian inference, which seeks to marginalize out the parameter values, since we
are really only interested in the assignment of sections to hidden states. Further, Bayesian inference
has been found favorable on small datasets (Gao and Johnson, 2008). We assume symmetric Dirichlet
priors on the transition and emission distributions, parameterized respectively by ? = 1 and ?
?
= 0.1.
We apply mean-field variational approximate inference as described by Beal (2003), which amounts to
an EM-like procedure. The E-step is identical to EM, and the M-step involves a transformation of the
expected counts, shown in Table 5. (We also explored Gibbs sampling; performance was less stable but
generally similar; for clarity we do not report the results here.)
889
3.3 Implementation Details
In modeling, the vocabulary excludes 429 stopwords,
11
words whose document frequency is less than
ten, and a set of terms specific to website privacy polices: privacy, policy, personal, information, service,
web, site, website, com, and please. After lemmatizing, the vocabulary contains V = 2,876 words.
We further exclude sections and paragraphs that contain less than 10 words. Many of these are not
meaningful statements, e.g., ?return to top.? This results in 9,935 sections and 27,594 paragraphs in the
experiments.
During estimation, we concatenate all segments into a single sequence, delimited by a special bound-
ary symbol. This does not affect the outcome (due to the first-order conditions; it essentially conflates
?start? and ?stop? states), but gave some efficiency gains in our implementation.
EM or VB iterations continue until one of two stopping criteria is met: either 100 iterations have
passed, or the relative change in log-likelihood (or the variational bound in the case of VB) falls below
10
?4
; this consistently happens within forty iterations.
After estimating parameters, we decode using the Viterbi algorithm.
4 Experiments
Our experiments compare three methods for aligning segments of privacy policies, at both the paragraph
and the section level:
? A greedy divising clustering algorithm, as implemented in CLUTO.
12
The algorithm performs a
sequence of bisections until the desired number of clusters is reached. In each step, a cluster is
selected and partitioned so as to optimize the clustering criterion. CLUTO demonstrated robust
performance in several related NLP tasks (Zhong and Ghosh, 2005; Lin and Bilmes, 2011; Chen et
al., 2011).
? The Viterbi state assignment from the EM-estimated HMM. We report averaged results over ten
runs, with random initialization.
? The Viterbi state assignment after VB inference, using the mean field parameters. We report aver-
aged results over ten runs, with random initialization.
Our evaluation metrics are precision, recall, and F -score on the identification of section or paragraph
pairs annotated ?yes.?
4.1 Results
In Figure 1, we present performance of different algorithms using a range of hidden state values K ?
{1, 2, . . . , 20}. The top row shows precision, recall and F -scores on section pairs, the bottom row on
paragraph pairs.
The algorithms mostly perform similarly. At the section level, we find the clustering algorithm to
perform better in terms of F -score than the HMM with larger K; at K = 10 the two are very close.
13
CLUTO?s best performance, 85%, was achieved by K = 14.
At the paragraph level, the HMMs outperform clustering in the K ? [5, 15) range, and this is where
the peak F -score is obtained (87%). We do not believe these differences among algorithms are espe-
cially important, noting only that the HMM?s advantage is that it does not require pairwise similarity
calculations between all section pairs.
890
Figure 1: Performance results against pairwise annotations when using different number of hidden states
K ? {1, . . . , 20}. The top row is at the section level, the bottom row at the paragraph level.
4.2 Upper Bounds
How do these automatic alignment methods compare with the levels of agreement reached among crowd-
workers? We consider the agreement rate of each method, at varying values of K, with the majority vote
of the annotators. Note that this is distinct from the positive-match?focused precision, recall, and F -
score measures presented in ?4.1. For each crowdworker who completed ten tasks or more, and therefore
for whom we have hope of a reliable estimate, we calculated her agreement rate with the majority. For
sections, this set included 65 out of 162 crowdworkers; for paragraphs, 76 out of 197.
In Figure 2 we show the three quartile points for this agreement measure, across the pool of ten-or-
more-item crowdworkers, in comparison to the various automatic methods. For sections, our systems
perform on par with the 25% of crowdworkers just below the median. For paragraphs, which show a
generally higher level of agreement among this subset of crowdworkers, our systems are on par with the
lowest 25% of workers. We take all of this to suggest that there is room for improvement in methods
overall.
Given the observation in ?2 that cosine similarity of two segments? tfidf vectors is a very strong pre-
dictor of human agreement on whether they are about the same issue, we also consider a threshold on
cosine similarity for deciding whether a pair is about the same issue. This is not a complete solution to
the problem of alignment, since pairwise scores only provide groupings if they are coupled with a tran-
sitivity constraint. The clustering and HMM methods can be understood as greedy approximations to
such an approach. We therefore view cosine similarity thresholding as an upper bound for bag of words
representations on the pairwise evaluation task. Figure 2 includes agreement levels for oracle cosine
similarity thresholding.
14
11
http://www.lextek.com/manuals/onix/stopwords1.html
12
http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
13
Ramanath et al. (2014) only considered K = 10 and found a K = 10 HMM to outperform clustering at the section level;
the scores reported there, on the earlier dataset, are much lower and not comparable to those reported here. There are numerous
differences between the setup here and the earlier one. The most important, we believe, are the improved quality of the dataset
and greater care given to preprocessing, most notably the pruning of documents and vocabulary, in the present experiments.
14
For comparison with the results in ?4.1, we found that, for sections, oracle thresholding (at 0.3) achieved F -score of 0.87,
and for paragraphs, oracle thresholding (at 0.2) achieved 0.90.
891
Figure 2: Agreement rates, as compared to crowdworkers and a cosine similarity oracle.
Taken together, this analysis suggests that?in principle?an automated approach based on word-level
similarity could close about half of the gap between our methods and median crowdworkers, and further
gains would require more sophisticated representations or similarity measures.
5 Related Work
There has been little work on applying NLP to privacy policies. Some have sought to parse privacy
policies into machine-readable representations (Brodie et al., 2006) or extract sub-policies from larger
documents (Xiao et al., 2012). Machine learning has been applied to assess certain attributes of policies
(Costante et al., 2012; Costante et al., 2013), e.g., compliance of privacy policies to legal regulations
(Krachina et al., 2007) or simple categorical questions about privacy policies (Ammar et al., 2012; Zim-
meck and Bellovin, 2014).
Our alignment-style analysis is motivated by an expectation that many policies will address similar
issues,
15
such as collection of a user?s contact, location, health, and financial information, sharing with
third parties, and deletion of data. This expectation is supported by recommendation by privacy experts
(Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector,
the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al.
(2013) describe our larger research initiative to incorporate automation into privacy policy analysis.
Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS
tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson,
2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques,
and future exploration might consider their use in automatically discovering document sections (Eisen-
stein and Barzilay, 2008), rather than fixing section or paragraph boundaries.
6 Conclusion
This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies.
We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an explo-
ration of clustering and HMM-based alignment methods. Our results show that these algorithms achieve
agreement on par with the lower half of crowdworkers, with about half of the difference from the median
due to the bag of words representation and half due to the inherent greediness of the methods.
Acknowledgments
The authors gratefully acknowledge helpful comments from Lorrie Cranor, Joel Reidenberg, Florian
Schaub, and several anonymous reviewers. This research was supported by NSF grant SaTC-1330596.
15
Personal communication, Joel Reidenberg.
892
References
Waleed Ammar, Shomir Wilson, Norman Sadeh, and Noah A. Smith. 2012. Automatic categorization of privacy
policies: A pilot study. Technical Report CMU-LTI-12-019, Carnegie Mellon University.
Matthew J. Beal. 2003. Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, Gatsby Com-
putational Neuroscience unit, University College London.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine
Learning Research.
Travis D. Breaux, Hanan Hibshi, and Ashwini Rao. 2014. Eddy, A formal language for specifying and analyzing
data flow specifications for conflicting privacy requirements. Requirements Engineering Journal.
Carolyn A. Brodie, Clare-Marie Karat, and John Karat. 2006. An empirical study of natural language parsing
of privacy policy rules using the SPARCLE policy workbench. In Proceedings of the Second Symposium on
Usable Privacy and Security (SOUPS).
Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay. 2011. In-domain relation discovery with meta-
constraints via posterior regularization. In Proceedings of ACL-HLT.
Elisa Costante, Yuanhao Sun, Milan Petkovi?c, and Jerry den Hartog. 2012. A machine learning solution to assess
privacy policy completeness. In Proceedings of the ACM Workshop on Privacy in the Electronic Society.
Elisa Costante, Jerry Hartog, and Milan Petkovi. 2013. What websites know about you. In Roberto Pietro, Javier
Herranz, Ernesto Damiani, and Radu State, editors, Data Privacy Management and Autonomous Spontaneous
Security, volume 7731 of Lecture Notes in Computer Science, pages 146?159. Springer Berlin Heidelberg.
Lorrie Faith Cranor. 2002. Web Privacy with P3P. O?Reilly & Associates.
Richard Durbin, Sean R. Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis:
Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of ACL.
Federal Trade Commission. 2012. Protecting consumer privacy in an era of rapid change: Recom-
mendations for businesses and policymakers. Available at http://www.ftc.gov/reports/
protecting-consumer-privacy-era-rapid-change-recommendations-businesses-policymakers.
Jianfeng Gao and Mark Johnson. 2008. A comparison of Bayesian estimators for unsupervised Hidden Markov
model POS taggers. In Proceedings of EMNLP.
Robert Gellman. 2014. Fair information practices: a basic history (v. 2.11). Available at http://www.
bobgellman.com/rg-docs/rg-FIPShistory.pdf.
Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of ACL.
Mark Johnson. 2007. Why doesnt EM find good HMM POS-taggers? In Proceedings of EMNLP?CoNLL.
Patrick Gage Kelley, Lucian Cesca, Joanna Bresee, and Lorrie Faith Cranor. 2010. Standardizing privacy notices:
An online study of the nutrition label approach. In Proceedings of CHI.
Olga Krachina, Victor Raskin, and Katrina Triezenberg. 2007. Reconciling privacy policies and regulations:
Ontological semantics perspective. In Michael J. Smith and Gavriel Salvendy, editors, Human Interface and the
Management of Information. Interacting in Information Environments, pages 730?739. Springer.
Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of
ACL.
Jialiu Lin, Shahriyar Amini, Jason I. Hong, Norman Sadeh, Janne Lindqvist, and Joy Zhang. 2012. Expectation
and purpose: Understanding users? mental models of mobile app privacy through crowdsourcing. In Proceed-
ings of ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp).
Bin Liu, Jialiu Lin, and Norman Sadeh. 2014. Reconciling mobile app privacy and usability on smartphones:
Could user privacy profiles help? In Proceedings of WWW.
Aleecia M. McDonald and Lorrie Faith Cranor. 2008. The cost of reading privacy policies. I/S: A Journal of Law
and Policy for the Information Society.
893
Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155?
171.
Rohan Ramanath, Fei Liu, Norman Sadeh, and Noah A. Smith. 2014. Unsupervised alignment of privacy policies
using hidden Markov models. In Proceedings of ACL.
Norman Sadeh, Alessandro Acquisti, Travis Breaux, Lorrie Cranor, Aleecia McDonald, Joel Reidenberg, Noah
Smith, Fei Liu, Cameron Russel, Florian Schaub, and Shomir Wilson. 2013. The usable privacy policy project:
Combining crowdsourcing, machine learning and natural language processing to semi-automatically answer
those privacy questions users care about. Technical Report CMU-ISR-13-119, Carnegie Mellon University.
Xusheng Xiao, Amit Paradkar, Suresh Thummalapenta, and Tao Xie. 2012. Automated extraction of security
policies from natural-language software documents. In Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering.
Shi Zhong and Joydeep Ghosh. 2005. Generative model-based document clustering: a comparative study. Knowl-
edge and Information Systems, 8(3):374?384.
Sebastian Zimmeck and Steven M. Bellovin. 2014. Privee: An architecture for automatically analyzing web
privacy policies. In Proceedings of the 23rd USENIX Security Symposium.
894
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1713?1722,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing
Reveals Turker Biases in Query Segmentation
Rohan Ramanath?
R. V. College of Engineering
Bangalore, India
ronramanath@gmail.com
Monojit Choudhury
Microsoft Research Lab India
Bangalore, India
monojitc@microsoft.com
Kalika Bali
Microsoft Research Lab India
Bangalore, India
kalikab@microsoft.com
Rishiraj Saha Roy?
Indian Institute of Technology Kharagpur
Kharagpur, India
rishiraj@cse.iitkgp.ernet.in
Abstract
Query segmentation, like text chunking,
is the first step towards query understand-
ing. In this study, we explore the effec-
tiveness of crowdsourcing for this task.
Through carefully designed control ex-
periments and Inter Annotator Agreement
metrics for analysis of experimental data,
we show that crowdsourcing may not be a
suitable approach for query segmentation
because the crowd seems to have a very
strong bias towards dividing the query into
roughly equal (often only two) parts. Sim-
ilarly, in the case of hierarchical or nested
segmentation, turkers have a strong prefer-
ence towards balanced binary trees.
1 Introduction
Text chunking of Natural Language (NL) sentences
is a well studied problem that is an essential pre-
processing step for many NLP applications (Ab-
ney, 1991; Abney, 1995). In the context of Web
search queries, query segmentation is similarly the
first step towards analysis and understanding of
queries (Hagen et al, 2011). The task in both the
cases is to divide the sentence or the query into
contiguous segments or chunks of words such that
the words from a segment are related to each other
more strongly than words from different segments
(Bendersky et al, 2009). It is typically assumed
that the segments are structurally and semantically
coherent and, therefore, the information contained
in them can be processed holistically.
?The work was done during author?s internship at Mi-
crosoft Research Lab India.
? This author was supported by Microsoft Corporation
and Microsoft Research India under the Microsoft Research
India PhD Fellowship Award.
f Pipe representation Boundary var.
4 apply | first aid course | on line 1 0 0 1 0
3 apply first aid course | on line 0 0 0 1 0
2 apply first aid | course on line 0 0 1 0 0
1 apply | first aid | course | on line 1 0 1 1 0
Table 1: Example of flat segmentation by Turkers.
f is the frequency of annotations; segment bound-
aries are represented by |.
f Bracket representation Boundary var.
4 ((apply first) ((aid course) (on line))) 0 2 0 1 0
2 (((apply (first aid)) course) (on line)) 1 0 2 3 0
2 ((apply ((first aid) course)) (on line)) 2 0 1 3 0
1 (apply (((first aid) course) (on line))) 3 0 1 2 0
1 ((apply (first aid)) (course (on line))) 1 0 2 1 0
Table 2: Example of nested segmentation by Turk-
ers. f is the frequency of annotations.
A majority of work on query segmentation re-
lies on manually segmented queries by human ex-
perts for training and evaluation of segmentation
algorithms. These are typically small datasets and
even with detailed annotation guidelines and/or
close supervision, low Inter Annotator Agreement
(IAA) remains an issue. For instance, Table 1 il-
lustrates the variation in flat segmentation by 10
annotators. This confusion is mainly because the
definition of a segment in a query is ambiguous
and of an unspecified granularity. This is fur-
ther compounded by the fact that other than eas-
ily recognizable and agreed upon segments such as
Named Entities or Multi-Word Expressions, there
is no established notion of linguistic grouping such
as phrases and clauses in a query.
Although there is little work on the use of
crowdsourcing for query segmentation (Hagen et
al., 2011; Hagen et al, 2012), the idea that the
1713
crowd could be a potential (and cheaper) source
for reliable segmentation seems a reasonable as-
sumption. The need for larger datasets makes this
an attractive proposition. Also, a larger number
of annotations could be appropriately distilled to
obtain better quality segmentations.
In this paper we explore crowdsourcing as an
option for query segmentation through experi-
ments designed using Amazon Mechanical Turk
(AMT)1. We compare the results against gold
datasets created by trained annotators. We ad-
dress the issues pertaining to disagreements due to
both ambiguity and granularity and attempt to ob-
jectively quantify their role in IAA. To this end,
we also conduct similar annotation experiments
for NL sentences and randomly generated queries.
While queries are not as structured as NL sen-
tences they are not simply a set of random words.
Thus, it is necessary to compare query segmenta-
tion to the u?ber-structure of NL sentences as well
as the unter-structure of random n-grams. This has
important implications for understanding any in-
herent biases annotators may have as a result of
the apparent lack of structure of the queries.
To quantify the effect of granularity on segmen-
tation, we also ask annotators to provide hierar-
chical or nested segmentations for real and ran-
dom queries, as well as sentences. Following
Abney?s (1992) proposal for hierarchical chunk-
ing of NL, we ask the annotators to group ex-
actly two words or segments at a time to recur-
sively form bigger segments. The concept is illus-
trated in Fig. 1. Table 2 shows annotations from
10 Turkers. It is important to constrain the join-
ing of exactly two segments or words at a time
to avoid the issue of fuzziness in granularity. We
shall refer to this style of annotation as Nested
segmentation, whereas the non-hierarchical non-
constrained chunking will be referred to as Flat
segmentation.
Through statistical analysis of the experimen-
tal data we show that crowdsourcing may not be
the best practice for query segmentation, not only
because of ambiguity and granularity issues, but
because there exist very strong biases amongst an-
notators to divide a query into two roughly equal
parts that result in misleadingly high agreements.
As a part of our analysis framework, we introduce
a new IAA metric for comparison across flat and
nested segmentations. This versatile metric can be
1https://www.mturk.com/mturk/welcome
3
2
1
apply 0
first aid
course
0
on line
Figure 1: Nested Segmentation: Illustration.
readily adapted for measuring IAA for other lin-
guistic annotation tasks, especially when done us-
ing crowdsourcing.
The rest of the paper is organized as follows.
Sec 2 provides a brief overview of related work.
Sec 3 describes the experiment design and proce-
dure. In Sec 4, we introduce a new metric for IAA,
that could be uniformly applied across flat and
nested segmentations. Results of the annotation
experiments are reported in Sec 5. In Sec 6, we an-
alyze the possible statistical and linguistic biases
in annotation. Sec 7 concludes the paper by sum-
marizing the work and discussing future research
directions. All the annotated datasets used in this
research are freely available for non-commercial
research purposes2.
2 Related Work
Query segmentation was introduced by Risvik et.
al. (2003) as a possible means to improve Informa-
tion Retrieval. Since then there has been a signif-
icant amount of research exploring various algo-
rithms for this task and its use in IR (see Hagen et.
al. (2011) for a survey). Most of the research and
evaluation considers query segmentation as a pro-
cess analogous to identification of phrases within
a query which when put within double-quotes (im-
plying exact matching of the quoted phrase in the
document) leads to better IR performance. How-
ever, this is a very restricted view of the process
and does not take into account the full potential of
query segmentation.
A more generic notion of segments leads to di-
verse and ambiguous definitions, making its eval-
uation a hard problem (see Saha Roy et. al. (2012)
for a discussion on issues with evaluation). Most
automatic segmentation techniques (Bergsma and
Wang, 2007; Tan and Peng, 2008; Zhang et al,
2Related datasets and supplementary material can be ac-
cessed from http://bit.ly/161Gkk9 or can be ob-
tained by directly emailing the authors.
1714
2009; Brenes et al, 2010; Hagen et al, 2011; Li et
al., 2011) have so far been evaluated only against
a small set of human-annotated queries (Bergsma
and Wang, 2007). The reported low IAA for such
datasets casts serious doubts on the reliability of
annotation and the performance of the algorithms
evaluated on them (Hagen et al, 2011; Saha Roy
et al, 2012).
To address the problem of data scarcity, Ha-
gen et. al. (2011) have created larger annotated
datasets through crowdsourcing3. However, in
their approach the crowd is provided with a few
(four) possible segmentations of a query to choose
from (known through a personal communication
with a authors). Thus, it presupposes an automatic
process that can generate the correct segmentation
of a query within top few options. It is far from
obvious how to generate these initial segmenta-
tions in a reliable manner. This may also result
in an over-optimistic IAA. An ideal segmentation
should be based on the annotators? own interpreta-
tion of the query. Nevertheless, if large scale data
has to be procured, crowdsourcing seems to be the
only efficient and effective model for this task, and
has been proven to be so for other IR and linguistic
annotations; see Carvalho et al (2011) for exam-
ples of crowdsourcing for IR resources and (Snow
et al, 2008; Callison-Burch, 2009) for language
resources.
In the context of NL text, segmentation has
been traditionally referred to as chunking and is
a well-studied problem. Abney (1991; 1992;
1995) defines a chunk as a sub-tree within a
syntactic phrase structure tree corresponding to
Noun, Prepositional, Adjectival, Adverbial and
Verb Phrases. Similarly, Bharati et al(1995) de-
fines it as Noun Group and Verb Group based only
on local surface information. However, cognitive
and annotation experiments for chunking of En-
glish (Abney, 1992) and other language text (Bali
et al, 2009) have shown that native speakers agree
on major clause and phrase boundaries, but may
not do so on more fine-grained chunks. One im-
portant implication of this is that annotators are
expected to agree more on the higher level bound-
aries for nested segmentation than the lower ones.
We note that hierarchical query segmentation was
proposed for the first time by Huang et al (2010),
where the authors recursively split a query (or its
fragment) into exactly two parts and evaluate the
3http://www.webis.de/research/corpora
final output against human annotations.
3 Experiments
The annotation experiments have been designed to
systematically study the various aspects of query
segmentation. In order to verify the effective-
ness and reliability of crowdsourcing, we designed
an AMT experiment for flat segmentation of Web
search queries. As a baseline, we would like to
compare these annotations with those from hu-
man experts trained for the task. We shall refer
to this baseline as the Gold annotation set. Since
we believe that the issue of granularity could be
the prime reason for previously reported low IAA
for segmentation, we also designed AMT-based
nested segmentation experiments for the same set
of queries, and obtained the corresponding gold
annotations.
Finally, to estimate the role of ambiguity inher-
ent in the structure of Web search queries on IAA,
we conducted two more control experiments, both
through crowdsourcing. First, flat and nested seg-
mentation of well-formed English, i.e., NL sen-
tences of similar length distribution; and second,
flat and nested segmentation of randomly gener-
ated queries. Higher IAA for NL sentences would
lead us to conclude that ambiguity and lack of
structure in queries is the main reason for low
agreements. On the other hand high or comparable
IAA for random queries would mean that annota-
tions have strong biases.
Thus, we have the following four pairs of anno-
tation experiments: flat and nested segmentation
of queries from crowdsourcing, corresponding flat
and nested gold annotations, flat and nested seg-
mentation of English sentences from crowdsourc-
ing, and flat and nested segmentations for ran-
domly generated queries through crowdsourcing.
3.1 Dataset
For our experiments, we need a set of Web search
queries and well-formed English sentences. Fur-
thermore, for generating the random queries, we
will use search query logs to learn n-gram mod-
els. In particular, we use the following datasets:
Q500, QG500: Saha Roy et al (2012) re-
leased a dataset of 500 queries, 5 to 8 words long,
for evaluation of various segmentation algorithms.
This dataset has flat segmentations from three an-
notators obtained under controlled experimental
settings, and can be considered as Gold annota-
1715
Figure 2: Length distribution of datasets.
tions. Hence, we select this set for our experiments
as well. We procured the corresponding nested
segmentation for these queries from two human
experts, who are regular search engine users, be-
tween 20 and 30 years old, and familiar with var-
ious linguistic annotation tasks. They annotated
the data under supervision. They were trained and
paid for the task. We shall refer to the set of flat
and nested gold annotations as QG500, whereas
Q500 will be reserved for AMT experiments.
Q700: Since 500 queries may not be enough
for reliable conclusion and since the queries may
not have been chosen specifically for the purpose
of annotation experiments, we expanded the set
with another 700 queries sampled from a slice of
the query logs of Bing Australia4 containing 16.7
million queries issued over a period of one month
(May 2010). We picked, uniformly at random,
queries that are 4 to 8 words long, have only En-
glish letters and numerals, and a high click entropy
because ?a query with a larger click entropy value
is more likely to be an informational or ambiguous
query? (Dou et al, 2008). Q500 consists of tail-
ish queries with frequency between 5 and 15 that
have at least one multiword named entity; but un-
like the case of Q700, click-entropy was not con-
sidered during sampling. As we shall see, this dif-
ference is clearly reflected in the results.
S300: We randomly selected 300 English sen-
tences from a collection of full texts of public do-
main books5 that were 5 to 15 words long, and
checked them for well-formedness. This set will
be referred to as S300.
QRand: Instead of generating search queries
by throwing in words randomly, we thought it
will be more interesting to explore annotation of
4http://www.bing.com/?cc=au
5http://www.gutenberg.org
Parameter Flat Details Nested Details
Time needed: actual (allotted) 49 sec (10 min) 1 min 52 sec (15 min)
Reward per HIT $0.02 $0.06
Instruction video duration 26 sec 1 min 40 sec
Turker qualification Completion rate >100 tasks
Turker approval rate Acceptance rate >60 %
Turker location United States of America
Table 3: Specifics of the HITs for AMT.
queries generated using n-gram models for n =
1, 2, 3. We estimated the models from the Bing
Australia log of 16.7 million queries. We gener-
ated 250 queries each of desired length distribu-
tion using the 1, 2 and 3-gram models. We shall
refer to these as U250, B250, T250 (for Uni, Bi
and Trigram) respectively, and the whole dataset
as QRand. Fig. 2 shows the query and sentence
length distribution for the various sets.
3.2 Crowdsourcing Experiments
We used AMT to get our annotations through
crowdsourcing. Pilot experiments were carried out
to test the instruction set and examples presented.
Based on the feedback, the precise instructions for
the final experiments were designed.
Two separate AMT Human Intelligence Tasks
(HITs) were designed for flat and nested query
segmentation. Also, the experiments for queries
(Q500+Q700) were conducted separately from
S300 and QRand. Thus, we had six HITs in
all. The concept of flat and nested segmentation
was introduced to the Turkers with the help of ex-
amples presented in two short videos6. When in
doubt regarding the meaning of a query, the Turk-
ers were advised to issue the query on a search
engine of their choice and find out its possible
interpretation(s). Note that we intentionally kept
definitions of flat and nested segmentation fuzzy
because (a) it would require very long instruction
manuals to cover all possible cases and (b) Turkers
do not tend to read verbose and complex instruc-
tions. Table 3 summarizes other specifics of HITs.
Honey pots or trap questions whose answers are
known a priori are often included in a HIT to iden-
tify turkers who are unable to solve the task ap-
propriately leading to incorrect annotations. How-
ever, this trick cannot be employed in our case be-
cause there is no notion of an absolutely correct
segmentation. We observe that even with unam-
biguous queries, even expert annotators may dis-
6Flat: http://youtu.be/eMeLjJIvIh0, Nested:
http://youtu.be/xE3rwANbFvU
1716
agree on some of the segment boundaries. Hence,
we decided to include annotations from all the
turkers, except for those that were syntactically ill-
formed (e.g., non-binary nested segmentation).
4 Inter Annotator Agreement
Inter Annotator Agreement is the only way to
judge the reliability of annotated data in absence
of an end application. Therefore, before we can
venture into analysis of the experimental data, we
need to formalize the notion of IAA for flat and
nested queries. The task is non-trivial for two
reasons. First, traditional IAA measures are de-
fined for a fixed set of annotators. However, for
crowdsourcing based annotations, different anno-
tators might have annotated different parts of the
dataset. For instance, we observed that a total
of 128 turkers have provided the flat annotations
for Q700, when we had only asked for 10 anno-
tations per query. Thus, on average, a turker has
annotated only 7.81% of the 700 queries. In fact,
we found that 31 turkers had annotated less than
5 queries. Hence, measures such as Cohen?s ?
(1960) cannot be directly applied in this context
because for crowdsourced annotations, we cannot
meaningfully compute annotator-specific distribu-
tion of the labels and biases.
Second, most of the standard annotation metrics
do not generalize for flat segmentation and trees.
Artstein and Poesio (2008) provides a comprehen-
sive survey of the IAA metrics and their usage in
NLP. They note that all the metrics assume that
a fixed set of labels are used for items. There-
fore, it is far from obvious how to compare chunk-
ing or segmentation that covers the whole text or
that might have overlapping units as in the case of
nested segmentation. Furthermore, we would like
to compare the reliability of flat and nested seg-
mentation, and therefore, ideally we would like to
have an IAA metric that can be meaningfully ap-
plied to both of these cases.
After considering various measures, we decided
to appropriately generalize one of the most versa-
tile and effective IAA metrics proposed till date,
the Kripendorff?s ? (2004). To be consistent with
prior work, we will stick to the notation used
in Artstein and Poesio (2008) and redefine the
? in the context of flat and nested segmentation.
Note that though the notations introduced here will
be from the perspective of queries, it is equally
applicable to sentences and the generalization is
straightforward.
4.1 Notations and Definitions
Let Q be the set of all queries with cardinality q.
A query q ? Q can be represented as a sequence of
|q| words: w1w2 . . . w|q|. We introduce |q?1| ran-
dom variables, b1, b2, . . . b|q|?1, such that bi rep-
resents the boundary between the words wi and
wi+1. A flat or nested segmentation of q, repre-
sented by qj , j varying from 1 to total number of
annotations c, is a particular instantiation of these
boundary variables as described below.
Definition. A flat segmentation, qj can be
uniquely defined by a binary assignment of the
boundary variables bj,i, where bj,i = 1 iff wi and
wi+1 belong to two different flat segments. Oth-
erwise, bj,i = 0. Thus, q has 2|q|?1 possible flat
segmentations.
Definition. A nested segmentation qj can also
be uniquely defined by assigning non-negative in-
tegers to the boundary variables such that bj,i = 0
iff words wi and wi+1 form an atomic segment
(i.e., they are grouped together), else bj,i = 1 +
max(lefti, righti), where lefti and righti are
the heights of the largest subtrees ending at wi and
beginning at wi+1 respectively.
This numbering scheme for nested segmenta-
tion can be understood through Fig. 1. Every in-
ternal node of the binary tree corresponding to the
nested segmentation is numbered according to its
height. The lowest internal nodes, both of whose
children are query words, are assigned a value of
0. Other internal nodes get a value of one greater
than the height of its higher child. Since every in-
ternal node corresponds to a boundary, we assign
the height of the node to the corresponding bound-
aries. The number of unique nested segmentations
of a query of length |q| is its corresponding Cata-
lan number7.
Boundary variables for flat and nested segmen-
tation are illustrated with an example of each kind
in Tables 1 and 2 (last column).
4.2 Krippendorff ?s ? for Segmentation
Krippendorff ?s ? (Krippendorff, 2004) is an ex-
tremely versatile agreement coefficient, which is
based on the assumption that the expected agree-
ment is calculated by looking at the overall distri-
bution of judgments without regard to which anno-
tator produced them (Artstein and Poesio, 2008).
7http://goo.gl/vKQvK
1717
Hence, it is appropriate for crowdsourced annota-
tion, where the judgments come from a large num-
ber of unrelated annotators. Moreover, it allows
for different magnitudes of disagreement, which
is a useful feature as we might want to differen-
tially penalize disagreements at various levels of
the tree for nested segmentation.
? is defined as
? = 1? DoDe
= 1? s
2
within
s2total
(1)
where Do and De are, respectively, the observed
and expected disagreements that are measured by
s2within ? the variance within the annotation of an
item and s2total ? variance across annotations of
all items. We adapt the equations presented in
pp.565-566 of Artstein and Poesio (2008) for mea-
suring these quantities for queries:
s2within =
1
2qc(c? 1)
?
q?Q
c?
m=1
c?
n=1
d(qm, qn)
(2)
s2total =
1
2qc(qc? 1)
?
q?Q
c?
m=1
?
q??Q
c?
n=1
d(qm, q?n)
(3)
where, d(qm, q?n) is a distance metric for the agree-
ment between annotations qm and q?n.
We define two different distance metrics d1 and
d2 that are applicable to flat and nested segmenta-
tion. We shall first define these metrics for com-
paring queries with equal length (i.e., |q| = |q?|):
d1(qm, q?n) =
1
|q| ? 1
|q|?1?
i=1
|bm,i ? b?n,i| (4)
d2(qm, q?n) =
1
|q| ? 1
|q|?1?
i=1
|b2m,i ? (b?n,i)2| (5)
While d1 penalizes all disagreements equally, d2
penalizes disagreements higher up the tree more.
d2 might be a desirable metric for nested seg-
mentation, because research on sentence chunk-
ing shows that annotators agree more on clause or
major phrase boundaries, even though they may
not always agree on intra-clausal or intra-phrasal
boundaries (Bali et al, 2009). Note that for flat
segmentation, d1 and d2 are identical, and hence
we will denote them as d.
We propose the following extension to these
metrics for queries of unequal lengths. Without
loss of generality, let us assume that |q| < |q?|. k
is 1 or 2; r = |q?| ? |q|+ 1.
dk(qm, q?n) =
1
r(|q| ? 1)
r?1?
a=0
|q|?1?
i=1
|bkm,i ? (b?n,i+a)k| (6)
4.3 IAA under Random Bias Assumption
Krippendorff?s ? uses the cross-item variance as
an estimate of chance agreement, which is reli-
able in general. However, this might result in mis-
leadingly low values of IAA, especially when the
items in the set are indeed expected to have sim-
ilar annotations. To resolve this, we also com-
pute the chance agreement under a random bias
model. The random model assumes that all the
structural annotations of q are equiprobable. For
flat segmentation, it boils down to the fact that
all the 2|q|?1 annotations are equally likely, which
is equivalent to the assumption that any boundary
variable bi has 0.5 probability of being 0 and 0.5
for 1.
Analytical computation of the expected proba-
bility distributions of d1(qm, qn) and d2(qm, qn)
is harder for nested segmentation. Therefore, we
programmatically generate all possible trees for q,
which is again dependent only on |q| and com-
pute d1 and d2 between all pairs of trees, from
which the expected distributions can be readily
estimated. Let us denote this expected cumula-
tive probability distribution for flat segmentation
as Pd(x; |q|) = the probability that for a pair
of randomly chosen flat segmentations of q, qm
and qn, d(qm, qn) ? x. Likewise, let Pd1(x; |q|)
and Pd2(x; |q|) be the respective probabilities that
for any two nested segmentations qm and qn of
q, the following holds: d1(qm, qn) ? x and
d2(qm, qn) ? x.
We define the IAA under random bias model as
(k is 1, 2 or null):
S = 1qc2
?
q?Q
c?
m=1
c?
n=1
Pdk(dk(qm, qn); |q|) (7)
Thus, S is the expected probability of observing a
similar or worse agreement by random chance, av-
eraged over all pairs of annotations for all queries,
and not a chance corrected IAA metric such as
?. Thus, S = 1 implies that the observed agree-
ment is almost always better than that by random
chance and S = 0.5 and 0 respectively imply that
the observed agreement is as good as and almost
always worse than that by random chance. We
1718
Dataset Flat Nested
d1 d1 d2
Q700 0.21(0.59) 0.21(0.89) 0.16(0.68)
Q500 0.22(0.62) 0.15(0.70) 0.15(0.44)
QG500 0.61(0.88) 0.66(0.88) 0.67(0.80)
S300 0.27(0.74) 0.18(0.94) 0.14(0.75)
U250 0.23(0.89) 0.42(0.90) 0.30(0.78)
B250 0.22(0.86) 0.34(0.88) 0.22(0.71)
T250 0.20(0.86) 0.44(0.89) 0.34(0.76)
Table 4: Agreement Statistics: ?(S).
also note that a high value of S and low value
of ? indicate that though the annotators agree on
the judgment of individual items, they also tend to
agree on judgments of two different items, which
in turn, could be due to strong annotator biases or
due to lack of variability of the dataset.
In the supplementary material, computations of
? and S have been explained in further details
through worked out examples. Tables for the ex-
pected distributions of d, d1 and d2 under the ran-
dom annotation assumption are also available.
5 Results
Table 4 reports the values of ? and S for flat
and nested segmentation on the various datasets.
For nested segmentation, the values were com-
puted for two different distance metrics d1 and
d2. As expected, the highest value of ? for both
flat and nested segmentation is observed for gold
annotations. An ? > 0.6 indicates quite good
IAA, and thus, reliable annotations. Higher ? for
nested segmentation QG500 than flat further vali-
dates our initial postulate that nested segmentation
may reduce disagreement from granularity issues
inherent in the definition of flat segmentation.
Opposite trends are observed for Q700, Q500
and S300, where ? for flat is the highest, followed
by that for nested using d1, and then d2. More-
over, except for flat segmentation of sentences, ?
lies between 0.14 and 0.22, which is quite low.
This clearly shows that segmentation, either flat
or nested, cannot be reliably procured through
crowdsourcing. Lower ? for d2 than d1 further
indicates that annotators disagree more for higher
levels of the trees, contrary to what we had ex-
pected. However, nearly equal IAA for sentences
and queries implies that low agreement may not be
an outcome of inherent ambiguity in the structure
of queries. Slightly higher ? for flat segmentation
and a much higher ? for nested segmentation of
QRand reinforce the fact that low IAA is not due
to a lack of structure in queries.
It is interesting to note that ? for nested segmen-
tation of S300 and all segmentations of QRand
are low or medium despite the fact that S is very
high in all these cases. Thus, it is clear that an-
notators have a strong bias towards certain struc-
tures across queries. In the next section, we will
analyze some of these biases. We also computed
the IAA between QG500 and Q500, and found
? = 0.27. This is much lower than ? for QG500,
though slightly higher than that for Q500. We did
not observe any significant variation in agreement
with respect to the length of the queries.
6 Biases in Annotation
The IAA statistics clearly show that there are cer-
tain strong biases in both flat and nested query
segmentation, especially those obtained through
crowdsourcing. To identify these biases, we went
through the annotations and came up with possi-
ble hypotheses, which we tried to verify through
statistical analysis of the data. Here, we report the
most prominent biases that were thus discovered.
Bias 1: During flat segmentation, annotators pre-
fer dividing the query into two segments of roughly
equal length.
As discussed earlier, one of the major problems
of flat segmentation is the fuzziness in granularity.
In our experiments, we intentionally left the de-
cision of whether to go for fine or coarse-grained
segmentation to the annotator. However, it is sur-
prising to observe that annotators typically divide
the query into two segments (see Fig. 3, plots A1
and A2), and at times three, but hardly ever more
than three. This bias is observed across queries,
sentences and random queries, where the percent-
age of annotations with 2 or 3 segments are greater
than 83%, 91% and 96% respectively. This bias
is most strongly visible for QRand because the
lack of syntactic or semantic cohesion between the
words provides no clue for segmentation.
Furthermore, we observe that typically seg-
ments tend to be of equal length. For this, we com-
puted standard deviations (sd) of segment lengths
for all annotations having 2 or 3 segments; the dis-
tribution of sd is shown in Fig. 3, plots B1 and B2.
We observe that for all datasets, sd lies mainly be-
tween 0.5 and 1 (for perspective, consider a query
1719
Figure 3: Analysis of annotation biases: A1, A2 ? number of segments per flat segmentation vs. length;
B1, B2 ? standard deviation of segment length for flat segmentation; C1, C2 ? distribution of the tree
heights in nested segmentation.
Length Expected Q500 QG500 Q700 S300 QRand
5 2.57 2.00 2.02 2.08 2.02 2.01
6 3.24 2.26 2.23 2.23 2.24 2.02
7 3.88 2.70 2.71 2.67 2.55 2.62
8 4.47 2.89 2.68 2.72 2.72 2.35
Table 5: Average height for nested segmentation.
with 7 words; with two segments of length 3 and
4 the sd is 0.5, and for 2 and 5, the sd is 1.5), im-
plying that segments are roughly of equal length.
It is likely that due to this bias, the S or observed
agreement is moderately high for queries and very
high for sentences, but then it also leads to high
agreement across different queries and sentences
(i.e., high s2total) especially when they are of equal
length, which in turn brings down the value of ? ?
the true agreement after bias correction.
Bias 2: During nested segmentation, annotators
prefer balanced binary trees.
Quite analogous to bias 1, for nested segmen-
tation we observe that annotators tend to prefer
more balanced binary trees. Fig. 3 plots C1 and C2
show the distribution of the tree heights for various
cases and Table 5 reports the corresponding aver-
age height of the trees for queries and sentences
of various lengths and the the expected value of
the height if all trees were equally likely. The ob-
served heights are much lower than the expected
values clearly implying the preference of the an-
notators for more balanced trees.
Thus, the crowd seems to choose the middle
path, avoiding extremes and hence may not be a
reliable source of annotation for query segmen-
tation. It can be argued that similar biases are
also observed for gold annotations, and therefore,
probably it is the inherent structure of the queries
and sentences that lead to such biased distribution
of segmentation patterns. However, note that ? for
QG500 is much higher than all other cases, which
shows that the true agreement between gold anno-
tators is immune to such biases or skewed distri-
butions in the datasets. Furthermore, high values
of ? for QRand despite the very strong biases in
annotation shows that there perhaps is very little
choice that the annotators have while segmenting
randomly generated queries. On the other hand,
the textual coherence of the real queries and sen-
tences provide many different choices for segmen-
tation and the Turker typically gets carried away
by these biases, leading to low ?.
Bias 3: Phrase structure drives segmentation only
when reconcilable with Bias 1. Whenever the sen-
tence or query has a verb phrase (VP) spanning
roughly half of it, annotators seem to chunk be-
fore the VP as one would expect, quite as of-
ten as just after the verb, which is quite unex-
pected. For instance, the sentence A gentle
sarcasm ruffled her anger. gathers as
many as eight flat annotations with a boundary be-
tween sarcasm and ruffled, and four with
a boundary between ruffled and her. How-
ever, if the VP is very short consisting of a single
1720
Position Q500 QG500 Q700 S300 QRand
Both 2.24 0.37 2.78 2.08 0.63
None 50.34 56.85 35.74 35.84 39.81
Right 23.86 21.50 19.02 12.52 15.23
Left 18.08 15.97 40.59 45.96 21.21
Table 6: Percentages of positions of segment
boundaries with respect to prepositions. Prepo-
sitions occurring in the beginning or end of a
query/sentence have been excluded from the anal-
ysis; hence, numbers in a column do not total 100.
verb, as in A fleeting and furtive air
of triumph erupted., annotators seem to
attempt for a balanced annotation due to Bias 1.
As a clear middle boundary is not present in such
sentences, the annotations show a lot more varia-
tion and disagreement. For instance, only 1 out of
10 annotations had a boundary before erupted
in the above example. In fact, at least one anno-
tation had a boundary after each word in the sen-
tence, with no clear majority.
Bias 4: Prepositions influence segment bound-
aries differently for queries and sentences. We
automatically labeled all the prepositions in the
flat annotations and classified them according to
the criterion of whether a boundary was placed
immediately before or after it, or on both sides
or neither side. The statistics, reported in Ta-
ble 6, show that for NL sentences a majority
of the boundaries are present before the prepo-
sition, marking the beginning of a prepositional
phrase. However, for queries, a much richer pat-
tern emerges depending on the specific preposi-
tion. For instance, to, of and for are often
chunked with the previous word (e.g., how to |
choose a bike size, birthday party
ideas for | one year old). We believe
that this difference is because in sentences due
to the presence of a verb, the PP has a well-
defined head, lack of which leads to preposition
in queries getting chunked with words that form
more commonly seen patterns (e.g., flights
to and tickets for).
Bias 3 and 4 present the complex interpretation
of the structure of queries by the annotators which
could be due to some emerging cognitive model of
queries among the search engine users. This is a
fascinating and unexplored aspect of query struc-
tures that demands deeper investigation through
cognitive and psycholinguistic experiments.
7 Conclusion
We have studied various aspects of query segmen-
tation through crowdsourcing by designing and
conducting suitable experiments. Analysis of ex-
perimental data leads us to conclude the follow-
ing: (a) crowdsoucing may not be a very effective
way to collect judgments for query segmentation;
(b) addressing fuzziness of granularity for flat seg-
mentation by introducing strict binary nested seg-
ments does not lead to better agreement in crowd-
sourced annotations, though it definitely improves
the IAA for gold standard segmentations, imply-
ing that low IAA in flat segmentation among ex-
perts is primarily an effect of unspecified granular-
ity of segments; (c) low IAA is not due to the in-
herent structural ambiguity in queries as this holds
true for sentences as well; (d) there are strong bi-
ases in crowdsourced annotations, mostly because
turkers prefer more balanced segment structures;
and (e) while annotators are by and large guided
by linguistic principles, application of these prin-
ciples differ between query and NL sentences and
also closely interact with other biases.
One of the important contributions of this work
is the formulation of a new IAA metric for com-
paring across flat and nested segmentations, espe-
cially for crowdsourcing based annotations. Since
trees are commonly used across various linguistic
annotations, this metric can have wide applicabil-
ity. The metric, moreover, can be easily adapted
to other annotation schemes as well by defining an
appropriate distance metric between annotations.
Since large scale data for query segmentation is
very useful, it would be interesting to see if the
problem can be rephrased to the Turkers in a way
so as to obtain more reliable judgments. Yet a
deeper question is regarding the theoretical status
of query structure, which though in an emergent
state is definitely an operating model for the anno-
tators. Our future work in this area would specifi-
cally target understanding and formalization of the
theoretical model underpinning a query.
Acknowledgments
We thank Ed Cutrell and Andrew Cross, Microsoft
Research Lab India, for their help in setting up the
AMT experiments. We would also like to thank
Anusha Suresh, IIT Kharagpur, India, for helping
us with data preparation.
1721
References
Steven P. Abney. 1991. Parsing By Chunks. Kluwer
Academic Publishers.
Steven P. Abney. 1992. Prosodic Structure, Perfor-
mance Structure And Phrase Structure. In Proceed-
ings 5th DARPA Workshop on Speech and Natural
Language, pages 425?428. Morgan Kaufmann.
Steven P. Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
Computational Linguistics and the Foundations of
Linguistic Theory, pages 145?164.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Kalika Bali, Monojit Choudhury, Diptesh Chatterjee,
Sankalan Prasad, and Arpit Maheswari. 2009. Cor-
relates between Performance, Prosodic and Phrase
Structures in Bangla and Hindi: Insights from a Psy-
cholinguistic Experiment. In Proceedings of Inter-
national Conference on Natural Language Process-
ing, pages 101 ? 110.
Michael Bendersky, W. B. Croft, and David A. Smith.
2009. Two-stage query segmentation for informa-
tion retrieval. In Proceedings of the 32nd interna-
tional ACM Special Interest Group on Information
Retrieval (SIGIR) Conference on Research and De-
velopment in Information Retrieval, pages 810?811.
ACM.
Shane Bergsma and Qin Iris Wang. 2007. Learning
Noun Phrase Query Segmentation. In Proceedings
of Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 819?826.
Akshar Bharati, Vineet Chaitanya, Rajeev Sangal, and
KV Ramakrishnamacharyulu. 1995. Natural lan-
guage processing: a Paninian perspective. Prentice-
Hall of India New Delhi.
David J. Brenes, Daniel Gayo-Avello, and Rodrigo
Garcia. 2010. On the fly query segmentation using
snippets. In CERI ?10, pages 259?266.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?09, pages 286?295. Associa-
tion for Computational Linguistics.
Vitor R Carvalho, Matthew Lease, and Emine Yilmaz.
2011. Crowdsourcing for search evaluation. ACM
Sigir forum, 44(2):17?22.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-
Rong Wen. 2008. Are Click-through Data Adequate
for Learning Web Search Rankings? In Proceed-
ings of the 17th ACM Conference on Information
and Knowledge Management, pages 73?82. ACM.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query Segmentation
Revisited. In Proceedings of the 20th Interna-
tional Conference on World Wide Web, pages 97?
106. ACM.
Matthias Hagen, Martin Potthast, Anna Beyer, and
Benno Stein. 2012. Towards Optimum Query Seg-
mentation: In Doubt Without. In Proceedings of the
Conference on Information and Knowledge Man-
agement, pages 1015?1024.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong
Li, Kuansan Wang, Fritz Behr, and C. Lee Giles.
2010. Exploring web scale language models for
search query processing. In Proceedings of the 19th
international conference on World wide web, WWW
?10, pages 451?460, New York, NY, USA. ACM.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to its Methodology. Sage,Thousand
Oaks, CA.
Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, and
Kuansan Wang. 2011. Unsupervised query segmen-
tation using clickthrough for information retrieval.
In SIGIR ?11, pages 285?294. ACM.
Knut Magne Risvik, Tomasz Mikolajewski, and Peter
Boros. 2003. Query segmentation for web search.
In WWW (Posters).
Rishiraj Saha Roy, Niloy Ganguly, Monojit Choud-
hury, and Srivatsan Laxman. 2012. An IR-based
Evaluation Framework for Web Search Query Seg-
mentation. In Proceedings of the International ACM
Special Interest Group on Information Retrieval (SI-
GIR) Conference on Research and Development in
Information Retrieval, pages 881?890. ACM.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 254?263, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bin Tan and Fuchun Peng. 2008. Unsupervised Query
Segmentation Using Generative Language Models
and Wikipedia. In Proceedings of the 17th Inter-
national Conference on World Wide Web (WWW),
pages 347?356. ACM.
Chao Zhang, Nan Sun, Xia Hu, Tingzhu Huang, and
Tat-Seng Chua. 2009. Query segmentation based on
eigenspace similarity. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 185?188, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1722
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 605?610,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Unsupervised Alignment of Privacy Policies using Hidden Markov Models
Rohan Ramanath Fei Liu Norman Sadeh Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{rrohan,feiliu,sadeh,nasmith}@cs.cmu.edu
Abstract
To support empirical study of online pri-
vacy policies, as well as tools for users
with privacy concerns, we consider the
problem of aligning sections of a thousand
policy documents, based on the issues they
address. We apply an unsupervised HMM;
in two new (and reusable) evaluations, we
find the approach more effective than clus-
tering and topic models.
1 Introduction
Privacy policy documents are verbose, often eso-
teric legal documents that many people encounter
as clients of companies that provide services on
the web. McDonald and Cranor (2008) showed
that, if users were to read the privacy policies of
every website they access during the course of a
year, they would end up spending a substantial
amount of their time doing just that and would
often still not be able to answer basic questions
about what these policies really say. Unsurpris-
ingly, many people do not read them (Federal
Trade Commission, 2012).
Such policies therefore offer an excellent op-
portunity for NLP tools that summarize or ex-
tract key information that (i) helps users under-
stand the implications of agreeing to these poli-
cies and (ii) helps legal analysts understand the
contents of these policies and make recommenda-
tions on how they can be improved or made more
clear. Past applications of NLP have sought to
parse privacy policies into machine-readable rep-
resentations (Brodie et al, 2006) or extract sub-
policies from larger documents (Xiao et al, 2012).
Machine learning has been applied to assess cer-
tain attributes of policies (Costante et al, 2012;
Ammar et al, 2012; Costante et al, 2013; Zim-
meck and Bellovin, 2013).
This paper instead analyzes policies in aggre-
gate, seeking to align sections of policies. This
task is motivated by an expectation that many poli-
cies will address similar issues,
1
such as collec-
tion of a user?s contact, location, health, and fi-
nancial information, sharing with third parties, and
deletion of data. This expectation is supported
by recommendation by privacy experts (Gellman,
2014) and policymakers (Federal Trade Commis-
sion, 2012); in the financial services sector, the
Gramm-Leach-Bliley Act requires these institu-
tions to address a specific set of issues. Aligning
policy sections is a first step toward our aforemen-
tioned summarization and extraction goals.
We present the following contributions:
? A new corpus of over 1,000 privacy policies
gathered from widely used websites, manually
segmented into subtitled sections by crowdwork-
ers (?2).
? An unsupervised approach to aligning the policy
sections based on the issues they discuss. For
example, sections that discuss ?user data on the
company?s server? should be grouped together.
The approach is inspired by the application of
hidden Markov models to sequence alignment in
computational biology (Durbin et al, 1998; ?3).
? Two reusable evaluation benchmarks for the re-
sulting alignment of policy sections (?4). We
demonstrate that our approach outperforms na??ve
methods (?5).
Our corpus and benchmarks are available at
http://usableprivacy.org/data.
2 Data Collection
We collected 1,010 unique privacy policy
documents from the top websites ranked by
Alexa.com.
2
These policies were collected during
a period of six weeks during December 2013 and
January 2014. They are a snapshot of privacy
policies of mainstream websites covering fifteen
1
Personal communication, Joel Reidenberg.
2
http://www.alexa.com
605
Business Computers Games Health
Home News Recreation Shopping
Arts Kids and Teens Reference Regional
Science Society Sports
Table 1: Fifteen website categories provided by Alexa.com.
We collect privacy policies from the top 100 websites in each.
of Alexa.com?s seventeen categories (Table 1).
3
Finding a website?s policy is not trivial. Though
many well-regulated commercial websites provide
a ?privacy? link on their homepages, not all do.
We found university websites to be exceptionally
unlikely to provide such a link. Even once the pol-
icy?s URL is identified, extracting the text presents
the usual challenges associated with scraping doc-
uments from the web. Since every site is differ-
ent in its placement of the document (e.g., buried
deep within the website, distributed across several
pages, or mingled together with Terms of Service)
and format (e.g., HTML, PDF, etc.), and since we
wish to preserve as much document structure as
possible (e.g., section labels), full automation was
not a viable solution.
We therefore crowdsourced the privacy policy
document collection using Amazon Mechanical
Turk. For each website, we created a HIT in
which a worker was asked to copy and paste the
following privacy policy-related information into
text boxes: (i) privacy policy URL; (ii) last up-
dated date (or effective date) of the current privacy
policy; (iii) privacy policy full text; and (iv) the
section subtitles in the top-most layer of the pri-
vacy policy. To identify the privacy policy URL,
workers were encouraged to go to the website and
search for the privacy link. Alternatively, they
could form a search query using the website name
and ?privacy policy? (e.g., ?Amazon.com privacy
policy?) and search in the returned results for the
most appropriate privacy policy URL. Given the
privacy policy full text and the section subtitles,
we partition the full privacy document into differ-
ent sections, delimited by the section subtitles. A
privacy policy is then converted into XML.
Each HIT was completed by three workers, paid
$0.05, for a total cost of $380 (including Ama-
zon?s surcharge).
3
The ?Adult? category was excluded; the ?World? cate-
gory was excluded since it contains mainly popular websites
in different languages, and we opted to focus on policies in
English in this first stage of research, though mulitlingual pol-
icy analysis presents interesting challenges for future work.
3 Approach
Given the corpus of privacy policies described in
?2, we designed a model to efficiently infer an
alignment of policy sections. While we expect that
different kinds of websites will likely address dif-
ferent privacy issues, we believe that many poli-
cies will discuss roughly the same set of issues.
Aligning the policies is a first step in a larger effort
to (i) automatically analyze policies to make them
less opaque to users and (ii) support legal experts
who wish to characterize the state of privacy on-
line and make recommendations (Costante et al,
2012; Ammar et al, 2012; Costante et al, 2013).
We are inspired by multiple sequence alignment
methods in computational biology (Durbin et al,
1998) and by Barzilay and Lee (2004), who de-
scribed a hidden Markov model (HMM) for doc-
ument content where each state corresponds to a
distinct topic and generates sentences relevant to
that topic according to a language model. We
estimate an HMM-like model on our corpus, ex-
ploiting similarity across privacy policies to the
extent it is evident in the data. In our formula-
tion, each hidden state corresponds to an issue or
topic, characterized by a distribution over words
and bigrams appearing in privacy policy sections
addressing that issue. The transition distribution
captures tendencies of privacy policy authors to
organize these sections in similar orders, though
with some variation.
The generative story for our model is as follows.
Let S denote the set of hidden states.
1. Choose a start state y
1
from S according to the
start-state distribution.
2. For t = 1, 2, . . ., until y
t
is the stopping state:
(a) Sample the tth section of the document by
drawing a bag of terms, o
t
, according to the
emission multinomial distribution for state y
t
.
Note the difference from traditional HMMs, in
which a single observation symbol is drawn
at each time step. o
t
is generated by repeat-
edly sampling from a distribution over terms
that includes all unigrams and bigrams except
those that occur in fewer than 5% of the doc-
uments and in more than 98% of the docu-
ments. This filtering rule was designed to
eliminate uninformative stopwords as well as
company-specific terms (e.g., the name of the
company).
4
4
The emission distributions are not a proper language
606
Websites with Unique privacy Unique privacy Ave. sections Ave. tokens
Category privacy URL policies policies w/ date per policy per policy
Arts 94 80 72 11.1 (? 3.8) 2894 (? 1815)
Business 100 95 75 10.1 (? 4.9) 2531 (? 1562)
Computers 100 78 62 10.7 (? 4.9) 2535 (? 1763)
Games 92 80 51 10.2 (? 4.9) 2662 (? 2267)
Health 92 86 57 10.0 (? 4.4) 2325 (? 1891)
Home 100 84 68 11.5 (? 3.8) 2493 (? 1405)
Kids and Teens 96 86 62 10.3 (? 4.5) 2683 (? 1979)
News 96 91 68 10.7 (? 3.9) 2588 (? 2493)
Recreation 98 97 67 11.9 (? 4.5) 2678 (? 1421)
Reference 84 86 55 9.9 (? 4.1) 2002 (? 1454)
Regional 98 91 72 11.2 (? 4.2) 2557 (? 1359)
Science 71 75 49 9.2 (? 4.1) 1705 (? 1136)
Shopping 100 99 84 12.0 (? 4.1) 2683 (? 1154)
Society 96 94 65 10.2 (? 4.6) 2505 (? 1587)
Sports 96 62 38 10.9 (? 4.0) 2222 (? 1241)
Average 94.2 85.6 63.0 10.7 (? 4.3) 2471 (? 1635)
Table 2: Statistics of each website category, including (i) the number of websites with an identified privacy policy link; (ii)
number of unique privacy policies in each category (note that in rare cases, multiple unique privacy policies were identified
for the same website, e.g., a website that contains links to both new and old versions of its privacy policy); (iii) number of
websites with an identified privacy modification date; (iv) average number of sections per policy; (v) average number of tokens
per policy.
(b) Sample the next state, y
t+1
, according to the
transition distribution over S.
This model can nearly be understood as a hid-
den semi-Markov model (Baum and Petrie, 1966),
though we treat the section lengths as observable.
Indeed, our model does not even generate these
lengths, since doing so would force the states to
?explain? the length of each section, not just its
content. The likelihood function for the model is
shown in Figure 1.
The parameters of the model are almost iden-
tical to those of a classic HMM (start state dis-
tribution, emission distributions, and transition
distributions), except that emissions are char-
acterized by multinomial rather than a cate-
gorical distributions. These are learned us-
ing Expectation-Maximization, with a forward-
backward algorithm to calculate marginals (E-
step) and smoothed maximum likelihood estima-
tion for the M-step (Rabiner, 1989). After learn-
ing, the most probable assignment of a policy?s
sections to states can be recovered using a variant
of the Viterbi algorithm.
We consider three HMM variants. ?Vanilla? al-
lows all transitions. The other two posit an order-
ing on the states S = {s
1
, s
2
, . . . , s
K
}, and re-
strict the set of transitions that are possible, impos-
ing bias on the learner. ?All Forward? only allows
models (e.g., a bigram may be generated by as many as three
draws from the emission distribution: once for each unigram
it contains and once for the bigram).
s
k
to transition to {s
k
, s
k+1
, . . . , s
K
}. ?Strict For-
ward? only allows s
k
to transition to s
k
or s
k+1
.
4 Evaluation
Developing a gold-standard alignment of privacy
policies would either require an interface that al-
lows each annotator to interact with the entire cor-
pus of previously aligned documents while read-
ing the one she is annotating, or the definition (and
likely iterative refinement) of a set of categories
for manually labeling policy sections. These were
too costly for us to consider, so we instead pro-
pose two generic methods to evaluate models
for sequence alignment of a collection of docu-
ments with generally similar content. Though our
model (particularly the restricted variants) treats
the problem as one of alignment, our evaluations
consider groupings of policy sections. In the se-
quel, a grouping on a set X is defined as a collec-
tion of subsets X
i
? X; these may overlap (i.e.,
there might be x ? X
i
?X
j
) and need not be ex-
haustive (i.e., there might be x ? X \
?
i
X
i
).
4.1 Evaluation by Human QA
This study was carried out as part of a larger col-
laboration with legal scholars who study privacy.
In that work, we have formulated a set of nine mul-
tiple choice questions about a single policy that
ask about collection of contact, location, health,
and financial information, sharing of each with
607
Ppi,?,? (?yt,ot?
n
t=1
| ?`
t
?
n
t=1
) = pi(y
1
)
n
?
t=1
(
`
t
?
i=1
?(o
t,i
| y
i
)
)
?(y
t+1
| y
t
)
Figure 1: The likelihood function for the alignment model (one privacy policy). y
t
is the hidden state for the tth section, o
t
is
the bag of unigram and bigram terms observed in that section, and `
t
is the size of the bag. Start-state, emission, and transition
distributions are denoted respectively by pi, ?, and ?. y
n+1
is the silent stopping state.
third parties, and deletion of data.
5
The questions
were inspired primarily by the substantive interest
of these domain experts?not by this particular al-
gorithmic study.
For thirty policies, we obtained answers from
each of six domain experts who were not involved
in designing the questions. For the purposes of this
study, the experts? answers are not important. In
addition to answering each question for each pol-
icy, we also asked each expert to copy and paste
the text of the policy that contains the answer.
Experts were allowed to select as many sections
for each question as they saw fit, since answering
some questions may require synthesizing informa-
tion from different sections.
For each of the nine questions, we take the
union of all policy sections that contain text se-
lected by any annotator as support for her answer.
This results in nine groups of policy sections,
which we call answer-sets denoted A
1
, . . . , A
9
.
Our method allows these to overlap (63% of the
sections in any A
i
occurred in more than one A
i
),
and they are not exhaustive (since many sections
of the policies were not deemed to contain answers
to any of the nine questions by any expert).
Together, these can be used as a gold standard
grouping of policy sections, against which we can
compare our system?s output. To do this, we define
the set of section pairs that are grouped together
in answer sets, G = |{?a, b? | ?A
i
3 a, b}|, and
a similar set of pairs H from a model?s grouping.
From these sets, we calculate estimates of preci-
sion (|G ?H|/|H|) and recall (|G ?H|/|G|).
One shortcoming of this approach, for which
the second evaluation seeks to compensate, is that
a very small, and likely biased, subset of the policy
sections is considered.
4.2 Evaluation by Direct Judgment
We created a separate gold standard of judgments
of pairs of privacy policy sections. The data se-
lected for judgment was a sample of pairs stratified
5
The questions are available in an online appendix at
http://usableprivacy.org/data.
by a simple measure of text similarity. We derived
unigram tfidf vectors for each section in each of
50 randomly sampled policies per category. We
then binned pairs of sections by cosine similarity
(into four bins bounded by 0.25, 0.5, and 0.75).
We sampled 994 section pairs uniformly across the
15 categories? four bins each.
Crowdsourcing was used to determine, for each
pair, whether the two sections should be grouped
together. A HIT consisted of a pair of policy sec-
tions and a multiple choice question, ?After read-
ing the two sections given below, would you say
that they broadly discuss the same topic?? The
possible answers were:
1. Yes, both the sections essentially convey the
same message in a privacy policy.
2. Although, the sections do not convey the same
message, the broadly discuss the same topic.
(For ease of understanding, some examples of
content on ?the same topic? were included.)
3. No, the sections discuss two different topics.
The first two options were considered a ?yes? for
the majority voting and for defining a gold stan-
dard. Every section-pair was annotated by at least
three annotators (as many as 15, increased until
an absolute majority was reached). Turkers with
an acceptance rate greater than 95% with an ex-
perience of at least 100 HITs were allowed and
paid $0.03 per annotation. The total cost includ-
ing some initial trials was $130. 535 out of the
994 pairs were annotated to be similar in topic. An
example is shown in Figure 2.
As in ?4.1, we calculate precision and recall on
pairs. This does not penalize the model for group-
ing together a ?no? pair; we chose it nonetheless
because it is interpretable.
5 Experiment
In this section, we evaluate the three HMM vari-
ants described in ?3, and two baselines, using the
methods in ?4. All of the methods require the
specification of the number of groups or hidden
states, which we fix to ten, the average number of
sections per policy.
608
Section 5 of classmates.com:
[46 words] . . . You may also be required to use a password to access certain pages on the Services where certain
types of your personal information can be changed or deleted. . . . [113 words]
Section 2 of 192.com:
[50 words] . . . This Policy sets out the means by which You can have Your Personal Information removed from
the Service. 192.com is also committed to keeping Personal Information of users of the Service secure and only to
use it for the purposes set out in this Policy and as agreed by You. . . . [24 words]
Figure 2: Selections from sections that discuss the issue of ?deletion of personal information? and were labeled as discussing
the same issue by crowdworkers. Both na??ve grouping and LDA put them in two different groups, but the Strict Forward variant
of our model correctly groups them together.
Precision Recall F
1
Mean S.D. Mean S.D. Mean S.D.
Clust. 0.63 ? 0.30 ? 0.40 ?
LDA 0.56 0.03 0.20 0.05 0.29 0.06
Vanilla 0.62 0.04 0.41 0.04 0.49 0.03
All F. 0.63 0.03 0.47 0.12 0.53 0.06
Strict F. 0.62 0.05 0.46 0.18 0.51 0.07
Clust. 0.62 ? 0.23 ? 0.34 ?
LDA 0.57 0.03 0.18 0.01 0.28 0.02
Vanilla 0.57 0.01 0.30 0.03 0.39 0.02
All F. 0.58 0.02 0.32 0.06 0.41 0.04
Strict F. 0.58 0.03 0.32 0.14 0.40 0.08
Table 3: Evaluation by human QA (above) and direct judg-
ment (below), aggregated across ten independent runs where
appropriate (see text). Vanilla, All F(orward), and Strict
F(orward) are three variants of our HMM.
Baselines. Our first baseline is a greedy divisive
clustering algorithm
6
to partition the policy sec-
tions into ten clusters. In this method, the de-
sired K-way clustering solution is computed by
performing a sequence of bisections. The imple-
mentation uses unigram features and cosine simi-
larity. Our second baseline is latent Dirichlet alo-
cation (LDA; Blei et al, 2003), with ten topics and
online variational Bayes for inference (Hoffman et
al., 2010).
7
To more closely match our models,
LDA is given access to the same unigram and bi-
gram tokens.
Results. Table 3 shows the results. For LDA
and the HMM variants (which use random initial-
ization), we report mean and standard deviation
across ten independent runs. All three variants
of the HMM improve over the baselines on both
tasks, in terms of F
1
. In the human QA evalu-
ation, this is mostly due to recall improvements
(i.e., more pairs of sections relevant to the same
policy question were grouped together).
The three variants of the model performed sim-
ilarly on average, though Strict Forward had very
high variance. Its maximum performance across
6
As implemented in CLUTO, http://glaros.dtc.
umn.edu/gkhome/cluto/cluto/overview
7
As implemented in gensim (
?
Reh?u?rek and Sojka, 2010).
ten runs was very high (67% and 53% F
1
on the
two tasks), suggesting the potential benefits of
good initialization or model selection.
6 Conclusion
We considered the task of aligning sections of
a collection of roughly similarly-structured legal
documents, based on the issues they address. We
introduced an unsupervised model for this task
along with two new (and reusable) evaluations.
Our experiments show the approach to be more ef-
fective than clustering and topic models. The cor-
pus and evaluation data have been made available
at http://usableprivacy.org/data . In
future work, policy section alignments will be
used in automated analysis to extract useful infor-
mation for users and privacy scholars.
Acknowledgments
The authors gratefully acknowledge helpful com-
ments from Lorrie Cranor, Joel Reidenberg, Flo-
rian Schaub, and several anonymous reviewers.
This research was supported by NSF grant SaTC-
1330596.
References
Waleed Ammar, Shomir Wilson, Norman Sadeh, and
Noah A. Smith. 2012. Automatic categorization of
privacy policies: A pilot study. Technical Report
CMU-LTI-12-019, Carnegie Mellon University.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proc. of HLT-
NAACL.
Leonard E. Baum and Ted Petrie. 1966. Statistical
inference for probabilistic functions of finite state
Markov chains. Annals of Mathematical Statistics,
37:1554?1563.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet alocation. the Journal of
machine Learning research, 3:993?1022.
609
Carolyn A. Brodie, Clare-Marie Karat, and John Karat.
2006. An empirical study of natural language pars-
ing of privacy policy rules using the SPARCLE pol-
icy workbench. In Proc. of the Symposium on Us-
able Privacy and Security.
Elisa Costante, Yuanhao Sun, Milan Petkovi?c, and
Jerry den Hartog. 2012. A machine learning solu-
tion to assess privacy policy completeness. In Proc.
of the ACM Workshop on Privacy in the Electronic
Society.
Elisa Costante, Jerry Hartog, and Milan Petkovi.
2013. What websites know about you. In Roberto
Pietro, Javier Herranz, Ernesto Damiani, and Radu
State, editors, Data Privacy Management and Au-
tonomous Spontaneous Security, volume 7731 of
Lecture Notes in Computer Science, pages 146?159.
Springer Berlin Heidelberg.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Federal Trade Commission. 2012. Protecting con-
sumer privacy in an era of rapid change: Recom-
mendations for businesses and policymakers.
Robert Gellman. 2014. Fair information prac-
tices: a basic history (v. 2.11). Available at
http://www.bobgellman.com/rg-docs/
rg-FIPShistory.pdf.
Matthew D Hoffman, David M Blei, and Francis R
Bach. 2010. Online learning for latent Dirichlet al
location. In NIPS.
Aleecia M. McDonald and Lorrie Faith Cranor. 2008.
The cost of reading privacy policies. I/S: A Journal
of Law and Policy for the Information Society, 4(3).
Lawrence Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In
Proc. of the LREC Workshop on New Challenges for
NLP Frameworks.
Xusheng Xiao, Amit Paradkar, Suresh Thum-
malapenta, and Tao Xie. 2012. Automated ex-
traction of security policies from natural-language
software documents. In Proc. of the ACM SIGSOFT
International Symposium on the Foundations of
Software Engineering.
Sebastian Zimmeck and Steven M. Bellovin. 2013.
Machine learning for privacy policy.
610
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 42?50,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Entailment: An Effective Metric for Comparing and Evaluating
Hierarchical and Non-hierarchical Annotation Schemes
Rohan Ramanath?
R. V. College of Engineering, India
ronramanath@gmail.com
Monojit Choudhury Kalika Bali
Microsoft Research Lab India
{monojitc, kalikab}@microsoft.com
Abstract
Hierarchical or nested annotation of lin-
guistic data often co-exists with simpler
non-hierarchical or flat counterparts, a
classic example being that of annotations
used for parsing and chunking. In this
work, we propose a general strategy for
comparing across these two schemes of
annotation using the concept of entailment
that formalizes a correspondence between
them. We use crowdsourcing to obtain
query and sentence chunking and show
that entailment can not only be used as
an effective evaluation metric to assess the
quality of annotations, but it can also be
employed to filter out noisy annotations.
1 Introduction
Linguistic annotations at all levels of linguistic or-
ganization ? phonological, morpho-syntactic, se-
mantic, discourse and pragmatic, are often hierar-
chical or nested in nature. For instance, syntac-
tic dependencies are annotated as phrase structure
or dependency trees (Jurafsky and Martin, 2000).
Nevertheless, the inherent cognitive load associ-
ated with nested segmentation and the sufficiency
of simpler annotation schemes for building NLP
applications have often lead researchers to define
non-hierarchical or flat annotation schemes. The
flat annotation, in essence, is a ?flattened? ver-
sion of the tree. For instance, chunking of Natu-
ral Language (NL) text, which is often considered
an essential preprocessing step for many NLP ap-
plications (Abney, 1991; Abney, 1995), is, loosely
speaking, a flattened version of the phrase struc-
ture tree. The closely related task of Query Seg-
mentation is of special interest to us here, as it is
?The work was done during author?s internship at Mi-
crosoft Research Lab India.
f Pipe representation Boundary var.
3 barbie dress up | games 0 0 1
3 barbie dress | up games 0 1 0
2 barbie | dress up | games 1 0 1
2 barbie | dress up games 1 0 0
Table 1: Example of flat segmentations from 10
Turkers. f is the frequency of annotations; seg-
ment boundaries are represented by |.
the first step in further analysis and understanding
of Web search queries (Hagen et al, 2011).
The task in both query and sentence chunking is
to divide the string of words into contiguous sub-
strings of words (commonly refered to as segments
or chunks) such that the words from a segment
are related to each other more strongly than words
from different segments. It is typically assumed
that the segments are syntactically and semanti-
cally coherent. Table 1 illustrates the concept of
segmentation of a query. The crowdsourced an-
notations for this data were obtained from 10 an-
notators, the experimental details of which will be
described in Sec. 5. We shall refer to this style of
text chunking as flat segmentation.
Nested segmentation of a query or a sentence,
on the other hand, is a recursive application of flat
segmentation, whereby the longer flat segments
are further divided into smaller chunks recursively.
The process stops when a segment consists of less
than three words or is a multiword entity that can-
not be segmented further. This style of segmenta-
tion can be represented through nested parenthe-
sization of the text, as illustrated in Table 2. These
annotations were also obtained through the same
crowdsourcing experiment (Sec. 5). Fig. 1 shows
an alternative visualization of a nested segmenta-
tion in the form of a tree.
An important problem that arises in the con-
text of flat segmentation is the issue of granular-
42
f Bracket representation Boundary var.
4 ((barbie dress)( up games)) 0 1 0
3 (barbie ((dress up) games)) 2 0 1
2 (barbie (dress (up games))) 2 1 0
1 ((barbie (dress up)) games) 1 0 2
Table 2: Example of nested segmentation from 10
Turkers. f is the frequency of annotations.
2
barbie 1
0
dress up
games
Figure 1: Tree representation of the nested seg-
mentation: (barbie ((dress up) games))
ity. For instance, in the case of NL chunking, it
is not clear whether the chunk boundaries should
correspond to the innermost parentheses in the
nested segmentation marking very short chunks,
or should one annotate the larger chunks corre-
sponding to clausal boundaries. For this reason,
Inter-Annotator Agreement (IAA) for flat annota-
tion tasks is often poor (Bali et al, 2009; Hagen
et al, 2011; Saha Roy et al, 2012). However, low
IAA does not necessarily imply low quality anno-
tation, and could as well be due to the inherent am-
biguity in the task definition with respect to gran-
ularity. Although we have illustrated the concept
and problems of flat and nested annotations using
the examples of sentence and query segmentation,
these issues are generic and typical of any flat an-
notation scheme which tries to flatten or approx-
imate an underlying hierarchical structure. There
are three important research questions pertaining
to the linguistic annotations of this kind:
? How to measure the true IAA and the quality
of the flat annotations?
? How to compare the agreement between the
flat and the nested annotations?
? How can we identify or construct the opti-
mal or error-free flat annotations from a noisy
mixture of nested and flat annotations?
In this paper, we introduce the concept of ?en-
tailment of a flat annotation by a nested annota-
tion?. For a given linguistic unit (a query or a sen-
tence, for example), a nested annotation is said to
entail a flat annotation if the structure of the lat-
ter does not contradict the more specific structure
represented by the former. Based on this simple
notion, which will be formalized in Sec. 3, we
develop effective techniques for comparing across
and evaluating the quality of flat and nested an-
notations, and identifying the optimal flat annota-
tion. We validate our theoretical framework on the
tasks of query and sentence segmentation. In par-
ticular, we conduct crowdsourcing based flat and
nested segmentation experiments for Web search
queries and sentences using Amazon Mechanical
Turk (AMT)1. We also obtain annotations for the
same datasets by trained experts which are ex-
pected to be of better quality than the AMT-based
annotations. Various statistical analyses of the an-
notated data bring out the effectiveness of entail-
ment as a metric for comparison and evaluation of
flat and nested annotations.
The rest of the paper is organized as fol-
lows. Sec. 2 provides some background on the
annotation tasks and related work on IAA. In
Sec. 3, we introduce the notion of entailment
and develop theoretical models and related
strategies for assessing the quality of annotation.
In Sec. 4, we introduce some strategies based
on entailment for the identification of error-free
annotations from a given set of noisy annotations.
Sec. 5 describes the annotation experiments
and results. Sec. 6 concludes the paper by
summarizing the work and discussing future
research directions. All the annotated datasets
used in this research can be obtained freely from
http://research.microsoft.com/
apps/pubs/default.aspx?id=192002
and used for non-commercial research purposes.
2 Background
Segmentation or chunking of NL text is a well-
studied problem. Abney (1991; 1992; 1995)
defines a chunk as a sub-tree within a syntac-
tic phrase structure tree corresponding to Noun,
Prepositional, Adjectival, Adverbial and Verb
Phrases. Similarly, Bharati et al(1995) define it
as Noun Group and Verb Group based only on lo-
cal surface information. Chunking is an important
preprocessing step towards parsing.
Like chunking, query segmentation is an im-
portant step towards query understanding and is
generally believed to be useful for Web search
1https://www.mturk.com/mturk/welcome
43
(see Hagen et al (2011) for a survey). Auto-
matic query segmentation algorithms are typically
evaluated against a small set of human-annotated
queries (Bergsma and Wang, 2007). The reported
low IAA for such datasets casts serious doubts on
the reliability of annotation and the performance
of the algorithms evaluated on them (Hagen et al,
2011; Saha Roy et al, 2012). To address the is-
sue of data scarcity, Hagen et al (2011) created
a large set of manually segmented queries through
crowdsourcing2. However, their approach has cer-
tain limitations because the crowd is already pro-
vided with a few possible segmentations of a query
to choose from. Nevertheless, if large scale data
has to be procured crowdsourcing seems to be the
only efficient and effective model for the task, and
has been proven to be so for other IR and lin-
guistic annotations (see Lease et al (2011) for
examples). It should be noted that almost all the
work on query segmentation, except (Huang et al,
2010), has considered only flat segments.
An important problem that arises in the context
of flat annotations is the issue of granularity. In the
absence of a set of guidelines that explicitly state
the granularity expected, Inter-Annotator Agree-
ment (IAA) for flat annotation tasks are often poor.
Bali et al (2009) showed that for NL chunking,
annotators typically agree on major (i.e., clausal)
boundaries but do not agree on minor (i.e., phrasal
or intra-phrasal) boundaries. Similarly, for query
segmentation, low IAA remains an issue (Hagen
et al, 2011; Saha Roy et al, 2012).
The issue of granularity is effectively addressed
in nested annotation, because the annotator is ex-
pected to mark the most atomic segments (such
as named entities and multiword expressions) and
then recursively combine them to obtain larger
segments. Certain amount of ambiguity, that may
arise because of lack of specific guidelines on the
number of valid segments at the last level (i.e., top-
most level of the nested segmentation tree), can
also be resolved by forcing the annotator to recur-
sively divide the sentence/query always into ex-
actly two parts (Abney, 1992; Bali et al, 2009).
The present study is an extension of our recent
work (Ramanath et al, 2013) on analysis of the
effectiveness of crowdsourcing for query and sen-
tence segmentation. We introduced a novel IAA
metric based on Kripendorff?s ?, and showed that
while the apparent agreement between the annota-
2http://www.webis.de/research/corpora
tors in a crowdsourced experiment might be high,
the chance corrected agreement is actually low for
both flat and nested segmentations (as compared
to gold annotations obtained from three experts).
The reason for the apparently high agreement is
due to an inherent bias of the crowd to divide
a piece of text in roughly two equal parts. The
present study extends this work by introducing a
metric to compare across flat and nested segmen-
tations that enables us to further analyze the relia-
bility of the crowdsourced annotations. This met-
ric is then employed to identify the optimal flat
segmentation(s) from a set of noisy annotations.
The study uses the same experimental setup and
annotated datasets as described in (Ramanath et
al., 2013). Nevertheless, for the sake of readability
and self-containedness, the relevant details will be
mentioned here again.
We do not know of any previous work that com-
pares flat and nested schemes of annotation. In
fact, Artstein and Poesio (2008), in a detailed sur-
vey of IAA metrics and their usage in NLP, men-
tion that defining IAA metrics for trees (hierarchi-
cal annotations) is a difficult problem due to the
existence of overlapping annotations. Vadas and
Curran (2011) and Brants (2000) discuss measur-
ing IAA of nested segmentations employing the
concepts of precision, recall, and f-score. How-
ever, neither of these studies apply statistical cor-
rection for chance agreement.
3 Entailment: Definition and Modeling
In this section, we shall introduce certain notations
and use them to formalize the notion of entail-
ment, which in turn, is used for the computation of
agreement between flat and nested segmentations.
Although we shall develop the whole framework
in the context of queries, it is applicable to sen-
tence segmentation and, in fact, more generally to
any flat and nested annotations.
3.1 Basic Definitions
Let Q be the set of all queries. A query q ? Q
can be represented as a sequence of |q| words:
w1w2 . . . w|q|. We introduce |q| ? 1 random vari-
ables, b1, b2, . . . b|q|?1, such that bi represents the
boundary between the words wi and wi+1. A flat
and nested segmentation of q, represented by F jq
and N jq respectively, j varying from 1 to total
number of annotations, c, is a particular instan-
tiation of these boundary variables as follows.
44
Definition. Flat Segmentation: A flat segmen-
tation, F jq , can be uniquely defined by a binary
assignment of the boundary variables bji , where
bji = 1 iff wi and wi+1 belong to two different flat
segments. Otherwise, bji = 0. Thus, q has 2
|q|?1
possible flat segmentations.
Definition. Nested Segmentation: A nested seg-
mentation, N jq , is defined as an assignment of
non-negative integers to the boundary variables
such that bji = 0 iff words wi and wi+1 form an
atomic segment (i.e., they are grouped together),
else bji = 1 + max(lefti, righti), where lefti
and righti are the heights of the largest subtrees
ending at wi and beginning at wi+1 respectively.
This numbering scheme can be understood
through Fig. 1. Every internal node of the binary
tree corresponding to the nested segmentation is
numbered according to its height. The lowest in-
ternal nodes, both of whose children are query
words, are assigned a value of 0. Other internal
nodes get a value of one greater than the height
of its higher child. Since every internal node cor-
responds to a boundary, we assign the height of
the node to the corresponding boundary variables.
The number of unique nested segmentations of q
is the corresponding Catalan number3 C|q|?1.
Note that, following Abney?s (1992) suggestion
for nested chunking, we define nested segmenta-
tion as a strict binary tree or binary bracketing of
the query. This is not only helpful for theoretical
analysis, but also necessary to ensure that there
is no ambiguity related to the granularity of seg-
ments.
3.2 Entailment
Given a nested segmentation N jq , there are several
possible ways to ?flatten? it. Flat segmentations of
q, where bi = 0 for all i (i.e., the whole query is
one segment) and bi = 1 for all i (i.e., all words are
in different segments) are trivially obtainable from
N jq , and therefore, are not neither informative nor
interesting. Intuitively, any flat segmentation, F kq ,
can be said to agree with N jq if for every flat seg-
ment in F kq there is a corresponding internal node
in N jq , such that the subgraph rooted at that node
spans (contains) all and only those words present
in the flat segment (Abney, 1991).
Let us take the examples of flat and nested
segmentations shown in Tables 1 and 2 to illus-
3http://en.wikipedia.org/wiki/Catalan\
_number
trate this notion. Consider two nested segmenta-
tions, N1q = ((barbie (dress up)) games), N
2
q =
(barbie ((dress up) games)) and three flat seg-
mentations, F 1q = barbie | dress up | games,
F 2q = barbie | dress up games, F
3
q =
barbie dress | up games. Figure 2 diagram-
matically compares the two nested segmentations
(the two rows) with the three flat segmentations
(columns A, B and C). There are three flat seg-
ments in F 1q , of which the two single word
segments barbie and games trivially coincide
with the corresponding leaf nodes. The segment
dressup coincides exactly with the words spanned
by the node marked 0 of N1q (Fig. 2, top row, col-
umn A). Hence, F 1q can be said to be in agree-
ment withN1q . On the other hand, there is no node
in N1q , which exactly coincides with the segment
dressupgames of F 2q (Fig. 2, top row, column B).
Hence, we say that N1q does not agree with F
2
q .
We formalize this notion of agreement in terms
of entailment, which is defined as follows.
Definition: Entailment. A nested segmentation,
N jq is said to entail a flat segmentation, F
k
q , (or
equivalently, F kq is entailed by N
j
q ) if and only if
for every multiword segment wi+1, wi+2, ..., wi+l
in F kq , the corresponding boundary variables in
N jq follows the constraint: bi > bi+m and bi+l >
bi+m for all 1 ? m < l.
It can be proved that this definition of entail-
ment is equivalent to the intuitive description pro-
vided earlier. Yet another equivalent definition of
entailment is presented in the form of Algorithm 1.
Due to paucity of space, the proofs of equivalence
are omitted.
Definition: Average Observed Entailment. For
the set of queries Q, and corresponding sets of
c flat and nested segmentations, there are |Q|c2
pairs of flat and nested segmentations that can be
compared for entailment. We define the average
observed entailment for this annotation set as the
fraction of these |Q|c2 annotation pairs for which
the flat segmentation is entailed by the correspond-
ing nested segmentation. We shall express this
fraction as percentage.
3.3 Entailment by Random Chance
Average observed Entailment can be considered
as a measure of the IAA, and hence, an indica-
tor of the quality of the annotations. However,
in order to interpret the significance of this value,
we need an estimate of the average entailment that
45
Figure 2: Every node of the tree represent boundary values, nested(flat). Column A: F 1q is entailed by
both N1q and N
2
q , Column B: F
2
q is entailed by N
2
q but not N
1
q , Column C: F
3
q is entailed by neither
N1q nor N
2
q . The nodes (or equivalently the boundaries) violating the entailment constraint are marked a
cross, and those agreeing are marked with ticks.
Algorithm 1 Algorithm: isEntail
1: procedure ISENTAIL(flat, nested) . flat,
nested are lists containing boundary values
2: if len(nested) ? 1 or len(flat) ? 1 then
3: return True
4: end if
5: h? largest element in nested
6: i? index of h
7: if flat[i] = 1 then
8: if ! isEntail(flat[: i], nested[: i]) or
! isEntail(flat[i+1 :], nested[i+1 :]) then
9: return False
10: else
11: return True
12: end if
13: else
14: while h 6= 0 do
15: nested[i]? ?nested[i]
16: h? largest element in nested
17: i? index of h
18: if flat[i] = 1 then
19: return False
20: end if
21: end while
22: return True
23: end if
24: end procedure
one would expect if the annotations, both flat and
nested, were drawn uniformly at random from the
set of all possible annotations. From our exper-
iments we observe that trivial flat segmentations
are, in fact, extremely rare, and a very large frac-
tion of the flat annotations have two or three seg-
ments. Therefore, for computing the chance en-
tailment, we assume that the number of segments
in the flat segmentation is known and fixed, which
is either 2 or 3, but all segmentations with these
many segments are equally likely to be chosen.
We also assume that all nested segmentations are
equally likely.
When there are 2 segments: For a query q, the
number of flat segmentations with two segments,
i.e., one boundary, is
(|q|?1
1
)
= |q| ? 1. Note
that for any nested segmentation N jq , all flat seg-
mentations that have at least one boundary and is
entailed by it must have a boundary between wi?
and wi?+1, where bi? has the highest value in N jq .
In other words, bi? is the boundary corresponding
to the root of the nested tree (the proof is intu-
itive and is omitted). Therefore, there is exactly
one ?flat segmentation with one boundary? that is
entailed by a given N jq . Therefore, the random
chance that a nested segmentation N jq will entail
a flat segmentation with one boundary is given by
(|q| ? 1)?1 (for |q| > 1).
When there are 3 segments: Number of flat
segmentations with two boundaries is
(|q|?1
2
)
. The
flat segmentation(s) entailed by N jq can be gener-
ated as follows. As argued above, every flat seg-
mentation entailed by N jq must have a boundary
46
at position i?. The second boundary can be either
in the left or right of i?. But in either case, the
choice of the boundary is unique which will corre-
spond to the highest node in the left or right sub-
tree of the root node. Thus, every nested segmen-
tation entails at most 2 flat segmentations. How-
ever, if i? = 1 or |q| ? 1 for a N jq , then, respec-
tively, the left or right subtrees do not exist. In
such cases, there is only one flat segmentation en-
tailed by N jq . Note that there are exactly C|q|?2
nested segmentations for which the i? = 1, and
similarly another C|q|?2 for which i
? = |q| ? 1.
Therefore, out of C|q|?1 ?
(|q|?1
2
)
pairs, exactly
2C|q|?1?2C|q|?2 pairs satisfy the entailment con-
ditions. Thus, the expected probability of entail-
ment by random chance when there are exactly
two boundaries in the flat segmentation of q is:
2(C|q|?1 ? C|q|?2)
C|q|?1
(|q|?1
2
) = 2
(
|q| ? 1
2
)?1
(1?
C|q|?2
C|q|?1
)
The values of the probability of observing a ran-
dom nested segmentation entailing a flat segmen-
tation with exactly two boundaries for |q| =
3, 4, 5, 6, 7 and 8 are 1, 0.4, 0.213. 0.133, 0.091
and 0.049 respectively.
3.4 Other IAA Metrics
Although entailment can be used as a measure of
agreement between flat and nested segmentations,
IAA within flat or within nested segmentations
cannot be computed using this notion. In (Ra-
manath et al, 2013), we have extensively dealt
with the issue of computing IAA for these cases.
Krippendorff?s ? (Krippendorff, 2004), which is
an extremely versatile agreement coefficient, has
been appropriately modified to be applicable to a
crowdsourced annotation scenario. ? = 1 im-
plies perfect agreement, ? = 0 implies that the
observed agreement is just as good as that by ran-
dom chance, whereas ? < 0 implies that the ob-
served agreement is less than that one would ex-
pect by random chance. Due to paucity of space
we omit any further discussion on this and refer
the reader to (Ramanath et al, 2013). Here, we
will use the ? values as an alternative indicator of
IAA and therefore, the quality of annotation.
4 Optimal Segmentation
Suppose that we have a large number of flat and
nested annotations coming from a noisy source
such as crowdsourcing; is it possible to employ
the notion of entailment to identify the annota-
tions which are most likely to be correct? Here,
we describe two such strategies to obtain the opti-
mal (error-free) flat segmentation.
Flat Entailed by Most Nested (FEMN): The
intuition behind this approach is that if a flat seg-
mentation F kq is entailed by most of the nested
segmentations of q, then it is very likely that F kq
is correct. Therefore, for each flat segmentations
of q, we count the number of nested segmentations
of q that entail it, and the one with highest count is
declared as the optimal FEMN segmentation. It is
interesting to note that while computing the opti-
mal FEMN segmentation, we never encountered a
tie between two flat segmentations. The trivial flat
segmentations (i.e., if the whole query is one seg-
ment or every word is in different segments) are
filtered as a preprocessing step.
Iterative Voting (IV): FEMN assumes that the
nested segmentations are relatively noise-free. If
most of the nested segmentations are erroneous,
FEMN would select an erroneous optimal flat seg-
mentation. To circumvent this issue, we propose a
more sophisticated iterative voting process, where
we count the number of flat segmentations entailed
by each nested segmentation of q, and similarly,
number of nested segmentations that entail each
flat segmentation. The flat and nested segmenta-
tions with the least scores are then removed from
the dataset. Then we recursively apply the IV pro-
cess on the reduced set of annotations until we are
left with a single flat segmentation.
5 Experiments and Results
We obtained nested and flat segmentation of Web
search queries through crowdsourcing as well as
from trained experts. Furthermore, we also con-
ducted similar crowdsourcing experiments for NL
sentences, which helped us understand the specific
challenges in annotating queries because of their
apparent lack of a well-defined syntactic structure.
In this section, we first describe the experimen-
tal setup and datasets, and then present the obser-
vations and results.
5.1 Crowdsourcing Experiment
In this study we use the same set of crowd-
sourced annotations as described in (Ramanath
et al, 2013). For the sake of completeness, we
briefly describe the annotation procedure here as
47
well. We used Amazon Mechanical Turk for the
crowdsourcing experiments. Two separate Hu-
man Intelligence Tasks were designed for flat and
nested segmentation. The concept of flat and
nested segmentation was introduced to the Turk-
ers with the help of two short videos4.
When in doubt regarding the meaning of a
query, the Turkers were advised to issue the query
on a search engine of their choice and find out its
possible interpretation(s). Only Turkers who had
completed more than 100 tasks at an acceptance
rate of ? 60% were allowed to participate in the
task and were paid $0.02 for a flat and $0.06 for a
nested segmentation. Every query was annotated
by 10 different annotators.
5.2 Dataset
The following sets of queries and sentences were
used for annotations:
Q500, QG500: Saha Roy et al (2012) re-
leased a dataset of 500 queries, 5 to 8 words long,
for the evaluation of various segmentation algo-
rithms. This dataset has flat segmentations from
three annotators obtained under controlled exper-
imental settings, and could be considered as Gold
annotation. Hence, we selected this set for our ex-
periments as well. We procured the correspond-
ing nested segmentation for these queries from
two human experts who are regular search engine
users. They annotated the data under supervision
and were trained and paid for the task. We shall
refer to the set of flat and nested gold annotations
as QG500, whereas Q500 will be reserved for the
dataset procured through the AMT experiments.
Q700: As 500 queries are not enough for mak-
ing reliable conclusions and also, since the queries
may not have been chosen specifically for the pur-
pose of annotation experiments, we expanded the
set with another 700 queries sampled from the
logs of a popular commercial search engine. We
picked, uniformly at random, queries that were 4
to 8 words long.
S300: We randomly selected 300 English sen-
tences from a collection of full texts of public do-
main books5 that were 5 to 15 words long, and
manually checked them for well-formedness.
4Flat: http://youtu.be/eMeLjJIvIh0, Nested:
http://youtu.be/xE3rwANbFvU
5http://www.gutenberg.org
5.3 Entailment Statistics
Table 3 reports two statistics ? the values of
Kripendorff?s ? and the average observed entail-
ment (expressed as %) for flat and nested segmen-
tations along with the corresponding expected val-
ues for entailment by chance. For nested segmen-
tation, the ? values were computed for two differ-
ent distance metrics6 d1 and d2.
As expected, the highest value of ? for both
flat and nested segmentation is observed for the
gold annotations. An ? > 0.6 indicates a rea-
sonably good7 IAA, and thus, reliable annota-
tions. We note that the entailment statistics fol-
low a very similar trend as ?, and for all the cases,
the observed average entailment is much higher
than what we would expect by random chance.
These two observations clearly point to the fact
that entailment is indeed a good indicator of the
agreement between the nested and flat segmenta-
tions, and consequently, the reliability of the an-
notations. We also observe that the average en-
tailment for S300 is in the same ballpark as for
the queries. This indicates that the apparent lack
of structure in queries does not specifically influ-
ence the annotations. Along the same lines, one
can also argue that the length of a text, which
is higher for sentences than queries, does not af-
fect the crowdsourced annotations. In fact, in our
previous study (Ramanath et al, 2013), we show
that it is the bias of the Turkers to divide a text
in approximately two segments of equal size (ir-
respective of other factors, like syntactic structure
or length), that leads to very similar IAA across
different types of texts. Our current study on en-
tailment further strengthens this fact.
Figure 3 plots the distribution of the entailment
values for the three datasets. The distributions are
normal-like implying that entailment is a robust
metric and its average value is a usable statistic.
In order to analyze the agreement between the
Turkers and the experts, we computed the av-
erage entailment between Q500 flat annotations
(from AMT) with QG500 nested annotations, and
similarly, Q500 nested annotations with QG500
6Intuitively, for d1 disagreements between segment
boundaries are equally penalized at all the levels of nested
tree, whereas for d2 disagreements higher up the tree (i.e.,
close to the root) are penalized more than those at lower lev-
els.
7It should be noted that there is no consensus on what is
a good value of ? for linguistic annotations, partly because
it is dependent on the nature of the annotation task and the
demand of the end applications that use the annotated data.
48
Dataset Krippendorff?s ? Entailment Statistics
Flat Nested Observed Chance
d1 d1 d2
Q700 0.21 0.21 0.16 49.68 12.63
Q500 0.22 0.15 0.15 56.69 19.08
QG500 0.61 0.66 0.67 87.07 11.91
S300 0.27 0.18 0.14 52.86 19.12
Table 3: ? and Average Entailment Statistics
Figure 3: Distribution of the entailment values (x-
axis) plotted as the % of comparable flat-nested
annotation pairs.
Figure 4: Distribution of percentage of entailed
pairs using QG500 as reference.
flat annotations, which turned out to be 70.42%
and 63.24% respectively. The corresponding dis-
tributions are shown as Nested and Flat in Fig.
4. Thus, the flat segmentations from the Turkers
seem to be more accurate than their nested seg-
mentations, a fact also supported by the ? values.
This could be due to the much higher cognitive
load associated with nested segmentation that de-
mands more time and concentration that an ordi-
nary Turker may not be willing to invest.
5.4 Optimal Segmentation Results
In order to evaluate the optimal flat segmentation
selection strategies, FEMN and IV, we computed
the percentage of queries in Q500 for which the
optimal flat segmentation (as obtained by apply-
ing these strategies on AMT annotations) is en-
tailed by the corresponding nested segmentations
in QG500. The average entailment values for
FEMN and IV turns out to be 79.60% and 82.80%
respectively. This shows that the strategies are in-
deed able to pull out the more accurate flat seg-
mentations from the set, though, as one would ex-
pect, IV performs better than FEMN, and its cho-
sen segmentations are almost as good as that by
expert annotators.
Another experiment was conducted to precisely
characterize the effectiveness of these strategies
whereby we mixed the annotations from the Q500
and QG500, and then applied FEMN and IV to
pull out the optimal flat segmentations. We ob-
served that for 63.71% and 91.44% of the queries,
the optimal segmentation chosen by FEMN and IV
respectively was indeed one of the three gold flat
annotations in QG500. This reinforces our con-
clusion that IV can effectively identify the optimal
flat segmentation of a query from a noisy set of flat
and nested segmentations.
6 Conclusion
In this paper, we proposed entailment as a theo-
retical model for comparing hierarchical and non-
hierarchical annotations. We present a formaliza-
tion of the notion of entailment and use it for de-
vising two strategies, FEMN and IV, for identify-
ing the optimal flat segmentation in a noisy set of
annotations. One of the main contributions of this
work resides in our following experimental find-
ing: Even though annotations obtained through
crowdsourcing for a difficult task like query seg-
mentation might be very noisy, a small fraction of
the annotations are nevertheless correct; it is pos-
sible to filter out these correct annotations using
the Iterative Voting strategy when both hierarchi-
cal and non-hierarchical segmentations are avail-
able from the crowd.
The proposed model is generic and we be-
lieve that the experimental findings extend beyond
query and sentence segmentation to other kinds of
linguistic annotations where hierarchical and non-
hierarchical schemes co-exist.
Acknowledgment
Thanks to Rishiraj Saha Roy, IIT Kharagpur, for
his valuable inputs during this work.
49
References
Steven P. Abney. 1991. Parsing By Chunks. Kluwer
Academic Publishers.
Steven P. Abney. 1992. Prosodic Structure, Perfor-
mance Structure And Phrase Structure. In Proceed-
ings 5th Darpa Workshop on Speech and Natural
Language, pages 425?428. Morgan Kaufmann.
Steven P. Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
Computational Linguistics and the Foundations of
Linguistic Theory, pages 145?164.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Kalika Bali, Monojit Choudhury, Diptesh Chatterjee,
Sankalan Prasad, and Arpit Maheswari. 2009. Cor-
relates between Performance, Prosodic and Phrase
Structures in Bangla and Hindi: Insights from a Psy-
cholinguistic Experiment. In ICON ?09, pages 101
? 110.
Shane Bergsma and Qin Iris Wang. 2007. Learn-
ing Noun Phrase Query Segmentation. In EMNLP-
CoNLL ?07, pages 819?826.
Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal.
1995. Natural Language Processing: A Paninian
Perspective. Prentice.
Thorsten Brants. 2000. Inter-annotator agreement for
a German newspaper corpus. In In Proceedings of
Second International Conference on Language Re-
sources and Evaluation LREC-2000.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Bra?utigam. 2011. Query segmentation re-
visited. In WWW ?11, pages 97?106.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,
Kuansan Wang, Fritz Behr, and C. Lee Giles. 2010.
Exploring Web Scale Language Models for Search
Query Processing. In WWW ?10, pages 451?460.
Dan Jurafsky and James H Martin. 2000. Speech &
Language Processing. Pearson Education India.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage,Thousand
Oaks, CA.
Matthew Lease, Vaughn Hester, Alexander Sorokin,
and Emine Yilmaz, editors. 2011. Proceedings of
the ACM SIGIR 2011 Workshop on Crowdsourcing
for Information Retrieval (CIR 2011).
Rohan Ramanath, Monojit Choudhury, Kalika Bali,
and Rishiraj Saha Roy. 2013. Crowd Prefers the
Middle Path: A New IAA Metric for Crowdsourc-
ing Reveals Turker Biases in Query Segmentation.
In Proceedings of ACL. ACL.
Rishiraj Saha Roy, Niloy Ganguly, Monojit Choud-
hury, and Srivatsan Laxman. 2012. An IR-based
Evaluation Framework for Web Search Query Seg-
mentation. In SIGIR ?12, pages 881?890. ACM.
David Vadas and James R. Curran. 2011. Parsing
Noun Phrases in the Penn Treebank. Comput. Lin-
guist., 37(4):753?809, December.
50
