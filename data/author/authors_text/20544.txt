Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059?1069,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Efficient Non-parametric Estimation of
Multiple Embeddings per Word in Vector Space
Arvind Neelakantan
*
, Jeevan Shankar
*
, Alexandre Passos, Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
Amherst, MA, 01003
{arvind,jshankar,apassos,mccallum}@cs.umass.edu
Abstract
There is rising interest in vector-space
word embeddings and their use in NLP,
especially given recent methods for their
fast estimation at very large scale. Nearly
all this work, however, assumes a sin-
gle vector per word type?ignoring poly-
semy and thus jeopardizing their useful-
ness for downstream tasks. We present
an extension to the Skip-gram model that
efficiently learns multiple embeddings per
word type. It differs from recent related
work by jointly performing word sense
discrimination and embedding learning,
by non-parametrically estimating the num-
ber of senses per word type, and by its ef-
ficiency and scalability. We present new
state-of-the-art results in the word similar-
ity in context task and demonstrate its scal-
ability by training with one machine on a
corpus of nearly 1 billion tokens in less
than 6 hours.
1 Introduction
Representing words by dense, real-valued vector
embeddings, also commonly called ?distributed
representations,? helps address the curse of di-
mensionality and improve generalization because
they can place near each other words having sim-
ilar semantic and syntactic roles. This has been
shown dramatically in state-of-the-art results on
language modeling (Bengio et al, 2003; Mnih and
Hinton, 2007) as well as improvements in other
natural language processing tasks (Collobert and
Weston, 2008; Turian et al, 2010). Substantial
benefit arises when embeddings can be trained on
large volumes of data. Hence the recent consider-
able interest in the CBOW and Skip-gram models
*
The first two authors contributed equally to this paper.
of Mikolov et al (2013a); Mikolov et al (2013b)?
relatively simple log-linear models that can be
trained to produce high-quality word embeddings
on the entirety of English Wikipedia text in less
than half a day on one machine.
There is rising enthusiasm for applying these
models to improve accuracy in natural language
processing, much like Brown clusters (Brown et
al, 1992) have become common input features
for many tasks, such as named entity extraction
(Miller et al, 2004; Ratinov and Roth, 2009) and
parsing (Koo et al, 2008; T?ackstr?om et al, 2012).
In comparison to Brown clusters, the vector em-
beddings have the advantages of substantially bet-
ter scalability in their training, and intriguing po-
tential for their continuous and multi-dimensional
interrelations. In fact, Passos et al (2014) present
new state-of-the-art results in CoNLL 2003 named
entity extraction by directly inputting continuous
vector embeddings obtained by a version of Skip-
gram that injects supervision with lexicons. Sim-
ilarly Bansal et al (2014) show results in depen-
dency parsing using Skip-gram embeddings. They
have also recently been applied to machine trans-
lation (Zou et al, 2013; Mikolov et al, 2013c).
A notable deficiency in this prior work is that
each word type (e.g. the word string plant) has
only one vector representation?polysemy and
hononymy are ignored. This results in the word
plant having an embedding that is approximately
the average of its different contextual seman-
tics relating to biology, placement, manufactur-
ing and power generation. In moderately high-
dimensional spaces a vector can be relatively
?close? to multiple regions at a time, but this does
not negate the unfortunate influence of the triangle
inequality
2
here: words that are not synonyms but
are synonymous with different senses of the same
word will be pulled together. For example, pollen
and refinery will be inappropriately pulled to a dis-
2
For distance d, d(a, c) ? d(a, b) + d(b, c).
1059
tance not more than the sum of the distances plant?
pollen and plant?refinery. Fitting the constraints of
legitimate continuous gradations of semantics are
challenge enough without the additional encum-
brance of these illegitimate triangle inequalities.
Discovering embeddings for multiple senses per
word type is the focus of work by Reisinger and
Mooney (2010a) and Huang et al (2012). They
both pre-cluster the contexts of a word type?s to-
kens into discriminated senses, use the clusters to
re-label the corpus? tokens according to sense, and
then learn embeddings for these re-labeled words.
The second paper improves upon the first by em-
ploying an earlier pass of non-discriminated em-
bedding learning to obtain vectors used to rep-
resent the contexts. Note that by pre-clustering,
these methods lose the opportunity to jointly learn
the sense-discriminated vectors and the cluster-
ing. Other weaknesses include their fixed num-
ber of sense per word type, and the computational
expense of the two-step process?the Huang et
al (2012) method took one week of computation
to learn multiple embeddings for a 6,000 subset
of the 30,000 vocabulary on a corpus containing
close to billion tokens.
3
This paper presents a new method for learn-
ing vector-space embeddings for multiple senses
per word type, designed to provide several ad-
vantages over previous approaches. (1) Sense-
discriminated vectors are learned jointly with the
assignment of token contexts to senses; thus we
can use the emerging sense representation to more
accurately perform the clustering. (2) A non-
parametric variant of our method automatically
discovers a varying number of senses per word
type. (3) Efficient online joint training makes
it fast and scalable. We refer to our method as
Multiple-sense Skip-gram, or MSSG, and its non-
parametric counterpart as NP-MSSG.
Our method builds on the Skip-gram model
(Mikolov et al, 2013a), but maintains multiple
vectors per word type. During online training
with a particular token, we use the average of its
context words? vectors to select the token?s sense
that is closest, and perform a gradient update on
that sense. In the non-parametric version of our
method, we build on facility location (Meyerson,
2001): a new cluster is created with probability
proportional to the distance from the context to the
3
Personal communication with authors Eric H. Huang and
Richard Socher.
nearest sense.
We present experimental results demonstrating
the benefits of our approach. We show quali-
tative improvements over single-sense Skip-gram
and Huang et al (2012), comparing against word
neighbors from our parametric and non-parametric
methods. We present quantitative results in three
tasks. On both the SCWS and WordSim353 data
sets our methods surpass the previous state-of-
the-art. The Google Analogy task is not espe-
cially well-suited for word-sense evaluation since
its lack of context makes selecting the sense dif-
ficult; however our method dramatically outper-
forms Huang et al (2012) on this task. Finally
we also demonstrate scalabilty, learning multiple
senses, training on nearly a billion tokens in less
than 6 hours?a 27x improvement on Huang et al.
2 Related Work
Much prior work has focused on learning vector
representations of words; here we will describe
only those most relevant to understanding this pa-
per. Our work is based on neural language mod-
els, proposed by Bengio et al (2003), which extend
the traditional idea of n-gram language models by
replacing the conditional probability table with a
neural network, representing each word token by
a small vector instead of an indicator variable, and
estimating the parameters of the neural network
and these vectors jointly. Since the Bengio et al
(2003) model is quite expensive to train, much re-
search has focused on optimizing it. Collobert and
Weston (2008) replaces the max-likelihood char-
acter of the model with a max-margin approach,
where the network is encouraged to score the cor-
rect n-grams higher than randomly chosen incor-
rect n-grams. Mnih and Hinton (2007) replaces
the global normalization of the Bengio model with
a tree-structured probability distribution, and also
considers multiple positions for each word in the
tree.
More relevantly, Mikolov et al (2013a) and
Mikolov et al (2013b) propose extremely com-
putationally efficient log-linear neural language
models by removing the hidden layers of the neu-
ral networks and training from larger context win-
dows with very aggressive subsampling. The
goal of the models in Mikolov et al (2013a) and
Mikolov et al (2013b) is not so much obtain-
ing a low-perplexity language model as learn-
ing word representations which will be useful in
1060
downstream tasks. Neural networks or log-linear
models also do not appear to be necessary to
learn high-quality word embeddings, as Dhillon
and Ungar (2011) estimate word vector repre-
sentations using Canonical Correlation Analysis
(CCA).
Word vector representations or embeddings
have been used in various NLP tasks such
as named entity recognition (Neelakantan and
Collins, 2014; Passos et al, 2014; Turian et al,
2010), dependency parsing (Bansal et al, 2014),
chunking (Turian et al, 2010; Dhillon and Ungar,
2011), sentiment analysis (Maas et al, 2011), para-
phrase detection (Socher et al, 2011) and learning
representations of paragraphs and documents (Le
and Mikolov, 2014). The word clusters obtained
from Brown clustering (Brown et al, 1992) have
similarly been used as features in named entity
recognition (Miller et al, 2004; Ratinov and Roth,
2009) and dependency parsing (Koo et al, 2008),
among other tasks.
There is considerably less prior work on learn-
ing multiple vector representations for the same
word type. Reisinger and Mooney (2010a) intro-
duce a method for constructing multiple sparse,
high-dimensional vector representations of words.
Huang et al (2012) extends this approach incor-
porating global document context to learn mul-
tiple dense, low-dimensional embeddings by us-
ing recursive neural networks. Both the meth-
ods perform word sense discrimination as a pre-
processing step by clustering contexts for each
word type, making training more expensive.
While methods such as those described in Dhillon
and Ungar (2011) and Reddy et al (2011) use
token-specific representations of words as part
of the learning algorithm, the final outputs are
still one-to-one mappings between word types and
word embeddings.
3 Background: Skip-gram model
The Skip-gram model learns word embeddings
such that they are useful in predicting the sur-
rounding words in a sentence. In the Skip-gram
model, v(w) ? R
d
is the vector representation of
the word w ? W , where W is the words vocabu-
lary and d is the embedding dimensionality.
Given a pair of words (w
t
, c), the probability
that the word c is observed in the context of word
w
t
is given by,
P (D = 1|v(w
t
), v(c)) =
1
1 + e
?v(w
t
)
T
v(c)
(1)
The probability of not observing word c in the con-
text of w
t
is given by,
P (D = 0|v(w
t
), v(c)) =
1? P (D = 1|v(w
t
), v(c))
Given a training set containing the sequence of
word types w
1
, w
2
, . . . , w
T
, the word embeddings
are learned by maximizing the following objective
function:
J(?) =
?
(w
t
,c
t
)?D
+
?
c?c
t
logP (D = 1|v(w
t
), v(c))
+
?
(w
t
,c
?
t
)?D
?
?
c
?
?c
?
t
logP (D = 0|v(w
t
), v(c
?
))
where w
t
is the t
th
word in the training set, c
t
is the set of observed context words of word w
t
and c
?
t
is the set of randomly sampled, noisy con-
text words for the word w
t
. D
+
consists of
the set of all observed word-context pairs (w
t
, c
t
)
(t = 1, 2 . . . , T ). D
?
consists of pairs (w
t
, c
?
t
)
(t = 1, 2 . . . , T ) where c
?
t
is the set of randomly
sampled, noisy context words for the word w
t
.
For each training word w
t
, the set of context
words c
t
= {w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
}
includesR
t
words to the left and right of the given
word as shown in Figure 1. R
t
is the window size
considered for the word w
t
uniformly randomly
sampled from the set {1, 2, . . . , N}, where N is
the maximum context window size.
The set of noisy context words c
?
t
for the word
w
t
is constructed by randomly sampling S noisy
context words for each word in the context c
t
. The
noisy context words are randomly sampled from
the following distribution,
P (w) =
p
unigram
(w)
3/4
Z
(2)
where p
unigram
(w) is the unigram distribution of
the words and Z is the normalization constant.
4 Multi-Sense Skip-gram (MSSG) model
To extend the Skip-gram model to learn multiple
embeddings per word we follow previous work
(Huang et al, 2012; Reisinger and Mooney, 2010a)
1061
Word 
Vector
word w
t
v(w
t+2
)
Context   
Vectors
v(w
t+1
)
v(w
t-1
)
v(w
t-2
)
v(w
t
)
Figure 1: Architecture of the Skip-gram model
with window size R
t
= 2. Context c
t
of word
w
t
consists of w
t?1
, w
t?2
, w
t+1
, w
t+2
.
and let each sense of word have its own embed-
ding, and induce the senses by clustering the em-
beddings of the context words around each token.
The vector representation of the context is the av-
erage of its context words? vectors. For every word
type, we maintain clusters of its contexts and the
sense of a word token is predicted as the cluster
that is closest to its context representation. After
predicting the sense of a word token, we perform
a gradient update on the embedding of that sense.
The crucial difference from previous approaches
is that word sense discrimination and learning em-
beddings are performed jointly by predicting the
sense of the word using the current parameter es-
timates.
In the MSSG model, each word w ? W is
associated with a global vector v
g
(w) and each
sense of the word has an embedding (sense vec-
tor) v
s
(w, k) (k = 1, 2, . . . ,K) and a context clus-
ter with center ?(w, k) (k = 1, 2, . . . ,K). The K
sense vectors and the global vectors are of dimen-
sion d and K is a hyperparameter.
Consider the word w
t
and let c
t
=
{w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
} be the
set of observed context words. The vector repre-
sentation of the context is defined as the average
of the global vector representation of the words in
the context. Let v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
be the vector representation of the context c
t
. We
use the global vectors of the context words instead
of its sense vectors to avoid the computational
complexity associated with predicting the sense
of the context words. We predict s
t
, the sense
Word 6ense 
Vectors
v(w
t
2)
v
J
(w
t+2
)
Context   
Vectors
v
J
(w
t+1
)
 v
J
(w
t-1
)
v
J
(w
t-2
)
$verDJe Context 
Vector
Context COXster 
Centers
v(w
t
1)
v(w
t
)
3redLcted 
6ense s
t
?(w
t
1)
v
context
(c
t
)
 
 
 
?(w
t
2)
?(w
t
)
 
Context   
Vectors
v
J
(w
t+2
)
v
J
(w
t+1
)
v
J
(w
t-1
)
v
J
(w
t-2
)
Figure 2: Architecture of Multi-Sense Skip-gram
(MSSG) model with window size R
t
= 2 and
K = 3. Context c
t
of word w
t
consists of
w
t?1
, w
t?2
, w
t+1
, w
t+2
. The sense is predicted by
finding the cluster center of the context that is clos-
est to the average of the context vectors.
of word w
t
when observed with context c
t
as
the context cluster membership of the vector
v
context
(c
t
) as shown in Figure 2. More formally,
s
t
= argmax
k=1,2,...,K
sim(?(w
t
, k), v
context
(c
t
)) (3)
The hard cluster assignment is similar to the k-
means algorithm. The cluster center is the aver-
age of the vector representations of all the contexts
which belong to that cluster. For sim we use co-
sine similarity in our experiments.
Here, the probability that the word c is observed
in the context of word w
t
given the sense of the
word w
t
is,
P (D = 1|s
t
,v
s
(w
t
, 1), . . . , v
s
(w
t
,K), v
g
(c))
= P (D = 1|v
s
(w
t
, s
t
), v
g
(c))
=
1
1 + e
?v
s
(w
t
,s
t
)
T
v
g
(c)
The probability of not observing word c in the con-
text of w
t
given the sense of the word w
t
is,
P (D = 0|s
t
,v
s
(w
t
, 1), . . . , v
s
(w
t
,K), v
g
(c))
= P (D = 0|v
s
(w
t
, s
t
), v
g
(c))
= 1? P (D = 1|v
s
(w
t
, s
t
), v
g
(c))
Given a training set containing the sequence of
word types w
1
, w
2
, ..., w
T
, the word embeddings
are learned by maximizing the following objective
1062
Algorithm 1 Training Algorithm of MSSG model
1: Input: w
1
, w
2
, ..., w
T
, d, K, N .
2: Initialize v
s
(w, k) and v
g
(w), ?w ? W,k ?
{1, . . . ,K} randomly, ?(w, k) ?w ? W,k ?
{1, . . . ,K} to 0.
3: for t = 1, 2, . . . , T do
4: R
t
? {1, . . . , N}
5: c
t
= {w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
}
6: v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
7: s
t
= argmax
k=1,2,...,K
{
sim(?(w
t
, k), v
context
(c
t
))}
8: Update context cluster center ?(w
t
, s
t
)
since context c
t
is added to context cluster s
t
of word w
t
.
9: c
?
t
= Noisy Samples(c
t
)
10: Gradient update on v
s
(w
t
, s
t
), global vec-
tors of words in c
t
and c
?
t
.
11: end for
12: Output: v
s
(w, k), v
g
(w) and context cluster
centers ?(w, k), ?w ?W,k ? {1, . . . ,K}
function:
J(?) =
?
(w
t
,c
t
)?D
+
?
c?c
t
logP (D = 1|v
s
(w
t
, s
t
), v
g
(c))+
?
(w
t
,c
?
t
)?D
?
?
c
?
?c
?
t
logP (D = 0|v
s
(w
t
, s
t
), v
g
(c
?
))
where w
t
is the t
th
word in the sequence, c
t
is the
set of observed context words and c
?
t
is the set of
noisy context words for the word w
t
. D
+
and D
?
are constructed in the same way as in the Skip-
gram model.
After predicting the sense of word w
t
, we up-
date the embedding of the predicted sense for
the word w
t
(v
s
(w
t
, s
t
)), the global vector of the
words in the context and the global vector of the
randomly sampled, noisy context words. The con-
text cluster center of cluster s
t
for the word w
t
(?(w
t
, s
t
)) is updated since context c
t
is added to
the cluster s
t
.
5 Non-Parametric MSSG model
(NP-MSSG)
The MSSG model learns a fixed number of senses
per word type. In this section, we describe a
non-parametric version of MSSG, the NP-MSSG
model, which learns varying number of senses per
word type. Our approach is closely related to
the online non-parametric clustering procedure de-
scribed in Meyerson (2001). We create a new clus-
ter (sense) for a word type with probability propor-
tional to the distance of its context to the nearest
cluster (sense).
Each wordw ?W is associated with sense vec-
tors, context clusters and a global vector v
g
(w) as
in the MSSG model. The number of senses for a
word is unknown and is learned during training.
Initially, the words do not have sense vectors and
context clusters. We create the first sense vector
and context cluster for each word on its first occur-
rence in the training data. After creating the first
context cluster for a word, a new context cluster
and a sense vector are created online during train-
ing when the word is observed with a context were
the similarity between the vector representation of
the context with every existing cluster center of the
word is less than ?, where ? is a hyperparameter
of the model.
Consider the word w
t
and let c
t
=
{w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
} be the
set of observed context words. The vector repre-
sentation of the context is defined as the average
of the global vector representation of the words in
the context. Let v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
be the vector representation of the context c
t
. Let
k(w
t
) be the number of context clusters or the
number of senses currently associated with word
w
t
. s
t
, the sense of word w
t
when k(w
t
) > 0 is
given by
s
t
=
?
?
?
?
?
k(w
t
) + 1, ifmax
k=1,2,...,k(w
t
)
{sim
(?(w
t
, k), v
context
(c
t
))} < ?
k
max
, otherwise
(4)
where ?(w
t
, k) is the cluster center of
the k
th
cluster of word w
t
and k
max
=
argmax
k=1,2,...,k(w
t
)
sim(?(w
t
, k), v
context
(c
t
)).
The cluster center is the average of the vector
representations of all the contexts which belong to
that cluster. If s
t
= k(w
t
) + 1, a new context
cluster and a new sense vector are created for the
word w
t
.
The NP-MSSG model and the MSSG model
described previously differ only in the way word
sense discrimination is performed. The objec-
tive function and the probabilistic model associ-
ated with observing a (word, context) pair given
the sense of the word remain the same.
1063
Model Time (in hours)
Huang et al 168
MSSG 50d 1
MSSG-300d 6
NP-MSSG-50d 1.83
NP-MSSG-300d 5
Skip-gram-50d 0.33
Skip-gram-300d 1.5
Table 1: Training Time Results. First five model
reported in the table are capable of learning mul-
tiple embeddings for each word and Skip-gram
is capable of learning only single embedding for
each word.
6 Experiments
To evaluate our algorithms we train embeddings
using the same corpus and vocabulary as used in
Huang et al (2012), which is the April 2010 snap-
shot of the Wikipedia corpus (Shaoul and West-
bury, 2010). It contains approximately 2 million
articles and 990 million tokens. In all our experi-
ments we remove all the words with less than 20
occurrences and use a maximum context window
(N ) of length 5 (5 words before and after the word
occurrence). We fix the number of senses (K) to
be 3 for the MSSG model unless otherwise speci-
fied. Our hyperparameter values were selected by
a small amount of manual exploration on a vali-
dation set. In NP-MSSG we set ? to -0.5. The
Skip-gram model, MSSG and NP-MSSG models
sample one noisy context word (S) for each of the
observed context words. We train our models us-
ing AdaGrad stochastic gradient decent (Duchi et
al, 2011) with initial learning rate set to 0.025.
Similarly to Huang et al (2012), we don?t use a
regularization penalty.
Below we describe qualitative results, display-
ing the embeddings and the nearest neighbors of
each word sense, and quantitative experiments in
two benchmark word similarity tasks.
Table 1 shows time to train our models, com-
pared with other models from previous work. All
these times are from single-machine implementa-
tions running on similar-sized corpora. We see
that our model shows significant improvement in
the training time over the model in Huang et
al (2012), being within well within an order-of-
magnitude of the training time for Skip-gram mod-
els.
APPLE
Skip-gram blackberry, macintosh, acorn, pear, plum
MSSG
pear, honey, pumpkin, potato, nut
microsoft, activision, sony, retail, gamestop
macintosh, pc, ibm, iigs, chipsets
NP-MSSG
apricot, blackberry, cabbage, blackberries, pear
microsoft, ibm, wordperfect, amiga, trs-80
FOX
Skip-gram abc, nbc, soapnet, espn, kttv
MSSG
beaver, wolf, moose, otter, swan
nbc, espn, cbs, ctv, pbs
dexter, myers, sawyer, kelly, griffith
NP-MSSG
rabbit, squirrel, wolf, badger, stoat
cbs,abc, nbc, wnyw, abc-tv
NET
Skip-gram profit, dividends, pegged, profits, nets
MSSG
snap, sideline, ball, game-trying, scoring
negative, offset, constant, hence, potential
pre-tax, billion, revenue, annualized, us$
NP-MSSG
negative, total, transfer, minimizes, loop
pre-tax, taxable, per, billion, us$, income
ball, yard, fouled, bounced, 50-yard
wnet, tvontorio, cable, tv, tv-5
ROCK
Skip-gram glam, indie, punk, band, pop
MSSG
rocks, basalt, boulders, sand, quartzite
alternative, progressive, roll, indie, blues-rock
rocks, pine, rocky, butte, deer
NP-MSSG
granite, basalt, outcropping, rocks, quartzite
alternative, indie, pop/rock, rock/metal, blues-rock
RUN
Skip-gram running, ran, runs, afoul, amok
MSSG
running, stretch, ran, pinch-hit, runs
operated , running, runs, operate, managed
running, runs, operate, drivers, configure
NP-MSSG
two-run, walk-off, runs, three-runs, starts
operated, runs, serviced, links, walk
running, operating, ran, go, configure
re-election, reelection, re-elect, unseat, term-limited
helmed, longest-running, mtv, promoted, produced
Table 2: Nearest neighbors of each sense of each
word, by cosine similarity, for different algo-
rithms. Note that the different senses closely cor-
respond to intuitions regarding the senses of the
given word types.
6.1 Nearest Neighbors
Table 2 shows qualitatively the results of dis-
covering multiple senses by presenting the near-
est neighbors associated with various embeddings.
The nearest neighbors of a word are computed by
comparing the cosine similarity between the em-
bedding for each sense of the word and the context
embeddings of all other words in the vocabulary.
Note that each of the discovered senses are indeed
semantically coherent, and that a reasonable num-
ber of senses are created by the non-parametric
method. Table 3 shows the nearest neighbors of
the word plant for Skip-gram, MSSG , NP-MSSG
and Haung?s model (Huang et al, 2012).
1064
Skip-
gram
plants, flowering, weed, fungus, biomass
MS
-SG
plants, tubers, soil, seed, biomass
refinery, reactor, coal-fired, factory, smelter
asteraceae, fabaceae, arecaceae, lamiaceae, eri-
caceae
NP
MS
-SG
plants, seeds, pollen, fungal, fungus
factory, manufacturing, refinery, bottling, steel
fabaceae, legume, asteraceae, apiaceae, flowering
power, coal-fired, hydro-power, hydroelectric, re-
finery
Hua
-ng
et al
insect, capable, food, solanaceous, subsurface
robust, belong, pitcher, comprises, eagles
food, animal, catching, catch, ecology, fly
seafood, equipment, oil, dairy, manufacturer
facility, expansion, corporation, camp, co.
treatment, skin, mechanism, sugar, drug
facility, theater, platform, structure, storage
natural, blast, energy, hurl, power
matter, physical, certain, expression, agents
vine, mute, chalcedony, quandong, excrete
Table 3: Nearest Neighbors of the word plant
for different models. We see that the discovered
senses in both our models are more semantically
coherent than Huang et al (2012) and NP-MSSG
is able to learn reasonable number of senses.
6.2 Word Similarity
We evaluate our embeddings on two related
datasets: the WordSim-353 (Finkelstein et al,
2001) dataset and the Contextual Word Similari-
ties (SCWS) dataset Huang et al (2012).
WordSim-353 is a standard dataset for evaluat-
ing word vector representations. It consists of a
list of pairs of word types, the similarity of which
is rated in an integral scale from 1 to 10. Pairs
include both monosemic and polysemic words.
These scores to each word pairs are given with-
out any contextual information, which makes them
tricky to interpret.
To overcome this issue, Stanford?s Contextual
Word Similarities (SCWS) dataset was developed
by Huang et al (2012). The dataset consists of
2003 word pairs and their sentential contexts. It
consists of 1328 noun-noun pairs, 399 verb-verb
pairs, 140 verb-noun, 97 adjective-adjective, 30
noun-adjective, 9 verb-adjective, and 241 same-
word pairs. We evaluate and compare our embed-
dings on both WordSim-353 and SCWS word sim-
ilarity corpus.
Since it is not trivial to deal with multiple em-
beddings per word, we consider the following sim-
ilarity measures between words w and w
?
given
their respective contexts c and c
?
, where P (w, c, k)
is the probability that w takes the k
th
sense given
the context c, and d(v
s
(w, i), v
s
(w
?
, j)) is the sim-
ilarity measure between the given embeddings
v
s
(w, i) and v
s
(w
?
, j).
The avgSim metric,
avgSim(w,w
?
)
=
1
K
2
K
?
i=1
K
?
j=1
d (v
s
(w, i), v
s
(w
?
, j)) ,
computes the average similarity over all embed-
dings for each word, ignoring information from
the context.
To address this, the avgSimC metric,
avgSimC(w,w
?
) =
K
?
j=1
K
?
i=1
P (w, c, i)P (w
?
, c
?
, j)
? d (v
s
(w, i), v
s
(w
?
, j))
weighs the similarity between each pair of senses
by how well does each sense fit the context at
hand.
The globalSim metric uses each word?s global
context vector, ignoring the many senses:
globalSim(w,w
?
) = d (v
g
(w), v
g
(w
?
)) .
Finally, localSim metric selects a single sense
for each word based independently on its context
and computes the similarity by
localSim(w,w
?
) = d (v
s
(w, k), v
s
(w
?
, k
?
)) ,
where k = argmax
i
P (w, c, i) and k
?
=
argmax
j
P (w
?
, c
?
, j) and P (w, c, i) is the prob-
ability that w takes the i
th
sense given context c.
The probability of being in a cluster is calculated
as the inverse of the cosine distance to the cluster
center (Huang et al, 2012).
We report the Spearman correlation between a
model?s similarity scores and the human judge-
ments in the datasets.
Table 5 shows the results on WordSim-353
task. C&W refers to the language model by Col-
lobert and Weston (2008) and HLBL model is the
method described in Mnih and Hinton (2007). On
WordSim-353 task, we see that our model per-
forms significantly better than the previous neural
network model for learning multi-representations
per word (Huang et al, 2012). Among the meth-
ods that learn low-dimensional and dense repre-
sentations, our model performs slightly better than
Skip-gram. Table 4 shows the results for the
SCWS task. In this task, when the words are
1065
Model globalSim avgSim avgSimC localSim
TF-IDF 26.3 - - -
Collobort & Weston-50d 57.0 - - -
Skip-gram-50d 63.4 - - -
Skip-gram-300d 65.2 - - -
Pruned TF-IDF 62.5 60.4 60.5 -
Huang et al-50d 58.6 62.8 65.7 26.1
MSSG-50d 62.1 64.2 66.9 49.17
MSSG-300d 65.3 67.2 69.3 57.26
NP-MSSG-50d 62.3 64.0 66.1 50.27
NP-MSSG-300d 65.5 67.3 69.1 59.80
Table 4: Experimental results in the SCWS task. The numbers are Spearmans correlation ? ? 100
between each model?s similarity judgments and the human judgments, in context. First three models
learn only a single embedding per model and hence, avgSim, avgSimC and localSim are not reported
for these models, as they?d be identical to globalSim. Both our parametric and non-parametric models
outperform the baseline models, and our best model achieves a score of 69.3 in this task. NP-MSSG
achieves the best results when globalSim, avgSim and localSim similarity measures are used. The best
results according to each metric are in bold face.
Model ?? 100
HLBL 33.2
C&W 55.3
Skip-gram-300d 70.4
Huang et al-G 22.8
Huang et al-M 64.2
MSSG 50d-G 60.6
MSSG 50d-M 63.2
MSSG 300d-G 69.2
MSSG 300d-M 70.9
NP-MSSG 50d-G 61.5
NP-MSSG 50d-M 62.4
NP-MSSG 300d-G 69.1
NP-MSSG 300d-M 68.6
Pruned TF-IDF 73.4
ESA 75
Tiered TF-IDF 76.9
Table 5: Results on the WordSim-353 dataset.
The table shows the Spearmans correlation ? be-
tween the model?s similarities and human judg-
ments. G indicates the globalSim similarity mea-
sure and M indicates avgSim measure.The best
results among models that learn low-dimensional
and dense representations are in bold face. Pruned
TF-IDF (Reisinger and Mooney, 2010a), ESA
(Gabrilovich and Markovitch, 2007) and Tiered
TF-IDF (Reisinger and Mooney, 2010b) construct
spare, high-dimensional representations.
Figure 3: The plot shows the distribution of num-
ber of senses learned per word type in NP-MSSG
model
given with their context, our model achieves new
state-of-the-art results on SCWS as shown in the
Table-4. The previous state-of-art model (Huang
et al, 2012) on this task achieves 65.7% using
the avgSimC measure, while the MSSG model
achieves the best score of 69.3% on this task. The
results on the other metrics are similar. For a
fixed embedding dimension, the model by Huang
et al (2012) has more parameters than our model
since it uses a hidden layer. The results show
that our model performs better than Huang et al
(2012) even when both the models use 50 dimen-
sional vectors and the performance of our model
improves as we increase the number of dimensions
to 300.
We evaluate the models in a word analogy task
1066
(a) (b)
Figure 4: Figures (a) and (b) show the effect of varying embedding dimensionality and number of senses
respectively of the MSSG Model on the SCWS task.
Model Task Sim ?? 100
Skip-gram WS-353 globalSim 70.4
MSSG WS-353 globalSim 68.4
MSSG WS-353 avgSim 71.2
NP MSSG WS-353 globalSim 68.3
NP MSSG WS-353 avgSim 69.66
MSSG SCWS localSim 59.3
MSSG SCWS globalSim 64.7
MSSG SCWS avgSim 67.2
MSSG SCWS avgSimC 69.2
NP MSSG SCWS localSim 60.11
NP MSSG SCWS globalSim 65.3
NP MSSG SCWS avgSim 67
NP MSSG SCWS avgSimC 68.6
Table 6: Experiment results on WordSim-353 and
SCWS Task. Multiple Embeddings are learned for
top 30,000 most frequent words in the vocabulary.
The embedding dimension size is 300 for all the
models for this task. The number of senses for
MSSG model is 3.
introduced by Mikolov et al (2013a) where both
MSSG and NP-MSSG models achieve 64% accu-
racy compared to 12% accuracy by Huang et al
(2012). Skip-gram which is the state-of-art model
for this task achieves 67% accuracy.
Figure 3 shows the distribution of number of
senses learned per word type in the NP-MSSG
model. We learn the multiple embeddings for the
same set of approximately 6000 words that were
used in Huang et al (2012) for all our experiments
to ensure fair comparision. These approximately
6000 words were choosen by Huang et al. mainly
from the top 30,00 frequent words in the vocab-
ulary. This selection was likely made to avoid
the noise of learning multiple senses for infre-
quent words. However, our method is robust to
noise, which can be seen by the good performance
of our model that learns multiple embeddings for
the top 30,000 most frequent words. We found
that even by learning multiple embeddings for the
top 30,000 most frequent words in the vocubu-
lary, MSSG model still achieves state-of-art result
on SCWS task with an avgSimC score of 69.2 as
shown in Table 6.
7 Conclusion
We present an extension to the Skip-gram model
that efficiently learns multiple embeddings per
word type. The model jointly performs word
sense discrimination and embedding learning, and
non-parametrically estimates the number of senses
per word type. Our method achieves new state-
of-the-art results in the word similarity in con-
text task and learns multiple senses, training on
close to billion tokens in less than 6 hours. The
global vectors, sense vectors and cluster centers of
our model and code for learning them are avail-
able at https://people.cs.umass.edu/
?
arvind/emnlp2014wordvectors. In fu-
ture work we plan to use the multiple embeddings
per word type in downstream NLP tasks.
1067
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
DARPA under agreement number FA8750-13-2-
0020. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
those of the sponsor.
References
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring Continuous Word Representations
for Dependency Parsing. Association for Computa-
tional Linguistics (ACL).
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR).
Peter F. Brown, Peter V. Desouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based N-gram models of natural language
Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Process-
ing: Deep Neural Networks with Multitask Learn-
ing. International Conference on Machine learning
(ICML).
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-View Learning of Word Embeddings via
CCA. Advances in Neural Information Processing
Systems (NIPS).
John Duchi, Elad Hazan, and Yoram Singer 2011.
Adaptive sub- gradient methods for online learn-
ing and stochastic optimization. Journal of Machine
Learning Research (JMLR).
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the con-
cept revisited. International Conference on World
Wide Web (WWW).
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. International Joint
Conference on Artificial Intelligence (IJCAI).
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. Association of Computational
Linguistics (ACL).
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Parsing.
Association for Computational Linguistics (ACL).
Quoc V. Le and Tomas Mikolov. 2014 Distributed
Representations of Sentences and Documents. Inter-
national Conference on Machine Learning (ICML)
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011 Learning Word Vectors for Sentiment Analysis
Association for Computational Linguistics (ACL)
Adam Meyerson. 2001 IEEE Symposium on Foun-
dations of Computer Science. International Confer-
ence on Machine Learning (ICML)
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient Estimation of Word
Representations in Vector Space. Workshop at In-
ternational Conference on Learning Representations
(ICLR).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed Repre-
sentations of Words and Phrases and their Composi-
tionality. Advances in Neural Information Process-
ing Systems (NIPS).
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013c. Exploiting Similarities among Languages
for Machine Translation. arXiv.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT).
Andriy Mnih and Geoffrey Hinton. 2007. Three
new graphical models for statistical language mod-
elling. International Conference on Machine learn-
ing (ICML).
Arvind Neelakantan and Michael Collins. 2014.
Learning Dictionaries for Named Entity Recogni-
tion using Minimal Supervision. European Chap-
ter of the Association for Computational Linguistics
(EACL).
Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon Infused Phrase Embeddings for
Named Entity Resolution. Conference on Natural
Language Learning (CoNLL).
Lev Ratinov and Dan Roth. 2009. Design Chal-
lenges and Misconceptions in Named Entity Recog-
nition. Conference on Natural Language Learning
(CoNLL).
Siva Reddy, Ioannis P. Klapaftis, and Diana McCarthy.
2011. Dynamic and Static Prototype Vectors for Se-
mantic Composition. International Joint Conference
on Artificial Intelligence (IJCNLP).
1068
Joseph Reisinger and Raymond J. Mooney. 2010a.
Multi-prototype vector-space models of word mean-
ing. North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT)
Joseph Reisinger and Raymond Mooney. 2010b. A
mixture model with sharing for lexical semantics.
Empirical Methods in Natural Language Processing
(EMNLP).
Cyrus Shaoul and Chris Westbury. 2010. The Westbury
lab wikipedia corpus.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. Advances in Neu-
ral Information Processing Systems (NIPS).
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkor-
eit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. Association
for Computational Linguistics (ACL).
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual Word Embed-
dings for Phrase-Based Machine Translation. Em-
pirical Methods in Natural Language Processing.
1069
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 452?461,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Learning Dictionaries for Named Entity Recognition using Minimal
Supervision
Arvind Neelakantan
Department of Computer Science
University of Massachusetts, Amherst
Amherst, MA, 01003
arvind@cs.umass.edu
Michael Collins
Department of Computer Science
Columbia University
New-York, NY 10027, USA
mcollins@cs.columbia.edu
Abstract
This paper describes an approach for au-
tomatic construction of dictionaries for
Named Entity Recognition (NER) using
large amounts of unlabeled data and a few
seed examples. We use Canonical Cor-
relation Analysis (CCA) to obtain lower
dimensional embeddings (representations)
for candidate phrases and classify these
phrases using a small number of labeled
examples. Our method achieves 16.5%
and 11.3% F-1 score improvement over
co-training on disease and virus NER re-
spectively. We also show that by adding
candidate phrase embeddings as features
in a sequence tagger gives better perfor-
mance compared to using word embed-
dings.
1 Introduction
Several works (e.g., Ratinov and Roth, 2009; Co-
hen and Sarawagi, 2004) have shown that inject-
ing dictionary matches as features in a sequence
tagger results in significant gains in NER perfor-
mance. However, building these dictionaries re-
quires a huge amount of human effort and it is of-
ten difficult to get good coverage for many named
entity types. The problem is more severe when we
consider named entity types such as gene, virus
and disease, because of the large (and growing)
number of names in use, the fact that the names are
heavily abbreviated and multiple names are used
to refer to the same entity (Leaman et al., 2010;
Dogan and Lu, 2012). Also, these dictionaries can
only be built by domain experts, making the pro-
cess very expensive.
This paper describes an approach for automatic
construction of dictionaries for NER using large
amounts of unlabeled data and a small number
of seed examples. Our approach consists of two
steps. First, we collect a high recall, low preci-
sion list of candidate phrases from the large unla-
beled data collection for every named entity type
using simple rules. In the second step, we con-
struct an accurate dictionary of named entities by
removing the noisy candidates from the list ob-
tained in the first step. This is done by learning a
classifier using the lower dimensional, real-valued
CCA (Hotelling, 1935) embeddings of the can-
didate phrases as features and training it using a
small number of labeled examples. The classifier
we use is a binary SVM which predicts whether a
candidate phrase is a named entity or not.
We compare our method to a widely used semi-
supervised algorithm based on co-training (Blum
and Mitchell, 1998). The dictionaries are first
evaluated on virus (GENIA, 2003) and disease
(Dogan and Lu, 2012) NER by using them directly
in dictionary based taggers. We also give results
comparing the dictionaries produced by the two
semi-supervised approaches with dictionaries that
are compiled manually. The effectiveness of the
dictionaries are also measured by injecting dictio-
nary matches as features in a Conditional Random
Field (CRF) based tagger. The results indicate
that our approach with minimal supervision pro-
duces dictionaries that are comparable to dictio-
naries compiled manually. Finally, we also com-
pare the quality of the candidate phrase embed-
dings with word embeddings (Dhillon et al., 2011)
by adding them as features in a CRF based se-
quence tagger.
2 Background
We first give background on Canonical Correla-
tion Analysis (CCA), and then give background on
452
CRFs for the NER problem.
2.1 Canonical Correlation Analysis (CCA)
The input to CCA consists of n paired observa-
tions (x
1
, z
1
), . . . , (x
n
, z
n
) where x
i
? R
d
1
, z
i
?
R
d
2
(?i ? {1, 2, . . . , n}) are the feature represen-
tations for the two views of a data point. CCA
simultaneously learns projection matrices ?
1
?
R
d
1
?k
,?
2
? R
d
2
?k
(k is a small number) which
are used to obtain the lower dimensional represen-
tations (x?
1
, z?
1
), . . . , (x?
n
, z?
n
) where x?
i
= ?
T
1
x
i
?
R
k
, z?
i
= ?
T
2
z
i
? R
k
, ?i ? {1, 2, . . . , n}. ?
1
,?
2
are chosen to maximize the correlation between x?
i
and z?
i
, ?i ? {1, 2, . . . , n}.
Consider the setting where we have a label for
the data point along with it?s two views and ei-
ther view is sufficient to make accurate predic-
tions. Kakade and Foster (2007) and Sridharan
and Kakade (2008) give strong theoretical guaran-
tees when the lower dimensional embeddings from
CCA are used for predicting the label of the data
point. This setting is similar to the one considered
in co-training (Collins and Singer, 1999) but there
is no assumption of independence between the two
views of the data point. Also, it is an exact al-
gorithm unlike the algorithm given in Collins and
Singer (1999). Since we are using lower dimen-
sional embeddings of the data point for prediction,
we can learn a predictor with fewer labeled exam-
ples.
2.2 CRFs for Named Entity Recognition
CRF based sequence taggers have been used for
a number of NER tasks (e.g., McCallum and Li,
2003) and in particular for biomedical NER (e.g.,
McDonald and Pereira, 2005; Burr Settles, 2004)
because they allow a great deal of flexibility in the
features which can be included. The input to a
CRF tagger is a sentence (w
1
, w
2
, . . . , w
n
) where
w
i
, ?i ? {1, 2, . . . , n} are words in the sentence.
The output is a sequence of tags y
1
, y
2
, . . . , y
n
where y
i
? {B, I, O}, ?i ? {1, 2, . . . , n}. B
is the tag given to the first word in a named entity,
I is the tag given to all words except the first word
in a named entity and O is the tag given to all other
words. We used the standard NER baseline fea-
tures (e.g., Dhillon et al., 2011; Ratinov and Roth,
2009) which include:
? Current Word w
i
and its lexical features
which include whether the word is capital-
ized and whether all the characters are cap-
italized. Prefix and suffixes of the word w
i
were also added.
? Word tokens in window of size two
around the current word which include
w
i?2
, w
i?1
, w
i+1
, w
i+2
and also the capital-
ization pattern in the window.
? Previous two predictions y
i?1
and y
i?2
.
The effectiveness of the dictionaries are evaluated
by adding dictionary matches as features along
with the baseline features (Ratinov and Roth,
2009; Cohen and Sarawagi, 2004) in the CRF tag-
ger. We also compared the quality of the candi-
date phrase embeddings with the word-level em-
beddings by adding them as features (Dhillon et
al., 2011) along with the baseline features in the
CRF tagger.
3 Method
This section describes the two steps in our ap-
proach: obtaining candidate phrases and classify-
ing them.
3.1 Obtaining Candidate Phrases
We used the full text of 110,369 biomedical pub-
lications in the BioMed Central corpus
1
to get the
high recall, low precision list of candidate phrases.
The advantages of using this huge collection of
publications are obvious: almost all (including
rare) named entities related to the biomedical do-
main will be mentioned and contains more re-
cent developments than a structured resource like
Wikipedia. The challenge however is that these
publications are unstructured and hence it is a dif-
ficult task to construct accurate dictionaries using
them with minimal supervision.
The list of virus candidate phrases were ob-
tained by extracting phrases that occur between
?the? and ?virus? in the simple pattern ?the ...
virus? during a single pass over the unlabeled doc-
ument collection. This noisy list had a lot of virus
names such as influenza, human immunodeficiency
and Epstein-Barr along with phrases that are not
virus names, like mutant, same, new, and so on.
A similar rule like ?the ... disease? did not give
a good coverage of disease names since it is not
the common way of how diseases are mentioned
in publications. So we took a different approach
1
The corpus can be downloaded at
http://www.biomedcentral.com/about/datamining
453
to obtain the noisy list of disease names. We col-
lected every sentence in the unlabeled data col-
lection that has the word ?disease? in it and ex-
tracted noun phrases
2
following the patterns ?dis-
eases like ....?, ?diseases such as ....? , ?diseases in-
cluding ....? , ?diagnosed with ....?, ?patients with
....? and ?suffering from ....?.
3.2 Classification of Candidate Phrases
Having found the list of candidate phrases, we
now describe how noisy words are filtered out
from them. We gather (spelling, context) pairs for
every instance of a candidate phrase in the unla-
beled data collection. spelling refers to the can-
didate phrase itself while context includes three
words each to the left and the right of the candidate
phrase in the sentence. The spelling and the con-
text of the candidate phrase provide a natural split
into two views which multi-view algorithms like
co-training and CCA can exploit. The only super-
vision in our method is to provide a few spelling
seed examples (10 in the case of virus, 18 in the
case of disease), for example, human immunodefi-
ciency is a virus and mutant is not a virus.
3.2.1 Approach using CCA embeddings
We use CCA described in the previous section
to obtain lower dimensional embeddings for the
candidate phrases using the (spelling, context)
views. Unlike previous works such as Dhillon et
al. (2011) and Dhillon et al. (2012), we use CCA to
learn embeddings for candidate phrases instead of
all words in the vocabulary so that we don?t miss
named entities which have two or more words.
Let the number of (spelling, context) pairs be n
(sum of total number of instances of every can-
didate phrase in the unlabeled data collection).
First, we map the spelling and context to high-
dimensional feature vectors. For the spelling view,
we define a feature for every candidate phrase and
also a boolean feature which indicates whether the
phrase is capitalized or not. For the context view,
we use features similar to Dhillon et al. (2011)
where a feature for every word in the context in
conjunction with its position is defined. Each
of the n (spelling, context) pairs are mapped to
a pair of high-dimensional feature vectors to get
n paired observations (x
1
, z
1
), . . . , (x
n
, z
n
) with
x
i
? R
d
1
, z
i
? R
d
2
, ?i ? {1, 2, . . . , n} (d
1
, d
2
are the feature space dimensions of the spelling
2
Noun phrases were obtained using
http://www.umiacs.umd.edu/ hal/TagChunk/
and context view respectively). Using CCA
3
, we
learn the projection matrices ?
1
? R
d
1
?k
,?
2
?
R
d
2
?k
(k << d
1
and k << d
2
) and obtain
spelling view projections x?
i
= ?
T
1
x
i
? R
k
, ?i ?
{1, 2, . . . , n}. The k-dimensional spelling view
projection of any instance of a candidate phrase
is used as it?s embedding
4
.
The k-dimensional candidate phrase embed-
dings are used as features to learn a binary SVM
with the seed spelling examples given in figure 1
as training data. The binary SVM predicts whether
a candidate phrase is a named entity or not. Since
the value of k is small, a small number of labeled
examples are sufficient to train an accurate clas-
sifier. The learned SVM is used to filter out the
noisy phrases from the list of candidate phrases
obtained in the previous step.
To summarize, our approach for classifying
candidate phrases has the following steps:
? Input: n (spelling, context) pairs, spelling
seed examples.
? Each of the n (spelling, context) pairs are
mapped to a pair of high-dimensional fea-
ture vectors to get n paired observations
(x
1
, z
1
), . . . , (x
n
, z
n
) with x
i
? R
d
1
, z
i
?
R
d
2
, ?i ? {1, 2, . . . , n}.
? Using CCA, we learn the projection matri-
ces ?
1
? R
d
1
?k
,?
2
? R
d
2
?k
and ob-
tain spelling view projections x?
i
= ?
T
1
x
i
?
R
k
,?i ? {1, 2, . . . , n}.
? The embedding of a candidate phrase is given
by the k-dimensional spelling view projec-
tion of any instance of the candidate phrase.
? We learn a binary SVM with the candi-
date phrase embeddings as features and the
spelling seed examples given in figure 1 as
training data. Using this SVM, we predict
whether a candidate phrase is a named entity
or not.
3.2.2 Approach based on Co-training
We discuss here briefly the DL-CoTrain algorithm
(Collins and Singer, 1999) which is based on co-
training (Blum and Mitchell, 1998), to classify
3
Similar to Dhillon et al. (2012) we used the method given
in Halko et al. (2011) to perform the SVD computation in
CCA for practical considerations.
4
Note that a candidate phrase gets the same spelling view
projection across it?s different instances since the spelling
features of a candidate phrase are identical across it?s in-
stances.
454
? Virus seed spelling examples
? Virus Names: human immunodeficiency, hepatitis C, influenza, Epstein-Barr, hepatitis B
? Non-virus Names: mutant, same, wild type, parental, recombinant
? Disease seed spelling examples
? Disease Names: tumor, malaria, breast cancer, cancer, IDDM, DM, A-T, tumors, VHL
? Non-disease Names: cells, patients, study, data, expression, breast, BRCA1, protein, mutant
1
Figure 1: Seed spelling examples
candidate phrases. We compare our approach us-
ing CCA embeddings with this approach. Here,
two decision list of rules are learned simultane-
ously one using the spelling view and the other
using the context view. The rules using the
spelling view are of the form: full-string=human
immunodeficiency?Virus, full-string=mutant?
Not a virus and so on. In the context view, we
used bigram
5
rules where we considered all pos-
sible bigrams using the context. The rules are of
two types: one which gives a positive label, for
example, full-string=human immunodeficiency?
Virus and the other which gives a negative label,
for example, full-string=mutant ? Not a virus.
The DL-CoTrain algorithm is as follows:
? Input: (spelling, context) pairs for every in-
stance of a candidate phrase in the corpus, m
specifying the number of rules to be added in
every iteration, precision threshold , spelling
seed examples.
? Algorithm:
1. Initialize the spelling decision list using
the spelling seed examples given in fig-
ure 1 and set i = 1.
2. Label the entire input collection using the
learned decision list of spelling rules.
3. Add i ? m new context rules of each
type to the decision list of context rules
using the current labeled data. The
rules are added using the same criterion
as given in Collins and Singer (1999),
i.e., among the rules whose strength is
greater than the precision threshold ,
the ones which are seen more often with
the corresponding label in the input data
collection are added.
5
We tried using unigram rules but they were very weak
predictors and the performance of the algorithm was poor
when they were considered.
4. Label the entire input collection using the
learned decision list of context rules.
5. Add i ? m new spelling rules of each
type to the decision list of spelling rules
using the current labeled data. The rules
are added using the same criterion as in
step 3. Set i = i+1. If rules were added
in the previous iteration, return to step 2.
The algorithm is run until no new rules are left to
be added. The spelling decision list along with
its strength (Collins and Singer, 1999) is used to
construct the dictionaries. The phrases present in
the spelling rules which give a positive label and
whose strength is greater than the precision thresh-
old, were added to the dictionary of named enti-
ties. We found the parameters m and  difficult
to tune and they could significantly affect the per-
formance of the algorithm. We give more details
regarding this in the experiments section.
4 Related Work
Previously, Collins and Singer (1999) introduced
a multi-view, semi-supervised algorithm based on
co-training (Blum and Mitchell, 1998) for collect-
ing names of people, organizations and locations.
This algorithm makes a strong independence as-
sumption about the data and employs many heuris-
tics to greedily optimize an objective function.
This greedy approach also introduces new param-
eters that are often difficult to tune.
In other works such as Toral and Mu?noz (2006)
and Kazama and Torisawa (2007) external struc-
tured resources like Wikipedia have been used to
construct dictionaries. Even though these meth-
ods are fairly successful they suffer from a num-
ber of drawbacks especially in the biomedical do-
main. The main drawback of these approaches is
that it is very difficult to accurately disambiguate
ambiguous entities especially when the entities are
455
abbreviations (Kazama and Torisawa, 2007). For
example, DM is the abbreviation for the disease
Diabetes Mellitus and the disambiguation page for
DM in Wikipedia associates it to more than 50 cat-
egories since DM can be expanded to Doctor of
Management, Dichroic mirror, and so on, each of
it belonging to a different category. Due to the
rapid growth of Wikipedia, the number of enti-
ties that have disambiguation pages is growing fast
and it is increasingly difficult to retrieve the article
we want. Also, it is tough to understand these ap-
proaches from a theoretical standpoint.
Dhillon et al. (2011) used CCA to learn word
embeddings and added them as features in a se-
quence tagger. They show that CCA learns bet-
ter word embeddings than CW embeddings (Col-
lobert and Weston , 2008), Hierarchical log-linear
(HLBL) embeddings (Mnih and Hinton, 2007)
and embeddings learned from many other tech-
niques for NER and chunking. Unlike PCA, a
widely used dimensionality reduction technique,
CCA is invariant to linear transformations of the
data. Our approach is motivated by the theoreti-
cal result in Kakade and Foster (2007) which is
developed in the co-training setting. We directly
use the CCA embeddings to predict the label of
a data point instead of using them as features in
a sequence tagger. Also, we learn CCA embed-
dings for candidate phrases instead of all words in
the vocabulary since named entities often contain
more than one word. Dhillon et al. (2012) learn
a multi-class SVM using the CCA word embed-
dings to predict the POS tag of a word type. We
extend this technique to NER by learning a binary
SVM using the CCA embeddings of a high recall,
low precision list of candidate phrases to predict
whether a candidate phrase is a named entity or
not.
5 Experiments
In this section, we give experimental results on
virus and disease NER.
5.1 Data
The noisy lists of both virus and disease names
were obtained from the BioMed Central corpus.
This corpus was also used to get the collection of
(spelling, context) pairs which are the input to the
CCA procedure and the DL-CoTrain algorithm de-
scribed in the previous section. We obtained CCA
embeddings for the 100, 000 most frequently oc-
curring word types in this collection along with
every word type present in the training and de-
velopment data of the virus and the disease NER
dataset. These word embeddings are similar to the
ones described in Dhillon et al. (2011) and Dhillon
et al. (2012).
We used the virus annotations in the GE-
NIA corpus (GENIA, 2003) for our experiments.
The dataset contains 18,546 annotated sentences.
We randomly selected 8,546 sentences for train-
ing and the remaining sentences were randomly
split equally into development and testing sen-
tences. The training sentences are used only for
experiments with the sequence taggers. Previ-
ously, Zhang et al. (2004) tested their HMM-based
named entity recognizer on this data. For disease
NER, we used the recent disease corpus (Dogan
and Lu, 2012) and used the same training, devel-
opment and test data split given by them. We used
a sentence segmenter
6
to get sentence segmented
data and Stanford Tokenizer
7
to tokenize the data.
Similar to Dogan and Lu (2012), all the different
disease categories were flattened into one single
category of disease mentions. The development
data was used to tune the hyperparameters and the
methods were evaluated on the test data.
5.2 Results using a dictionary-based tagger
First, we compare the dictionaries compiled us-
ing different methods by using them directly in
a dictionary-based tagger. This is a simple and
informative way to understand the quality of the
dictionaries before using them in a CRF-tagger.
Since these taggers can be trained using a hand-
ful of training examples, we can use them to build
NER systems even when there are no labeled sen-
tences to train. The input to a dictionary tagger is
a list of named entities and a sentence. If there is
an exact match between a phrase in the input list
to the words in the given sentence then it is tagged
as a named entity. All other words are labeled as
non-entities. We evaluated the performance of the
following methods for building dictionaries:
? Candidate List: This dictionary contains all
the candidate phrases that were obtained us-
ing the method described in Section 3.1. The
noisy list of virus candidates and disease can-
didates had 3,100 and 60,080 entries respec-
tively.
6
https://pypi.python.org/pypi/text-sentence/0.13
7
http://nlp.stanford.edu/software/tokenizer.shtml
456
Method
Virus NER Disease NER
Precision Recall F-1 Score Precision Recall F-1 Score
Candidate List 2.20 69.58 4.27 4.86 60.32 8.99
Manual 42.69 68.75 52.67 51.39 45.08 48.03
Co-Training 48.33 66.46 55.96 58.87 23.17 33.26
CCA 57.24 68.33 62.30 38.34 44.55 41.21
Table 1: Precision, recall, F- 1 scores of dictionary-based taggers
? Manual: Manually constructed dictionaries,
which requires a large amount of human ef-
fort, are employed for the task. We used the
list of virus names given in Wikipedia
8
. Un-
fortunately, abbreviations of virus names are
not present in this list and we could not find
any other more complete list of virus names.
Hence, we constructed abbreviations by con-
catenating the first letters of all the strings in
a virus name, for every virus name given in
the Wikipedia list.
For diseases, we used the list of disease
names given in the Unified Medical Lan-
guage System (UMLS) Metathesaurus. This
dictionary has been widely used in disease
NER (e.g., Dogan and Lu, 2012; Leaman et
al., 2010)
9
.
? Co-Training: The dictionaries are con-
structed using the DL-CoTrain algorithm de-
scribed previously. The parameters used
were m = 5 and  = 0.95 as given in Collins
and Singer (1999). The phrases present in
the spelling rules which give a positive label
and whose strength is greater than the preci-
sion threshold, were added to the dictionary
of named entities.
In our experiment to construct a dictionary
of virus names, the algorithm stopped after
just 12 iterations and hence the dictionary had
only 390 virus names. This was because there
were no spelling rules with strength greater
than 0.95 to be added. We tried varying
both the parameters but in all cases, the algo-
rithm did not progress after a few iterations.
We adopted a simple heuristic to increase the
coverage of virus names by using the strength
of the spelling rules obtained after the 12
th
it-
eration. All spelling rules that give a positive
8
http://en.wikipedia.org/wiki/List of viruses
9
The list of disease names from UMLS can be found at
https://sites.google.com/site/fmchowdhury2/bioenex .
label and which has a strength greater than
? were added to the decision list of spelling
rules. The phrases present in these rules are
added to the dictionary. We picked the ? pa-
rameter from the set [0.1, 0.2, 0.3, 0.4, 0.5,
0.6, 0.7, 0.8, 0.9] using the development data.
The co-training algorithm for constructing
the dictionary of disease names ran for close
to 50 iterations and hence we obtained bet-
ter coverage for disease names. We still used
the same heuristic of adding more named en-
tities using the strength of the rule since it
performed better.
? CCA: Using the CCA embeddings of the
candidate phrases
10
as features we learned a
binary SVM
11
to predict whether a candidate
phrase is a named entity or not. We consid-
ered using 10 to 30 dimensions of candidate
phrase embeddings and the regularizer was
picked from the set [0.0001, 0.001, 0.01, 0.1,
1, 10, 100]. Both the regularizer and the num-
ber of dimensions to be used were tuned us-
ing the development data.
Table 1 gives the results of the dictionary based
taggers using the different methods described
above. As expected, when the noisy list of candi-
date phrases are used as dictionaries the recall of
the system is quite high but the precision is very
low. The low precision of the Wikipedia virus
lists was due to the heuristic used to obtain ab-
breviations which produced a few noisy abbrevia-
tions but this heuristic was crucial to get a high re-
call. The list of disease names from UMLS gives
a low recall because the list does not contain many
disease abbreviations and composite disease men-
tions such as breast and ovarian cancer. The pres-
10
The performance of the dictionaries learned from word
embeddings was very poor and we do not report it?s perfor-
mance here.
11
we used LIBSVM (http://www.csie.ntu.edu.tw/ cjlin/libsvm/)
in our SVM experiments
457
0 1000 2000 3000 4000 5000 6000 7000 8000 90000.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Number of Training Sentences
F?1
 Sco
re
Virus NER
 
 
baseline
manual
co?training
cca
0 1000 2000 3000 4000 5000 60000.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
F?1
 Sco
re
Number of Training Sentences
Disease NER
 
 
baseline
manual
co?training
cca
1
Figure 2: Virus and Disease NER F-1 scores for varying training data size when dictionaries obtained
from different methods are injected
ence of ambiguous abbreviations affected the ac-
curacy of this dictionary.
The virus dictionary constructed using the CCA
embeddings was very accurate and the false pos-
itives were mainly due to ambiguous phrases,
for example, in the phrase HIV replication, HIV
which usually refers to the name of a virus is
tagged as a RNA molecule. The accuracy of the
disease dictionary produced using CCA embed-
dings was mainly affected by noisy abbreviations.
We can see that the dictionaries obtained us-
ing CCA embeddings perform better than the dic-
tionaries obtained from co-training on both dis-
ease and virus NER even after improving the co-
training algorithm?s coverage using the heuristic
described in this section. It is important to note
that the dictionaries constructed using the CCA
embeddings and a small number of labeled exam-
ples performs competitively with dictionaries that
are entirely built by domain experts. These re-
sults show that by using the CCA based approach
we can build NER systems that give reasonable
performance even for difficult named entity types
with almost no supervision.
5.3 Results using a CRF tagger
We did two sets of experiments using a CRF tag-
ger. In the first experiment, we add dictionary fea-
tures to the CRF tagger while in the second ex-
periment we add the embeddings as features to the
CRF tagger. The same baseline model is used in
both the experiments whose features are described
in Section 2.2. For both the CRF
12
experiments
the regularizers from the set [0.0001, 0.001, 0.01,
0.1, 1.0, 10.0] were considered and it was tuned
on the development set.
5.3.1 Dictionary Features
Here, we inject dictionary matches as features
(e.g., Ratinov and Roth, 2009; Cohen and
Sarawagi, 2004) in the CRF tagger. Given a dic-
tionary of named entities, every word in the input
sentence has a dictionary feature associated with
it. When there is an exact match between a phrase
in the dictionary with the words in the input sen-
tence, the dictionary feature of the first word in
the named entity is set to B and the dictionary fea-
ture of the remaining words in the named entity
is set to I. The dictionary feature of all the other
words in the input sentence which are not part of
any named entity in the dictionary is set to O. The
effectiveness of the dictionaries constructed from
various methods are compared by adding dictio-
nary match features to the CRF tagger. These dic-
tionary match features were added along with the
baseline features.
Figure 2 indicates that the dictionary features in
general are helpful to the CRF model. We can see
that the dictionaries produced from our approach
using CCA are much more helpful than the dictio-
naries produced from co-training especially when
there are fewer labeled sentences to train. Simi-
lar to the dictionary tagger experiments discussed
12
We used CRFsuite (www.chokkan.org/software/crfsuite/)
for our experiments with CRFs.
458
0 1000 2000 3000 4000 5000 6000 7000 8000 90000.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
F?1
 Sco
re
Number of Training Sentences
Virus NER
 
 
baseline
cca?word
cca?phrase
0 1000 2000 3000 4000 5000 60000.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of Training Sentences
F?1
 Sco
re
Disease NER
 
 
baseline
cca?word
cca?phrase
1
Figure 3: Virus and Disease NER F-1 scores for varying training data size when embeddings obtained
from different methods are used as features
previously, the dictionaries produced from our ap-
proach performs competitively with dictionaries
that are entirely built by domain experts.
5.3.2 Embedding Features
The quality of the candidate phrase embeddings
are compared with word embeddings by adding
the embeddings as features in the CRF tagger.
Along with the baseline features, CCA-word
model adds word embeddings as features while the
CCA-phrase model adds candidate phrase em-
beddings as features. CCA-word model is similar
to the one used in Dhillon et al. (2011).
We considered adding 10, 20, 30, 40 and 50 di-
mensional word embeddings as features for every
training data size and the best performing model
on the development data was picked for the exper-
iments on the test data. For candidate phrase em-
beddings we used the same number of dimensions
that was used for training the SVMs to construct
the best dictionary.
When candidate phrase embeddings are ob-
tained using CCA, we do not have embeddings
for words which are not in the list of candidate
phrases. Also, a candidate phrase having more
than one word has a joint representation, i.e., the
phrase ?human immunodeficiency? has a lower
dimensional representation while the words ?hu-
man? and ?immunodeficiency? do not have their
own lower dimensional representations (assuming
they are not part of the candidate list). To over-
come this issue, we used a simple technique to dif-
ferentiate between candidate phrases and the rest
of the words. Let x be the highest real valued can-
didate phrase embedding and the candidate phrase
embedding be a d dimensional real valued vector.
If a candidate phrase occurs in a sentence, the em-
beddings of that candidate phrase are added as fea-
tures to the first word of that candidate phrase. If
the candidate phrase has more than one word, the
other words in the candidate phrase are given an
embedding of dimension d with each dimension
having the value 2 ? x. All the other words are
given an embedding of dimension d with each di-
mension having the value 4? x.
Figure 3 shows that almost always the candi-
date phrase embeddings help the CRF model. It is
also interesting to note that sometimes the word-
level embeddings have an adverse affect on the
performance of the CRF model. The CCA-phrase
model performs significantly better than the other
two models when there are fewer labeled sen-
tences to train and the separation of the candidate
phrases from the other words seems to have helped
the CRF model.
6 Conclusion
We described an approach for automatic construc-
tion of dictionaries for NER using minimal super-
vision. Compared to the previous approaches, our
method is free from overly-stringent assumptions
about the data, uses SVD that can be solved ex-
actly and achieves better empirical performance.
Our approach which uses a small number of seed
examples performs competitively with dictionar-
ies that are compiled manually.
459
Acknowledgments
We are grateful to Alexander Rush, Alexandre
Passos and the anonymous reviewers for their
useful feedback. This work was supported by
the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Interior Na-
tional Business Center (DoI/NBC) contract num-
ber D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copy-
right annotation thereon. The views and conclu-
sions contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either ex-
pressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
References
Andrew McCallum and Wei Li. Early Results for
Named Entity Recognition with Conditional Ran-
dom Fields, Feature Induction and Web-Enhanced
Lexicons. 2003. Conference on Natural Language
Learning (CoNLL).
Andriy Mnih and Geoffrey Hinton. Three New Graph-
ical Models for Statistical Language Modelling.
2007. International Conference on Machine learn-
ing (ICML).
Antonio Toral and Rafael Mu?noz. A proposal to auto-
matically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia. 2006.
Workshop On New Text Wikis And Blogs And
Other Dynamic Text Sources.
Avrin Blum and Tom M. Mitchell. Combining Labeled
and Unlabeled Data with Co-Training. 1998. Con-
ference on Learning Theory (COLT).
Burr Settles. Biomedical Named Entity Recognition
Using Conditional Random Fields and Rich Feature
Sets. 2004. International Joint Workshop on Natural
Language Processing in Biomedicine and its Appli-
cations (NLPBA).
H. Hotelling. Canonical correlation analysis (cca)
1935. Journal of Educational Psychology.
Jie Zhang, Dan Shen, Guodong Zhou, Jian Su and
Chew-Lim Tan. Enhancing HMM-based Biomed-
ical Named Entity Recognition by Studying Special
Phenomena. 2004. Journal of Biomedical Informat-
ics.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi and
Jun?ichi Tsujii. GENIA corpus - a semantically an-
notated corpus for bio-textmining. 2003. ISMB.
Junichi Kazama and Kentaro Torisawa. Exploiting
Wikipedia as External Knowledge for Named Entity
Recognition. 2007. Association for Computational
Linguistics (ACL).
Karthik Sridharan and Sham M. Kakade. An Informa-
tion Theoretic Framework for Multi-view Learning.
2008. Conference on Learning Theory (COLT).
Lev Ratinov and Dan Roth. Design Challenges
and Misconceptions in Named Entity Recognition.
2009. Conference on Natural Language Learning
(CoNLL).
Michael Collins and Yoram Singer. Unsupervised
Models for Named Entity Classification. 1999. In
Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora.
Nathan Halko, Per-Gunnar Martinsson, Joel A. Tropp.
Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix de-
compositions. 2011. Society for Industrial and Ap-
plied Mathematics.
Paramveer S. Dhillon, Dean Foster and Lyle Ungar.
Multi-View Learning of Word Embeddings via CCA.
2011. Advances in Neural Information Processing
Systems (NIPS).
Paramveer Dhillon, Jordan Rodu, Dean Foster and Lyle
Ungar. Two Step CCA: A new spectral method for
estimating vector models of words. 2012. Interna-
tional Conference on Machine learning (ICML).
Rezarta Islamaj Dogan and Zhiyong Lu. An improved
corpus of disease mentions in PubMed citations.
2012. Workshop on Biomedical Natural Language
Processing, Association for Computational Linguis-
tics (ACL).
Robert Leaman, Christopher Miller and Graciela Gon-
zalez. Enabling Recognition of Diseases in Biomed-
ical Text with Machine Learning: Corpus and
Benchmark. 2010. Workshop on Biomedical Nat-
ural Language Processing, Association for Compu-
tational Linguistics (ACL).
Ronan Collobert and Jason Weston. A unified architec-
ture for natural language processing: deep neural
networks with multitask learning. 2008. Interna-
tional Conference on Machine learning (ICML).
Ryan McDonald and Fernando Pereira. Identifying
Gene and Protein Mentions in Text Using Condi-
tional Random Fields. 2005. BMC Bioinformatics.
Sham M. Kakade and Dean P. Foster. Multi-view re-
gression via canonical correlation analysis. 2007.
Conference on Learning Theory (COLT).
William W. Cohen and Sunita Sarawagi. Exploiting
Dictionaries in Named Entity Extraction: Combin-
ing Semi-Markov Extraction Processes and Data In-
tegration Methods. 2004. Semi-Markov Extraction
460
Processes and Data Integration Methods, Proceed-
ings of KDD.
461
