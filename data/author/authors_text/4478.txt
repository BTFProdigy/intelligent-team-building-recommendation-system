Proceedings of the 8th Workshop on Asian Language Resources, pages 14?21,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Considerations on Automatic Mapping Large-Scale Heterogene-
ous Language Resources: Sejong Semantic Classes and KorLex 
 
Heum Park 
Center for U-Port IT  
Research and Education 
Pusan National University 
parheum2@empal.com 
Aesun Yoon 
LI Lab. Dept. of French 
Pusan National University
asyoon@pusan.ac.kr
Woo Chul Park and 
Hyuk-Chul Kwon* 
AI Lab Dept. of Computer 
Science 
Pusan National University
hckwon@pusan.ac.kr
 
Abstract 
This paper presents an automatic map-
ping method among large-scale hetero-
geneous language resources: Sejong 
Semantic Classes (SJSC) and KorLex. 
KorLex is a large-scale Korean Word-
Net, but  it lacks specific syntactic & 
semantic information. Sejong Electron-
ic Dictionary (SJD), of which semantic 
segmentation depends on SJSC, has 
much lower lexical coverage than 
KorLex, but shows refined syntactic & 
semantic information. The goal of this 
study is to build a rich language re-
source for improving Korean semanti-
co-syntactic parsing technology. There-
fore, we consider integration of them 
and propose automatic mapping me-
thod with three approaches: 1) Infor-
mation of Monosemy/Polysemy of 
Word senses (IMPW), 2) Instances be-
tween Nouns of SJD and Word senses 
of KorLex (INW), and 3) Semantically 
Related words between Nouns of SJD 
and Synsets of KorLex (SRNS). We 
obtain good performance using com-
bined three approaches: recall 0.837, 
precision 0.717, and F1 0.773. 
1 Introduction 
While remarkable progress has been made in 
Korean language engineering on morphologi-
cal level during last two decades, syntactic and 
semantic processing has progressed more 
slowly.  The syntactic and semantic processing 
requires 1) linguistically and formally well 
defined argument structures with the selection-
al restrictions of each argument, 2) large and 
semantically well segmented lexica, 3) most 
importantly, interrelationship between the ar-
gument structures and lexica. A couple of lan-
guage resources have been developed or can be 
used for this end. Sejong Electronic Dictiona-
ries (SJD) for nouns and predicates (verbs and 
adjectives) along with semantic classes (Hong 
2007) were developed for syntactic and seman-
tic analysis, but the current versions do not 
contain enough entries for concrete applica-
tions, and they show inconsistency problem. A 
Korean WordNet, named KorLex (Yoon & al,  
2009), which was built on Princeton WordNet 
2.0 (PWN) as its reference model, can provide 
means for shallow semantic processing but 
does not contain refined syntactic and semantic 
information specific to Korean language. Ko-
rean Standard Dictionary (STD) provides a 
large number of entries but it lacks systematic 
description and formal representation of word 
senses, like other traditional dictionaries for 
humans. Given these resources which were 
developed through long-term projects (5 ? 10 
years), integrating them should result in signif-
icant benefits to Korean syntactic and semantic 
processing. 
The primary goal of our recent work includ-
ing the work reported in this paper is to build a 
language resource, which will improve Korean 
semantico-syntactic parsing technology. We 
proceed by integrating the argument structures 
as provided by SJD, and the lexical-semantic 
hierarchy as provided by KorLex. SJD is a 
language resource, of which all word senses 
are labeled according to Sejong semantic 
classes (SJSC), and in which selectional re-
* Corresponding Author 
14
strictions are represented in SJSC as for the 
argument structures of predicates. KorLex is a 
large scale language resource, of which the 
lexical-semantic hierarchies and other lan-
guage?independent semantic relations between 
synsets (synonym sets) share with those of 
PWN, and of which Korean language specific 
information comes from STD. The secondary 
goal is the improvement of three resources as a 
result of comparing and integrating them. 
In this paper, we report on one of the operat-
ing steps toward to our goals. We linked each 
word sense of KorLex to that of STD by hand, 
when the former was built in our previous 
work (Yoon & al. 2009). All predicates in SJD 
were mapped to those of STD on word sense 
level by semi-automatic mapping (Yoon, 
2010). Thus KorLexVerb and KorLexAdj have 
syntactico-semantic information on argument 
structures via this SJD - STD mapping. How-
ever, the selectional restrictions provided by 
SJD are not useful, if SJSC which represents 
the selectional restrictions in SJD is not linked 
to KorLex. We thus conduct two mapping me-
thods between SJSC and upper nodes of Kor-
LexNoun: 1) manual mapping by a PH.D in 
computational semantics (Bae & al. 2010), and 
2) automatic mapping. This paper reports the 
latter. Reliable automatic mapping methods 
among heterogeneous language resources 
should be considered, since the manual map-
ping among large-scale resources is a very 
time and labor consuming job, and might lack 
consistency. Less clean resources are, much 
harder and more confusing manual mapping is.  
In this paper, we propose an automatic map-
ping method of those two resources with three 
approaches to determine mapping candidate 
synsets of KorLex to a terminal node of SJSC: 
1) using information of monosemy/polysemy 
of word senses, 2) using instances between 
nouns of SJD and word senses of KorLex, and 
3) using semantically related words between 
nouns of SJD and word senses of KorLex. We 
compared the results of automatic mapping 
method with three approaches with those of 
manual mapping aforementioned.  
In the following Section 2, we discuss re-
lated studies concerning language resources 
and automatic mapping methods of heteroge-
neous language resources. In Section 3, we 
introduce KorLex and SJD. In Section 4, we 
propose an automatic mapping method with 
three approaches from semantic classes of SJD 
to synsets of KorLex. In Section 5, we com-
pare the results of automatic mapping with 
those of manual mapping. In Section 6, we 
draw conclusions and future works. 
2 Related Works 
Most existing mappings of heterogeneous lan-
guage resources were conducted manually by 
language experts. The Suggested Upper 
Merged Ontology (SUMO) had been fully 
linked to PWN. For manual mapping of be-
tween PWN and SUMO, it was considered 
synonymy, hypernymy and instantiation be-
tween synsets of PWN and concepts of SUMO, 
and found the nearest instances of SUMO for 
synsets of PWN. Because the concept items of 
SUMO are much larger than those of PWN, it 
could be mapped between high level concepts 
of PWN and synonymy concepts of SUMO 
easily. (Ian Niles et al2003). Dennis Spohr 
(2008) presented a general methodology to 
mapping EuroWordNet to the SUMO for ex-
traction of selectional preferences for French. 
Jan Scheffczyk et al (2006) introduced the 
connection of FrameNet to SUMO. They pre-
sented general-domain links between Frame-
Net Semantic Types and SUMO classes in 
SUOKIF and developed a semi-automatic, 
domain-specific approach for linking Frame-
Net Frame Elements to SUMO classes 
(Scheffczyk & al. 2006). Sara Tonelli et al 
(2009) presented a supervised learning frame-
work for the mapping of FrameNet lexical 
units onto PWN synsets to solve limited cover-
age of semantic phenomena for NLP applica-
tions. Their best results were recall 0.613, pre-
cision 0.761 and F1 measure 0.679.  
Considerations on automatic mapping me-
thods among language resources were always 
attempted for the sake of efficiency, using si-
milarity measuring and evaluating methods. 
Typical traditional evaluating methods be-
tween concepts of heterogeneous language re-
sources were the dictionary-based approaches 
(Kozima & al 1993), the semantic distance 
algorithm using PWN (Hirst & al 1998), the 
scaling method by semantic distance between 
concepts (Sussna 1997), conceptual similarity 
between concepts (Wu & al 1994), the scaled 
15
semantic similarity between concepts (Leacock 
1998), the semantic similarity between con-
cepts using IS-A relation (Resnik 1995), the 
measure of similarity between concepts (Lin 
1998), Jiang and Conrath?s (1997) similarity 
computations to synthesize edge and node 
based techniques, etc.  
Satanjeev et al (2003) presented a new 
measure of semantic relatedness between con-
cepts that was based on the number of shared 
words (overlaps) in their definitions (glosses) 
for word sense disambiguation. The perfor-
mances of their extended gloss overlap meas-
ure with 3-word window were recall 0.342, 
precision 0.351 and F1 0.346. Siddharth et al 
(2003) presented the Adapted Lesk Algorithm 
to a method of word sense disambiguation 
based on semantic relatedness. In addition, 
Alexander et al(2006) introduced the 5 exist-
ing evaluating methods for PWN-based meas-
ures of lexical semantic relatedness and com-
pared the performance of typical five measures 
of semantic relatedness for NLP applications 
and information retrieval. Among them, Jiang-
Conrath?s method showed the best perfor-
mances: precision 0.247, recall 0.231 and F1 
0.211 for Detection.  
In many studies, it was presented a variety 
of the adapted evaluating algorithms. Among 
them, Jiang-Conrath?s method, Lin?s the 
measure of similarity and Resnik?s the seman-
tic similarity show good performances (Alex-
ander & al 2006, Daniele 2009). 
3 Language resources to be mapped 
3.1 KorLex 1.5 
KorLex 1.5 was constructed from 2004 to 
2007. Different from its previous version 
(KorLex 1.0) which preserves all semantic 
relations among synsets of PWN, KorLex 1.5 
modifies them by deletion/correction of 
existing synsets, addition of new synsets and 
conversion of hierarchical structure. Currently, 
KorLex includes nouns, verbs, adjectives, 
adverbs and classifiers: KorLexNoun, 
KorLexVerb, KorLexAdj, KorLexAdv and 
KorLexClas, respectively. Table 1 shows the 
size of KorLex 1.5, in which ?Trans? means the 
number of synsets translated from PWN 2.0 
and ?Total? is the number of manually added 
synsets including translated ones.  
 Word Forms
Synsets Word 
SensesTrans Total 
KorLexNoun 89,125 79,689 90,134 102,358
KorLexVerb 17,956 13,508 16,923 20,133
KorLexAdj 19,698 18,563 18,563 20,905
KorLexAdv 3,032 3,664 3,664 3,123
KorLexClas 1,181 - 1,377 1,377
Total 130,992 115,424 130,661 147,896
Table  1. Product of KorLex 1.5 
KorLexNoun includes 25 semantic domains 
with 11 unique beginners with maximum 17 
levels in depth and KorLexVerb includes 15 
semantic domains with 11 unique beginners 
with maximum 12 levels in depth. Basically, 
KorLex synsets inherit the semantic informa-
tion of PWN synsets mapped to them. The 
synset information of PWN consists of synset 
ID, semantic domain, POS, word senses, se-
mantic relations, frame information, and so on.  
We linked each word sense of KorLex 1.5 to 
that of STD by hand, when the former was 
built in our previous work (Yoon & al. 2009).  
STD includes 509,076 word entries with about 
590,000 word senses. It contains a wide cover-
age for general words and a variety of example 
sentences for each meaning. More than 60% of 
word senses in KorLex 1.5 are linked to those 
of STD. KorLex 1.5, thus, inherits lexical rela-
tions described in STD, but both resources lack 
refined semantic-syntactic information. 
3.2 Sejong Electronic Dictionary  
SJD was developed during 1998-2007 manual-
ly by linguists for a variety of Korean NLP 
application as a general-purpose machine read-
able dictionary. Based on Sejong semantic 
classes (SJSC), approximately 25,000 nouns 
and 20,000 predicates (verbs and adjectives, 
SJPD) contain refined syntactic and semantic 
information. 
SJSC is a set of hierarchical meta-languages 
classifying word senses and it includes 474 
terminal nodes and 139 non-terminal nodes, 
and 6 unique beginners. Each unique beginner 
has levels from minimum 2 to maximum 7 le-
vels in depth. Sejong Noun Dictionary (SJND) 
contains 25,458 entries and 35,854 word 
senses having lexical information for each en-
try: semantic classes of SJSC, argument struc-
tures, selectional restrictions, semantically re-
lated words, derivatioinal relations/words et al  
16
 
Figure 1. Correlation of lexical information 
among SJND, SJPD and SJSC 
Figure 1 shows the correlation of lexical in-
formation among SJND, SJPD and SJSC. Cer-
tainly, that information of SJD should be ap-
plied to a variety of NLP applications: infor-
mation retrieval, text analysis/generation, ma-
chine translations, and various studies and 
educations. However, SJD has much lower 
lexical coverage than KorLex. More serious 
problem is that SJND and SJPD are still noisy: 
internal consistency inside each dictionary and 
external interrelationship between SJND, SJPD, 
and SJSC need to be ameliorated, as indicated 
by dot line in Fig. 1. 
4 Automatic Mapping from Semantic 
Class of SJSC to Synsets of KorLex  
KorLex and SJSC have different hierarchical 
structures, grain sizes, and lexical information 
as aforementioned. For example, the semantic 
classes of SJSC are much bigger concepts in 
grain size than the synsets of KorLex: 623 
concepts in SJSC vs.130,000 synsets in Kor-
Lex. Determining their semantic equivalence 
thus needs to be firmly based on linguistic 
clues.  
Using following 3 linguistic clues that we 
found, we propose an automatic mapping me-
thod from semantic classes of SJSC to synsets 
of KorLex with three approaches to determine 
mapping candidate synsets: 1) Information of 
Monosemy/Polysemy of Word senses (IMPW), 
2) Instances between Nouns of SJD and Word 
senses of KorLex (INW), and 3) Semantically 
Related words between Nouns of SJD and 
Synsets of KorLex (SRNS). 
For automatic mapping method, following 
processes were conducted. First, to find word 
senses of synsets that matched to nouns of 
SJND for each semantic class. Second, to se-
lect mapping candidate synsets among them 
with three approaches aforementioned. Third, 
to determine the least upper bound (LUB) syn-
sets and mapping synsets among candidates. 
Finally, to link each semantic class of SJSC to 
all lower-level synsets of LUB synsets. 
4.1 Finding matched word senses between 
synsets and nouns of SJND  
For a semantic class of SJSC, we first find 
word senses and synsets from KorLex that 
matched with nouns of SJND classified to that 
semantic class. Figure 2 shows the matched 
word senses and synsets between nouns of 
SJND, then synsets of KorLex for a semantic 
class. The left side of Figure 2 shows nodes of 
semantic classes with hierarchical structure 
and the center box shows the matched words 
(bold ones) among nouns of SJND with word 
senses of synsets in KorLex, and the right side 
shows matched word senses and synsets in 
KorLex? hierarchical structure. 
 
Figure  2. Matched word senses and synsets 
with nouns of SJND for a semantic class 
For example, a semantic class  
?Atmospheric Phenomena? (rectangle in the 
left) has nouns of SJND (words in the center), 
the bold words are the matched words with 
word senses of synsets from KorLex, and the 
underlined synsets of the right side are the 
matched ones and synset IDs in KorLex. The 
notations for automatic mapping process be-
tween semantic classes of SJSC and synsets of 
KorLex are as follows: noun of SJND is ns, 
matched noun nsm, un-matched nsu , semantic 
class of SJSC is sc, synset is ss and word sense 
of a synset is ws in KorLex, and monosemy 
word is wmono and polysemy word is wpoly. 
A semantic class sc has nouns ns of SJND 
having matched noun nsm and un-matched nsu 
by comparing with word senses ws of a synset 
ss in KorLex. Thus a synset has word senses as 
ss1={ws1, ws2, ?, wsn}={ nsm1, nsm2, ?, nsmk, 
nsu k+1, nsu k+2, ?}. And nouns of SJND for a 
semantic class sc1 is presented ns(sc1)={nsm1, 
17
nsm2, ?, nsmk, nsu k, nsu k+1, ?}. Therefore, we 
can find the matched word senses nsm1 ~ nsmk 
for a semantic class sc from nouns of SJND 
and word senses of a synset ss in KorLex. 
4.2 Selecting Mapping Candidate Synsets 
Using those matched synsets and word senses, 
we select mapping candidate synsets with three 
different approaches. 
4.2.1 Using Information of Monosemy 
and Polysemy of KorLex 
Using information of monosemy/polysemy of 
word senses of a synset, the first approach eva-
luates mapping candidate synsets. The candi-
date synsets are evaluated into three catego-
ries: mc(A) is a most relevant candidate synset, 
mc(B) is a relevant candidate synset and mc(C) 
is a reserved synset. Evaluation begins from 
lowest level synsets to top-level beginner. The 
process of first approach is as follows. 
1) For a synset which contains a single word 
sense, ss={ws1}, if the word sense is a mo-
nosemy, it is categorized as a a candidate 
synset mc(A). If it is a polysemy, categori-
zation is postponed for evaluating related-
ness among siblings: candidate mc(C).  
2) In the case of a synset having more than 
one word sense, ss={ws1, ws2, ?}, if the 
matched words nsm among word senses of a 
synset are over 60%: Pss(ws)=(count(nsm)/ 
count(ws)) ? 0.6, we evaluate whether that 
synset is mapping candidate in the next step. 
3) If all matched words nsm of a synset are 
monosemic, we categorize it as a candidate 
synset mc(A). If monosemic words among 
matched words are over 50%: Pss(wmono| 
nsm) ? 0.5, it is evaluated as a mc(B). A 
synset containing polysemies over 50%: 
Pss(wpoly|nsm) ? 0.5, categorization is post-
poned for evaluating relatedness among 
siblings: candidate synset mc(C). 
4) To repeat from step 1) to 3) for all of syn-
sets, in order to evaluate mapping candidate 
synsets. And then, to construct hierarchical 
structure for all those synsets. 
4.2.2 Using Instances between Nouns of 
SJND and Word senses of KorLex 
The second approach is to evaluate mapping 
candidate synsets using comparison of in-
stances between nouns of SJND and word 
senses of a synset. As for KorLex, we used the 
examples of STD linked to word senses of 
KorLex. Figure 3 shows instances of STD and 
SJND for a word sense ?Apple?. 
 
Figure  3. Instances of STD and SJND for 
word sense ?Apple?  
We reformulated the Lesk algorithm (Lesk 
1987, Banerjee and Pedersen 2002) for com-
paring instances and evaluating mapping can-
didate synsets. The process of evaluating map-
ping candidate synsets is as follows. 
1) To compare instances of a noun ns of 
SJND with examples of a word of STD 
linked to word sense ws of a synset ss, and 
to compute the Relatedness-A(ns, ws) = 
score(instance(ns), example(ws)). 
2) To compare all nouns ns of SJND for a 
semantic class with all nouns in instances 
of STD linked to word senses ws, and  to 
compute the Relatedness-B(ns, ws) = 
score(?ns, nouns(example(ws))). 
3) If Relatedness-A(ns, ws) ? ?1 and Related-
ness-B(ns, ws) ? ?2, a synset is evaluated as 
a candidate synset mc(A). If either Related-
ness-A(ns, ws) ? ?1 or Relatedness-B(ns, 
ws) ? ?2, evaluated as a candidate synset 
mc(B). When threshold ?1 and ?2 were 1~4, 
we had good performances.  
4) To repeat from step 1) to 3) for all of syn-
sets, in order to determine mapping candi-
date synsets. And then, to construct hierar-
chical structure for all those synsets. 
4.2.3 Using Semantically Relatedness be-
tween Nouns of SJND and Synsets of 
KorLex 
The third approach is to evaluate mapping 
candidate synsets using comparison of seman-
tic relations and their semantically related 
words between a noun of SJND and word 
senses of a synset. To compute the relatedness 
between them, we reformulated the computa-
18
tional formula of relatedness based on the Lesk 
algorithm (Lesk 1987, Banerjee & al 2002). 
The process of evaluating mapping candidate 
synsets is as follows. 
1) To compare semantically related words: 
between synonyms, hypernyms, hyponyms 
and antonyms of a noun of SJND and those 
of a synset of KorLex. To compute the Re-
latedness-C(ns, ss) = score (relations(ns), 
relations(ss)). 
2) To compare all nouns ns of SJND for a 
semantic class with synonyms, hypernyms 
and hyponyms of a synset of KorLex, and 
compute the Relatedness-D(ns, ss) = score 
(?ns, relations(ss)). 
3) If Relatedness-C(ns, ss) ? ?3 and Related-
ness-D(ns, ss) ? ?4, a synset is evaluated as 
a candidate synset mc(A). If either Related-
ness-C(ns, ss) ? ?3 or Relatedness-D(ns, ss) 
? ?4, evaluated as a candidate synset mc(B). 
When threshold ?3 and ?4 were 1~4, we 
have good performances.  
4) To repeat from step 1) to 3) for all of syn-
sets, in order to determine mapping synsets. 
And then, to construct hierarchical struc-
ture for all those synsets. 
4.3 Determining Least Upper Bound 
(LUB) Synsets and Mapping Synsets 
Next, we determine the LUB synsets using 
mapping candidate synsets and hierarchical 
structure having semantic relations: parent, 
child and sibling. In order to determine LUB 
and mapping synsets, we begin evaluation with 
bottom-up direction. Using relatedness among 
child-sibling candidate synsets, we evaluated 
whether their parent synset is a LUB synset or 
not. If the parent is a LUB synset, we evaluate 
its parent (grand-parent of the candidate) syn-
set using relatedness among its sibling synsets. 
If the parent is not a LUB, the candidate syn-
sets mc(A) or mc(B) are determined as map-
ping synsets (or LUB) and stop finding LUB. 
For all semantic classes, we determine LUB 
and mapping synsets. Finally, we link the LUB 
and mapping synsets to each semantic class of 
SJSC. The process of determining of LUB and 
mapping synsets is as follows. 
1) Using candidate synsets and their sibling, 
for all candidate synsets mc(A), mc(B) or 
mc(C) selected from the processes of ?4.2 
Select Mapping Candidate Synsets?, to de-
termine whether it is a LUB or not and fi-
nal mapping synsets. 
2) Among sibling synsets, if the ratio of 
count(mc(A)) to count(mc(A)+mc(B)+ 
mc(C)) is over 60%, the parent synset of 
siblings is evaluated as a candidate synset 
mc(A) and as a LUB. 
3) If the ratio of count(mc(A)+mc(B))  to 
count(mc(A)+mc(B)+mc(C)) is over 70%, 
the parent of siblings is evaluated as a can-
didate synset mc(A) and as a LUB. If the 
ratio of count(mc(A)+mc(B)) to count 
(mc(A) +mc(B)+mc(C)) is between 50% 
and 69%, the parent of siblings is evaluated 
as a candidate synset mc(B) and as a LUB. 
4) And if the others, to stop finding LUB for 
that synset and to determine final mapping 
synsets with its own level of candidate. 
5) To repeat from step 1) to 4) until finding 
LUB synsets and final mapping synsets. 
 
Figure  4. Hierarchical structure of mapping 
candidate synsets for a semantic class 
Figure 4 shows hierarchical structure of 
mapping candidate synsets for a semantic class 
?Furniture? and when candidate synsets? ID are  
?04004316? (Chair & Seat): mc(B), ?04209815? 
(Table & Desk): mc(B), ?14441331? (Table): 
mc(C), and ?14436072? (Shoe shelf & Shoe 
rack): mc(A), we determine whether their par-
ent synset ?03281101? (Furniture) is a LUB or 
not, and evaluate it as a candidate synset mc(A) 
or mc(B). In this case, synset ?03281101? (Fur-
niture) is a candidate mc(A) and a LUB synset. 
For all semantic classes, we find their map-
ping LUB and mapping synsets using informa-
tion of hierarchical structure and candidate 
synsets. Finally, we link each semantic class of 
SJSC to all lower level synsets of matched 
LUB synsets. 
19
5 Experiments and Results 
We experimented automatic mapping between 
623 semantic classes of SJSC and 90,134 noun 
synsets of KorLex using the proposed automat-
ic mapping method with three approaches. To 
evaluate the performances, we used the results 
of manual mapping as correct answers, that 
was mapped 474 semantic classes (terminal 
nodes) of SJSC to 65,820 synsets (73%) (in-
clude 6,487 LUB) among total 90,134 noun 
synsets of KorLex. We compared the results of 
automatic mapping with those of manual map-
ping. For evaluation of performances, we em-
ployed Recall, Precision and the F1 measure: 
F1 = (2*Recall*Precision)/(Recall+ Precision).  
Approaches Recall Precision F1 
1) 0.904 0.502 0.645 
2) 0.774 0.732 0.752 
3) 0.670 0.802 0.730 
1)+2) 0.805 0.731 0.766 
1)+3) 0.761 0.758 0.759 
2)+3) 0.636 0.823 0.718 
1)+2)+3) 0.838 0.718 0.774 
Table  2. Performances of automatic mapping 
with three approaches  
Table 2 shows the performances of automat-
ic mapping with three approaches: 1) IMPW, 
2) INW, and 3) SRNS. The ?1)?, ?2)? or ?3) in 
the Table present the results using for each 
approach method and ?1)+2)?, ?1)+3)? or 
?2)+3)? present those of combining two ap-
proaches. The ?1)+2)+3)? presents those of the 
combining three approaches and we can see 
the best performances using the last approach 
among results: recall 0.837, precision 0.717 
and F1 0.773. The first approach ?1)? method 
shows high recall, but low precision and the 
third approach ?3)? method present low recall 
and high precision. ?1)+3)? and ?2)+3)? shows 
good performances overall. Thus, we could see 
good performances using the combined ap-
proach methods. 
Second, we compared the numbers of se-
mantic classes, nouns entries of SJND, noun 
synsets and word senses of KorLex for each 
approach, after mapping processes.  
As shown in Table 3, we can see the most 
numbers of mapping synsets using the ?1)? ap-
proach. The ?1)+2)+3)? shows the results simi-
lar to ?1)?, but has the best performances (see 
Table 2). The percentages in the round bracket 
present the ratio of the results of automatic 
mapping to original lexical data of Sejong and 
KorLex: 474 semantic classes of SJSC, 25,245 
nouns of SJND and 90,134 noun synsets and 
147,896 word senses in KorLex. 
 SJD KorLex 
Approaches SC (SJSC)
Nouns  
(SJND) Synsets 
Word 
Senses 
1) 473 18,575 54,943 69,970 
2) 445 18,402 52,109 66,936 
3) 413 18,047 49,768 64,003 
1)+2) 463 18,521 52,563 67,109 
1)+3) 457 18,460 51,786 66,157 
2)+3) 383 17,651 48,398 62,063 
1)+2)+3) 466 (98.3%)
18,542 
(72.8%) 
54,083 
(60%) 
69,259 
(46.8%)
Table  3. Numbers of semantic class, noun of 
SJD, synset and word sense of KorLex 
In manual mapping, we mapped 73% 
(65,820) synsets of KorLex for 474 semantic 
classes of SJSC. The 24,314 synsets was ex-
cluded in manual mapping among 90,134 total 
nouns synsets. The reasons of excluded synsets 
in manual mapping were 1) inconsistency of 
inheritance for lexical relations of parent-child 
in SJSC or KorLex, 2) inconsistency between 
criteria for SJSC and candidate synsets, 3) 
candidate synsets belonging to more than two 
semantic classes, 4) specific proper nouns 
(chemical compound names), and 5) polysemic 
abstract synsets (Bae & al. 2010). 
In automatic mapping, we could map 60% 
(54,083) synsets among total nouns synsets 
(90,134) of KorLex, and it is 82.2% of the re-
sults of manual mapping. The 11,737 synsets 
was excluded in automatic mapping by com-
paring with manual mapping. Most of them 
were 1) tiny-grained synsets found in the low-
est levels, 2) synsets having no matched word 
senses with those of SJND, 3) synsets with 
polysemic word senses, 4) word senses having 
poor instances in KorLex and in SJND, 5) 
word senses in SJND having poor semantic 
relations.  
Level LUB Ratio Level LUB Ratio
1 18 0.6% 9 230 7.3%
2 18 0.6% 10 98 3.1%
3 174 5.5% 11 32 1.0%
4 452 14.3% 12 20 0.6%
5 616 19.5% 13 4 0.1%
6 570 18.0% 14 4 0.1%
7 486 15.4% 15 2 0.1%
8 442 14.0% 16-17 0 0%
Table  4. Numbers and Ratio of LUB synsets 
excluded in automatic mapping 
20
Table 4 shows the numbers and ratio of the 
LUB synsets excluded in automatic mapping 
for each level in depth. Most synsets are 4-8 
levels synsets among 17 levels in depth. 
6 Conclusions 
We proposed a novel automatic mapping me-
thod with three approaches to link Sejong Se-
mantic Classes and KorLex using 1) informa-
tion of monosemy/polysemy of word senses, 2) 
instances of nouns of SJD and word senses of 
KorLex, 3) semantically related words of 
nouns of SJD and synsets of KorLex. To find 
common clues from lexical information among 
those language resources is important process 
in automatic mapping method. Our proposed 
automatic mapping method with three ap-
proaches shows notable performances by com-
paring with other studies on automatic map-
ping among language resources: recall 0.837, 
precision 0.717 and F1 0.773. Therefore, from 
those studies, we can improve Korean seman-
tico-syntactic parsing technology by integrat-
ing the argument structures as provided by SJD, 
and the lexical-semantic hierarchy as provided 
by KorLex. In addition, we can enrich three 
resources: KorLex, SJD and STD as results of 
comparing and integrating them. We expect to 
improve automatic mapping technology among 
other Korean language resources through this 
study. 
Acknowledgement 
This work was supported by the National Re-
search Foundation of Korea (NRF) grant 
funded by the Korea government(MEST) (No. 
2007-0054887). 
References 
Jan Scheffczyk, Adam Pease, Michael Ellsworth. 
2006. Linking FrameNet to the Suggested Upper 
Merged Ontology. Proc of the 2006 conference 
on Formal Ontology in Information Systems 
(FOIS 2006): 289-300. 
Ian Niles and Adam Pease. 2003. Linking lexicons 
and ontologies: Mapping wordnet to the sug-
gested upper merged ontology. In Proceedings of 
the 2003 International Conference on Informa-
tion and Knowledge Engineering (IKE 03). 
KorLex, 2007. Korean WordNet, Korean Language 
processing Lab, Pusan National University. 
Available at http://korlex.cs.pusan.ac.kr 
C. Hong. 2007. The Research Report of Develop-
ment 21th century Sejong Dictionary, Ministry 
of Culture, Sports and Tourism, The National In-
stitute of the Korean Language. 
Dennis Spohr. 2008. A General Methodology for 
Mapping EuroWordNets to the Suggested Up-
per Merged Ontology, Proceedings of the 6th 
LREC 2008:1-5. 
Alexander Budanitsky and Graeme Hirst. 2006. 
Evaluating WordNet-based Measures of Lexi-
cal Semantic Relatedness, Computational Lin-
guistics,Vol 32: Issue 1:13- 47. 
Siddharth Patwardhan, Satanjeev Banerjee and Ted 
Pedersen. 2003. Using Measures of Semantic 
Relatedness for Word Sense Disambiguation, 
CICLing 2003, LNCS(vol 2588):241-257. 
Satanjeev Banerjee and Ted Pedersen. 2002. An 
Adapted Lesk Algorithm for Word Sense Dis-
ambiguation Using WordNet, Proceedings of 
CICLing 2002, LNCS 2276:136-145 
Sara Tonelli and Daniele Pighin. 2009. New Fea-
tures for FrameNet -WordNet Mapping, Pro-
ceedings of the 13th Conference on Computa-
tional Natural Language Learning: 219-227. 
Aesun Yoon, Soonhee Hwang, E. Lee, Hyuk-Chul 
Kwon. 2009. Consruction of Korean WordNet 
?KorLex 1.5?, JourNal of KIISE: Sortware and 
Applications, Vol 36: Issue 1:92-108. 
Soonhee Hwang, A. Yoon, H. Kwon. 2010. KorLex 
1.5: A Lexical Sematic Network for Korean 
Numeral Classifiers, JourNal of KIISE: Sort-
ware and Applications, Vol 37: Issue 1:60-73. 
Sun-Mee Bae, Kyoungup Im, Aesun Yoon. 2010. 
Mapping Heterogeneous Ontologies for the 
HLT Applications: Sejong Semantic Classes 
and KorLexNoun 1.5, Korean Journal of Cog-
nitive Science. Vol. 21: Issue 1: 95-126. 
Aesun Yoon. 2010. Mapping Word Senses of Ko-
rean Predicates Between STD(STandard Dic-
tionary) and SJD(SeJong Electronic Dictio-
nary) for the HLT Applications,  Journal of the 
Linguistic Society of Korea. No 56: 197-235. 
Hyopil Shin. 2010. KOLON: Mapping Korean 
Words onto the Microkosmos Ontology and 
Combining Lexical Resources. Journal of the 
Linguistic Society of Korea. No 56: 159-196. 
21
Proceedings of the Fifth Law Workshop (LAW V), pages 38?46,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Consistency Maintenance in Prosodic Labeling for Reliable Prediction of 
Prosodic Breaks 
 
Youngim Jung Hyuk-Chul Kwon 
Dept. Knowledge Resources at Korea Insti-
tute of Science and Technology Information/  
Dept. Computer Science and Engineering at 
Pusan National University/ 
245 Daehang-no Yuseong-gu, San 30, Jangjeon-dong, Geumjeon-gu 
305-806 Daejeon, Republic of Korea Busan, 609-735, Republic of Korea 
acorn@kisti.re.kr hckwon@pusan.ac.kr 
  
Abstract 
For the implementation of the prosody predic-
tion model, large scale annotated speech corpo-
ra have been widely applied. Reliability among 
transcribers, however, was too low for success-
ful learning of an automatic prosodic prediction. 
This paper reveals our observations on perfor-
mance deterioration of the learning model due 
to inconsistent tagging of prosodic breaks in the 
established corpora. Then, we suggest a method 
for consistent prosodic labeling among multiple 
transcribers. As a result, we obtain a corpus 
with consistent annotation of prosodic breaks. 
The estimated pairwise agreement of annotation 
of the main corpus is between 0.7477 and 
0.7916, and the value of K is between 0.7057 
and 0.7569. Considering the estimated K, anno-
tation of the main corpus has reliable consisten-
cy among multiple transcribers. 
1 Introduction 
The naturalness and comprehensibility of text-to-
speech (TTS) synthesis systems are strongly af-
fected by the accuracy of prosody prediction from 
text input. For the implementation of the prosody 
prediction model, large annotated speech corpora 
have been widely applied to both linguistic re-
search and speech processing technologies as in 
(Syrdal and McGory, 2000). Since an increasing 
number of annotated speech corpora become avail-
able, a number of self-learning or probabilistic 
models for prosodic prediction have been sug-
gested. To obtain reliable results from data-driven 
models, the corpus must be large scale, noise-free 
and annotated consistently. However, due to the 
limited range of tagged data with prosodic breaks 
that is used to learn or establish stochastic models 
at present, reliable results cannot be obtained. Thus, 
the reliability among transcribers was too low for 
successful learning of a prosodic model 
(Wightman and Ostendorf, 1994). In addition, the 
performance of ASR systems degrades significant-
ly when training data are limited or noisy as in 
(Alwan, 2008). 
In this study we propose a new methodology of 
training transcribers, annotating a corpus by mul-
tiple transcribers, and validating the reliability of 
intertranscriber agreement. This paper is organized 
as follows: we review related work on corpus an-
notation for speech and language processing tasks 
and method of measuring the reliability of consis-
tency among multiple annotators in Section 2. Sec-
tion 3 describes our observations on performance 
deterioration of the learning model due to inconsis-
tent tagging of prosodic breaks in the established 
corpora. In Section 4, we suggest a procedure of 
constructing a medium-scale corpus, which are 
aimed at maintaining consistency in prosodic labe-
ling among multiple annotators. Through a series 
of experiments during the training phase, the im-
provement of the agreement of multiple annotators 
is shown. The final experiment is performed in 
order to guarantee labeling agreement among five 
annotators. A brief summary and future work are 
presented in the final section. 
2 Related Work 
As linguistically-annotated corpora became critical 
resources, science of corpus annotation has been 
highlighted and evolved to reflect various interests 
in the field as shown in (Ide, 2007). In order to an-
notate linguistic information to large-scale corpora, 
two methods have been used; existing natural lan-
38
guage processing (NLP) tools such as part-of-
speech taggers, syntactic parsers, sentence boun-
dary recognizers, named entity recognizers as have 
been used to generate annotations for ANC data 
(Ide and Suderman, 2006). Big advantages of using 
existing tools are that much cost and time can be 
saved and that the annotation result is consistent. 
In addition, it could obtain reliable accuracies and 
reduce the prohibitive cost of hand-validation by 
combining results of multiple NLP tools. However, 
tagging for all other linguistic phenomena is still 
mainly a manual effort as presented in (Eugenio, 
2000). Thus, human annotators are required for 
tagging, correcting or validating the linguistic in-
formation although human annotators are very ex-
pensive and inconsistent in various aspects.  
Linguists and language engineers have recog-
nized the importance of the consistency of annota-
tion among multiple annotators while they 
construct a large-scale corpus and have focused on 
how to measure the inter-annotator agreement. 
Their annotators had difficulties in discriminating 
one annotation category from others that are close-
ly related to each other. Fellbaum et al (1999) who 
performed a semantic annotation project which 
aimed at linking each content word in a text to a 
corresponding synset in WordNet found out that, 
with increasing polysemy, both inter-annotator and 
annotator-expert matches decreased significantly. 
As to measure the rate of agreement, Fellbaum et 
al. (1999) used a very simple measurement, the 
percentage of agreement in semantic annotation 
task. A greedy algorithm for increasing the inter-
annotator agreement has been suggested by Ng et 
al. (1999). However, automatic correction of the 
manual tagging cannot reflect natural linguistic 
information tagged by human. 
On the other hand, in prosodic annotation, the re-
liable measurement of intertranscriber agreement 
was studied by Beckman et al (1994) initially, 
since the goal of the original ToBI system design-
ers was to design a system with ?reliability (agree-
ment between different transcribers must be at 
least 80%)?, ?coverage?, ?learnability?, and ?capa-
bility?. The designers and developers of adapta-
tions of ToBI for other languages and dialects such 
as G-ToBI, GlaToBI and K-ToBI have proved the 
usability of their labeling system rather than have 
suggested the method of maintaining the intertran-
scriber agreement based on the aforementioned 
criteria (Grice et al, 1996; Mayo et al, 1996; Jun 
et al, 2000).  
3 Problem Description 
3.1 Obtaining a Large Scale Speech Anno-
tated Corpus 
In order to design and implement a prediction 
model of prosodic break, annotated corpus should 
be prepared. Recorded speech files and text scripts 
of Korean Broadcasting Station (KBS) News 9 
were collected and manual annotation was con-
ducted by two linguistic specialists. Each hand-
labeled half of the selected script for prosodic 
breaks was cross-checked with the other half. The 
resultant corpus had 47,368 eo-jeol1s. The size of 
this corpus, however, does not seem to be suffi-
cient. An easy way to construct a larger-scale cor-
pus is using existing corpora in the field. To build 
a large volume of learning and testing data, anno-
tated speech data from Postech speech groups were 
obtained. The Postech data included 122,025 eo-
jeols from Munhwa Broadcasting Corporation 
(MBC) news. Three types of break, viz., major 
breaks, minor breaks and no breaks, were anno-
tated after each eo-jeol in KBS data (our initial 
data) and MBC data.  
3.2 Performance Deterioration of Learning 
Models due to Inconsistent Annotation 
KBS and MBC news data were selected, to ex-
amine the effect of prosodic breaks in corpora con-
structed by different groups on learning and testing. 
Only 46,526 eo-jeols were randomly sampled from 
the MBC News corpus, whereas the entire KBS 
News data was used for learning and testing, to 
avoid potential side effects from the differing data 
size. 
 
 KBS MBC (Postech data) 
Training Data 38,243 37,258 
Testing Data  9,103 9,268 
Table 1 Size of Training and Test data 
 
                                                          
1 An eo-jeol in Korean can be composed up of one morpheme 
or several concatenated morphemes of different linguistic 
features which are equivalent to a phrase in English. This 
spacing unit is referred as an ?eo-jeol?, ?word?, or ?morpheme 
cluster? in Koeran linguistic literatures. We adopt ?eo-jeol? in 
order to refer to ?an alphanumeric cluster of morphemes with a 
space on either side?.  
39
C4.5 and CRFs were adapted in this experiment. 
The learning and testing was conducted in two 
phases. First, learning and testing of the prosodic 
break prediction models used a corpus constructed 
by a single group. Five-fold cross-validation was 
used for evaluating the models. Second, learning 
and evaluation of the models used a different cor-
pus constructed by each group. The ratio of train-
ing to testing data (held-out data) was four to one. 
The results obtained from the first and second 
phases of learning and testing are presented in Ta-
ble 2. 
 
Algo-
rithm 
1st Phase Precision 
(Learning -Testing) 
2nd Phase Precision 
(Learning -Testing) 
KBS-KBS MBC-MBC KBS-MBC MBC-KBS 
C4.5 85.30% 62.53% 38.78% 44.96% 
CRFs 84.65% 67.52% 37.96% 45.01% 
Table 2 Experimental Results for Impact Analysis of 
Inconsistent Tagging 
 
The prediction models performed well with C4.5 
and CRFs learning algorithms when the model was 
trained and tested with KBS news data. However, 
its performance decreased drastically when the 
model was initially trained with KBS news data 
and subsequently tested with MBC news data. The 
performance of the learning model trained with 
MBC news data also deteriorated when tested with 
KBS data. These results suggest that serious per-
formance deterioration is caused by data inconsis-
tency rather than by the learning algorithm per se.  
3.3 Analysis on Inconsistent Annotation 
The deterioration of the performance presented in 
Section 3.2 is quite considerable, despite the fact 
that the same genre and level of prosodic break 
labeling system was selected. After analyzing the 
data, we identified three main reasons as follows. 
(1) Perceptual Prominence of Prosodic Labeling 
Systems 
Despite the fact that three types of prosodic break 
have been commonly used in the speech engineer-
ing field for a considerable time as shown in (Os-
tendorf and Veilleux, 1994), they have not been 
clearly defined or referenced in standard prosodic 
labeling conventions. In particular, the notion of 
the minor break is rather vague, whereas those of 
no break and major break are intuitively clear as in 
(Mayo et al, 1996).   
In the MBC news data labeled by Postech, sen-
tences that had all prosodic breaks tagged as no 
break were frequently found, even if two long 
clauses exist in a sentence. Most sentences had 
been annotated only with no break. The speaking 
rate of news announcers on air is relatively fast and 
no obvious audible break seems to exist in their 
speech. However, Kim (1991) showed that even 
well-trained news announcers rarely read a sen-
tence without breaks. Therefore, minor breaks need 
to be recognized not only by the duration of the 
break, but also by the tonal changes or lengthening 
of the final syllable as shown in (Kim, 1991; Jun, 
2006; Jung et al, 2008). 
(2) Different Perceptibility of Prosodic Breaks 
among Transcribers 
Grice et al (1996), Mayo et al (1996) and Jun et al 
(2000) have focused on reliability-agreement be-
tween different transcribers as the main criterion of 
evaluation. This fact indicates that individual labe-
ling of a single utterance can differ, because each 
transcriber?s recognition of the prosodic labeling 
system varies. And, the perceptibility of each tran-
scriber differs. A large-scale corpus is necessary 
for modeling a data-driven framework, and the 
greater the number of transcribers cooperating, the 
poorer the intertranscriber agreement becomes. 
However, maintaining the intertranscriber agree-
ments is often neglected as empirical work when 
researchers build and analyze a speech annotated 
corpus for implementation of the prosody model. 
(3) Syntactic or Semantic Ambiguities 
A single sentence with syntactic ambiguities has 
several different interpretations. In spoken lan-
guage, prosody prevents garden path sentences and 
enables resolution of syntactic ambiguity as shown 
in (Kjelgaard and Speer, 1999; Schafer, 1997). 
Sentences such as the one in the following exam-
ple (E1) can be grammatically constructed with 
multiple syntactic structures2.  
 
(E1) ????? ???? ??? ????  
???? ???????. 
a. Gosogbeoseuga // jung-angseon-eul # chimbeom-
hae /// maju-odeon # seung-yongchaleul  // deul-
ibad-ass-seubnida 
?An express bus drove over the center line and 
                                                          
2 In examples, letters in italics denote phonetic transliteration 
of Korean; hyphens in transliteration are used for segmenta-
tion of syllables. 
40
rammed into an oncoming car.? 
 
b. Gosogbeoseuga /// jung-angseon-eul # chim-
beomhae // maju-odeon # seung-yongchaleul  /// 
deul-ibad-ass-seubnida 
?An express bus rammed into an oncoming car 
which drove over the center line.? 
#: no break,  //: minor break, ///: major break 
 
The prosodic phrasing in both (a) or (b) can be cor-
rect, depending on the sentence?s syntactic struc-
ture. The pattern in (E1) is quite frequent in 
Korean, particularly in situations where the topic is 
broad. This kind of syntactic ambiguity needs to be 
resolved by semantic or pragmatic information, 
since it cannot be resolved using syntactic informa-
tion only. 
As we previously mentioned, three main prob-
lems arise when annotated speech data are both 
constructed by multiple labelers in a research 
group and the data are collected from different 
groups. Considering the impact of the quality of 
annotated corpora on the data-driven models, the 
overall procedure of corpus construction including 
the data collection and preprocess, labeling system 
selection and intertranscriber agreement mainten-
ance should be designed and then evaluated as 
shown in Section 4. 
4 Corpus Building 
4.1 Selection of Prosodic Labeling System 
In this paper, we define seven types of prosodic 
break in combination with phrasal boundary tones 
since a prosodic break cannot be separated from a 
boundary tone. Our seven types are defined as fol-
lows: 
 
(1) Major break with falling tone: For cases 
with a strong phrasal disjuncture and a strong 
subjective sense of pause. The positions of major 
breaks generally correspond to the boundaries of 
intonational phrases (marked ?///L?). 
(2) Major break with rising tone: For cases 
with a strong phrasal disjuncture but a weak sub-
jective sense of pause length (marked ?///H?). 
(3) Major break with middle tone: In real data, 
major breaks with middle tone (or major breaks 
without tonal change) are observed as in (Lee, 
2004), although they have no definition or ex-
planation in K-ToBI. They have been observed 
in very fast speech such as headline news utter-
ances (marked ?///M?).  
(4) Minor break with rising tone: For cases 
with a minimal phrasal disjuncture and no strong 
subjective sense of pause. The positions of mi-
nor breaks correspond to the boundaries of ac-
centual phrases with rising tone. When an 
utterance is so fast that a pause cannot be recog-
nized clearly, minor breaks are realized by tonal 
changes or segment lengthening of the final syl-
lable (marked ?//H?). 
(5) Minor break with middle tone: For cases 
with prosodic words in compound words, such 
as compound nouns or compound verbs. Breaks 
between noun groups in a compound word or be-
tween verbs in a compound verb may be realized 
when the overall length of a compound word is 
long, whereas a break is absent in a short com-
pound word (marked ?//M?). 
(6) Minor break with falling tone: For cases 
with minimal phrasal disjuncture and no strong 
subjective sense of pause. The positions of mi-
nor breaks correspond to the boundaries of ac-
centual phrases with falling tone.  
(7) No break: For internal phrase word bounda-
ries. There is no prosodic break between one-
word modifiers and their one-word partners or 
between a word-level argument and its predicate, 
because the two words are syntactically and se-
mantically combined (marked ?#?). 
 
The seven types of prosodic break are mapped to 
K-ToBI break indices, enabling further reusability 
of the corpus labeled by the suggested break types.  
 
K-ToBI Suggested Prosodic Breaks 
Break  
Index 
0  No Break (#) 
1  Minor Break (//L) 
2  Minor Break (//H, //M) 
3  Major Break (///H, ///M, ///L) 
Tone 
Index 
Ha, H% H 
La, L% L 
L+  M 
Table 3 Mapping between break indices of K-ToBI and 
the suggested prosodic breaks 
 
Jun et al (2000) showed that the tonal pattern 
agreement for each word was approximately 36% 
41
for all labelers and this low level of agreement ap-
pears to be due to the nature of the tonal pattern. 
Although fourteen possible AP (Accent Phrase) 
tonal patterns exist, these variations are neither 
meaningful nor phonologically correct. We con-
cluded that the final phrasal tones are sufficient for 
the recognition of prosodic boundaries. 
4.2 Data Selection and Preprocessing 
In this study, KBS news scripts (issued January, 
2005 ~ June, 2006) were collected as a raw corpus 
from web. Although the speech rate of TV news 
speech is faster than that of general read speech, 
announcers are trained to speak Standard Korean 
Language and to generate standard pronunciations, 
tones and breaks. In addition, individual stylistic 
variation is restricted in the announcer?s speech.  
The text formats of news scripts extracted from 
the web are unified. Then, sentences or expressions 
in news scripts differing from those in real sen-
tences in multimedia files are revised according to 
the real utterances of the announcer. The selection 
and revision of the sentences is performed accord-
ing to the following criteria.  
 
1) Headline news sentences uttered by one female 
announcer are collected. 
2) Minimum of five eo-jeols are included in one 
sentence. 
3) Real speech of news script read by the announc-
er is considered as primary source of prosodic 
break tagging for transcribers.  
4) Sentences in the news script are deleted unless 
they are read by the announcer in real speech files.  
5) Between 1-3 eo-jeols in news scripts differing 
from those in speech files are revised according to 
the real speech if there is no semantic change.  
6) Sentences in the news script differing consider-
ably from those in speech files are deleted.  
7) Words or phrases in the news script differing 
from those in speech files due to spelling/grammar 
errors are not corrected manually. They are cor-
rected automatically by the PNU grammar checker, 
which shows over 95% accuracy as in (Kwon et 
al., 2004). 
4.3 Training Transcribers 
The most reliable method of maintaining the con-
sistency and accuracy of prosodic breaks by mul-
tiple transcribers is for each well-trained 
transcriber to annotate prosodic breaks in the entire 
corpus. Then the majority of the tagging results 
among multiple transcribers are selected as an an-
swer for the target eo-jeol. However, this method 
where all transcribers annotate the same corpus in 
depth is too time consuming and costly.  Due to 
time and cost constraints, most related studies use 
a simpler method. If the size of the corpus is small, 
then a professional linguist annotates the entire 
corpus as in (Maragoudakis et al, 2003). If the size 
of corpus is large, more than two transcribers di-
vide the corpus by the number of transcribers and 
each transcriber annotates his/her own part as in 
(Wightman and Ostendorf, 1994; Viana et al, 
2003). Unless the transcribers are trained and the 
reliability of the intertranscriber agreement is vali-
dated, consistency of annotation  by multiple tran-
scribers cannot be assured. Hence, a method for 
maintaining the reliability of the intertranscriber 
agreement of prosodic breaks is suggested in this 
paper.  
The overall procedure of training the transcribers, 
annotating the main corpus with prosodic breaks 
and validating the reliability of tagging consistency 
among multiple transcribers is illustrated in Figure 
1. 
 
Training Transcribers
YES
Validating Reliability
Education of cases
Guideline Education
Annotating identical data 
thoroughly by n 
transcribers
Measuring intertranscriber 
agreement
K>0.67
New training 
data
NO
Main corpus
?
N 
parts
?
n transcribers annotate 
individually
Annotating Main Corpus
New data for validating 
intertranscriber agreement
Annotating identical data 
thoroughly by n 
transcribers
Measuring intertranscriber 
agreement
K: kappa coefficient
K>0.67
Validated reliability of 
intertranscriber agreement
YES
+
Analysis 
corpus
Evaluation 
corpus 1
Evaluation 
corpus 2
 Figure 1 Overall Procedure of Corpus Building 
 
Firstly, guidelines are provided for transcribers to 
familiarize themselves with the prosodic labeling 
system suggested in Section 4.1. Secondly, in order 
to improve the awareness of the length or strength 
of each prosodic break type in detail, transcribers 
repeatedly listen to speech files corresponding to 
several paragraphs in news scripts. In addition, 
WaveSurfer Version.1.8.5, which is an open source 
42
program for visualizing and manipulating speech, 
is utilized for transcribers to examine the pitch 
contour, waveform, and power plot of speech files.  
In the training phase, five transcribers annotate 
the same data with prosodic breaks at the same 
time and then compare the results of their annota-
tions, and discuss and repeatedly correct the vari-
ous errors until reliable agreement among them is 
reached. The data used for this intertranscriber 
agreement training is given in Table 4.  
 
 1
st 2nd 3rd 4th 
#  eo-jeols 422 544 491 711 
# sentences 35 49 42 32 
Table 4 Data used in intertranscribers training 
 
After mastering the guidelines and training with 
each data set, specific reasons for inconsistency 
among transcribers were analyzed and their solu-
tions were educated as follows: 
 
(1) Prosodic breaks were inserted due to announc-
ers' emphasis on a certain eo-jeol, mistakes in read-
ing the sentence or the habit of slowing down two 
or three eo-jeols from the end of a sentence. Some 
transcribers recognized these as speakers? errors 
and corrected them in their annotations. On the 
other hand, others annotated prosodic breaks ac-
cording to what they heard, regardless of errors. 
Due to these differing policies on annotation, the 
resultant annotation of prosodic breaks among 
transcribers is not consistent, as shown in example 
(E2).  
 
(E2) ??   ????? (///H, #)3 ????. 
deo-ug  simgaghaejigo       iss-seubnida. 
more   serious become       progress +EM4  
  ?(sth) becomes more serious? 
 
Inconsistency derived from these speakers? errors 
should be deleted. 
 
(2) If the speech rate of the announcer is too fast 
for some transcribers to perceive audible breaks 
                                                          
3 The correct answer among different annotations is under-
lined. 
4 Notes on abbreviations of Korean grammatical morphemes 
are as follows: EM for ending markers, TP for topical postpo-
sition, LCM for locative case marker, OCM for objective case 
marker, PEC for pre-ending denoting continuous 
between two eo-jeols, they omitted the minor break, 
whereas others put a minor break in the same place, 
as shown in (E3).  
(E3) ??? (#, //L)  ??????? 
geuleona        jilbyeonggwanlibonbu-neun 
however        Korea Center for Disease Control+TP 
and Prevention+TP 
?However, the Korea Center for Disease Control 
and Prevention? 
 
In this case, transcribers need to pay attention to 
whether the final tone of the target eo-jeol is rising 
or falling. In order to reduce inconsistency derived 
from missing breaks, transcribers repeatedly prac-
tice while listening to similar patterns. 
 
(3) If only one annotator selects a different type of 
prosodic break than the others for the answer of the 
same place, he/she must change his approach in 
annotating prosodic breaks. 
 
(4) Wightman and Ostendorf (1994) and Ross and 
Ostendorf (1996) have revealed that there is pro-
sodic variability even for news speech data. The 
announcer showed variability in the location, 
strength or length, and tonal change in our news 
data as well. For example, the announcer occasio-
nally put a minor break between two eo-jeols con-
sisting of a time expression, as shown in (E4).  
 
(E4)  a. ?? //H  2002?      ??,    
jinan //H   2002nyeon   oneul,    
past       2002year    this day   
?(on) this day 2002,?  
 
b. ?? #  2000?     1?  
jinan #   2000nyeon  1wol   
past     2000year    January  
?(in) January 2000,? 
 
For a time expression including less than four eo-
jeols, no break should be marked in it.  
Discussion and education such cases described 
above after annotating new training data sets re-
peats till the intertranscriber agreement is suffi-
ciently high. The intertranscriber agreement in 
annotating seven-level prosodic breaks including 
tonal changes is shown in Table 5.  
 
Agreement Cumulative rate (%) 
43
1st 2nd  3rd  4th  
Five (all) agreed 43.84 50.55 55.80 57.67 
At least four agreed 60.90 68.20 73.52 75.53 
At least three agreed 81.75 87.50 90.84 91.70 
Table 5 Intertranscriber agreement in training 
 
The cumulative rate of agreement of more than 
half of the transcribers (n+1/2) is measured by ap-
proximate figures. Specifically, the rate of the in-
tertranscriber agreement is calculated with the 
cumulative rate at which all five transcribers 
agreed, at least four of them agreed, and at least 
three of them agreed. The resultant agreement of 
the first experiment is quite low, though the first 
experiment was performed after the transcribers 
had familiarized themselves with the guidelines 
and studied many examples. The intertranscriber 
agreement in annotating data with seven-level pro-
sodic breaks increases continuously with repeated 
training and experiments. This indicates that edu-
cating transcribers with guidelines and examples is 
not sufficient, and training of transcribers is re-
quired prior to annotation of the main corpus with 
specified tagging classes by multiple transcribers. 
In order to review how accurately each individual 
transcriber annotates the corpus, the annotation 
accuracy of each individual transcriber is estimated. 
The prosodic break type for which at least three of 
them agreed is considered as the answer. The an-
notation result of each transcriber is compared to 
the answer, and then the accuracy is estimated by 
counting the number of annotations that match the 
answers. Table 6 shows the estimated annotation 
accuracy of five transcribers from the 1st to the 4th 
experiment.  
 
Transcriber 
Estimated accuracy (%) 
1st 2nd  3rd  4th 
A 94.51 84.00 86.32 91.56 
B 78.03 85.26 89.24 93.25 
C 78.03 93.05 94.39 94.02 
D 88.44 90.32 90.36 90.64 
E 82.37 83.79 84.08 89.11 
Table 6 Estimated accuracy of each  transcriber 
 
Although there are individual variations, the esti-
mated accuracy of the transcribers increases steadi-
ly.  
After the four experiments, the cumulative rate of 
agreement of more than half of the transcribers 
reached 91.70% and the estimated accuracy of in-
dividual transcribers increased to 89.11~94.02%. 
Hence, an objective and reliable measurement for 
intertranscriber agreement is required in order to 
decide whether the training is sufficient.  
The most commonly used methods to assess the 
level of agreement among transcribers are pairwise 
analysis and Kappa statistics. The reliability of 
intertranscriber agreement of the four experiments 
has been assessed with these two measurements 
and the result is given in Table 7.  
 
Measurement 1st  2nd   3rd   4th 
Pairwise analysis 0.6385 0.6969 0.7375 0.7477 
Kappa statistics 0.5783 0.6464 0.6938 0.7057 
Table 7 Reliability of intertranscriber agreement  
 
Since the value of K is greater than 0.67 in the 3rd 
and 4th experiment, the intertranscriber agreement 
for annotating prosodic breaks is considered to 
have reached a reliable level as shown in (Carletta, 
1996). Then annotation of the main corpus is per-
formed. 
The main corpus comprising 29,686 eo-jeols is 
divided into five parts. Each partition is assigned to 
the trained five transcribers and annotation is inde-
pendently performed. WaveSurfer, which is used 
in the training phase, is also used in the annotation 
phase for the display and annotation of speech. 
Transcribers may openly discuss their annotations, 
even though they annotated different parts of the 
main corpus.   
4.4 Validation of Reliability of Intertran-
scriber Agreement 
Since each individual transcriber annotated a dif-
ferent part of the main corpus, the reliability of 
intertranscriber agreement cannot be measured di-
rectly. We assume that intranscriber agreement 
does not change dramatically before and after  an-
notation of the main corpus.  
Hence, another data set including 1,149 eo-jeols 
(46 sentences), with a size 1.5x larger than that of 
the data  set used in the 4th  experiment, is collected 
and used instead, in order to validate the reliability 
of agreement. Immediately after annotation of the 
main corpus, the final experiment is performed 
following the procedure performed in the training 
44
phase, except for the education steps. The five 
transcribers annotated the same data in depth, 
however, they worked independently. They were 
not allowed to discuss prosodic labeling. Pairwise 
analysis and Kappa statistics are used in measuring 
intertranscriber agreement on the validation data 
set. The pairwise agreement and K found in the 
validation experiment after annotation of the main 
corpus was 0.79 and 0.76, respectively. 
Both agreement figures are greater than those 
found in the prior experiments, which were re-
peated four times in the training phase. Based on 
this result, annotation of the main corpus is also 
considered to be part of training of transcribers.  
According to our assumption, the estimated inter-
transcriber agreement of annotation of the main 
corpus annotation is between the agreement of the 
prior and post experiments, as shown in Figure 2. 
 
 
Figure 2 Estimated intertranscriber reliability in annota-
tion of main corpus  
 
The estimated pairwise agreement of annotation of 
the main corpus is between 0.7477 and 0.7916, and 
the value of K is between 0.7057 and 0.7569. Con-
sidering the estimated K, annotation of the main 
corpus has reliable consistency among multiple 
transcribers. 
As a result, we obtained a corpus with consistent 
annotation of prosodic breaks. The data used in 
validation experiment is included as well. The sta-
tistics of the constructed corpus is shown in Table 
8.  
 
Data # eo-jeols # sentences 
Data set from valida-
tion experiment 
1,149 46 
Main corpus 29,663 1,319 
Total 30,812 1,365 
Table 8 Size of resultant corpus  
 
It took approximately three months for us to train 
transcribers, annotate main corpus and validate the 
reliability of intertranscriber agreement in the main 
corpus. Considering the size of the constructed 
corpus, three months might be regarded as a consi-
derable amount of time for researchers who want 
to build a large-scale annotated corpus. However, 
most time was spent on analyzing the inconsisten-
cies among transcribers in initial experiments dur-
ing the training step. Hence, if transcribers are 
trained following the suggested method in this pa-
per, the amount of time for transcribers to annotate 
the target corpus with reliable consistency will de-
crease dramatically compared with the time for all 
transcribers to annotate prosodic breaks in the en-
tire corpus. 
5 Conclusions 
In this study, potential problems in the construction, 
collection and utilization of a speech annotation 
corpus have been identified, and a solution for 
each type of problem has been suggested. The 
overall procedure of training transcribers, tagging 
the main corpus and validating the reliability of 
intertranscriber agreement on the main corpus has 
also been specifically described. As a result, we 
obtained a corpus with consistent annotation of 
prosodic breaks. The estimated pairwise agreement 
of annotation of the main corpus is between 0.7477 
and 0.7916 and K is between 0.7057 and 0.7569. 
The suggested method for constructing a consis-
tently annotated corpus and validating the consis-
tency of the resultant annotation must be applied 
prior to implementation of data-driven models for 
predicting prosodic breaks. As our future work, the 
resultant corpus will be used for building a robust 
prediction model of prosodic boundary. 
In addition, the method can be utilized for seman-
tic annotation tasks, discourse tagging and others, 
which have a similar problem due to the differing 
perceptions of transcribers in recognizing the 
closely related categories.  
Acknowledgement 
This work was supported by the National Re-
search Foundation of Korea(NRF) grant funded by 
the Korea government(MEST) (2010-0028784). 
 
45
References  
Abeer Alwan. 2008. Dealing with Limited and Noisy 
Data in ASR: a Hybrid Knowledge-based and Statis-
tical Approach, Proc. Interspeech 2008, Brisbane 
Australia, , pp. 11-15. 
Amy J. Schafer. 1997. Prosodic Parsing: The Role of 
Prosody in Sentence Comprehension, University of 
Massachusetts. 
Ann K. Syrdal and Julia McGory. 2000. Inter-
transcriber Reliability of ToBI Prosodic Labeling, 
Proc.Interspeech 2000, pp. 235-238. 
Barbara Di Eugenio. 2000. On the usage of Kappa to 
evaluate agreement on coding tasks, Proc. Second In-
ternational Conference on Language Resources and 
Evaluation, pp.441-444. 
Catherine Mayo, Matthew Aylett, D. Robert Ladd. 1996. 
Prosodic Transcription of Glasgow English: An 
Evaluation Study of GlaToBI, Proc. ESCA Work-
shop on Intonation: Theory, Models and Applications, 
Athens Greece, pp.231-234. 
Christiane Fellbaum, Joachim Grabowski and Shari 
Landes. 1999. Performance and Confidence in a Se-
mantic Annotation Task, WordNet: An Electronic 
Lexical Database etd. Fellbaum, MIT Press, London. 
Colin W. Wightman and Mari Ostendorf. 1994. Auto-
matic Labeling of Prosodic Patterns, IEEE Transac-
tions on Speech and Audio Processing, 2(4):469-481. 
Hee Tou Ng, Chung Yong Lim and Shou King Foo. 
1999. A Case Study on Inter-Annotator Agreement 
for Word Sense Disambiguation, Proc. ACL 
SIGLEX Workshop on Standardizing Lexical Re-
sources pp. 9-13. 
Ho-Young Lee. 2004. H and L are Not Enough in Into-
national Phonology, Korean Journal of Linguistics, 
39:71-79. 
Hyuk-Chul Kwon, Mi-young Kang and Sung-Ja Choi. 
2004. Stochastic Korean Word Spacing with Smooth-
ing Using Korean Spelling Checker, Computer 
Processing of Oriental Languages, 17:239-252. 
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic, Computational Lin-
guistics,  22( 2):249-254. 
K. Ross and M. Ostendorf. 1996. Prediction of abstract 
prosodic labels for speech synthesis, Computer 
Speech and Language, 10(3):155-185. 
M. C?u Viana, Lu?s C. Oliveira and Ana I. Mata. 2003. 
Prosodic Phrasing: Machine and Human Evaluation, 
International Journal of Speech Technology, 6:83-94. 
M. Maragoudakis, P. Zervas, N. Fakotakis and G. Kok-
kinakis. 2003. A Data-Driven Framework for Intona-
tional Phrase Break Prediction, Lecture Notes in 
Computer Science, 2807: 189-197. 
M. Ostendorf and N. Veilleux. 1994. A Hierarchical 
Stochastic Model for Automatic Prediction of Pro-
sodic Boundary Location, Computational Linguistics, 
20(1):27-54. 
Margaret M. Kjelgaard and Shari R. Speer.  1999. Pro-
sodic Facilitation and Interference in the Resolution 
of Temporary Syntactic Closure Ambiguity, Journal 
of Memory and Language, 40:153-194. 
Martine Grice, Matthias Reyelt, Ralf Benzmuller, J?rg 
Mayer and Anton Batliner. 1996.  Consistency in 
Transcription and Labelling of German Intonation 
with GToBI, Proc. Interspeech1996, pp. 1716-1719. 
Mary E. Beckman, John F. Pitrelli and Julia Hirschberg. 
1994. Evaluation of Prosodic Transcription Labeling 
Reliability in the ToBI Framework, Proc. Interspeech 
1994, pp. 123-126. 
Nancy Ide. 2007. Annotation Science From theory to 
Practice and Use: Data Structures for Linguistics Re-
sources and Applications, Proc. Bienniel GLDV 
Conference, T?bingen, Germany. 
Nancy Ide and Keith Suderman. 2006. Integrating Lin-
guistic Resources: The American National Corpus 
Model, Proceedings of the Fifth Language Resources 
and Evaluation Conference, Genoa, Italy. 
Sangjun Kim. 1991. Study on Broadcast Language, 
Hongwon, Seoul. 
Sun-Ah Jun. 2006. Prosody in Sentence Processing: 
Korean vs. English, UCLA Working Papers in Pho-
netics, 104:26-45. 
Sun-Ah Jun, Sook-Hyang Lee, Keeho Kim, Yong-Ju 
Lee. 2000. Labler agreement in Transcribing Korean 
Intonation with K-ToBI, Proc. Interspeech 2000, pp. 
211-214. 
Youngim Jung, Sunho Cho, Aesun Yoon and Hyuk-
Chul Kwon. 2008. Prediction of Prosodic Break Us-
ing Syntactic Relations and Prosodic Features, Ko-
rean Journal of Cognitive Science, 19(1):89 -105. 
WaveSurfer. WaveSurfer ver.1.8.5, 
http://crfpp.sourceforge.net/. 
46
