Confidence Estimation for Information Extraction
Aron Culotta
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
culotta@cs.umass.edu
Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Information extraction techniques automati-
cally create structured databases from un-
structured data sources, such as the Web or
newswire documents. Despite the successes of
these systems, accuracy will always be imper-
fect. For many reasons, it is highly desirable to
accurately estimate the confidence the system
has in the correctness of each extracted field.
The information extraction system we evalu-
ate is based on a linear-chain conditional ran-
dom field (CRF), a probabilistic model which
has performed well on information extraction
tasks because of its ability to capture arbitrary,
overlapping features of the input in a Markov
model. We implement several techniques to es-
timate the confidence of both extracted fields
and entire multi-field records, obtaining an av-
erage precision of 98% for retrieving correct
fields and 87% for multi-field records.
1 Introduction
Information extraction usually consists of tagging a se-
quence of words (e.g. a Web document) with semantic
labels (e.g. PERSONNAME, PHONENUMBER) and de-
positing these extracted fields into a database. Because
automated information extraction will never be perfectly
accurate, it is helpful to have an effective measure of
the confidence that the proposed database entries are cor-
rect. There are at least three important applications of
accurate confidence estimation. First, accuracy-coverage
trade-offs are a common way to improve data integrity in
databases. Efficiently making these trade-offs requires an
accurate prediction of correctness.
Second, confidence estimates are essential for inter-
active information extraction, in which users may cor-
rect incorrectly extracted fields. These corrections are
then automatically propagated in order to correct other
mistakes in the same record. Directing the user to
the least confident field allows the system to improve
its performance with a minimal amount of user effort.
Kristjannson et al (2004) show that using accurate con-
fidence estimation reduces error rates by 46%.
Third, confidence estimates can improve performance
of data mining algorithms that depend upon databases
created by information extraction systems (McCallum
and Jensen, 2003). Confidence estimates provide data
mining applications with a richer set of ?bottom-up? hy-
potheses, resulting in more accurate inferences. An ex-
ample of this occurs in the task of citation co-reference
resolution. An information extraction system labels each
field of a paper citation (e.g. AUTHOR, TITLE), and then
co-reference resolution merges disparate references to the
same paper. Attaching a confidence value to each field
allows the system to examine alternate labelings for less
confident fields to improve performance.
Sound probabilistic extraction models are most con-
ducive to accurate confidence estimation because of their
intelligent handling of uncertainty information. In this
work we use conditional random fields (Lafferty et al,
2001), a type of undirected graphical model, to automat-
ically label fields of contact records. Here, a record is an
entire block of a person?s contact information, and a field
is one element of that record (e.g. COMPANYNAME). We
implement several techniques to estimate both field con-
fidence and record confidence, obtaining an average pre-
cision of 98% for fields and 87% for records.
2 Conditional Random Fields
Conditional random fields (Lafferty et al, 2001) are undi-
rected graphical models to calculate the conditional prob-
ability of values on designated output nodes given val-
ues on designated input nodes. In the special case in
which the designated output nodes are linked by edges in
a linear chain, CRFs make a first-order Markov indepen-
dence assumption among output nodes, and thus corre-
spond to finite state machines (FSMs). In this case CRFs
can be roughly understood as conditionally-trained hid-
den Markov models, with additional flexibility to effec-
tively take advantage of complex overlapping features.
Let o = ?o1, o2, ...oT ? be some observed input data se-
quence, such as a sequence of words in a document (the
values on T input nodes of the graphical model). Let S be
a set of FSM states, each of which is associated with a la-
bel (such as COMPANYNAME). Let s = ?s1, s2, ...sT ? be
some sequence of states (the values on T output nodes).
CRFs define the conditional probability of a state se-
quence given an input sequence as
p?(s|o) =
1
Zo
exp
( T
?
t=1
?
k
?kfk(st?1, st,o, t)
)
,
(1)
where Zo is a normalization factor over all state se-
quences, fk(st?1, st,o, t) is an arbitrary feature func-
tion over its arguments, and ?k is a learned weight for
each feature function. Zo is efficiently calculated using
dynamic programming. Inference (very much like the
Viterbi algorithm in this case) is also a matter of dynamic
programming. Maximum aposteriori training of these
models is efficiently performed by hill-climbing methods
such as conjugate gradient, or its improved second-order
cousin, limited-memory BFGS.
3 Field Confidence Estimation
The Viterbi algorithm finds the most likely state sequence
matching the observed word sequence. The word that
Viterbi matches with a particular FSM state is extracted
as belonging to the corresponding database field. We can
obtain a numeric score for an entire sequence, and then
turn this into a probability for the entire sequence by nor-
malizing. However, to estimate the confidence of an indi-
vidual field, we desire the probability of a subsequence,
marginalizing out the state selection for all other parts
of the sequence. A specialization of Forward-Backward,
termed Constrained Forward-Backward (CFB), returns
exactly this probability.
Because CRFs are conditional models, Viterbi finds
the most likely state sequence given an observation se-
quence, defined as s? = argmaxs p?(s|o). To avoid an
exponential-time search over all possible settings of s,
Viterbi stores the probability of the most likely path at
time t that accounts for the first t observations and ends
in state si. Following traditional notation, we define this
probability to be ?t(si), where ?0(si) is the probability of
starting in each state si, and the recursive formula is:
?t+1(si) = max
s?
[
?t(s?) exp
(
?
k
?kfk(s?, si,o, t)
)]
(2)
terminating in s? = argmax
s1?si?sN
[?T (si)].
The Forward-Backward algorithm can be viewed as a
generalization of the Viterbi algorithm: instead of choos-
ing the optimal state sequence, Forward-Backward eval-
uates all possible state sequences given the observation
sequence. The ?forward values? ?t+1(si) are recursively
defined similarly as in Eq. 2, except the max is replaced
by a summation. Thus we have
?t+1(si) =
?
s?
[
?t(s?) exp
(
?
k
?kfk(s?, si,o, t)
)]
.
(3)
terminating in Zo =
?
i ?T (si) from Eq. 1.
To estimate the probability that a field is extracted
correctly, we constrain the Forward-Backward algorithm
such that each path conforms to some subpath of con-
straints C = ?sq . . . sr? from time step q to r. Here,
sq ? C can be either a positive constraint (the sequence
must pass through sq) or a negative constraint (the se-
quence must not pass through sq).
In the context of information extraction, C corresponds
to an extracted field. The positive constraints specify the
observation tokens labeled inside the field, and the neg-
ative constraints specify the field boundary. For exam-
ple, if we use states names B-TITLE and I-JOBTITLE to
label tokens that begin and continue a JOBTITLE field,
and the system labels observation sequence ?o2, . . . , o5?
as a JOBTITLE field, then C = ?s2 = B-JOBTITLE,
s3 = . . . = s5 = I-JOBTITLE, s6 6= I-JOBTITLE?.
The calculations of the forward values can be made to
conform to C by the recursion ??q(si) =
{
P
s?
h
??q?1(s?) exp
?
P
k ?kfk(s?, si, o, t)
?i
if si ' sq
0 otherwise
for all sq ? C, where the operator si ' sq means si
conforms to constraint sq . For time steps not constrained
by C, Eq. 3 is used instead.
If ??t+1(si) is the constrained forward value, then
Z ?o =
?
i ??T (si) is the value of the constrained lat-
tice, the set of all paths that conform to C. Our confi-
dence estimate is obtained by normalizing Z ?o using Zo,
i.e. Z ?o ? Zo.
We also implement an alternative method that uses the
state probability distributions for each state in the ex-
tracted field. Let ?t(si) = p(si|o1, . . . , oT ) be the prob-
ability of being in state i at time t given the observation
sequence . We define the confidence measure GAMMA
to be
?v
i=u ?i(si), where u and v are the start and end
indices of the extracted field.
4 Record Confidence Estimation
We can similarly use CFB to estimate the probability that
an entire record is labeled correctly. The procedure is
the same as in the previous section, except that C now
specifies the labels for all fields in the record.
We also implement three alternative record confidence
estimates. FIELDPRODUCT calculates the confidence of
each field in the record using CFB, then multiplies these
values together to obtain the record confidence. FIELD-
MIN instead uses the minimum field confidence as the
record confidence. VITERBIRATIO uses the ratio of the
probabilities of the top two Viterbi paths, capturing how
much more likely s? is than its closest alternative.
5 Reranking with Maximum Entropy
We also trained two conditional maximum entropy clas-
sifiers to classify fields and records as being labeled cor-
rectly or incorrectly. The resulting posterior probabil-
ity of the ?correct? label is used as the confidence mea-
sure. The approach is inspired by results from (Collins,
2000), which show discriminative classifiers can improve
the ranking of parses produced by a generative parser.
After initial experimentation, the most informative in-
puts for the field confidence classifier were field length,
the predicted label of the field, whether or not this field
has been extracted elsewhere in this record, and the CFB
confidence estimate for this field. For the record confi-
dence classifier, we incorporated the following features:
record length, whether or not two fields were tagged with
the same label, and the CFB confidence estimate.
6 Experiments
2187 contact records (27,560 words) were collected from
Web pages and email and 25 classes of data fields were
hand-labeled.1 The features for the CRF consist of the
token text, capitalization features, 24 regular expressions
over the token text (e.g. CONTAINSHYPHEN), and off-
sets of these features within a window of size 5. We also
use 19 lexicons, including ?US Last Names,? ?US First
Names,? and ?State Names.? Feature induction is not
used in these experiments. The CRF is trained on 60% of
the data, and the remaining 40% is split evenly into de-
velopment and testing sets. The development set is used
to train the maximum entropy classifiers, and the testing
set is used to measure the accuracy of the confidence es-
timates. The CRF achieves an overall token accuracy of
87.32 on the testing data, with a field-level performance
of F1 = 84.11, precision = 85.43, and recall = 82.83.
To evaluate confidence estimation, we use three meth-
ods. The first is Pearson?s r, a correlation coefficient
ranging from -1 to 1 that measures the correlation be-
tween a confidence score and whether or not the field
(or record) is correctly labeled. The second is average
precision, used in the Information Retrieval community
1The 25 fields are: FirstName, MiddleName, LastName,
NickName, Suffix, Title, JobTitle, CompanyName, Depart-
ment, AddressLine, City1, City2, State, Country, PostalCode,
HomePhone, Fax, CompanyPhone, DirectCompanyPhone, Mo-
bile, Pager, VoiceMail, URL, Email, InstantMessage
Pearson?s r Avg. Prec
CFB .573 .976
MaxEnt .571 .976
Gamma .418 .912
Random .012 .858
WorstCase ? .672
Table 1: Evaluation of confidence estimates for field confi-
dence. CFB and MAXENT outperform competing methods.
Pearson?s r Avg. Prec
CFB .626 .863
MaxEnt .630 .867
FieldProduct .608 .858
FieldMin .588 .843
ViterbiRatio .313 .842
Random .043 .526
WorstCase ? .304
Table 2: Evaluation of confidence estimates for record confi-
dence. CFB, MAXENT again perform best.
to evaluate ranked lists. It calculates the precision at
each point in the ranked list where a relevant document
is found and then averages these values. Instead of rank-
ing documents by their relevance score, here we rank
fields (and records) by their confidence score, where a
correctly labeled field is analogous to a relevant docu-
ment. WORSTCASE is the average precision obtained
by ranking all incorrect instances above all correct in-
stances. Tables 1 and 2 show that CFB and MAXENT are
statistically similar, and that both outperform competing
methods. Note that WORSTCASE achieves a high aver-
age precision simply because so many fields are correctly
labeled. In all experiments, RANDOM assigns confidence
values chosen uniformly at random between 0 and 1.
The third measure is an accuracy-coverage graph. Bet-
ter confidence estimates push the curve to the upper-right.
Figure 1 shows that CFB and MAXENT dramatically out-
perform GAMMA. Although omitted for space, similar
results are also achieved on a noun-phrase chunking task
(CFB r = .516, GAMMA r = .432) and a named-entity
extraction task (CFB r = .508, GAMMA r = .480).
7 Related Work
While there has been previous work using probabilistic
estimates for token confidence, and heuristic estimates
for field confidence, to the best of our knowledge this pa-
per is the first to use a sound, probabilistic estimate for
confidence of multi-word fields and records in informa-
tion extraction.
Much of the work in confidence estimation
for IE has been in the active learning literature.
Scheffer et al (2001) derive confidence estimates using
hidden Markov models in an information extraction
system. However, they do not estimate the confidence
of entire fields, only singleton tokens. They estimate
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 1
 0  0.2  0.4  0.6  0.8  1
a
cc
u
ra
cy
coverage
"Optimal"
"CFB"
"MaxEnt"
"Gamma"
"Random"
Figure 1: The precision-recall curve for fields shows that CFB
and MAXENT outperform GAMMA.
the confidence of a token by the difference between
the probabilities of its first and second most likely
labels, whereas CFB considers the full distribution of
all suboptimal paths. Scheffer et al (2001) also explore
an idea similar to CFB to perform Baum-Welch training
with partially labeled data, where the provided labels
are constraints. However, these constraints are again for
singleton tokens only.
Rule-based extraction methods (Thompson et al,
1999) estimate confidence based on a rule?s coverage in
the training data. Other areas where confidence estima-
tion is used include document classification (Bennett et
al., 2002), where classifiers are built using meta-features
of the document; speech recognition (Gunawardana et al,
1998), where the confidence of a recognized word is esti-
mated by considering a list of commonly confused words;
and machine translation (Gandrabur and Foster, 2003),
where neural networks are used to learn the probability of
a correct word translation using text features and knowl-
edge of alternate translations.
8 Conclusion
We have shown that CFB is a mathematically and empir-
ically sound confidence estimator for finite state informa-
tion extraction systems, providing strong correlation with
correctness and obtaining an average precision of 97.6%
for estimating field correctness. Unlike methods margin
maximization methods such as SVMs and M3Ns (Taskar
et al, 2003), CRFs are trained to maximize conditional
probability and are thus more naturally appropriate for
confidence estimation. Interestingly, reranking by MAX-
ENT does not seem to improve performance, despite the
benefit Collins (2000) has shown discriminative rerank-
ing to provide generative parsers. We hypothesize this is
because CRFs are already discriminative (not joint, gen-
erative) models; furthermore, this may suggest that future
discriminative parsing methods will also have the benefits
of discriminative reranking built-in directly.
Acknowledgments
We thank the reviewers for helpful suggestions and refer-
ences. This work was supported in part by the Center for
Intelligent Information Retrieval, by the Advanced Research
and Development Activity under contract number MDA904-
01-C-0984, by The Central Intelligence Agency, the Na-
tional Security Agency and National Science Foundation un-
der NSF grant #IIS-0326249, and by the Defense Advanced
Research Projects Agency, through the Department of the Inte-
rior, NBC, Acquisition Services Division, under contract num-
ber NBCHD030010.
References
Paul N. Bennett, Susan T. Dumais, and Eric Horvitz. 2002.
Probabilistic combination of text classifiers using reliability
indicators: models and results. In Proceedings of the 25th
annual international ACM SIGIR conference on Research
and development in information retrieval, pages 207?214.
ACM Press.
Michael Collins. 2000. Discriminative reranking for natu-
ral language parsing. In Proc. 17th International Conf. on
Machine Learning, pages 175?182. Morgan Kaufmann, San
Francisco, CA.
Simona Gandrabur and George Foster. 2003. Confidence esti-
mation for text prediction. In Proceedings of the Conference
on Natural Language Learning (CoNLL 2003), Edmonton,
Canada.
A. Gunawardana, H. Hon, and L. Jiang. 1998. Word-based
acoustic confidence measures for large-vocabulary speech
recognition. In Proc. ICSLP-98, pages 791?794, Sydney,
Australia.
Trausti Kristjannson, Aron Culotta, Paul Viola, and Andrew
McCallum. 2004. Interactive information extraction with
conditional random fields. To appear in Nineteenth National
Conference on Artificial Intelligence (AAAI 2004).
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th Interna-
tional Conf. on Machine Learning, pages 282?289. Morgan
Kaufmann, San Francisco, CA.
Andrew McCallum and David Jensen. 2003. A note on the
unification of information extraction and data mining using
conditional-probability, relational models. In IJCAI03 Work-
shop on Learning Statistical Models from Relational Data.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001.
Active hidden markov models for information extraction.
In Advances in Intelligent Data Analysis, 4th International
Conference, IDA 2001.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-
margin markov networks. In Proceedings of Neural Infor-
mation Processing Systems Conference.
Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.
Mooney. 1999. Active learning for natural language pars-
ing and information extraction. In Proc. 16th International
Conf. on Machine Learning, pages 406?414. Morgan Kauf-
mann, San Francisco, CA.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 296?303,
New York, June 2006. c?2006 Association for Computational Linguistics
Integrating Probabilistic Extraction Models and Data Mining
to Discover Relations and Patterns in Text
Aron Culotta
University of Massachusetts
Amherst, MA 01003
culotta@cs.umass.edu
Andrew McCallum
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Jonathan Betz
Google, Inc.
New York, NY 10018
jtb@google.com
Abstract
In order for relation extraction systems
to obtain human-level performance, they
must be able to incorporate relational pat-
terns inherent in the data (for example,
that one?s sister is likely one?s mother?s
daughter, or that children are likely to
attend the same college as their par-
ents). Hand-coding such knowledge can
be time-consuming and inadequate. Addi-
tionally, there may exist many interesting,
unknown relational patterns that both im-
prove extraction performance and provide
insight into text. We describe a probabilis-
tic extraction model that provides mutual
benefits to both ?top-down? relational pat-
tern discovery and ?bottom-up? relation
extraction.
1 Introduction
Consider these four sentences:
1. George W. Bush?s father is George H. W. Bush.
2. George H.W. Bush?s sister is Nancy Bush Ellis.
3. Nancy Bush Ellis?s son is John Prescott Ellis.
4. John Prescott Ellis analyzed George W. Bush?s
campaign.
We would like to build an automated system to
extract the set of relations shown in Figure 1.
cousin
Nancy Ellis Bush
sibling
George HW Bush
George W Bush
son
John Prescott Ellis
son
Figure 1: Bush family tree
State of the art extraction algorithms may be able
to detect the son and sibling relations from local lan-
guage clues. However, the cousin relation is only
implied by the text and requires additional knowl-
edge to be extracted. Specifically, the system re-
quires knowledge of familial relation patterns.
One could imagine a system that accepts such
rules as input (e.g. cousin = father?s sister?s son)
and applies them to extract implicit relations. How-
ever, exhaustively enumerating all possible rules can
be tedious and incomplete. More importantly, many
relational patterns unknown a priori may both im-
prove extraction accuracy and uncover informative
trends in the data (e.g. that children often adopt the
religion of their parents). Indeed, the goal of data
mining is to learn such patterns from database reg-
ularities. Since these patterns will not always hold,
we would like to handle them probabilistically.
We propose an integrated supervised machine
learning method that learns both contextual and re-
lational patterns to extract relations. In particular,
we construct a linear-chain conditional random field
(Lafferty et al, 2001; Sutton and McCallum, 2006)
to extract relations from biographical texts while si-
multaneously discovering interesting relational pat-
terns that improve extraction performance.
296
2 Related Work
This work can be viewed as a step toward the in-
tegration of information extraction and data mining
technology, a direction of growing interest. Nahm
and Mooney (2000) present a system that mines as-
sociation rules from a database constructed from au-
tomatically extracted data, then applies these learned
rules to improve data field recall without revisiting
the text. Our work attempts to more tightly inte-
grate the extraction and mining tasks by learning
relational patterns that can be included probabilis-
tically into extraction to improve its accuracy; also,
our work focuses on mining from relational graphs,
rather than single-table databases.
McCallum and Jensen (2003) argue the theoreti-
cal benefits of an integrated probabilistic model for
extraction and mining, but do not construct such a
system. Our work is a step in the direction of their
proposal, using an inference procedure based on a
closed-loop iteration between extraction and rela-
tional pattern discovery.
Most other work in this area mines raw text, rather
than a database automatically populated via extrac-
tion (Hearst, 1999; Craven et al, 1998).
This work can also be viewed as part of a trend
to perform joint inference across multiple language
processing tasks (Miller et al, 2000; Roth and tau
Yih, 2002; Sutton and McCallum, 2004).
Finally, using relational paths between entities is
also examined in (Richards and Mooney, 1992) to
escape local maxima in a first-order learning system.
3 Relation Extraction as Sequence
Labeling
Relation extraction is the task of discovering seman-
tic connections between entities. In text, this usu-
ally amounts to examining pairs of entities in a doc-
ument and determining (from local language cues)
whether a relation exists between them. Common
approaches to this problem include pattern match-
ing (Brin, 1998; Agichtein and Gravano, 2000),
kernel methods (Zelenko et al, 2003; Culotta and
Sorensen, 2004; Bunescu and Mooney, 2006), lo-
gistic regression (Kambhatla, 2004), and augmented
parsing (Miller et al, 2000).
The pairwise classification approach of kernel
methods and logistic regression is commonly a two-
phase method: first the entities in a document are
identified, then a relation type is predicted for each
pair of entities. This approach presents at least
two difficulties: (1) enumerating all pairs of enti-
ties, even when restricted to pairs within a sentence,
results in a low density of positive relation exam-
ples; and (2) errors in the entity recognition phase
can propagate to errors in the relation classification
stage. As an example of the latter difficulty, if a per-
son is mislabeled as a company, then the relation
classifier will be unsuccessful in finding a brother
relation, despite local evidence.
We avoid these difficulties by restricting our in-
vestigation to biographical texts, e.g. encyclopedia
articles. A biographical text mostly discusses one
entity, which we refer to as the principal entity. We
refer to other mentioned entities as secondary enti-
ties. For each secondary entity, our goal is to predict
what relation, if any, it has to the principal entity.
This formulation allows us to treat relation ex-
traction as a sequence labeling task such as named-
entity recognition or part-of-speech tagging, and we
can now apply models that have been successful on
those tasks. By anchoring one argument of relations
to be the principal entity, we alleviate the difficulty
of enumerating all pairs of entities in a document.
By converting to a sequence labeling task, we fold
the entity recognition step into the relation extrac-
tion task. There is no initial pass to label each entity
as a person or company. Instead, an entity?s label is
its relation to the principal entity. Below is an exam-
ple of a labeled article:
George W. Bush
George is the son of George H. W. Bush
? ?? ?
father
and Barbara Bush
? ?? ?
mother
.
Additionally, by using a sequence model we can
capture the dependence between adjacent labels. For
example, in our data it is common to see phrases
such as ?son of the Republican president George H.
W. Bush? for which the labels politicalParty, jobTi-
tle, and father occur consecutively. Sequence mod-
els are specifically designed to handle these kinds
of dependencies. We now discuss the details of our
extraction model.
297
3.1 Conditional Random Fields
We build a model to extract relations using linear-
chain conditional random fields (CRFs) (Lafferty
et al, 2001; Sutton and McCallum, 2006). CRFs
are undirected graphical models (i.e. Markov net-
works) that are discriminatively-trained to maximize
the conditional probability of a set of output vari-
ables y given a set of input variables x. This condi-
tional distribution has the form
p?(y|x) =
1
Zx
?
c?C
?c(yc,xc; ?) (1)
where ? are potential functions parameterized by ?
and Zx =
?
y
?
c?C ?(yc,xc) is a normalization
factor. Assuming ?c factorizes as a log-linear com-
bination of arbitrary features computed over clique
c, then ?c(yc,xc; ?) = exp (
?
k ?kfk(yc,xc)),
where f is a set of arbitrary feature functions over
the input, each of which has an associate model
parameter ?k. Parameters ? = {?k} are a set
of real-valued weights typically estimated from la-
beled training data by maximizing the data likeli-
hood function using gradient ascent.
In these experiments, we make a first-order
Markov assumption on the dependencies among y,
resulting in a linear-chain CRF.
4 Relational Patterns
The modeling flexibility of CRFs permits the fea-
ture functions to be complex, overlapping features of
the input without requiring additional assumptions
on their inter-dependencies. In addition to common
language features (e.g. neighboring words and syn-
tactic information), in this work we explore features
that cull relational patterns from a database of enti-
ties.
As described in the introductory example (Figure
1), context alone is often insufficient to extract re-
lations. Even in simpler examples, it may be the
case that modeling relational patterns can improve
extraction accuracy.
To capture this evidence, we compute features
from a database to indicate relational connections
between entities, similar to the relational path-
finding performed in Richards and Mooney (1992).
Imagine that the four sentence example about the
Bush family is included in a training set, and the en-
cousin
father son
X Y
sibling
Figure 2: A feature template for the cousin relation.
tities are labeled with their correct relations. In this
case, the cousin relation in sentence 4 would also be
labeled. From this data, we can create a relational
database that contains the relations in Figure 1.
Assume sentence 4 comes from a biography about
John Ellis. We calculate a feature for the entity
George W. Bush that indicates the path from John
Ellis to George W. Bush in the database, annotat-
ing each edge in the path with its relation label; i.e.
father-sibling-son. By abstracting away the actual
entity names, we have created a cousin template fea-
ture, as shown in Figure 2.
By adding these relational paths as features to
the model, we can learn interesting relational pat-
terns that may have low precision (e.g. ?people are
likely to be friends with their classmates?) without
hampering extraction performance. This is in con-
trast to the system described in Nahm and Mooney
(2000), in which patterns are induced from a noisy
database and then applied directly to extraction. In
our system, since each learned path has an associ-
ated weight, it is simply another piece of evidence
to help the extractor. Low precision patterns may
have lower weights than high precision patterns, but
they will still influence the extractor.
A nice property of this approach is that examin-
ing highly weighted patterns can provide insight into
regularities of the data.
4.1 Feature Induction
During CRF training, weights are learned for each
relational pattern. Patterns that increase extraction
performance will receive higher weights, while pat-
terns that have little effect on performance will re-
ceive low weights.
We can explore the space of possible conjunctions
of these patterns using feature induction for CRFs,
as described in McCallum (2003). Search through
the large space of possible conjunctions is guided
298
by adding features that are estimated to increase the
likelihood function most.
When feature induction is used with relational
patterns, we can view this as a type of data mining,
in which patterns are created based on their influ-
ence on an extraction model. This is similar to work
by Dehaspe (1997), where inductive logic program-
ming is embedded as a feature induction technique
for a maximum entropy classifier. Our work restricts
induced features to conjunctions of base features,
rather than using first-order clauses. However, the
patterns we learn are based on information extracted
from natural language.
4.2 Iterative Database Construction
The top-down knowledge provided by data min-
ing algorithms has the potential to improve the per-
formance of information extraction systems. Con-
versely, bottom-up knowledge generated by ex-
traction systems can be used to populate a large
database, from which more top-down knowledge
can be discovered. By carefully communicating the
uncertainty between these systems, we hope to iter-
atively expand a knowledge base, while minimizing
fallacious inferences.
In this work, the top-down knowledge consists of
relational patterns describing the database path be-
tween entities in text. The uncertainty of this knowl-
edge is handled by associating a real-valued CRF
weight with each pattern, which increases when the
pattern is predictive of other relations. Thus, the ex-
traction model can adapt to noise in these patterns.
Since we also desire to extract relations between
entities that appear in text but not in the database, we
first populate the database with relations extracted
by a CRF that does not use relational patterns. We
then do further extraction with a CRF that incorpo-
rates the relational patterns found in this automati-
cally generated database. In this manner, we create a
closed-loop system that alternates between bottom-
up extraction and top-down pattern discovery. This
approach can be viewed as a type of alternating opti-
mization, with analogies to formal methods such as
expectation-maximization.
The uncertainty in the bottom-up extraction step
is handled by estimating the confidence of each ex-
traction and pruning the database to remove en-
tries with low confidence. One of the benefits of
a probabilistic extraction model is that confidence
estimates can be straight-forwardly obtained. Cu-
lotta and McCallum (2004) describe the constrained
forward-backward algorithm to efficiently estimate
the conditional probability that a segment of text is
correctly extracted by a CRF.
Using this algorithm, we associate a confidence
value with each relation extracted by the CRF. This
confidence value is then used to limit the noise
introduced by incorrect extractions. This differs
from Nahm and Mooney (2000) and Mooney and
Bunescu (2005), in which standard decision tree rule
learners are applied to the unfiltered output of ex-
traction.
4.3 Extracting Implicit Relations
An implicit relation is one that does not have direct
contextual evidence, for example the cousin relation
in our initial example. Implicit relations generally
require some background knowledge to be detected,
such as relational patterns (e.g. rules about familial
relations). These are the sorts of relations on which
current extraction models perform most poorly.
Notably, these are exactly the sorts of relations
that are likely to have the biggest impact on informa-
tion access. A system that can accurately discover
knowledge that is only implied by the text will dra-
matically increase the amount of information a user
can uncover, effectively providing access to the im-
plications of a corpus.
We argue that integrating top-down and bottom-
up knowledge discovery algorithms discussed in
Section 4.2 can enable this technology. By per-
forming pattern discovery in conjunction with infor-
mation extraction, we can collate facts from multi-
ple sources to infer new relations. This is an ex-
ample of cross-document fusion or cross-document
information extraction, a growing area of research
transforming raw extractions into usable knowledge
bases (Mann and Yarowsky, 2005; Masterson and
Kushmerik, 2003).
5 Experiments
5.1 Data
We sampled 1127 paragraphs from 271 articles from
the online encyclopediaWikipedia1 and labeled a to-
1http://www.wikipedia.org
299
George W. Bush
Dick Cheney
underling
Yale
education
Republican
partyPresident
jobTitle
George H. W. Bush
son
underlingHarken Energy
executive
education party
jobTitle
Prescott Bush
son
education
Bill Clinton
rival
Bob Dole
rival
education
Democrat
party
jobTitle
Hillary Clinton
husband
education
party
Halliburton
executiveeducation
Pres Medal of Freedom
awardparty
Nelson Rockefeller
award
Elizabeth Dole
wife
WWII
participant
awardparty
party
Martin Luther King, Jr.
award
Figure 3: An example of the connectivity of the entities in the data.
birthday birth year death day
death year nationality visited
birth place death place religion
job title member of cousin
friend discovered education
employer associate opus
participant influence award
brother wife supported idea
executive of political party supported person
founder son father
rival underling superior
role inventor husband
grandfather sister brother-in-law
nephew mother daughter
granddaughter grandson great-grandson
grandmother rival organization owner of
uncle descendant ancestor
great-grandfather aunt
Table 1: The set of labeled relations.
tal of 4701 relation instances. In addition to a large
set of person-to-person relations, we also included
links between people and organizations, as well as
biographical facts such as birthday and jobTitle. In
all, there are 53 labels in the training data (Table 1).
We sample articles that result in a high density
of interesting relations by choosing, for example, a
collection of related family members and associates.
Figure 3 shows a small example of the type of con-
nections in the data. We then split the data into train-
ing and testing sets (70-30 split), attempting to sep-
arate the entities into connected components. For
example, all Bush family members were placed in
the training set, while all Kennedy family members
were placed in the testing set. While there are still
occasional paths connecting entities in the training
set to those in the test set, we believe this method-
ology reflects a typical real-world scenario in which
we would like to extend an existing database to a
different, but slightly related, domain.
The structure of the Wikipedia articles somewhat
simplifies the extraction task, since important enti-
ties are hyper-linked within the text. This provides
an automated way to detect entities in the text, al-
though these entities are not classified by type. This
also allows us to easily construct database queries,
since we can reason at the entity level, rather than
the token level. (Although, see Sarawagi and Cohen
(2004) for extensions of CRFs that model the en-
tity length distribution.) The results we report here
are constrained to predict relations only for hyper-
linked entities. Note that despite this property, we
still desire to use a sequence model to capture the
dependencies between adjacent labels.
We use the MALLET CRF implementation (Mc-
Callum, 2002) with the default regularization pa-
rameters.
Based on initial experiments, we restrict relational
path features to length two or three. Paths of length
one will learn trivial paths and can lead to over-
fitting. Paths longer than three can increase compu-
tational costs without adding much new information.
In addition to the relational pattern features de-
scribed in Section 4, the list of local features in-
cludes context words (such as the token identity
within a 6 word window of the target token), lexi-
cons (such as whether a token appears in a list of
cities, people, or companies), regular expressions
(such as whether the token is capitalized or contains
digits or punctuation), part-of-speech (predicted by
a CRF that was trained separately for part of speech
tagging), prefix/suffix (such as whether a word ends
in -ed or begins with ch-), and offset conjunctions
(combinations of adjacent features within a window
of size six).
300
ME CRF0 CRFr CRFr0.9 CRFr0.5 CRFt CRFt0.5
F1 .5489 .5995 .6100 .6008 .6136 .6791 .6363
P .6475 .7019 .6799 .7177 .7095 .7553 .7343
R .4763 .5232 .5531 .5166 .5406 .6169 .5614
Table 2: Results comparing the relative benefits of using relational patterns in extraction.
5.2 Extraction Results
We evaluate performance by calculating the preci-
sion (P) and recall (R) of extracted relations, as well
as the F1 measure, which is the harmonic mean of
precision and recall.
CRF0 is the conditional random field constructed
without relational features. Results for CRF0 are
displayed in the second column of Table 2. ME is
a maximum entropy classifier trained on the same
feature set as CRF0. The difference between these
two models is that CRF0 models the dependence of
relations that appear consecutively in the text. The
superior performance of CRF0 suggests that this de-
pendence is important to capture.
The remaining models incorporate the relational
patterns described in Section 4. We compare three
different confidence thresholds for the construction
of the initial testing database, as described in Sec-
tion 4.2. CRFr uses no threshold, while CRFr0.9
andCRFr0.5 restrict the database to extractions with
confidence greater than 0.9 and 0.5, respectively.
As shown by comparing CRF0 and CRFr in Ta-
ble 2, the relational features constructed from the
database with no confidence threshold provides a
considerable boost in recall (reducing error by 7%),
at the cost of a decrease in precision. Here we see
the effect of making fallacious inferences on a noisy
database.
In column four, we see the opposite effect for
the overly conservative threshold of CRFr0.9. Here,
precision improves slightly over CRF0, and consid-
erably over CRFr (12% error reduction), but this is
accompanied by a drop in recall (8% reduction).
Finally, in column five, a confidence of 0.5 results
in the best F1 measure (a 3.5% error reduction over
CRF0). CRFr0.5 also obtains better recall and preci-
sion than CRF0, reducing recall error by 3.6%, pre-
cision error by 2.5%.
Comparing the performance on different relation
types, we find that the biggest increase from CRF0
to CRFr0.5 is on the memberOf relation, for which
the F1 score improves from 0.4211 to 0.6093. We
conjecture that the reason for this is that the patterns
most useful for thememberOf label contain relations
that are well-detected by the first-pass CRF. Also,
the local language context seems inadequate to prop-
erly extract this relation, given the low performance
of CRF0.
To better gauge how much relational pattern fea-
tures are affected by errors in the database, we run
two additional experiments for which the relational
features are fixed to be correct. That is, imagine that
we construct a database from the true labeling of the
testing data, and create the relational pattern features
from this database. Note that this does not trivialize
the problem, since there are no relational path fea-
tures of length one (e.g., if X is the wife of Y, there
will be no feature indicating this).
We construct two experiments under this scheme,
one where the entire test database is used (CRFt),
and another where only half the relations are in-
cluded in the test database, selected uniformly at
random (CRFt0.5).
Column six shows the improvements enabled by
using the complete testing database. More inter-
estingly, column seven shows that even with only
half the database accurately known, performance
improves considerably over both CRF and CRFr0.5.
A realistic scenario for CRFt0.5 is a semi-automated
system, in which a partially-filled database is used to
bootstrap extraction.
5.3 Mining Results
Comparing the impact of discovered patterns on ex-
traction is a way to objectively measure mining per-
formance. We now give a brief subjective evaluation
of the learned patterns. By examining relational pat-
terns with high weights for a particular label, we can
glean some regularities from our dataset. Examples
of such patterns are in Table 3.
301
Relation Relational Path Feature
mother father ? wife
cousin mother ? husband ? nephew
friend education ? student
education father ? education
boss boss ? son
memberOf grandfather ? memberOf
rival politicalParty ? member ? rival
Table 3: Examples of highly weighted relational pat-
terns.
From the familial relations in our training data, we
are able to discover many equivalences for mothers,
cousins, grandfathers, and husbands. In addition to
these high precision patterns, the system also gener-
ates interesting, low precision patterns. Row 3-7 of
Table 3 can be summarized by the following gener-
alizations: friends tend to be classmates; children of
alumni often attend the same school as their parents;
a boss? child often becomes the boss; grandchildren
are often members of the same organizations as their
grandparents; and rivals of a person from one polit-
ical party are often rivals of other members of the
same political party. While many of these patterns
reflect the high concentration of political entities and
familial relations in our training database, many will
have applicability across domains.
5.4 Implicit Relations
It is difficult to measure system performance on im-
plicit relations, since our labeled data does not dis-
tinguish between explicit and implicit relations. Ad-
ditionally, accurately labeling all implicit relations
is challenging even for a human annotator.
We perform a simple exploratory analysis to de-
termine how relational patterns can help discover
implicit relations. We construct a small set of syn-
thetic sentences for which CRF0 successfully ex-
tracts relations using contextual features. We then
add sentences with slightly more ambiguous lan-
guage and measure whether CRFr can overcome this
ambiguity using relational pattern features.
For example, we create an article about an en-
tity named ?Bob Smith? that includes the sentences
?His brother, Bill Smith, was a biologist? and ?His
companion, Bill Smith, was a biologist.? CRF0 suc-
cessfully returns the brother relation in the first sen-
tence, but not the second. After a fact is added to
the database that says Bob and Bill have a brother in
common named John, CRFr is able to correctly label
the second sentence in spite of the ambiguous word
?companion,? because CRF0 has a highly-weighted
relational pattern feature for brother.
Similar behavior is observed for low precision
patterns like ?associates tend to win the same
awards.? A synthetic article for the entity ?Tom
Jones? contains the sentences ?He was awarded the
Pulitzer Prize in 1998? and ?Tom got the Pulitzer
Prize in 1998.? Because CRF0 is highly-reliant on
the presence of the verb ?awarded? or ?won? to indi-
cate a prize fact, it fails to label the second sentence
correctly. After the database is augmented to include
the fact that Tom?s associate Jill received the Pulitzer
Prize, CRFr labels the second sentence correctly.
However, we also observed that CRFr still re-
quires some contextual clues to extract implicit re-
lations. For example, if the Tom Jones article in-
stead contains the sentence ?The Pulitzer Prize was
awarded to him in 1998,? neither CRF labels the
prize fact correctly, since this passive construction
is rarely seen in the training data.
We conclude from this brief analysis that rela-
tional patterns used by CRFr can help extract im-
plicit relations when (1) the database contains ac-
curate relational information, and (2) the sentence
contains limited contextual clues. Since relational
patterns are treated only as additional features by
CRFr, they are generally not powerful enough to
overcome a complete absence of contextual clues.
From this perspective, relational patterns can be seen
as enhancing the signal from contextual clues. This
differs from deterministically applying learned rules
independent of context, which may boost recall at
the cost of precision.
6 Conclusions and Future Work
We have shown that integrating pattern discovery
with relation extraction can lead to improved per-
formance on each task.
In the future, we wish to explore extending this
methods to larger datasets, where we expect rela-
tional patterns to be even more interesting. Also,
we plan to improve upon iterative database construc-
tion by performing joint inference among distant
302
relations in an article. Inference in these highly-
connected models will likely require approximate
methods. Additionally, we wish to focus on extract-
ing implicit relations, dealing more formally with
the precision-recall trade-off inherent in applying
noisy rules to improve extraction.
7 Acknowledgments
Thanks to the Google internship program, and to Charles Sutton
for providing the CRF POS tagger. This work was supported in
part by the Center for Intelligent Information Retrieval, in part
by U.S. Government contract #NBCH040171 through a sub-
contract with BBNT Solutions LLC, in part by The Central In-
telligence Agency, the National Security Agency and National
Science Foundation under NSF grant #IIS-0326249, and in part
by the Defense Advanced Research Projects Agency (DARPA),
through the Department of the Interior, NBC, Acquisition Ser-
vices Division, under contract number NBCHD030010. Any
opinions, findings and conclusions or recommendations ex-
pressed in this material are the author(s) and do not necessarily
reflect those of the sponsor.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball: Extract-
ing relations from large plain-text collections. In Proceed-
ings of the Fifth ACM International Conference on Digital
Libraries.
Sergey Brin. 1998. Extracting patterns and relations from the
world wide web. In WebDB Workshop at 6th International
Conference on Extending Database Technology.
Razvan Bunescu and Raymond Mooney. 2006. Subsequence
kernels for relation extraction. In Y. Weiss, B. Scho?lkopf,
and J. Platt, editors, Advances in Neural Information Pro-
cessing Systems 18. MIT Press, Cambridge, MA.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew K. Mc-
Callum, Tom M. Mitchell, Kamal Nigam, and Sea?n Slattery.
1998. Learning to extract symbolic knowledge from the
World Wide Web. In Proceedings of AAAI-98, 15th Confer-
ence of the American Association for Artificial Intelligence,
pages 509?516, Madison, US. AAAI Press, Menlo Park, US.
Aron Culotta and Andrew McCallum. 2004. Confidence es-
timation for information extraction. In Human Langauge
Technology Conference (HLT 2004), Boston, MA.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In ACL.
L. Dehaspe. 1997. Maximum entropy modeling with clausal
constraints. In Proceedings of the Seventh International
Workshop on Inductive Logic Programming, pages 109?125,
Prague, Czech Republic.
M. Hearst. 1999. Untangling text data mining. In 37th Annual
Meeting of the Association for Computational Linguistics.
Nanda Kambhatla. 2004. Combining lexical, syntactic, and se-
mantic features with maximum entropy models for extract-
ing relations. In ACL.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th Interna-
tional Conf. on Machine Learning, pages 282?289. Morgan
Kaufmann, San Francisco, CA.
Gideon Mann and David Yarowsky. 2005. Multi-field informa-
tion extraction and cross-document fusion. In ACL.
D. Masterson and N. Kushmerik. 2003. Information extraction
from multi-document threads. In ECML-2003: Workshop on
Adaptive Text Extraction and Mining, pages 34?41.
Andrew McCallum and David Jensen. 2003. A note on the
unification of information extraction and data mining us-
ing conditional-probability, relational models. In IJCAI03
Workshop on Learning Statistical Models from Relational
Data.
Andrew McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Andrew McCallum. 2003. Efficiently inducing features of con-
ditional random fields. In Nineteenth Conference on Uncer-
tainty in Artificial Intelligence (UAI03).
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing to ex-
tract information from text. In ANLP.
Raymond J. Mooney and Razvan Bunescu. 2005. Mining
knowledge from text using information extraction. SigKDD
Explorations on Text Mining and Natural Language Process-
ing.
Un Yong Nahm and Raymond J. Mooney. 2000. A mutually
beneficial integration of data mining and information extrac-
tion. In AAAI/IAAI.
Bradley L. Richards and Raymond J. Mooney. 1992. Learning
relations by pathfinding. In Proceedings of the Tenth Na-
tional Conference on Artificial Intelligence (AAAI-92), pages
50?55, San Jose, CA.
Dan Roth and Wen tau Yih. 2002. Probabilistic reasoning for
entity and relation recognition. In COLING.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov
conditional random fields for information extraction. In
NIPS 04.
Charles Sutton and Andrew McCallum. 2004. Dynamic condi-
tional random fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proceedings of the
Twenty-First International Conference on Machine Learning
(ICML).
Charles Sutton and Andrew McCallum. 2006. An introduction
to conditional random fields for relational learning. In Lise
Getoor and Ben Taskar, editors, Introduction to Statistical
Relational Learning. MIT Press. To appear.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella.
2003. Kernel methods for relation extraction. Journal of
Machine Learning Research, 3:1083?1106.
303
Proceedings of NAACL HLT 2007, pages 81?88,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
First-Order Probabilistic Models for Coreference Resolution
Aron Culotta and Michael Wick and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{culotta,mwick,mccallum}@cs.umass.edu
Abstract
Traditional noun phrase coreference res-
olution systems represent features only
of pairs of noun phrases. In this paper,
we propose a machine learning method
that enables features over sets of noun
phrases, resulting in a first-order proba-
bilistic model for coreference. We out-
line a set of approximations that make this
approach practical, and apply our method
to the ACE coreference dataset, achiev-
ing a 45% error reduction over a com-
parable method that only considers fea-
tures of pairs of noun phrases. This result
demonstrates an example of how a first-
order logic representation can be incorpo-
rated into a probabilistic model and scaled
efficiently.
1 Introduction
Noun phrase coreference resolution is the problem
of clustering noun phrases into anaphoric sets. A
standard machine learning approach is to perform a
set of independent binary classifications of the form
?Is mention a coreferent with mention b??
This approach of decomposing the problem into
pairwise decisions presents at least two related diffi-
culties. First, it is not clear how best to convert the
set of pairwise classifications into a disjoint cluster-
ing of noun phrases. The problem stems from the
transitivity constraints of coreference: If a and b are
coreferent, and b and c are coreferent, then a and c
must be coreferent.
This problem has recently been addressed by a
number of researchers. A simple approach is to per-
form the transitive closure of the pairwise decisions.
However, as shown in recent work (McCallum and
Wellner, 2003; Singla and Domingos, 2005), bet-
ter performance can be obtained by performing rela-
tional inference to directly consider the dependence
among a set of predictions. For example, McCal-
lum and Wellner (2005) apply a graph partitioning
algorithm on a weighted, undirected graph in which
vertices are noun phrases and edges are weighted by
the pairwise score between noun phrases.
A second and less studied difficulty is that the
pairwise decomposition restricts the feature set to
evidence about pairs of noun phrases only. This re-
striction can be detrimental if there exist features of
sets of noun phrases that cannot be captured by a
combination of pairwise features. As a simple exam-
ple, consider prohibiting coreferent sets that consist
only of pronouns. That is, we would like to require
that there be at least one antecedent for a set of pro-
nouns. The pairwise decomposition does not make
it possible to capture this constraint.
In general, we would like to construct arbitrary
features over a cluster of noun phrases using the
full expressivity of first-order logic. Enabling this
sort of flexible representation within a statistical
model has been the subject of a long line of research
on first-order probabilistic models (Gaifman, 1964;
Halpern, 1990; Paskin, 2002; Poole, 2003; Richard-
son and Domingos, 2006).
Conceptually, a first-order probabilistic model
can be described quite compactly. A configura-
tion of the world is represented by a set of predi-
81
He
President Bush 
Laura Bush
She
0.2
0.9
0.7
0.4
0.001
0.6
Figure 1: An example noun coreference graph in
which vertices are noun phrases and edge weights
are proportional to the probability that the two nouns
are coreferent. Partitioning such a graph into disjoint
clusters corresponds to performing coreference res-
olution on the noun phrases.
cates, each of which has an associated real-valued
parameter. The likelihood of each configuration of
the world is proportional to a combination of these
weighted predicates. In practice, however, enu-
merating all possible configurations, or even all the
predicates of one configuration, can result in in-
tractable combinatorial growth (de Salvo Braz et al,
2005; Culotta and McCallum, 2006).
In this paper, we present a practical method to per-
form training and inference in first-order models of
coreference. We empirically validate our approach
on the ACE coreference dataset, showing that the
first-order features can lead to an 45% error reduc-
tion.
2 Pairwise Model
In this section we briefly review the standard pair-
wise coreference model. Given a pair of noun
phrases xij = {xi, xj}, let the binary random vari-
able yij be 1 if xi and xj are coreferent. Let F =
{fk(xij , y)} be a set of features over xij . For exam-
ple, fk(xij , y) may indicate whether xi and xj have
the same gender or number. Each feature fk has an
associated real-valued parameter ?k. The pairwise
model is
p(yij |xij) =
1
Zxij
exp
?
k
?kfk(xij , yij)
where Zxij is a normalizer that sums over the two
settings of yij .
This is a maximum-entropy classifier (i.e. logis-
tic regression) in which p(yij |xij) is the probability
that xi and xj are coreferent. To estimate ? = {?k}
from labeled training data, we perform gradient as-
cent to maximize the log-likelihood of the labeled
data.
Two critical decisions for this method are (1) how
to sample the training data, and (2) how to combine
the pairwise predictions at test time. Systems of-
ten perform better when these decisions complement
each other.
Given a data set in which noun phrases have been
manually clustered, the training data can be cre-
ated by simply enumerating over each pair of noun
phrases xij , where yij is true if xi and xj are in
the same cluster. However, this approach generates
a highly unbalanced training set, with negative ex-
amples outnumbering positive examples. Instead,
Soon et al (2001) propose the following sampling
method: Scan the document from left to right. Com-
pare each noun phrase xi to each preceding noun
phrase xj , scanning from right to left. For each pair
xi, xj , create a training instance ?xij , yij?, where yij
is 1 if xi and xj are coreferent. The scan for xj ter-
minates when a positive example is constructed, or
the beginning of the document is reached. This re-
sults in a training set that has been pruned of distant
noun phrase pairs.
At testing time, we can construct an undirected,
weighted graph in which vertices correspond to
noun phrases and edge weights are proportional to
p(yij |xij). The problem is then to partition the graph
into clusters with high intra-cluster edge weights and
low inter-cluster edge weights. An example of such
a graph is shown in Figure 1.
Any partitioning method is applicable here; how-
ever, perhaps most common for coreference is to
perform greedy clustering guided by the word or-
der of the document to complement the sampling
method described above (Soon et al, 2001). More
precisely, scan the document from left-to-right, as-
signing each noun phrase xi to the same cluster
as the closest preceding noun phrase xj for which
p(yij |xij) > ?, where ? is some classification
threshold (typically 0.5). Note that this method con-
trasts with standard greedy agglomerative cluster-
ing, in which each noun phrase would be assigned
to the most probable cluster according to p(yij |xij).
82
Choosing the closest preceding phrase is common
because nearby phrases are a priori more likely to
be coreferent.
We refer to the training and inference methods de-
scribed in this section as the Pairwise Model.
3 First-Order Logic Model
We propose augmenting the Pairwise Model to
enable classification decisions over sets of noun
phrases.
Given a set of noun phrases xj = {xi}, let the bi-
nary random variable yj be 1 if all the noun phrases
xi ? xj are coreferent. The features fk and weights
?k are defined as before, but now the features can
represent arbitrary attributes over the entire set xj .
This allows us to use the full flexibility of first-order
logic to construct features about sets of nouns. The
First-Order Logic Model is
p(yj |xj) =
1
Zxj
exp
?
k
?kfk(x
j , yj)
where Zxj is a normalizer that sums over the two
settings of yj .
Note that this model gives us the representational
power of recently proposed Markov logic networks
(Richardson and Domingos, 2006); that is, we can
construct arbitrary formulae in first-order logic to
characterize the noun coreference task, and can learn
weights for instantiations of these formulae. How-
ever, naively grounding the corresponding Markov
logic network results in a combinatorial explosion of
variables. Below we outline methods to scale train-
ing and prediction with this representation.
As in the Pairwise Model, we must decide how to
sample training examples and how to combine inde-
pendent classifications at testing time. It is impor-
tant to note that by moving to the First-Order Logic
Model, the number of possible predictions has in-
creased exponentially. In the Pairwise Model, the
number of possible y variables is O(|x|2), where
x is the set of noun phrases. In the First-Order
Logic Model, the number of possible y variables is
O(2|x|): There is a y variable for each possible el-
ement of the powerset of x. Of course, we do not
enumerate this set; rather, we incrementally instan-
tiate y variables as needed during prediction.
A simple method to generate training examples
is to sample positive and negative cluster examples
uniformly at random from the training data. Positive
examples are generated by first sampling a true clus-
ter, then sampling a subset of that cluster. Negative
examples are generated by sampling two positive ex-
amples and merging them into the same cluster.
At testing time, we perform standard greedy ag-
glomerative clustering, where the score for each
merger is proportional to the probability of the
newly formed clustering according to the model.
Clustering terminates when there exists no addi-
tional merge that improves the probability of the
clustering.
We refer to the system described in this section as
First-Order Uniform.
4 Error-driven and Rank-based training
of the First-Order Model
In this section we propose two enhancements to
the training procedure for the First-Order Uniform
model.
First, because each training example consists of
a subset of noun phrases, the number of possible
training examples we can generate is exponential in
the number of noun phrases. We propose an error-
driven sampling method that generates training ex-
amples from errors the model makes on the training
data. The algorithm is as follows: Given initial pa-
rameters ?, perform greedy agglomerative cluster-
ing on training document i until an incorrect cluster
is formed. Update the parameter vector according to
this mistake, then repeat for the next training docu-
ment. This process is repeated for a fixed number of
iterations.
Exactly how to update the parameter vector is ad-
dressed by the second enhancement. We propose
modifying the optimization criterion of training to
perform ranking rather than classification of clus-
ters. Consider a training example cluster with a neg-
ative label, indicating that not all of the noun phrases
it contains are coreferent. A classification training
algorithm will ?penalize? all the features associated
with this cluster, since they correspond to a negative
example. However, because there may exists subsets
of the cluster that are coreferent, features represent-
ing these positive subsets may be unjustly penalized.
To address this problem, we propose constructing
training examples consisting of one negative exam-
83
fc
y
12
x
2
x
1
y
23
x
3
y
13
f
c
f
c
f
t
Figure 2: An example noun coreference factor graph
for the Pairwise Model in which factors fc model the
coreference between two nouns, and ft enforce the
transitivity among related decisions. The number of
y variables increases quadratically in the number of
x variables.
ple and one ?nearby? positive example. In particular,
when agglomerative clustering incorrectly merges
two clusters, we select the resulting cluster as the
negative example, and select as the positive example
a cluster that can be created by merging other exist-
ing clusters.1 We then update the weight vector so
that the positive example is assigned a higher score
than the negative example. This approach allows
the update to only penalize the difference between
the two features of examples, thereby not penaliz-
ing features representing any overlapping coreferent
clusters.
To implement this update, we use MIRA (Mar-
gin Infused Relaxed Algorithm), a relaxed, online
maximum margin training algorithm (Crammer and
Singer, 2003). It updates the parameter vector with
two constraints: (1) the positive example must have
a higher score by a given margin, and (2) the change
to ? should be minimal. This second constraint is
to reduce fluctuations in ?. Let s+(?,xj) be the
unnormalized score for the positive example and
s?(?,xk) be the unnormalized score of the neg-
ative example. Each update solves the following
1Of the possible positive examples, we choose the one with
the highest probability under the current model to guard against
large fluctuations in parameter updates
f
c
y
12
x
2
x
1
y
23
x
3
y
13
f
c
f
c
f
t
y
123
f
c
Figure 3: An example noun coreference factor graph
for the First-Order Model in which factors fc model
the coreference between sets of nouns, and ft en-
force the transitivity among related decisions. Here,
the additional node y123 indicates whether nouns
{x1, x2, x3} are all coreferent. The number of y
variables increases exponentially in the number of
x variables.
quadratic program:
?t+1 = argmin
?
||?t ? ?||2
s.t.
s+(?,xj) ? s?(?,xk) ? 1
In this case, MIRA with a single constraint can be
efficiently solved in one iteration of the Hildreth and
D?Esopo method (Censor and Zenios, 1997). Ad-
ditionally, we average the parameters calculated at
each iteration to improve convergence.
We refer to the system described in this section as
First-Order MIRA.
5 Probabilistic Interpretation
In this section, we describe the Pairwise and First-
Order models in terms of the factor graphs they ap-
proximate.
For the Pairwise Model, a corresponding undi-
rected graphical model can be defined as
P (y|x) =
1
Zx
?
yij?y
fc(yij , xij)
?
yij ,yjk?y
ft(yij , yj,k, yik, xij , xjk, xik)
84
where Zx is the input-dependent normalizer and fac-
tor fc parameterizes the pairwise noun phrase com-
patibility as fc(yij , xij) = exp(
?
k ?kfk(yij , xij)).
Factor ft enforces the transitivity constraints by
ft(?) = ?? if transitivity is not satisfied, 1 oth-
erwise. This is similar to the model presented in
McCallum and Wellner (2005). A factor graph for
the Pairwise Model is presented in Figure 2 for three
noun phrases.
For the First-Order model, an undirected graphi-
cal model can be defined as
P (y|x) =
1
Zx
?
yj?y
fc(yj ,xj)
?
yj?y
ft(yj ,xj)
where Zx is the input-dependent nor-
malizer and factor fc parameterizes the
cluster-wise noun phrase compatibility as
fc(yj ,xj) = exp(
?
k ?kfk(yj , x
j)). Again,
factor ft enforces the transitivity constraints by
ft(?) = ?? if transitivity is not satisfied, 1 other-
wise. Here, transitivity is a bit more complicated,
since it also requires that if yj = 1, then for any
subset xk ? xj , yk = 1. A factor graph for the
First-Order Model is presented in Figure 3 for three
noun phrases.
The methods described in Sections 2, 3 and 4 can
be viewed as estimating the parameters of each fac-
tor fc independently. This approach can therefore
be viewed as a type of piecewise approximation of
exact parameter estimation in these models (Sutton
and McCallum, 2005). Here, each fc is a ?piece?
of the model trained independently. These pieces
are combined at prediction time using clustering al-
gorithms to enforce transitivity. Sutton and McCal-
lum (2005) show that such a piecewise approxima-
tion can be theoretically justified as minimizing an
upper bound of the exact loss function.
6 Experiments
6.1 Data
We apply our approach to the noun coreference ACE
2004 data, containing 443 news documents with
28,135 noun phrases to be coreferenced. 336 doc-
uments are used for training, and the remainder for
testing. All entity types are candidates for corefer-
ence (pronouns, named entities, and nominal enti-
ties). We use the true entity segmentation, and parse
each sentence in the corpus using a phrase-structure
grammar, as is common for this task.
6.2 Features
We follow Soon et al (2001) and Ng and Cardie
(2002) to generate most of our features for the Pair-
wise Model. These include:
? Match features - Check whether gender, num-
ber, head text, or entire phrase matches
? Mention type (pronoun, name, nominal)
? Aliases - Heuristically decide if one noun is the
acronym of the other
? Apposition - Heuristically decide if one noun is
in apposition to the other
? Relative Pronoun - Heuristically decide if one
noun is a relative pronoun referring to the other.
? Wordnet features - Use Wordnet to decide if
one noun is a hypernym, synonym, or antonym
of another, or if they share a hypernym.
? Both speak - True if both contain an adjacent
context word that is a synonym of ?said.? This
is a domain-specific feature that helps for many
newswire articles.
? Modifiers Match - for example, in the phrase
?President Clinton?, ?President? is a modifier
of ?Clinton?. This feature indicates if one noun
is a modifier of the other, or they share a modi-
fier.
? Substring - True if one noun is a substring of
the other (e.g. ?Egypt? and ?Egyptian?).
The First-OrderModel includes the following fea-
tures:
? Enumerate each pair of noun phrases and com-
pute the features listed above. All-X is true if
all pairs share a featureX ,Most-True-X is true
if the majority of pairs share a feature X , and
Most-False-X is true if most of the pairs do not
share feature X .
85
? Use the output of the Pairwise Model for each
pair of nouns. All-True is true if all pairs are
predicted to be coreferent, Most-True is true if
most pairs are predicted to be coreferent, and
Most-False is true if most pairs are predicted
to not be coreferent. Additionally, Max-True
is true if the maximum pairwise score is above
threshold, and Min-True if the minimum pair-
wise score is above threshold.
? Cluster Size indicates the size of the cluster.
? Count how many phrases in the cluster are
of each mention type (name, pronoun, nom-
inal), number (singular/plural) and gender
(male/female). The features All-X and Most-
True-X indicate how frequent each feature is
in the cluster. This feature can capture the soft
constraint such that no cluster consists only of
pronouns.
In addition to the listed features, we also include
conjunctions of size 2, for example ?Genders match
AND numbers match?.
6.3 Evaluation
We use the B3 algorithm to evaluate the predicted
coreferent clusters (Amit and Baldwin, 1998). B3
is common in coreference evaluation and is similar
to the precision and recall of coreferent links, ex-
cept that systems are rewarded for singleton clus-
ters. For each noun phrase xi, let ci be the number
of mentions in xi?s predicted cluster that are in fact
coreferent with xi (including xi itself). Precision for
xi is defined as ci divided by the number of noun
phrases in xi?s cluster. Recall for xi is defined as
the ci divided by the number of mentions in the gold
standard cluster for xi. F1 is the harmonic mean of
recall and precision.
6.4 Results
In addition to Pairwise, First-Order Uniform, and
First-Order MIRA, we also compare against Pair-
wise MIRA, which differs from First-Order MIRA
only by the fact that it is restricted to pairwise fea-
tures.
Table 1 suggests both that first-order features and
error-driven training can greatly improve perfor-
mance. The First-OrderModel outperforms the Pair-
F1 Prec Rec
First-Order MIRA 79.3 86.7 73.2
Pairwise MIRA 72.5 92.0 59.8
First-Order Uniform 69.2 79.0 61.5
Pairwise 62.4 62.5 62.3
Table 1: B3 results for ACE noun phrase corefer-
ence. FIRST-ORDER MIRA is our proposed model
that takes advantage of first-order features of the
data and is trained with error-driven and rank-based
methods. We see that both the first-order features
and the training enhancements improve performance
consistently.
wise Model in F1 measure for both standard train-
ing and error-driven training. We attribute some of
this improvement to the capability of the First-Order
model to capture features of entire clusters that may
indicate some phrases are not coreferent. Also, we
attribute the gains from error-driven training to the
fact that training examples are generated based on
errors made on the training data. (However, we
should note that there are also small differences in
the feature sets used for error-driven and standard
training results.)
Error analysis indicates that often noun xi is cor-
rectly not merged with a cluster xj when xj has a
strong internal coherence. For example, if all 5 men-
tions of France in a document are string identical,
then the system will be extremely cautious of merg-
ing a noun that is not equivalent to France into xj ,
since this will turn off the ?All-String-Match? fea-
ture for cluster xj .
To our knowledge, the best results on this dataset
were obtained by the meta-classification scheme of
Ng (2005). Although our train-test splits may differ
slightly, the best B-Cubed F1 score reported in Ng
(2005) is 69.3%, which is considerably lower than
the 79.3% obtained with our method. Also note that
the Pairwise baseline obtains results similar to those
in Ng and Cardie (2002).
7 Related Work
There has been a recent interest in training methods
that enable the use of first-order features (Paskin,
2002; Daume? III and Marcu, 2005b; Richardson
and Domingos, 2006). Perhaps the most related is
86
?learning as search optimization? (LASO) (Daume?
III and Marcu, 2005b; Daume? III and Marcu,
2005a). Like the current paper, LASO is also an
error-driven training method that integrates predic-
tion and training. However, whereas we explic-
itly use a ranking-based loss function, LASO uses
a binary classification loss function that labels each
candidate structure as correct or incorrect. Thus,
each LASO training example contains all candidate
predictions, whereas our training examples contain
only the highest scoring incorrect prediction and the
highest scoring correct prediction. Our experiments
show the advantages of this ranking-based loss func-
tion. Additionally, we provide an empirical study to
quantify the effects of different example generation
and loss function decisions.
Collins and Roark (2004) present an incremental
perceptron algorithm for parsing that uses ?early up-
date? to update the parameters when an error is en-
countered. Our method uses a similar ?early update?
in that training examples are only generated for the
first mistake made during prediction. However, they
do not investigate rank-based loss functions.
Others have attempted to train global scoring
functions using Gibbs sampling (Finkel et al, 2005),
message propagation, (Bunescu and Mooney, 2004;
Sutton and McCallum, 2004), and integer linear pro-
gramming (Roth and Yih, 2004). The main distinc-
tions of our approach are that it is simple to imple-
ment, not computationally intensive, and adaptable
to arbitrary loss functions.
There have been a number of machine learning
approaches to coreference resolution, traditionally
factored into classification decisions over pairs of
nouns (Soon et al, 2001; Ng and Cardie, 2002).
Nicolae and Nicolae (2006) combine pairwise clas-
sification with graph-cut algorithms. Luo et al
(2004) do enable features between mention-cluster
pairs, but do not perform the error-driven and rank-
ing enhancements proposed in our work. Denis and
Baldridge (2007) use a ranking loss function for pro-
noun coreference; however the examples are still
pairs of pronouns, and the example generation is not
error driven. Ng (2005) learns a meta-classifier to
choose the best prediction from the output of sev-
eral coreference systems. While in theory a meta-
classifier can flexibly represent features, they do not
explore features using the full flexibility of first-
order logic. Also, their method is neither error-
driven nor rank-based.
McCallum and Wellner (2003) use a conditional
random field that factors into a product of pairwise
decisions about pairs of nouns. These pairwise de-
cisions are made collectively using relational infer-
ence; however, as pointed out in Milch et al (2004),
this model has limited representational power since
it does not capture features of entities, only of pairs
of mention. Milch et al (2005) address these issues
by constructing a generative probabilistic model,
where noun clusters are sampled from a generative
process. Our current work has similar representa-
tional flexibility as Milch et al (2005) but is discrim-
inatively trained.
8 Conclusions and Future Work
We have presented learning and inference proce-
dures for coreference models using first-order fea-
tures. By relying on sampling methods at training
time and approximate inference methods at testing
time, this approach can be made scalable. This re-
sults in a coreference model that can capture features
over sets of noun phrases, rather than simply pairs of
noun phrases.
This is an example of a model with extremely
flexible representational power, but for which exact
inference is intractable. The simple approximations
we have described here have enabled this more flex-
ible model to outperform a model that is simplified
for tractability.
A short-term extension would be to consider fea-
tures over entire clusterings, such as the number of
clusters. This could be incorporated in a ranking
scheme, as in Ng (2005).
Future work will extend our approach to a wider
variety of tasks. The model we have described here
is specific to clustering tasks; however a similar for-
mulation could be used to approach a number of lan-
guage processing tasks, such as parsing and relation
extraction. These tasks could benefit from first-order
features, and the present work can guide the approx-
imations required in those domains.
Additionally, we are investigating more sophis-
ticated inference algorithms that will reduce the
greediness of the search procedures described here.
87
Acknowledgments
We thank Robert Hall for helpful contributions. This work
was supported in part by the Defense Advanced Research
Projects Agency (DARPA), through the Department of the
Interior, NBC, Acquisition Services Division, under con-
tract #NBCHD030010, in part by U.S. Government contract
#NBCH040171 through a subcontract with BBNT Solutions
LLC, in part by The Central Intelligence Agency, the National
Security Agency and National Science Foundation under NSF
grant #IIS-0326249, in part by Microsoft Live Labs, and in part
by the Defense Advanced Research Projects Agency (DARPA)
under contract #HR0011-06-C-0023. Any opinions, findings
and conclusions or recommendations expressed in this mate-
rial are the author(s)? and do not necessarily reflect those of the
sponsor.
References
B. Amit and B. Baldwin. 1998. Algorithms for scoring coref-
erence chains. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC7).
Razvan Bunescu and Raymond J. Mooney. 2004. Collective
information extraction with relational markov networks. In
ACL.
Y. Censor and S.A. Zenios. 1997. Parallel optimization : the-
ory, algorithms, and applications. Oxford University Press.
Michael Collins and Brian Roark. 2004. Incremental parsing
with the perceptron algorithm. In ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. JMLR, 3:951?
991.
Aron Culotta and Andrew McCallum. 2006. Tractable learn-
ing and inference with high-order representations. In ICML
Workshop on Open Problems in Statistical Relational Learn-
ing, Pittsburgh, PA.
Hal Daume? III and Daniel Marcu. 2005a. A large-scale explo-
ration of effective global features for a joint entity detection
and tracking model. In HLT/EMNLP, Vancouver, Canada.
Hal Daume? III and Daniel Marcu. 2005b. Learning as search
optimization: Approximate large margin methods for struc-
tured prediction. In ICML, Bonn, Germany.
Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth. 2005. Lifted
first-order probabilistic inference. In IJCAI, pages 1319?
1325.
Pascal Denis and Jason Baldridge. 2007. A ranking approach
to pronoun resolution. In IJCAI.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into information
extraction systems by gibbs sampling. In ACL, pages 363?
370.
H. Gaifman. 1964. Concerning measures in first order calculi.
Israel J. Math, 2:1?18.
J. Y. Halpern. 1990. An analysis of first-order logics of proba-
bility. Artificial Intelligence, 46:311?350.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kamb-
hatla, and Salim Roukos. 2004. A mention-synchronous
coreference resolution algorithm based on the Bell tree. In
ACL, page 135.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In IJCAI Workshop on Information Integration
on the Web.
Andrew McCallum and Ben Wellner. 2005. Conditional mod-
els of identity uncertainty with application to noun corefer-
ence. In Lawrence K. Saul, Yair Weiss, and Le?on Bottou,
editors, NIPS17. MIT Press, Cambridge, MA.
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2004.
BLOG: Relational modeling with unknown objects. In
ICML 2004 Workshop on Statistical Relational Learning and
Its Connections to Other Fields.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag,
Daniel L. Ong, and Andrey Kolobov. 2005. BLOG: Proba-
bilistic models with unknown objects. In IJCAI.
Vincent Ng and Claire Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In ACL.
Vincent Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In ACL.
Cristina Nicolae and Gabriel Nicolae. 2006. Bestcut: A graph
algorithm for coreference resolution. In EMNLP, pages
275?283, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Mark A. Paskin. 2002. Maximum entropy probabilistic logic.
Technical Report UCB/CSD-01-1161, University of Califor-
nia, Berkeley.
D. Poole. 2003. First-order probabilistic inference. In IJCAI,
pages 985?991, Acapulco, Mexico. Morgan Kaufman.
Matthew Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62:107?136.
D. Roth and W. Yih. 2004. A linear programming formulation
for global inference in natural language tasks. In The 8th
Conference on Compuational Natural Language Learning,
May.
Parag Singla and Pedro Domingos. 2005. Discriminative train-
ing of markov logic networks. In AAAI, Pittsburgh, PA.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim.
2001. A machine learning approach to coreference resolu-
tion of noun phrases. Comput. Linguist., 27(4):521?544.
Charles Sutton and Andrew McCallum. 2004. Collective seg-
mentation and labeling of distant entities in information ex-
traction. Technical Report TR # 04-49, University of Mas-
sachusetts, July.
Charles Sutton and Andrew McCallum. 2005. Piecewise train-
ing of undirected models. In 21st Conference on Uncertainty
in Artificial Intelligence.
88
Dependency Tree Kernels for Relation Extraction
Aron Culotta
University of Massachusetts
Amherst, MA 01002
USA
culotta@cs.umass.edu
Jeffrey Sorensen
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
USA
sorenj@us.ibm.com
Abstract
We extend previous work on tree kernels to estimate
the similarity between the dependency trees of sen-
tences. Using this kernel within a Support Vector
Machine, we detect and classify relations between
entities in the Automatic Content Extraction (ACE)
corpus of news articles. We examine the utility of
different features such as Wordnet hypernyms, parts
of speech, and entity types, and find that the depen-
dency tree kernel achieves a 20% F1 improvement
over a ?bag-of-words? kernel.
1 Introduction
The ability to detect complex patterns in data is lim-
ited by the complexity of the data?s representation.
In the case of text, a more structured data source
(e.g. a relational database) allows richer queries
than does an unstructured data source (e.g. a col-
lection of news articles). For example, current web
search engines would not perform well on the query,
?list all California-based CEOs who have social ties
with a United States Senator.? Only a structured
representation of the data can effectively provide
such a list.
The goal of Information Extraction (IE) is to dis-
cover relevant segments of information in a data
stream that will be useful for structuring the data.
In the case of text, this usually amounts to finding
mentions of interesting entities and the relations that
join them, transforming a large corpus of unstruc-
tured text into a relational database with entries such
as those in Table 1.
IE is commonly viewed as a three stage process:
first, an entity tagger detects all mentions of interest;
second, coreference resolution resolves disparate
mentions of the same entity; third, a relation extrac-
tor finds relations between these entities. Entity tag-
ging has been thoroughly addressed by many statis-
tical machine learning techniques, obtaining greater
than 90% F1 on many datasets (Tjong Kim Sang
and De Meulder, 2003). Coreference resolution is
an active area of research not investigated here (Pa-
Entity Type Location
Apple Organization Cupertino, CA
Microsoft Organization Redmond, WA
Table 1: An example of extracted fields
sula et al, 2002; McCallum and Wellner, 2003).
We describe a relation extraction technique based
on kernel methods. Kernel methods are non-
parametric density estimation techniques that com-
pute a kernel function between data instances,
where a kernel function can be thought of as a sim-
ilarity measure. Given a set of labeled instances,
kernel methods determine the label of a novel in-
stance by comparing it to the labeled training in-
stances using this kernel function. Nearest neighbor
classification and support-vector machines (SVMs)
are two popular examples of kernel methods (Fuku-
naga, 1990; Cortes and Vapnik, 1995).
An advantage of kernel methods is that they can
search a feature space much larger than could be
represented by a feature extraction-based approach.
This is possible because the kernel function can ex-
plore an implicit feature space when calculating the
similarity between two instances, as described in the
Section 3.
Working in such a large feature space can lead to
over-fitting in many machine learning algorithms.
To address this problem, we apply SVMs to the task
of relation extraction. SVMs find a boundary be-
tween instances of different classes such that the
distance between the boundary and the nearest in-
stances is maximized. This characteristic, in addi-
tion to empirical validation, indicates that SVMs are
particularly robust to over-fitting.
Here we are interested in detecting and classify-
ing instances of relations, where a relation is some
meaningful connection between two entities (Table
2). We represent each relation instance as an aug-
mented dependency tree. A dependency tree repre-
sents the grammatical dependencies in a sentence;
we augment this tree with features for each node
AT NEAR PART ROLE SOCIAL
Based-In Relative-location Part-of Affiliate, Founder Associate, Grandparent
Located Subsidiary Citizen-of, Management Parent, Sibling
Residence Other Client, Member Spouse, Other-professional
Owner, Other, Staff Other-relative, Other-personal
Table 2: Relation types and subtypes.
(e.g. part of speech) We choose this representation
because we hypothesize that instances containing
similar relations will share similar substructures in
their dependency trees. The task of the kernel func-
tion is to find these similarities.
We define a tree kernel over dependency trees and
incorporate this kernel within an SVM to extract
relations from newswire documents. The tree ker-
nel approach consistently outperforms the bag-of-
words kernel, suggesting that this highly-structured
representation of sentences is more informative for
detecting and distinguishing relations.
2 Related Work
Kernel methods (Vapnik, 1998; Cristianini and
Shawe-Taylor, 2000) have become increasingly
popular because of their ability to map arbitrary ob-
jects to a Euclidian feature space. Haussler (1999)
describes a framework for calculating kernels over
discrete structures such as strings and trees. String
kernels for text classification are explored in Lodhi
et al (2000), and tree kernel variants are described
in (Zelenko et al, 2003; Collins and Duffy, 2002;
Cumby and Roth, 2003). Our algorithm is similar
to that described by Zelenko et al (2003). Our
contributions are a richer sentence representation, a
more general framework to allow feature weighting,
as well as the use of composite kernels to reduce
kernel sparsity.
Brin (1998) and Agichtein and Gravano (2000)
apply pattern matching and wrapper techniques for
relation extraction, but these approaches do not
scale well to fastly evolving corpora. Miller et al
(2000) propose an integrated statistical parsing tech-
nique that augments parse trees with semantic la-
bels denoting entity and relation types. Whereas
Miller et al (2000) use a generative model to pro-
duce parse information as well as relation informa-
tion, we hypothesize that a technique discrimina-
tively trained to classify relations will achieve bet-
ter performance. Also, Roth and Yih (2002) learn a
Bayesian network to tag entities and their relations
simultaneously. We experiment with a more chal-
lenging set of relation types and a larger corpus.
3 Kernel Methods
In traditional machine learning, we are provided
a set of training instances S = {x1 . . . xN},
where each instance xi is represented by some d-
dimensional feature vector. Much time is spent on
the task of feature engineering ? searching for the
optimal feature set either manually by consulting
domain experts or automatically through feature in-
duction and selection (Scott and Matwin, 1999).
For example, in entity detection the original in-
stance representation is generally a word vector cor-
responding to a sentence. Feature extraction and
induction may result in features such as part-of-
speech, word n-grams, character n-grams, capital-
ization, and conjunctions of these features. In the
case of more structured objects, such as parse trees,
features may include some description of the ob-
ject?s structure, such as ?has an NP-VP subtree.?
Kernel methods can be particularly effective at re-
ducing the feature engineering burden for structured
objects. By calculating the similarity between two
objects, kernel methods can employ dynamic pro-
gramming solutions to efficiently enumerate over
substructures that would be too costly to explicitly
include as features.
Formally, a kernel function K is a mapping
K : X ? X ? [0,?] from instance space X
to a similarity score K(x, y) =
?
i ?i(x)?i(y) =
?(x) ? ?(y). Here, ?i(x) is some feature func-
tion over the instance x. The kernel function must
be symmetric [K(x, y) = K(y, x)] and positive-
semidefinite. By positive-semidefinite, we require
that the if x1, . . . , xn ? X, then the n ? n matrix
G defined by Gij = K(xi, xj) is positive semi-
definite. It has been shown that any function that
takes the dot product of feature vectors is a kernel
function (Haussler, 1999).
A simple kernel function takes the dot product of
the vector representation of instances being com-
pared. For example, in document classification,
each document can be represented by a binary vec-
tor, where each element corresponds to the presence
or absence of a particular word in that document.
Here, ?i(x) = 1 if word i occurs in document x.
Thus, the kernel function K(x, y) returns the num-
ber of words in common between x and y. We refer
to this kernel as the ?bag-of-words? kernel, since it
ignores word order.
When instances are more structured, as in the
case of dependency trees, more complex kernels
become necessary. Haussler (1999) describes con-
volution kernels, which find the similarity between
two structures by summing the similarity of their
substructures. As an example, consider a kernel
over strings. To determine the similarity between
two strings, string kernels (Lodhi et al, 2000) count
the number of common subsequences in the two
strings, and weight these matches by their length.
Thus, ?i(x) is the number of times string x contains
the subsequence referenced by i. These matches can
be found efficiently through a dynamic program,
allowing string kernels to examine long-range fea-
tures that would be computationally infeasible in a
feature-based method.
Given a training set S = {x1 . . . xN}, kernel
methods compute the Gram matrix G such that
Gij = K(xi, xj). Given G, the classifier finds a
hyperplane which separates instances of different
classes. To classify an unseen instance x, the classi-
fier first projects x into the feature space defined by
the kernel function. Classification then consists of
determining on which side of the separating hyper-
plane x lies.
A support vector machine (SVM) is a type of
classifier that formulates the task of finding the sep-
arating hyperplane as the solution to a quadratic pro-
gramming problem (Cristianini and Shawe-Taylor,
2000). Support vector machines attempt to find a
hyperplane that not only separates the classes but
also maximizes the margin between them. The hope
is that this will lead to better generalization perfor-
mance on unseen instances.
4 Augmented Dependency Trees
Our task is to detect and classify relations between
entities in text. We assume that entity tagging has
been performed; so to generate potential relation
instances, we iterate over all pairs of entities oc-
curring in the same sentence. For each entity pair,
we create an augmented dependency tree (described
below) representing this instance. Given a labeled
training set of potential relations, we define a tree
kernel over dependency trees which we then use in
an SVM to classify test instances.
A dependency tree is a representation that de-
notes grammatical relations between words in a sen-
tence (Figure 1). A set of rules maps a parse tree to
a dependency tree. For example, subjects are de-
pendent on their verbs and adjectives are dependent
Troops
Tikrit
advanced
near
t
t
t
t 0
1 2
3
Figure 1: A dependency tree for the sentence
Troops advanced near Tikrit.
Feature Example
word troops, Tikrit
part-of-speech (24 values) NN, NNP
general-pos (5 values) noun, verb, adj
chunk-tag NP, VP, ADJP
entity-type person, geo-political-entity
entity-level name, nominal, pronoun
Wordnet hypernyms social group, city
relation-argument ARG A, ARG B
Table 3: List of features assigned to each node in
the dependency tree.
on the nouns they modify. Note that for the pur-
poses of this paper, we do not consider the link la-
bels (e.g. ?object?, ?subject?); instead we use only
the dependency structure. To generate the parse tree
of each sentence, we use MXPOST, a maximum en-
tropy statistical parser1; we then convert this parse
tree to a dependency tree. Note that the left-to-right
ordering of the sentence is maintained in the depen-
dency tree only among siblings (i.e. the dependency
tree does not specify an order to traverse the tree to
recover the original sentence).
For each pair of entities in a sentence, we find
the smallest common subtree in the dependency tree
that includes both entities. We choose to use this
subtree instead of the entire tree to reduce noise
and emphasize the local characteristics of relations.
We then augment each node of the tree with a fea-
ture vector (Table 3). The relation-argument feature
specifies whether an entity is the first or second ar-
gument in a relation. This is required to learn asym-
metric relations (e.g. X OWNS Y).
Formally, a relation instance is a dependency tree
1http://www.cis.upenn.edu/?adwait/statnlp.html
T with nodes {t0 . . . tn}. The features of node ti
are given by ?(ti) = {v1 . . . vd}. We refer to the
jth child of node ti as ti[j], and we denote the set
of all children of node ti as ti[c]. We reference a
subset j of children of ti by ti[j] ? ti[c]. Finally, we
refer to the parent of node ti as ti.p.
From the example in Figure 1, t0[1] = t2,
t0[{0, 1}] = {t1, t2}, and t1.p = t0.
5 Tree kernels for dependency trees
We now define a kernel function for dependency
trees. The tree kernel is a function K(T1, T2) that
returns a normalized, symmetric similarity score in
the range (0, 1) for two trees T1 and T2. We de-
fine a slightly more general version of the kernel
described by Zelenko et al (2003).
We first define two functions over the features of
tree nodes: a matching function m(ti, tj) ? {0, 1}
and a similarity function s(ti, tj) ? (0,?]. Let the
feature vector ?(ti) = {v1 . . . vd} consist of two
possibly overlapping subsets ?m(ti) ? ?(ti) and
?s(ti) ? ?(ti). We use ?m(ti) in the matching
function and ?s(ti) in the similarity function. We
define
m(ti, tj) =
{
1 if ?m(ti) = ?m(tj)
0 otherwise
and
s(ti, tj) =
?
vq??s(ti)
?
vr??s(tj)
C(vq, vr)
where C(vq, vr) is some compatibility function
between two feature values. For example, in the
simplest case where
C(vq, vr) =
{
1 if vq = vr
0 otherwise
s(ti, tj) returns the number of feature values in
common between feature vectors ?s(ti) and ?s(tj).
We can think of the distinction between functions
m(ti, tj) and s(ti, tj) as a way to discretize the sim-
ilarity between two nodes. If ?m(ti) 6= ?m(tj),
then we declare the two nodes completely dissimi-
lar. However, if ?m(ti) = ?m(tj), then we proceed
to compute the similarity s(ti, tj). Thus, restrict-
ing nodes by m(ti, tj) is a way to prune the search
space of matching subtrees, as shown below.
For two dependency trees T1, T2, with root nodes
r1 and r2, we define the tree kernel K(T1, T2) as
follows:
K(T1, T2) =
?
??
??
0 if m(r1, r2) = 0
s(r1, r2)+
Kc(r1[c], r2[c]) otherwise
where Kc is a kernel function over children. Let
a and b be sequences of indices such that a is a
sequence a1 ? a2 ? . . . ? an, and likewise for b.
Let d(a) = an ? a1 +1 and l(a) be the length of a.
Then we have Kc(ti[c], tj [c]) =
?
a,b,l(a)=l(b)
?d(a)?d(b)K(ti[a], tj [b])
The constant 0 < ? < 1 is a decay factor that
penalizes matching subsequences that are spread
out within the child sequences. See Zelenko et al
(2003) for a proof that K is kernel function.
Intuitively, whenever we find a pair of matching
nodes, we search for all matching subsequences of
the children of each node. A matching subsequence
of children is a sequence of children a and b such
that m(ai, bi) = 1 (?i < n). For each matching
pair of nodes (ai, bi) in a matching subsequence,
we accumulate the result of the similarity function
s(ai, bj) and then recursively search for matching
subsequences of their children ai[c], bj [c].
We implement two types of tree kernels. A
contiguous kernel only matches children subse-
quences that are uninterrupted by non-matching
nodes. Therefore, d(a) = l(a). A sparse tree ker-
nel, by contrast, allows non-matching nodes within
matching subsequences.
Figure 2 shows two relation instances, where
each node contains the original text plus the features
used for the matching function, ?m(ti) = {general-
pos, entity-type, relation-argument}. (?NA? de-
notes the feature is not present for this node.) The
contiguous kernel matches the following substruc-
tures: {t0[0], u0[0]}, {t0[2], u0[1]}, {t3[0], u2[0]}.
Because the sparse kernel allows non-contiguous
matching sequences, it matches an additional sub-
structure {t0[0, ?, 2], u0[0, ?, 1]}, where (?) indi-
cates an arbitrary number of non-matching nodes.
Zelenko et al (2003) have shown the contiguous
kernel to be computable in O(mn) and the sparse
kernel in O(mn3), where m and n are the number
of children in trees T1 and T2 respectively.
6 Experiments
We extract relations from the Automatic Content
Extraction (ACE) corpus provided by the National
Institute for Standards and Technology (NIST). The
personnoun
NANA
verb
ARG_Bgeo?political
1
0
troops
advanced
noun
Tikrit
ARG_A
personnoun
forces
NANA
verbmoved
NANA
preptoward
ARG_B
t
t
t t
t
1
0
2 3
4
geo?politicalnoun
Baghdad
quickly
adverbNANA
ARG_A
nearprepNANA
2
3
u
u
u
u
Figure 2: Two instances of the NEAR relation.
data consists of about 800 annotated text documents
gathered from various newspapers and broadcasts.
Five entities have been annotated (PERSON, ORGA-
NIZATION, GEO-POLITICAL ENTITY, LOCATION,
FACILITY), along with 24 types of relations (Table
2). As noted from the distribution of relationship
types in the training data (Figure 3), data imbalance
and sparsity are potential problems.
In addition to the contiguous and sparse tree
kernels, we also implement a bag-of-words ker-
nel, which treats the tree as a vector of features
over nodes, disregarding any structural informa-
tion. We also create composite kernels by combin-
ing the sparse and contiguous kernels with the bag-
of-words kernel. Joachims et al (2001) have shown
that given two kernels K1, K2, the composite ker-
nel K12(xi, xj) = K1(xi, xj)+K2(xi, xj) is also a
kernel. We find that this composite kernel improves
performance when the Gram matrix G is sparse (i.e.
our instances are far apart in the kernel space).
The features used to represent each node are
shown in Table 3. After initial experimentation,
the set of features we use in the matching func-
tion is ?m(ti) = {general-pos, entity-type, relation-
argument}, and the similarity function examines the
Figure 3: Distribution over relation types in train-
ing data.
remaining features.
In our experiments we tested the following five
kernels:
K0 = sparse kernel
K1 = contiguous kernel
K2 = bag-of-words kernel
K3 = K0 + K2
K4 = K1 + K2
We also experimented with the function C(vq, vr),
the compatibility function between two feature val-
ues. For example, we can increase the importance
of two nodes having the same Wordnet hypernym2.
If vq, vr are hypernym features, then we can define
C(vq, vr) =
{
? if vq = vr
0 otherwise
When ? > 1, we increase the similarity of
nodes that share a hypernym. We tested a num-
ber of weighting schemes, but did not obtain a set
of weights that produced consistent significant im-
provements. See Section 8 for alternate approaches
to setting C.
2http://www.cogsci.princeton.edu/?wn/
Avg. Prec. Avg. Rec. Avg. F1
K1 69.6 25.3 36.8
K2 47.0 10.0 14.2
K3 68.9 24.3 35.5
K4 70.3 26.3 38.0
Table 4: Kernel performance comparison.
Table 4 shows the results of each kernel within
an SVM. (We augment the LibSVM3 implementa-
tion to include our dependency tree kernel.) Note
that, although training was done over all 24 rela-
tion subtypes, we evaluate only over the 5 high-level
relation types. Thus, classifying a RESIDENCE re-
lation as a LOCATED relation is deemed correct4.
Note also that K0 is not included in Table 4 because
of burdensome computational time. Table 4 shows
that precision is adequate, but recall is low. This
is a result of the aforementioned class imbalance ?
very few of the training examples are relations, so
the classifier is less likely to identify a testing in-
stances as a relation. Because we treat every pair
of mentions in a sentence as a possible relation, our
training set contains fewer than 15% positive rela-
tion instances.
To remedy this, we retrain each SVMs for a bi-
nary classification task. Here, we detect, but do not
classify, relations. This allows us to combine all
positive relation instances into one class, which pro-
vides us more training samples to estimate the class
boundary. We then threshold our output to achieve
an optimal operating point. As seen in Table 5, this
method of relation detection outperforms that of the
multi-class classifier.
We then use these binary classifiers in a cascading
scheme as follows: First, we use the binary SVM
to detect possible relations. Then, we use the SVM
trained only on positive relation instances to classify
each predicted relation. These results are shown in
Table 6.
The first result of interest is that the sparse tree
kernel, K0, does not perform as well as the con-
tiguous tree kernel, K1. Suspecting that noise was
introduced by the non-matching nodes allowed in
the sparse tree kernel, we performed the experi-
ment with different values for the decay factor ? =
{.9, .5, .1}, but obtained no improvement.
The second result of interest is that all tree ker-
nels outperform the bag-of-words kernel, K2, most
noticeably in recall performance, implying that the
3http://www.csie.ntu.edu.tw/?cjlin/libsvm/
4This is to compensate for the small amount of training data
for many classes.
Prec. Rec. F1
K0 ? ? ?
K0 (B) 83.4 45.5 58.8
K1 91.4 37.1 52.8
K1 (B) 84.7 49.3 62.3
K2 92.7 10.6 19.0
K2 (B) 72.5 40.2 51.7
K3 91.3 35.1 50.8
K3 (B) 80.1 49.9 61.5
K4 91.8 37.5 53.3
K4 (B) 81.2 51.8 63.2
Table 5: Relation detection performance. (B) de-
notes binary classification.
D C Avg. Prec. Avg. Rec. Avg. F1
K0 K0 66.0 29.0 40.1
K1 K1 66.6 32.4 43.5
K2 K2 62.5 27.7 38.1
K3 K3 67.5 34.3 45.3
K4 K4 67.1 35.0 45.8
K1 K4 67.4 33.9 45.0
K4 K1 65.3 32.5 43.3
Table 6: Results on the cascading classification. D
and C denote the kernel used for relation detection
and classification, respectively.
structural information the tree kernel provides is ex-
tremely useful for relation detection.
Note that the average results reported here are
representative of the performance per relation, ex-
cept for the NEAR relation, which had slightly lower
results overall due to its infrequency in training.
7 Conclusions
We have shown that using a dependency tree ker-
nel for relation extraction provides a vast improve-
ment over a bag-of-words kernel. While the de-
pendency tree kernel appears to perform well at the
task of classifying relations, recall is still relatively
low. Detecting relations is a difficult task for a ker-
nel method because the set of all non-relation in-
stances is extremely heterogeneous, and is therefore
difficult to characterize with a similarity metric. An
improved system might use a different method to
detect candidate relations and then use this kernel
method to classify the relations.
8 Future Work
The most immediate extension is to automatically
learn the feature compatibility function C(vq, vr).
A first approach might use tf-idf to weight each fea-
ture. Another approach might be to calculate the
information gain for each feature and use that as
its weight. A more complex system might learn a
weight for each pair of features; however this seems
computationally infeasible for large numbers of fea-
tures.
One could also perform latent semantic indexing
to collapse feature values into similar ?categories?
? for example, the words ?football? and ?baseball?
might fall into the same category. Here, C(vq, vr)
might return ?1 if vq = vr, and ?2 if vq and vr are
in the same category, where ?1 > ?2 > 0. Any
method that provides a ?soft? match between fea-
ture values will sharpen the granularity of the kernel
and enhance its modeling power.
Further investigation is also needed to understand
why the sparse kernel performs worse than the con-
tiguous kernel. These results contradict those given
in Zelenko et al (2003), where the sparse kernel
achieves 2-3% better F1 performance than the con-
tiguous kernel. It is worthwhile to characterize rela-
tion types that are better captured by the sparse ker-
nel, and to determine when using the sparse kernel
is worth the increased computational burden.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text
collections. In Proceedings of the Fifth ACM In-
ternational Conference on Digital Libraries.
Sergey Brin. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extend-
ing Database Technology, EDBT?98.
M. Collins and N. Duffy. 2002. Convolution ker-
nels for natural language. In T. G. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances
in Neural Information Processing Systems 14,
Cambridge, MA. MIT Press.
Corinna Cortes and Vladimir Vapnik. 1995.
Support-vector networks. Machine Learning,
20(3):273?297.
N. Cristianini and J. Shawe-Taylor. 2000. An intro-
duction to support vector machines. Cambridge
University Press.
Chad M. Cumby and Dan Roth. 2003. On kernel
methods for relational learning. In Tom Fawcett
and Nina Mishra, editors, Machine Learning,
Proceedings of the Twentieth International Con-
ference (ICML 2003), August 21-24, 2003, Wash-
ington, DC, USA. AAAI Press.
K. Fukunaga. 1990. Introduction to Statistical Pat-
tern Recognition. Academic Press, second edi-
tion.
D. Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCS-CRL-99-
10, University of California, Santa Cruz.
Thorsten Joachims, Nello Cristianini, and John
Shawe-Taylor. 2001. Composite kernels for hy-
pertext categorisation. In Carla Brodley and An-
drea Danyluk, editors, Proceedings of ICML-
01, 18th International Conference on Machine
Learning, pages 250?257, Williams College, US.
Morgan Kaufmann Publishers, San Francisco,
US.
Huma Lodhi, John Shawe-Taylor, Nello Cristian-
ini, and Christopher J. C. H. Watkins. 2000. Text
classification using string kernels. In NIPS, pages
563?569.
A. McCallum and B. Wellner. 2003. Toward con-
ditional models of identity uncertainty with ap-
plication to proper noun coreference. In IJCAI
Workshop on Information Integration on the Web.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel.
2000. A novel use of statistical parsing to ex-
tract information from text. In 6th Applied Nat-
ural Language Processing Conference.
H. Pasula, B. Marthi, B. Milch, S. Russell, and
I. Shpitser. 2002. Identity uncertainty and cita-
tion matching.
Dan Roth and Wen-tau Yih. 2002. Probabilistic
reasoning for entity and relation recognition. In
19th International Conference on Computational
Linguistics.
Sam Scott and Stan Matwin. 1999. Feature engi-
neering for text classification. In Proceedings of
ICML-99, 16th International Conference on Ma-
chine Learning.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recog-
nition. In Walter Daelemans and Miles Osborne,
editors, Proceedings of CoNLL-2003, pages 142?
147. Edmonton, Canada.
Vladimir Vapnik. 1998. Statistical Learning The-
ory. Whiley, Chichester, GB.
D. Zelenko, C. Aone, and A. Richardella. 2003.
Kernel methods for relation extraction. Jour-
nal of Machine Learning Research, pages 1083?
1106.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 603?611,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Field Compatibilities
to Extract Database Records from Unstructured Text
Michael Wick, Aron Culotta and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{mwick, culotta, mccallum}@cs.umass.edu
Abstract
Named-entity recognition systems extract
entities such as people, organizations, and
locations from unstructured text. Rather
than extract these mentions in isolation,
this paper presents a record extraction sys-
tem that assembles mentions into records
(i.e. database tuples). We construct a
probabilistic model of the compatibility
between field values, then employ graph
partitioning algorithms to cluster fields
into cohesive records. We also investigate
compatibility functions over sets of fields,
rather than simply pairs of fields, to ex-
amine how higher representational power
can impact performance. We apply our
techniques to the task of extracting contact
records from faculty and student home-
pages, demonstrating a 53% error reduc-
tion over baseline approaches.
1 Introduction
Information extraction (IE) algorithms populate a
database with facts discovered from unstructured
text. This database is often used by higher-level
tasks such as question answering or knowledge
discovery. The richer the structure of the database,
the more useful it is to higher-level tasks.
A common IE task is named-entity recognition
(NER), the problem of locating mentions of en-
tities in text, such as people, places, and organi-
zations. NER techniques range from regular ex-
pressions to finite-state sequence models (Bikel et
al., 1999; Grishman, 1997; Sutton and McCallum,
2006). NER can be viewed as method of populat-
ing a database with single-tuple records, e.g. PER-
SON=Cecil Conner or ORGANIZATION= IBM.
We can add richer structure to these single-tuple
records by extracting the associations among en-
tities. For example, we can populate multi-field
records such as a contact record [PERSON=Steve
Jobs, JOBTITLE = CEO, COMPANY = Apple,
CITY = Cupertino, STATE = CA]. The relational
information in these types of records presents a
greater opportunity for text analysis.
The task of associating together entities is of-
ten framed as a binary relation extraction task:
Given a pair of entities, label the relation be-
tween them (e.g. Steve Jobs LOCATED-IN Cuper-
tino). Common approaches to relation extraction
include pattern matching (Brin, 1998; Agichtein
and Gravano, 2000) and classification (Zelenko et
al., 2003; Kambhatla, 2004).
However, binary relation extraction alone is not
well-suited for the contact record example above,
which requires associating together many fields
into one record. We refer to this task of piecing
together many fields into a single record as record
extraction.
Consider the task of extracting contact records
from personal homepages. An NER system may
label all mentions of cities, people, organizations,
phone numbers, job titles, etc. on a page, from
both semi-structured an unstructured text. Even
with a highly accurate NER system, it is not obvi-
ous which fields belong to the same record. For
example, a single document could contain five
names, three phone numbers and only one email.
Additionally, the layout of certain fields may be
convoluted or vary across documents.
Intuitively, we would like to learn the compat-
ibility among fields, for example the likelihood
that the organization University of North Dakota
is located in the state North Dakota, or that phone
numbers with area code 212 co-occur with the
603
city New York. Additionally, the system should
take into account page layout information, so that
nearby fields are more likely to be grouped into the
same record.
In this paper, we describe a method to induce a
probabilistic compatibility function between sets
of fields. Embedding this compatibility func-
tion within a graph partitioning method, we de-
scribe how to cluster highly compatible fields into
records.
We evaluate our approach on personal home-
pages that have been manually annotated with
contact record information, and demonstrate a
53% error reduction over baseline methods.
2 Related Work
McDonald et al (2005) present clustering tech-
niques to extract complex relations, i.e. relations
with more than two arguments. Record extraction
can be viewed as an instance of complex relation
extraction. We build upon this work in three ways:
(1) Our system learns the compatibility between
sets of fields, rather than just pairs of field; (2) our
system is not restricted to relations between en-
tities in the same sentence; and (3) our problem
domain has a varying number of fields per record,
as opposed to the fixed schema in McDonald et al
(2005).
Bansal et al (2004) present algorithms for the
related task of correlational clustering: finding an
optimal clustering from a matrix of pairwise com-
patibility scores. The correlational clustering ap-
proach does not handle compatibility scores calcu-
lated over sets of nodes, which we address in this
paper.
McCallum and Wellner (2005) discriminatively
train a model to learn binary coreference deci-
sions, then perform joint inference using graph
partitioning. This is analogous to our work, with
two distinctions. First, instead of binary coref-
erence decisions, our model makes binary com-
patibility decisions, reflecting whether a set of
fields belong together in the same record. Second,
whereas McCallum and Wellner (2005) factor the
coreference decisions into pairs of vertices, our
compatibility decisions are made between sets of
vertices. As we show in our experiments, factoring
decisions into sets of vertices enables more power-
ful features that can improve performance. These
higher-order features have also recently been in-
vestigated in other models of coreference, both
discriminative (Culotta and McCallum, 2006) and
generative (Milch et al, 2005).
Viola and Narasimhan (2005) present a prob-
abilistic grammar to parse contact information
blocks. While this model is capable of learn-
ing long-distance compatibilities (such as City and
State relations), features to enable this are not ex-
plored. Additionally, their work focuses on la-
beling fields in documents that have been pre-
segmented into records. This record segmentation
is precisely what we address in this paper.
Borkar et al (2001) and Kristjannson et al
(2004) also label contact address blocks, but ig-
nore the problem of clustering fields into records.
Also, Culotta et al (2004) automatically extract
contact records from web pages, but use heuristics
to cluster fields into records.
Embley et al (1999) provide heuristics to de-
tect record boundaries in highly structured web
documents, such as classified ads, and Embley
and Xu (2000) improve upon these heuristics for
slightly more ambiguous domains using a vector
space model. Both of these techniques apply to
data for which the records are highly contiguous
and have a distinctive separator between records.
These heuristic approaches are unlikely to be suc-
cessful in the unstructured text domain we address
in this paper.
Most other work on relation extraction focuses
only on binary relations (Zelenko et al, 2003;
Miller et al, 2000; Agichtein and Gravano, 2000;
Culotta and Sorensen, 2004). A serious difficulty
in applying binary relation extractors to the record
extraction task is that rather than enumerating over
all pairs of entities, the system must enumerate
over all subsets of entities, up to subsets of size
k, the maximum number of fields per record. We
address this difficulty by employing two sampling
methods: one that samples uniformly, and another
that samples on a focused subset of the combina-
torial space.
3 From Fields to Records
3.1 Problem Definition
Let a fieldF be a pair ?a, v?, where a is an attribute
(column label) and v is a value, e.g. Fi = ?CITY,
San Francisco?. Let record R be a set of fields,
R = {F1 . . . Fn}. Note that R may contain mul-
tiple fields with the same attribute but different
values (e.g. a person may have multiple job ti-
tles). Assume we are given the output of a named-
604
entity recognizer, which labels tokens in a doc-
ument with their attribute type (e.g. NAME or
CITY). Thus, a document initially contains a set
of fields, {F1 . . . Fm}.
The task is to partition the fields in each anno-
tated document into a set of records {R1 . . . Rk}
such that each record Ri contains exactly the set
of fields pertinent to that record. In this paper, we
assume each field belongs to exactly one record.
3.2 Solution Overview
For each document, we construct a fully-
connected weighted graph G = (V,E), with ver-
tices V and weighted edges E. Each field in the
document is represented by a vertex in V , and the
edges are weighted by the compatibility of adja-
cent fields, i.e. a measure of how likely it is that
Fi and Fj belong to the same record.
Partitioning V into k disjoint clusters uniquely
maps the set of fields to a set of k records. Be-
low, we provide more detail on the two principal
steps in our solution: (1) estimating the compati-
bility function and (2) partitioning V into disjoint
clusters.
3.3 Learning field compatibility
Let F be a candidate cluster of fields forming a
partial record. We construct a compatibility func-
tion C that maps two sets of fields to a real value,
i.e. C : Fi ? Fj ? R. We abbreviate the value
C(Fi,Fj) as Cij . The higher the value of Cij the
more likely it is that Fi and Fj belong to the same
record.
For example, in the contact record domain, Cij
can reflect whether a city and state should co-
occur, or how likely a company is to have a certain
job title.
We represent Cij by a maximum-entropy clas-
sifier over the binary variable Sij , which is true if
and only if field set Fi belongs to the same record
as field set Fj . Thus, we model the conditional
distribution
P?(Sij |Fi,Fj) ? exp
(
?
k
?kfk(Sij ,Fi,Fj)
)
where fk is a binary feature function that com-
putes attributes over the field sets, and ? = {?k}
is the set of real-valued weights that are the pa-
rameters of the maximum-entropy model. We set
Cij = P?(Sij =true|Fi,Fj). This approach can
be viewed as a logistic regression model for field
compatibility.
Examples of feature functions include format-
ting evidence (Fi appears at the top of the docu-
ment, Fj at the bottom), conflicting value infor-
mation (Fi and Fj contain conflicting values for
the state field), or other measures of compatibility
(a city value in Fi is known to exist in a state in
Fj). A feature may involve more than one field,
for example, if a name, title and university occurs
consecutively in some order. We give a more de-
tailed description of the feature functions in Sec-
tion 4.3.
We propose learning the ? weights for each of
these features using supervised machine learning.
Given a set of documents D for which the true
mapping from fields to set of records is known,
we wish to estimate P (Sij |Fi,Fj) for all pairs of
field sets Fi,Fj .
Enumerating all positive and negative pairs of
field sets is computationally infeasible for large
datasets, so we instead propose two sampling
methods to generate training examples. The first
simply samples pairs of field sets uniformly from
the training data. For example, given a document
D containing true records {R1 . . . Rk}, we sam-
ple positive and negative examples of field sets of
varying sizes from {Ri . . . Rj}. The second sam-
pling method first trains the model using the exam-
ples generated by uniform sampling. This model
is then used to cluster the training data. Additional
training examples are created during the clustering
process and are used to retrain the model parame-
ters. This second sampling method is an attempt to
more closely align the characteristics of the train-
ing and testing examples.
Given a sample of labeled training data, we set
the parameters of the maximum-entropy classi-
fier in standard maximum-likelihood fashion, per-
forming gradient ascent on the log-likelihood of
the training data. The resulting weights indi-
cate how important each feature is in determin-
ing whether two sets of fields belong to the same
record.
3.4 Partitioning Fields into Records
One could employ the estimated classifier to con-
vert fields into records as follows: Classify each
pair of fields as positive or negative, and perform
transitive closure to enforce transitivity of deci-
sions. That is, if the classifier determines that A
and B belong to the same record and that B and
C belong to the same record, then by transitivity
605
A and C must belong to the same record. The
drawback of this approach is that the compatibility
between A and C is ignored. In cases where the
classifier determines that A and C are highly in-
compatible, transitive closure can lead to poor pre-
cision. McCallum and Wellner (2005) explore this
issue in depth for the related task of noun corefer-
ence resolution.
With this in mind, we choose to avoid transitive
closure, and instead employ a graph partitioning
method to make record merging decisions jointly.
Given a document D with fields {F1 . . . Fn},
we construct a fully connected graph G = (V,E),
with edge weights determined by the learned com-
patibility functionC. We wish to partition vertices
V into clusters with high intra-cluster compatibil-
ity.
One approach is to simply use greedy agglom-
erative clustering: initialize each vertex to its own
cluster, then iteratively merge clusters with the
highest inter-cluster edge weights. The compati-
bility between two clusters can be measured using
single-link or average-link clustering. The clus-
tering algorithm converges when the inter-cluster
edge weight between any pair of clusters is below
a specified threshold.
We propose a modification to this approach.
Since the compatibility function we have de-
scribed maps two sets of vertices to a real value,
we can use this directly to calculate the compati-
bility between two clusters, rather than performing
average or single link clustering.
We now describe the algorithmmore concretely.
? Input: (1) Graph G = (V,E), where each
vertex vi represents a field Fi. (2) A threshold
value ? .
? Initialization: Place each vertex vi in its own
cluster R?i. (The hat notation indicates that
this cluster represents a possible record.)
? Iterate: Re-calculate the compatibility func-
tion Cij between each pair of clusters. Merge
the two most compatible clusters, R??i , R?
?
j .
? Termination: If there does not exist a pair of
clusters R?i, R?j such that Cij > ? , the algo-
rithm terminates and returns the current set of
clusters.
A natural threshold value is ? = 0.5, since this
is the point at which the binary compatibility clas-
sifier predicts that the fields belong to different
records. In Section 4.4, we examine how perfor-
mance varies with ? .
3.5 Representational power of cluster
compatibility functions
Most previous work on inducing compatibility
functions learns the compatibility between pairs of
vertices, not clusters of vertices. In this section,
we provide intuition to explain why directly mod-
eling the compatibility of clusters of vertices may
be advantageous. We refer to the cluster compat-
ibility function as Cij , and the pairwise (binary)
compatibility function as Bij .
First, we note that Cij is a generalization of
single-link and average-link clustering methods
that use Bij , since the output of these methods
can simply be included as features in Cij . For ex-
ample, given two clusters R?i = {v1, v2, v3} and
R?j = {v4, v5, v6}, average-link clustering calcu-
lates the inter-cluster score between R?i and R?j as
SAL(R?i, R?j) =
1
|R?i||R?j |
?
a?R?i,b?R?j
Bab
SAL(R?i, R?j) can be included as a feature for
the compatibility function Cij , with an associated
weight estimated from training data.
Second, there may exist phenomena of the data
that can only be captured by a classifier that con-
siders ?higher-order? features. Below we describe
two such cases.
In the first example, consider three vertices of
mild compatibility, as in Figure 1(a). (For these
examples, let Bij , Cij ? [0, 1].) Suppose that
these three phone numbers occur nearby in a doc-
ument. Since it is not uncommon for a person to
have two phone numbers with different area codes,
the pairwise compatibility function may score any
pair of nearby phone numbers as relatively com-
patible. However, since it is fairly uncommon for
a person to have three phone numbers with three
different area codes, we would not like all three
numbers to be merged into the same record.
Assume an average-link clustering algorithm.
After merging together the 333 and 444 numbers,
Bij will recompute the new inter-cluster compat-
ibility as 0.51, the average of the inter-cluster
edges. In contrast, the cluster compatibility func-
tion Cij can represent the fact that three numbers
with different area codes are to be merged, and can
penalize their compatibility accordingly. Thus, in
606
333-555-5555
666-555-5555444-555-5555
.6
.49
.53
333-555-5555
666-555-5555
444-555-5555
.6
C = 0.1
  B = 0.51
(a)
Univ of North 
Dakota, 
Pleasantville
Pleasantville
North 
Dakota
.48
.49
.9
Univ of North 
Dakota, 
Pleasantville
Pleasantville
North 
Dakota
C = 0.8
    B = 0.485
.9
(b)
Figure 1: Two motivating examples illustrating why the cluster compatibility measure (C) may have
higher representational power than the pairwise compatibility measure (B). In (a), the pairwise measure
over-estimates the inter-cluster compatibility when there exist higher-order features such as A person
is unlikely to have phone numbers with three different area codes. In (b), the pairwise measure under-
estimates inter-cluster compatibility when weak features like string comparisons can be combined into a
more powerful feature by examining multiple field values.
this example, the pairwise compatibility function
over-estimates the true compatibility.
In the second example (Figure 1(b)), we con-
sider the opposite case. Consider three edges,
two of which have weak compatibility, and one of
which has high compatibility. For example, per-
haps the system has access to a list of city-state
pairs, and can reliably conclude that Pleasantville
is a city in the state North Dakota.
Deciding that Univ of North Dakota, Pleas-
antville belongs in the same record as North
Dakota and Pleasantville is a bit more difficult.
Suppose a feature function measures the string
similarity between the city field Pleasantville and
the company field Univ of North Dakota, Pleas-
antville. Alone, this string similarity might not
be very strong, and so the pairwise compatibil-
ity is low. However, after Pleasantville and North
Dakota are merged together, the cluster compat-
ibility function can compute the string similarity
of the concatenation of the city and state fields,
resulting in a higher compatibility. In this ex-
ample, the pairwise compatibility function under-
estimates the true compatibility.
These two examples show that the cluster com-
patibility score can have more representational
power than the average of pairwise compatibility
scores.
FirstName MiddleName
LastName NickName
Suffix Title
JobTitle CompanyName
Department AddressLine
City1 City2
State Country
PostalCode HomePhone
Fax CompanyPhone
DirectCompanyPhone Mobile
Pager VoiceMail
URL Email
InstantMessage
Table 1: The 25 fields annotated in the contact
record dataset.
4 Experiments
4.1 Data
We hand-labeled a subset of faculty and student
homepages from the WebKB dataset1. Each page
was labeled with the 25 fields listed in Table 1.
In addition, we labeled the records to which each
field belonged. For example, in Figure 2, we la-
beled the contact information for Professor Smith
into a separate record from that of her administra-
tive assistant. There are 252 labeled pages in total,
containing 8996 fields and 16679 word tokens. We
perform ten random samples of 70-30 splits of the
data for all experiments.
4.2 Systems
We evaluate five different record extraction sys-
tems. With the exception of Transitive Closure,
all methods employ the agglomerative clustering
1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-
20/www/data/
607
Professor Jane Smith
Somesuch University
555-555-5555
Professor Smith is the Director of the Knowledge Lab ...
Mr. John Doe
Administrative Assistant
555-367-7777
Record 1
Record 2
Figure 2: A synthetic example representative of the labeled data. Note that Record 1 contains information
both from an address block and from free text, and that Record 2 must be separated from Record 1 even
though fields from each may be nearby in the text.
algorithm described previously. The difference is
in how the inter-cluster compatibility is calculated.
? Transitive Closure: The method described
in the beginning of Section 3.4, where hard
classification decisions are made, and transi-
tivity is enforced.
? Pairwise Compatibility: In this approach,
the compatibility function only estimates the
compatibility between pairs of fields, not sets
of fields. To compute inter-cluster compat-
ibility, the mean of the edges between the
clusters is calculated.
? McDonald: This method uses the pairwise
compatibility function, but instead of calcu-
lating the mean of inter-cluster edges, it cal-
culates the geometric mean of all pairs of
edges in the potential new cluster. That is,
to calculate the compatibility of records Ri
and Rj , we construct a new record Rij that
contains all fields of Ri and Rj , then calcu-
late the geometric mean of all pairs of fields
in Rij . This is analogous to the method used
in McDonald et al (2005) for relation extrac-
tion.
? Cluster Compatibility (uniform): Inter-
cluster compatibility is calculated directly by
the cluster compatibility function. This is the
method we advocate in Section 3. Training
examples are sampled uniformly as described
in Section 3.3.
? Cluster Compatibility (iterative): Same as
above, but training examples are sampled us-
ing the iterative method described in Section
3.3.
4.3 Features
For the pairwise compatibility classifier, we ex-
ploit various formatting as well as knowledge-
based features. Formatting features include the
number of hard returns between fields, whether
the fields occur on the same line, and whether the
fields occur consecutively. Knowledge-based fea-
tures include a mapping we compiled of cities and
states in the United States and Canada. Addition-
ally, we used compatibility features, such as which
fields are of the same type but have different val-
ues.
In building the cluster compatibility classifier,
we use many of the same features as in the bi-
nary classifier, but cast them as first-order existen-
tial features that are generated if the feature exists
between any pair of fields in the two clusters. Ad-
ditionally, we are able to exploit more powerful
compatibility and knowledge-base features. For
example, we examine if a title, a first name and a
last name occur consecutively (i.e., no other fields
occur in-between them). Also, we examine multi-
ple telephone numbers to ensure that they have the
same area codes. Additionally, we employ count
features that indicate if a certain field occurs more
than a given threshold.
4.4 Results
For these experiments, we compare performance
on the true record for each page. That is, we cal-
culate how often each system returns a complete
and accurate extraction of the contact record per-
taining to the owner of the webpage. We refer to
608
this record as the canonical record and measure
performance in terms of precision, recall and F1
for each field in the canonical record.
Table 2 compares precision, recall and F1 across
the various systems. The cluster compatibility
method with iterative sampling has the highest F1,
demonstrating a 14% error reduction over the next
best method and a 53% error reduction over the
transitive closure baseline.
Transitive closure has the highest recall, but it
comes at the expense of precision, and hence ob-
tains lower F1 scores than more conservative com-
patibility methods. The McDonald method also
has high recall, but drastically improves precision
over the transitivity method by taking into consid-
eration all edge weights.
The pairwise measure yields a slightly higher
F1 score than McDonald mostly due to precision
improvements. Because the McDonald method
calculates the mean of all edge weights rather
than just the inter-cluster edge weights, inter-
cluster weights are often outweighed by intra-
cluster weights. This can cause two densely-
connected clusters to be merged despite low inter-
cluster edge weights.
To further investigate performance differences,
we perform three additional experiments. The first
measures how sensitive the algorithms are to the
threshold value ? . Figure 3 plots the precision-
recall curve obtained by varying ? from 1.0 to 0.1.
As expected, high values of ? result in low recall
but high precision, since the algorithms halt with
a large number of small clusters. The highlighted
points correspond to ? = 0.5. These results indi-
cate that setting ? to 0.5 is near optimal, and that
the cluster compatibility method outperforms the
pairwise across a wide range of values for ? .
In the second experiment, we plot F1 versus
the size of the canonical record. Figure 4 indi-
cates that most of the performance gain occurs
in smaller canonical records (containing between
6 and 12 fields). Small canonical records are
most susceptible to precision errors simply be-
cause there are more extraneous fields that may
be incorrectly assigned to them. These precision
errors are often addressed by the cluster compati-
bility method, as shown in Table 2.
In the final experiment, we plot F1 versus the
total number of fields on the page. Figure 5 indi-
cates that the cluster compatibility method is best
at handling documents with large number of fields.
F1 Precision Recall
Cluster (I) 91.81 (.013) 92.87 (.005) 90.78 (.007)
Cluster (U) 90.02 (.012) 93.56 (.007) 86.74 (.011)
Pairwise 90.51 (.013) 91.07 (.004) 89.95 (.006)
McDonald 88.36 (.012) 83.55 (.004) 93.75 (.005)
Trans Clos 82.37 (.002) 70.75 (.009) 98.56 (.020)
Table 2: Precision, recall, and F1 performance for
the record extraction task. The standard error is
calculated over 10 cross-validation trials.
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9
 
0.95 1  0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9
 
1
precision
recall
cluste
r
pairwis
e
Figure 3: Precision-recall curve for cluster, pair-
wise, and mcdonald. The graph is obtained by
varying the stopping threshold ? from 1.0 to 0.1.
The highlighted points correspond to ? = 0.5.
When there are over 80 fields in the document, the
performance of the pairwise method drops dramat-
ically, while cluster compatibility only declines
slightly. We believe the improved precision of the
cluster compatibility method explains this trend as
well.
We also examine documents where cluster com-
patibility outperforms the pairwise methods. Typ-
ically, these documents contain interleaving con-
tact records. Often, it is the case that a single pair
of fields is sufficient to determine whether a clus-
ter should not be merged. For example, the cluster
classifier can directly model the fact that a con-
tact record should not have multiple first or last
names. It can also associate a weight with the fact
that several fields overlap (e.g., the chances that
a cluster has two first names, two last names and
two cities). In contrast, the binary classifier only
examines pairs of fields in isolation and averages
these probabilities with other edges. This averag-
ing can dilute the evidence from a single pair of
fields. Embarrassing errors may result, such as
a contact record with two first names or two last
609
0.740.760.78
0.80.820.84
0.860.880.9
0.920.940.96
6-9 9-12 12+number fields per record
F1
pairwisemcdonaldcluster
Figure 4: Field F1 as the size of the canonical
record increases. This figure suggests that clus-
ter compatibility is most helpful for small records.
0.80.82
0.840.86
0.880.9
0.920.94
0.96
0-20 20-40 40-60 60-80 80+number fields per document
F1
pairwisemcdonaldcluster
Figure 5: Field F1 as the number of fields in
the document increases. This figure suggests that
cluster compatibility is most helpful when the doc-
ument has more than 80 fields.
names. These errors are particularly prevalent in
interleaving contact records since adjacent fields
often belong to the same record.
5 Conclusions and Future Work
We have investigated graph partitioning methods
for discovering database records from fields anno-
tated in text. We have proposed a cluster compat-
ibility function that measures how likely it is that
two sets of fields belong to the same cluster. We
argue that this enhancement to existing techniques
provides more representational power.
We have evaluated these methods on a set of
hand-annotated data and concluded that (1) graph
partitioning techniques are more accurate than per-
forming transitive closure, and (2) cluster compat-
ibility methods can avoid common mistakes made
by pairwise compatibility methods.
As information extraction systems become
more reliable, it will become increasingly impor-
tant to develop accurate ways of associating dis-
parate fields into cohesive records. This will en-
able more complex reasoning over text.
One shortcoming of this approach is that fields
are not allowed to belong to multiple records,
because the partitioning algorithm returns non-
overlapping clusters. Exploring overlapping clus-
tering techniques is an area of future work.
Another avenue of future research is to consider
syntactic information in the compatibility func-
tion. While performance on contact record extrac-
tion is highly influenced by formatting features,
many fields occur within sentences, and syntactic
information (such as dependency trees or phrase-
structure trees) may improve performance.
Overall performance can also be improved by
increasing the sophistication of the partitioning
method. For example, we can examine ?block
moves? to swap multiple fields between clusters
in unison, possibly avoiding local minima of the
greedy method (Kanani et al, 2006). This can be
especially helpful because many mistakes may be
made at the start of clustering, before clusters are
large enough to reflect true records.
Additionally, many personal web pages con-
tain a time-line of information that describe a per-
son?s educational and professional history. Learn-
ing to associate time information with each con-
tact record enables career path modeling, which
presents interesting opportunities for knowledge
discovery techniques, a subject of ongoing work.
Acknowledgments
We thank the anonymous reviewers for helpful
suggestions. This work was supported in part by
the Center for Intelligent Information Retrieval, in
part by U.S. Government contract #NBCH040171
through a subcontract with BBNT Solutions LLC,
in part by The Central Intelligence Agency, the
National Security Agency and National Science
Foundation under NSF grant #IIS-0326249, and in
part by the Defense Advanced Research Projects
Agency (DARPA), through the Department of the
Interior, NBC, Acquisition Services Division, un-
der contract number NBCHD030010. Any opin-
610
ions, findings and conclusions or recommenda-
tions expressed in this material are the author(s)
and do not necessarily reflect those of the sponsor.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the Fifth ACM Interna-
tional Conference on Digital Libraries.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learining, 56:89?
113.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34:211?231.
Vinayak R. Borkar, Kaustubh Deshmukh, and Sunita
Sarawagi. 2001. Automatic segmentation of text
into structured records. In SIGMOD Conference.
Sergey Brin. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology.
Aron Culotta and Andrew McCallum. 2006. Practical
Markov logic containing first-order quantifiers with
application to identity uncertainty. In HLT Work-
shop on Computationally Hard Problems and Joint
Inference in Speech and Language Processing, June.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and contact
information from email and the web. In First Con-
ference on Email and Anti-Spam (CEAS), Mountain
View, CA.
David W. Embley and Lin Xu. 2000. Record location
and reconfiguration in unstructured multiple-record
web documents. In WebDB, pages 123?128.
David W. Embley, Xiaoyi Jiang, and Yiu-Kai Ng.
1999. Record-boundary discovery in web docu-
ments. In SIGMOD Conference, pages 467?478.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In SCIE, pages 10?27.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In ACL.
Pallika Kanani, Andrew McCallum, and Chris Pal.
2006. Improving author coreference by resource-
bounded information gathering from the web. Tech-
nical note.
Trausti Kristjannson, Aron Culotta, Paul Viola, and
Andrew McCallum. 2004. Interactive information
extraction with conditional random fields. Nine-
teenth National Conference on Artificial Intelligence
(AAAI 2004).
Andrew McCallum and Ben Wellner. 2005. Condi-
tional models of identity uncertainty with applica-
tion to noun coreference. In Lawrence K. Saul, Yair
Weiss, and Le?on Bottou, editors, Advances in Neu-
ral Information Processing Systems 17. MIT Press,
Cambridge, MA.
Ryan McDonald, Fernando Pereira, Seth Kulick, Scott
Winters, Yang Jin, and Pete White. 2005. Simple
algorithms for complex relation extraction with ap-
plications to biomedical ie. In 43rd Annual Meeting
of the Association for Computational Linguistics.
Brian Milch, Bhaskara Marthi, and Stuart Russell.
2005. BLOG: Probabilistic models with unknown
objects. In IJCAI.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In ANLP.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press. To appear.
Paul Viola and Mukund Narasimhan. 2005. Learning
to extract information from semi-structured text us-
ing a discriminative context free grammar. In SIGIR
?05: Proceedings of the 28th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 330?337, New
York, NY, USA. ACM Press.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
611
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 41?48,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Practical Markov Logic Containing First-Order Quantifiers
with Application to Identity Uncertainty
Aron Culotta and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{culotta, mccallum}@cs.umass.edu
Abstract
Markov logic is a highly expressive language
recently introduced to specify the connec-
tivity of a Markov network using first-order
logic. While Markov logic is capable of
constructing arbitrary first-order formulae
over the data, the complexity of these for-
mulae is often limited in practice because
of the size and connectivity of the result-
ing network. In this paper, we present ap-
proximate inference and estimation meth-
ods that incrementally instantiate portions
of the network as needed to enable first-
order existential and universal quantifiers
in Markov logic networks. When applied
to the problem of identity uncertainty, this
approach results in a conditional probabilis-
tic model that can reason about objects,
combining the expressivity of recently in-
troduced BLOG models with the predic-
tive power of conditional training. We vali-
date our algorithms on the tasks of citation
matching and author disambiguation.
1 Introduction
Markov logic networks (MLNs) combine the proba-
bilistic semantics of graphical models with the ex-
pressivity of first-order logic to model relational de-
pendencies (Richardson and Domingos, 2004). They
provide a method to instantiate Markov networks
from a set of constants and first-order formulae.
While MLNs have the power to specify Markov
networks with complex, finely-tuned dependencies,
the difficulty of instantiating these networks grows
with the complexity of the formulae. In particular,
expressions with first-order quantifiers can lead to
networks that are large and densely connected, mak-
ing exact probabilistic inference intractable. Because
of this, existing applications of MLNs have not ex-
ploited the full richness of expressions available in
first-order logic.
For example, consider the database of researchers
described in Richardson and Domingos (2004),
where predicates include Professor(person),
Student(person), AdvisedBy(person, per-
son), and Published(author, paper). First-
order formulae include statements such as ?students
are not professors? and ?each student has at most
one advisor.? Consider instead statements such as
?all the students of an advisor publish papers with
similar words in the title? or ?this subset of stu-
dents belong to the same lab.? To instantiate an
MLN with such predicates requires existential and
universal quantifiers, resulting in either a densely
connected network, or a network with prohibitively
many nodes. (In the latter example, it may be nec-
essary to ground the predicate for each element of
the power set of students.)
However, as discussed in Section 2, there may
be cases where these aggregate predicates increase
predictive power. For example, in predicting
the value of HaveSameAdvisor(ai . . . ai+k),
it may be useful to know the values
of aggregate evidence predicates such as
CoauthoredAtLeastTwoPapers(ai . . . ai+k),
which indicates whether there are at least two papers
that some combination of authors from ai . . . ai+k
have co-authored. Additionally, we can construct
predicates such as NumberOfStudents(ai) to
model the number of students a researcher is likely
to advise simultaneously.
These aggregate predicates are examples of uni-
versal and existentially quantified predicates over ob-
served and unobserved values. To enable these sorts
41
of predicates while limiting the complexity of the
ground Markov network, we present an algorithm
that incrementally expands the set of aggregate pred-
icates during the inference procedure. In this paper,
we describe a general algorithm for incremental ex-
pansion of predicates in MLNs, then present an im-
plementation of the algorithm applied to the problem
of identity uncertainty.
2 Related Work
MLNs were designed to subsume various previously
proposed statistical relational models. Probabilistic
relational models (Friedman et al, 1999) combine
descriptive logic with directed graphical models, but
are restricted to acyclic graphs. Relational Markov
networks (Taskar et al, 2002) use SQL queries to
specify the structure of undirected graphical mod-
els. Since first-order logic subsumes SQL, MLNs
can be viewed as more expressive than relational
Markov networks, although existing applications of
MLNs have not fully utilized this increased expres-
sivity. Other approaches combining logic program-
ming and log-linear models include stochastic logic
programs (Cussens, 2003) and MACCENT(Dehaspe,
1997), although MLNs can be shown to represent
both of these.
Viewed as a method to avoid grounding an in-
tractable number of predicates, this paper has similar
motivations to recent work in lifted inference (Poole,
2003; de Salvo Braz et al, 2005), which performs
inference directly at the first-order level to avoid in-
stantiating all predicates. Although our model is not
an instance of lifted inference, it does attempt to re-
duce the number of predicates by instantiating them
incrementally.
Identity uncertainty (also known as record linkage,
deduplication, object identification, and co-reference
resolution) is the problem of determining whether a
set of constants (mentions) refer to the same object
(entity). Successful identity resolution enables vi-
sion systems to track objects, database systems to
deduplicate redundant records, and text processing
systems to resolve disparate mentions of people, or-
ganizations, and locations.
Many probabilistic models of object identification
have been proposed in the past 40 years in databases
(Fellegi and Sunter, 1969; Winkler, 1993) and nat-
ural language processing (McCarthy and Lehnert,
1995; Soon et al, 2001). With the introduction
of statistical relational learning, more sophisticated
models of identity uncertainty have been developed
that consider the dependencies between related con-
solidation decisions.
Most relevant to this work are the recent relational
models of identity uncertainty (Milch et al, 2005;
McCallum and Wellner, 2003; Parag and Domingos,
2004). McCallum and Wellner (2003) present exper-
iments using a conditional random field that factor-
izes into a product of pairwise decisions about men-
tion pairs (Model 3). These pairwise decisions are
made collectively using relational inference; however,
as pointed out in Milch et al (2004), there are short-
comings to this model that stem from the fact that it
does not capture features of objects, only of mention
pairs. For example, aggregate features such as ?a re-
searcher is unlikely to publish in more than 2 differ-
ent fields? or ?a person is unlikely to be referred to by
three different names? cannot be captured by solely
examining pairs of mentions. Additionally, decom-
posing an object into a set of mention pairs results
in ?double-counting? of attributes, which can skew
reasoning about a single object (Milch et al, 2004).
Similar problems apply to the model in Parag and
Domingos (2004).
Milch et al (2005) address these issues by con-
structing a generative probabilistic model over pos-
sible worlds called BLOG, where realizations of ob-
jects are typically sampled from a generative process.
While BLOG model provides attractive semantics for
reasoning about unknown objects, the transition to
generatively trained models sacrifices some of the at-
tractive properties of the discriminative model in Mc-
Callum and Wellner (2003) and Parag and Domin-
gos (2004), such as the ability to easily incorporate
many overlapping features of the observed mentions.
In contrast, generative models are constrained either
to assume the independence of these features or to
explicitly model their interactions.
Object identification can also be seen as an in-
stance of supervised clustering. Daume? III and
Marcu (2004) and Carbonetto et al (2005) present
similar Bayesian supervised clustering algorithms
that use a Dirichlet process to model the number
of clusters. As a generative model, it has similar ad-
vantages and disadvantages as Milch et al (2005),
with the added capability of integrating out the un-
certainty in the true number of objects.
In this paper, we present of identity uncertainty
that incorporates the attractive properties of Mc-
Callum and Wellner (2003) and Milch et al (2005),
resulting in a discriminative model to reason about
objects.
3 Markov logic networks
Let F = {Fi} be a set of first order formulae with
corresponding real-valued weights w = {wi}. Given
a set of constants C = {ci}, define ni(x) to be the
number of true groundings of Fi realized in a setting
42
of the world given by atomic formulae x. A Markov
logic network (MLN) (Richardson and Domingos,
2004) defines a joint probability distribution over
possible worlds x. In this paper, we will work with
discriminative MLNs (Singla and Domingos, 2005),
which define the conditional distribution over a set
of query atoms y given a set of evidence atoms x.
Using the normalizing constant Zx, the conditional
distribution is given by
P (Y = y|X = x) =
1
Zx
exp
?
?
|Fy|?
i=1
wini(x, y)
?
? (1)
where Fy ? F is the set of clauses for which at least
one grounding contains a query atom, and ni(x, y)
is the number of true groundings of the ith clause
containing evidence atom x and query atom y.
3.1 Inference Complexity in Ground
Markov Networks
The set of predicates and constants in Markov logic
define the structure of a Markov network, called a
ground Markov network. In discriminative Markov
logic networks, this resulting network is a conditional
Markov network (also known as a conditional ran-
dom field (Lafferty et al, 2001)).
From Equation 1, the formulae Fy specify the
structure of the corresponding Markov network as
follows: Each grounding of a predicate specified in
Fy has a corresponding node in the Markov network;
and an edge connects two nodes in the network if and
only if their corresponding predicates co-occur in a
grounding of a formula Fy. Thus, the complexity
of the formulae in Fy will determine the complexity
of the resulting Markov network, and therefore the
complexity of inference. When Fy contains complex
first-order quantifiers, the resulting Markov network
may contain a prohibitively large number of nodes.
For example, let the set of constants C be the set of
authors {ai}, papers {pi}, and conferences {ci} from
a research publication database. Predicates may in-
clude AuthorOf(ai, pj), AdvisorOf(ai, aj), and
ProgramCommittee(ai, cj). Each grounding of a
predicate corresponds to a random variable in the
corresponding Markov network.
It is important to notice how query predicates and
evidence predicates differ in their impact on inference
complexity. Grounded evidence predicates result in
observed random variables that can be highly con-
nected without resulting in an increase in inference
complexity. For example, consider the binary evi-
dence predicate HaveSameLastName(ai . . . ai+k).
This aggregate predicate reflects informa-
tion about a subset of (k ? i + 1) constants.
The value of this predicate is dependent on
the values of HaveSameLastName(ai, ai+1),
HaveSameLastName(ai, ai+2), etc. However,
since all of the corresponding variables are observed,
inference does not need to ensure their consistency
or model their interaction.
In contrast, complex query predicates can make
inference more difficult. Consider the query
predicate HaveSameAdvisor(ai . . . ai+k). Here,
the related predicatesHaveSameAdvisor(ai, ai+1),
HaveSameAdvisor(ai, ai+2), etc., all correspond
to unobserved binary random variables that the
model must predict. To ensure their consistency,
the resulting Markov network must contain depen-
dency edges between each of these variables, result-
ing in a densely connected network. Since inference
in general in Markov networks scales exponentially
with the size of the largest clique, inference in the
grounded network quickly becomes intractable.
One solution is to limit the expressivity of the
predicates. In the previous example, we can decom-
pose the predicate HaveSameAdvisor(ai . . . ai+k)
into its (k ? i + 1)2 corresponding pairwise pred-
icates, such as HaveSameAdvisor(ai, ai+1). An-
swering an aggregate query about the advisors of a
group of students can be handled by a conjunction
of these pairwise predicates.
However, as discussed in Sections 1 and 2, we
would like to reason about objects, not just pairs
of mentions, because this enables richer evidence
predicates. For example, the evidence predicates
AtLeastTwoCoauthoredPapers(ai . . . ai+k)
and NumberOfStudents(ai) can be
highly predictive of the query predicate
HaveSameAdvisor(ai . . . ai+k).
Below, we describe a discriminative MLN for iden-
tity uncertainty that is able to reason at the object
level.
3.2 Identity uncertainty
Typically, MLNs make a unique names assumption,
requiring that different constants refer to distinct ob-
jects. In the publications database example, each
author constant ai is a string representation of one
author mention found in the text of a citation. The
unique names assumption assumes that each ai refers
to a distinct author in the real-world. This simplifies
the network structure at the risk of weak or fallacious
predictions (e.g., AdvisorOf(ai, aj) is erroneous if
ai and aj actually refer to the same author). The
identity uncertainty problem is the task of removing
the unique names assumption by determining which
43
constants refer to the same real-world objects.
Richardson and Domingos (2004) address this con-
cern by creating the predicate Equals(ci, cj) be-
tween each pair of constants. While this retains the
coherence of the model, the restriction to pairwise
predicates can be a drawback if there exist informa-
tive features over sets of constants. In particular,
by only capturing features of pairs of constants, this
solution cannot model the compatibility of object at-
tributes, only of constant attributes (Section 2).
Instead, we desire a conditional model that allows
predicates to be defined over a set of constants.
One approach is to introduce constants that repre-
sent objects, and connect them to their mentions by
predicates such as IsMentionOf(ci, cj). In addition
to computational issues, this approach also some-
what problematically requires choosing the number
of objects. (See Richardson and Domingos (2004) for
a brief discussion.)
Instead, we propose instantiating aggregate pred-
icates over sets of constants, such that a setting of
these predicates implicitly determines the number of
objects. This approach allows us to model attributes
over entire objects, rather than only pairs of con-
stants. In the following sections, we describe aggre-
gate predicates in more detail, as well as the approx-
imations necessary to implement them efficiently.
3.3 Aggregate predicates
Aggregate predicates are predicates that take as ar-
guments an arbitrary number of constants. For ex-
ample, the HaveSameAdvisor(ai . . . ai+k) predi-
cate in the previous section is an example of an ag-
gregate predicate over k ? i + 1 constants.
Let IC = {1 . . . N} be the set of indices into the set
of constants C, with power set P(IC). For any subset
d ? P(IC), an aggregate predicate A(d) defines a
property over the subset of constants d.
Note that aggregate predicates can be trans-
lated into first-order formulae. For example,
HaveSameAdvisor(ai . . . ai+k) can be re-written
as ?(ax, ay) ? {ai . . . ai+k} SameAdvisor(ax, ay).
By using aggregate predicates we make explicit the
fact that we are modeling the attributes at the object
level.
We distinguish between aggregate query predi-
cates, which represent unobserved aggregate vari-
ables, and aggregate evidence predicates, which rep-
resent observed aggregate variables. Note that using
aggregate query predicates can complicate inference,
since they represent a collection of fully connected
hidden variables. The main point of this paper is
that although these aggregate query predicates are
specifiable in MLNs, they have not been utilized be-
cause of the resulting inference complexity. We show
that the gains made possible by these predicates of-
ten outweigh the approximations required for infer-
ence.
As discussed in Section 3.1, for each aggregate
query predicates A(d), it is critical that the model
predict consistent values for every related subset of d.
Enforcing this consistency requires introducing de-
pendency edges between aggregate query predicates
that share arguments. In general, this can be a diffi-
cult problem. Here, we focus on the special case for
identity uncertainty where the main query predicate
under consideration is AreEqual(d).
The aggregate query predicate AreEqual(d) is
true if and only if all constants di ? d refer to the
same object. Since each subset of constants corre-
sponds to a candidate object, a (consistent) setting
of all the AreEqual predicates results in a solution
to the object identification problem. The number
of objects is chosen based on the optimal grounding
of each of these aggregate predicates, and therefore
does not require a prior over the number of objects.
That is, once all the AreEqual predicates are set,
they determine a clustering with a fixed number of
objects. The number of objects is not modeled or set
directly, but is implied by the result of MAP infer-
ence. (However, a posterior over the number of ob-
jects could be modeled discriminatively in an MLN
(Richardson and Domingos, 2004).)
This formulation also allows us to compute aggre-
gate evidence predicates over objects to help predict
the values of each AreEqual predicate. For exam-
ple, NumberFirstNames(d) returns the number of
different first names used to refer to the object ref-
erenced by constants d. In this way, we can model
aggregate features of an object, capturing the com-
patibility among its attributes.
For a given C, there are |P(IC)| possible ground-
ings of the AreEqual query predicates. Naively im-
plemented, such an approach would require enumer-
ating all subsets of constants, ultimately resulting in
an unwieldy network.
An equivalent way to state the problem is that
using N -ary predicates results in a Markov network
with one node for each grounding of the predicate.
Since in the general case there is one grounding
for each subset of C, the size of the corresponding
Markov network will be exponential in |C|. See Fig-
ure 1 for an example instantiation of an MLN with
three constants (a, b, c) and one AreEqual predi-
cate.
In this paper, we provide algorithms to per-
form approximate inference and parameter estima-
tion by incrementally instantiating these predicates
44
AreEqual(a,b) AreEqual(a,c) AreEqual(b,c)
AreEqual(a,b,c)
Figure 1: An example of the network instantiated
by an MLN with three constants and the aggregate
predicate AreEqual, instantiated for all possible
subsets with size ? 2.
as needed.
3.4 MAP Inference
Maximum a posteriori (MAP) inference seeks the so-
lution to
y? = argmax
y
P (Y = y|X = x)
where y? is the setting of all the query predicates
Fy (e.g. AreEqual) with the maximal conditional
density.
In large, densely connected Markov networks, a
common approximate inference technique is loopy
belief propagation (i.e. the max-product algorithm
applied to a cyclic graph). However, the use of ag-
gregate predicates makes it intractable even to in-
stantiate the entire network, making max-product
an inappropriate solution.
Instead, we employ an incremental inference tech-
nique that grounds aggregate query predicates in
an agglomerative fashion based on the model?s cur-
rent MAP estimates. This algorithm can be viewed
as a greedy agglomerative search for a local opti-
mum of P (Y |X), and has connections to recent work
on correlational clustering (Bansal et al, 2004) and
graph partitioning for MAP estimation (Boykov et
al., 2001).
First, note that finding the MAP estimate does not
require computing Zx, since we are only interested in
the relative values of each configuration, and Zx is
fixed for a given x. Thus, at iteration t, we compute
an unnormalized score for yt (the current setting of
the query predicates) given the evidence predicates
x as follows:
S(yt, x) = exp
?
?
|F t|?
i=0
wini(x, y
t)
?
?
where F t ? Fy is the set of aggregate predicates
representing a partial solution to the object identifi-
cation task for constants C, specified by yt.
Algorithm 1 Approximate MAP Inference Algo-
rithm
1: Given initial predicates F 0
2: while ScoreIsIncreased do
3: F ?i ? FindMostLikelyPredicate(F
t)
4: F ?i ? true
5: F t ? ExpandPredicates(F ?i , F
t)
6: end while
Algorithm 1 outlines a high-level description of the
approximate MAP inference algorithm. The algo-
rithm first initializes the set of query predicated F 0
such that all AreEqual predicates are restricted
to pairs of constants, i.e. AreEqual(ci, cj) ?(i, j).
This is equivalent to a Markov network containing
one unobserved random variable for each pair of con-
stants, where each variable indicates whether a pair
of constants refer to the same object.
Initially, each AreEqual predicate is assumed
false. In line 3, the procedure FindMostLike-
lyPredicate iterates through each query predicate
in F t, setting each to true in turn and calculating its
impact on the scoring function. The procedure re-
turns the predicate F ?i such that setting F
?
i to True
results in the greatest increase in the scoring function
S(yt, x).
Let (c?i . . . c
?
j ) be the set of constants ?merged?
by setting their AreEqual predicate to true. The
ExpandPredicates procedure creates new predi-
cates AreEqual(c?i . . . c
?
j , ck . . . cl) corresponding to
all the potential predicates created by merging the
constants c?i . . . c
?
j with any of the other previously
merged constants. For example, after the first it-
eration, a pair of constants (c?i , c
?
j ) are merged.
The set of predicates are expanded to include
AreEqual(c?i , c
?
j , ck) ?ck, reflecting all possible ad-
ditional references to the proposed object referenced
by c?i , c
?
j .
This algorithm continues until there is no predi-
cate that can be set to true that increases the score
function.
In this way, the final setting of Fy is a local max-
imum of the score function. As in other search
algorithms, we can employ look-ahead to reduce
the greediness of the search (i.e., consider multiple
merges simultaneously), although we do not include
look-ahead in experiments reported here.
It is important to note that each expansion of the
aggregate query predicates Fy has a corresponding
set of aggregate evidence predicates. These evidence
predicates characterize the compatibility of the at-
tributes of each hypothesized object.
45
3.4.1 Pruning
The space required for the above algorithm scales
?(|C|2), since in the initialization step we must
ground a predicate for each pair of constants. We use
the canopy method of McCallum et al (2000), which
thresholds a ?cheap? similarity metric to prune un-
necessary comparisons. This pruning can be done
at subsequent stages of inference to restrict which
predicates variables will be introduced.
Additionally, we must ensure that predicate set-
tings at time t do not contradict settings at t ? 1
(e.g. if F t(a, b, c) = 1, then F t+1(a, b) = 1). By
greedily setting unobserved nodes to their MAP es-
timates, the inference algorithm ignores inconsistent
settings and removes them from the search space.
3.5 Parameter estimation
Given a fully labeled training set D of constants an-
notated with their referent objects, we would like to
estimate the value of w that maximizes the likelihood
of D. That is, w? = argmaxw Pw(y|x).
When the data are few, we can explicitly instan-
tiate all AreEqual(d) predicates, setting their cor-
responding nodes to the values implied by D. The
likelihood is given by Equation 1, where the normal-
izer is Zx =
?
y? exp
(?|F ?y|
i=1 wini(x, y
?)
)
.
Although this sum over y? to calculate Zx is ex-
ponential in |y|, many inconsistent settings can be
pruned as discussed in Section 3.4.1.
In general, however, instantiating the entire set
of predicates denoted by y and calculating Zx is
intractable. Existing methods for MLN parame-
ter estimation include pseudo-likelihood and voted
perceptron (Richardson and Domingos, 2004; Singla
and Domingos, 2005). We instead follow the recent
success in piecewise training for complex undirected
graphical models (Sutton and McCallum, 2005) by
making the following two approximations. First, we
avoid calculating the global normalizer Zx by calcu-
lating local normalizers, which sum only over the two
values for each aggregate query predicate grounded
in the training data. We therefore maximize the sum
of local probabilities for each query predicate given
the evidence predicates.
This approximation can be viewed as constructing
a log-linear binary classifier to predict whether an
isolated set of constants refer to the same object.
Input features include arbitrary first-order features
over the input constants, and the output is a binary
variable. The parameters of this classifier correspond
to the w weights in the MLN. This simplification
results in a convex optimization problem, which we
solve using gradient descent with L-BFGS, a second-
order optimization method (Liu and Nocedal, 1989).
The second approximation addresses the fact that
all query predicates from the training set cannot be
instantiated. We instead sample a subset FS ? Fy
and maximize the likelihood of this subset. The sam-
pling is not strictly uniform, but is instead obtained
by collecting the predicates created while perform-
ing object identification using a weak method (e.g.
string comparisons). More explicitly, predicates are
sampled from the training data by performing greedy
agglomerative clustering on the training mentions,
using a scoring function that computes the similar-
ity between two nodes by string edit distance. The
goal of this clustering is not to exactly reproduce the
training clusters, but to generate correct and incor-
rect clusters that have similar characteristics (size,
homogeneity) to what will be present in the testing
data.
4 Experiments
We perform experiments on two object identification
tasks: citation matching and author disambiguation.
Citation matching is the task of determining whether
two research paper citation strings refer to the same
paper. We use the Citeseer corpus (Lawrence et al,
1999), containing approximately 1500 citations, 900
of which are unique. The citations are manually la-
beled with cluster identifiers, and the strings are seg-
mented into fields such as author, title, etc. The cita-
tion data is split into four disjoint categories by topic,
and the results presented are obtained by training on
three categories and testing on the fourth.
Using first-order logic, we create a number of ag-
gregate predicates such as AllTitlesMatch, Al-
lAuthorsMatch, AllJournalsMatch, etc., as
well as their existential counterparts, ThereExist-
sTitleMatch, etc. We also include count predi-
cates, which indicate the number of these matches in
a set of constants.
Additionally, we add edit distance predicates,
which calculate approximate matches1 between title
fields, etc., for each pair of citations in a set of cita-
tions. Aggregate features are used for these, such as
?there exists a pair of citations in this cluster which
have titles that are less than 30% similar? and ?the
minimum edit distance between titles in a cluster is
greater than 50%.?
We evaluate using pairwise precision, recall, and
F1, which measure the system?s ability to predict
whether each pair of constants refer to the same ob-
ject or not. Table 1 shows the advantage of our
1We use the Secondstring package, found at
http://secondstring.sourceforge.net
46
Table 1: Precision, recall, and F1 performance for
citation matching task, where Objects is an MLN
using aggregate predicates, and Pairs is an MLN us-
ing only pairwise predicates. Objects outperforms
Pairs on three of the four testing sets.
Objects Pairs
pr re f1 pr re f1
constraint 85.8 79.1 82.3 63.0 98.0 76.7
reinforce 97.0 90.0 93.4 65.6 98.2 78.7
face 93.4 84.8 88.9 74.2 94.7 83.2
reason 97.4 69.3 81.0 76.4 95.5 84.9
Table 2: Performance on the author disambiguation
task. Objects outperforms Pairs on two of the
three testing sets.
Objects Pairs
pr re f1 pr re f1
miller d 73.9 29.3 41.9 44.6 1.0 61.7
li w 39.4 47.9 43.2 22.1 1.0 36.2
smith b 61.2 70.1 65.4 14.5 1.0 25.4
proposed model (Objects) over a model that only
considers pairwise predicates of the same features
(Pairs). Note that Pairs is a strong baseline that
performs collective inference of citation matching de-
cisions, but is restricted to use only IsEqual(ci, cj)
predicates over pairs of citations. Thus, the perfor-
mance difference is due to the ability to model first-
order features of the data.
Author disambiguation is the task of deciding
whether two strings refer to the same author. To in-
crease the task complexity, we collect citations from
the Web containing different authors with matching
last names and first initials. Thus, simply performing
a string match on the author?s name would be insuffi-
cient in many cases. We searched for three common
last name / first initial combinations (Miller, D;
Li, W; Smith, B). From this set, we collected 400
citations referring to 56 unique authors. For these
experiments, we train on two subsets and test on the
third.
We generate aggregate predicates similar to those
used for citation matching. Additionally, we in-
clude features indicating the overlap of tokens from
the titles and indicating whether there exists a pair
of authors in this cluster that have different mid-
dle names. This last feature exemplifies the sort of
reasoning enabled by aggregate predicates: For ex-
ample, consider a pairwise predicate that indicates
whether two authors have the same middle name.
Very often, middle name information is unavailable,
so the name ?Miller, A.? may have high similarity to
both ?Miller, A. B.? and ?Miller, A. C.?. However,
it is unlikely that the same person has two different
middle names, and our model learns a weight for this
feature. Table 2 demonstrates the advantage of this
method.
Overall, Objects achieves F1 scores superior to
Pairs on 5 of the 7 datasets. These results indicate
the potential advantages of using complex first-order
quantifiers in MLNs. The cases in which Pairs out-
performs Objects are likely due to the fact that the
approximate inference used in Objects is greedy.
Increasing the robustness of inference is a topic of
future research.
5 Conclusions and Future Work
We have presented an algorithm that enables practi-
cal inference in MLNs containing first-order existen-
tial and universal quantifiers, and have demonstrated
the advantages of this approach on two real-world
datasets. Future work will investigate efficient ways
to improve the approximations made during infer-
ence, for example by reducing its greediness by revis-
ing the MAP estimates made at previous iterations.
Although the optimal number of objects is cho-
sen implicitly by the inference algorithm, there may
be reasons to explicitly model this number. For ex-
ample, if there exist global features of the data that
suggest there are many objects, then the inference al-
gorithm should be less inclined to merge constants.
Additionally, the data may exhibit ?preferential at-
tachment? such that the probability of a constant
being added to an existing object is proportional to
the number of constants that refer to that object.
Future work will examine the feasibility of adding
aggregate query predicates to represent these values.
More subtly, one may also want to directly model
the size of the object population. For example, given
a database of authors, we may want to estimate not
only how many distinct authors exist in the database,
but also how many distinct authors exist outside of
the database, as discussed in Milch et al (2005).
Discriminatively-trained models cannot easily reason
about objects for which they have no observations;
so a generative/discriminative hybrid model may be
required to properly estimate this value.
Finally, while the inference algorithm we describe
is evaluated only on the object uncertainty task, we
would like to extend it to perform inference over ar-
bitrary query predicates.
47
6 Acknowledgments
We would like to thank the reviewers, and Pallika Kanani
for helpful discussions. This work was supported in
part by the Center for Intelligent Information Retrieval,
in part by U.S. Government contract #NBCH040171
through a subcontract with BBNT Solutions LLC, in
part by The Central Intelligence Agency, the National
Security Agency and National Science Foundation un-
der NSF grant #IIS-0326249, and in part by the Defense
Advanced Research Projects Agency (DARPA), through
the Department of the Interior, NBC, Acquisition Ser-
vices Division, under contract number NBCHD030010.
Any opinions, findings and conclusions or recommenda-
tions expressed in this material are the author(s)? and do
not necessarily reflect those of the sponsor.
References
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learining, 56:89?113.
Yuri Boykov, Olga Veksler, and Ramin Zabih. 2001. Fast
approximate energy minimization via graph cuts. In
IEEE transactions on Pattern Analysis and Machine
Intelligence (PAMI), 23(11):1222?1239.
Peter Carbonetto, Jacek Kisynski, Nando de Freitas, and
David Poole. 2005. Nonparametric bayesian logic. In
UAI.
J. Cussens. 2003. Individuals, relations and structures
in probabilistic models. In Proceedings of the Fifteenth
Conference on Uncertainty in Artificial Intelligence,
pages 126?133, Acapulco, Mexico.
Hal Daume? III and Daniel Marcu. 2004. Supervised clus-
tering with the dirichlet process. In NIPS?04 Learn-
ing With Structured Outputs Workshop, Whistler,
Canada.
Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth. 2005.
Lifted first-order probabilistic inference. In IJCAI,
pages 1319?1325.
L. Dehaspe. 1997. Maximum entropy modeling with
clausal constraints. In Proceedings of the Seventh
International Workshop on Inductive Logic Program-
ming, pages 109?125, Prague, Czech Republic.
I. P. Fellegi and A. B. Sunter. 1969. A theory for record
linkage. Journal of the American Statistical Associa-
tion, 64:1183?1210.
Nir Friedman, Lise Getoor, Daphne Koller, and Avi Pf-
effer. 1999. Learning probabilistic relational models.
In IJCAI, pages 1300?1309.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
S. Lawrence, C. L. Giles, and K. Bollaker. 1999. Digi-
tal libraries and autonomous citation indexing. IEEE
Computer, 32:67?71.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45(3, (Ser. B)):503?528.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on
Information Integration on the Web.
Andrew K. McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data sets
with application to reference matching. In Proceed-
ings of the Sixth International Conference On Knowl-
edge Discovery and Data Mining (KDD-2000), Boston,
MA.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In IJCAI,
pages 1050?1055.
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2004.
Blog: Relational modeling with unknown objects. In
ICML 2004 Workshop on Statistical Relational Learn-
ing and Its Connections to Other Fields.
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2005.
BLOG: Probabilistic models with unknown objects. In
IJCAI.
Parag and Pedro Domingos. 2004. Multi-relational
record linkage. In Proceedings of the KDD-2004 Work-
shop on Multi-Relational Data Mining, pages 31?48,
August.
D. Poole. 2003. First-order probabilistic inference. In
Proceedings of the Eighteenth International Joint Con-
ference on Artificial Intelligence, pages 985?991, Aca-
pulco, Mexico. Morgan Kaufman.
M. Richardson and P. Domingos. 2004. Markov logic
networks. Technical report, University of Washington,
Seattle, WA.
Parag Singla and Pedro Domingos. 2005. Discriminative
training of markov logic networks. In Proceedings of
the Twentieth National Conference of Artificial Intel-
ligence, Pittsburgh, PA.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Comput. Linguist.,
27(4):521?544.
Charles Sutton and Andrew McCallum. 2005. Piecewise
training of undirected models. In Submitted to 21st
Conference on Uncertainty in Artificial Intelligence.
Ben Taskar, Abbeel Pieter, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Uncertainty in Artificial Intelligence: Proceedings of
the Eighteenth Conference (UAI-2002), pages 485?492,
San Francisco, CA. Morgan Kaufmann Publishers.
William E. Winkler. 1993. Improved decision rules in
the fellegi-sunter model of record linkage. Technical
report, Statistical Research Division, U.S. Census Bu-
reau, Washington, DC.
48
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 27?36,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A Demographic Analysis of Online Sentiment during Hurricane Irene
Benjamin Mandel?, Aron Culotta?, John Boulahanis+,
Danielle Stark+, Bonnie Lewis+, Jeremy Rodrigue+
?Department of Computer Science and Industrial Technology
+Department of Sociology and Criminal Justice
Southeastern Louisiana University
Hammond, LA 70402
Abstract
We examine the response to the recent nat-
ural disaster Hurricane Irene on Twitter.com.
We collect over 65,000 Twitter messages re-
lating to Hurricane Irene from August 18th to
August 31st, 2011, and group them by loca-
tion and gender. We train a sentiment classi-
fier to categorize messages based on level of
concern, and then use this classifier to investi-
gate demographic differences. We report three
principal findings: (1) the number of Twit-
ter messages related to Hurricane Irene in di-
rectly affected regions peaks around the time
the hurricane hits that region; (2) the level of
concern in the days leading up to the hurri-
cane?s arrival is dependent on region; and (3)
the level of concern is dependent on gender,
with females being more likely to express con-
cern than males. Qualitative linguistic vari-
ations further support these differences. We
conclude that social media analysis provides
a viable, real-time complement to traditional
survey methods for understanding public per-
ception towards an impending disaster.
Introduction
In 2011, natural disasters cost the United States
more than 1,000 lives and $52 billion. The num-
ber of disasters costing over $1 billion in 2011
(twelve) is more than in the entire decade of the
1980s.1 As the number of people living in disaster-
prone areas grows, it becomes increasingly impor-
tant to have reliable, up-to-the-minute assessments
of emergency preparedness during impending disas-
1?Record year for billion-dollar disasters?, CBS News, De-
cember 7, 2011.
ters. Understanding issues such as personal risk per-
ception, preparedness, and evacuation plans helps
public agencies better tailor emergency warnings,
preparations, and response.
Social scientists typically investigate these issues
using polling data. The research shows significant
demographic differences in response to government
warnings, personal risk assessment, and evacuation
decisions (Perry and Mushkatel, 1986; Perry and
Lindell, 1991; Goltz et al, 1992; Fothergill et al,
1999; West and Orr, 2007; Enarson, 1998). For ex-
ample, Fothergill et al (1999) find that minorities
differ in their risk perception and in their response to
emergency warnings, with some groups having fa-
talistic sentiments that lead to greater fear and less
preparedness. Goltz et al (1992) find that people
with lower income and education, Hispanics, and
women all expressed greater fear of earthquakes.
This past research suggests governments could
benefit by tailoring their messaging and response to
address the variability between groups. While sur-
vey data have advanced our knowledge of these is-
sues, they have two major drawbacks for use in
disaster research. First, most surveys rely on re-
sponses to hypothetical scenarios, for example by
asking subjects if they would evacuate under cer-
tain scenarios. This hypothetical bias is well-known
(Murphy et al, 2005). Second, surveys are often im-
practical in disaster scenarios. In a rapidly-changing
environment, governments cannot wait for a time-
consuming survey to be conducted and the results
analyzed before making warning and response de-
cisions. Additionally, survey response rates shortly
before or after a disaster are likely to be quite low, as
citizens are either without power or are busy prepar-
ing or rebuilding. Thus, it is difficult to collect data
27
during the critical times immediately before and af-
ter the disaster.
In this paper, we investigate the feasibility of
assessing public risk perception using social me-
dia analysis. Social media analysis has recently
been used to estimate trends of interest such as
stock prices (Gilbert and Karahalios, 2010), movie
sales (Asur and Huberman, 2010), political mood
(O?Connor et al, 2010a), and influenza rates (Lam-
pos and Cristianini, 2010; Culotta, 2010; Culotta,
2012). We apply a similar methodology here to as-
sess the public?s level of concern toward an impend-
ing natural disaster.
As a case study, we examine attitudes toward
Hurricane Irene expressed on Twitter.com. We col-
lect over 65,000 Twitter messages referencing Hur-
ricane Irene between August 18th and August 31st,
2011; and we train a sentiment classifier to annotate
messages by level of concern. We specifically look
at how message volume and sentiment varies over
time, location, and gender.
Our findings indicate that message volume in-
creases over the days leading up to the hurricane,
and then sharply decreases following its dispersal.
The timing of the increase and subsequent decrease
in messages differs based on the location relative to
the storm. There is also an increasing proportion of
concerned messages leading up to Hurricane Irene?s
arrival, which then decreases after Irene dissipation.
A demographic analysis of the proportion of con-
cerned messages shows significant differences both
by region and gender. The gender differences in par-
ticular are supported by previous survey results from
the social science literature (West and Orr, 2007).
These results suggest that social media analysis is a
viable technology for understanding public percep-
tion during a hurricane.
The remainder of the paper is organized as fol-
lows: First, we describe the data collection method-
ology, including how messages are annotated with
location and gender. Next, we present sentiment
classification experiments comparing various classi-
fiers, tokenization procedures, and feature sets. Fi-
nally, we apply this classifier to the entire message
set and analyze demographic variation in levels of
concern.
Data Collection
Irene became a tropical storm on August 20th, 2011,
and hit the east coast of the United States between
August 26th and 28th. This hurricane provides a
compelling case to investigate for several reasons.
First, Irene affected many people in many states,
meaning that regional differences in responses can
be investigated. Second, there was considerable me-
dia and political attention surrounding Hurricane
Irene, leading to it being a popular topic on social
network sites. Third, the fact that there was fore-
warning of the hurricane means that responses to it
can be evaluated over time.
Twitter is a social networking site that allows
users to post brief, public messages to their follow-
ers. Using Twitter?s API2, we can sample many mes-
sages as well as their meta-data, such as time, loca-
tion, and user name. Also, since Twitter can be used
on smart phones with batteries, power outages due
to natural disasters will presumably have less of an
effect on the volume of messages.
Using Twitter?s sampling API (?spritzer?), we
sample approximately uniformly from all messages
between August 18 and August 31. We then per-
form keyword filtering to collect messages contain-
ing the words ?Irene? or ?Hurricane?, or the hashtag
?#Irene?. During the period of August 18th to Au-
gust 31st, messages containing these keywords are
overwhelmingly related to Hurricane Irene and not
some other event. This results in 65,062 messages.
Inferring Location
In order to determine the location of the message
sender, we process the user-reported location data
from that user?s profile. Since not all users enter ac-
curate location data, we search for specific keywords
in order to classify the messages by state. For exam-
ple, if the location data contains a token ?VT? or
?Vermont,? it is labeled as coming from Vermont.
(See Appendix A for more details.) The locations
we consider are the 13 states directly affected by
Hurricane Irene, plus Washington DC. These loca-
tions are then grouped into 3 regions. First, the New
England region consists of the states of Connecticut,
Massachusetts, Rhode Island, New Hampshire, Ver-
mont, and Maine. Second, the Middle States region
2http://dev.twitter.com
28
L?
M?
N?
H?
K?
B?A?
C?
D?
E?
F?G?
I?
J?
O?
0?
0.005?
0.01?
0.015?
0.02?
0.025?
0.03?
8/18
/11?1
2:00
?AM?
8/18
/11?2
:00?P
M?
8/19
/11?4
:00?A
M?
8/19
/11?6
:00?P
M?
8/20
/11?8
:00?A
M?
8/20
/11?1
0:00
?PM?
8/21
/11?1
2:00
?PM?
8/22
/11?2
:00?A
M?
8/22
/11?4
:00?P
M?
8/23
/11?6
:00?A
M?
8/23
/11?8
:00?P
M?
8/24
/11?1
0:00
?AM?
8/25
/11?1
2:00
?AM?
8/25
/11?2
:00?P
M?
8/26
/11?4
:00?A
M?
8/26
/11?6
:00?P
M?
8/27
/11?8
:00?A
M?
8/27
/11?1
0:00
?PM?
8/28
/11?1
2:00
?PM?
8/29
/11?2
:00?A
M?
8/29
/11?4
:00?P
M?
8/30
/11?6
:00?A
M?
8/30
/11?8
:00?P
M?
8/31
/11?1
0:00
?AM?
Mess
age?P
ropo
r?n?
?New?England??
?Middle?States??
?Upper?South?
A:?8?22?5:00am???Irene?becomes?a?Cat.?1?hurricane?B:?8?22?8:30pm???Irene?becomes?a?Cat.?2?hurricane?C:?8?23?1:51pm???Strong?earthquake?hits?near?Richmond,?VA.??Earlier?on?8?23,?Irene?had?been?forecast?to?hit?East?Coast?;?FEMA?held?press?conference.?D:?8?24?8:00am???Irene?becomes?a?Cat.?3?hurricane??E:?8?25?5:00am???Hurricane?and?Tropical?Storm?Watches?Issued?for?coast?in?SC,?NC?F:?8?25?5:00pm???New?Hurricane?Watches?issued?for?coastal?areas?from?VA?to?NJ.?G:?8?26?5:00am???Hurr.?Watches?in?NC?to?NJ?upgraded?to?Warnings;?new?Watches?for?NY?coast?H:?8?26?2:00pm???Irene?weakens?a?li?e,?Tropical?Storm?force?winds?arriving?along?NC?coast?I:?8?27?8:00am???Center?of?Irene?makes?landfall?at?Cape?Lookout,?NC?as?a?Cat.?1?Hurricane?J:?8?27?7:00pm???Irene?re?emerges?over?Atlan??Ocean?at?NC/VA?coastal?border?K:?8?27?11:00pm???Irene?drenching?Mid?Atlan??states?L:?8?28?11:00am???Irene?now?Tropical?Storm;?over?Southeastern?NY;?Southern?New?England?M:?8?28?5:00pm???Center?of?Irene?nearing?northern?New?England?N:?8?28?8:00pm???Major?flooding?occurring?in?parts?of?New?England?O:?8?29?5:00am???Remnants?of?Irene?moving?into?Quebec?and?Newfoundland;?Major?flooding?con?ues?in?parts?of?Northeast ?
Figure 1: Results from Hurricane Irene Twitter data showing the influence of disaster-related events on the number
of messages from each region. The y-axis is the proportion of all Irene-related messages from each region that were
posted during each hour.
consists of New York, New Jersey, and Pennsylva-
nia. Third, the Upper South region consists of North
Carolina, Virginia, Maryland, Delaware, and Wash-
ington DC.
Of the messages that we collect between Au-
gust 18th and August 31st, 15,721 are identified
as belonging to one of the directly affected areas.
Grouped into regions, we find that 2,424 are from
New England, 8,665 are from the Middle-States re-
gion, and 4,632 are from the Upper South region.
Figure 1 displays the messages per hour from
each of the three regions. The y-axis is normalized
over all messages from that region ? e.g., a value
of 0.02 for New England means that 2% of all mes-
sages from New England over the 10 day span were
posted in that hour. This allows us to see which time
periods were the most active for that region. Indeed,
we see that the spikes occur in geographical order
of the hurricane?s path, from the South, to the Mid-
Atlantic region, and finally New England. Addition-
ally, Figure 1 is marked with data points indicating
which events were occurring at that time.
There are several obvious limitations of this ap-
proach (as explored in Hecht et al (2011)). For ex-
ample, users may enter false location information,
have an outdated profile, or may be posting mes-
sages from a different location. Assuming these is-
sues introduce no systemic bias, aggregate analyses
should not be significantly impacted (as supported
by the observed trends in Figure 1).
Inferring Gender
To determine the gender of the message sender, we
process the name field from the user?s profile ob-
tained from the Twitter API. The U.S. Census Bu-
reau provides a list of the most popular male and
female names in the United States. The lists con-
tain over 1,000 of the most common male names
and over 4,000 of the most common female names.
After removing names that can be either male or fe-
male (for example, Chris or Dana), we match the
first name of the user to the list of names obtained
from the census. Users that cannot be classified in
such a manner are labeled as unsure. The data con-
tains a total of 60,808 distinct users, of which 46%
are assigned a gender (of those, 55% are female,
45%male). We find that many of the unlabeled users
are news agencies. A similar methodology is used by
Mislove et al (2011). As with geographic inference,
29
Total Sample 8/18/2011-8/31/2011 25,253,444
Matching Irene Keywords 65,062
Female-indicative names 16,326
Male-indicative names 13,597
Mid-Atlantic states 8,665
Upper-South states 4,632
New England states 2,424
Table 1: Number of messages in sample for each filter.
we make no attempt to model any errors introduced
by this process (e.g., users providing false names).
Table 1 displays statistics of the overall dataset. A
sample of 100 messages revealed no misattributed
location or gender information.
Sentiment Classification
In this section, we describe experiments applying
sentiment classification to assess the level of con-
cern of each message. Our goal is not to investigate
new sentiment classification techniques, but instead
to determine whether existing, well-known methods
are applicable to this domain. While there is an ex-
tensive literature in sentiment classification technol-
ogy (Pang and Lee, 2008), binary classification us-
ing a bag-of-words assumption has been shown to
provide a strong baseline, so that is the approach we
use here. We also evaluate the impact of lexicons and
tokenization strategies.
We define ?concerned? messages to be those
showing some degree of apprehension, fear, or gen-
eral concern regarding Hurricane Irene. Examples of
unconcerned messages include links to news reports
or messages expressing an explicit lack of concern.
The idea is to assess how seriously a particular group
is reacting to an impeding disaster.
To train the classifier, we sample 408 messages
from the 66,205 message collection and manually
annotate them as concerned or unconcerned. The fi-
nal training set contains 170 concerned messages.
Examples are shown in Table 2. To estimate inter-
annotator agreement, we had a second annotator
sample 100 labeled messages (50 concerned, 50
unconcerned) for re-annotation. The inter-annotator
agreement is 93% (Cohen?s kappa ? = .86).
Examples of concerned messages
wonderful, praying tht this hurricane goes
back out to sea.
Im actually scared for this hurricane...
This hurricane is freaking me out.
hope everyone is #safe during #irene
Examples of unconcerned messages
for the very latest on hurricane irene
like our fb page ...
am i the only one who doesn?t give a
shit about this hurricane??
tropical storm irene?s track threatens
south florida - miamiherald.com
Table 2: Examples of concerned and unconcerned mes-
sages from the training set.
Tokenization and features
We train a simple bag-of-words classifier, where the
basic feature set is the list of word frequencies in
each message. Given the brevity and informality
of Twitter messages, tokenization choices can have
a significant impact on classification accuracy. We
consider two alternatives:
? Tokenizer0: The tokenizer of O?Connor et
al. (2010b), which does very little normaliza-
tion. Punctuation is preserved (for the purpose
of identifying semantics such as emoticons),
URLs remain intact, and text is lower-cased.
? Tokenizer1: A simple tokenizer that removes
all punctuation and converts to lowercase.
We also consider two feature pruning options:
? Stop Words: Remove words matching a list of
524 common English words.
? Frequency Pruning: Remove words occurring
fewer than 2 times in the labeled data.
We also consider the following features:
? Worry lexicon: We heuristically create a small
lexicon containing words expressing worry of
some kind, based on a brief review of the data.3
We replace all such tokens with a WORRIED
feature.
3The words are afraid, anxiety, cautious, die, died, nervous,
pray, prayers, prayin, praying, safe, safety, scared, scary, terri-
fied, thoughts, worried, worry, worrying
30
Classifier Acc Pr Re F1
MaxEnt 84.27 ? 2.0 90.15 70.00 78.81
Dec. Tree 81.35 ? 1.8 79.72 67.06 72.84
Naive Bayes 78.63 ? 2.2 75.78 71.76 73.72
Worry Lex. 79.41 95.74 52.94 68.18
Table 3: Average accuracy (with standard error) and
micro-averaged precision, recall, and F1 for the three sen-
timent classifiers, using their best configurations. The dif-
ference in accuracy between MaxEnt and the other clas-
sifiers is statistically significant (paired t-test, p < 0.01).
? Humor lexicon: Similarly, we create a small
lexicon containing words expressing humor.4
We replace all such tokens with a HUMOR
feature.
? Emoticon: Two common emoticons ?:)? and
?:(? are detected (prior to tokenization in the
case of Tokenizer 1).
Finally, we consider three classifiers: MaxEnt
(i.e., logistic regression), Naive Bayes, and a De-
cision Tree (ID3) classifier, as implemented in
the MALLET machine learning toolkit (McCallum,
2002). We use all the default settings, except we set
the maximum decision tree depth to 50 (after pre-
liminary results suggested that the default size of 4
was too small).
Enumerating the possible tokenization, features,
and classifier choices results in 192 possible sys-
tem configurations. For each configuration, 10-fold
cross-validation is performed on the labeled training
data. Table 3 reports the results for each classifier
using its best configuration. The configuration To-
kenizer1/Remove Stop Words/Freq. Pruning/Worry
lexicon/Humor lexicon/Emoticons was the best con-
figuration for both MaxEnt and Naive Bayes. Deci-
sion Tree differed only in that its best configuration
did not use Frequency Pruning. Table 3 also com-
pares to a simple baseline that classifies messages
as concerned if they contain any of the words in the
worry lexicon (while accuracy is competitive, recall
is quite low).
MaxEnt exhibits the best accuracy, precision, and
F1; Naive Bayes has slightly better recall. Table 4
provides a summary of the numerical impact each
4The words are lol, lmao, rofl, rotfl, ha, haha.
System Configuration Avg Acc Max Acc
Tokenizer0 77.78 81.10
Tokenizer1 80.59 84.27
Keep Stop Words 77.99 81.34
Remove Stop Words 80.38 84.27
No Freq. Pruning 79.67 83.29
Freq. Pruning 78.71 84.27
No Worry lexicon 77.62 81.82
Worry lexicon 80.76 84.27
No Humor Lexicon 79.15 83.78
Humor Lexicon 79.23 84.27
No Emoticons 79.26 84.27
Emoticons 79.11 84.27
Table 4: Summary of the impact of various tokenization
and feature choices. The second and third columns list the
average and maximum accuracy over all possible system
configurations with that setting. All results use the Max-
Ent classifier and 10-fold cross-validation. Tokenizer1,
Remove Stop Words, and Worry Lexicon result in the
largest improvements in accuracy.
configuration choice has. Using MaxEnt, we com-
pute the accuracy over every possible system config-
uration, then average the accuracies to obtain each
row. Thus, the Tokenizer1 row reports the average
accuracy over all configurations that use Tokenizer1.
Additionally, we report the highest accuracy of any
configuration using that setting. These results in-
dicate that Tokenizer1, Remove Stop Words, and
Worry Lexicon result in the largest accuracy gains.
Thus, while some unsupervised learning research
has suggested that only light normalization should
be used for social media text analysis (O?Connor et
al., 2010b), for this supervised learning task it ap-
pears that more aggressive normalization and feature
pruning can improve accuracy.
We select the best performing MaxEnt classifier
for use in subsequent experiments. First we retrain
the classifier on all the labeled data, then use it
to label all of the unlabeled data from the original
65,062 messages. To estimate performance on this
new data, we sample 200 additional documents of
this testing data and manually label them (35 posi-
tive, 165 negative). We find that the automated clas-
sifications are accurate in 86% of these documents.
Many of the remaining errors appear to be diffi-
cult cases. For example, consider the message: ?1st
an earthquake, now a hurricane? Damn NY do you
31
miss me that bad?? The classifier labels this as con-
cerned, but the message is likely intended to be hu-
morous. In another message (?#PrayForNYC and
everyone that will experience Hurricane Irene?), a
hashtag #PrayForNYC complicates tokenization, so
the word ?pray? (highly correlated with concern) is
not detected, resulting in a false negative.
Demographic Analysis
We next apply this classifier to assess the demo-
graphic determinants of concerned messages. By
classifying all remaining messages, we can analyze
trends in sentiment over time by gender and region.
Figure 2 displays the total number of messages
by day as well as the subset (and percentage) that
are classified as concerned. Consulting the timeline
in Figure 1, we see that the peak volume occurs on
August 27th, the day the eye of the hurricane makes
landfall. The percentage of messages labeled as con-
cerned actually peaks a day earlier, on August 26th.
Geographic Analysis
We first make several observations concerning Fig-
ure 1, which does not use the sentiment classifier,
but only displays message volume. There appears to
be a regional difference in when message volume
peaks. Data point C in the figure, which marks the
time around 2pm on August 23rd, represents the first
noticeable spike in message count, particularly in the
Upper South region. Two important events were oc-
curring around this time period. First, the strongest
earthquake to hit the Eastern United States since
WWII (measured as 5.8 on the Richter scale) oc-
curs near Richmond, Virginia. Also on August 23rd,
a few hours prior to the earthquake, FEMA holds a
press conference regarding the impeding threat that
Hurricane Irene will pose to East Coast states. It
appears likely that the combination of these events
leads to the increase in messages on August 23rd
as revealed in the figure. In fact, in examining some
of the messages posted on Twitter during that time
period, we notice some people commenting on the
unlikeliness that two natural disasters would hit the
region in such a narrow time frame.
Also in Figure 1, we see that the frequency of
Twitter messages relating to Hurricane Irene for
each region increases greatly over roughly the pe-
0?
5?
10?
15?
20?
25?
0?
5000?
10000?
15000?
20000?
25000?
Aug
?21?
Aug
?22?
Aug
?23?
Aug
?24?
Aug
?25?
Aug
?26?
Aug
?27?
Aug
?28?
Aug
?29?
Aug
?30?
Aug
?31?
%?co
ncer
ned
?
#?me
ssag
es?
Irene?Messages? Concerned?Irene?Messages? %?Concerned?
Figure 2: Total number of Twitter messages related to
Hurricane Irene, as well as the count and percentage clas-
sified as concerned by the sentiment classifier.
riod of August 25th to August 28th, before decreas-
ing later on August 28th and beyond. The increase
and decrease roughly parallel the approach of Hurri-
cane Irene toward and then beyond each region. Data
point I represents the time (August 27th at 8am)
when the center of Hurricane Irene makes landfall
on the North Carolina coast. This point represents
the highest message count for the Upper South re-
gion. Later on August 27th, as the hurricane moves
north toward New Jersey and then New York, we
see the peak message count for the Middle States
region (Data point K). Finally, on August 28th in
the late morning, as Hurricane Irene moves into the
New England region, we see that the New England
regions peak message count occurs (Data Point L).
With the sentiment classifier from the previous
section, we can perform a more detailed analysis
of the regional differences than can be performed
using message volume alone. Figure 3 applies the
sentiment classifier to assess the proportion of mes-
sages from each region that express concern. Figure
3 (top) shows the raw percentage of messages from
each region by day, while the bottom figure shows
the proportion of messages from each region that ex-
press concern. While the New England region has
the lowest volume of messages, on many days it has
the highest proportion of concerned messages.
Comparing regional differences in aggregate
across all 10 days would be misleading ? after the
hurricane passes a region, it is expected that the level
of concern should decrease. Indeed, these aggregate
regional differences are not statistically significant
(NE=15.59%, MID=15.4%, SOUTH=15.69%). In-
stead, for each day we compare the levels of concern
32
0?2?
4?6?
8?10?
12?14?
16?
Aug
?21?
Aug
?22?
Aug
?23?
Aug
?24?
Aug
?25?
Aug
?26?
Aug
?27?
Aug
?28?
Aug
?29?
Aug
?30?
Aug
?31?
%?m
ess
age
s?
NE?
MID?
SOUTH?
0?
5?
10?
15?
20?
25?
30?
Aug
?22?
Aug
?23?
Aug
?24?
Aug
?25?
Aug
?26?
Aug
?27?
Aug
?28?
Aug
?29?
Aug
?30?
%?c
onc
ern
ed?
NE?
MID?
SOUTH?
Figure 3: Message proportion and percent classified as
concerned by the sentiment classifier, by region.
for each region, testing for significance using a Chi-
squared test. Two days show significant differences:
August 25 and August 27. On both days, the propor-
tion of concerned messages in New England is sig-
nificantly higher (p < 0.05) than that of the South-
ern region (August 25: NE=21.6%, SOUTH=14.6%;
August 26: NE=18.5%, SOUTH=15.1%). It is diffi-
cult to directly attribute causes to these differences,
although on August 25, a Hurricane Watch was is-
sued for the New England area, and on August 27
that Watch was upgraded to a Warning. It is also
possible that states that experience hurricanes more
frequently express lower levels of concern. Further
sociological research is necessary to fully address
these differences.
Gender Analysis
We apply a similar analysis to assess the differ-
ences in levels of concern by gender. Figure 4 shows
that for roughly the period between August 24th
and August 29th, messages written by females are
more likely to express concern than those written
by males. Over the entire period, 18.7% of female-
authored messages are labeled as concerned, while
over the same period 13.9% of male-authored mes-
sages are labeled as concerned. We perform a Chi-
0?
5?
10?
15?
20?
25?
30?
Aug
?21?
Aug
?22?
Aug
?23?
Aug
?24?
Aug
?25?
Aug
?26?
Aug
?27?
Aug
?28?
Aug
?29?
Aug
?30?
Aug
?31?
%?m
ess
age
s?
male?
female?
0?
5?
10?
15?
20?
25?
Aug
?21?
Aug
?22?
Aug
?23?
Aug
?24?
Aug
?25?
Aug
?26?
Aug
?27?
Aug
?28?
Aug
?29?
Aug
?30?
Aug
?31?
%?c
onc
ern
ed? male?
female?
Figure 4: Message proportion and percent classified as
concerned by the sentiment classifier, by gender.
squared test over the entire period, and find that gen-
der differences in concern are significant (p < .01).
We conclude that messages attributed to female au-
thors are significantly more likely to be classified as
concerned than messages authored by males.
In order to assess a possible gender bias in our
classifier, we examine the proportion of concern for
males and females in the labeled training set. We
find that of the original 408 labeled messages, 69
are from males, 112 are from females, and 227 can-
not be determined. 24 male messages, or 34.8%, are
marked as concerned. In contrast, 57 female mes-
sages, or 50.9%, are marked as concerned. 88 of the
undetermined gender messages, or 38.9%, are con-
cerned. We therefore down-sample the female mes-
sages from our labeled training set until the propor-
tion of female-concerned messages matches that of
male-concerned messages. Repeating our classifica-
tion experiments shows no significant difference in
the relative proportions of messages labeled as con-
cerned by gender. We therefore conclude that the
training set is not injecting a gender bias in the clas-
sifier.
33
Female: i my safe praying this everyone died jada
butistillloveu brenda who love t me thank school pets
retweet respects all please here so stay
neverapologizefor wine sleep rainbow prayers lord
Male: http co de en el hurac media breaking la
rooftoproofing track obama jimnorton gay ron blames
smem change seattle orkaan becomes disaster zona zan
lean vivo por es location dolphin
New England: boston MAirene ct vt ri england sunday
connecticut malloy ma vermont tropical maine wtnh
massachusetts haven rhode VTirene va power
CThurricane cambridge mass lls gilsimmons
mbta gunna storm slut NHirene
Middle States: nyc ny nj nycmayorsoffice york jersey
mta brooklyn zone nytmetro va ryan philly shut
dc mayor city manhattan lls new subways con
team longisland bloomberg evacuation evacuate
yorkers catskills queens
South: nc dc va lls earthquake raleigh maryland
dmv ncwx virginia ncirene richmond isabelle perdue
isabel mdhurricane bout carolina capitalweather sniper
rva norfolk goin feeds nycmayorsoffice baltimore ilm
mema tho aint
Table 5: Top 30 words for each demographic ranked by
Information Gain.
Qualitative Analysis
In Table 5 we provide a brief qualitative analy-
sis by displaying the top 30 words for each demo-
graphic obtained using Information Gain (Manning
and Schtze, 1999), a method of detecting features
that discriminate between document classes. To pro-
vide some of the missing context: ?jada? refers to
the divorce of celebrities Will Smith and Jada Pin-
kett; ?hurac? refers to the Spanish word Huraca?n;
?smem? stands for Social Media for Emergency
Management; ?dolphin? refers to a joke that was cir-
culated referencing the hurricane; ?lls? is an abbre-
viation for ?laughing like shit?.
Some broad trends appear: male users tend to ref-
erence news, politics, or jokes; the Middle States
reference the evacuation of New York City, and the
South refers back to other disasters (the earthquake,
the sniper attacks of 2002, Hurricane Isabel).
Related Work
Recent research has investigated the effectiveness of
social media for crisis communication (Savelyev et
al., 2011) ? indeed, the U.S. Federal Emergency
Management Agency now uses Twitter to dissem-
inate information during natural disasters (Kalish,
2011). Other work has examined the spread of
false rumors during earthquakes (Mendoza et al,
2010) and tsunamis (Acar and Muraki, 2011) and
characterized social network dynamics during floods
(Cheong and Cheong, 2011), fires (Vieweg et al,
2010), and violence (Heverin and Zach, 2010).
While some of this past research organizes messages
by topic, to our knowledge no work has analyzed
disaster sentiment or its demographic determinants.
Survey research by West and Orr (2007) con-
cluded that women may feel more vulnerable dur-
ing hurricanes because they are more likely to have
children and belong to a lower socio-economic class.
Richer people, they find, tend to have an easier time
dealing with natural disasters like hurricanes. These
reasons might explain our finding that women are
more likely on Twitter to show concern than men
about Hurricane Irene. West and Orr also find dif-
ferences in regional perceptions of vulnerability be-
tween coastal areas and non-coastal areas. Our loca-
tion annotation must be more precise before we can
perform a similar analysis.
More generally, our approach can be considered
a type of computational social science, an emerging
area of study applying computer science algorithms
to social science research (Lazer et al, 2009; Hop-
kins and King, 2010).
Conclusion and Future Work
Our results show that analyzing Twitter messages
relating to Hurricane Irene reveals differences in
sentiment depending on a person?s gender or loca-
tion. We conclude that social media analysis is a vi-
able complement to existing survey methodologies,
providing real-time insight into public perceptions
of a disaster. Future directions include investigating
how to account for classifier error in hypothesis test-
ing (Fuller, 1987), adjusting classification propor-
tions using quantification methods (Forman, 2007),
as well as applying the approach to different disas-
ters and identifying additional sentiment classes of
interest. Finally, it will be important to infer a greater
variety of demographic attributes and also to adjust
for the demographic bias inherent in social media.
34
References
Adam Acar and Yuya Muraki. 2011. Twitter for crisis
communication: lessons learned from Japan?s tsunami
disaster. International Journal of Web Based Commu-
nities, 7(3):392?402.
S. Asur and B. A. Huberman. 2010. Predicting the future
with social media. In Proceedings of the ACM Inter-
national Conference on Web Intelligence.
France Cheong and Christopher Cheong. 2011. So-
cial media data mining: A social network analysis of
tweets during the 2010?2011 Australian floods. In
PACIS 2011 Proceedings.
Aron Culotta. 2010. Towards detecting influenza epi-
demics by analyzing Twitter messages. In Workshop
on Social Media Analytics at the 16th ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing.
Aron Culotta. 2012. Lightweight methods to estimate
influenza rates and alcohol sales volume from Twitter
messages. Language Resources and Evaluation, Spe-
cial Issue on Analysis of Short Texts on the Web. to
appear.
Elaine Enarson. 1998. Through women?s eyes: A gen-
dered research agenda for disaster social science. Dis-
asters, 22(2):157?73.
George Forman. 2007. Quantifying counts, costs, and
trends accurately via machine learning. Technical re-
port, HP Laboratories, Palo Alto, CA.
A. Fothergill, E.G. Maestas, and J.D. Darlington. 1999.
Race, ethnicity and disasters in the united states: A re-
view of the literature. Disasters, 23(2):156?73, Jun.
W.A. Fuller. 1987. Measurement error models. Wiley,
New York.
Eric Gilbert and Karrie Karahalios. 2010. Widespread
worry and the stock market. In Proceedings of the 4th
International AAAI Conference on Weblogs and Social
Media, Washington, D.C., May.
J.D. Goltz, L.A. Russell, and L.B. Bourque. 1992. Initial
behavioral response to a rapid onset disaster: A case
study. International Journal of Mass Emergencies and
Disasters, 10(1):43?69.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi.
2011. Tweets from justin bieber?s heart: the dynamics
of the location field in user profiles. In Proceedings of
the 2011 annual conference on Human factors in com-
puting systems, CHI ?11, pages 237?246, New York,
NY, USA.
T. Heverin and L. Zach. 2010. Microblogging for
crisis communication: Examination of Twitter use in
response to a 2009 violent crisis in Seattle-Tacoma,
Washington area. In Proceedings of the Seventh Inter-
national Information Systems for Crisis Response and
Management Conference, Seattle, WA.
Daniel J. Hopkins and Gary King. 2010. A method
of automated nonparametric content analysis for so-
cial science. American Journal of Political Science,
54(1):229?247.
Brian Kalish. 2011. FEMA will use social media
through all stages of a disaster. Next Gov, February.
Vasileios Lampos and Nello Cristianini. 2010. Tracking
the flu pandemic by monitoring the social web. In 2nd
IAPR Workshop on Cognitive Information Processing
(CIP 2010), pages 411?416.
David Lazer, Alex Pentland, Lada Adamic, Sinan
Aral, Albert-Laszlo Barabasi, Devon Brewer, Nicholas
Christakis, Noshir Contractor, James Fowler, Myron
Gutmann, Tony Jebara, Gary King, Michael Macy,
Deb Roy, and Marshall Van Alstyne. 2009. Computa-
tional social science. Science, 323(5915):721?723.
Chris Manning and Hinrich Schtze. 1999. Founda-
tions of Statistical Natural Language Processing. MIT
Press, Cambridge, MA, May.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Marcelo Mendoza, Barbara Poblete, and Carlos Castillo.
2010. Twitter under crisis: Can we trust what we RT?
In 1st Workshop on Social Media Analytics (SOMA
?10), July.
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-
Pekka Onnela, , and J. Niels Rosenquist. 2011. Un-
derstanding the demographics of twitter users. In
Proceedings of the Fifth International AAAI Con-
ference on Weblogs and Social Media (ICWSM?11),
Barcelona, Spain.
James Murphy, P. Allen, Thomas Stevens, and Darryl
Weatherhead. 2005. A meta-analysis of hypothetical
bias in stated preference valuation. Environmental and
Resource Economics, 30(3):313?325.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From Tweets to polls: Linking text sentiment to public
opinion time series. In International AAAI Conference
on Weblogs and Social Media, Washington, D.C.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. Tweetmotif: Exploratory search and topic sum-
marization for twitter. In ICWSM.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1?2):1?135.
R.W. Perry and M.K. Lindell. 1991. The effects of eth-
nicity on decision-making. International journal of
mass emergencies and disasters, 9(1):47?68.
R.W. Perry and A.H. Mushkatel. 1986. Minority citizens
in disasters. University of Georgia Press, Athens, GA.
35
Alexander Savelyev, Justine Blanford, and Prasenjit Mi-
tra. 2011. Geo-twitter analytics: Applications in cri-
sis management. In 25th International Cartographic
Conference, pages 1?8.
Sarah Vieweg, Amanda L. Hughes, Kate Starbird, and
Leysia Palen. 2010. Microblogging during two nat-
ural hazards events: what twitter may contribute to sit-
uational awareness. In Proceedings of the 28th inter-
national conference on Human factors in computing
systems, pages 1079?1088, New York, NY, USA.
Darrell M. West and Marion Orr. 2007. Race, gender,
and communications in natural disasters. The Policy
Studies Journal, 35(4).
Appendix A: Location String Matching
The following strings were matched against the user
location field of each message to determine the loca-
tion of the message. Matches were case insensitive,
except for abbreviations (e.g., VT must be capital-
ized to match).
Vermont, VT, Maine, ME, New Hampshire,
Rhode Island, RI, Delaware, DE, Connecticut, CT,
Maryland, MD, Baltimore, North Carolina, NC,
Massachusetts, MA, Boston, Mass, W Virginia,
West Virginia, Virginia, VA, RVA, DC, D.C., PA,
Philadelphia, Pittsburgh, Philly, New Jersey, At-
lantic City, New York, NY, NYC, Long Island, Man-
hattan, Brooklyn, Staten Island, The Bronx, Queens,
NY, N.Y.
36
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 7?16,
Baltimore, Maryland USA, 27 June 2014.
c?2014 Association for Computational Linguistics
Using county demographics to infer attributes of Twitter users
Ehsan Mohammady and Aron Culotta
Department of Computer Science
Illinois Institute of Technology
Chicago, IL 60616
emohamm1@hawk.iit.edu, culotta@cs.iit.edu
Abstract
Social media are increasingly being used
to complement traditional survey methods
in health, politics, and marketing. How-
ever, little has been done to adjust for the
sampling bias inherent in this approach.
Inferring demographic attributes of social
media users is thus a critical step to im-
proving the validity of such studies. While
there have been a number of supervised
machine learning approaches to this prob-
lem, these rely on a training set of users
annotated with attributes, which can be
difficult to obtain. We instead propose
training a demographic attribute classi-
fiers that uses county-level supervision.
By pairing geolocated social media with
county demographics, we build a regres-
sion model mapping text to demographics.
We then adopt this model to make predic-
tions at the user level. Our experiments
using Twitter data show that this approach
is surprisingly competitive with a fully su-
pervised approach, estimating the race of
a user with 80% accuracy.
1 Introduction
Researchers are increasingly using social media
analysis to complement traditional survey meth-
ods in areas such as public health (Dredze, 2012),
politics (O?Connor et al., 2010), and market-
ing (Gopinath et al., 2014). It is generally ac-
cepted that social media users are not a representa-
tive sample of the population (e.g., urban and mi-
nority populations tend to be overrepresented on
Twitter (Mislove et al., 2011)). Nevertheless, few
researchers have attempted to adjust for this bias.
(Gayo-Avello (2011) is an exception.) This can
in part be explained by the difficulty of obtaining
demographic information of social media users
? while gender can sometimes be inferred from
the user?s name, other attributes such as age and
race/ethnicity are more difficult to deduce. This
problem of user attribute prediction is thus crit-
ical to such applications of social media analysis.
A common approach to user attribute prediction
is supervised classification ? from a training set
of annotated users, a model is fit to predict user at-
tributes from the content of their writings and their
social connections (Argamon et al., 2005; Schler
et al., 2006; Rao et al., 2010; Pennacchiotti and
Popescu, 2011; Burger et al., 2011; Rao et al.,
2011; Al Zamal et al., 2012). Because collecting
human annotations is costly and error-prone, la-
beled data are often collected serendipitously; for
example, Al Zamal et al. (2012) collect age anno-
tations by searching for tweets with phrases such
as ?Happy 21st birthday to me?; Pennacchiotti and
Popescu (2011) collect race annotations by search-
ing for profiles with explicit self identification
(e.g., ?I am a black lawyer from Sacramento.?).
While convenient, such an approach likely suffer
from selection bias (Liu and Ruths, 2013).
In this paper, we propose fitting classification
models on population-level data, then applying
them to predict user attributes. Specifically, we
fit regression models to predict the race distribu-
tion of 100 U.S. counties (based on Census data)
from geolocated Twitter messages. We then ex-
tend this learned model to predict user-level at-
tributes. This lightly supervised approach reduces
the need for human annotation, which is important
not only because of the reduction of human effort,
but also because many other attributes may be dif-
ficult even for humans to annotate at the user-level
(e.g., health status, political orientation). We in-
vestigate this new approach through the following
three research questions:
RQ1. Can models trained on county statistics
be used to infer user attributes? We find
that a classifier trained on county statis-
7
tics can make accurate predictions at the
user level. Accuracy is slightly lower (by
less than 1%) than a fully supervised ap-
proach using logistic regression trained on
hundreds of labeled instances.
RQ2. How do models trained on county data
differ from those using standard super-
vised methods? We analyze the highly-
weighted features of competing models,
and find that while both models discern lex-
ical differences (e.g., slang, word choice),
the county-based model also learns geo-
graphical correlates of race (e.g., city, state).
RQ3. What bias does serendipitously labeled
data introduce? By comparing training
datasets collected uniformly at random with
those collected by searching for certain key-
words, we find that the search approach pro-
duces a very biased class distribution. Addi-
tionally, the classifier trained on such biased
data tends to overweight features matching
the original search keywords.
2 Related Work
Predicting attributes of social media users is a
growing area of interest, with recent work focus-
ing on age (Schler et al., 2006; Rosenthal and
McKeown, 2011; Nguyen et al., 2011; Al Zamal
et al., 2012), sex (Rao et al., 2010; Burger et al.,
2011; Liu and Ruths, 2013), race/ethnicity (Pen-
nacchiotti and Popescu, 2011; Rao et al., 2011),
and personality (Argamon et al., 2005; Schwartz
et al., 2013b). Other work predicts demographics
from web browsing histories (Goel et al., 2012).
The majority of these approaches rely on hand-
annotated training data, require explicit self-
identification by the user, or are limited to very
coarse attribute values (e.g., above or below 25-
years-old). Pennacchiotti and Popescu (2011)
train a supervised classifier to predict whether a
Twitter user is African-American or not based
on linguistic and social features. To construct
a labeled training set, they collect 6,000 Twitter
accounts in which the user description matches
phrases like ?I am a 20 year old African-
American.? In our experiments below, we demon-
strate how such serendipitously labeled data can
introduce selection bias in the estimate of clas-
sification accuracy. Their final classifier obtains
a 65.5% F1 measure on this binary classification
task (compared with the 76.5% F1 we report be-
low for a different dataset labeled with four race
categories).
A related lightly supervised approach includes
Chang et al. (2010), who infer user-level eth-
nicity using name/ethnicity distributions provided
by the Census; however, that approach uses evi-
dence from first and last names, which are often
not available, and thus are more appropriate for
population-level estimates. Rao et al. (2011) ex-
tend this approach to also include evidence from
other linguistic features to infer gender and ethnic-
ity of Facebook users; they evaluate on the fine-
grained ethnicity classes of Nigeria and use very
limited training data.
Viewed as a way to make individual inferences
from aggregate data, our approach is related to
ecological inference (King, 1997); however, here
we have the advantage of user-level observations
(linguistic data), which are typically absent in eco-
logical inference settings.
There have been several studies predicting
population-level statistics from social media.
Eisenstein et al. (2011) use geolocated tweets to
predict zip-code statistics of race/ethnicity, in-
come, and other variables using Census data;
Schwartz et al. (2013b) and Culotta (2014) simi-
larly predict county health statistics from Twitter.
However, none of this prior work attempts to pre-
dict or evaluate at the user level.
Schwartz et al. (2013a) collect Facebook pro-
files labeled with personality type, gender, and age
by administering a survey of users embedded in a
personality test application. While this approach
was able to collect over 75K labeled profiles, it
can be difficult to reproduce, and is also challeng-
ing to update over time without re-administering
the survey.
Compared to this related work, our core con-
tribution is to propose and evaluate a classifier
trained only on county statistics to estimate the
race of a Twitter user. The resulting accuracy
is competitive with a fully supervised baseline as
well as with prior work. By avoiding the use of la-
beled data, the method is simple to train and easier
to update as linguistic patterns evolve over time.
3 Methods
Our approach to user attribute prediction is as fol-
lows: First, we collect population-level statistics,
for example the racial makeup of a county. Sec-
8
ond, we collect a sample of tweets from the same
population areas and distill them into one fea-
ture vector per location. Third, we fit a regres-
sion model to predict the population-level statis-
tics from the linguistic feature vector. Finally, we
adapt the regression coefficients to predict the at-
tributes of individual Twitter user. Below, we de-
scribe the data, the regression and classification
models, and the experimental setup.
3.1 Data
We collect three types of data: (1) Census data,
listing the racial makeup of U.S. Counties; (2)
geolocated Twitter data from each county; (3) a
validation set of Twitter users manually annotated
with race, for evaluation purposes.
3.1.1 Census Data
The U.S. Census produces annual estimates of
the race and Hispanic origin proportions for each
county in the United States. These estimates are
derived using the most recent decennial census and
estimates of population changes (deaths, birth, mi-
gration) since that census. The census question-
naire allows respondents to select one or more of 6
racial categories: White, Black or African Ameri-
can, American Indian and Alaska Native, Asian,
Native Hawaiian and Other Pacific Islander, or
Other. Additionally, each respondent is asked
whether they consider themselves to be of His-
panic, Latino, or Spanish origin (ethnicity). Since
respondents may select multiple races in addition
to ethnicity, the Census reports many different
combinations of results.
While race/ethnicity is indeed a complex is-
sue, for the purposes of this study we simplify
by considering only four categories: Asian, Black,
Latino, White. (For simplicity, we ignore the Cen-
sus? distinction between race and ethnicity; due
to small proportions, we also omit Other, Amer-
ican Indian/Alaska Native, and Native Hawaiian
and Other Pacific Islander.) For the three cate-
gories other than Latino, we collect the proportion
of each county for that race, possibly in combina-
tions with others. For example, the percentage of
Asians in a county corresponds to the Census cat-
egory: ?NHAAC: Not Hispanic, Asian alone or in
combination.? The Latino proportion corresponds
to the ?H? category, indicating the percentage of
a county identifying themselves as of Hispanic,
Latino, or Spanish origin (our terminology again
ignores the distinction between the terms ?Latino?
and ?Hispanic?). We use the 2012 estimates for
this study.
1
We collect the proportion of residents
from each of these four categories for the 100 most
populous counties in the U.S.
3.1.2 Twitter County Data
For each of the 100 most populous counties in
the U.S., we identify its geographical coordinates
(from the U.S. Census), and construct a geograph-
ical Twitter query (bounding box) consisting of a
50 square mile area centered at the county coordi-
nates. This approximation introduces a very small
amount of noise ? less than .02% of tweets come
from areas of overlapping bounding boxes.
2
We
submit each of these 100 queries in turn from De-
cember 5, 2012 to November 14, 2013. These
geographical queries return tweets that carry ge-
ographical coordinates, typically those sent from
mobile devices with this preference enabled.
3
This
resulted in 5.7M tweets from 839K unique users.
3.1.3 Validation Data
Uniform Data: For validation purposes, we cate-
gorized 770 Twitter profiles into one of four cate-
gories (Asian, Black, Latino, White). These were
collected as follows: First, we used the Twit-
ter Streaming API to obtain a random sample of
users, filtered to the United States (using time
zone and the place country code from the pro-
file). From six days? worth of data (December
6-12, 2013), we sampled 1,000 profiles at ran-
dom and categorized them by analyzing the pro-
file, tweets, and profile image for each user. Those
for which race could not be determined were dis-
carded (230/1,000; 23%).
4
The category fre-
quency is Asian (22), Black (263), Latino (158),
White (327). To estimate inter-annotator agree-
ment, a second annotator sampled and categorized
120 users. Among users for which both annota-
tors selected one of the four categories, 74/76 la-
bels agreed (97%). There was some disagreement
over when the category could be determined: for
1
http://www.census.gov/popest/
data/counties/asrh/2012/files/
CC-EST2012-ALLDATA.csv
2
The Census also publishes polygon data for each county,
which could be used to remove this small source of noise.
3
Only considering geolocated tweets introduces some
bias into the types of tweets observed. However, we com-
pared the unigram frequency vectors from geolocated tweets
with a sample of non-geolocated tweets and found a strong
correlation (0.93).
4
This introduces some bias towards accounts with identi-
fiable race; we leave an investigation of this for future work.
9
21/120 labels (17.5%), one annotator indicated the
category could not be determined, while the other
selected a category. For each user, we collected
their 200 most recent tweets using the Twitter API.
We refer to this as the Uniform dataset.
Search Data: It is common in prior work
to search for keywords indicating user attributes,
rather than sampling uniformly at random and then
labeling (Pennacchiotti and Popescu, 2011; Al Za-
mal et al., 2012). This is typically done for con-
venience; a large number of annotations can be
collected with little or no manual annotation. We
hypothesize that this approach results in a biased
sample of users, since it is restricted to those with
a predetermined set of keywords. This bias may
affect the estimate of the generalization accuracy
of the resulting classifier.
To investigate this, we used the Twitter Search
API to collect profiles containing a predefined set
of keywords indicating race. Examples include
the terms ?African?, ?Black?, ?Hispanic?, ?Latin?,
?Latino?, ?Spanish?, ?Chinese?, ?Italian?, ?Irish.?
Profiles containing such words in the description
field were collected. These were further filtered
in an attempt to remove businesses (e.g., Chinese
restaurants) by excluding profiles with the key-
words in the name field as well as those whose
name fields did not contain terms on the Census?
list of common first and last names. Remaining
profiles were then manually reviewed for accu-
racy. This resulted in 2,000 annotated users with
the following distribution: Asian (377), Black
(373), Latino (356), White (894). For each user,
we collected their 200 most recent tweets using the
Twitter API. We refer to this as the Search dataset.
Table 1 compares the race distribution for each
of the two datasets. It is apparent that the Search
dataset oversamples Asian users and undersam-
ples Black users as compared to the Uniform
dataset. This may in part due to the greater num-
ber of keywords used to identify Asian users (e.g.,
Chinese, Japanese, Korean). This highlights the
difficulty of obtaining a representative sample of
Twitter users with the search approach, since the
inclusion of a single keyword can result in a very
different distribution of labels.
3.2 Models
3.2.1 County Regression
We build a text regression model to predict the
racial makeup of a county (from the Census data)
Uniform Search
Asian 3% 19%
Black 34% 19%
Latino 21% 18%
White 42% 44%
Table 1: Percentage of users by race in the two
validation datasets.
based on the linguistic patterns in tweets from that
county. For each county, we create a feature vector
as follows: for each unigram, we compute the pro-
portion of users in the county who have used that
unigram. We also distinguish between unigrams in
the text of a tweet and a unigram in the description
field of the user?s profile. Thus, two sample fea-
ture values are (china, 0.1) and (desc china, 0.05),
indicating that 10% of users in the county wrote a
tweet containing the unigram china, and 5% have
the word china in their profile description. We ig-
nore mentions and collapse URLs (replacing them
with the token ?http?), but retain hashtags.
We fit four separate ridge regression models,
one per race.
5
For each model, the independent
variables are the unigram proportions from above;
the dependent variable is the percentage of each
county of a particular race. Ridge regression is
an L2 regularized form of linear regression, where
? determines the regularization strength, y
i
is a
vector of dependent variables for category i, X is
a matrix of independent variables, and ? are the
model parameters:
?
?
i
= argmin
?
||y
i
?X?
i
||
2
2
+ ?||?||
2
2
Thus, we have one parameter vector for each race
category
?
? = {
?
?
A
,
?
?
B
,
?
?
L
,
?
?
W
}. Related ap-
proaches have been used in prior work to estimate
county demographics and health statistics (Eisen-
stein et al., 2011; Schwartz et al., 2013b; Culotta,
2014).
Our core hypothesis is that the
?
? coefficients
learned above can be used to categorize individ-
ual users by race. We propose a very simple ap-
proach that simply treats
?
? as parameters of a lin-
ear classifier. For each user in the labeled dataset,
we construct a binary feature vector x using the
same unigram vocabulary from the county regres-
sion task. Then, we classify each user according to
5
Subsequent experiments with lasso, elastic net, and
multi-output elastic net performed no better.
10
the dot product between this binary feature vector
x and the parameter vector for each category:
y? = argmax
i
(
x ?
?
?
i
)
3.2.2 Baseline 1: Logistic Regression
For comparison, we also train a logistic regres-
sion classifier using the user-annotated data (either
Uniform or Search). We perform 10-fold classifi-
cation, using the same binary feature vectors de-
scribed above (preliminary results using term fre-
quency instead of binary vectors resulted in lower
accuracy). We again use L2 regularization, con-
trolled by tunable parameter ?.
3.2.3 Baseline 2: Name Heuristic
Inspired by the approach of Chang et al. (2010),
we collect Census data containing the frequency
of racial categories by last name. We use the top
1000 most popular last names with their race dis-
tribution from Census database. If the last name
in the user?s Twitter profile matches names on
this list, we categorize the user with the most
probable race according to the Census data. For
example, the Census indicates that 91% of peo-
ple with the last name Garcia identify themselves
as Latino/Hispanic. We would thus label Twit-
ter users with Garcia as a last name as Hispanic.
Users whose last names are not matched are cate-
gorized as White (the most common label).
3.3 Experiments
We performed experiments to estimate the accu-
racy of each approach, as well as how different
training sets affect performance. The systems are:
1. County: The county regression approach of
Section 3.2.1, trained only using county-level
supervision.
2. Uniform: A logistic regression classifier
trained on the Uniform dataset.
3. Search: A logistic regression classifier
trained on the Search dataset.
4. Name heuristic: The name heuristic of Sec-
tion 3.2.3.
We compare testing accuracy on both the Uni-
form dataset and Search datasets. For experiments
in which systems are trained and tested on the
same dataset, we report the average results of 10-
fold cross-validation.
Figure 1: Learning curve for the Uniform dataset.
The solid black line is the cross-validation accu-
racy of a logistic regression classifier trained using
increasingly more labeled examples.
Figure 2: Learning curve for the Search dataset.
The solid black line is the cross-validation accu-
racy of a logistic regression classifier trained using
increasingly more labeled examples.
We tune the ? regularization parameter for both
ridge and logistic regression, reporting the best
accuracy for each approach. Systems are imple-
mented in Python using the scikit-learn li-
brary (Pedregosa and others, 2011).
4 Results
Figure 1 plots cross-validation accuracy on the
Uniform dataset as the number of labeled exam-
ples increases. Surprisingly, the County model,
which uses no user-labeled data, performs only
slightly worse than the fully supervised approach
(81.7% versus 82.2%). This suggests that the lin-
guistic patterns learned from the county data can
11
PP
P
P
P
P
P
P
Train
Test
Search Uniform
Search 0.7715 0.8000
Uniform 0.5535 0.8221
County 0.5490 0.8169
Name heuristic 0.4955 0.4519
Table 2: Accuracy of each system.
P
P
P
P
P
P
P
P
Train
Test
Search Uniform
Search 0.7650 0.8074
Uniform 0.4721 0.8130
County 0.4738 0.8050
Name heuristic 0.3838 0.3178
Table 3: F1 of each system.
be transferred to make inferences at the user level.
Figure 1 also shows slightly lower accuracy
from training on the Search dataset and testing on
the Uniform dataset (80%). This may in part be
due to the different label distributions between the
datasets, as well as the different characteristics of
the linguistic patterns, discussed more below.
The Name heuristic does poorly overall, mainly
because few users provide their last names in their
profiles, and only a fraction of those names are on
the Census? name list.
Figure 2 plots the learning curve for the Search
dataset. Here, the County approach performs con-
siderably worse than logistic regression trained on
the Search data. However, the County approach
again performs comparable to the supervised Uni-
form approach. That is, training a supervised clas-
sifier on the Uniform dataset is only slightly more
accurate than training only using county supervi-
sion (54.9% versus 55.3%). By F1, county super-
vision does slightly better than the Uniform ap-
proach. This again highlights the very different
characteristics of the Uniform and Search datasets.
Importantly, if we remove features from the user
description field, then the cross-validation accu-
racy of the Search classifier is reduced from 77%
to 67%. Since a small set of keywords in the de-
scription field were used to collect the Search data,
the Search classifier simply recovers those key-
words, thus inflating its performance.
Tables 2-4 show the accuracy, F1, and precision
for each method (averaged over each class label).
The relative trends are the same for each metric.
The primary difference is the high precision of the
P
P
P
P
P
P
P
P
Train
Test
Search Uniform
Search 0.7909 0.8250
Uniform 0.6659 0.8155
County 0.4781 0.7967
Name heuristic 0.5897 0.6886
Table 4: Precision of each system.
P
P
P
P
P
P
P
P
Train
Test
County
Search 0.0190
Uniform 0.0361
County 0.0186
Name heuristic 0.0154
Table 5: Mean Squared Error of each system
on the task of predicting the racial makeup of a
county. Values are averages over the four race cat-
egories.
Name heuristic ? when users do provide a last
name on the Census list, this heuristic predicts the
correct race 69% of the time on the Uniform data,
and 59% of the time on the Search data.
We additionally compute how well the differ-
ent approaches predict the county demographics.
For the County method, we perform 10-fold cross-
validation, using the original county feature vec-
tors as independent variables. For the logistic re-
gression methods, we train the classifier on one of
the user datasets (Uniform or Search), then clas-
sify each user in the county dataset. These pre-
dictions are aggregated to compute the proportion
of each race per county. For the name heuristic,
we only consider users who match a name in the
Census list, and use the heuristic to compute the
proportion of users of each race.
Table 5 displays the mean squared error be-
tween the predicted and true race proportions, av-
eraged over all counties and races. The name
heuristic outperforms all other systems on this
task, in contrast to the previous results showing the
name heuristic is the least accurate predictor at the
user level. This is most likely because the name
heuristic can ignore many users without penalty
when predicting county proportions. The County
method does better than the Search or Uniform
methods, which is to be expected, since it was
trained specifically for this task. It is possible that
the Search and Uniform error can be reduced by
adjusting for quantification bias (Forman, 2008),
12
Black White Latino Asian
black white spanish asian
african italian latin asian
american irish hispanic filipino
black british spanish korean
the french latino chinese
african german de korean
young girl en japanese
smh boy el philippines
to own que vietnamese
male italian latin japanese
yall russian es filipino
niggas pretty la asians
woman fucking por japan
rip christmas latino chinese
man buying hispanic many
Table 6: Top-weighted features for the classifier
trained on the Search dataset. Terms from the de-
scription field are in italics.
though we do not investigate this here.
4.1 Analysis of top features
Tables 6-8 show the top 15 features for each sys-
tem, sorted by their corresponding model parame-
ters. In both our training and testing process, we
distinguish between words in the user description
field and words in tweets. We also include a fea-
ture that indicates whether the user has any text at
all in their profile description. In addition, we ig-
nore mentions but retain hashtags. In these tables,
words in description are shown in italics.
Because the Search dataset is collected by
matching description keywords, in Table 6 many
of these keywords are top-weighted features (e.g.,
?black?, ?white?, ?spanish?, ?asian?). However in
Table 7, there is no top feature word from the de-
scription. This observation shows how our search
dataset collection biases the resulting classifier.
The top features for the Uniform method (Ta-
ble 7) tend to represent lexical variations and slang
common among these groups. Interestingly, no
terms from the profile description are strongly
weighted, most likely a result of the uniform sam-
pling approach, which does not bias the data to
users with keywords in their profile.
For the County approach, it is less revealing
to simply report the features with the highest
weights. Since the regression models for each race
were fit independently, many of the top-weighted
Black White Latino Asian
ain makes pizza were
lmao please 3rd sorry
somebody seriously drunk bit
tryna guys ti hahaha
bout whenever gets ma
nigga snow el hurts
niggas pretty estoy keep
black literally self team
smh thing lucky aw
tf isn special food
lil such everywhere sad
been am sleep packed
real red la care
everybody glass chicken goodbye
gon sucks tried forever
Table 7: Top-weighted features for the classifier
trained on the Uniform dataset.
words are stop words (as opposed to the logistic
regression approach, which treats this as a multi-
class classification problem). To report a more
useful list of terms, we took the following steps:
(1) we normalized the parameter vectors for each
class by vector length; (2) from the parameter vec-
tor of each class we subtracted the vectors of the
other three classes (i.e., ?
B
? ?
B
? (?
A
+ ?
L
+
?
W
)). The resulting vectors better reflect the fea-
tures weighted more highly in one class than oth-
ers. We report the top 15 features per class.
The top features for the County method (Ta-
ble 8) reveal a mixture of lexical variations as
well as geographical indicators, which act as prox-
ies for race. There are many Spanish words for
Latino-American users, for example ?de?, ?la?, and
?que.? In addition there are some state names
(?texas?, ?hawaii?), part of city names (?san?), and
abbreviations (?sfo? is the code for the San Fran-
cisco airport). Texas is 37.6% Hispanic-American,
and San Francisco is 34.2% Asian-American. Ref-
erences to the photo-sharing site Instagram are
found to be strongly indicative of Latino users.
This is further supported by a survey conducted
by the Pew Research Internet Project,
6
which
found that while an equal percentage of White
and Latino online adults use Twitter (16%), online
Latinos were almost twice as likely to use Insta-
gram (23% versus 12%). Additionally, the term
6
http://www.pewinternet.org/files/
2013/12/PIP_Social-Networking-2013.pdf
13
Black White Latino Asian
follow you texas ca
my NoDesc lol san
be and la hawaii
got so de hawaii
up you que hi
this can el http
ain re de california
university have no haha
bout is la francisco
get university tx #hawaii
all haha instagram ca
nigga are tx beach
on justin san ig
smh to en com
niggas would god sfo
Table 8: Top-weighted features for the regression
model trained on the County dataset. Terms from
the description field are in italics.
Truth Predicted Top Features
white latino de, la, que, no, la, el, san,
en, amp, me
white black this, on, be, got, up, in,
shit, at, the, all
black white you, and, to, you, the, is,
so, of, have, re
Table 9: Misclassified by the County method.
?justin? in the user profile description is a strong
indicator of White users ? an inspection of the
County dataset reveals that this is largely in ref-
erence to the pop musician Justin Bieber. (Recall
that users typically do not enter their own names
in the description field.)
We find some similarities with the results of
Eisenstein et al. (2011) ? e.g., the term ?smh?
(?shaking my head?) is a highly-ranked term for
African-Americans.
4.2 Error Analysis
We sample a number of users who were misclas-
sified, then identify the highest weighted features
(using the dot product of the feature vector and pa-
rameter vector). Table 9 displays the top features
of a sample of users in the Uniform dataset that
were correctly classified by the Uniform method
but misclassified by the County method. Similarly,
Table 10 shows examples that were misclassified
by the Uniform approach but correctly classified
Truth Predicted Top Features
black white makes, guys, thing, isn,
am, again, haha, every-
one, remember, very
black white please, guys, snow, pretty,
literally, isn, am, again,
happen, midnight
black white makes, snow, pretty, lit-
erally, am, again, happen,
yay, beer, amazing
Table 10: Misclassified by the classifier trained on
the Uniform dataset.
by the County approach.
One common theme across all models is that be-
cause White is the most common class label, many
common terms are correlated with it (e.g., the, is,
of). Thus, for users that use only very common
terms, the models tend to select the White label.
Indeed, examining the confusion matrix reveals
that the most common type of error is to misclas-
sify a non-White user as White.
5 Conclusions and Future Work
Our results suggest that models fit on aggregate,
geolocated social media data can be used estimate
individual user attributes. While further analysis
is needed to test how this generalizes to other at-
tributes, this approach may provide a low-cost way
of inferring user attributes. This in turn will bene-
fit growing attempts to use social media as a com-
plement to traditional polling methods ? by quan-
tifying the bias in a sample of social media users,
we can then adjust inferences using approaches
such as survey weighting (Gelman, 2007).
There are clear ethical concerns with how such
a capability might be used, particularly if it is ex-
tended to estimate more sensitive user attributes
(e.g., health status). Studies such as this may help
elucidate what we reveal about ourselves through
our language, intentionally or not.
In future work, we will consider richer user
representations (e.g., social media activity, social
connections), which have also been found to be
indicative of user attributes. Additionally, we will
consider combining labeled and unlabeled data us-
ing semi-supervised learning from label propor-
tions (Quadrianto et al., 2009; Ganchev et al.,
2010; Mann and McCallum, 2010).
14
References
F Al Zamal, W Liu, and D Ruths. 2012. Homophily
and latent attribute inference: Inferring latent at-
tributes of twitter users from neighbors. In ICWSM.
Shlomo Argamon, Sushant Dhawle, Moshe Koppel,
and James W. Pennebaker. 2005. Lexical predictors
of personality type. In In proceedings of the Joint
Annual Meeting of the Interface and the Classifica-
tion Society of North America.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1301?1309, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jonathan Chang, Itamar Rosenn, Lars Backstrom, and
Cameron Marlow. 2010. ePluribus: ethnicity on
social networks. In Fourth International AAAI Con-
ference on Weblogs and Social Media.
Aron Culotta. 2014. Estimating county health statis-
tics with twitter. In CHI.
Mark Dredze. 2012. How social media will change
public health. IEEE Intelligent Systems, 27(4):81?
84.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1365?1374, Stroudsburg, PA,
USA. Association for Computational Linguistics.
George Forman. 2008. Quantifying counts and
costs via classification. Data Min. Knowl. Discov.,
17(2):164?206, October.
Kuzman Ganchev, Joo Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. J. Mach. Learn. Res.,
11:2001?2049, August.
Daniel Gayo-Avello. 2011. Don?t turn social media
into another ?Literary digest? poll. Commun. ACM,
54(10):121?128, October.
Andrew Gelman. 2007. Struggles with survey weight-
ing and regression modeling. Statistical Science,
22(2):153?164.
Sharad Goel, Jake M Hofman, and M Irmak Sirer.
2012. Who does what on the web: A large-scale
study of browsing behavior. In ICWSM.
Shyam Gopinath, Jacquelyn S. Thomas, and Lakshman
Krishnamurthi. 2014. Investigating the relationship
between the content of online word of mouth, adver-
tising, and brand performance. Marketing Science.
Published online in Articles in Advance 10 Jan 2014.
Gary King. 1997. A solution to the ecological infer-
ence problem: Reconstructing individual behavior
from aggregate data. Princeton University Press.
Wendy Liu and Derek Ruths. 2013. What?s in a name?
using first names as features for gender inference in
twitter. In AAAI Spring Symposium on Analyzing
Microtext.
Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. J. Mach. Learn.
Res., 11:955?984, March.
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-
Pekka Onnela, and J. Niels Rosenquist. 2011. Un-
derstanding the demographics of twitter users. In
Proceedings of the Fifth International AAAI Con-
ference on Weblogs and Social Media (ICWSM?11),
Barcelona, Spain.
Dong Nguyen, Noah A. Smith, and Carolyn P. Ros.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-HLT
Workshop on Language Technology for Cultural
Heritage, Social Scie nces, and Humanities, LaT-
eCH ?11, pages 115?123, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to polls: Linking text sentiment
to public opinion time series. In International
AAAI Conference on Weblogs and Social Media,
Washington, D.C.
F. Pedregosa et al. 2011. Scikit-learn: Ma-
chine learning in Python. Machine Learning Re-
search, 12:2825?2830. http://dl.acm.org/
citation.cfm?id=2078195.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to twitter user classifi-
cation. In Lada A. Adamic, Ricardo A. Baeza-Yates,
and Scott Counts, editors, ICWSM. The AAAI Press.
Novi Quadrianto, Alex J. Smola, Tibrio S. Caetano, and
Quoc V. Le. 2009. Estimating labels from label pro-
portions. J. Mach. Learn. Res., 10:2349?2374, De-
cember.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2Nd In-
ternational Workshop on Search and Mining User-
generated Contents, SMUC ?10, pages 37?44, New
York, NY, USA. ACM.
Delip Rao, Michael J. Paul, Clayton Fink, David
Yarowsky, Timothy Oates, and Glen Coppersmith.
2011. Hierarchical bayesian models for latent at-
tribute detection in social media. In ICWSM.
Sara Rosenthal and Kathleen McKeown. 2011. Age
prediction in blogs: A study of style, content, and
15
online behavior in pre- and post-social media gen-
erations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 763?772, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W Pennebaker. 2006. Effects of age
and gender on blogging. In AAAI 2006 Spring Sym-
posium on Computational Approaches to Analysing
Weblogs (AAAI-CAAW), pages 06?03.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M
Ramones, Megha Agrawal, Achal Shah, Michal
Kosinski, David Stillwell, Martin E P Seligman,
and Lyle H Ungar. 2013a. Personality, gen-
der, and age in the language of social media: the
open-vocabulary approach. PloS one, 8(9):e73791.
PMID: 24086296.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin EP Seligman, et al.
2013b. Characterizing geographic variation in well-
being using tweets. In Seventh International AAAI
Conference on Weblogs and Social Media.
16
