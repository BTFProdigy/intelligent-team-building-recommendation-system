Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 13?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Detecting Novelty in the context of Progressive Summarization
Praveen Bysani
Language Technologies Research Center
IIIT Hyderabad
lvsnpraveen@research.iiit.ac.in
Abstract
A Progressive summary helps a user to moni-
tor changes in evolving news topics over a pe-
riod of time. Detecting novel information is
the essential part of progressive summariza-
tion that differentiates it from normal multi
document summarization. In this work, we
explore the possibility of detecting novelty at
various stages of summarization. New scoring
features, Re-ranking criterions and filtering
strategies are proposed to identify ?relevant
novel? information. We compare these tech-
niques using an automated evaluation frame-
work ROUGE, and determine the best. Over-
all, our summarizer is able to perform on par
with existing prime methods in progressive
summarization.
1 Introduction
Summarization is the process of condensing text to
its most essential facts. Summarization is challeng-
ing for its associated cognitive task and interesting
because of its practical usage. It has been success-
fully applied for text content such as news articles 1,
scientific papers (Teufel and Moens, 2002) that fol-
low a discourse structure. Update summarization is
an emerging area with in summarization, acquiring
significant research focus during recent times. The
task was introduced at DUC 20072 and continued
during TAC 2008, 20093. We refer to update sum-
mariztion as ?Progressive Summarization? in rest of
1http://newsblaster.cs.columbia.edu/
2http://duc.nist.gov/duc2007/tasks.html
3http://www.nist.gov/tac
this paper, as summaries are produced periodically
in a progressive manner and the latter title is more
apt to the task. Progressive summaries contain infor-
mation which is both relevant and novel, since they
are produced under the assumption that user has al-
ready read some previous documents/articles on the
topic. Such summaries are extremely useful in track-
ing news stories, tracing new product reviews etc.
Unlike dynamic summarization (Jatowt, 2004)
where a single summary transforms periodically, re-
flecting changes in source text, Progressive summa-
rizer produce multiple summaries at specific time
intervals updating user knowledge. Temporal Sum-
marization (Allan et al, 2001) generate summaries,
similar to progressive summaries by ranking sen-
tences as combination of relevant and new scores.
In this work, summaries are produced not just by
reforming ranking scheme but also altering scoring
and extraction stages of summarization.
Progressive summarization requires differentiat-
ing Relevant and Novel Vs Non-Relevant and Novel
Vs Relevant and Redundant information. Such dis-
crimination is feasible only with efficient Novelty
detection techniques. We define Novelty detection
as identifying relevant sentences containing new in-
formation. This task shares similarity with TREC
Novelty Track 4, that is designed to investigate sys-
tems abilities to locate sentences containing relevant
and/or new information given the topic and a set of
relevant documents ordered by date. A progressive
summarizer needs to identify, score and then finally
rank ?relevant novel? sentences to produce a sum-
mary.
4http://trec.nist.gov/data/novelty.html
13
Previous approaches to Novelty detection at
TREC (Soboroff, 2004) include cosine filter-
ing (Abdul-Jaleel et al, 2004), where a sentence
having maximum cosine similarity value with pre-
vious set of sentences, lower than a preset thresh-
old is considered novel. Alternatively, (Schiffman
and McKeown, 2004) considered previously unseen
words as an evidence of Novelty. (Eichmannac et
al., 2004) expanded all noun phrases in a sentence
using wordnet and used corresponding sysnsets for
novelty comparisions.
Our work targets exploring the effect of detect-
ing novelty at different stages of summarization on
the quality of progressive summaries. Unlike most
of the previous work (Li et al, 2009) (Zhang et
al., 2009) in progressive summarization, we em-
ploy multiple novelty detection techniques at differ-
ent stages and analyze them all to find the best.
2 Document Summarization
The Focus of this paper is only on extrac-
tive summarization, henceforth term summariza-
tion/summarizer implies sentence extractive multi
document summarization. Our Summarizer has 4
major stages as shown in Figure 1,
Figure 1: Stages in a Multi Document Summarizer
Every news article/document is cleaned from
news heads, HTML tags and split into sentences dur-
ing Pre-processing stage. At scoring, several sen-
tence scoring features assign scores for each sen-
tence, reflecting its topic relevance. Feature scores
are combined to get a final rank for the sentence
in ranking stage. Rank of a sentence is predicted
from regression model built on feature vectors of
sentences in the training data using support vector
machine as explained in (Schilder and Kondadandi,
2008). Finally during summary extraction, a sub-
set of ranked sentences are selected to produce sum-
mary after a redundancy check to filter duplicate
sentences.
2.1 Normal Summarizers
Two normal summarizers (DocSumm, TacBaseline)
are developed in a similar fashion described in
Figure 1.
DocSumm produce summaries with two scoring
features, Document Frequency Score (DF) (Schilder
and Kondadandi, 2008) and Sentence Position
(SP). DocSumm serves as a baseline to depict the
effect of novelty detection techniques described
in Section 3 on normal summarizers. Document
frequency (DF), of a word (w) in the document set
(docs) is defined as ratio of number of documents in
which it occured to the total number of documents.
Normalized DF score of all content words in a
sentence is considered its feature score.
DFdocs(w) =
{|d| : w ? d}
|docs|
Sentence Position (SP) assigns positional index (n)
of a sentence (sn) in the document (d) it occurs as
its feature score. Training model will learn the opti-
mum sentence position for the dataset.
SP (snd) = n
TacBaseline is a conventional baseline at TAC, that
creates a n word length summary from first n words
of the most recent article. It provides a lower bound
on what can be achieved with automatic multi docu-
ment summarizers.
3 Novelty Detection
Progressive summaries are generated at regular time
intervals to update user knowledge on a particular
news topic. Imagine a set of articles published on
a evolving news topic over time period T, with td
being publishing timestamp of article d. All the arti-
cles published from time 0 to time t are assumed to
14
have been read previously, hence prior knowledge,
pdocs. Articles published in the interval t to T that
contain new information are considered ndocs.
ndocs = {d : td > t}
pdocs = {d : td <= t}
Progressive summarization needs a novelty detec-
tion technique to identify sentences that contain rel-
evant new information. The task of detecting nov-
elty can be carried out at 3 stages of summarization
shown in Figure 1.
3.1 At Scoring
New Sentence scoring features are devised to
capture sentence novelty along with its relevance.
Two features Novelty Factor (NF) (Varma et al,
2009), and New Words (NW) are used at scoring
level.
Novelty Factor (NF)
NF measures both topic relevancy of a sentence
and its novelty given prior knowledge of the user
through pdocs. NF score for a word w is calculated
as,
NF (w) =
|ndt|
|pdt|+ |ndocs|
ndt = {d : w ? d ? d ? ndocs}
pdt = {d : w ? d ? d ? pdocs}
|ndt| captures the relevancy of w, and |pdt| elevates
the novelty by penalizing words occurring fre-
quently in pdocs. Score of a sentence is the average
NF value of its content words.
New Words (NW)
Unlike NF, NW captures only novelty of a sentence.
Novelty of a sentence is assessed by the amount of
new words it contains. Words that never occurred
before in pdocs are considered new. Normalized
term frequency of a word (w) is used in calculating
feature score of sentence. Score of a sentence(s) is
given by,
Score(s) =
?
w?s NW (w)
|s|
NW (w) = 0 if w ? pdocs
= n/N else
n is frequency of w in ndocs
N is total term frequency of ndocs
3.2 At Ranking
Ranked sentence set is re-ordered using Maximal
Marginal relevance (Carbonell and Goldstein, 1998)
criterion, such that prior knowledge is neglected and
sentences with new information are promoted in the
ranked list. Final rank (?Rank?) of a sentence is
computed as,
Rank = relweight ? rank ?
(1? relweight) ? redundancy score
Where ?rank? is the original sentence rank predicted
by regression model as described in section 2, and
?redundancy score? is an estimate for the amount
of prior information a sentence contains. Parameter
?relweight? adjusts relevancy and novelty of a
sentence. Two similarity measures ITSim, CoSim
are used for calculating redundancy score.
Information Theoretic Similarity (ITSim)
According to information theory, Entropy quantifies
the amount of information carried with a message.
Extending this analogy to text content, Entropy
I(w) of a word w is calculated as,
I(w) = ?p(w) ? log(p(w))
p(w) = n/N
Motivated by the information theoretic definition of
similarity by (Lin, 1998), we define similarity be-
tween two sentences s1 and s2 as,
ITSim(s1, s2) =
2 ?
?
w?s1?s2 I(w)
?
w?s1 I(w) +
?
w?s2 I(w)
Numerator is proportional to the commonality
between s1 and s2 and denominator reflects differ-
ences between them.
Cosine Similarity (CoSim)
Cosine similarity is a popular technique in TREC
Novelty track to compute sentence similarity.
Sentences are viewed as tf-idf vectors (Salton
and Buckley, 1987) of words they contain in a n-
dimension space. Similarity between two sentences
is measured as,
CoSim(s1, s2) = cos(?) =
s1.s2
|s1||s2|
Average similarity value of a sentence with all sen-
tences in pdocs is considered as its redundancy
score.
15
3.3 At summary extraction
Novelty Pool (NP)
Sentences that possibly contain prior information
are filtered out from summary by creating Novelty
Pool (NP), a pool of sentences containing one or
more novelwords. Two sets of ?dominant? words
are generated one for each pdocs and ndocs.
domndocs = {w : DFndocs(w) > threshold}
dompdocs = {w : DFpdocs(w) > threshold}
A word is considered dominant if it appears in more
than a predefined ?threshold ? of articles, thus mea-
suring its topic relevance. Difference of the two dom
sets gives us a list of novelwords that are both rele-
vant and new.
novelwords = domndocs ? dompdocs
4 Experiments and Results
We conducted all the experiments on TAC 2009 Up-
date Summarization dataset. It consists of 48 topics,
each having 20 documents divided into two clusters
?A? and ?B? based on their chronological coverage
of topic. It serves as an ideal setting for evaluat-
ing our progressive summaries. Summary for clus-
ter A (pdocs) is a normal multi document summary
where as summary for cluster B (ndocs) is a Pro-
gressive summary, both of length 100 words. Each
topic has associated 4 model summaries written by
human assessors. TAC 2008 Update summarization
data that follow similar structure is used to build
training model for support vectors as mentioned in
Section 2. Thresholds for domndocs, dompdocs are
set to 0.6, 0.3 respectively and relweight to 0.8 for
optimal results.
Summaries are evaluated using ROUGE (Lin,
2004), a recall oriented metric that automatically
assess machine generated summaries based on their
overlap with models. ROUGE-2 and ROUGE-SU4
are standard measures for automated summary
evaluation. In Table 1 ROUGE scores of baseline
systems(Section 2.1) are presented.
Five progressive runs are generated, each having a
novelty detection scheme at either scoring, ranking
or summary extraction stages. ROUGE scores of
these runs are presented in Table 2.
ROUGE-2 ROUGE-SU4
DocSumm 0.09346 0.13233
TacBaseline 0.05865 0.09333
Table 1: Average ROUGE-2, ROUGE-SU4 recall scores
of baselines for TAC 2009, cluster B
NF+DocSumm : Sentence scoring is done with an
additional feature NF, along with default features of
DocSumm
NW+DocSumm : An additional feature NW is
used to score sentences for DocSumm
ITSim+DocSumm : ITSim is used for computing
similarity between a sentence in ndocs and set of all
sentences in pdocs. Maximum similarity value is
considered as redundancy score. Re-ordered ranked
list is used for summary extraction
Cosim+DocSumm : CoSim is used as a similarity
measure instead of ITSim
NP+DocSumm : Only members of NP are consid-
ered while extracting DocSumm summaries
Results of top systems at TAC 2009, ICSI (Gillick
et al, 2009) and THUSUM (Long et al, 2009) are
also provided for comparison.
ROUGE-2 ROUGE-SU4
ICSI 0.10417 0.13959
NF+DocSumm 0.10273 0.13922
NW+DocSumm 0.09645 0.13955
NP+DocSumm 0.09873 0.13977
THUSUM 0.09608 0.13499
ITSim+DocSumm 0.09461 0.13306
Cosim+DocSumm 0.08338 0.12607
Table 2: Average ROUGE-2, ROUGE-SU4 recall scores
for TAC 2009, cluster B
Next level of experiments are carried out on combi-
nation of these techniques. Each run is produced by
combining two or more of the above(Section 3) de-
scribed techniques in conjunction with DocSumm.
Results of these runs are presented in table 3
NF+NW : Both NF and NW are used for sentence
scoring along with default features of DocSumm
NF+NW+ITSim : Sentences scored in NF+NW are
re-ranked by their ITSim score
NF+NW+NP : Only members of NP are selected
while extracting NF+NW summaries
16
NF+NW+ITSim+NP : Sentences are selected from
NP during extraction of NF+NW+ITSim summaries
ROUGE-2 ROUGE-SU4
NF+NW 0.09807 0.14058
NF+NW+ITSim 0.09704 0.13978
NF+NW+NP 0.09875 0.14010
{NP+NW+
ITSim+NP} 0.09664 0.13812
Table 3: Average ROUGE-2, ROUGE-SU4 recall scores
for TAC 2009, cluster B
5 Conclusion and Discussion
Experimental results prove that proposed Novelty
Detection techniques, particularly at scoring stage
are very effective in the context of progressive sum-
marization. Both NF, a language modeling tech-
nique and NW, a heuristic based feature are able
to capture relevant novelty successfully. An ap-
proximate 6% increase in ROUGE-2 and 3% in-
crease in ROUGE-SU4 scores over DocSumm sup-
port our argument. Scores of NF+DocSumm and
NW+DocSumm are comparable with existing best
approaches. Since CoSim is a word overlap mea-
sure, and novel information is often embedded
within a sentence containing formerly known infor-
mation, quality of progressive summaries declined.
ITSim performs better than Cosim because it con-
siders entropy of a word in similarity computations,
which is a better estimate of information. There is a
need for improved similarity measures that can cap-
ture semantic relatedness between sentences. Nov-
elty pool (NP) is a simple filtering technique, that
improved quality of progressive summaries by dis-
carding probable redundant sentences into summary.
From the results in Table 2, it can be hypothesized
that Novelty is best captured at sentence scoring
stage of summarization, rather than at ranking or
summary extraction.
A slight improvement of ROUGE scores is ob-
served in table 3, when novelty detection techniques
at scoring, ranking and extracting stages are com-
bined together. As Novel sentences are already
scored high through NF and NW, the effect of Re-
Ranking and Filtering is not significant in the com-
bination.
The major contribution of this work is to iden-
tify the possibility of novelty detection at different
stages of summarization. Two new sentence scoring
features (NF and NW), a filtering strategy (NP), a
sentence similarity measure (ITSim) are introduced
to capture relevant novelty. Although proposed ap-
proaches are simple, we hope that this novel treat-
ment could inspire new methodologies in progres-
sive summarization. Nevertheless, the problem of
progressive summarization is far from being solved
given the complexity involved in novelty detection.
Acknowledgements
I would like to thank Dr. Vasudeva Varma at IIIT
Hyderabad, for his support and guidance throughout
this work. I also thank Rahul Katragadda at Yahoo
Research and other anonymous reviewers, for their
valuable suggestions and comments.
References
Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fer-
nando Diaz, Leah Larkey, and Xiaoyan Li. 2004.
Umass at trec 2004: Novelty and hard.
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of news topics.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In SIGIR ?98: Pro-
ceedings of the 21st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 335?336, New York, NY, USA.
ACM.
David Eichmannac, Yi Zhangb, Shannon Bradshawbc,
Xin Ying Qiub, Padmini Srinivasanabc, and Aditya
Kumar. 2004. Novelty, question answering and ge-
nomics: The university of iowa response.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd
summarization system at tac 2009.
Adam Jatowt. 2004. Web page summarization using dy-
namic content. In WWW Alt. ?04: Proceedings of the
13th international World Wide Web conference on Al-
ternate track papers and posters, pages 344?345, New
York, NY, USA. ACM.
Sujian Li, Wei Wang, and Yongwei Zhang. 2009. Tac
2009 update summarization with unsupervised meth-
ods.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In ICML ?98: Proceedings of the Fif-
teenth International Conference on Machine Learn-
17
ing, pages 296?304, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. pages 74?81, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Chong Long, Minlie Huang, and Xiaoyan Zhu. 2009.
Tsinghua university at tac 2009: Summarizing multi-
documents by information distance.
Gerard Salton and Chris Buckley. 1987. Term weight-
ing approaches in automatic text retrieval. Technical
report, Ithaca, NY, USA.
Barry Schiffman and Kathleen R. McKeown. 2004.
Columbia university in the novelty track at trec 2004.
Frank Schilder and Ravikumar Kondadandi. 2008. Fast-
sum: fast and accurate query-based multi-document
summarization. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics on Human Language Technologies. Human Lan-
guage Technology Conference.
Ian Soboroff. 2004. Overview of the trec 2004 novelty
track. National Institute of Standards and Technol-
ogy,Gaithersburg, MD 20899.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Vasudeva Varma, Praveen Bysani, Kranthi Reddy, Vijay
Bharat, Santosh GSK, Karuna Kumar, Sudheer Kove-
lamudi, Kiran Kumar N, and Nitin Maganti. 2009. iiit
hyderabad at tac 2009. Technical report, Gaithersburg,
Maryland USA.
Jin Zhang, Pan Du, Hongbo Xu, and Xueqi Cheng. 2009.
Ictgrasper at tac2009: Temporal preferred update sum-
marization.
18
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 83?87,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Integrating User-Generated Content in the ACL Anthology
Praveen Bysani
Web IR / NLP Group (WING)
National University of Singapore
13 Computing Link, Singapore 117590
bpraveen@comp.nus.edu.sg
Min-Yen Kan
Web IR / NLP Group (WING)
National University of Singapore
13 Computing Link, Singapore 117590
kanmy@comp.nus.edu.sg
Abstract
The ACL Anthology was revamped in 2012
to its second major version, encompassing
faceted navigation, social media use, as well
as author- and reader-generated content and
comments on published work as part of the re-
vised frontend user interface. At the backend,
the Anthology was updated to incorporate its
publication records into a database. We de-
scribe the ACL Anthology?s previous legacy,
redesign and revamp process and technolo-
gies, and its resulting functionality.
1 Introduction
To most of its users, the ACL Anthology1 is a useful
open-access repository of scholarly articles on the
topics of computational linguistics and natural lan-
guage processing. The liberal use and access policy
granted by the Association of Computational Lin-
guistics (ACL) to the authors of works published by
the ACL makes discovery, access, and use of its re-
search results easily available to both members and
the general readership. The ACL Anthology initia-
tive has contributed to the success of this mission,
both as an archiving and dissemination vehicle for
published works.
Started as a means to collect and preserve arti-
cles published by the ACL in 2001, the Anthology
has since matured and now has well-defined work-
flows for its core missions. In 2009, the Anthology
Praveen Bysani?s work was supported from the National
Research Foundations grant no. R-252-000-325-279.
1http://aclweb.org/anthology/; beta version 2
currently at http://aclantho3.herokuapp.com/.
staff embarked to expand the Anthology?s mission to
meet two specific goals: on the backend, to enforce
a proper data model onto the publication metadata;
on the frontend, to expand the scope of the Anthol-
ogy to encompass services that would best serve its
constituents. Where possible, we adopted widely-
deployed open source software, customizing it for
the Anthology where needed.
With respect to the backend, the revamp adopted a
database model to describe the publication metadata,
implemented using MySQL. On top of this database
layer, we chose Ruby on Rails as the application
framework to interact with the data, and built suit-
able web interfaces to support both administrative
and end-users. The backend also needed to support
resource discovery by automated agents, and meta-
data export to sites that ingest ACL metadata.
With respect to the frontend, the Anthology web-
site needed to meet the rising expectations in search
and discovery of documents both by content and by
fielded metadata. To satisfy both, we incorporated
a faceted browsing interface that exposes metadata
facets to the user. These metadata fields can be
used to restrict subsequent browsing and searching
actions to the values specified (e.g., Year = 2001?
2011). Aside from resource discovery, the fron-
tend also incorporated changes to support the work-
flow of readers and authors. We added both per-
author and per-publication webpages. The publica-
tion pages invite the public to define content for the
Anthology: anyone can report errors in the meta-
data, authors can supply revisions and errata, soft-
ware and dataset links post-publication, readers can
discuss the papers using the commenting framework
83
in the system, and automated agents can use NLP
and CL technology to extract, process and post in-
formation related to individual papers.
2 Revamp Design
Prior to our revamp, the Anthology?s basic mission
was to transcribe the metadata of ACL proceed-
ings into a suitable form for the Web. To ensure
widespread adoption, a simple XML format for the
requisite metadata of author and title was created,
with each ACL event?s publication chair providing a
single XML file describing the publications in each
event and the details of the event (e.g., the volume?s
booktitle and year). Other fields were optional and
could be included in the XML. The Anthology ed-
itor further added a unique identifier, an Anthology
ID, for each publication record (e.g., ?A00-1001?).
Mandatory fields in the XML were extracted by a
collection of programs to create the visible HTML
pages in the Anthology website and the service ex-
port files, used to update the Association of Com-
puting Machinery?s (ACM) Portal2 and the DBLP
Computer Science Bibliography3. Prior to the re-
vamp, this set of XML files ? collected over various
years ? represented the canonical record of all pub-
lication data.
While easing adoption, storing canonical pub-
lication metadata as XML is not ideal. As it
is stored across multiple files, even simple ques-
tions of inventory are hard to answer. As there
was no set document type definition, the XML
schema and enforcement of mandatory fields var-
ied per document. In the revamp, we migrated
the publication data into a database schema shown
in Figure 1. The database form allows easy in-
corporation of additional fields that can be pro-
vided post-publication (including the Document
Object Identifier, DOI, currently provided by the
ACM by mutual agreement). The database struc-
ture also promotes publications, venues, and au-
thors to first-class objects, enabling joins and views
on the data, such as paper?author and venue?
special interest group.The database currently has
21,107 papers, authored by 19,955 authors. These
2http://dl.acm.org
3http://www.informatik.uni-trier.de/
?ley/db/
papers encompass one journal, 17 conferences and
hundreds of workshops sponsored by 14 SIG groups.
The publication years of these papers range from
1965 to 2012.
Figure 1: Current database schema for the Anthology.
The database?s content is further indexed in in-
verted search indices using Apache Solr4. Solr
allows indexing and querying in XML/JSON for-
mats via HTTP requests, powering the frontend
website search facility and enabling programmatic
search by automated agents in the Anthology?s fu-
ture roadmap. We employ Ruby on Rails (or
?Rails?, version 3.1), a widely-deployed and mature
web development framework, to build the frontend.
It follows a Model-View-Controller (MVC) archi-
tecture, and favors convention over customization,
expediting development and maintenance. Rails
provides a closely tied model for basic database in-
teractions, page rendering, web server deployment
and provides a platform for integrating plugins for
additional functionality. To enable faceted browsing
and search, the revamped Anthology integrates the
Project Blacklight5 plugin, which provides the web
search interface via our Solr indices. Rails appli-
cations can be deployed on many commercial web
hosts but not on the current hosting service used
by the primary ACL website. We have deployed
the new Anthology interface on Heroku, a commer-
cial cloud-based platform that caters to Rails deploy-
ment.
3 Frontend Form and Function
Of most interest to Anthology users will be the pub-
lic website. The remainder of this paper describes
4http://lucene.apache.org/solr/
5http://projectblacklight.org/, version 3.2
84
Figure 2: A screenshot of a faceted keyword search, showing additional restrictions on Author and Year (as a range).
the individual features that have been incorporated
in the new interface.
Faceted Browsing: Facets let a paper (or other
first-class object, such as authors) be classified along
multiple dimensions. Faceted browsing combines
both browsing- and search-based navigation: An-
thology users can progressively filter the collection
in each dimension by selecting a facet and value, and
concurrently have the freedom of searching by key-
word. It is a prevailing user interface technique in
e-commerce sites and catching on in digital libraries.
The current Anthology defines five facets for pa-
pers. ?Author?, ?Publication Year?, ?Venue?, ?Attach-
ments? and ?SIG? (Special Interest Group) of the cor-
responding volume. The ?Year? facet further exposes
an interface for date range filtering, while the ?At-
tachments? allows the selection of papers with soft-
ware, errata, revisions and/or datasets easily. The
website also has a standard search box that sup-
ports complex Boolean queries. Figure 2 illustrates
some of these functions in a complex query involv-
ing both facets and keyword search. This is an im-
provement over the previous version that employed
Google custom search, which can not leverage our
structured data to add filtering functionality. Tak-
ing back search from Google?s custom search also
means that our search logs can be provided to our
own community for research, that could enable an
improved future Anthology.
Programmatic Contributions: The ACL com-
munity is uniquely positioned to enhance the An-
thology by applying natural language technology
on its own publication output. The ACL Anthol-
ogy Reference Corpus (Bird et al, 2008) previously
standardized a version of the Anthology?s articles
for comparative benchmarking. We take this idea
farther by allowing automated agents to post-process
information about any publication directly into the
publication?s corresponding page. An agent can cur-
rently provide per-paper supplementary material in
an XML format (shown below) to the editor. After
suitable validation as non-spam, the editor can ingest
the XML content into the Anthology, incorporating
it into the paper?s webpage. Such functionality could
be used to highlight summarization, information ex-
traction and other applications that can process the
text of papers and enrich them.
We use the Anthology ID to uniquely identify
the associated paper. Currently the system is pro-
visioned to support supplementary data provided as
1) text (as shown in Figure 3), 2) an embedded web-
page, and 3) hyperlinks to websites (similar to how
attachments are shown).
<paper id="P11-1110">
<content name="keywords", type="text">
<item>
discourse, implicit reference, coherence,
readability
</item>
</content>
</paper>
...
Figure 3: Excerpt of a programmatic contribution to the
Anthology. The excerpt shows a keyword contribution on
paper P11-1110.
85
Figure 4: (Compressed) individual publication view with
callout highlights of features.
Bibliographic Metadata Export: The previous
Anthology exposed bibliographic metadata in Bib-
TeX format, but its production was separate from
the canonical XML data. In the revamp, we trans-
form the database field values into the MODS bibli-
ography interchange format. We then integrated the
Bibutils6 software module that exports MODS into
four end-user formats: BibTeX, RIS, EndNote and
Word. This lessens the effort for users to cite works
in the Anthology by matching major bibliography
management systems. Our use of Blacklight also
enhances this ability, allowing the selection of mul-
tiple items to be exported to bibliographic exporting
formats or to be shared by email.
User Contributed Data: While social media fea-
tures are quintessential in today?s Web, scholarly
digital libraries and academic networks have yet to
utilize them productively. One vehicle is to allow
the readership to comment on papers and for those
comments to become part of the public record. To
6http://sourceforge.net/p/bibutils/home/
Bibutils/
accomplish this, we integrated a commenting plugin
from Disqus7, which enables users logged into other
social media platforms to leave comments.
We also want to tighten the loop between reader
feedback and Anthology management. Our revamp
allows users to submit corrections and additions to
any paper directly through a web form on the indi-
vidual paper?s webpage. Post-publication datasets,
corrections to author name?s and paper errata can
be easily processed in this way. To avoid spam
changes, this feature requires the Anthology editor
to manually validate the changes. Figure 4 shows the
individual publication view, with metadata, biblio-
graphic export, metadata editing, commenting, and
user (programmatic) contribution sections.
Author Pages: As a consequence of using Rails,
it becomes trivially easy to create pages for other
first-class data elements. Currently, we have created
webpages per author, as shown in Figure 5. It gives
the canonical listing of each author?s publications
within the Anthology in reverse chronological or-
der and includes a list of the popular co-authors and
publication venues. This feature brings the Anthol-
ogy up to parity with other similar digital libraries.
We hope it will spur authors to report publications
under different variants of their names so a naming
authority for ACL authors can result partially from
community effort.
Figure 5: (Compressed) author page with corresponding
co-author and venue information.
7http://www.disqus.com
86
4 Usage Analysis
The revised Anthology interface is already seeing
heavy use. We analyzed the application logs of the
new Anthology website over a period of five days to
understand the impact and usage of the new features.
During this period the website has received 16,930
page requests. This is an increase over the original
website, which garnered less than 7,000 page views
during the same period. The average response time
of the server is 0.73 seconds, while the average load
time of a page is measured at 5.6 seconds. This is
slow ? web usability guidelines suggest load times
over 200 milliseconds are suboptimal ? but as the
website is deployed on the cloud, server response
can be easily improved by provisioning additional
resources for money. Currently the new Anthology
interface is run on a no-cost plan which provides
minimal CPU bandwidth to serve the dynamically
generated webpages to the readership.
The majority of the requests (11,398) use the new
faceting feature; indeed only 30 requests use the tra-
ditional search box. The most used facet patterns
include ?Author, Venue? (51.6%) followed by ?Au-
thor, Venue, Year? (14.8%). While we believe that it
is too early to draw conclusions on user behavior, the
overwhelming preference to use facets reveals that
faceted browsing is a preferable navigational choice
for the bulk of the Anthology users.
3,180 requests reached individual (detailed) pub-
lication views, while 2,455 requests accessed au-
thor pages. Approximately 62% of the total requests
had a visit duration under 10 seconds, but 22% re-
quests last between 11 seconds to 3 minutes, with
the remaining 16% sessions being up to 30 minutes
in length. The noticeable large ratio of long visits
support our belief that the newly-added features en-
courages more user engagement with the Anthology.
Since the website went live, we have received 3 valid
requests for metadata changes through the new in-
terface. Up to now, there has not been any use of
the social media features, but we believe Anthology
users will adopt them in due course.
5 Conclusion and Future Work
S.R. Ranganathan, arguably the father of faceted
classification, proposed that ?the library is a grow-
ing organism? as one of his laws of library science
(Ranganathan, 1931). We observe that this is true in
the digital context as well.
We will support the legacy ACL Anthology inter-
face until the end of 2012 in parallel with the new
interface, gradually phasing in the new interface as
the primary one. Our immediate goal is to flesh
out the per-author, -venue, -SIG views of the data,
and to enable resource discovery via Open Archives
Initiative?s Protocol for Metadata Harvesting (OAI-
PMH) (Lagoze et al, 2002), an open protocol for
harvesting metadata by web crawlers. Our medium
term outlook hopes to furtherincorporate grassroot
ACL resources such as the ACL Anthology Net-
work (Radev et al, 2009) and the ACL Search-
bench (Scha?fer et al, 2011).
We are most excited by the ability to incorporate
programmatic contributions made by NLP software
into the Anthology. We hope that the community
makes full use of this ability to showcase the impor-
tance of our natural language processing on schol-
arly data and improve its accessibility and relevance
to others.
References
Steven Bird, Robert Dale, Bonnie J. Dorr, Bryan Gib-
son, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir R. Radev, and Yee Fan Tan. 2008.
The ACL Anthology Reference Corpus: A reference
dataset for bibliographic research in computational lin-
guistics. In LREC?08.
Carl Lagoze, Hebert Van de Sompel, Michael Nelson,
and Simeon Warner. 2002. The open archives
initiative protocol for metadata harvesting, version
2.0. http://www.openarchives.org/OAI/2.0/ openar-
chivesprotocol.htm, June.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The ACL Anthology Network cor-
pus. In Proceedings of the 2009 Workshop on Text
and Citation Analysis for Scholarly Digital Libraries,
pages 54?61.
S. R. Ranganathan. 1931. The Five Laws of Library Sci-
ence. Madras Library Association (Madras, India) and
Edward Goldston (London, UK).
Ulrich Scha?fer, Bernd Kiefer, Christian Spurk, Jo?rg Stef-
fen, and Rui Wang. 2011. The ACL Anthology
Searchbench. In Proceedings of the 49th Associa-
tion for Computational Linguistics: Human Language
Technologies: Systems Demonstrations, pages 7?13.
87
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 110?113,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Linking Citations to their Bibliographic references
Huy Do Hoang Nhat
Web IR / NLP Group (WING)
National University of Singapore
huydo@comp.nus.edu.sg
Praveen Bysani
Web IR / NLP Group (WING)
National University of Singapore
bpraveen@comp.nus.edu.sg
Abstract
In this paper we describe our participation
in the contributed task at ACL Special work-
shop 2012. We contribute to the goal of en-
riching the textual content of ACL Anthology
by identifying the citation contexts in a paper
and linking them to their corresponding ref-
erences in the bibliography section. We use
Parscit, to process the Bibliography of each
paper. Pattern matching heuristics are then
used to connect the citations with their ref-
erences. Furthermore, we prepared a small
evaluation dataset, to test the efficiency of our
method. We achieved 95% precision and 80%
recall on this dataset.
1 Introduction
ACL Anthology represents the enduring effort to
digitally archive all the publications related to CL
and NLP, over the years. Recent work by (Bird
et al, 2008) to standardize the corpus in ACL An-
thology, makes it more than just a digital reposi-
tory of research results. The corpus has metadata
information such as ?title?, ?author (s)?, ?publication
venue? and ?year? about each paper along with their
extracted text content. However it lacks vital in-
formation about a scientific article such as position
of footnote (s), table (s) and figure captions, biblio-
graphic references, italics/emphasized text portions,
non-latin scripts, etc.
We would like to acknowledge funding support in part by
the Global Asia Institute under grant no. GAI-CP/20091116
and from the National Research Foundations grant no. R-252-
000-325-279.
The special workshop at ACL 2012, celebrates
50 years of ACL legacy by gathering contributions
about the history, evolution and future of compu-
tational linguistics. Apart from the technical pro-
gramme, the workshop also hosts a contributed task
to enrich the current state of Anthology corpus. A
rich-text format of the corpus will serve as a source
of study for research applications like citation anal-
ysis, summarization, argumentative zoning among
many others.
We contribute to this effort of enriching the An-
thology, by providing a means to link citations in
an article to their corresponding bibliographic refer-
ences. Robert Dale 1 defines citation, as a text string
in the document body that points to a reference at the
end of the document. Several citations may co-refer
to a single reference string. As an example consider
the following sentence,
Few approaches to parsing have tried to handle
disfluent utterances (notable exceptions are Core
& Schubert, 1999; Hindle, 1983; Nakatani &
Hirschberg, 1994).
The portion of texts in italics are the citations and
we intend to annotate each citation with an unique
identifier of their bibliographic reference.
<ref target="BI10">Hindle, 1983</ref>
Such annotations are useful for navigating be-
tween research articles and creating citation net-
works among them. These networks can be used to
understand the bibliometric analysis of a corpus.
1http://web.science.mq.edu.au/ rdale/
110
2 Design
The task organizers distribute the entire Anthol-
ogy in two different XML formats, ?paperXML?
that is obtained from Optical Character Recognition
(OCR) software and ?TEI P5 XML? that is generated
by PDFExtract (?yvind Raddum Berg, 2011). We
chose to process the PDFExtract format as it has no
character recognition errors. Since the expected out-
put should also follow ?TEI P5? guidelines, the lat-
ter input simplifies the process of target XML gen-
eration. The task of linking citations to references
primarily consists of three modules.
1. Processing the ?Bibliography? section of a pa-
per using Parscit.
2. Formatting the Parscit output to TEI P5 guide-
lines and merging it with the input XML.
3. Generating an identifier and citation marker for
each reference and annotating the text.
Figure 1 illustrates the overall design of our work.
Below we describe in detail about the modules used
to accomplish this task.
Figure 1: Overall design for linking citation text to refer-
ences
Bibliography Parser: Parscit (Councill et al,
2008) is a freely available, open-source implementa-
tion of a reference string parsing package. It formu-
lates this task as a sequence labelling problem that is
common to a large set of NLP tasks including POS
tagging and chunking. Parscit uses a conditional
random field formalism to learn a supervised model
and apply it on unseen data. During training, each
reference is represented using different classes of
features such as n-gram, token identity, punctuation
and other numeric properties. Parscit can label each
reference with 13 classes that correspond to com-
mon fields used in bibliographic reference manage-
ment software. Unlike heuristic methods, Parscit?s
supervised learning model can handle different stan-
dards followed by different communities and inad-
vertent manual errors in the Bibliography. Prior to
processing, Parscit segments the Bibliography sec-
tion from the rest of the paper using SectLabel (Lu-
ong et al, 2010), its extension for logical document
structure discovery.
Parscit works either with plain text or the Omni-
page output of a paper. Omnipage 2 is a state of
the art OCR engine that provides detailed informa-
tion about the layout of a document. Omnipage also
handles older, scanned papers. It gives the logical
index of every line in terms of page, column, para-
graph, and line number. The layout information is
used by Parscit to remove noise such as page num-
bers and footnotes between references and properly
divide them. Following is the Omnipage output for
the word ?Rahul Agarwal? in the original pdf,
<ln l="558" t="266" r="695" b="284"
bold=true superscript="none"
fontSize="1250" fontFamily="roman">
<wd l="558" t="266" r="609" b="284">
Rahul </wd> <space/>
<wd l="619" t="266" r="695" b="283">
Agarwal </wd>
</ln>
The ?l? (left), ?r? (right), ?t? (top), ?b? (bottom) at-
tributes gives the exact location of an element in a
page. Further, features such as ?bold?, ?underlined?,
?superscript/ subscript? and ?fontFamily? contribute
towards an accurate identification and parsing of ref-
erences. For example, the change from one font
family to another usually serves as a separator be-
tween two different fields like ?author? and ?title? of
the paper. As PDFExtract currently does not provide
such information, we processed the original ?pdf?
file using Omnipage and then finally parsed it us-
ing Parscit. Below is the XML output from Parscit
for a single reference,
2www.nuance.com/omnipage/
111
<citation valid="true">
<authors>
<author>R Agarwal</author>
<author>L Boggess</author>
</authors>
<title>A simple but useful approach
to conjunct identification.</title>
<date>1992</date>
<marker>Agarwal, Boggess, 1992
</marker> </citation>
We used Parscit to segment the Bibliography
section into individual references. Additionally we
use the author, title, publication year information
together with the original marker of each reference
to generate citation markers that are used to find the
context of each reference (explained later). During
this process, we generated the Omnipage output
for the present Anthology that consists of 21,107
publications. As the ACL ARC has Omnipage
outputs only till 2007, our contribution will help to
update the corpus.
XML Merger: The original XML output from
Parscit doesn?t conform with the TEI P5 guidelines.
The ?XML Merger? module formats the Parscit
output into a ?listBibl? element and merges it with
the PDFExtract. The ?listBibl? element contains a
list of ?biblStruct? elements, in which bibliographic
sub-elements of each reference appear in a specified
order. Each reference is also assigned a ?unique
id? within the paper to link them with their citation
texts. The Bibliography section in the PDFExtract
is replaced with the TEI compatible Parscit output
such as below,
<listBibl>
<bibl xml:id="BI2">
<monogr>
<author>R Agarwal</author>
<author>L Boggess</author>
<title>A simple but useful approach
to conjunct identification.</title>
<imprint>
<date>1992</date>
</imprint>
</monogr>
</bibl>
To ensure a proper insertion, we search for labels
such as ?References?, ?Bibliography?, ?References
and Notes?, or common variations of those strings.
In the case of having more than one match, the
context of first reference is used to resolve the
ambiguity. The match is considered as the starting
point of the Bibliography section, and the terminal
reference string from the Parscit output is used to
mark the end of it. After validating the matched
portion based on the position of its starting and
ending markers, it is replaced with the formatted
?listBibl? element.
Context Annotator: The final step is to bridge the
links between references and citation strings in the
merged XML. Several morphologically different
markers are generated for each reference based
on the ?author? and ?publication year? information
provided by Parscit. These markers are used to
find the corresponding citation string in the merged
XML. The markers may vary depending upon the
number of authors in a reference or the bibliography
style of the paper. Sample markers for a reference
with multiple authors are listed below,
Author1, Author2, Author3, Year
Author1 et.al, Year
Author1 and Author2, Year
Although Parscit provide the citation markers for
each reference, the recall is very low. We extended
these citation markers to make them more robust and
thus improve the overall recall. Below are the exten-
sions we made to the default markers.
1. Additional marker to allow square brackets and
round brackets in the parentheses. Such mark-
ers help to identify citations such as (Author,
Year), [Author, Year], (Author, [year])
2. Parscit markers only identify the citations with
the 4-digit format of the year. We modified it to
recognize both 4-digit and 2-digit format of the
year. e.g. Lin, 1996 and Lin, 96
3. Parscit doesn?t differentiate between identical
reference strings with same author and year in-
formation. We resolved it by including the ver-
sion number of the reference in the marker. e.g.
Lin, 2004a and Lin, 2004b
4. Heuristics are added to accommodate the de-
fault citation texts as specified in the reference
strings. For example in the reference string,
112
[Appelt85] Appelt, D. 1985 Planning English
Referring Expressions. Artificial Intelligence
26: 1-33.
[Appelt85] is identified as the citation marker. Each
marker is represented using a regular expression.
These regular expressions are applied on the text
from merged XML. The matches are annotated with
the unique id of its corresponding reference such as
?<ref target= BI10>?
3 Challenges
The accuracy of Parscit is a bottle-neck for the per-
formance of this task. The false negatives produced
by Parscit leads to erroneous linkage between cita-
tion texts and reference ids. In certain cases Parscit
fails to identify portions of Bibliography section and
skips them while processing. This results in an in-
correct parsing and thus faulty linkage. Apart from
Parscit, we faced problems due to the character mis-
matching between Omnipage and PDFExtract out-
puts of a paper. For example the string ?Pulman?
is recognized as Pullan by Omnipage and as Pul-
man by PDFExtract. The citation markers generated
from Parscit output in this case fails to identify the
context in the PDFExtract.
4 Evaluation
As there is no dataset to test the efficiency of our
method, we prepared a small dataset for evaluation
purposes. We manually sampled 20 papers from
the Anthology, making sure that all the publication
venues are included. The citation strings in each
paper are manually listed out along with the corre-
sponding reference id. For citation styles where no
Author and Year information is present, we used the
contextual words to identify the citation text. The
citation strings are listed in the same order as they
appear in the paper. Below we provide an extract of
the dataset, consisting of papers with three different
citation styles,
P92-1006 proposed [13] BI13
T87-1018 Mann&Thompson83 BI6
W00-0100 Krymolowski 1998 BI9
The first column is the Anthology id of the paper,
second column is the citation string from the paper
and third column is the unique id of the reference.
We measure the performance in terms of precision
and recall of the recognized citations. There are a to-
tal of 330 citation strings in the dataset. Our method
identified 280 strings as citations, out of which 266
are correct. Hence the precision is 0.95 (266/280)
and the recall is 0.801 (266/330). The low recall is
due to the incorrect recognition of author and year
strings by Parscit which lead to erroneous marker
generation. The precision is affected due to the flaws
in Parscit while differentiating citations with naked
numbers.
In future we plan to devise more flexible mark-
ers which can handle spelling mistakes, using edit
distance metric. Partial matches and sub-sequence
matches need to be incorporated to support long dis-
tance citations. Parscit can further be improved to
accurately parse and identify the reference strings.
Acknowledgements
We would like to thank Dr. Min-Yen Kan at National
University of Singapore for his valuable support and
guidance during this work.
References
Steven Bird, Robert Dale, Bonnie J. Dorr, Bryan Gib-
son, Mark T. Joseph, Min yen Kan, Dongwon Lee,
Brett Powley, Dragomir R. Radev, and Yee Fan Tan.
2008. The acl anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics.
Isaac G. Councill, C. Lee Giles, and Min yen Kan. 2008.
Parscit: An open-source crf reference string parsing
package. In International Language Resources and
Evaluation. European Language Resources Associa-
tion.
Minh-Thang Luong, Thuy-Dung Nguyen, and Min-Yen
Kan. 2010. Logical structure recovery in scholarly ar-
ticles with rich document features. International Jour-
nal of Digital Library Systems, 1(4):1?23.
?yvind Raddum Berg. 2011. High precision text extrac-
tion from PDF documents.
113
