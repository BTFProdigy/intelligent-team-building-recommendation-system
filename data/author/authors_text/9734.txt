Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 44?47,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Experiences with English-Hindi, English-Tamil and English-Kannada
Transliteration Tasks at NEWS 2009
Manoj Kumar Chinnakotla and Om P. Damani
Department of Computer Science and Engineering,
IIT Bombay,
Mumbai, India
{manoj,damani}@cse.iitb.ac.in
Abstract
We use a Phrase-Based Statistical Ma-
chine Translation approach to Translitera-
tion where the words are replaced by char-
acters and sentences by words. We employ
the standard SMT tools like GIZA++ for
learning alignments and Moses for learn-
ing the phrase tables and decoding. Be-
sides tuning the standard SMT parame-
ters, we focus on tuning the Character Se-
quence Model (CSM) related parameters
like order of the CSM, weight assigned to
CSM during decoding and corpus used for
CSM estimation. Our results show that
paying sufficient attention to CSM pays
off in terms of increased transliteration ac-
curacies.
1 Introduction
Transliteration of Named-Entities (NEs) is an im-
portant problem that affects the accuracy of many
NLP applications like Cross Lingual Search and
Machine Translation. Transliteration is defined
as the process of automatically mapping a given
grapheme sequence in the source language to a
grapheme sequence in the target language such
that it preserves the pronunciation of the origi-
nal source word. A Grapheme refers to the unit
of written language which expresses a phoneme
in the language. Multiple alphabets could be
used to express a grapheme. For example, sh
is considered a single grapheme expressing the
phoneme /SH/. For phonetic orthography like De-
vanagari, each grapheme corresponds to a unique
phoneme. However, for English, a grapheme like
c may map to multiple phonemes /S/,/K/. An ex-
ample of transliteration is mapping the Devana-
gari grapheme sequence E?s h{rF to its phoneti-
cally equivalent grapheme sequence Prince Harry
in English.
This paper discusses our transliteration ap-
proach taken for the NEWS 2009 Machine
Transliteration Shared Task [Li et al2009b, Li et
al.2009a]. We model the transliteration problem
as a Phrased-Based Machine Translation prob-
lem. Later, using the development set, we tune
the various parameters of the system like order of
the Character Sequence Model (CSM), typically
called language model, weight assigned to CSM
during decoding and corpus used to estimate the
CSM. Our results show that paying sufficient at-
tention to the CSM pays off in terms of improved
accuracies.
2 Phrase-Based SMT Approach to
Transliteration
In the Phrase-Based SMT Approach to Transliter-
ation [Sherif and Kondrak2007, Huang2005], the
words are replaced by characters and sentences are
replaced by words. The corresponding noisy chan-
nel model formulation where a given english word
e is to be transliterated into a foreign word h, is
given as:
h? = argmax
h
Pr(h|e)
= argmax
h
Pr(e|h) ? Pr(h) (1)
In Equation 1, Pr(e|h) is known as the translation
model which gives the probability that the char-
acter sequence h could be transliterated to e and
Pr(h) is known as the character sequence model
typically called language model which gives the
probability that the character sequence h forms a
valid word in the target language.
44
Ta
sk
Ru
n
Op
tim
al 
Pa
ram
ete
r S
et
Ac
cu
rac
y 
in 
top
-1 
 
Me
an
 F-
 
sc
ore
  
MR
R  
MA
Pr
ef
MA
P1
0  
MA
Ps
ys
En
gli
sh
-H
ind
i
Sta
nd
ard
 
LM
 O
rde
r: 5
, 
LM
 W
eig
ht:
 
0.6
0.4
7
0.8
6
0.5
8
0.4
7
0.1
8
0.2
0
En
gli
sh
-H
ind
i
No
n- 
sta
nd
ard
LM
 O
rde
r: 5
, 
LM
 W
eig
ht:
 
0.6
0.5
2
0.8
7
0.6
2
0.5
2
0.1
9
0.2
1
En
gli
sh
-Ta
mi
l
Sta
nd
ard
 
LM
 O
rde
r: 5
, 
LM
 W
eig
ht:
 
0.3
0.4
5
0.8
8
0.5
6
0.4
5
0.1
8
0.1
8
En
gli
sh
- 
Ka
nn
ad
a
Sta
nd
ard
 
LM
 O
rde
r: 5
, 
LM
 W
eig
ht:
 
0.3
0.4
4
0.8
7
0.5
5
0.4
4
0.1
7
0.1
8
Figure 1: NEWS 2009 Development Set Results
Ta
sk
Ru
n  
Ac
cu
rac
y i
n 
top
-1 
 
Me
an
 F-
 
sc
ore
  
MR
R  
MA
Pr
ef
MA
P1
0  
MA
Ps
ys
En
gli
sh
-H
ind
i
Sta
nd
ard
 
0.4
2
0.8
6
0.5
4
0.4
2
0.1
8
0.2
0
En
gli
sh
-H
ind
i
No
n-s
tan
da
rd
0.4
9
0.8
7
0.5
9
0.4
8
0.2
0
0.2
3
En
gli
sh
-Ta
mi
l
Sta
nd
ard
 
0.4
1
0.8
9
0.5
4
0.4
0
0.1
8
0.1
8
En
gli
sh
-K
an
na
da
Sta
nd
ard
 
0.3
6
0.8
6
0.4
8
0.3
5
0.1
6
0.1
6
Figure 2: NEWS 2009 Test Set Results
Given the parallel training data pairs, we pre-
processed the source (English) and target (Hindi,
Tamil and Kannada) strings into character se-
quences. We then ran the GIZA++ [Och and
Ney2003] aligner with default options to obtain
the character-level alignments. For alignment, ex-
cept for Hindi, we used single character-level units
without any segmentation. In case of Hindi, we
did a simple segmentation where we added the
halant character (U094D) to the previous Hindi
character. Moses Toolkit [Hoang et al2007] was
then used to learn the phrase-tables for English-
Hindi, English-Tamil and English-Kannada. We
also learnt the character sequence models on the
target language training words using the SRILM
toolkit [Stolcke2002]. Given a new English word,
we split the word into sequence of characters and
run the Moses decoder with the phrase-table of tar-
get language obtained above to get the transliter-
ated word. We ran Moses with the DISTINCT op-
tion to obtain the top k distinct transliterated op-
tions.
2.1 Moses Parameter Tuning
The Moses decoder computes the cost of each
translation as a product of probability costs of four
models: a) translation model b) language model
c) distortion model and d) word penalty as shown
in Equation 2. The distortion model controls the
Ta
sk
Ru
n  
Bas
eli
ne
 
Mo
de
l (LM
 
Or
de
r N=3)
Bes
t R
un
% Imp
rove
me
nt
En
gli
sh
-H
ind
i
Sta
nd
ard
 
0.4
0.4
2
5.0
0
En
gli
sh
-H
ind
i
No
n-s
tan
da
rd
0.3
7
0.4
9
32
.43
En
gli
sh
-Ta
mi
l
Sta
nd
ard
 
0.3
9
0.4
5
15
.38
En
gli
sh
- 
Ka
nn
ad
a
Sta
nd
ard
 
0.3
6
0.3
6
0.0
0
Figure 3: Improvements Obtained over Baseline
on Test Set due to Language Model Tuning
cost of re-ordering phrases (transliteration units)
in a given sentence (word) and the word penalty
model controls the length of the final translation.
The parameters ?T , ?CSM , ?D and ?W control
the relative importance given to each of the above
models.
Pr(h|e) = PrT (e|h)
?T ? PrCSM (h)
?CSM ?
PrD(h, e)
?D ? ?length(h)??W (2)
Since no re-ordering of phrases is required during
translation task, we assign a zero weight to ?D.
Similarly, we varied the word penalty factor ?W
between {?1, 0,+1} and found that it achieves
maximum accuracy at 0. All the above tuning was
done with a trigram CSM and default weight (0.5)
in Moses for ?T .
45
2.2 Improving CSM Performance
In addition to the above mentioned parameters,
we varied the order of the CSM and the mono-
lingual corpus used to estimate the CSM. For each
task, we started with a trigram CSM as mentioned
above and tuned both the order of the CSM and
?CSM on the development set. The optimal set
of parameters and the development set results are
shown in Figure 1. In addition, we use a mono-
lingual Hindi corpus of around 0.4 million doc-
uments called Guruji corpus. We extracted the
2.6 million unique words from the above corpus
and trained a CSM on that. This CSM which was
learnt on the monolingual Hindi corpus was used
for the non-standard Hindi run. We repeat the
above procedure of tuning the order of CSM and
?CSM and find the optimal set of parameters for
the non-standard run on the development set.
3 Results and Discussion
The details of the NEWS 2009 dataset for Hindi,
Kannada and Tamil are given in [Li et al2009a,
Kumaran and Kellner2007]. The final results of
our system on the test set are shown in Figure 2.
Figure 3 shows the improvements obtained on test
set by tuning the CSM parameters. The trigram
CSM model used along with the optimal Moses
parameter set tuned on development set was taken
as baseline for the above experiments. The results
show that a major improvement (32.43%) was ob-
tained in the non-standard run where the monolin-
gual Hindi corpus was used to learn the CSM. Be-
cause of the use of monolingual Hindi corpus in
the non-standard run, the transliteration accuracy
improved by 22.5% when compared to the stan-
dard run. The improvements (15.38%) obtained in
Tamil are also significant. However, the improve-
ment in Hindi standard run was not significant. In
Kannada, there was no improvement due to tuning
of LM parameters. This needs further investiga-
tion.
The above results clearly highlight the impor-
tance of improving CSM accuracy since it helps
in improving the transliteration accuracy. More-
over, improving the CSM accuracy only requires
monolingual language resources which are easy
to obtain when compared to parallel transliteration
training data.
4 Conclusion
We presented the transliteration system which we
used for our participation in the NEWS 2009 Ma-
chine Transliteration Shared Task on Translitera-
tion. We took a Phrase-Based SMT approach to
transliteration where words are replaced by char-
acters and sentences by words. In addition to the
standard SMT parameters, we tuned the CSM re-
lated parameters like order of the CSM, weight as-
signed to CSM and corpus used to estimate the
CSM. Our results show that improving the ac-
curacy of CSM pays off in terms of improved
transliteration accuracies.
Acknowledgements
We would like to thank the Indian search-engine
company Guruji (http://www.guruji.com)
for providing us the Hindi web content which was
used to train the language model for our non-
standard Hindi runs.
References
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and
Ondej Bojar. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In In Proceed-
ings of ACL, Demonstration Session, pages 177?
180.
Fei Huang. 2005. Cluster-specific Named Entity
Transliteration. In HLT ?05: Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 435?442, Morristown, NJ, USA. Association
for Computational Linguistics.
A. Kumaran and Tobias Kellner. 2007. A Generic
Framework for Machine Transliteration. In SIGIR
?07: Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 721?722, New
York, NY, USA. ACM.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report on NEWS 2009 Ma-
chine Transliteration Shared Task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of NEWS 2009
Machine Transliteration Shared Task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
46
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
Based Transliteration. In In Proceedings of ACL
2007. The Association for Computer Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In In Proceedings of Intl.
Conf. on Spoken Language Processing.
47
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1058?1068,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lexical Co-occurrence, Statistical Significance, and Word Association
Dipak L. Chaudhari
Computer Science and Engg.
IIT Bombay
dipakc@cse.iitb.ac.in
Om P. Damani
Computer Science and Engg.
IIT Bombay
damani@cse.iitb.ac.in
Srivatsan Laxman
Microsoft Research India
Bangalore
slaxman@microsoft.com
Abstract
Lexical co-occurrence is an important cue for
detecting word associations. We propose a
new measure of word association based on a
new notion of statistical significance for lex-
ical co-occurrences. Existing measures typ-
ically rely on global unigram frequencies to
determine expected co-occurrence counts. In-
stead, we focus only on documents that con-
tain both terms (of a candidate word-pair) and
ask if the distribution of the observed spans of
the word-pair resembles that under a random
null model. This would imply that the words
in the pair are not related strongly enough
for one word to influence placement of the
other. However, if the words are found to oc-
cur closer together than explainable by the null
model, then we hypothesize a more direct as-
sociation between the words. Through exten-
sive empirical evaluation on most of the pub-
licly available benchmark data sets, we show
the advantages of our measure over existing
co-occurrence measures.
1 Introduction
Lexical co-occurrence is an important indicator of
word association and this has motivated several
co-occurrence1 measures for word association like
PMI (Church and Hanks, 1989), LLR (Dunning,
1993), Dice (Dice, 1945), and CWCD (Washtell and
Markert, 2009). In this paper, we present a new mea-
sure of word association based on a new notion of
statistical significance for lexical co-occurrences. In
general, a lexical co-occurrence could refer to a pair
1We use the term co-occurrence to refer to a pair of words
that co-occur in a document with an arbitrary number of inter-
vening words.
of words that co-occur in a large number of docu-
ments; or it could refer to a pair of words that, al-
though co-occur only in a small number of docu-
ments, occur close to each other within those docu-
ments. We formalize these ideas and construct a sig-
nificance test that allows us to detect different kinds
of co-occurrences within a single unified framework
(a feature which is absent in current measures for
co-occurrence). Another distinguishing feature of
our measure is that it is based solely on the co-
occurrence counts in the documents containing both
words of the pair, unlike all existing measures which
also take global unigram frequencies in account.
We need a null hypothesis that can account for
an observed co-occurrence as a pure chance event
and this in-turn requires a corpus generation model.
Documents in a corpus can be assumed to be gen-
erated independent of each other. Existing co-
occurrence measures further assume that each docu-
ment is drawn from a multinomial distribution based
on global unigram frequencies. The main concern
with such a null model is the overbearing influence
of the unigram frequencies on the detection of word
associations. For example, the association between
anomochilidae (dwarf pipe snakes) and snake could
go undetected in our wikipedia corpus, since less
than 0.1% of the pages containing snake also con-
tained anomochilidae. Also, under current models,
the expected span2 of a word pair is very sensitive
to the associated unigram frequencies: the expected
span of a word pair composed of low frequency un-
igrams is much larger than that with high frequency
unigrams. This is contrary to how word associa-
2The span of an occurrence of a word-pair is the ?unsigned
distance? between the positions of the corresponding word oc-
currences.
1058
tions appear in language, where semantic relation-
ships manifest with small inter-word distances irre-
spective of the underlying unigram distributions.
Based on these considerations we employ a null
model that represents each document as a bag of
words 3. A random permutation of the associated
bag of words gives a linear representation for the
document. Under this null model, the locations of
an unrelated pair of words will likely be randomly
distributed in the documents in which they co-occur.
If the observed span distribution of a word-pair re-
sembles that under the (random permutation) null
model, then the relation between the words is not
strong enough for one word to influence the place-
ment of the other. However, if the words are found
to occur closer together than explainable by our null
model, then we hypothesize a more direct associa-
tion between the words. Therefore, this null model
detects biases in span distributions of word-pairs
while being agnostic to variations in global unigram
frequencies.
In this paper, we propose a new measure of word
association based on the statistical significance of
the observed span distribution of a word-pair. We
perform extensive experiments on all the publicly
available benchmark data sets4 and compare our
measure against other popular co-occurrence mea-
sures. Our experiments demonstrate the advan-
tages of our measure over all the competing mea-
sures. The ranked list of word associations output
by our measure has the best correlation with the
corresponding gold-standard in three (out of seven)
data sets in our experiments, while remaining in
the top three in other four datasets. While differ-
ent measures perform best on different data sets,
our measure outperforms other measures by being
consistently either the best measure or very close
to the best measure on all the data sets. The aver-
age deviation of our measure?s correlation with the
gold-standard from the best measure?s correlation
with the gold-standard (average taken across all the
3There can be many ways to associate a bag of words with a
document. Details of this association are not important for us,
except that the bag of words provides some kind of quantitative
summary of the words within the document.
4We exclude very small data sets of 80 word pairs or less.
Sizes of the seven datasets we used range from 351 word-pairs
to 83,713 word-pairs.
datasets) is 0.02, which is the least average deviation
among all the measures, the next best deviations be-
ing 0.04 and 0.06.
The paper is organized as follows. We present our
notion of statistical significance of span distribution
in Section 2. Algorithm for computing the proposed
word association measure is described in Section 3.
We discuss related work in Section 4. Performance
evaluation is presented in Section 5 followed with
conclusions in Section 6.
2 Lexically significant co-occurrences
Evidence for significant lexical co-occurrences can
be gathered at two levels in the data ? document-
level and corpus-level. First, at the document level,
we may find that for a given word-pair, a surpris-
ingly high proportion of its occurrences within a
document have smaller spans than they would have
by random chance. Second, at the corpus-level,
we may find a pair of words appearing closer-than-
random in multiple documents in the corpus. We
now describe how to combine both kinds of evidence
to decide whether the nearby occurrences of a word-
pair are statistically significant or not.
Let the frequency f of a word-pair ? in a
document D, be the maximum number of non-
overlapped occurrences of ? in D. A set of occur-
rences of a word-pair is said to be non-overlapped if
the words corresponding to one occurrence from the
set do not appear in-between the words correspond-
ing to any other occurrence from the set.
Let f?x denote the maximum number of non-
overlapped occurrences of ? in D with span less
than a given threshold x. We refer to f?x as the span-
constrained frequency of ? in D. Note that f?x can-
not exceed f .
2.1 Document-level significant co-occurrence
To assess the statistical significance of the word-pair
?we ask if the span-constrained frequency f?x (of ?)
is more than what we would expect in a document
of size ` containing f ?random? occurrences of ?.
Our intuition is that if two words are associated in
some way, they will often appear close to each other
in the document and so the distribution of the spans
will typically exhibit a bias toward values less than
a suitably chosen threshold x.
1059
Definition 1 Consider the null hypothesis that the
linear representation of a document is generated by
choosing a random permutation of the bag of words
associated with the document. Let ` be the length of
the document and f denote the frequency of a word-
pair in the document. For a given a span threshold
x, we define pix(f?x, f, `) as the probability under the
null that the word-pair will appear in the document
with a span-constrained frequency of at least f?x.
Observe that pix(0, f, `) = 1 for any x > 0;
also, for x ? ` we have pix(f, f, `) = 1 (i.e. all f
occurrences will always have span less than x for
x ? `). However, for typical values of x (i.e. for
x  `) the probability pix(f?x, f, `) decreases with
increasing f?x. For example, consider a document
of length 400 with 4 non-overlapped occurrences
of ?. The probabilities of observing at least 4, 3,
2, 1 and 0 occurrences of ? within a span of 20
words are 0.007, 0.09, 0.41, 0.83, and 1.0 respec-
tively. Since pi20(3, 4, 400) = 0.09, even if 3 of the
4 occurrences of ? have span less than 20 words,
there is 9% chance that the occurrences were a con-
sequence of a random event. As a result, if we de-
sired a confidence-level of at least 95%, we would
have to declare observed co-occurrences of ? as in-
significant.
Given an  (0 <  < 1) and a span threshold
x (? 0) the document D is said to support the hy-
pothesis ?? is an -significant word-pair within the
document? if we have [pix(f?x, f, `) < ]. We re-
fer to  as the document-level evidence of the lexical
co-occurrence of ?.
2.2 Corpus-level significant co-occurrence
We now describe how to aggregate evidence for lex-
ical significance by considering the occurrence of
? across multiple documents in the corpus. Let
{D1, . . . , DK} denote the set ofK documents (from
out of the entire corpus) that contain at least one
occurrence of ?. Let `i be the length of Di, fi
be the frequency of ? in Di, and, f?xi be the span-
constrained frequency of ? in Di. Define indicator
variables zi, i = 1, . . . ,K as:
zi =
{
1 if pix(f?xi , fi, `i) < 
0 otherwise (1)
As discussed previously, zi indicates whether ??
is an -significant word-pair within the document
Di.? Note that we view f?xi as the only random quan-
tity here, with x fixed by the user, and `i and fi
fixed given the document Di and word-pair ?. Let
Z =
?K
i=1 zi; Z models the number of documents
(out of K) that support the hypothesis ?? is an -
significant word-pair.? The expected value of Z is
given by
E(Z) =
K?
i=1
E(zi)
=
K?
i=1
pix(g,x(fi, `i), fi, `i) (2)
where g,x(fi, `i) is given by Definition 2 below.
Definition 2 Given a document of length ` in which
a word-pair has a frequency of f , and given a span
threshold x, we define g,x(f, `) as the smallest r for
which the inequality [pix(r, f, `) < ] holds.
Note that g,x(f, `) is well-defined since pix(r, f, `)
is non-increasing with respect to r. For the
example given earlier, g0.2,20(4, 400) = 3 and
g0.05,20(4, 400) = 4. Since each document in the
corpus is assumed to be generated independently,
zi?s are independent random variables and we can
bound the deviation of the observed value of Z from
its expectation using Hoeffding?s Inequality ? for
any t > 0, we have
P [Z ? E(Z) +Kt] ? exp(?2Kt2)
= ? (3)
Recall that Z models the number of documents sup-
porting the hypothesis ?? is an -significant word-
pair.?). Thus, the upper-bound ? (= exp(?2Kt2)),
0 < ? < 1 denotes the upper-bound on the prob-
ability that just due to random chance, more than
(E(Z) +Kt) documents out of K will support the
hypothesis ?? is an -significant word-pair.? We
call ? the corpus-level evidence of the lexical co-
occurrence ?. For example, in our corpus, the word-
pair (canyon, landscape) occurs in K = 416 doc-
uments. For  = 0.1, we have -significant occur-
rences in Z = 33 documents (out of 416) , while
E(Z) = 14.34. Suppose we want to be 99% sure
that the occurrences of (canyon, landscape) in the
33 documents were a consequence of non-random
phenomena. Let ? = 1 ? 0.99 = 0.01. By setting
1060
word-1 word-2
(0.1, 0.1) (0.1, 0.4) (0.4, 0.1)
algae green mold pool
amuse entertain clown amaze
damn hell mad bad
rat dirty ugly disease
sedative drug narcotic calm
topping chocolate flavour caramel
umbrella rain dry shade
unknown known dark secret
worm insect dirt fishing
wrap cover seal bandage
Table 1: Examples of word-pairs from Florida dataset
having statistically significant co-occurrences in the
wikipedia corpus for different (, ?) combinations under
a span constraint of 20 words.
t =
?
ln ?/(?2K) = 0.07, we get E(Z) + Kt =
43.46. Only ifZ was 44 or more, there would be less
than 1% chance of that being a random phenomena.
Thus, we cannot be 99% sure that the observed co-
occurrences in the 33 documents are non-random.
Hence, our test declares (canyon, landscape) as in-
significant at  = 0.1, ? = 0.01. We now summarize
our significance test in the definition below.
Definition 3 (Significant lexical co-occurrence)
Consider a word-pair ? and a set of K documents
containing at least one occurrence each of ?. Fix
a span threshold of x (> 0), a document-level
evidence of  (0 <  < 1) and a corpus-level
evidence of ? (0 < ? < 1). Let Z denote
the number of documents (out of K) that sup-
port the hypothesis ?? is -significant within
the document.? The word-pair ? is said to be
(, ?)-significant if we have [Z ? E(Z) + Kt],
where t = ?log ?/(?2K) and E(Z) is given by
Eq. (2). The ratio [Z/(E(Z) + Kt)] is called the
Co-occurrence Significance Ratio (CSR) for ?.
2.3 Discussion
The significance test of Definition 3 gathers both
document-level and corpus-level evidence from data
in calibrated amounts. Prescribing  fixes the
strength of the document-level hypothesis in our
test, while, ?, controls the extent of corpus-level ev-
idence we need to declare a word-pair as significant.
A small ? demands that there must be multiple doc-
uments in the corpus, each of which, individually
have some evidence of relatedness for the pair of
words.
By running the significance test with different val-
ues of  and ?, the CSR test can be used to detect dif-
ferent types of lexically significant co-occurrences.
For example, the strongest lexical co-occurrences
would have both strong document-level evidence
(low ) as well as high corpus-level evidence (low
?). Informally, these would represent pairs of words
that appear multiple times with small spans within a
document, in many documents, and in-practice, we
find that multi-word expressions or pairs of words
separated by stop words tend to dominate this type.
On the other hand, a higher  would represent word-
pairs that appear relatively farther apart within a
document, or a higher ? would represent word-pairs
that appear together in relatively fewer documents.
Note that to detect co-occurrences that exclusively
correspond to (say) low  and high ?, we would have
to run the test with low  and high ?, and then re-
move word-pairs that were also found significant at
low  and low ?.
In Table 1, we present some examples of different
types of co-occurrences. The table lists word-pairs
that were found to be statistically significant for dif-
ferent choices of (, ?). Note that a word-pair is re-
ported under ( = 0.1, ? = 0.4) or ( = 0.4, ? =
0.1) only if it was not also found significant under
other two parameter settings. The strongest corre-
lations are the word-pairs corresponding to ( =
0.1, ? = 0.1) e.g., algae-green, rat-dirty and worm-
insect. Different sets of weaker co-occurrences are
detected depending on whether we relaxed ? or .
For example, algae-mold is significant at a higher ?,
while algae-pool is significant for higher .
The semantic notion of word association is an
abstract concept and different kinds of associations
(with potentially different statistical characteriza-
tions) may be preferred by human judges in differ-
ent situations. While in Section 5, we discuss in de-
tail various datasets used, the evaluation methodol-
ogy, and the performance of CSR across datasets,
we wish to point out here that in 3 out of 5 cross-
validation runs for wordsim dataset, the best per-
forming CSR parameters were x = 50w,  = 0.1
and ? = 0.9, while in 3 out of 5 runs for Minnesota
dataset, the best performing CSR parameters were
x = 20w,  = 0.3 and ? = 0.5. This gives us some
indication that different kinds of word associations
were preferred in different data sets.
1061
3 Computing Co-occurrence Significance
Ratio(CSR)
There are three main steps for computing CSR
and the pseudocodes for these are listed in Proce-
dures 1, 2 & 3. Of these, the first two can be run
offline since they do not depend on the text corpus.
They need to be run only once, after which CSR can
be computed for any word-pair on any given corpus
of documents. We describe these steps in the sub-
sections below.
3.1 Computing histogram histf,`,x(?)
The first step is to compute a histogram for the span-
constrained frequency, f?x, of a word-pair whose fre-
quency is f in a document of length `, given a cho-
sen span threshold of x (under our null model).
Definition 4 Given a document of length ` and a
span threshold of x, we define histf,`,x(f?x) as the
number of ways to embed f non-overlapped occur-
rences of a word-pair in the document such that ex-
actly f?x occurrences have span less than x.
Procedure 1 ComputeHist(f, `, x) ? Offline
Input f - number of non-overlapped occurrences; ` - document length;
x - span threshold
Computes histf,`,x[?] as per Definition 4
1: Initialize histf,`,x[f?x]? 0 for f?x = 0, . . . , f2: if f > ` then
3: return
4: if f = 0 then
5: histf,`,x[0]? 16: return
7: for i? 1 to (`? 1) do
8: for j ? (i+ 1) to ` do
9: histf?1,`?j,x ? ComputeHist(f ? 1, `? j, x)10: for k ? 0 to f ? 1 do
11: if (j ? i) < x then
12: histf,`,x[k + 1]? histf,`,x[k + 1]
+ histf?1,`?j,x[k]13: else
14: histf,`,x[k]? histf,`,x[k] + histf?1,`?j,x[k]
Procedure 1 lists the pseudocode for computing
the histogram histf,`,x. The main steps involve se-
lecting a start and end position for embedding the
very first occurrence (lines 7-8) and then recursively
calling ComputeHist(?, ?, ?) (line 9). The i-loop
selects a start position for the first occurrence of
the word-pair, and the j-loop selects the end posi-
tion. The recursion step now computes the num-
ber of ways to embed the remaining (f ? 1) non-
overlapped occurrences in the remaining (` ? j)
positions. Once we have histf?1,`?j , we check
whether the occurrence introduced at positions (i, j)
will contribute to the f?x count. If (j ? i) < x,
whenever there are k span-constrained occurrences
in positions (j + 1) to `, there will be (k + 1)
span-constrained occurrences in positions 1 to `.
Thus, we increment histf,`[k + 1] by the quantity
histf?1,`?j [k] (lines 10-12). However, if (j ? i) >
x, there is no contribution to the span-constrained
frequency from the (i, j) occurrence, and so we in-
crement histf,`[k] by the quantity histf?1,`?j [k]
(lines 10-11, 13-14). Finally, we note that in our
implementation we use memorization to avoid re-
dundant recursive calls.
3.2 Computing pix(?, f, `) distribution
Procedure 2 ComputeP iDist(f, `, x) ? Offline
Input f - number of non-overlapped occurrences; ` - document length;
x - span threshold
Computes Distribution pix[f, `, ?] as per Definition 1 and g,x[f, `] as
per Definition 2
1: N [f, `, x] =?fk=0 histf,`,x[k]
2: for f?x ? 0 to f do
3: Nx[f?x, f, `]??fk=f?x histf,`,x[k]
4: pix[f?x, f, `]? Nx[f?x,f,`]N [f,`,x]
5: g,x[f, `]? min{r | pix[r, f, `] < }
The second offline step is computation of the
pix(?, f, `) distribution. We store the number of ways
of embedding f non-overlapped occurrences of a
word-pair in a document of length ` in the array
N [f, `]. Similarly, the array Nx[f?x, f, `] stores the
number of ways of embedding f non-overlapped oc-
currences of the word-pair in a document of length `,
such that at least f?x of the f occurrences have span
less than x. To compute N [f, `, x] and Nx[f?x, f, `],
we need the histogram histf,`,x[?] which is the out-
put of Procedure 1. Procedure 2 lists the pseu-
docode for computing pix(f?x, f, `) fromN(f, `) and
Nx(f?x, f, `) given histf,` from Procedure 1 (For the
sake of readability the pseudocode does not describe
some optimizations that we used in our implementa-
tion).
The Procedure 1 is exponential in f and ` but
it does not depend on the data corpus. Hence, we
can run the Procedures 1 and 2 off-line, and publish
the pix[] and g,x[] tables for various x, f?x, f and
1062
`. Using these tables5, anyone wishing to compute
CSR needs to only run Procedure 3.
3.3 Computing CSR for a given word-pair
Procedure 3 ComputeCSR(?, , ?, x)
Input ? - word-pair;  - document-level evidence; ? - corpus-level ev-
idence; x - span threshold; Corpus of documents
Computes CSR(?) - Co-occurrence Significance Ratio (CSR) for ?
as per Definition 3
1: D ? {D1, . . . , DK} // Set of documents from the corpus that
each contain at least one occurrence of ?.
2: t??log ?/(?2K)
3: Z ? 0 and ZE ? 04: for i? 1 to K do
5: `i = Length of Di6: fi = Frequency of ? in Di
7: f?xi = Span-constrained frequency of ? in Di
8: if pix[f?xi , fi, `i] <  then9: zi ? 110: else
11: zi ? 012: Z ? Z + zi13: ZE ? ZE + pix[g[fi, `i, x], fi, `i]14: CSR(?) = Z/(ZE +Kt)
Procedure 3 implements the significance test
given in Definition 3 and requires that the pix[] and
g,x[] tables have already been computed offline.
The first step is to determine the subsetD of docu-
ments containing the given word-pair (line 1). Then
we compute t based on ? and K (the size of D)
(line 2). Next we determine how many of the K
documents support the hypothesis ?? is -significant
within the document? (lines 3-12). The expected
number of documents supporting the hypothesis is
accumulated in ZE (line 13). CSR is then computed
as the ratio of Z to (ZE +Kt) (line 14).
3.4 Run-time overhead
The computation of Co-occurrence Significance Ra-
tio (CSR) as given in Definition 3 might appear
more complex than the simple formulae for other
co-occurrence measures given in Table 2. However,
bulk of the complexity in calculating CSR lies in
the one-time (data independent) off-line computa-
tion of the pix[] and g,x[] tables. Once these tables
are published, the cost of comparing CSR for a given
word pair is comparable to the cost of computing
any other (spanned) measure in Table 2. The main
data-dependent computations for a spanned measure
5http://www.cse.iitb.ac.in/?damani/papers/EMNLP11/
resources.html
are in determining span-constrained frequencies; all
other steps are simple arithmetic operations or mem-
ory lookups. To illustrate this, Procedure 4 gives de-
tails of computing PMI. The comparison of Proce-
dures 3 and 4 shows their almost parallel structures.
The main overhead in these procedures is incurred
in line 7, where span-constrained frequencies in a
given document are computed.
Procedure 4 ComputePMI(a, b)
Input (x, y) - word pair;
Computes PMI (Table 2) for (x, y).
1: let D = {D1, . . . , DK} // set of documents containing at least
one occurrence of ?.
2: N = total number of words in corpus
3: (fx,fy) = unigram frequencies of x, y in corpus
4: (px,py) = (fx/N ,fy/N )
5: f? = 0
6: for i? 1 to K do
7: f?i = span-constrained frequency of ? in Di
8: f? = f? + f?i9: p?x,y = f?/N
10: PMI = log( p?x,ypxpy )
4 Related Work
Existing word association measures can be divided
into three broad categories: (i) Co-occurrence mea-
sures that rely on co-occurrence frequencies of both
words in a corpus in addition to the individual
unigram frequencies (Table 2), (ii) Distributional
similarity-based measures that characterize a word
by the distribution of other words around it (Agirre
et al, 2009; Bollegala et al, 2007; Chen et al, 2006;
Wandmacher et al, 2008), and (iii) Knowledge-
based measures that use knowledge-sources like
thesauri, semantic networks, or taxonomies (Milne
and Witten, 2008; Hughes and Ramage, 2007;
Gabrilovich and Markovitch, 2007; Yeh et al, 2009;
Strube and Ponzetto, 2006; Finkelstein et al, 2002;
Liberman and Markovitch, 2009).
In this paper, we focus on comparison with
other co-occurrence measures. These measures
are used in several domains like ecology, psy-
chology, medicine, and language processing. Ta-
ble 2 lists several measures chosen from all these
domains. Except Ochiai (Ochiai, 1957; Janson
and Vegelius, 1981) and the recently introduced
1063
Method Formula
CSR (this work) Z/(E(Z) +Kt)
CWCD (Washtell and
Markert, 2009)
f?(x,y)
p(x)
1/max(p(x),p(y))
M
Dice (Dice, 1945) 2f?(x,y)f(x)+f(y)
LLR (Dunning, 1993)
?
x? ? {x,?x}
y? ? {y,?y}
p(x?, y?)log p(x
?,y?)
p(x?)p(y?)
Jaccard (Jaccard, 1912) f?(x,y)f(x)+f(y)?f?(x,y)
Ochiai (Janson and Veg-
elius, 1981)
f?(x,y)?
f(x)f(y)
Pearson?s ?2 test
?
x? ? {x,?x}
y? ? {y,?y}
(f?(x?,y?)?Ef?(x?,y?))2
Ef?(x?,y?)
PMI (Church and Hanks,
1989)
log p(x,y)p(x)p(y)
SCI (Washtell and Mark-
ert, 2009)
p(x,y)
p(x)
?
p(y)
T-test f?(x,y)?Ef?(x,y)?
f?(x,y)
(
1? f?(x,y)N
)
N Total number of tokens in the corpus
f(x), f(y) unigram frequencies of x, y in the corpus
p(x), p(y) f(x)/N, f(y)/N
f?(x, y) Span-constrained (x, y) word pair frequency in corpus
p?(x, y) f?(x, y)/N
M Harmonic mean of the spans of f?(x, y) occurrences
Ef?(x, y) Expected value of f?(x, y)
Table 2: Co-occurrence measures.
CWCD6 (Washtell and Markert, 2009) all other
measures are well-known in the NLP commu-
nity (Pecina and Schlesinger, 2006). Our results
show that Ochiai and Chi-Square have almost iden-
tical performance, differing only in 3rd decimal dig-
its. Rankings produced by Chi-square is almost
monotonic with respect to the rankings produced by
Ochiai. This is because, for most word pairs (x, y),
[f(x)  N ], [f(y)  N ], [f(x, y)  f(x)], and
[f(x, y)  f(y)]. Therefore three of the four terms
in the Chi-square summation become zero7 and the
fourth term approximates to the square of Ochiai.
Similarly Jaccard and Dice coincide. While present-
ing our experimental results, we report these pairs of
measures together.
6CWCD was reported in (Washtell and Markert, 2009) as
the best performing variant among the so-called windowless (or
spanless) measures. In our experiments, we implemented win-
dowed (spanned) version of the CWCD measure.
7For example, f?(x,?y) ? Ef(x,?y) = f(x) ? N ?
pf(x) ? pf(?y) = f(x) ? 1N ? f(x) ? f(?y) = f(x) ?1
N ? f(x)?N = 0.
Aspect Data Set No. of
Respon-
dents
No. of
Word
Pairs
No. of
Filtered
Word
Pairs
Semantic
relatedness
wordsim 16 353 351
(Finkelstein et al,
2002)
Edinburg (Kiss et al,
1973)
100 325,588 83,713
Florida (Nelson et
al., 1980)
5,019 65,523 59,852
Free-
Association
Goldfarb-Halpern
(Goldfarb and
Halpern, 1984)
316 410 384
Kent (Kent and
Rosanoff, 1910)
1,000 14,576 14,086
Minnesota (Russell
and Jenkins, 1954)
1,007 10,447 9,649
White-Abrams
(White and Abrams,
2004)
440 745 652
Table 3: Characteristics of data sets used.
5 Performance Evaluation
Two main aspects of word association studied in lit-
erature are: a) semantic relatedness, and b) free as-
sociation. Semantic relatedness encompasses many
different relationships between words, like syn-
onymy, meronymy, antonymy, and functional asso-
ciation (Budanitsky and Hirst, 2006). Free associ-
ation refers to the first response-words that come
to mind when presented with a stimulus. (ESSLLI,
2008). We experiment with all the publicly available
datasets that come with gold standard judgement of
these aspects, except the very small ones with less
than 80 word-pairs8.
5.1 Datasets
Details9 of the datasets used in our experiments are
listed in Table 3. Each data set comes with a gold-
standard of human judgments - a ranked list of asso-
ciation scores for the word-pairs in the data set. The
wordsim dataset was prepared by asking the subjects
to estimate the relatedness of the word pairs on a
8(MillerCharles (Miller and Charles, 1991), Rubenstein-
Goodenough (Rubenstein and Goodenough, 1965) and
TOEFL (Landauer and Dumais, 1997))
9We removed word-pairs containing multiword expressions.
For data sets with more than 10,000 word-pairs, we filtered out
pairs that contain stop words listed in (StopWordList, 2010).
For Edinburg (size 275393 after previous filtering), we further
filtered word-pairs where the response was supported by only
one respondant. Original and filtered data sets are available at
http://www.cse.iitb.ac.in/?damani/papers/EMNLP11/resources.html
1064
Ed
inb
urg
(83
,71
3)
Flo
rid
a
(59
,85
2)
Ke
nt
(14
,08
6)
Mi
nn
eso
ta
(9,
64
9)
W
hit
e-
Ab
ram
s
(65
2)
Go
ldf
arb
-
Ha
lpe
rn
(38
4)
wo
rds
im
(35
1)
CSR 0.25 0.30 0.42 0.31 0.34 0.10 0.63
CWCD 0.23 0.23 0.40 0.30 0.21 0.19 0.54
Dice (Jaccard) 0.20 0.27 0.43 0.32 0.21 0.09 0.59
LLR 0.20 0.26 0.40 0.29 0.18 0.03 0.51
Ochiai (?2) 0.24 0.30 0.43 0.31 0.29 0.08 0.62
PMI 0.22 0.25 0.36 0.26 0.22 0.11 0.69
SCI 0.24 0.27 0.38 0.27 0.23 0.06 0.37
TTest 0.17 0.23 0.37 0.26 0.17 -0.02 0.45
Table 4: Comparison of the average Spearman coefficients obtained across five cross-validation runs by different
measures. The best performing measure for each data-set is shown in bold. All standard deviations for Edinburg and
Florida were less than 0.01, for Kent and Minnesota were between 0.01 and 0.02, for White-Abrams were between
0.05 and 0.08, for Goldfarb-Halpern between 0.05 and 0.15 and for wordsim were between 0.02 and 0.15. Number of
word-pairs in each dataset is shown in brackets against its name.
Ed
inb
urg
(83
,71
3)
Flo
rid
a
(59
,85
2)
Ke
nt
(14
,08
6)
Mi
nn
eso
ta
(9,
64
9)
W
hit
e-
Ab
ram
s
(65
2)
Go
ldf
arb
-
Ha
lpe
rn
(38
4)
wo
rds
im
(35
1)
Wo
rst
Ra
nk
Av
g.
De
via
tio
n
Wo
rst
De
via
tio
n
CSR 0.00 (1) 0.00 (1) 0.01 (3) 0.01 (2) 0.00 (1) 0.09 (3) 0.06 (2) 3 0.02 0.09
CWCD 0.02 (4) 0.07 (7) 0.03 (4) 0.02 (4) 0.13 (5) 0.00 (1) 0.15 (5) 7 0.06 0.15
Dice (Jaccard) 0.05 (6) 0.03 (3) 0.00 (1) 0.00 (1) 0.13 (5) 0.10 (4) 0.10 (4) 6 0.06 0.13
LLR 0.05 (6) 0.04 (5) 0.03 (4) 0.03 (5) 0.16 (7) 0.16 (7) 0.18 (6) 7 0.09 0.18
Ochiai (?2) 0.01 (2) 0.00 (1) 0.00 (1) 0.01 (2) 0.05 (2) 0.11 (5) 0.07 (3) 5 0.04 0.11
PMI 0.03 (5) 0.05 (6) 0.07 (8) 0.06 (7) 0.12 (4) 0.08 (2) 0.00 (1) 8 0.06 0.12
SCI 0.01 (2) 0.03 (3) 0.05 (6) 0.05 (6) 0.11 (3) 0.13 (6) 0.32 (8) 8 0.10 0.32
TTest 0.08 (8) 0.07 (7) 0.06 (7) 0.06 (7) 0.17 (8) 0.21 (8) 0.24 (7) 8 0.13 0.24
Table 5: Comparison of deviations from the best performing measure on each data set. Number of word-pairs in each
dataset is shown in brackets against its name. Figures in brackets against the deviation values denote the ranks of the
measures in the corresponding data sets.
scale from 0 to 10 (Finkelstein et al, 2002). The
methodology for collecting free association data is
explained at (ESSLLI, 2008): The degree of free as-
sociation between a stimulus (S) and response (R) is
the percentage of respondents who respond R as the
first response when presented with stimulus S.
These datasets are of varying size, and they were
constructed at different point in time, in different ge-
ographies. This allows us to compare different mea-
sures comprehensively under varying range of cir-
cumstances. To the best of our knowledge, no pre-
vious work has reported such a detailed comparison
of co-occurrence measures.
5.2 Resources Used
We use the Wikipedia (Wikipedia, April 2008) cor-
pus with 2.7 million articles (total of 1.24 Giga-
words). We did no pre-processing - no lemmatiza-
tion or function-word removal. When counting doc-
ument size (in words), punctuations were ignored.
Documents larger than 1500 words were partitioned
such that each part was at most 1500 words10. We
indexed the corpus using Lucene search engine li-
brary and used Lucene APIs to obtain various statis-
tics and documents containing given word-pairs.
5.3 Methodology
Each measure listed in Table 2 produces a ranked
list of association scores for the word-pairs in a data
set. We evaluate each measure by the Spearman?s
rank correlation between the ranking produced by
the measure and the gold-standard ranking.
The span threshold (or window-width) x is a user-
defined parameter in all measures. In addition, CSR
has the parameters  and ?. For any measure, the
ranking of word-pairs will likely change with chang-
10While this limit can be raised using heavier computing re-
sources, we believe that partitioning documents of sizes greater
than 1500 words was reasonable (especially since typical span
values we used were less than 50, much less than 1500).
1065
Method Resource
wordsim
wordsim sim rel Esslli
(353) (203) (252) (272)
PMI Wikipedia 0.69 0.72 0.68 0.32
Ochiai (?2) Wikipedia 0.62 0.68 0.62 0.44
Significance Ratio (CSR) Wikipedia 0.63 0.70 0.64 0.43
Latent Semantic Analysis (Wandmacher et al, 2008) Newspaper corpus - - - 0.38
Graph Traversal (WN30g) (Agirre et al, 2009)) Wordnet 0.66 0.72 0.56 -
Bag of Words based Distributional Similarity (BoW) (Agirre et al, 2009)) Web corpus 0.65 0.70 0.62 -
Context Window based Distributional Similarity (CW) (Agirre et al, 2009)) Web corpus 0.60 0.77 0.46 -
Hyperlink Graph (Milne and Witten, 2008) Wikipedia hyperlinks graph 0.69 - - -
Random Graph Walk (Hughes and Ramage, 2007) WordNet 0.55 - - -
Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) Wikipedia concepts 0.75 - - -
(reimplemented in (Yeh et al, 2009)) (0.71)
Normalized Path-length (lch) (Strube and Ponzetto, 2006) Wikipedia category tree 0.55 - - -
Thesarus based (Jarmasz, 2003) Roget?s thesaurus 0.55 - - -
Latent Semantic Analysis (Finkelstein et al, 2002) Web corpus 0.56 - - -
Table 6: Comparison of co-occurrence based measures with knowledge-based and distributional similarity based
measures. These other measures have not been applied to the free association datasets shown in Table 3. Data for
missing entries is not available. Note that sim and rel are subsets of wordsim dataset. Number of word-pairs in each
dataset is shown in brackets against its name.
ing parameter values. Hence we follow the standard
methodology of fixing parameters through cross val-
idation. Specifically, we partition the data into five
folds, four of which are used for training and one
hold-out fold is used for testing. For each mea-
sure, the parameter values that achieve best corre-
lation with human judgments on 4 training folds are
used to predict on the 1 hold-out testing fold. This
experiment is repeated 5 times for different training
and test folds. The average rank correlation obtained
by each measure over 5 cross-validation runs is re-
ported for each dataset. We varied  and ? between
0.01 and 0.90 and x between 5 and 50 words.
5.4 Results
For each measure and for each data set, the aver-
age correlation over the 5 cross-validation runs is
reported in Table 4. The corresponding standard de-
viations are mentioned in the table?s caption. The
best performing measure in each case is highlighted
in bold. While different measures performed best on
different data sets, the results in Table 4 shows that
CSR performs consistently well across all data sets.
In all data sets the correlation for CSR was always
either the best or close to the best.
As expected, our results are statistically more sig-
nificant for the larger data sets, compared to the
smaller ones. The standard deviations of the results
are small for two largest data sets (less than 0.01
for Edinburg and Florida), gradually increasing (less
than 0.02 for Kent and Minnesota), and becoming
high (upto .15) for the three smallest datasets.
Although, among all measures, CSR has the best
average correlation over all datasets, taking average
of correlations across widely different dataset is not
a meaningful way to decide on which measure to
use. Ideally one would like to access an oracle to
learn which measure will perform best on a particu-
lar unseen application dataset. Short of such an ora-
cle, if one were to pick a fixed measure a-priori, then
one would like to know how much worse off one is
compared to the best measure for that dataset.
To compare different measures from this perspec-
tive, we compute the deviation of the correlation for
each measure from the correlation of the best mea-
sure for each data set. These deviations are reported
in Table 5, along with the corresponding ranks. The
average deviation of CSR over all the data sets is
0.02, which is the least among all the measures, the
next two being 0.04 and 0.06. CSR also has the least
worst-deviation among all measures. Also, CSR is
never ranked worse than 3 in any of the data sets.
This is also the smallest worst-rank among all mea-
sures. Based on these results, we infer that CSR
is overall the best performing co-occurrence based
word association measure.
While the focus of our work is on the co-
occurrence measures, for completeness, we present
all the known results for knowledge and distribu-
tional similarity-based measures on the datasets un-
1066
der consideration in Table 6. Note that in (Agirre et
al., 2009), the wordsim data set was partitioned into
two sets, namely sim and rel, and in Esslli shared
task (ESSLLI, 2008), a 272 word pair subset of the
Edinburgh dataset was chosen. To facilitate compar-
ison, in addition to CSR, we also present results for
PMI and Ochiai (Chi-Square) which are the best per-
forming co-occurrence measures on wordsim, and
Esslli datasets. For co-occurrence-based measures,
we used 5-fold cross validation, which is inapplica-
ble for parameterless measures. Results show that
co-occurrence-based measures compare well with
other resource-heavy measures.
6 Conclusions
In this paper, we introduced a new measure called
CSR for word-association based on statistical sig-
nificance of lexical co-occurrences. Our measure,
while being agnostic to global unigram frequencies,
detects skews in span distributions of word-pairs in
documents containing both words. We carried out
extensive evaluation on several benchmark datasets.
Our experiments demonstrate the advantages of our
measure over all the competing measures.
Acknowledgments
This work was supported in part by the Ministry
of Human Resources Development, Government of
India and by the Tata Research Development and
Design Center (TRDDC). We thank Mr. Justin
Washtell (University of Leeds) for providing us with
various datasets.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL-HLT.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. Measuring semantic similarity be-
tween words using web search engines. In WWW,
pages 757?766.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguists, 32(1):13?47.
Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.
2006. Novel association measures using web search
with double checking. In ACL.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicogra-
phy. In ACL, pages 76?83.
L. R. Dice. 1945. Measures of the amount of ecological
association between species. Ecology, 26:297?302.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
ESSLLI. 2008. Free association task at lexical seman-
tics workshop esslli 2008. http://wordspace.
collocations.de/doku.php/workshop:
esslli:task.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Trans. Inf. Syst., 20(1):116?131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI.
Robert Goldfarb and Harvey Halpern. 1984. Word asso-
ciation responses in normal adult subjects. Journal of
Psycholinguistic Research, 13(1):37?55.
T Hughes and D Ramage. 2007. Lexical semantic relat-
edness with random graph walks. In EMNLP.
P. Jaccard. 1912. The distribution of the flora of the
alpine zone. New Phytologist, 11:37?50.
Svante Janson and Jan Vegelius. 1981. Measures of eco-
logical association. Oecologia, 49:371?376.
Mario Jarmasz. 2003. Rogets thesaurus as a lexical re-
source for natural language processing. Technical re-
port, University of Ottowa.
G. Kent and A. Rosanoff. 1910. A study of association
in insanity. American Journal of Insanity, pages 317?
390.
G. Kiss, C. Armstrong, R. Milroy, and J. Piper. 1973.
An associative thesaurus of english and its computer
analysis. In The Computer and Literary Studies, pages
379?382. Edinburgh University Press.
T. Landauer and S. Dumais. 1997. The latent semantic
analysis theory of acquisition, induction, and represen-
tation of knowledge. In Psychological Review, volume
104/2, pages 211?240.
Sonya Liberman and Shaul Markovitch. 2009. Com-
pact hierarchical explicit semantic representation. In
Proceedings of the IJCAI 2009 Workshop on User-
Contributed Knowledge and Artificial Intelligence: An
Evolving Synergy (WikiAI09), Pasadena, CA, July.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
David Milne and Ian H. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
wikipedia links. In ACL.
1067
D. Nelson, C. McEvoy, J. Walling, and J. Wheeler.
1980. The university of south florida homograph
norms. Behaviour Research Methods and Instrumen-
tation, 12:16?37.
A Ochiai. 1957. Zoogeografical studies on the soleoid
fishes found in japan and its neighbouring regions-ii.
Bulletin of the Japanese Society of Scientific Fisheries,
22.
Pavel Pecina and Pavel Schlesinger. 2006. Combin-
ing association measures for collocation extraction. In
ACL.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 8(10):627?633, October.
W.A. Russell and J.J. Jenkins. 1954. The complete
minnesota norms for responses to 100 words from the
kent-rosanoff word association test. Technical report,
Office of Naval Research and University of Minnesota.
StopWordList. 2010. http://ir.dcs.gla.ac.
uk/resources/linguistic_utils/stop_
words. The Information Retrieval Group, University
of Glasgow. Accessed: November 15, 2010.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In AAAI, pages 1419?1424.
T. Wandmacher, E. Ovchinnikova, and T. Alexandrov.
2008. Does latent semantic analysis reflect human as-
sociations? In European Summer School in Logic,
Language and Information (ESSLLI?08).
Justin Washtell and Katja Markert. 2009. A comparison
of windowless and window-based computational asso-
ciation measures as predictors of syntagmatic human
associations. In EMNLP, pages 628?637.
Katherine K. White and Lise Abrams. 2004. Free as-
sociations and dominance ratings of homophones for
young and older adults. Behavior Research Methods,
Instruments, & Computers, 36(3):408?420.
Wikipedia. April 2008. http://www.wikipedia.
org.
Eric Yeh, Daniel Ramage, Chris Manning, Eneko Agirre,
and Aitor Soroa. 2009. Wikiwalk: Random walks
on wikipedia for semantic relatedness. In ACL work-
shop ?TextGraphs-4: Graph-based Methods for Natu-
ral Language Processing?.
1068
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 163?169,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Appropriately Incorporating Statistical Significance in PMI
Om P. Damani and Shweta Ghonge
IIT Bombay
India
{damani,shwetaghonge}@cse.iitb.ac.in
Abstract
Two recent measures incorporate the notion of
statistical significance in basic PMI formula-
tion. In some tasks, we find that the new mea-
sures perform worse than the PMI. Our anal-
ysis shows that while the basic ideas in incor-
porating statistical significance in PMI are rea-
sonable, they have been applied slightly inap-
propriately. By fixing this, we get new mea-
sures that improve performance over not just
PMI but on other popular co-occurrence mea-
sures as well. In fact, the revised measures
perform reasonably well compared with more
resource intensive non co-occurrence based
methods also.
1 Introduction
The notion of word association is used in many lan-
guage processing and information retrieval appli-
cations and it is important to have low-cost, high-
quality association measures. Lexical co-occurrence
based word association measures are popular be-
cause they are computationally efficient and they can
be applied to any language easily. One of the most
popular co-occurrence measure is Pointwise Mutual
Information (PMI) (Church and Hanks, 1989).
One of the limitations of PMI is that it only works
with relative probabilities and ignores the absolute
amount of evidence. To overcome this, recently two
new measures have been proposed that incorporate
the notion of statistical significance in basic PMI
formulation. In (Washtell and Markert, 2009), sta-
tistical significance is introduced in PMIsig by mul-
tiplying PMI value with the square root of the ev-
idence. In contrast, in (Damani, 2013), cPMId is
introduced by bounding the probability of observing
a given deviation between a given word pair?s co-
occurrence count and its expected value under a null
model where with each word a global unigram gen-
eration probability is associated. In Table 1, we give
the definitions of PMI, PMIsig, and cPMId.
While these new measures perform better than
PMI on some of the tasks, on many other tasks,
we find that the new measures perform worse than
the PMI. In Table 3, we show how these measures
perform compared to PMI on four different tasks.
We find that PMIsig degrades performance in three
out of these four tasks while cPMId degrades per-
formance in two out of these four tasks. The ex-
perimental details and discussion are given in Sec-
tion 4.2.
Our analysis shows that while the basic ideas in
incorporating statistical significance are reasonable,
they have been applied slightly inappropriately. By
fixing this, we get new measures that improve per-
formance over not just PMI, but also on other pop-
ular co-occurrence measures on most of these tasks.
In fact, the revised measures perform reasonably
well compared with more resource intensive non co-
occurrence based methods also.
2 Adapting PMI for Statistical Significance
In (Washtell and Markert, 2009), it is assumed that
the statistical significance of a word pair association
is proportional to the square root of the evidence.
The question of what constitutes the evidence is an-
swered by taking the lesser of the frequencies of the
two words in the word pair, since at most that many
pairings are possible. Hence the PMI value is multi-
163
Method Formula Revised Formula
PMI (Church and
Hanks, 1989)
log f(x,y)f(x)?f(y)/W
PMIsig (Washtell
and Markert, 2009)
log f(x,y)f(x)?f(y)/W ?
?
min(f(x),f(y)) PMIs: log f(x,y)f(x)?f(y)/W ?
?
max(f(x),f(y))
cPMId (Damani,
2013)
log d(x,y)
d(x)*d(y)/D+
?
d(x)?
?
ln ?
(?2.0)
sPMId: log d(x,y)
max(d(x),d(y))*min(d(x),d(y))/D+
?
max(d(x),d(y))?
?
ln ?
(?2.0)
Terminology:
W Total number of words in the corpus D Total number of documents in the corpus
f(x), f(y) unigram frequencies of x, y respectively in the corpus d(x), d(y) Total number of documents in the corpus containing
at least one occurrence of x and y respectively
f(x, y) Span-constrained (x, y) word pair frequency in the corpus d(x, y) Total number of documents in the corpus having at-least
one span-constrained occurrence of the word pair (x, y)
? a parameter varying between 0 and 1
Table 1: Definitions of PMI and its statistically significant adaptations. The sub-parts in bold represent the changes
between the original formulas and the revised formulas. The product max(d(x), d(y)) ?min(d(x), d(y)) in sPMId
formula can be simplified to f(x) ? f(y), however, we left it this way to emphasize the transformation from cPMId.
plied by
?
min(f(x), f(y)) to get PMIsig.
In (Damani, 2013), statistical significance is
introduced by bounding the probability of observing
a given number of word-pair occurrences in the
corpus, just by chance, under a null model of inde-
pendent unigram occurrences. For this computation,
one needs to decide what constitutes a random trial
when looking for a word-pair occurrence. Is it the
occurrence of the first word (say x) in the pair, or
the second (say y). In (Damani, 2013), occurrences
of x are arbitrarily chosen to represent the sites of
the random trial. Using Hoeffdings Inequality:
P [f(x, y) ? f(x) ? f(y)/W + f(x) ? t]
? exp(?2 ? f(x) ? t2)
By setting t =
?
ln ?/(?2 ? f(x)), we get ? as an
upper bound on probability of observing more than
f(x)?f(y)/W +f(x)? t bigram occurrences in the
corpus, just by chance. Based on this Corpus Level
Significant PMI(cPMI) is defined as:
cPMI(x, y) = log
f(x, y)
f(x) ? f(y)/W + f(x) ? t
= log
f(x, y)
f(x) ? f(y)/W +
?
f(x) ?
?
ln ?/(?2)
In (Damani, 2013), several variants of cPMI are in-
troduced that incorporate different notions of sta-
tistical significance. Of these Corpus Level Signif-
icant PMI based on Document count(cPMId - de-
fined in Table 1) is found to be the best performing,
and hence we consider this variant only in this work.
2.1 Choice of Random Trial
While considering statistical significance, one has
to decide what constitutes a random trial. When
looking for a word-pair (x, y)?s occurrences, y can
potentially occur near each occurrence of x, or x
can potentially occur near each occurrence of y.
Which of these two set of occurrences should be
considered the sites of random trial. We believe
that the occurrences of the more frequent of x and y
should be considered, since near each of these occur-
rences the other word could have occurred. Hence
f(x) and f(y) in cPMI definition should be re-
placed with max(f(x), f(y)) and min(f(x), f(y))
respectively. Similarly, d(x) and d(y) in cPMId for-
mula should be replaced with max(d(x), d(y)) and
min(d(x), d(y)) respectively to give a new measure
Significant PMI based on Document count(sPMId).
Using the same logic,
?
min(f(x), f(y))
in PMIsig formula should be replaced with?
max(f(x), f(y)) to give the formula for a new
measure PMI-significant(PMIs). The definitions of
sPMId and PMIs are also given in Table 1.
3 Related Work
There are three main types of word association mea-
sures: Knowledge based, Distributional Similarity
based, and Lexical Co-occurrence based.
Based on Firth?s You shall know a word by the
company it keeps (Firth, 1957), distributional sim-
ilarity based measures characterize a word by the
distribution of other words around it and compare
164
Method Formula
ChiSquare (?2)
?
i,j
(f(i,j)?Ef(i,j))2
Ef(i,j)
Dice (Dice, 1945) f(x,y)f(x)+f(y)
GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))?log d(x,y)logD?min(log d(x),log d(y))
Jaccard (Jaccard, 1912) f(x,y)f(x)+f(y)?f(x,y)
LLR (Dunning, 1993)
?
x? ? {x,?x}
y? ? {y,?y}
f(x?, y?)log f(x
?,y?)
f(x?)f(y?)
nPMI (Bouma, 2009)
log f(x,y)f(x)?f(y)/W
log 1f(x,y)/W
Ochiai (Janson and Vegelius, 1981) f(x,y)?
f(x)f(y)
PMI2 (Daille, 1994) log
f(x,y)
f(x)?f(y)/W
1
f(x,y)/W
= log f(x,y)
2
f(x)?f(y)
Simpson (Simpson, 1943) f(x,y)min(f(x),f(y))
SCI (Washtell and Markert, 2009) f(x,y)
f(x)
?
f(y)
T-test f(x,y)?Ef(x,y)?
f(x,y)(1? f(x,y)W )
Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same
as that in Table 1, except that E in front of a variable name means the expected value of that variable.
Task
Semantic Sentence Synonym
Relatedness Similarity Selection
Dataset WordSim Li ESL TOEFL
Metric Spearman Rank
Correlation
Pearson Cor-
relation
Fraction
Correct
Fraction
Correct
PMI 0.68 0.69 0.62 0.59
PMIsig 0.67 0.85 0.58 0.56
cPMId 0.72 0.67 0.56 0.59
PMIs 0.66 0.85 0.66 0.61
sPMId 0.72 0.75 0.70 0.61
ChiSquare (?2) 0.62 0.80 0.62 0.58
Dice 0.58 0.76 0.56 0.57
GoogleDistance 0.53 0.75 0.09 0.19
Jaccard 0.58 0.76 0.56 0.57
LLR 0.50 0.18 0.18 0.27
nPMI 0.72 0.35 0.54 0.54
Ochiai/ PMI2 0.62 0.77 0.62 0.60
SCI 0.65 0.85 0.62 0.60
Simpson 0.59 0.78 0.58 0.57
TTest 0.44 0.63 0.44 0.52
Semantic Net (Li et al, 2006) 0.82
ESA (Gabrilovich and Markovitch, 2007) 0.74
(reimplemented in (Yeh et al, 2009)) 0.71
Distributional Similarity (on web corpus) (Agirre et
al., 2009))
0.65
Context Window based Distributional Similar-
ity (Agirre et al, 2009))
0.60
Latent Semantic Analysis (on web corpus) (Finkel-
stein et al, 2002)
0.56
WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87
PMI-IR3 (using context) (Turney, 2001) 0.73
Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best
co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and
LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known
result for different non co-occurrence based methods is also shown.
165
two words for distributional similarity (Agirre et
al., 2009; Wandmacher et al, 2008; Bollegala et
al., 2007; Chen et al, 2006). They are also used
for modeling the meaning of a phrase or a sen-
tence (Grefenstette and Sadrzadeh, 2011; Wartena,
2013; Mitchell, 2011; G. Dinu and Baroni, 2013;
Kartsaklis et al, 2013).
Knowledge-based measures use knowledge-
sources like thesauri, semantic networks, or
taxonomies (Milne and Witten, 2008; Hughes
and Ramage, 2007; Gabrilovich and Markovitch,
2007; Yeh et al, 2009; Strube and Ponzetto, 2006;
Finkelstein et al, 2002; Liberman and Markovitch,
2009).
Co-occurrence based measures (Pecina and
Schlesinger, 2006) simply rely on unigram and bi-
gram frequencies of the words in a pair. In this work,
our focus is on the co-occurrence based measures,
since they are resource-light and can easily be used
for resource-scarce languages.
3.1 Co-occurrence Measures being Compared
Co-occurrence based measures of association be-
tween two entities are used in several domains like
ecology, psychology, medicine, language process-
ing, etc. To compare the performance of our newly
introduced measures with other co-occurrence mea-
sures, we have selected a number of popu-
lar co-occurrence measures like ChiSquare (?2),
Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and
Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dun-
ning, 1993), Simpson (Simpson, 1943), and T-test
from these domains.
In addition to these popular measures, we
also experiment with other known variations of
PMI like nPMI (Bouma, 2009), PMI2 (Daille,
1994), Ochiai (Janson and Vegelius, 1981), and
SCI (Washtell and Markert, 2009). Since PMI2 is
a monotonic transformation of Ochiai, we present
their results together. In Table 2, we present the def-
initions of these measures. While the motivation
given for SCI in (Washtell and Markert, 2009) is
slightly different, in light of the discussion in Sec-
tion 2.1, we can assume that SCI is PMI adapted for
statistical significance (multiplied by
?
f(y)), where
the site of random trial is taken to be the occurrences
of the second word y, instead of the less frequent
word, as in the case of PMIsig.
When counting co-occurrences, we only con-
sider the non-overlapping span-constrained occur-
rences. The span of a word-pair?s occurrence is the
direction-independent distance between the occur-
rences of the members of the pair. We consider only
those co-occurrences where span is less than a given
threshold. Therefore, span threshold is a parameter
for all the co-occurrence measures being considered.
4 Performance Evaluation
Having introduced the revised measures PMIs and
sPMId, we need to evaluate the performance of these
measures compared to PMI and the original mea-
sures introducing significance. In addition, we also
wish to compare the performance of these measures
with other co-occurrence measures. To compare the
performance of these measures with more resource
heavy non co-occurrence based measures, we have
chosen those tasks and datasets on which published
results exist for distributional similarity and knowl-
edge based word association measures.
4.1 Task Details
We evaluate these measures on three tasks: Sen-
tence Similarity(65 sentence-pairs from (Li et al,
2006)), Synonym Selection(50 questions ESL (Tur-
ney, 2001) and 80 questions TOEFL (Landauer and
Dutnais, 1997) datasets), and, Semantic Related-
ness(353 words Wordsim (Finkelstein et al, 2002)
dataset).
For each of these tasks, gold standard human
judgment results exist. For sentence similarity, fol-
lowing (Li et al, 2006), we evaluate a measure by
the Pearsons correlation between the ranking pro-
duced by the measure and the human ranking. For
synonym selection, we compute the percentage of
correct answers, since there is a unique answer for
each challenge word in the datasets. Semantic relat-
edness has been evaluated by Spearman?s rank cor-
relation with human judgment instead of Pearsons
correlation in literature and we follow the same prac-
tice to make results comparable.
For sentence similarity detection, the algorithm
used by us (Li et al, 2006) assumes that the asso-
ciation scores are between 0 and 1. Hence we nor-
malize the value produced by each measure using
166
Challenge
x
Option y
(correct)
Option z
(incorrect)
f(x) f(y) f(z) f(x, y) f(x, z) PMIsig
(x, y)
PMIsig
(x, z)
PMIs
(x, y)
PMIs
(x, z)
brass metal plastic 15923 125088 24985 228 75 14 24 40 30
twist intertwine curl 11407 153 2047 1 9 7 17 61 41
saucer dish frisbee 2091 12453 1186 5 1 9 14 21 18
mass lump element 90398 1595 43321 14 189 4 10 29 15
applause approval friends 1998 19673 11689 8 6 9 11 29 28
confession statement plea 7687 47299 5232 76 12 18 22 45 26
swing sway bounce 33580 2994 4462 13 17 7 8 24 21
sheet leaf book 20470 20979 586581 20 194 7 2 7 12
Table 4: Details of ESL word-pairs, correctness of whose answers changes between PMIsig and PMIs. Except for the
gray-row, for all other questions, incorrect answers becomes correct on using PMIs instead of PMIsig , and vice-versa
for the gray-row. The association values have been suitably scaled for readability. To save space, of the four choices,
options not selected by either of the methods have been omitted. These results are for a 10 word span.
max-min normalization:
v? =
v ?min
max?min
where max and min are computed over all associa-
tion scores for the entire task for a given measure.
4.2 Experimental Results
We use a 1.24 Gigawords Wikipedia corpus for get-
ting co-occurrence statistics. Since co-occurrence
methods have span-threshold as a parameter, we fol-
low the standard methodology of five-fold cross val-
idation. Note that, in addition to span-threshold, cP-
MId and sPMId have an additional parameter ?.
In Table 3, we present the performance of all the
co-occurrence measures considered on all the tasks.
Note that, except GoogleDistance and LLR, all re-
sults for all co-occurrence measures are statistically
significant at p = .05. For completeness of compari-
son, we also include the best known results from lit-
erature for different non co-occurrence based word
association measures on these tasks.
4.3 Performance Analysis and Conclusions
We find that on average, PMIsig and cPMId, the re-
cently introduced measures that incorporate signif-
icance in PMI, do not perform better than PMI on
the given datasets. Both of them perform worse
than PMI on three out of four datasets. By ap-
propriately incorporating significance, we get new
measures PMIs and sPMId that perform better than
PMI(also PMIsig and cPMId respectively) on most
datasets. PMIs improves performance over PMI on
three out of four datasets, while sPMId improves
performance on all four datasets.
The performance improvement of PMIs over
PMIsig and of sPMId over cPMId, is not random.
For example, on the ESL dataset, while the percent-
age of correct answers increases from 58 to 66 from
PMIsig to PMIs, it is not the case that on moving
from PMIsig to PMIs, several correct answers be-
come incorrect and an even larger number of in-
correct answers become correct. As shown in Ta-
ble 4, only one correct answers become incorrect
while seven incorrect answers get corrected. The
same trend holds for most parameters values, and
for moving from cPMId to sPMId. This substanti-
ates the claim that the improvement is not random,
but due to the appropriate incorporation of signifi-
cance, as discussed in Section 2.1.
PMIs and sPMId perform better than not just
PMI, but they perform better than all popular co-
occurrence measures on most of these tasks. When
compared with any other co-occurrence measure,
on three out of four datasets each, both PMIs and
sPMId perform better than that measure. In fact,
PMIs and sPMId perform reasonably well compared
with more resource intensive non co-occurrence
based methods as well. Note that different non co-
occurrence based measures perform well on differ-
ent tasks. We are comparing the performance of a
single measure (say sPMId or PMIs) against the best
measure for each task.
Acknowledgements
We thank Dipak Chaudhari for his help with the im-
plementation.
167
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL-HLT 2009,
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. Measuring semantic similarity be-
tween words using web search engines. In WWW
2007, The World Wide Web Conference, pages 757?
766.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction, from form to
meaning: Processing texts automatically. In GSCL
2009, Proceedings of the Biennial International Con-
ference of the German Society for Computational Lin-
guistics and Language Technology.
Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.
2006. Novel association measures using web search
with double checking. In COLING/ACL 2006, Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicogra-
phy. In ACL 1989, Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 76?83.
B. Daille. 1994. Approche mixte pour l?extraction au-
tomatique de terminologie: statistiques lexicales etl-
tres linguistiques. Ph.D. thesis, Universitie Paris 7.
Om P. Damani. 2013. Improving pointwise mutual
information (pmi) by incorporating significant co-
occurrence. In CoNLL 2013, Conference on Compu-
tational Natural Language Learning.
L. R. Dice. 1945. Measures of the amount of ecological
association between species. Ecology, 26:297?302.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
J. R. Firth. 1957. A synopsis of linguistics theory. Stud-
ies in Linguistic Analysis, pages 1930?1955.
N. Pham G. Dinu and M. Baroni. 2013. General esti-
mation and evaluation of compositional distributional
semantic models. In CVSC 2013, Proceedings of the
ACL Workshop on Continuous Vector Space Models
and their Compositionality.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI 2007, International
Joint Conference on Artificial Intelligence.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In EMNLP 2011,
Conference on Empirical Methods on Natural Lan-
guage Processing, pages 1394?1404.
T Hughes and D Ramage. 2007. Lexical semantic relat-
edness with random graph walks. In EMNLP 2007,
Conference on Empirical Methods on Natural Lan-
guage Processing.
P. Jaccard. 1912. The distribution of the flora of the
alpine zone. New Phytologist, 11:37?50.
Svante Janson and Jan Vegelius. 1981. Measures of eco-
logical association. Oecologia, 49:371?376.
Dimitrios Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating disambiguation from
composition in distributional semantics. In CoNLL
2013, Conference on Computational Natural Lan-
guage Learning.
Thomas K Landauer and Susan T. Dutnais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211?240.
Rudi L.Cilibrasi and Paul M.B. Vitany. 2007. The google
similarity distance. Psychological review, 19(3).
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 18(8):1138?1150, August.
Sonya Liberman and Shaul Markovitch. 2009. Com-
pact hierarchical explicit semantic representation. In
WikiAI 2009, Proceedings of the IJCAI Workshop
on User-Contributed Knowledge and Artificial Intel-
ligence: An Evolving Synergy, Pasadena, CA, July.
David Milne and Ian H. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
wikipedia links. In ACL 2008, Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Jeffrey Mitchell. 2011. Composition in Distributional
Models of Semantics. Ph.D. thesis, The University of
Edinburgh.
Pavel Pecina and Pavel Schlesinger. 2006. Combin-
ing association measures for collocation extraction. In
ACL 2006, Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
168
Gabriel Recchia and Michael N. Jones. 2009. More data
trumps smarter algorithms: Comparing pointwise mu-
tual information with latent semantic analysis. Behav-
ior Research Methods, 3(41):647?656.
George G. Simpson. 1943. Mammals and the nature of
continents. American Journal of Science, pages 1?31.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In AAAI 2006, Conference on Artificial In-
telligence, pages 1419?1424.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In ECML 2001, European
Conference on Machine Learning.
T. Wandmacher, E. Ovchinnikova, and T. Alexandrov.
2008. Does latent semantic analysis reflect human
associations? In ESSLLI 2008, European Summer
School in Logic, Language and Information.
Christian Wartena. 2013. Hsh: Estimating semantic sim-
ilarity of words and short phrases with frequency nor-
malized distance measures. In SemEval 2013, Inter-
national Workshop on Semantic Evaluation.
Justin Washtell and Katja Markert. 2009. A comparison
of windowless and window-based computational asso-
ciation measures as predictors of syntagmatic human
associations. In EMNLP 2009, Conference on Empir-
ical Methods on Natural Language Processing, pages
628?637.
Eric Yeh, Daniel Ramage, Chris Manning, Eneko Agirre,
and Aitor Soroa. 2009. Wikiwalk: Random walks
on wikipedia for semantic relatedness. In TextGraphs
2009, Proceedings of the ACL workshop on Graph-
based Methods for Natural Language Processing.
169
Hindi and Marathi to English Cross Language Information 
 
Manoj Kumar Chinnakotla, Sagar Ranadive, Om P. Damani and Pushpak 
Bhattacharyya 
 
Abstract 
 
In this paper, we present our Hindi ->English and Marathi ->English CLIR 
systems developed as part of our participation in the CLEF 2007 Ad-Hoc 
Bilingual task. We take a query translation based approach using bi-lingual 
dictionaries. Query words not found in the dictionary are transliterated using 
a simple lookup table based transliteration approach. The resultant 
transliteration is then compared with the index items of the corpus to return 
the `k' closest English index words of the given Hindi/Marathi word. The 
resulting multiple translation/transliteration choices for each query word are 
disambiguated using an iterative page-rank style algorithm, proposed in the 
literature, which makes use of term-term co-occurrence statistics to produce 
the final translated query. Using the above approach, for Hindi, we achieve a 
Mean Average Precision (MAP) of 0.2366 in title which is 61.36% of 
monolingual performance and a MAP of 0.2952 in title and description 
which is 67.06% of monolingual performance. For Marathi, we achieve a 
MAP of 0.2163 in title which is 56.09% of monolingual performance. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 20?28,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Improving Pointwise Mutual Information (PMI) by Incorporating
Significant Co-occurrence
Om P. Damani
IIT Bombay
damani@cse.iitb.ac.in
Abstract
We design a new co-occurrence based
word association measure by incorpo-
rating the concept of significant co-
occurrence in the popular word associ-
ation measure Pointwise Mutual Infor-
mation (PMI). By extensive experiments
with a large number of publicly available
datasets we show that the newly intro-
duced measure performs better than other
co-occurrence based measures and de-
spite being resource-light, compares well
with the best known resource-heavy dis-
tributional similarity and knowledge based
word association measures. We investi-
gate the source of this performance im-
provement and find that of the two types
of significant co-occurrence - corpus-level
and document-level, the concept of cor-
pus level significance combined with the
use of document counts in place of word
counts is responsible for all the perfor-
mance gains observed. The concept of
document level significance is not helpful
for PMI adaptation.
1 Introduction
Co-occurrence based word association measures
like PMI, LLR, and Dice are popular since they
are easy to understand and computationally effi-
cient. They measure the strength of association
between two words by comparing the word pair?s
corpus-level bigram frequency to some function of
the unigram frequencies of the individual words.
Recently a new measure called Co-occurrence
Significance Ratio (CSR) was introduced
in (Chaudhari et al, 2011) based on the no-
tion of significant co-occurrence. Since CSR was
found to perform better than other co-occurrence
measures, in this work, our goal was to incorpo-
rate the concept of significant co-occurrence in
traditional word-association measures to design
new measures that may perform better than both
CSR and the traditional measures.
Two different notions of significant co-
occurrence are employed in CSR:
? Corpus-level significant co-occurrence de-
termines whether the ratio of observed bi-
gram occurrences to their expected occur-
rences across the corpus can be explained as
a pure chance phenomenon, and,
? Document-level significant co-occurrence
determines whether a large fraction of a
word-pair?s occurrences within a given doc-
ument have smaller spans than that under a
null model where the words in the document
are permuted randomly.
While both these notions are employed in an
integrated fashion in CSR, on analyzing CSR de-
tails, we realized that these two concepts are in-
dependent and can be applied separately to any
word association measure which is a ratio of
some variable?s observed frequency to its ex-
pected frequency. We incorporate the concepts
of corpus-level and document-level significant co-
occurrence in PMI to design a new measure that
performs better than both PMI and CSR, as well as
other co-occurrence based word association mea-
sures. To incorporate document level significance,
we need to use document level counts instead of
word level counts (this distinction is explained
in detail in Section 4.3). To investigate whether
the performance gains observed are because of
the concept of significant co-occurrence or sim-
ply because of the fact that we are using docu-
ment counts instead of the word counts, we also
design document count based baseline version of
PMI called PMId, and several intermediate vari-
ants whose definitions are given in Table 1.
To our surprise, we discover that the concept
of document level significant co-occurrence does
20
without corpus with corpus
level significance level significance
word-based PMI: log f(x,y)f(x)?f(y)/W cPMI: log f(x,y)f(x)?f(y)/W+?f(x)??ln ?/(?2)
document-based PMId: log d(x,y)d(x)?d(y)/D cPMId: log d(x,y)d(x)?d(y)/D+?d(x)??ln ?/(?2)
with document level
significance
PMIz: log Zd(x)?d(y)/D cPMIz: log Zd(x)?d(y)/D+?d(x)??ln ?/(?2)
CSR: Z
E(Z)+
?
K?
?
ln ?/(?2)
f(x, y) Span-constrained (x, y) word pair frequency in the corpus
f(x), f(y) unigram frequencies of x, y respectively in the corpus
W Total number of words in the corpus
d(x, y) Total number of documents in the corpus having at-least
one span-constrained occurrence of the word pair (x, y)
d(x), d(y) Total number of documents in the corpus containing
at least one occurrence of x and y respectively
D Total number of documents in the corpus
? a parameter varying between 0 and 1
Z as per Definition 4.3
E(Z) Expected value of Z as given in Section 2.2 of (Chaudhari et al, 2011)
K Total number of documents in the corpus having at-least
one occurrence of the word pair (x, y) regardless of the span
Table 1: Definitions0 of PMI, CSR, and various measures developed in this work.
not contribute to the PMI performance improve-
ment. Two newly designed, best-performing mea-
sures cPMId and cPMIz have almost identical per-
formance. As the definitions in Table 1 show,
cPMId incorporates corpus level significance in a
document count based version of PMI but does
not employ the concept of document level signif-
icance, whereas cPMIz employs both corpus and
document level significance. This demonstrates
that the concept of corpus level significance com-
bined with document counts is responsible for all
the performance gains observed.
To summarize, we make the following contribu-
tions in this work:
? We incorporate the notion of significant co-
occurrence in PMI to design a new measure
cPMId that performs better than PMI as well
as other popular co-occurrence based word-
association measures on both free association
and semantic relatedness tasks. In addition,
despite being resource-light, cPMId performs
as well as the best known distributional sim-
ilarity and knowledge based measures which
are resource-intensive.
? We investigate the source of this performance
improvement and find that of the two notions
0We consider only those word-pair occurrences where
inter-word distance between x and y is atmost s, the span
threshold. For a particular occurrence of x, we get a window
of size s on either side within which y can occur. Strictly
speaking, there should be a factor 2s in the denominator of
the formula for PMI. Since we are only interested in the rel-
ative rankings of word-pairs, we follow the standard practice
of ignoring the 2s factor, as its removal affects only the abso-
lute PMI values but not the relative rankings.
of significance - corpus-level and document-
level significant co-occurrence, the concept
of document level significant co-occurrence
is not helpful for PMI adaptation. The con-
cept of corpus level significance combined
with document counts is responsible for all
the performance gains observed.
2 Related Work
Word association measures can be divided into
three broad categories: knowledge based, dis-
tributional similarity based, and co-occurrence
based measures. Knowledge-based measures
are based on thesauri, semantic networks, tax-
onomies, or other knowledge sources (Liberman
and Markovitch, 2009; Yeh et al, 2009; Milne
and Witten, 2008; Hughes and Ramage, 2007).
Distributional similarity-based measures compare
two words by comparing distributional similar-
ity of other words around them (Agirre et al,
2009; Wandmacher et al, 2008; Bollegala et al,
2007). In this work, our focus is on Co-occurrence
based measures and hence we do not discuss
Knowledge-based and Distributional similarity-
based measures further.
Co-occurrence based measures estimate asso-
ciation between two words by computing some
function of the words unigram and bigram fre-
quencies. Table 2 contains definitions of popu-
lar co-occurrence measures. The concept of docu-
ment and corpus level significance can be applied
to any word association measure which is defined
as the ratio of a variable?s observed frequency to
its expected frequency. While Chi-Square (?2),
21
Measure Definition
Chi-Square(?2)
?
x? ? {x,?x}
y? ? {y,?y}
(f(x?,y?)?Ef(x?,y?))2
Ef(x?,y?)
Dice (Dice,
1945)
2f(x,y)
f(x)+f(y)
Jaccard (Jac-
card, 1912)
f(x,y)
f(x)+f(y)?f(x,y)
Log Like-
lihood Ra-
tio(LLR) (Dun-
ning, 1993)
?
x? ? {x,?x}
y? ? {y,?y}
p(x?, y?)log p(x
?,y?)
p(x?)p(y?)
Pointwise Mu-
tual Informa-
tion(PMI) (Church
and Hanks,
1989)
log f(x,y)f(x)?f(y)/W
T-test f(x,y)?Ef(x,y)?
f(x,y)
(
1? f(x,y)W
)
W Total number of tokens in the corpus
f(x), f(y) unigram frequencies of x, y in the corpus
p(x), p(y) f(x)/W, f(y)/W
f(x, y) Span-constrained (x, y) word pair frequency in corpus
p(x, y) f(x, y)/W
Table 2: Definition of popular co-occurrence based word
association measures.
LLR, and T-test already incorporate some notion
of statistical significance, among Dice, Jaccard,
and PMI, only the PMI meets this requirement.
Hence our focus in this work is on designing new
measures by incorporating the notion of signifi-
cant co-occurrence in PMI.
3 Incorporating Corpus Level
Significance
In (Chaudhari et al, 2011), the concept of corpus
level significance was introduced by bounding the
probability of observing a given corpus level phe-
nomenon under a particular null model. In the for-
mula for PMI, the observed frequency of a word
pair?s occurrences is compared with its expected
frequency under a null model which assumes in-
dependent unigram occurrences. Near a given oc-
currence of the word x in the corpus, the word y
can be observed with probability f(y)/W . Hence
the expected value of f(x, y) is f(x) ? f(y)/W .
Adapting from (Chaudhari et al, 2011) and using
Hoeffding?s Inequality, the probability of observ-
ing a given deviation between f(x, y) and its ex-
pected value f(x) ? f(y)/W can be bounded. For
any t > 0:
P [f(x, y) ? f(x) ? f(y)/W + f(x) ? t]
? exp(?2 ? f(x) ? t2)
= ?
The upper-bound ? (= exp(?2?f(x)?t2)) denotes
the probability of observing more than f(x) ?
f(y)/W + f(x)? t bigram occurrences in the cor-
pus, just by chance, under the given independent
unigram occurrence null model. With ? as a pa-
rameter (0 < ? < 1) and t = ?ln ?/(?2 ? f(x)),
we can define a new word association measure
called Corpus Level Significant PMI(cPMI) as:
cPMI(x, y) = log f(x, y)f(x) ? f(y)/W + f(x) ? t
= log f(x, y)
f(x) ? f(y)/W +
?
f(x) ?
?
ln ?/(?2)
where t = ?ln ?/(?2 ? f(x)).
By taking the probability of observing a given
deviation between f(x, y) and its expected value
f(x) ? f(y)/W in account, cPMI addresses one
of the main weakness of PMI of working only
with probabilities and completely ignoring the ab-
solute amount of evidence. In two scenarios where
all frequency ratios (that of f(x), f(y), f(x, y),
and W ) are equal, PMI values will be same while
cPMI value will be higher for the case where ab-
solute number of occurrences are higher. This can
be seen easily by multiplying all of f(x), f(y),
f(x, y), and W with some constant n:
log n ? f(x, y)
n ? f(x) ? n ? f(y)/n ?W +
?
n ? f(x) ?
?
ln ?/(?2)
= log f(x, y)
f(x) ? f(y)/W +
?
1/n ?
?
f(x) ?
?
ln ?/(?2)
> log f(x, y)
f(x) ? f(y)/W +
?
f(x) ?
?
ln ?/(?2)
= cPMI(x, y)
4 Incorporating Document Level
Significant Co-occurrence
Traditional measures like PMI can be viewed as
working with a null hypothesis where each word
in a document is generated completely indepen-
dently of the other words in that document. With
each word, a global unigram generation probabil-
ity is associated and all documents are assumed
to be generated as per a multinomial distribution.
Such a null model generates different expected
span (inter-word gap) for high frequency words vs.
low frequency words. In reality, if strongly associ-
ated words co-occur in a document then they do so
with low span, i.e., they occur close to each-other
regardless of the underlying unigram frequencies.
22
4.1 Determining Document Level
Significance
To correct this span bias of traditional measures,
a new null model is employed in (Chaudhari et
al., 2011). A bag of word is associated with each
document. The null model assumes that the ob-
served document is a random permutation of the
associated bag of words. Given the occurrences of
a word-pair in the document, if the number of oc-
currences with span less than a given threshold can
be explained by this null model then the word pair
is assumed to be unassociated in the document.
Else, some form of association is assumed. Fol-
lowing definitions are introduced in (Chaudhari et
al., 2011) to formalize this concept.
Definition 1 (span-constrained frequency) Let
f be the maximum number of non-overlapped
occurrences of a word-pair ? in a document. Let
f?s(0 ? f?s ? f) be the maximum number of
non-overlapped occurrences of ? with span less
than a given threshold s. We refer to f?s as the
span-constrained frequency of ? in the document.
For a given document of length ` and a word-
pair with f occurrences in it, as we vary the span
threshold s, the number of occurrences of the
word-pair with span less than s, i.e. its span-
constrained frequency f?s varies. For a given s
and the f?s resulting from it, we can ask, what is
the probability that f? s out of f occurrences of a
word-pair in a document of length `will have span
less than s, if the words in the document were to
be permuted randomly. If this probability is less
than some threshold , then we can assume that
the words in the pair have some tendency of co-
occurring in the document. Formally,
Definition 2 (-significant co-occurrence) Let `
be the length of a document and let f be the fre-
quency of a word-pair ? in it. For a given a span
threshold s, define pis(f?s, f, `) as the probability
under the null that ? will appear in the document
with a span-constrained frequency of at least f?s.
Given a probability threshold  (0 <  < 1) and
a span threshold s, the document is said to support
the hypothesis ?? is an -significant word-pair
within the document? if we have [pis(f? s, f, `) <
].
The key idea is that we should concentrate on
those documents where a word pair has an -
significant occurrence and ignore its occurrences
in non -significant documents. This point is more
subtle than it appears. Earlier, if the span of an oc-
currence was less than a threshold, it was counted,
else it was ignored. In the new null model, in-
stead of an individual occurrence, all occurrences
of the word-pair in the document are considered as
a single unit. Either all occurrences confirm to the
null model or they do not. Of course, some occur-
rences will have span less than the threshold while
others will have higher span, but when consider-
ing significance, all occurrences in the document
are considered significant or insignificant as a unit.
This point is discussed further in Section 4.3.
4.2 pis[] Computation Overhead
The detailed discussion of the computation of pis[]
table can be found in (Chaudhari et al, 2011). For
our work, it suffices to know that pis[] table needs
to be computed only once and hence it can be done
offline. We use the pis[] table made publicly avail-
able1 by CSR researchers. The use of pis[] table
simply entails a memory lookup and does not in-
crease the computation cost of a measure.
4.3 Adapting PMI for Document Level
Significance
Consider the cPMI definition given earlier. One
way to adapt it for document significance is to alter
the numerator such that only the span-constrained
bigram occurrences in -significant documents are
considered in computing f(x, y).
However, this simple adaptation is problem-
atic. Consider a document with f occurrences of
a word-pair of which span of f?s occurrences is at-
most s, the given span threshold. In the definition
of cPMI, the numerator takes in account only those
occurrences whose span is less than s, i.e., only the
f?s occurrences from a document. As discussed
earlier, the -significance of a document is deter-
mined by looking at all f occurrences as a whole.
In the null model, whether a particular occurrence
has span less than or greater than s is not so impor-
tant, what matters is that span of f? s occurrences
out of f is at most s. The word-pair is considered
an -significant pair within the document if the ob-
served span of all f occurrences of the pair can be
explained by the null model. Hence, when adapt-
ing for -significance, it is improper to count only
f?s occurrences out of f .
The way out of this difficulty is to count the doc-
uments and not the words. We do this adaptation
1http://www.cse.iitb.ac.in/ damani/papers/EMNLP11/resources.html
23
in two steps. First, we replace the word counts
with document counts in the definition of cPMI,
giving a new measure called Corpus Level Signifi-
cant PMI based on Document count (cPMId):
cPMId(x, y) = log d(x, y)
d(x) ? d(y)/D +
?
d(x) ?
?
ln ?/(?2)
where d(x, y) indicates the number of documents
containing at least one span constrained occur-
rence of (x, y), and d(x) and d(y) indicate the
number of document containing x and y, D indi-
cates the total number of documents in the corpus,
and as before, ? is a parameter varying between 0
and 1.
Having replaced the word counts with docu-
ment counts, we now incorporate the concept of
document level significant co-occurrence (as dis-
cussed in Section 4.1) in cPMId by replacing
d(x, y) in numerator with Z which is defined as:
Definition 3 (Z) Let Z be the number of doc-
uments that support the hypothesis ?the given
word-pair is an -significant word-pair?, i.e., Z is
the number of documents for which pis(f? s, f, `) <
.
The new measure is called Document and Corpus
Level Significant PMI (cPMIz) and is defined as:
cPMIz(x, y) = log Z
d(x) ? d(y)/D +
?
d(x) ?
?
ln ?/(?2)
Note that cPMIz has three parameters: span
threshold s, the corpus level significant parame-
ter ? (0 < ? < 1) and the document level signif-
icant parameter  (0 <  < 1). In comparison,
cPMI/cPMId have s and ? as parameters while
PMI has only s as the parameter. The three pa-
rameters of cPMId are similar to those of CSR.
cPMIz and cPMId differ in the fact that cP-
MId does not incorporate the document level sig-
nificance. Similarly, we can design another mea-
sure that differs from cPMIz in that it does not in-
corporate corpus level significance. This measure
is called Document Level Significant PMI (PMIz)
and is defined as:
PMIz(x, y) = log Zd(x) ? d(y)/D
Baseline Measure: Suppose cPMIz were to do
better than the PMI. One could ask whether the
improvement achieved is due to the concept of sig-
nificant co-occurrence or is it simply a result of
the fact that we are counting documents instead of
words. To answer this, we design a baseline ver-
sion of PMI where we simply replace word counts
with document counts. The new baseline measure
is called PMI based on Document count (PMId)
and is defined as:
PMId(x, y) = log d(x, y)
d(x) ? d(y)/D
5 Performance Evaluation
Having introduced various measures, we wish to
determine whether the incorporation of corpus and
document level significance improves the perfor-
mance of PMI. Also, if the adapted versions per-
form better than PMI, what are the sources of the
improvements. Is it the concept of corpus level or
document level significance or both, or is the per-
formance gain simply a result of the fact that we
are counting documents instead of words? Since
the newly introduced measures have multiple pa-
rameters, how sensitive is their performance to the
parameter values.
To answer these questions, we repeat the exper-
iments performed in (Chaudhari et al, 2011), us-
ing the exact same dataset, resources, and method-
ology - the same 1.24 Gigawords Wikipedia cor-
pus and the same eight publicly available datasets
- Edinburgh (Kiss et al, 1973), Florida (Nelson
et al, 1980), Kent (Kent and Rosanoff, 1910),
Minnesota (Russell and Jenkins, 1954), White-
Abrams (White and Abrams, 2004), Goldfarb-
Halpern (Goldfarb and Halpern, 1984), Word-
sim (Finkelstein et al, 2002), and Esslli (ESSLLI,
2008). Of these, Wordsim measures semantic re-
latedness which encompasses relations like syn-
onymy, meronymy, antonymy, and functional as-
sociation (Budanitsky and Hirst, 2006). All other
datasets measure free association which refers to
the first response given by a subject on being given
a stimulus word (ESSLLI, 2008).
5.1 Evaluation Methodology
Each measure is evaluated by the correlation be-
tween the ranking of word-associations produced
by the measure and the gold-standard human rank-
ing for that dataset. Since all methods have at least
one parameter, we perform five-fold cross valida-
tion. The span parameter s is varied between 5 and
50 words, and  and ? are varied between 0 and 1.
Each dataset is partitioned into five folds - four for
24
Ed
inb
urg
h
(83
,71
3)
Flo
rid
a
(59
,85
2)
Ke
nt
(14
,08
6)
Mi
nn
eso
ta
(9,
64
9)
W
hit
e-
Ab
ram
s
(65
2)
Go
ldf
arb
-
Ha
lpe
rn
(38
4)
Wo
rds
im
(35
1)
Es
sll
i(2
72
)
PMI 0.22 0.25 0.35 0.25 0.27 0.16 0.69 0.38
cPMI 0.23 0.28 0.40 0.29 0.29 0.17 0.70 0.46
PMId 0.22 0.26 0.37 0.26 0.28 0.17 0.71 0.42
cPMId 0.27 0.32 0.44 0.33 0.36 0.16 0.72 0.54
PMIz 0.24 0.26 0.38 0.26 0.28 0.18 0.71 0.39
cPMIz 0.27 0.32 0.44 0.34 0.35 0.18 0.71 0.53
CSR 0.25 0.30 0.42 0.31 0.34 0.10 0.63 0.43
Table 3: 5-fold cross validation comparison of rank coefficients for different measures. The number of word-pairs in each
dataset is shown against its name. The best performing measures for each dataset are shown in bold.
without corpus with corpus
level significance level signifi-
cance
word-based PMI: 0.075 cPMI: 0.044
document-based PMId: 0.060 cPMId: 0.004
with document
level significance
PMIz: 0.059 cPMIz: 0.004
CSR: 0.049
Table 4: Average deviation of various measures from the best performing measure for each dataset.
training and one for testing. For each association
measure, the parameter values that perform best
on four training folds is used for the remaining
one testing fold. The performance of a measure
on a dataset is its average Spearman rank correla-
tion over 5 runs with 5 different test folds.
5.2 Experimental Results
Results of the 5-fold cross validation are shown
in Table 3. From the results we conclude that the
concept of significant co-occurrence improves the
performance of PMI. The newly designed mea-
sures cPMId and cPMIz perform better than both
PMI and CSR on all eight datasets.
5.3 Performance Improvement Analysis
We can infer from Table 3 that the concept of cor-
pus level significant co-occurrence and not that
of document level significant co-occurrence is re-
sponsible for the PMI performance improvement.
The Spearman rank correlation for cPMIz and cP-
MId are almost identical. cPMId incorporates cor-
pus level significance in a document count based
version of PMI but unlike cPMIz, it does not em-
ploy the concept of document level significance.
To underscore this point, we also compute the
difference between the correlation of each mea-
sure from the correlation of the best measure for
each data set. For each measure we can then com-
pute the average deviation of the measure from the
best performing measure across datasets. In Ta-
ble 4 we present these average deviations. We ob-
serve that:
? Average deviation reduces as we move hor-
izontally across a row - from PMI to cPMI,
from PMId to cPMId, and from PMIz to cP-
MIz. This shows that the incorporation of
corpus level significance helps improve the
performance.
? The average deviation reduces as we move
vertically from the first row to the second -
from PMI to PMId, and from cPMI to cP-
MId. This shows that the performance gain
achieved is also due to the fact that we are
counting documents instead of words.
? Finally, the average deviation remains practi-
cally unchanged as we move vertically from
the second row to the third - from PMId to
PMIz, from cPMId to cPMIz. This shows
that the incorporation of document level sig-
nificance does not help improve the perfor-
mance.
5.4 Parameter Sensitivity Analysis
To find out the sensitivity of cPMId performance
to the parameter values, we evaluate it for different
parameter combinations and present the results in
Table 5. To save space, we show some of the com-
binations only, though one can see the continuity
of performance with gradually changing parame-
ter values.
25
Pa
ram
ete
rs
(s,
?)
Ed
inb
urg
h
(83
,71
3)
Flo
rid
a
(59
,85
2)
Ke
nt
(14
,08
6)
Mi
nn
eso
ta
(9,
64
9)
W
hit
e-
Ab
ram
s
(65
2)
Go
ldf
arb
-
Ha
lpe
rn
(38
4)
Wo
rds
im
(35
1)
Es
sll
i(2
72
)
*, 0.1 0.27 0.32 0.43 0.33 0.35 0.12 0.65 0.55
*, 0.3 0.27 0.32 0.44 0.33 0.36 0.14 0.67 0.55
*, 0.5 0.27 0.32 0.43 0.33 0.36 0.15 0.68 0.54
*, 0.7 0.27 0.32 0.43 0.33 0.36 0.14 0.70 0.54
*, 0.9 0.27 0.31 0.43 0.32 0.35 0.16 0.72 0.53
5w, * 0.27 0.31 0.43 0.33 0.35 0.18 0.66 0.49
10w, * 0.27 0.32 0.43 0.33 0.36 0.18 0.70 0.52
20w, * 0.27 0.32 0.43 0.33 0.36 0.18 0.71 0.54
30w, * 0.27 0.32 0.42 0.32 0.36 0.18 0.71 0.54
40w, * 0.27 0.31 0.42 0.32 0.35 0.17 0.71 0.54
50w, * 0.27 0.31 0.42 0.31 0.36 0.17 0.72 0.53
*, * 0.27 0.32 0.44 0.33 0.36 0.16 0.72 0.54
20w,0.7 0.27 0.32 0.43 0.33 0.36 0.16 0.70 0.54
50w,0.9 0.27 0.31 0.41 0.31 0.35 0.17 0.72 0.53
Table 5: 5-fold cross validation performance of cPMId for various parameter combinations. * indicates a varying parameter.
Ed
inb
urg
h
(83
,71
3)
Flo
rid
a
(59
,85
2)
Ke
nt
(14
,08
6)
Mi
nn
eso
ta
(9,
64
9)
W
hit
e-
Ab
ram
s
(65
2)
Go
ldf
arb
-
Ha
lpe
rn
(38
4)
Wo
rds
im
(35
1)
Es
sll
i(2
72
)
PMI 0.22 0.25 0.35 0.25 0.27 0.16 0.69 0.38
PMId 0.22 0.26 0.37 0.26 0.28 0.17 0.71 0.42
PMI2 0.24 0.30 0.43 0.31 0.29 0.08 0.62 0.44
PMI2d 0.23 0.29 0.42 0.31 0.30 0.06 0.61 0.43
nPMI 0.25 0.30 0.41 0.30 0.31 0.13 0.72 0.47
nPMId 0.23 0.26 0.28 0.24 0.30 0.15 0.71 0.46
cPMId(? : 0.9) 0.27 0.31 0.43 0.32 0.35 0.16 0.72 0.53
Table 6: 5-fold cross validation comparison of cPMId with other PMI variants.
From the results we conclude that the per-
formance of cPMId is reasonably insensitive to
the actual parameter values. For a large range
of parameter combinations, cPMId?s performance
varies marginally and most of the parameter com-
binations perform close to the best. If one does not
have a training corpus then one can chose the best
performing (20w, 0.7) as default parameter values.
As an aside, introducing extra tunable pa-
rameter occasionally reduces performance, as is
the case for Goldfarb-Halpern and Esslli datasets
where (*,*) is not the best performing cobination.
This happens when the parameters combination
that performs best on the four training fold turns
out particularly bad for the testing fold.
5.5 Comparison with other measures
Before comparing cPMId with other measures, we
note that while all co-occurrence measures being
compared have span threshold s as a parameter,
cPMId has an extra tunable parameter ?. While
we would like to argue that part of power of cP-
MId comes from this extra tunable parameter, for
an arguably fairer comparison, we would like to
fix the ? value and then compare so that all meth-
ods have only one tunable parameter s. In Table 5
we find that ? = 0.9 performs best on the fewest
number of datasets and hence we select this fixed
value for comparison. However most of the con-
clusions that follow do not change if we were to
fix some other ? value, or keep it variable.
5.5.1 Comparison with other PMI variants
In Section 3 we pointed out the PMI only works
with probabilities and ignores the absolute amount
of evidence. Another side-effect of this phe-
nomenon is that PMI over-values sparseness. All
frequency ratios (that of f(x), f(y), and f(x, y))
being equal, bigrams composed of low frequency
words get higher score than those composed of
high frequency words. In particular, in case of
perfect dependence, i.e. f(x) = f(y) = f(x, y),
PMI(x, y) = log Wf(x,y) . cPMId addresses thisweakness by explicitly bounding the probability
of observing a given deviation between f(x, y)
and its expected value f(x) ? f(y)/W . Other re-
26
Ed
inb
urg
h
(83
,71
3)
Flo
rid
a
(59
,85
2)
Ke
nt
(14
,08
6)
Mi
nn
eso
ta
(9,
64
9)
W
hit
e-
Ab
ram
s
(65
2)
Go
ldf
arb
-
Ha
lpe
rn
(38
4)
Wo
rds
im
(35
1)
Es
sll
i(2
72
)
Dice 0.20 0.27 0.43 0.32 0.21 0.09 0.59 0.36
Jaccard 0.20 0.27 0.43 0.32 0.21 0.09 0.59 0.36
?2 0.24 0.30 0.43 0.31 0.29 0.08 0.62 0.44
LLR 0.20 0.26 0.40 0.29 0.18 0.03 0.51 0.38
TTest 0.17 0.23 0.37 0.26 0.17 -0.02 0.45 0.33
cPMId(? : 0.9) 0.27 0.31 0.43 0.32 0.35 0.16 0.72 0.53
Table 7: 5-fold cross validation comparison of cPMId with other co-occurrence based measures.
Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) 0.74
(reimplemented in (Yeh et al, 2009)) 0.71
Compact Hierarchical ESA (Liberman and Markovitch, 2009) 0.71
Hyperlink Graph (Milne and Witten, 2008) 0.69
Graph Traversal (Agirre et al, 2009)) 0.66
Distributional Similarity (Agirre et al, 2009)) 0.65
Latent Semantic Analysis (Finkelstein et al, 2002) 0.56
Random Graph Walk (Hughes and Ramage, 2007) 0.55
Normalized Path-length (lch) (Strube and Ponzetto, 2006) 0.55
cPMId(? : 0.9) 0.72
Table 8: Comparison of cPMId with knowledge-based and distributional similarity based measures for the Wordsim dataset.
searchers have addressed this issue by modifying
PMI such that its upper value gets bounded.
Since the maximum value of f(x,y)f(x)?f(y)/W is
1
f(x,y)/W , one way to bound the former is to di-
vide it by later. (Daille, 1994) defined PMI2 as:
PMI2(x, y) = log
f(x,y)
f(x)?f(y)/W
1
f(x,y)/W
= log f(x, y)
2
f(x) ? f(y)
In (Bouma, 2009), it was noted that max. and min.
value of PMI2 are 0,??, whereas one can get1,-1 as the bounds if one normalize PMI as nPMI:
nPMI(x, y) =
log f(x,y)f(x)?f(y)/W
log 1f(x,y)/W
In Table 6, we compare the performance of
word and document count based variants of PMI2
and nPMI with PMI and cPMId. We find that
while both nPMI and PMI2 perform better than
PMI,cPMId performs better than both variants of
nPMI and PMI2 on almost all datasets.
5.5.2 Comparison with other co-occurrence
based measures
In Table 7, we compare cPMId with other co-
occurrence based measures defined in Table 2.
We find that cPMId performs better than all other
co-occurrence based measures. Note that perfor-
mance of Jaccard and Dice measure is identical to
the second decimal place. This is because for our
datasets f(x, y)  f(x) and f(x, y)  f(y) for
most word-pairs under consideration.
5.5.3 Comparison with non co-occurrence
based measures
For completeness of comparison, we also compare
the performance of cPMId with distributional sim-
ilarity and knowledge based measures discussed in
Section 2. Of the datasets discussed here, these
measures have only been tested on the Wordsim
dataset. In Table 8, we compare the performance
of cPMId with these other measures on the Word-
sim dataset. We can see that cPMId compares well
with the best non co-occurrence based measures.
6 Conclusions and Future Work
By incorporating the concept of significant co-
occurrence in PMI, we get a new measure which
performs better than other co-occurrence based
measures. We investigate the source of the perfor-
mance improvement and find that of the two no-
tions of significance: corpus-level and document-
level significant co-occurrence, the concept of cor-
pus level significance combined with use docu-
ment counts in place of word counts is responsi-
ble for all the performance gains observed. We
also find that the performance of the newly intro-
duced measure cPMId is reasonably insensitive to
the values of its tunable parameters.
Acknowledgements
We thank Dipak Chaudhari and Shweta Ghonghe
for their help with the implementation.
27
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In NAACL-
HLT.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. Measuring semantic similarity be-
tween words using web search engines. In WWW,
pages 757?766.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction, from form to
meaning: Processing texts automatically. In Pro-
ceedings of the Biennial GSCL Conference.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguists, 32(1):13?47.
Dipak L. Chaudhari, Om P. Damani, and Srivatsan Lax-
man. 2011. Lexical co-occurrence, statistical sig-
nificance, and word association. In EMNLP.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicog-
raphy. In ACL, pages 76?83.
B. Daille. 1994. Approche mixte pour l?extraction au-
tomatique de terminologie: statistiques lexicales etl-
tres linguistiques. Ph.D. thesis, Universitie Paris 7.
L. R. Dice. 1945. Measures of the amount of ecolog-
ical association between species. Ecology, 26:297?
302.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
ESSLLI. 2008. Free association task at
lexical semantics workshop esslli 2008.
http://wordspace.collocations.
de/doku.php/workshop:esslli:task.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: the
concept revisited. ACM Trans. Inf. Syst., 20(1):116?
131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI.
Robert Goldfarb and Harvey Halpern. 1984. Word as-
sociation responses in normal adult subjects. Jour-
nal of Psycholinguistic Research, 13(1):37?55.
T Hughes and D Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In EMNLP.
P. Jaccard. 1912. The distribution of the flora of the
alpine zone. New Phytologist, 11:37?50.
G. Kent and A. Rosanoff. 1910. A study of associa-
tion in insanity. American Journal of Insanity, pages
317?390.
G. Kiss, C. Armstrong, R. Milroy, and J. Piper. 1973.
An associative thesaurus of english and its computer
analysis. In The Computer and Literary Studies,
pages 379?382. Edinburgh University Press.
Sonya Liberman and Shaul Markovitch. 2009. Com-
pact hierarchical explicit semantic representation. In
Proceedings of the IJCAI 2009 Workshop on User-
Contributed Knowledge and Artificial Intelligence:
An Evolving Synergy (WikiAI09), Pasadena, CA,
July.
David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from wikipedia links. In ACL.
D. Nelson, C. McEvoy, J. Walling, and J. Wheeler.
1980. The university of south florida homograph
norms. Behaviour Research Methods and Instru-
mentation, 12:16?37.
W.A. Russell and J.J. Jenkins. 1954. The complete
minnesota norms for responses to 100 words from
the kent-rosanoff word association test. Technical
report, Office of Naval Research and University of
Minnesota.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In AAAI, pages 1419?1424.
T. Wandmacher, E. Ovchinnikova, and T. Alexandrov.
2008. Does latent semantic analysis reflect human
associations? In European Summer School in Logic,
Language and Information (ESSLLI?08).
Katherine K. White and Lise Abrams. 2004. Free as-
sociations and dominance ratings of homophones for
young and older adults. Behavior Research Meth-
ods, Instruments, & Computers, 36(3):408?420.
Eric Yeh, Daniel Ramage, Chris Manning, Eneko
Agirre, and Aitor Soroa. 2009. Wikiwalk: Ran-
dom walks on wikipedia for semantic relatedness. In
ACL workshop ?TextGraphs-4: Graph-based Meth-
ods for Natural Language Processing?.
28
