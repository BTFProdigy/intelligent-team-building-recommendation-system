Proceedings of NAACL HLT 2007, Companion Volume, pages 93?96,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Detection of Non-native Sentences using Machine-translated Training Data
John Lee
Spoken Language Systems
MIT CSAIL
Cambridge, MA 02139, USA
jsylee@csail.mit.edu
Ming Zhou, Xiaohua Liu
Natural Language Computing Group
Microsoft Research Asia
Beijing, 100080, China
{mingzhou,xiaoliu}@microsoft.com
Abstract
Training statistical models to detect non-
native sentences requires a large corpus
of non-native writing samples, which is
often not readily available. This paper
examines the extent to which machine-
translated (MT) sentences can substitute
as training data.
Two tasks are examined. For the na-
tive vs non-native classication task, non-
native training data yields better perfor-
mance; for the ranking task, however,
models trained with a large, publicly avail-
able set of MT data perform as well as
those trained with non-native data.
1 Introduction
For non-native speakers writing in a foreign lan-
guage, feedback from native speakers is indispens-
able. While humans are likely to provide higher-
quality feedback, a computer system can offer bet-
ter availability and privacy. A system that can dis-
tinguish non-native (?ill-formed?) English sentences
from native (?well-formed?) ones would provide
valuable assistance in improving their writing.
Classifying a sentence into discrete categories can
be difficult: a sentence that seems fluent to one judge
might not be good enough to another. An alternative
is to rank sentences by their relative fluency. This
would be useful when a non-native speaker is un-
sure which one of several possible ways of writing a
sentence is the best.
We therefore formulate two tasks on this problem.
The classification task gives one sentence to the sys-
tem, and asks whether it is native or non-native. The
ranking task submits sentences with the same in-
tended meaning, and asks which one is best.
To tackle these tasks, hand-crafting formal rules
would be daunting. Statistical methods, however,
require a large corpus of non-native writing sam-
ples, which can be difficult to compile. Since
machine-translated (MT) sentences are readily avail-
able in abundance, we wish to address the question
of whether they can substitute as training data.
The next section provides background on related
research. Sections 3 and 4 describe our experiments,
followed by conclusions and future directions.
2 Related Research
Previous research has paid little attention to rank-
ing sentences by fluency. As for classification, one
line of research in MT evaluation is to evaluate the
fluency of an output sentence without its reference
translations, such as in (Corston-Oliver et al, 2001)
and (Gamon et al, 2005). Our task here is simi-
lar, but is applied on non-native sentences, arguably
more challenging than MT output.
Evaluation of non-native writing has encom-
passed both the document and sentence levels. At
the document level, automatic essay scorers, such
as (Burstein et al, 2004) and (Ishioka and Kameda,
2006), can provide holistic scores that correlate well
with those of human judges.
At the sentence level, which is the focus of this
paper, previous work follows two trends. Some re-
searchers explicitly focus on individual classes of er-
93
rors, e.g., mass vs count nouns in (Brockett et al,
2006) and (Nagata et al, 2006). Others implicitly do
so with hand-crafted rules, via templates (Heidorn,
2000) or mal-rules in context-free grammars, such
as (Michaud et al, 2000) and (Bender et al, 2004).
Typically, however, non-native writing exhibits a
wide variety of errors, in grammar, style and word
collocations. In this research, we allow unrestricted
classes of errors1, and in this regard our goal is clos-
est to that of (Tomokiyo and Jones, 2001). How-
ever, they focus on non-native speech, and assume
the availability of non-native training data.
3 Experimental Set-Up
3.1 Data
Our data consists of pairs of English sentences, one
native and the other non-native, with the same ?in-
tended meaning?. In our MT data (MT), both sen-
tences are translated, by machine or human, from
the same sentence in a foreign language. In our non-
native data (JLE), the non-native sentence has been
edited by a native speaker2. Table 1 gives some ex-
amples, and Table 2 presents some statistics.
MT (Multiple-Translation Chinese and Multiple-
Translation Arabic corpora) English MT out-
put, and human reference translations, of Chi-
nese and Arabic newspaper articles.
JLE (Japanese Learners of English Corpus) Tran-
scripts of Japanese examinees in the Standard
Speaking Test. False starts and disfluencies
were then cleaned up, and grammatical mis-
takes tagged (Izumi et al, 2003). The speaking
style is more formal than spontaneous English,
due to the examination setting.
3.2 Machine Learning Framework
SVM-Light (Joachims, 1999), an implementation
of Support Vector Machines (SVM), is used for the
classification task.
For the ranking task, we utilize the ranking mode
of SVM-Light. In this mode, the SVM algorithm
is adapted for learning ranking functions, origi-
nally used for ranking web pages with respect to a
1Except spelling mistakes, which we consider to be a sepa-
rate problem that should be dealt with in a pre-processing step.
2The nature of the non-native data constrains the ranking to
two sentences at a time.
query (Joachims, 2002). In our context, given a set
of English sentences with similar semantic content,
say s1, . . . , sn, and a ranking based on their fluency,
the learning algorithm estimates the weights ~w to
satisfy the inequalities:
~w ? ?(sj) > ~w ? ?(sk) (1)
where sj is more fluent than sk, and where ? maps
a sentence to a feature vector. This is in contrast to
standard SVMs, which learn a hyperplane boundary
between native and non-native sentences from the
inequalities:
yi(~w ? ?(si) + w0) ? 1 ? 0 (2)
where yi = ?1 are the labels. Linear kernels are
used in our experiments, and the regularization pa-
rameter is tuned on the development sets.
3.3 Features
The following features are extracted from each sen-
tence. The first two are real numbers; the rest are
indicator functions of the presence of the lexical
and/or syntactic properties in question.
Ent Entropy3 from a trigram language model
trained on 4.4 million English sentences with
the SRILM toolkit (Stolcke, 2002). The tri-
grams are intended to detect local mistakes.
Parse Parse score from Model 2 of the statisti-
cal parser (Collins, 1997), normalized by the
number of words. We hypothesize that non-
native sentences are more likely to receive
lower scores.
Deriv Parse tree derivations, i.e., from each parent
node to its children nodes, such as S ? NP VP.
Some non-native sentences have plausible N -
grams, but have derivations infrequently seen
in well-formed sentences, due to their unusual
syntactic structures.
DtNoun Head word of a base noun phrase, and its
determiner, e.g., (the, markets) from the human
non-native sentence in Table 1. The usage of ar-
ticles has been found to be the most frequent er-
ror class in the JLE corpus (Izumi et al, 2003).
3Entropy H(x) is related to perplexity PP (x) by the equa-
tion PP (x) = 2H(x).
94
Type Sentence
Native Human New York and London stock markets went up
Non-native Human The stock markets in New York and London were increasing together
MT The same step of stock market of London of New York rises
Table 1: Examples of sentences translated from a Chinese source sentence by a native speaker, by a non-
native speaker, and by a machine translation system.
Data Set Corpus # sentences (for classification) # pairs (for
total native non-native ranking)
MT train LDC{2002T01, 2003T18, 2006T04} 30075 17508 12567 91795
MT dev LDC2003T17 (Zaobao only) 1995 1328 667 2668
MT test LDC2003T17 (Xinhua only) 3255 2184 1071 4284
JLE train Japanese Learners of English 9848 4924 4924 4924
JLE dev 1000 500 500 500
JLE test 1000 500 500 500
Table 2: Data sets used in this paper.
Colloc An in-house dependency parser extracts
five types of word dependencies4: subject-verb,
verb-object, adjective-noun, verb-adverb and
preposition-object. For the human non-native
sentence in Table 1, the unusual subject-verb
collocation ?market increase? is a useful clue
in this otherwise well-formed sentence.
4 Analysis
4.1 An Upper Bound
To gauge the performance upper bound, we first at-
tempt to classify and rank the MT test data, which
should be less challenging than non-native data. Af-
ter training the SVM on MT train, classification
accuracy on MT test improves with the addition
of each feature, culminating at 89.24% with all
five features. This result compares favorably with
the state-of-the-art5. Ranking performance reaches
96.73% with all five features.
We now turn our attention to non-native test data,
and contrast the performance on JLE test using
models trained by MT data (MT train), and by
non-native data (JLE train).
4Proper nouns and numbers are replaced with special sym-
bols. The words are further stemmed using Porter?s Stemmer.
5Direct comparison is impossible since the corpora were dif-
ferent. (Corston-Oliver et al, 2001) reports 82.89% accuracy
on English software manuals and online help documents, and
(Gamon et al, 2005) reports 77.59% on French technical docu-
ments.
Test Set: Train Set
JLE test MT train JLE train
Ent+ 57.2 57.7
Parse (+) 48.6 (+) 70.6
(-) 65.8 (-) 44.8
+Deriv 58.4 64.7
(+) 54.6 (+)72.2
(-) 62.2 (-) 57.2
+DtNoun 59.0 66.4
(+) 57.6 (+) 72.8
(-) 60.4 (-) 60.0
+Colloc 58.6 65.9
(+) 54.2 (+) 72.6
(-) 63.2 (-) 59.2
Table 3: Classication accuracy on JLE test. (-)
indicates accuracy on non-native sentences, and (+)
indicates accuracy on native sentences. The overall
accuracy is their average.
4.2 Classification
As shown in Table 3, classification accuracy on JLE
test is higher with the JLE train set (66.4%)
than with the larger MT train set (59.0%). The
SVM trained on MT train consistently misclas-
sifies more native sentences than non-native ones.
One reason might be that speech transcripts have a
less formal style than written news sentences. Tran-
scripts of even good conversational English do not
always resemble sentences in the news domain.
4.3 Ranking
In the ranking task, the relative performance be-
tween MT and non-native training data is reversed.
95
Test Set: Train Set
JLE test MT train JLE train
Ent+Parse 72.8 71.4
+Deriv 73.4 73.6
+DtNoun 75.4 73.8
+Colloc 76.2 74.6
Table 4: Ranking accuracy on JLE test.
As shown in Table 4, models trained on MT train
yield higher ranking accuracy (76.2%) than those
trained on JLE train (74.6%). This indicates that
MT training data can generalize well enough to per-
form better than a non-native training corpus of size
up to 10000.
The contrast between the classification and rank-
ing results suggests that train/test data mismatch is
less harmful for the latter task. Weights trained on
the classification inequalities in (2) and on the rank-
ing inequalities in (1) both try to separate native and
MT sentences maximally. The absolute boundary
learned in (2) is inherently specific to the nature
of the training sentences, as we have seen in ?4.2.
In comparison, the relative scores learned from (1)
have a better chance to carry over to other domains,
as long as some gap still exists between the scores
of the native and non-native sentences.
5 Conclusions & Future Work
We explored two tasks in sentence-level fluency
evaluation: ranking and classifying native vs. non-
native sentences. In an SVM framework, we exam-
ined how well MT data can replace non-native data
in training.
For the classification task, training with MT data
is less effective than with non-native data. How-
ever, for the ranking task, models trained on pub-
licly available MT data generalize well, performing
as well as those trained with a non-native corpus of
size 10000.
In the future, we would like to search for more
salient features through a careful study of non-native
errors, using error-tagged corpora such as (Izumi et
al., 2003). We also plan to explore techniques for
combining large MT training corpora and smaller
non-native training corpora. Our ultimate goal is to
identify the errors in the non-native sentences and
propose corrections.
References
E. Bender, D. Flickinger, S. Oepen, A. Walsh, and T.
Baldwin. 2004. Arboretum: Using a Precision Gram-
mar for Grammar Checking in CALL. Proc. In-
STIL/ICALL Symposium on Computer Assisted Learn-
ing.
C. Brockett, W. Dolan, and M. Gamon. 2006. Correct-
ing ESL Errors using Phrasal SMT Techniques. Proc.
ACL.
J. Burstein, M. Chodorow and C. Leacock. 2004. Auto-
mated Essay Evaluation: The Criterion online Writing
Service. AI Magazine, 25(3):27?36.
M. Collins. 1997. Three Generative, Lexicalised Models
for Statistical Parsing. Proc. ACL.
S. Corston-Oliver, M. Gamon and C. Brockett. 2001. A
Machine Learning Approach to the Automatic Evalu-
ation of Machine Translation. Proc. ACL.
M. Gamon, A. Aue, and M. Smets. 2005. Sentence-
Level MT Evaluation without Reference Translations:
Beyond Language Modeling. Proc. EAMT.
G. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert
Dale, Hermann Moisi and Harold Somers (ed.). Mar-
cel Dekker, Inc.
T. Ishioka and M. Kameda. 2006. Automated Japanese
Essay Scoring System based on Articles Written by
Experts. Proc. ACL.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H.
Isahara. 2003. Automatic Error Detection in the
Japanese Learners? English Spoken Data. Proc. ACL.
T. Joachims. 1999. Making Large-Scale SVM Learning
Practical. Advances in Kernel Methods - Support Vec-
tor Learning. B. Scho?lkopf, C. Burges and A. Smola
(ed.), MIT-Press.
T. Joachims. 2002. Optimizing Search Engines using
Clickthrough Data. Proc. SIGKDD.
L. Michaud, K. McCoy and C. Pennington. 2000. An In-
telligent Tutoring System for Deaf Learners of Written
English. Proc. 4th International ACM Conference on
Assistive Technologies.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A
Feedback-Augmented Method for Detecting Errors in
the Writing of Learners of English. Proc. ACL.
A. Stolcke. 2002. SRILM ? An Extensible Language
Modeling Toolkit Proc. ICSLP.
L. Tomokiyo and R. Jones. 2001. You?re not from ?round
here, are you? Na??ve Bayes Detection of Non-native
Utterance Text. Proc. NAACL.
96
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81?88,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Detecting Erroneous Sentences using Automatically Mined Sequential
Patterns
Guihua Sun ? Xiaohua Liu Gao Cong Ming Zhou
Chongqing University Microsoft Research Asia
sunguihua5018@163.com {xiaoliu, gaocong, mingzhou}@microsoft.com
Zhongyang Xiong John Lee ? Chin-Yew Lin
Chongqing University MIT Microsoft Research Asia
zyxiong@cqu.edu.cn jsylee@mit.edu cyl@microsoft.com
Abstract
This paper studies the problem of identify-
ing erroneous/correct sentences. The prob-
lem has important applications, e.g., pro-
viding feedback for writers of English as
a Second Language, controlling the quality
of parallel bilingual sentences mined from
the Web, and evaluating machine translation
results. In this paper, we propose a new
approach to detecting erroneous sentences
by integrating pattern discovery with super-
vised learning models. Experimental results
show that our techniques are promising.
1 Introduction
Detecting erroneous/correct sentences has the fol-
lowing applications. First, it can provide feedback
for writers of English as a Second Language (ESL)
as to whether a sentence contains errors. Second, it
can be applied to control the quality of parallel bilin-
gual sentences mined from the Web, which are criti-
cal sources for a wide range of applications, such as
statistical machine translation (Brown et al, 1993)
and cross-lingual information retrieval (Nie et al,
1999). Third, it can be used to evaluate machine
translation results. As demonstrated in (Corston-
Oliver et al, 2001; Gamon et al, 2005), the better
human reference translations can be distinguished
from machine translations by a classification model,
the worse the machine translation system is.
?Work done while the author was a visiting student at MSRA
?Work done while the author was a visiting student at MSRA
The previous work on identifying erroneous sen-
tences mainly aims to find errors from the writing of
ESL learners. The common mistakes (Yukio et al,
2001; Gui and Yang, 2003) made by ESL learners
include spelling, lexical collocation, sentence struc-
ture, tense, agreement, verb formation, wrong Part-
Of-Speech (POS), article usage, etc. The previous
work focuses on grammar errors, including tense,
agreement, verb formation, article usage, etc. How-
ever, little work has been done to detect sentence
structure and lexical collocation errors.
Some methods of detecting erroneous sentences
are based on manual rules. These methods (Hei-
dorn, 2000; Michaud et al, 2000; Bender et al,
2004) have been shown to be effective in detect-
ing certain kinds of grammatical errors in the writ-
ing of English learners. However, it could be ex-
pensive to write rules manually. Linguistic experts
are needed to write rules of high quality; Also, it
is difficult to produce and maintain a large num-
ber of non-conflicting rules to cover a wide range of
grammatical errors. Moreover, ESL writers of differ-
ent first-language backgrounds and skill levels may
make different errors, and thus different sets of rules
may be required. Worse still, it is hard to write rules
for some grammatical errors, for example, detecting
errors concerning the articles and singular plural us-
age (Nagata et al, 2006).
Instead of asking experts to write hand-crafted
rules, statistical approaches (Chodorow and Lea-
cock, 2000; Izumi et al, 2003; Brockett et al, 2006;
Nagata et al, 2006) build statistical models to iden-
tify sentences containing errors. However, existing
81
statistical approaches focus on some pre-defined er-
rors and the reported results are not attractive. More-
over, these approaches, e.g., (Izumi et al, 2003;
Brockett et al, 2006) usually need errors to be spec-
ified and tagged in the training sentences, which re-
quires expert help to be recruited and is time con-
suming and labor intensive.
Considering the limitations of the previous work,
in this paper we propose a novel approach that is
based on pattern discovery and supervised learn-
ing to successfully identify erroneous/correct sen-
tences. The basic idea of our approach is to build
a machine learning model to automatically classify
each sentence into one of the two classes, ?erro-
neous? and ?correct.? To build the learning model,
we automatically extract labeled sequential patterns
(LSPs) from both erroneous sentences and correct
sentences, and use them as input features for classi-
fication models. Our main contributions are:
? We mine labeled sequential patterns(LSPs)
from the preprocessed training data to build
leaning models. Note that LSPs are also very
different from N-gram language models that
only consider continuous sequences.
? We also enrich the LSP features with other auto-
matically computed linguistic features, includ-
ing lexical collocation, language model, syn-
tactic score, and function word density. In con-
trast with previous work focusing on (a spe-
cific type of) grammatical errors, our model can
handle a wide range of errors, including gram-
mar, sentence structure, and lexical choice.
? We empirically evaluate our methods on two
datasets consisting of sentences written by
Japanese and Chinese, respectively. Experi-
mental results show that labeled sequential pat-
terns are highly useful for the classification
results, and greatly outperform other features.
Our method outperforms Microsoft Word03
and ALEK (Chodorow and Leacock, 2000)
from Educational Testing Service (ETS) in
some cases. We also apply our learning model
to machine translation (MT) data as a comple-
mentary measure to evaluate MT results.
The rest of this paper is organized as follows.
The next section discusses related work. Section 3
presents the proposed technique. We evaluate our
proposed technique in Section 4. Section 5 con-
cludes this paper and discusses future work.
2 Related Work
Research on detecting erroneous sentences can be
classified into two categories. The first category
makes use of hand-crafted rules, e.g., template
rules (Heidorn, 2000) and mal-rules in context-free
grammars (Michaud et al, 2000; Bender et al,
2004). As discussed in Section 1, manual rule based
methods have some shortcomings.
The second category uses statistical techniques
to detect erroneous sentences. An unsupervised
method (Chodorow and Leacock, 2000) is em-
ployed to detect grammatical errors by inferring
negative evidence from TOEFL administrated by
ETS. The method (Izumi et al, 2003) aims to de-
tect omission-type and replacement-type errors and
transformation-based leaning is employed in (Shi
and Zhou, 2005) to learn rules to detect errors for
speech recognition outputs. They also require spec-
ifying error tags that can tell the specific errors
and their corrections in the training corpus. The
phrasal Statistical Machine Translation (SMT) tech-
nique is employed to identify and correct writing er-
rors (Brockett et al, 2006). This method must col-
lect a large number of parallel corpora (pairs of er-
roneous sentences and their corrections) and perfor-
mance depends on SMT techniques that are not yet
mature. The work in (Nagata et al, 2006) focuses
on a type of error, namely mass vs. count nouns.
In contrast to existing statistical methods, our tech-
nique needs neither errors tagged nor parallel cor-
pora, and is not limited to a specific type of gram-
matical error.
There are also studies on automatic essay scoring
at document-level. For example, E-rater (Burstein
et al, 1998), developed by the ETS, and Intelligent
Essay Assessor (Foltz et al, 1999). The evaluation
criteria for documents are different from those for
sentences. A document is evaluated mainly by its or-
ganization, topic, diversity of vocabulary, and gram-
mar while a sentence is done by grammar, sentence
structure, and lexical choice.
Another related work is Machine Translation (MT)
evaluation. Classification models are employed
in (Corston-Oliver et al, 2001; Gamon et al, 2005)
82
to evaluate the well-formedness of machine transla-
tion outputs. The writers of ESL and MT normally
make different mistakes: in general, ESL writers can
write overall grammatically correct sentences with
some local mistakes while MT outputs normally pro-
duce locally well-formed phrases with overall gram-
matically wrong sentences. Hence, the manual fea-
tures designed for MT evaluation are not applicable
to detect erroneous sentences from ESL learners.
LSPs differ from the traditional sequential pat-
terns, e.g., (Agrawal and Srikant, 1995; Pei et al,
2001) in that LSPs are attached with class labels and
we prefer those with discriminating ability to build
classification model. In our other work (Sun et al,
2007), labeled sequential patterns, together with la-
beled tree patterns, are used to build pattern-based
classifier to detect erroneous sentences. The clas-
sification method in (Sun et al, 2007) is different
from those used in this paper. Moreover, instead of
labeled sequential patterns, in (Sun et al, 2007) the
most significant k labeled sequential patterns with
constraints for each training sentence are mined to
build classifiers. Another related work is (Jindal and
Liu, 2006), where sequential patterns with labels are
used to identify comparative sentences.
3 Proposed Technique
This section first gives our problem statement and
then presents our proposed technique to build learn-
ing models.
3.1 Problem Statement
In this paper we study the problem of identifying
erroneous/correct sentences. A set of training data
containing correct and erroneous sentences is given.
Unlike some previous work, our technique requires
neither that the erroneous sentences are tagged with
detailed errors, nor that the training data consist of
parallel pairs of sentences (an error sentence and its
correction). The erroneous sentence contains a wide
range of errors on grammar, sentence structure, and
lexical choice. We do not consider spelling errors in
this paper.
We address the problem by building classifica-
tion models. The main challenge is to automatically
extract representative features for both correct and
erroneous sentences to build effective classification
models. We illustrate the challenge with an exam-
ple. Consider an erroneous sentence, ?If Maggie will
go to supermarket, she will buy a bag for you.? It is
difficult for previous methods using statistical tech-
niques to capture such an error. For example, N-
gram language model is considered to be effective
in writing evaluation (Burstein et al, 1998; Corston-
Oliver et al, 2001). However, it becomes very ex-
pensive if N > 3 and N-grams only consider contin-
uous sequence of words, which is unable to detect
the above error ?if...will...will?.
We propose labeled sequential patterns to effec-
tively characterize the features of correct and er-
roneous sentences (Section 3.2), and design some
complementary features ( Section 3.3).
3.2 Mining Labeled Sequential Patterns ( LSP )
Labeled Sequential Patterns (LSP). A labeled se-
quential pattern, p, is in the form of LHS? c, where
LHS is a sequence and c is a class label. Let I be a
set of items and L be a set of class labels. Let D be a
sequence database in which each tuple is composed
of a list of items in I and a class label in L. We say
that a sequence s1 =< a1, ..., am > is contained in
a sequence s2 =< b1, ..., bn > if there exist integers
i1, ...im such that 1 ? i1 < i2 < ... < im ? n and
aj = bij for all j ? 1, ...,m. Similarly, we say that
a LSP p1 is contained by p2 if the sequence p1.LHS
is contained by p2.LHS and p1.c = p2.c. Note that
it is not required that s1 appears continuously in s2.
We will further refine the definition of ?contain? by
imposing some constraints (to be explained soon).
A LSP p is attached with two measures, support and
confidence. The support of p, denoted by sup(p),
is the percentage of tuples in database D that con-
tain the LSP p. The probability of the LSP p being
true is referred to as ?the confidence of p ?, denoted
by conf(p), and is computed as sup(p)sup(p.LHS) . The
support is to measure the generality of the pattern p
and minimum confidence is a statement of predictive
ability of p.
Example 1: Consider a sequence database contain-
ing three tuples t1 = (< a, d, e, f >,E), t2 = (<
a, f, e, f >,E) and t3 = (< d, a, f >,C). One
example LSP p1 = < a, e, f >? E, which is con-
tained in tuples t1 and t2. Its support is 66.7% and
its confidence is 100%. As another example, LSP p2
83
= < a, f >? E with support 66.7% and confidence
66.7%. p1 is a better indication of class E than p2.
2
Generating Sequence Database. We generate the
database by applying Part-Of-Speech (POS) tagger
to tag each training sentence while keeping func-
tion words1 and time words2. After the process-
ing, each sentence together with its label becomes
a database tuple. The function words and POS tags
play important roles in both grammars and sentence
structures. In addition, the time words are key
clues in detecting errors of tense usage. The com-
bination of them allows us to capture representative
features for correct/erroneous sentences by mining
LSPs. Some example LSPs include ?<a, NNS> ?
Error?(singular determiner preceding plural noun),
and ?<yesterday, is>?Error?. Note that the con-
fidences of these LSPs are not necessary 100%.
First, we use MXPOST-Maximum Entropy Part of
Speech Tagger Toolkit3 for POS tags. The MXPOST
tagger can provide fine-grained tag information. For
example, noun can be tagged with ?NN?(singular
noun) and ?NNS?(plural noun); verb can be tagged
with ?VB?, ?VBG?, ?VBN?, ?VBP?, ?VBD? and
?VBZ?. Second, the function words and time words
that we use form a key word list. If a word in a
training sentence is not contained in the key word
list, then the word will be replaced by its POS. The
processed sentence consists of POS and the words of
key word list. For example, after the processing, the
sentence ?In the past, John was kind to his sister? is
converted into ?In the past, NNP was JJ to his NN?,
where the words ?in?, ?the?, ?was?, ?to? and ?his?
are function words, the word ?past? is time word,
and ?NNP?, ?JJ?, and ?NN? are POS tags.
Mining LSPs. The length of the discovered LSPs
is flexible and they can be composed of contiguous
or distant words/tags. Existing frequent sequential
pattern mining algorithms (e.g. (Pei et al, 2001))
use minimum support threshold to mine frequent se-
quential patterns whose support is larger than the
threshold. These algorithms are not sufficient for our
problem of mining LSPs. In order to ensure that all
our discovered LSPs are discriminating and are capa-
1http://www.marlodge.supanet.com/museum/funcword.html
2http://www.wjh.harvard.edu/%7Einquirer/Time%40.html
3http://www.cogsci.ed.ac.uk/?jamesc/taggers/MXPOST.html
ble of predicting correct or erroneous sentences, we
impose another constraint minimum confidence. Re-
call that the higher the confidence of a pattern is, the
better it can distinguish between correct sentences
and erroneous sentences. In our experiments, we
empirically set minimum support at 0.1% and mini-
mum confidence at 75%.
Mining LSPs is nontrivial since its search space
is exponential, althought there have been a host of
algorithms for mining frequent sequential patterns.
We adapt the frequent sequence mining algorithm
in (Pei et al, 2001) for mining LSPs with constraints.
Converting LSPs to Features. Each discovered LSP
forms a binary feature as the input for classification
model. If a sentence includes a LSP, the correspond-
ing feature is set at 1.
The LSPs can characterize the correct/erroneous
sentence structure and grammar. We give some ex-
amples of the discovered LSPs. (1) LSPs for erro-
neous sentences. For example, ?<this, NNS>?(e.g.
contained in ?this books is stolen.?), ?<past,
is>?(e.g. contained in ?in the past, John is kind to
his sister.?), ?<one, of, NN>?(e.g. contained in ?it is
one of important working language?, ?<although,
but>?(e.g. contained in ?although he likes it, but
he can?t buy it.?), and ?<only, if, I, am>?(e.g. con-
tained in ?only if my teacher has given permission,
I am allowed to enter this room?). (2) LSPs for cor-
rect sentences. For instance, ?<would, VB>?(e.g.
contained in ?he would buy it.?), and ?<VBD,
yeserday>?(e.g. contained in ?I bought this book
yesterday.?).
3.3 Other Linguistic Features
We use some linguistic features that can be com-
puted automatically as complementary features.
Lexical Collocation (LC) Lexical collocation er-
ror (Yukio et al, 2001; Gui and Yang, 2003) is com-
mon in the writing of ESL learners, such as ?strong
tea? but not ?powerful tea.? Our LSP features can-
not capture all LCs since we replace some words
with POS tags in mining LSPs. We collect five types
of collocations: verb-object, adjective-noun, verb-
adverb, subject-verb, and preposition-object from a
general English corpus4. Correct LCs are collected
4The general English corpus consists of about 4.4 million
native sentences.
84
by extracting collocations of high frequency from
the general English corpus. Erroneous LC candi-
dates are generated by replacing the word in correct
collocations with its confusion words, obtained from
WordNet, including synonyms and words with sim-
ilar spelling or pronunciation. Experts are consulted
to see if a candidate is a true erroneous collocation.
We compute three statistical features for each sen-
tence below. (1) The first feature is computed by
m?
i=1
p(coi)/n, where m is the number of CLs, n is
the number of collocations in each sentence, and
probability p(coi) of each CL coi is calculated us-
ing the method (Lu? and Zhou, 2004). (2) The sec-
ond feature is computed by the ratio of the number
of unknown collocations (neither correct LCs nor er-
roneous LCs) to the number of collocations in each
sentence. (3) The last feature is computed by the ra-
tio of the number of erroneous LCs to the number of
collocations in each sentence.
Perplexity from Language Model (PLM) Perplex-
ity measures are extracted from a trigram language
model trained on a general English corpus using
the SRILM-SRI Language Modeling Toolkit (Stolcke,
2002). We calculate two values for each sentence:
lexicalized trigram perplexity and part of speech
(POS) trigram perplexity. The erroneous sentences
would have higher perplexity.
Syntactic Score (SC) Some erroneous sentences of-
ten contain words and concepts that are locally cor-
rect but cannot form coherent sentences (Liu and
Gildea, 2005). To measure the coherence of sen-
tences, we use a statistical parser Toolkit (Collins,
1997) to assign each sentence a parser?s score that
is the related log probability of parsing. We assume
that erroneous sentences with undesirable sentence
structures are more likely to receive lower scores.
Function Word Density (FWD) We consider the
density of function words (Corston-Oliver et al,
2001), i.e. the ratio of function words to content
words. This is inspired by the work (Corston-Oliver
et al, 2001) showing that function word density can
be effective in distinguishing between human refer-
ences and machine outputs. In this paper, we calcu-
late the densities of seven kinds of function words 5
5including determiners/quantifiers, all pronouns, different
pronoun types: Wh, 1st, 2nd, and 3rd person pronouns, prepo-
Dataset Type Source Number
JC
(+) the Japan Times newspaperand Model English Essay 16,857
(-)
HEL (Hiroshima English
Learners? Corpus) and JLE
(Japanese Learners of En-
glish Corpus)
17,301
CC (+) the 21st Century newspaper 3,200
(-)
CLEC (Chinese Learner Er-
ror Corpus) 3,199
Table 1: Corpora ((+): correct; (-): erroneous)
respectively as 7 features.
4 Experimental Evaluation
We evaluated the performance of our techniques
with support vector machine (SVM) and Naive
Bayesian (NB) classification models. We also com-
pared the effectiveness of various features. In ad-
dition, we compared our technique with two other
methods of checking errors, Microsoft Word03 and
ALEK method (Chodorow and Leacock, 2000). Fi-
nally, we also applied our technique to evaluate the
Machine Translation outputs.
4.1 Experimental Setup
Classification Models. We used two classification
models, SVM6 and NB classification model.
Data. We collected two datasets from different do-
mains, Japanese Corpus (JC) and Chinese Corpus
(CC). Table 1 gives the details of our corpora. In
the learner?s corpora, all of the sentences are erro-
neous. Note that our data does not consist of parallel
pairs of sentences (one error sentence and its correc-
tion). The erroneous sentences includes grammar,
sentence structure and lexical choice errors, but not
spelling errors.
For each sentence, we generated five kinds of fea-
tures as presented in Section 3. For a non-binary
feature X , its value x is normalized by z-score,
norm(x) = x?mean(X)?var(X) , where mean(x) is the em-
pirical mean of X and var(X) is the variance of X .
Thus each sentence is represented by a vector.
Metrics We calculated the precision, recall,
and F-score for correct and erroneous sentences,
respectively, and also report the overall accuracy.
sitions and adverbs, auxiliary verbs, and conjunctions.
6http://svmlight.joachims.org/
85
All the experimental results are obtained thorough
10-fold cross-validation.
4.2 Experimental Results
The Effectiveness of Various Features. The exper-
iment is to evaluate the contribution of each feature
to the classification. The results of SVM are given in
Table 2. We can see that the performance of labeled
sequential patterns (LSP) feature consistently out-
performs those of all the other individual features. It
also performs better even if we use all the other fea-
tures together. This is because other features only
provide some relatively abstract and simple linguis-
tic information, whereas the discovered LSP s char-
acterize significant linguistic features as discussed
before. We also found that the results of NB are a
little worse than those of SVM. However, all the fea-
tures perform consistently on the two classification
models and we can observe the same trend. Due to
space limitation, we do not give results of NB.
In addition, the discovered LSPs themselves are
intuitive and meaningful since they are intuitive fea-
tures that can distinguish correct sentences from er-
roneous sentences. We discovered 6309 LSPs in
JC data and 3742 LSPs in CC data. Some exam-
ple LSPs discovered from erroneous sentences are
<a, NNS> (support:0.39%, confidence:85.71%),
<to, VBD> (support:0.11%, confidence:84.21%),
and <the, more, the, JJ> (support:0.19%, confi-
dence:0.93%) 7; Similarly, we also give some exam-
ple LSPs mined from correct sentences: <NN, VBZ>
(support:2.29%, confidence:75.23%), and <have,
VBN, since> (support:0.11%, confidence:85.71%)
8. However, other features are abstract and it is hard
to derive some intuitive knowledge from the opaque
statistical values of these features.
As shown in Table 2, our technique achieves
the highest accuracy, e.g. 81.75% on the Japanese
dataset, when we use all the features. However, we
also notice that the improvement is not very signif-
icant compared with using LSP feature individually
(e.g. 79.63% on the Japanese dataset). The similar
results are observed when we combined the features
PLM, SC, FWD, and LC. This could be explained
7a + plural noun; to + past tense format; the more + the +
base form of adjective
8singular or mass noun + the 3rd person singular present
format; have + past participle format + since
by two reasons: (1) A sentence may contain sev-
eral kinds of errors. A sentence detected to be er-
roneous by one feature may also be detected by an-
other feature; and (2) Various features give conflict-
ing results. The two aspects suggest the directions
of our future efforts to improve the performance of
our models.
Comparing with Other Methods. It is difficult
to find benchmark methods to compare with our
technique because, as discussed in Section 2, exist-
ing methods often require error tagged corpora or
parallel corpora, or focus on a specific type of er-
rors. In this paper, we compare our technique with
the grammar checker of Microsoft Word03 and the
ALEK (Chodorow and Leacock, 2000) method used
by ETS. ALEK is used to detect inappropriate usage
of specific vocabulary words. Note that we do not
consider spelling errors. Due to space limitation, we
only report the precision, recall, F-score
for erroneous sentences, and the overall accuracy.
As can be seen from Table 3, our method out-
performs the other two methods in terms of over-
all accuracy, F-score, and recall, while the three
methods achieve comparable precision. We realize
that the grammar checker of Word is a general tool
and the performance of ALEK (Chodorow and Lea-
cock, 2000) can be improved if larger training data is
used. We found that Word and ALEK usually cannot
find sentence structure and lexical collocation errors,
e.g., ?The more you listen to English, the easy it be-
comes.? contains the discovered LSP <the, more, the,
JJ>? Error.
Cross-domain Results. To study the performance
of our method on cross-domain data from writers
of the same first-language background, we collected
two datasets from Japanese writers, one is composed
of 694 parallel sentences (+:347, -:347), and the
other 1,671 non-parallel sentences (+:795, -:876).
The two datasets are used as test data while we use
JC dataset for training. Note that the test sentences
come from different domains from the JC data. The
results are given in the first two rows of Table 4. This
experiment shows that our leaning model trained for
one domain can be effectively applied to indepen-
dent data in the other domains from the writes of the
same first-language background, no matter whether
the test data is parallel or not. We also noticed that
86
Dataset Feature A (-)F (-)R (-)P (+)F (+)R (+)P
JC
LSP 79.63 80.65 85.56 76.29 78.49 73.79 83.85
LC 69.55 71.72 77.87 66.47 67.02 61.36 73.82
PLM 61.60 55.46 50.81 64.91 62 70.28 58.43
SC 53.66 57.29 68.40 56.12 34.18 39.04 32.22
FWD 68.01 72.82 86.37 62.95 61.14 49.94 78.82
LC + PLM + SC + FWD 71.64 73.52 79.38 68.46 69.48 64.03 75.94
LSP + LC + PLM + SC + FWD 81.75 81.60 81.46 81.74 81.90 82.04 81.76
CC
LSP 78.19 76.40 70.64 83.20 79.71 85.72 74.50
LC 63.82 62.36 60.12 64.77 65.17 67.49 63.01
PLM 55.46 64.41 80.72 53.61 40.41 30.22 61.30
SC 50.52 62.58 87.31 50.64 13.75 14.33 13.22
FWD 61.36 60.80 60.70 60.90 61.90 61.99 61.80
LC + PLM + SC + FWD 67.69 67.62 67.51 67.77 67.74 67.87 67.64
LSP + LC + PLM + SC + FWD 79.81 78.33 72.76 84.84 81.10 86.92 76.02
Table 2: The Experimental Results (A: overall accuracy; (-): erroneous sentences; (+): correct sentences; F:
F-score; R: recall; P: precision)
Dataset Model A (-)F (-)R (-)P
JC
Ours 81.39 81.25 81.24 81.28
Word 58.87 33.67 21.03 84.73
ALEK 54.69 20.33 11.67 78.95
CC
Ours 79.14 77.81 73.17 83.09
Word 58.47 32.02 19.81 84.22
ALEK 55.21 22.83 13.42 76.36
Table 3: The Comparison Results
LSPs play dominating role in achieving the results.
Due to space limitation, no details are reported.
To further see the performance of our method
on data written by writers with different first-
language backgrounds, we conducted two experi-
ments. (1) We merge the JC dataset and CC dataset.
The 10-fold cross-validation results on the merged
dataset are given in the third row of Table 4. The
results demonstrate that our models work well when
the training data and test data contain sentences from
different first-language backgrounds. (2) We use the
JC dataset (resp. CC dataset) for training while the
CC dataset (resp. JC dataset) is used as test data. As
shown in the fourth (resp. fifth) row of Table 4, the
results are worse than their corresponding results of
Word given in Table 3. The reason is that the mis-
takes made by Japanese and Chinese are different,
thus the learning model trained on one data does not
work well on the other data. Note that our method is
not designed to work in this scenario.
Application to Machine Translation Evaluation.
Our learning models could be used to evaluate the
MT results as an complementary measure. This is
based on the assumption that if the MT results can
be accurately distinguished from human references
Dataset A (-)F (-)R (-)P
JC(Train)+nonparallel(Test) 72.49 68.55 57.51 84.84
JC(Train)+parallel(Test) 71.33 69.53 65.42 74.18
JC + CC 79.98 79.72 79.24 80.23
JC(Train)+ CC(Test) 55.62 41.71 31.32 62.40
CC(Train)+ JC(Test) 57.57 23.64 16.94 39.11
Table 4: The Cross-domain Results of our Method
by our technique, the MT results are not natural and
may contain errors as well.
The experiment was conducted using 10-fold
cross validation on two LDC data, low-ranked and
high-ranked data9. The results using SVM as classi-
fication model are given in Table 5. As expected, the
classification accuracy on low-ranked data is higher
than that on high-ranked data since low-ranked MT
results are more different from human references
than high-ranked MT results. We also found that
LSPs are the most effective features. In addition, our
discovered LSPs could indicate the common errors
made by the MT systems and provide some sugges-
tions for improving machine translation results.
As a summary, the mined LSPs are indeed effec-
tive for the classification models and our proposed
technique is effective.
5 Conclusions and Future Work
This paper proposed a new approach to identifying
erroneous/correct sentences. Empirical evaluating
using diverse data demonstrated the effectiveness of
9One LDC data contains 14,604 low ranked (score 1-3) ma-
chine translations and the corresponding human references; the
other LDC data contains 808 high ranked (score 3-5) machine
translations and the corresponding human references
87
Data Feature A (-)F (-)R (-)P (+)F (+)R (+)P
Low-ranked data (1-3 score) LSP 84.20 83.95 82.19 85.82 84.44 86.25 82.73
LSP+LC+PLM+SC+FWD 86.60 86.84 88.96 84.83 86.35 84.27 88.56
High-ranked data (3-5 score) LSP 71.74 73.01 79.56 67.59 70.23 64.47 77.40
LSP+LC+PLM+SC+FWD 72.87 73.68 68.95 69.20 71.92 67.22 77.60
Table 5: The Results on Machine Translation Data
our techniques. Moreover, we proposed to mine
LSPs as the input of classification models from a set
of data containing correct and erroneous sentences.
The LSPs were shown to be much more effective than
the other linguistic features although the other fea-
tures were also beneficial.
We will investigate the following problems in the
future: (1) to make use of the discovered LSPs to pro-
vide detailed feedback for ESL learners, e.g. the er-
rors in a sentence and suggested corrections; (2) to
integrate the features effectively to achieve better re-
sults; (3) to further investigate the application of our
techniques for MT evaluation.
References
Rakesh Agrawal and Ramakrishnan Srikant. 1995. Mining se-
quential patterns. In ICDE.
Emily M. Bender, Dan Flickinger, Stephan Oepen, Annemarie
Walsh, and Timothy Baldwin. 2004. Arboretum: Using a
precision grammar for grammmar checking in call. In Proc.
InSTIL/ICALL Symposium on Computer Assisted Learning.
Chris Brockett, William Dolan, and Michael Gamon. 2006.
Correcting esl errors using phrasal smt techniques. In ACL.
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Martin
Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998.
Automated scoring using a hybrid feature identification tech-
nique. In Proc. ACL.
Martin Chodorow and Claudia Leacock. 2000. An unsuper-
vised method for detecting grammatical errors. In NAACL.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. ACL.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proc. ACL.
P.W. Foltz, D. Laham, and T.K. Landauer. 1999. Automated
essay scoring: Application to educational technology. In Ed-
Media ?99.
Michael Gamon, Anthony Aue, and Martine Smets. 2005.
Sentence-level mt evaluation without reference translations:
Beyond language modeling. In Proc. EAMT.
Shicun Gui and Huizhong Yang. 2003. Zhongguo Xuexizhe
Yingyu Yuliaohu. (Chinese Learner English Corpus). Shang-
hai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).
George E. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert Dale,
Hermann Moisi and Harold Somers (ed.). Marcel Dekker.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Sup-
nithi, and Hitoshi Isahara. 2003. Automatic error detection
in the japanese learners? english spoken data. In Proc. ACL.
Nitin Jindal and Bing Liu. 2006. Identifying comparative sen-
tences in text documents. In SIGIR.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In Proc. ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization.
Yajuan Lu? and Ming Zhou. 2004. Collocation translation ac-
quisition using monolingual corpora. In Proc. ACL.
Lisa N. Michaud, Kathleen F. McCoy, and Christopher A. Pen-
nington. 2000. An intelligent tutoring system for deaf learn-
ers of written english. In Proc. 4th International ACM Con-
ference on Assistive Technologies.
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and Naoki Isu.
2006. A feedback-augmented method for detecting errors in
the writing of learners of english. In Proc. ACL.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Du-
rand. 1999. Cross-language information retrieval based on
parallel texts and automatic mining of parallel texts from the
web. In SIGIR, pages 74?81.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, and Helen Pinto.
2001. Prefixspan: Mining sequential patterns efficiently by
prefix-projected pattern growth. In Proc. ICDE.
Yongmei Shi and Lina Zhou. 2005. Error detection using lin-
guistic features. In HLT/EMNLP.
Andreas Stolcke. 2002. Srilm-an extensible language modeling
toolkit. In Proc. ICSLP.
Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and Ming
Zhou. 2007. Mining sequential patterns and tree patterns to
detect erroneous sentences. In AAAI.
Tono Yukio, T. Kaneko, H. Isahara, T. Saiga, and E. Izumi.
2001. The standard speaking test corpus: A 1 million-word
spoken corpus of japanese learners of english and its impli-
cations for l2 lexicography. In ASIALEX: Asian Bilingualism
and the Dictionary.
88
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 870?878,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Mining Bilingual Data from the Web with Adaptively Learnt Patterns 
 
 
Long Jiang1, Shiquan Yang2, Ming Zhou1, Xiaohua Liu1, Qingsheng Zhu2 
1Microsoft Research Asia 
Beijing, 100190, P.R.China 
2Chongqing University,  
Chongqing, 400044, P.R.China 
{longj,mingzhou,xiaoliu}@microsoft.com shiquany@gmail.com,qszhu@cqu.edu.cn 
 
  
 
Abstract 
 
Mining bilingual data (including bilingual sen-
tences and terms1) from the Web can benefit 
many NLP applications, such as machine 
translation and cross language information re-
trieval. In this paper, based on the observation 
that bilingual data in many web pages appear 
collectively following similar patterns, an 
adaptive pattern-based bilingual data mining 
method is proposed. Specifically, given a web 
page, the method contains four steps: 1) pre-
processing: parse the web page into a DOM 
tree and segment the inner text of each node 
into snippets; 2) seed mining: identify poten-
tial translation pairs (seeds) using a word 
based alignment model which takes both trans-
lation and transliteration into consideration; 3) 
pattern learning: learn generalized patterns 
with the identified seeds; 4) pattern based min-
ing: extract all bilingual data in the page using 
the learned patterns. Our experiments on Chi-
nese web pages produced more than 7.5 mil-
lion pairs of bilingual sentences and more than 
5 million pairs of bilingual terms, both with 
over 80% accuracy. 
1 Introduction 
Bilingual data (including bilingual sentences and 
bilingual terms) are critical resources for build-
ing many applications, such as machine transla-
tion (Brown, 1993) and cross language informa-
tion retrieval (Nie et al, 1999). However, most 
existing bilingual data sets are (i) not adequate 
for their intended uses, (ii) not up-to-date, (iii) 
apply only to limited domains. Because it?s very 
hard and expensive to create a large scale bilin-
                                                 
1 In this paper terms refer to proper nouns, technical terms, 
movie names, and so on. And bilingual terms/sentences 
mean terms/sentences and their translations. 
gual dataset with human effort, recently many 
researchers have turned to automatically mining 
them from the Web. 
If the content of a web page is written in two 
languages, we call the page a Bilingual Web 
Page. Many such pages exist in non-English web 
sites. Most of them have a primary language 
(usually a non-English language) and a second-
ary language (usually English). The content in 
the secondary language is often the translation of 
some primary language text in the page.  
Since bilingual web pages are very common in 
non-English web sites, mining bilingual data 
from them should be an important task. However, 
as far as we know, there is no publication availa-
ble on mining bilingual sentences directly from 
bilingual web pages. Most existing methods for 
mining bilingual sentences from the Web, such 
as (Nie et al, 1999; Resnik and Smith, 2003; Shi 
et al, 2006), try to mine parallel web documents 
within bilingual web sites first and then extract 
bilingual sentences from mined parallel docu-
ments using sentence alignment methods.  
As to mining term translations from bilingual 
web pages, Cao et al (2007) and Lin et al (2008) 
proposed two different methods to extract term 
translations based on the observation that authors 
of many bilingual web pages, especially those 
whose primary language is Chinese, Japanese or 
Korean, sometimes annotate terms with their 
English translations inside a pair of parentheses, 
like ?c1c2...cn(e1 e2 ... em)? (c1c2...cn is a primary 
language term and e1 e2 ... em is its English trans-
lation).  
Actually, in addition to the parenthesis pattern, 
there is another interesting phenomenon that in 
many bilingual web pages bilingual data appear 
collectively and follow similar surface patterns. 
Figure 1 shows an excerpt of a page which intro-
duces different kinds of dogs2. The page provides 
                                                 
2 http://www.chinapet.net 
870
a list of dog names in both English and Chinese. 
Note that those bilingual names do not follow the 
parenthesis pattern. However, most of them are 
identically formatted as: ?{Number}?{English 
name}{Chinese name}{EndOfLine}?. One ex-
ceptional pair (?1.Alaskan Malamute ????
??? ?) differs only slightly. Furthermore, 
there are also many pages containing consistently 
formatted bilingual sentences (see Figure 2). The 
page3 lists the (claimed) 200 most common oral 
sentences in English and their Chinese transla-
tions to facilitate English learning. 
 
Figure 1. Consistently formatted term translation 
pairs 
 
Figure 2. Consistently formatted sentence trans-
lation pairs 
People create such web pages for various rea-
sons. Some online stores list their products in 
two languages to make them understandable to 
foreigners. Some pages aim to help readers with 
foreign language learning. And in some pages 
where foreign names or technical terms are men-
tioned, the authors provide the translations for 
disambiguation. For easy reference, from now on 
we will call pages which contain many consis-
tently formatted translation pairs Collective Bi-
lingual Pages. 
According to our estimation, at least tens of 
millions of collective bilingual pages exist in 
Chinese web sites. Most importantly, each such 
page usually contains a large amount of bilingual 
                                                 
3 http://cul.beelink.com/20060205/2021119.shtml 
data. This shows the great potential of bilingual 
data mining. However, the mining task is not 
straightforward, for the following reasons: 
1) The patterns vary in different pages, so 
it?s impossible to mine the translation 
pairs using predefined templates; 
2) Some pages contain consistently format-
ted texts in two languages but they are not 
translation pairs; 
3) Not all translations in a collective bilin-
gual page necessarily follow an exactly 
consistent format. As shown in Figure 1, 
the ten translation pairs are supposed to 
follow the same pattern, however, due to 
typos, the pattern of the first pair is 
slightly different. 
Because of these difficulties, simply using a 
classifier to extract translation pairs from adja-
cent bilingual texts in a collective bilingual page 
may not achieve satisfactory results. Therefore in 
this paper, we propose a pattern-based approach: 
learning patterns adaptively from collective bi-
lingual pages instead of using the parenthesis 
pattern, then using the learned patterns to extract 
translation pairs from corresponding web pages. 
Specifically, our approach contains four steps: 
1) Preprocessing: parse the web page into a 
DOM tree and segment the inner text of 
each node into snippets; 
2) Seed mining: identify potential translation 
pairs (seeds) using an alignment model 
which takes both translation and translite-
ration into consideration; 
3) Pattern learning: learn generalized pat-
terns with the identified seeds; 
4) Pattern based mining: extract all bilingual 
data in the page using the learnt patterns.  
Let us take mining bilingual data from the text 
shown in Figure 1 as an example. Our method 
identifies ?Boxer??? and ?Eskimo Dog??
???? as two potential translation pairs based 
on a dictionary and a transliteration model (Step 
2 above). Then we learn a generalized pattern 
that both pairs follow as ?{BulletNumb-
er}{Punctuation}{English term}{Chinese 
term}{EndOfLine}?, (Step 3 above). Finally, we 
apply it to match in the entire text and get al 
translation pairs following the pattern (Step 4 
above). 
The remainder of this paper is organized as 
follows. In Section 2, we list some related work. 
The overview of our mining approach is pre-
sented in Section 3. In Section 4, we give de-
871
tailed introduction to each of the four modules in 
our mining approach. The experimental results 
are reported in Section 5 followed by our conclu-
sion and some future work in Section 6. 
Please note that in this paper we describe our 
method using example bilingual web pages in 
English and Chinese, however, the method can 
be applied to extract bilingual data from web 
pages written in any other pair of languages, 
such as Japanese and English, Korean and Eng-
lish etc. 
2 Related Work 
Mining Bilingual Data from the Web 
As far as we know, there is no publication avail-
able on mining parallel sentences directly from 
bilingual web pages. Most existing methods of 
mining bilingual sentences from the Web, such 
as (Nie et al, 1999; Resnik and Smith, 2003; Shi 
et al, 2006), mine parallel web documents within 
bilingual web sites first and then extract bilingual 
sentences from mined parallel documents using 
sentence alignment methods. However, since the 
number of bilingual web sites is quite small, 
these methods can not yield a large number of 
bilingual sentences. (Shi et al, 2006), mined a 
total of 1,069,423 pairs of English-Chinese paral-
lel sentences. In addition to mining from parallel 
documents, (Munteanu and Marcu, 2005) pro-
posed a method for discovering bilingual sen-
tences in comparable corpora. 
As to the term translation extraction from bi-
lingual web pages, (Cao et al, 2007) and (Lin et 
al., 2008) proposed two different methods utiliz-
ing the parenthesis pattern. The primary insight 
is that authors of many bilingual web pages, es-
pecially those whose primary language is Chi-
nese, Japanese or Korean sometimes annotate 
terms with their English translations inside a pair 
of parentheses. Their methods are tested on a 
large set of web pages and achieve promising 
results. However, since not all translations in 
bilingual web pages follow the parenthesis pat-
tern, these methods may miss a lot of translations 
appearing on the Web.  
Apart from mining term translations directly 
from bilingual web pages, more approaches have 
been proposed to mine term translations from 
text snippets returned by a web search engine 
(Jiang et al, 2007; Zhang and Vines, 2004; 
Cheng et al, 2004; Huang et al, 2005). In their 
methods the source language term is usually giv-
en and the goal is to find the target language 
translations from the Web. To obtain web pages 
containing the target translations, they submit the 
source term to the web search engine and collect 
returned snippets. Various techniques have been 
proposed to extract the target translations from 
the snippets. Though these methods achieve high 
accuracy, they are not suitable for compiling a 
large-scale bilingual dictionary for the following 
reasons: 1) they need a list of predefined source 
terms which is not easy to obtain; 2) the relev-
ance ranking in web search engines is almost 
entirely orthogonal to the intent of finding the 
bilingual web pages containing the target transla-
tion, so many desired bilingual web pages may 
never be returned; 3) most such methods rely 
heavily on the frequency of the target translation 
in the collected snippets which makes mining 
low-frequency translations difficult. 
Moreover, based on the assumption that anc-
hor texts in different languages referring to the 
same web page are possibly translations of each 
other, (Lu et al, 2004) propose a novel approach 
to construct a multilingual lexicon by making use 
of web anchor texts and their linking structure. 
However, since only famous web pages may 
have inner links from other pages in multiple 
languages, the number of translations that can be 
obtained with this method is limited. 
Pattern-based Relation Extraction 
Pattern-based relation extraction has also been 
studied for years. For instance, (Hearst, 1992; 
Finkelstein-Landau and Morin, 1999) proposed 
an iterative pattern learning method for extract-
ing semantic relationships between terms. (Brin, 
1998) proposed a method called DIPRE (Dual 
Iterative Pattern Relation Expansion) to extract a 
relation of books (author, title) pairs from the 
Web. Since translation can be regarded as a kind 
of relation, those ideas can be leveraged for ex-
tracting translation pairs. 
3 Overview of the Proposed Approach 
Web 
pages
Seed mining
Pattern-based 
mining
Pattern 
learning
Preprocessing
Bilingual 
dictionary
input
output
depend
Translation 
pairs
Transliteration 
model depend
 
Figure 3. The framework of our approach 
872
As illustrated in Figure 3, our mining system 
consists of four main steps: preprocessing, seed 
mining, pattern learning and pattern based min-
ing. The input is a set of web documents and the 
output is mined bilingual data. 
In the preprocessing step, the input web doc-
uments are parsed into DOM trees and the inner 
text of each tree node is segment into snippets. 
Then we select those tree nodes whose inner 
texts are likely to contain translation pairs collec-
tively with a simple rule. 
The seed mining module receives the inner 
text of each selected tree node and uses a word-
based alignment model to identify potential 
translation pairs. The alignment model can han-
dle both translation and transliteration in a uni-
fied framework.  
The pattern learning module receives identi-
fied potential translation pairs from the seed min-
ing as input, and then extracts generalized pattern 
candidates with the PAT tree algorithm. Then a 
SVM classifier is trained to select good patterns 
from all extracted pattern candidates. 
In the pattern-based mining step, the selected 
patterns were used to match within the whole 
inner text to extract all translation pairs follow-
ing the patterns. 
4 Adaptive Pattern-based Bilingual Da-
ta Mining 
In this section, we will present the details about 
the four steps in the proposed approach. 
4.1 Preprocessing 
HTML Page Parsing 
The Document Object Model (DOM) is an appli-
cation programming interface used for parsing 
HTML documents. With DOM, an HTML doc-
ument is parsed into a tree structure, where each 
node belongs to some predefined types (e.g. DIV, 
TABLE, TEXT, COMMENT, etc.). We removed 
nodes with types of ?B?, ?FONT?, ?I? and so on, 
because they are mainly used for controlling vis-
ual effect. After removal, their child nodes will 
be directly connected to their parents. 
Text Segmentation 
After an HTML document is parsed, the inner 
text of each node in the DOM tree will be seg-
mented into a list of text snippets according to 
their languages. That means each snippet will be 
labeled as either an English snippet (E) or a Chi-
nese snippet (C).  
The text segmentation was performed based 
on the Unicode values of characters 4  first and 
then guided by the following rules to decide the 
boundary of a snippet under some special situa-
tions: 
1) Open punctuations (such as ?(?) are pad-
ded into next snippet, and close punctua-
tions (such as ?)?) are padded into pre-
vious snippet; other punctuations (such as 
?;?) are padded into previous snippet; 
2) English snippets which contains only 1 or 
2 ASCII letters are merged with previous 
and next Chinese snippets (if exist). Since 
sometimes Chinese sentences or terms al-
so contain some abbreviations in English. 
Table 1 gives some examples of how the inner 
texts are segmented. 
Inner text 
China Development Bank (??) ?
????? 
Segmentation 
China Development Bank |(?? ) 
?????? 
Inner text Windows XP ?????? XP? 
Segmentation Windows XP |?????? XP? 
Table 1. Example segmentations (?|? indicates the 
separator between adjacent snippets) 
Since a node?s inner text includes all inner 
texts of its children, the segmentation to all texts 
of a DOM tree has to be performed from the leaf 
nodes up to the root in order to avoid repetitive 
work. When segmenting a node?s inner text, we 
first segment the texts immediately dominated by 
this node and then combine those results with its 
children?s segmented inner texts in sequence. 
As a result of the segmentation, the inner text 
of every node will look like ??ECECC 5EC??. 
Two adjacent snippets in different languages (in-
dicated as ?EC? or ?CE?) are considered a Bilin-
gual Snippet Pair (BSP). 
Collective Nodes Selection 
Since our goal is to mine bilingual knowledge 
from collective bilingual pages, we have to de-
cide if a page is really a collective bilingual page. 
In this paper, the criterion is that a collective 
page must contain at least one Collective Node 
which is defined as a node whose inner text con-
tains no fewer than 10 non-overlapping bilingual 
snippet pairs and which contains less than 10 
                                                 
4 For languages with the same character zone, other tech-
niques are needed to segment the text. 
5 Adjacent snippets in the same language only appear in the 
inner texts of some non-leaf nodes. 
873
percent of other snippets which do not belong to 
any bilingual snippet pairs. 
4.2 Seed Mining 
The input of this module is a collective node 
whose inner text has been segmented into conti-
nuous text snippets, such 
as ?EkChEk+1Ch+1Ch+2?. In this step, every ad-
jacent snippet pair in different languages will be 
checked by an alignment model to see if it is a 
potential translation pair. The alignment model 
combines a translation and a transliteration mod-
el to compute the likelihood of a bilingual snip-
pet pair being a translation pair. If it is, we call 
the snippet pair as a Translation Snippet Pair 
(TSP). If both of two adjacent pairs, e.g. EkCh 
and ChEk+1, are considered as TSPs, the one with 
lower translation score will be regarded as a 
NON-TSP. 
Before computing the likelihood of a bilingual 
snippet pair being a TSP, we preprocess it via the 
following steps: 
a) Isolating the English and Chinese con-
tents from their contexts in the bilingual 
snippet pair. Here, we use a very simple 
rule: in the English snippet, we regard all 
characters within (and including) the first 
and the last English letter in the snippet as 
the English content; similarly, in the Chi-
nese snippet we regard all characters 
within (and including) the first and the 
last Chinese character in the snippet as 
the Chinese content; 
b) Word segmentation of the Chinese con-
tent. Here, the Forward Maximum Match-
ing algorithm (Chen and Liu, 1992) based 
on a dictionary is adopted; 
c) Stop words filtering. We compiled a 
small list of stop words manually (for ex-
ample, ?of?, ?to?, ???, etc.) and remove 
them from the English and Chinese con-
tent; 
d) Stemming of the English content. We use 
an in-house stemming tool to get the un-
inflected form of all English words. 
After preprocessing, all English words form a 
collection E={e1,e2,?,em } and all Chinese 
words constitute a collection C={c1,c2,?,cn}, 
where ei is an English word, and ci is a Chinese 
word. We then use a linking algorithm which 
takes both translation and transliteration into 
consideration to link words across the two col-
lections. 
In our linking algorithm, there are three situa-
tions in which two words will be linked. The first 
is that the two words are considered translations 
of each other by the translation dictionary. The 
second is that the pronunciation similarity of the 
two words is above a certain threshold so that 
one can be considered the transliteration of the 
other. The third is that the two words are identic-
al (this rule is especially designed for linking 
numbers or English abbreviations in Chinese 
snippets). The dictionary is an in-house dictio-
nary and the transliteration model is adapted 
from (Jiang et al, 2007).  
After the linking, a translation score over the 
English and Chinese content is computed by cal-
culating the percentage of words which can be 
linked in the two collections. For some pairs, 
there are many conflicting links, for example, 
some words have multiple senses in the dictio-
nary. Then we select the one with highest trans-
lation score.  
For example, given the bilingual snippet pair 
of ?Little Smoky River? and ???????, its 
English part is separated as ?Little/Smoky/River?, 
and its Chinese part is separated as ??/?/?/?/
??. According to the dictionary, ?Little? can be 
linked with ???, and ?River? can be linked with 
???. However, ?Smoky? is translated as ???
?? in the dictionary which does not match any 
Chinese characters in the Chinese snippet. How-
ever the transliteration score (pronunciation simi-
larity) between ?Smoky? (IPA: s.m.o.k.i) and 
??/?/?? (Pinyin: si mo ji) is higher than the 
threshold, so the English word ?Smoky? can be 
linked to three Chinese characters ???, ??? and 
???. The result is a translation score of 1.0 for 
the pair ?Little Smoky River? and ???????. 
4.3 Pattern Learning 
The pattern learning module is critical for mining 
bilingual data from collective pages, because 
many translation pairs whose translation scores 
are not high enough may still be extracted by 
pattern based mining methods. 
In previous modules, the inner texts of all 
nodes are segmented into continuous text snip-
pets, and translation snippet pairs (TSP) are iden-
tified in all bilingual snippet pairs. Next, in the 
pattern learning module, those translation snippet 
pairs are used to find candidate patterns and then 
a SVM classifier is built to select the most useful 
patterns shared by most translation pairs in the 
whole text. 
874
Candidate Pattern Extraction 
First, as in the seed mining module, we isolate 
the English and Chinese contents from their con-
texts in a TSP and then replace the contents with 
two placeholders ?[E]? and ?[C]? respectively. 
Second, we merge the two snippets of a TSP 
into a string and add a starting tag ?[#]? and an 
ending tag ?[#]? to its start and end. Following 
(Chang and Lui, 2001), all processed strings are 
used to build a PAT tree, and we then extract all 
substrings containing ?E? and ?C? as pattern 
candidates from the PAT tree. However, pattern 
candidates which start or end with ?[E]? (or 
?[C]?) will be removed, since they cannot speci-
fy unambiguous boundaries when being matched 
in a string.  
Web page authors commonly commit format-
ting errors when authoring the content into an 
html page, as shown in Figure 1. There, the ten 
bilingual terms should have been written in the 
same pattern, however, because of the mistaken 
use of ?.? instead of ???, the first translation 
pair follows a slightly different pattern. Some 
other typical errors may include varying length 
or types of white space, adjacent punctuation 
marks instead of one punctuation mark, and so 
on. To make the patterns robust enough to handle 
such variation, we generalized all pattern candi-
dates through the following two steps: 
1) Replace characters in a pattern with their 
classes. We define three classes of cha-
racters: Punctuation (P), Number (N), and 
White Space (S). Table 2 lists the three 
classes and the corresponding regular ex-
pressions in Microsoft .Net Framework6. 
2) Merge identical adjacent classes. 
Class Corresponding regular expression 
P [\p{P}] 
N [\d] 
S [\s] 
Table 2. Character classes 
For example, from the translation snippet pair 
of ?7. Don?t worry.? and ??????, we will 
learn the following pattern candidates: 
? ?#[N][P][S][E][P][S][C][P]#?; 
?  ?[N][P][S][E][P][S][C][P]#?; 
?  ?[N][P][S][E][P][S][C][P]?; 
? ? 
?  ?[S][E][P][S][C][P]?; 
                                                 
6 In System.Text.RegularExpressions namespace 
Pattern Selection 
After all pattern candidates are extracted, a SVM 
classifier is used to select the good ones: 
??? xwxfw ???? ,)(  
where, x?  is the feature vector of a pattern 
candidate pi, and w?  is the vector of weights. 
????,  stands for an inner product. f is the decision 
function to decide which candidates are good. 
In this SVM model, each pattern candidate pi 
has the following four features: 
1) Generality: the percentage of those bi-
lingual snippet pairs which can match pi 
in all bilingual snippet pairs. This feature 
measures if the pattern is a common pat-
tern shared by many bilingual snippet 
pairs; 
2) Average translation score: the average 
translation score of all bilingual snippet 
pairs which can match pi. This feature 
helps decide if those pairs sharing the 
same pattern are really translations; 
3) Length: the length of pi. In general, long-
er patterns are more specific and can pro-
duce more accurate translations, however, 
they are likely to produce fewer matches; 
4) Irregularity: the standard deviation of 
the numbers of noisy snippets. Here noisy 
snippets mean those snippets between any 
two adjacent translation pairs which can 
match pi. If the irregularity of a pattern is 
low, we can be confident that pairs shar-
ing this pattern have a reliably similar in-
ner relationship with each other. 
To estimate the weight vector, we extracted all 
pattern candidates from 300 bilingual web pages 
and asked 2 human annotators to label each of 
the candidates as positive or negative. The anno-
tation took each of them about 20 hours. Then 
with the labeled training examples, we use SVM 
light7 to estimate the weights. 
4.4 Pattern-based Mining 
After good patterns are selected, every two adja-
cent snippets in different languages in the inner 
text will be merged as a target string. As we 
mentioned previously, we add a starting tag ?[#]? 
and an ending tag ?[#]? to the start and end of 
every target string. Then we attempt to match 
each of the selected patterns in each of the target 
strings and extract translation pairs. If the target 
                                                 
7 http://svmlight.joachims.org/ 
875
string was matched with more than one pattern, 
the matched string with highest translation score 
will be kept. 
The matching process is actually quite simple, 
since we transform the learnt patterns into stan-
dard regular expressions and then make use of 
existing regular expression matching tools (e.g., 
Microsoft .Net Framework) to extract translation 
pairs. 
However, to make our patterns more robust, 
when transforming the selected patterns into 
standard regular expressions, we allow each cha-
racter class to match more than once. That means 
?[N]?, ?[P]? and ?[S]? will be transformed into 
?[\d]+?, ?[\p{P}]+? and ?[\s]+? respectively. And 
?[E]? and ?[C]? will be transformed into 
?[^\u4e00-\u9fa5]+? (any character except Chi-
nese character) and ?.+?, respectively. 
5 Experimental Results 
In the following subsections, first, we will report 
the results of our bilingual data mining on a large 
set of Chinese web pages and compare them with 
previous work. Second, we will report some ex-
perimental results on a manually constructed test 
data set to analyze the impact of each part of our 
method. 
5.1 Evaluation on a Large Set of Pages  
With the proposed method, we performed bilin-
gual data extraction on about 3.5 billion web 
pages crawled from Chinese web sites. Out of 
them, about 20 million were determined to con-
tain bilingual collective nodes. From the inner 
texts of those nodes, we extracted 12,610,626 
unique translation pairs. If we consider those 
pairs whose English parts contain more than 5 
words as sentence translations and all others as 
term translations, we get 7,522,803 sentence 
translations and 5,087,823 term translations. We 
evaluated the quality of these mined translations 
by sampling 200 sentence translations and 200 
term translations and presenting those to human 
judges, with a resulting precision of 83.5% for 
sentence translations and 80.5% for term transla-
tions. 
As we mentioned in Section 2, (Shi et al, 
2006) reported that in total they mined 1,069,423 
pairs of English-Chinese parallel sentences from 
bilingual web sites. However, our method yields 
about 7.5 million pairs, about seven times as 
many.  
We also re-implemented the extraction method 
using the parenthesis pattern proposed by (Lin et 
al., 2008) and were able to mine 6,538,164 bilin-
gual terms from the same web pages. A sample 
of 200 terms was submitted for human judgment, 
resulting in a precision of 78.5% which is a little 
lower than that of our original result. Further 
analysis showed that fewer than 20% of the bi-
lingual terms mined with our method overlap 
with the data mined using the re-implemented 
method proposed by (Lin et al, 2008). This indi-
cates that our method can find many translations 
which are not covered by the parenthesis pattern 
and therefore can be used together with the pa-
renthesis pattern based method to build a bilin-
gual lexicon. 
Out of the term translations we mined, we 
found many which co-occur with their source 
terms only once in the Web. We check this by 
searching in Google with a Boolean query made 
of the term and its translation and then get the 
number of pages containing the query. If one 
attempts to extract this kind of low-frequency 
translation using a search engine-based method, 
the desired bilingual page which contains the 
target translation is not likely to be returned in 
the top n results when searching with the source 
term as the query. Even if the desired page is 
returned, the translation itself may be difficult to 
extract due to its low frequency. 
5.2 Evaluation on a Human Made Test Da-
ta Set 
Besides the evaluation of our method on a huge 
set of web pages, we also carried out some expe-
riments on a human-constructed test data set. We 
randomly selected 500 collective nodes from the 
huge set of Chinese web pages and asked two 
annotators to label all bilingual data in their inner 
texts. Half of the labeled data are then used as 
the development data set and the rest as the test 
data set to evaluate our systems with different 
settings. Table 3 shows the evaluation results. 
Setting Type Recall Precision F-Score 
Without 
pattern 
Exact 52.2 75.4 61.7 
Fuzzy 56.3 79.3 65.8 
Without 
PG 
Exact 69.2 78.6 73.6 
Fuzzy 74.3 82.9 78.4 
With PG 
Exact 79.3 80.5 79.9 
Fuzzy 86.7 87.9 87.3 
Table 3. Performance of different settings 
In Table 3, ?Without pattern? means that we 
simply treat those seed pairs found by the align-
ment model as final bilingual data. ?Without PG? 
and ?With PG? mean not generalizing and gene-
ralizing the learnt patterns to class based form, 
876
respectively. Evaluation type ?Exact? means the 
mined bilingual data are considered correct only 
if they are exactly same as the data labeled by 
human, while ?Fuzzy? means the mined bilin-
gual data are considered correct if they contain 
the data labeled by the human. 
As shown in Table 3, the system without pat-
tern-based extraction yields only 52.2% recall. 
However, after adding pattern-based extraction, 
recall is improved sharply, to 69.2% for ?With-
out PG? and to 79.3% for ?With PG?. Most of 
the improvement comes from those translations 
which have very low translation scores and 
therefore are discarded by the seed mining mod-
ule, however, most of them are found with the 
help of the learnt patterns. 
From Table 3, we can also see that the system 
?With PG? outperforms ?Without PG? in terms 
of both precision and recall. The reason may be 
that web writers often make mistakes when writ-
ing on web pages, such as punctuation misuse, 
punctuation loss, and extra spaces etc., so ex-
tracting with a strict surface pattern will often 
miss those translations which follow slightly dif-
ferent patterns.  
To find out the reasons why some non-
translation pairs are extracted, we checked 20 
pairs which are not translations but extracted by 
the system. Out of them, 5 are caused by wrong 
segmentations. For example, ????????
????? Double Concerto for Violin and 
Cello D??????? Symphony No.2 in D 
Major? is segmented into ??????????
????, ?Double Concerto for Violin and Cello 
D?, ?????????, and ?Symphony No.2 in 
D Major?. However, the ending letter ?D? of the 
second segment should have been padded into 
the third segment. For 9 pairs, the Chinese parts 
are explanative texts of corresponding English 
texts, but not translations. Because they contain 
the translations of the key words in the English 
text, our seed mining module failed to identify 
them as non-translation pairs. For 3 pairs, they 
follow the same pattern with some genuine trans-
lation pairs and therefore were extracted by the 
pattern based mining module. However, they are 
not translation pairs. For the other 3 pairs, the 
errors came from the pattern generalization. 
To evaluate the contribution of each feature 
used in the pattern selection module, we elimi-
nated one feature at a time in turn from the fea-
ture set to see how the performance changed in 
the absence of any single feature. The results are 
reported below. 
Eliminated feature F-Score (Exact) 
Null 79.9 
Generality 72.3 
Avg. translation score 74.3 
Length 77.5 
Irregularity 76.6 
Table 4. Contribution of every feature 
From the table above, we can see that every 
feature contributes to the final performance and 
that Generality is the most useful feature among 
all four features. 
6 Conclusions  
Bilingual web pages have shown great potential 
as a source of up-to-date bilingual 
terms/sentences which cover many domains and 
application types. Based on the observation that 
many web pages contain bilingual data collec-
tions which follow a mostly consistent but possi-
bly somewhat variable pattern, we propose a uni-
fied approach for mining bilingual sentences and 
terms from such pages. Our approach can adap-
tively learn translation patterns according to dif-
ferent formatting styles in various web pages and 
then use the learnt patterns to extract more bilin-
gual data. The patterns are generalized to minim-
ize the impact of format variation and typos. Ac-
cording to experimental results on a large set of 
web pages as well as on a manually made test 
data set, our method is quite promising. 
In the future, we would like to integrate the 
text segmentation module with the seed mining 
and pattern learning module to improve the accu-
racy of text segmentation. We also want to eva-
luate the usefulness of our mined data for ma-
chine translation or other applications. 
 
 
References  
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra and 
R. L. Mercer. 1993. The mathematics of statistical 
machine translation: parameter estimation. Compu-
tational Linguistics, 19:2, 263-311. 
Sergey Brin. 1998. Extracting patterns and relations 
from the World Wide Web. In Proc. of the 1998 In-
ternational Workshop on the Web and Databases. 
Pp: 172-183. 
G.H. Cao, J.F. Gao and J.Y. Nie. 2007. A system to 
mine large-scale bilingual dictionaries from mono-
lingual web pages. MT summit. Pp: 57-64. 
877
Chia-Hui Chang and Shao-Chen Lui. 2001. IEPAD: 
Inform extract based on pattern discovery. In Proc. 
of the 10th ACM WWW conference.  
Keh-Jiann Chen, Shing-Huan Liu. 1992. Word Identi-
fication for Mandarin Chinese Sentences. In the 
Proceedings of COLING 1992. Pp:101-107. 
Cheng, P., Teng, J., Chen, R., Wang, J., Lu, W., and 
Cheng, L. 2004. Translating Unknown Queries 
with Web Corpora for Cross-Language Information 
Retrieval. In the Proceedings of SIGIR 2004, pp 
162-169.  
Michal Finkelstein-Landau, Emmanuel Morin. 1999. 
Extracting Semantic Relationships between Terms: 
Supervised vs. Unsupervised Methods. In Proceed-
ings of International Workshop on Ontological En-
gineering on the Global Information Infrastructure. 
Pp:71-80. 
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In the Proceed-
ings of COLING-92. Pp: 539-545. 
Huang, F., Zhang, Y., and Vogel, S. 2005. Mining 
Key phrase Translations from Web Corpora. In the 
Proceedings of HLT-EMNLP. 
L. Jiang, M. Zhou, L.-F. Chien, C. Niu. 2007. Named 
Entity Translation with Web Mining and Translite-
ration, Proceedings of the 20th IJCAI. Pp: 1629-
1634. 
D. Lin, S. Zhao, B. Durme and M. Pasca. 2008. Min-
ing Parenthetical Translations from the Web by 
Word Alignment. In ACL-08. pp 994-1002. 
Lu, W. and Lee, H. 2004. Anchor text mining for 
translation of Web queries: A transitive translation 
approach. ACM transactions on Information Sys-
tems, Vol.22, April 2004, pages 242-269.  
D. S. Munteanu, D. Marcu. Improving Machine 
Translation Performance by Exploiting Non-
Parallel Corpora. 2005. Computational Linguistics. 
31(4). Pp: 477-504. 
J-Y Nie, M. Simard, P. Isabelle, and R. Durand. 1999. 
Cross-Language Information Retrieval Based on 
Parallel Texts and Automatic Mining of parallel 
Text from the Web. In SIGIR 1999. Pp: 74-81. 
Philip Resnik, Noah A. Smith. 2003. The Web as a 
Parallel Corpus. Computational Linguistics. 29(3). 
Pp: 349-380. 
Li Shao and Hwee Tou Ng. 2004. Mining new word 
translations from comparable corpora. In Proc. of 
COLING 2004. Pp: 618?624. 
Lei Shi, Cheng Niu, Ming Zhou, Jianfeng Gao. 2006. 
A DOM Tree Alignment Model for Mining Paral-
lel Data from the Web. In ACL 2006. 
Jung H. Shin, Young S. Han and Key-Sun Choi. 1996. 
Bilingual knowledge acquisition from Korean-
English parallel corpus using alignment method: 
Korean-English alignment at word and phrase level. 
In Proceedings of the 16th conference on Computa-
tional linguistics, Copenhagen, Denmark. 
J.C. Wu, T. Lin and J.S. Chang. 2005. Learning 
Source-Target Surface Patterns for Web-based 
Terminology Translation. ACL Interactive Poster 
and Demonstration Sessions,. Pp 37-40, Ann Arbor. 
Zhang, Y. and Vines, P.. 2004. Using the Web for 
Automated Translation Extraction in Cross-
Language Information Retrieval. In the Proceed-
ings of SIGIR 2004. Pp: 162-169. 
878
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 698?706,
Beijing, August 2010
Semantic Role Labeling for News Tweets 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 3Zhongyang Xiong and 2Changning Huang 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
zyxiong@cqu.edu.cn 
v-cnh@microsoft.com 
 
Abstract 
News tweets that report what is happen-
ing have become an important real-time 
information source. We raise the prob-
lem of Semantic Role Labeling (SRL) 
for news tweets, which is meaningful for 
fine grained information extraction and 
retrieval. We present a self-supervised 
learning approach to train a domain spe-
cific SRL system to resolve the problem. 
A large volume of training data is auto-
matically labeled, by leveraging the ex-
isting SRL system on news domain and 
content similarity between news and 
news tweets. On a human annotated test 
set, our system achieves  state-of-the-art 
performance, outperforming the SRL 
system trained on news. 
1 Introduction 
Tweets are text messages up to 140 characters. 
Every day, more than 50 million tweets are gen-
erated by millions of Twitter users. According to 
the investigation by Pear Analytics (2009), about 
4% tweets are related to news1. 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
1 http://blog.twitter.com/2010/02/measuring-tweets.html 
We divide news related tweets into two cate-
gories: those excerpted from news articles and 
those not. The former kind of tweets, hereafter 
called news excerpt, is formally written while 
the latter, hereafter called news tweet, varies in 
style and often is not grammatically correct. To 
understand the proportion of news tweets, we 
randomly selected 1000 tweets related to news, 
and got 865 news tweets. Following is an exam-
ple of anews tweet, containing oh, yea, which 
usually appear in spoken language, and :-(, an 
emoticon. 
oh yea and Chile earthquake the earth off it's 
axis according to NASA and shorten the day 
by a wee second :-(                                     (S1) 
News tweets arean important information 
source because they keep reporting what is hap-
pening in real time. For example, the earthquake 
near Los Angeles that happened on Tuesday, 
July 29, 2008 was first reported through news 
tweets only seconds later than the outbreak of 
the quake. Official news did not emerge about 
this event until four minutes later. By then, 
"Earthquake" was trending on Twitter Search 
with thousands of updates2. 
However, it is a daunting task for people to 
find out information they are interested in from 
such a huge number of news tweets, thus moti-
vating us to conduct some kind of information 
                                                 
2 http://blog.twitter.com/2008/07/twitter-as-news-wire.html 
698
extraction such as event mining, where SRL 
plays a crucial  role (Surdeanu et al, 2003). 
Considering Sentence 1, suppose the agent 
earthquake and the patient day for the predicate 
shorten are identified. Then it is straightforward 
to output the event Chile earthquake shorten the 
day, which captures the essential information 
encoded in this tweet. 
Following M?rquez (2009), we define SRL 
for news tweets as the task of identifying the 
arguments of a given verb as predicate in a news 
tweet and assigning them semantic labels de-
scribing the roles they play for the predicate. To 
make our method applicable to general infor-
mation extraction tasks,  rather than only to 
some special scenarios such as arresting event 
extraction, we adopt general semantic roles, i.e., 
Agent(A0), Patient(A1), Location(AM-LOC), 
Temporal(AM-TMP),etc., instead of situation-
specific roles (Fillmore et al, 2004) such as 
Suspect, Authorities, and Offense in an arrest 
frame.  
Our first attempt is to directly apply the state-
of-art SRL system (Meza-Ruiz and Riedel, 2009) 
that trained on the CoNLL 08 shared task da-
taset(Surdeanu et al, 2008), hereafter called 
SRL-BS, to news tweets. Not surprisingly, we 
observe its F1 score drops sharply from 75.5% 
on news corpus to 43.3% on our human annotat-
ed news tweets, owing much to the informal 
written style of news tweets. 
Therefore, we have to build a domain specific 
SRL system for news tweets. Given the diversi-
fied styles of news tweets, building such a sys-
tem requires a larger number of annotated news 
tweets, which are not available, and are not af-
fordable for human labeling. We propose a novel 
method to automatically annotate news tweets, 
which leverages the existing resources of SRL 
for news domain, and content similarity between 
news and news tweets. We argue that the same 
event is likely to be reported by both news and 
news tweets, which results in  content similarity 
between the news and news tweet. Further, we 
argue that the news and news tweets reporting 
the same event tend to have similar predicate-
argument structures. We tested our assumptions 
on the event Chile earthquake that happened on 
Match 2nd, 2010. We got 261 news and 722 news 
tweets published on the same day that described 
this event.  Sentence 2 and 3 are two examples 
of the news excerpts and Sentence 1 is one ex-
ample of news tweets for this event.   
Chile Earthquake Shortened Earth Day    (S2) 
Chile Earthquake Shortened Day              (S3) 
Obviously Sentence 1, 2 and 3 all have predi-
FDWH ?shortened? with the same A0 and A1 ar-
guments. Our manually checking showed that in 
average each news tweet in those 993 samples 
had 2.4 news excerpts that had the same predi-
cate-argument structures.  
Our news tweet annotation approach consists 
of four steps. First, we submit hot queries to 
Twitter and for each query we obtain a list of 
tweets. Second, for each list of tweets, we single 
out news excerpts using heuristic rules and re-
move them from the list, conduct SRL on news 
excerpts using SRL-BS, and cluster them in 
terms of the similarity in content and predicate-
argument structures. Third, for each list of 
tweets, we try to merge every remaining tweet 
into one news excerpt cluster according to its 
content similarity to the cluster. Those that can 
be put into one news group are regarded as news 
tweet. Finally, semantic structures of news ex-
cerpts are passed to the news tweet in the same 
group through word alignment. 
Our domain specific SRL system is then 
trained on automatically constructed training 
data using the Conditional Random Field (CRF: 
Lafferty et al, 2001) learning framework. Our 
system is evaluated on a human labeled dataset, 
and achieves state-of-the-art performance, out-
performing the baseline SRL-BS.  
Our contributions can be summarized as fol-
lows: 
1) We propose to conduct SRL for news 
tweets for fine grained information ex-
traction and retrieval;  
2) We present a semi-supervised learning 
approach to train a domain specific SRL 
system for news tweets, which outper-
forms SRL-BS and achieves the state-of-
the-art performance on a human labeled 
dataset. 
The rest of this paper is organized as follows: 
In the next section, we review related work.  In 
Section 3 we detail key components of our ap-
proach. In Section 4, we setup experiments and 
evaluate the effectiveness of our method.  Final-
699
ly, Section 5 concludes and presents the future 
work. 
2 Related Work 
Our related work falls into two categories: SRL 
on news and domain adaption. 
As for SRL on news, most researchers used 
the pipelined approach, i.e., dividing the task 
into several phases such as argument identifica-
tion, argument classification, global inference, 
etc.,  and conquering them individually (Xue and 
Palmer, 2004; Koomen et al, 2005; Cohn and 
Blunsom, 2005; Punyakanok et al, 2008; 
Toutanova et al, 2005; Toutanova et al, 2008). 
Exceptions to the pipelined approach exist.  
M?rquez et al (2005) sequentially labeled the 
words according to their positions relative to an 
argument (i.e., inside, outside or at the beginning 
of it). Carreras et al (2004) and Surdeanu et al 
(2007) jointly labeled all the predicates. Vickrey 
and Koller(2008) simplified the input sentence 
by hand-written and machine learnt rules before 
conducting SRL. Some other approaches simul-
taneously resolved all the sub-tasks by integrat-
ing syntactic parsing and SRL into a single mod-
el (Musillo and Merlo, 2006; Merlo and Musillo, 
2008), or by using Markov Logic Networks 
(MLN, Richardson and Domingos, 2005) as the 
learning framework (Riedel and Meza-Ruiz, 
2008; Meza-Ruiz and Riedel, 2009). 
All the above approaches focus on sentences 
from news articles or other formal documents, 
and depend on human annotated corpus for 
training. To our knowledge, little study has been 
carried out on SRL for news tweets.  
As for domain adaption, some researchers re-
garded the out-of-GRPDLQ GDWD DV ?SULRU
NQRZOHGJH?DQGestimated the model parameters 
by maximizing the posterior under this prior dis-
tribution, and successfully applied their ap-
proach to language modeling (Bacchiani and 
Roark, 2003) and parsing (Roark and Bacchiani, 
2003). Daum? III and Marcu (2006) presented a 
QRYHO IUDPHZRUN E\ GHILQLQJ D ?JHQHUDO Go-
PDLQ?EHWZHHQWKH?WUXO\LQ-GRPDLQ?DQG?WUXO\
out-of-GRPDLQ?   
Unlike existing domain adaption approaches, 
our method is about adapting SRL system on 
news domain to the news tweets domain, two 
domains that differ in writing style but are linked 
through content similarity. 
3 Our Method 
Our method of SRL for news tweets is to train a 
domain specific SRL on automatically annotated 
training data as briefed in Section 1.  
In this section we present details of the five 
crucial components of our method, i.e., news 
excerpt identification, news excerpt clustering, 
news tweets identification, semantic structure 
mapping, and the domain specific SRL system 
constructing. 
3.1 News Excerpt Identification 
We use one heuristic rule to decide whether or 
not a tweet is news excerpt:  if a tweet has a link 
to a news article and its text content is included 
by the news article, it is news excerpt, otherwise 
not. 
Given a tweet, to apply this rule, we first ex-
tract the content link and expand it, if any, into 
the full link with the unshorten service3. This 
step is necessary because content link in tweet is 
usually shortened to reduce the total amount of 
characters. Next, we check if the full link points 
to any of the pre-defined news sites, which, in 
our experiments, are 57 English news websites. 
If yes, we download the web page and check if it 
exactly contains the text content of the input 
tweet. Figure 1 illustrates this process.  
Figure 1. An illustration of news excerpt identi-
fication. 
To test the precision of this approach, while 
preparing for the training data for the experi-
ments, we checked 100 tweets that were identi-
fied as news excerpt by this rule to find out they 
all are excerpted from news. 
                                                 
3 http://unshort.me 
700
3.2 News Excerpt Clustering 
Given as input a list of news excerpts concerning 
the same query and published in the same time 
scope, this component uses the hierarchical ag-
glomerative clustering algorithm (Manning et 
al., 2008) to divide news excerpts into groups in 
terms of the similarity in content and predicate-
argument structures.  
Before clustering, for every news excerpt, we 
remove the content link and other metadata such 
as author, retweet marks (starting with RT @), 
reply marks (starting with @ immediately after 
the author), hash tags (starting with #), etc., and 
keep only the text content; then it is further 
parsed into tokens, POS tags, chunks and syntac-
tic tree using the OpenNLP toolkit4.  After that,  
SRL is conducted with SRL-BS to get predicate-
argument structures. Finally, every news excerpt 
is represented as frequency a vector of terms, 
including tokens, POS tagger, chunks, predicate-
argument structures, etc. A news cluster is re-
garded as a ?macro? news excerpt and is also 
represented as a term frequency vector, i.e., the 
sum of all the term vectors in the cluster.  Noisy 
terms, such as numbers and predefined stop 
words are excluded from the frequency vector. 
To reduce data sparseness, words are stemmed 
by Porter stemmer (Martin F. Porter, 1980). 
The cosine similarity is used to measure the 
relevance between two clusters, as defined in 
Formula 1.  
   ,
'
'
'
CVCV
CVCV
CCCS
u
?               (1) 
Where C, &? denote two clusters, CV, CV? de-
note  the term frequency vectors of C and  &? 
respectively, and CS(C, &?) stands for the  co-
sine similarity between C and  &?. 
Initially, one news excerpt forms one cluster.  
Then the clustering process repeats merging the 
two most similar clusters into one till the simi-
larity between any pair of clusters is below a 
threshold, which is experimentally set to 0.7 in 
our experiments. 
During the training data preparation process, 
we randomly selected 100 clusters, each with 3.2 
pieces of news in average. For every pair of 
news excerpts in the same cluster, we checked if 
                                                 
4 http://opennlp.sourceforge.net/ 
they shared similar contents and semantic struc-
tures, and found out that 91.1% were the cases. 
3.3 News Tweets Identification 
After news excerpts are identified and removed 
from the list, every remaining tweet is checked if 
it is a news tweet. Here we group news excerpts 
and news tweets together in two steps because 1) 
news excerpts count for only a small proportion 
of all the tweets in the list, making our two-step 
clustering algorithm more efficient; and 2) one-
step clustering tends to output meaningless clus-
ters that include no news tweets. 
Intuitively, news tweet, more often than not, 
have news counterparts that report similar con-
tents. Thus we use the following rule to identify 
news tweets: if the content similarity between 
the tweet and any news excerpt cluster is greater 
than a threshold, which is experimentally set to 
0.7 in our experiments, the tweet is a news tweet, 
otherwise it is not. Furthermore, each news 
tweet is merged into the cluster with most simi-
lar content. Finally, we re-label any news tweet 
as news excerpt, which is then process by SRL-
BS, if its content similarity to the cluster exceeds 
a threshold, which is experimentally set to 0.9 in 
our experiments. 
Again, the cosine similarity is used to meas-
ure the content similarity between tweet and 
news excerpt cluster. Each tweet is repressed as 
a term frequency vector. Before extracting terms 
from tweet, tweet metadata is removed and a 
rule-based normalization process is conducted to 
restore abnormal strLQJVVD\?	DSRV?LQWRWKHLU
KXPDQ IULHQGO\ IRUP VD\ ?? ? 1H[W VWHPPLQJ
tools and OpenNLP are applied to get lemmas, 
POS tags, chunks, etc., and noisy terms are fil-
tered.  
We evaluated the performance of this ap-
proach when preparing for the training data. We 
randomly sampled 500 tweets that were identi-
fied as news tweets, to find that 93.8% were true 
news tweets. 
3.4 Semantic Structure Mapping 
Semantic structure mapping is formed as the 
task of word alignment from news excerpt to 
news tweet. A HMM alignment model is trained 
with GIZA++ (Franz and Hermann, 2000) on all 
(news excerpt, news tweet) pairs in the same 
cluster. After word alignment is done, semantic 
701
information attached to a word in a news excerpt 
is passed to the corresponding word in the news 
tweet as illustrated in Figure 2. 
 
Chile Earthquake Shortened Earth Day
A0 predicate A1
NASA and shorten the day by a wee second :-(
oh yea and Chile earthquake the earth off it's axis according to
 
Figure 2. An example of mapping semantic 
structures from news excerpts to news tweets. 
In Figure 2, shorten, earthquake and day in 
two sentences are aligned, respectively; and two 
predicate-argument structures in the first sen-
tence, i.e., (shortened, earthquake, A0), (short-
ened, day, A1), are passed to the second. 
News tweets may receive no semantic infor-
mation from related news excerpts after mapping, 
because of word alignment errors or no news 
excerpt in the cluster with similar semantic 
structures.  Such tweets are dropped. 
Mapping may also introduce cases that violate 
the following two structural constraints in SRL 
(Meza-Ruiz and Riedel, 2009): 1) one (predi-
cate, argument) pair has only one role label in 
one sentence; and 2) for each predicate, each of 
the proper arguments (A0~A5) can occur at most 
once. Those conflicts are largely owing to the 
noisy outputs of SRL trained on news and to the 
alignment errors. While preparing for the train-
ing data for our experiments, we found 38.9% of 
news tweets had such conflicts.  
A majority voting schema and the structural 
constrains are used to resolve the conflicts as 
described below.   
1) Step 1, for every cluster, each (predicate, 
argument, role) is weighted according to 
its frequency in the cluster; 
2) Step 2, for every cluster, detect conflicts 
using the structural constrains; if no con-
flicts exist, stop; otherwise go to Step 3;   
3) Step 3, for every cluster, keep the one 
with higher weight in each conflicting 
(predicate, argument, role) pair; if the 
weights are equal,  drop both; 
Here is an example to show the conflicting 
resolution process.  Consider the cluster includ-
ing Sentence 1, 2 and 3, where (shorten, earth-
quake, A0), (shorten, earthquake, A1), (shorten, 
axis, A0), and (shorten, day, A1) occur 6, 4, 1 
and 3 times, respectively.  This cluster includes 
three conflicting pairs:   
1) (shorten, earthquake, A0) vs. (shorten, 
earthquake, A1); 
2) (shorten, earthquake, A1) vs. (shorten, 
day, A1); 
3) (shorten, earthquake, A0) vs. (shorten, ax-
is, A0); 
The first pair is first resolved, causing (short-
en, earthquake, A0) to be kept and (shorten, 
earthquake, A1) removed, which leads to the 
second pair being resolved as well; then we pro-
cess the third pair resulting in (shorten, earth-
quake, A0) being kept and (shorten, axis, A0) 
dropped; finally (shorten, earthquake, A0) and 
(shorten, day, A1) stay in the cluster. 
The conflicting resolution algorithm is sensi-
tive to the order of conflict resolution in Step 3. 
Still consider the three conflicting pairs listed 
above. If the second pair is first processed, only 
(shorten, earthquake, A0) will be left. Our strat-
egy is to first handle the conflict resolving which 
leads to most conflicts resolved. 
We tested the performance of this semantic 
structure mapping strategy while preparing for 
the training data. We randomly selected 56 news 
tweets with conflicts and manually annotated 
them with SRL. After the conflict resolution 
method was done, we observed that 38 news 
tweets were resolved correctly, 9 resolved but 
incorrectly, and 9 remain unresolved, suggesting 
the high precision of this method, which fits our 
task.  We leave it to our future work to study 
more advanced approach for semantic structure 
mapping. 
3.5 SRL System for News Tweets 
Following M?rquez et al (2005), we regard SRL 
for tweets as a sequential labeling task, because 
of its joint inference ability and its openness to 
support other languages. 
We adopt conventional features for each token 
defined in M?rquez et al(2005),  such as the 
lemma/POS tag of the current/previous/next to-
ken, the lemma of predicate and its combination 
with the lemma/POS tag of the current token, the 
voice of the predicate (active/passive), the dis-
tance between the current token and the predi-
cate, the relative position of the current token to 
702
the predicate, and so on. We do not use features 
related to syntactic parsing trees, to allow our 
system not to rely on any syntactic parser, whose 
performance depends on style and language of 
text, which limits the generality of our system. 
Before extracting features, we perform a pre-
processing step to remove tweet metadata and 
normalize tweet text content, as described in 
Section 3.3. The OpenNLP toolkit is used for 
feature extraction, and the CRF++ toolkit 5  is 
used to train the model. 
4 Experiments 
In this section, we evaluate our SRL system on a 
gold-standard dataset consisting of 1,110 human 
annotated news tweets and show that our system 
achieves the state-of-the-art performance com-
pared with SRL-BS that is trained on news. Fur-
thermore, we study the contribution of automati-
cally generated training data. 
4.1 Evaluation Metric 
We adopt the widely used precision (Pre.), recall 
(Rec.) and F-score (F., the harmonic mean of 
precision and recall) as evaluation metrics.  
4.2 Baseline System 
We use SRL-BS as our baseline because of its 
state-of-art performance on news domain, and its 
readiness to use as well. 
4.3 Data Preparation 
We restrict to English news tweets to test our 
method. Our method can label news tweets of 
other languages, given that the related tools such 
as the SRL system on news domain, the word 
alignment tool, OpenNLP, etc., can support oth-
er languages.  
We build two corpora for our experiments: 
one is the training dataset of 10,000 news tweets 
with semantic roles automatically labeled; the 
other is the gold-standard dataset of 1,110 news 
tweets with semantic roles manually labeled. 
Training Dataset 
We randomly sample 80 queries from 300 
English queries extracted from the top stories of 
Bing news, Google news and Twitter trending 
topics from March 1, 2010 to March 4, 2010.  
                                                 
5 http://crfpp.sourceforge.net/ 
Submitting the 80 queries to Twitter search, 
we retrieve and download 512,000 tweets, from 
which we got 4,785 news excerpts and 11,427 
news tweets, which were automatically annotat-
ed using the method described in Section 3.   
Furthermore, 10,000 tweets are randomly se-
lected from the automatically annotated news 
tweets, forming the training dataset, while the 
other 1,427 news tweets are used to construct the 
gold-standard dataset. 
Gold-standard Dataset 
We ask two people to annotate the 1,427 news 
tweets, following the Annotation guidelines for 
PropBank6 with one exception: for phrasal ar-
guments, only the head word is labeled as the 
argument, because our system and SRL-BS con-
duct word level SRL. 
317 news tweets are dropped because of in-
consistent annotation, and the remaining 1,110 
news tweets form the gold-standard dataset.  
Quality of Training dataset 
Since the news tweets in the gold-standard da-
taset are randomly sampled from the automati-
cally labeled corpus and are labeled by both hu-
man and machine, we use them to estimate the 
quality of training data, i.e., to which degree the 
automatically generated results are similar to 
humans?.   
We find that our method achieves 75.6% F1 
score, much higher than the baseline, suggesting 
the relatively high quality of the training data. 
4.4 Result and Analysis 
Table 1 reports the experimental results of our 
system (SRL-TS) and the baseline on the gold-
standard dataset. 
 
 Precision Recall F-Score 
SRL-BS 36.0 % 54.5% 43.3% 
SRL-TS 78.0% 57.1% 66.0% 
Table 1. Performances of our system and the 
baseline on the gold-standard dataset. 
As shown in Table 1, our system performs 
much better than the baseline on the gold-
standard dataset in terms of all metrics. We ob-
serve two types of errors that are often made by 
                                                 
6 http://verbs.colorado.edu/~mpalmer/projects/ace/PB 
guidelines.pdf 
703
SRL-BS but not so often by our system, which 
largely explains the difference in performance.  
The first type of errors, which accounts for 
25.3% of the total errors made by SRL-BS, is 
caused by the informal written style, such as el-
lipsis, of news tweets. For instance, for the ex-
ample Sentence 1 listed in Section 1, the SRL-
BS incorrectly identify earth as the A0 argument 
of the predicate shorten. The other type of errors, 
which accounts for 10.2% of the total errors 
made by SRL-BS, is related to the discretionary 
combination of news snippets. For example, 
consider the following news tweet: 
The Chile earthquake shifted the earth's axis, 
"shortened the length of an Earth day by 1.26 
miliseconds".                                              (S4) 
We analyze the errors made by our system 
and find that 12.5% errors are attributed to the 
complex syntactic structures, suggesting that 
combining our system with systems on news 
domain is a promising direction. For example, 
our system cannot identify the A0 argument of 
the predicate shortened, because of its blindness 
of attributive clause; in contrast, SRL-BS works 
on this case.  
wow..the earthquake that caused the 2004 In-
dian Ocean tsunami shortened the day by al-
most 3 microseconds..what does that even 
mean?! HOW?                                           (S5) 
We also find that 32.3% of the errors made by 
our system are more or less related to the train-
ing data, which has noise and cannot fully repre-
sent the knowledge of SRL on news tweets. For 
instance, our system fails to label the following 
sentence, partially because the predicate strike 
does not occur in the training set. 
8.8-Magnitude-Earthquake-Strikes-Chile (S6) 
We further study how the size of automatical-
O\ODEHOHGWUDLQLQJGDWDDIIHFWVRXUV\VWHP?VSHr-
formance, as illustrated in Figure 3. We conduct 
two sets of experiments: in the first set, the train-
ing data is automatically labeled and the testing 
data is the gold-standard dataset; in the second 
set, half of the news tweets from the gold-
standard dataset are added to the training data, 
the remaining half forms the testing dataset. 
Curve 1 and 2 represent the experimental results 
of set 1 and 2, respectively. 
From Curve 1, we see that RXUV\VWHP?VSHr-
formance increases sharply when the training 
data size varies from 5,000 to 6,000; then in-
creases relatively slowly with more training data; 
and finally reaches the highest when all training 
data is used.  Curve 2 reveals a similar trend. 
 
 
Figure 3. Performance on training data of vary-
ing size. 
This phenomenon is largely due to the com-
peting between two forces: the noise in the train-
ing data, and the knowledge of SRL encoded in 
the training data.  
Interestingly, from Figure 3, we observe that 
the contribution of human labeled data is no 
longer significant after 6,000 automatically la-
beled training data is used, reaffirming the effec-
tiveness of the training data. 
5 Conclusions and Future Work 
We propose to conduct SRL on news tweets for 
fine grained information extraction and retrieval. 
We present a self-supervised learning approach 
to train a domain specific SRL system for news 
tweets. Leveraging the SRL system on news 
domain and content similarity between news and 
news tweets, our approach automatically labels a 
large volume of training data by mapping SRL-
BS generated results of news excerpts to news 
tweets. Experimental results show that our sys-
tem outperforms the baseline and achieves the 
state-of-the-art performance.  
In the future, we plan to enlarge training data 
size and test our system on a larger dataset; we 
also plan to further boost the performance of our 
system by incorporating tweets specific features 
such as hash tags, reply/re-tweet marks into our 
704
CRF model, and by combining our system with 
SRL systems trained on news.  
 
References 
Bacchiani, Michiel and Brian Roark. 2003. Unsuper-
vised language model adaptation. Proceedings of 
the 2003 International Conference on Acoustics, 
Speech and Signal Processing, volume 1, pages: 
224-227 
Carreras, Xavier, Llu?s M?rquez, and Grzegorz 
&KUXSD?D+LHUDUFKLFDOUHFRJQLWLRQRISURSo-
sitional arguments with Perceptrons. Proceedings 
of the Eighth Conference on Computational Natu-
ral Language Learning, pages: 106-109. 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labeling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Daum?, Hal III and Daniel Marcu. 2006. Domain 
adaptation for statistical classifiers. Journal of Ar-
tificial Intelligence Research, 26(1), 101-126. 
Fillmore, Charles J., Josef Ruppenhofer, Collin F. 
Baker. 2004. FrameNet and Representing the Link 
between Semantic and Syntactic Relations. Com-
putational Linguistics and Beyond, Institute of 
Linguistics, Academia Sinica. 
Kelly, Ryan, ed. 2009. Twitter Study Reveals Inter-
esting Results About Usage. San Antonio, Texas: 
Pear Analytics. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
Lafferty, John D., Andrew McCallum, Fernando C. 
N. Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Labeling 
Sequence Data. Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, 
pages: 282-289. 
Manning, Christopher D., Prabhakar Raghavan and 
Hinrich Schtze. 2008. Introduction to Information 
Retrieval. Cambridge University Press, Cam-
bridge, UK. 
M?rquez, Llu?s, Jesus Gim?nez Pere Comas and 
Neus Catal?. 2005. Semantic Role Labeling as Se-
quential Tagging. Proceedings of the Ninth Con-
ference on Computational Natural Language 
Learning, pages: 193-196. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses us-
ing Markov Logic. Human Language Technolo-
gies: The 2009 Annual Conference of the North 
American Chapter of the ACL, pages: 155-163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Och, Franz Josef, Hermann Ney. Improved Statistical 
Alignment Models. Proceedings of the 38th Annu-
al Meeting of the Association for Computational 
Linguistics, pages: 440-447. 
Porter, Martin F. 1980. An algorithm for suffix strip-
ping. Program, 14(3), 130-137. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and in-
ference in semantic role labeling. Journal of Com-
putational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. Collec-
tive semantic role labelling with Markov Logic. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 193-
197. 
Roark, Brian and Michiel Bacchiani. 2003. Super-
vised and unsupervised PCFG adaptation to novel 
domains. Proceedings of the 2003 Conference of 
the North American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology, volume 1, pages: 126-133. 
Surdeanu, Mihai, Sanda Harabagiu, JohnWilliams 
and Paul Aarseth. 2003. Using predicate-argument 
structures for information extraction. Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics, volume 1, pages: 8-15. 
Surdeanu, Mihai, Llu?s M?rquez, Xavier Carreras and 
Pere R. Comas. 2007. Combination strategies for 
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29(1), 105-151. 
705
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Vickrey, David and Daphne Koller. 2008. Applying 
sentence simplification to the conll-2008 shared 
task. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, pag-
es: 268-272  
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
706
Coling 2010: Poster Volume, pages 725?729,
Beijing, August 2010
Collective Semantic Role Labeling on Open News Corpus  
by Leveraging Redundancy 
 
 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 5Daniel Tse* and 3Zhongyang Xiong 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
5School of Information Technologies 
The University of Sydney 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
dtse6695@it.usyd.edu.au 
zyxiong@cqu.edu.cn 
 
  
Abstract 
We propose a novel MLN-based method 
that collectively conducts SRL on 
groups of news sentences. Our method is 
built upon a baseline SRL, which uses 
no parsers and leverages redundancy. 
We evaluate our method on a manually 
labeled news corpus and demonstrate 
that news redundancy significantly im-
proves the performance of the baseline, 
e.g., it improves the F-score from 
64.13% to 67.66%.  * 
1 Introduction 
Semantic Role Labeling (SRL, M?rquez, 2009) 
is generally understood as the task of identifying 
the arguments of a given predicate and assigning 
them semantic labels describing the roles they 
play. For example, given a sentence The luxury 
auto maker sold 1,214 cars., the goal is to iden-
tify the arguments of sold and produce the fol-
lowing output: [A0 The luxury auto maker] [V 
sold] [A1 1,214 cars]. Here A0 represents the 
seller, and A1 represents the things sold (CoNLL 
2008 shared task, Surdeanu et al, 2008). 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
Gildea and Jurafsky (2002) first tackled SRL 
as an independent task, which is divided into 
several sub-tasks such as argument identifica-
tion, argument classification, global inference, 
etc. Some researchers (Xue and Palmer, 2004; 
Koomen et al, 2005; Cohn and Blunsom, 2005; 
Punyakanok et al, 2008; Toutanova et al, 2005; 
Toutanova et al, 2008) used a pipelined ap-
proach to attack the task. Some others resolved 
the sub-tasks simultaneously. For example, some 
work (Musillo and Merlo, 2006; Merlo and Mu-
sillo, 2008) integrated syntactic parsing and SRL 
into a single model, and another (Riedel and 
Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009) 
jointly handled all sub-tasks using Markov Log-
ic Networks (MLN, Richardson and Domingos, 
2005). 
All the above methods conduct sentence level 
SRL, and rely on parsers. Parsers have showed 
great effects on SRL performance. For example, 
Xue and Palmer (2004) reported that SRL per-
formance dropped more than 10% when they 
used syntactic features from an automatic parser 
instead of the gold standard parsing trees. Even 
worse, parsers are not robust and cannot always 
analyze any input, due to the fact that some in-
puts are not in the language described by the 
parser?s formal grammar, or adequately repre-
sented within the parser?s training data. 
725
We propose a novel MLN-based method that 
collectively conducts SRL on groups of news 
sentences to leverage the content redundancy in 
news. To isolate the negative effect of noise 
from parsers and thus focus on the study of the 
contribution of redundancy to SRL, we use no 
parsers in our approach. We built a baseline SRL, 
which depends on no parsers, and use the MLN 
framework to exploit  redundancy. Our intuition 
is that SRL on one sentence can help that on 
other differently phrased sentences with similar 
meaning. For example, consider the following 
sentence from a news article: 
A suicide bomber blew himself up Sunday in 
market in Pakistan's northwest crowded with 
shoppers ahead of a Muslim holiday, killing 
12 people, including a mayor who once sup-
ported but had turned against the Taliban, of-
ficials said. 
The state-of-art MLN-based system (Meza-Ruiz 
and Riedel, 2009), hereafter referred to as 
MLNBS for brevity, incorrectly labels northwest 
instead of bomber as A0 of killing. Now consider 
another sentence from another news article: 
Police in northwestern Pakistan say that a su-
icide bomber has killed at least 13 people and 
wounded dozens of others. 
Here MLNBS correctly identify bomber as A0 
of killing. When more sentences are observed 
where bomber as A0 of killing is correctly identi-
fied, we will be more confident that bomber 
should be labeled as A0 of killing, and that 
northwest should not be the A0 of killing accord-
ing to the constraint that one predicate has at 
most one A0. 
We manually construct a news corpus to 
evaluate our method. In the corpus, semantic 
role information is annotated and sentences with 
similar meanings are grouped together. Experi-
mental results show that news redundancy can 
significantly improve the performance of the 
baseline system. 
Our contributions can be summarized as fol-
lows: 
1. We present a novel method that conducts 
SRL on a set of sentences collectively, in-
stead of on a single sentence, by extend-
ing MLNBS to leverage redundancy. 
2. We show redundancy can significantly 
improve the performance of the baseline 
system, indicating a promising research 
direction towards open SRL. 
In the next section, we introduce news sen-
tence extraction and clustering. In Section 3, we 
describe our collective inference method. In Sec-
tion 4, we show our experimental results. Finally, 
in Section 5 we conclude our paper with a dis-
cussion of future work. 
2 Extraction and Clustering of News 
Sentences 
To construct a corpus to evaluate our method, 
we extract sentences from clustered news arti-
cles returned by news search engines such as 
Bing and Google, and divide them into groups 
so that sentences in a group have similar mean-
ing. 
News articles in the same cluster are supposed 
to report the same event. Thus we first group 
sentences according to the news cluster they 
come from. Then we split sentences in the same 
cluster into several groups according to the simi-
larity of meaning. We assume that two sentences 
are more similar in meaning if they share more 
synonymous proper nouns and verbs. The syno-
nyms of verbs, like plod and trudge, are mainly 
extracted from the Microsoft Encarta Diction-
ary1, and the proper nouns thesaurus, containing 
synonyms such as U.S. and the United States, is 
manually compiled. 
As examples, below are two sentence groups 
which are extracted from a news cluster describ-
ing Hurricane Ida. 
Group 1: 
? Hurricane Ida, the first Atlantic hurri-
cane to target the U.S. this year, plod-
ded yesterday toward the Gulf Coast? 
? Hurricane Ida trudged toward the Gulf 
Coast? 
? ? 
Group 2: 
? It could make landfall as early as Tues-
day morning, although it was forecast to 
weaken further. 
                                                 
1
http://uk.encarta.msn.com/encnet/features/dictionary/dictio
naryhome.aspx 
726
? Authorities said Ida could make landfall 
as early as Tuesday morning, although 
it was forecast to weaken by then. 
? ? 
3 Collective Inference Based on MLN 
Our method includes two core components: a 
baseline system that conducts SRL on every sen-
tence; and a collective inference system that ac-
cepts as input a group of sentences with prelimi-
nary SRL information provided by the baseline. 
We build the baseline by removing formulas 
involving syntactic parsing information from 
MLNBS (while keeping other rules) and retrain-
ing the system using the tool and scripts provid-
ed by Riedel and Meza-Ruiz (2008) on the man-
ually annotated news corpus described in Sec-
tion 4. 
A collective inference system is constructed 
to leverage redundancy in the SRL information 
from the baseline.  
We first redefine the predicate role and treat it 
as observed: 
predicate role: Int x Int x Int x Role; 
role has four parameters: the first one stands for 
the number of sentence in the input, which is 
necessary to distinguish the sentences in a group; 
the other three are taken from the arguments of 
the role predicate defined by Riedel and Meza-
Ruiz (2008), which denote the positions of the 
predicate and the argument in the sentence and 
the role of the argument, respectively. If the 
predication holds, it returns 1, otherwise 0.  
A hidden predicate final_role is defined to 
present the final output, which has the same pa-
rameters as the predicate role: 
predicate final_role: Int x Int x Int x Role; 
We introduce the following formula, which 
directly passes the semantic role from the base-
line to the final output: 
role(s, p, a, +r)=> final_role (s, p, a, +r)    (1) 
Here s is the sentence number in a group; p and 
a denote the positions of the predicate and ar-
gument in s, respectively; r stands for the role of 
the argument; the ?+? before the variable r indi-
cates that different r has different weight. 
Then we define another formula for collective 
inference: 
s1?s2^lemma(s1,p1,p_lemma)^lemma(s2,p2, 
p_lemma)^lemma(s1,a1,a_lemma)^lemma(s2,
a2,a_lemma)^role(s2,p2,a2,+r)=>final_role 
(s1,p1,a1,+r)                                                 (2) 
Here p_lemma(a_lemma) stands for the lemma 
of the predicate(argument), which is obtained 
from the lemma dictionary. This dictionary is 
extracted from the dataset of CoNLL 2008 
shared task and is normalized using synonym 
dictionary described in Section 2; lemma is an 
observed predicate that states whether or not the 
word has the lemma. 
Formula 2 encodes our basic ideas about col-
lective SRL: given several sentences expressing 
similar meaning, if one sentence has a predicate 
p with an argument a of role r, the other sen-
tences would be likely to have a predicate p? 
with an argument a? of role r, where p? and a? 
are the same or synonymous with p and a, re-
spectively, as illustrated by the example in Sec-
tion 1. 
Besides, we also apply structural constraints 
(Riedel and Meza-Ruiz, 2008) to final_role. 
To learn parameters of the collective infer-
ence system, we use  thebeast (Riedel and Meza-
Ruiz, 2008),  which is an open Markov Logic 
Engine, and train it on manually annotated news 
corpus described in Section 4. 
4 Experiments 
To train and test the collective inference system, 
we extract 1000 sentences from news clusters, 
and group them into 200 clusters using the 
method described in Section 2. For every sen-
tence, POS tagging is conducted with the 
OpenNLP toolkit (Jason Baldridge et al, 2009), 
lemma of each word is obtained through the 
normalized lemma dictionary described in Sec-
tion 3, and SRL is manually labeled. To reduce 
human labeling efforts, we retrain our baseline 
on the WSJ corpus of CoNLL 2008 shared task 
and run it on our news corpus, and then edit the 
SRL outputs by hand. 
We implement the collective inference system 
with the thebeast toolkit. Precision, recall, and 
F-score are used as evaluation metrics.  In both 
training and evaluation, we follow the CoNLL 
2008 shared task and regard only heads of 
phrases as arguments. 
727
Table 1 shows the averaged 10-fold cross val-
idation results of our systems and the baseline, 
where the third and second line report the results 
of using and not using Formula 1 in our collec-
tive inference system, respectively. 
 
Systems Pre. (%) Rec. (%) F-score (%) 
Baseline 69.87 59.26 64.13 
CI-1 62.99 72.96 67.61 
CI 67.01 68.33 67.66 
Table 1. Averaged 10-fold cross validation re-
sults (Pre.: precision; Rec.: recall). 
Experimental results show that the two collec-
tive inference engines (CI-1 and CI) perform 
significantly better than the baseline in terms of 
the recall and F-score, though a little worse in 
the precision. We observe that predicate-
argument relationships in sentences with com-
plex syntax are usually not recognized by the 
baseline, but some of them are correctly identi-
fied by the collective inference systems. This, 
we guess, explains in large part the difference in 
performance. For instance, consider the follow-
ing sentences in a group, where order and tell 
are synonyms: 
? Colombia said on Sunday it will appeal 
to the U.N. Security Council and the 
OAS after Hugo Chavez, the fiery leftist 
president of neighboring Venezuela, or-
dered his army to prepare for war in or-
der to assure peace. 
? President Hugo Chavez ordered Vene-
zuela's military to prepare for a possible 
armed conflict with Colombia, saying 
yesterday that his country's soldiers 
should be ready if the U.S. tries to pro-
voke a war between the South American 
neighbors. 
? Venezuelan President Hugo Chavez told 
his military and civil militias to prepare 
for a possible war with Colombia as ten-
sions mount over an agreement giving 
U.S. troops access to Colombian mili-
tary bases. 
The baseline cannot label (ordered, Chavez, A0) 
for the first sentence, partially owing to the syn-
tactic complexity of the sentence, but can identi-
fy the relationship for the second and third sen-
tence. In contrast, the collective inference sys-
tems can identify Chavez in the first sentence as 
A0 of order because of its occurrence in the oth-
er sentences of the same group. 
As Table 1 shows, the CI system achieves the 
highest F-score (67.66%), and a higher precision 
than the CI-1 system, indicating the effective-
ness of Formula 1. Consider the above three sen-
tences. CI-1 mislabels (ordered, Venezuela, A1) 
for the first sentence because the baseline labels 
it for the second sentence. In contrast, CI does 
not label it for the first sentence because the 
baseline does not and (ordered, Venezuela, A1) 
rarely occurs in the outputs of the baseline for 
this sentence group. 
We also find cases where the collective infer-
ence systems do not but should help. For exam-
ple, consider the following group of sentences: 
? A Brazilian university expelled a woman 
who was heckled by hundreds of fellow 
students when she wore a short, pink 
dress to class, taking out newspaper ads 
Sunday to publicly accuse her of immo-
rality.  
? The university also published newspaper 
ads accusing the student, Geisy Arruda, 
of immorality. 
The baseline has identified (published, univer-
sity, A0) for the second sentence. But neither 
the baseline nor our method labels (taking, uni-
versity, A0) for the first one.  This happens be-
cause publish is not considered as a synonym 
of take, and thus (published, university, A0) in 
the second provides no evidence for (taking, 
university, A0) in the first. We plan to develop 
a context based synonym detection component 
to address this issue in the future. 
5 Conclusions and Future Work 
We present a novel MLN-based method that col-
lectively conducts SRL on groups of sentences. 
To help build training and test corpora, we de-
sign a method to collect news sentences and to 
divide them into groups so that sentences of sim-
ilar meaning fall into the same cluster. Experi-
mental results on a manually labeled news cor-
pus show that collective inference, which lever-
ages redundancy, can effectively improve the 
performance of the baseline. 
728
In the future, we plan to evaluate our method 
on larger news corpora, and to extend our meth-
od to other genres of corpora, such as tweets. 
 
References  
Baldridge, Jason, Tom Morton, and Gann. 2009. 
OpenNLP, http://opennlp.sourceforge.net/ 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labelling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Journal of Computa-
tional Linguistics, 28(3):245?288. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses 
using Markov Logic. Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the ACL, pages: 155-
163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and 
inference in semantic role labeling. Journal of 
Computational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. 
Collective semantic role labelling with Markov 
Logic. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 193-197. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
 
729
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068?1076,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
 
 
Abstract 
In this paper we develop an approach to tackle 
the problem of verb selection for learners of 
English as a second language (ESL) by using 
features from the output of Semantic Role La-
beling (SRL). Unlike existing approaches to 
verb selection that use local features such as 
n-grams, our approach exploits semantic fea-
tures which explicitly model the usage context 
of the verb. The verb choice highly depends 
on its usage context which is not consistently 
captured by local features. We then combine 
these semantic features with other local fea-
tures under the generalized perceptron learn-
ing framework. Experiments on both in-
domain and out-of-domain corpora show that 
our approach outperforms the baseline and 
achieves state-of-the-art performance.1 
1 Introduction 
Verbs in English convey actions or states of being. 
In addition, they also communicate sentiments and 
imply circumstances, e.g., in ?He got [gained] the 
scholarship after three interviews.?, the verb 
?gained? may indicate that the ?scholarship? was 
competitive and required the agent?s efforts; in 
contrast, ?got? sounds neutral and less descriptive. 
                                                          
* This work has been done while the author was visiting Mi-
crosoft Research Asia. 
Since verbs carry multiple important functions, 
misusing them can be misleading, e.g., the native 
speaker could be confused when reading ?I like 
looking [reading] books?. Unfortunately, accord-
ing to (Gui and Yang, 2002; Yi et al, 2008), more 
than 30% of the errors in the Chinese Learner Eng-
lish Corpus (CLEC) are verb choice errors. Hence, 
it is useful to develop an approach to automatically 
detect and correct verb selection errors made by 
ESL learners. 
However, verb selection is a challenging task 
because verbs often exhibit a variety of usages and 
each usage depends on a particular context, which 
can hardly be adequately described by convention-
al n-gram features. For instance, both ?made? and 
?received? can complete ?I have __ a telephone 
call.?, where the usage context can be represented 
as ?made/received a telephone call?; however, in 
?I have __ a telephone call from my boss?, the 
prepositional phrase ?from my boss? becomes a 
critical part of the context, which now cannot be 
described by n-gram features, resulting in only 
?received? being suitable. 
Some researchers (Tetreault and Chodorow, 
2008) exploited syntactic information and n-gram 
features to represent verb usage context. Yi et al 
(2008) introduced an unsupervised web-based 
proofing method for correcting verb-noun colloca-
tion errors. Brockett et al (2006) employed phrasal 
Statistical Machine Translation (SMT) techniques 
to correct countability errors. None of their meth-
ods incorporated semantic information. 
SRL-based Verb Selection for ESL 
1,2Xiaohua Liu, 3Bo Han*, 4Kuan Li*, 5Stephan Hyeonjun Stiller and 2Ming Zhou 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3Department of Computer Science and Software Engineering 
The University of Melbourne 
4College of Computer Science 
Chongqing University 
5Computer Science Department 
Stanford University 
{xiaoliu,  mingzhou, v-kuli}@microsoft.com 
 b.han@pgrad.unimelb.edu.au 
sstiller@stanford.edu 
1068
Unlike the other papers, we derive features from 
the output of an SRL (M?rquez, 2009) system to 
explicitly model verb usage context. SRL is gener-
ally understood as the task of identifying the argu-
ments of a given verb and assigning them semantic 
labels describing the roles they play. For example, 
given a sentence ?I want to watch TV tonight? and 
the target predicate ?watch?, the output of SRL 
will be something like ?I [A0] want to watch [tar-
get predicate] TV [A1] tonight [AM-TMP].?, 
meaning that the action ?watch? is conducted by 
the agent ?I?, on the patient ?TV?, and the action 
happens ?tonight?. 
We believe that SRL results are excellent fea-
tures for characterizing verb usage context for 
three reasons: (i) Intuitively, the predicate-
argument structures generated by SRL systems 
capture major relationships between a verb and its 
contextual participants and consequently largely 
determine whether or not the verb usage is proper. 
For example, in ?I want to watch a match tonight.?, 
?match? is the patient of ?watch?, and ?watch ? 
match? forms a collocation, suggesting ?watch? is 
appropriately used. (ii) Predicate-argument struc-
tures abstract away syntactic differences in sen-
tences with similar meanings, and therefore can 
potentially filter out lots of noise from the usage 
context. For example, consider ?I want to watch a 
football match on TV tonight?: if ?match? is suc-
cessfully identified as the agent of ?watch?, 
?watch ? football?, which is unrelated to the us-
age of ?watch? in this case, can be easily excluded 
from the usage context. (iii) Research on SRL has 
made great achievements, including human-
annotated training corpora and state-of-the-art sys-
tems, which can be directly leveraged. 
Taking an English sentence as input, our method 
first generates correction candidates by replacing 
each verb with verbs in its pre-defined confusion 
set; then for every candidate, it extracts SRL-
derived features; finally our method scores every 
candidate using a linear function trained by the 
generalized perceptron learning algorithm (Collins, 
2002) and selects the best candidate as output. 
Experimental results show that SRL-derived fea-
tures are effective in verb selection, but we also 
observe that noise in SRL output adversely in-
creases feature space dimensions and the number 
of false suggestions. To alleviate this issue, we use 
local features, e.g., n-gram-related features, and 
achieve state-of-the-art performance when all fea-
tures are integrated. 
Our contributions can be summarized as follows: 
1. We propose to exploit SRL-derived fea-
tures to explicitly model verb usage con-
text. 
2. We propose to use the generalized percep-
tron framework to integrate SRL-derived 
(and other) features  and achieve state-of-
the-art performance on both in-domain and 
out-of-domain test sets. 
Our paper is organized as follows: In the next 
section, we introduce related work. In Section 3, 
we describe our method. Experimental results and 
analysis on both in-domain and out-of-domain cor-
pora are presented in Section 4. Finally, we con-
clude our paper with a discussion of future work in 
Section 5. 
2 Related Work 
SRL results are used in various tasks. Moldovan et 
al. (2004) classify the semantic relations of noun 
phrases based on SRL. Ye and Baldwin (2006) 
apply semantic role?related information to verb 
sense disambiguation. Narayanan and Harabagiu 
(2004) use semantic role structures for question 
answering. Surdeanu et al (2003) employ predi-
cate-argument structures for information extrac-
tion. 
However, in the context of ESL error detection 
and correction, little study has been carried out on 
clearly exploiting semantic information. Brockett 
et al (2006) propose the use of the phrasal statisti-
cal machine translation (SMT) technique to identi-
fy and correct ESL errors. They devise several 
heuristic rules to generate synthetic data from a 
high-quality newswire corpus and then use the syn-
thetic data together with their original counterparts 
for SMT training. The SMT approach on the artifi-
cial data set achieves encouraging results for cor-
recting countability errors. Yi et al (2008) use web 
frequency counts to identify and correct determiner 
and verb-noun collocation errors. Compared with 
these methods, our approach explicitly models 
verb usage context by leveraging the SRL output. 
The SRL-based semantic features are integrated, 
along with the local features, into the generalized 
perceptron model. 
 
1069
3 Our Approach 
Our method can be regarded as a pipeline consist-
ing of three steps. Given as input an English sen-
tence written by ESL learners, the system first 
checks every verb and generates correction candi-
dates by replacing each verb with its confusion set. 
Then a feature vector that represents verb usage 
context is derived from the outputs of an SRL sys-
tem and then multiplied with the feature weight 
vector trained by the generalized perceptron. Final-
ly, the candidate with the highest score is selected 
as the output. 
3.1 Formulation 
We formulate the task as a process of generating 
and then selecting correction candidates: 
           
? ? ? ?sScores sGENs 'maxarg* ??
                     (1) 
Here 's  denotes the input sentence for proofing, 
? ?'sGEN  is the set of correction candidates, and 
? ?sScore  is the linear model trained by the percep-
tron learning algorithm, which will be discussed in 
section 3.4. 
We call every target verb in 's  a checkpoint. 
For example, ?sees? is a checkpoint in ?Jane sees 
TV every day.?. Correction candidates are generat-
ed by replacing each checkpoint with its confu-
sions. Table 1 shows a sentence with one 
checkpoint and the corresponding correction can-
didates. 
 
Input Jane sees TV every day. 
Candidates Jane watches TV every day. 
Jane looks TV every day. 
? 
Table 1. Correction candidate list. 
One state-of-the-art SRL system (Riedel and 
Meza-Ruiz, 2008) is then utilized to extract predi-
cate-argument structures for each verb in the input, 
as illustrated in Table 2. 
Semantic features are generated by combining 
the predicate with each of its arguments; e.g., 
?watches_A0_Jane?, ?sees_A0_Jane?, ?watch-
es_A1_TV? and ?sees_A1_TV? are semantic fea-
tures derived from the semantic roles listed in Ta-
ble 2. 
 
Sentence Semantic roles 
Jane sees TV every day Predicate: sees; 
A0: Jane; 
A1: TV; 
Jane watches TV every 
day 
Predicate: watches; 
A0: Jane; 
A1: TV; 
Table 2. Examples of SRL outputs. 
At the training stage, each sentence is labeled by 
the SRL system. Each correction candidate s  is 
represented as a feature vector dRs ?? )( , where 
d  is the total number of features. The feature 
weight vector is denoted as dRw?? , and ? ?sScore  
is computed as follows: 
             ? ? wssScore ???? )(                        (2) 
Finally, ? ?sScore  is applied to each candidate, 
and *s , the one with the highest score, is selected 
as the output, as shown in Table 3. 
 
 Correction candidate Score 
*s  Jane watches TV every day. 10.8 
 Jane looks TV every day. 0.8 
 Jane reads TV every day. 0.2 
 ? ? 
Table 3.  Correction candidate scoring. 
In the above framework, the basic idea is to 
generate correction candidates with the help of pre-
defined confusion sets and apply the global linear 
model to each candidate to compute the degree of 
its fitness to the usage context that is represented 
as features derived from SRL results. 
To make our idea practical, we need to solve the 
following three subtasks: (i) generating the confu-
sion set that includes possible replacements for a 
given verb; (ii) representing the context with se-
mantic features and other complementary features; 
and (iii) training the feature weight. We will de-
scribe our solutions to those subtasks in the rest of 
this section. 
1070
3.2 Generation of Verb Confusion Sets 
Verb confusion sets are used to generate correction 
candidates. Due to the great number of verbs and 
their diversified usages, manually collecting all 
verb confusions in all scenarios is prohibitively 
time-consuming. To focus on the study of the ef-
fectiveness of semantic role features, we restrict 
our research scope to correcting verb selection er-
rors made by Chinese ESL learners and select fifty 
representative verbs which are among the most 
frequent ones and account for more than 50% of 
ESL verb errors in the CLEC data set. For every 
selected verb we manually compile a confusion set 
using the following data sources: 
1. Encarta treasures. We extract all the syno-
nyms of verbs from the Microsoft Encarta Diction-
ary, and this forms the major source for our 
confusion sets. 
2. English-Chinese Dictionaries. ESL learners 
may get interference from their mother tongue (Liu 
et al, 2000). For example, some Chinese people 
mistakenly say ?see newspaper?, partially because 
the translation of ?see? co-occurs with ?newspa-
per? in Chinese. Therefore English verbs in the 
dictionary sharing more than two Chinese mean-
ings are collected. For example, ?see? and ?read? 
are in a confusion set because they share the mean-
ings of both ??? (?to see?, ?to read?) and ???? 
(?to grasp?) in Chinese. 
3. An SMT translation table. We extract para-
phrasing verb expressions from a phrasal SMT 
translation table learnt from parallel corpora (Och 
and Ney, 2004). This may help us use the implicit 
semantics of verbs that SMT can capture but a dic-
tionary cannot, such as the fact that the verb  
Note that verbs in any confusion set that we are 
not interested in are dropped, and that the verb it-
self is included in its own confusion set. We leave 
it to our future work to automatically construct 
verb confusions. 
3.3 Verb Usage Context Features 
The verb usage context1 refers to its surrounding 
text, which influences the way one understands the 
expression. Intuitively, verb usage context can take 
the form of a collocation, e.g., ?watch ? TV? in ?I 
saw [watched] TV yesterday.? ; it can also simply 
be idioms, e.g., we say ?kick one?s habit? instead 
of ?remove one?s habit?.  
We use features derived from the SRL output to 
represent verb usage context. The SRL system ac-
cepts a sentence as input and outputs all arguments 
and the semantic roles they play for every verb in 
the sentence. For instance, given the sentence ?I 
have opened an American bank account in Bos-
ton.? and the predicate ?opened?, the output of 
SRL is listed in Table 4, where A0 and A1 are two 
core roles, representing the agent and patient of an 
action, respectively, and other roles starting with 
?AM-?are adjunct roles, e.g., AM-LOC indicates 
the location of an action. Predicate-argument struc-
tures keep the key participants of a given verb 
while dropping other unrelated words from its us-
age context. For instance, in ?My teacher said Chi-
nese is not easy to learn.?, the SRL system 
recognizes that ?Chinese? is not the A1-argument 
of ?said?. So ?say _ Chinese?, which is irrelevant 
to the usage of said, is not extracted as a feature. 
The SRL system, however, may output 
erroneous predicate-argument structures, which 
negatively affect the performance of verb 
selection.  For instance,  for the sentence ?He 
hasn?t done anything but take [make] a lot of 
money?, ?lot? is incorrectly identified as the patient 
of ?take?, making it hard to select ?make? as the 
proper verb even though ?make money? forms a 
sound collocation. To tackle this issue, we use 
local textual features, namely features related to n-
gram, chunk and chunk headword, as shown in 
Table 5.  Back-off features are generated by 
replacing the word with its POS tag to alleviate 
data sparseness. 
 
                                                          
1 http://en.wikipedia.org/wiki/Context_(language_use) 
I have made[opened] an American bank account in Boston . 
[A0] 
 
[Predicate] 
 
 
 
[A1] [AM-LOC] 
 
 
Table 4. An example of SRL output. 
1071
Table 5. An example of feature set. 
3.4 Perceptron Learning 
We choose the generalized perceptron algorithm as 
our training method because of its easy implemen-
tation and its capability of incorporating various 
features. However, there are still two concerns 
about this perceptron learning approach: its inef-
fectiveness in dealing with inseparable samples 
and its ignorance of weight normalization that po-
tentially limits its ability to generalize. In section 
4.4 we show that the training error rate drops sig-
nificantly to a very low level after several rounds 
of training, suggesting that the correct candidates 
can almost be separated from others. We also ob-
serve that our method performs well on an out-of-
domain test corpus, indicating the good generaliza-
tion ability of this method. We leave it to our fu-
ture work to replace perceptron learning with other 
models like Support Vector Machines (Vapnik, 
1995). 
In Figure 1, 
is  is the ith correct sentence within 
the training data. T and N represent the number of 
training iterations and training examples, respec-
tively. )( isGEN  is the function that outputs all the 
possible corrections for the input sentence is  with 
each checkpoint substituted by one of its confu-
sions, as described in Section 3.1. We observe that 
the generated candidates sometimes contain rea-
sonable outputs for the verb selection task, which 
should be removed. For instance, in ?? reporters 
could not take [make] notes or tape the conversa-
tion?, both ?take? and ?make? are suitable verbs in 
this context. To fix this issue, we trained a trigram 
language model using SRILM (Stolcke, 2002) on 
LDC data21, and calculated the logarithms of the 
language model score for the original sentence and 
its artificial manipulations. We only kept manipu-
lations with a language model score that is t lower 
than that of the original sentence. We experimen-
tally set t = 5. 
 
Inputs: training examples is , i=1?N 
Initialization: 0?w?  
Algorithm: 
   For r= 1.. T, i= 1..N    
   Calculate 
wso isGens ???? ? )(maxarg )(
 
   If os i ?  
         )()( osww o ????? ??  
Outputs: w?  
Figure 1. The perceptron algorithm, adapted from Co-
lins (2002). 
?  in Figure 1 is the feature extraction function. 
)(o? and )( is? are vectors extracted from the out-
put and oracle, respectively. A vector field is filled 
with 1 if the corresponding feature exists, or 0 oth-
erwise; w?  is the feature weight vector, where posi-
tive elements suggest that the corresponding 
features support the hypothesis that the candidate 
is correct. 
The training process is to update w? , when the 
output differs from the oracle. For example, when 
o is ?I want to look TV? and is  is ?I want to watch 
TV?, w?  will be updated. 
We use the averaged Perceptron algorithm (Col-
lins, 2002) to alleviate overfitting on the training 
data. The averaged perceptron weight vector is 
defined as 
                 
?
??
?
TrN
riwTN ..1,..1i
,1 ???
                    (3) 
where riw ,? is the weight vector immediately af-
ter the ith sentence in the  rth iteration. 
 
                                                          
2 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?cata-
logId=LDC2005T12 
Local: trigrams 
   have_opened 
   have_opened_a 
   opened_an_American 
   PRP_VBP_opened 
   VBP_opened_DT 
   opened_DT_JJ 
Local: chunk 
   have_opened 
   opened_an_American_investment_bank 
_account 
   PRP_opened 
   opened_NN 
Semantic: SRL derived features 
   A0_I_opened 
   opened_A1_account 
   opened_AM-LOC_in 
   ... 
1072
4 Experiments 
In this section, we compare our approach with the 
SMT-based approach. Furthermore, we study the 
contribution of predicate-argument-related 
features, and the performances on verbs with 
varying distance to their arguments. 
4.1 Experiment Preparation 
The training corpus for perceptron learning was 
taken from LDC2005T12. We randomly selected 
newswires containing target verbs from the New 
York Times as the training data. We then used the 
OpenNLP package31to extract sentences from the 
newswire text and to parse them into the corre-
sponding tokens, POS tags, and chunks. The SRL 
system is built according to Riedel and Meza-Ruiz 
(2008), using the CoNLL-2008 shared task data for 
training. We assume that the newswire data is of 
high quality and free of linguistic errors, and final-
ly we gathered 20000 sentences that contain any of 
the target verbs we were focusing on.  We experi-
mentally set the number of training rounds to T = 
50. 
We constructed two sets of testing data for in-
domain and out-of-domain test purposes, respec-
tively. To construct the in-domain test data, we 
first collected all the sentences that contain any of 
the verbs we were interested in from the previous 
unused LDC dataset; then we replaced any target 
verb in our list with a verb in its confusion set; 
next, we used the language-model-based pruning 
strategy described in 3.4 to drop possibly correct 
manipulations from the test data; and finally we 
randomly sampled 5000 sentences for testing. 
To build the out-of-domain test dataset, we 
gathered 186 samples that contained errors related 
to the verbs we were interested in from English 
blogs written by Chinese and from the CLEC cor-
pus, which were then corrected by an English na-
tive speaker. Furthermore, for every error 
involving the verbs in our target list, both the verb 
and the word that determines the error are marked 
by the English native speaker. 
4.2 Baseline 
We built up a phrasal SMT system with the word 
re-ordering feature disabled, since our task only 
concerns the substitution of the target verb. To 
                                                          
3 http://opennlp.sourceforge.net/ 
construct the training corpus, we followed the idea 
in Brockett et al (2006), and applied a similar 
strategy described in section 3.4 to the SRL sys-
tem?s training data to generate aligned pairs. 
4.3 Evaluation Metric 
We employed the following metrics adapted from 
(Yi et al, 2008): revised precision (RP), recall of 
the correction (RC) and false alarm (FA). 
         
 sCheckpoint All of #
Proofings Correct of #RP ?
                      (4)      
RP reflects how many outputs are correct usag-
es. The output is regarded as a correct suggestion if 
and only if it is exactly the same as the answer. 
Paraphrasing scenarios, for example, the case that 
the output is ?take notes? and the answer is ?make 
notes?, are counted as errors. 
Errors Total of# 
Proofings Modified Correct of# RC ?
                (5) 
RC indicates how many erroneous sentences are 
corrected among all the errors. It measures the sys-
tem?s coverage of verb selection errors. 
     
sCheckpoint All of# 
sCheckpoint Modified Incorrect of# FA ?
        (6) 
FA is related to the cases where a correct verb is 
mistakenly replaced by an inappropriate one. The-
se false suggestions are likely to disturb or even 
annoy users, and thus should be avoided as much 
as possible. 
4.4 Results and Analysis 
The training error curves of perceptron learning 
with different feature sets are shown in Figure 2. 
They drop to a low error rate and then stabilize 
after a few number of training rounds, indicating 
that most of the cases are linearly separable and 
that perceptron learning is applicable to the verb 
selection task. 
We conducted feature selection by dropping fea-
tures that occur less than N times. Here N was ex-
perimentally set to 5. We observe that, after feature 
selection, some useful features such as 
?watch_A1_TV? and ?see_A1_TV? were kept, but 
some noisy features like ?Jane_A0_sees? and 
?Jane_A0_watches? were removed, suggesting the 
effectiveness of this feature selection approach. 
 
1073
 Figure 2. Training error curves of the perceptron. 
We tested the baseline and our approach on the 
in-domain and out-of-domain corpora. The results 
are shown in Table 7 and 8, respectively. 
In the in-domain test, the SMT-based approach 
has the highest false alarm rate, though its output 
with word insertions or deletions is not considered 
wrong if the substituted verb is correct. Our ap-
proach, regardless of what feature sets are used, 
outperforms the SMT-based approach in terms of 
all metrics, showing the effectiveness of percep-
tron learning for the verb selection task. Under the 
perceptron learning framework, we can see that the 
system using only SRL-related features has higher 
revised precision and recall of correction, but also 
a slightly higher false alarm rate than the system 
based on only local features. When local features 
and SRL-derived features are integrated together, 
the state-of-the-art performance is achieved with a 
5% increase in recall, and minor changes in preci-
sion and false alarm. 
In the out-of-domain test, the SMT-based ap-
proach performs much better than in the in-domain 
test, especially in terms of false alarm rate, indicat-
ing the SMT-based approach may favor short sen-
tences. However, its recall drops greatly. We ob-
serve similar performance differences between the 
systems with different feature sets under the same 
perceptron learning framework, reaffirming the 
usefulness of the SRL-based features for verb se-
lection. 
We also conducted significance test. The results 
confirm that the improvements (SRL+Local vs. 
SMT-based) are statistically significant (p-value < 
0.001) for both the open-domain and the in-domain 
experiments. 
Furthermore, we studied the performance of our 
system on verbs with varying distance to their ar-
guments on the out-of-domain test corpus. 
 
Local d<=2 2<d<=4 d>4 
RP 64.3% 60.3% 59.4% 
RC 34.6% 33.1% 28.9% 
FA 3.0% 6.3% 5.0% 
SRL d<=2 2<d<=4 d>4 
RP 65.1% 60.1% 62.1% 
RC 40.3% 34.0% 36.9% 
FA 5.0% 6.7% 6.3% 
Table 9. Performance on verbs with different distance to 
their arguments on out-of-domain test data. 
Table 9 shows that the system with only SRL-
derived features performs significantly better than 
the system with only local features on the verb 
whose usage depends on a distant argument, i.e., 
one where the number of words between the predi-
cate and the argument is larger than 4. To under-
stand the reason, consider the following sentence: 
?It's raining outside. Please wear[take] the 
black raincoat with you.? 
 SMT-based Our method 
SRL Local SRL + Local 
RP 48.4% 64.5% 62.2% 66.4% 
RC 23.5% 40.2% 32.9% 46.4% 
FA 13.3% 5.6% 4.2% 6.8% 
Table 7. In-domain test results. 
 SMT-based Our method 
SRL Local SRL + Local 
RP 50.7% 64.0% 62.6% 65.5% 
RC 13.5% 39.0% 33.3% 44.0% 
FA 6.1% 5.5% 4.0% 6.5% 
Table 8. Out-of-domain test results. 
 
1074
Intuitively, ?wear? and ?take? seem to fill the 
blank well, since they both form a collocation with 
?raincoat?; however, when ?with [AM-MNR] you? 
is considered as part of the context, ?wear? no 
longer fits it and ?take? wins. In this case, the long-
distance feature devised from AM-MNR helps se-
lect the suitable verb, while the trigram features 
cannot because they cannot represent the long dis-
tance verb usage context. 
We also find some typical cases that are beyond 
the reach of the SRL-derived features. For instance, 
consider ?Everyone doubts [suspects] that Tom is 
a spy.?. Both of the verbs can be followed by a 
clause. However, the SRL system regards ?is?, the 
predicate of the clause, as the patient, resulting in 
features like ?doubt_A1_is? and ?suspect_A1_is?, 
which capture nothing about verb usage context. 
However, if we consider the whole clause ?sus-
pect_Tom is a spy? as the patient, this could result 
in a very sparse feature that would be filtered. In 
the future, we will combine word-level and phrase-
level SRL systems to address this problem. 
Besides its incapability of handling verb selec-
tion errors involving clauses, the SRL-derived fea-
tures fail to work when verb selection depends on 
deep meanings that cannot be captured by current 
shallow predicate-argument structures. For exam-
ple, in ?He was wandering in the park, spending 
[killing] his time watching the children playing.?, 
though ?spending? and ?killing? fit the syntactic 
structure and collocation agreement, and express 
the meaning ?to allocate some time doing some-
thing?, the word ?wandering? suggests that ?kill-
ing? may be more appropriate. Current SRL 
systems cannot represent the semantic connection 
between two predicates and thus are helpless for 
this case. We argue that the performance of our 
system can be improved along with the progress of 
SRL. 
5 Conclusions and Future Work 
Verb selection is challenging because verb usage 
highly depends on the usage context, which is hard 
to capture and represent. In this paper, we propose 
to utilize the output of an SRL system to explicitly 
model verb usage context. We also propose to use 
the generalized perceptron learning framework to 
integrate SRL-derived features with other features. 
Experimental results show that our method outper-
forms the SMT-based system and achieves state-
of-the-art performance when SRL-related features 
and other local features are integrated. We also 
show that, for cases where the particular verb us-
age mainly depends on its distant arguments, a sys-
tem with only SRL-derived features performs 
much better than the system with only local fea-
tures. 
In the future, we plan to automatically construct 
confusion sets, expand our approach to more verbs 
and test our approach on a larger size of real data. 
We will try to combine the outputs of several SRL 
systems to make our system more robust. We also 
plan to further validate the effectiveness of the 
SRL-derived features under other learning methods 
like SVMs. 
Acknowledgment 
We thank the anonymous reviewers for their valu-
able comments. We also thank Changning Huang, 
Yunbo Cao, Dongdong Zhang, Henry Li and Mu 
Li for helpful discussions. 
References  
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. 
Countability and number in Japanese to English ma-
chine translation. Proc. of the 15th conference on 
Computational Linguistics, pages 32-38. 
Chris Brockett, William B. Dolan, and Michael Gamon. 
2006. Correcting ESL errors using phrasal SMT 
techniques. Proc. of the 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting on Association for Computational 
Linguistics, pages 249-256. 
Michael Collins. 2002. Discriminative training methods 
for hidden Markov models: theory and experiments 
with perceptron algorithms. Proc. of the ACL-02 
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1-8. 
Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic 
Grammar Checking for Second Language Learners ? 
the Use of Prepositions. Proc. of NoDaliDa. 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitrtiy Belen-
ko, and Lucy Vanderwende. 2008. Using Contextual 
Speller Techniques and Language Modeling for ESL 
Error Correction. Proc. of the International Joint 
Conference on Natural Language Processing. 
Shichun Gui and Huizhong Yang. 2002. Chinese Learn-
er English Corpus. Shanghai Foreign Languages Ed-
ucation Press, Shanghai, China. 
1075
Julia E. Heine. 1998. Definiteness predictions for Japa-
nese noun phrases. Proc. of the 36th Annual Meeting 
of the Association for Computational Linguistics and 
17th International Conference on Computational 
Linguistics, pages 519-525. 
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. Proc. of the 46th Annual Meeting 
on Association for Computational Linguistics, pages 
174-182. 
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun and 
Changning Huang. 2000. PENS: A Machine-aided 
English Writing System for Chinese Users. Proc. of 
the 38th Annual Meeting on Association for Compu-
tational Linguistics, pages 529-536. 
Llu?s M?rquez. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 2009.   
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel 
Antohe and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Proc. of the 
HLT-NAACL Workshop on Computational Lexical 
Semantics, pages 60-67. 
Srini Narayanan and Sanda Harabagiu. 2004. Question 
answering based on semantic structures. Proc. of the 
20th International Conference on Computational 
Linguistics, pages 693-701. 
Franz J. Och and Hermann Ney. 2004. The Alignment 
Template Approach to Statistical Machine Transla-
tion. Journal of Computational Linguistics, 30(4), 
pages 417-449. 
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective 
semantic role labelling with Markov Logic. Proc. of 
the Twelfth Conference on Computational Natural 
Language Learning, pages 193-197. 
Andreas Stolcke. 2002. SRILM -- An Extensible Lan-
guage Modeling Toolkit. Proc. of International Con-
ference on Spoken Language Processing, pages: 901-
904. 
Mihai Surdeanu, Lluis M?rquez, Xavier Carreras, and 
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence 
Research, page 105-151. 
Mihai Surdeanu, Sanda Harabagiu, John Williams, and 
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. Proc. of the 41st 
Annual Meeting on Association for Computational 
Linguistics, pages 8-15. 
Joel R. Tetreault and Martin Chodorow. 2008. The ups 
and downs of preposition error detection in ESL writ-
ing. Proc. of the 22nd international Conference on 
Computational Linguistics, pages 865-872. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Patrick Ye and Timothy Baldwin. 2006. Verb Sense 
Disambiguation Using Selectional Preferences 
Extracted with a State-of-the-art Semantic Role 
Labeler. Proc. of the Australasian Language 
Technology Workshop, pages 141-148. 
Xing Yi, Jianfeng Gao, and William B. Dolan. 2008. A 
Web-based English Proofing System for English as a 
Second Language Users. Proc. of International Joint 
Conference on Natural Language Processing, pages 
619-624. 
1076
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 151?160,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Target-dependent Twitter Sentiment Classification 
 
 
Long Jiang1   Mo Yu2   Ming Zhou1   Xiaohua Liu1   Tiejun Zhao2 
1 Microsoft Research Asia 2 School of Computer Science & Technology 
Beijing, China Harbin Institute of Technology 
 Harbin, China 
{longj,mingzhou,xiaoliu}@microsoft.com {yumo,tjzhao}@mtlab.hit.edu.cn 
 
  
  
 
 
Abstract 
Sentiment analysis on Twitter data has attract-
ed much attention recently. In this paper, we 
focus on target-dependent Twitter sentiment 
classification; namely, given a query, we clas-
sify the sentiments of the tweets as positive, 
negative or neutral according to whether they 
contain positive, negative or neutral senti-
ments about that query. Here the query serves 
as the target of the sentiments. The state-of-
the-art approaches for solving this problem 
always adopt the target-independent strategy, 
which may assign irrelevant sentiments to the 
given target. Moreover, the state-of-the-art 
approaches only take the tweet to be classified 
into consideration when classifying the senti-
ment; they ignore its context (i.e., related 
tweets). However, because tweets are usually 
short and more ambiguous, sometimes it is not 
enough to consider only the current tweet for 
sentiment classification. In this paper, we pro-
pose to improve target-dependent Twitter sen-
timent classification by 1) incorporating 
target-dependent features; and 2) taking relat-
ed tweets into consideration. According to the 
experimental results, our approach greatly im-
proves the performance of target-dependent 
sentiment classification. 
1 Introduction 
Twitter, as a micro-blogging system, allows users 
to publish tweets of up to 140 characters in length 
to tell others what they are doing, what they are 
thinking, or what is happening around them. Over 
the past few years, Twitter has become very popu-
lar. According to the latest Twitter entry in Wik-
ipedia, the number of Twitter users has climbed to 
190 million and the number of tweets published on 
Twitter every day is over 65 million1.  
As a result of the rapidly increasing number of 
tweets, mining people?s sentiments expressed in 
tweets has attracted more and more attention. In 
fact, there are already many web sites built on the 
Internet providing a Twitter sentiment search ser-
vice, such as Tweetfeel2 , Twendz3 , and Twitter 
Sentiment4. In those web sites, the user can input a 
sentiment target as a query, and search for tweets 
containing positive or negative sentiments towards 
the target. The problem needing to be addressed 
can be formally named as Target-dependent Sen-
timent Classification of Tweets; namely, given a 
query, classifying the sentiments of the tweets as 
positive, negative or neutral according to whether 
they contain positive, negative or neutral senti-
ments about that query. Here the query serves as 
the target of the sentiments. 
The state-of-the-art approaches for solving this 
problem, such as (Go et al, 20095; Barbosa and 
Feng, 2010), basically follow (Pang et al, 2002), 
who utilize machine learning based classifiers for 
the sentiment classification of texts. However, their 
classifiers actually work in a target-independent 
way: all the features used in the classifiers are in-
dependent of the target, so the sentiment is decided 
no matter what the target is. Since (Pang et al, 
2002) (or later research on sentiment classification 
                                                          
1 http://en.wikipedia.org/wiki/Twitter 
2 http://www.tweetfeel.com/ 
3 http://twendz.waggeneredstrom.com/ 
4 http://twittersentiment.appspot.com/ 
5 The algorithm used in Twitter Sentiment 
151
of product reviews) aim to classify the polarities of 
movie (or product) reviews and each movie (or 
product) review is assumed to express sentiments 
only about the target movie (or product), it is rea-
sonable for them to adopt the target-independent 
approach. However, for target-dependent sentiment 
classification of tweets, it is not suitable to exactly 
adopt that approach. Because people may mention 
multiple targets in one tweet or comment on a tar-
get in a tweet while saying many other unrelated 
things in the same tweet, target-independent ap-
proaches are likely to yield unsatisfactory results:  
1. Tweets that do not express any sentiments 
to the given target but express sentiments 
to other things will be considered as being 
opinionated about the target. For example, 
the following tweet expresses no sentiment 
to Bill Gates but is very likely to be classi-
fied as positive about Bill Gates by target-
independent approaches. 
"People everywhere love Windows & vista. 
Bill Gates" 
2. The polarities of some tweets towards the 
given target are misclassified because of 
the interference from sentiments towards 
other targets in the tweets. For example, 
the following tweet expresses a positive 
sentiment to Windows 7 and a negative 
sentiment to Vista. However, with target-
independent sentiment classification, both 
of the targets would get positive polarity. 
?Windows 7 is much better than Vista!? 
In fact, it is easy to find many such cases by 
looking at the output of Twitter Sentiment or other 
Twitter sentiment analysis web sites. Based on our 
manual evaluation of Twitter Sentiment output, 
about 40% of errors are because of this (see Sec-
tion 6.1 for more details).  
In addition, tweets are usually shorter and more 
ambiguous than other sentiment data commonly 
used for sentiment analysis, such as reviews and 
blogs. Consequently, it is more difficult to classify 
the sentiment of a tweet only based on its content. 
For instance, for the following tweet, which con-
tains only three words, it is difficult for any exist-
ing approaches to classify its sentiment correctly. 
?First game: Lakers!? 
However, relations between individual tweets 
are more common than those in other sentiment 
data. We can easily find many related tweets of a 
given tweet, such as the tweets published by the 
same person, the tweets replying to or replied by 
the given tweet, and retweets of the given tweet. 
These related tweets provide rich information 
about what the given tweet expresses and should 
definitely be taken into consideration for classify-
ing the sentiment of the given tweet. 
In this paper, we propose to improve target-
dependent sentiment classification of tweets by 
using both target-dependent and context-aware 
approaches. Specifically, the target-dependent ap-
proach refers to incorporating syntactic features 
generated using words syntactically connected 
with the given target in the tweet to decide whether 
or not the sentiment is about the given target. For 
instance, in the second example, using syntactic 
parsing, we know that ?Windows 7? is connected 
to ?better? by a copula, while ?Vista? is connected 
to ?better? by a preposition. By learning from 
training data, we can probably predict that ?Win-
dows 7? should get a positive sentiment and 
?Vista? should get a negative sentiment.  
In addition, we also propose to incorporate the 
contexts of tweets into classification, which we call 
a context-aware approach. By considering the sen-
timent labels of the related tweets, we can further 
boost the performance of the sentiment classifica-
tion, especially for very short and ambiguous 
tweets. For example, in the third example we men-
tioned above, if we find that the previous and fol-
lowing tweets published by the same person are 
both positive about the Lakers, we can confidently 
classify this tweet as positive. 
The remainder of this paper is structured as fol-
lows. In Section 2, we briefly summarize related 
work. Section 3 gives an overview of our approach. 
We explain the target-dependent and context-
aware approaches in detail in Sections 4 and 5 re-
spectively. Experimental results are reported in 
Section 6 and Section 7 concludes our work. 
2 Related Work  
In recent years, sentiment analysis (SA) has be-
come a hot topic in the NLP research community. 
A lot of papers have been published on this topic. 
152
2.1 Target-independent SA 
Specifically, Turney (2002) proposes an unsuper-
vised method for classifying product or movie re-
views as positive or negative. In this method, 
sentimental phrases are first selected from the re-
views according to predefined part-of-speech pat-
terns. Then the semantic orientation score of each 
phrase is calculated according to the mutual infor-
mation values between the phrase and two prede-
fined seed words. Finally, a review is classified 
based on the average semantic orientation of the 
sentimental phrases in the review. 
In contrast, (Pang et al, 2002) treat the senti-
ment classification of movie reviews simply as a 
special case of a topic-based text categorization 
problem and investigate three classification algo-
rithms: Naive Bayes, Maximum Entropy, and Sup-
port Vector Machines. According to the 
experimental results, machine learning based clas-
sifiers outperform the unsupervised approach, 
where the best performance is achieved by the 
SVM classifier with unigram presences as features. 
2.2 Target-dependent SA 
Besides the above mentioned work for target-
independent sentiment classification, there are also 
several approaches proposed for target-dependent 
classification, such as (Nasukawa and Yi, 2003; 
Hu and Liu, 2004; Ding and Liu, 2007). (Nasuka-
wa and Yi, 2003) adopt a rule based approach, 
where rules are created by humans for adjectives, 
verbs, nouns, and so on. Given a sentiment target 
and its context, part-of-speech tagging and de-
pendency parsing are first performed on the con-
text. Then predefined rules are matched in the 
context to determine the sentiment about the target. 
In (Hu and Liu, 2004), opinions are extracted from 
product reviews, where the features of the product 
are considered opinion targets. The sentiment 
about each target in each sentence of the review is 
determined based on the dominant orientation of 
the opinion words appearing in the sentence. 
As mentioned in Section 1, target-dependent 
sentiment classification of review sentences is 
quite different from that of tweets. In reviews, if 
any sentiment is expressed in a sentence containing 
a feature, it is very likely that the sentiment is 
about the feature. However, the assumption does 
not hold in tweets. 
2.3 SA of Tweets 
As Twitter becomes more popular, sentiment anal-
ysis on Twitter data becomes more attractive. (Go 
et al, 2009; Parikh and Movassate, 2009; Barbosa 
and Feng, 2010; Davidiv et al, 2010) all follow the 
machine learning based approach for sentiment 
classification of tweets. Specifically, (Davidiv et 
al., 2010) propose to classify tweets into multiple 
sentiment types using hashtags and smileys as la-
bels. In their approach, a supervised KNN-like 
classifier is used. In contrast, (Barbosa and Feng, 
2010) propose a two-step approach to classify the 
sentiments of tweets using SVM classifiers with 
abstract features. The training data is collected 
from the outputs of three existing Twitter senti-
ment classification web sites. As mentioned above, 
these approaches work in a target-independent way, 
and so need to be adapted for target-dependent sen-
timent classification. 
3 Approach Overview  
The problem we address in this paper is target-
dependent sentiment classification of tweets. So 
the input of our task is a collection of tweets con-
taining the target and the output is labels assigned 
to each of the tweets. Inspired by (Barbosa and 
Feng, 2010; Pang and Lee, 2004), we design a 
three-step approach in this paper:  
1. Subjectivity classification as the first step 
to decide if the tweet is subjective or neu-
tral about the target;  
2. Polarity classification as the second step to 
decide if the tweet is positive or negative 
about the target if it is classified as subjec-
tive in Step 1;  
3. Graph-based optimization as the third step 
to further boost the performance by taking 
the related tweets into consideration.  
In each of the first two steps, a binary SVM 
classifier is built to perform the classification. To 
train the classifiers, we use SVM-Light 6  with a 
linear kernel; the default setting is adopted in all 
experiments. 
                                                          
6 http://svmlight.joachims.org/ 
153
3.1 Preprocessing 
In our approach, rich feature representations are 
used to distinguish between sentiments expressed 
towards different targets. In order to generate such 
features, much NLP work has to be done before-
hand, such as tweet normalization, POS tagging, 
word stemming, and syntactic parsing.  
In our experiments, POS tagging is performed 
by the OpenNLP POS tagger7. Word stemming is 
performed by using a word stem mapping table 
consisting of about 20,000 entries. We also built a 
simple rule-based model for tweet normalization 
which can correct simple spelling errors and varia-
tions into normal form, such as ?gooood? to 
?good? and ?luve? to ?love?. For syntactic parsing 
we use a Maximum Spanning Tree dependency 
parser (McDonald et al, 2005). 
3.2 Target-independent Features 
Previous work (Barbosa and Feng, 2010; Davidiv 
et al, 2010) has discovered many effective features 
for sentiment analysis of tweets, such as emoticons, 
punctuation, prior subjectivity and polarity of a 
word. In our classifiers, most of these features are 
also used. Since these features are all generated 
without considering the target, we call them target-
independent features. In both the subjectivity clas-
sifier and polarity classifier, the same target-
independent feature set is used. Specifically, we 
use two kinds of target-independent features: 
1. Content features, including words, punctu-
ation, emoticons, and hashtags (hashtags 
are provided by the author to indicate the 
topic of the tweet). 
2. Sentiment lexicon features, indicating how 
many positive or negative words are in-
cluded in the tweet according to a prede-
fined lexicon. In our experiments, we use 
the lexicon downloaded from General In-
quirer8. 
4 Target-dependent Sentiment Classifica-
tion  
Besides target-independent features, we also incor-
porate target-dependent features in both the subjec-
                                                          
7 http://opennlp.sourceforge.net/projects.html 
8 http://www.wjh.harvard.edu/~inquirer/ 
tivity classifier and polarity classifier. We will ex-
plain them in detail below. 
4.1 Extended Targets 
It is quite common that people express their senti-
ments about a target by commenting not on the 
target itself but on some related things of the target. 
For example, one may express a sentiment about a 
company by commenting on its products or tech-
nologies. To express a sentiment about a product, 
one may choose to comment on the features or 
functionalities of the product. It is assumed that 
readers or audiences can clearly infer the sentiment 
about the target based on those sentiments about 
the related things. As shown in the tweet below, 
the author expresses a positive sentiment about 
?Microsoft? by expressing a positive sentiment 
directly about ?Microsoft technologies?. 
?I am passionate about Microsoft technologies 
especially Silverlight.? 
In this paper, we define those aforementioned 
related things as Extended Targets. Tweets ex-
pressing positive or negative sentiments towards 
the extended targets are also regarded as positive 
or negative about the target. Therefore, for target-
dependent sentiment classification of tweets, the 
first thing is identifying all extended targets in the 
input tweet collection.  
In this paper, we first regard all noun phrases, 
including the target, as extended targets for sim-
plicity. However, it would be interesting to know 
under what circumstances the sentiment towards 
the target is truly consistent with that towards its 
extended targets. For example, a sentiment about 
someone?s behavior usually means a sentiment 
about the person, while a sentiment about some-
one?s colleague usually has nothing to do with the 
person. This could be a future work direction for 
target-dependent sentiment classification. 
In addition to the noun phrases including the 
target, we further expand the extended target set 
with the following three methods:  
1. Adding mentions co-referring to the target 
as new extended targets. It is common that 
people use definite or demonstrative noun 
phrases or pronouns referring to the target 
in a tweet and express sentiments directly 
on them. For instance, in ?Oh, Jon Stewart. 
How I love you so.?, the author expresses 
154
a positive sentiment to ?you? which actual-
ly refers to ?Jon Stewart?. By using a sim-
ple co-reference resolution tool adapted 
from (Soon et al, 2001), we add all the 
mentions referring to the target into the ex-
tended target set. 
2. Identifying the top K nouns and noun 
phrases which have the strongest associa-
tion with the target. Here, we use 
Pointwise Mutual Information (PMI) to 
measure the association. 
)()(
),(log),( tpwp
twptwPMI ?
 
Where p(w,t), p(w), and p(t) are probabili-
ties of w and t co-occurring, w appearing, 
and t appearing in a tweet respectively. In 
the experiments, we estimate them on a 
tweet corpus containing 20 million tweets. 
We set K = 20 in the experiments based on 
empirical observations. 
3. Extracting head nouns of all extended tar-
gets, whose PMI values with the target are 
above some predefined threshold, as new 
extended targets. For instance, suppose we 
have found ?Microsoft Technologies? as 
the extended target, we will further add 
?technologies? into the extended target set 
if the PMI value for ?technologies? and 
?Microsoft? is above the threshold. Simi-
larly, we can find ?price? as the extended 
targets for ?iPhone? from ?the price of 
iPhone? and ?LoveGame? for ?Lady Ga-
ga? from ?LoveGame by Lady Gaga?. 
4.2 Target-dependent Features 
Target-dependent sentiment classification needs to 
distinguish the expressions describing the target 
from other expressions. In this paper, we rely on 
the syntactic parse tree to satisfy this need. Specif-
ically, for any word stem wi in a tweet which has 
one of the following relations with the given target 
T or any from the extended target set, we generate 
corresponding target-dependent features with the 
following rules:  
? wi is a transitive verb and T (or any of the 
extended target) is its object; we generate a 
feature wi _arg2. ?arg? is short for ?argu-
ment?. For example, for the target iPhone 
in ?I love iPhone?, we generate 
?love_arg2? as a feature. 
? wi is a transitive verb and T (or any of the 
extended target) is its subject; we generate 
a feature wi_arg1 similar to Rule 1. 
? wi is a intransitive verb and T (or any of the 
extended target) is its subject; we generate 
a feature wi_it_arg1. 
?  wi is an adjective or noun and T (or any of 
the extended target) is its head; we gener-
ate a feature wi_arg1. 
?  wi is an adjective or noun and it (or its 
head) is connected by a copula with T (or 
any of the extended target); we generate a 
feature wi_cp_arg1. 
? wi is an adjective or intransitive verb ap-
pearing alone as a sentence and T (or any 
of the extended target) appears in the pre-
vious sentence; we generate a feature 
wi_arg. For example, in ?John did that. 
Great!?, ?Great? appears alone as a sen-
tence, so we generate ?great_arg? for the 
target ?John?. 
? wi is an adverb, and the verb it modifies 
has T (or any of the extended target) as its 
subject; we generate a feature arg1_v_wi. 
For example, for the target iPhone in the 
tweet ?iPhone works better with the Cell-
Band?, we will generate the feature 
?arg1_v_well?. 
Moreover, if any word included in the generated 
target-dependent features is modified by a nega-
tion9, then we will add a prefix ?neg-? to it in the 
generated features. For example, for the target iPh-
one in the tweet ?iPhone does not work better with 
the CellBand?, we will generate the features 
?arg1_v_neg-well? and ?neg-work_it_arg1?. 
To overcome the sparsity of target-dependent 
features mentioned above, we design a special bi-
nary feature indicating whether or not the tweet 
contains at least one of the above target-dependent 
features. Target-dependent features are binary fea-
tures, each of which corresponds to the presence of 
the feature in the tweet. If the feature is present, the 
entry will be 1; otherwise it will be 0. 
                                                          
9 Seven negations are used in the experiments: not, no, never, 
n?t, neither, seldom, hardly. 
155
5 Graph-based Sentiment Optimization  
As we mentioned in Section 1, since tweets are 
usually shorter and more ambiguous, it would be 
useful to take their contexts into consideration 
when classifying the sentiments. In this paper, we 
regard the following three kinds of related tweets 
as context for a tweet. 
1. Retweets. Retweeting in Twitter is essen-
tially the forwarding of a previous message. 
People usually do not change the content 
of the original tweet when retweeting. So 
retweets usually have the same sentiment 
as the original tweets.  
2. Tweets containing the target and published 
by the same person. Intuitively, the tweets 
published by the same person within a 
short timeframe should have a consistent 
sentiment about the same target.  
3. Tweets replying to or replied by the tweet 
to be classified.  
Based on these three kinds of relations, we can 
construct a graph using the input tweet collection 
of a given target. As illustrated in Figure 1, each 
circle in the graph indicates a tweet. The three 
kinds of edges indicate being published by the 
same person (solid line), retweeting (dash line), 
and replying relations (round dotted line) respec-
tively. 
 
 
 
Figure 1. An example graph of tweets about a target 
 
If we consider that the sentiment of a tweet only 
depends on its content and immediate neighbors, 
we can leverage a graph-based method for senti-
ment classification of tweets. Specifically, the 
probability of a tweet belonging to a specific sen-
timent class can be computed with the following 
formula: 
??
)(
))(())(|()|(),|(
dN
dNpdNcpcpGcp ??
 
Where c is the sentiment label of a tweet which 
belongs to {positive, negative, neutral}, G is the 
tweet graph, N(d) is a specific assignment of sen-
timent labels to all immediate neighbors of the 
tweet, and ? is the content of the tweet. 
We can convert the output scores of a tweet by 
the subjectivity and polarity classifiers into proba-
bilistic form and use them to approximate p(c| ?). 
Then a relaxation labeling algorithm described in 
(Angelova and Weikum, 2006) can be used on the 
graph to iteratively estimate p(c|?,G) for all tweets. 
After the iteration ends, for any tweet in the graph, 
the sentiment label that has the maximum p(c| ?,G) 
is considered the final label. 
6 Experiments  
Because there is no annotated tweet corpus public-
ly available for evaluation of target-dependent 
Twitter sentiment classification, we have to create 
our own. Since people are most interested in sen-
timents towards celebrities, companies and prod-
ucts, we selected 5 popular queries of these kinds: 
{Obama, Google, iPad, Lakers, Lady Gaga}. For 
each of those queries, we downloaded 400 English 
tweets10 containing the query using the Twitter API.  
We manually classify each tweet as positive, 
negative or neutral towards the query with which it 
is downloaded. After removing duplicate tweets, 
we finally obtain 459 positive, 268 negative and 
1,212 neutral tweets. 
Among the tweets, 100 are labeled by two hu-
man annotators for inter-annotator study. The re-
sults show that for 86% of them, both annotators 
gave identical labels. Among the 14 tweets which 
the two annotators disagree on, only 1 case is a 
positive-negative disagreement (one annotator con-
siders it positive while the other negative), and the 
other 13 are all neutral-subjective disagreement. 
This probably indicates that it is harder for humans 
to decide if a tweet is neutral or subjective than to 
decide if it is positive or negative. 
                                                          
10 In this paper, we use sentiment classification of English 
tweets as a case study; however, our approach is applicable to 
other languages as well. 
156
6.1 Error Analysis of Twitter Sentiment Out-
put 
We first analyze the output of Twitter Sentiment 
(TS) using the five test queries. For each query, we 
randomly select 20 tweets labeled as positive or 
negative by TS. We also manually classify each 
tweet as positive, negative or neutral about the cor-
responding query. Then, we analyze those tweets 
that get different labels from TS and humans. Fi-
nally we find two major types of error: 1) Tweets 
which are totally neutral (for any target) are classi-
fied as subjective by TS; 2) sentiments in some 
tweets are classified correctly but the sentiments 
are not truly about the query. The two types take 
up about 35% and 40% of the total errors, respec-
tively.  
The second type is actually what we want to re-
solve in this paper. After further checking those 
tweets of the second type, we found that most of 
them are actually neutral for the target, which 
means that the dominant error in Twitter Sentiment 
is classifying neutral tweets as subjective. Below 
are several examples of the second type where the 
bolded words are the targets. 
 ?No debate needed, heat can't beat lakers or 
celtics? (negative by TS but positive by human) 
?why am i getting spams from weird people ask-
ing me if i want to chat with lady gaga? (positive 
by TS but neutral by human) 
?Bringing iPhone and iPad apps into cars? 
http://www.speakwithme.com/ will be out soon and 
alpha is awesome in my car.? (positive by TS but 
neutral by human) 
?Here's a great article about Monte Veronese 
cheese. It's in Italian so just put the url into Google 
translate and enjoy http://ow.ly/3oQ77? (positive 
by TS but neutral by human) 
6.2 Evaluation of Subjectivity Classification 
We conduct several experiments to evaluate sub-
jectivity classifiers using different features. In the 
experiments, we consider the positive and negative 
tweets annotated by humans as subjective tweets 
(i.e., positive instances in the SVM classifiers), 
which amount to 727 tweets. Following (Pang et 
al., 2002), we balance the evaluation data set by 
randomly selecting 727 tweets from all neutral 
tweets annotated by humans and consider them as 
objective tweets (i.e., negative instances in the 
classifiers). We perform 10-fold cross-validations 
on the selected data. Following (Go et al, 2009; 
Pang et al, 2002), we use accuracy as a metric in 
our experiments. The results are listed below. 
 
Features Accuracy (%) 
Content features 61.1 
+ Sentiment lexicon features 63.8 
+ Target-dependent features 68.2 
Re-implementation of (Bar-
bosa and Feng, 2010) 
60.3 
 
Table 1. Evaluation of subjectivity classifiers. 
 
As shown in Table 1, the classifier using only 
the content features achieves an accuracy of 61.1%. 
Adding sentiment lexicon features improves the 
accuracy to 63.8%. Finally, the best performance 
(68.2%) is achieved by combining target-
dependent features and other features (t-test: p < 
0.005). This clearly shows that target-dependent 
features do help remove many sentiments not truly 
about the target. We also re-implemented the 
method proposed in (Barbosa and Feng, 2010) for 
comparison. From Table 1, we can see that all our 
systems perform better than (Barbosa and Feng, 
2010) on our data set. One possible reason is that 
(Barbosa and Feng, 2010) use only abstract fea-
tures while our systems use more lexical features. 
To further evaluate the contribution of target ex-
tension, we compare the system using the exact 
target and all extended targets with that using only 
the exact target. We also eliminate the extended 
targets generated by each of the three target exten-
sion methods and reevaluate the performances. 
 
Target Accuracy (%) 
Exact target 65.6 
+ all extended targets 68.2 
- co-references 68.0 
- targets found by PMI 67.8 
- head nouns 67.3 
 
Table 2. Evaluation of target extension methods. 
 
As shown in Table 2, without extended targets, 
the accuracy is 65.6%, which is still higher than 
those using only target-independent features. After 
adding all extended targets, the accuracy is im-
proved significantly to 68.2% (p < 0.005), which 
suggests that target extension does help find indi-
157
rectly expressed sentiments about the target. In 
addition, all of the three methods contribute to the 
overall improvement, with the head noun method 
contributing most. However, the other two meth-
ods do not contribute significantly.  
6.3 Evaluation of Polarity Classification  
Similarly, we conduct several experiments on posi-
tive and negative tweets to compare the polarity 
classifiers with different features, where we use 
268 negative and 268 randomly selected positive 
tweets. The results are listed below. 
 
Features Accuracy (%) 
Content features 78.8 
+ Sentiment lexicon features 84.2 
+ Target-dependent features 85.6 
Re-implementation of (Bar-
bosa and Feng, 2010) 
83.9 
 
Table 3. Evaluation of polarity classifiers. 
 
From Table 3, we can see that the classifier us-
ing only the content features achieves the worst 
accuracy (78.8%). Sentiment lexicon features are 
shown to be very helpful for improving the per-
formance. Similarly, we re-implemented the meth-
od proposed by (Barbosa and Feng, 2010) in this 
experiment. The results show that our system using 
both content features and sentiment lexicon fea-
tures performs slightly better than (Barbosa and 
Feng, 2010). The reason may be same as that we 
explained above. 
Again, the classifier using all features achieves 
the best performance. Both the classifiers with all 
features and with the combination of content and 
sentiment lexicon features are significantly better 
than that with only the content features (p < 0.01). 
However, the classifier with all features does not 
significantly outperform that using the combina-
tion of content and sentiment lexicon features. We 
also note that the improvement by target-dependent 
features here is not as large as that in subjectivity 
classification. Both of these indicate that target-
dependent features are more useful for improving 
subjectivity classification than for polarity classifi-
cation. This is consistent with our observation in 
Subsection 6.2 that most errors caused by incorrect 
target association are made in subjectivity classifi-
cation. We also note that all numbers in Table 3 
are much bigger than those in Table 1, which sug-
gests that subjectivity classification of tweets is 
more difficult than polarity classification. 
Similarly, we evaluated the contribution of tar-
get extension for polarity classification. According 
to the results, adding all extended targets improves 
the accuracy by about 1 point. However, the con-
tributions from the three individual methods are 
not statistically significant. 
6.4 Evaluation of Graph-based Optimization  
As seen in Figure 1, there are several tweets which 
are not connected with any other tweets. For these 
tweets, our graph-based optimization approach will 
have no effect. The following table shows the per-
centages of the tweets in our evaluation data set 
which have at least one related tweet according to 
various relation types.  
 
Relation type Percentage 
Published by the same person11 41.6 
Retweet 23.0 
Reply 21.0 
All 66.2 
 
Table 4. Percentages of tweets having at least one relat-
ed tweet according to various relation types. 
 
According to Table 4, for 66.2% of the tweets 
concerning the test queries, we can find at least one 
related tweet. That means our context-aware ap-
proach is potentially useful for most of the tweets. 
To evaluate the effectiveness of our context-
aware approach, we compared the systems with 
and without considering the context.  
 
System Accuracy 
F1-score (%) 
pos neu neg 
Target-dependent 
sentiment classifier 
66.0 57.5 70.1 66.1 
+Graph-based op-
timization 
68.3 63.5 71.0 68.5 
 
Table 5. Effectiveness of the context-aware approach. 
 
As shown in Table 5, the overall accuracy of the 
target-dependent classifiers over three classes is 
66.0%. The graph-based optimization improves the 
performance by over 2 points (p < 0.005), which 
clearly shows that the context information is very 
                                                          
11 We limit the time frame from one week before to one week 
after the post time of the current tweet. 
158
useful for classifying the sentiments of tweets. 
From the detailed improvement for each sentiment 
class, we find that the context-aware approach is 
especially helpful for positive and negative classes. 
 
Relation type Accuracy (%) 
Published by the same person 67.8 
Retweet 66.0 
Reply 67.0 
 
Table 6. Contribution comparison between relations. 
 
We further compared the three types of relations 
for context-aware sentiment classification; the re-
sults are reported in Table 6. Clearly, being pub-
lished by the same person is the most useful 
relation for sentiment classification, which is con-
sistent with the percentage distribution of the 
tweets over relation types; using retweet only does 
not help. One possible reason for this is that the 
retweets and their original tweets are nearly the 
same, so it is very likely that they have already got 
the same labels in previous classifications. 
7 Conclusions and Future Work 
Twitter sentiment analysis has attracted much at-
tention recently. In this paper, we address target-
dependent sentiment classification of tweets. Dif-
ferent from previous work using target-
independent classification, we propose to incorpo-
rate syntactic features to distinguish texts used for 
expressing sentiments towards different targets in a 
tweet. According to the experimental results, the 
classifiers incorporating target-dependent features 
significantly outperform the previous target-
independent classifiers.  
In addition, different from previous work using 
only information on the current tweet for sentiment 
classification, we propose to take the related tweets 
of the current tweet into consideration by utilizing 
graph-based optimization. According to the exper-
imental results, the graph-based optimization sig-
nificantly improves the performance. 
As mentioned in Section 4.1, in future we would 
like to explore the relations between a target and 
any of its extended targets. We are also interested 
in exploring relations between Twitter accounts for 
classifying the sentiments of the tweets published 
by them. 
Acknowledgments 
We would like to thank Matt Callcut for refining 
the language of this paper, and thank Yuki Arase 
and the anonymous reviewers for many valuable 
comments and helpful suggestions. We would also 
thank Furu Wei and Xiaolong Wang for their help 
with some of the experiments and the preparation 
of the camera-ready version of the paper. 
References  
Ralitsa Angelova, Gerhard Weikum. 2006. Graph-based 
text classification: learn from your neighbors. SIGIR 
2006: 485-492 
Luciano Barbosa and Junlan Feng. 2010. Robust Senti-
ment Detection on Twitter from Biased and Noisy 
Data. Coling 2010. 
Christopher Burges. 1998. A Tutorial on Support Vector 
Machines for Pattern Recognition. Data Mining and 
Knowledge Discovery, 2(2):121-167. 
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth 
Patwardhan S. 2005. Identifying sources of opinions 
with conditional random fields and extraction pat-
terns. In Proc. of the 2005 Human Language Tech-
nology Conf. and Conf. on Empirical Methods in 
Natural Language Processing (HLT/EMNLP 2005). 
pp. 355-362 
Dmitry Davidiv, Oren Tsur and Ari Rappoport. 2010. 
Enhanced Sentiment Learning Using Twitter Hash-
tags and Smileys. Coling 2010. 
Xiaowen Ding and Bing Liu. 2007. The Utility of Lin-
guistic Rules in Opinion Mining. SIGIR-2007 (poster 
paper), 23-27 July 2007, Amsterdam.  
Alec Go, Richa Bhayani, Lei Huang. 2009. Twitter Sen-
timent Classification using Distant Supervision. 
Vasileios Hatzivassiloglou and Kathleen.R. McKeown. 
2002. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th ACL and the 8th 
Conference of the European Chapter of the ACL. 
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining (KDD-2004, full paper), 
Seattle, Washington, USA, Aug 22-25, 2004. 
Thorsten Joachims. Making Large-scale Support Vector 
Machine Learning Practical. In B. Sch?olkopf, C. J. 
C. Burges, and A. J. Smola, editors, Advances in 
kernel methods: support vector learning, pages 169-
184. MIT Press, Cambridge, MA, USA, 1999. 
159
Soo-Min Kim and Eduard Hovy 2006. Extracting opi-
nions, opinion holders, and topics expressed in online 
news media text, In Proc. of ACL Workshop on Sen-
timent and Subjectivity in Text, pp.1-8, Sydney, Aus-
tralia.  
Ryan McDonald, F. Pereira, K. Ribarov, and J. Haji?c. 
2005. Non-projective dependency parsing using 
spanning tree algorithms. In Proc. HLT/EMNLP. 
Tetsuya Nasukawa, Jeonghee Yi. 2003. Sentiment anal-
ysis: capturing favorability using natural language 
processing. In Proceedings of K-CAP. 
Bo Pang, Lillian Lee. 2004. A Sentimental Education: 
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of 
ACL 2004. 
Bo Pang, Lillian Lee, Shivakumar Vaithyanathan. 2002. 
Thumbs up? Sentiment Classification using Machine 
Learning Techniques.  
Ravi Parikh and Matin Movassate. 2009. Sentiment 
Analysis of User-Generated Twitter Updates using 
Various Classification Techniques. 
Wee. M. Soon, Hwee. T. Ng, and Danial. C. Y. Lim. 
2001. A Machine Learning Approach to Coreference 
Resolution of Noun Phrases. Computational Linguis-
tics, 27(4):521?544. 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. In proceedings of ACL 2002. 
Janyce Wiebe. 2000. Learning subjective adjectives 
from corpora. In Proceedings of AAAI-2000. 
Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005. 
Recognizing Contextual Polarity in Phrase-Level 
Sentiment Analysis. In Proceedings of NAACL 2005. 
160
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 359?367,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Recognizing Named Entities in Tweets
Xiaohua Liu ? ?, Shaodian Zhang? ?, Furu Wei ?, Ming Zhou ?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, 200240, China
?Microsoft Research Asia
Beijing, 100190, China
?{xiaoliu, fuwei, mingzhou}@microsoft.com
? zhangsd.sjtu@gmail.com
Abstract
The challenges of Named Entities Recogni-
tion (NER) for tweets lie in the insufficient
information in a tweet and the unavailabil-
ity of training data. We propose to com-
bine a K-Nearest Neighbors (KNN) classi-
fier with a linear Conditional Random Fields
(CRF) model under a semi-supervised learn-
ing framework to tackle these challenges. The
KNN based classifier conducts pre-labeling to
collect global coarse evidence across tweets
while the CRF model conducts sequential la-
beling to capture fine-grained information en-
coded in a tweet. The semi-supervised learn-
ing plus the gazetteers alleviate the lack of
training data. Extensive experiments show the
advantages of our method over the baselines
as well as the effectiveness of KNN and semi-
supervised learning.
1 Introduction
Named Entities Recognition (NER) is generally un-
derstood as the task of identifying mentions of rigid
designators from text belonging to named-entity
types such as persons, organizations and locations
(Nadeau and Sekine, 2007). Proposed solutions to
NER fall into three categories: 1) The rule-based
(Krupka and Hausman, 1998); 2) the machine learn-
ing based (Finkel and Manning, 2009; Singh et al,
2010) ; and 3) hybrid methods (Jansche and Abney,
2002). With the availability of annotated corpora,
such as ACE05, Enron (Minkov et al, 2005) and
? This work has been done while the author was visiting
Microsoft Research Asia.
CoNLL03 (Tjong Kim Sang and DeMeulder, 2003),
the data driven methods now become the dominating
methods.
However, current NER mainly focuses on for-
mal text such as news articles (Mccallum and Li,
2003; Etzioni et al, 2005). Exceptions include stud-
ies on informal text such as emails, blogs, clini-
cal notes (Wang, 2009). Because of the domain
mismatch, current systems trained on non-tweets
perform poorly on tweets, a new genre of text,
which are short, informal, ungrammatical and noise
prone. For example, the average F1 of the Stan-
ford NER (Finkel et al, 2005) , which is trained
on the CoNLL03 shared task data set and achieves
state-of-the-art performance on that task, drops from
90.8% (Ratinov and Roth, 2009) to 45.8% on tweets.
Thus, building a domain specific NER for tweets
is necessary, which requires a lot of annotated tweets
or rules. However, manually creating them is tedious
and prohibitively unaffordable. Proposed solutions
to alleviate this issue include: 1) Domain adaption,
which aims to reuse the knowledge of the source do-
main in a target domain. Two recent examples are
Wu et al (2009), which uses data that is informa-
tive about the target domain and also easy to be la-
beled to bridge the two domains, and Chiticariu et
al. (2010), which introduces a high-level rule lan-
guage, called NERL, to build the general and do-
main specific NER systems; and 2) semi-supervised
learning, which aims to use the abundant unlabeled
data to compensate for the lack of annotated data.
Suzuki and Isozaki (2008) is one such example.
Another challenge is the limited information in
tweet. Two factors contribute to this difficulty. One
359
is the tweet?s informal nature, making conventional
features such as part-of-speech (POS) and capital-
ization not reliable. The performance of current
NLP tools drops sharply on tweets. For example,
OpenNLP 1, the state-of-the-art POS tagger, gets
only an accuracy of 74.0% on our test data set. The
other is the tweet?s short nature, leading to the ex-
cessive abbreviations or shorthand in tweets, and
the availability of very limited context information.
Tackling this challenge, ideally, requires adapting
related NLP tools to fit tweets, or normalizing tweets
to accommodate existing tools, both of which are
hard tasks.
We propose a novel NER system to address these
challenges. Firstly, a K-Nearest Neighbors (KNN)
based classifier is adopted to conduct word level
classification, leveraging the similar and recently
labeled tweets. Following the two-stage predic-
tion aggregation methods (Krishnan and Manning,
2006), such pre-labeled results, together with other
conventional features used by the state-of-the-art
NER systems, are fed into a linear Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001) model,
which conducts fine-grained tweet level NER. Fur-
thermore, the KNN and CRF model are repeat-
edly retrained with an incrementally augmented
training set, into which high confidently labeled
tweets are added. Indeed, it is the combination of
KNN and CRF under a semi-supervised learning
framework that differentiates ours from the exist-
ing. Finally, following Lev Ratinov and Dan Roth
(2009), 30 gazetteers are used, which cover com-
mon names, countries, locations, temporal expres-
sions, etc. These gazetteers represent general knowl-
edge across domains. The underlying idea of our
method is to combine global evidence from KNN
and the gazetteers with local contextual information,
and to use common knowledge and unlabeled tweets
to make up for the lack of training data.
12,245 tweets are manually annotated as the test
data set. Experimental results show that our method
outperforms the baselines. It is also demonstrated
that integrating KNN classified results into the CRF
model and semi-supervised learning considerably
boost the performance.
Our contributions are summarized as follows.
1http://sourceforge.net/projects/opennlp/
1. We propose to a novel method that combines
a KNN classifier with a conventional CRF
based labeler under a semi-supervised learning
framework to combat the lack of information in
tweet and the unavailability of training data.
2. We evaluate our method on a human anno-
tated data set, and show that our method outper-
forms the baselines and that both the combina-
tion with KNN and the semi-supervised learn-
ing strategy are effective.
The rest of our paper is organized as follows. In
the next section, we introduce related work. In Sec-
tion 3, we formally define the task and present the
challenges. In Section 4, we detail our method. In
Section 5, we evaluate our method. Finally, Section
6 concludes our work.
2 Related Work
Related work can be roughly divided into three cat-
egories: NER on tweets, NER on non-tweets (e.g.,
news, bio-logical medicine, and clinical notes), and
semi-supervised learning for NER.
2.1 NER on Tweets
Finin et al (2010) use Amazons Mechanical Turk
service 2 and CrowdFlower 3 to annotate named en-
tities in tweets and train a CRF model to evaluate
the effectiveness of human labeling. In contrast, our
work aims to build a system that can automatically
identify named entities in tweets. To achieve this,
a KNN classifier with a CRF model is combined
to leverage cross tweets information, and the semi-
supervised learning is adopted to leverage unlabeled
tweets.
2.2 NER on Non-Tweets
NER has been extensively studied on formal text,
such as news, and various approaches have been pro-
posed. For example, Krupka and Hausman (1998)
use manual rules to extract entities of predefined
types; Zhou and Ju (2002) adopt Hidden Markov
Models (HMM) while Finkel et al (2005) use CRF
to train a sequential NE labeler, in which the BIO
(meaning Beginning, the Inside and the Outside of
2https://www.mturk.com/mturk/
3http://crowdflower.com/
360
an entity, respectively) schema is applied. Other
methods, such as classification based on Maximum
Entropy models and sequential application of Per-
ceptron or Winnow (Collins, 2002), are also prac-
ticed. The state-of-the-art system, e.g., the Stanford
NER, can achieve an F1 score of over 92.0% on its
test set.
Biomedical NER represents another line of active
research. Machine learning based systems are com-
monly used and outperform the rule based systems.
A state-of-the-art biomedical NER system (Yoshida
and Tsujii, 2007) uses lexical features, orthographic
features, semantic features and syntactic features,
such as part-of-speech (POS) and shallow parsing.
A handful of work on other domains exists. For
example, Wang (2009) introduces NER on clinical
notes. A data set is manually annotated and a linear
CRF model is trained, which achieves an F-score of
81.48% on their test data set; Downey et al (2007)
employ capitalization cues and n-gram statistics to
locate names of a variety of classes in web text;
most recently, Chiticariu et al (2010) design and im-
plement a high-level language NERL that is tuned
to simplify the process of building, understanding,
and customizing complex rule-based named-entity
annotators for different domains.
Ratinov and Roth (2009) systematically study
the challenges in NER, compare several solutions
and report some interesting findings. For exam-
ple, they show that a conditional model that does
not consider interactions at the output level per-
forms comparably to beam search or Viterbi, and
that the BILOU (Beginning, the Inside and the Last
tokens of multi-token chunks as well as Unit-length
chunks) encoding scheme significantly outperforms
the BIO schema (Beginning, the Inside and Outside
of a chunk).
In contrast to the above work, our study focuses
on NER for tweets, a new genre of texts, which are
short, noise prone and ungrammatical.
2.3 Semi-supervised Learning for NER
Semi-supervised learning exploits both labeled and
un-labeled data. It proves useful when labeled data
is scarce and hard to construct while unlabeled data
is abundant and easy to access.
Bootstrapping is a typical semi-supervised learn-
ing method. It iteratively adds data that has been
confidently labeled but is also informative to its
training set, which is used to re-train its model. Jiang
and Zhai (2007) propose a balanced bootstrapping
algorithm and successfully apply it to NER. Their
method is based on instance re-weighting, which
allows the small amount of the bootstrapped train-
ing sets to have an equal weight to the large source
domain training set. Wu et al (2009) propose an-
other bootstrapping algorithm that selects bridging
instances from an unlabeled target domain, which
are informative about the target domain and are also
easy to be correctly labeled. We adopt bootstrapping
as well, but use human labeled tweets as seeds.
Another representative of semi-supervised learn-
ing is learning a robust representation of the input
from unlabeled data. Miller et al (2004) use word
clusters (Brown et al, 1992) learned from unla-
beled text, resulting in a performance improvement
of NER. Guo et al (2009) introduce Latent Seman-
tic Association (LSA) for NER. In our pilot study of
NER for tweets, we adopt bag-of-words models to
represent a word in tweet, to concentrate our efforts
on combining global evidence with local informa-
tion and semi-supervised learning. We leave it to
our future work to explore which is the best input
representation for our task.
3 Task Definition
We first introduce some background about tweets,
then give a formal definition of the task.
3.1 The Tweets
A tweet is a short text message containing no
more than 140 characters in Twitter, the biggest
micro-blog service. Here is an example of
tweets: ?mycraftingworld: #Win Microsoft Of-
fice 2010 Home and Student *2Winners* #Con-
test from @office and @momtobedby8 #Giveaway
http://bit.ly/bCsLOr ends 11/14?, where ?mycraft-
ingworld? is the name of the user who published
this tweet. Words beginning with the ?#? char-
acter, like ??#Win?, ?#Contest? and ?#Giveaway?,
are hash tags, usually indicating the topics of the
tweet; words starting with ?@?, like ?@office?
and ?@momtobedby8?, represent user names, and
?http://bit.ly/bCsLOr? is a shortened link.
Twitter users are interested in named entities, such
361
Figure 1: Portion of different types of named entities in
tweets. This is based on an investigation of 12,245 ran-
domly sampled tweets, which are manually labeled.
as person names, organization names and product
names, as evidenced by the abundant named entities
in tweets. According to our investigation on 12,245
randomly sampled tweets that are manually labeled,
about 46.8% have at least one named entity. Figure
1 shows the portion of named entities of different
types.
3.2 The Task
Given a tweet as input, our task is to identify both the
boundary and the class of each mention of entities of
predefined types. We focus on four types of entities
in our study, i.e., persons, organizations, products,
and locations, which, according to our investigation
as shown in Figure 1, account for 89.0% of all the
named entities.
Here is an example illustrating our task.
The input is ?...Me without you is like an
iphone without apps, Justin Bieber without
his hair, Lady gaga without her telephone, it
just wouldn...? The expected output is as fol-
lows:?...Me without you is like an <PRODUCT
>iphone</PRODUCT>without apps,
<PERSON>Justin Bieber</PERSON>without his
hair,<PERSON>Lady gaga</PERSON> without
her telephone, it just wouldn...?, meaning that
?iphone? is a product, while ?Justin Bieber? and
?Lady gaga? are persons.
4 Our Method
Now we present our solution to the challenging task
of NER for tweets. An overview of our method
is first given, followed by detailed discussion of its
core components.
4.1 Method Overview
NER task can be naturally divided into two sub-
tasks, i.e., boundary detection and type classifica-
tion. Following the common practice , we adopt
a sequential labeling approach to jointly resolve
these sub-tasks, i.e., for each word in the input
tweet, a label is assigned to it, indicating both the
boundary and entity type. Inspired by Ratinov and
Roth (2009), we use the BILOU schema.
Algorithm 1 outlines our method, where: trains
and traink denote two machine learning processes
to get the CRF labeler and the KNN classifier, re-
spectively; reprw converts a word in a tweet into a
bag-of-words vector; the reprt function transforms
a tweet into a feature matrix that is later fed into the
CRF model; the knn function predicts the class of
a word; the update function applies the predicted
class by KNN to the inputted tweet; the crf function
conducts word level NE labeling;? and ? represent
the minimum labeling confidence of KNN and CRF,
respectively, which are experimentally set to 0.1 and
0.001; N (1,000 in our work) denotes the maximum
number of new accumulated training data.
Our method, as illustrated in Algorithm 1, repeat-
edly adds the new confidently labeled tweets to the
training set 4 and retrains itself once the number
of new accumulated training data goes above the
threshold N . Algorithm 1 also demonstrates one
striking characteristic of our method: A KNN clas-
sifier is applied to determine the label of the current
word before the CRF model. The labels of the words
that confidently assigned by the KNN classifier are
treated as visible variables for the CRF model.
4.2 Model
Our model is hybrid in the sense that a KNN clas-
sifier and a CRF model are sequentially applied to
the target tweet, with the goal that the KNN classi-
fier captures global coarse evidence while the CRF
model fine-grained information encoded in a single
tweet and in the gazetteers. Algorithm 2 outlines the
training process of KNN, which records the labeled
word vector for every type of label.
Algorithm 3 describes how the KNN classifier
4The training set ts has a maximum allowable number of
items, which is 10,000 in our work. Adding an item into it will
cause the oldest one being removed if it is full.
362
Algorithm 1 NER for Tweets.
Require: Tweet stream i; output stream o.
Require: Training tweets ts; gazetteers ga.
1: Initialize ls, the CRF labeler: ls = trains(ts).
2: Initialize lk, the KNN classifier: lk = traink(ts).
3: Initialize n, the # of new training tweets: n = 0.
4: while Pop a tweet t from i and t ?= null do
5: for Each word w ? t do
6: Get the feature vector w?: w? =
reprw(w, t).
7: Classify w? with knn: (c, cf) =
knn(lk, w?).
8: if cf > ? then
9: Pre-label: t = update(t, w, c).
10: end if
11: end for
12: Get the feature vector t?: t? = reprt(t, ga).
13: Label t? with crf : (t, cf) = crf(ls, t?).
14: Put labeled result (t, cf) into o.
15: if cf > ? then
16: Add labeled result t to ts , n = n + 1.
17: end if
18: if n > N then
19: Retrain ls: ls = trains(ts).
20: Retrain lk: lk = traink(ts).
21: n = 0.
22: end if
23: end while
24: return o.
Algorithm 2 KNN Training.
Require: Training tweets ts.
1: Initialize the classifier lk:lk = ?.
2: for Each tweet t ? ts do
3: for Each word,label pair (w, c) ? t do
4: Get the feature vector w?: w? =
reprw(w, t).
5: Add the w? and c pair to the classifier: lk =
lk ? {(w?, c)}.
6: end for
7: end for
8: return KNN classifier lk.
predicts the label of the word. In our work, K is
experimentally set to 20, which yields the best per-
formance.
Two desirable properties of KNN make it stand
out from its alternatives: 1) It can straightforwardly
incorporate evidence from new labeled tweets and
retraining is fast; and 2) combining with a CRF
Algorithm 3 KNN predication.
Require: KNN classifier lk ;word vector w?.
1: Initialize nb, the neighbors of w?: nb =
neigbors(lk, w?).
2: Calculate the predicted class c?: c? =
argmaxc
?
(w?? ,c? )?nb ?(c, c
?) ? cos(w?, w??).
3: Calculate the labeling confidence cf : cf =
?
(w?? ,c? )?nb ?(c,c
?
)?cos(w?,w?
?
)
?
(w?? ,c? )?nb cos(w?,w?
? ) .
4: return The predicted label c? and its confidence cf .
model, which is good at encoding the subtle interac-
tions between words and their labels, compensates
for KNN?s incapability to capture fine-grained evi-
dence involving multiple decision points.
The Linear CRF model is used as the fine model,
with the following considerations: 1) It is well-
studied and has been successfully used in state-of-
the-art NER systems (Finkel et al, 2005; Wang,
2009); 2) it can output the probability of a label
sequence, which can be used as the labeling con-
fidence that is necessary for the semi-supervised
learning framework.
In our experiments, the CRF++ 5 toolkit is used to
train a linear CRF model. We have written a Viterbi
decoder that can incorporate partially observed la-
bels to implement the crf function in Algorithm 1.
4.3 Features
Given a word in a tweet, the KNN classifier consid-
ers a text window of size 5 with the word in the mid-
dle (Zhang and Johnson, 2003), and extracts bag-of-
word features from the window as features. For each
word, our CRF model extracts similar features as
Wang (2009) and Ratinov and Roth (2009), namely,
orthographic features, lexical features and gazetteers
related features. In our work, we use the gazetteers
provided by Ratinov and Roth (2009).
Two points are worth noting here. One is that
before feature extraction for either the KNN or the
CRF, stop words are removed. The stop words
used here are mainly from a set of frequently-used
words 6. The other is that tweet meta data is normal-
ized, that is, every link becomes *LINK* and every
5http://crfpp.sourceforge.net/
6http://www.textfixer.com/resources/common-english-
words.txt
363
account name becomes *ACCOUNT*. Hash tags
are treated as common words.
4.4 Discussion
We now discuss several design considerations re-
lated to the performance of our method, i.e., addi-
tional features, gazetteers and alternative models.
Additional Features. Features related to chunking
and parsing are not adopted in our final system, be-
cause they give only a slight performance improve-
ment while a lot of computing resources are required
to extract such features. The ineffectiveness of these
features is linked to the noisy and informal nature of
tweets. Word class (Brown et al, 1992) features are
not used either, which prove to be unhelpful for our
system. We are interested in exploring other tweet
representations, which may fit our NER task, for ex-
ample the LSA models (Guo et al, 2009).
Gazetteers. In our work, gazetteers prove to be sub-
stantially useful, which is consistent with the obser-
vation of Ratinov and Roth (2009). However, the
gazetteers used in our work contain noise, which
hurts the performance. Moreover, they are static,
directly from Ratinov and Roth (2009), thus with
a relatively lower coverage, especially for person
names and product names in tweets. We are devel-
oping tools to clean the gazetteers. In future, we plan
to feed the fresh entities correctly identified from
tweets back into the gazetteers. The correctness of
an entity can rely on its frequency or other evidence.
Alternative Models. We have replaced KNN by
other classifiers, such as those based on Maximum
Entropy and Support Vector Machines, respectively.
KNN consistently yields comparable performance,
while enjoying a faster retraining speed. Similarly,
to study the effectiveness of the CRF model, it is re-
placed by its alternations, such as the HMM labeler
and a beam search plus a maximum entropy based
classifier. In contrast to what is reported by Ratinov
and Roth (2009), it turns out that the CRF model
gives remarkably better results than its competitors.
Note that all these evaluations are on the same train-
ing and testing data sets as described in Section 5.1.
5 Experiments
In this section, we evaluate our method on a man-
ually annotated data set and show that our system
outperforms the baselines. The contributions of the
combination of KNN and CRF as well as the semi-
supervised learning are studied, respectively.
5.1 Data Preparation
We use the Twigg SDK 7 to crawl all tweets
from April 20th 2010 to April 25th 2010, then drop
non-English tweets and get about 11,371,389, from
which 15,800 tweets are randomly sampled, and are
then labeled by two independent annotators, so that
the beginning and the end of each named entity are
marked with<TYPE> and</TYPE>, respectively.
Here TYPE is PERSON, PRODUCT, ORGANIZA-
TION or LOCATION. 3555 tweets are dropped be-
cause of inconsistent annotation. Finally we get
12,245 tweets, forming the gold-standard data set.
Figure 1 shows the portion of named entities of dif-
ferent types. On average, a named entity has 1.2
words. The gold-standard data set is evenly split into
two parts: One for training and the other for testing.
5.2 Evaluation Metrics
For every type of named entity, Precision (Pre.), re-
call (Rec.) and F1 are used as the evaluation met-
rics. Precision is a measure of what percentage the
output labels are correct, and recall tells us to what
percentage the labels in the gold-standard data set
are correctly labeled, while F1 is the harmonic mean
of precision and recall. For the overall performance,
we use the average Precision, Recall and F1, where
the weight of each name entity type is proportional
to the number of entities of that type. These metrics
are widely used by existing NER systems to evaluate
their performance.
5.3 Baselines
Two systems are used as baselines: One is the
dictionary look-up system based on the gazetteers;
the other is the modified version of our system
without KNN and semi-supervised learning. Here-
after these two baselines are called NERDIC and
NERBA, respectively. The OpenNLP and the Stan-
ford parser (Klein and Manning, 2003) are used to
extract linguistic features for the baselines and our
method.
7It is developed by the Bing social search team, and cur-
rently is only internally available.
364
System Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NERBA 83.6 68.6 75.4
NERDIC 32.6 25.4 28.6
Table 1: Overall experimental results.
System Pre.(%) Rec.(%) F1(%)
NERCB 78.4 74.5 76.4
NERBA 83.6 68.4 75.2
NERDIC 37.1 29.7 33.0
Table 2: Experimental results on PERSON.
5.4 Basic Results
Table 1 shows the overall results for the baselines
and ours with the name NERCB . Here our sys-
tem is trained as described in Algorithm 1, combin-
ing a KNN classifier and a CRF labeler, with semi-
supervised learning enabled. As can be seen from
Table 1, on the whole, our method significantly out-
performs (with p < 0.001) the baselines. Tables 2-5
report the results on each entity type, indicating that
our method consistently yields better results on all
entity types.
5.5 Effects of KNN Classifier
Table 6 shows the performance of our method
without combining the KNN classifier, denoted by
NERCB?KNN . A drop in performance is observed
then. We further check the confidently predicted la-
bels of the KNN classifier, which account for about
22.2% of all predications, and find that its F1 is as
high as 80.2% while the baseline system based on
the CRF model achieves only an F1 of 75.4%. This
largely explains why the KNN classifier helps the
CRF labeler. The KNN classifier is replaced with
its competitors, and only a slight difference in per-
formance is observed. We do observe that retraining
KNN is obviously faster.
System Pre.(%) Rec.(%) F1(%)
NERCB 81.3 65.4 72.5
NERBA 82.5 58.4 68.4
NERDIC 8.2 6.1 7.0
Table 3: Experimental results on PRODUCT.
System Pre.(%) Rec.(%) F1(%)
NERCB 80.3 77.5 78.9
NERBA 81.6 69.7 75.2
NERDIC 30.2 30.0 30.1
Table 4: Experimental results on LOCATION.
System Pre.(%) Rec.(%) F1(%)
NERCB 83.2 60.4 70.0
NERBA 87.6 52.5 65.7
NERDIC 54.5 11.8 19.4
Table 5: Experimental results on ORGANIZATION.
5.6 Effects of the CRF Labeler
Similarly, the CRF model is replaced by its alterna-
tives. As is opposite to the finding of Ratinov and
Roth (2009), the CRF model gives remarkably bet-
ter results, i.e., 2.1% higher in F1 than its best fol-
lowers (with p < 0.001). Table 7 shows the overall
performance of the CRF labeler with various feature
set combinations, where Fo, Fl and Fg denote the
orthographic features, the lexical features and the
gazetteers related features, respectively. It can be
seen from Table 7 that the lexical and gazetteer re-
lated features are helpful. Other advanced features
such as chunking are also explored but with no sig-
nificant improvement.
5.7 Effects of Semi-supervised Learning
Table 8 compares our method with its modified ver-
sion without semi-supervised learning, suggesting
that semi-supervised learning considerably boosts
the performance. To get more details about self-
training, we evenly divide the test data into 10 parts
and feed them into our method sequentially; we
record the average F1 score on each part, as shown
in Figure 2.
5.8 Error Analysis
Errors made by our system on the test set fall into
three categories. The first kind of error, accounting
for 35.5% of all errors, is largely related to slang ex-
pressions and informal abbreviations. For example,
our method identifies ?Cali?, which actually means
?California?, as a PERSON in the tweet ?i love Cali
so much?. In future, we can design a normalization
365
System Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NERCB?KNN 82.6 74.8 78.5
Table 6: Overall performance of our system with and
without the KNN classifier, respectively.
Features Pre.(%) Rec.(%) F1(%)
Fo 71.3 42.8 53.5
Fo + Fl 76.2 44.2 55.9
Fo + Fg 80.5 66.2 72.7
Fo + Fl + Fg 82.6 74.8 78.5
Table 7: Overview performance of the CRF labeler (com-
bined with KNN) with different feature sets.
component to handle such slang expressions and in-
formal abbreviations.
The second kind of error, accounting for 37.2%
of all errors, is mainly attributed to the data sparse-
ness. For example, for this tweet ?come to see jaxon
someday?, our method mistakenly labels ?jaxon?
as a LOCATION, which actually denotes a PER-
SON. This error is understandable somehow, since
this tweet is one of the earliest tweets that mention
?jaxon?, and at that time there was no strong evi-
dence supporting that it represents a person. Possi-
ble solutions to these errors include continually en-
riching the gazetteers and aggregating additional ex-
ternal knowledge from other channels such as tradi-
tional news.
The last kind of error, which represents 27.3%
of all errors, somehow links to the noise prone na-
ture of tweets. Consider this tweet ?wesley snipes
ws cought 4 nt payin tax coz ths celebz dnt take it
cirus.?, in which ?wesley snipes? is not identified
as a PERSON but simply ignored by our method,
because this tweet is too noisy to provide effective
features. Tweet normalization technology seems a
possible solution to alleviate this kind of error.
Features Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NER?CB 82.1 71.9 76.7
Table 8: Performance of our system with and without
semi-supervised learning, respectively.
Figure 2: F1 score on 10 test data sets sequentially fed
into the system, each with 600 instances. Horizontal and
vertical axes represent the sequential number of the test
data set and the averaged F1 score (%), respectively.
6 Conclusions and Future work
We propose a novel NER system for tweets, which
combines a KNN classifier with a CRF labeler under
a semi-supervised learning framework. The KNN
classifier collects global information across recently
labeled tweets while the CRF labeler exploits infor-
mation from a single tweet and from the gazetteers.
A serials of experiments show the effectiveness of
our method, and particularly, show the positive ef-
fects of KNN and semi-supervised learning.
In future, we plan to further improve the per-
formance of our method through two directions.
Firstly, we hope to develop tweet normalization
technology to make tweets friendlier to the NER
task. Secondly, we are interested in integrating
new entities from tweets or other channels into the
gazetteers.
Acknowledgments
We thank Long Jiang, Changning Huang, Yunbo
Cao, Dongdong Zhang, Zaiqing Nie for helpful dis-
cussions, and the anonymous reviewers for their
valuable comments. We also thank Matt Callcut for
his careful proofreading of an early draft of this pa-
per.
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479.
366
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP, pages
1002?1012.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating Complex Named Entities in Web Text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In CSLDAMT, pages 80?88.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In EMNLP, pages
141?150.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL, pages 363?370.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL, pages 281?289.
Martin Jansche and Steven P. Abney. 2002. Informa-
tion extraction from voicemail transcripts. In EMNLP,
pages 320?327.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In ACL, pages 264?
271.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In ACL, pages
1121?1128.
George R. Krupka and Kevin Hausman. 1998. Isoquest:
Description of the netowlTM extractor system as used
in muc-7. In MUC-7.
John D. Lafferty, AndrewMcCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Andrew Mccallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In HLT-NAACL, pages 188?191.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In HLT-NAACL, pages 337?342.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In HLT,
pages 443?450.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Linguisti-
cae Investigationes, 30:3?26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147?155.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In HLT-NAACL, pages 73?81.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In ACL, pages 665?673.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL, pages 142?147.
Yefeng Wang. 2009. Annotating and recognising named
entities in clinical notes. In ACL-IJCNLP, pages 18?
26.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In EMNLP, pages 1523?1532.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In BioNLP,
pages 209?216.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition system.
In HLT-NAACL, pages 204?207.
GuoDong Zhou and Jian Su. 2002. Named entity recog-
nition using an hmm-based chunk tagger. In ACL,
pages 473?480.
367
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 44?49,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Engkoo: Mining the Web for Language Learning
Matthew R. Scott, Xiaohua Liu, Ming Zhou, Microsoft Engkoo Team
Microsoft Research Asia
No. 5, Dan Ling Street, Haidian District, Beijing, 100080, China
{mrscott, xiaoliu, mingzhou, engkoo}@microsoft.com
Abstract
This paper presents Engkoo 1, a system for
exploring and learning language. It is built
primarily by mining translation knowledge
from billions of web pages - using the Inter-
net to catch language in motion. Currently
Engkoo is built for Chinese users who are
learning English; however the technology it-
self is language independent and can be ex-
tended in the future. At a system level, En-
gkoo is an application platform that supports a
multitude of NLP technologies such as cross
language retrieval, alignment, sentence clas-
sification, and statistical machine translation.
The data set that supports this system is pri-
marily built from mining a massive set of
bilingual terms and sentences from across the
web. Specifically, web pages that contain
both Chinese and English are discovered and
analyzed for parallelism, extracted and for-
mulated into clear term definitions and sam-
ple sentences. This approach allows us to
build perhaps the world?s largest lexicon link-
ing both Chinese and English together - at the
same time covering the most up-to-date terms
as captured by the net.
1 Introduction
Learning and using a foreign language is a signif-
icant challenge for most people. Existing tools,
though helpful, have several limitations. Firstly,
they often depend on static contents compiled by
experts, and therefore cannot cover fresh words or
new usages of existing words. Secondly, their search
1http://www.engkoo.com.
functions are often limited, making it hard for users
to effectively find information they are interested in.
Lastly, existing tools tend to focus exclusively on
dictionary, machine translation or language learning,
losing out on synergy that can reduce inefficiencies
in the user experience.
This paper presents Engkoo, a system for explor-
ing and learning language. Different from exist-
ing tools, it discovers fresh and authentic transla-
tion knowledge from billions of web pages - using
the Internet to catch language in motion, and offer-
ing novel search functions that allow users efficient
access to massive knowledge resources. Addition-
ally, the system unifies the scenarios of dictionary,
machine translation, and language learning into a
seamless and more productive user experience. En-
gkoo derives its data from a process that continu-
ously culls bilingual term/sentence pairs from the
web, filters noise and conducts a series of NLP pro-
cesses including POS tagging, dependency parsing
and classification. Meanwhile, statistical knowledge
such as collocations is extracted. Next, the mined
bilingual pairs, together with the extracted linguistic
knowledge, are indexed. Finally, it exposes a set of
web services through which users can: 1) look up
the definition of a word/phrase; 2) retrieve example
sentences using keywords, POS tags or collocations;
and 3) get the translation of a word/phrase/sentence.
While Engkoo is currently built for Chinese users
who are learning English, the technology itself is
language independent and can be extended to sup-
port other language pairs in the future.
We have deployed Engkoo online to Chinese in-
ternet users and gathered log data that suggests its
44
utility. From the logs we can see on average 62.0%
of daily users are return users and 71.0% are active
users (make at least 1 query); active users make 8
queries per day on average. The service receives
more than one million page views per day.
This paper is organized as follows. In the next
section, we briefly introduce related work. In Sec-
tion 3, we describe our system. Finally, Section 4
concludes and presents future work.
2 Related Work
Online Dictionary Lookup Services. Online dic-
tionary lookup services can be divided into two cat-
egories. The first mainly relies on the dictionar-
ies edited by experts, e.g., Oxford dictionaries 2
and Longman contemporary English dictionary 3.
Examples of these kinds of services include iCiba
4 and Lingoes 5. The second depends mainly on
mined bilingual term/sentence pairs, e.g., Youdao 6.
In contrast to those services, our system has a higher
recall and fresher results, unique search functions
(e.g., fuzzy POS-based search, classifier filtering),
and an integrated language learning experience (e.g.,
translation with interactive word alignment, and
photorealistic lip-synced video tutors).
Bilingual CorpusMining and Postprocessing. Shi
et al (2006) uses document object model (DOM)
tree mapping to extract bilingual sentence pairs
from aligned bilingual web pages. Jiang et al
(2009b) exploits collective patterns to extract bilin-
gual term/sentence pairs from one web page. Liu et
al. (2010) proposes training a SVM-based classi-
fier with multiple linguistic features to evaluate the
quality of mined corpora. Some methods are pro-
posed to detect/correct errors in English (Liu et al,
2010; Sun et al, 2007). Following this line of work,
Engkoo implements its mining pipeline with a focus
on robustness and speed, and is designed to work on
a very large volume of web pages.
3 System Description
In this section, we first present the architecture fol-
lowed by a discussion of the basic components; we
2http://oxforddictionaries.com
3http://www.ldoceonline.com/
4http://dict.en.iciba.com/
5http://www.lingoes.cn/
6http://dict.youdao.com
Figure 1: System architecture of Engkoo.
then demonstrate the main scenarios.
3.1 System Overview
Figure 1 presents the architecture of Engkoo. It
can be seen that the components of Engkoo are or-
ganized into four layers. The first layer consists
of the crawler and the raw web page storage. The
crawler periodically downloads two kinds of web
pages, which are put into the storage. The first kind
of web pages are parallel web pages (describe the
same contents but with different languages, often
from bilingual sites, e.g., government sites), and the
second are those containing bilingual contents. A
list of seed URLs are maintained and updated after
each round of the mining process.
The second layer consists of the extractor, the
filter, the classifiers and the readability evaluator,
which are applied sequentially. The extractor scans
the raw web page storage and identifies bilingual
45
web page pairs using URL patterns. For example,
two web pages are parallel if their URLs are in
the form of ?? ? ? /zh/? ? ? ? and ?? ? ? /en/? ? ? ?, respec-
tively. Following the method of Shi et al (2006)
the extractor then extracts bilingual term/sentence
pairs from parallel web pages. Meanwhile, it
identifies web pages with bilingual contents, and
mines bilingual term/sentence pairs from them us-
ing the method proposed by Jiang et al (2009b).
The filter removes repeated pairs, and uses the
method introduced by Liu et al (2010) to sin-
gle out low quality pairs, which are further pro-
cessed by a noisy-channel based sub-model that at-
tempts to correct common spelling and grammar er-
rors. If the quality is still unacceptable after cor-
rection, they will be dropped. The classifiers, i.e.,
oral/non-oral, technical/non-technical, title/non-title
classifiers, are applied to each term/sentence pair.
The readability evaluator assigns a score to each
term/sentence pair according to Formula 1 7.
206.835?1.015? #words
#sentences
?84.6?#syllables
#words
(1)
Two points are worth noting here. Firstly, a list
of top sites from which a good number of high
quality pairs are obtained, is figured out; these are
used as seeds by the crawler. Secondly, bilingual
term/sentence pairs extracted from traditional dic-
tionaries are fed into this layer as well, but with the
quality checking process ignored.
The third layer consists of a series of NLP com-
ponents, which conduct POS tagging, dependency
parsing, and word alignment, respectively. It also
includes components that learn translation informa-
tion and collocations from the parsed term/sentence
pairs. Based on the learned statistical informa-
tion, two phrase-based statistical machine transla-
tion (SMT) systems are trained, which can then
translate sentences from one language to the other
and vice versa. Finally, the mined bilingual
term/sentence pairs, together with their parsed in-
formation, are stored and indexed with a multi-level
indexing engine, a core component of this layer. The
indexer is called multi-level since it uses not only
keywords but also POS tags and dependency triples
(e.g., ?TobjvwatchvTV?, which means ?TV? is the
7http://www.editcentral.com/gwt1/EditCentral.html
object of ?watch?) as lookup entries.
The fourth layer consists of a set of services that
expose the mined term/sentence pairs and the lin-
guistic knowledge based on the built index. On top
of these services, we construct a web application,
supporting a wide range of functions, such as search-
ing bilingual terms/sentences, translation and so on.
3.2 Main Components
Now we present the basic components of Engkoo,
namely: 1) the crawler, 2) the extractor, 3) the filter,
4) the classifiers, 5) the SMT systems, and 6) the in-
dexer.
Crawler. The crawler scans the Internet to get par-
allel and bilingual web pages. It employs a set of
heuristic rules related to URLs and contents to filter
unwanted pages. It uses a list of potential URLs to
guide its crawling. That is, it uses these URLs as
seeds, and then conducts a deep-first crawling with
a maximum allowable depth of 5. While crawling,
it maintains a cache of the URLs of the pages it has
recently downloaded. It processes a URL if and only
if it is not in the cache. In this way, the crawler tries
to avoid repeatedly downloading the same web page.
By now, about 2 billion pages have been scanned and
about 0.1 parallel/bilingual pages have been down-
loaded.
Extractor. A bilingual term/sentence extractor is
implemented following Shi et al (2006) and Jiang
et al (2009b). It works in two modes, mining from
parallel web pages and from bilingual web pages.
Parallel web pages are identified recursively in the
following way. Given a pair of parallel web pages,
the URLs in two pages are extracted respectively,
and are further aligned according to their positions
in DOM trees, so that more parallel pages can be ob-
tained. The method proposed by Jiang et al (2007)
is implemented as well to mine the definition of a
given term using search engines. By now, we have
obtained about 1,050 million bilingual term pairs
and 100 million bilingual sentence pairs.
Filter. The filter takes three steps to drop low qual-
ity pairs. Firstly, it checks each pair if it contains
any malicious word, say, a noisy symbol. Secondly,
it adopts the method of Liu et al (2010) to estimate
the quality of mined pairs. Finally, following the
work related to English as a second language (ESL)
errors detection/correction (Liu et al, 2010; Sun et
46
al., 2007), it implements a text normalization com-
ponent based on the noisy-channel model to correct
common spelling and grammar errors. That is, given
a sentence s? possibly with noise, find the sentence
s? = argmaxs p(s)p(s
? |s), where p(s) and p(s? |s)
are called the language model and the translation
model, respectively. In Engkoo, the language model
is a 5-gram language model trained on news articles
using SRILM (Stolcke, 2002), while the translation
model is based on a manually compiled translation
table. We have got about 20 million bilingual term
pairs and 15 million bilingual sentence pairs after
filtering noise.
Classifiers. All classifiers adopt SVM as mod-
els, and bag of words, bi-grams as well as sen-
tence length as features. For each classifier, about
10,000 sentence pairs are manually annotated for
training/development/testing. Experimental results
show that on average these classifiers can achieve an
accuracy of more than 90.0%.
SMT Systems. Our SMT systems are phrase-based,
trained on the web mined bilingual sentence pairs
using the GIZA++ (Och and Ney, 2000) alignment
package, with a collaborative decoder similar to Li
et al (2009). The Chinese-to-English/English-
to-Chinese SMT system achieves a case-insensitive
BLUE score of 29.6% / 47.1% on the NIST 2008
evaluation data set.
Indexer. At the heart of the indexer is the inverted
lists, each of which contains an entry pointing to
an ordered list of the related term/sentence pairs.
Compared with its alternatives, the indexer has two
unique features: 1) it contains various kinds of en-
tries, including common keywords, POS taggers,
dependency triples, collocations, readability scores
and class labels; and 2) the term/sentence pairs re-
lated to the entry are ranked according to their qual-
ities computed by the filter.
3.3 Using the System
Definition Lookup. Looking up a word or phrase on
Engkoo is a core scenario. The traditional dictionary
interface is extended with a blending of web-mined
and ranked term definitions, sample sentences, syn-
onyms, collocations, and phonetically similar terms.
The result page user experience includes an intu-
itive comparable tabs interface described in Jiang et
al. (2009a) that effectively exposes differences be-
tween similar terms. The search experience is aug-
mented with a fuzzy auto completion experience,
which besides traditional prefix matching is also ro-
bust against errors and allows for alternative inputs.
All of these contain inline micro translations to help
users narrow in on their intended search. Errors are
resolved by a blend of edit-distance and phonetic
search algorithms tuned for Chinese user behavior
patterns identified by user study. Alternative input
accepted includes Pinyin (Romanization of Chinese
characters) which returns transliteration, as well as
multiple wild card operators.
Take for example the query ?tweet,? illustrated in
Figure 2(a). The definitions for the term derived
from traditional dictionary sources are included in
the main definition area and refer to the noise of a
small bird. Augmenting the definition area are ?Web
translations,? which include the contemporary use of
the word standing for micro-blogging. Web-mined
bilingual sample sentences are also presented and
ranked by popularity metrics; this demonstrates the
modern usage of the term.
Search of Example Sentences. Engkoo exposes a
novel search and interactive exploration interface for
the ever-growing web-mined bilingual sample sen-
tences in its database. Emphasis is placed on sample
sentences in Engkoo because of their crucial role in
language learning. Engkoo offers new methods for
the self-exploration of language based on the applied
linguistic theories of ?learning as discovery? and
Data-Driven Learning (DDL) introduced by Johns
(1991). One can search for sentences as they would
in traditional search engines or concordancers. Ex-
tensions include allowing for mixed input of English
and Chinese, and POS wild cards enabled by multi-
level indexing. Further, sentences can be filtered
based on classifiers such as oral, written, and techni-
cal styles, source, and language difficulty. Addition-
ally sample sentences for terms can be filtered by
their inflection and the semantics of a particular def-
inition. Interactivity can be found in the word align-
ment between the languages as one moves his or her
mouse over the words, which can also be clicked
on for deeper exploration. And in addition to tra-
ditional text-to-speech, a visual representation of a
human language tutor pronouncing each sentence is
also included. Sample sentences between two simi-
lar words can be displayed side-by-side in a tabbed
47
(a) A screenshot of the definition and sample sentence areas of a Engkoo
result page.
(b) A screenshot of samples sentences for the POS-wildcard query ?v. tv?
(meaning ?verb TV?).
(c) A screenshot of machine translation integrated into the dictionary expe-
rience, where the top pane shows results of machine translation while the
bottom pane displays example sentences mined from the web.
Figure 2: Three scenarios of Engkoo.
48
user interface to easily expose the subtleties between
usages.
In the example seen in Figure 2(b), a user has
searched for the collocation verb+TV, represented
by the query ?v. TV? to find commonly used verbs
describing actions for the noun ?TV.? In the results,
we find fresh and authentic sample sentences mined
from the web, the first of which contains ?watch
TV,? the most common collocation, as the top result.
Additionally, the corresponding keyword in Chinese
is automatically highlighted using statistical align-
ment techniques.
Machine Translation. For many users, the differ-
ence between a machine translation (MT) system
and a translation dictionary are not entirely clear. In
Engkoo, if a term or phrase is out-of-vocabulary, a
MT result is dynamically returned. For shorter MT
queries, sample sentences might also be returned as
one can see in Figure 2(c) which expands the search
and also raises confidence in a translation as one can
observe it used on the web. Like the sample sen-
tences, word alignment is also exposed on the ma-
chine translation. As the alignment naturally serves
as a word breaker, users can click the selection for
a lookup which would open a new tab with the def-
inition. This is especially useful in cases where a
user might want to find alternatives to a particular
part of a translation. Note that the seemingly single
line dictionary search box is also adapted to MT be-
havior, allowing users to paste in multi-line text as
it can detect and unfold itself to a larger text area as
needed.
4 Conclusions and Future work
We have presented Engkoo, a novel online transla-
tion system which uniquely unifies the scenarios of
dictionary, machine translation, and language learn-
ing. The features of the offering are based on an
ever-expanding data set derived from state-of-the-art
web mining and NLP techniques. The contribution
of the work is a complete software system that max-
imizes the web?s pedagogical potential by exploiting
its massive language resources. Direct user feed-
back and implicit log data suggest that the service
is effective for both translation utility and language
learning, with advantages over existing services. In
future work, we are examining extracting language
knowledge from the real-time web for translation in
news scenarios. Additionally, we are actively min-
ing other language pairs to build a multi-language
learning system.
Acknowledgments
We thank Cheng Niu, Dongdong Zhang, Frank
Soong, Gang Chen, Henry Li, Hao Wei, Kan Wang,
Long Jiang, Lijuan Wang, Mu Li, Tantan Feng, Wei-
jiang Xu and Yuki Arase for their valuable contribu-
tions to this paper, and the anonymous reviewers for
their valuable comments.
References
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with web min-
ing and transliteration. In IJCAI, pages 1629?1634.
Gonglue Jiang, Chen Zhao, Matthew R. Scott, and Fang
Zou. 2009a. Combinable tabs: An interactive method
of information comparison using a combinable tabbed
document interface. In INTERACT, pages 432?435.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,
and Qingsheng Zhu. 2009b. Mining bilingual data
from the web with adaptively learnt patterns. In
ACL/AFNLP, pages 870?878.
Tim Johns. 1991. From printout to handout: grammar
and vocabulary teaching in the context of data driven
learning. Special issue of ELR Journal, pages 27?45.
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009. Collaborative decoding: Partial
hypothesis re-ranking using translation consensus be-
tween decoders. In ACL/AFNLP, pages 585?592.
Xiaohua Liu and Ming Zhou. 2010. Evaluating the qual-
ity of web-mined bilingual sentences using multiple
linguistic features. In IALP, pages 281?284.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. Srl-based verb selec-
tion for esl. In EMNLP, pages 1068?1076.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao. 2006.
A dom tree alignment model for mining parallel data
from the web. In ACL, pages 489?496.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In ICSLP, volume 2, pages 901?904.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In ACL.
49
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526?535,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Joint Inference of Named Entity Recognition and Normalization for Tweets
Xiaohua Liu ? ?, Ming Zhou ?, Furu Wei ?, Zhongyang Fu ?, Xiangyang Zhou ?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, 200240, China
?School of Computer Science and Technology
Shandong University, Jinan, 250100, China
?Microsoft Research Asia
Beijing, 100190, China
?{xiaoliu, fuwei, mingzhou}@microsoft.com
? zhongyang.fu@gmail.com ? v-xzho@microsoft.com
Abstract
Tweets represent a critical source of fresh in-
formation, in which named entities occur fre-
quently with rich variations. We study the
problem of named entity normalization (NEN)
for tweets. Two main challenges are the er-
rors propagated from named entity recogni-
tion (NER) and the dearth of information in
a single tweet. We propose a novel graphi-
cal model to simultaneously conduct NER and
NEN on multiple tweets to address these chal-
lenges. Particularly, our model introduces a
binary random variable for each pair of words
with the same lemma across similar tweets,
whose value indicates whether the two related
words are mentions of the same entity. We
evaluate our method on a manually annotated
data set, and show that our method outper-
forms the baseline that handles these two tasks
separately, boosting the F1 from 80.2% to
83.6% for NER, and the Accuracy from 79.4%
to 82.6% for NEN, respectively.
1 Introduction
Tweets, short messages of less than 140 characters
shared through the Twitter service 1, have become
an important source of fresh information. As a re-
sult, the task of named entity recognition (NER)
for tweets, which aims to identify mentions of rigid
designators from tweets belonging to named-entity
types such as persons, organizations and locations
(2007), has attracted increasing research interest.
For example, Ritter et al (2011) develop a sys-
tem that exploits a CRF model to segment named
1http://www.twitter.com
entities and then uses a distantly supervised ap-
proach based on LabeledLDA to classify named en-
tities. Liu et al (2011) combine a classifier based
on the k-nearest neighbors algorithm with a CRF-
based model to leverage cross tweets information,
and adopt the semi-supervised learning to leverage
unlabeled tweets.
However, named entity normalization (NEN) for
tweets, which transforms named entities mentioned
in tweets to their unambiguous canonical forms, has
not been well studied. Owing to the informal nature
of tweets, there are rich variations of named enti-
ties in them. According to our investigation on the
data set provided by Liu et al (2011), every named
entity in tweets has an average of 3.3 variations 2.
As an illustrative example, we show ?Anneke Gron-
loh?, which may occur as ?Mw.,Gronloh?, ?Anneke
Kronloh? or ?Mevrouw G?. We thus propose NEN
for tweets, which plays an important role in entity
retrieval, trend detection, and event and entity track-
ing. For example, Khalid et al (2008) show that
even a simple normalization method leads to im-
provements of early precision, for both document
and passage retrieval, and better normalization re-
sults in better retrieval performance.
Traditionally, NEN is regarded as a septated task,
which takes the output of NER as its input (Li et al,
2002; Cohen, 2005; Jijkoun et al, 2008; Dai et al,
2011). One limitation of this cascaded approach is
that errors propagate from NER to NEN and there is
no feedback from NEN to NER. As demonstrated by
Khalid et al (2008), most NEN errors are caused
2This data set consists of 12,245 randomly sampled tweets
within five days.
526
by recognition errors. Another challenge of NEN
is the dearth of information in a single tweet, due
to the short and noise-prone nature of tweets. Re-
portedly, the accuracy of a baseline NEN system
based on Wikipedia drops considerably from 94%
on edited news to 77% on news comments, a kind of
user generated content (UGC) with similar style to
tweets (Jijkoun et al, 2008).
We propose jointly conducting NER and NEN
on multiple tweets using a graphical model, to
address these challenges. Intuitively, improving
the performance of NER boosts the performance
of NEN. For example, consider the following two
tweets: ?? ? ?Alex?s jokes. Justin?s smartness. Max?s
randomnes? ? ? ? and ?? ? ?Alex Russo was like the
best character on Disney Channel? ? ? ?. Identify-
ing ?Alex? and ?Alex Russo? as PERSON will en-
courage NEN systems to normalize ?Alex? into
?Alex Russo?. On the other hand, NEN can guide
NER. For instance, consider the following two
tweets: ?? ? ? she knew Burger King when he was a
Prince!? ? ? ? and ?? ? ? I?m craving all sorts of food:
mcdonalds, burger king, pizza, chinese? ? ? ?. Sup-
pose the NEN system believes that ?burger king?
cannot be mapped to ?Burger King? since these two
tweets are not similar in content. This will help NER
to assign them different types of labels. Our method
optimizes these two tasks simultaneously by en-
abling them to interact with each other. This largely
differentiates our method from existing work.
Furthermore, considering multiple tweets simul-
taneously allows us to exploit the redundancy in
tweets, as suggested by Liu et al (2011). For exam-
ple, consider the following two tweets: ?? ? ?Bobby
Shaw you don?t invite the wind? ? ? ? and ?? ? ? I own
yah ! Loool bobby shaw? ? ? ?. Recognizing ?Bobby
Shaw? in the first tweet as a PERSON is easy owing
to its capitalization and the following word ?you?,
which in turn helps to identify ?bobby shaw? in the
second tweet as a PERSON.
We adopt a factor graph as our graphical model,
which is constructed in the following manner. We
first introduce a random variable for each word in
every tweet, which represents the BILOU (Begin-
ning, the Inside and the Last tokens of multi-token
entities as well as Unit-length entities) label of the
corresponding word. Then we add a factor to con-
nect two neighboring variables, forming a conven-
tional linear chain CRFs. Hereafter, we use tm to
denote the mth tweet ,tim and yim to denote the ith
word of of tm and itsBILOU label, respectively, and
f im to denote the factor related to yi?1m and yim. Next,
for each word pair with the same lemma, denoted by
tim and t
j
n, we introduce a binary random variable,
denoted by zijmn, whose value indicates whether tim
and tjn belong to two mentions of the same entity. Fi-
nally, for any zijmn we add a factor, denoted by f ijmn,
to connect yim, y
j
n and zijmn. Factors in the same
group ({f ijmn} or {f im}) share the same set of fea-
ture templates. Figure 1 illustrates an example of
our factor graph for two tweets.
Figure 1: A factor graph that jointly conducts NER and
NEN on multiple tweets. Blue and green circles rep-
resent NE type (y-serials) and normalization variables
(z-serials), respectively; filled circles indicate observed
random variables; blue rectangles represent the factors
connecting neighboring y-serial variables while red rect-
angles stand for the factors connecting distant y-serial
and z-serial variables.
It is worth noting that our factor graph is differ-
ent from the skip-chain CRFs (Galley, 2006) in the
sense that any skip-chain factor of our model con-
sists not only of two NE type variables (yim and y
j
n),
which is the case for skip-chain CRFs, but also a nor-
malization variable (zijmn). It is these normalization
variables that enable us to conduct NER and NEN
jointly.
We manually add normalization information to
the data set shared by Liu et al (2011), to eval-
uate our method. Experimental results show that
our method achieves 83.6% F1 for NER and 82.6%
Accuracy for NEN, outperforming the baseline with
80.2%F1 for NER and 79.4% Accuracy for NEN.
We summarize our contributions as follows.
1. We introduce the task of NEN for tweets, and
propose jointly conducting NER and NEN for
527
multiple tweets using a factor graph, which
leverages redundancy in tweets to make up for
the dearth of information in a single tweet and
allows these two tasks to inform each other.
2. We evaluate our method on a human annotated
data set, and show that our method compares
favorably with the baseline, achieving better
performance in both tasks.
Our paper is organized as follows. In the next sec-
tion, we introduce related work. In Section 3 and 4,
we formally define the task and present our method.
In Section 5, we evaluate our method. And finally
we conclude our work in Section 6.
2 Related Work
Related work can be divided into two categories:
NER and NEN.
2.1 NER
NER has been well studied and its solutions can be
divided into three categories: 1) Rule-based (Krupka
and Hausman, 1998); 2) machine learning based
(Finkel and Manning, 2009; Singh et al, 2010); and
3) hybrid methods (Jansche and Abney, 2002). Ow-
ing to the availability of annotated corpora, such as
ACE05, Enron (Minkov et al, 2005) and CoNLL03
(Tjong Kim Sang and De Meulder, 2003), data
driven methods are now dominant.
Current studies of NER mainly focus on formal
text such as news articles (Mccallum and Li, 2003;
Etzioni et al, 2005). A representative work is that
of Ratinov and Roth (2009), in which they system-
atically study the challenges of NER, compare sev-
eral solutions, and show some interesting findings.
For example, they show that the BILOU encoding
scheme significantly outperforms the BIO schema
(Beginning, the Inside and Outside of a chunk).
A handful of work on other genres of texts exists.
For example, Yoshida and Tsujii build a biomedi-
cal NER system (2007) using lexical features, or-
thographic features, semantic features and syntactic
features, such as part-of-speech (POS) and shallow
parsing; Downey et al (2007) employ capitaliza-
tion cues and n-gram statistics to locate names of a
variety of classes in web text; Wang (2009) intro-
duces NER to clinical notes. A linear CRF model
is trained on a manually annotated data set, which
achieves an F1 of 81.48% on the test data set; Chiti-
cariu et al (2010) design and implement a high-
level language NERL which simplifies the process
of building, understanding, and customizing com-
plex rule-based named-entity annotators for differ-
ent domains.
Recently, NER for Tweets attracts growing inter-
est. Finin et al (2010) use Amazons Mechani-
cal Turk service 3 and CrowdFlower 4 to annotate
named entities in tweets and train a CRF model to
evaluate the effectiveness of human labeling. Rit-
ter et al (2011) re-build the NLP pipeline for
tweets beginning with POS tagging, through chunk-
ing, to NER, which first exploits a CRF model to
segment named entities and then uses a distantly su-
pervised approach based on LabeledLDA to clas-
sify named entities. Unlike this work, our work de-
tects the boundary and type of a named entity si-
multaneously using sequential labeling techniques.
Liu et al (2011) combine a classifier based on
the k-nearest neighbors algorithm with a CRF-based
model to leverage cross tweets information, and
adopt the semi-supervised learning to leverage un-
labeled tweets. Our method leverages redundance
in similar tweets, using a factor graph rather than a
two-stage labeling strategy. One advantage of our
method is that local and global information can in-
teract with each other.
2.2 NEN
There is a large body of studies into normalizing
various types of entities for formally written texts.
For instance, Cohen (2005) normalizes gene/protein
names using dictionaries automatically extracted
from gene databases; Magdy et al (2007) address
cross-document Arabic name normalization using a
machine learning approach, a dictionary of person
names and frequency information for names in a
collection; Cucerzan (2007) demostrates a large-
scale system for the recognition and semantic dis-
ambiguation of named entities based on informa-
tion extracted from a large encyclopedic collection
and Web search results; Dai et al (2011) employ
a Markov logic network to model interweaved con-
3https://www.mturk.com/mturk/
4http://crowdflower.com/
528
straints in a setting of gene mention normalization.
Jijkoun et al (2008) study NEN for UGC. They
report that the accuracy of a baseline NEN system
based on Wikipedia drops considerably from 94%
on edited news to 77% on UGC. They identify three
main error sources, i.e., entity recognition errors,
multiple ways of referring to the same entity and am-
biguous references, and exploit hand-crafted rules to
improve the baseline NEN system.
We introduce the task of NEN for tweets, a new
genre of texts with rich entity variations. In contrast
to existing NEN systems, which take the output of
NER systems as their input, our method conducts
NER and NEN at the same time, allowing them to
reinforce each other, as demonstrated by the experi-
mental results.
3 Task Definition
A tweet is a short text message with no more than
140 characters. Here is an example of a tweet: ?my-
craftingworld: #Win Microsoft Office 2010 Home
and Student #Contest from @office http://bit.ly/ ? ? ?
?, where ?mycraftingworld? is the name of the user
who published this tweet. Words beginning with
?#? like ??#Win? are hash tags; words starting
with ?@? like ?@office? represent user names; and
?http://bit.ly/? is a shortened link.
Given a set of tweets, e.g., tweets within some pe-
riod or related to some query, our task is: 1) To rec-
ognize each mention of entities of predefined types
for each tweet; and 2) to restore each entity mention
into its unambiguous canonical form. Following Liu
et al (2011), we focus on four types of entities, i.e.,
PERSON, ORGANIZATION, PRODUCT, and LO-
CATION, and constrain our scope to English tweets.
Note that the NEN sub-task can be transformed as
follows. Given each pair of entity mentions, decide
whether they denote the same entity. Once this is
achieved, we can link all the mentions of the same
entity, and choose a representative mention, e.g., the
longest mention, as their canonical form.
As an illustrative example, consider the following
three tweets: ?? ? ?Gaga?s Christmas dinner with her
family. Awwwwn? ? ? ?, ?? ? ?Lady Gaaaaga with her
family on Christmas? ? ? ? and ?? ? ?Buying a maga-
zine just because Lady Gaga?s on the cover? ? ? ?. It
is expected that ?Gaga?, ?Lady Gaaaaga? and ?Lady
Gaga? are all labeled as PERSON, and can be re-
stored as ?Lady Gaga?.
4 Our Method
In contrast to existing work, our method jointly
conducts NER and NEN for multiple tweets. We
first give an overview of our method, then detail its
model and features.
4.1 Overview
Given a set of tweets as input, our method recog-
nizes predefined types of named entities and for each
entity outputs its unambiguous canonical form.
To resolve NER, we assign a label to each
word in a tweet, indicating both the boundary
and entity type. Following Ratinov and Roth
(2009), we use the BILOU schema. For ex-
ample, consider the tweet ?? ? ?without you is
like an iphone without apps; Lady gaga with-
out her telephone? ? ? ?, the labeled sequence us-
ing the BILOU schema is: ?? ? ?withoutO youO
isO likeO anO iphoneU?PRODUCT withoutO appsO;
LadyB?PERSON gagaL?PERSON withoutO herO
telephoneO? ? ? ? , where ?iphoneU?PRODUCT ? indi-
cates that ?iphone? is a product name of unit length;
?LadyB?PERSON? means ?Lady? is the beginning
of a person name while ?gagaL?PERSON? suggests
that ?gaga? is the last token of a person name.
To resolve NEN, we assign a binary value label
zijmn to each pair of words tim and t
j
n which share the
same lemma. zijmn = 1 or -1, indicating whether tim
and tjn belong to two mentions of the same entity 5.
For example, consider the three tweets presented in
Section 3. ?Gaga11? 6 and ?Gaga13? will be assigned
a ?1? label, since they are part of two mentions of the
same entity ?Lady Gaga?; similarly, ?Lady12? and
?Lady13? are connected with a ?1? label. Note that
there are no NEN labels for pairs like ?her11? and
?her12? or ?with11 and ?with12?, since words like ?her?
and ?with? are stop words.
With NE type and normalization labels obtained,
we judge two mentions, denoted by ti1???ikm and
5Stop words have no normalization labels. The stop words
are mainly from http://www.textfixer.com/resources/common-
english-words.txt.
6We use wim to denote word w?s ith appearance in the mth
tweet. For example, ?Gaga11? denotes the first occurance of
?Gaga? in the first tweet.
529
tj1???jln , respectively, refer to the same entity if and
only if: 1) The two mentions share the same entity
type; 2) ti1???ikm is a sub-string of t
j1???jln or vise versa;
and 3) zijmn = 1, i = i1, ? ? ? , ik and j = j1, ? ? ? , jl,
if zijmn exists. Still take the three tweets presented
in Section 3 for example. Suppose ?Gaga11? and
?Lady Gaga13? are labeled as PERSON, and there
is only one related NE normalization label, which
is associated with ??Gaga11? and ?Gaga13? and has 1
as its value. We then consider that these two men-
tions can be normalized into the same entity; in a
similar way, we can align ?Lady12 Gaaaaga? with
?Lady13 Gaga?. Combining these pieces informa-
tion together, we can infer that ??Gaga11?, ?Lady12
Gaaaaga? and ?Lady13 Gaga? are three mentions of
the same entity. Finally, we can select ?Lady13 Gaga?
as the representative, and output ?Lady Gaga? as
their canonical form. We choose the mention with
the maximum number of words as the representa-
tive. In case of a tie, we prefer the mention with an
Wikipedia entry 7.
The central problem with our method is infer-
ring all the NE type (y-serial) and normalization
(z-serial) variables. To achieve this, we construct
a factor graph according to the input tweets, which
can evaluate the probability of every possible assign-
ment of y-serials and z-serials, by checking the
characteristics of the assignment. Each character-
istic is called a feature. In this way, we can select
the assignment with the highest probability. Next
we will introduce our model in detail, including its
training and inference procedure and features.
4.2 Model
We adopt a factor graph as our model. One advan-
tage of our model is that it allows y-serials and
z-serials variables to interact with each other to
jointly optimize NER and NEN.
Given a set of tweets T = {tm}Nm=1, we can build
a factor graph G = (Y,Z, F,E), where: Y and Z
denote y-serials and z-serials variables, respec-
tively; F represents factor vertices, consisting of
{f im} and {f
ij
mn}, f im = f im(yi?1m , yim) and f
ij
mn =
f ijmn(yim, y
j
n, zijmn); E stands for edges, which de-
pends on F , and consists of edges between yi?1m and
yim, and those between yim,y
j
n and f ijmn.
7If it still ends up as a draw, we will randomly choose one
from the best.
G = (Y, Z, F,E) defines a probability distribu-
tion according to Formula 1.
lnP (Y, Z|G, T ) ?
?
m,i
ln f im(yi?1m , yim)+
?
m,n,i,j
?ijmn ? ln f ijmn(yim, yjn, zijmn)
(1)
where ?ijmn = 1 if and only if tim and t
j
n have the
same lemma and are not stop words, otherwise zero.
A factor factorizes according to a set of features, so
that:
ln f im(yi?1m , yim) =
?
k
?(1)k ?
(1)
k (y
i?1
m , yim)
ln f ijmn(yim, yjn, zijmn) =
?
k
?(2)k ?
(2)
k (y
i
m, yjn, zijmn)
(2)
{?(1)k }
K1
k=1 and {?
(2)
k }
K2
k=1 are two feature sets. ? =
{?(1)k }
K1
k=1
?
{?(2)k }
K2
k=1 is called the feature weight
set or parameter set of G. Each feature has a real
value as its weight.
Training ? is learnt from annotated tweets T , by
maximizing the data likelihood, i.e.,
?? = argmax
?
lnP (Y,Z|?, T ) (3)
To solve this optimization problem, we first calcu-
late its gradient:
? lnP (Y, Z|T ; ?)
??1k
=
?
m,i
?(1)k (y
i?1
m , yim)
?
?
m,i
?
yi?1m ,yim
p(yi?1m , yim|T ; ?)?
(1)
k (y
i?1
m , yim)
(4)
? lnP (Y, Z|T ; ?)
??2k
=
?
m,n,i,j
?ijmn ? ?
(2)
k (y
i
m, yjn, zijmn)
?
?
m,n,i,j
?ijmn
?
yim,y
j
n,zijmn
p(yim, yjn, zijmn|T ; ?)
??(2)k (y
i
m, yjn, zijmn)
(5)
Here, the two marginal probabilities
p(yi?1m , yim|T ; ?) and p(yim, y
j
n, zijmn|T ; ?) are
computed using loopy belief propagation (Murphy
et al, 1999). Once we have computed the gradient,
?? can be worked out by standard techniques such
as steepest descent, conjugate gradient and the
530
limited-memory BFGS algorithm (L-BFGS). We
choose L-BFGS because it is particularly well suited
for optimization problems with a large number of
variables.
Inference Supposing the parameters ? have been
set to ??, the inference problem is: Given a set
of testing tweets T , output the most probable
assignment of Y and Z, i.e.,
(Y, Z)? = argmax
(Y,Z)
lnP (Y,Z|??, T ) (6)
We adopt the max-product algorithm to solve this
inference problem. The max-product algorithm is
nearly identical to the loopy belief propagation al-
gorithm, with the sums replaced by maxima in the
definitions. Note that in both the training and test-
ing stage, the factor graph is constructed in the same
way as described in Section 1.
Efficiency We take several actions to improve our
model?s efficiency. Firstly, we manually compile a
comprehensive named entity dictionary from vari-
ous sources including Wikipedia, Freebase 8, news
articles and the gazetteers shared by Ratinov and
Roth (2009). In total this dictionary contains 350
million entries 9. By looking up this dictionary 10,
we generate the possible BILOU labels, denoted by
Y im hereafter, for each word tim. For instance, con-
sider ?? ? ?Good Morning new11 york11? ? ? ?. Suppose
?New York City? and ?New York Times? are in
our dictionary, then ?new11 york11? is the matched
string with two corresponding entities. As a re-
sult, ?B-LOCATION? and ?B-ORGANIZATION?
will be added to Ynew11 , and ?I-LOCATION? and
?I-ORGANIZATION? will be added to Yyork11 . If
Y im ?= ?, we enforce the constraint for training and
testing that yim ? Y im , to reduce the search space.
Secondly, in the testing phase, we introduce three
rules related to zijmn: 1) zijmm = 1, which says two
words sharing the same lemma in the same tweet
denote the same entity; 2) set zijmn to 1, if the sim-
ilarity between tm and tn is above a threshold (0.8
in our work), or tm and tn share one hash tag; and
3)zmnij = ?1, if the similarity between tm and
tn is below a threshold (0.3 in work). To compute
8http://freebase.com/view/military
9One phrase refereing to L entities has L entries.
10We use case-insensitive leftmost longest match.
the similarity, each tweet is represented as a bag-of-
words vector with the stop words removed, and the
cosine similarity is adopted, as defined in Formula
7. These rules pre-label a significant part of z-serial
variables (accounting for 22.5%), with an accuracy
of 93.5%.
sim(tm, tn) =
t?m ? t?n
|?tm||?tn|
(7)
Note that in our experiments, these measures reduce
the training and testing time by 36.2% and 62.8%,
respectively, while no obvious performance drop is
observed.
4.3 Features
A feature in {?(1)k }
K1
k=1 involves a pair of neighbor-
ing NE-type labels, i.e., yi?1m and yim, while a fea-
ture in {?(2)k }
K2
k=1 concerns a pair of distant NE-type
labels and its associated normalization label, i.e.,
yim,y
j
n and zijmn. Details are given below.
4.3.1 Feature Set One: {?(1)k }
K1
k=1
We adopts features similar to Wang (2009), and
Ratinov and Roth (2009), i.e., orthographic features,
lexical features and gazetteer-related features. These
features are defined on the observation. Combining
them with yi?1m and yim constitutes {?
(1)
k }
K1
k=1.
Orthographic features: Whether tim is capitalized
or upper case; whether it is alphanumeric or contains
any slashes; wether it is a stop word; word prefixes
and suffixes.
Lexical features: Lemma of tim, ti?1m and ti+1m ,
respectively; whether tim is an out-of-vocabulary
(OOV) word 11; POS of tim, ti?1m and ti+1m , respec-
tively; whether tim is a hash tag, a link, or a user
account.
Gazetteer-related features: Whether Y im is empty;
the dominating label/entity type in Y im. Which one
is dominant is decided by majority voting of the en-
tities in our dictionary. In case of a tie, we randomly
choose one from the best.
4.3.2 Feature Set Two: {?(2)k }
K2
k=1
Similarly, we define orthographic, lexical features
and gazetteer-related features on the observation, yim
11We first conduct a simple dictionary-lookup based normal-
ization with the incorrect/correct word pair list provided by Han
et al (2011) to correct common ill-formed words. Then we call
an online dictionary service to judge whether a word is OOV.
531
and yjn; and then we combine these features with
zijmn, forming {?(2)k }
K2
k=1.
Orthographic features: Whether tim / t
j
n is capital-
ized or upper case; whether tim / t
j
n is alphanumeric
or contains any slashes; prefixes and suffixes of tim.
Lexical features: Lemma of tim; whether tim is
OOV; whether tim / ti+1m / ti?1m and t
j
n / tj+1n / tj?1n
have the same POS; whether yim and y
j
n have the
same label/entity type.
Gazetteer-related features: Whether Y im
?
Y jn /
Y i+1m
?
Y j+1n / Y i?1m
?
Y j?1n is empty; whether the
dominating label/entity type in Y im is the same as
that in Y jn .
5 Experiments
We manually annotate a data set to evaluate our
method. We show that our method outperforms the
baseline, a cascaded system that conducts NER and
NEN individually.
5.1 Data Preparation
We use the data set provided by Liu et al (2011),
which consists of 12,245 tweets with four types of
entities annotated: PERSON, LOCATION, ORGA-
NIZATION and PRODUCT. We enrich this data set
by adding entity normalization information. Two
annotators 12 are involved. For any entity mention,
two annotators independently annotate its canonical
form. The inter-rater agreement measured by kappa
is 0.72. Any inconsistent case is discussed by the
two annotators till a consensus is reached. 2, 245
tweets are used for development, and the remainder
are used for 5-fold cross validation.
5.2 Evaluation Metrics
We adopt the widely-used Precision, Recall and F1
to measure the performance of NER for a partic-
ular type of entity, and the average Precision, Re-
call and F1 to measure the overall performance of
NER (Liu et al, 2011; Ritter et al, 2011). As for
NEN, we adopt the widely-used Accuracy, i.e., to
what percentage the outputted canonical forms are
correct (Jijkoun et al, 2008; Cucerzan, 2007; Li et
al., 2002).
12Two native English speakers.
5.3 Baseline
We develop a cascaded system as the baseline,
which conducts NER and NEN sequentially. Its
NER module, denoted by SBR, is based on the state-
of-the-art method introduced by Liu et al (2011);
and its NEN model , denoted by SBN , follows
the NEN system for user-generated news comments
proposed by Jijkoun et al (2008), which uses
handcrafted rules to improve a typical NEN system
that normalizes surface forms to Wikipedia page ti-
tles. We use the POS tagger developed by Ritter et
al. (2011) to extract POS related features, and the
OpenNLP toolkit to get lemma related features.
5.4 Results
Tables 1- 2 show the overall performance of the
baseline and ours (denoted by SRN ). It can be
seen that, our method yields a significantly higher
F1 (with p < 0.01) than SBR, and a moderate im-
provement of accuracy as compared with SBN (with
p < 0.05). As a case study, we show that our system
successfully identified ?jaxon11? as a PERSON in the
tweet ?? ? ? come to see jaxon11 someday? ? ? ?, which
is mistakenly labeled as a LOCATION by SBR.
This is largely owing to the fact that our system
aligns ?jaxon11? with ?Jaxson12? in the tweet ?? ? ? I
love Jaxson12,Hes like my little brother? ? ? ?, in which
?Jaxson12? is identified as a PERSON. As a result,
this encourages our system to consider ?jaxon11? as
a PERSON. We also find cases where our system
works but SBN fails. For example, ?Goldman11?
in the tweet ?? ? ?Goldman sees massive upside risk
in oil prices? ? ? ? is normalized into ?Albert Gold-
man? by SBR, because it is mistakenly identified as
a PERSON by SBS ; in contrast, our system recog-
nizes ?Goldman12 Sachs? as an ORGANIZATION,
and successfully links ?Goldman12? to ?Goldman11?,
resulting that ?Goldman11? is identified as an ORGA-
NIZATION and normalized into ?Goldman Sachs?.
Table 3 reports the NER performance of our
method for each entity type, from which we see that
our system consistently yields better F1 on all entity
types than SBR. We also see that our system boosts
the F1 for ORGANIZATION most significantly, re-
flecting the fact that a large number of organizations
that are incorrectly labeled as PERSON by SBR, are
now correctly recognized by our method.
532
System Pre Rec F1
SRN 84.7 82.5 83.6
SBR 81.6 78.8 80.2
Table 1: Overall performance (%) of NER.
System Accuracy
SRN 82.6
SBN 79.4
Table 2: Overall Accuracy (%) of NEN .
System PER PRO LOC ORG
SRN 84.2 80.5 82.1 85.2
SBR 83.9 78.7 81.3 79.8
Table 3: F1 (%) of NER on different entity types.
Features NER (F1) NEN (Accuracy)
Fo 59.2 61.3
Fo + Fl 65.8 68.7
Fo + Fg 80.1 77.2
Fo + Fl + Fg 83.6 82.6
Table 4: Overall F1 (%) of NER and Accuracy (%) of
NEN with different feature sets.
Table 4 shows the overall performance of our
method with various feature set combinations,
where Fo, Fl and Fg denote the orthographic fea-
tures, the lexical features, and the gazetteer-related
features, respectively. From Table 4 we see that
gazetteer-related features significantly boost the F1
for NER and Accuracy for NEN, suggesting the im-
portance of external knowledge for this task.
5.5 Discussion
One main error source for NER and NEN, which
accounts for more than half of all the errors, is
slang expressions and informal abbreviations. For
instance, our method recognizes ?California11? in
the tweet ?? ? ?And Now, He Lives All The Way In
California11? ? ? ? as a LOCATION, however, it mis-
takenly identifies ?Cali12? in the tweet ?? ? ? i love
Cali so much? ? ? ? as a PERSON. One reason is our
system does not generate any z-serial variable for
?California11? and ?Cali12? since they have differ-
ent lemmas. A more complicated case is ?BS11? in
the tweet ?? ? ? I, bobby shaw, am gonna put BS11 on
everything? ? ? ?, in which ?BS11? is the abbreviation
of ?bobby shaw?. Our method fails to recognize
?BS11? as an entity. There are two possible ways to
fix these errors: 1) Extending the scope of z-serial
variables to each word pairs with a common prefix;
and 2) developing advanced normalization compo-
nents to restore such slang expressions and informal
abbreviations into their canonical forms.
Our method does not directly exploit Wikipedia
for NEN. This explains the cases where our system
correctly links multiple entity mentions but fails to
generate canonical forms. Take the following two
tweets for example: ?? ? ? nitip link win711 sp1? ? ? ?
and ?? ? ?Hit the 3TB wall on SRT installing fresh
Win712? ? ? ?. Our system recognizes ?win711? and
?Win712? as two mentions of the same product, but
cannot output their canonical forms ?Windows 7?.
One possible solution is to exploit Wikipedia to
compile a dictionary consisting of entities and their
variations.
6 Conclusions and Future work
We study the task of NEN for tweets, a new genre
of texts that are short and prone to noise. Two chal-
lenges of this task are the dearth of information in
a single tweet and errors propagated from the NER
component. We propose jointly conducting NER
and NEN for multiple tweets using a factor graph, to
address these challenges. One unique characteristic
of our model is that a NE normalization variable is
introduced to indicate whether a word pair belongs
to the mentions of the same entity. We evaluate our
method on a manually annotated data set. Experi-
mental results show our method yields better F1 for
NER and Accuracy for NEN than the state-of-the-art
baseline that conducts two tasks sequentially.
In the future, we plan to explore two directions to
improve our method. First, we are going to develop
advanced tweet normalization technologies to re-
solve slang expressions and informal abbreviations.
Second, we are interested in incorporating knowl-
edge mined from Wikipedia into our factor graph.
Acknowledgments
We thank Yunbo Cao, Dongdong Zhang, and Mu Li
for helpful discussions, and the anonymous review-
ers for their valuable comments.
533
References
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP, pages
1002?1012.
Aaron Cohen. 2005. Unsupervised gene/protein named
entity normalization using automatically extracted dic-
tionaries. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontologies and
Databases: Mining Biological Semantics, pages 17?
24, Detroit, June. Association for Computational Lin-
guistics.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In In Proc. 2007
Joint Conference on EMNLP and CNLL, pages 708?
716.
Hong-Jie Dai, Richard Tzong-Han Tsai, and Wen-Lian
Hsu. 2011. Entity disambiguation using a markov-
logic network. In Proceedings of 5th International
Joint Conference on Natural Language Processing,
pages 846?855, Chiang Mai, Thailand, November.
Asian Federation of Natural Language Processing.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating Complex Named Entities in Web Text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In CSLDAMT, pages 80?88.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In EMNLP, pages
141?150.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Association for Computational Linguistics, pages
364?372.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
ACL HLT.
Martin Jansche and Steven P. Abney. 2002. Informa-
tion extraction from voicemail transcripts. In EMNLP,
pages 320?327.
Valentin Jijkoun, Mahboob Alam Khalid, Maarten Marx,
and Maarten de Rijke. 2008. Named entity normal-
ization in user generated content. In Proceedings of
the second workshop on Analytics for noisy unstruc-
tured text data, AND ?08, pages 23?30, New York,
NY, USA. ACM.
Mahboob Khalid, Valentin Jijkoun, and Maarten de Ri-
jke. 2008. The impact of named entity normaliza-
tion on information retrieval for question answering.
In Craig Macdonald, Iadh Ounis, Vassilis Plachouras,
Ian Ruthven, and Ryen White, editors, Advances in In-
formation Retrieval, volume 4956 of Lecture Notes in
Computer Science, pages 705?710. Springer Berlin /
Heidelberg.
George R. Krupka and Kevin Hausman. 1998. Isoquest:
Description of the netowlTM extractor system as used
in muc-7. In MUC-7.
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li.
2002. Location normalization for information extrac-
tion. In COLING.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In ACL.
Walid Magdy, Kareem Darwish, Ossama Emam, and
Hany Hassan. 2007. Arabic cross-document person
name normalization. In In CASL Workshop 07, pages
25?32.
Andrew Mccallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In HLT-NAACL, pages 188?191.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In HLT,
pages 443?450.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In In Proceedings of Un-
certainty in AI, pages 467?475.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Linguisti-
cae Investigationes, 30:3?26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147?155.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1524?1534, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In HLT-NAACL, pages 73?81.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL, pages 142?147.
534
Yefeng Wang. 2009. Annotating and recognising named
entities in clinical notes. In ACL-IJCNLP, pages 18?
26.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In BioNLP,
pages 209?216.
535
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572?581,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-Lingual Mixture Model for Sentiment Classification
Xinfan Meng? ?Furu Wei? Xiaohua Liu? Ming Zhou? Ge Xu? Houfeng Wang?
?MOE Key Lab of Computational Linguistics, Peking University
?Microsoft Research Asia
?{mxf, xuge, wanghf}@pku.edu.cn
?{fuwei,xiaoliu,mingzhou}@microsoft.com
Abstract
The amount of labeled sentiment data in En-
glish is much larger than that in other lan-
guages. Such a disproportion arouse interest
in cross-lingual sentiment classification, which
aims to conduct sentiment classification in the
target language (e.g. Chinese) using labeled
data in the source language (e.g. English).
Most existing work relies on machine trans-
lation engines to directly adapt labeled data
from the source language to the target lan-
guage. This approach suffers from the limited
coverage of vocabulary in the machine transla-
tion results. In this paper, we propose a gen-
erative cross-lingual mixture model (CLMM)
to leverage unlabeled bilingual parallel data.
By fitting parameters to maximize the likeli-
hood of the bilingual parallel data, the pro-
posed model learns previously unseen senti-
ment words from the large bilingual parallel
data and improves vocabulary coverage signifi-
cantly. Experiments on multiple data sets show
that CLMM is consistently effective in two set-
tings: (1) labeled data in the target language are
unavailable; and (2) labeled data in the target
language are also available.
1 Introduction
Sentiment Analysis (also known as opinion min-
ing), which aims to extract the sentiment informa-
tion from text, has attracted extensive attention in
recent years. Sentiment classification, the task of
determining the sentiment orientation (positive, neg-
ative or neutral) of text, has been the most exten-
sively studied task in sentiment analysis. There is
?Contribution during internship atMicrosoft ResearchAsia.
already a large amount of work on sentiment classi-
fication of text in various genres and in many lan-
guages. For example, Pang et al (2002) focus on
sentiment classification of movie reviews in English,
and Zagibalov and Carroll (2008) study the problem
of classifying product reviews in Chinese. During
the past few years, NTCIR1 organized several pi-
lot tasks for sentiment classification of news articles
written in English, Chinese and Japanese (Seki et
al., 2007; Seki et al, 2008).
For English sentiment classification, there are sev-
eral labeled corpora available (Hu and Liu, 2004;
Pang et al, 2002; Wiebe et al, 2005). However, la-
beled resources in other languages are often insuf-
ficient or even unavailable. Therefore, it is desir-
able to use the English labeled data to improve senti-
ment classification of documents in other languages.
One direct approach to leveraging the labeled data
in English is to use machine translation engines as a
black box to translate the labeled data from English
to the target language (e.g. Chinese), and then us-
ing the translated training data directly for the devel-
opment of the sentiment classifier in the target lan-
guage (Wan, 2009; Pan et al, 2011).
Although the machine-translation-based methods
are intuitive, they have certain limitations. First,
the vocabulary covered by the translated labeled
data is limited, hence many sentiment indicative
words can not be learned from the translated labeled
data. Duh et al (2011) report low overlapping
between vocabulary of natural English documents
and the vocabulary of documents translated to En-
glish from Japanese, and the experiments of Duh
1http://research.nii.ac.jp/ntcir/index-en.html
572
et al (2011) show that vocabulary coverage has a
strong correlation with sentiment classification ac-
curacy. Second, machine translation may change the
sentiment polarity of the original text. For exam-
ple, the negative English sentence ?It is too good to
be true? is translated to a positive sentence in Chi-
nese ?????????? by Google Translate
(http://translate.google.com/), which literally means
?It is good and true?.
In this paper we propose a cross-lingual mixture
model (CLMM) for cross-lingual sentiment classifi-
cation. Instead of relying on the unreliable machine
translated labeled data, CLMM leverages bilingual
parallel data to bridge the language gap between the
source language and the target language. CLMM is
a generative model that treats the source language
and target language words in parallel data as gener-
ated simultaneously by a set of mixture components.
By ?synchronizing? the generation of words in the
source language and the target language in a parallel
corpus, the proposed model can (1) improve vocabu-
lary coverage by learning sentiment words from the
unlabeled parallel corpus; (2) transfer polarity label
information between the source language and target
language using a parallel corpus. Besides, CLMM
can improve the accuracy of cross-lingual sentiment
classification consistently regardless of whether la-
beled data in the target language are present or not.
We evaluate the model on sentiment classification
of Chinese using English labeled data. The exper-
iment results show that CLMM yields 71% in accu-
racy when no Chinese labeled data are used, which
significantly improves Chinese sentiment classifica-
tion and is superior to the SVMand co-training based
methods. When Chinese labeled data are employed,
CLMMyields 83% in accuracy, which is remarkably
better than the SVM and achieve state-of-the-art per-
formance.
This paper makes two contributions: (1) we pro-
pose a model to effectively leverage large bilin-
gual parallel data for improving vocabulary cover-
age; and (2) the proposed model is applicable in both
settings of cross-lingual sentiment classification, ir-
respective of the availability of labeled data in the
target language.
The paper is organized as follows. We review re-
lated work in Section 2, and present the cross-lingual
mixture model in Section 3. Then we present the ex-
perimental studies in Section 4, and finally conclude
the paper and outline the future plan in Section 5.
2 Related Work
In this section, we present a brief review of the re-
lated work on monolingual sentiment classification
and cross-lingual sentiment classification.
2.1 Sentiment Classification
Early work of sentiment classification focuses on
English product reviews or movie reviews (Pang et
al., 2002; Turney, 2002; Hu and Liu, 2004). Since
then, sentiment classification has been investigated
in various domains and different languages (Zag-
ibalov and Carroll, 2008; Seki et al, 2007; Seki et
al., 2008; Davidov et al, 2010). There exist two
main approaches to extracting sentiment orientation
automatically. The Dictionary-based approach (Tur-
ney, 2002; Taboada et al, 2011) aims to aggregate
the sentiment orientation of a sentence (or docu-
ment) from the sentiment orientations of words or
phrases found in the sentence (or document), while
the corpus-based approach (Pang et al, 2002) treats
the sentiment orientation detection as a conventional
classification task and focuses on building classifier
from a set of sentences (or documents) labeled with
sentiment orientations.
Dictionary-based methods involve in creating or
using sentiment lexicons. Turney (2002) derives
sentiment scores for phrases by measuring the mu-
tual information between the given phrase and the
words ?excellent? and ?poor?, and then uses the av-
erage scores of the phrases in a document as the
sentiment of the document. Corpus-based meth-
ods are often built upon machine learning mod-
els. Pang et al (2002) compare the performance
of three commonly used machine learning models
(Naive Bayes, Maximum Entropy and SVM). Ga-
mon (2004) shows that introducing deeper linguistic
features into SVM can help to improve the perfor-
mance. The interested readers are referred to (Pang
and Lee, 2008) for a comprehensive review of senti-
ment classification.
2.2 Cross-Lingual Sentiment Classification
Cross-lingual sentiment classification, which aims
to conduct sentiment classification in the target lan-
guage (e.g. Chinese) with labeled data in the source
573
language (e.g. English), has been extensively stud-
ied in the very recent years. The basic idea is to ex-
plore the abundant labeled sentiment data in source
language to alleviate the shortage of labeled data in
the target language.
Most existing work relies on machine translation
engines to directly adapt labeled data from the source
language to target language. Wan (2009) proposes
to use ensemble method to train better Chinese sen-
timent classification model on English labeled data
and their Chinese translation. English Labeled data
are first translated to Chinese, and then two SVM
classifiers are trained on English andChinese labeled
data respectively. After that, co-training (Blum and
Mitchell, 1998) approach is adopted to leverage Chi-
nese unlabeled data and their English translation to
improve the SVM classifier for Chinese sentiment
classification. The same idea is used in (Wan, 2008),
but the ensemble techniques used are various vot-
ing methods and the individual classifiers used are
dictionary-based classifiers.
Instead of ensemblemethods, Pan et al (2011) use
matrix factorization formulation. They extend Non-
negative Matrix Tri-Factorization model (Li et al,
2009) to bilingual view setting. Their bilingual view
is also constructed by using machine translation en-
gines to translate original documents. Prettenhofer
and Stein (2011) use machine translation engines in
a different way. They generalize Structural Corre-
spondence Learning (Blitzer et al, 2006) to multi-
lingual setting. Instead of using machine translation
engines to translate labeled text, the authors use it to
construct the word translation oracle for pivot words
translation.
Lu et al (2011) focus on the task of jointly im-
proving the performance of sentiment classification
on two languages (e.g. English and Chinese) . the
authors use an unlabeled parallel corpus instead of
machine translation engines. They assume paral-
lel sentences in the corpus should have the same
sentiment polarity. Besides, they assume labeled
data in both language are available. They propose
a method of training two classifiers based on maxi-
mum entropy formulation to maximize their predic-
tion agreement on the parallel corpus. However, this
method requires labeled data in both the source lan-
guage and the target language, which are not always
readily available.
3 Cross-Lingual Mixture Model for
Sentiment Classification
In this section we present the cross-lingual mix-
ture model (CLMM) for sentiment classification.
We first formalize the task of cross-lingual sentiment
classification. Then we describe the CLMM model
and present the parameter estimation algorithm for
CLMM.
3.1 Cross-lingual Sentiment Classification
Formally, the task we are concerned about is to de-
velop a sentiment classifier for the target language T
(e.g. Chinese), given labeled sentiment data DS in
the source language S (e.g. English), unlabeled par-
allel corpus U of the source language and the target
language, and optional labeled dataDT in target lan-
guage T . Aligning with previous work (Wan, 2008;
Wan, 2009), we only consider binary sentiment clas-
sification scheme (positive or negative) in this paper,
but the proposed method can be used in other classi-
fication schemes with minor modifications.
3.2 The Cross-Lingual Mixture Model
The basic idea underlying CLMM is to enlarge
the vocabulary by learning sentiment words from the
parallel corpus. CLMM defines an intuitive genera-
tion process as follows. Suppose we are going to
generate a positive or negative Chinese sentence, we
have two ways of generating words. The first way
is to directly generate a Chinese word according to
the polarity of the sentence. The other way is to first
generate an English word with the same polarity and
meaning, and then translate it to a Chinese word.
More formally, CLMM defines a generative mix-
ture model for generating a parallel corpus. The un-
observed polarities of the unlabeled parallel corpus
are modeled as hidden variables, and the observed
words in parallel corpus are modeled as generated by
a set of words generation distributions conditioned
on the hidden variables. Given a parallel corpus, we
fit CLMM model by maximizing the likelihood of
generating this parallel corpus. By maximizing the
likelihood, CLMM can estimate words generation
probabilities for words unseen in the labeled data but
present in the parallel corpus, hence expand the vo-
cabulary. In addition, CLMM can utilize words in
both the source language and target language for de-
574
termining polarity classes of the parallel sentences.
POS 
NEG 
POS 
NEG 
...? 
Source 
Target 
U 
u wt 
ws 
Figure 1: The generation process of the
cross-lingual mixture model
Figure 1 illustrates the detailed process of gener-
ating words in the source language and target lan-
guage respectively for the parallel corpus U , from
the four mixture components in CLMM. Particu-
larly, for each pair of parallel sentences ui ? U , we
generate the words as follows.
1. Document class generation: Generating the
polarity class.
(a) Generating a polarity class cs from a
Bernoulli distribution Ps(C).
(b) Generating a polarity class ct from a
Bernoulli distribution Pt(C)
2. Words generation: Generating the words
(a) Generating source language wordsws from
a Multinomial distribution P (ws|cs)
(b) Generating target language words wt from
a Multinomial distribution P (wt|ct)
3. Words projection: Projecting the words onto
the other language
(a) Projecting the source language wordsws to
target language words wt by word projec-
tion probability P (wt|ws)
(b) Projecting the target language words wt to
source language words ws by word projec-
tion probability P (ws|wt)
CLMM finds parameters by using MLE (Maxi-
mum Likelihood Estimation). The parameters to be
estimated include conditional probabilities of word
to class, P (ws|c) and P (wt|c), and word projection
probabilities, P (ws|wt) and P (wt|ws). We will de-
scribe the log-likelihood function and then show how
to estimate the parameters in subsection 3.3. The
obtained word-class conditional probability P (wt|c)
can then be used to classify text in the target lan-
guages using Bayes Theorem and the Naive Bayes
independence assumption.
Formally, we have the following log-likelihood
function for a parallel corpus U2.
L(?|U) =
|Us|
?
i=1
|C|
?
j=1
|Vs|
?
s=1
[
Nsi log
(
P (ws|cj) + P (ws|wt)P (wt|cj)
)]
+
|Ut|
?
i=1
|C|
?
j=1
|Vt|
?
t=1
[
Nti log
(
P (wt|cj) + P (wt|ws)P (ws|cj)
)]
(1)
where ? is the model parameters;Nsi (Nti) is the oc-
currences of thewordws (wt) in document di; |Ds| is
the number of documents; |C| is the number of class
labels; Vs and Vt are the vocabulary in the source lan-
guage and the vocabulary in the target language.|Us|
and |Ut| are the number of unlabeled sentences in the
source language and target language.
Meanwhile, we have the following log-likelihood
function for labeled data in the source language Ds.
L(?|Ds) =
|Ds|
?
i=1
|C|
?
j=1
|Vs|
?
s=1
Nsi logP (ws|cj)?ij (2)
where ?ij = 1 if the label of di is cj , and 0 otherwise.
In addition, when labeled data in the target lan-
guage is available, we have the following log-
likelihood function.
L(?|Dt) =
|Dt|
?
i=1
|C|
?
j=1
|Vt|
?
t=1
Nti logP (wt|cj)?ij (3)
Combining the above three likelihood functions
together, we have the following likelihood function.
L(?|Dt, Ds, U) = L(?|U) + L(?|Ds) + L(?|Dt)
(4)
Note that the third term on the right hand side
(L(?|Dt)) is optional.
2For simplicity, we assume the prior distribution P (C) is
uniform and drop it from the formulas.
575
3.3 Parameter Estimation
Instead of estimating word projection probability
(P (ws|wt) and P (wt|ws)) and conditional proba-
bility of word to class (P (wt|c) and P (ws|c)) si-
multaneously in the training procedure, we estimate
them separately since the word projection probabil-
ity stays invariant when estimating other parame-
ters. We estimate word projection probability using
word alignment probability generated by the Berke-
ley aligner (Liang et al, 2006). The word align-
ment probabilities serves two purposes. First, they
connect the corresponding words between the source
language and the target language. Second, they ad-
just the strength of influences between the corre-
sponding words. Figure 2 gives an example of word
alignment probability. As is shown, the three words
?tour de force? altogether express a positive mean-
ing, while in Chinese the same meaning is expressed
with only one word ???? (masterpiece). CLMM
use word alignment probability to decrease the in-
fluences from ???? (masterpiece) to ?tour?, ?de?
and ?force? individually, using the word projection
probability (i.e. word alignment probability), which
is 0.3 in this case.
Herman Melville's Moby Dick was a tour de force.  
 
??? ???? ? ?????? ?? ??? 
1  1  .5  .5  1  1  .3  . 3  . 3  
Figure 2: Word Alignment Probability
We use Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977) to estimate the con-
ditional probability of word ws and wt given class
c, P (ws|c) and P (wt|c) respectively. We derive the
equations for EM algorithm, using notations similar
to (Nigam et al, 2000).
In the E-step, the distribution of hidden variables
(i.e. class label for unlabeled parallel sentences) is
computed according to the following equations.
P (cj |usi) = Z(cusi = cj) =
?
ws?usi [P (ws|cj) +
?
P (ws|wt)>0 P (ws|wt)P (wt|cj)]
?
cj
?
ws?usi [P (ws|cj) +
?
P (ws|wt)>0 P (ws|wt)P (wt|cj)]
(5)
P (cj |uti) = Z(cuti = cj) =
?
wt?uti [P (wt|cj) +
?
P (wt|ws)>0 P (wt|ws)P (ws|cj)]
?
cj
?
wt?uti [P (wt|cj) +
?
P (wt|ws)>0 P (wt|ws)P (ws|cj)]
(6)
whereZ(cusi = cj)
(
Z(cuti) = cj
)
is the probability
of the source (target) language sentence usi (uti) in
the i-th pair of sentences ui having class label cj .
In the M-step, the parameters are computed by the
following equations.
P (ws|cj) =
1 +
?|Ds|
i=1 ?s(i)NsiP (cj |di)
|V | +
?|Vs|
s=1 ?(i)NsiP (cj |di)
(7)
P (wt|cj) =
1 +
?|Dt|
i=1 ?t(i)NtiP (cj |di)
|V | +
?|Vt|
t=1 ?(i)NtiP (cj |di)
(8)
where ?s(i) and ?t(i) are weighting factor to con-
trol the influence of the unlabeled data. We set ?s(i)
(
?t(i)
)
to ?s
(
?t
)
when di belongs to unlabeled
data, 1 otherwise. When di belongs to labeled data,
P (cj |di) is 1 when its label is cj and 0 otherwise.
When di belongs to unlabeled data, P (cj |di) is com-
puted according to Equation 5 or 6.
4 Experiment
4.1 Experiment Setup and Data Sets
Experiment setup: We conduct experiments on
two common cross-lingual sentiment classification
settings. In the first setting, no labeled data in the
target language are available. This setting has real-
istic significance, since in some situations we need to
quickly develop a sentiment classifier for languages
that we do not have labeled data in hand. In this
case, we classify text in the target language using
only labeled data in the source language. In the sec-
ond setting, labeled data in the target language are
also available. In this case, a more reasonable strat-
egy is to make full use of both labeled data in the
source language and target language to develop the
sentiment classifier for the target language. In our
experiments, we consider English as the source lan-
guage and Chinese as the target language.
Data sets: For Chinese sentiment classification,
we use the same data set described in (Lu et al,
2011). The labeled data sets consist of two English
data sets and one Chinese data set. The English data
set is from the Multi-Perspective Question Answer-
ing (MPQA) corpus (Wiebe et al, 2005) and the NT-
CIR Opinion Analysis Pilot Task data set (Seki et
al., 2008; Seki et al, 2007). The Chinese data set
also comes from the NTCIR Opinion Analysis Pi-
lot Task data set. The unlabeled parallel sentences
576
are selected from ISI Chinese-English parallel cor-
pus (Munteanu and Marcu, 2005). Following the
description in (Lu et al, 2011), we remove neutral
sentences and keep only high confident positive and
negative sentences as predicted by a maximum en-
tropy classifier trained on the labeled data. Table 1
shows the statistics for the data sets used in the ex-
periments. We conduct experiments on two data set-
tings: (1) MPQA + NTCIR-CH and (2) NTCIR-EN
+ NTCIR-CH.
MPQA NTCIR-EN NTCIR-CH
Positive 1,471(30%) 528 (30%) 2,378 (55%)
Negative 3,487(70%) 1,209(70%) 1,916(44%)
Total 4,958 1,737 4,294
Table 1: Statistics about the Data
CLMM includes two hyper-parameters (?s and
?t) controlling the contribution of unlabeled parallel
data. Larger weights indicate larger influence from
the unlabeled data. We set the hyper-parameters
by conducting cross validations on the labeled data.
WhenChinese labeled data are unavailable, we set?t
to 1 and ?s to 0.1, since no Chinese labeled data are
used and the contribution of target language to the
source language is limited. When Chinese labeled
data are available, we set ?s and ?t to 0.2.
To prevent long sentences from dominating the pa-
rameter estimation, we preprocess the data set by
normalizing the length of all sentences to the same
constant (Nigam et al, 2000), the average length of
the sentences.
4.2 Baseline Methods
For the purpose of comparison, we implement the
following baseline methods.
MT-SVM:We translate the English labeled data to
Chinese using Google Translate and use the transla-
tion results to train the SVM classifier for Chinese.
SVM: We train a SVM classifier on the Chinese
labeled data.
MT-Cotrain: This is the co-training based ap-
proach described in (Wan, 2009). We summarize
the main steps as follows. First, two monolingual
SVM classifiers are trained on English labeled data
and Chinese data translated from English labeled
data. Second, the two classifiers make prediction on
Chinese unlabeled data and their English translation,
respectively. Third, the 100 most confidently pre-
dicted English and Chinese sentences are added to
the training set and the twomonolingual SVMclassi-
fiers are re-trained on the expanded training set. The
second and the third steps are repeated for 100 times
to obtain the final classifiers.
Para-Cotrain: The training process is the same as
MT-Cotrain. However, we use a different set of En-
glish unlabeled sentences. Instead of using the corre-
sponding machine translation of Chinese unlabeled
sentences, we use the parallel English sentences of
the Chinese unlabeled sentences.
Joint-Train: This is the state-of-the-art method de-
scribed in (Lu et al, 2011). This model use En-
glish labeled data and Chinese labeled data to obtain
initial parameters for two maximum entropy clas-
sifiers (for English documents and Chinese docu-
ments), and then conduct EM-iterations to update
the parameters to gradually improve the agreement
of the two monolingual classifiers on the unlabeled
parallel sentences.
4.3 Classification Using Only English Labeled
Data
The first set of experiments are conducted on us-
ing only English labeled data to create the sentiment
classifier for Chinese. This is a challenging task,
since we do not use any Chinese labeled data. And
MPQA and NTCIR data sets are compiled by differ-
ent groups using different annotation guidelines.
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM N/A N/A
MT-Cotrain 65.13 59.11
Para-Cotrain 67.21 60.71
Joint-Train N/A N/A
CLMM 70.96 71.52
Table 2: Classification Accuracy Using Only
English Labeled Data
Table 2 shows the accuracy of the baseline sys-
tems as well as the proposed model (CLMM). As
is shown, sentiment classification does not bene-
fit much from the direct machine translation. For
NTCIR-EN+NTCIR-CH, the accuracy of MT-SVM
577
is only 62.34%. For MPQA-EN+NTCIR-CH, the
accuracy is 54.33%, even lower than a trivial
method, which achieves 55.4% by predicting all sen-
tences to be positive. The underlying reason is that
the vocabulary coverage in machine translated data
is low, therefore the classifier learned from the la-
beled data is unable to generalize well on the test
data. Meanwhile, the accuracy of MT-SVM on
NTCIR-EN+NTCIR-CH data set is much better than
that on MPQA+NTCIR-CH data set. That is be-
cause NTCIR-EN and NTCIR-CH cover similar top-
ics. The other two methods using machine translated
data, MT-Cotrain and Para-Cotrain also do not per-
form verywell. This result is reasonable, because the
initial Chinese classifier trained on machine trans-
lated data (MT-SVM) is relatively weak. We also
observe that using a parallel corpus instead of ma-
chine translations can improve classification accu-
racy. It should be noted that we do not have the result
for Joint-Train model in this setting, since it requires
both English labeled data and Chinese labeled data.
4.4 Classification Using English and Chinese
Labeled Data
The second set of experiments are conducted on
using both English labeled data and Chinese labeled
data to develop the Chinese sentiment classifier. We
conduct 5-fold cross validations on Chinese labeled
data. We use the same baseline methods as described
in Section 4.2, but we use natural Chinese sentences
instead of translated Chinese sentences as labeled
data in MT-Cotrain and Para-Cotrain. Table 3 shows
the accuracy of baseline systems as well as CLMM.
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM 80.58 80.58
MT-Cotrain 82.28 80.93
Para-Cotrain 82.35 82.18
Joint-Train 83.11 83.42
CLMM 82.73 83.02
Table 3: Classification Accuracy Using English and
Chinese Labeled Data
As is seen, SVMperforms significantly better than
MT-SVM. One reason is that we use natural Chi-
nese labeled data instead of translated Chinese la-
beled data. Another reason is that we use 5-fold
cross validations in this setting, while the previous
setting is an open test setting. In this setting, SVM
is a strong baseline with 80.6% accuracy. Never-
theless, all three methods which leverage an unla-
beled parallel corpus, namely Para-Cotrain, Joint-
Train and CLMM, still show big improvements over
the SVM baseline. Their results are comparable and
all achieve state-of-the-art accuracy of about 83%,
but in terms of training speed, CLMM is the fastest
method (Table 4). Similar to the previous setting,We
also have the same observation that using a parallel
corpus is better than using translations.
Method Iterations Total Time
Para-Cotrain 100 6 hours
Joint-Train 10 55 seconds
CLMM 10 30 seconds
Table 4: Training Speed Comparison
4.5 The Influence of Unlabeled Parallel Data
We investigate how the size of the unlabeled par-
allel data affects the sentiment classification in this
subsection. We vary the number of sentences in the
unlabeled parallel from 2,000 to 20,000. We use
only English labeled data in this experiment, since
this more directly reflects the effectiveness of each
model in utilizing unlabeled parallel data. From Fig-
ure 3 and Figure 4, we can see that when more unla-
beled parallel data are added, the accuracy of CLMM
consistently improves. The performance of CLMM
is remarkably superior than Para-Cotrain and MT-
Cotrain. When we have 10,000 parallel sentences,
the accuracy of CLMM on the two data sets quickly
increases to 68.77% and 68.91%, respectively. By
contrast, we observe that the performance of Para-
Cotrain and MT-Cotrain is able to obtain accuracy
improvement only after about 10,000 sentences are
added. The reason is that the two methods use ma-
chine translated labeled data to create initial Chinese
classifiers. As is depicted in Table 2, these classifiers
are relatively weak. As a result, in the initial itera-
tions of co-training based methods, the predictions
made by the Chinese classifiers are inaccurate, and
co-training based methods need to see more parallel
578
Number of Sentences
Accur
acy
62
64
66
68
70
l
l l l
l l
l l l l
5000 10000 15000 20000
Modell CLMM MT?Cotrain Para?Cotrain
Figure 3: Accuracy with different size of
unlabeled data for NTICR-EN+NTCIR-CH
Number of Sentences
Accur
acy
55
60
65
70
l
l
l
l l
l l l l l
5000 10000 15000 20000
Modell CLMM MT?Cotrain Para?Cotrain
Figure 4: Accuracy with different size of
unlabeled data for MPQA+NTCIR-CH
Number of Sentences
Accur
acy
65
70
75
80
l l
l l
l l
l
500 1000 1500 2000 2500 3000 3500
Modell CLMM Joint?Train Para?Cotrain SVM
Figure 5: Accuracy with different size of
labeled data for NTCIR-EN+NTCIR-CH
Number of Sentences
Accur
acy
65
70
75
80
l l
l l
l l
l
500 1000 1500 2000 2500 3000 3500
Modell CLMM Joint?Train Para?Cotrain SVM
Figure 6: Accuracy with different size of
labeled data for MPQA+NTCIR-CH
sentences to refine the initial classifiers.
4.6 The Influence of Chinese Labeled Data
In this subsection, we investigate how the size of
the Chinese labeled data affects the sentiment classi-
fication. As is shown in Figure 5 and Figure 6, when
only 500 labeled sentences are used, CLMM is capa-
ble of achieving 72.52% and 74.48% in accuracy on
the two data sets, obtaining 10% and 8% improve-
ments over the SVM baseline, respectively. This
indicates that our method leverages the unlabeled
data effectively. When more sentences are used,
CLMM consistently shows further improvement in
accuracy. Para-Cotrain and Joint-Train show simi-
lar trends. When 3500 labeled sentences are used,
SVM achieves 80.58%, a relatively high accuracy
for sentiment classification. However, CLMM and
the other two models can still gain improvements.
This further demonstrates the advantages of expand-
ing vocabulary using bilingual parallel data.
5 Conclusion and Future Work
In this paper, we propose a cross-lingual mix-
ture model (CLMM) to tackle the problem of cross-
lingual sentiment classification. This method has
two advantages over the existing methods. First, the
proposed model can learn previously unseen senti-
ment words from large unlabeled data, which are not
covered by the limited vocabulary in machine trans-
lation of the labeled data. Second, CLMM can ef-
fectively utilize unlabeled parallel data regardless of
whether labeled data in the target language are used
or not. Extensive experiments suggest that CLMM
consistently improve classification accuracy in both
settings. In the future, we will work on leverag-
ing parallel sentences and word alignments for other
tasks in sentiment analysis, such as building multi-
lingual sentiment lexicons.
Acknowledgment We thank Bin Lu and Lei Wang for
their help. This research was partly supported by National High
Technology Research and Development Program of China (863
Program) (No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009, No.60973053)
579
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, page 120?128.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, page 92?100.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
page 241?249.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), page 1?38.
Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for Cross-Lingual sentiment
classification? In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, page 429?433,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. InProceedings of the
20th international conference on Computational Lin-
guistics, page 841.
Mingqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge
discovery and data mining, page 168?177.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to sentiment
classification with lexical prior knowledge. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, page 244?252, Suntec, Singapore, August.
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, page 104?111.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies-
Volume 1, page 320?330.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine learning, 39(2):103?134.
Junfeng Pan, Gui-Rong Xue, Yong Yu, and Yang Wang.
2011. Cross-lingual sentiment classification via bi-
view non-negative matrix tri-factorization. Advances
in Knowledge Discovery and Data Mining, page
289?300.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, page 79?86.
Peter Prettenhofer and Benno Stein. 2011. Cross-lingual
adaptation using structural correspondence learning.
ACM Transactions on Intelligent Systems and Technol-
ogy (TIST), 3(1):13.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of opinion analysis pilot task at NTCIR-6.
In Proceedings of NTCIR-6 Workshop Meeting, page
265?278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin.
2008. Overview of multilingual opinion analysis task
at NTCIR-7. In Proc. of the Seventh NTCIR Workshop.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based meth-
ods for sentiment analysis. Comput. Linguist., page to
appear.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics,
page 417?424.
Xiaojun Wan. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised chinese sentiment
analysis. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
?08, page 553?561, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Xiaojun Wan. 2009. Co-training for cross-lingual senti-
ment classification. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
580
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume 1,
page 235?243.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation,
39(2):165?210.
Taras Zagibalov and John Carroll. 2008. Automatic seed
word selection for unsupervised sentiment classifica-
tion of chinese text. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, page 1073?1080.
581
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 13?18,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
QuickView: NLP-based Tweet Search
Xiaohua Liu ? ?, Furu Wei ?, Ming Zhou ?, Microsoft QuickView Team ?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?Microsoft Research Asia
Beijing, 100190, China
?{xiaoliu, fuwei, mingzhou,qv}@microsoft.com
Abstract
Tweets have become a comprehensive repos-
itory for real-time information. However, it
is often hard for users to quickly get informa-
tion they are interested in from tweets, ow-
ing to the sheer volume of tweets as well as
their noisy and informal nature. We present
QuickView, an NLP-based tweet search plat-
form to tackle this issue. Specifically, it ex-
ploits a series of natural language process-
ing technologies, such as tweet normalization,
named entity recognition, semantic role label-
ing, sentiment analysis, tweet classification, to
extract useful information, i.e., named entities,
events, opinions, etc., from a large volume
of tweets. Then, non-noisy tweets, together
with the mined information, are indexed, on
top of which two brand new scenarios are en-
abled, i.e., categorized browsing and advanced
search, allowing users to effectively access
either the tweets or fine-grained information
they are interested in.
1 Introduction
Tweets represent a comprehensive fresh informa-
tion repository. However, users often have diffi-
culty finding information they are interested in from
tweets, because of the huge number of tweets as well
as their noisy and informal nature. Tweet search,
e.g., Twitter 1, is a kind of service aiming to tackle
this issue. Nevertheless, existing tweet search ser-
vices provide limited functionality. For example, in
Twitter, only a simple keyword-based search is sup-
1http://twitter.com/
ported, and the returned list often contains meaning-
less results.
This demonstration introduces QuickView, which
employs a series of NLP technologies to extract
useful information from a large volume of tweets.
Specifically, for each tweet, it first conducts nor-
malization, followed by named entity recognition
(NER). Then it conducts semantic role labeling
(SRL) to get predicate-argument structures, which
are further converted into events, i.e., triples of who
did what. After that, it performs sentiment analysis
(SA), i.e., extracting positive or negative comments
about something/somebody. Next, tweets are clas-
sified into predefined categories. Finally, non-noisy
tweets together with the mined information are in-
dexed.
On top of the index, QuickView enables two brand
new scenarios, allowing users to effectively access
the tweets or fine-grained information mined from
tweets.
Categorized Browsing. As illustrated in Figure
1(a), QuickView shows recent popular tweets, enti-
ties, events, opinions and so on, which are organized
by categories. It also extracts and classifies URL
links in tweets and allows users to check out popular
links in a categorized way.
Advanced Search. As shown in Figure 1(b), Quick-
View provides four advanced search functions: 1)
search results are clustered so that tweets about the
same/similar topic are grouped together, and for
each cluster only the informative tweets are kept;
2) when the query refers to a person or a company,
two bars are presented followed by the words that
strongly suggest opinion polarity. The bar?s width
13
is proportional to the number of associated opin-
ions; 3) similarly, the top six most frequent words
that most clearly express event occurrences are pre-
sented; 4) users can search tweets with opinions
or events, e.g., search tweets containing any posi-
tive/negative opinion about ?Obama? or any event
involving ?Obama?.
The implementation of QuickView requires adapt-
ing existing NLP components trained on formal
texts, which often performs poorly on tweets. For
example, the average F1 of the Stanford NER
(Finkel et al, 2005) drops from 90.8% (Ratinov
and Roth, 2009) to 45.8% on tweets, while Liu et
al. (2010) report that the F1 score of a state-of-
the-art SRL system (Meza-Ruiz and Riedel, 2009)
falls to 42.5% on tweets as apposed to 75.5% on
news. However, the adaptation of those components
is challenging, owing to the lack of annotated tweets
and the inadequate signals provided by a noisy and
short tweet. Our general strategy is to leverage ex-
isting resources as well as unsupervised or semi-
supervised learning methods to reduce the labeling
efforts, and to aggregate as much evidence as pos-
sible from a broader context to compensate for the
lack of information in a tweet.
This strategy is embodied by various components
we have developed. For example, our NER com-
ponent combines a k-nearest neighbors (KNN) clas-
sifier, which collects global information across re-
cently labeled tweets with a Conditional Random
Fields (CRF) labeler, which exploits information
from a single tweet and the gazetteers. Both the
KNN classifier and the CRF labeler are repeatedly
retrained using the results that they have confidently
labeled. The SRL component caches and clusters
recent labeled tweets, and aggregates information
from the cluster containing the tweet. Similarly, the
classifier considers not only the current tweet but
also its neighbors in a tweet graph, where two tweets
are connected if they are similar in content or have a
tweet/retweet relationship.
QuickView has been internally deployed, and re-
ceived extremely positive feedback. Experimental
results on a human annotated dataset alo indicate
the effectiveness of our adaptation strategy.
Our contributions are summarized as follows.
1. We demonstrate QuickView, an NLP-based
tweet search. Different from existing methods,
it exploits a series of NLP technologies to ex-
tract useful information from a large volume
of tweets, and enables categorized browsing
and advanced search scenarios, allowing users
to efficiently access information they are inter-
ested in from tweets.
2. We present core components of QuickView, fo-
cusing on how to leverage existing resources
and technologies as well as how to make up
for the limited information in a short and often
noisy tweet by aggregating information from a
broader context.
The rest of this paper is organized as follows. In
the next section, we introduce related work. In Sec-
tion 3, we describe our system. In Section 4, we
evaluate our system. Finally, Section 5 concludes
and presents future work.
2 Related Work
Information Extraction Systems. Essentially,
QuickView is an information extraction (IE) system.
However, unlike existing IE systems, such as Evita
(Saur?? et al, 2005), a robust event recognizer for QA
system, and SRES (Rozenfeld and Feldman, 2008),
a self-supervised relation extractor for the web, it
targets tweets, a new genre of text, which are short
and informal, and its focus is on adapting existing IE
components to tweets.
Tweet Search Services. A couple of tweet search
services exist, including Twitter, Bing social search
2 and Google social search 3. Most of them provide
only keyword-based search interfaces, i.e., return-
ing a list of tweets related to a given word/phrase.
In contrast, our system extracts fine-grained in-
formation from tweets and allows a new end-to-
end search experience beyond keyword search, such
as clustering of search results, and search with
events/opinions.
NLP Components. The NLP technologies adopted
in our system , e.g., NER, SRL and classification,
have been extensively studied on formal text but
rarely on tweets. At the heart of our system is
the re-use of existing resources, methodologies as
2http://www.bing.com/social
3http://www.google.com/realtime
14
(a) A screenshot of the categorized browsing scenario.
(b) A screenshot of the advanced search scenario.
Figure 1: Two scenarios of QuickView.
well as components, and the the adaptation of them
to tweets. The adaptation process, though varying
across components, consists of three common steps:
1) annotating tweets; 2) defining the decision con-
text that usually involves more than one tweet, such
as a cluster of similar tweets; and 3) re-training mod-
els (often incrementally) with both conventional fea-
tures and features derived from the context defined
in step 2.
3 System Description
We first give an overview of our system, then present
more details about NER and SRL, as two represen-
tative core components, to illustrate the adaptation
process.
3.1 Overview
Architecture. QuickView can be divided into four
parts, as illustrated in Figure 2. The first part in-
cludes a crawler and a buffer of raw tweets. The
crawler repeatedly downloads tweets using the Twit-
ter APIs, and then pre-filters noisy tweets using
some heuristic rules, e.g., removing a tweet if it is
too short, say, less than 3 words, or if it contains
any predefined banned word. At the moment, we
focus on English tweets, so non-English tweets are
filtered as well. Finally, the un-filtered are put into
the buffer.
The second part consists of several tweet extrac-
tion pipelines. Each pipeline has the same configura-
tion, constantly fetching a tweet from the raw tweet
buffer, and conducting the following processes se-
15
Figure 2: System architecture of QuickView.
quentially: 1) normalization; 2) parsing including
part-of-speech (POS), chunking, and dependency
parsing; 3) NER; 4) SRL; 5) SA and 6) classifica-
tion. The normalization model identifies and cor-
rects ill-formed words. For example, after normal-
ization, ?loooove? in ?? ? ? I loooove my icon? ? ? ?
will be transformed to ?love?. A phrase-based trans-
lation system without re-ordering is used to imple-
ment this model. The translation table includes man-
ually compiled ill/good form pairs, and the language
model is a trigram trained on LDC data 4 using
SRILM (Stolcke, 2002). The OpenNLP 5 toolkit
is directly used to implement the parsing model.
In future, the parsing model will be re-trained us-
ing annotated tweets. The SA component is imple-
mented according to Jiang et al (2011), which incor-
porates target-dependent features and considers re-
lated tweets by utilizing a graph-based optimization.
The classification model is a KNN-based classifier
that caches confidently labeled results to re-train it-
self, which also recognizes and drops noisy tweets.
4http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?cata-
logId=LDC2005T12
5http://sourceforge.net/projects/opennlp/
Each processed tweet, if not identified as noise, is
put into a shared buffer for indexing.
The third part is responsible for indexing and
querying. It constantly takes from the indexing
buffer a processed tweet, which is then indexed with
various entries including words, phrases, metadata
(e.g., source, publish time, and account), named en-
tities, events, and opinions. On top of this, it answers
any search request, and returns a list of matched re-
sults, each of which contains both the original tweet
and the extracted information from that tweet. We
implement an indexing/querying engine similar to
Lucene 6 in C#. This part also maintains a cache of
recent processed tweets, from which the following
information is extracted and indexed: 1) top tweets;
2) top entities/events/opinions in tweets; and 3)
top accounts. Whether a tweet/entity/event/opinion
ranks top depends on their re-tweeted/mentioned
times as well as its publisher, while whether an ac-
count is top relies on the number of his/her followers
and tweets.
The fourth part is a web application that returns
related information to end users according to their
browsing or search request. The implementation of
the web application is organized with the model-
view-control pattern so that other kinds of user in-
terfaces, e.g., a mobile application, can be easily im-
plemented.
Deployment. QuickView is deployed into 5 work-
stations 7 including 2 processing pipelines, as illus-
trated in Table 1. The communication between com-
ponents is through TCP/IP. On average, it takes 0.01
seconds to process each tweet, and in total about
10 million tweets are indexed every day. Note that
QuickView?s processing capability can be enhanced
in a straightforward manner by deploying additional
pipelines.
3.2 Core Components
Because of limited space, we only discuss two core
components of QuickView: NER and SRL.
NER. NER is the task of identifying mentions of
rigid designators from text belonging to named-
entity types such as persons, organizations and loca-
tions. Existing solutions fall into three categories: 1)
6http://lucene.apache.org/java/docs/index.html
7Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM,
OS of Windows Server 2003 EnterpriseX64 version
16
Table 1: Current deployment of QuickView.
Workstation Hosted components
#1 Crawler,Raw tweet buffer
#2, 3 Process pipeline
#4 Indexing Buffer, Indexer/Querier
#5 Web application
the rule-based (Krupka and Hausman, 1998); 2) the
machine learning based (Finkel and Manning, 2009;
Singh et al, 2010); and 3) hybrid methods (Jansche
and Abney, 2002). With the availability of annotated
corpora, such as ACE05, Enron and CoNLL03, the
data-driven methods become the dominating meth-
ods. However, because of domain mismatch, cur-
rent systems trained on non-tweets perform poorly
on tweets.
Our NER system takes three steps to address
this problem. Firstly, it defines those recently la-
beled tweets that are similar to the current tweet
as its recognition context, under which a KNN-
based classifier is used to conduct word level clas-
sification. Following the two-stage prediction ag-
gregation methods (Krishnan and Manning, 2006),
such pre-labeled results, together with other con-
ventional features used by the state-of-the-art NER
systems, are fed into a linear CRF models, which
conducts fine-grained tweet level NER. Secondly,
the KNN and CRF model are repeatedly retrained
with an incrementally augmented training set, into
which highly confidently labeled tweets are added.
Finally, following Lev Ratinov and Dan Roth
(2009), 30 gazetteers are used, which cover common
names, countries, locations, temporal expressions,
etc. These gazetteers represent general knowledge
across domains, and help to make up for the lack of
training data.
SRL. Given a sentence, the SRL component identi-
fies every predicate, and for each predicate further
identifies its arguments. This task has been exten-
sively studied on well-written corpora like news, and
a couple of solutions exist. Examples include: 1)
the pipelined approach, i.e., dividing the task into
several successive components such as argument
identification, argument classification, global infer-
ence, etc., and conquering them individually (Xue,
2004; Koomen et al, 2005); 2) sequentially labeling
based approach (Ma`rquez et al, 2005), i.e., label-
ing the words according to their positions relative
to an argument (i.e., inside, outside, or at the be-
ginning); and 3) Markov Logic Networks (MLN)
based approach (Meza-Ruiz and Riedel, 2009),
i.e., simultaneously resolving all the sub-tasks using
learnt weighted formulas. Unsurprisingly, the per-
formance of the state-of-the-art SRL system (Meza-
Ruiz and Riedel, 2009) drops sharply when applied
to tweets.
The SRL component of QuickView is based on
CRF, and uses the recently labeled tweets that are
similar to the current tweet as the broader context.
Algorithm 1 outlines its implementation, where:
train denotes a machine learning process to get a
labeler l, which in our work is a linear CRF model;
the cluster function puts the new tweet into a clus-
ter; the label function generates predicate-argument
structures for the input tweet with the help of the
trained model and the cluster; p, s and cf denote a
predicate, a set of argument and role pairs related to
the predicate and the predicted confidence, respec-
tively. To prepare the initial clusters required by the
SRL component as its input, we adopt the predicate-
argument mapping method (Liu et al, 2010) to
get some automatically labeled tweets, which (plus
the manually labeled tweets) are then organized into
groups using a bottom-up clustering procedure.
It is worth noting that: 1) our SRL component
uses the general role schema defined by PropBank,
which includes core roles such as A0, A1 (usually
indicating the agent and patient of the predicate, re-
spectively), and auxiliary roles such as AM-TMP
and AM-LOC (representing the temporal and loca-
tion information of the predicate, respectively); 2)
only verbal predicates are considered, which is con-
sistent with most existing SRL systems; and 3) fol-
lowing Ma`rquez et al (2005), it conducts word level
labeling.
4 Evaluation
Overall Performance. We provide a textbox in the
home page of QuickView to collect feedback. We
have got 165 feedbacks, of which 85.5% are posi-
tive. The main complaint is related to the quality of
the extracted information.
Core Components. We manually labeled the POS,
17
Algorithm 1 SRL of QuickView.
Require: Tweet stream i;clusters cl;output stream o.
1: Initialize l, the CRF labeler: l = train(cl).
2: while Pop a tweet t from i and t ?= null do
3: Put t to a cluster c: c = cluster(cl, t).
4: Label t with l:(t, {(p, s, cf)}) = label(l, c, t).
5: Update cluster c with labeled results
(t, {(p, s, cf)}).
6: Output labeled results (t, {(p, s, cf)}) to o.
7: end while
8: return o.
NER, SRL and SA information for about 10,000
tweets, based on which the NER and SRL com-
ponents are evaluated. Experimental results show
that: 1) our NER component achieves an average
F1 of 80.2%, as opposed to 75.4% of the baseline,
which is a CRF-based system similar to Ratinov and
Roth?s (2009) but re-trained on annotated tweets;
and 2) our SRL component gets an F1 of 59.7%, out-
performing both the state-of-the-art system (Meza-
Ruiz and Riedel, 2009) (42.5%) and the system of
Liu et al (2010) (42.3%), which is trained on au-
tomatically annotated news tweets (tweets reporting
news).
5 Conclusions and Future work
We have described the motivation, scenarios, archi-
tecture, deployment and implementation of Quick-
View, an NLP-based tweet search. At the heart of
QuickView is the adaptation of existing NLP tech-
nologies, e.g., NER, SRL and SA, to tweets, a new
genre of text, which are short and informal. We
have illustrated our strategy to tackle this challeng-
ing task, i.e., leveraging existing resources and ag-
gregating as much information as possible from a
broader context, using NER and SRL as case stud-
ies. Preliminary positive feedback suggests the use-
fulness of QuickView and its advantages over exist-
ing tweet search services. Experimental results on
a human annotated dataset indicate the effectiveness
of our adaptation strategy.
We are improving the quality of the core compo-
nents of QuickView by labeling more tweets and ex-
ploring alternative models. We are also customizing
QuickView for non-English tweets. As it progresses,
we will release QuickView to the public.
References
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In EMNLP, pages
141?150.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL, pages 363?370.
Martin Jansche and Steven P. Abney. 2002. Informa-
tion extraction from voicemail transcripts. In EMNLP,
pages 320?327.
Long Jiang, Mo Yu, Ming Zhou, and Xiaohua Liu. 2011.
Target-dependent twitter sentiment classification. In
ACL.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-
tau Yih. 2005. Generalized inference with multi-
ple semantic role labeling systems. In CONLL, pages
181?184.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In ACL, pages
1121?1128.
George R. Krupka and Kevin Hausman. 1998. Isoquest:
Description of the netowlTM extractor system as used
in muc-7. In MUC-7.
Xiaohua Liu, Kuan Li, Bo Han, Ming Zhou, Long Jiang,
Zhongyang Xiong, and Changning Huang. 2010. Se-
mantic role labeling for news tweets. In Coling, pages
698?706.
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez, and Neus
Catala`. 2005. Semantic role labeling as sequential
tagging. In CONLL, pages 193?196.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In NAACL, pages 155?163.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147?155.
Benjamin Rozenfeld and Ronen Feldman. 2008. Self-
supervised relation extraction from the web. Knowl.
Inf. Syst., 17:17?33, October.
Roser Saur??, Robert Knippen, Marc Verhagen, and James
Pustejovsky. 2005. Evita: A robust event recognizer
for qa systems. In EMNLP, pages 700?707.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In HLT-NAACL, pages 73?81.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In ICSLP, volume 2, pages 901?904.
Nianwen Xue. 2004. Calibrating features for seman-
tic role labeling. In In Proceedings of EMNLP 2004,
pages 88?94.
18
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1304?1311,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Entity Linking for Tweets
Xiaohua Liu?, Yitong Li?, Haocheng Wu?, Ming Zhou?, Furu Wei?, Yi Lu?
?Microsoft Research Asia, Beijing, 100190, China
?School of Electronic and Information Engineering
Beihang University, Beijing, 100191, China
?University of Science and Technology of China
No. 96, Jinzhai Road, Hefei, Anhui, China
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?{xiaoliu, mingzhou, fuwei}@microsoft.com
?tong91222@126.com ?v-haowu@microsoft.com ?v-y@microsoft.com
Abstract
We study the task of entity linking for
tweets, which tries to associate each
mention in a tweet with a knowledge base
entry. Two main challenges of this task are
the dearth of information in a single tweet
and the rich entity mention variations.
To address these challenges, we propose
a collective inference method that
simultaneously resolves a set of mentions.
Particularly, our model integrates three
kinds of similarities, i.e., mention-entry
similarity, entry-entry similarity, and
mention-mention similarity, to enrich
the context for entity linking, and to
address irregular mentions that are not
covered by the entity-variation dictionary.
We evaluate our method on a publicly
available data set and demonstrate the
effectiveness of our method.
1 Introduction
Twitter is a widely used social networking service.
With millions of active users and hundreds of
millions of new published tweets every day1,
it has become a popular platform to capture
and transmit the human experiences of the
moment. Many tweet related researches are
inspired, from named entity recognition (Liu et al,
2012), topic detection (Mathioudakis and Koudas,
2010), clustering (Rosa et al, 2010), to event
extraction (Grinev et al, 2009).
In this work, we study the entity linking task
for tweets, which maps each entity mention in
a tweet to a unique entity, i.e., an entry ID
of a knowledge base like Wikipedia. Entity
1http://siteanalytics.compete.com/twitter.com/
linking task is generally considered as a bridge
between unstructured text and structured machine-
readable knowledge base, and represents a critical
role in machine reading program (Singh et al,
2011). Entity linking for tweets is particularly
meaningful, considering that tweets are often hard
to read owing to its informal written style and
length limitation of 140 characters.
Current entity linking methods are built on top
of a large scale knowledge base such asWikipedia.
A knowledge base consists of a set of entities,
and each entity can have a variation list2. To
decide which entity should be mapped, they may
compute: 1) the similarity between the context of
a mention, e.g., a text window around the mention,
and the content of an entity, e.g., the entity page of
Wikipedia (Mihalcea and Csomai, 2007; Han and
Zhao, 2009); 2) the coherence among the mapped
entities for a set of related mentions, e.g, multiple
mentions in a document (Milne and Witten, 2008;
Kulkarni et al, 2009; Han and Zhao, 2010; Han et
al., 2011).
Tweets pose special challenges to entity linking.
First, a tweet is often too concise and too
noisy to provide enough information for similarity
computing, owing to its short and grass root
nature. Second, tweets have rich variations of
named entities3, and many of them fall out of
the scope of the existing dictionaries mined from
Wikipedia (called OOV mentions hereafter). On
2Entity variation lists can be extracted from the
entity resolution pages of Wikipedia. For example, the
link ?http://en.wikipedia.org/wiki/Svm? will lead us to a
resolution page, where ?Svm? are linked to entities like
?Space vector modulation? and ?Support vector machine?.
As a result, ?Svm? will be added into the variation lists of
?Space vector modulation? and ?Support vector machine? ,
respectively.
3According to Liu et al (2012), on average a named entity
has 3.3 different surface forms in tweets.
1304
the other hand, the huge redundancy in tweets
offers opportunities. That means, an entity
mention often occurs in many tweets, which
allows us to aggregate all related tweets to
compute mention-mention similarity and mention-
entity similarity.
We propose a collective inference method
that leverages tweet redundancy to address those
two challenges. Given a set of mentions, our
model tries to ensure that similar mentions are
linked to similar entities while pursuing the
high total similarity between matched mention-
entity pairs. More specifically, we define
local features, including context similarity and
edit distance, to model the similarity between
a mention and an entity. We adopt in-link
based similarity (Milne and Witten, 2008), to
measure the similarity between entities. Finally,
we introduce a set of features to compute
the similarity between mentions, including how
similar the tweets containing the mentions are,
whether they come from the tweets of the same
account, and their edit distance. Notably, our
model can resolve OOV mentions with the help
of their similar mentions. For example, for the
OOVmention ?LukeBryanOnline?, our model can
find similar mentions like ?TheLukeBryan? and
?LukeBryan?. Considering that most of its similar
mentions are mapped to the American country
singer ?Luke Bryan?, our model tends to link
?LukeBryanOnline? to the same entity.
We evaluate our method on the public available
data set shared by Meij et al (2012)4.
Experimental results show that our method
outperforms two baselines, i.e., Wikify! (Mihalcea
and Csomai, 2007) and system proposed by Meij
et al (2012). We also study the effectiveness
of features related to each kind of similarity, and
demonstrate the advantage of our method for OOV
mention linkage.
We summarize our contributions as follows.
1. We introduce a novel collective inference
method that integrates three kinds of
similarities, i.e., mention-entity similarity,
entity-entity similarity, and mention-mention
similarity, to simultaneously map a set of
tweet mentions to their proper entities.
2. We propose modeling the mention-mention
similarity and demonstrate its effectiveness
4http://ilps.science.uva.nl/resources/wsdm2012-adding-
semantics-to-microblog-posts/
in entity linking for tweets, particularly for
OOV mentions.
3. We evaluate our method on a public data
set, and show our method compares favorably
with the baselines.
Our paper is organized as follows. In the next
section, we introduce related work. In Section
3, we give the formal definition of the task. In
Section 4, we present our solution, including
the framework, features related to different kinds
of similarities, and the training and decoding
procedures. We evaluate our method in Section 5.
Finally in Section 6, we conclude with suggestions
of future work.
2 Related Work
Existing entity linking work can roughly be
divided into two categories. Methods of the
first category resolve one mention at each time,
and mainly consider the similarity between
a mention-entity pair. In contrast, methods
of the second category take a set of related
mentions (e.g., mentions in the same document)
as input, and figure out their corresponding entities
simultaneously.
Examples of the first category include the first
Web-scale entity linking system SemTag (Dill
et al, 2003), Wikify! (Mihalcea and Csomai,
2007), and the recent work of Milne and Witten
(2008). SemTag uses the TAP knowledge
base5, and employs the cosine similarity with
TF-IDF weighting scheme to compute the
match degree between a mention and an entity,
achieving an accuracy of around 82%. Wikify!
identifies the important concepts in the text
and automatically links these concepts to the
corresponding Wikipedia pages. It introduces two
approaches to define mention-entity similarity,
i.e., the contextual overlap between the paragraph
where the mention occurs and the corresponding
Wikipedia pages, and a Naive Bayes classifier
that predicts whether a mention should be linked
to an entity. It achieves 80.69% F1 when two
approaches are combined. Milne and Witten
work on the same task of Wikify!, and also
train a classifier. However, they cleverly use the
5TAB (http://www.w3.org/2002/05/tap/) is a shallow
knowledge base that contains a broad range of lexical and
taxonomic information about popular objects like music,
movies, authors, sports, autos, health, etc.
1305
links found within Wikipedia articles for training,
exploiting the fact that for every link, aWikipedian
has manually selected the correct destination to
represent the intended sense of the anchor. Their
method achieves an F1 score of 75.0%.
Representative studies of the second category
include the work of Kulkarni et al (2009),
Han et al (2011), and Shen et al (2012).
One common feature of these studies is that
they leverage the global coherence between
entities. Kulkarni et al (2009) propose
a graphical model that explicitly models the
combination of evidence from local mention-
entity compatibility and global document-level
topical coherence of the entities, and show that
considering global coherence between entities
significantly improves the performance. Han et
al. (2011) introduce a graph-based representation,
called Referent Graph, to model the global
interdependence between different entity linking
decisions, and jointly infer the referent entities of
all name mentions in a document by exploiting
the interdependence captured in Referent Graph.
Shen et al (2012) propose LIEGE, a framework
to link the entities in web lists with the knowledge
base, with the assumption that entities mentioned
in a Web list tend to be a collection of entities of
the same conceptual type.
Most work of entity linking focuses on web
pages. Recently, Meij et al (2012) study
this task for tweets. They propose a machine
learning based approach using n-gram features,
concept features, and tweet features, to identify
concepts semantically related to a tweet, and
for every entity mention to generate links to its
corresponding Wikipedia article. Their method
belongs to the first category, in the sense that
they only consider the similarity between mention
(tweet) and entity (Wikipedia article).
Our method belongs to the second category.
However, in contrast with existing collective
approaches, our method works on tweets which
are short and often noisy. Furthermore, our
method is based on the ?similar mention with
similar entity? assumption, and explicitly models
and integrates the mention similarity into the
optimization framework. Compared with Meij et
al. (2012), our method is collective, and integrates
more features.
3 Task Definition
Given a sequence of mentions, denoted by
M? = (m1,m2, ? ? ? ,mn), our task is to
output a sequence of entities, denoted by
E? = (e1, e2, ? ? ? , en), where ei is the entity
corresponding to mi. Here, an entity refers
to an item of a knowledge base. Following
most existing work, we use Wikipedia as the
knowledge base, and an entity is a definition page
in Wikipedia; a mention denotes a sequence of
tokens in a tweet that can be potentially linked to
an entity.
Several notes should be made. First, we
assume that mentions are given, e.g., identified by
some named entity recognition system. Second,
mentions may come from multiple tweets. Third,
mentions with the same token sequence may
refer to different entities, depending on mention
context. Finally, we assume each entity e has
a variation list6, and a unique ID through which
all related information about that entity can be
accessed.
Here is an example to illustrate the task. Given
mentions ?nbcbightlynews?, ?Santiago?, ?WH?
and ?Libya? from the following tweet ?Chuck
Todd: Prepping for @nbcnightlynews here in
Santiago, reporting on WH handling of Libya
situation.?, the expected output is ?NBC Nightly
News(194735)?, ?Santiago Chile(51572)?,
?White House(33057)? and ?Libya(17633)?,
where the numbers in the parentheses are the IDs
of the corresponding entities.
4 Our Method
In this section, we first present the framework of
our entity linking method. Then we introduce
features related to different kinds of similarities,
followed by a detailed discussion of the training
and decoding procedures.
4.1 Framework
Given the input mention sequence M? =
(m1,m2, ? ? ? ,mn), our method outputs the entity
sequence E?? = (e?1, e?2, ? ? ? , e?n) according to
Formula 1:
6For example, the variation list of the entity ?Obama? may
contain ?Barack Obama?, ?Barack Hussein Obama II?, etc.
1306
E?? = argmax?E??C(M?)?
n?
i=1
w? ? f?(ei,mi)
+(1 ? ?)
?
i ?=j
r(ei, ej)s(mi,mj)
(1)
Where:
? C(M?) is the set of all possible entity
sequences for the mention sequence M? ;
? E? denotes an entity sequence instance,
consisting of e1, e2, ? ? ? , en;
? f?(ei,mi) is the feature vector that models the
similarity between mention mi and its linked
entity ei;
? w? is the feature weight vector related to f? ,
which is trained on the training data set; w? ?
f?(ei,mi) is the similarity between mention
mi and entity ei;
? r(ei, ej) is the function that returns the
similarity between two entities ei and ej ;
? s(mi,mj) is the function that returns the
similarity between two mentions mi and mj ;
? ? ? (0, 1) is a systematic parameter, which
is determined on the development data set; it
is used to adjust the tradeoff between local
compatibility and global consistence. It is
experimentally set to 0.8 in our work.
From Formula 1, we can see that: 1) our
method considers the mention-entity similarly,
entity-entity similarity and mention-mention
similarity. Mention-entity similarly is used to
model local compatibility, while entity-entity
similarity and mention-mention similarity
combined are to model global consistence; and 2)
our method prefers configurations where similar
mentions have similar entities and with high local
compatibility.
C(M?) is worth of more discussion here.
It represents the search space, which can be
generated using the entity variation list. To
achieve this, we first build an inverted index
of all entity variation lists, with each unique
variation as an entry pointing to a list of entities.
Then for any mention m, we look up the index,
and get al possible entities, denoted by C(m).
In this way, given a mention sequence M? =
(m1,m2, ? ? ? ,mn), we can enumerate all possible
entity sequence E? = (e1, e2, ? ? ? , en), where ei ?
C(m). This means |C(M?)| = ?m?M |C(m)| ,
which is often large. There is one special case:
if m is an OOV mention, i.e., |C(m)| = 0, then
|C(M?)| = 0, and we get no solution. To address
this problem, we can generate a list of candidates
for an OOV mention using its similar mentions.
Let S(m) denote OOV mention m?s similar
mentions, we define C(m) = ?m??S(m) C(m
?).
If still C(m) = 0, we remove m from M? , and
report we cannot map it to any entity.
Here is an example to illustrate our framework.
Suppose we have the following tweets:
? UserA: Yeaaahhgg #habemusfut..
I love monday night futbol =)
#EnglishPremierLeague ManU vs
Liverpool1
? UserA: Manchester United 3 - Liverpool2
2 #EnglishPremierLeague GLORY, GLORY,
MAN.UNITED!
? ? ? ?
Figure 1: An illustrative example to show our
framework. Ovals in orange and in blue represent
mentions and entities, respectively. Each mention
pair, entity pair, and mention entity pair have
a similarity score represented by s, r and f ,
respectively.
We need find out the best entity sequence
E?? for mentions M? = { ?Liverpool1?,
?Manchester United?, ?ManU?, ?Liverpool2?},
from the entity sequences C(M?) = { (Liverpool
(film), Manchester United F.C., Manchester
United F.C., Liverpool (film)), ? ? ? , (Liverpool,
F.C.,Manchester United, F.C., Manchester United
F.C., Liverpool (film) }. Figure 1 illustrate
our solution, where ?Liverpool1? (on the left)
and ?Liverpool2? (on the right) are linked
1307
to ?Liverpool F.C.? (the football club), and
?Manchester United? and ?ManU? are linked to
?Manchester United F.C.?. Notably, ?ManU?
is an OOV mention, but has a similar mention
?Manchester United?, with which ?ManU? is
successfully mapped.
4.2 Features
We group features into three categories: local
features related to mention-entity similarity
(f?(e,m)), features related to entity-entity
similarity (r(ei, ej)) , and features related to
mention-mention similarity (s(mi,mj)).
4.2.1 Local Features
? Prior Probability:
f1(mi, ei) =
count(ei)?
?ek?C(mi) count(ek)
(2)
where count(e) denotes the frequency of
entity e in Wikipedia?s anchor texts.
? Context Similarity:
f2(mi, ei) =
coocurence number
tweet length (3)
where: coccurence number is the the
number of the words that occur in both the
tweet containing mi and the Wikipedia page
of ei; tweet length denotes the number of
tokens of the tweet containing mention mi.
? Edit Distance Similarity:
IfLength(mi)+ED(mi, ei) = Length(ei),
f3(mi, ei) = 1, otherwise 0. ED(?, ?)
computes the character level edit distance.
This feature helps to detect whether
a mention is an abbreviation of its
corresponding entity7.
? Mention Contains Title: If the mention
contains the entity title, namely the title of
the Wikipedia page introducing the entity ei,
f4(mi, ei) = 1, else 0.
? Title Contains Mention: If the entry title
contains the mention, f5(mi, ei) = 1,
otherwise 0.
7Take ?ms? and ?Microsoft? for example. The length of
?ms? is 2, and the edit distance between them is 7. 2 plus 7
equals to 9, which is the length of ?Microsoft?.
4.2.2 Features Related to Entity Similarity
There are two representative definitions of entity
similarity: in-link based similarity (Milne and
Witten, 2008) and category based similarity (Shen
et al, 2012). Considering that the Wikipedia
categories are often noisy (Milne and Witten,
2008), we adopt in-link based similarity, as
defined in Formula 4:
r(ei, ej) =
log|g(ei) ? g(ej)| ? log max(|g(ei)|, |g(ej)|)
log(Total)? log min(|g(ei)|, |g(ej)|)
(4)
Where:
? Total is the total number of knowledge base
entities;
? g(e) is the number of Wikipedia definition
pages that have a link to entity e.
4.2.3 Features Related to Mention Similarity
We define 5 features to model the similarity
between two mentions mi and mj , as listed
below, where t(m) denotes the tweet that contains
mention m:
? s1(mi,mj): The cosine similarity of t(mi)
and t(mj); and tweets are represented as TF-
IDF vectors;
? s2(mi,mj): The cosine similarity of t(mi)
and t(mj); and tweets are represented as
topic distribution vectors;
? s3(mi,mj): Whether t(mi) and t(mj) are
published by the same account;
? s4(mi,mj): Whether t(mi) and t(mj)
contain any common hash tag;
? s5(mi,mj): Edit distance related similarity
between mi and mj , as defined in Formula 5.
s5(mi,mj) = 1, if min{Length(mi), Length(mj)}
+ED(mi,mj) = max{Length(mi), Length(mj)},
else s5(mi,mj) = 1 ? ED(mi,mj)max{Length(mi), Length(mj)}
(5)
Note that: 1) before computing TF-IDF vectors,
stop words are removed; 2) we use the Stanford
Topic Modeling Toolbox8 to compute the topic
model, and experimentally set the number of
topics to 50.
8http://nlp.stanford.edu/software/tmt/tmt-0.4/
1308
Finally, Formula 6 is used to integrate all the
features. a? = (a1, a2, a3, a4, a5) is the feature
weight vector for mention similarity, where ak ?
(0, 1), k = 1, 2, 3, 4, 5, and?5k=1 ak = 1.
s(mi,mj) =
5?
k=1
aksk(mi,mj) (6)
4.3 Training and Decoding
Given n mentions m1,m2, ? ? ? ,mn and their
corresponding entities e1, e2, ? ? ? , en, the goal of
training is to determine: w??, the weights of local
features, and a??, the weights of the features related
to mention similarity, according to Formula 7 9.
(w??, a??) = arg minw?,?a{
1
n
n?
i=1
L1(ei,mi)
+?1||w?||2 +
?2
2
n?
i,j=1
s(mi,mj)L2(?a, ei, ej)}
(7)
Where:
? L1 is the loss function related to local
compatibility, which is defined as
1
w??f?(ei,mi)+1
;
? L2(?a, ei, ej) is the loss function related
to global coherence, which is defined as
1
r(ei,ej)
?5
k=1 aksk(mi,mj)+1
;
? ?1 is the weight of regularization, which is
experimentally set to 1.0;
? ?2 is the weight of L2 loss, which is
experimentally set to 0.2.
Since the decoding problem defined by
Formula 1 is NP hard (Kulkarni et al, 2009), we
develop a greedy hill-climbing approach to tackle
this challenge, as demonstrated in Algorithm 1.
In Algorithm 1, it is the number of iterations;
Score(E?, M?) = ??ni=1 w? ? f?(ei,mi) + (1 ?
?)
?
i?=j r(ei, ej)s(mi,mj); E?ij is the vector after
replacing ei with ej ? C(mi) for current E?;
scij is the score of E?ij , i.e., Score(E?ij , M?). In
each iteration, this rounding solution iteratively
substitute entry ei in E? to increase the total score
cur. If the score cannot be further improved, it
stops and returns current E?.
9This optimization problem is non-convex. We use
coordinate descent to get a local optimal solution.
Algorithm 1 Decoding Algorithm.
Input: Mention Set M? = (m1,m2, ? ? ? ,mn)
Output: Entity Set E? = (e1, e2, ? ? ? , en)
1: for i = 1 to n do
2: Initialize e(0)i as the entity with the largest prior
probability given mention mi.
3: end for
4: cur = Score(E?(0), M?)
5: it = 1
6: while true do
7: for i = 1 to n do
8: for ej ? C(mi) do
9: if ej ?= e(it?1)i then
10: E?(it)ij = E?(it?1) ? {e(it?1)i } + {ej}.
11: end if
12: scij = Score(E?(it)ij , M?).
13: end for
14: end for
15: (l,m) = argmax(i,j)scij .
16: sc? = sclm
17: if sc? > cur then
18: cur = sc?.
19: E?(it) = E?(it?1) ? {e(it?1)l } + {em}.
20: it = it + 1.
21: else
22: break
23: end if
24: end while
25: return E?(it).
5 Experiments
In this section, we introduce the data set and
experimental settings, and present results.
5.1 Data Preparation
Following most existing studies, we choose
Wikipedia as our knowledge base10. We index
the Wikipedia definition pages, and prepare all
required prior knowledge, such as count(e), g(e),
and entity variation lists. We also build an inverted
index with about 60 million entries for the entity
variation lists.
For tweets, we use the data set shared by Meij et
al. (2012)11. This data set is annotated manually
by two volunteers. We get 502 annotated tweets
from this data set. We keep 55 of them for
10We download the December 2012 version of Wikipedia,
which contains about four million articles.
11http://ilps.science.uva.nl/resources/wsdm2012-adding-
semantics-to-microblog-posts/.
1309
development, and the remaining for 5 fold cross-
validation.
5.2 Settings
We consider following settings to evaluate our
method.
? Comparing our method with two baselines,
i.e., Wikify! (Mihalcea and Csomai, 2007)
and the system proposed byMeij et al (2012)
12;
? Using only local features;
? Using various mention similarity features;
? Experiments on OOV mentions.
5.3 Results
Table 1 reports the comparison results. Our
method outperforms both systems in terms of
all metrics. Since the main difference between
our method and the baselines is that our method
considers not only local features, but also global
features related to entity similarity and mention
similarity, these results indicate the effectiveness
of collective inference and global features. For
example, we find two baselines incorrectly link
?Nickelodeon? in the tweet ?BOH will make a
special appearance on Nickelodeon?s ?Yo Gabba
Gabba? tomorrow? to the theater instead of a TV
channel. In contrast, our method notices that ?Yo
Gabba Gabba? in the same tweet can be linked
to ?Yo Gabba Gabba (TV show)?, and thus it
correctly maps ?Nickelodeon? to ?Nickelodeon
(TV channel)?.
System Pre. Rec. F1
Wikify! 0.375 0.421 0.396
Meij?s Method 0.734 0.632 0.679
Our Method 0.752 0.675 0.711
Table 1: Comparison with Baselines.
Table 2 shows the results when local features
are incrementally added. It can be seen that:
1) using only Prior Probability feature already
yields a reasonable F1; and 2) Context Similarity
and Edit Distance Similarity feature have little
contribution to the F1, while Mention and Entity
Title Similarity feature greatly boosts the F1.
12We re-implement Wikify! since we use a new evaluation
data set.
Local Feature Pre. Rec. F1
P.P. 0.700 0.599 0.646
+C.S. 0.694 0.597 0.642
+E.D.S. 0.696 0.598 0.643
+M.E.T.S. 0.735 0.632 0.680
Table 2: Local Feature Analysis. P.P.,C.S., E.D.S.,
and M.E.T.S. denote Prior Probability, Context
Similarity, Edit Distance Similarity, and Mention
and Entity Title Similarity, respectively.
The performance of our method with various
mention similarity features is reported in Table 3.
First, we can see that with this kind of features,
the F1 can be significantly improved from 0.680
to 0.704. Second, we notice that TF-IDF (s1) and
Topic Model (s2) features perform equally well,
and combining all mention similarity features
yields the best performance.
Global Feature Pre. Rec. F1
s3+s4+s5 0.744 0.653 0.700
s3+s4+s5 +s1 0.759 0.652 0.702
s3+s4+s5+s2 0.760 0.653 0.703
s3+s4+s5+s1+s2 0.764 0.653 0.704
Table 3: Mention Similarity Feature Analysis.
For any OOV mention, we use the strategy
of guessing its possible entity candidates using
similar mentions, as discussed in Section 4.1.
Table 4 shows the performance of our system for
OOV mentions. It can be seen that with our
OOV strategy, the recall is improved from 0.653
to 0.675 (with p < 0.05) while the Precision is
slightly dropped and the overall F1 still gets better.
A further study reveals that among all the 125
OOV mentions, there are 48 for which our method
cannot find any entity; and nearly half of these
48 OOV mentions do have corresponding entities
13. This suggests that we may need enlarge the
size of variation lists or develop some mention
normalization techniques.
OOV Method Precision Recall F1
Ignore OOV Mention 0.764 0.653 0.704
+ OOV Method 0.752 0.675 0.711
Table 4: Performance for OOV Mentions.
13?NATO-ukraine cooperations? is such an example. It
is mapped to NULL but actually has a corresponding entity
?Ukraine-NATO relations?
1310
6 Conclusions and Future work
We have presented a collective inference method
that jointly links a set of tweet mentions to
their corresponding entities. One distinguished
characteristic of our method is that it integrates
mention-entity similarity, entity-entity similarity,
and mention-mention similarity, to address the
information lack in a tweet and rich OOV
mentions. We evaluate our method on a
public data set. Experimental results show our
method outperforms two baselines, and suggests
the effectiveness of modeling mention-mention
similarity, particularly for OOV mention linking.
In the future, we plan to explore two directions.
First, we are going to enlarge the size of entity
variation lists. Second, we want to integrate
the entity mention normalization techniques as
introduced by Liu et al (2012).
Acknowledgments
We thank the anonymous reviewers for their
valuable comments. We also thank all the
QuickView team members for the helpful
discussions.
References
S. Dill, N. Eiron, D. Gibson, D. Gruhl, and R. Guha.
2003. Semtag and seeker: bootstrapping the
semantic web via automated semantic annotation. In
Proceedings of the 12th international conference on
World Wide Web, WWW ?03, pages 178?186, New
York, NY, USA. ACM.
Maxim Grinev, Maria Grineva, Alexander Boldakov,
Leonid Novak, Andrey Syssoev, and Dmitry
Lizorkin. 2009. Sifting micro-blogging stream for
events of user interest. In Proceedings of the 32nd
international ACM SIGIR conference on Research
and development in information retrieval, SIGIR
?09, pages 837?837, New York, NY, USA. ACM.
Xianpei Han and Jun Zhao. 2009. Nlpr-kbp in tac 2009
kbp track: A two-stage method to entity linking. In
Proceedings of Test Analysis Conference.
Xianpei Han and Jun Zhao. 2010. Structural
semantic relatedness: a knowledge-based method
to named entity disambiguation. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: A graph-based method.
In SIGIR?11.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective
annotation of wikipedia entities in web text.
In Proceedings of the 15th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 457?465.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012. Joint inference
of named entity recognition and normalization for
tweets. In ACL (1), pages 526?535.
Michael Mathioudakis and Nick Koudas. 2010.
Twittermonitor: trend detection over the twitter
stream. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data,
SIGMOD ?10, pages 1155?1158, New York, NY,
USA. ACM.
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts.
In Proceedings of the fifth ACM international
conference on Web search and data mining.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge.
In Proceedings of the sixteenth ACM conference
on Conference on information and knowledge
management, CIKM ?07, pages 233?242, NewYork,
NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning
to link with wikipedia. In Proceeding of the 17th
ACM conference on Information and knowledge
management.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole
Gershman, and Robert Frederking. 2010. Topical
clustering of tweets. In SWSM?10.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Liege: Link entities in web lists with
knowledge base. In KDD?12.
Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-
scale cross-document coreference using distributed
inference and hierarchical models. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 793?
803, Stroudsburg, PA, USA. Association for
Computational Linguistics.
1311
