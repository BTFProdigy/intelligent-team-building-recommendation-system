Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 398?408,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Lexical surprisal as a general predictor of reading time
Irene Fernandez Monsalve, Stefan L. Frank and Gabriella Vigliocco
Division of Psychology and Language Sciences
University College London
{ucjtife, s.frank, g.vigliocco}@ucl.ac.uk
Abstract
Probabilistic accounts of language process-
ing can be psychologically tested by com-
paring word-reading times (RT) to the con-
ditional word probabilities estimated by
language models. Using surprisal as a link-
ing function, a significant correlation be-
tween unlexicalized surprisal and RT has
been reported (e.g., Demberg and Keller,
2008), but success using lexicalized models
has been limited. In this study, phrase struc-
ture grammars and recurrent neural net-
works estimated both lexicalized and unlex-
icalized surprisal for words of independent
sentences from narrative sources. These
same sentences were used as stimuli in
a self-paced reading experiment to obtain
RTs. The results show that lexicalized sur-
prisal according to both models is a signif-
icant predictor of RT, outperforming its un-
lexicalized counterparts.
1 Introduction
Context-sensitive, prediction-based processing
has been proposed as a fundamental mechanism
of cognition (Bar, 2007): Faced with the prob-
lem of responding in real-time to complex stim-
uli, the human brain would use basic information
from the environment, in conjunction with previ-
ous experience, in order to extract meaning and
anticipate the immediate future. Such a cognitive
style is a well-established finding in low level sen-
sory processing (e.g., Kveraga et al 2007), but
has also been proposed as a relevant mechanism
in higher order processes, such as language. In-
deed, there is ample evidence to show that human
language comprehension is both incremental and
predictive. For example, on-line detection of se-
mantic or syntactic anomalies can be observed in
the brain?s EEG signal (Hagoort et al 2004) and
eye gaze is directed in anticipation at depictions
of plausible sentence completions (Kamide et al
2003). Moreover, probabilistic accounts of lan-
guage processing have identified unpredictability
as a major cause of processing difficulty in lan-
guage comprehension. In such incremental pro-
cessing, parsing would entail a pre-allocation of
resources to expected interpretations, so that ef-
fort would be related to the suitability of such
an allocation to the actually encountered stimulus
(Levy, 2008).
Possible sentence interpretations can be con-
strained by both linguistic and extra-linguistic
context, but while the latter is difficult to evalu-
ate, the former can be easily modeled: The pre-
dictability of a word for the human parser can be
expressed as the conditional probability of a word
given the sentence so far, which can in turn be es-
timated by language models trained on text cor-
pora. These probabilistic accounts of language
processing difficulty can then be validated against
empirical data, by taking reading time (RT) on a
word as a measure of the effort involved in its pro-
cessing.
Recently, several studies have followed this ap-
proach, using ?surprisal? (see Section 1.1) as the
linking function between effort and predictabil-
ity. These can be computed for each word in a
text, or alternatively for the words? parts of speech
(POS). In the latter case, the obtained estimates
can give an indication of the importance of syn-
tactic structure in developing upcoming-word ex-
pectations, but ignore the rich lexical information
that is doubtlessly employed by the human parser
398
to constrain predictions. However, whereas such
an unlexicalized (i.e., POS-based) surprisal has
been shown to significantly predict RTs, success
with lexical (i.e., word-based) surprisal has been
limited. This can be attributed to data sparsity
(larger training corpora might be needed to pro-
vide accurate lexical surprisal than for the unlex-
icalized counterpart), or to the noise introduced
by participant?s world knowledge, inaccessible to
the models. The present study thus sets out to find
such a lexical surprisal effect, trying to overcome
possible limitations of previous research.
1.1 Surprisal theory
The concept of surprisal originated in the field of
information theory, as a measure of the amount of
information conveyed by a particular event. Im-
probable (?surprising?) events carry more infor-
mation than expected ones, so that surprisal is in-
versely related to probability, through a logarith-
mic function. In the context of sentence process-
ing, if w1, ..., wt?1 denotes the sentence so far,
then the cognitive effort required for processing
the next word, wt, is assumed to be proportional
to its surprisal:
effort(t) ? surprisal(wt)
= ? log(P (wt|w1, ..., wt?1)) (1)
Different theoretical groundings for this rela-
tionship have been proposed (Hale, 2001; Levy
2008; Smith and Levy, 2008). Smith and Levy
derive it by taking a scale free assumption: Any
linguistic unit can be subdivided into smaller en-
tities (e.g., a sentence is comprised of words, a
word of phonemes), so that time to process the
whole will equal the sum of processing times for
each part. Since the probability of the whole can
be expressed as the product of the probabilities of
the subunits, the function relating probability and
effort must be logarithmic. Levy (2008), on the
other hand, grounds surprisal in its information-
theoretical context, describing difficulty encoun-
tered in on-line sentence processing as a result of
the need to update a probability distribution over
possible parses, being directly proportional to the
difference between the previous and updated dis-
tributions. By expressing the difference between
these in terms of relative entropy, Levy shows that
difficulty at each newly encountered word should
be equal to its surprisal.
1.2 Empirical evidence for surprisal
The simplest statistical language models that can
be used to estimate surprisal values are n-gram
models or Markov chains, which condition the
probability of a given word only on its n? 1 pre-
ceding ones. Although Markov models theoret-
ically limit the amount of prior information that
is relevant for prediction of the next step, they
are often used in linguistic context as an approx-
imation to the full conditional probability. The
effect of bigram probability (or forward transi-
tional probability) has been repeatedly observed
(e.g. McDonald and Shillcock, 2003), and Smith
and Levy (2008) report an effect of lexical sur-
prisal as estimated by a trigram model on RTs
for the Dundee corpus (a collection of newspaper
texts with eye-tracking data from ten participants;
Kennedy and Pynte, 2005).
Phrase structure grammars (PSGs) have also
been amply used as language models (Boston et
al., 2008; Brouwer et al 2010; Demberg and
Keller, 2008; Hale, 2001; Levy, 2008). PSGs
can combine statistical exposure effects with ex-
plicit syntactic rules, by annotating norms with
their respective probabilities, which can be es-
timated from occurrence counts in text corpora.
Information about hierarchical sentence structure
can thus be included in the models. In this way,
Brouwer et altrained a probabilistic context-
free grammar (PCFG) on 204,000 sentences ex-
tracted from Dutch newspapers to estimate lexi-
cal surprisal (using an Earley-Stolcke parser; Stol-
cke, 1995), showing that it could account for
the noun phrase coordination bias previously de-
scribed and explained by Frazier (1987) in terms
of a minimal-attachment preference of the human
parser. In contrast, Demberg and Keller used texts
from a naturalistic source (the Dundee corpus) as
the experimental stimuli, thus evaluating surprisal
as a wide-coverage account of processing diffi-
culty. They also employed a PSG, trained on a
one-million-word language sample from the Wall
Street Journal (part of the Penn Treebank II, Mar-
cus et al 1993). Using Roark?s (2001) incremen-
tal parser, they found significant effects of unlexi-
calized surprisal on RTs (see also Boston et alfor
a similar approach and results for German texts).
However, they failed to find an effect for lexical-
ized surprisal, over and above forward transitional
probability. Roark et al(2009) also looked at the
399
effects of syntactic and lexical surprisal, using RT
data for short narrative texts. However, their es-
timates of these two surprisal values differ from
those described above: In order to tease apart se-
mantic and syntactic effects, they used Demberg
and Keller?s lexicalized surprisal as a total sur-
prisal measure, which they decompose into syn-
tactic and lexical components. Their results show
significant effects of both syntactic and lexical
surprisal, although the latter was found to hold
only for closed class words. Lack of a wider effect
was attributed to data sparsity: The models were
trained on the relatively small Brown corpus (over
one million words from 500 samples of American
English text), so that surprisal estimates for the
less frequent content words would not have been
accurate enough.
Using the same training and experimental lan-
guage samples as Demberg and Keller (2008),
and only unlexicalized surprisal estimates, Frank
(2009) and Frank and Bod (2011) focused on
comparing different language models, including
various n-gram models, PSGs and recurrent net-
works (RNN). The latter were found to be the bet-
ter predictors of RTs, and PSGs could not explain
any variance in RT over and above the RNNs,
suggesting that human processing relies on linear
rather than hierarchical representations.
Summing up, the only models taking into ac-
count actual words that have been consistently
shown to simulate human behaviour with natural-
istic text samples are bigram models.1 A possi-
ble limitation in previous studies can be found in
the stimuli employed. In reading real newspaper
texts, prior knowledge of current affairs is likely
to highly influence RTs, however, this source of
variability cannot be accounted for by the mod-
els. In addition, whereas the models treat each
sentence as an independent unit, in the text cor-
pora employed they make up coherent texts, and
are therefore clearly dependent. Thirdly, the stim-
uli used by Demberg and Keller (2008) comprise
a very particular linguistic style: journalistic edi-
torials, reducing the ability to generalize conclu-
sions to language in general. Finally, failure to
find lexical surprisal effects can also be attributed
to the training texts. Larger corpora are likely to
be needed for training language models on actual
1Although Smith and Levy (2008) report an effect of tri-
grams, they did not check if it exceeded that of simpler bi-
grams.
words than on POS (both the Brown corpus and
the WSJ are relatively small), and in addition, the
particular journalistic style of the WSJ might not
be the best alternative for modeling human be-
haviour. Although similarity between the train-
ing and experimental data sets (both from news-
paper sources) can improve the linguistic perfor-
mance of the models, their ability to simulate hu-
man behaviour might be limited: Newspaper texts
probably form just a small fraction of a person?s
linguistic experience. This study thus aims to
tackle some of the identified limitations: Rather
than cohesive texts, independent sentences, from
a narrative style are used as experimental stim-
uli for which word-reading times are collected
(as explained in Section 3). In addition, as dis-
cussed in the following section, language mod-
els are trained on a larger corpus, from a more
representative language sample. Following Frank
(2009) and Frank and Bod (2011), two contrasting
types of models are employed: hierarchical PSGs
and linear RNNs.
2 Models
2.1 Training data
The training texts were extracted from the writ-
ten section of the British National Corpus (BNC),
a collection of language samples from a variety
of sources, designed to provide a comprehensive
representation of current British English. A total
of 702,412 sentences, containing only the 7,754
most frequent words (the open-class words used
by Andrews et al 2009, plus the 200 most fre-
quent words in English) were selected, making up
a 7.6-million-word training corpus. In addition to
providing a larger amount of data than the WSJ,
this training set thus provides a more representa-
tive language sample.
2.2 Experimental sentences
Three hundred and sixty-one sentences, all com-
prehensible out of context and containing only
words included in the subset of the BNC used
to train the models, were randomly selected from
three freely accessible on-line novels2 (for addi-
tional details, see Frank, 2012). The fictional
narrative provides a good contrast to the pre-
2Obtained from www.free-online-novels.com.
Having not been published elsewhere, it is unlikely partici-
pants had read the novels previously.
400
viously examined newspaper editorials from the
Dundee corpus, since participants did not need
prior knowledge regarding the details of the sto-
ries, and a less specialised language and style
were employed. In addition, the randomly se-
lected sentences did not make up coherent texts
(in contrast, Roark et al 2009, employed short
stories), so that they were independent from each
other, both for the models and the readers.
2.3 Part-of-speech tagging
In order to produce POS-based surprisal esti-
mates, versions of both the training and exper-
imental texts with their words replaced by POS
were developed: The BNC sentences were parsed
by the Stanford Parser, version 1.6.7 (Klein and
Manning, 2003), whilst the experimental texts
were tagged by an automatic tagger (Tsuruoka
and Tsujii, 2005), with posterior review and cor-
rection by hand following the Penn Treebank
Project Guidelines (Santorini, 1991). By training
language models and subsequently running them
on the POS versions of the texts, unlexicalized
surprisal values were estimated.
2.4 Phrase-structure grammars
The Treebank formed by the parsed BNC sen-
tences served as training data for Roark?s (2001)
incremental parser. Following Frank and Bod
(2011), a range of grammars was induced, dif-
fering in the features of the tree structure upon
which rule probabilities were conditioned. In
four grammars, probabilities depended on the left-
hand side?s ancestors, from one up to four levels
up in the parse tree (these grammars will be de-
noted a1 to a4). In four other grammars (s1 to
s4), the ancestors? left siblings were also taken
into account. In addition, probabilities were con-
ditioned on the current head node in all grammars.
Subsequently, Roark?s (2001) incremental parser
parsed the experimental sentences under each of
the eight grammars, obtaining eight surprisal val-
ues for each word. Since earlier research (Frank,
2009) showed that decreasing the parser?s base
beam width parameter improves performance, it
was set to 10?18 (the default being 10?12).
2.5 Recurrent neural network
The RNN (see Figure 1) was trained in three
stages, each taking the selected (unparsed) BNC
sentences as training data.
7,754 word types
probability distributionover 7,754 word types
400
400
500
200
Figure 1: Architecture of neural network language
model, and its three learning stages. Numbers indicate
the number of units in each network layer.
Stage 1: Developing word representations
Neural network language models can bene-
fit from using distributed word representations:
Each word is assigned a vector in a continu-
ous, high-dimensional space, such that words that
are paradigmatically more similar are closer to-
gether (e.g., Bengio et al 2003; Mnih and Hin-
ton, 2007). Usually, these representations are
learned together with the rest of the model, but
here we used a more efficient approach in which
word representations are learned in an unsuper-
vised manner from simple co-occurrences in the
training data. First, vectors of word co-occurrence
frequencies were developed using Good-Turing
(Gale and Sampson, 1995) smoothed frequency
counts from the training corpus. Values in the
vector corresponded to the smoothed frequencies
with which each word directly preceded or fol-
lowed the represented word. Thus, each word
w was assigned a vector (fw,1, ..., fw,15508), such
that fw,v is the number of times word v directly
precedes (for v ? 7754) or follows (for v >
7754) word w. Next, the frequency counts were
transformed into Pointwise Mutual Information
(PMI) values (see Equation 2), following Bulli-
naria and Levy?s (2007) findings that PMI pro-
duced more psychologically accurate predictions
than other measures:
401
PMI(w, v) = log
(
fw,v
?
i,j fi,j
?
i fi,v
?
j fw,j
)
(2)
Finally, the 400 columns with the highest vari-
ance were selected from the 7754?15508-matrix
of row vectors, making them more computation-
ally manageable, but not significantly less infor-
mative.
Stage 2: Learning temporal structure
Using the standard backpropagation algorithm,
a simple recurrent network (SRN) learned to pre-
dict, at each point in the training corpus, the next
word?s vector given the sequence of word vectors
corresponding to the sentence so far. The total
corpus was presented five times, each time with
the sentences in a different random order.
Stage 3: Decoding predicted word
representations
The distributed output of the trained SRN
served as training input to the feedforward ?de-
coder? network, that learned to map the dis-
tributed representations back to localist ones.
This network, too, used standard backpropaga-
tion. Its output units had softmax activation func-
tions, so that the output vector constitutes a prob-
ability distribution over word types. These trans-
late directly into surprisal values, which were col-
lected over the experimental sentences at ten in-
tervals over the course of Stage 3 training (after
presenting 2K, 5K, 10K, 20K, 50K, 100K, 200K,
and 350K sentences, and after presenting the full
training corpus once and twice). These will be
denoted by RNN-1 to RNN-10.
A much simpler RNN model suffices for ob-
taining unlexicalized surprisal. Here, we used
the same models as described by Frank and Bod
(2011), albeit trained on the POS tags of our
BNC training corpus. These models employed
so-called Echo State Networks (ESN; Jaeger and
Haas, 2004), which are RNNs that do not develop
internal representations because weights of input
and recurrent connections remain fixed at ran-
dom values (only the output connection weights
are trained). Networks of six different sizes were
used. Of each size, three networks were trained,
using different random weights. The best and
worst model of each size were discarded to reduce
the effect of the random weights.
3 Experiment
3.1 Procedure
Text display followed a self-paced reading
paradigm: Sentences were presented on a com-
puter screen one word at a time, with onset of
the next word being controlled by the subject
through a key press. The time between word
onset and subsequent key press was recorded as
the RT (measured in milliseconds) on that word
by that subject.3 Words were presented centrally
aligned in the screen, and punctuation marks ap-
peared with the word that preceded them. A fixed-
width font type (Courier New) was used, so that
physical size of a word equalled number of char-
acters. Order of presentation was randomized for
each subject. The experiment was time-bounded
to 40 minutes, and the number of sentences read
by each participant varied between 120 and 349,
with an average of 224. Yes-no comprehension
questions followed 46% of the sentences.
3.2 Participants
A total of 117 first year psychology students took
part in the experiment. Subjects unable to an-
swer correctly to more than 20% of the questions
and 47 participants who were non-native English
speakers were excluded from the analysis, leaving
a total of 54 subjects.
3.3 Design
The obtained RTs served as the dependent vari-
able against which a mixed-effects multiple re-
gression analysis with crossed random effects for
subjects and items (Baayen et al 2008) was per-
formed. In order to control for low-level lexical
factors that are known to influence RTs, such as
word length or frequency, a baseline regression
model taking them into account was built. Subse-
quently, the decrease in the model?s deviance, af-
ter the inclusion of surprisal as a fixed factor to the
baseline, was assessed using likelihood tests. The
resulting ?2 statistic indicates the extent to which
each surprisal estimate accounts for RT, and can
thus serve as a measure of the psychological ac-
curacy of each model.
However, this kind of analysis assumes that RT
for a word reflects processing of only that word,
3The collected RT data are available for download at
www.stefanfrank.info/EACL2012.
402
but spill-over effects (in which processing diffi-
culty at word wt shows up in the RT on wt+1)
have been found in self-paced and natural read-
ing (Just et al 1982; Rayner, 1998; Rayner and
Pollatsek, 1987). To evaluate these effects, the
decrease in deviance after adding surprisal of the
previous item to the baseline was also assessed.
The following control predictors were included
in the baseline regression model:
Lexical factors:
? Number of characters: Both physical size
and number of characters have been found
to affect RTs for a word (Rayner and Pollat-
sek, 1987), but the fixed-width font used in
the experiment assured number of characters
also encoded physical word length.
? Frequency and forward transitional proba-
bility: The effects of these two factors have
been repeatedly reported (e.g. Juhasz and
Rayner, 2003; Rayner, 1998). Given the high
correlations between surprisal and these two
measures, their inclusion in the baseline as-
sures that the results can be attributed to pre-
dictability in context, over and above fre-
quency and bigram probability. Frequency
was estimated from occurrence counts of
each word in the full BNC corpus (written
section). The same transformation (nega-
tive logarithm) was applied as for computing
surprisal, thus obtaining ?unconditional? and
bigram surprisal values.
? Previous word lexical factors: Lexical fac-
tors for the previous word were included in
the analysis to control for spill-over effects.
Temporal factors and autocorrelation:
RT data over naturalistic texts violate the re-
gression assumption of independence of obser-
vations in several ways, and important word-by-
word sequential correlations exist. In order to en-
sure validity of the statistical analysis, as well as
providing a better model fit, the following factors
were also included:
? Sentence position: Fatigue and practice ef-
fects can influence RTs. Sentence position
in the experiment was included both as linear
and quadratic factor, allowing for the model-
ing of initial speed-up due to practice, fol-
lowed by a slowing down due to fatigue.
? Word position: Low-level effects of word or-
der, not related to predictability itself, were
modeled by including word position in the
sentence, both as a linear and quadratic fac-
tor (some of the sentences were quite long,
so that the effect of word position is unlikely
to be linear).
? Reading time for previous word: As sug-
gested by Baayen and Milin (2010), includ-
ing RT on the previous word can control for
several autocorrelation effects.
4 Results
Data were analysed using the free statistical soft-
ware package R (R Development Core Team,
2009) and the lme4 library (Bates et al 2011).
Two analyses were performed for each language
model, using surprisal for either current or pre-
vious word as the dependent variable. Unlikely
reading times (lower than 50ms or over 3000ms)
were removed from the analysis, as were clitics,
words followed by punctuation, words follow-
ing punctuation or clitics (since factors for pre-
vious word were included in the analysis), and
sentence-initial words, leaving a total of 132,298
data points (between 1,335 and 3,829 per subject).
4.1 Baseline model
Theoretical considerations guided the selection
of the initial predictors presented above, but an
empirical approach led actual regression model
building. Initial models with the original set of
fixed effects, all two-way interactions, plus ran-
dom intercepts for subjects and items were evalu-
ated, and least significant factors were removed
one at a time, until only significant predictors
were left (|t| > 2). A different strategy was
used to assess which by-subject and by item ran-
dom slopes to include in the model. Given the
large number of predictors, starting from the sat-
urated model with all random slopes generated
non-convergence problems and excessively long
running times. By-subject and by-item random
slopes for each fixed effect were therefore as-
sessed individually, using likelihood tests. The
final baseline model included by-subject random
intercepts, by-subject random slopes for sentence
position and word position, and by-item slopes for
previous RT. All factors (random slopes and fixed
effects) were centred and standardized to avoid
403
-6.6 -6.4 -6.2 -6 -5.8 -5.6 -5.4 -5.2 -5
0
10
20
30
40
50
60
70
1
2
3
4
5
6
7
8 910
1
23
4
1
2
34
Lexicalized models
Linguistic accuracy
-2.55 -2.5 -2.45 -2.4 -2.35 -2.3 -2.25 -2.2 -2.15 -2.1
0
5
10
15
20
25
30
1
2
34
1
2
3 4
5
1 2
3
4
6
Unlexicalized models
PSG-a PSG-s RNN(-average surprisal)
Ps
yc
ho
log
ica
l a
cc
ur
ac
y
(?
?)
Figure 2: Psychological accuracy (combined effect of current and previous surprisal) against linguistic accuracy
of the different models. Numbered labels denote the maximum number of levels up in the tree from which
conditional information is used (PSG); point in training when estimates were collected (word-based RNN); or
network size (POS-based RNN).
multicollinearity-related problems.
4.2 Surprisal effects
All model categories (PSGs and RNNs) produced
lexicalized surprisal estimates that led to a signif-
icant (p < 0.05) decrease in deviance when in-
cluded as a fixed factor in the baseline, with pos-
itive coefficients: Higher surprisal led to longer
RTs. Significant effects were also found for their
unlexicalized counterparts, albeit with consider-
ably smaller ?2-values.
Both for the lexicalized and unlexicalized ver-
sions, these effects persisted whether surprisal for
the previous or current word was taken as the in-
dependent variable. However, the effect size was
much larger for previous surprisal, indicating the
presence of strong spill-over effects (e.g. lexical-
ized PSG-s3: current surprisal: ?2(1) = 7.29,
p = 0.007; previous surprisal: ?2(1) = 36.73,
p 0.001).
From hereon, only results for the combined ef-
fect of both (inclusion of previous and current
surprisal as fixed factors in the baseline) are re-
ported. Figure 2 shows the psychological accu-
racy of each model (?2(2) values) plotted against
its linguistic accuracy (i.e., its quality as a lan-
guage model, measured by the negative aver-
age surprisal on the experimental sentences: the
higher this value, the ?less surprised? the model
is by the test corpus). For the lexicalized models,
RNNs clearly outperform PSGs. Moreover, the
RNN?s accuracy increases as training progresses
(the highest psychological accuracy is achieved
at point 8, when 350K training sentences were
presented). The PSGs taking into account sib-
ling nodes are slightly better than their ancestor-
only counterparts (the best psychological model
is PSG-s3). Contrary to the trend reported by
Frank and Bod (2011), the unlexicalized PSGs
and RNNs reach similar levels of psychological
accuracy, with the PSG-s4 achieving the highest
?2-value.
Model comparison ?2(2) p-value
PSG over RNN 12.45 0.002
RNN over PSG 30.46 0.001
Table 1: Model comparison between best performing
word-based PSG and RNN.
Although RNNs outperform PSGs in the lexi-
calized estimates, comparisons between the best
performing model (i.e. highest ?2) in each cate-
gory showed both were able to explain variance
over and above each other (see Table 1). It is
worth noting, however, that if comparisons are
made amongst models including surprisal for cur-
rent, but not previous word, the PSG is unable
404
to explain a significant amount of variance over
and above the RNN (?2(1) = 2.28; p = 0.13).4
Lexicalized models achieved greater psychologi-
cal accuracy than their unlexicalized counterparts,
but the latter could still explain a small amount of
variance over and above the former (see Table 2).5
Model comparison ?2(2) p-value
Best models overall:
POS- over word-based 10.40 0.006
word- over POS-based 47.02 0.001
PSGs:
POS- over word-based 6.89 0.032
word- over POS-based 25.50 0.001
RNNs:
POS- over word-based 5.80 0.055
word- over POS-based 49.74 0.001
Table 2: Word- vs. POS-based models: comparisons
between best models overall, and best models within
each category.
4.3 Differences across word classes
In order to make sure that the lexicalized sur-
prisal effects found were not limited to closed-
class words (as Roark et al 2009, report), a fur-
ther model comparison was performed by adding
by-POS random slopes of surprisal to the models
containing the baseline plus surprisal. If particu-
lar syntactic categories were contributing to the
overall effect of surprisal more than others, in-
cluding such random slopes would lead to addi-
tional variance being explained. However, this
was not the case: inclusion of by-POS random
slopes of surprisal did not lead to a significant im-
provement in model fit (PSG: ?2(1) = 0.86, p =
0.35; RNN: ?2(1) = 3.20, p = 0.07).6
5 Discussion
The present study aimed to find further evidence
for surprisal as a wide-coverage account of lan-
guage processing difficulty, and indeed, the re-
4Best models in this case were PSG-a3 and RNN-7.
5Since best performing lexicalized and unlexicalized
models belonged to different groups: RNN and PSG, respec-
tively, Table 2 also shows comparisons within model type.
6Comparison was made on the basis of previous word
surprisal (best models in this case were PSG-s3 and RNN-
9).
sults show the ability of lexicalized surprisal to
explain a significant amount of variance in RT
data for naturalistic texts, over and above that
accounted for by other low-level lexical factors,
such as frequency, length, and forward transi-
tional probability. Although previous studies had
presented results supporting such a probabilistic
language processing account, evidence for word-
based surprisal was limited: Brouwer et al(2010)
only examined a specific psycholinguistic phe-
nomenon, rather than a random language sample;
Demberg and Keller (2008) reported effects that
were only significant for POS but not word-based
surprisal; and Smith and Levy (2008) found an
effect of lexicalized surprisal (according to a tri-
gram model), but did not assess whether simpler
predictability estimates (i.e., by a bigram model)
could have accounted for those effects.
Demberg and Keller?s (2008) failure to find lex-
icalized surprisal effects can be attributed both to
the language corpus used to train the language
models, as well as to the experimental texts used.
Both were sourced from newspaper texts: As
training corpora these are unrepresentative of a
person?s linguistic experience, and as experimen-
tal texts they are heavily dependent on partici-
pant?s world knowledge. Roark et al(2009), in
contrast, used a more representative, albeit rela-
tively small, training corpus, as well as narrative-
style stimuli, thus obtaining RTs less dependent
on participant?s prior knowledge. With such an
experimental set-up, they were able to demon-
strate the effects of lexical surprisal for RT of
closed-class, but not open-class, words, which
they attributed to their differential frequency and
to training-data sparsity: The limited Brown cor-
pus would have been enough to produce accurate
estimates of surprisal for function words, but not
for the less frequent content words. A larger train-
ing corpus, constituting a broad language sample,
was used in our study, and the detected surprisal
effects were shown to hold across syntactic cate-
gory (modeling slopes for POS separately did not
improve model fit). However, direct comparison
with Roark et als results is not possible: They
employed alternative definitions of structural and
lexical surprisal, which they derived by decom-
posing the total surprisal as obtained with a fully
lexicalized PSG model.
In the current study, a similar approach to that
taken by Demberg and Keller (2008) was used to
405
define structural (or unlexicalized), and lexical-
ized surprisal, but the results are strikingly differ-
ent: Whereas Demberg and Keller report a signif-
icant effect for POS-based estimates, but not for
word-based surprisal, our results show that lexi-
calized surprisal is a far better predictor of RTs
than its unlexicalized counterpart. This is not sur-
prising, given that while the unlexicalized mod-
els only have access to syntactic sources of in-
formation, the lexicalized models, like the hu-
man parser, can also take into account lexical co-
occurrence trends. However, when a training cor-
pus is not large enough to accurately capture the
latter, it might still be able to model the former,
given the higher frequency of occurrence of each
possible item (POS vs. word) in the training data.
Roark et al(2009) also included in their analysis
a POS-based surprisal estimate, which lost signif-
icance when the two components of the lexical-
ized surprisal were present, suggesting that such
unlexicalized estimates can be interpreted only as
a coarse version of the fully lexicalized surprisal,
incorporating both syntactic and lexical sources
of information at the same time. The results pre-
sented here do not replicate this finding: The best
unlexicalized estimates were able to explain ad-
ditional variance over and above the best word-
based estimates. However, this comparison con-
trasted two different model types: a word-based
RNN and a POS-based PSG, so that the observed
effects could be attributed to the model represen-
tations (hierarchical vs. linear) rather than to the
item of analysis (POS vs. words). Within-model
comparisons showed that unlexicalized estimates
were still able to account for additional variance,
although only reaching significance at the 0.05
level for the PSGs.
Previous results reported by Frank (2009) and
Frank and Bod (2011) regarding the higher psy-
chological accuracy of RNNs and the inability of
the PSGs to explain any additional variance in
RT, were not replicated. Although for the word-
based estimates RNNs outperform the PSGs, we
found both to have independent effects. Further-
more, in the POS-based analysis, performance of
PSGs and RNNs reaches similarly high levels of
psychological accuracy, with the best-performing
PSG producing slightly better results than the
best-performing RNN. This discrepancy in the re-
sults could reflect contrasting reading styles in
the two studies: natural reading of newspaper
texts, or self-paced reading of independent, nar-
rative sentences. The absence of global context,
or the unnatural reading methodology employed
in the current experiment, could have led to an
increased reliance on hierarchical structure for
sentence comprehension. The sources and struc-
tures relied upon by the human parser to elabo-
rate upcoming-word expectations could therefore
be task-dependent. On the other hand, our re-
sults show that the independent effects of word-
based PSG estimates only become apparent when
investigating the effect of surprisal of the previous
word. That is, considering only the current word?s
surprisal, as in Frank and Bod?s analysis, did not
reveal a significant contribution of PSGs over and
above RNNs. Thus, additional effects of PSG sur-
prisal might only be apparent when spill-over ef-
fects are investigated by taking previous word sur-
prisal as a predictor of RT.
6 Conclusion
The results here presented show that lexicalized
surprisal can indeed model RT over naturalistic
texts, thus providing a wide-coverage account of
language processing difficulty. Failure of previ-
ous studies to find such an effect could be at-
tributed to the size or nature of the training cor-
pus, suggesting that larger and more general cor-
pora are needed to model successfully both the
structural and lexical regularities used by the hu-
man parser to generate predictions. Another cru-
cial finding presented here is the importance of
spill-over effects: Surprisal of a word had a much
larger influence on RT of the following item than
of the word itself. Previous studies where lexi-
calized surprisal was only analysed in relation to
current RT could have missed a significant effect
only manifested on the following item. Whether
spill-over effects are as important for different RT
collection paradigms (e.g., eye-tracking) remains
to be tested.
Acknowledgments
The research presented here was funded by the
European Union Seventh Framework Programme
(FP7/2007-2013) under grant number 253803.
The authors acknowledge the use of the UCL Le-
gion High Performance Computing Facility, and
associated support services, in the completion of
this work.
406
References
Gerry T.M. Altmann and Yuki Kamide. 1999. Incre-
mental interpretation at verbs: Restricting the do-
main of subsequent reference. Cognition, 73:247?
264.
Mark Andrews, Gabriella Vigliocco, and David P. Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116:463?498.
R. Harald Baayen and Petar Milin. 2010. Analyzing
reaction times. International Journal of Psycholog-
ical Research, 3:12?28.
R. Harald Baayen, Doug J. Davidson, and Douglas M.
Bates. 2008. Mixed-effects modeling with crossed
random effects for subjects and items. Journal of
Memory and Language, 59:390?412.
Moshe Bar. 2007. The proactive brain: using
analogies and associations to generate predictions.
Trends in Cognitive Sciences, 11:280?289.
Douglas Bates, Martin Maechler, and Ben Bolker,
2011. lme4: Linear mixed-effects models using
S4 classes. Available from: http://CRAN.R-
project.org/package=lme4 (R package version
0.999375-39).
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent,
and Christian Jauvin. 2003. A neural probabilis-
tic language model. Journal of Machine Learning
Research, 3:1137?1155.
Marisa Ferrara Boston, John Hale, Reinhold Kliegl,
Umesh Patil, and Shravan Vasishth. 2008. Parsing
costs as predictors of reading difficulty: An evalua-
tion using the potsdam sentence corpus. Journal of
Eye Movement Research,, 2:1?12.
Harm Brouwer, Hartmut Fitz, and John C. J. Hoeks.
2010. Modeling the noun phrase versus sentence
coordination ambiguity in Dutch: evidence from
surprisal theory. In Proceedings of the 2010 Work-
shop on Cognitive Modeling and Computational
Linguistics, pages 72?80, Stroudsburg, PA, USA.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39:510?526.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syn-
tactic processing complexity. Cognition, 109:193?
210.
Stefan L. Frank and Rens Bod. 2011. Insensitivity of
the human sentence-processing system to hierarchi-
cal structure. Psychological Science, 22:829?834.
Stefan L. Frank. 2009. Surprisal-based comparison
between a symbolic and a connectionist model of
sentence processing. In Proceedings of the 31st An-
nual Conference of the Cognitive Science Society,
pages 1139?1144, Austin, TX.
Stefan L. Frank. 2012. Uncertainty reduction as a
measure of cognitive processing load in sentence
comprehension. Manuscript submitted for publica-
tion.
Peter Hagoort, Lea Hald, Marcel Bastiaansen, and
Karl Magnus Petersson. 2004. Integration of word
meaning and world knowledge in language compre-
hension. Science, 304:438?441.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage technologies, pages 1?8, Stroudsburg, PA.
Herbert Jaeger and Harald Haas. 2004. Harnessing
nonlinearity: predicting chaotic systems and saving
energy in wireless communication. Science, pages
78?80.
Barbara J. Juhasz and Keith Rayner. 2003. Investigat-
ing the effects of a set of intercorrelated variables on
eye fixation durations in reading. Journal of Exper-
imental Psychology: Learning, Memory and Cogni-
tion, 29:1312?1318.
Marcel A. Just, Patricia A. Carpenter, and Jacque-
line D. Woolley. 1982. Paradigms and processes
in reading comprehension. Journal of Experimen-
tal Psychology: General, 111:228?238.
Yuki Kamide, Christoph Scheepers, and Gerry T. M.
Altmann. 2003. Integration of syntactic and se-
mantic information in predictive processing: cross-
linguistic evidence from German and English.
Journal of Psycholinguistic Research, 32:37?55.
Alan Kennedy and Joe?l Pynte. 2005. Parafoveal-on
foveal effects in normal reading. Vision Research,
45:153?168.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics,, pages 423?430.
Kestutis Kveraga, Avniel S. Ghuman, and Moshe Bar.
2007. Top-down predictions in the cognitive brain.
Brain and Cognition, 65:145?168.
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106:1126?1177.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19:313?330.
Scott A. McDonald and Richard C. Shillcock. 2003.
Low-level predictive inference in reading: the influ-
ence of transitional probabilities on eye movements.
Vision Research, 43:1735?1751.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
Proceedings of the 25th International Conference of
Machine Learning, pages 641?648.
Keith Rayner and Alexander Pollatsek. 1987. Eye
movements in reading: A tutorial review. In
407
M. Coltheart, editor, Attention and performance
XII: the psychology of reading., pages 327?362.
Lawrence Erlbaum Associates, London, UK.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124:372?422.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 1 - Volume 1, pages 324?333, Stroudsburg, PA.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27:249?276.
Beatrice Santorini. 1991. Part-of-speech tagging
guidelines for the Penn Treebank Project. Technical
report, Philadelphia, PA.
Nathaniel J. Smith and Roger Levy. 2008. Optimal
processing times in reading: a formal model and
empirical investigation. In Proceedings of the 30th
Annual Conference of the Cognitive Science Soci-
ety, pages 595?600, Austin,TX.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational linguistics, 21:165?
201.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 467?474, Stroudsburg, PA.
408
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 878?883,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Word surprisal predicts N400 amplitude during reading
Stefan L. Frank1,2 Leun J. Otten3 Giulia Galli3 Gabriella Vigliocco2
{s.frank, l.otten, g.galli, g.vigliocco}@ucl.ac.uk
1Centre for Language Studies, Radboud University Nijmegen
2Department of Cognitive, Perceptual and Brain Sciences, University College London
3Institute of Cognitive Neuroscience, University College London
Abstract
We investigated the effect of word sur-
prisal on the EEG signal during sen-
tence reading. On each word of 205 ex-
perimental sentences, surprisal was esti-
mated by three types of language model:
Markov models, probabilistic phrase-
structure grammars, and recurrent neu-
ral networks. Four event-related poten-
tial components were extracted from the
EEG of 24 readers of the same sentences.
Surprisal estimates under each model type
formed a significant predictor of the am-
plitude of the N400 component only, with
more surprising words resulting in more
negative N400s. This effect was mostly
due to content words. These findings
provide support for surprisal as a gener-
ally applicable measure of processing dif-
ficulty during language comprehension.
1 Introduction
Many studies of human language comprehension
measure the brain?s electrical activity during read-
ing. Such electroencephalography (EEG) experi-
ments have revealed that the EEG signal displays
systematic variation in response to the appearance
of each word. The different components that can
be observed in this signal are known as event-
related potentials (ERPs). Probably the most reli-
ably observed (and most studied) of these compo-
nents is a negative-going deflection at centropari-
etal electrodes that peaks at around 400 ms after
word onset and is therefore referred to as the N400
component.
It is well known that the N400 increases in am-
plitude (i.e., becomes more negative) when the
word leads to comprehension difficulty. To study
the general relation between word predictability
and the N400, Dambacher et al (2006) obtained
subjective word-probability estimates (so-called
cloze probabilities) by asking participants to pre-
dict the upcoming word at each point in a large
number of sentences. A different group of subjects
read these same sentences while their EEG signal
was recorded. Results showed a correlation be-
tween N400 amplitude and cloze probability: Less
predictable words yielded stronger N400s.
We investigated whether similar results can be
obtained using more objective, model-based word
probabilities. For each word in a collection of
English sentences, estimates of its surprisal (i.e.,
its negative log-transformed conditional probabil-
ity: ? logP (wt|w1, . . . , wt?1)) were generated
by three types of language model: Markov (i.e., n-
gram) models, phrase-structure grammars (PSGs),
and recurrent neural networks (RNNs). Next, EEG
signals of participants reading the same sentences
were recorded. A comparison of word surprisal to
different ERP components revealed that, indeed,
N400 amplitude was predicted by surprisal values:
More surprising words resulted in more negative
N400s, at least for content words.
2 Language models
A range of models of each type was trained, al-
lowing to investigate whether models that capture
the language statistics more accurately also yield
better predictions of ERP size. Such a relation is
generally found in studies that use word-reading
time as the dependent variable (Fernandez Mon-
salve et al, 2012; Frank and Bod, 2011; Frank
and Thompson, 2012), providing additional sup-
port that these psychological data are indeed ex-
plained by the surprisal values and not by some
confounding variable.
2.1 Corpus data
All models were trained on sentences from the
written texts in the British National Corpus
(BNC). First, the 10,000 word types with highest
878
frequency were selected from the BNC. Next, all
sentences were extracted that contained only those
words. This resulted in a training corpus of 1.06
million sentences (12.6 million word tokens).
Each trained model estimated a surprisal value
for each word of the 205 sentences (1931 word to-
kens) for which eye-tracking data are available in
the UCL corpus of reading times (Frank et al, in
press). These sentences, which were selected from
three unpublished novels, only contained words
from the 10,000 high-frequency word list.
2.2 Markov models
Markov models were trained with modified
Kneser-Ney smoothing (Chen and Goodman,
1999) as implemented in SRILM (Stolcke, 2002).
Model order was varied: n = 2, 3, 4. No unigram
model was computed because word frequency was
factored out during data analysis (see Section 4.2).
2.3 Recurrent neural networks
The RNN model architecture has been thoroughly
described elsewhere (Fernandez Monsalve et al,
2012; Frank, in press) so it is not discussed here.
The only difference with previous versions was
that the current RNN was trained on a substantially
larger data set with more word types. A range of
RNN models was obtained by training on nine in-
creasingly large subsets of the BNC data, compris-
ing 2K, 5K, 10K, 20K, 50K, 100K, 200K, 400K,
and all 1.06M sentences. In addition, the network
was trained on the full set twice, making a total of
ten instantiations of the RNN model.
2.4 Phrase-structure grammars
To prepare data for PSG training, the selected
BNC sentences were parsed by the Stanford parser
(Klein and Manning, 2003). The resulting tree-
bank was divided into nine increasingly large sub-
sets, equal to those used for RNN training.1 Gram-
mars were induced from these subsets using the
algorithm by Roark (2001) with its standard set-
tings. Next, surprisal values on the experimental
sentences were generated by Roark?s incremental
parser. Since increasing the parser?s beam width
has been shown to improve both word-probability
estimates and the fit to word-reading times (Frank,
2009), the parser?s ?base beam threshold? parame-
ter was reduced to 10?20.
1Because not all experimental sentences could be parsed
when the treebank comprised only 2K sentences, 1K sen-
tences were added to the smallest subset.
3 EEG data collection
Twenty-four healthy, adult volunteers from the
UCL Psychology subject pool took part in the
reading study. Their EEG was recorded contin-
uously from 32 channels during the presentation
of 5 practice sentences and the 205 experimental
items. Participants were asked to minimise blinks,
eye movements, and head movements during sen-
tence presentation.
Each sentence was preceded by a centrally pre-
sented fixation cross. As soon as the partici-
pant pressed a key, the cross was replaced by the
sentence?s first word, which was then automati-
cally replaced by each subsequent word. Word
presentation duration (in milliseconds) equalled
190 + 20k, where k is the number of characters
in the word (including any attached punctuation).
After the word disappeared, there was a 390 ms
interval before the next word appeared.
The sentences were presented in random or-
der, one word at a time, always centrally located
on the monitor. One-hundred and ten of the ex-
perimental sentences were followed by a yes/no-
comprehension question, to ensure that partici-
pants tried to understand the sentences. All par-
ticipants answered at least 80% of the comprehen-
sion questions correctly.
4 Data analysis
4.1 ERP components
Four ERP components of interest were identified
from the literature on EEG and sentence reading:
Early Left Anterior Negativity (ELAN), P200,
N400, and a post-N400 positivity (PNP). Table 1
lists the corresponding time windows and approx-
imate electrode sites.2 For each component, the
average electrode potential over the corresponding
time window and electrodes was computed. These
average ERP amplitudes served as the four depen-
dent variables for data analysis.
The ELAN component is generally thought of
as indicative of difficulty with constructing syntac-
tic phrase structure (Friederici et al, 1999; Gunter
et al, 1999; Neville et al, 1991). Hence, if any
of the model types predicts ELAN size, we would
expect this to be the PSG.
Dambacher et al (2006) found effects of word
frequency or length (which are strongly correlated
2The P600 component (Osterhout and Holcomb, 1992)
was not included because the shortest interval between con-
secutive word onsets was only 600 ms.
879
Component Time window Location
ELAN 125?175 ms left anterior
P200 140?200 ms frontocentral
N400 300?500 ms centroparietal
PNP 400?600 ms frontopolar
Table 1: Investigated ERP components, their time
windows, and approximate scalp locations.
and therefore difficult to tease apart) on the P200
amplitude. Since we factor out these two lexical
factors in the analysis, we expect no additional ef-
fect of surprisal on P200.
If any of the components is sensitive to word
surprisal, this is most likely to be the N400 as
many studies have already shown that N400 am-
plitude depends on subjective word predictabil-
ity (Dambacher et al, 2006; Kutas and Hillyard,
1984; Moreno et al, 2002). Whether an effect
will appear on the PNP is more doubtful. Van
Petten and Luka (2012) argue that word expec-
tations that are confirmed result in reduced N400
size, whereas expectations that are disconfirmed
increase the PNP. However, in a probabilistic set-
ting, expectations are not all-or-nothing so there
is no strict distinction between confirmation and
disconfirmation. Nevertheless, surprisal effects on
PNP may occur. Since the PNP has received rel-
atively little attention, the component may not be
such a reliable index of comprehension difficulty
as the N400 has proven to be.
4.2 Regression analysis
Data were discarded on words attached to a
comma, clitics, sentence-initial, and sentence-
final words. Moreover, artifacts in the EEG data
(mostly due to eye blinks) were identified and re-
moved, leaving 32,010 analysed data points per in-
vestigated ERP component. For each data point
and ERP component, a baseline potential was de-
termined by averaging over the component?s elec-
trodes in the 100 ms leading up to word onset.
In order to quantify the fit of surprisal to ERP
size, a linear mixed-effects regression model was
fitted to each of the four ERPs, using the pre-
dictors: baseline potential, log-transformed word
frequency, word length (number of characters),
word position in the sentence, and sentence po-
sition in the experiment.3 Also, all significant
3For word and sentence position, both linear and squared
factors were included in order to capture possible non-linear
two-way interactions were included (main effects
were removed if they were not significant and did
not appear in any interaction). In addition, there
were by-subject and by-item random intervals, as
well as significant by-subject and by-item random
slopes. Parameters for the correlation between
random intercept and slope where also estimated,
if they significantly contributed to model fit.
When the surprisal estimates by a particular lan-
guage model are included in the analysis, the re-
gression model?s deviance decreases. The size of
this decrease is the ?2-statistic of a likelihood-
ratio test for significance of the surprisal effect,
and was taken as the measure of the surprisal val-
ues? fit to the ERP data.4 Negative values will be
used to indicate effects in the negative direction,
that is, when higher surprisal results in more neg-
ative (or less positive) going ERP deflections.
5 Results
5.1 Surprisal effects
Figure 1 plots the fit of each model?s surprisal esti-
mates to ERP amplitude as a function of the aver-
age natural logP (wt|w1, . . . , wt?1), which quan-
tifies to what extent the model has acquired accu-
rate language statistics.5 For the ELAN, P200 and
PNP components, there were no significant effects
after correcting for multiple comparisons. In con-
trast, effects on the N400 were highly significant.
5.2 Model comparison
Table 2 shows results of pairwise comparisons be-
tween the best models of each type (i.e., those
whose surprisal estimates fit the N400 data best).
Clearly, RNN-based surprisal explains variance
over and above each of the other two models
whereas neither the n-gram nor the PSG model
outperforms the RNN. Moreover, the RNN?s sur-
prisals explain a marginally significant (?2 =
3.47; p < .07) amount of variance over and above
the combined PSG and n-gram surprisals.
changes over the course of the sentence or experiment.
4This definition equals what Frank and Bod (2011) call
?psychological accuracy? in an analysis of reading times.
5This measure, which Frank and Bod (2011) call ?linguis-
tic accuracy?, equals the negative logarithm of the model?s
perplexity. Increasing the amount of training data (or the
value of n) resulted in higher linguistic accuracy, except for
the three PSG models trained on the smallest amounts of data.
This shows that the models did not suffer from overfitting.
880
?6.5 ?6 ?5.5 ?5
?20
?10
0
10
ELAN
av. log P(wt|w1,?,wt?1)
fit
 o
f s
ur
pr
isa
l (?
?2
)
?6.5 ?6 ?5.5 ?5
?20
?10
0
10
P200
av. log P(wt|w1,?,wt?1)
 
 
?6.5 ?6 ?5.5 ?5
?20
?10
0
10
N400
av. log P(wt|w1,?,wt?1)
?6.5 ?6 ?5.5 ?5
?20
?10
0
10
PNP
av. log P(wt|w1,?,wt?1)
RNN
PSG
n?gram
Figure 1: Fit to surprisal of ERP amplitude (for ELAN, P200, N400, and PNP components) as a function
of average logP (wt|w1, . . . , wt?1). Each plotted point corresponds to predictions by one of the trained
models. Dotted lines indicate ?2 = ?3.84, beyond which effects are statistically significant (p < .05)
if no correction for multiple comparisons is applied. The dashed line indicates the level below which
effects are significant after applying the correction proposed by Benjamini and Hochberg (1995), on
each ERP component separately because of our prior expectation that effects would occur mostly (if not
exclusively) on the N400 component.
Model n-gram RNN PSG
n-gram ?2 = 1.34 ?2 = 1.66
p > .2 p > .1
RNN ?2 = 6.52 ?2 = 4.78
p < .02 p < .05
PSG ?2 = 4.20 ?2 = 2.14
p < .05 p > .1
Table 2: Pairwise comparisons between surprisal
estimates by the best models of each type. Shown
are the results of likelihood-ratio tests for the ef-
fect of one set of surprisal estimates (rows) over
and above the other (columns).
5.3 Comparing word classes
N400 effects are nearly exclusively investigated
on content (i.e., open-class) words. Dambacher et
al. (2006), too, investigated the relation between
ERP amplitudes and cloze probabilities on con-
tent words only. When running separate analyses
on content and function words (constituting 53.2%
and 46.8% of the data, respectively), we found that
the N400 effect of Figure 1 is nearly fully driven
by content words (see Figure 2). None of the mod-
els? surprisal estimates formed a significant pre-
dictor of N400 amplitude on function words, after
correction for multiple comparisons.
6 Discussion
We demonstrated a clear effect of word surprisal,
as estimated by different language models, on the
EEG signal: The larger a (content) word?s sur-
prisal value, the more negative the resulting N400.
?8.5 ?8 ?7.5 ?7 ?6.5
?20
?10
0
content words
av. log P(wt|w1,?,wt?1)
fit
 o
f s
ur
pr
isa
l (?
?2
)
?4.5 ?4 ?3.5
?20
?10
0
av. log P(wt|w1,?,wt?1)
function words
 
 
RNN
PSG
n?gram
Figure 2: Fit to surprisal of N400 amplitude, for
content words (left) and function words (right).
Dotted lines indicate ?2 = ?3.84, beyond which
effects are statistically significant (p < .05) with-
out correcting for multiple comparisons. Dashed
lines indicates the levels beyond which effects
are significant after multiple-comparison correc-
tion (Benjamini and Hochberg, 1995).
The N400 component is generally viewed as in-
dicative of lexical rather than syntactic processing
(Kaan, 2007), which may explain why surprisal
under the PSG model did not have any significant
explanatory value over and above RNN-based sur-
prisal. The relatively weak performance of our
Markov models is most likely due to their strict
(and cognitively unrealistic) limit on the size of
the prior context upon which word-probability es-
timates are conditioned.
Unlike the ELAN, P200, and PNP components,
the N400 is known to be sensitive to the cloze
probability of content words. The fact that sur-
prisal effects were found on the N400 only, there-
fore suggests that subjective predictability scores
and model-based surprisal estimates form opera-
881
tionalisations of one and the same underlying cog-
nitive factor. Needless to say, our statistical mod-
els fail to capture many information sources, such
as semantics and discourse, that do affect cloze
probabilities. However, it is possible in principle
to integrate these into probabilistic language mod-
els (Dubey et al, 2011; Mitchell et al, 2010).
To the best of our knowledge, only one other
published study relates language model predic-
tions to the N400: Parviz et al (2011) found that
surprisal estimates (corrected for word frequency)
from an n = 4 Markov model predicted N400 size
as measured by magnetoencephalography (rather
than EEG). Although their PSG-based surprisals
did not correlate with N400 size, a related measure
derived from the PSG ?lexical entropy? did. How-
ever, Parviz et al (2011) only looked at effects
on the sentence-final content word of items con-
structed for a speech perception experiment (Ka-
likow et al, 1977), rather than investigating sur-
prisal?s general predictive value across words of
naturally occurring sentences, as we did here.
Our experimental design was parametric rather
than factorial, which allowed us to study the effect
of surprisal over a sample of English sentences
rather than carefully manipulating surprisal while
holding other factors constant. This has the ad-
vantage that our findings are likely to generalise to
other sentence stimuli, but it can also raise a pos-
sible concern: The N400 effect may not be due
to surprisal itself, but to an unknown confound-
ing variable that was not included in the regression
analysis. However, this seems unlikely because of
two additional findings that only follow naturally
if surprisal is indeed the relevant predictor: Signif-
icant results only appeared where they were most
expected a priori (i.e., on N400 but not on other
components) and there was a nearly monotonic re-
lation between the models? word-prediction accu-
racy and their ability to account for N400 size.
7 Conclusion
Although word surprisal has often been shown
to be predictive of word-reading time (Fernan-
dez Monsalve et al, 2012; Frank and Thompson,
2012; Smith and Levy, in press), a general effect
on the EEG signal has not before been demon-
strated. Hence, these results provide additional ev-
idence in support of surprisal as a reliable measure
of cognitive processing difficulty during sentence
comprehension (Hale, 2001; Levy, 2008).
Acknowledgments
The research presented here was funded by the
European Union Seventh Framework Programme
(FP7/2007-2013) under grant number 253803.
The authors acknowledge the use of the UCL Le-
gion High Performance Computing Facility, and
associated support services, in the completion of
this work.
References
Y. Benjamini and Y. Hochberg. 1995. Controlling the
false discovery rate: a practical and powerful ap-
proach to multiple testing. Journal of the Royal Sta-
tistical Society B, 57:289?300.
S. F. Chen and J. Goodman. 1999. An empirical
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13:359?394.
M. Dambacher, R. Kliegl, M. Hofmann, and A. M. Ja-
cobs. 2006. Frequency and predictability effect on
event-related potentials during reading. Brain Re-
search, 1084:89?103.
A. Dubey, F. Keller, and P. Sturt. 2011. A model of
discourse predictions in human sentence processing.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
304?312. Edinburgh, UK: Association for Compu-
tational Linguistics.
I. Fernandez Monsalve, S. L. Frank, and G. Vigliocco.
2012. Lexical surprisal as a general predictor of
reading time. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, pages 398?408. Avi-
gnon, France: Association for Computational Lin-
guistics.
S. L. Frank and R. Bod. 2011. Insensitivity of the
human sentence-processing system to hierarchical
structure. Psychological Science, 22:829?834.
S. L. Frank and R. L. Thompson. 2012. Early ef-
fects of word surprisal on pupil size during read-
ing. In Proceedings of the 34th Annual Conference
of the Cognitive Science Society, pages 1554?1559.
Austin, TX: Cognitive Science Society.
S. L. Frank, I. Fernandez Monsalve, R. L. Thompson,
and G. Vigliocco. in press. Reading time data for
evaluating broad-coverage models of English sen-
tence processing. Behavior Research Methods.
S. L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In N. A. Taatgen and H. van Rijn,
editors, Proceedings of the 31st Annual Conference
of the Cognitive Science Society, pages 1139?1144.
Austin, TX: Cognitive Science Society.
882
S. L. Frank. in press. Uncertainty reduction as a mea-
sure of cognitive processing load in sentence com-
prehension. Topics in Cognitive Science.
A. D. Friederici, K. Steinhauer, and S. Frisch. 1999.
Lexical integration: sequential effects of syntactic
and semantic information. Memory & Cognition,
27:438?453.
T. C. Gunter, A. D. Friederici, and A. Hahne. 1999.
Brain responses during sentence reading: Visual
input affects central processes. NeuroReport,
10:3175?3178.
J. T. Hale. 2001. A probabilistic Early parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter of
the Association for Computational Linguistics, vol-
ume 2, pages 159?166. Pittsburgh, PA: Association
for Computational Linguistics.
E. Kaan. 2007. Event-related potentials and language
processing: a brief overview. Language and Lin-
guistics Compass, 1:571?591.
D. N. Kalikow, K. N. Stevens, and L. L. Elliott. 1977.
Development of a test of speech intelligibility in
noise using sentence materials with controlled word
predictability. Journal of the Acoustical Society of
America, 61:1337?1351.
D. Klein and C. D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st Meet-
ing of the Association for Computational Linguis-
tics, pages 423?430. Sapporo, Japan: Association
for Computational Linguistics.
M. Kutas and S. A. Hillyard. 1984. Brain potentials
during reading reflect word expectancy and semantic
association. Nature, 307:161?163.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106:1126?1177.
J. Mitchell, M. Lapata, V. Demberg, and F. Keller.
2010. Syntactic and semantic factors in process-
ing difficulty: An integrated measure. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 196?206. Up-
psala, Sweden: Association for Computational Lin-
guistics.
E. M. Moreno, K. D. Federmeier, and M. Kutas. 2002.
Switching languages, switching palabras (words):
an electrophysiological study of code switching.
Brain and Language, 80:188?207.
H. Neville, J. L. Nicol, A. Barss, K. I. Forster, and M. F.
Garrett. 1991. Syntactically based sentence pro-
cessing classes: evidence from event-related brain
potentials. Journal of Cognitive Neuroscience,
3:151?165.
L. Osterhout and P. J. Holcomb. 1992. Event-related
brain potentials elicited by syntactic anomaly. Jour-
nal of Memory and Language, 31:785?806.
M. Parviz, M. Johnson, B. Johnson, and J. Brock.
2011. Using language models and Latent Semantic
Analysis to characterise the N400m neural response.
In Proceedings of the Australasian Language Tech-
nology Association Workshop 2011, pages 38?46.
Canberra, Australia.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27:249?276.
N. J. Smith and R. Levy. in press. The effect of word
predictability on reading time is logarithmic. Cog-
nition.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing,
pages 901?904. Denver, Colorado.
C. Van Petten and B. J. Luka. 2012. Prediction during
language comprehension: benefits, costs, and ERP
components. International Journal of Psychophysi-
ology, 83:176?190.
883
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 81?89,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Uncertainty reduction as a measure of cognitive processing effort
Stefan L. Frank
University of Amsterdam
Amsterdam, The Netherlands
s.l.frank@uva.nl
Abstract
The amount of cognitive effort required to
process a word has been argued to depend
on the word?s effect on the uncertainty
about the incoming sentence, as quanti-
fied by the entropy over sentence probabil-
ities. The current paper tests this hypoth-
esis more thoroughly than has been done
before by using recurrent neural networks
for entropy-reduction estimation. A com-
parison between these estimates and word-
reading times shows that entropy reduc-
tion is positively related to processing ef-
fort, confirming the entropy-reduction hy-
pothesis. This effect is independent from
the effect of surprisal.
1 Introduction
In the field of computational psycholinguistics, a
currently popular approach is to account for read-
ing times on a sentence?s words by estimates of the
amount of information conveyed by these words.
Processing a word that conveys more information
is assumed to involve more cognitive effort, which
is reflected in the time required to read the word.
In this context, the most common formaliza-
tion of a word?s information content is its sur-
prisal (Hale, 2001; Levy, 2008). If word string
wt1 (short for w1, w2, . . . wt) is the sentence so
far and P (wt+1|wt1) the occurrence probability of
the next word wt+1, then that word?s surprisal is
defined as ? log P (wt+1|wt1). It is well estab-
lished by now that word-reading times indeed cor-
relate positively with surprisal values as estimated
by any sufficiently accurate generative language
model (Boston et al, 2008; Demberg and Keller,
2008; Frank, 2009; Roark et al, 2009; Smith and
Levy, 2008).
A lesser known alternative operationalization of
a word?s information content is based on the un-
certainty about the rest of the sentence, quantified
by Hale (2003, 2006) as the entropy of the prob-
ability distribution over possible sentence struc-
tures. The reduction in entropy that results from
processing a word is taken to be the amount of
information conveyed by that word, and was ar-
gued by Hale to be predictive of word-reading
time. However, this entropy-reduction hypothesis
has not yet been comprehensively tested, possibly
because of the difficulty of computing the required
entropies. Although Hale (2006) shows how sen-
tence entropy can be computed given a PCFG, this
computation is not feasible when the grammar is
of realistic size.
Here, we empirically investigate the entropy-
reduction hypothesis more thoroughly than has
been done before, by using recurrent neural net-
works as language models. Since these networks
do not derive any structure, they provide estimates
of sentence entropy rather than sentence-structure
entropy. In practice, these two entropies will gen-
erally be similar: If the rest of the sentence is
highly uncertain, so is its structure. Sentence en-
tropy can therefore be viewed as a simplification
of structure entropy; one that is less theory depen-
dent since it does not rely on any particular gram-
mar. The distinction between entropy over sen-
tences and entropy over structures will simply be
ignored in the remainder of this paper.
Results show that, indeed, a significant fraction
of variance in reading-time data is accounted for
by entropy reduction, over and above surprisal.
2 Entropy and sentence processing
2.1 Sentence entropy
Let W be the set of words in the language and W i
the set of all word strings of length i. The set of
complete sentences, denoted S, contains all word
strings of any length (i.e., ??i=0 W i), except that a
special end-of-sentence marker </s> is attached
to the end of each string.
81
A generative language model defines a proba-
bility distribution over S. The entropy of this dis-
tribution is
H = ?
?
wj1?S
P (wj1) log P (w
j
1).
As words are processed one by one, the sen-
tence probabilities change. When the first t words
(i.e., the string wt1 ? W t) of a sentence have been
processed, the entropy of the probability distribu-
tion over sentences is
H(t) = ?
?
wj1?S
P (wj1|wt1) log P (w
j
1|wt1). (1)
In order to simplify later equations, we define
the function h(y|x) = ?P (y|x) log P (y|x), such
that Eq. 1 becomes
H(t) =
?
wj1?S
h(wj1|wt1).
If the first t words of wj1 do not equal wt1 (or wj1
has fewer than t + 1 words),1 then P (wj1|wt1) = 0
so h(wj1|wt1) = 0. This means that, for computing
H(t), only the words from t + 1 onwards need to
be taken into account:
H(t) =
?
wjt+1?S
h(wjt+1|wt1).
The reduction in entropy due to processing the
next word, wt+1, is
?H(t + 1) = H(t)?H(t + 1). (2)
Note that positive ?H corresponds to a
decrease in entropy. According to Hale
(2006), the nonnegative reduction in entropy (i.e.,
max{0, ?H}) reflects the cognitive effort in-
volved in processing wt+1 and should therefore be
predictive of reading time on that word.
2.2 Suffix entropy
Computing H(t) is computationally feasible only
when there are very few sentences in S, or when
the language can be described by a small grammar.
To estimate entropy in more realistic situations, an
1Since wj1 ends with < /s > and wt1 does not, the two
strings must be different. Consequently, if wj1 is t words long,
then P (wj1|wt1) = 0.
obvious solution is to look only at the next few
words instead of all complete continuations of wt1.
Let Sm be the subset of S containing all (and
only) sentences of length m or less, counting also
the </s> at the end of each sentence. Note that
this set includes the ?empty sentence? consisting
of only </s>. The set of length-m word strings
that do not end in </s> is Wm. Together, these
sets form Wm = Wm ? Sm, which contains all
the relevant strings for defining the entropy over
strings up to length m.2 After processing wt1, the
entropy over strings up to length t + n is:
Hn(t) =
?
wj1?Wt+n
h(wj1|wt1) =
?
wjt+1?Wn
h(wjt+1|wt1).
It now seems straightforward to define suffix-
entropy reduction by analogy with sentence-
entropy reduction as expressed in Eq. 2: Simply
replace H by Hn to obtain
?Hsufn (t + 1) = Hn(t)?Hn(t + 1). (3)
As indicated by its superscript label, ?Hsufn
quantifies the reduction in uncertainty about the
upcoming n-word suffix. However, this is concep-
tually different from the original ?H of Eq. 2,
which is the reduction in uncertainty about the
identity of the current sentence. The difference
becomes clear when we view the sentence proces-
sor?s task as that of selecting the correct element
from S. If this set of complete sentences is ap-
proximated by Wt+n, and the task is to select one
element from that set, an alternative definition of
suffix-entropy reduction arises:
?Hsentn (t + 1)
=
?
wj1?Wt+n
h(wj1|wt1) ?
?
wj1?Wt+n
h(wj1|wt+11 )
=
?
wjt+1?Wn
h(wjt+1|wt1) ?
?
wjt+2?Wn?1
h(wjt+2|wt+11 )
= Hn(t)?Hn?1(t + 1). (4)
The label ?sent? indicates that ?Hsentn quantifies
the reduction in uncertainty about which sentence
forms the current input. This uncertainty is ap-
proximated by marginalizing over all word strings
longer than t + n.
It is easy to see that
lim
n??
?Hsufn = limn???H
sent
n = ?H,
2The probability of a string wm1 ? W m is the summed
probability of all sentences with prefix wm1 .
82
so both approximations of entropy reduction ap-
propriately converge to ?H in the limit. Nev-
ertheless, they formalize different quantities and
may well correspond to different cognitive factors.
If it is true that cognitive effort is predicted by
the reduction in uncertainty about the identity of
the incoming sentence, we should find that word-
reading times are predicted more accurately by
?Hsentn than by ?Hsufn .
2.3 Relation to next-word entropy
In the extreme case of n = 1, Eq. 4 reduces to
?Hsent1 (t + 1) = H1(t)?H0(t + 1) = H1(t),
so the reduction of entropy over the single next
word wt+1 equals the next-word entropy just be-
fore processing that word. Note that ?Hsent1 (t+1)
is independent of the word at t + 1, making it a
severely impoverished measure of the uncertainty
reduction caused by that word. We would there-
fore expect reading times to be predicted more ac-
curately by ?Hsentn with n > 1, and possibly even
by ?Hsuf1 .
Roark et al (2009) investigated the relation be-
tween H1(t + 1) and reading time on wt+1, and
found a significant positive effect: Larger next-
word entropy directly after processing wt+1 cor-
responded to longer reading time on that word.
This is of particular interest because H1(t + 1)
necessarily correlates negatively with entropy re-
duction ?Hsentn (t + 1): If entropy is large after
wt+1, chances are that it did not reduce much
through processing of wt+1. Indeed, in our data
set, H1(t + 1) and ?Hsentn (t + 1) correlate be-
tween r = ?.29 and r = ?.26 (for n = 2 to
n = 4) which is highly significantly (p ? 0) dif-
ferent from 0. Roark et al?s finding of a positive
relation between H1(t + 1) and reading time on
wt+1 therefore seems to disconfirm the entropy-
reduction hypothesis.
3 Method
A set of language models was trained on a corpus
of POS tags of sentences. The advantage of using
POS tags rather than words is that their probabil-
ities can be estimated much more accurately and,
consequently, more accurate prediction of word-
reading time is possible (Demberg and Keller,
2008; Roark et al, 2009). Subsequent to training,
the models were made to generate estimates of sur-
prisal and entropy reductions ?Hsufn and ?Hsentn
over a test corpus. These estimates were then com-
pared to reading times measured over the words
of the same test corpus. This section presents the
data sets that were used, language-model details,
and the evaluation metric.
3.1 Data
The models were trained on the POS tag se-
quences of the full WSJ corpus (Marcus et al,
1993). They were evaluated on the POS-tagged
Dundee corpus (Kennedy and Pynte, 2005), which
has been used in several studies that investigate the
relation between word surprisal and reading time
(Demberg and Keller, 2008; Frank, 2009; Smith
and Levy, 2008). This 2 368-sentence (51 501
words) collection of British newspaper editorials
comes with eye-tracking data of 10 participants.
POS tags for the Dundee corpus were taken from
Frank (2009).
For each word and each participant, reading
time was defined as the total fixation time on that
word before any fixation on a later word of the
same sentence. Following Demberg and Keller
(2008), data points (i.e., word/participant pairs)
were removed if the word was not fixated, was
presented as the first or last on a line, contained
more than one capital letter or a non-letter (e.g.,
the apostrophe in a clitic), or was attached to punc-
tuation. Mainly due to the large number (over
46%) of nonfixations, 62.8% of data points were
removed, leaving 191 380 data points (between
16 469 and 21 770 per participant).
3.2 Language model
Entropy is more time consuming to compute than
surprisal, even for n = 1, because it requires es-
timates of the occurrence probabilities at t + 1 of
all word types, rather than just of the actual next
word. Moreover, the number of suffixes rises ex-
ponentially as suffix length n grows, and, conse-
quently, so does computation time.
Roark et al (2009) used an incremental PCFG
parser to obtain H1 but this method rapidly be-
comes infeasible as n grows. Low-order Markov
models (e.g., a bigram model) are more efficient
and can be used for larger n but they do not form
particularly accurate language models. Moreover,
Markov models lack cognitive plausibility.
Here, Simple Recurrent Networks (SRNs) (El-
man, 1990) are used as language models. When
trained to predict the upcoming input in a word se-
quence, these networks can generate estimates of
83
P (wt+1|wt1) efficiently and relatively accurately.
They thereby allow to approximate sentence en-
tropy more closely than the incremental parsers
used in previous studies. Unlike Markov models,
SRNs have been claimed to form cognitively re-
alistic sentence-processing models (Christiansen
and MacDonald, 2009). Moreover, it has been
shown that SRN-based surprisal estimates can cor-
relate more strongly to reading times than surprisal
values estimated by a phrase-structure grammar
(Frank, 2009).
3.2.1 Network architecture and processing
The SRNs comprised three layers of units: the in-
put layer, the recurrent (hidden) layer, and the out-
put layer. Each input unit corresponds to one POS
tag, making 45 input units since there are 45 dif-
ferent POS tags in the WSJ corpus. The network?s
output units represent predictions of subsequent
inputs. The output layer also has one unit for each
POS tag, plus an extra unit that represents </s>,
that is, the absence of any further input. Hence,
there were 46 output units. The number of recur-
rent units was fairly arbitrarily set to 100.
As is common in these networks, the input layer
was fully connected to the recurrent layer, which
in turn was fully connected to the output layer.
Also, there were time-delayed connections from
the recurrent layer to itself. In addition, each re-
current and output unit received a bias input.
The vectors of recurrent- and output-layer ac-
tivations after processing wt1 are denoted arec(t)
and aout(t), respectively. At the beginning of each
sentence, arec(0) = 0.5.
The input vector aiin, representing POS tag i,
consists of zeros except for a single element (cor-
responding to i) that equals one. When input i is
processed, the recurrent layer?s state is updated ac-
cording to:
arec(t) = frec(Wrecarec(t? 1) + Winaiin + brec),
where matrices Win and Wrec contain the net-
work?s input and recurrent connection weights, re-
spectively; brec is the vector of recurrent-layer bi-
ases; and activation function frec(x) is the logistic
function f(x) = (1+e?x)?1 applied elementwise
to x. The new output vector is now given by
aout(t) = fout(Woutarec(t) + bout),
where Wout is the matrix of output connection
weights; bout the vector of output-layer biases; and
fout(x) the softmax function
fi,out(x1, . . . , x46) =
exi
?
j e
xj .
This function makes sure that aout sums to one
and can therefore be viewed as a probability dis-
tribution: The i-th element of aout(t) is the SRN?s
estimate of the probability that the i-th POS tag
will be the input at t + 1, or, in case i corresponds
to < /s >, the probability that the sentence ends
after t POS tags.
3.2.2 Network training
Ten SRNs, differing only in their random initial
connection weights and biases, were trained us-
ing the standard backpropagation algorithm. Each
string of WSJ POS tags was presented once, with
the sentences in random order. After each POS in-
put, connection weights were updated to minimize
the cross-entropy between the network outputs and
a 46-element vector that encoded the next input (or
marked the end of the sentence) by the correspond-
ing element having a value of one and all others
being zero.
3.3 Evaluation
3.3.1 Obtaining surprisal and entropy
Since aout(t) is basically the probability distribu-
tion P (wt+1|wt1), surprisal and H1 can be read off
directly. To obtain H2, H3, and H4, we use the
fact that
P (wt+nt+1 |wt1) =
n
?
i=1
P (wt+i|wt+i?11 ). (5)
Surprisal and entropy estimates were averaged
over the ten SRNs. So, for each POS tag of the
Dundee corpus, there was one estimate of surprisal
and four of entropy (for n = 1 to n = 4).
Since Hn(t) approximates H(t) more closely
as n grows, it would be natural to expect a better
fit to reading times for larger n. On the other hand,
it goes without saying that Hn is only a very rough
measure of a reader?s actual uncertainty about the
upcoming n inputs, no matter how accurate the
language model that was used to compute these
entropies. Crucially, the correspondence between
Hn and the uncertainty experienced by a reader
will grow even weaker with larger n. This is ap-
parent from the fact that, as proven in the Ap-
pendix, Hn can be expressed in terms of H1 and
Hn?1:
Hn(t) = H1(t) + E(Hn?1(t + 1)),
84
1 2 3 4
0
0.25
0.5
suffix length n
co
rr
e
la
tio
n 
w
ith
 s
ur
pr
isa
l
?H
n
suf
?H
n
sent
Figure 1: Coefficient of correlation between es-
timates of surprisal and entropy reduction, as a
function of suffix length n.
where E(x) is the expected value of x. Obviously,
the expected value of Hn?1 is less appropriate as
an uncertainty measure than is Hn?1 itself. Hence,
Hn can be less accurate than Hn?1 as a quantifi-
cation of the actual cognitive uncertainty. For this
reason, we may expect larger n to result in worse
fit to reading-time data.3
3.3.2 Negative entropy reduction
Hale (2006) argued for nonnegative entropy re-
duction max{0, ?H}, rather than ?H itself, as
a measure of processing effort. For ?Hsent, the
difference between the two is negligible because
only about 0.03% of entropy reductions are neg-
ative. As for ?Hsuf, approximately 42% of val-
ues are negative so whether these are left out
makes quite a difference. Since preliminary ex-
periments showed that word-reading times are pre-
dicted much more accurately by ?Hsuf than by
max{0, ?Hsuf}, only ?Hsuf and ?Hsent were
used here, that is, negative values were included.
3.3.3 Relation between information measures
Both surprisal and entropy reduction can be taken
as measures for the amount of information con-
veyed by a word, so it is to be expected that they
are positively correlated. However, as shown in
Figure 1, this correlation is in fact quite weak,
ranging from .14 for ?Hsuf4 to .38 for ?Hsent1 .
In contrast, ?Hsufn and ?Hsentn correlate very
strongly to each other: The coefficients of correla-
tion range from .73 when n = 1 to .97 for n = 4.
3Not to mention the realistic possibility that the cognitive
sentence-processing system does not abide by the normative
chain rule expressed in Eq. 5.
0 4 8 12
10?4
10?3
10?2
10?1
100
Effect size
Si
gn
ific
an
ce
 (p
?v
alu
e)
p = .05
3.84
Figure 2: Cumulative ?2 distribution with 1 de-
gree of freedom, plotting statistical significance
(p-value) as a function of effect size.
3.3.4 Fit to reading times
A generalized linear regression model for gamma-
distributed data was fitted to the reading times.4
This model contained several well-known predic-
tors of word-reading time: the number of letters
in the word, the word?s position in the sentence,
whether the next word was fixated, whether the
previous word was fixated, log of the word?s rel-
ative frequency, log of the word?s forward and
backward transitional probabilities,5 and surprisal
of the part-of-speech. Next, one set of entropy-
reduction estimates was added to the regression.
The effect size is the resulting decrease in the re-
gression model?s deviance, which is indicative of
the amount of variance in reading time accounted
for by those estimates of entropy reduction. Fig-
ure 2 shows how effect size is related to statis-
tical significance: A factor forms a significant
(p < .05) predictor of reading time if its effect
size is greater than 3.84.
4 Results and Discussion
4.1 Effect of entropy reduction
Figure 3 shows the effect sizes for both measures
of entropy reduction, and their relation to suffix
length n. All effects are in the correct direction,
that is, larger entropy reduction corresponds to
longer reading time. These results clearly support
the entropy-reduction hypothesis: A significant
4The reading times, which are approximately gamma dis-
tributed, were first normalized to make the scale parameters
of the gamma distributions the same across participants.
5These are, respectively, the relative frequency of the
word given the previous word, and its relative frequency
given the next word.
85
1 2 3 4
0
5
10
suffix length n
?H
n
 
e
ffe
ct
 s
ize
?H
n
suf
?H
n
sent
Figure 3: Size of the effect of ?Hsufn and ?Hsentn
as a function of suffix length n.
fraction of variance in reading time is accounted
for by the entropy-reduction estimates ?Hsentn ,
over and above what is explained by the other fac-
tors in the regression analysis, including surprisal.
Moreover, the effect of ?Hsentn is larger than that
of ?Hsufn , indicating that it is indeed uncertainty
about the identity of the current sentence, rather
than uncertainty about the upcoming input(s), that
matters for cognitive processing effort. Only at
n = 1 was the effect size of ?Hsentn smaller than
that of ?Hsufn , but it should be kept in mind that
?Hsent1 is independent of the incoming word and
is therefore quite impoverished as a measure of the
effort involved in processing the word. Moreover,
the difference between ?Hsent1 and ?Hsuf1 is not
significant (p > .4), as determined by the boot-
strap method (Efron and Tibshirani, 1986). In con-
trast, the differences are significant when n > 1
(all p < .01), in spite of the high correlation be-
tween ?Hsentn and ?Hsufn .
Another indication that cognitive processing ef-
fort is modeled more accurately by ?Hsentn than by
?Hsufn is that the effect size of ?Hsentn seems less
affected by n. Even though ?H , the reduction in
entropy over complete sentences, is approximated
more closely as suffix length grows, increasing n
is strongly detrimental to the effect of ?Hsufn : It
is no longer significant for n > 2. Presumably,
this can be (partly) attributed to the impoverished
relation between formal entropy and psychologi-
cal uncertainty, as explained in Section 3.3.1. In
any case, the effect of ?Hsentn is more stable. Al-
though ?Hsufn and ?Hsentn necessarily converge as
n ??, the two effect sizes seem to diverge up to
1 2 3 4
5
10
15
20
e
ffe
ct
 s
ize
suffix length n
H1
surprisal
?H
n
sent
Figure 4: Effect size of entropy reduction
(?Hsentn ), next-word entropy (H1), or surprisal,
over and above the other two predictors.
n = 3: The difference between the effect sizes
of ?Hsentn and ?Hsufn is marginally significantly
(p < .07) larger for n = 3 than for n = 2.
4.2 Effects of other factors
It is also of interest that surprisal has a significant
effect over and above entropy reduction, in the cor-
rect (i.e., positive) direction. When surprisal esti-
mates are added to a regression model that already
contains ?Hsentn , the effect size ranges from 8.7
for n = 1 to 13.9 for n = 4. This show that there
exist independent effects of surprisal and entropy
reduction on processing effort.
Be reminded from Section 2.3 that Roark et al
(2009) found a positive relation between reading
time on wt+1 and H1(t + 1), the next-word en-
tropy after processing wt+1. When that value is
added as a predictor in the regression model that
already contains surprisal and entropy reduction
?Hsentn , model fit greatly improves. In fact, as can
be seen from comparing Figures 3 and 4, the ef-
fect of ?Hsentn is strengthened by including next-
word entropy in the regression model. Moreover,
each of the factors surprisal, entropy reduction,
and next-word entropy has a significant effect over
and above the other two. In all cases, these ef-
fects were in the positive direction. This confirms
Roark et al?s finding and shows that it is in fact
compatible with the entropy-reduction hypothesis,
in contrast to what was suggested in Section 2.3.
86
5 Discussion and conclusion
The current results contribute to a growing body of
evidence that the amount of information conveyed
by a word in sentence context is indicative of the
amount of cognitive effort required for processing,
as can be observed from reading time on the word.
Several previous studies have shown that surprisal
can serve as a cognitively relevant measure for a
word?s information content. In contrast, the rele-
vance of entropy reduction as a cognitive measure
has not been investigated this thoroughly before.
Hale (2003; 2006) presents entropy-reduction ac-
counts of particular psycholinguistic phenomena,
but does not show that entropy reduction gener-
ally correlates with word-reading times. Roark et
al. (2009) presented data that could be taken as ev-
idence against the entropy-reduction hypothesis,
but the current paper showed that the next-word
entropy effect, found by Roark et al, is indepen-
dent of the entropy-reduction effect.
It is tempting to take the independent effects
of surprisal and entropy reduction as evidence
for two distinct cognitive representations or pro-
cesses, one related to surprisal, the other to en-
tropy reduction. However, it is very well possible
that these two information measures are merely
complementary formalizations of a single, cogni-
tively relevant notion of word information. Since
the quantitative results presented here provide no
evidence for either view, a more detailed qualita-
tive analysis is needed.
In addition, the relation between reading time
and the two measures of word information may
be further clarified by the development of mech-
anistic sentence-processing models. Both the sur-
prisal and entropy-reduction theories provide only
functional-level descriptions (Marr, 1982) of the
relation between information content and process-
ing effort, so the question remains which under-
lying mechanism is responsible for longer read-
ing times on words that convey more information.
That is, we are still without a model that pro-
poses, at Marr?s computational level, some spe-
cific sentence-processing mechanism that takes
longer to process a word that has higher surprisal
or leads to greater reduction in sentence entropy.
For surprisal, Levy (2008) makes a first step in
that direction by presenting a mechanistic account
of why surprisal would predict word-reading time:
If the state of the sentence-processing system is
viewed as a probability distribution over all possi-
ble interpretations of complete sentences, and pro-
cessing a word comes down to updating this distri-
bution to incorporate the new information, then the
word?s surprisal equals the Kullback-Leibler di-
vergence from the old distribution to the new. This
divergence is presumed to quantify the amount of
work (and, therefore, time) needed to update the
distribution. Likewise, Smith and Levy (2008) ex-
plain the surprisal effect in terms of a reader?s opti-
mal preparation to incoming input. When it comes
to entropy reduction, however, no reading-time
predicting mechanism has been proposed. Ideally,
of course, there should be a single computational-
level model that predicts the effects of both sur-
prisal and entropy reduction.
One recent model (Frank, 2010) shows that the
reading-time effects of both surprisal and entropy
reduction can indeed result from a single pro-
cessing mechanism. The model simulates sen-
tence comprehension as the incremental and dy-
namical update of a non-linguistic representation
of the state-of-affairs described by the sentence.
In this framework, surprisal and entropy reduc-
tion are defined with respect to a probabilistic
model of the world, rather than a model of the
language: The amount of information conveyed
by a word depends on what is asserted by the
sentence-so-far, and not on how the sentence?s
form matches the statistical patterns of the lan-
guage. As it turns out, word-processing times in
the sentence-comprehension model correlate pos-
itively with both surprisal and entropy reduction.
The model thereby forms a computational-level
account of the relation between reading time and
both measures of word information. According
to this account, the two information measures do
not correspond to two distinct cognitive processes.
Rather, there is one comprehension mechanism
that is responsible for the incremental revision of
a mental representation. Surprisal and entropy re-
duction form two complementary quantifications
of the extent of this revision.
Acknowledgments
The research presented here was supported by
grant 277-70-006 of the Netherlands Organization
for Scientific Research (NWO). I would like to
thank Rens Bod, Reut Tsarfaty, and two anony-
mous reviewers for their helpful comments.
87
References
M. F. Boston, J. Hale, U. Patil, R. Kliegl, and S. Va-
sishth. 2008. Parsing costs as predictors of read-
ing difficulty: An evaluation using the Potsdam Sen-
tence Corpus. Journal of Eye Movement Research,
2:1?12.
M. H. Christiansen and M. C. MacDonald. 2009. A
usage-based approach to recursion in sentence pro-
cessing. Language Learning, 59:129?164.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193?210.
B. Efron and R. Tibshirani. 1986. Bootstrap methods
for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1:54?75.
J. L. Elman. 1990. Finding structure in time. Cogni-
tive Science, 14:179?211.
S. L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In N. A. Taatgen and H. van Rijn,
editors, Proceedings of the 31st Annual Conference
of the Cognitive Science Society, pages 1139?1144.
Austin, TX: Cognitive Science Society.
S. L. Frank. 2010. The role of world knowledge in
sentence comprehension: an information-theoretic
analysis and a connectionist simulation. Manuscript
in preparation.
J. Hale. 2001. A probabilistic Early parser as a psy-
cholinguistic model. In Proceedings of the sec-
ond conference of the North American chapter of
the Association for Computational Linguistics, vol-
ume 2, pages 159?166. Pittsburgh, PA: Association
for Computational Linguistics.
J. Hale. 2003. The information conveyed by words.
Journal of Psycholinguistic Research, 32:101?123.
J. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30:643?672.
A. Kennedy and J. Pynte. 2005. Parafoveal-on-foveal
effects in normal reading. Vision Research, 45:153?
168.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106:1126?1177.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the Penn Treebank. Computational Linguis-
tics, 19:313?330.
D. Marr. 1982. Vision. San Francisco: W.H. Freeman
and Company.
B. Roark, A. Bachrach, C. Cardenas, and C. Pallier.
2009. Deriving lexical and syntactic expectation-
based measures for psycholinguistic modeling via
incremental top-down parsing. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 324?333. Associ-
ation for Computational Linguistics.
N. J. Smith and R. Levy. 2008. Optimal processing
times in reading: a formal model and empirical in-
vestigation. In B. C. Love, K. McRae, and V. M.
Sloutsky, editors, Proceedings of the 30th Annual
Conference of the Cognitive Science Society, pages
595?600. Austin, TX: Cognitive Science Society.
88
Appendix
It is of some interest that Hn can be expressed in
terms of H1 and the expected value of Hn?1. First,
note that
h(wjt+1|wt1) = ?P (w
j
t+1|wt1) log P (w
j
t+1|wt1)
= ?P (wt+1|wt1)P (wjt+2|wt+11 ) log
(
P (wt+1|wt1)P (wjt+2|wt+11 )
)
= P (wjt+2|wt+11 )h(wt+1|wt1) + P (wt+1|wt1)h(w
j
t+2|wt+11 ).
For entropy Hn(t), this makes
Hn(t) =
?
wjt+1?Wn
h(wjt+1|wt1)
=
?
wjt+1?Wn
P (wjt+2|wt+11 )h(wt+1|wt1) +
?
wjt+1?Wn
P (wt+1|wt1)h(wjt+2|wt+11 )
=
?
wt+1?W1
?
?
?
h(wt+1|wt1)
?
wjt+2?Wn?1
P (wjt+2|wt+11 )
?
?
?
+
?
wt+1?W1
?
?
?
P (wt+1|wt1)
?
wjt+2?Wn?1
h(wjt+2|wt+11 )
?
?
?
=
?
wt+1?W1
h(wt+1|wt1) +
?
wt+1?W1
P (wt+1|wt1)Hn?1(t + 1)
= H1(t) + E(Hn?1(t + 1)).
89
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 58?67,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
Generating Subjective Responses to Opinionated Articles in Social Media:
An Agenda-Driven Architecture and a Turing-Like Test
Tomer Cagan
School of Computer Science
The Interdisciplinary Center
Herzeliya, Israel
cagan.tomer@idc.ac.il
Stefan L. Frank
Centre for Language Studies
Radboud University
Nijmegen, The Netherlands
s.frank@let.ru.nl
Reut Tsarfaty
Mathematics and Computer Science
Weizmann Institute of Science
Rehovot, Israel
tsarfaty@weizmann.ac.il
Abstract
Natural language traffic in social media
(blogs, microblogs, talkbacks) enjoys vast
monitoring and analysis efforts. How-
ever, the question whether computer sys-
tems can generate such content in order
to effectively interact with humans has
been only sparsely attended to. This pa-
per presents an architecture for generat-
ing subjective responses to opinionated
articles based on users? agenda, docu-
ments? topics, sentiments and a knowledge
graph. We present an empirical evalua-
tion method for quantifying the human-
likeness and relevance of the generated re-
sponses. We show that responses gen-
erated using world knowledge in the in-
put are regarded as more human-like than
those that rely on topic, sentiment and
agenda only, whereas the use of world
knowledge does not affect perceived rel-
evance.
1 Introduction
Digital media, user-generated content and social
networks enable effective human interaction; so
much so that much of our day-to-day interaction
is conducted online (Viswanath et al., 2009). In-
teraction in social media fundamentally changes
the way businesses and consumers behave (Qual-
man, 2012), can be instrumental to the success
of individuals and businesses (Haenlein and Ka-
plan, 2009), and even affects the stability of polit-
ical regimes (Howard et al., 2011; Lamer, 2012).
These facts force organizations (businesses, gov-
ernments, and non-profit organizations) to be con-
stantly involved in the monitoring of, and the inter-
action with, human agents in digital environments
(Langheinrich and Karjoth, 2011).
Automatic analysis of user-generated online
content benefits from extensive research and com-
mercial opportunities. In natural language pro-
cessing, there is ample research on the analysis
of subjectivity and sentiment of content in social
media. The development of tools for sentiment
analysis (Davidov et al., 2010), mood aggregation
(Agichtein et al., 2008), opinion mining (Mishne,
2006), and many more, now enjoys wide inter-
est and exposure, as is also evident by the many
workshops and dedicated tracks at ACL venues.
1
Methods are also developed for the analysis of po-
litical texts (O?Connor et al., 2010; O?Connor et
al., 2013) and for text-driven forecasting based on
these data (Yano et al., 2009). A related strand
of research uses computational methods to find
out what kind of published utterances are influ-
ential, and how they affect linguistic communi-
ties (Danescu-Niculescu-Mizil et al., 2009). Such
work complements, and contributes to, studies
from sociology and sociolinguistics that aim to de-
lineate the process of generating meaningful re-
sponses (e.g., Amabile (1981)).
In contrast to these analysis efforts, the topic
of generating responses to content in social me-
dia is only sparsely explored. Commercially, there
is movement towards online response automation
(Owyang, 2012; Mah, 2012).
2
Research on user
interfaces is trying to move away from script-
based interaction towards the development of chat
bots that attempt natural human-like interaction
(Mori et al., 2003; Feng et al., 2006). However,
these chat bots are typically designed to provide
an automated one-size-fits-all type of interaction.
A study by Ritter et al. (2011) addresses
the generation of responses to natural language
tweets in a data-driven setup. It applies a
machine-translation approach to response gener-
ation, where moods and sentiments already ex-
1
E.g., the ACL series LASM http://tinyurl.com/
ludyrkz; WASSA http://tinyurl.com/kjjdhax.
2
There is a general debate on the efficiency of automated
tools (Nall, 2013) and whether such tools are desirable in so-
cial media (McConnell (2012); responses to Owyang (2012)).
58
pressed in the past are replicated or reused. A re-
cent study by Hasegawa et al. (2013) modifies Rit-
ter?s approach to produce responses that elicit an
emotion from the addressee. Yet, these responses
do not target particular topics and are not driven
by a user agenda.
The present paper addresses the problem of
generating novel, subjective, responses to on-
line opinionated articles. We formally define the
document-to-response mapping problem and sug-
gest an end-to-end system to solve it. Our sys-
tem integrates a range of NLP and NLG technolo-
gies (including topic models, sentiment analysis,
and the integration of a knowledge graph) to de-
sign a flexible generation mechanism that allows
us to vary the information in the input to the gen-
eration procedure. We then use a Turing-inspired
test to study the different factors that contribute to
the perceived human-likeness and relevance of the
generated responses, and show how the perception
of responses depends on external knowledge and
the expressed sentiment.
The remainder of this paper is organized as fol-
lows. The next section presents our proposal: Sec-
tion 2.1 describes our approach, Section 2.2 for-
malizes the proposal, and Section 2.3 presents our
end-to-end architecture. This is followed by our
evaluation method and empirical results in Sec-
tion 3. We discuss related and future work in Sec-
tion 4, and in Section 5 we conclude.
2 The Proposal: Generating Subjective
Responses
2.1 Our Approach
Natural language is, above all, a communicative
device that we employ to achieve certain goals.
In social media, the driving force behind generat-
ing responses is a responder?s disposition towards
some topic. This topic could be a political cam-
paign or a candidate, a product, or some abstract
idea, which the responder has a motive to promote.
Let us call this goal our user?s agenda.
User response generation, like any other natu-
ral language utterance generation, is triggered by
a certain event that is related to the communica-
tive goal. In a social media setting, this event
is often a new online document. The document
and the agenda thus form the input to our gener-
ation system. Each document and each agenda
contain (possibly many) topics, each of which is
associated with a (positive or negative) sentiment.
Document sentiments are attributed to the author,
whereas agenda sentiments are attributed to the
user (henceforth: the responder).
For each non-empty intersection of the topics
in the document and in the agenda, our response-
generation system aims to generate utterances that
are fluent, human-like, and effectively engage
readers. The generation is based on three assump-
tions, roughly reflecting the Gricean maxims of
cooperative interaction (Grice, 1967). Online user
responses should then be:
? Economic (Maxim of Quantity): Responses
are brief and concise;
? Relevant (Maxim of Relation): Responses di-
rectly address the documents? content.
? Opinionated (Maxim of Quality): Responses
express responders beliefs, sentiments, or
dispositions towards the topic(s).
2.2 The Formal Model
Let D be a set of documents and let A be a set
of user agendas as we define shortly. Let S be a
set of English sentences over a finite vocabulary
S = ?
?
. Our system implements a function that
maps each ?document, agenda? pair to a natural
language response sentence s ? S.
f
response
: D ?A? S
Response generation takes place in two phases,
roughly corresponding to macro and micro plan-
ning in Reiter and Dale (1997):
? Macro Planning (below, the analysis phase):
What are we going to talk about?
? Micro Planning (below, the generation
phase): How are we going to say it?
The analysis function p : D ? C maps a docu-
ment to a subjective representation of its content.
3
The generation function g : C ? A ? S inter-
sects the content elements in the document and in
the user agenda, and generates a response based
on the content of the intersection. All in all, our
system implements a composition of the analysis
and the generation functions:
f
response
(d, a) = g(p(d), a) = s
3
A content element may conceivably encompass a topic,
its sentiment, its objectivity, its evidentiality, its perceived
truthfulness, and so on. In this paper we focus on topic and
sentiment, and leave the rest for future research.
59
Each content element c ? C or an agenda item
a ? A is composed of a topic t associated with a
sentiment value sentiment
t
? [?n..n] that sig-
nifies the (negative or positive) disposition of the
document?s author (if c ? C) or the user?s agenda
(if a ? A) towards the topic. We assume here that
a topic is simply a bag of words from our vocabu-
lary ?. Thus, we have the following:
A,C ? P(?)? [?n..n]
Our generation component accepts the result of
the intersection as input and relies on a template-
based grammar and a set of functions for generat-
ing referring expressions in order to construct the
output. To make the responses economic, we limit
the content of a response to one statement about
the document or its author, followed by a state-
ment on the relevant topic. To make the response
relevant, the templates that generate the response
make use of topics in the intersection of the docu-
ment and the agenda. To make the response opin-
ionated, the sentiment of the response depends on
the (mis)match between the sentiment values for
the topic in the document and in the agenda. Con-
cretely, the response is positive if the sentiments
for the topic in the document and agenda are the
same (both positive or both negative) and it is neg-
ative otherwise.
We suggest two variants of the generation func-
tion g. The basic variant implements the baseline
function defined above:
g
base
(c, a) = s
c ? C, a ? A, s ? ?
?
For the other variant we define a knowledge
base (KB) as a directed graph in which words
w ? ? from the topic models correspond to nodes
in the graph, and relations r ? R between the
words are predicates that hold in the real world.
Our second generation function now becomes:
g
kb
(c, a,KB) = s
KB ? {(w
i
, r, w
j
)|w
i
, w
j
? ?, r ? R}
with c ? C, a ? A, s ? ?
?
as defined in g
base
above.
2.3 The Architecture
The system architecture from a bird?s eye view
is presented in Figure 1. In a nutshell, a docu-
ment enters the analysis phase, where topic infer-
ence and sentiment scoring take place, resulting
in ?topic, sentiment?-pairs. During the subsequent
generation phase, these are intersected with the
?topic, sentiment?-pairs in the user agenda. This
intersection, possibly augmented with a knowl-
edge graph, forms the input for a template-based
generation component.
Analysis phase For the task of inferring the top-
ics of the document we use topic modeling: a
probabilistic generative modeling technique that
allows for the discovery of abstract topics over
a large body of documents (Papadimitriou et al.,
1998; Hofmann, 1999; Blei et al., 2003). Specif-
ically, we use topic modeling based on Latent
Dirichlet Allocation (LDA) (Blei et al., 2003; Blei,
2012). Given a new document and a trained
model, the inference method provides a weighted
mix of topics for that document, where each topic
is represented as a vector containing keywords as-
sociated with probabilities. For training the topic
model and inferring the topics in new documents
we use Gensim (Rehurek and Sojka, 2010), a fast
and easy-to-use implementation of LDA.
Next, we wish to infer the sentiment that is ex-
pressed in the text with relation to the topic(s)
identified in the document. We use the seman-
tic/lexical method as implemented in Kathuria
(2012). We rely on a WSD sentiment classifier
that uses the SentiWordNet (Baccianella et al.,
2010) database and calculates the positivity and
negativity scores of a document based on the pos-
itivity and negativity of individual words. The re-
sult of the sentiment analysis is a pair of values,
indicating the positive and negative sentiments of
the document-based scores for individual words.
We use the larger of these two values as the senti-
ment value for the whole document.
4
Generation phase Our generation function first
intersects the set of topics in the document and the
set of topics in the agenda in order to discover rel-
evant topics to which the system would generate
responses. A response may in principle integrate
content from a range of topics in the topic model
distribution, but, for the sake of generating concise
responses, in the current implementation we focus
on the single most prevalent, topic. We pick the
highest scoring word of the highest scoring topic,
and intersect it with topics in the agenda. The sys-
tem generates a response based on the identified
4
Clearly, this is a simplifying assumption. We discuss this
assumption further in Section 4.
60
Figure 1: The system architecture from a bird?s eye view. Components on gray background are executed
offline.
topic, the sentiment for the topic in the document,
and the sentiment for that topic in the user agenda.
The generation component relies on a template-
based approach similar to Reiter and Dale (1997)
and Van Deemter et al. (2005). Templates are
essentially subtrees with leaves that are place-
holders for other templates or for functions gener-
ating referring expressions (Theune et al., 2001).
These functions receive (relevant parts of) the in-
put and emit the sequence of fine-grained part-of-
speech (POS) tags that realizes the relevant refer-
ring expression. The POS tags in the resulting
sequences are ultimately place holders for words
from a lexicon ?. In order to generate a variety of
expression forms ? nouns, adjectives and verbs
? these items are selected randomly from a fine-
grained lexicon we defined. The sentiment (posi-
tive or negative) is expressed in a similar fashion
via templates and randomly selected lexical en-
tries for the POS slots, after calculating the over-
all sentiment for the intersection as stated above.
Our generation implementation is based on Sim-
pleNLG (Gatt and Reiter, 2009) which is a surface
realizer API that allows us to create the desired
templates and functions, and aggregates content
into coherent sentences. The templates and func-
tions that we defined are depicted in Figure 2.
In addition, we handcrafted a simple knowledge
graph (termed here KB) containing the words in a
set of pre-defined user agendas. Table 1 shows a
snippet of the constructed knowledge graph. The
knowledge graph can be used to expand the re-
sponse in the following fashion: The topic of the
response is a node in the KB. We randomly se-
lect one of its outgoing edges for creating a related
Source Relation Target
Apple CompetesWith Samsung
Apple CompetesWith Google
Apple Creates iOS
Table 1: A knowledge graph snippet.
statement that has the target node of this relation
as its subject. The related sentence generation uses
the same template-based mechanism as before. In
principle, this process may be repeated any num-
ber of times and express larger parts of the KB.
Here we only add one single knowledge-base re-
lation per response, to keep the responses concise.
3 Evaluation
We set out to evaluate how computer-generated re-
sponses compare to human responses in their per-
ceived human-likeness and relevance. More in
particular, we compare different system variants
in order to investigate what makes responses seem
more human-like or relevant.
3.1 Materials
Our empirical evaluation is restricted to topics re-
lated to mobile telephones, specifically Apple?s
iPhone and devices based on the Android operat-
ing system. We collected 300 articles from lead-
ing technology sites in the domain to train the
topic models on, settling on 10 topics models.
Next, we generated a set of user agendas refer-
ring to the same 10 topics. Each agenda is rep-
resented by a single keyword from a topic model
distribution and a sentiment value sentiment
t
?
{?8,?4, 0, 4, 8}. Finally, we selected 10 new ar-
ticles from similar sites and generated a pool of
61
Sresponse
NP
I
VP
V?
belief
SBAR
that (S
response
)
S
response
S
article
S
item
(S
relation
)
S
item
NP?
itemRef
VP?
senti
i
S
article
NP?
articleRef
VP?
senti
a
S
relation
NP?
relationRef
VP?
senti
r
articleRef ? ExpressArticle(...)
itemRef ? ExpressItem(...)
relationRef ? ExpressRelation(...)
sentiment
a
? ExpressArticleSentiment(...)
sentiment
i
? ExpressItemSentiment(...)
sentiment
r
? ExpressRelationSentiment(...)
belief ? ExpressBelief(...)
Figure 2: Template-based response generation. The templates are on the left. The Express* functions on
the right uses regular expressions over the arguments and vocabulary items from a closed lexicon.
1000 responses for each, comprising 100 unique
responses for each combination of sentiment
t
and system variant (i.e., with or without a knowl-
edge base). Table 2 presents an example response
for each such combination. In addition, we ran-
domly collected 5 to 10 real, short or medium-
length, online human responses for each article.
3.2 Surveys
We collected evaluation data via two online
surveys on Amazon Mechanical Turk (www.
mturk.com). In Survey 1, participants judged
whether responses to articles were written by hu-
man or computer, akin to (a simplified version of)
the Turing test (Turing, 1950). In Survey 2, re-
sponses were rated on their relevance to the ar-
ticle, in effect testing whether they abide by the
Gricean Maxim of Relation. This is comparable
to the study by Ritter et al. (2011) where people
judged which of two responses was ?best?.
Each survey comprises 10 randomly ordered tri-
als, corresponding to the 10 selected articles. First,
the participant was presented with a snippet from
the article. When clicking a button, the text was
removed and its presentation duration recorded.
Next, a multiple-choice question asked about the
snippet?s topic. Data on a trial was discarded from
analysis if the participant answered incorrectly or
if the snippet was presented for less than 10 msec
per character; we took these to be cases where the
snippet was not properly read. Next, the partic-
ipant was shown a randomly ordered list of re-
sponses to the article.
In Survey 1, four responses were presented for
each article: three randomly selected from the
pool of human responses to that article and one
generated by our system. The task was to cate-
gorize each response on a 7-point scale with la-
bels ?Certainly human/computer?, ?Probably hu-
man/computer?, ?Maybe human/computer? and
?Unsure?. In Survey 2, five responses were pre-
sented: three human responses and two computer-
generated. The task was to rate the responses?
relevance on a 7-point scale labeled ?Completely
(not) relevant?, ?Mostly (not) relevant?, ?Some-
what (not) relevant?, and ?Unsure?. As a con-
trol condition, one of the human responses and
one of the computer responses were actually taken
from another article than the one just presented.
In both surveys, the computer-generated responses
presented to each participant were balanced across
sentiment levels and generation functions (g
base
and g
kb
). After completing the 10 trials, partic-
ipants provided basic demographic information,
including native language. Data from non-native
English speakers was discarded. Surveys 1 and 2
were completed by 62 and 60 native speakers, re-
spectively.
3.3 Analysis and Results
Survey 1: Computer-Likeness Rating. Table 3
shows the mean ?computer-likeness?-ratings from
1 (?Certainly human?) to 7 (?Certainly computer?)
for each response category. Clearly, the human
responses are rated as more human-like than the
computer-generated ones: our model did not gen-
erally mislead the participants. This may be due
to the template-based response structure: over the
course of the survey, human raters are likely to
notice this structure and infer that such responses
are computer-generated. To investigate whether
such learning indeed occurs, a linear mixed-
effects model was fitted, with predictor variables
IS COMP (+1:computer-generated, ?1:human re-
sponses), POS (position of the trial in the survey, 0
to 9), and the interaction between the two. Table 4
62
Sent. KB Response
?8
No Android is horrendous so I think that the writer is completely correct!!!
Yes Apple is horrendous so I feel that the author is not really right!!! iOS is horrendous as well.
?4
No I think that the writer is mistaken because apple actually is unexceptional.
Yes I think that the author is wrong because Nokia is mediocre. Apple on the other hand is pretty good ...
0
No The text is accurate. Apple is okay.
Yes Galaxy is okay so I think that the content is accurate. All-in-all samsung makes fantastic gadgets.
4
No Android is pretty good so I feel that the author is right.
Yes Nokia is nice. The article is precise. Samsung on the other hand is fabulous...
8
No Galaxy is great!!! The text is completely precise.
Yes Galaxy is awesome!!! The author is not completely correct. In fact I think that samsung makes
awesome products.
Table 2: Responses generated by the system with or without a knowledge-base (KB), with different
sentiment levels.
Response Type Mean and CI
Human 3.33 ? 0.08
Computer (all) 4.49 ? 0.15
Computer (?KB) 4.66 ? 0.20
Computer (+KB) 4.32 ? 0.22
Table 3: Mean and 95% confidence interval of
computer-likeness rating per response category.
?KB indicates whether g
base
or g
kb
was used.
Factor b t P (b < 0)
(intercept) 3.590
IS COMP 0.193 2.11 0.015
POS 0.069 4.76 0.000
IS COMP ? POS 0.085 6.27 0.000
Table 4: Computer-likeness rating regression re-
sults, comparing human to computer responses.
presents, for each factor in the regression analysis,
the coefficient b and its t-statistic. The coefficient
equals the increase in computer-likeness rating for
each unit increase in the predictor variable. The t-
statistic is indicative of how much variance in the
ratings is accounted for by the predictor. We also
obtained a probability distribution over each co-
efficient by Markov Chain Monte Carlo sampling
using the R package lme4 version 0.99 (Bates,
2005). From each coefficient?s distribution, we es-
timate the posterior probability that b is negative,
which quantifies the reliability of the effect.
The positive b value for POS shows that re-
sponses drift towards the ?computer?-end of the
scale. More importantly, a positive interaction
with IS COMP indicates that the difference be-
tween human and computer responses becomes
more noticeable as the survey progresses ?
the participants did learn to identify computer-
generated responses. However, the positive coef-
ficient for IS COMP means that even at the very
first trial, computer responses are considered to be
more computer-like than human responses.
Factors Affecting Human-Likeness. Our find-
ing that the identifiability of computer-generated
responses cannot be fully attributed to their repet-
itiveness, raises the question: What makes a such
a response more human-like? The results provide
several insights into this matter.
First, the mean scores in Table 3 suggest that in-
cluding a knowledge base increases the responses?
human-likeness. To further investigate this, we
performed a separate regression analysis, using
only the data on computer-generated responses.
This analysis also included predictors KB (+1:
knowledge base included, ?1: otherwise), SENT
(sentiment
t
, from ?8 to +8), absolute value of
SENT, and the interaction between KB and POS.
As can be seen in Table 5, there is no reliable in-
teraction between KB and POS: the effect of in-
cluding the KB on the human-likeness of responses
remained constant over the course of the survey.
Furthermore, we see evidence that responses
with a more positive sentiment are considered
more computer-like. The (only weakly reliable)
negative effect of the absolute value of senti-
ment suggests that more extreme sentiments are
considered more human-like. Apparently, people
count on computer responses to be mildly positive,
whereas human responses are expected to be more
extreme, and extremely negative in particular.
Survey 2: Relevance Rating. The mean rele-
vance scores in Table 6 reveal that a response is
rated as more relevant to a snippet if it was actu-
ally a response to that snippet, rather than to a dif-
ferent snippet. This reinforces our design choice
63
Factor b t P (b < 0)
(intercept) 4.022
KB ?0.240 ?2.13 0.987
POS 0.144 5.82 0.000
SENT 0.035 2.98 0.002
abs(SENT) ?0.041 ?1.97 0.967
KB ? POS 0.023 1.03 0.121
Table 5: Computer-likeness rating regression re-
sults, comparing systems with and without KB.
Response Type Source Mean and CI
Human
this 4.85 ? 0.11
other 3.56 ? 0.18
Computer (all)
this 4.52 ? 0.16
other 2.52 ? 0.15
Computer (?KB)
this 4.53 ? 0.23
other 2.46 ? 0.21
Computer (+KB)
this 4.51 ? 0.23
other 2.58 ? 0.22
Table 6: Mean and 95% confidence interval of
relevance rating per response category. ?Source?
indicates whether the response is from the pre-
sented text snippet or a random other snippet.
?KB indicates whether g
base
or g
kb
was used.
Factor b t P (b < 0)
(intercept) 3.861
IS COMP ?0.339 ?7.10 1.000
SOURCE 0.824 16.80 0.000
IS COMP ? PRES 0.179 5.03 0.000
Table 7: Relevance ratings regression results,
comparing human to computer responses.
Factor b t P (b < 0)
(intercept) 3.603
KB 0.026 0.49 0.322
SOURCE 1.003 15.90 0.000
SENT 0.023 1.94 0.029
abs(SENT) ?0.017 ?0.93 0.819
KB ? SOURCE ?0.032 ?0.61 0.731
Table 8: Relevance ratings regression results,
comparing systems with and without KB.
to include input items referring specifically to the
topic and sentiment of the author. However, hu-
man responses are considered more relevant than
the computer-generated ones. This is confirmed
by a reliably negative regression coefficient for
IS COMP (see regression results in Table 7).
The analysis included the binary factor SOURCE
(+1 if the response came from the presented snip-
pet, ?1 if it came from a random article). We
see a positive interaction between SOURCE and
IS COMP, indicating that presenting a response
from a random article is more detrimental to rel-
evance of computer-generated responses than that
of the human responses. This is not surprising, as
the computer-generated responses (unlike the hu-
man responses) always includes the article?s topic.
When analyzing only data on computer-
generated responses, and including predictors for
agenda sentiment and for presence of the knowl-
edge base, we see that including the KB does not
affect response relevance (see Table 8). Also, there
is no interaction between KB and SOURCE, that
is, the effect of presenting a response from a dif-
ferent article does not differ between the models
with and without the knowledge base. Possibly,
responses are considered as more relevant if they
have more positive sentiment, but the evidence for
this is fairly weak.
4 Related and Future Work
In contrast to the vast amount of research on sen-
timent and topic analysis, as well as generation
tasks in which the input is artificial or pre-defined,
our system implements a full end-to-end cycle
from natural language analysis to natural language
generation with applications in social media and
automated interaction in real-world settings.
The only two other studies on response gener-
ation in social media we know of are Ritter et al.
(2011) and Hasegawa et al. (2013). Ritter?s and
Hasegawa?s approaches differ from ours in their
objective and their approach to generation. Specif-
ically, Ritter?s approach is based on machine trans-
lation, creating responses by directly re-using pre-
vious content. Their data-driven approach gener-
ates relevant, but not opinionated responses. In
addition, both Ritter?s and Hasegawa?s systems re-
spond to tweets, while our system analyzes and re-
sponds to complete articles. Hasegawa?s approach
is closer to ours in that it generates responses that
are intended to elicit a specific emotion from the
addressee. However, it still differs considerably in
settings (dialogues versus online posting) and in
the goal itself (eliciting emotion versus expressing
opinion). Thus, we see these studies as comple-
mentary to ours in the realm of response genera-
tion in social media.
64
A natural contact point of our work with exist-
ing work in social media analysis is the investiga-
tion of how a change in the implementation of in-
dividual components (e.g., topic inference or sen-
timent scoring) would affect the result of the over-
all generation. In particular, it would be interesting
to test whether a novel mechanism for joint infer-
ence of topic/sentiment distributions could lead to
improvement in the human-likeness of the gener-
ated responses.
The syntactic and semantic means of expres-
sion that we use are based on bare bone templates
and fine-grained POS tags (Theune et al., 2001).
These may potentially be expanded with different
ways to express subject/object relations, relations
between phrases, polarity of sentences, and so on.
Additional approaches to generation can factor in
such aspects, e.g., the template-based methods in
Becker (2002) and Narayan et al. (2011), or gram-
mar based methods, as in DeVault et al. (2008).
Using more sophisticated generation methods with
a rich grammatical backbone may combat the sen-
sitivity to computer-generated response patterns as
acquired by our human raters over time.
Furthermore, our result concerning the human-
likeness of g
kb
clearly demonstrates that semantic
knowledge must be brought in to support better,
and more human-like, response generation. Large-
scale knowledge graphs such as Freebase support
many semantic tasks (Jacobs, 1985), and can be
used for providing richer context for automatically
generating human-like responses.
From a theoretical viewpoint, the system will
clearly benefit from rigorous analysis of human
interaction in online media. Responses to user-
generated content on the Internet share some
linguistic characteristics in structure, length and
manner of expression. Studying these features the-
oretically and then examining them empirically
using a Turing-like evaluation as presented here
can take us a big step in the direction of better gen-
eration, and also better understanding of the pro-
cesses underlying human response generation.
This latter understanding may be complemented
with insights into the causes, motivations and in-
tricacies of human interaction in such environ-
ments, as studied by sociologists and psychol-
ogists. In particular, our preliminary interac-
tion with colleagues from communication stud-
ies suggests that the present endeavor nicely com-
plements that of ?persuasive computing? (Fogg,
1998; Fogg, 2002), and we hope that this collabo-
ration will lead to valuable synergies.
Finally, bridging the gap between the technical
and the theoretical, it would be fascinating to test
the responses in the context for which they are
generated ? social media. Generated texts may
be posted as a response to the original article, or
shared with a link of the original article, followed
by measuring the responses to, and shares of, that
response. Such real-world evaluation could indi-
cate that generated responses are indeed believable
and engaging, and may better simulate a Turing-
like test in which machine-generated responses
cannot be distinguished from human responses.
5 Conclusion
We presented a system for generating responses
that are directly tied to responders? agendas and
document content. To the best of our knowledge,
this is the first system to generate subjective re-
sponses directly reflecting users? agendas. Our re-
sponse generation architecture provides an easy-
to-use and easy-to-extend solution encompassing
a range of NLP and NLG techniques. We evalu-
ated both the human-likeness and the relevance of
the generated content, thereby empirically quan-
tifying the efficacy of computer-generated re-
sponses compared head-to-head against human re-
sponses.
Generating concise, relevant, and opinionated
responses that are also human-like is hard ? it
requires the integration of text-understanding and
sentiment analysis, and it is also contingent on the
expression of the agents? prior knowledge, reasons
and motives. We suggest our architecture and eval-
uation method as a baseline for future research
on generated content that would effectively pass
a Turing-like test, and successfully convince hu-
mans of the authenticity of generated responses.
5
Acknowledgments
We thank Yoav Francis for his contribution in the
early stages of this research. We further thank
our anonymous reviewers for their insightful com-
ments on an earlier draft.
5
Our code, training data, experimental data (computer and
human responses) and analysis scripts are publicly available
via www.tsarfaty.com/nlg-sd/.
65
References
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceed-
ings of the international conference on Web search
and web data mining, pages 183?194. ACM.
Teresa M. Amabile. 1981. Brilliant but Cruel: Per-
ceptions of Negative Evaluators. Washington, DC:
ERIC Clearinghouse.
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. European Language Re-
sources Association (ELRA).
Douglas M. Bates. 2005. Fitting linear mixed models
in R. R News, 5:27?30.
Tilman Becker. 2002. Practical, template-based natu-
ral language generation with TAG. In Proceedings
of the 6th International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+6).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res., 3:993?1022.
David M. Blei. 2012. Probabilistic topic models.
Commun. ACM, 55(4):77?84.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opinions
are received by online communities: A case study
on amazon.com helpfulness votes. In Proceedings
of the 18th International Conference on World Wide
Web, WWW ?09, pages 141?150, New York, NY,
USA. ACM.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 241?249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Proceedings of the Fifth International Natural Lan-
guage Generation Conference, INLG ?08, pages 77?
85, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard
Hovy. 2006. An intelligent discussion-bot for
answering student queries in threaded discussions.
In Proceedings of Intelligent User Interface (IUI-
2006), pages 171?177.
B. J. Fogg. 1998. Persuasive computers: Perspec-
tives and research directions. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, CHI ?98, pages 225?232, New York,
NY, USA. ACM Press/Addison-Wesley Publishing
Co.
B. J. Fogg. 2002. Persuasive technology: Using com-
puters to change what we think and do. Ubiquity,
December.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: A
realisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation, ENLG ?09, pages 90?93,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
H. P. Grice. 1967. Logic and conversation. In H. P.
Grice, editor, Studies in the ways of words, pages
22?40. Harvard University Press.
Michael Haenlein and Andreas M. Kaplan. 2009.
Flagship brand stores within virtual worlds: The im-
pact of virtual store exposure on real-life attitude
toward the brand and purchase intent. Recherche
et Applications en Marketing (English Edition),
24(3):57?79.
Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga,
and Masashi Toyoda. 2013. Predicting and eliciting
addressee?s emotion in online dialogue. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 964?972, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
?99, pages 50?57, New York, NY, USA. ACM.
Philip N. Howard, Aiden Duffy, Deen Freelon, Muza-
mmil Hussain, Will Mari, and Marwa Mazaid.
2011. Opening closed regimes: What was the role
of social media during the Arab spring? Project on
Information Technology and Political Islam.
Paul S Jacobs. 1985. A knowledge-based approach to
language production. Technical report, University
of California at Berkeley, Berkeley, CA, USA.
Pulkit Kathuria. 2012. Sentiment Clas-
sification using WSD, Maximum En-
tropy and Naive Bayes Classifiers.
https://github.com/kevincobain2000/sentiment classifier.
Visited March 2014.
Wiebke Lamer. 2012. Twitter and tyrants: New me-
dia and its effects on sovereignty in the Middle East.
Arab Media and Society.
Marc Langheinrich and G?unter Karjoth. 2011. Social
networking and the risk to companies and institu-
tions. Information Security Technical Report. Spe-
cial Issue: Identity Reconstruction and Theft, pages
51?56.
66
Paul Mah. 2012. Tools to automate your
customer service response on social me-
dia. http://www.itbusinessedge.com/blogs/smb-
tech/tools-to-automate-your-customer-service-
response-on-social-media.html. Visited August
2013.
Chris McConnell. 2012. When brands auto-
mate Twitter and Facebook responses I?ll re-
volt. http://dailytekk.com/2012/06/07/brands-
automating-social-media/. Visited August 2013.
Gilad Mishne. 2006. Multiple ranking strategies for
opinion retrieval in blogs. In Proceedings of the 15th
Text Retrieval Conference.
Kyoshi Mori, Adam Jatowt, and Mitsuru Ishizuka.
2003. Enhancing conversational flexibility in multi-
modal interactions with embodied lifelike agent. In
Proceedings of the 8th International Conference on
Intelligent User Interfaces, IUI ?03, pages 270?272,
New York, NY, USA. ACM.
Mickey Nall. 2013. You can?t automate so-
cial media engagement, argues PRSA?s Mickey
Nall. http://www.prmoment.com/1359/you-cant-
automate-social-media-engagement-argues-prsas-
mickey-nall.aspx. Visited August 2013.
Karthik Sankaran Narayan, Charles Lee Isbell Jr., and
David L. Roberts. 2011. Dextor: Reduced effort
authoring for template-based natural language gen-
eration. In Vadim Bulitko and Mark O. Riedl, ed-
itors, Proceedings of the Seventh Artificial Intelli-
gence and Interactive Digital Entertainment Confer-
ence. The AAAI Press.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen
and Samuel Gosling, editors, ICWSM. The AAAI
Press.
Brendan O?Connor, Brandon M. Stewart, and Noah A.
Smith. 2013. Learning to extract international rela-
tions from political context. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1094?1104. The Association for Computer Linguis-
tics.
Jeremiah Owyang. 2012. Brands Start Automating
Social Media Responses on Facebook and Twitter.
http://techcrunch.com/2012/06/07/brands-start-
automating-social-media-responses-on-facebook-
and-twitter/. Visited August 2013.
Christos H. Papadimitriou, Hisao Tamaki, Prabhakar
Raghavan, and Santosh Vempala. 1998. La-
tent Semantic Indexing: A probabilistic analy-
sis. In Proceedings of the Seventeenth ACM
SIGACT-SIGMOD-SIGART Symposium on Princi-
ples of Database Systems, PODS ?98, pages 159?
168, New York, NY, USA. ACM.
Erik Qualman. 2012. Socialnomics: How social media
transforms the way we live and do business. John
Wiley & Sons, Hoboken, NJ, USA, 2nd edition.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta, May. ELRA.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Nat. Lang.
Eng., 3(1):57?87.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 583?593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
M. Theune, E. Klabbers, J. R. De Pijper, E. Krahmer,
and J. Odijk. 2001. From data to speech: A general
approach. Nat. Lang. Eng., 7(1):47?86.
Alan M. Turing. 1950. Computing machinery and in-
telligence. Mind, LIX:433?460.
Kees Van Deemter, Emiel Krahmer, and Mari?et The-
une. 2005. Real versus template-based natural lan-
guage generation: A false opposition? Comput. Lin-
guist., 31(1):15?24.
Bimal Viswanath, Alan Mislove, Meeyoung Cha, and
Krishna P. Gummadi. 2009. On the evolution of
user interaction in Facebook. In Proceedings of
the 2nd ACM Workshop on Online Social Networks,
WOSN ?09, pages 37?42, New York, NY, USA.
ACM.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
477?485, Stroudsburg, PA, USA. Association for
Computational Linguistics.
67
