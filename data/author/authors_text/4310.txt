Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 449?456
Manchester, August 2008
The Choice of Features for Classification of Verbs in Biomedical Texts
Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
alk23@cl.cam.ac.uk
Yuval Krymolowski
Dept. of Computer Science
Haifa University
Israel
yuvalkry@gmail.com
Nigel Collier
National Institute of Informatics
Hitotsubashi 2-1-2
Chiyoda-ku, Tokyo 101-8430
Japan
collier@nii.ac.jp
Abstract
We conduct large-scale experiments to in-
vestigate optimal features for classification
of verbs in biomedical texts. We intro-
duce a range of feature sets and associated
extraction techniques, and evaluate them
thoroughly using a robust method new to
the task: cost-based framework for pair-
wise clustering. Our best results compare
favourably with earlier ones. Interestingly,
they are obtained with sophisticated fea-
ture sets which include lexical and seman-
tic information about selectional prefer-
ences of verbs. The latter are acquired au-
tomatically from corpus data using a fully
unsupervised method.
1 Introduction
Recent years have seen a massive growth in the
scientific literature in the domain of biomedicine.
Because future research in the biomedical sciences
depends on making use of all this existing knowl-
edge, there is a strong need for the development of
natural language processing (NLP) tools which can
be used to automatically locate, organize and man-
age facts related to published experimental results.
Major progress has been made on information
retrieval and on the extraction of specific rela-
tions (e.g. between proteins and cell types) from
biomedical texts (Ananiadou et al, 2006). Other
tasks, such as the extraction of factual information,
remain a bigger challenge.
Researchers have recently begun to use deeper
NLP techniques (e.g. statistical parsing) for im-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
PROTEINS: p53
p53
Tp53
Dmp53
...
ACTIVATE
suggestsdemonstrates
indicatesimplies...
GENES: WAF1
WAF1CIP1p21
...
It
INDICATE
that
activates
up-regulates
induces
stimulates...
...
Figure 1: Sample lexical classes
proved processing of the challenging linguistic
structures (e.g. complex nominals, modal subordi-
nation, anaphoric links) in biomedical texts. For
optimal performance, many of these techniques
require richer syntactic and semantic informa-
tion than is provided by existing domain lexicons
(e.g. UMLS metathesaurus and lexicon
1
). This par-
ticularly applies to verbs, which are central to the
structure and meaning of sentences.
Where the information is absent, lexical classi-
fication can compensate for it, or aid in obtaining
it. Lexical classes which capture the close rela-
tion between the syntax and semantics of verbs
provide generalizations about a range of linguis-
tic properties (Levin, 1993). For example, con-
sider the INDICATE and ACTIVATE verb classes in
Figure 1. Their members have similar subcatego-
rization frames SCFs (e.g. activate / up-regulate /
induce / stimulate NP) and selectional preferences
(e.g. activate / up-regulate / induce / stimulate
GENES:WAF1), and they can be used to make sim-
ilar statements describing similar events (e.g. PRO-
TEINS:P53 ACTIVATE GENES:WAF1).
Lexical classes can be used to abstract away
from individual words, or to build a lexical or-
ganization which predicts much of the behaviour
of a new word by associating it with an appro-
priate class. They have proved useful for various
NLP application tasks, e.g. parsing, word sense dis-
1
http://www.nlm.nih.gov/research/umls
449
ambiguation, semantic role labeling, information
extraction, question-answering, machine transla-
tion (Dorr, 1997; Prescher et al, 2000; Swier
and Stevenson, 2004; Dang, 2004; Shi and Mi-
halcea, 2005). A large-scale classification spe-
cific to the biomedical data could support key BIO-
NLP tasks such as anaphora resolution, predicate-
argument identification, event extraction and the
identification of biomedical (e.g. interaction) rela-
tions. However, no such classification is available.
Recent research shows that it is possible to auto-
matically induce lexical classes from corpora with
promising accuracy (Schulte im Walde, 2006; Joa-
nis et al, 2007; Sun et al, 2008). A number of
machine learning (ML) methods have been applied
to classify mainly syntactic features (e.g. subcat-
egorization frames (SCFs)) extracted from cross-
domain corpora using e.g. part-of-speech tagging
or robust statistical parsing techniques. Korho-
nen et al (2006) have recently applied such an
approach to biomedical texts. Their preliminary
experiment shows encouraging results but further
work is required before such an approach can be
used to benefit practical BIO-NLP.
We conduct a large-scale investigation to find
optimal features for biomedical verb classification.
We introduce a range of theoretically-motivated
feature sets and evaluate them thoroughly using
a robust method new to the task: a cost-based
framework for pairwise clustering. Our best re-
sults compare favourably with earlier ones. Inter-
estingly, they are obtained using feature sets which
have proved challenging in general language verb
classification: ones which incorporate information
about selectional preferences of verbs. Unlike in
earlier work, we acquire the latter from corpus data
using a fully unsupervised method.
We present our lexical classification approach in
section 2 and data in section 3. Experimental eval-
uation is reported in section 4. Section 5 provides
discussion and section 6 concludes.
2 Approach
Our lexical classification approach involves (i) ex-
tracting features from corpus data and (ii) cluster-
ing them. These steps are described in the follow-
ing two sections, respectively.
2.1 Features
Lexical classifications are based on diathesis alter-
nations which manifest in alternating sets of syn-
tactic frames (Levin, 1993). Most verb classifi-
cation approaches have therefore employed shal-
low syntactic slots or SCFs as basic features. Some
have supplemented them with further information
about verb tense, voice, and/or semantic selec-
tional preferences on argument heads.
2
The preliminary experiment on biomedical verb
classification (Korhonen et al, 2006) employed
basic syntactic features only: SCFs extracted
from corpus data using the system of Briscoe
and Carroll (1997) which operates on the output
of a domain-independent robust statistical parser
(RASP) (Briscoe and Carroll, 2002). Because such
deep syntactic features seem ideally suited for
challenging biomedical data, we adopted the same
basic approach, but we designed and extracted a
range of novel feature sets which include addi-
tional syntactic and semantic information.
The SCF extraction system assigns each occur-
rence of a verb in the parsed data as a member of
one of the 163 verbal SCFs, builds a lexical entry
for each verb (type) and SCF combination, and fil-
ters noisy entries out of the lexicon. We do not
employ the filter in our work because its primary
aim is to filter out SCFs containing adjuncts (as op-
posed to arguments). Adjuncts have been shown
to be beneficial for general language verb classifi-
cation (Sun et al, 2008; Joanis et al, 2007) and
particularly meaningful in biomedical texts (Co-
hen and Hunter, 2006).
The lexical entries provide various information
useful for verb classification, including e.g. the fre-
quency of the entry in the data, the part-of-speech
(POS) tags of verb tokens, the argument heads in
argument positions, the prepositions in PP frames,
and the number of verbal occurrences in active and
passive. Making use of this information we de-
signed ten feature sets for experimentation.
The first three feature sets F1-F3 include basic
SCF frequency information for each verb:
F1: SCFs and their relative frequencies. The SCFs
abstract over lexically governed particles and
prepositions.
F2: F1 with two high frequency PP frames pa-
rameterized for prepositions: the simple PP
and NP-PP frames refined according to the
prepositions provided in the lexical entries
(e.g. PP at, PP on, PP in).
2
See section 5 for discussion on previous work.
450
F3: F2 with 13 additional high frequency PP
frames parameterized for prepositions.
Although prepositions are an important part of
the syntactic description of lexical classes and
therefore F3 should be the most informative fea-
ture set, we controlled the number of PP frames
parameterized for prepositions to examine the ef-
fect of sparse data in automatic classification.
F4-F7 build on the most refined SCF-based fea-
ture set F3, supplementing it with information
about verb tense (F4-F5) and voice (F6-F7):
F4: The frequencies of POS tags (e.g. VVD for
activated) calculated over all the SCFs of the
verb.
F5: The frequencies of POS tags calculated spe-
cific to each SCF of the verb.
F6: The frequency of the active and passive oc-
currences of the verb (calculated over all the
SCFs of the verb).
F7: The frequency of the active and passive occur-
rences of the verb (calculated specific to each
SCF of the verb).
Also F8-F10 build on feature set F3. They sup-
plement it with information about lexical or se-
mantic selectional preferences (SPs) of the verbs
in the following slots: subject, direct object, sec-
ond object, and the NP within the PP complement.
The SPs are acquired using argument head data in
the ten most frequent SCFs. We use two baseline
methods (F8 and F9) which employ raw data and
one method based on clustering (F10):
F8: The raw argument head types are considered
as SP classes.
F9: Only those raw argument head types which
occur with four or more verbs with frequency
of ? 3 are considered as SP classes.
F10: SPs are acquired by clustering those argu-
ment heads which occur with ten or more
verbs with frequency of ? 3. We used the PC
clustering method described below in section
2. The number of clusters K
np
was set to 10,
20, and 50 to produce SP classes. We call the
feature sets corresponding to these different
values of K
np
F10A, F10B and F10C, respec-
tively. Since the clustering algorithms have
an element of randomness, clustering was ran
100 times. The output is a result of voting
among the outputs of the runs.
F3-F10 are entirely novel feature sets in biomed-
ical verb classification. Variations of some of them
have been used in earlier work on general language
classification (see section 5 for details).
2.2 Classification
The clustering method which proved the best in the
preliminary experiment on biomedical verb classi-
fication was Information Bottleneck (IB) (Tishby
et al, 1999). We compare this method against a
probabilistic method: a cost-based framework for
pairwise clustering (PC) (Puzicha et al, 2000).
2.2.1 Information Bottleneck
IB is an information-theoretic method which
controls the balance between: (i) the loss of
information by representing verbs as clusters
(I(Clusters;V erbs)), which has to be min-
imal, and (ii) the relevance of the output
clusters for representing the SCF distribution
(I(Clusters; SCFs)) which has to be maximal.
The balance between these two quantities ensures
optimal compression of data through clusters. The
trade-off between the two constraints is realized
through minimising the cost function:
L
IB
= I(Clusters;V erbs)
? ?I(Clusters; SCFs) ,
where ? is a parameter that balances the con-
straints. IB takes three inputs: (i) SCF-verb -based
distributions, (ii) the desired number of clusters K,
and (iii) the initial value of ?. It then looks for the
minimal ? that decreases L
IB
compared to its value
with the initial ?, using the given K. IB delivers as
output the probabilities p(K|V ).
2.2.2 Pairwise Clustering
PC is a method where a cost criterion guides
the search for a suitable clustering configuration.
This criterion is realized through a cost function
H(S,M) where
(i) S = {sim(a, b)}, a, b ? A : a collection of pairwise
similarity values, each of which pertains to a pair of
data elements a, b ? A.
(ii) M = (A
1
, . . . , A
k
) : a candidate clustering configu-
ration, specifying assignments of all elements into the
disjoint clusters (that is ?A
j
= A and A
j
? A
j
?
= ?
for every 1 ? j < j
?
? k).
451
1 Have an effect on activity (BIO/29) 9 Report (GEN/30)
1.1 Activate / Inactivate 9.1 Investigate
1.1.1 Change activity: activate, inhibit 9.1.1 Examine: evaluate, analyze
1.1.2 Suppress: suppress, repres s 9.1.2 Establish: test, investigate
1.1.3 Stimulate: stimulate 9.1.3 Confirm: verify, determine
1.1.4 Inactivate: delay, diminish 9.2 Suggest
1.2 Affect 9.2.1 Presentational:
1.2.1 Modulate: stabilize, modulate hypothesize, conclude
1.2.2 Regulate: control, support 9.2.2 Cognitive:
1.3 Increase / decrease: increase, decrease consider, believe
1.4 Modify: modify, catalyze 9.3 Indicate: demonstrate, imply
Table 1: Sample classes from the gold standard
Journal Years Words
Genes & Development 2003-5 4.7M
Journal of Biological Chemistry 2004 5.2M
(Vol.1-9)
The Journal of Cell Biology 2003-5 5.6M
Cancer Research 2005 6.5M
Carcinogenesis 2003-5 3.4M
Nature Immunology 2003-5 2.3M
Drug Metabolism and Disposition 2003-5 2.3M
Toxicological Sciences 2003-5 3.1M
Total: 33.1M
Table 2: Data from MEDLINE
The cost function is defined as follows:
H = ?
P
n
j
?Avgsim
j
,
Avgsim
j
=
1
n
j
?(n
j
?1)
P
{a,b?A
j
}
sim(a, b)
where n
j
is the size of the j
th
cluster and Avgsim
j
is the average similarity between cluster members.
We used the Jensen-Shannon divergence (JS) as the
similarity measure.
3 Data
3.1 Test Verbs and Gold Standard
We employed in our experiments the same gold
standard as earlier employed by Korhonen et al
(2006). This three level gold standard was created
by a team of human experts: 4 domain experts and
2 linguists. It includes 192 test verbs (typically fre-
quent verbs in biomedical journal articles) classi-
fied into 16, 34 and 50 classes, respectively. The
classes created by domain experts are labeled as
BIO and those created by linguists as GEN. BIO
classes include 116 verbs whose analysis required
domain knowledge (e.g. activate, solubilize, har-
vest). GEN classes include 76 general or scientific
text verbs (e.g. demonstrate, hypothesize, appear).
Each class is associated with 1-30 member verbs.
Table 1 illustrates two of the gold standard classes
with 1-2 example verbs per (sub-)class.
3.2 Test Data
We downloaded the data from the MEDLINE
database, from eight journals covering various ar-
SCF F1 98 39
F2 247 64
F3 486 75
F3 + tense F4 490 79
F5 920 176
F3 + voice F6 488 77
F7 682 153
F3 + SP F8 150407 2112
F9 13352 344
F10A 110280 2091
F10B 115208 2091
F10C 114793 2091
Table 3: (i) The total number of features and (ii)
the average per verb for all the feature sets
eas of biomedicine. The first column in table 2
lists each journal, the second shows the years from
which the articles were downloaded, and the third
indicates the size of the data. We experimented
with two test sets: 1) The 15.5M word sub-set
shown in the first three rows of the table (this was
used for creating the gold standard). 2) All the
data: this new larger data was necessary for exper-
iments with new feature sets as the most refined
ones do not appear in 1) with sufficient frequency.
4 Experimental Evaluation
4.1 Processing the Data
The data was first processed using the feature ex-
traction module. Table 3 shows (i) the total num-
ber of features in each feature set and (ii) the av-
erage per verb in the resulting lexicon. The clas-
sification module was then applied. We requested
K = 2 to 60 clusters from both clustering meth-
ods. We did not want to enforce the actual num-
ber of classes but preferred to let the class hierar-
chy emerge from the clustering results. In order
to find the values of K where the clustering output
might correspond to a level in the class hierarchy
we used the relevance criterion. For each method
(clustering method and feature set combination)
we choose as informative K?s the values for which
the relevance information I(Clusters; SCFs)) in-
creases more sharply between K?1 and K clusters
than between K and K+1. We then chose for eval-
uation the outputs corresponding only to informa-
tive values of K. The clustering was run 50 times
for each method. The output is a result of voting
among the outputs of the runs.
4.2 Measures
The clusters were evaluated against the gold stan-
dard using four methods. The first measure, the
452
adjusted pairwise precision, evaluates clusters in
terms of verb pairs:
APP =
1
K
K
P
i=1
num. of correct pairs in k
i
num. of pairs in k
i
?
|k
i
|?1
|k
i
|+1
APP is the average proportion of all within-
cluster pairs that are correctly co-assigned. Mul-
tiplied by a factor that increases with cluster size it
compensates for a bias towards small clusters.
The second measure is modified purity, a global
measure which evaluates the mean precision of
clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by n
prevalent
(K). Verbs
that do not take it are considered as errors. Clusters
where n
prevalent
(K) = 1 are disregarded as not to
introduce a bias towards singletons:
mPUR =
P
n
prevalent
(k
i
)?2
n
prevalent
(k
i
)
number of verbs
The third measure is the weighted class accu-
racy, the proportion of members of dominant clus-
ters DOM-CLUST
i
within all classes c
i
.
ACC =
C
P
i=1
verbs in DOM-CLUST
i
number of verbs
mPUR can be seen to measure the precision of
clusters and ACC the recall. We define an F mea-
sure as the harmonic mean of mPUR and ACC:
F =
2 ?mPUR ? ACC
mPUR + ACC
The experiments were run 50 times on each in-
put to get the distribution of performance due to
the randomness in the initial clustering. We calcu-
lated the average performance and standard devia-
tion from the results of these runs.
4.3 Results for Test Set 1
We first compared IB and PC on the smaller test set
1 using feature set F2. We chose for evaluation the
outputs corresponding to the most informative val-
ues of K: 20, 33, 53 for IB, and 19, 26, 51 for PC.
In the results included in table 4 IB shows slightly
better performance than PC, but the difference is
not significant for K=34 and 50. We decided to use
PC for larger experiments because it has two ad-
vantages over IB: 1) It can cluster the large test set
2 with K = 10 ? 60 in minutes, while IB requires
a day for this. 2) It can deal with (and combine)
different feature sets, while IB runs into numeri-
cal problems. Due to its speed and flexibility PC
is thus more suitable for larger-scale experiments
involving comparison of complex feature sets.
4.4 Results for Test Set 2
Tables 5 and 6 include the PC results on the larger
test set 2. Table 5 shows the results for each in-
dividual feature set (indicated in the second col-
umn). It shows also the standard deviations (?
avg
)
of the four performance measures averaged across
all the runs. These are very similar for 16, 34, and
50 classes and hence only included in one of the
columns. In addition, ?
diff
is indicated. This is
?
2 ? ?
avg
and used for calculating the significance
of the performance differences. In the following
discussion we consider a difference of more than
2?
diff
(p > 97.7%) as significant.
The first feature sets F1-F3 include basic SCF
(frequency) information for each verb, F2-F3 re-
fined with prepositions. F2 shows clearly better
results than F1 (over 10 F-measure) at all the levels
of gold standard. This demonstrates the usefulness
of prepositions for the task. When moving to F3
the performance decreases for 34 and 50 classes,
while improving for 16 classes, but these differ-
ences are not statistically significant.
Feature sets F4-F10 build on F3. F4-F5 include
information about verb tense. This information
proves quite useful for verb classification, partic-
ularly when specific to individual SCFs. When
compared against the baseline featureset F3, F5
is clearly better - particularly at 50 classes where
the difference is 3.9 in F-measure (2?
diff
). Verb
voice information is not equally helpful: F6-F7 are
not better than F3. In some comparisons they are
worse, e.g. F7 vs. F3 at 16 classes.
F8-F10 supplement F3 with information about
SPs. Surprisingly, these lexical and semantic fea-
tures prove the most useful for our task. At the
level of 34 and 50 classes, the best SP features are
even better than the best tense features (the dif-
ference is statistically significant), and they yield
notable improvement over the baseline features
(e.g. 6.8 difference in F-measure between F9 and
F3). The performance is not equally good at 16
classes. This makes perfect sense because class
members are unlikely to have similar SPs at such a
coarse level of semantic classification.
When comparing the five sets of SPs features
against each other, F9 and F10C produce the best
results at 34 and 50 classes. F9 uses raw (filtered)
argument head data for SP acquisition while F10C
uses clustering. It is interesting that the differ-
ence between these two very different methods is
not statistically significant. Whether one employs
453
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
IB 74 77 66 71 69 75 81 77 54 72 79 75
PC 71 78 58 67 64 71 81 75 63 71 73 72
? 1.1 1.0 1.0 0.8 1.8 1.6 1.3 1.4 2.1 1.5 1.6 1.1
Table 4: Performance on test set 1
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
SCF F1 62.7 68.2 54.6 60.6 50.4 58.4 53.4 55.8 41.5 50.3 55.7 52.9
F2 68.7 76.4 66.4 71.1 61.9 65.5 65.8 65.6 53.9 61.2 65.4 63.2
F3 69.3 77.7 67.6 72.3 61.6 66.0 64.0 65.0 53.7 60.2 65.9 62.9
F3 + tense F4 70.1 77.5 65.5 71.0 62.0 70.3 69.4 69.8 53.3 60.6 68.0 64.1
F5 68.5 75.4 71.7 73.5 61.9 67.8 68.2 68.0 58.2 62.7 71.7 66.8
F3 + voice F6 70.6 78.1 64.0 70.4 61.2 66.0 65.8 65.9 54.3 59.6 70.1 64.4
F7 74.0 79.5 59.7 68.2 62.6 65.4 65.1 65.2 55.1 60.9 69.2 64.7
F3 + SP F8 77.1 78.2 61.6 68.9 69.6 69.3 71.2 70.2 61.3 62.7 71.1 66.6
F9 72.4 77.1 64.0 69.9 72.2 72.0 71.6 71.8 62.3 65.6 72.4 68.8
F10A 75.6 80.0 63.2 70.6 66.1 69.2 70.6 69.9 59.4 63.5 69.0 66.2
F10B 68.8 77.1 69.2 72.9 65.3 67.2 69.8 68.5 59.9 61.9 70.5 65.9
F10C 74.1 78.9 65.7 71.7 68.8 71.7 69.7 70.7 59.8 63.4 71.1 67.0
?
avg
2.2 1.5 1.8 1.4
?
diff
3.1 2.1 2.5 2.0
Table 5: Performance on test set 2: PC clustering results for individual feature sets at the three levels of
gold standard. ?
avg
and ?
diff
were calculated across all the three classification levels.
16 CL. F5+F9 F4+ F10C F5 F5+ F8
APP 72.3 68.2 68.5 72.2
mPUR 76.4 77.0 75.4 76.5
ACC 73.6 70.9 71.7 69.9
F 75.0 73.8 73.5 73.0
34 CL. F5+ F9 F5+ F8 F9 F4+ F10A
APP 68.7 71.0 72.2 62.9
mPUR 70.1 71.0 72.0 68.4
ACC 74.8 73.4 71.6 75.0
F 72.4 72.2 71.8 71.5
50 CL. F9 F5+ F9 F5+ F8 F4+ F9
APP 62.3 59.8 62.8 59.7
mPUR 65.6 63.8 64.1 63.1
ACC 72.4 72.7 71.0 71.8
F 68.8 68.0 67.4 67.1
Table 6: Results for the top four feature set combi-
nations. All the feature sets build on F3.
fine grained clusters (F10C) or coarse-grained ones
(F10A) as SPs does not make much difference.
We next combined various feature sets. Table 6
shows the performance for the top four combina-
tions. Comparing these results against the ones in
Table 5, (see the ?
diff
values in Table 5) we can see
that combining feature sets does not result in better
performance
3
. The only exception is the difference
in APP and mPUR between F9 and F4 + F10A at
N=34. However, these results show similar ten-
dencies as the earlier ones: at 16 classes the most
3
Recall that all F4-F10 are actually already ?combined?
with F3 - we do not refer to this combination here.
useful features are based on verb tense, while at 34
and 50 classes they are based on SPs.
5 Discussion
The results presented in the previous section are
in interesting contrast with those reported in ear-
lier work. In previous work on general lan-
guage verb classification, syntactic features (slots
or SCFs) have proved generally the most help-
ful features, e.g. (Schulte im Walde, 2006; Joa-
nis et al, 2007). The preliminary experiment on
biomedical verb classification (Korhonen et al,
2006) experimented only with them. In our ex-
periments, SCFs proved useful baseline features.
When we refined them further, we faced sparse
data problems: considerable improvement was ob-
tained when moving from F1 to F2, but not when
moving to F3. Although many verb classes are
sensitive to preposition types, many of the types
are low in frequency. Future work could address
this problem by employing smoothing techniques,
or backing off to preposition classes.
Joanis et al (2007) experimented with tense
and voice -based features in general English verb
classification. They offered no significant im-
provement over basic syntactic features. Also in
our experiments, we obtained little improvement
with voice features. This could be due to the
454
un-distinctiveness of passive in biomedical texts
where it is used typically with high frequency.
However, tense-based features clearly improved
the baseline performance in our experiments. This
could be partly because we ?parameterize? POS in-
formation for SCFs, and partly because semanti-
cally similar verbs in biomedical language tend to
behave similarly also in terms of tense (Friedman
et al, 2002).
Joanis (2002) and Schulte im Walde (2006) used
SP-based features in general English and German
verb classifications, respectively. The former ac-
quired them from WordNet (Miller, 1990) and
the latter from GermaNet (Kunze, 2000). Joa-
nis (2002) obtained no improvement over syntactic
features while Schulte im Walde (2006) obtained,
but the improvement was not significant. In our
experiments, SP features gave the best results and
the clearest improvement over the baseline features
at the finer-grained levels of classification where
class members are indeed likely to be the most uni-
form in terms of their SPs.
We obtained this improvement despite using
a fully unsupervised approach to SP acquisition.
We did not exploit lexical resources like Joa-
nis (2002) and Schulte im Walde (2006) because
it would have required combining general re-
sources (e.g. WordNet) with domain specific ones
(e.g. UMLS). We opted for a simpler approach in
this initial work ? using raw argument heads and
clustering ? and obtained surprisingly good results.
In our experiments filtering of raw argument heads
and clustering with N=50 produced equivalent re-
sults, suggesting that relatively fine-grained clus-
ters are optimal. Future work will require quali-
tative analysis of noun clusters and comparison of
these against classes in lexical resources to deter-
mine an optimal method for SP acquisition.
Does the fact that we obtain good results with
features which have not proved helpful in general
language classification indicate a need for domain-
specific feature engineering? We do not believe
so. The feature sets we experimented with are the-
oretically well-motivated and should, in principle,
also aid general language verb classification. We
believe they proved helpful in our experiments be-
cause being domain-specific, biomedical language
is conventionalised and therefore less varied in
terms of verb sense and usage than general lan-
guage. For example, verbs have stronger SPs for
their argument heads when many of their corpus
occurrences are of similar sense. This renders SP-
based features more useful for classification.
Due to differences in the data, methods, and ex-
perimental setup, direct comparison of our perfor-
mance figures with previously published ones is
difficult. The closest comparison point with gen-
eral language is (Korhonen et al, 2003) which re-
ported 59% mPUR using IB to assign 110 polyse-
mous English verbs into 34 classes. Our best re-
sults are substantially better (72-80% mPUR). It
is encouraging that we obtained such good results
despite focusing on a linguistically challenging do-
main.
In addition to the points mentioned earlier, our
future plans include seeding automatic classifica-
tion with more sophisticated information acquired
automatically from domain-specific texts (e.g. us-
ing named entity recognition and anaphoric link-
ing (Vlachos et al, 2006)). We will also explore
semi-automatic ML technology and active learn-
ing in aiding the classification. Finally, we plan to
conduct a bigger experiment with a larger number
of verbs, make the resulting classification publicly
available, and demonstrate its usefulness for prac-
tical BIO-NLP application tasks.
6 Conclusion
We reported large-scale experiments to investigate
the optimal characteristics of features required for
biomedical verb classification. A range of feature
sets and associated extraction methods were intro-
duced for this work, along with a robust cluster-
ing method capable of dealing with large data and
complex feature sets. A number of experiments
were reported. The best performing feature sets
proved to be the ones which include information
about SCFs supplemented with information about
verb tense and SPs in particular. The latter were
acquired automatically from corpus data using an
unsupervised method. Similar feature sets have
not proved equally useful in earlier work in gen-
eral language verb classification. We discussed
reasons for this and highlighted several areas for
future work.
Acknowledgement
Work on this paper was funded by the Royal So-
ciety, EPSRC (?ACLEX? project, GR/T19919/01)
and MRC (?CRAB? project, G0601766), UK.
455
References
Ananiadou, S., B. D. Kell, and J. Tsujii. 2006. Text
mining and its potential applications in systems biol-
ogy. Trends in Biotechnology, 24(12):571?579.
Briscoe, E. J. and J. Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In 5
th
ACL
Conference on Applied Natural Language Process-
ing, pages 356?363, Washington DC.
Briscoe, E. J. and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In 3
rd
Interna-
tional Conference on Language Resources and Eval-
uation, pages 1499?1504, Las Palmas, Gran Canaria.
Cohen, K. B. and L. Hunter. 2006. A critical review of
PASBio?s argument structures for biomedical verbs.
BMC Bioinformatics, 7(3).
Dang, H. T. 2004. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. Ph.D.
thesis, CIS, University of Pennsylvania.
Dorr, B. J. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual ma-
chine translation. Machine Translation, 12(4):271?
322.
Friedman, C., P. Kra, and A. Rzhetsky. 2002. Two
biomedical sublanguages: a description based on the
theories of zellig harris. Journal of Biomedical In-
formatics, 35(4):222?235.
Joanis, E., S. Stevenson, and D. James. 2007. A gen-
eral feature space for automatic verb classification.
Natural Language Engineering.
Joanis, E. 2002. Automatic verb classification using a
general feature space. Master?s thesis, University of
Toronto.
Korhonen, A., Y. Krymolowski, and N. Collier. 2006.
Automatic classification of verbs in biomedical texts.
In ACL-COLING, Sydney, Australia.
Kunze, C. 2000. Extension and use of germanet,
a lexical-semantic database. In 2nd International
Conference on Language Resources and Evaluation,
Athens, Greece.
Levin, B. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago.
Miller, G. A. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
Prescher, D., S. Riezler, and M. Rooth. 2000. Using
a probabilistic class-based lexicon for lexical am-
biguity resolution. In 18th International Confer-
ence on Computational Linguistics, pages 649?655,
Saarbr?ucken, Germany.
Puzicha, J., T. Hofmann, and J. M. Buhmann. 2000.
A theory of proximity-based clustering: structure
detection by optimization. Pattern Recognition,
33(4):617?634.
Schulte im Walde, S. 2006. Experiments on the au-
tomatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159?194.
Shi, L. and R. Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings of
the Sixth International Conference on Intelligent Text
Processing and Computational Linguistics, Mexico
City, Mexico.
Sun, L., A. Korhonen, and Y. Krymolowski. 2008.
Verb class discovery from rich syntactic data. In
9th International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Haifa, Is-
rael.
Swier, R. and S. Stevenson. 2004. Unsupervised se-
mantic role labelling. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 95?102, Barcelona, Spain,
August.
Tishby, N., F. C. Pereira, and W. Bialek. 1999. The
information bottleneck method. In Proc. of the
37
th
Annual Allerton Conference on Communica-
tion, Control and Computing, pages 368?377.
Vlachos, A., C. Gasperin, I. Lewin, and E. J. Briscoe.
2006. Bootstrapping the recognition and anaphoric
linking of named entitites in drosophila articles. In
Pacific Symposium in Biocomputing, Maui, Hawaii.
456
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 638?647,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Improving Verb Clustering with Automatically Acquired Selectional
Preferences
Lin Sun and Anna Korhonen
University of Cambridge, Computer Laboratory
15 JJ Thomson Avenue, Cambridge CB3 0GD, UK
ls418,alk23@cl.cam.ac.uk
Abstract
In previous research in automatic verb
classification, syntactic features have
proved the most useful features, although
manual classifications rely heavily on se-
mantic features. We show, in contrast
with previous work, that considerable ad-
ditional improvement can be obtained by
using semantic features in automatic clas-
sification: verb selectional preferences ac-
quired from corpus data using a fully unsu-
pervised method. We report these promis-
ing results using a new framework for
verb clustering which incorporates a re-
cent subcategorization acquisition system,
rich syntactic-semantic feature sets, and
a variation of spectral clustering which
performs particularly well in high dimen-
sional feature space.
1 Introduction
Verb classifications have attracted a great deal
of interest in natural language processing (NLP).
They have proved useful for various important
NLP tasks and applications, including e.g. parsing,
word sense disambiguation, semantic role label-
ing, information extraction, question-answering,
and machine translation (Swier and Stevenson,
2004; Dang, 2004; Shi and Mihalcea, 2005; Za-
pirain et al, 2008).
Verb classes are useful because they offer a
powerful tool for generalization and abstraction
which can be beneficial when faced e.g. with the
problem of data sparsity. Particularly useful can
be classes which capture generalizations over a
range of (cross-)linguistic properties, such as the
ones proposed by Levin (1993). Being defined in
terms of similar meaning and (morpho-)syntactic
behaviour of words, Levin style classes gener-
ally incorporate a wider range of properties than
e.g. classes defined solely on semantic grounds
(Miller, 1995).
In recent years, a variety of approaches have
been proposed for automatic induction of verb
classes from corpus data (Schulte im Walde, 2006;
Joanis et al, 2008; Sun et al, 2008; Li and Brew,
2008; Korhonen et al, 2008;
?
O S?eaghdha and
Copestake, 2008; Vlachos et al, 2009). This work
opens up the opportunity of learning and tuning
classifications tailored to the application and do-
main in question. Although manual classification
may always yields higher accuracy, automatic verb
classification is cost-effective and gathers statisti-
cal information as a side-effect of the acquisition
process which is difficult for humans to gather but
can be highly useful for NLP applications.
To date, both supervised and unsupervised ma-
chine learning (ML) methods have been proposed
for verb classification and used to classify a vari-
ety of features extracted from raw, tagged and/or
parsed corpus data. The best performing features
on cross-domain verb classification have been syn-
tactic in nature (e.g. syntactic slots, subcategoriza-
tion frames (SCFs)). Disappointingly, semantic
features have not yielded significant additional im-
provement, although they play a key role in man-
ual and theoretical work on verb classification and
could thus be expected to offer a considerable con-
tribution to classification performance.
Since the accuracy of automatic verb classifi-
cation shows room for improvement, we further
investigate the potential of semantic features ?
verb selectional preferences (SPs) ? for the task.
We introduce a novel approach to verb cluster-
ing which involves the use of (i) a recent subcate-
gorization frame (SCF) acquisition system (Preiss
et al, 2007) which produces rich lexical, SCF and
syntactic data, (ii) novel syntactic-semantic fea-
ture sets extracted from this data which incorpo-
rate a variety of linguistic information, including
SPs, and (iii) a new variation of spectral cluster-
638
ing based on the MNCut algorithm (Meila and Shi,
2001) which is well-suited for dealing with the re-
sulting, high dimensional feature space.
Using this approach, we show on two well-
established test sets that automatically acquired
SPs can be highly useful for verb clustering. They
yield high performance when used in combination
with syntactic features. We obtain our promis-
ing results using a fully unsupervised approach
to SP acquisition which differs from previous
approaches in that it does not exploit WordNet
(Miller, 1995) or other lexical resources. It is
based on clustering argument head data in the
grammatical relations associated with verbs.
We describe our features in section 2 and the
clustering methods in section 3. Experimental
evaluation and results are reported in sections 4
and 5, respectively. Section 6 provides discus-
sion and describes related work, and section 7 con-
cludes.
2 Features
Our target classification is the taxonomy of Levin
(1993) where verbs taking similar diathesis al-
ternations are assumed to share meaning compo-
nents and are organized into semantically coherent
classes. The main feature of this classification is a
diathesis alternation which manifests at the level
of syntax in alternating sets of SCF (e.g. in the
causative/inchoative alternation an NP frame alter-
nates with an intransitive frame: Tony broke the
window? The window broke).
Since automatic detection of diathesis alterna-
tions is very challenging (McCarthy, 2001), most
work on automatic classification has exploited the
fact that similar alternations tend to result in sim-
ilar SCFs. The research reported so far
1
has used
mainly syntactic features for classification, rang-
ing from shallow syntactic slots (e.g. NPs preced-
ing or following the verb) to SCFs. Some re-
searchers have discovered that supplementing ba-
sic syntactic features with information about ad-
juncts, co-occurrences, tense, and/or voice of the
verb have resulted in better performance.
However, additional information about seman-
tic SPs of verbs has not yielded considerable im-
provement on verb classification although SPs can
be strong indicators of diathesis alternations (Mc-
Carthy, 2001) and although fairly precise semantic
descriptions, including information about verb se-
1
See section 6 for discussion on previous work.
lectional restrictions, can be assigned to the major-
ity of Levin classes, as demonstrated by VerbNet
(Kipper-Schuler, 2005).
SP acquisition from undisambiguated corpus
data is arguably challenging (Brockmann and La-
pata, 2003; Erk, 2007; Bergsma et al, 2008). It is
especially challenging in the context of verb clas-
sification where SP models are needed for specific
syntactic slots for which the data may be sparse,
and the resulting feature vectors integrating both
syntactic and semantic features may be high di-
mensional. However, we wanted to investigate
whether better results could be obtained if the fea-
tures were optimised for richness, the feature ex-
traction for accuracy, and a clustering method ca-
pable of dealing with the resulting high dimen-
sional feature space was employed.
2.1 Feature extraction
We adopted a recent SCF acquisition system which
has proved more accurate than previous compa-
rable systems
2
but which has not been employed
for verb clustering before: the system of Preiss
et al (2007). This system tags, lemmatizes and
parses corpus data using the current version of the
RASP (Robust Accurate Statistical Parsing) toolkit
(Briscoe et al, 2006), and on the basis of resulting
grammatical relations (GRs) assigns each occur-
rence of a verb to one of 168 verbal SCFs classes
3
.
The system provides a filter which can be used
to remove adjuncts from the resulting lexicon.
We do not employ this filter since adjuncts have
proved informative for verb classification (Sun
et al, 2008; Joanis et al, 2008). However, we
do frequency-based thresholding to minimise the
noise (e.g. erroneous scfs) and sparse data in verb
classification and to ensure that only features sup-
ported by several verbs are used in classification:
we only consider SCFs and GRs which have fre-
quency larger than 40 with 5 or more verbs
4
.
The system produces a rich lexicon which in-
cludes raw and processed input sentences and pro-
vides a variety of material for verb clustering, in-
cluding e.g. (statistical) information related to the
part-of-speech (POS) tags, GRs, SCFs, argument
heads, and adjuncts of verbs. Using this mate-
rial, we constructed a wide range of feature sets
2
See Preiss et al (2007) for the details of evaluation.
3
We used an implementation of the SCF classifier pro-
vided by Paula Buttery.
4
These and other threshold values mentioned in this paper
were determined empirically on corpus data.
639
for experimentation, both shallow and deep syn-
tactic and semantic features. As described below,
some of the feature types have been employed in
previous works and some are novel.
2.2 Feature sets
The first feature set F1 includes information
about the lexical context (co-occurrences) of verbs
which has proved useful for supervised verb clas-
sification (Li and Brew, 2008):
F1: Co-occurrence (CO): We adopt the best
method of Li and Brew (2008) where collo-
cations are extracted from the four words im-
mediately preceding and following a lemma-
tized verb. Stop words are removed prior to
extraction, and the 600 most frequent result-
ing COs are kept.
F2-F3 provide information about lexical prefer-
ences of verbs in argument head positions of spe-
cific GRs associated with the verb:
F2: Prepositional preference (PP): the type and
frequency of prepositions in the indirect ob-
ject relation.
F3: Lexical preference (LP): the type and fre-
quency of nouns and prepositions in the sub-
ject, object, and indirect object relation.
All the other feature sets include information
about SCFs which have been widely employed in
verb classification, e.g. (Schulte im Walde, 2006;
Sun et al, 2008; Li and Brew, 2008; Korhonen
et al, 2008). F4-F7 include basic SCF information
and/or refine it with additional information which
has proved useful in previous works:
F4: SCFs and relative frequencies with verbs.
SCFs abstract over particles and prepositions.
F5: F4 with COs (F1). The SCF and CO feature
vectors are concatenated.
F6: F4 with the tense of the verb. The frequency
of verbal POS tags is calculated specific to
each SCF.
F7: F4 with PPs (F2). This feature parameterizes
SCFs for prepositions.
F8: Basic SCF feature corresponding to F4 but ex-
tracted from the VALEX lexicon (Korhonen
et al, 2006)
5
.
The following 9 feature sets are novel. They
build on F7, refining it further. F9-F11 refine F7
with information about LPs:
F9: F7 with F3 (subject only)
F10: F7 with F3 (object only)
F11: F7 with F3 (subject, object, indirect object)
F12-17 refine F7 with SPs. We adopt a fully un-
supervised approach to SP acquisition. We acquire
the SPs by
1. taking the GR relations (subject, object, indi-
rect object) associated with verbs,
2. extracting all the argument heads in these re-
lations which occur with frequency> 20 with
more than 3 verbs, and
3. clustering the resulting N most frequent ar-
gument heads into M classes using the spec-
tral clustering method described in the fol-
lowing section.
We tried the N settings {200, 500} and the M
settings {10, 20, 30, 80}. The best settings N =
200,M = 20 and N = 500,M = 30 are reported
in this paper. We enforce the features to be shared
by all the potential members of a verb class. The
expected class size is approximatelyN/K, and we
allow for 10% outliers (the features occurring less
than (N/K)? 0.9 verbs are thus removed).
The resulting SPs are combined with SCFs in a
similar fashion as LPs are combined with SCFs in
F9-F11:
F12-F14: as F9-F11 but SPs (20 clusters from 200
argument heads) are used instead of LPs
F15-F17: as F9-F11 but SPs (30 clusters from 500
argument heads) are used instead of LPs
5
This feature was included to enable comparing the con-
tribution of the recent SCF system to that of an older, com-
parable system which was used for constructing the VALEX
lexicon.
640
3 Clustering methods
We use two clustering methods: (i) pairwise clus-
tering (PC) which obtained the best performance
in comparison with several other methods in re-
cent work on biomedical verb clustering (Korho-
nen et al, 2008), and (ii) a method which is
new to the task (and to the best of our knowl-
edge, to NLP): a variation of spectral clustering
which exploits the MNCut algorithm (Meila and
Shi, 2001) (SPEC). Spectral clustering has been
shown to be effective for high dimensional and
non-convex data in NLP (Chen et al, 2006) and
it has been applied to German verb clustering by
Brew and Schulte im Walde (2002). However, pre-
vious work has used Ng et al (2002)?s algorithm,
while we adopt the MNCut algorithm. The lat-
ter has shown a wider applicability (von Luxburg,
2007; Verma and Meila, 2003) and it can be justi-
fied from the random walk view, which has a clear
probabilistic interpretation.
Clustering groups a given set of items (verbs in
our experiment) V = {v
n
}
N
n=1
into a disjoint par-
tition of K classes I = {I
k
}
K
k=1
. Both our algo-
rithms take a similarity matrix as input. We con-
struct this from the skew divergence (Lee, 2001).
The skew divergence between two feature vectors
v and v
?
is d
skew
(v, v
?
) = D(v
?
||a ?v+(1?a) ?v
?
)
where D is the KL-divergence. v is smoothed
with v
?
. The level of smoothing is controlled by
a whose value is set to a value close to 1 (e.g.
0.9999). We symmetrize the skew divergence
as follows: d(v, v
?
)
sskew
=
1
2
(d
skew
(v, v
?
) +
d
skew
(v
?
, v)).
SPEC is typically used with the Radial Basis
Function (RBF) kernel. We adopt a new kernel
similar to the symmetrized KL divergence kernel
(Moreno et al, 2004) which avoids the need for
scale parameter estimation.
w(v, v
?
) = exp(?d
sskew
(v, v
?
))
The similarity matrix W is constructed where
W
ij
= w(v
i
, v
j
).
Pairwise clustering
PC (Puzicha et al, 2000) is a method where a cost
criterion guides the search for a suitable partition.
This criterion is realized through a cost function of
the similarity matrix W and partition I:
H = ?
?
n
j
? Avgsim
j
,
Avgsim
j
=
P
{a,b?A
j
}
w(a,b)
n
j
?(n
j
?1)
where n
j
is the size of the j
th
cluster and Avgsim
j
is the average similarity between cluster members.
Spectral clustering
In SPEC, the similarities W
ij
are viewed as the
weight on the edges ij of a graph G over V . The
similarity matrix W is thus the adjacency matrix
for G. The degree of a vertex i is d
i
=
?
N
j=1
w
ij
.
A cut between two partitions A and A
?
is defined
to be Cut(A,A
?
) =
?
m?A,n?A
?
W
mn
.
In MNCut algorithm, the similarity matrixW is
transformed to a stochastic matrix P .
P = D
?1
W (1)
The degree matrix D is a diagonal matrix where
D
ii
= d
i
.
It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constant
6
with respect to a partition I
?
and their
eigenvalues are not zero, then I
?
minimizes the
multiway normalized cut(MNCut):
MNCut(I) = K ?
?
K
k=1
Cut(I
k
,I
k
)
Cut(I
k
,I)
P
mn
can be interpreted as the transition probabil-
ity between vertices m,n. The criterion can thus
be expressed as MNCut(I) =
?
K
k=1
(1?P (I
k
?
I
k
|I
k
)) (Meila, 2001), which is the sum of transi-
tion probabilities across different clusters. The cri-
terion finds the partition where the random walks
are most likely to happen within the same cluster.
In practice, the K leading eigenvectors of P is
not piecewise constant. But we can extract the
partition by finding the approximately equal ele-
ments in the eigenvectors using a clustering algo-
rithm like K-means.
The numerator of MNCut is similar to the cost
function of PC. The main differences between the
two algorithms are: 1) MNCut takes into account
of the cross cluster similarity, while PC does not.
2) PC optimizes the cost function using determin-
istic annealing, whereas SPEC uses eigensystem
decomposition.
The spectral clustering algorithm is based on the
Multicut algorithm (Meila and Shi, 2001).
6
The eigenvector v is piecewise constant with respect to I
if v(i) = v(j)?i, j ? I
k
and k ? 1, 2...K
641
Input: Dataset S, Number of clusters K
1. Compute similarity matrixW and Degree ma-
trix D
2. Construct stochastic matrix P using equation
1
3. Compute the eigenvalues and eigenvectors
{?
n
, x
n
}
N
n=1
of P , where ?
n
? ?
n+1
, form a
matrix X = [x
2
, . . . , x
k
] by stacking the eigen-
vectors in columns.
4. Form a matrix Y from X by normalizing the
row sums to have norm 1: Y
ij
= X
ij
/(
?
j
X
2
ij
)
1
2
5. Consider the row of Y to be the transformed
feature vectors for each verb and cluster them
into clusters C
1
. . . C
k
usingK-means clustering
algorithm.
Output: Clusters C
1
. . . C
k
4 Experimental evaluation
4.1 Test sets
We employed two test sets which have been used
to evaluate previous work on English verb classi-
fication:
T1 The test set of Joanis et al (2008) provides
a classification of 835 verbs into 15 (some
coarse, some fine-grained) Levin classes. 11
tests are provided for 2-14 way classifica-
tions. We employ the 14 way classifica-
tion because this corresponds the closest to
our target (Levin?s fine-grained) classifica-
tion
7
. We select 586 verbs according to Joa-
nis et al?s selection criteria, resulting in 10-
120 verbs per class. We restrict the class
imbalance to 1:1.5.
8
. This yields 205 verbs
(10-15 verbs per class) which is similar to
the sub-set of T1 employed by Stevenson and
Joanis (2003).
T2 The test set of Sun et al (2008) classifies 204
verbs to 17 fine-grained Levin classes, so that
each class has 12 member verbs.
Table 1 shows the classes in T1 and T2.
4.2 Data processing
For each verb in T1 and T2, we extracted all
the occurrences (up to 10,000) from the raw cor-
pus data gathered originally for constructing the
7
However, the correspondence is not perfect with half
of the classes including two or more Levin?s fine-grained
classes.
8
Otherwise, in the case of a large class imbalance the eval-
uation measure would be dominated by the classes with large
population.
T1
Object Drop 26.{1,3,7}
Recipient 13.{1,3}
Admire 31.2
Amuse 31.1
Run 51.3.2
Sound 43.2
Light &
43.{1,4}
Substance
Cheat 10.6
Steal &
10.{5,1}
Remove
Wipe 10.4.{1,2}
Spray / Load 9.7
Fill 9.8
Putting 9.1-6
Change of State 45.1-4
T2
Remove 10.1
Send 11.1
Get 13.5.1
Hit 18.1
Amalgamate 22.2
Characterize 29.2
Peer 30.3
Amuse 31.1
Correspond 36.1
Manner
37.3
of speaking
Say 37.7
Nonverbal
40.2
expression
Light 43.1
Other change
45.4
of state
Mode with
47.3
Motion
Run 51.3.2
Put 9.1
Table 1: Levin classes in T1 and T2
T1 T2
total avg total avg
CO F1 1328 764 743 382
LP (p) F2 61 37 55 25
LP (all) F3 2521 526 1481 295
SCF F4 88 46 86 38
SCF+CO F5 1466 833 856 422
SCF+POS F6 319 114 299 87
SCF+P F7 282 96 273 76
SCF (V) F8 - - 92 45
SCF+LP (s) F9 1747 324 1474 225
SCF+LP (o) F10 2817 424 2319 279
SCF+LP (all) F11 4250 649 3515 426
SCF+SP20 (s) F12 821 235 690 145
SCF+SP20 (o) F13 792 218 706 135
SCF+SP20 (all) F14 1333 357 1200 231
SCF+SP30 (s) F15 977 274 903 202
SCF+SP30 (o) F16 1026 273 1012 205
SCF+SP30 (all) F17 1720 451 1640 330
Table 2: (i) The total number of features and (ii)
the average per verb for all the feature sets
VALEX lexicon (Korhonen et al, 2006). The data
was gathered from five corpora, including e.g. the
British National Corpus (Leech, 1992) and the
North American News Text Corpus (Graff, 1995).
The average frequency of verbs in T1 was 1448
and T2 2166, showing that T1 is a more sparse
data set.
The data was first processed using the feature
extraction module. Table 2 shows (i) the total
number of features in each feature set and (ii) the
average per verb in the resulting lexicons for T1
and T2.
We normalized the feature vectors by the sum
of the feature values before applying the clustering
techniques. Since both clustering algorithms have
642
an element of randomness, we run them multiple
times. The step 5 of SPEC (K-means) was run for
50 times. The result that minimizes the distortion
(the distances to cluster centroid) is reported. PC
was run 20 times, and the results are averaged.
4.3 Evaluation measures
To facilitate meaningful comparisons, we em-
ploy the same measures for evaluation as previ-
ously employed e.g. by Korhonen et al (2008);
?
O
S?eaghdha and Copestake (2008).
The first measure is modified purity (mPUR) ?
a global measure which evaluates the mean preci-
sion of clusters. Each cluster is associated with its
prevalent class. The number of verbs in a cluster
K that take this class is denoted by n
prevalent
(K).
Verbs that do not take it are considered as errors.
Clusters where n
prevalent
(K) = 1 are disregarded
as not to introduce a bias towards singletons:
mPUR =
?
n
prevalent(k
i
)>2
n
prevalent(k
i
)
number of verbs
The second measure is weighted class accuracy
(ACC): the proportion of members of dominant
clusters DOM-CLUST
i
within all classes c
i
.
ACC =
?
C
i=1
verbs in DOM-CLUST
i
number of verbs
mPUR and ACC can be seen as a measure of pre-
cision(P) and recall(R) respectively. We calculate
F measure as the harmonic mean of P and R:
F =
2 ? mPUR ? ACC
mPUR + ACC
The random baseline(BL) is calculated as follows:
BL = 1/number of classes
5 Results
5.1 Quantitative evaluation
Table 3 includes the F-measure results for all the
feature sets when the two methods (PC and SPEC)
are used to cluster verbs in the test sets T1 and T2,
respectively. A number of tendencies can be ob-
served in the results. Firstly, the results for T2 are
clearly better than those for T1. Including a higher
number of verbs lower in frequency from classes
of variable granularity, T1 is probably a more chal-
lenging test set than T2. T2 is controlled for the
number and frequency of verbs to facilitate cross-
class comparisons. While this may contribute to
better results, T2 is a more accurate test set for us
in the sense that it offers a better correspondence
with our target (fine-grained Levin) classes.
T1 T2
PC SPEC PC SPEC
BL 7.14 7.14 5.88 5.88
CO F1 15.62 33.85 17.86 40.94
LP (p) F2 40.40 38.97 50.98 49.02
LP (all) F3 42.94 47.50 41.08 74.55
SCF F4 34.22 36.16 52.33 57.78
SCF+CO F5 26.43 28.70 19.52 29.10
SCF+POS F6 36.14 34.75 44.44 46.70
SCF+P F7 43.57 43.85 63.40 63.28
SCF (V) F8 - - 34.08 38.30
SCF+LP (s) F9 47.72 56.09 65.94 71.65
SCF+LP (o) F10 43.09 48.43 57.11 73.97
SCF+LP (all) F11 45.87 54.63 56.30 72.97
SCF+SP20 (s) F12 46.67 57.75 39.52 71.67
SCF+SP20 (o) F13 44.95 51.70 40.76 70.78
SCF+SP20(all) F14 48.19 55.12 39.68 73.09
SCF+SP30 (s) F15 45.89 56.10 64.44 80.35
SCF+SP30 (o) F16 42.01 48.74 52.75 70.52
SCF+SP30(all) F17 46.66 52.68 51.07 68.67
Table 3: Results on testsets T1 and T2
Secondly, the difference between the two clus-
tering methods is clear: the new SPEC outperforms
PC on both test sets and across all the feature sets.
The performance of the two methods is still fairly
similar with the more basic, less sparse feature sets
(F1-F2, F4, F6-7) but when the more sophisticated
feature sets are used (F3, F5, F9-F17) SPEC per-
forms considerably better. This demonstrates that
it is clearly a better suited method for high dimen-
sional feature sets.
Comparing the feature sets, the simple co-
occurrence based F1 performs clearly better than
the random baseline. F2 and F3 which exploit lex-
ical data in the argument head positions of GRs
prove significantly better than F1. F3 yields sur-
prisingly good results on T2: it is the second best
feature set on this test set. Also on T1, F3 per-
forms better than the SCF-based feature sets F4-
F7. This demonstrates the usefulness of lexical
data when obtained from argument positions in
relevant GRs.
Our basic SCF feature set F4 performs consid-
erably better than the comparable feature set F8
obtained from the VALEX lexicon. The difference
is 19.50 in F-measure. As both lexicons were ex-
tracted from the same corpus data, the improve-
ment can be attributed to improved parser and SCF
acquisition performance (Preiss et al, 2007).
F5-F7 refine the basic SCF feature set F4 fur-
ther. F5 which combines a SCF with CO in-
formation proved the best feature set in the su-
pervised verb classification experiment of Li and
Brew (2008). In our experiment, F5 produces sub-
stantially lower result than CO and SCF alone (i.e.
643
F1 and F4). However, our corpus is smaller (Li
and Brew used the large Gigaword corpus), our
SCFs are different, and our approach is unsuper-
vised, making meaningful comparisons difficult.
F6 combines F4 with information about verb
tense. This was not helpful: F6 produces worse re-
sults than F4. F7, on the other hand, yields better
results than F4 on both test sets. This demonstrates
what the previous research has shown: SCF per-
form better when parameterized for prepositions.
Looking at our novel feature sets F9-F17, F9-
F11 combine the most accurate SCF feature set
F4 with the LP-based features F2-F3. Although
the feature space gets more sparse, all the feature
sets outperform F2-F3 on T1. On T2, F3 per-
forms exceptionally well, and thus yields a better
result than F9-F11, but F9-F11 nevertheless per-
form clearly better than the best SCF-based feature
set F4 alone. The differences among F9, F10 and
F11 are small on T2, but on T1 F9 yields the best
performance. It could be that F9 works the best
for the more sparse T1 because it suffers the least
from data sparsity (it uses LPs only for the subject
relation).
F12-F17 replace the LPs in F9-F11 by semantic
SPs. When only 20 clusters are used as SP models
and acquired from the smaller sample of (200) ar-
gument heads (F12-F14), SPs do not perform bet-
ter than LPs on T2. A small improvement can be
observed on T1, especially with F12 which uses
only the subject data (yielding the best F measure
on T1: 57.75%). However, when 30 more fine-
grained clusters are acquired from a bigger sample
of (500) argument heads (F15-F17), lower results
can be seen on T1. On T2, on the other hand, F15
yields dramatic improvement and we get the best
performance for this test set: 80.35% F-measure.
The fact that no improvement is observed when
using F16 and F17 on T2 could be explained by
the fact that SPs are stronger for the subject posi-
tion which also suffers less from the sparse data
problem than e.g. i. object position. The fact that
no improvement is observed on T1 is likely to be
due to the fact that verbs have strong SPs only at
the finer-grained level of Levin classification. Re-
call that in T1, as many as half of the classes are
coarser-grained.
5.2 Qualitative evaluation
The best performing feature sets on both T1 and
T2 were thus our new SP-based feature sets. We
conducted qualitative analysis of the best 30 SP
Human mother, wife, parent, girl, child
Role patient, student, user, worker, teacher
Body-part neck, shoulder, back, knee, corner
Authority committee, police, court, council, board
Organization society, firm, union, bank, institution
Money cash, currency, pound, dollar, fund
Amount proportion, value, size, speed, degree
Time minute, moment, night, hour, year
Path street, track, road, stair, route
Building office, shop, hotel, hospital, house
Region site, field, area, land, island
Technology system, model, facility, engine, machine
Task operation, test, study, analysis, duty
Arrangement agreement, policy, term, rule, procedure
Matter aspect, subject, issue, question, case
Problem difficulty, challenge, loss, pressure, fear
Idea argument, concept, idea, theory, belief
Power control, lead, influence, confidence, ability
Form colour, style, pattern, shape, design
Item letter, book, goods, flower, card
Table 4: Cluster analysis: 20 clusters, their SP la-
bels, and prototypical member nouns
clusters in the T2 data created using SPEC to find
out whether these clusters were really semantic in
nature, i.e. captured semantically meaningful pref-
erences. As no gold standard specific to our verb
classification task was available, we did manual
cluster analysis using VerbNet (VN) as aid. In VN,
Levin classes are assigned with semantic descrip-
tions: the arguments of SCFs involved in diathesis
alternations are labeled with thematic roles some
of which are labeled with selectional restrictions.
From the 30 thematic role types in VN, as many
as 20 are associated with the 17 Levin classes in
T2. The most frequent role in T2 is agent, fol-
lowed by theme, location, patient, recipient, and
source. From the 36 possible selectional restric-
tion types, 7 appear in T2; the most frequent ones
being +animate and +organization, followed by
+concrete, +location, and +communication.
As SP clusters capture selectional preferences
rather than restrictions, we examined manu-
ally whether the 30 clusters (i) capture seman-
tically meaningful classes, and whether they (ii)
are plausible given the VN semantic descrip-
tions/restrictions for the classes in T2.
The analysis revealed that all the 30 clusters had
a predominant, semantically motivated SP sup-
ported by the majority of the member nouns. Al-
though many clusters could be further divided into
more specific SPs (and despite the fact that some
nouns were clearly misclassified), we were able to
assign each cluster a descriptive label characteriz-
ing the predominant SP. Table 4 shows 15 sam-
644
ple clusters, the SP labels assigned to them, and a
number of example nouns in these clusters.
When comparing each SP cluster against the
VN semantic descriptions/restrictions for T2, we
found that each predominant SP was plausible.
Also, the SPs frequent in our data were also fre-
quent among the 17 classes according to VN. For
example, the many SP clusters labeled as arrange-
ments, issues, ideas and other abstract concepts
were also frequent in T2, e.g. among COMMUNI-
CATION (37), CHARACTERISE (29.2), AMALGA-
MATE (22.2) and other classes.
This analysis showed that the SP models which
performed well in verb clustering were semanti-
cally meaningful for our task. An independent
evaluation using one of the standard datasets avail-
able for SP acquisition research (Brockmann and
Lapata, 2003) is of course needed to determine
how well the acquisition method performs in com-
parison with other existing methods.
Finally, we evaluated the quality of the verb
clusters created using the SP-based features. We
found that some of the errors were similar to those
seen on T2 when using syntactic features: errors
due to polysemy and syntactic idiosyncracy. How-
ever, a new error type clearly due to the SP-based
feature was detected. A small number of classes
got confused because of strong similar SPs in the
subject (agent) position. For example, some PEER
(30.3) verbs (e.g. look, peer) were found in the
same cluster with SAY (37.7) verbs (e.g. shout,
yell) ? an error which purely syntactic features do
not produce. Such errors were not numerous and
could be addressed by developing more balanced
SP models across different GRs.
6 Discussion and related work
Although features incorporating semantic infor-
mation about verb SPs make theoretical sense they
have not proved equally promising in previous ex-
periments which have compared them against syn-
tactic features in verb classification. Joanis et al
(2008) incorporated an ?animacy? feature (a kind
of a ?SP?) which was determined by classifying
e.g. pronouns and proper names in data to this sin-
gle SP class. A small improvement was obtained
when this feature was used in conjunction with
syntactic features in supervised classification.
Joanis (2002) and Schulte im Walde (2006) ex-
perimented with more conventional SPs with syn-
tactic features in English and German verb clas-
sification, respectively. They employing top level
Method Result
T1
Li et al 2008 supervised 66.3
Joanis et al 2008 supervised 58.4
Stevenson et al 2003
semi-supervised 29
unsupervised 31
SPEC unsupervised 57.55
T2 Sun et al 2008
supervised 62.50
unsupervised 51.6
?
O S?eaghdha et al 2008 supervised 67.3
SPEC unsupervised 80.35
Table 5: Previous verb classification results
WordNet (Miller, 1995) and Germanet (Kunze and
Lemnitzer, 2002) classes as SP models. Joanis
(2002) obtained no improvement over syntactic
features, whereas Schulte im Walde (2006) ob-
tained insignificant improvement.
Korhonen et al (2008) combined SPs with SCFs
when clustering biomedical verbs. The SPs were
acquired automatically from syntactic slots of
SCFs (not from GRs as in our experiment) using
PC clustering. A small improvement was obtained
using LPs extracted from the same syntactic slots,
but the SP clusters offered no improvement. Re-
cently, Schulte im Walde et al (2008) proposed an
interesting SP acquisition method which involves
combining EM training and the MDL principle for
an verb classification incorporating SPs. However,
no comparison against purely syntactic features is
provided.
In our experiment, we obtained a considerable
improvement over syntactic features, despite using
a fully unsupervised approach to both verb clus-
tering and SP acquisition. In addition to the rich,
syntactic-semantic feature sets, our good results
can be attributed to the clustering technique capa-
ble of dealing with them. The potential of spectral
clustering for the task was recognised earlier by
Brew and Schulte im Walde (2002). Although a
different version of the algorithm was employed
and applied to German (rather than to English),
and although no SP features were used, these ear-
lier experiments did demonstrate the ability of the
method to perform well in high dimensional fea-
ture space.
To get an idea of how our performance com-
pares with that of related approaches, we exam-
ined recent works on verb classification (super-
vised and unsupervised) which were evaluated on
same test sets using comparable evaluation mea-
sures. These works are summarized in table 5.
ACC and F-measure are shown for T1 and T2, re-
spectively.
645
On T1, the best performing supervised method
reported so far is that of Li and Brew (2008). Li
and Brew used Bayesian Multinomial Regression
for classification. A range of feature sets integrat-
ing COs, SCFs and/or LPs were evaluated. The
combination of COs and SCFs gave the best result,
shown in the table. Joanis et al (2008) report the
second best supervised result on T1, using Support
Vector Machines for classification and features de-
rived from linguistic analysis: syntactic slots, slot
overlaps, tense, voice, aspect, and animacy of NPs.
Stevenson and Joanis (2003) report a semi- and
unsupervised experiment on T1. A feature set sim-
ilar to that of Joanis et al (2008) was employed
(features were selected in a semi-supervised fash-
ion) and hierarchical clustering was used.
Our unsupervised method SPEC performs sub-
stantially better than the unsupervised method of
Stevenson et al and nearly as well as the super-
vised approach of Joanis et al (2008) (note, how-
ever, that the different experiments involved differ-
ent sub-sets of T1 so are not entirely comparable).
On T2, the best performing supervised method
so far is that of
?
O S?eaghdha and Copestake (2008)
which employs a distributional kernel method to
classify SCF features parameterized for preposi-
tions in the automatically acquired VALEX lexicon.
Using exactly the same data and feature set, Sun
et al (2008) obtain a slightly lower result when us-
ing a supervised method (Gaussian) and a notably
lower result when using an unsupervised method
(PC clustering). Our method performs consider-
ably better and also outperforms the supervised
method of
?
O S?eaghdha and Copestake (2008).
7 Conclusion and Future Work
We introduced a new approach to verb cluster-
ing which involves the use of (i) rich lexical, SCF
and GR data produced by a recent SCF system, (ii)
novel syntactic-semantic feature sets which com-
bine a variety of linguistic information, and (iii) a
new variation of spectral clustering which is par-
ticularly suited for dealing with the resulting, high
dimensional feature space. Using this approach,
we showed on two well-established test sets that
automatically acquired SPs can be highly useful
for verb clustering. This result contrasts with most
previous works but is in line with theoretical work
on verb classification which relies not only on syn-
tactic but also on semantic features (Levin, 1993).
In addition to the ideas mentioned earlier, our
future plans include looking into optimal ways
of acquiring SPs for verb classification. Consid-
erable research has been done on SP acquisition
most of which has involved collecting argument
headwords from data and generalizing to Word-
Net classes. Brockmann and Lapata (2003) have
showed that WordNet-based approaches do not
always outperform simple frequency-based mod-
els, and a number of techniques have been re-
cently proposed which may offer ideas for refin-
ing our current unsupervised approach (Erk, 2007;
Bergsma et al, 2008). The number and type (and
combination) of GRs for which SPs can be reliably
acquired, especially when the data is sparse, re-
quires also further investigation.
In addition, we plan to investigate other po-
tentially useful features for verb classification
(e.g. named entities and preposition classes) and
explore semi-automatic ML technology and active
learning for guiding the classification. Finally, we
plan to conduct a bigger experiment with a larger
number of verbs, and conduct evaluation in the
context of practical application tasks.
Acknowledgments
Our work was funded by the Dorothy Hodgkin
Postgraduate Award, the Royal Society Univer-
sity Research Fellowship, and the EPSRC grant
EP/F030061/1, UK. We would like to thank Paula
Buttery for letting us use her implementation of
the SCF classifier and Yuval Krymolowski for the
support he provided for feature extraction.
References
Shane Bergsma, Dekang Lin, and Randy Goebel. Dis-
criminative learning of selectional preference from
unlabeled text. In Proc. of EMNLP, 2008.
Chris Brew and Sabine Schulte im Walde. Spectral
clustering for german verbs. In Proc. of EMNLP,
2002.
Ted Briscoe, John Carroll, and Rebecca Watson. The
second release of the rasp system. In Proc. of the
COLING/ACL on Interactive presentation sessions,
2006.
Carsten Brockmann and Mirella Lapata. Evaluating
and combining approaches to selectional preference
acquisition. In Proc. of EACL, 2003.
Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and
Zheng-Yu Niu. Unsupervised relation disambigua-
tion using spectral clustering. In Proc. of COL-
ING/ACL, 2006.
Hoa Trang Dang. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. PhD
thesis, CIS, University of Pennsylvania, 2004.
Katrin Erk. A simple, similarity-based model for selec-
tional preferences. In Proc. of ACL, 2007.
646
David Graff. North american news text corpus. Lin-
guistic Data Consortium, 1995.
Eric Joanis. Automatic Verb Classification Using a
General Feature Space. Master?s thesis, University
of Toronto, 2002.
Eric Joanis, Suzanne Stevenson, and David James. A
general feature space for automatic verb classifica-
tion. Natural Language Engineering, 2008.
Karin Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. 2005.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
A large subcategorization lexicon for natural lan-
guage processing applications. In Proc. of the 5th
LREC, 2006.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. The Choice of Features for Classification of
Verbs in Biomedical Texts. In Proc. of COLING,
2008.
Claudia Kunze and Lothar Lemnitzer. GermaNet-
representation, visualization, application. In Proc.
of LREC, 2002.
Lillian. Lee. On the effectiveness of the skew diver-
gence for statistical language analysis. In Artificial
Intelligence and Statistics, 2001.
Geoffrey Leech. 100 million words of english: the
british national corpus. Language Research, 1992.
Beth. Levin. English verb classes and alternations: A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Fea-
tures for Automatic Verb Classification. In Proc. of
ACL, 2008.
Diana McCarthy. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Alternations, Sub-
categorization Frames and Selectional Preferences.
PhD thesis, University of Sussex, UK, 2001.
Marina. Meila. The multicut lemma. Technical report,
University of Washington, 2001.
Marina Meila and Jianbo Shi. A random walks view of
spectral segmentation. AISTATS, 2001.
George A. Miller. WordNet: a lexical database for En-
glish. Communications of the ACM, 1995.
Pedro J. Moreno, Purdy P. Ho, and Nuno Vasconce-
los. A Kullback-Leibler divergence based kernel for
SVM classification in multimedia applications. In
Proc. of NIPS, 2004.
Andrew Y. Ng, Michael Jordan, and Yair Weiss. On
spectral clustering: Analysis and an algorithm. In
Proc. of NIPS, 2002.
Diarmuid
?
O S?eaghdha and Ann Copestake. Semantic
classification with distributional kernels. In Proc. of
COLING, 2008.
Judita Preiss, Ted Briscoe, and Anna Korhonen. A sys-
tem for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from cor-
pora. In Proc. of ACL, 2007.
Jan Puzicha, Thomas Hofmann, and Joachim M. Buh-
mann. A theory of proximity based clustering:
Structure detection by optimization. Pattern Recog-
nition, 2000.
Sabine Schulte im Walde. Experiments on the auto-
matic induction of german semantic verb classes.
Computational Linguistics, 2006.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. Combining EM
Training and the MDL Principle for an Automatic
Verb Classification incorporating Selectional Pref-
erences. In Proc. of ACL, pages 496?504, 2008.
Lei Shi and Rada Mihalcea. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proc. of CICLING, 2005.
Suzanne Stevenson and Eric Joanis. Semi-supervised
verb class discovery using noisy features. In Proc.
of HLT-NAACL 2003, pages 71?78, 2003.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
Verb class discovery from rich syntactic data. Lec-
ture Notes in Computer Science, 4919:16, 2008.
Robert Swier and Suzanne Stevenson. Unsupervised
semantic role labelling. In Proc. of EMNLP, 2004.
Deepak Verma and Marina Meila. Comparison of spec-
tral clustering methods. Advances in Neural Infor-
mation Processing Systems (NIPS 15), 2003.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. Unsupervised and constrained dirich-
let process mixture models for verb clustering. In
Proc. of the Workshop on Geometrical Models of
Natural Language Semantics, 2009.
Ulrike von Luxburg. A tutorial on spectral clustering.
Statistics and Computing, 2007.
Be?nat Zapirain, Eneko Agirre, and Llu??s M`arquez. Ro-
bustness and generalization of role sets: PropBank
vs. VerbNet. In Proc. of ACL, 2008.
647
Automatic Classification of English Verbs Using Rich Syntactic Features
Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
ls418,alk23@cl.cam.ac.uk
Yuval Krymolowski
Department of Computer Science
University of Haifa
31905, Haifa
Israel
yuvalkry@gmail.com
Abstract
Previous research has shown that syntactic
features are the most informative features
in automatic verb classification. We exper-
iment with a new, rich feature set, extracted
from a large automatically acquired subcate-
gorisation lexicon for English, which incor-
porates information about arguments as well
as adjuncts. We evaluate this feature set us-
ing a set of supervised classifiers, most of
which are new to the task. The best classi-
fier (based on Maximum Entropy) yields the
promising accuracy of 60.1% in classifying
204 verbs to 17 Levin (1993) classes. We
discuss the impact of this result on the state-
of-art, and propose avenues for future work.
1 Introduction
Recent research shows that it is possible, using cur-
rent natural language processing (NLP) and machine
learning technology, to automatically induce lex-
ical classes from corpus data with promising ac-
curacy (Merlo and Stevenson, 2001; Korhonen et
al., 2003; Schulte im Walde, 2006; Joanis et al,
2007). This research is interesting, since lexi-
cal classifications, when tailored to the application
and domain in question, can provide an effective
means to deal with a number of important NLP
tasks (e.g. parsing, word sense disambiguation, se-
mantic role labeling), as well as enhance perfor-
mance in many applications (e.g. information ex-
traction, question-answering, machine translation)
(Dorr, 1997; Prescher et al, 2000; Swier and Steven-
son, 2004; Dang, 2004; Shi and Mihalcea, 2005).
Lexical classes are useful because they capture
generalizations over a range of (cross-)linguistic
properties. Being defined in terms of similar mean-
ing components and (morpho-)syntactic behaviour
of words (Jackendoff, 1990; Levin, 1993) they
generally incorporate a wider range of properties
than e.g. classes defined solely on semantic grounds
(Miller, 1990). They can be used to build a lexical
organization which effectively captures generaliza-
tions and predicts much of the syntax and semantics
of a new word by associating it with an appropriate
class. This can help compensate for lack of data for
individual words in NLP.
Large-scale exploitation of lexical classes in real-
world or domain-sensitive tasks has not been pos-
sible because existing manually built classifications
are incomprehensive. They are expensive to extend
and do not incorporate important statistical infor-
mation about the likelihood of different classes for
words. Automatic classification is a better alterna-
tive. It is cost-effective and gathers statistical infor-
mation as a side-effect of the acquisition process.
Most work on automatic classification has fo-
cussed on verbs which are typically the main pred-
icates in sentences. Syntactic features have proved
the most informative in verb classification. Exper-
iments have been reported using both (i) deep syn-
tactic features (e.g. subcategorization frames (SCFs))
extracted using parsers and subcategorisation acqui-
sition systems (Schulte im Walde, 2000; Korhonen
et al, 2003; Schulte im Walde, 2006) and (ii) shal-
low ones (e.g. NPs/PPs preceding/following verbs)
extracted using taggers and chunkers (Merlo and
Stevenson, 2001; Joanis et al, 2007).
769
(i) correspond closely with features used for
manual classification (Levin, 1993). They have
proved successful in the classification of German
(Schulte im Walde, 2006) and English verbs (Ko-
rhonen et al, 2003). Yet promising results have also
been reported when using (ii) for English verb clas-
sification (Merlo and Stevenson, 2001; Joanis et al,
2007). This may indicate that (i) are optimal for the
task when combined with additional syntactic infor-
mation from (ii).
We investigate this matter by experimenting with
a new, rich feature set which incorporates informa-
tion about SCFs (arguments) as well as adjuncts. It
was extracted from VALEX, a large automatically
acquired SCF lexicon for English (Korhonen et al,
2006). We evaluate the feature set thoroughly us-
ing set of supervised classifiers, most of which are
new in verb classification. The best performing clas-
sifier (Maximum Entropy) yields the accuracy of
60.1% on classifying 204 verbs into 17 Levin (1993)
classes. This result is good, considering that we per-
formed no sophisticated feature engineering or se-
lection based on the properties of the target classi-
fication (Joanis et al, 2007). We propose various
avenues for future work.
We introduce our target classification in section 2
and syntactic features in section 3. The classifica-
tion techniques are presented in section 4. Details
of the experimental evaluation are supplied in sec-
tion 5. Section 6 provides discussion and concludes
with directions for future work.
2 Test Verbs and Classes
We adopt as a target classification Levin?s (1993)
well-known taxonomy where verbs taking similar
diathesis alternations are assumed to share meaning
components and are organized into a semantically
coherent class. For instance, the class of ?Break
Verbs? (class 45.1) is partially characterized by its
participation in the following alternations:
1. Causative/inchoative alternation:
Tony broke the window ? The window broke
2. Middle alternation:
Tony broke the window ? The window broke easily
3. Instrument subject alternation:
Tony broke the window with the hammer ? The hammer
broke the window
LEVIN CLASS EXAMPLE VERBS
9.1 PUT bury, place, install, mount, put
10.1 REMOVE remove, abolish, eject, extract, deduct
11.1 SEND ship, post, send, mail, transmit
13.5.1 GET win, gain, earn, buy, get
18.1 HIT beat, slap, bang, knock, pound
22.2 AMALGAMATE contrast, match, overlap, unite, unify
29.2 CHARACTERIZE envisage, portray, regard, treat, enlist
30.3 PEER listen, stare, look, glance, gaze
31.1 AMUSE delight, scare, shock, confuse, upset
36.1 CORRESPOND cooperate, collide, concur, mate, flirt
37.3 MANNER OF shout, yell, moan, mutter, murmur
SPEAKING
37.7 SAY say, reply, mention, state, report
40.2 NONVERBAL smile, laugh, grin, sigh, gas
EXPRESSION
43.1 LIGHT EMISSION shine, flash, flare, glow, blaze
45.4 CHANGE OF STATE soften, weaken, melt, narrow, deepen
47.3 MODES OF BEING quake, falter, sway, swirl, teeter
WITH MOTION
51.3.2 RUN swim, fly, walk, slide, run
Table 1: Test classes and example verbs
Alternations are expressed as pairs of SCFs. Addi-
tional properties related to syntax, morphology and
extended meanings of member verbs are specified
with some classes. The taxonomy provides a classi-
fication of 4,186 verb senses into 48 broad and 192
fine-grained classes according to their participation
in 79 alternations involving NP and PP complements.
We selected 17 fine-grained classes and 12 mem-
ber verbs per class (table 2) for experimentation.
The small test set enabled us to evaluate our results
thoroughly. The classes were selected to (i) include
both syntactically and semantically similar and dif-
ferent classes (to vary the difficulty of the classifi-
cation task), and to (ii) have enough member verbs
whose predominant sense belongs to the class in
question (we verified this according to the method
described in (Korhonen et al, 2006)). As VALEX
was designed to maximise coverage most test verbs
had 1000-9000 occurrences in the lexicon.
3 Syntactic Features
We employed as features distributions of SCFs spe-
cific to given verbs. We extracted them from the re-
cent VALEX (Korhonen et al, 2006) lexicon which
provides SCF frequency information for 6,397 En-
glish verbs. VALEX was acquired automatically
from five large corpora and the Web (using up to
10,000 occurrences per verb) using the subcatego-
rization acquisition system of Briscoe and Carroll
(1997). The system incorporates RASP, a domain-
independent robust statistical parser (Briscoe and
770
Carroll, 2002), and a SCF classifier which iden-
tifies 163 verbal SCFs. The basic SCFs abstract
over lexically-governed particles and prepositions
and predicate selectional preferences.
We used the noisy unfiltered version of VALEX
which includes 33 SCFs per verb on average1. Some
are genuine SCFs but some express adjuncts (e.g.
I sang in the party could be SCF PP). A lexical
entry for each verb and SCF combination provides
e.g. the frequency of the entry (in active and passive)
in corpora, the POS tags of verb tokens, the argument
heads in argument positions, and the prepositions in
PP slots. We experimented with three feature sets:
1. Feature set 1: SCFs and their frequencies
2. Feature set 2: Feature set 1 with two high frequency
PP frames parameterized for prepositions: the simple PP
(e.g. they apologized to him) and NP-PP (e.g. he removed
the shoes from the bag) frames.
3. Feature set 3: Feature set 2 with three additional high
frequency PP frames parameterized for prepositions: the
NP-FOR-NP (e.g. he bought a book for him), NP-TO-NP
(e.g. he gave a kiss to her), and OC-AP, EQUI, AS (e.g. he
condemned him as stupid) frames.
In feature sets 2 and 3, 2-5 PP SCFs were refined ac-
cording to the prepositions provided in the VALEX
SCF entries (e.g. PP at, PP on, PP in) because Levin
specifies prepositions with some SCFs / classes. The
scope was restricted to the 3-5 highest ranked PP
SCFs to reduce the effects of sparse data.
4 Classification
4.1 Preparing the Data
A feature vector was constructed for each verb.
VALEX includes 107, 287 and 305 SCF types for fea-
ture sets 1, 2, and 3, respectively. Each feature corre-
sponds to a SCF type, and its value is the relative fre-
quency of the SCF with the verb in question. Some
of the feature values are zero, because most verbs
take only a subset of the possible SCFs.
4.2 Machine Learning Methods
We implemented three methods for classification:
the K nearest neighbours (KNN), support vector ma-
chines (SVM), and maximum entropy (ME). To our
knowledge, only SVM has been previously used for
1The SCF accuracy of this lexicon is 23.7 F-measure, see
(Korhonen et al, 2006) for details.
verb classification. The free parameters were opti-
mised for each feature set by (i) defining the value
range (as explained below), and by (ii) searching for
the optimal value on the training data using 10 fold
cross validation (section 5.2).
4.2.1 K Nearest Neighbours
KNN is a memory-based classification method
based on the distances between verbs in the feature
space. For each verb in the test data, we measure
its distance to each verb in the training data. The
verb class label is the most frequent label in the
top K closest training verbs. We use the entropy-
based Jensen-Shannon (JS) divergence as the dis-
tance measure:
JS(P,Q) = 12
?D(P?P+Q2 ) +D(Q?P+Q2 )
?
The range of the parameter K is 2-20.
4.2.2 Support Vector Machines
SVM (Vapnik, 1995) tries to find a maximal mar-
gin hyperplane to separate between two groups of
verb feature vectors. In practice, a linear hyperplane
does not always exist. SVM uses a kernel function
to map the original feature vectors to higher dimen-
sion space. The ?maximal margin? optimizes our
choice of dimensionality to avoid over-fitting. We
use Chang and Lin (2001) ?s LIBSVM library to im-
plement the SVM. Following Hsu et al (2003), we
use the radial basis function as the kernel function:
K(xi, xj) = exp (??||xi ? xj ||2), ? > 0
? and the cost of the error term C (the penalty for
margin errors) are optimized. The search ranges of
Hsu et al (2003) are used:
C = 2?5, 2?3, . . . , 215, 217 ; ? = 2?17, 2?15, . . . , 21, 23
4.2.3 Maximum Entropy
ME constructs a probabilistic model that maxi-
mizes entropy on test data subject to a set of feature
constraints. If verb x is in class 10.1 and takes the
SCF 49 (NP-PP) with the relative frequency of 0.6 in
feature function f , we have
f(x, y) = 0.6 if y = 10.1 and x = 49
The expected value of a feature f with respect to the
empirical distribution (training data) is
E?(f) ?Px,y p?(x, y)f(x, y)
The expected value of the feature f (on test data)
with respect to the model p(y|x) is
771
E(f) ?Px,y p?(x)p(y|x)f(x, y)
p?(x) is the empirical distribution of x in the train-
ing data. We constrain E(f) to be the same as E?(f)
E(f) = E?(f)
The model must maximize the entropy H(Y |X)
H(Y |X) ? ?Px,y p?(x)p(y|x) log p(y|x)
The constraint-optimization problem is solved by
the Lagrange multiplier (Pietra et al, 1997). We
used Zhang (2004)?s maximum entropy toolkit for
implementation. The number of iterations i (5-50)
of the parameter estimation algorithm is optimised.
5 Experiments
5.1 Methodology
We split the data into training and test sets using two
methods. The first is ?leave one out? cross-validation
where one verb in each class is held out as test data,
and the remaining N-1 (i.e. 11) verbs are used as
training data. The overall accuracy is the average
accuracy of N rounds. The second method is re-
sampling. For each class, 3 verbs are selected ran-
domly as test data, and 9 are used as training data.
The process is repeated 30 times, and the average
result is recorded.
5.2 Measures
The methods are evaluated using first accuracy ? the
percentage of correct classifications out of all the
classifications:
Accuracy = truePositivestruePositives+falseNegatives
When evaluating the performance at class level, pre-
cision and recall are calculated as follows:
Precision = truePositivestruePositives+falsePositives
Recall = truePositivestruePositives+falseNegatives
F-score is the balance over recall and precision. We
report the average F-score over the 17 classes. Given
there are 17 classes in the data, the accuracy of ran-
domly assigning a verb into one of the 17 classes is
1/17 ? 5.8%.
5.3 Results from Quantitative Evaluation
Table 2 shows the average performance of each clas-
sifier and feature set according to ?leave one out?
cross-validation2. Each classifier performs consid-
erably better than the random baseline. The simple
2Recall is not shown as it is identical here with accuracy.
KNN method produces the lowest accuracy (44.1-
54.9) and SVM and ME the best (47.1-57.9 and 47.5-
59.3, respectively).
The performance of all methods improves sharply
when moving from the feature set 1 to the refined
feature set 2: both accuracy and F-measure improve
by over 10%. When moving from feature set 2 to
the sparser feature set 3 (which includes a higher
number of low frequency PP features) KNN worsens
clearly (c. 5% in accuracy and F-measure) while the
improvement in other methods is very small. This
suggests that KNN deals worse than other methods
with sparse data.
The resampling results in table 3 reveal that some
classifiers perform worse than others when less
training data is available3. KNN produces consid-
erably lower results, particularly with the sparse
feature set 3: 28.2 F-measure vs. 48.2 with cross-
validation. Also SVM performs worse with fea-
ture set 3: 54.6 F-measure vs. 58.2 with cross-
validation. ME thus appears the most robust method
with smaller training data, producing results compa-
rable with those in cross-validation.
Figure 1 shows the F-measure for 17 individual
classes when the methods are used with feature set
3. Levin classes 40.2, 29.2, and 37.3 (see table 2)
(the ones taking fewer prepositions with higher fre-
quency) have the best average performance (65% or
more), and classes 47.3, 45.4 and 18.1 the worst
(40% or less). ME outperforms SVM with 9 of the
17 classes.
5.4 Qualitative Evaluation
We did some qualitative analysis to trace the ori-
gin of error types produced by ME with feature set
3. Examination of the worst performing class 47.3
(MODES OF BEING INVOLVING MOTION verbs) il-
lustrates well the various error types. 10 of the 12
verbs in this class are classified incorrectly:
? 3 in class 43.1 (LIGHT EMISSION verbs): Verbs in 47.3
and 43.1 describe intrinsic properties of their subjects
(e.g. a jewel sparkles, a flag flutters). Their similar al-
ternations and PP SCFs make it difficult to separate them
on syntactic grounds.
? 2 in class 51.3.2 (RUN verbs): 47.3 and 51.3.2 share the
meaning component of motion. Their members take sim-
ilar alternations and SCFs, which causes the confusion.
3Recall that the amount of training data is smaller with re-
sampling evaluation, see section 5.2.
772
Feature set 1 Feature set 2 Feature set 3
ACC P F ACC P F ACC P F
RAND 5.8 5.8 5.8
KNN 44.1 48.4 44.0 54.9 56.9 53.9 49.5 47.0 48.2
ME 47.5 49.4 47.6 59.3 61.4 59.9 59.3 61.9 60.0
SVM 47.1 50.4 47.8 57.8 59.4 57.9 57.8 60.1 58.2
Table 2: ?Leave one out? cross-validation results for KNN, ME, and SVM
Feature set 1 Feature set 2 Feature set 3
ACC P F ACC P F ACC P F
RAND 5.8 5.8 5.8
KNN 37.3 39.9 36.5 42.7 47.2 42.6 27.1 34.2 28.2
ME 47.1 47.3 47.0 58.1 59.1 58.1 60.1 60.5 59.8
SVM 47.3 50.2 47.7 56.8 59.5 57.1 54.4 56.5 54.6
Table 3: Re-sampling results for KNN, ME, and SVM
? 2 in class 37.7 (SAY verbs) and 1 in class 37.3 (MANNER
OF SPEAKING verbs): 47.3 differs in semantics and syn-
tax from 37.7 and 37.3. The confusion is due to idiosyn-
cratic properties of individual verbs (e.g. quake, wiggle).
? 1 in class 36.1 (CORRESPOND verbs): 47.3 and 36.1 are
semantically very different, but their members take simi-
lar intransitive and PP SCFs with high frequency.
? 1 in class 45.4 (OTHER CHANGE OF STATE verbs):
Classes 47.3 and 45.3 are semantically different. Their
similar PP SCFs explains the misclassification.
Most errors concern classes which are in fact se-
mantically related. Unfortunately there is no gold
standard which would comprehensively capture the
semantic relatedness of Levin classes. Other er-
rors concern semantically unrelated but syntactically
similar classes ? cases which we may be able to ad-
dress in the future with careful feature engineering.
Some errors relate to syntactic idiosyncracy. These
show the true limits of lexical classification - the fact
that the correspondence between the syntax and se-
mantics of verbs is not always perfect.
6 Discussion and Conclusion
Our best results (e.g. 60.1 accuracy and 59.8 F-
measure of ME) are good, considering that no so-
phisticated feature engineering / selection based on
the properties of the target classification was per-
formed in these experiments. The closest compari-
son point is the recent experiment reported by Joanis
et al (2007) which involved classifying 835 English
verbs to 14 Levin classes using SVM. Features were
specifically selected via analysis of alternations that
are used to characterize Levin classes. Both shal-
low syntactic features (syntactic slots obtained us-
ing a chunker) and deep ones (SCFs extracted using
Briscoe and Carroll?s system) were used. The accu-
racy was 58% with the former and only 38% with
the latter. This experiment is not directly compa-
rable with ours as we classified a smaller number
of verbs (204) to a higher number of Levin classes
(17) (i.e. we had less training data) and did not se-
lect the optimal set of features using Levin?s alter-
nations. We nevertheless obtained better accuracy
with our best performing method, and better accu-
racy (47%) with the same method (SVM) when the
comparable feature set 1 was acquired using the very
same subcategorization acquisition system.
It is likely that using larger and noisier SCF data
explains the better result, suggesting that rich syn-
tactic features incorporating information about both
arguments and adjuncts are ideal for verb classifica-
tion. Further experiments are required to determine
the optimal set of features. In the future, we plan
to experiment with different (noisy and filtered) ver-
sions of VALEX and add to the comparison a shal-
lower set of features (e.g. NP and PP slots in VALEX
regardless of the specific SCFs). We will also im-
prove the features e.g. by enriching them with addi-
tional syntactic information available in VALEX lex-
ical entries.
Acknowledgement
This work was partially supported by the Royal So-
ciety, UK.
773
Figure 1: Class level F-score for feature set 3 (cross-validation)
References
E. J. Briscoe and J. Carroll. 1997. Automatic extraction
of subcategorization from corpora. In Proceedings of
the 5th ACL Conference on Applied Natural Language
Processing, pages 356?363, Washington DC.
E. J. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd LREC, pages 1499?1504, Las Palmas, Gran Ca-
naria.
C. Chang and J. Lin, 2001. LIBSVM: a library for sup-
port vector machines.
H. T. Dang. 2004. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. Ph.D.
thesis, CIS, University of Pennsylvania.
B. J. Dorr. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?322.
W. Hsu, C. Chang, and J. Lin. 2003. A practical guide to
support vector classification.
R. Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, Massachusetts.
E. Joanis, S. Stevenson, and D. James. 2007. A general
feature space for automatic verb classification. Natu-
ral Language Engineering, Forthcoming.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame distri-
butions semantically. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 64?71.
A. Korhonen, Y. Krymolowski, and T. Briscoe. 2006.
A large subcategorization lexicon for natural language
processing applications. In Proceedings of LREC.
B. Levin. 1993. English Verb Classes and Alternations.
Chicago University Press, Chicago.
P. Merlo and S. Stevenson. 2001. Automatic verb clas-
sification based on statistical distributions of argument
structure. Computational Linguistics, 27(3):373?408.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235?312.
S. D. Pietra, J. D. Pietra, and J. D. Lafferty. 1997. Induc-
ing features of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(4):380?
393.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using a
probabilistic class-based lexicon for lexical ambiguity
resolution. In 18th International Conference on Com-
putational Linguistics, pages 649?655, Saarbru?cken,
Germany.
S. Schulte im Walde. 2000. Clustering verbs semanti-
cally according to their alternation behaviour. In Pro-
ceedings of COLING, pages 747?753, Saarbru?cken,
Germany.
S. Schulte im Walde. 2006. Experiments on the au-
tomatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159?194.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proceedings of the Sixth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, Mexico City, Mexico.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labelling. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 95?102, Barcelona, Spain, August.
V. N. Vapnik. 1995. The nature of statistical learning
theory. Springer-Verlag New York, Inc., New York,
NY, USA.
L. Zhang, 2004. Maximum Entropy Modeling Toolkit for
Python and C++, December.
774
Proceedings of NAACL HLT 2009: Tutorials, pages 13?14,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
VerbNet overview, extensions, mappings and applications 
(Karin Kipper Schuler, Anna Korhonen, Susan Brown) 
 
Abstract: 
The goal of this tutorial is to introduce and discuss VerbNet, a broad coverage verb lexicon 
freely available on-line. VerbNet contains explicit syntactic and semantic information for classes 
of verbs and has mappings to several other widely-used lexical resources, including WordNet, 
PropBank, and FrameNet.  Since its first release in 2005 VerbNet is being used by a large 
number of researchers as a means of characterizing verbs and verb classes.  
The first part of the tutorial will include an overview of the original Levin verb classification; 
introduce the main VerbNet components, such as thematic roles and syntactic and semantic 
representations, and present a comparison with other available lexical resources.  
During the second part of the tutorial, we will explore VerbNet extensions (how new classes 
were derived and created through manual and semi-automatic processes), and we will present 
on-going work on automatic acquisition of Levin-style classes in corpora. The latter is useful for 
domain-adaptation and tuning of VerbNet for real-world applications which require this.  
The last part of the tutorial will be devoted to discussing the current status of VerbNet; 
including recent work mapping to other lexical resources, such as PropBank, FrameNet, 
WordNet, OntoNotes sense groupings, and the Omega ontology.  We will also present changes 
designed to regularize the syntactic frames and to make the naming conventions more 
transparent and user friendly.  Finally, we will describe some applications in which VerbNet has 
been used. 
Tutorial Outline:  
VerbNet overview: 
- Original Levin classes 
- VerbNet components (roles, syntactic and semantic descriptions) 
- Related work in lexical resources 
VerbNet extensions: 
- Manual and semi-automatic extension of VerbNet with new classes  
- On-going work on automatic acquisition of Levin-style classes in corpora 
13
VerbNet mappings and applications: 
- Mappings to other resources (PropBank, FrameNet, WordNet, OntoNotes sense 
groupings, Omega ontology) 
- Current status of VerbNet 
- Ongoing improvements 
- VerbNet applications 
Short biographical description of the presenters: 
Karin Kipper Schuler  
University of Colorado 
kipper@verbs.colorado.edu 
 
Karin Kipper Schuler received her PhD from the Computer Science Department of the 
University of Pennsylvania. Her primary research is aimed at the development and 
evaluation of large-scale computational lexical resources. During the past couple of 
years she worked for the Mayo Clinic where she was involved in applications related to 
bio/medical informatics including automatic extraction of named entities and relations 
from clinical data and development of knowledge models to guide this data extraction.  
 
Anna Korhonen 
University of Cambridge 
alk23@cam.ac.uk 
 
Anna Korhonen received her PhD from the Computer Laboratory of the University of 
Cambridge in the UK. Her research focuses mainly on automatic acquisition of lexical 
information from texts. She has developed techniques and tools for automatic lexical 
acquisition for English and other languages, and has used them to acquire large lexical 
resources , extend manually built resources, and help NLP application tasks (e.g. parsing, 
word sense disambiguation, information extraction) . She has applied the techniques to 
different domains (e.g. biomedical) and has also used them to advance on research in 
related fields (e.g. cognitive sciences).  
 
Susan Brown 
University of Colorado 
susan.brown@colorado.edu 
 
Susan Brown is currently a PhD student in Linguistics and Cognitive Science at the 
University of Colorado under Dr. Martha Palmer?s supervision. Susan?s research focuses 
on lexical ambiguity, especially as it pertains to natural language processing tasks.  Her 
research methodologies include psycholinguistic experimentation, corpus study, and 
annotation analysis.  She has also been involved in the development of large-scale 
lexical resources and the design and construction of a lexically based ontology. 
14
 
	Clustering Polysemic Subcategorization Frame Distributions Semantically
Anna Korhonen?
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
alk23@cl.cam.ac.uk
Yuval Krymolowski
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland, UK
ykrymolo@inf.ed.ac.uk
Zvika Marx
Interdisciplinary Center
for Neural Computation,
The Hebrew University
Jerusalem, Israel
zvim@cs.huji.ac.il
Abstract
Previous research has demonstrated the
utility of clustering in inducing semantic
verb classes from undisambiguated cor-
pus data. We describe a new approach
which involves clustering subcategoriza-
tion frame (SCF) distributions using the
Information Bottleneck and nearest neigh-
bour methods. In contrast to previous
work, we particularly focus on cluster-
ing polysemic verbs. A novel evaluation
scheme is proposed which accounts for
the effect of polysemy on the clusters, of-
fering us a good insight into the potential
and limitations of semantically classifying
undisambiguated SCF data.
1 Introduction
Classifications which aim to capture the close rela-
tion between the syntax and semantics of verbs have
attracted a considerable research interest in both lin-
guistics and computational linguistics (e.g. (Jack-
endoff, 1990; Levin, 1993; Pinker, 1989; Dang et
al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).
While such classifications may not provide a means
for full semantic inferencing, they can capture gen-
eralizations over a range of linguistic properties, and
can therefore be used as a means of reducing redun-
dancy in the lexicon and for filling gaps in lexical
knowledge.
?This work was partly supported by UK EPSRC project
GR/N36462/93: ?Robust Accurate Statistical Parsing (RASP)?.
Verb classifications have, in fact, been used to
support many natural language processing (NLP)
tasks, such as language generation, machine transla-
tion (Dorr, 1997), document classification (Klavans
and Kan, 1998), word sense disambiguation (Dorr
and Jones, 1996) and subcategorization acquisition
(Korhonen, 2002).
One attractive property of these classifications is
that they make it possible, to a certain extent, to in-
fer the semantics of a verb on the basis of its syn-
tactic behaviour. In recent years several attempts
have been made to automatically induce semantic
verb classes from (mainly) syntactic information
in corpus data (Joanis, 2002; Merlo et al, 2002;
Schulte im Walde and Brew, 2002).
In this paper, we focus on the particular task
of classifying subcategorization frame (SCF) distri-
butions in a semantically motivated manner. Pre-
vious research has demonstrated that clustering
can be useful in inferring Levin-style semantic
classes (Levin, 1993) from both English and Ger-
man verb subcategorization information (Brew and
Schulte im Walde, 2002; Schulte im Walde, 2000;
Schulte im Walde and Brew, 2002).
We propose a novel approach, which involves: (i)
obtaining SCF frequency information from a lexi-
con extracted automatically using the comprehen-
sive system of Briscoe and Carroll (1997) and (ii)
applying a clustering mechanism to this informa-
tion. We use clustering methods that process raw
distributional data directly, avoiding complex pre-
processing steps required by many advanced meth-
ods (e.g. Brew and Schulte im Walde (2002)).
In contrast to earlier work, we give special empha-
sis to polysemy. Earlier work has largely ignored
this issue by assuming a single gold standard class
for each verb (whether polysemic or not). The rel-
atively good clustering results obtained suggest that
many polysemic verbs do have some predominating
sense in corpus data. However, this sense can vary
across corpora (Roland et al, 2000), and assuming a
single sense is inadequate for an important group of
medium and high frequency verbs whose distribu-
tion of senses in balanced corpus data is flat rather
than zipfian (Preiss and Korhonen, 2002).
To allow for sense variation, we introduce a new
evaluation scheme against a polysemic gold stan-
dard. This helps to explain the results and offers
a better insight into the potential and limitations of
clustering undisambiguated SCF data semantically.
We discuss our gold standards and the choice of
test verbs in section 2. Section 3 describes the
method for subcategorization acquisition and sec-
tion 4 presents the approach to clustering. Details
of the experimental evaluation are supplied in sec-
tion 5. Section 6 concludes with directions for future
work.
2 Semantic Verb Classes and Test Verbs
Levin?s taxonomy of verbs and their classes (Levin,
1993) is the largest syntactic-semantic verb classifi-
cation in English, employed widely in evaluation of
automatic classifications. It provides a classification
of 3,024 verbs (4,186 senses) into 48 broad / 192
fine grained classes. Although it is quite extensive,
it is not exhaustive. As it primarily concentrates on
verbs taking NP and PP complements and does not
provide a comprehensive set of senses for verbs, it
is not suitable for evaluation of polysemic classifi-
cations.
We employed as a gold standard a substan-
tially extended version of Levin?s classification
constructed by Korhonen (2003). This incorpo-
rates Levin?s classes, 26 additional classes by
Dorr (1997)1, and 57 new classes for verb types not
covered comprehensively by Levin or Dorr.
110 test verbs were chosen from this gold stan-
dard, 78 polysemic and 32 monosemous ones. Some
low frequency verbs were included to investigate the
1These classes are incorporated in the ?LCS database?
(http://www.umiacs.umd.edu/?bonnie/verbs-English.lcs).
effect of sparse data on clustering performance. To
ensure that our gold standard covers all (or most)
senses of these verbs, we looked into WordNet
(Miller, 1990) and assigned all the WordNet senses
of the verbs to gold standard classes.2
Two versions of the gold standard were created:
monosemous and polysemic. The monosemous one
lists only a single sense for each test verb, that cor-
responding to its predominant (most frequent) sense
in WordNet. The polysemic one provides a compre-
hensive list of senses for each verb. The test verbs
and their classes are shown in table 1. The classes
are indicated by number codes from the classifica-
tions of Levin, Dorr (the classes starting with 0) and
Korhonen (the classes starting with A).3 The pre-
dominant sense is indicated by bold font.
3 Subcategorization Information
We obtain our SCF data using the subcategorization
acquisition system of Briscoe and Carroll (1997).
We expect the use of this system to be benefi-
cial: it employs a robust statistical parser (Briscoe
and Carroll, 2002) which yields complete though
shallow parses, and a comprehensive SCF classifier,
which incorporates 163 SCF distinctions, a super-
set of those found in the ANLT (Boguraev et al,
1987) and COMLEX (Grishman et al, 1994) dictio-
naries. The SCFs abstract over specific lexically-
governed particles and prepositions and specific
predicate selectional preferences but include some
derived semi-predictable bounded dependency con-
structions, such as particle and dative movement.
78 of these ?coarse-grained? SCFs appeared in our
data. In addition, a set of 160 fine grained frames
were employed. These were obtained by parameter-
izing two high frequency SCFs for prepositions: the
simple PP and NP + PP frames. The scope was re-
stricted to these two frames to prevent sparse data
problems in clustering.
A SCF lexicon was acquired using this system
from the British National Corpus (Leech, 1992,
BNC) so that the maximum of 7000 citations were
2As WordNet incorporates particularly fine grained sense
distinctions, some senses were found which did not appear in
our gold standard. As many of them appeared marginal and/or
low in frequency, we did not consider these additional senses in
our experiment.
3The gold standard assumes Levin?s broad classes (e.g. class
10) instead of possible fine-grained ones (e.g. class 10.1).
TEST GOLD STANDARD TEST GOLD STANDARD TEST GOLD STANDARD TEST GOLD STANDARD
VERB CLASSES VERB CLASSES VERB CLASSES VERB CLASSES
place 9 dye 24, 21, 41 focus 31, 45 stare 30
lay 9 build 26, 45 force 002, 11 glow 43
drop 9, 45, 004, 47, bake 26, 45 persuade 002 sparkle 43
51, A54, A30
pour 9, 43, 26, 57, 13, 31 invent 26, 27 urge 002, 37 dry 45
load 9 publish 26, 25 want 002, 005, 29, 32 shut 45
settle 9, 46, A16, 36, 55 cause 27, 002 need 002, 005, 29, 32 hang 47, 9, 42, 40
fill 9, 45, 47 generate 27, 13, 26 grasp 30, 15 sit 47, 9
remove 10, 11, 42 induce 27, 002, 26 understand 30 disappear 48
withdraw 10, A30 acknowledge 29, A25, A35 conceive 30, 29, A56 vanish 48
wipe 10, 9 proclaim 29, 37, A25 consider 30, 29 march 51
brush 10, 9, 41, 18 remember 29, 30 perceive 30 walk 51
filter 10 imagine 29, 30 analyse 34, 35 travel 51
send 11, A55 specify 29 evaluate 34, 35 hurry 53, 51
ship 11, A58 establish 29, A56 explore 35, 34 rush 53, 51
transport 11, 31 suppose 29, 37 investigate 35, 34 begin 55
carry 11, 54 assume 29, A35, A57 agree 36, 22, A42 continue 55, 47, 51
drag 11, 35, 51, 002 think 29, 005 communicate 36, 11 snow 57, 002
push 11, 12, 23, 9, 002 confirm 29 shout 37 rain 57
pull 11, 12, 13, 23, 40, 016 believe 29, 31, 33 whisper 37 sin 003
give 13 admit 29, 024, 045, 37 talk 37 rebel 003
lend 13 allow 29, 024, 13, 002 speak 37 risk 008, A7
study 14, 30, 34, 35 act 29 say 37, 002 gamble 008, 009
hit 18, 17, 47, A56, 31, 42 behave 29 mention 37 beg 015, 32
bang 18, 43, 9, 47, 36 feel 30, 31, 35, 29 eat 39 pray 015, 32
carve 21, 25, 26 see 30, 29 drink 39 seem 020
add 22, 37, A56 hear 30, A32 laugh 40, 37 appear 020, 48, 29
mix 22, 26, 36 notice 30, A32 smile 40, 37
colour 24, 31, 45 concentrate 31, 45 look 30, 35
Table 1: Test verbs and their monosemous/polysemic gold standard senses
used per test verb. The lexicon was evaluated against
manually analysed corpus data after an empirically
defined threshold of 0.025 was set on relative fre-
quencies of SCFs to remove noisy SCFs. The method
yielded 71.8% precision and 34.5% recall. When we
removed the filtering threshold, and evaluated the
noisy distribution, F-measure4 dropped from 44.9 to
38.51.5
4 Clustering Method
Data clustering is a process which aims to partition a
given set into subsets (clusters) of elements that are
similar to one another, while ensuring that elements
that are not similar are assigned to different clusters.
We use clustering for partitioning a set of verbs. Our
hypothesis is that information about SCFs and their
associated frequencies is relevant for identifying se-
mantically related verbs. Hence, we use SCFs as rel-
evance features to guide the clustering process.6
4F = 2?precision?recallprecision+recall
5These figures are not particularly impressive because our
evaluation is exceptionally hard. We use 1) highly polysemic
test verbs, 2) a high number of SCFs and 3) evaluate against
manually analysed data rather than dictionaries (the latter have
high precision but low recall).
6The relevance of the features to the task is evident when
comparing the probability of a randomly chosen pair of verbs
verbi and verbj to share the same predominant sense (4.5%)
with the probability obtained when verbj is the JS-divergence
We chose two clustering methods which do not in-
volve task-oriented tuning (such as pre-fixed thresh-
olds or restricted cluster sizes) and which approach
data straightforwardly, in its distributional form: (i)
a simple hard method that collects the nearest neigh-
bours (NN) of each verb (figure 1), and (ii) the In-
formation Bottleneck (IB), an iterative soft method
(Tishby et al, 1999) based on information-theoretic
grounds.
The NN method is very simple, but it has some
disadvantages. It outputs only one clustering config-
uration, and therefore does not allow examination
of different cluster granularities. It is also highly
sensitive to noise. Few exceptional neighbourhood
relations contradicting the typical trends in the data
are enough to cause the formation of a single cluster
which encompasses all elements.
Therefore we employed the more sophisticated
IB method as well. The IB quantifies the rele-
vance information of a SCF distribution with re-
spect to output clusters, through their mutual infor-
mation I(Clusters; SCFs). The relevance informa-
tion is maximized, while the compression informa-
tion I(Clusters;V erbs) is minimized. This en-
sures optimal compression of data through clusters.
The tradeoff between the two constraints is realized
nearest neighbour of verbi (36%).
NN Clustering:
1. For each verb v:
2. Calculate the JS divergence between the SCF
distributions of v and all other verbs:
JS(p, q) = 12
[
D
(
p
?
?
?
p+q
2
)
+ D
(
q
?
?
?
p+q
2
)]
3. Connect v with the most similar verb;
4. Find all the connected components
Figure 1: Connected components nearest neighbour (NN)
clustering. D is the Kullback-Leibler distance.
through minimizing the cost term:
L = I(Clusters;V erbs)? ?I(Clusters; SCFs) ,
where ? is a parameter that balances the constraints.
The IB iterative algorithm finds a local minimum
of the above cost term. It takes three inputs: (i) SCF-
verb distributions, (ii) the desired number of clusters
K, and (iii) the value of ?.
Starting from a random configuration, the algo-
rithm repeatedly calculates, for each cluster K, verb
V and SCF S, the following probabilities: (i) the
marginal proportion of the cluster p(K); (ii) the
probability p(S|K) for a SCF to occur with mem-
bers of the cluster; and (iii) the probability p(K|V )
for a verb to be assigned to the cluster. These prob-
abilities are used, each in its turn, for calculating the
other probabilities (figure 2). The collection of all
p(S|K)?s for a fixed cluster K can be regarded as a
probabilistic center (centroid) of that cluster in the
SCF space.
The IB method gives an indication of the
most informative values of K.7 Intensifying the
weight ? attached to the relevance information
I(Clusters; SCFs) allows us to increase the num-
ber K of distinct clusters being produced (while too
small ? would cause some of the output clusters to
be identical to one another). Hence, the relevance in-
formation grows with K. Accordingly, we consider
as the most informative output configurations those
for which the relevance information increases more
sharply between K? 1 and K clusters than between
K and K + 1.
7Most works on clustering ignore this issue and refer to an
arbitrarily chosen number of clusters, or to the number of gold
standard classes, which cannot be assumed in realistic applica-
tions.
IB Clustering (fixed ?):
Perform till convergence, for each time step
t = 1, 2, . . . :
1. zt(K,V ) = pt?1(K) e??D[p(S|V )?pt?1(S|K)]
(When t = 1, initialize zt(K,V ) arbitrarily)
2. pt(K|V ) = zt(K,V )?
K? zt(K
?,V )
3. pt(K) =
?
V p(V )pt(K|V )
4. pt(S|K) =
?
V p(S|V )pt(V |K)
Figure 2: Information Bottleneck (IB) iterative clustering. D
is the Kullback-Leibler distance.
When the weight of relevance grows, the assign-
ment to clusters is more constrained and p(K|V ) be-
comes more similar to hard clustering. Let
K(V ) = argmax
K
p(K|V )
denote the most probable cluster of a verb V .
For K ? 30, more than 85% of the verbs have
p(K(V )|V ) > 90% which makes the output cluster-
ing approximately hard. For this reason, we decided
to use only K(V ) as output and defer a further ex-
ploration of the soft output to future work.
5 Experimental Evaluation
5.1 Data
The input data to clustering was obtained from the
automatically acquired SCF lexicon for our 110 test
verbs (section 2). The counts were extracted from
unfiltered (noisy) SCF distributions in this lexicon.8
The NN algorithm produced 24 clusters on this in-
put. From the IB algorithm, we requested K = 2
to 60 clusters. The upper limit was chosen so as
to slightly exceed the case when the average clus-
ter size 110/K = 2. We chose for evaluation the
IB results for K = 25, 35 and 42. For these val-
ues, the SCF relevance satisfies our criterion for a
notable improvement in cluster quality (section 4).
The value K=35 is very close to the actual number
(34) of predominant senses in the gold standard. In
this way, the IB yields structural information beyond
clustering.
8This yielded better results, which might indicate that the
unfiltered ?noisy? SCFs contain information which is valuable
for the task.
5.2 Method
A number of different strategies have been proposed
for evaluation of clustering. We concentrate here on
those which deliver a numerical value which is easy
to interpret, and do not introduce biases towards spe-
cific numbers of classes or class sizes. As we cur-
rently assign a single sense to each polysemic verb
(sec. 5.4) the measures we use are also applicable
for evaluation against a polysemous gold standard.
Our first measure, the adjusted pairwise preci-
sion (APP), evaluates clusters in terms of verb pairs
(Schulte im Walde and Brew, 2002) 9:
APP = 1K
K?
i=1
num. of correct pairs in ki
num. of pairs in ki ?
|ki|?1
|ki|+1
.
APP is the average proportion of all within-cluster
pairs that are correctly co-assigned. It is multiplied
by a factor that increases with cluster size. This fac-
tor compensates for a bias towards small clusters.
Our second measure is derived from purity, a
global measure which evaluates the mean precision
of the clusters, weighted according to the cluster size
(Stevenson and Joanis, 2003). We associate with
each cluster its most prevalent semantic class, and
denote the number of verbs in a cluster K that take
its prevalent class by nprevalent(K). Verbs that do
not take this class are considered as errors. Given
our task, we are only interested in classes which con-
tain two or more verbs. We therefore disregard those
clusters where nprevalent(K) = 1. This leads us to
define modified purity:
mPUR =
?
nprevalent(ki)?2
nprevalent(ki)
number of verbs .
The modification we introduce to purity removes the
bias towards the trivial configuration comprised of
only singletons.
5.3 Evaluation Against the Predominant Sense
We first evaluated the clusters against the predom-
inant sense, i.e. using the monosemous gold stan-
dard. The results, shown in Table 2, demonstrate
that both clustering methods perform significantly
9Our definition differs by a factor of 2 from that of
Schulte im Walde and Brew (2002).
Alg. K +PP ?PP +PP ?PP
APP: mPUR:
NN (24) 21% 19% 48% 45%
25 12% 9% 39% 32%
IB 35 14% 9% 48% 38%
42 15% 9% 50% 39%
RAND 25 3% 15%
Table 2: Clustering performance on the predominant senses,
with and without prepositions. The last entry presents the per-
formance of random clustering with K = 25, which yielded the
best results among the three values K=25, 35 and 42.
better on the task than our random clustering base-
line. Both methods show clearly better performance
with fine-grained SCFs (with prepositions, +PP) than
with coarse-grained ones (-PP).
Surprisingly, the simple NN method performs
very similarly to the more sophisticated IB. Being
based on pairwise similarities, it shows better per-
formance than IB on the pairwise measure. The IB
is, however, slightly better according to the global
measure (2% with K = 42). The fact that the NN
method performs better than the IB with similar K
values (NN K = 24 vs. IB K = 25) seems to suggest
that the JS divergence provides a better model for
the predominant class than the compression model
of the IB. However, it is likely that the IB perfor-
mance suffered due to our choice of test data. As the
method is global, it performs better when the target
classes are represented by a high number of verbs.
In our experiment, many semantic classes were rep-
resented by two verbs only (section 2).
Nevertheless, the IB method has the clear advan-
tage that it allows for more clusters to be produced.
At best it classified half of the verbs correctly ac-
cording to their predominant sense (mPUR = 50%).
Although this leaves room for improvement, the re-
sult compares favourably to previously published re-
sults10. We argue, however, that evaluation against a
monosemous gold standard reveals only part of the
picture.
10Due to differences in task definition and experimental
setup, a direct comparison with earlier results is impossible.
For example, Stevenson and Joanis (2003) report an accuracy
of 29% (which implies mPUR ? 29%), but their task involves
classifying 841 verbs to 14 classes based on differences in the
predicate-argument structure.
K Pred. Multiple Pred. Multiple
sense senses sense senses
APP: mPUR:
NN:
(24) 21% 29% (23% + 5?) 48% 60% (46%+ 2?)
IB:
25 12% 18% (14% + 5?) 39% 48% (43%+ 3?)
35 14% 20% (16% + 6?) 47% 59% (50%+ 4?)
42 15% 19% (16% + 3?) 50% 59% (54%+ 2?)
Table 3: Evaluation against the monosemous (Pred.) and pol-
ysemous (Multiple) gold standards. The figures in parentheses
are results of evaluation on randomly polysemous data + sig-
nificance of the actual figure. Results were obtained with fine-
grained SCFs (including prepositions).
5.4 Evaluation Against Multiple Senses
In evaluation against the polysemic gold standard,
we assume that a verb which is polysemous in our
corpus data may appear in a cluster with verbs that
share any of its senses. In order to evaluate the clus-
ters against polysemous data, we assigned each pol-
ysemic verb V a single sense: the one it shares with
the highest number of verbs in the cluster K(V ).
Table 3 shows the results against polysemic and
monosemous gold standards. The former are notice-
ably better than the latter (e.g. IB with K = 42 is 9%
better). Clearly, allowing for multiple gold standard
classes makes it easier to obtain better results with
evaluation.
In order to show that polysemy makes a non-
trivial contribution in shaping the clusters, we mea-
sured the improvement that can be due to pure
chance by creating randomly polysemous gold stan-
dards. We constructed 100 sets of random gold stan-
dards. In each iteration, the verbs kept their original
predominant senses, but the set of additional senses
was taken entirely from another verb - chosen at ran-
dom. By doing so, we preserved the dominant sense
of each verb, the total frequency of all senses and the
correlations between the additional senses.
The results included in table 3 indicate, with
99.5% confidence (3? and above), that the improve-
ment obtained with the polysemous gold standard is
not artificial (except in two cases with 95% confi-
dence).
5.5 Qualitative Analysis of Polysemy
We performed qualitative analysis to further inves-
tigate the effect of polysemy on clustering perfor-
Different Pairs Fraction
Senses in cluster
0 39 51%
1 85 10%
2 625 7%
3 1284 3%
4 1437 3%
Table 4: The fraction of verb pairs clustered together, as a
function of the number of different senses between pair mem-
bers (results of the NN algorithm)
Common one irregular no irregular
Senses Pairs in cluster Pairs in cluster
0 2180 3% 3018 3%
1 388 9% 331 12%
2 44 20% 31 35%
Table 5: The fraction of verb pairs clustered together, as a
function of the number of shared senses (results of the NN algo-
rithm)
mance. The results in table 4 demonstrate that the
more two verbs differ in their senses, the lower their
chance of ending up in the same cluster. From the
figures in table 5 we see that the probability of two
verbs to appear in the same cluster increases with
the number of senses they share. Interestingly, it is
not only the degree of polysemy which influences
the results, but also the type. For verb pairs where at
least one of the members displays ?irregular? poly-
semy (i.e. it does not share its full set of senses with
any other verb), the probability of co-occurrence in
the same cluster is far lower than for verbs which are
polysemic in a ?regular? manner (Table 5).
Manual cluster analysis against the polysemic
gold standard revealed a yet more comprehensive
picture. Consider the following clusters (the IB out-
put with K = 42):
A1: talk (37), speak (37)
A2: look (30, 35), stare (30)
A3: focus (31, 45), concentrate (31, 45)
A4: add (22, 37, A56)
We identified a close relation between the clus-
tering performance and the following patterns of se-
mantic behaviour:
1) Monosemy: We had 32 monosemous test
verbs. 10 gold standard classes included 2 or more
or these. 7 classes were correctly acquired us-
ing clustering (e.g. A1), indicating that clustering
monosemous verbs is fairly ?easy?.
2) Predominant sense: 10 clusters were exam-
ined by hand whose members got correctly classi-
fied together, despite one of them being polysemous
(e.g. A2). In 8 cases there was a clear indication in
the data (when examining SCFs and the selectional
preferences on argument heads) that the polysemous
verb indeed had its predominant sense in the rele-
vant class and that the co-occurrence was not due to
noise.
3) Regular Polysemy: Several clusters were pro-
duced which represent linguistically plausible inter-
sective classes (e.g. A3) (Dang et al, 1998) rather
than single classes.
4) Irregular Polysemy: Verbs with irregular pol-
ysemy11 were frequently assigned to singleton clus-
ters. For example, add (A4) has a ?combining and
attaching? sense in class 22 which involves NP and
PP SCFs and another ?communication? sense in 37
which takes sentential SCFs. Irregular polysemy was
not a marginal phenomenon: it explains 5 of the 10
singletons in our data.
These observations confirm that evaluation
against a polysemic gold standard is necessary in
order to fully explain the results from clustering.
5.6 Qualitative Analysis of Errors
Finally, to provide feedback for further development
of our verb classification approach, we performed a
qualitative analysis of errors not resulting from poly-
semy. Consider the following clusters (the IB output
for K = 42):
B1: place (9), build (26, 45),
publish (26, 25), carve (21, 25, 26)
B2: sin (003), rain (57), snow (57, 002)
B3: agree (36, 22, A42), appear (020, 48, 29),
begin (55), continue (55, 47, 51)
B4: beg (015, 32)
Three main error types were identified:
1) Syntactic idiosyncracy: This was the most fre-
quent error type, exemplified in B1, where place is
incorrectly clustered with build, publish and carve
merely because it takes similar prepositions to these
verbs (e.g. in, on, into).
2) Sparse data: Many of the low frequency verbs
(we had 12 with frequency less than 300) performed
11Recall our definition of irregular polysemy, section 5.4.
poorly. In B2, sin (which had 53 occurrences) is
classified with rain and snow because it does not
occur in our data with the preposition against -
the ?hallmark? of its gold standard class (?Conspire
Verbs?).
3) Problems in SCF acquisition: These were not
numerous but occurred e.g. when the system could
not distinguish between different control (e.g. sub-
ject/object equi/raising) constructions (B3).
6 Discussion and Conclusions
This paper has presented a novel approach to auto-
matic semantic classification of verbs. This involved
applying the NN and IB methods to cluster polysemic
SCF distributions extracted from corpus data using
Briscoe and Carroll?s (1997) system. A principled
evaluation scheme was introduced which enabled us
to investigate the effect of polysemy on the resulting
classification.
Our investigation revealed that polysemy has a
considerable impact on the clusters formed: pol-
ysemic verbs with a clear predominant sense and
those with similar regular polysemy are frequently
classified together. Homonymic verbs or verbs with
strong irregular polysemy tend to resist any classifi-
cation.
While it is clear that evaluation should account
for these cases rather than ignore them, the issue of
polysemy is related to another, bigger issue: the po-
tential and limitations of clustering in inducing se-
mantic information from polysemic SCF data. Our
results show that it is unrealistic to expect that the
?important? (high frequency) verbs in language fall
into classes corresponding to single senses. How-
ever, they also suggest that clustering can be used
for novel, previously unexplored purposes: to de-
tect from corpus data general patterns of seman-
tic behaviour (monosemy, predominant sense, reg-
ular/irregular polysemy).
In the future, we plan to investigate the use of soft
clustering (without hardening the output) and de-
velop methods for evaluating the soft output against
polysemous gold standards. We also plan to work
on improving the accuracy of subcategorization ac-
quisition, investigating the role of noise (irregular /
regular) in clustering, examining whether different
syntactic/semantic verb types require different ap-
proaches in clustering, developing our gold standard
classification further, and extending our experiments
to a larger number of verbs and verb classes.
References
B. Boguraev, E. J. Briscoe, J. Carroll, D. Carter, and
C. Grover. 1987. The derivation of a grammatically-
indexed lexicon from the longman dictionary of con-
temporary english. In Proc. of the 25th ACL, pages
193?200, Stanford, CA.
C. Brew and S. Schulte im Walde. 2002. Spectral clus-
tering for german verbs. In Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, USA.
E. J. Briscoe and J. Carroll. 1997. Automatic extraction
of subcategorization from corpora. In 5th ACL Confer-
ence on Applied Natural Language Processing, pages
356?363, Washington DC.
E. J. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In 3rd International
Conference on Language Resources and Evaluation,
pages 1499?1504, Las Palmas, Gran Canaria.
H. T. Dang, K. Kipper, M. Palmer, and J. Rosenzweig.
1998. Investigating regular sense extensions based on
intersective Levin classes. In Proc. of COLING/ACL,
pages 293?299, Montreal, Canada.
B. Dorr and D. Jones. 1996. Role of word sense disam-
biguation in lexical acquisition: predicting semantics
from syntactic cues. In 16th International Conference
on Computational Linguistics, pages 322?333, Copen-
hagen, Denmark.
B. Dorr. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?325.
R. Grishman, C. Macleod, and A. Meyers. 1994. Com-
lex syntax: building a computational lexicon. In In-
ternational Conference on Computational Linguistics,
pages 268?272, Kyoto, Japan.
R. Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, Massachusetts.
E. Joanis. 2002. Automatic verb classification using a
general feature space. Master?s thesis, University of
Toronto.
J. L. Klavans and M. Kan. 1998. Role of verbs in docu-
ment analysis. In Proc. of COLING/ACL, pages 680?
686, Montreal, Canada.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
A. Korhonen. 2003. Extending Levin?s Classification
with New Verb Classes. Unpublished manuscript, Uni-
versity of Cambridge Computer Laboratory.
G. Leech. 1992. 100 million words of english: the british
national corpus. Language Research, 28(1):1?13.
B. Levin. 1993. English Verb Classes and Alternations.
Chicago University Press, Chicago.
P. Merlo and S. Stevenson. 2001. Automatic verb clas-
sification based on statistical distributions of argument
structure. Computational Linguistics, 27(3):373?408.
P. Merlo, S. Stevenson, V. Tsang, and G. Allaria. 2002.
A multilingual paradigm for automatic verb classifica-
tion. In Proc. of the 40th ACL, Pennsylvania, USA.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235?312.
S. Pinker. 1989. Learnability and Cognition: The Acqui-
sition of Argument Structure. MIT Press, Cambridge,
Massachusetts.
J. Preiss and A. Korhonen. 2002. Improving subcate-
gorization acquisition with WSD. In ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia, USA.
D. Roland, D. Jurafsky, L. Menn, S. Gahl, E. Elder, and
C. Riddoch. 2000. Verb subcatecorization frequency
differences between business-news and balanced cor-
pora. In ACL Workshop on Comparing Corpora, pages
28?34.
S. Schulte im Walde and C. Brew. 2002. Inducing ger-
man semantic verb classes from purely syntactic sub-
categorisation information. In Proc. of the 40th ACL,
Philadephia, USA.
S. Schulte im Walde. 2000. Clustering verbs seman-
tically according to their alternation behaviour. In
Proc. of COLING-2000, pages 747?753, Saarbru?cken,
Germany.
S. Stevenson and E. Joanis. 2003. Semi-supervised
verb-class discovery using noisy features. In Proc. of
CoNLL-2003, Edmonton, Canada.
N. Tishby, F. C. Pereira, and W. Bialek. 1999. The infor-
mation bottleneck method. In Proc. of the 37th Annual
Allerton Conference on Communication, Control and
Computing, pages 368?377.
Proceedings of the 43rd Annual Meeting of the ACL, pages 614?621,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic Acquisition of Adjectival Subcategorization from Corpora
Jeremy Yallop?, Anna Korhonen, and Ted Briscoe
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 OFD, UK
yallop@cantab.net, {Anna.Korhonen, Ted.Briscoe}@cl.cam.ac.uk
Abstract
This paper describes a novel system
for acquiring adjectival subcategorization
frames (SCFs) and associated frequency
information from English corpus data.
The system incorporates a decision-tree
classifier for 30 SCF types which tests
for the presence of grammatical relations
(GRs) in the output of a robust statisti-
cal parser. It uses a powerful pattern-
matching language to classify GRs into
frames hierarchically in a way that mirrors
inheritance-based lexica. The experiments
show that the system is able to detect SCF
types with 70% precision and 66% recall
rate. A new tool for linguistic annotation
of SCFs in corpus data is also introduced
which can considerably alleviate the pro-
cess of obtaining training and test data for
subcategorization acquisition.
1 Introduction
Research into automatic acquisition of lexical in-
formation from large repositories of unannotated
text (such as the web, corpora of published text,
etc.) is starting to produce large scale lexical re-
sources which include frequency and usage infor-
mation tuned to genres and sublanguages. Such
resources are critical for natural language process-
ing (NLP), both for enhancing the performance of
?Part of this research was conducted while this author was
at the University of Edinburgh Laboratory for Foundations of
Computer Science.
state-of-art statistical systems and for improving the
portability of these systems between domains.
One type of lexical information with particular
importance for NLP is subcategorization. Access
to an accurate and comprehensive subcategoriza-
tion lexicon is vital for the development of success-
ful parsing technology (e.g. (Carroll et al, 1998b),
important for many NLP tasks (e.g. automatic verb
classification (Schulte im Walde and Brew, 2002))
and useful for any application which can benefit
from information about predicate-argument struc-
ture (e.g. Information Extraction (IE) (Surdeanu et
al., 2003)).
The first systems capable of automatically learn-
ing a small number of verbal subcategorization
frames (SCFs) from English corpora emerged over
a decade ago (Brent, 1991; Manning, 1993). Subse-
quent research has yielded systems for English (Car-
roll and Rooth, 1998; Briscoe and Carroll, 1997; Ko-
rhonen, 2002) capable of detecting comprehensive
sets of SCFs with promising accuracy and demon-
strated success in application tasks (e.g. (Carroll et
al., 1998b; Korhonen et al, 2003)), besides systems
for a number of other languages (e.g. (Kawahara and
Kurohashi, 2002; Ferrer, 2004)).
While there has been considerable research into
acquisition of verb subcategorization, we are not
aware of any systems built for adjectives. Al-
though adjectives are syntactically less multivalent
than verbs, and although verb subcategorization dis-
tribution data appears to offer the greatest potential
boost in parser performance, accurate and compre-
hensive knowledge of the many adjective SCFs can
improve the accuracy of parsing at several levels
614
(from tagging to syntactic and semantic analysis).
Automatic SCF acquisition techniques are particu-
larly important for adjectives because extant syntax
dictionaries provide very limited coverage of adjec-
tive subcategorization.
In this paper we propose a method for automatic
acquisition of adjectival SCFs from English corpus
data. Our method has been implemented using a
decision-tree classifier which tests for the presence
of grammatical relations (GRs) in the output of the
RASP (Robust Accurate Statistical Parsing) system
(Briscoe and Carroll, 2002). It uses a powerful task-
specific pattern-matching language which enables
the frames to be classified hierarchically in a way
that mirrors inheritance-based lexica. As reported
later, the system is capable of detecting 30 SCFs
with an accuracy comparable to that of best state-of-
art verbal SCF acquisition systems (e.g. (Korhonen,
2002)).
Additionally, we present a novel tool for linguistic
annotation of SCFs in corpus data aimed at alleviat-
ing the process of obtaining training and test data for
subcategorization acquisition. The tool incorporates
an intuitive interface with the ability to significantly
reduce the number of frames presented to the user
for each sentence.
We discuss adjectival subcategorization in sec-
tion 2 and introduce the system for SCF acquisition
in section 3. Details of the annotation tool and the
experimental evaluation are supplied in section 4.
Section 5 provides discussion on our results and fu-
ture work, and section 6 summarises the paper.
2 Adjectival Subcategorization
Although the number of SCF types for adjectives
is smaller than the number reported for verbs
(e.g. (Briscoe and Carroll, 1997)), adjectives never-
theless exhibit rich syntactic behaviour. Besides the
common attributive and predicative positions there
are at least six further positions in which adjec-
tives commonly occur (see figure 1). Adjectives in
predicative position can be further classified accord-
ing to the nature of the arguments with which they
combine ? finite and non-finite clauses and noun
phrases, phrases with and without complementisers,
etc. ? and whether they occur as subject or ob-
ject. Additional distinctions can be made concern-
Attributive ?The young man?
Predicative ?He is young?
Postpositive ?Anyone [who is] young can do it?
Predeterminer ?such a young man?;
?so young a man?
Fused modifier-head ?the younger of them?; ?the young?
Predicative adjunct ?he died young?
Supplementive clause ?Young, he was plain
in appearance?
Contingent clause ?When young, he was lonely?
Figure 1: Fundamental adjectival frames
ing such features as the mood of the complement
(mandative, interrogative, etc.), preferences for par-
ticular prepositions and whether the subject is extra-
posed.
Even ignoring preposition preference, there are
more than 30 distinguishable adjectival SCFs. Some
fairly extensive frame sets can be found in large syn-
tax dictionaries, such as COMLEX (31 SCFs) (Wolff
et al, 1998) and ANLT (24 SCFs) (Boguraev et al,
1987). While such resources are generally accu-
rate, they are disappointingly incomplete: none of
the proposed frame sets in the well-known resources
subsumes the others, the coverage of SCF types for
individual adjectives is low, and (accurate) informa-
tion on the relative frequency of SCFs for each ad-
jective is absent.
The inadequacy of manually-created dictionaries
and the difficulty of adequately enhancing and main-
taining the information by hand was a central moti-
vation for early research into automatic subcatego-
rization acquisition. The focus heretofore has re-
mained firmly on verb subcategorization, but this is
not sufficient, as countless examples show. Knowl-
edge of adjectival subcategorization can yield fur-
ther improvements in tagging (e.g. distinguishing
between ?to? as an infinitive marker and as a true
preposition), parsing (e.g. distinguishing between
PP-arguments and adjuncts), and semantic analysis.
For example, if John is both easy and eager to please
then we know that he is the recipient of pleasure in
the first instance and desirous of providing it in the
second, but a computational system cannot deter-
mine this without knowledge of the subcategoriza-
tion of the two adjectives. Likewise, a natural lan-
guage generation system can legitimately apply the
extraposition transformation to the first case, but not
to the second: It is ?easy to please John?, but not
615
?eager? to do so, at least if ?it? be expletive. Similar
examples abound.
Many of the difficulties described in the litera-
ture on acquiring verb subcategorization also arise
in the adjectival case. The most apparent is data
sparsity: among the 100M-word British National
Corpus (BNC) (Burnard, 1995), the RASP tools find
124,120 distinct adjectives, of which 70,246 occur
only once, 106,464 fewer than ten times and 119,337
fewer than a hundred times. There are fewer than
1,000 adjectives in the corpus which have more than
1,000 occurrences. Both adjective and SCF frequen-
cies have Zipfian distributions; consequently, even
the largest corpora may contain only single instances
of a particular adjective-SCF combination, which is
generally insufficient for classification.
3 Description of the System
Besides focusing on adjectives, our approach to SCF
acquisition differs from earlier work in a number
of ways. A common strategy in existing systems
(e.g. (Briscoe and Carroll, 1997)) is to extract SCFs
from parse trees, introducing an unnecessary depen-
dence on the details of a particular parser. In our ap-
proach the patterns are extracted from GRs ? repre-
sentations of head-complement relations which are
designed to be largely parser-independent ? mak-
ing the techniques more widely applicable and al-
lowing classification to operate at a higher level.
Further, most existing systems work by classifying
corpus occurrences into individual, mutually inde-
pendent SCFs. We adopt instead a hierarchical ap-
proach, viewing frames that share features as de-
scendants of a common parent frame. The benefits
are severalfold: specifying each feature only once
makes the system both more efficient and easier to
understand and maintain, and the multiple inheri-
tance hierarchy reflects the hierarchy of lexical types
found in modern grammars where relationships be-
tween similar frames are represented explicitly1 .
Our acquisition process consists of two main
steps: 1) extracting GRs from corpus data, and 2)
feeding the GRs as input to the classifier which in-
crementally matches parts of the GR sets to decide
which branches of a decision-tree to follow. The
1Compare the cogent argument for a inheritance-based lexi-
con in (Flickinger and Nerbonne, 1992), much of which can be
applied unchanged to the taxonomy of SCFs.
dependent
mod arg mod arg aux conj
subj or dobjncmod xmod cmod detmod
subj comp
ncsubj xsubj csubj obj clausal
dobj obj2 iobj xcomp ccomp
Figure 2: The GR hierarchy used by RASP
leaves of the tree correspond to SCFs. The details of
these two steps are provided in the subsequent sec-
tions, respectively2 .
3.1 Obtaining Grammatical Relations
Attempts to acquire verb subcategorization have
benefited from increasingly sophisticated parsers.
We have made use of the RASP toolkit (Briscoe and
Carroll, 2002) ? a modular statistical parsing sys-
tem which includes a tokenizer, tagger, lemmatiser,
and a wide-coverage unification-based tag-sequence
parser. The parser has several modes of operation;
we invoked it in a mode in which GRs with asso-
ciated probabilities are emitted even when a com-
plete analysis of the sentence could not be found. In
this mode there is wide coverage (over 98% of the
BNC receives at least a partial analysis (Carroll and
Briscoe, 2002)) which is useful in view of the in-
frequent occurrence of some of the SCFs, although
combining the results of competing parses may in
some cases result in an inconsistent or misleading
combination of GRs.
The parser uses a scheme of GRs between lemma-
tised lexical heads (Carroll et al, 1998a; Briscoe et
al., 2002). The relations are organized as a multiple-
inheritance subsumption hierarchy where each sub-
relation extends the meaning, and perhaps the argu-
ment structure, of its parents (figure 2). For descrip-
tions and examples of each relation, see (Carroll et
al., 1998a).
The dependency relationships which the GRs em-
body correspond closely to the head-complement
2In contrast to almost all earlier work, there was no filtering
stage involved in SCF acquisition. The classifier was designed
to operate with high precision, so filtering was less necessary.
616
26
6
6
6
6
6
4
SUBJECT NP 1 ,
ADJ-COMPS
*
PP
"
PVAL ?for?
NP 3
#
,
VP
2
6
6
4
MOOD to-infinitive
SUBJECT 3
OMISSION 1
3
7
7
5
+
3
7
7
7
7
7
7
5
Figure 3: Feature structure for SCF
adj-obj-for-to-inf
(|These:1_DD2| |example+s:2_NN2| |of:3_IO|
|animal:4_JJ| |senses:5_NN2| |be+:6_VBR|
|relatively:7_RR| |easy:8_JJ| |for:9_IF|
|we+:10_PPIO2| |to:11_TO| |comprehend:12_VV0|)
...
xcomp(_ be+[6] easy:[8])
xmod(to[11] be+[6] comprehend:[12])
ncsubj(be+[6] example+s[2] _)
ncmod(for[9] easy[8] we+[10])
ncsubj(comprehend[12] we+[10], _)
...
Figure 4: GRs from RASP for adj-obj-for-to-inf
structure which subcategorization acquisition at-
tempts to recover, which makes GRs ideal input to
the SCF classifier. Consider the arguments of ?easy?
in the sentence:
These examples of animal senses are rel-
atively easy for us to comprehend as they
are not too far removed from our own ex-
perience.
According to the COMLEX classification, this is an
example of the frame adj-obj-for-to-inf, shown
in figure 3, (using AVM notation in place of COMLEX
s-expressions). Part of the output of RASP for this
sentence (the full output includes 87 weighted GRs)
is shown in figure 43.
Each instantiated GR in figure 4 corresponds to
one or more parts of the feature structure in figure
3. xcomp( be[6] easy[8]) establishes be[6] as
the head of the VP in which easy[8] occurs as a
complement. The first (PP)-complement is ?for us?,
as indicated by ncmod(for[9] easy[8] we+[10]),
with ?for? as PFORM and we+ (?us?) as NP. The
second complement is represented by xmod(to[11]
be+[6] comprehend[12]): a to-infinitive VP. The
NP headed by ?examples? is marked as the subject
of the frame by ncsubj(be[6] examples[2]), and
ncsubj(comprehend[12] we+[10]) corresponds to
the coindexation marked by 3 : the subject of the
3The format is slightly more complicated than that shown
in (Carroll et al, 1998a): each argument that corresponds to a
word consists of three parts: the lexeme, the part of speech tag,
and the position (index) of the word in the sentence.
xcomp(_, [*;1;be-verb], ?)
xmod([to;*;to], 1, [*;2;vv0])
ncsubj(1, [*;3;noun/pronoun], _)
ncmod([for;*;if], ?, [*;4;noun/pronoun])
ncsubj(2, 4)
Figure 5: A pattern to match the frame
adj-obj-for-to-inf
VP is the NP of the PP. The only part of the feature
structure which is not represented by the GRs is coin-
dexation between the omitted direct object 1 of the
VP-complement and the subject of the whole clause.
3.2 SCF Classifier
3.2.1 SCF Frames
We used for our classifier a modified version of
the fairly extensive COMLEX frameset, including 30
SCFs. The COMLEX frameset includes mutually in-
consistent frames, such as sentential complement
with obligatory complementiser that and sentential
complement with optional that. We modified the
frameset so that an adjective can legitimately instan-
tiate any combination of frames, which simplifies
classification. We also added simple-predicative
and attributive SCFs to the set, since these ac-
count for a substantial proportion of frame instances.
Finally, frames which could only be distinguished
by information not retained in the GRs scheme of the
current version of the shallow parser were merged
(e.g. the COMLEX frames adj-subj-to-inf-rs
(?She was kind to invite me?) and adj-to-inf (?She
was able to climb the mountain?)).
3.2.2 Classifier
The classifier operates by attempting to match the
set of GRs associated with each sentence against var-
ious patterns. The patterns were developed by a
combination of knowledge of the GRs and examin-
ing a set of training sentences to determine which re-
lations were actually emitted by the parser for each
SCF. The data used during development consisted
of the sentences in the BNC in which one of the 23
adjectives4 given as examples for SCFs in (Macleod
4The adjectives used for training were: able, anxious, ap-
parent, certain, convenient, curious, desirable, disappointed,
easy, happy, helpful, imperative, impractical, insistent, kind,
obvious, practical, preferable, probable, ridiculous, unaware,
uncertain and unclear.
617
et al, 1998) occur.
In our pattern matching language a pattern is a
disjunction of sets of partially instantiated GRs with
logic variables (slots) in place of indices, augmented
by ordering constraints that restrict the possible in-
stantiations of slots. A match is considered success-
ful if the set of GRs can be unified with any of the
disjuncts. Unification of a sentence-relation and a
pattern-relation occurs when there is a one-to-one
correspondence between sentence elements and pat-
tern elements that includes a mapping from slots to
indices (a substitution), and where atomic elements
in corresponding positions share a common subtype.
Figure 5 shows a pattern for matching the SCF
adj-obj-for-to-inf. For a match to suc-
ceed there must be GRs associated with the sen-
tence that match each part of the pattern. Each ar-
gument matches either anything at all (*), the ?cur-
rent? adjective (?), an empty GR argument ( ), a
[word;id;part-of-speech] 3-tuple or a nu-
meric id. In a successful match, equal ids in different
parts of the pattern must match the same word posi-
tion, and distinct ids must match different positions.
The various patterns are arranged in a tree, where
a parent node contains the elements common to all
of its children. This kind of once-only representa-
tion of particular features, together with the succes-
sive refinements provided by child nodes reflects the
organization of inheritance-based lexica. The inher-
itance structure naturally involves multiple inheri-
tance, since each frame typically includes multiple
features (such as the presence of a to-infinitive
complement or an expletive subject argument) inher-
ited from abstract parent classes, and each feature is
instantiated in several frames.
The tree structure also improves the efficiency of
the pattern matching process, which then occurs in
stages: at each matching node the classifier attempts
to match a set of relations with each child pattern
to yield a substitution that subsumes the substitution
resulting from the parent match.
Both the patterns and the pattern language itself
underwent successive refinements after investigation
of the performance on training data made it increas-
ingly clear what sort of distinctions were useful to
express. The initial pattern language had no slots; it
was easy to understand and implement, but insuffi-
ciently expressive. The final refinement was the ad-
unspecified 285 improbable 350
unsure 570 doubtful 1147
generous 2052 sure 13591
difficult 18470 clear 19617
important 33303
Table 1: Test adjectives and frequencies in the BNC
dition of ordering constraints between instantiated
slots, which are indispensable for detecting, e.g., ex-
traposition.
4 Experimental Evaluation
4.1 Data
In order to evaluate the system we selected a set of
9 adjectives which between them could instantiate
all of the frames. The test set was intentionally kept
fairly small for these first experiments with adjec-
tival SCF acquisition so that we could carry out a
thorough evaluation of all the test instances. We ex-
cluded the adjectives used during development and
adjectives with fewer than 200 instances in the cor-
pus. The final test set, together with their frequen-
cies in the tagged version of the BNC, is shown in ta-
ble 1. For each adjective we extracted 200 sentences
(evenly spaced throughout the BNC) which we pro-
cessed using the SCF acquisition system described in
the previous section.
4.2 Method
4.2.1 Annotation Tool and Gold Standard
Our gold standard was human-annotated data.
Two annotators associated a SCF with each sen-
tence/adjective pair in the test data. To alleviate the
process we developed a program which first uses re-
liable heuristics to reduce the number of SCF choices
and then allows the annotator to select the preferred
choice with a single mouse click in a browser win-
dow. The heuristics reduced the average number
of SCFs presented alongside each sentence from 30
to 9. Through the same browser interface we pro-
vided annotators with information and instructions
(with links to COMLEX documentation), the ability
to inspect and review previous decisions and deci-
sion summaries5 and an option to record that partic-
5The varying number of SCFs presented to the user and the
ability to revisit previous decisions precluded accurate measure-
618
Figure 6: Sample classification screen for web an-
notation tool
ular sentences could not be classified (which is use-
ful for further system development, as discussed in
section 5). A screenshot is shown in figure 6. The
resulting annotation revealed 19 of the 30 SCFs in
the test data.
4.2.2 Evaluation Measures
We use the standard evaluation metrics: type and
token precision, recall and F-measure. Token recall
is the proportion of annotated (sentence, frame) pairs
that the system recovered correctly. Token precision
is the proportion of classified (sentence, frame) pairs
that were correct. Type precision and type recall are
analogously defined for (adjective, frame) pairs. The
F-measure (? = 1) is a weighted combination of
precision and recall.
4.3 Results
Running the system on the test data yielded the re-
sults summarised in table 2. The greater expres-
siveness of the final pattern language resulted in a
classifier that performed better than the ?regression?
versions which ignored either ordering constraints,
or both ordering constraints and slots. As expected,
removing features from the classifier translated di-
rectly into degraded accuracy. The performance of
the best classifier (67.8% F-measure) is quite simi-
lar to that of the best current verbal SCF acquisition
systems (e.g. (Korhonen, 2002)).
Results for individual adjectives are given in table
3. The first column shows the number of SCFs ac-
quired for each adjective, ranging from 2 for unspec-
ments of inter-annotator agreement, but this was judged less im-
portant than the enhanced ease of use arising from the reduced
set of choices.
Type performance
System Precision Recall F
Final 69.6 66.1 67.8
No order constraints 67.3 62.7 64.9
No slots 62.7 51.4 56.5
Token performance
System Precision Recall F
Final 63.0 70.5 66.5
No order constraints 58.8 68.3 63.2
No slots 58.3 67.6 62.6
Table 2: Overall performance of the classifier and of
regression systems with restricted pattern-matching
ified to 11 for doubtful. Looking at the F-measure,
the best performing adjectives are unspecified, diffi-
cult and sure (80%) and the worst performing unsure
(50%) and and improbable (60%).
There appears to be no obvious connection be-
tween performance figures and the number of ac-
quired SCF types; differences are rather due to the
difficulty of detecting individual SCF types ? an is-
sue directly related to data sparsity.
Despite the size of the BNC, 5 SCFs were not
seen at all, either for the test adjectives or for any
others. Frames involving to-infinitive complements
were particularly rare: 4 such SCFs had no exam-
ples in the corpus and a further 3 occurred 5 times or
fewer in the test data. It is more difficult to develop
patterns for SCFs that occur infrequently, and the few
instances of such SCFs are unlikely to include a set
of GRs that is adequate for classification. The ef-
fect on the results was clear: of the 9 SCFs which
the classifier did not correctly recognise at all, 4 oc-
curred 5 times or fewer in the test data and a further
2 occurred 5?10 times.
The most common error made by the clas-
sifier was to mistake a complex frame (e.g.
adj-obj-for-to-inf, or to-inf-wh-adj)
for simple-predicative, which subsumes all
such frames. This occurred whenever the GRs emit-
ted by the parser failed to include any information
about the complements of the adjective.
5 Discussion
Data sparsity is perhaps the greatest hindrance both
to recovering adjectival subcategorization and to
lexical acquisition in general. In the future, we plan
to carry out experiments with a larger set of adjec-
619
Adjective SCFs Precision Recall F-measure
unspecified 2 66.7 100.0 80.0
generous 3 60.0 100.0 75.0
improbable 5 60.0 60.0 60.0
unsure 6 50.0 50.0 50.0
important 7 55.6 71.4 62.5
clear 8 83.3 62.5 71.4
difficult 8 85.7 75.0 80.0
sure 9 100.0 66.7 80.0
doubtful 11 66.7 54.5 60.0
Table 3: SCF count and classifier performance for
each adjective.
tives using more data (possibly from several corpora
and the web) to determine how severe this problem
is for adjectives. One possible way to address the
problem is to smooth the acquired SCF distributions
using SCF ?back-off? (probability) estimates based
on lexical classes of adjectives in the manner pro-
posed by (Korhonen, 2002). This helps to correct the
acquired distributions and to detect low frequency
and unseen SCFs.
However, our experiment also revealed other
problems which require attention in the future.
One such is that GRs output by RASP (the ver-
sion we used in our experiments) do not re-
tain certain distinctions which are essential for
distinguishing particular SCFs. For example,
a sentential complement of an adjective with
a that-complementiser should be annotated with
ccomp(that, adjective, verbal-head), but this
relation (with that as the type argument) does not
occur in the parsed BNC. As a consequence the clas-
sifier is unable to distinguish the frame.
Another problem arises from the fact that our cur-
rent classifier operates on a predefined set of SCFs.
The COMLEX SCFs, from which ours were derived,
are extremely incomplete. Almost a quarter (477 of
1931) of sentences were annotated as ?undefined?.
For example, while there are SCFs for sentential
and infinitival complement in subject position with
what6, there is no SCF for the case with a what-
prefixed complement in object position, where the
subject is an NP. The lack is especially perplexing,
because COMLEX does include the corresponding
SCFs for verbs. There is a frame for ?He wondered
6(adj-subj-what-s: ?What he will do is uncertain?;
adj-subj-what-to-inf: ?What to do was unclear?), to-
gether with the extraposed versions (extrap-adj-what-s
and extrap-adj-what-to-inf).
what to do? (what-to-inf), but none for ?He was
unsure what to do?.
While we can easily extend the current frame-
set by looking for further SCF types from dictio-
naries and from among the corpus occurrences la-
belled by our annotators as unclassified, we also plan
to extend the classifier to automatically induce pre-
viously unseen frames from data. A possible ap-
proach is to use restricted generalization on sets of
GRs to group similar sentences together. General-
ization (anti-unification) is an intersection operation
on two structures which retains the features common
to both; generalization over the sets of GRs associ-
ated with the sentences which instantiate a particular
frame can produce a pattern such as we used for clas-
sification in the experiments described above. This
approach also offers the possibility of associating
confidence levels with each pattern, corresponding
to the degree to which the generalized pattern cap-
tures the features common to the members of the
associated class. It is possible that frames could
be induced by grouping sentences according to the
?best? (e.g. most information-preserving) general-
izations for various combinations, but it is not clear
how this can be implemented with acceptable effi-
ciency.
The hierarchical approach described in this paper
may also helpful in the discovery of new frames:
missing combinations of parent classes can be ex-
plored readily, and it may be possible to combine the
various features in an SCF feature structure to gen-
erate example sentences which a human could then
inspect to judge grammaticality.
6 Conclusion
We have described a novel system for automati-
cally acquiring adjectival subcategorization and as-
sociated frequency information from corpora, along
with an annotation tool for producing training and
test data for the task. The acquisition system, which
is capable of distinguishing 30 SCF types, performs
sophisticated pattern matching on sets of GRs pro-
duced by a robust statistical parser. The informa-
tion provided by GRs closely matches the structure
that subcategorization acquisition seeks to recover.
The figures reported demonstrate the feasibility of
the approach: our classifier achieved 70% type pre-
620
cision and 66% type recall on the test data. The dis-
cussion suggests several ways in which the system
may be improved, refined and extended in the fu-
ture.
Acknowledgements
We would like to thank Ann Copestake for all her
help during this work.
References
B. Boguraev, J. Carroll, E. Briscoe, D. Carter, and
C. Grover. 1987. The derivation of a grammatically-
indexed lexicon from the Longman Dictionary of Con-
temporary English. In Proceedings of the 25th Annual
Meeting of the Association for Computational Linguis-
tics, pages 193?200, Stanford, CA.
Michael R. Brent. 1991. Automatic acquisition of sub-
categorization frames from untagged text. In Meet-
ing of the Association for Computational Linguistics,
pages 209?214.
E. J. Briscoe and J. Carroll. 1997. Automatic Extraction
of Subcategorization from Corpora. In Proceedings
of the 5th Conference on Applied Natural Language
Processing, Washington DC, USA.
E. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation, pages 1499?1504, Las Pal-
mas, Canary Islands, May.
E. Briscoe, J. Carroll, Jonathan Graham, and Ann Copes-
take. 2002. Relational evaluation schemes. In Pro-
ceedings of the Beyond PARSEVAL Workshop at the
3rd International Conference on Language Resources
and Evaluation, pages 4?8, Las Palmas, Gran Canaria.
Lou Burnard, 1995. The BNC Users Reference Guide.
British National Corpus Consortium, Oxford, May.
J. Carroll and E. Briscoe. 2002. High precision extrac-
tion of grammatical relations. In Proceedings of the
19th International Conference on Computational Lin-
guistics, pages 134?140, Taipei, Taiwan.
Glenn Carroll and Mats Rooth. 1998. Valence induction
with a head-lexicalized pcfg. In Proc. of the 3rd Con-
ference on Empirical Methods in Natural Language
Processing, Granada, Spain.
J. Carroll, E. Briscoe, and A. Sanfilippo. 1998a. Parser
evaluation: a survey and a new proposal. In Proceed-
ings of the 1st International Conference on Language
Resources and Evaluation, pages 447?454, Granada,
Spain.
John Carroll, Guido Minnen, and Edward Briscoe.
1998b. Can Subcategorisation Probabilities Help
a Statistical Parser? In Proceedings of the 6th
ACL/SIGDAT Workshop on Very Large Corpora, pages
118?126, Montreal, Canada. Association for Compu-
tational Linguistics.
Eva Esteve Ferrer. 2004. Towards a Semantic Clas-
sification of Spanish Verbs Based on Subcategorisa-
tion Information. In ACL Student Research Workshop,
Barcelona, Spain.
Dan Flickinger and John Nerbonne. 1992. Inheritance
and complementation: A case study of easy adjec-
tives and related nouns. Computational Linguistics,
18(3):269?309.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of Case Frame Dictionary for Robust Japanese
Case Analysis. In 19th International Conference on
Computational Linguistics.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering Polysemic Subcategorization Frame
Distributions Semantically. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 64?71, Sapporo, Japan.
Anna Korhonen. 2002. Subcategorization acquisition.
Ph.D. thesis, University of Cambridge Computer Lab-
oratory, February.
Catherine Macleod, Ralph Grishman, and Adam Meyers,
1998. COMLEX Syntax Reference Manual. Computer
Science Department, New York University.
Christopher D. Manning. 1993. Automatic Acquisition
of a Large Subcategorization Dictionary from Cor-
pora. In Meeting of the Association for Computational
Linguistics, pages 235?242.
S. Schulte im Walde and C. Brew. 2002. Inducing
german semantic verb classes from purely syntactic
subcategorisation information. In 40th Annual Meet-
ing of the Association for Computational Linguistics,
Philadephia, USA.
Mihai Surdeanu, Sanda Harabagiu, JohnWilliams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proc. of the 41st
Annual Meeting of the Association for Computational
Linguistics, Sapporo.
Susanne Rohen Wolff, Catherine Macleod, and Adam
Meyers, 1998. COMLEX Word Classes Manual. Com-
puter Science Department, New York University ,
June.
621
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 345?352,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Classification of Verbs in Biomedical Texts
Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0GD, UK
alk23@cl.cam.ac.uk
Yuval Krymolowski
Dept. of Computer Science
Technion
Haifa 32000
Israel
yuvalkr@cs.technion.ac.il
Nigel Collier
National Institute of Informatics
Hitotsubashi 2-1-2
Chiyoda-ku, Tokyo 101-8430
Japan
collier@nii.ac.jp
Abstract
Lexical classes, when tailored to the appli-
cation and domain in question, can provide
an effective means to deal with a num-
ber of natural language processing (NLP)
tasks. While manual construction of such
classes is difficult, recent research shows
that it is possible to automatically induce
verb classes from cross-domain corpora
with promising accuracy. We report a
novel experiment where similar technol-
ogy is applied to the important, challeng-
ing domain of biomedicine. We show that
the resulting classification, acquired from
a corpus of biomedical journal articles,
is highly accurate and strongly domain-
specific. It can be used to aid BIO-NLP
directly or as useful material for investi-
gating the syntax and semantics of verbs
in biomedical texts.
1 Introduction
Lexical classes which capture the close relation
between the syntax and semantics of verbs have
attracted considerable interest in NLP (Jackendoff,
1990; Levin, 1993; Dorr, 1997; Prescher et al,
2000). Such classes are useful for their ability to
capture generalizations about a range of linguis-
tic properties. For example, verbs which share the
meaning of ?manner of motion? (such as travel,
run, walk), behave similarly also in terms of
subcategorization (I traveled/ran/walked, I trav-
eled/ran/walked to London, I traveled/ran/walked
five miles). Although the correspondence between
the syntax and semantics of words is not perfect
and the classes do not provide means for full se-
mantic inferencing, their predictive power is nev-
ertheless considerable.
NLP systems can benefit from lexical classes
in many ways. Such classes define the mapping
from surface realization of arguments to predicate-
argument structure, and are therefore an impor-
tant component of any system which needs the
latter. As the classes can capture higher level
abstractions they can be used as a means to ab-
stract away from individual words when required.
They are also helpful in many operational contexts
where lexical information must be acquired from
small application-specific corpora. Their predic-
tive power can help compensate for lack of data
fully exemplifying the behavior of relevant words.
Lexical verb classes have been used to sup-
port various (multilingual) tasks, such as compu-
tational lexicography, language generation, ma-
chine translation, word sense disambiguation, se-
mantic role labeling, and subcategorization acqui-
sition (Dorr, 1997; Prescher et al, 2000; Korho-
nen, 2002). However, large-scale exploitation of
the classes in real-world or domain-sensitive tasks
has not been possible because the existing classi-
fications, e.g. (Levin, 1993), are incomprehensive
and unsuitable for specific domains.
While manual classification of large numbers of
words has proved difficult and time-consuming,
recent research shows that it is possible to auto-
matically induce lexical classes from corpus data
with promising accuracy (Merlo and Stevenson,
2001; Brew and Schulte im Walde, 2002; Ko-
rhonen et al, 2003). A number of ML methods
have been applied to classify words using features
pertaining to mainly syntactic structure (e.g. sta-
tistical distributions of subcategorization frames
(SCFs) or general patterns of syntactic behaviour,
e.g. transitivity, passivisability) which have been
extracted from corpora using e.g. part-of-speech
tagging or robust statistical parsing techniques.
345
This research has been encouraging but it has
so far concentrated on general language. Domain-
specific lexical classification remains unexplored,
although it is arguably important: existing clas-
sifications are unsuitable for domain-specific ap-
plications and these often challenging applications
might benefit from improved performance by uti-
lizing lexical classes the most.
In this paper, we extend an existing approach
to lexical classification (Korhonen et al, 2003)
and apply it (without any domain specific tun-
ing) to the domain of biomedicine. We focus on
biomedicine for several reasons: (i) NLP is criti-
cally needed to assist the processing, mining and
extraction of knowledge from the rapidly growing
literature in this area, (ii) the domain lexical re-
sources (e.g. UMLS metathesaurus and lexicon1)
do not provide sufficient information about verbs
and (iii) being linguistically challenging, the do-
main provides a good test case for examining the
potential of automatic classification.
We report an experiment where a classifica-
tion is induced for 192 relatively frequent verbs
from a corpus of 2230 biomedical journal articles.
The results, evaluated with domain experts, show
that the approach is capable of acquiring classes
with accuracy higher than that reported in previous
work on general language. We discuss reasons for
this and show that the resulting classes differ sub-
stantially from those in extant lexical resources.
They constitute the first syntactic-semantic verb
classification for the biomedical domain and could
be readily applied to support BIO-NLP.
We discuss the domain-specific issues related to
our task in section 2. The approach to automatic
classification is presented in section 3. Details of
the experimental evaluation are supplied in sec-
tion 4. Section 5 provides discussion and section
6 concludes with directions for future work.
2 The Biomedical Domain and Our Task
Recent years have seen a massive growth in the
scientific literature in the domain of biomedicine.
For example, the MEDLINE database2 which cur-
rently contains around 16M references to journal
articles, expands with 0.5M new references each
year. Because future research in the biomedical
sciences depends on making use of all this existing
knowledge, there is a strong need for the develop-
1http://www.nlm.nih.gov/research/umls
2http://www.ncbi.nlm.nih.gov/PubMed/
ment of NLP tools which can be used to automat-
ically locate, organize and manage facts related to
published experimental results.
In recent years, major progress has been made
on information retrieval and on the extraction of
specific relations e.g. between proteins and cell
types from biomedical texts (Hirschman et al,
2002). Other tasks, such as the extraction of fac-
tual information, remain a bigger challenge. This
is partly due to the challenging nature of biomedi-
cal texts. They are complex both in terms of syn-
tax and semantics, containing complex nominals,
modal subordination, anaphoric links, etc.
Researchers have recently began to use deeper
NLP techniques (e.g. statistical parsing) in the do-
main because they are not challenged by the com-
plex structures to the same extent than shallow
techniques (e.g. regular expression patterns) are
(Lease and Charniak, 2005). However, deeper
techniques require richer domain-specific lexical
information for optimal performance than is pro-
vided by existing lexicons (e.g. UMLS). This is
particularly important for verbs, which are central
to the structure and meaning of sentences.
Where the lexical information is absent, lexical
classes can compensate for it or aid in obtaining
it in the ways described in section 1. Consider
e.g. the INDICATE and ACTIVATE verb classes in
Figure 1. They capture the fact that their members
are similar in terms of syntax and semantics: they
have similar SCFs and selectional preferences, and
they can be used to make similar statements which
describe similar events. Such information can be
used to build a richer lexicon capable of support-
ing key tasks such as parsing, predicate-argument
identification, event extraction and the identifica-
tion of biomedical (e.g. interaction) relations.
While an abundance of work has been con-
ducted on semantic classification of biomedical
terms and nouns, less work has been done on the
(manual or automatic) semantic classification of
verbs in the biomedical domain (Friedman et al,
2002; Hatzivassiloglou and Weng, 2002; Spasic et
al., 2005). No previous work exists in this domain
on the type of lexical (i.e. syntactic-semantic) verb
classification this paper focuses on.
To get an initial idea about the differences be-
tween our target classification and a general lan-
guage classification, we examined the extent to
which individual verbs and their frequencies dif-
fer in biomedical and general language texts. We
346
PROTEINS: p53
p53Tp53Dmp53...
ACTIVATE
suggestsdemonstratesindicatesimplies...
GENES: WAF1
WAF1CIP1p21...
It
INDICATE
that
activatesup-regulatesinducesstimulates...
...
Figure 1: Sample lexical classes
BIO BNC
show do
suggest say
use make
indicate go
contain see
describe take
express get
bind know
require come
observe give
find think
determine use
demonstrate find
perform look
induce want
Table 1: The 15 most frequent verbs in the
biomedical data and in the BNC
created a corpus of 2230 biomedical journal arti-
cles (see section 4.1 for details) and compared the
distribution of verbs in this corpus with that in the
British National Corpus (BNC) (Leech, 1992). We
calculated the Spearman rank correlation between
the 1165 verbs which occurred in both corpora.
The result was only a weak correlation: 0.37 ?
0.03. When the scope was restricted to the 100
most frequent verbs in the biomedical data, the
correlation was 0.12 ? 0.10 which is only 1.2?
away from zero. The dissimilarity between the
distributions is further indicated by the Kullback-
Leibler distance of 0.97. Table 1 illustrates some
of these big differences by showing the list of 15
most frequent verbs in the two corpora.
3 Approach
We extended the system of Korhonen et al (2003)
with additional clustering techniques (introduced
in sections 3.2.2 and 3.2.4) and used it to ob-
tain the classification for the biomedical domain.
The system (i) extracts features from corpus data
and (ii) clusters them using five different methods.
These steps are described in the following two sec-
tions, respectively.
3.1 Feature Extraction
We employ as features distributions of SCFs spe-
cific to given verbs. We extract them from cor-
pus data using the comprehensive subcategoriza-
tion acquisition system of Briscoe and Carroll
(1997) (Korhonen, 2002). The system incorpo-
rates RASP, a domain-independent robust statis-
tical parser (Briscoe and Carroll, 2002), which
tags, lemmatizes and parses data yielding com-
plete though shallow parses and a SCF classifier
which incorporates an extensive inventory of 163
verbal SCFs3. The SCFs abstract over specific
lexically-governed particles and prepositions and
specific predicate selectional preferences. In our
work, we parameterized two high frequency SCFs
for prepositions (PP and NP + PP SCFs). No filter-
ing of potentially noisy SCFs was done to provide
clustering with as much information as possible.
3.2 Classification
The SCF frequency distributions constitute the in-
put data to automatic classification. We experi-
ment with five clustering methods: the simple hard
nearest neighbours method and four probabilis-
tic methods ? two variants of Probabilistic Latent
Semantic Analysis and two information theoretic
methods (the Information Bottleneck and the In-
formation Distortion).
3.2.1 Nearest Neighbours
The first method collects the nearest neighbours
(NN) of each verb. It (i) calculates the Jensen-
Shannon divergence (JS) between the SCF distri-
butions of each pair of verbs, (ii) connects each
verb with the most similar other verb, and finally
(iii) finds all the connected components. The NN
method is very simple. It outputs only one clus-
tering configuration and therefore does not allow
examining different cluster granularities.
3.2.2 Probabilistic Latent Semantic Analysis
The Probabilistic Latent Semantic Analysis
(PLSA, Hoffman (2001)) assumes a generative
model for the data, defined by selecting (i) a verb
verbi, (ii) a semantic class classk from the dis-
tribution p(Classes | verbi), and (iii) a SCF scfj
from the distribution p(SCFs | classk). PLSA uses
Expectation Maximization (EM) to find the dis-
tribution p?(SCFs |Clusters, V erbs) which max-
imises the likelihood of the observed counts. It
does this by minimising the cost function
F = ?? log Likelihood(p? | data) +H(p?) .
3See http://www.cl.cam.ac.uk/users/alk23/subcat/subcat.html
for further detail.
347
For ? = 1 minimising F is equivalent to the stan-
dard EM procedure while for ? < 1 the distri-
bution p? tends to be more evenly spread. We use
? = 1 (PLSA/EM) and ? = 0.75 (PLSA?=0.75).
We currently ?harden? the output and assign each
verb to the most probable cluster only4.
3.2.3 Information Bottleneck
The Information Bottleneck (Tishby et al,
1999) (IB) is an information-theoretic method
which controls the balance between: (i) the
loss of information by representing verbs as
clusters (I(Clusters;V erbs)), which has to be
minimal, and (ii) the relevance of the output
clusters for representing the SCF distribution
(I(Clusters; SCFs)) which has to be maximal.
The balance between these two quantities ensures
optimal compression of data through clusters. The
trade-off between the two constraints is realized
through minimising the cost function:
LIB = I(Clusters;V erbs)
? ?I(Clusters; SCFs) ,
where ? is a parameter that balances the con-
straints. IB takes three inputs: (i) SCF-verb dis-
tributions, (ii) the desired number of clusters K,
and (iii) the initial value of ?. It then looks for
the minimal ? that decreases LIB compared to its
value with the initial ?, using the given K. IB de-
livers as output the probabilities p(K|V ). It gives
an indication for the most informative number of
output configurations: the ones for which the rele-
vance information increases more sharply between
K ? 1 and K clusters than between K and K + 1.
3.2.4 Information Distortion
The Information Distortion method (Dimitrov
and Miller, 2001) (ID) is otherwise similar to IB
but LID differs from LIB by an additional term that
adds a bias towards clusters of similar size:
LID = ?H(Clusters |V erbs)
? ?I(Clusters; SCFs)
= LIB ?H(Clusters) .
ID yields more evenly divided clusters than IB.
4 Experimental Evaluation
4.1 Data
We downloaded the data for our experiment from
the MEDLINE database, from three of the 10 lead-
4The same approach was used with the information theo-
retic methods. It made sense in this initial work on biomedi-
cal classification. In the future we could use soft clustering a
means to investigate polysemy.
ing journals in biomedicine: 1) Genes & Devel-
opment (molecular biology, molecular genetics),
2) Journal of Biological Chemistry (biochemistry
and molecular biology) and 3) Journal of Cell Bi-
ology (cellular structure and function). 2230 full-
text articles from years 2003-2004 were used. The
data included 11.5M words and 323,307 sentences
in total. 192 medium to high frequency verbs (with
the minimum of 300 occurrences in the data) were
selected for experimentation5. This test set was
big enough to produce a useful classification but
small enough to enable thorough evaluation in this
first attempt to classify verbs in the biomedical do-
main.
4.2 Processing the Data
The data was first processed using the feature ex-
traction module. 233 (preposition-specific) SCF
types appeared in the resulting lexicon, 36 per verb
on average.6 The classification module was then
applied. NN produced Knn = 42 clusters. From
the other methods we requested K = 2 to 60 clus-
ters. We chose for evaluation the outputs corre-
sponding to the most informative values of K: 20,
33, 53 for IB, and 17, 33, 53 for ID.
4.3 Gold Standard
Because no target lexical classification was avail-
able for the biomedical domain, human experts (4
domain experts and 2 linguists) were used to cre-
ate the gold standard. They were asked to examine
whether the test verbs similar in terms of their syn-
tactic properties (i.e. verbs with similar SCF distri-
butions) are similar also in terms of semantics (i.e.
they share a common meaning). Where this was
the case, a verb class was identified and named.
The domain experts examined the 116 verbs
whose analysis required domain knowledge
(e.g. activate, solubilize, harvest), while the lin-
guists analysed the remaining 76 general or scien-
tific text verbs (e.g. demonstrate, hypothesize, ap-
pear). The linguists used Levin (1993) classes as
gold standard classes whenever possible and cre-
ated novel ones when needed. The domain ex-
perts used two purely semantic classifications of
biomedical verbs (Friedman et al, 2002; Spasic et
al., 2005)7 as a starting point where this was pos-
5230 verbs were employed initially but 38 were dropped
later so that each (coarse-grained) class would have the min-
imum of 2 members in the gold standard.
6This number is high because no filtering of potentially
noisy SCFs was done.
7See http://www.cbr-masterclass.org.
348
1 Have an effect on activity (BIO/29) 8 Physical Relation
1.1 Activate / Inactivate Between Molecules (BIO/20)
1.1.1 Change activity: activate, inhibit 8.1 Binding: bind, attach
1.1.2 Suppress: suppress, repress 8.2 Translocate and Segregate
1.1.3 Stimulate: stimulate 8.2.1 Translocate: shift, switch
1.1.4 Inactivate: delay, diminish 8.2.2 Segregate: segregate, export
1.2 Affect 8.3 Transmit
1.2.1 Modulate: stabilize, modulate 8.3.1 Transport: deliver, transmit
1.2.2 Regulate: control, support 8.3.2 Link: connect, map
1.3 Increase / decrease: increase, decrease 9 Report (GEN/30)
1.4 Modify: modify, catalyze 9.1 Investigate
2 Biochemical events (BIO/12) 9.1.1 Examine: evaluate, analyze
2.1 Express: express, overexpress 9.1.2 Establish: test, investigate
2.2 Modification 9.1.3 Confirm: verify, determine
2.2.1 Biochemical modification: 9.2 Suggest
dephosphorylate, phosphorylate 9.2.1 Presentational:
2.2.2 Cleave: cleave hypothesize, conclude
2.3 Interact: react, interfere 9.2.2 Cognitive:
3 Removal (BIO/6) consider, believe
3.1 Omit: displace, deplete 9.3 Indicate: demonstrate, imply
3.2 Subtract: draw, dissect 10 Perform (GEN/10)
4 Experimental Procedures (BIO/30) 10.1 Quantify
4.1 Prepare 10.1.1 Quantitate: quantify, measure
4.1.1 Wash: wash, rinse 10.1.2 Calculate: calculate, record
4.1.2 Mix: mix 10.1.3 Conduct: perform, conduct
4.1.3 Label: stain, immunoblot 10.2 Score: score, count
4.1.4 Incubate: preincubate, incubate 11 Release (BIO/4): detach, dissociate
4.1.5 Elute: elute 12 Use (GEN/4): utilize, employ
4.2 Precipitate: coprecipitate 13 Include (GEN/11)
coimmunoprecipitate 13.1 Encompass: encompass, span
4.3 Solubilize: solubilize,lyse 13.2 Include: contain, carry
4.4 Dissolve: homogenize, dissolve 14 Call (GEN/3): name, designate
4.5 Place: load, mount 15 Move (GEN/12)
5 Process (BIO/5): linearize, overlap 15.1 Proceed:
6 Transfect (BIO/4): inject, microinject progress, proceed
7 Collect (BIO/6) 15.2 Emerge:
7.1 Collect: harvest, select arise, emerge
7.2 Process: centrifuge, recover 16 Appear (GEN/6): appear, occur
Table 2: The gold standard classification with a
few example verbs per class
sible (i.e. where they included our test verbs and
also captured their relevant senses)8.
The experts created a 3-level gold standard
which includes both broad and finer-grained
classes. Only those classes / memberships were
included which all the experts (in the two teams)
agreed on.9 The resulting gold standard includ-
ing 16, 34 and 50 classes is illustrated in table 2
with 1-2 example verbs per class. The table in-
dicates which classes were created by domain ex-
perts (BIO) and which by linguists (GEN). Each
class was associated with 1-30 member verbs10.
The total number of verbs is indicated in the table
(e.g. 10 for PERFORM class).
4.4 Measures
The clusters were evaluated against the gold stan-
dard using measures which are applicable to all the
8Purely semantic classes tend to be finer-grained than lex-
ical classes and not necessarily syntactic in nature. Only these
two classifications were found to be similar enough to our tar-
get classification to provide a useful starting point. Section 5
includes a summary of the similarities/differences between
our gold standard and these other classifications.
9Experts were allowed to discuss the problematic cases
to obtain maximal accuracy - hence no inter-annotator agree-
ment is reported.
10The minimum of 2 member verbs were required at the
coarser-grained levels of 16 and 34 classes.
classification methods and which deliver a numer-
ical value easy to interpret.
The first measure, the adjusted pairwise preci-
sion, evaluates clusters in terms of verb pairs:
APP = 1K
K?
i=1
num. of correct pairs in ki
num. of pairs in ki ?
|ki|?1
|ki|+1
APP is the average proportion of all within-
cluster pairs that are correctly co-assigned. Multi-
plied by a factor that increases with cluster size it
compensates for a bias towards small clusters.
The second measure is modified purity, a global
measure which evaluates the mean precision of
clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by nprevalent(K). Verbs
that do not take it are considered as errors. Clus-
ters where nprevalent(K) = 1 are disregarded as
not to introduce a bias towards singletons:
mPUR =
?
nprevalent(ki)?2
nprevalent(ki)
number of verbs
The third measure is the weighted class accu-
racy, the proportion of members of dominant clus-
ters DOM-CLUSTi within all classes ci.
ACC =
C?
i=1
verbs in DOM-CLUSTi
number of verbs
mPUR can be seen to measure the precision of
clusters and ACC the recall. We define an F mea-
sure as the harmonic mean of mPUR and ACC:
F = 2 ?mPUR ? ACCmPUR + ACC
The statistical significance of the results is mea-
sured by randomisation tests where verbs are
swapped between the clusters and the resulting
clusters are evaluated. The swapping is repeated
100 times for each output and the average avswaps
and the standard deviation ?swaps is measured.
The significance is the scaled difference signif =
(result? avswaps)/?swaps .
4.5 Results from Quantitative Evaluation
Table 3 shows the performance of the five clus-
tering methods for K = 42 clusters (as produced
by the NN method) at the 3 levels of gold stan-
dard classification. Although the two PLSA vari-
ants (particularly PLSA?=0.75) produce a fairly ac-
curate coarse grained classification, they perform
worse than all the other methods at the finer-
grained levels of gold standard, particularly ac-
cording to the global measures. Being based on
349
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
NN 81 86 39 53 64 74 62 67 54 67 73 69
IB 74 88 47 61 61 76 74 75 55 69 87 76
ID 79 89 37 52 63 78 65 70 53 70 77 73
PLSA/EM 55 72 49 58 43 53 61 57 35 47 66 55
PLSA?=0.75 65 71 68 70 53 48 76 58 41 34 77 47
Table 3: The performance of the NN, PLSA, IB and ID methods with Knn = 42 clusters
16 Classes 34 Classes 50 Classes
K APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
20 IB 74 77 66 71 60 56 86 67 54 48 93 63
17 ID 67 76 60 67 43 56 81 66 34 46 91 61
33 IB 78 87 52 65 69 75 81 77 61 67 93 77
ID 81 88 43 57 65 75 70 72 54 67 82 73
53 IB 71 87 41 55 61 78 66 71 54 72 79 75
ID 79 89 33 48 66 79 55 64 53 72 68 69
Table 4: The performance of IB and ID for the 3 levels of class hierarchy for informative values of K
pairwise similarities, NN shows mostly better per-
formance than IB and ID on the pairwise measure
APP but the global measures are better for IB and
ID. The differences are smaller in mPUR (yet sig-
nificant: 2? between NN and IB and 3? between
NN and ID) but more notable in ACC (which is
e.g. 8 ? 12% better for IB than for NN). Also
the F results suggest that the two information the-
oretic methods are better overall than the simple
NN method.
IB and ID also have the advantage (over NN) that
they can be used to produce a hierarchical verb
classification. Table 4 shows the results for IB and
ID for the informative values of K. The bold font
indicates the results when the match between the
values of K and the number of classes at the par-
ticular level of the gold standard is the closest.
IB is clearly better than ID at all levels of gold
standard. It yields its best results at the medium
level (34 classes) with K = 33: F = 77 and APP
= 69 (the results for ID are F = 72 and APP =
65). At the most fine-grained level (50 classes),
IB is equally good according to F with K = 33,
but APP is 8% lower. Although ID is occasion-
ally better than IB according to APP and mPUR
(see e.g. the results for 16 classes with K = 53)
this never happens in the case where the corre-
spondence between the number of gold standard
classes and the values of K is the closest. In other
words, the informative values of K prove really
informative for IB. The lower performance of ID
seems to be due to its tendency to create evenly
sized clusters.
All the methods perform significantly better
than our random baseline. The significance of the
results with respect to two swaps was at the 2?
level, corresponding to a 97% confidence that the
results are above random.
4.6 Qualitative Evaluation
We performed further, qualitative analysis of clus-
ters produced by the best performing method IB.
Consider the following clusters:
A: inject, transfect, microinfect, contransfect (6)
B: harvest, select, collect (7.1)
centrifuge, process, recover (7.2)
C: wash, rinse (4.1.1)
immunoblot (4.1.3)
overlap (5)
D: activate (1.1.1)
When looking at coarse-grained outputs, in-
terestingly, K as low as 8 learned the broad
distinction between biomedical and general lan-
guage verbs (the two verb types appeared only
rarely in the same clusters) and produced large se-
mantically meaningful groups of classes (e.g. the
coarse-grained classes EXPERIMENTAL PROCE-
DURES, TRANSFECT and COLLECT were mapped
together). K = 12 was sufficient to iden-
tify several classes with very particular syntax
One of them was TRANSFECT (see A above)
whose members were distinguished easily be-
cause of their typical SCFs (e.g. inject /trans-
fect/microinfect/contransfect X with/into Y).
On the other hand, even K = 53 could not iden-
tify classes with very similar (yet un-identical)
syntax. These included many semantically similar
sub-classes (e.g. the two sub-classes of COLLECT
350
shown in B whose members take similar NP and
PP SCFs). However, also a few semantically dif-
ferent verbs clustered wrongly because of this rea-
son, such as the ones exemplified in C. In C, im-
munoblot (from the LABEL class) is still somewhat
related to wash and rinse (the WASH class) because
they all belong to the larger EXPERIMENTAL PRO-
CEDURES class, but overlap (from the PROCESS
class) shows up in the cluster merely because of
syntactic idiosyncracy.
While parser errors caused by the challeng-
ing biomedical texts were visible in some SCFs
(e.g. looking at a sample of SCFs, some adjunct
instances were listed in the argument slots of the
frames), the cases where this resulted in incorrect
classification were not numerous11.
One representative singleton resulting from
these errors is exemplified in D. Activate ap-
pears in relatively complicated sentence struc-
tures, which gives rise to incorrect SCFs. For ex-
ample, MECs cultured on 2D planar substrates
transiently activate MAP kinase in response to
EGF, whereas... gets incorrectly analysed as SCF
NP-NP, while The effect of the constitutively ac-
tivated ARF6-Q67L mutant was investigated... re-
ceives the incorrect SCF analysis NP-SCOMP. Most
parser errors are caused by unknown domain-
specific words and phrases.
5 Discussion
Due to differences in the task and experimental
setup, direct comparison of our results with pre-
viously published ones is impossible. The clos-
est possible comparison point is (Korhonen et al,
2003) which reported 50-59% mPUR and 15-19%
APP on using IB to assign 110 polysemous (gen-
eral language) verbs into 34 classes. Our results
are substantially better, although we made no ef-
fort to restrict our scope to monosemous verbs12
and although we focussed on a linguistically chal-
lenging domain.
It seems that our better result is largely due
to the higher uniformity of verb senses in the
biomedical domain. We could not investigate this
effect systematically because no manually sense
11This is partly because the mistakes of the parser are
somewhat consistent (similar for similar verbs) and partly be-
cause the SCFs gather data from hundreds of corpus instances,
many of which are analysed correctly.
12Most of our test verbs are polysemous according to
WordNet (WN) (Miller, 1990), but this is not a fully reliable
indication because WN is not specific to this domain.
annotated data (or a comprehensive list of verb
senses) exists for the domain. However, exami-
nation of a number of corpus instances suggests
that the use of verbs is fairly conventionalized in
our data13. Where verbs show less sense varia-
tion, they show less SCF variation, which aids the
discovery of verb classes. Korhonen et al (2003)
observed the opposite with general language data.
We examined, class by class, to what extent our
domain-specific gold standard differs from the re-
lated general (Levin, 1993) and domain classifica-
tions (Spasic et al, 2005; Friedman et al, 2002)
(recall that the latter were purely semantic clas-
sifications as no lexical ones were available for
biomedicine):
33 (of the 50) classes in the gold standard are
biomedical. Only 6 of these correspond (fully or
mostly) to the semantic classes in the domain clas-
sifications. 17 are unrelated to any of the classes in
Levin (1993) while 16 bear vague resemblance to
them (e.g. our TRANSPORT verbs are also listed
under Levin?s SEND verbs) but are too different
(semantically and syntactically) to be combined.
17 (of the 50) classes are general (scientific)
classes. 4 of these are absent in Levin (e.g. QUAN-
TITATE). 13 are included in Levin, but 8 of them
have a more restricted sense (and fewer members)
than the corresponding Levin class. Only the re-
maining 5 classes are identical (in terms of mem-
bers and their properties) to Levin classes.
These results highlight the importance of build-
ing or tuning lexical resources specific to different
domains, and demonstrate the usefulness of auto-
matic lexical acquisition for this work.
6 Conclusion
This paper has shown that current domain-
independent NLP and ML technology can be used
to automatically induce a relatively high accu-
racy verb classification from a linguistically chal-
lenging corpus of biomedical texts. The lexical
classification resulting from our work is strongly
domain-specific (it differs substantially from pre-
vious ones) and it can be readily used to aid BIO-
NLP. It can provide useful material for investigat-
ing the syntax and semantics of verbs in biomed-
ical data or for supplementing existing domain
lexical resources with additional information (e.g.
13The different sub-domains of the biomedical domain
may, of course, be even more conventionalized (Friedman et
al., 2002).
351
semantic classifications with additional member
verbs). Lexical resources enriched with verb class
information can, in turn, better benefit practical
tasks such as parsing, predicate-argument identifi-
cation, event extraction, identification of biomedi-
cal relation patterns, among others.
In the future, we plan to improve the accu-
racy of automatic classification by seeding it with
domain-specific information (e.g. using named en-
tity recognition and anaphoric linking techniques
similar to those of Vlachos et al (2006)). We also
plan to conduct a bigger experiment with a larger
number of verbs and demonstrate the usefulness of
the bigger classification for practical BIO-NLP ap-
plication tasks. In addition, we plan to apply sim-
ilar technology to other interesting domains (e.g.
tourism, law, astronomy). This will not only en-
able us to experiment with cross-domain lexical
class variation but also help to determine whether
automatic acquisition techniques benefit, in gen-
eral, from domain-specific tuning.
Acknowledgement
We would like to thank Yoko Mizuta, Shoko
Kawamato, Sven Demiya, and Parantu Shah for
their help in creating the gold standard.
References
C. Brew and S. Schulte im Walde. 2002. Spectral
clustering for German verbs. In Conference on Em-
pirical Methods in Natural Language Processing,
Philadelphia, USA.
E. J. Briscoe and J. Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In 5th ACL
Conference on Applied Natural Language Process-
ing, pages 356?363, Washington DC.
E. J. Briscoe and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In 3rd Interna-
tional Conference on Language Resources and Eval-
uation, pages 1499?1504, Las Palmas, Gran Ca-
naria.
A. G. Dimitrov and J. P. Miller. 2001. Neural coding
and decoding: communication channels and quanti-
zation. Network: Computation in Neural Systems,
12(4):441?472.
B. Dorr. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?325.
C. Friedman, P. Kra, and A. Rzhetsky. 2002. Two
biomedical sublanguages: a description based on the
theories of Zellig Harris. Journal of Biomedical In-
formatics, 35(4):222?235.
V. Hatzivassiloglou and W. Weng. 2002. Learning an-
chor verbs for biological interaction patterns from
published text articles. International Journal of
Medical Inf., 67:19?32.
L. Hirschman, J. C. Park, J. Tsujii, L. Wong, and C. H.
Wu. 2002. Accomplishments and challenges in lit-
erature data mining for biology. Journal of Bioin-
formatics, 18(12):1553?1561.
T. Hoffman. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177?196.
R. Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, Massachusetts.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame distri-
butions semantically. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 64?71, Sapporo, Japan.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
M. Lease and E. Charniak. 2005. Parsing biomedical
literature. In Second International Joint Conference
on Natural Language Processing, pages 58?69.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
B. Levin. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago.
P. Merlo and S. Stevenson. 2001. Automatic verb
classification based on statistical distributions of
argument structure. Computational Linguistics,
27(3):373?408.
G. A. Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using
a probabilistic class-based lexicon for lexical am-
biguity resolution. In 18th International Confer-
ence on Computational Linguistics, pages 649?655,
Saarbru?cken, Germany.
I. Spasic, S. Ananiadou, and J. Tsujii. 2005. Master-
class: A case-based reasoning system for the clas-
sification of biomedical terms. Journal of Bioinfor-
matics, 21(11):2748?2758.
N. Tishby, F. C. Pereira, and W. Bialek. 1999. The
information bottleneck method. In Proc. of the
37th Annual Allerton Conference on Communica-
tion, Control and Computing, pages 368?377.
A. Vlachos, C. Gasperin, I. Lewin, and E. J. Briscoe.
2006. Bootstrapping the recognition and anaphoric
linking of named entitites in drosophila articles. In
Pacific Symposium in Biocomputing.
352
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 912?919,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival
Subcategorization Frames from Corpora
Judita Preiss, Ted Briscoe, and Anna Korhonen
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
Judita.Preiss, Ted.Briscoe, Anna.Korhonen@cl.cam.ac.uk
Abstract
This paper describes the first system for
large-scale acquisition of subcategorization
frames (SCFs) from English corpus data
which can be used to acquire comprehen-
sive lexicons for verbs, nouns and adjectives.
The system incorporates an extensive rule-
based classifier which identifies 168 verbal,
37 adjectival and 31 nominal frames from
grammatical relations (GRs) output by a ro-
bust parser. The system achieves state-of-
the-art performance on all three sets.
1 Introduction
Research into automatic acquisition of lexical in-
formation from large repositories of unannotated
text (such as the web, corpora of published text,
etc.) is starting to produce large scale lexical re-
sources which include frequency and usage infor-
mation tuned to genres and sublanguages. Such
resources are critical for natural language process-
ing (NLP), both for enhancing the performance of
state-of-art statistical systems and for improving the
portability of these systems between domains.
One type of lexical information with particular
importance for NLP is subcategorization. Access
to an accurate and comprehensive subcategoriza-
tion lexicon is vital for the development of success-
ful parsing technology (e.g. (Carroll et al, 1998),
important for many NLP tasks (e.g. automatic verb
classification (Schulte im Walde and Brew, 2002))
and useful for any application which can benefit
from information about predicate-argument struc-
ture (e.g. Information Extraction (IE) ((Surdeanu et
al., 2003)).
The first systems capable of automatically learn-
ing a small number of verbal subcategorization
frames (SCFs) from unannotated English corpora
emerged over a decade ago (Brent, 1991; Manning,
1993). Subsequent research has yielded systems for
English (Carroll and Rooth, 1998; Briscoe and Car-
roll, 1997; Korhonen, 2002) capable of detecting
comprehensive sets of SCFs with promising accu-
racy and demonstrated success in application tasks
(e.g. (Carroll et al, 1998; Korhonen et al, 2003)).
Recently, a large publicly available subcategoriza-
tion lexicon was produced using such technology
which contains frame and frequency information for
over 6,300 English verbs ? the VALEX lexicon (Ko-
rhonen et al, 2006).
While there has been considerable work in the
area, most of it has focussed on verbs. Although
verbs are the richest words in terms of subcatego-
rization and although verb SCF distribution data is
likely to offer the greatest boost in parser perfor-
mance, accurate and comprehensive knowledge of
the many noun and adjective SCFs in English could
improve the accuracy of parsing at several levels
(from tagging to syntactic and semantic analysis).
Furthermore the selection of the correct analysis
from the set returned by a parser which does not ini-
tially utilize fine-grained lexico-syntactic informa-
tion can depend on the interaction of conditional
probabilities of lemmas of different classes occur-
912
ring with specific SCFs. For example, a) and b) be-
low indicate the most plausible analyses in which the
sentential complement attaches to the noun and verb
respectively
a) Kim (VP believes (NP the evidence (Scomp that
Sandy was present)))
b) Kim (VP persuaded (NP the judge) (Scomp that
Sandy was present))
However, both a) and b) consist of an identical
sequence of coarse-grained lexical syntactic cate-
gories, so correctly ranking them requires learn-
ing that P (NP | believe).P (Scomp | evidence) >
P (NP&Scomp | believe).P (None | evidence)
and P (NP | persuade).P (Scomp | judge) <
P (NP&Scomp | persuade).P (None | judge). If
we acquired frames and frame frequencies for all
open-class predicates taking SCFs using a single sys-
tem applied to similar data, we would have a better
chance of modeling such interactions accurately.
In this paper we present the first system for large-
scale acquisition of SCFs from English corpus data
which can be used to acquire comprehensive lexi-
cons for verbs, nouns and adjectives. The classifier
incorporates 168 verbal, 37 adjectival and 31 nomi-
nal SCF distinctions. An improved acquisition tech-
nique is used which expands on the ideas Yallop et
al. (2005) recently explored for a small experiment
on adjectival SCF acquisition. It involves identifying
SCFs on the basis of grammatical relations (GRs) in
the output of the RASP (Robust Accurate Statistical
Parsing) system (Briscoe et al, 2006).
As detailed later, the system performs better with
verbs than previous comparable state-of-art systems,
achieving 68.9 F-measure in detecting SCF types. It
achieves similarly good performance with nouns and
adjectives (62.2 and 71.9 F-measure, respectively).
Additionally, we have developed a tool for lin-
guistic annotation of SCFs in corpus data aimed at
alleviating the process of obtaining training and test
data for subcategorization acquisition. The tool in-
corporates an intuitive interface with the ability to
significantly reduce the number of frames presented
to the user for each sentence.
We introduce the new system for SCF acquisition
in section 2. Details of the experimental evaluation
are supplied in section 3. Section 4 provides discus-
sion of our results and future work, and section 5
concludes.
2 Description of the System
A common strategy in existing large-scale SCF ac-
quisition systems (e.g. (Briscoe and Carroll, 1997))
is to extract SCFs from parse trees, introducing an
unnecessary dependence on the details of a particu-
lar parser. In our approach SCFs are extracted from
GRs ? representations of head-dependent relations
which are more parser/grammar independent but at
the appropriate level of abstraction for extraction of
SCFs.
A similar approach was recently motivated and
explored by Yallop et al (2005). A decision-tree
classifier was developed for 30 adjectival SCF types
which tests for the presence of GRs in the GR out-
put of the RASP (Robust Accurate Statistical Pars-
ing) system (Briscoe and Carroll, 2002). The results
reported with 9 test adjectives were promising (68.9
F-measure in detecting SCF types).
Our acquisition process consists of four main
steps: 1) extracting GRs from corpus data, 2) feeding
the GR sets as input to a rule-based classifier which
incrementally matches them with the corresponding
SCFs, 3) building lexical entries from the classified
data, and 4) filtering those entries to obtain a more
accurate lexicon. The details of these steps are pro-
vided in the subsequent sections.
2.1 Obtaining Grammatical Relations
We obtain the GRs using the recent, second release
of the RASP toolkit (Briscoe et al, 2006). RASP is a
modular statistical parsing system which includes a
tokenizer, tagger, lemmatizer, and a wide-coverage
unification-based tag-sequence parser. We use the
standard scripts supplied with RASP to output the set
of GRs for the most probable analysis returned by the
parser or, in the case of parse failures, the GRs for
the most likely sequence of subanalyses. The GRs
are organized as a subsumption hierarchy as shown
in Figure 1.
The dependency relationships which the GRs em-
body correspond closely to the head-complement
structure which subcategorization acquisition at-
tempts to recover, which makes GRs ideal input to
the SCF classifier. Consider the arguments of easy
913
dependent
ta arg mod det aux conj
mod arg
ncmod xmod cmod pmod
subj dobj
subj comp
ncsubj xsubj csubj obj pcomp clausal
dobj obj2 iobj xcomp ccomp
Figure 1: The GR hierarchy used by RASP
?
?
?
?
?
?
?
?
SUBJECT NP 1 ,
ADJ-COMPS
?
PP
[
PVAL for
NP 3
]
,
VP
?
?
?
?
MOOD to-infinitive
SUBJECT 3
OMISSION 1
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Feature structure for SCF
adj-obj-for-to-inf
(|These:1_DD2| |example+s:2_NN2| |of:3_IO|
|animal:4_JJ| |senses:5_NN2| |be+:6_VBR|
|relatively:7_RR| |easy:8_JJ| |for:9_IF|
|we+:10_PPIO2| |to:11_TO| |comprehend:12_VV0|)
...
xcomp(_ be+[6] easy:[8])
xcomp(to[11] be+[6] comprehend:[12])
ncsubj(be+[6] example+s[2] _)
ncmod(for[9] easy[8] we+[10])
ncsubj(comprehend[12] we+[10], _)
...
Figure 3: GRs from RASP for adj-obj-for-to-inf
in the sentence: These examples of animal senses
are relatively easy for us to comprehend as they are
not too far removed from our own experience. Ac-
cording to the COMLEX classification, this is an ex-
ample of the frame adj-obj-for-to-inf, shown in
Figure 2, (using AVM notation in place of COMLEX
s-expressions). Part of the output of RASP for this
sentence is shown in Figure 3.
Each instantiated GR in Figure 3 corresponds to
one or more parts of the feature structure in Fig-
ure 2. xcomp( be[6] easy[8]) establishes be[6]
as the head of the VP in which easy[8] occurs as
a complement. The first (PP)-complement is for us,
as indicated by ncmod(for[9] easy[8] we+[10]),
with for as PFORM and we+ (us) as NP. The sec-
ond complement is represented by xcomp(to[11]
be+[6] comprehend[12]): a to-infinitive VP. The
xcomp ?Y : pos=vb,val=be ?X : pos=adj
xcomp ?S : val=to ?Y : pos=vb,val=be ?W : pos=VV0
ncsubj ?Y : pos=vb,val=be ?Z : pos=noun
ncmod ?T : val=for ?X : pos=adj ?Y: pos=pron
ncsubj ?W : pos=VV0 ?V : pos=pron
Figure 4: Pattern for frame adj-obj-for-to-inf
NP headed by examples is marked as the subject
of the frame by ncsubj(be[6] examples[2]), and
ncsubj(comprehend[12] we+[10]) corresponds to
the coindexation marked by 3 : the subject of the
VP is the NP of the PP. The only part of the feature
structure which is not represented by the GRs is coin-
dexation between the omitted direct object 1 of the
VP-complement and the subject of the whole clause.
2.2 SCF Classifier
SCF Frames
The SCFs recognized by the classifier were ob-
tained by manually merging the frames exempli-
fied in the COMLEX Syntax (Grishman et al, 1994),
ANLT (Boguraev et al, 1987) and/or NOMLEX
(Macleod et al, 1997) dictionaries and including
additional frames found by manual inspection of
unclassifiable examples during development of the
classifier. These consisted of e.g. some occurrences
of phrasal verbs with complex complementation and
with flexible ordering of the preposition/particle,
some non-passivizable words with a surface direct
object, and some rarer combinations of governed
preposition and complementizer combinations.
The frames were created so that they abstract
over specific lexically-governed particles and prepo-
sitions and specific predicate selectional preferences
914
but include some derived semi-predictable bounded
dependency constructions.
Classifier
The classifier operates by attempting to match the
set of GRs associated with each sentence against one
or more rules which express the possible mappings
from GRs to SCFs. The rules were manually devel-
oped by examining a set of development sentences
to determine which relations were actually emitted
by the parser for each SCF.
In our rule representation, a GR pattern is a set of
partially instantiated GRs with variables in place of
heads and dependents, augmented with constraints
that restrict the possible instantiations of the vari-
ables. A match is successful if the set of GRs for
a sentence can be unified with any rule. Unifica-
tion of sentence GRs and a rule GR pattern occurs
when there is a one-to-one correspondence between
sentence elements and rule elements that includes a
consistent mapping from variables to values.
A sample pattern for matching
adj-obj-for-to-inf can be seen in Fig-
ure 4. Each element matches either an empty GR
slot ( ), a variable with possible constraints on part
of speech (pos) and word value (val), or an already
instantiated variable. Unlike in Yallop?s work (Yal-
lop et al, 2005), our rules are declarative rather than
procedural and these rules, written independently
of the acquisition system, are expanded by the
system in a number of ways prior to execution. For
example, the verb rules which contain an ncsubj
relation will not contain one inside an embedded
clause. For verbs, the basic rule set contains 248
rules but automatic expansion gives rise to 1088
classifier rules for verbs.
Numerous approaches were investigated to allow
an efficient execution of the system: for example, for
each target word in a sentence, we initially find the
number of ARGument GRs (see Figure 1) containing
it in head position, as the word must appear in ex-
actly the same set in a matching rule. This allows
us to discard all patterns which specify a different
number of GRs: for example, for verbs each group
only contains an average of 109 patterns.
For a further increase in speed, both the sentence
GRs and the GRs within the patterns are ordered (ac-
cording to frequency) and matching is performed us-
ing a backing off strategy allowing us to exploit the
relatively low number of possible GRs (compared
to the number of possible rules). The system exe-
cutes on 3500 sentences in approx. 1.5 seconds of
real time on a machine with a 3.2 GHz Intel Xenon
processor and 4GB of RAM.
Lexicon Creation and Filtering
Lexical entries are constructed for each word and
SCF combination found in the corpus data. Each lex-
ical entry includes the raw and relative frequency of
the SCF with the word in question, and includes var-
ious additional information e.g. about the syntax of
detected arguments and the argument heads in dif-
ferent argument positions1.
Finally the entries are filtered to obtain a more
accurate lexicon. A way to maximise the accu-
racy of the lexicon would be to smooth (correct) the
acquired SCF distributions with back-off estimates
based on lexical-semantic classes of verbs (Korho-
nen, 2002) (see section 4) before filtering them.
However, in this first experiment with the new sys-
tem we filtered the entries directly so that we could
evaluate the performance of the new classifier with-
out any additional modules. For the same reason, the
filtering was done by using a very simple method:
by setting empirically determined thresholds on the
relative frequencies of SCFs.
3 Experimental Evaluation
3.1 Data
In order to test the accuracy of our system, we se-
lected a set of 183 verbs, 30 nouns and 30 adjec-
tives for experimentation. The words were selected
at random, subject to the constraint that they exhib-
ited multiple complementation patterns and had a
sufficient number of corpus occurrences (> 150) for
experimentation. We took the 100M-word British
National Corpus (BNC) (Burnard, 1995), and ex-
tracted all sentences containing an occurrence of one
of the test words. The sentences were processed us-
ing the SCF acquisition system described in the pre-
vious section. The citations from which entries were
derived totaled approximately 744K for verbs and
219K for nouns and adjectives, respectively.
1The lexical entries are similar to those in the VALEX lexi-
con. See (Korhonen et al, 2006) for a sample entry.
915
3.2 Gold Standard
Our gold standard was based on a manual analysis
of some of the test corpus data, supplemented with
additional frames from the ANLT, COMLEX, and/or
NOMLEX dictionaries. The gold standard for verbs
was available, but it was extended to include addi-
tional SCFs missing from the old system. For nouns
and adjectives the gold standard was created. For
each noun and adjective, 100-300 sentences from the
BNC (an average of 267 per word) were randomly
extracted. The resulting c. 16K sentences were then
manually associated with appropriate SCFs, and the
SCF frequency counts were recorded.
To alleviate the manual analysis we developed
a tool which first uses the RASP parser with some
heuristics to reduce the number of SCF presented,
and then allows an annotator to select the preferred
choice in a window. The heuristics reduced the av-
erage number of SCFs presented alongside each sen-
tence from 52 to 7. The annotator was also presented
with an example sentence of each SCF and an intu-
itive name for the frame, such as PRED (e.g. Kim
is silly). The program includes an option to record
that particular sentences could not (initially) be clas-
sified. A screenshot of the tool is shown in Figure 5.
The manual analysis was done by two linguists;
one who did the first annotation for the whole data,
and another who re-evaluated and corrected some of
the initial frame assignments, and classified most of
the data left unclassified by the first annotator2). A
total of 27 SCF types were found for the nouns and
30 for the adjectives in the annotated data. The av-
erage number of SCFs taken by nouns was 9 (with
the average of 2 added from dictionaries to supple-
ment the manual annotation) and by adjectives 11
(3 of which were from dictionaries). The latter are
rare and may not be exemplified in the data given the
extraction system.
3.3 Evaluation Measures
We used the standard evaluation metrics to evaluate
the accuracy of the SCF lexicons: type precision (the
percentage of SCF types that the system proposes
2The process precluded measurements of inter-annotator
agreement, but this was judged less important than the enhanced
accuracy of the gold standard data.
Figure 5: Sample screen of the annotation tool
which are correct), type recall (the percentage of SCF
types in the gold standard that the system proposes)
and the F-measure which is the harmonic mean of
type precision and recall.
We also compared the similarity between the ac-
quired unfiltered3 SCF distributions and gold stan-
dard SCF distributions using various measures of
distributional similarity: the Spearman rank corre-
lation (RC), Kullback-Leibler distance (KL), Jensen-
Shannon divergence (JS), cross entropy (CE), skew
divergence (SD) and intersection (IS). The details of
these measures and their application to subcatego-
rization acquisition can be found in (Korhonen and
Krymolowski, 2002).
Finally, we recorded the total number of gold
standard SCFs unseen in the system output, i.e. the
type of false negatives which were never detected
by the classifier.
3.4 Results
Table 1 includes the average results for the 183
verbs. The first column shows the results for Briscoe
and Carroll?s (1997) (B&C) system when this sys-
tem is run with the original classifier but a more
recent version of the parser (Briscoe and Carroll,
2002) and the same filtering technique as our new
system (thresholding based on the relative frequen-
cies of SCFs). The classifier of B&C system is com-
parable to our classifier in the sense that it targets al-
most the same set of verbal SCFs (165 out of the 168;
the 3 additional ones are infrequent in language and
thus unlikely to affect the comparison). The second
column shows the results for our new system (New).
3No threshold was applied to remove the noisy SCFs from
the distributions.
916
Verbs - Method
Measures B&C New
Precision (%) 47.3 81.8
Recall (%) 40.4 59.5
F-measure 43.6 68.9
KL 3.24 1.57
JS 0.20 0.11
CE 4.85 3.10
SD 1.39 0.74
RC 0.33 0.66
IS 0.49 0.76
Unseen SCFs 28 17
Table 1: Average results for verbs
The figures show that the new system clearly per-
forms better than the B&C system. It yields 68.9 F-
measure which is a 25.3 absolute improvement over
the B&C system. The better performance can be ob-
served on all measures, but particularly on SCF type
precision (81.8% with our system vs. 47.3% with the
B&C system) and on measures of distributional sim-
ilarity. The clearly higher IS (0.76 vs. 0.49) and the
fewer gold standard SCFs unseen in the output of the
classifier (17 vs. 28) indicate that the new system is
capable of detecting a higher number of SCFs.
The main reason for better performance is the
ability of the new system to detect a number of chal-
lenging or complex SCFs which the B&C system
could not detect4. The improvement is partly at-
tributable to more accurate parses produced by the
second release of RASP and partly to the improved
SCF classifier developed here. For example, the new
system is now able to distinguish predicative PP ar-
guments, such as I sent him as a messenger from the
wider class of referential PP arguments, supporting
discrimination of several syntactically similar SCFs
with distinct semantics.
Running our system on the adjective and noun test
data yielded the results summarized in Table 2. The
F-measure is lower for nouns (62.2) than for verbs
(68.9); for adjectives it is slightly better (71.9).5
4The results reported here for the B&C system are lower
than those recently reported in (Korhonen et al, 2006) for the
same set of 183 test verbs. This is because we use an improved
gold standard. However, the results for the B&C system re-
ported using the less ambitious gold standard are still less ac-
curate (58.6 F-measure) than the ones reported here for the new
system.
5The results for different word classes are not directly com-
parable because they are affected by the total number of SCFs
evaluated for each word class, which is higher for verbs and
Measures Nouns Adjectives
Precision (%) 91.2 95.5
Recall (%) 47.2 57.6
F-measure 62.2 71.9
KL 0.91 0.69
JS 0.09 0.05
CE 2.03 2.01
SD 0.48 0.36
RC 0.70 0.77
IS 0.62 0.72
Unseen SCFs 15 7
Table 2: Average results for nouns and adjectives
The noun and adjective classifiers yield very high
precision compared to recall. The lower recall fig-
ures are mostly due to the higher number of gold
standard SCFs unseen in the classifier output (rather
than, for example, the filtering step). This is par-
ticularly evident for nouns for which 15 of the 27
frames exemplified in the gold standard are missing
in the classifier output. For adjectives only 7 of the
30 gold standard SCFs are unseen, resulting in better
recall (57.6% vs. 47.2% for nouns).
For verbs, subcategorization acquisition perfor-
mance often correlates with the size of the input
data to acquisition (the more data, the better perfor-
mance). When considering the F-measure results for
the individual words shown in Table 3 there appears
to be little such correlation for nouns and adjectives.
For example, although there are individual high fre-
quency nouns with high performance (e.g. plan,
freq. 5046, F 90.9) and low frequency nouns with
low performance (e.g. characterisation, freq. 91, F
40.0), there are also many nouns which contradict
the trend (compare e.g. answer, freq. 2510, F 50.0
with fondness, freq. 71, F 85.7).6
Although the SCF distributions for nouns and ad-
jectives appear Zipfian (i.e. the most frequent frames
are highly probable, but most frames are infre-
quent), the total number of SCFs per word is typi-
cally smaller than for verbs, resulting in better resis-
tance to sparse data problems.
There is, however, a clear correlation between
the performance and the type of gold standard SCFs
taken by individual words. Many of the gold stan-
lower for nouns and adjectives. This particularly applies to the
sensitive measures of distributional similarity.
6The frequencies here refer to the number of citations suc-
cessfully processed by the parser and the classifier.
917
Noun F Adjective F
abundance 75.0 able 66.7
acknowledgement 47.1 angry 62.5
answer 50.0 anxious 82.4
anxiety 53.3 aware 87.5
apology 50.0 certain 73.7
appearance 46.2 clear 77.8
appointment 66.7 curious 57.1
belief 76.9 desperate 83.3
call 58.8 difficult 77.8
characterisation 40.0 doubtful 63.6
communication 40.0 eager 83.3
condition 66.7 easy 66.7
danger 76.9 generous 57.1
decision 70.6 imperative 81.8
definition 42.8 important 60.9
demand 66.7 impractical 71.4
desire 71.4 improbable 54.6
doubt 66.7 insistent 80.0
evidence 66.7 kind 66.7
examination 54.6 likely 66.7
experimentation 60.0 practical 88.9
fondness 85.7 probable 80.0
message 66.7 sure 84.2
obsession 54.6 unaware 85.7
plan 90.9 uncertain 60.0
provision 70.6 unclear 63.2
reminder 63.2 unimportant 61.5
rumour 61.5 unlikely 69.6
temptation 71.4 unspecified 50.0
use 60.0 unsure 90.0
Table 3: System performance for each test noun and
adjective
dard nominal and adjectival SCFs unseen by the
classifier involve complex complementation patterns
which are challenging to extract, e.g. those exem-
plified in The argument of Jo with Kim about Fido
surfaced, Jo?s preference that Kim be sacked sur-
faced, and that Sandy came is certain. In addition,
many of these SCFs unseen in the data are also very
low in frequency, and some may even be true nega-
tives (recall that the gold standard was supplemented
with additional SCFs from dictionaries, which may
not necessarily appear in the test data).
The main problem is that the RASP parser system-
atically fails to select the correct analysis for some
SCFs with nouns and adjectives regardless of their
context of occurrence. In future work, we hope to al-
leviate this problem by using the weighted GR output
from the top n-ranked parses returned by the parser
as input to the SCF classifier.
4 Discussion
The current system needs refinement to alleviate the
bias against some SCFs introduced by the parser?s
unlexicalized parse selection model. We plan to in-
vestigate using weighted GR output with the clas-
sifier rather than just the GR set from the highest
ranked parse. Some SCF classes also need to be fur-
ther resolved mainly to differentiate control options
with predicative complementation. This requires a
lexico-semantic classification of predicate classes.
Experiments with Briscoe and Carroll?s system
have shown that it is possible to incorporate some
semantic information in the acquisition process us-
ing a technique that smooths the acquired SCF dis-
tributions using back-off (i.e. probability) estimates
based on lexical-semantic classes of verbs (Korho-
nen, 2002). The estimates help to correct the ac-
quired SCF distributions and predict SCFs which are
rare or unseen e.g. due to sparse data. They could
also form the basis for predicting control of predica-
tive complements.
We plan to modify and extend this technique for
the new system and use it to improve the perfor-
mance further. The technique has so far been applied
to verbs only, but it can also be applied to nouns
and adjectives because they can also be classified on
lexical-semantic grounds. For example, the adjec-
tive simple belongs to the class of EASY adjectives,
and this knowledge can help to predict that it takes
similar SCFs to the other class members and that
control of ?understood? arguments will pattern with
easy (e.g. easy, difficult, convenient): The problem
will be simple for John to solve, For John to solve
the problem will be simple, The problem will be sim-
ple to solve, etc.
Further research is needed before highly accurate
lexicons encoding information also about semantic
aspects of subcategorization (e.g. different predicate
senses, the mapping from syntactic arguments to
semantic representation of argument structure, se-
lectional preferences on argument heads, diathesis
alternations, etc.) can be obtained automatically.
However, with the extensions suggested above, the
system presented here is sufficiently accurate for
building an extensive SCF lexicon capable of sup-
porting various NLP application tasks. Such a lex-
icon will be built and distributed for research pur-
918
poses along with the gold standard described here.
5 Conclusion
We have described the first system for automatically
acquiring verbal, nominal and adjectival subcat-
egorization and associated frequency information
from English corpora, which can be used to build
large-scale lexicons for NLP purposes. We have
also described a new annotation tool for producing
training and test data for the task. The acquisition
system, which is capable of distinguishing 168
verbal, 37 adjectival and 31 nominal frames, clas-
sifies corpus occurrences to SCFs on the basis of
GRs produced by a robust statistical parser. The
information provided by GRs closely matches the
structure that subcategorization acquisition seeks
to recover. Our experiment shows that the system
achieves state-of-the-art performance with each
word class. The discussion suggests ways in which
we could improve the system further before using it
to build a large subcategorization lexicon capable of
supporting various NLP application tasks.
Acknowledgements
This work was supported by the Royal Society and
UK EPSRC project ?Accurate and Comprehensive
Lexical Classification for Natural Language Pro-
cessing Applications? (ACLEX). We would like to
thank Diane Nicholls for her help during this work.
References
B. Boguraev, J. Carroll, E. J. Briscoe, D. Carter, and C. Grover.
1987. The derivation of a grammatically-indexed lexicon
from the Longman Dictionary of Contemporary English. In
Proc. of the 25th Annual Meeting of ACL, pages 193?200,
Stanford, CA.
M. Brent. 1991. Automatic acquisition of subcategorization
frames from untagged text. In Proc. of the 29th Meeting of
ACL, pages 209?214.
E. J. Briscoe and J. Carroll. 1997. Automatic Extraction of
Subcategorization from Corpora. In Proc. of the 5th ANLP,
Washington DC, USA.
E. J. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proc. of the 3rd LREC, pages
1499?1504, Las Palmas, Canary Islands, May.
E. J. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the rasp system. In Proc. of the COLING/ACL
2006 Interactive Presentation Sessions, Sydney, Australia.
L. Burnard, 1995. The BNC Users Reference Guide. British
National Corpus Consortium, Oxford, May.
G. Carroll and M. Rooth. 1998. Valence induction with a head-
lexicalized pcfg. In Proc. of the 3rd Conference on EMNLP,
Granada, Spain.
J. Carroll, G. Minnen, and E. J. Briscoe. 1998. Can Subcat-
egorisation Probabilities Help a Statistical Parser? In Pro-
ceedings of the 6th ACL/SIGDAT Workshop on Very Large
Corpora, pages 118?126, Montreal, Canada.
R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX
Syntax: Building a Computational Lexicon. In COLING,
Kyoto.
A. Korhonen and Y. Krymolowski. 2002. On the Robustness
of Entropy-Based Similarity Measures in Evaluation of Sub-
categorization Acquisition Systems. In Proc. of the Sixth
CoNLL, pages 91?97, Taipei, Taiwan.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003. Clustering
Polysemic Subcategorization Frame Distributions Semanti-
cally. In Proc. of the 41st Annual Meeting of ACL, pages
64?71, Sapporo, Japan.
A. Korhonen, Y. Krymolowski, and E. J. Briscoe. 2006. A
large subcategorization lexicon for natural language process-
ing applications. In Proc. of the 5th LREC, Genova, Italy.
A. Korhonen. 2002. Subcategorization acquisition. Ph.D. the-
sis, University of Cambridge Computer Laboratory.
C. Macleod, A. Meyers, R. Grishman, L. Barrett, and R. Reeves.
1997. Designing a dictionary of derived nominals. In Proc.
of RANLP, Tzigov Chark, Bulgaria.
C. Manning. 1993. Automatic Acquisition of a Large Subcat-
egorization Dictionary from Corpora. In Proc. of the 31st
Meeting of ACL, pages 235?242.
S. Schulte im Walde and C. Brew. 2002. Inducing german se-
mantic verb classes from purely syntactic subcategorisation
information. In Proc. of the 40th Annual Meeting of ACL,
Philadephia, USA.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proc. of the 41st Annual Meeting of ACL, Sapporo.
J. Yallop, A. Korhonen, and E. J. Briscoe. 2005. Auto-
matic acquisition of adjectival subcategorization from cor-
pora. In Proc. of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 614?621, Ann Arbor,
Michigan.
919
Statistical Filtering and Subcategorization Frame Acquisition 
Anna Korhonen and  Genev ieve  Gor re l l  
Computer Laboratory, University of Cambridge 
Pembroke Street, Cambridge CB2 3QG, UK 
alk23@cl, cam. ac. uk, genevieve, gorrel l@netdecis ions,  co. uk 
Diana  McCar thy  
School of Cognitive and Computing Sciences 
University of Sussex, Brighton, BN1 9QH, UK 
dianam@cogs, usx. ac.  uk 
Abst rac t  
Research "into the automatic acquisition of 
subcategorization frames (SCFS) from corpora 
is starting to produce large-scale computa- 
tional lexicons which include valuable fre- 
quency information. However, the accuracy 
of the resulting lexicons shows room for im- 
provement. One significant source of error 
lies in the statistical filtering used by some re- 
searchers to remove noise from automatically 
acquired subcategorization frames. In this pa- 
per, we compare three different approaches to 
filtering out spurious hypotheses. Two hy- 
pothesis tests perform poorly, compared to 
filtering frames on the basis of relative fre- 
quency. We discuss reasons for this and con- 
sider directions for future research. 
1 In t roduct ion  
Subcategorization information is vital for suc- 
cessful parsing, however, manual develop- 
ment of large subcategorized lexicons has 
proved difficult because predicates change be- 
haviour between sublanguages, domains and 
over time. Additionally, manually devel- 
oped sucategorization lexicons do not provide 
the relative frequency of different SCFs for a 
given predicate, ssential in a probabilistic ap- 
proach. 
Over the past years acquiring subcatego- 
rization dictionaries from textual corpora has 
become increasingly popular. The different 
approaches (e.g. Brent, !991, 1993; Ushioda 
et al, 1993; Briscoe and Carroll, 1997; Man- 
ning, 1993; Carroll and Rooth, 1998; Gahl, 
1998; Lapata, 1999; Sarkar and Zeman, 2000) 
vary largely according to the methods used 
and the number of SCFS being extracted. Re- 
gardless of this, there is a ceiling on the perfor- 
mance of these systems at around 80% token 
recall 1 
zWhere token recall is the percentage .ofSCF to- 
kens in a sample of manually analysed text that were 
The approaches to extracting SCF informa- 
tion from corpora have frequently employed 
statistical methods for filtering (e.g. Brent, 
1993; Manning 1993; Briscoe and Carroll, 
1997; Lapata, 1999). This has been done to 
remove the noise that arises when dealing with 
naturally occurring data, and from mistakes 
made by the SCF acquisition system, for ex- 
ample, parser errors. 
Filtering is usually done with a hypothe- 
sis test, and frequently with a variation of 
the binomial filter introduced by Brent (1991, 
1993). Hypothesis testing is performed by for- 
mulating a null hypothesis, (H0), which is as- 
sumed true unless there is evidence to the con- 
trary. If there is evidence to the contrary, 
H0 is rejected and the alternative hypothe- 
sis (H1) is accepted. In SCF acquisition, H0 is 
that there is no association between aparticu- 
lar verb (verbj) and a SCF (SCFi), meanwhile 
H1 is that there is such an association. For 
SCF acquisition, the test is one-tailed since H1 
states the direction of the association, a pos- 
itive correlation between verbj and scfi. We 
compare the expected probability of scfi oc- 
curring with verbj if H0 is true, to the ob- 
served probability of co-occurrence obtained 
from the corpus data. If the observed proba- 
bility is greater than the expected probability 
we reject Ho and accept H1, and if not, we 
retain H0. 
Despite the popularity of this method, it 
has been reported as problematic. Accord- 
ing to one account (Briscoe and Carroll, 1997) 
the majority of errors arise because of the sta- 
tistical filtering process, which is reported to 
be particularly unreliable for low frequency 
SCFs (Brent, 1993; Briscoe and Carroll, 1997; 
Manning, 1993; Manning and Schiitze, 1999). 
Lapata (1999) reported that a threshold on 
the relative frequencies produced slightly bet- 
ter results than those achieved with a Brent- 
correctly acquired by the system. 
199 
style binomial filter when establishing SCFs for 
diathesis alternation detection. Lapata deter- 
mined thresholds for each SCF using the fre- 
quency of the SCF in COMLEX Syntax dictio- 
nary (Grishman et al, 1994). 
Adopting the SCF acquisition system of 
Briscoe and Carroll, we have experimented 
with an alternative hypothesis test, the bi- 
nomial log-likelihood ratio (LLR) test (Dun- 
ning, 1993). Sarkar and Zeman (2000) have 
also used this test when filtering SCFs auto- 
matically acquired for Czech. This test has 
been recommended for use in NLP since it 
does not assume a normal distribution, which 
invalidates many other parametric tests for 
use with natural language phenomena. LLR 
can be used in a form (-2logA) which is 
X 2 distributed. Moreover, this asymptote is 
appropriate at quite low frequencies, which 
makes the hypothesis test particularly useful 
when dealing with natural anguage phenom- 
ena, where low frequency events are common- 
place. 
A problem with using hypothesis testing for 
filtering automatically acquired SCFs is ob- 
taining a good estimation of the expected oc- 
currence of scfi with verbj. This is often 
performed using the unconditional distribu- 
tion, that is the probability distribution over 
all SCFS, regardless of the verb. It is as- 
sumed that verbj must occur with scfi sig- 
nificantly more than is expected given this 
estimate. Our paper addresses the problem 
that the conditional distribution, dependent 
on the verb, and unconditional distribution 
are rarely correlated. Therefore statistical fil- 
ters which assume such correlation for H0 will 
be susceptible to error, 
In this paper, we compare the results of 
the Brent style binomial filter of Briscoe and 
Carroll and the LLR filter to a simple method 
which uses a threshold on the relative frequen- 
cies of the verb and SCF combinations. We 
do this within the framework of the Briscoe 
and Carroll SCF acquisition system, which is 
described in section 2.1. The details of the 
two statistical filters are described in section 
2.2, along with the details of the threshold ap- 
plied to the relative frequencies output from 
the SCF acquisition system. The details of the 
experimental evaluation are supplied in sec- 
tion 3. We discuss our findings in section 3.3 
and conclude with directions for future work 
(section 4). 
2 Method  
2.1 F ramework  for SCF  Acquisit ion 
Briscoe and Carroll's (1997) verbal acquisition 
system distinguishes 163 SCFs and returns rel- 
ative frequencies for each SCF found for a given 
predicate. The SCFs are a superset of classes 
found in the Alvey NL Tools (ANLT) dictio- 
nary, Boguraev et al (1987) and the COML~X 
Syntax dictionary, Grishman et al (1994). 
They incorporate information about control 
of predicative arguments, as well as alterna- 
tions such as extraposition and particle move- 
ment. The system employs a shallow parser to 
obtain the subcategorization information. Po- 
tential SCF entries are filtered before the final 
SCF lexicon is produced. The filter is the only 
component of this system which we experi- 
ment with here. The three filtering methods 
which we compare are described below. 
2.2 Fi l ter ing Methods  
2.2.1 B inomia l  Hypothes is  Test 
Briscoe and Carroll (1997) used a binomial 
hypothesis test (BHT) to filter the acquired 
SCFs. They applied BHT as follows. The sys- 
tem recorded the total number of sets of SCF 
cues (n) found for a given predicate, and the 
number of these sets for a given SCF (ra). The 
system estimated the error probability (pe) 
that a cue for a SCF (scfi) occurred with a 
verb which did not take scfi. pe was esti- 
mated in two stages, as shown in equation 1. 
Firstly, the number of verbs which are mem- 
bers of the target SCF in the ANLT dictionary 
were extracted. This number was converted 
to a probability of class membership by divid- 
ing by the total number of verbs in the dic- 
tionary. The complement of this probability 
provided an estimate for the probability of a 
verb not taking scfi. Secondly, this proba- 
bility was multiplied by an estimate for the 
probability of observing the cue for scfi. This 
was estimated using the number of cues for i 
extracted from the Susanue corpus (Sampson, 
1995), divided by the total number of cues. 
pe = (1  - Iverbsl    i  cZass il I eSlc e l, for il (1) 
The probability of an event with probability p
happening exactly rn times out of n attempts 
is given by the following binomial distribution: 
20O 
n~ P(m,n,p) = m!(n-  m)! pro(1 _p)n-m (2) 
The probability of the event happening m or 
more times is: 
= (3) 
k=rn 
Finally, P(m+, n,p e) is the probabil ity that 
m or more occurrences of cues for scfi will oc- 
cur with a verb which is not a member ofscfi, 
given n occurrences of that verb. A threshold 
on this probability, P(m+,n, pe), was set at 
less than or equal to 0.05. This yielded a 95% 
or better confidence that a high enough pro- 
portion of cues for scfi have been observed for 
the verb to be legitimately assigned scfi. 
Other approaches which use a binomial fil- 
ter differ in respect of the calculation of the 
error probability. Brent (1993) estimated the 
error probabilities for each SCF experimen- 
tally from the behaviour of his SCF extrac- 
tor, which detected simple morpho-syntactic 
cues in the corpus data. Manning (1993) in- 
Creased the number of available cues at the ex- 
pense of the reliability of these cues. To main- 
tain high levels of accuracy, Manning applied 
higher bounds on the error probabilities for 
certain cues. These bounds were determined 
experimentally. A similar approach was taken 
by Briscoe, Carroll and Korhonen (1997) in a 
modification to the Briscoe and Carroll sys- 
tem. The overall performance was increased 
by changing the estimates of pe according to 
the performance of the system for the target 
SCF. In the work described here, we use the 
original BHT proposed by Briscoe and Carroll. 
2.2.2 The  B inomia l  Log L ike l ihood  
Rat io  as a S ta t i s t i ca l  F i l te r  
Dunning (1993) demonstrates the benefits of 
the LLR statistic, compared to Pearson's chi- 
squared, on the task of ranking bigram data. 
The binomial log-likelihood ratio test is 
simple to calculate. For each verb and SCF 
combination four counts are required. These 
are the number of times that: 
1. the target verb occurs with the target SCF 
(kl) 
2. the target verb occurs with any other SCF 
(nl - kl) 
3. any other verb occurs with the target SCF 
(k2) 
4. any other verb occurs with any other SCF 
- k2) 
The statistic -21ogA is calculated as follows:- 
log-likelihood = 
where 
2\[logL(pl, kl, nl ) 
+logL(p2, k2, n2) 
-logL(p, kl, nl) 
-logL(p, k2, n2) \] (4) 
logL(p, n, k) = k x logp + (n - k) x log(1 -p )  
and 
kl k2 kl + k2 
P l=- - ,  P2------ ,  P - -  nl n2 nl -4- n2 
The LLR statistic provides a score that re- 
flects the difference in (i) the number of bits 
it takes to describe the observed data, using 
pl = p(SCFIverb ) and p2 = p(SCFl-~verb ), 
and (ii) the number of bits it takes to de- 
scribe the expected ata using the probability 
p = p(scFlany verb). 
The LLR statistic detects differences be- 
tween pl  and p2. The difference could 
potentially be in either direction, but we are 
interested in LLRS where p l  > p2, i.e. where 
there is a positive association between the SCF 
and the verb. For these cases, we compared 
the value of -2logA to the threshold value 
obtained from Pearson's Chi-Squared table, 
to see if it was significant at the 95% level 2. 
2.2.3 Us ing  a Thresho ld  on the  
Re la t ive  Frequenc ies  as a 
Base l ine  
In order to examine the baseline performance 
of this system without employing any notion 
of the significance of the observations, we 
used a threshold on relative frequencies. This 
was done by extracting the SCFS, and rank- 
ing them in the order of the probability of 
their occurrence with the verb. The probabil- 
ities were estimated using a maximum likeli- 
hood estimate (MLE) from the observed rela- 
tive frequencies. A threshold, determined em- 
pirically, was applied to these probability esti- 
mates to filter out the low probability entries 
for each verb. .... 
2See (Gorrell, 1999) for details of this" method. 
201 
3 Eva luat ion  
3.1 Method  
To evaluate the different approaches, we took 
a sample of 10 million words of the BNC cor- 
pus (Leech, 1992). We extracted all sentences 
containing an occurrence of one of fourteen 
verbs 3. The verbs were chosen at random, 
subject to the constraint that they exhibited 
multiple complementation patterns. After the 
extraction process, we retained 3000 citations, 
on average, for each verb. The sentences con- 
taining these verbs were processed by the SCF 
acquisition system, and then we applied the 
three filtering methods described above. We 
also obtained results for a baseline without 
any filtering. 
The results were evaluated against a man- 
ual analysis of corpus data 4. This was ob- 
tained by analysing up to a maximum of 300 
occurrences for each of the 14 test verbs in 
LOB (Garside et al, 1987), Susanne and SEC 
(Taylor and Knowles, 1988) corpora. Follow- 
ing Briscoe and Carroll (1997), we calculated 
precision (percentage of SCFS acquired which 
were also exemplified in the manual analysis) 
and recall (percentage of the SCFs exemplified 
in the manual analysis which were acquired 
automatically). We also combined precision 
and recall into a single measure of overall per- 
formance using the F measure (MA.nniug and 
Schiitze, 1999). 
F = 2.precis ion.  recall (5) 
precision + recall 
3.2 Resu l ts  
Table 1 gives the raw results for the 14 verbs 
using each method. It shows the number of 
true positives (TP), .false positives (FP), and 
.false negatives (FN), as determined accord- 
ing to the manual analysis. The results for 
high frequency SCFs (above 0.01 relative fre- 
quency), medium frequency (between 0.001 
and 0.01) and low frequency (below 0.001) 
SCFs are listed respectively in the second, 
3These verbs were ask, begin, believe, cause, expect, 
find, give, help, like, move, produce, provide, seem, 
swing. 
4The importance of the manual analysis is outlined 
in Briscoe and Carroll (1997). We use the same man- 
ual analysis as Briscoe and Carroll, Le. one from the 
Susanne, LOB, and SEC corpora. A manual analysis of 
the BNC data might produce better results. However, 
since the BNC is a heterogeneous corpus we felt it was 
reasonable to test the data on a different corpus, which 
is also heterogeneous. 
third and fourth columns, and the final col- 
umn includes the total results for all frequency 
ranges. 
Table 2 shows precision and recall for the 14 
verbs and the F measure, which combines pre- 
cision and recall. We also provide the baseline 
results, if all SCFs were accepted. 
From the results given in tables 1 and 2, the 
MLE approach outperformed both hypothesis 
tests. For both BHT and LLR there was an 
increase in FNs at high frequencies, and an 
increase in FPs at medium and low frequen- 
cies, when compared to MLE. The number of 
errors was typically larger for LLR than BHT. 
The hypothesis tests reduced the number of 
FNS at medium and low frequencies, however, 
this was countered by the substantial increase 
in FPs that they gave. While BHT nearly al- 
ways acquired the three most frequent SCFs of 
verbs correctly, LLR tended to reject these. 
While the high number of FNS can be ex- 
plained by reports which have shown LLR to 
be over-conservative (Ribas, 1995; Pedersen, 
1996), the high number of FPs is surprising. 
Although theoretically, the strength of LLR 
lies in its suitability for low frequency data, 
the results displayed in table 1 do not suggest 
that the method performs better than BHT on 
low frequency frames. 
MLE thresholding produced better results 
than the two statistical tests used. Preci- 
sion improved considerably, showing that the 
classes occurring in the data with the high- 
est frequency are often correct. Although MLE 
thresholding clearly makes no attempt to solve 
the sparse data problem, it performs better 
than BHT or LLR overall. MLE is not adept at 
finding low frequency SCFS, however, the other 
methods are problematic in that they wrongly 
accept more than they correctly reject. The 
baseline, of accepting all SCFS, obtained a high 
recall at the expense of precision. 
3.3 D iscuss ion  
Our results indicate that MLE outperforms 
both hypothesis tests. There are two explana- 
tions for this, and these are jointly responsible 
for the results. 
Firstly, the SCF distribution is zipfian, as 
are many distributions concerned with nat- 
ural language (Manning and Schiitze, 1999). 
Figure 1 shows the conditional distribution 
for the verb find. This ~mf~ltered SCF prob- 
ability distribution was obtained from 20 M 
words of BNC data output from the SCF sys- 
202 
High Freq 
TP FP  FN 
BHT 75 29 23 
LLR 66 30 32 
MLE 92 31 6 
Med ium Freq Low Freq 
TP FP  FN TP FP  I FN 
11 37 31 4 23 15 
9 52 33 2 23 17 
0 0 42 0 0 19 
Totals 
TP FP I FN 
m 
90 89 69 
77 105 82 
92 31 67 
Table 1: Raw results for 14 test verbs 
~r31ff.t: Precision % Recall % F measure 
BHT 50.3 56.6 53.3 
LLR 42.3 48.4 45.1 
MLE 74.8 57.8 65.2 
baseline 24.3 83.5 37.6 
Table 2: Precision, Recall, and F measure 
0.1  
0.01 
& 
0.001 
0.0001 
. . . . . . . . .  i . . . . . . . .  
!. 
o 
I , r i i , , i , l  , , i i i i , 
10 100 
rank 
0.1  
0.01 
o.oo~ 
0.01~ 
10  4 
\ 
, , , , t , , r  , i , , i , , ,1  
10 100 
rank 
Figure 1: Hypothesised SCF distribution for 
find 
tern. The unconditional distribution obtained 
from the observed istribution of SCFs in the 
20 M words of BNC is shown in figure 2. The 
figures show SCF rank on the X-axis versus 
SCF frequency on the Y-axis, using logarith- 
mic scales. The line indicates the closest Zipf- 
like power law fit to the data. 
Secondly, the hypothesis tests make the 
false assumption (H0) that the unconditional 
and conditional distributions are correlated. 
The fact that a significant improvement in
performance is made by correcting the prior 
probabilities according to the performance of
the system (Briscoe, Carroll and Korhonen, 
Figure 2: Hypothesised unconditional SCF dis- 
tribution 
1997) suggests the discrepancy between the 
unconditional and the conditional distribu- 
tions. 
We examined the correlation between the 
manual analysis for the 14 verbs, and the 
unconditional distribution of verb types over 
all SCFs estimated from the ANLT using the 
Spearman Rank Correlation Coefficient. The 
results included in table 3 show that only a 
moderate correlation was found averaged over 
all verb types. 
Both LLR and BHT work by comparing the 
observed value of p(scfi\[verbj) to that ex- 
pected by chance. They both use the observed 
203 
\[ Verb Rank  Correlation 
ask 0.10 
begin 0.83 
believe 0.77 
cause 0.19 
expect 
find 
0.45 
0.33 
give 0.06 
help 0.43 
like 0.56 
move 0.53 
produce 0.95 
provide 0.65 
seem 0.16 
swing 
Average 
0.50 
0.47 
Table 3: Rank correlation between the condi- 
tional SCF distributions of the test verbs and 
the unconditional distribution 
value for p(sc.filverbj) from the system's out- 
put, and they both use an estimate for the un- 
conditional probability distribution (p(scfi)) 
for estimating the expected probability. They 
differ in the way that the estimate for the un- 
conditional probability is obtained, and the 
way that it is used in hypothesis testing. 
For  BHT, the null hypothesis i that the ob- 
served value ofp(scfiIverbj) arose by chance, 
because of noise in the data. We estimate the 
probability that the value observed could have 
arisen by chance using p(m+,  n,pe), pe is cal- 
culated using: 
? the SCF acquisition system's raw (until- 
tered) estimate for the unconditional dis- 
tribution, which is obtained from the Su- 
sanne corpus and 
? the ANLT estimate of the unconditional 
distribution of a verb not taking scf~, 
across all SCFs 
For LLR, both the conditional (pl) and un- 
conditional distributions (p2) are estimated 
from the BNC data. The unconditional proba- 
bility distribution uses the occurrence of scfi 
with any verb other than our target. 
The binomial tests look at one point in the 
SCF distribution at a time, for a given verb. 
The expected value is determined using the 
unconditional distribution, on the assumption 
that if the null hypothesis true then this dis- 
tribution will correlate with the conditional 
distribution. However, this is rarely the case. 
Moreover, because of the zipfian nature of 
the distributions, the frequency differences at 
any point can be substantial. In these exper- 
iments, we used one-tailed tests because we 
were looking for cases where there was a pos- 
itive association between the SCF and verb, 
however, in a two-tailed test the null hypoth- 
esis would rarely be accepted, because of the 
substantial differences in the conditional and 
unconditional distributions. 
A large number of false negatives occurred 
for high frequency SCFs because the probabil- 
ity we compared them to was too high. This 
probability was estimated from the combina- 
tion of many verbs genuinely occurring with 
the frame in question, rather than from an es- 
timate of background noise from verbs which 
did not occur with the frame. We did not use 
an estimate from verbs which do not take the 
SCF, since this would require a priori knowl- 
edge about the phenomena that we were en- 
deavouring to acquire automatically. For LLR 
the unconditional probability estimate (p2) 
was high, simply because this SCF was a com- 
mon one, rather than because the data was 
particularly noisy. For BHT, R e was likewise 
too high as the SCF was also common in the 
Susanne data. The ANLT estimate went some- 
way to compensating for this, thus we ob- 
tained fewer false negatives with BHT than 
LLR. 
A large number of false positives occurred 
for low frequency SCFs because the estimate 
for p(scf) was low. This estimate was more 
readily exceeded by the conditional estimate. 
For BHT false positives arose because of the 
low estimate of p(scf) (from Susanne) and 
because the estimate of p(-,SCF) from ANLT 
did not compensate enough for this. For LLR, 
there was no mean~ to compensate for the fact 
that p2 was lower than pl .  
In contrast, MLE did not compare two dis- 
tributions. Simply rejecting the low frequency 
data produced better results overall by avoid- 
ing the false positives with the low frequency 
data, and the false negatives with the high 
frequency data. 
4 Conc lus ion  
This paper explored three possibilities for fil- 
tering out the SCF entries produced by a SCF 
acquisition system. These were (i) a version 
of Brent's binomial filter, commonly used for 
this purpose, (ii) the binomial og-likelihood 
204 
ratio test, recommended for use with low fre- 
quency data and (iii) a simple method using 
a threshold on the MLEs of  the SCFS output 
from the system. Surprisingly, the simple MLE 
thresholding method worked best. The BHT 
and LLR both produced an astounding mlm- 
ber of FPs, particularly at low frequencies. 
Further work on handling low frequency 
data in SCF acquisition is warranted. A non- 
parametric statistical test, such as Fisher's ex- 
act test, recommended by Pedersen (1996), 
might improve on the results obtained using 
parametric tests. However, it seems from our 
experiments hat it would be better to avoid 
hypothesis tests that make use of the uncon- 
ditional distribution. 
One possibility is to put more effort into the 
estimation of pe, and to avoid use of the un- 
conditional distribution for this. In some re- 
cent experiments, we tried optimising the es- 
timates for pe depending on the performance 
of the system for the target SCF, using the 
method proposed by Briscoe, Carroll and Ko- 
rhonen (1997). The estimates of pe were ob- 
tained from a training set separate to the held- 
out BNC data used for testing. Results using 
the new estimates for pe gave an improvement 
of 10% precision and 6% recall, compared to 
the BHT results reported here. Nevertheless, 
the precision result was 14% worse for preci- 
sion than MLE, though there was a 4% im- 
provement in recall, making the overall per- 
formance 3.9 worse than MLE according to the 
F measure. Lapata (1999) also reported that 
a simple relative frequency cut off produced 
slightly better esults than a Brent style BHT. 
If MLE thresholding persistently achieves 
better results, it would be worth investi- 
gating ways of handling the low frequency 
data, such as smoothing, for integration with 
this method. However, more sophisticated 
smoothing methods, which back-off to an un- 
Conditional distribution, will also suffer from 
the lack of correlation between conditional 
and unconditional SCF distributions. Any sta- 
tistical test would work better at low frequen- 
cies than the MLE, since this simply disregards 
all low frequency SCFs. In our experiments, ff 
we had used MLE only for the high frequency 
data, and BHT for medium and low, then over- 
all we would have had 54% precision and 67% 
recall. It certainly seems worth employing hy- 
pothesis tests which do not rely on the un- 
conditional distribution for the low frequency 
SCFS. 
5 Acknowledgements  
We thank Ted Briscoe for many helpful dis- 
cussions and suggestions concerning this work. 
We also acknowledge Yuval Krymolowski for 
useful comments on this paper. 
Re ferences  
Boguraev, B., Briscoe, E., Carroll, J., Carter, 
D. and Grover, C. 1987. The derivation of a 
grammatically-indexed lexicon from the Long- 
man Dictionary of Contemporary English. In 
Proceedings of the 25th Annual Meeting of 
the Association for Computational Linguis- 
tics, Stanford, CA. 193-200. 
Brent, M. 1991. Automatic acquisition of 
subcategorization frames from untagged text. 
In Proceedings of the 29th Annual Meeting 
of the Association for Computational Linguis- 
tics, Berkeley, CA. 209-214. 
Brent, M. 1993. From gra.mmar to lexicon: 
unsupervised learning of lexical syntax. Com- 
putational Linguistics 19.3: 243-262. 
Briscoe, E.J. and J. Carroll 1997. Automatic 
extraction of subcategorization from corpora. 
In Proceedings of the 5th ACL Conf. on Ap- 
plied Nat. Lg. Proc., Washington, DC. 356- 
363. 
Briscoe, E., Carroll, J. and Korhonen, A. 
1997. Automatic extraction of subcategoriza- 
tion frames from corpora - a framework and 
3 experiments. '97 Sparkle WP5 Deliverable, 
available in http://www.ilc.pi.cnr.it/. 
Carroll, G. and Rooth, M. 1998. Valence 
induction with a head-lexicalized PCFG. In 
Proceedings of the 3rd Conference on Empir- 
ical Methods in Natural Language Processing, 
Granada, Spain. 
Dunning, T. 1993. Accurate methods for the 
Statistics of Surprise and Coincidence. Com- 
putational Linguistics 19.1: 61-74. 
Gahl, S. 1998. Automatic extraction of sub- 
corpora based on subcategorization frames 
from a part-of-speech tagged corpus. In Pro- 
ceedings of the COLING-A CL'98, Montreal, 
Canada. 
Garside, R., Leech, G. and Sampson, G. 1987. 
The computational nalysis of English: A 
corpus-based approach. Longman, London. 
Gorrell, G. 1999. Acquiring Subcategorisation 
from Textual Corpora. MPhil dissertation, 
University of Cambridge, UK. 
205 
Grishman, R., Macleod, C. and Meyers, A. 
1994. Comlex syntax: building a computa- 
tional lexicon. In Proceedings of the Interna- 
tional Conference on Computational Linguis- 
tics, COLING-94, Kyoto, Japan. 268-272. 
Lapata, M. 1999. Acquiring lexical gener- 
alizations from corpora: A case study for 
diathesis alternations. In Proceedings of the 
37th Annual Meeting of the Association for 
Computational Linguistics, Maryland. 397- 
404. 
Leech, G. 1992. 100 million words of English: 
the British National Corpus. Language Re- 
search 28(1): 1-13. 
Manning, C. 1993. Automatic acquisition of 
a large subcategorization dictionary from cor- 
pora. In Proceedings of the 31st Annual Meet- 
ing of the Association .for Computational Lin- 
guistics, Columbus, Ohio. 235-242. 
Manning, C. and Schiitze, H. 1999. Founda- 
tions of Statistical Natural Language Process- 
ing. MIT Press, Cambridge MA. 
Pedersen, T. 1996. Fishing for Exactness. In 
Proceedings of the South-Central SAS Users 
Group Conference SCSUG-96, Austin, Texas. 
Ribas, F. 1995. On Acquiring Appropriate Se- 
lectional Restrictions from Corpora Using a 
Semantic Taxonomy. Ph.D thesis, University 
of Catalonia. 
Sampson, G. 1995. English for the computer. 
Oxford University Press, Oxford UK. 
Sarkar, A. and Zeman, D. 2000. Auto- 
matic Extraction of Subcategorization Frames 
for Czech. In Proceedings of the Inter- 
national Conference on Computational Lin- 
guistics, COLING-O0, Saarbrucken, Germany. 
691-697. 
Taylor, L. and Knowles, G. 1988. Manual 
of information to accompany the SEC cor- 
pus: the machine-readable corpus of spoken 
English. University of Lancaster, UK, Ms. 
Ushioda, A., Evans, D., Gibson, T. and 
Waibel, A. 1993. The automatic acquisition of 
frequencies of verb subcategorization frames 
from tagged corpora. In Boguraev, B. and 
Pustejovsky, J. eds. SIGLEX A CL Workshop 
on the Acquisition of Lexieal Knowledge .from 
Text. Columbus, Ohio: 95-106. 
206 
i 
Using Semantically Motivated Estimates to Help 
Subcategorization Acquisition 
Anna Korhonen 
Computer Laboratory, University of Cambridge 
Pembroke Street, Cambridge CB2 3QG, UK 
alk23@cl, cam. ac.  uk 
Abst ract  
Research into the automatic acquisition of 
subcategorization frames from corpora is 
starting to produce large-scale computational 
lexicons which include valuable frequency in- 
formation. However, the accuracy of the 
resulting lexicons shows room for improve- 
ment. One source of error lies in the lack 
of accurate back-off estimates for subcatego- 
rization frames, delimiting the performance 
of statistical techniques frequently employed 
in verbal acquisition. In this paper, we 
propose a method of obtaining more accu- 
rate, semantically motivated back-off esti- 
mates, demonstrate how these estimates can 
be used to improve the learning of subcatego- 
rization frames, and discuss using the method 
to benefit large-scale l xical acquisition. 
1 In t roduct ion  
Manual development of large subcategorised 
lexicons has proved difficult because pred- 
icates change behaviour between sublan- 
guages, domains and over time. Yet parsers 
depend crucially on such information, and 
probabilistic parsers would greatly benefit 
from accurate information concerning the rel- 
ative frequency of different subcategorization 
frames (SCFs) for a given predicate. 
Over the past years acquiring subcatego- 
rization dictionaries from textual corpora has 
become increasingly popular (e.g. Brent, 
1991, 1993; Ushioda et al, 1993; Briscoe and 
Carroll, 1997; Manning, 1993; Carroll and 
Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar 
and Zeman, 2000). The different approaches 
vary according to the methods used and the 
number of SCFs being extracted. Regardless 
of this, there is a ceiling on the performance 
of these systems at around 80% token recall*. 
*Token recall is the percentage of SCF tokens in a 
sample of manually analysed text that were correctly 
acquired by the system. 
One significant source of error lies in the 
statistical filtering methods frequently used 
to remove noise from automatically acquired 
SCFs. These methods are reported to be par- 
ticularly unreliable for low frequency scFs 
(Brent, 1991, 1993; Briscoe and Carroll, 1997; 
Manning, 1993; Manning and Schiitze, 1999; 
Korhonen, Gorrell and McCarthy, 2000), re- 
sulting in a poor overall performance. 
According to Korhonen, Gorrell and Mc- 
Carthy (2000), the poor performance o f sta- 
tistical filtering can be largely explained by 
the zipfian nature of the data, coupled with 
the fact that many statistical tests are based 
on the assumption of two zipfian distributions 
correlating: the conditional SCF distribution 
of an individual verb (p(scfilverbj)) and the 
unconditional SCF distribution of all verbs in 
general (p(scfl)). Contrary to this assump- 
tion, however, there is no significant correla- 
tion between the two distributions. 
Korhonen, Gorrell and McCarthy (2000) 
have showed that a simple method of filtering 
SCFs on the basis of their relative frequency 
performs more accurately than statistical fil- 
tering. This method sensitive to the sparse 
data problem is best integrated with smooth- 
ing. Yet the performance of the sophisticated 
smoothing techniques which back-off to an un- 
conditional distribution also suffer from the 
lack of correlation between p(scfi\[verbj) and 
p(scf0. 
In this paper, we propose a method for ob- 
taining more accurate back-off estimates for 
SCF acquisition. Taking Levin's verb classifi- 
cation (Levin, 1993) as a starting point, we 
show that in terms of SCF distributions, in- 
dividual verbs correlate better with other se- 
mantically similar verbs than with all verbs 
in general. On the basis of this observation, 
we propose classifying verbs according to their 
semantic lass and using the conditional SCF 
distributions of a few other members in the 
216 
same class as back-off estimates of the class 
(p( sc fi lsernantic class j)). 
Adopting the SCF acquisition system of 
Briscoe and Carroll (1997) we report an ex- 
periment which demonstrates how these esti- 
mates can be used in filtering. This is done 
by acquiring the conditional SCF distributions 
for selected test verbs, smoothing these dis- 
tributions with the unconditional distribution 
of the respective verb class, and applying a 
simple method for filtering the resulting set 
of SCFs. Our results show that the proposed 
method improves the acquisition of SCFs sig- 
nificantly. We discuss how this method can be 
used to benefit large-scale SCF acquisition. 
We begin by reporting our findings that the 
SCF distributions of semantically similar verbs 
correlate well (section 2). We then introduce 
the method we adopted for constructing the 
back-~off estimates for the data used in our ex- 
periment (section 3.1), summarise the main 
features of the SCF acquisition approach (sec- 
tion 3.2), and describe the smoothing tech- 
niques adopted (section 3.3). Finally, we re- 
view the empirical evaluation (section 4) and 
discuss directions for future work (section 5). 
2 Examin ing  SCF  Cor re la t ion  
between Semant ica l ly  S imi la r  
Verbs  
To examine the degree of SCF correlation 
between semantically similar verbs, we took 
Levin's verb classification (1993) as a start- 
ing point. Levin verb classes are based on 
the ability of a verb to occur in specific 
diathesis alternations, i.e. specific pairs of 
syntactic frames which are assumed to be 
meaning preserving. The classification pro- 
vides semantically-motivated setsof syntactic 
frames associated with individual classes. 
While Levin's shows that there is corre- 
lation between the SCFs related to the verb 
sense, our aim is to examine whether there is 
also correlation between the SCFs specific to 
the verb form. Unlike Levin, we are concerned 
with polysemic scF distributions involving all 
senses of verbs. In addition, we are not only 
interested in the degree of correlation between 
sets of SCFs, but also in comparing the rank- 
ing of SCFs between distributions. Neverthe- 
less, Levin classes provide us a useful starting 
point. 
Focusing on five broad Levin classes 
change of possession, assessment, killing, mo- 
tion, and destroy verbs - we chose four test 
verbs from each class and examined the de- 
gree with which the SCF distribution for these 
verbs correlates with the SCF distributions for 
two other verbs from the same Levin class. 
The latter verbs were chosen so that one of 
the verbs is a synonym, and the other a hyper- 
nym, of a test verb. We used WordNet (Miller 
et al, 1990) for defining and recognising these 
semantic relations. We defined a hypernym 
as a test verb's hypernym in WordNet, and a 
synonym as a verb which, in WordNet, shares 
this same hypernym with a test verb. We also 
examined how well the SCF distribution for 
the different test verbs correlates with the SCF 
distribution of all English verbs in general and 
with that of a semantically different verb (i.e. 
a verb belonging to a different Levin class). 
We used two methods for obtaining the 
scF distributions. The first was to acquire 
an unfiltered subcategorization lexicon for 20 
million words of the British National Corpus 
(BN?) (Leech, 1992) data using Briscoe and 
Carroll's (1997) system (introduced in sec- 
tion 3.2). This gives us the observed istribu- 
tion of SCFs for individual verbs and that for 
all verbs in the BNC data. The second method 
was to manually analyse around 300 occur- 
rences of each test verb in the BNC data. This 
gives us an estimate of the correct SCF distri- 
butions for the individual verbs. The estimate 
for the correct distribution of SCFs over all 
English verbs was obtained by extracting the 
number of verbs which are members of each 
SCF class in the ANLT dictionary (Boguraev et 
al., 1987). 
The degree of correlation was examined by 
calculating the Kullback-Leibler distance (KL) 
(Cover and Thomas, 1991) and the Spearman 
rank correlation coefficient (Re) (Spearman, 
1904) between the different distributions 2. 
The results given in tables 1 and 2 were ob- 
tained by correlating the observed SCF distri- 
butions from the BNC data. Table 1 shows 
an example of correlating the SCF distribu- 
tion of the motion verb .fly against hat of (i) 
its hypernym move, (ii) synonym sail, (iii) all 
verbs in general, and (iv) agree, which is not 
related semantically. The results show that 
the SCF distribution for .fly clearly correlates 
better with the SCF distribution for move and 
sail than that for all verbs and agree. The av- 
2Note that Io., >_ 0, with IO., near to 0 denoting 
strong association, and -1  _< RC < 1, with RC near to 
0 denoting a-low degree of association and ttc near to 
-1 and 1 denoting strong association. 
217 
1 I I KL I cl 
fly move 0.25 0.83 
.fly sail 0.62 0.61 
.fly all verbs 2.13 0.51 
.fly agree 2.27 0.12 
Table 1: Correlating the SCF distribution of.fly against other SCF distributions 
\[-' KL I RC 
hype~nym 0.65 0.71 
synonym 0.71 0.66 
all verbs 1.59 0.41 
semantically different verb 1.74 0.38 
Table 2: Overall 
erage results for all test verbs given in table 2 
indicate that the degree of SCF correlation is
the best with semantically similar verbs. Hy- 
pernym and synonym relations are nearly as 
good, the majority of verbs showing slightly 
better SCF correlation with hypernyms. The 
SCF correlation between individual verbs, and 
verbs in general, is poor, but still better than 
with semantically unrelated verbs. 
These findings with the observed SCF dis- 
tributions hold as well with the correct SCF 
distributions, as seen in table 3. The results 
show that in terms of SCF distributions, verbs 
in all classes examined correlate better with 
their hypernym verbs than with all verbs in 
general. 
As one might expect, the polysemy of the 
individual verbs affects the degree of SCF cor- 
relation between semantically similar verbs. 
The degree of SCF correlation is higher with 
those verbs whose predominant 3 sense is in- 
volved with the Levin class examined. For 
example, the SCF distribution for the killing 
verb murder correlates better with that for 
the verb kill than that for the verb execute, 
whose predominant sense is not involved with 
killing verbs. 
These results how that the verb sense spe- 
cific SCF correlation observed by Levin ex- 
tends to the verb form specific SCF correlation 
and applies to the ranking of SCFs as well. 
This suggests that we can obtain more accu- 
rate back-off estimates for verbal acquisition 
by basing them on a semantic verb type. To 
find out whether such semantically motiwted 
SPredomlnant sense refers here to the most frequent 
sense of verbs in WordNet. 
correlation results 
estimates can be used to improve SCF acqui- 
sition, we performed an experiment which we 
describe below. 
3 Exper iment  
3.1 Back-off Es t imates  for the  Data  
The test data consisted of a total of 60 verbs 
from 12 broad Levin classes, listed in table 4. 
Two of the examined Levin classes were col- 
lapsed together with another similar Levin 
class, making the total number of test classes 
10. The verbs were chosen at random, sub- 
ject to the constraint that they occurred fre- 
quently enough in corpus data 4and when ap- 
plicable, represented different sub-classes of 
each examined Levin class. To reduce the 
problem of polysemy, we required that the 
predominant sense of each verb corresponds to
the Levin class in question. This was ensured 
by manually verifying that the most frequent 
sense of a verb in WordNet corresponds to the 
sense involved in the particular Levin class. 
To obtain the back-off estimates, we chose 
4-5 representative rbs from each verb class 
and obtained correct SCF distributions for 
these verbs by manually analysing around 300 
occurrences of each verb in the ant  data. We 
merged the resulting set of SCF distributions 
to construct he unconditional SCF distribu- 
tion for the verb class. This approach was 
taken to minimise the sparse data problem 
and cover SCF variations within verb classes 
and due to polysemy. The bazk-off estimates 
4We required at least 300 occurrences foreach verb. 
This was merely to guarantee accurate enough testing, 
as we evaluated our results against manual analysis of 
corpus data (see section 4). 
218 
Verb class 
change of possession 
assessment 
killing 
destroy 
motion 
AVERAGE 
Hypernym 
KL RC 
0.61 0.64 
0.28 0.71 
0.70 0.63 
0.30 0.60 
0.29 0.73 
0.44 0.66 
All Verbs 
KL RC 
1.16 0.38 
0.73 0.48 
1.14 0.37 
1.19 0.29 
1.72 0.42 
1.19 0.39 
Table 3: Correlation results for five verb classes 
Verb class 
putting 
sending and carrying, 
exerting force 
change of possession 
assessment, 
searching 
social interaction 
killing 
destroy 
appearance, disappearance 
and occurrence 
motion 
aspectuM 
Test verbs 
place, lay, drop, pour, load, fill 
send, ship, carry, bring, transport 
pull, push 
give, lend, contribute, donate, offer 
provide, supply, acquire, buy 
analyse 
fish, explore, investigate 
agree, communicate, struggle, marry, meet, visit 
kill, murder, slaughter, strangle 
demolish, destroy, ruin, devastate 
arise, emerge, disappear, vanish 
arrive, depart, march, move, slide, swing 
travel, walk, fly, sail, dance 
begin, end, start, terminate, complete 
Table 4: Test data 
for motion verbs, for example, were obtained 
by merging the SCF distributions of the verbs 
march, move, fly, slide and sail. Each verb 
used in obtaining the estimates was excluded 
when testing the verb itself. For example, 
when acquiring subcategorization f rthe verb 
fly, estimates were obtained only using verbs 
march, move, slide and sail. 
3.2 F ramework  for SCF Acquisit ion 
Briscoe and Carroll's (1997) verbal acquisition 
system distinguishes 163 SCFs and returns rel- 
ative frequencies for each SCF found for a given 
predicate. The SCFs are a superset of classes 
found in the ANLT and COMLEX (Grishman 
et al, 1994) dictionaries. They incorporate 
information about control of predicative ar- 
guments, as well as alternations such as ex- 
traposition and particle movement. The sys- 
tem employs a shallow parser to obtain the 
subcategorization information. Potential SCF 
entries are filtered before the final SCF lexi- 
con is produced. While Briscoe and Carroll 
(1997) used a statistical filter based on bi- 
nomial hypothesis test, we adopted another 
method, where the conditional SCF distribu- 
tion from the system is smoothed before fil- 
tering the SCFS, using the different techniques 
introduced in section 3.3. After smoothing, 
filtering is performed by applying a threshold 
to the resulting set of probability estimates. 
We used training data to find an optimal av- 
erage threshold for each verb class examined. 
This filtering method allows us to examine 
the benefits of smoothing without introducing 
problems based on the statistical filter. 
3.3 Smoothing 
3.3.1 Add One Smoothing 
Add one smoothing s has the effect of giving 
some of the probability space to the SCFs un- 
seen in the conditional distribution. As it as- 
sumes a uniform prior on events, it provides a
baseline smoothing method against which the 
5See (Manning and Schiltze, 1999) for detailed in- 
formation about the smoothing techniques discussed 
here. 
219 
more sophisticated methods can be compared. 
Let c(x=) be the frequency of a SCF given a 
verb, N the total number of SCF tokens for 
this verb in the conditional distribution, and 
C the total number of SCF types. The esti- 
mated probability of the SCF is: 
P(xn) - c(xn) + 1 
N + C (1) 
3.3.2 Katz  Backing-off  
In Katz backing-off (Katz, 1987), some of the 
probability space is given to the SCFs unseen 
or of low frequency in the conditional distri- 
bution. This is done by backing-off to an un- 
conditional distribution. Let p(xn) be a prob- 
ability of a SCF in the conditional distribution, 
and p(xnv) its probability in the unconditional 
distribution, obtained by maximum likelihood 
estimation. The estimated probability of the 
scF is calculated as follows: 
P(xn)= { (1 -d )  xp(xn) ifc(x=) > cl 
c~ x p(xnp) otherwise (2) 
The cut off frequency ci is an empiri- 
cally defined threshold etermining whether 
to back-off or not. When counts are lower 
than cl they are held too low to give an accu- 
rate estimate, and we back-off to an uncondi- 
tional distribution. In this case, we discount 
p(x~) a certain amount o reserve some of the 
probablity space for unseen and very low fre- 
quency scFs. The discount (d) is defined em- 
pirically, and a is a normalization constant 
which ensures that the probabilities of the re- 
sulting distribution sum to 1. 
3.3.3 L inear Interpo lat ion 
While Katz backing-off consults different es- 
timates depending on their specificity, linear 
interpolation makes a linear combination of 
them. Linear interpolation is used here for the 
simple task of combining a conditional distri- 
bution with an unconditional one. The esti- 
mated probability of the SCF is given by 
P(xn) = Al(p(z,~)) + )~2(p(xnp)) (3) 
where the Ai denotes weights for differ- 
ent context sizes (obtained by optimising the 
smoothing performance on the training data 
for all zn) and sum to 1. 
4 Eva luat ion  
4.1 Method 
To evaluate the approach, we took a sample of 
20 million words of the BNC and extracted all 
sentences containing an occurrence of one of 
the 60 test verbs on average of 3000 citations 
of each. The sentences containing these verbs 
were processed by the SCF acquisition system, 
and the smoothing methods were applied be- 
fore filtering. We also obtained results for a 
baseline without any smoothing. 
The results were evaluated against a man- 
ual analysis of the corpus data. This was 
obtained by analysing up to a maximum of 
300 occurrences for each test verb in BN? 
or LOB (Garside et al, 1987), Susanne and 
SEC (Taylor and Knowles, 1988) corpora. We 
calculated type precision (percentage of SCFs 
acquired which were also exemplified in the 
manual analysis) and recall (percentage of the 
SCFs exemplified in the manual analysis which 
were also acquired automatically), and com- 
bined them into a single measure of overall 
performance using the F measure (Manning 
and Schiitze, 1999). 
F = 2. precision, recall (4) 
precision -4- recall 
We estimated accuracy with which the sys- 
tem ranks true positive classes against he cor- 
rect ranking. This was computed by calculat- 
ing the percentage of pairs of SCFs at posi- 
tions (n, m) such that n < m in the system 
ranking that occur in the same order in the 
ranking from the manual analysis. This gives 
us an estimate of the accuracy of the relative 
frequencies of SCFs output by the system. In 
addition to the system results, we also calcu- 
lated KL and Rc between the acquired unfil- 
tered SCF distributions and the distributions 
obtained from the manual analysis. 
4.2 Results  
Table 5 gives average results for the 60 test 
verbs using each method. The results indi- 
cate that both add one smoothing and Katz 
backing-off improve the baseline performance 
only slightly. Linear interpolation outper- 
forms these methods, achieving better esults 
on all measures. The improved KL indicates 
that the method improves the overall accu- 
racy of SCF distributions. The  results with 
rtc and system accuracy show that it helps 
to correct the ranking of SCFs. The fact 
220 
Method KL RC accuracy 
Baseline 0.63 0.72 79.2 
Add one 0.64 0.74 79.0 
Katz backing-off 0.61 0.75 79.0 
Linear interpolation 0.51 0.82 84.4 
System results (%) 
\]precision I recall F measure 
78.5 63.3 70.1 
85.3 59.7 70.2 
76.4 67.6 71.7 
87.8 68.7 77.1 
Table 5: Average results with different methods using semantically motivated back-off estimates 
for smoothing 
Method KL  ac accuracy 
Baseline 0.63 0.72 79.2 
Katz backing-off 0.68 0.69 77.2 
Linear interpolation 0.79 0.64 76.7 
System resu l t s  (%) 
I precision I recall I F  measure 
78.5 63.3 70.1 
75.2 61.7 67.8 
71.4 64.1 67.6 
Table 6: Average results using the SCF distribution of all verbs as back-off estimates for smooth- 
ing 
that both precision and recall show clear im- 
provement over the baseline results demon- 
strates that linear interpolation can be suc- 
cessfully combined with the filtering method 
employed. These results eem to suggest that 
a smoothing method which affects both the 
highly ranked SCFs and SCFs of low frequency 
is profitable for this task. 
In this experiment, he semantically moti- 
vated back-off estimates helped to reduce the 
sparse data problem significantly. While a to- 
tal of 151 correct SCFs were missing in the test 
data, only three were missing after smoothing 
with Katz backing-off or linear interpolation. 
For comparison, we re-run these experi- 
ments using the general SCF distribution of 
all verbs as back-off estimates for smooth- 
ing 6. The average results for the 60 test verbs 
given in table 6 show that when using these 
estimates, we obtain worse results than with 
the baseline method. This demonstrates that 
while such estimates provide an easy solution 
to the sparse data problem, they can actually 
degrade the accuracy of verbal acquisition. 
Table 7 displays individual results for the 
different verb classes. It lists the results ob- 
tained with KL and Rc using the baseline 
method and linear interpolation with semanti- 
cally motivated estimates. Examining the re- 
sults obtained with linear interpolation allows 
us to consider the accuracy of the back-off es- 
6These estimates were obtained by extracting the 
number of verbs which are members of each SCF class 
in the ANLT dictionary. See section 2 for details. 
timates for each verb class. Out of ten verb 
classes, eight show improvement with linear 
interpolation, with both KL and Rc. However, 
two verb classes - aspectual verbs, and verbs 
of appearance, disappearance and occurrence 
- show worse results when linear interpolation 
is used. 
According to Levin (1993), these two verb 
classes need further classification before a full 
semantic account can be made. The prob- 
lem with aspectual verbs is that the class 
contains verbs taking sentential complements. 
As Levin does not classify verbs on basis 
of their sentential complement-taking proper- 
ties, more classification work is required be- 
fore we can obtain accurate SCF estimates for 
this type of verb. 
The problem with verbs of appearance is 
more specific to the verb class. Levin remarks 
that the definition of appearance verbs may 
be too loose. In addition, there are signifi- 
cant syntactic differences between the verbs 
belonging to the different sub-classes. 
This suggests that we should examine the 
degree of SCF correlation between verbs from 
different sub-classes before deciding on the fi- 
nal (sub-)class for which we obtain the es- 
timates. As the results with the combined 
Levin classes how, estimates can also be suc- 
cessfully built using verbs fromdifferent Levin 
classes, provided that the classes are similar 
enough. 
221 
RC 
Verb class baseline linear i. 
putting 0.68 0.70 
sending and carrying, 0.72 0.96 
exerting force 
change of possession 0.61 0.75 
assessment, 0.61 0.70 
searching 
social interaction 0.72 0.80 
killing 0.91 0.95 
destroy 0.70 0.97 
appearance, disappearance 
and occurrence 
0.91 
aspectual 
KL 
baseline linear i. 
0.70 0.66 
0.64 0.50 
0.61 0.60 
0.81 0.62 
0.65 0.58 
0.69 0.67 
0.95 0.2O 
0.14 0.17 0.83 
motion 0.66 0.58 0.56 0.66 
0.48 0.54 0.86 0.89 
Table 7: Baseline and linear interpolation results for the verb classes 
5 Conc lus ion  
In this paper, we have shown that the verb 
form specific SCF distributions of semantically 
similar verbs correlate well. On the basis 
of this observation, we have proposed using 
verb class specific back-off estimates in SCF 
acquisition. Employing the SCF acquisition 
framework of Briscoe and Carroll (1997), we 
have demonstrated that these estimates can 
be used to improve SCF acquisition signifi- 
cantly, when combined with smoothing and 
a simple filtering method. 
We have not yet explored the possibility 
of using the semantically motivated estimates 
with statistical filtering. In principle, this 
should help to improve the performance of the 
statistical methods which make use of back-off 
estimates. If filtering based on relative fre- 
quencies till achieves better results, it would 
be worth investigating ways of handling the 
low frequency data for integration with this 
method. As Korhonen, Gorrell and McCarthy 
(2000) discuss, any statistical filtering method 
would work better at low frequences than the 
one applied, since this simply disregards all 
low frequency SCFS. 
In addition to refining the filtering method, 
our future work will focus on integrating 
this approach with large-scale scF acquisition. 
This will involve (i) defining the set of seman- 
tic verb classes across the lexicon, (ii) obtain- 
ing back-off estimates for each verb class, and 
(iii) implementing a method capable of auto- 
matically classifying verbs to semantic lasses. 
The latter can be done by linking the Word- 
Net synonym sets with semantic lasses, using 
a similar method to that employed by Dorr 
(1997). With the research reported, verbs 
were classified to semantic lasses according 
to their most frequent sense. While this ap- 
proach proved satisfactory, our future work 
will include investigating ways of addressing 
the problem of polysemy better. 
The manual effort needed for obtaining the 
back-off estimates was quite high for this pre- 
liminary experiment. However, our recent in- 
vestigation shows that the total number of se- 
mantic classes across the whole lexicon is un- 
likely to exceed 50. This is because many of 
the Levin classes have proved similar enough 
in terms of SCF distributions that they can be 
combined together. Therefore the additional 
effort required to carry out the proposed work 
seems justified, given the accuracy enhance- 
ment reported. 
6 Acknowledgements  
I thank Ted Briscoe and Diana McCarthy for 
useful comments on this paper. 
Re ferences  
Boguraev, B., Briscoe, E., Carroll, J., Carter, 
D. and Grover, C. 1987. The derivation of a 
grammatically-indexed lexicon from the Long- 
man Dictionary of Contemporary English. In 
Proceedings of the 25th Annual Meeting of 
the Association .for Computational Linguis- 
tics, Stanford, CA. 193-200. 
Brent, M. 1991. Automatic acquisition of 
subcategorization frames from untagged text. 
222 
In Proceedings of the 29th Annual Meeting 
of the Association for Computational Linguis- 
tics, Berkeley, CA. 209-214. 
Brent, M. 1993. From grammar to lexicon: 
unsupervised learning of lexical syntax. Com- 
putational Linguistics 19.3: 243-262. 
Briscoe, E. and Carroll, J. 1993. Gener- 
alised probabilistic Lt~ parsing for unification- 
based grammars. Computational Linguistics 
19.1: 25-60. 
Briscoe, E.J. and J. Carroll 1997. Automatic 
extraction of subcategorization from corpora. 
In Proceedings of the 5th ACL Conf. on Ap- 
plied Natural Language Processing, Washing- 
ton, DC. 356-363. 
Briscoe, T., Carroll, J. and Korhonen, A. 
1997. Automatic extraction of subcategoriza- 
tion frames from corpora - a framework and 
3 experiments. Sparkle WP5 Deliverable. 
Available in http://www.ilc.pi.cnr.it/. 
Carroll, G. and Rooth, M. 1998. Valence 
induction with a head-lexicMized PCFG. In 
Proceedings of the 3rd Conference on Empir- 
ical Methods in Natural Language Processing, 
Granada, Spain. 
Cover, Thomas, M. and Thomas, J.A. 1991. 
Elements of Information Theory. Wiley- 
Interscience, New York. 
Dorr, B. 1997. Large-scale dictionary con- 
struction for foreign language tutoring and 
interlingual machine translation. Machine 
Translation 12.4: 271-325. 
Gahl, S. 1998. Automatic extraction of sub- 
corpora based on subcategorization frames 
from a part-of-speech tagged corpus. In Pro- 
ceedings of the COLING-ACL'98, Montreal, 
Canada. 
Grishman, R., Macleod, C. and Meyers, A. 
1994. Comlex syntax: building a computa- 
tional lexicon. In Proceedings of the Interna- 
tional Conference on Computational Linguis- 
tics, COLING-94, Kyoto, Japan. 268-272. 
Garside, R., Leech, G. and Sampson, G. 1987. 
The computational nalysis of English: A 
corpus-based approach. Longman, London. 
Katz, S. M. 1987. Estimation of probabili- 
ties from sparse data for the language model 
component of speech recogniser. IEEE Trans- 
actions on Acoustics, Speech, and Signal Pro- 
cessing 35.3: 400-401. 
Korhonen, A., Gorrell, G. and McCarthy, 
D. 2000. Statistical filtering and subcatego- 
rization frame acquisition. In Proceedings of 
the Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and 
Very Large Corpora, Hong Kong. 
Leech, G. 1992. 100 million words of English: 
the British NationM Corpus. Language Re- 
search 28(1): 1-13. 
Levin, B. 1993. English Verb Classes and 
Alternations. Chicago University Press, 
Chicago. 
Manning, C. 1993. Automatic acquisition of 
a large subcategorization dictionary from cor- 
pora. In Proceedings of the 31st Annual Meet- 
ing of the Association for Computational Lin- 
guistics, Columbus, Ohio. 235-242. 
Manning, C. and Schiitze, H. 1999. Founda- 
tions of Statistical Natural Language Process- 
ing. New York University, Ms. 
Miller, G., Beckwith, R., Felbaum, C., Gross, 
D. and Miller, K. 1993. Introduction to 
WordNet: An On-Line Lexical Database. 
ftp//clarity.princeton.edu/pub/WordNet/ 
5papers.ps. 
Sampson, G. 1995. English for the computer. 
Oxford, UK: Oxford University Press. 
Sarkar, A. and Zeman, D. 2000. Auto- 
matic Extraction of Subcategorization Frames 
for Czech. In Proceedings of the Inter- 
national Conference on Computational Lin- 
guistics, COLING-O0, Saarbrucken, Germany. 
691-697. 
Spearman, C. 1904. The proof and mea- 
surement of association between two things. 
American Journal of Psychology 15: 72-101. 
Taylor, L. and Knowles, G. 1988. Manual 
of information to accompany the SEC cor- 
pus: the machine-readable corpus of spoken 
English. University of Lancaster, UK, Ms. 
Ushioda, A., Ewns, D., Gibson, T. and 
Waibel, A. 1993. The automatic acquisition of 
frequencies of verb subcategorization frames 
from tagged corpora. In Boguraev, B. and 
Pustejovsky, J. eds. SIGLEX A CL Workshop 
on the Acquisition of Lexical Knowledge from 
Text. Columbus, Ohio: 95-106. 
223 
 
	 
	 	 	
 
	   	
Extended Lexical-Semantic Classication of English Verbs
Anna Korhonen and Ted Briscoe
University of Cambridge, Computer Laboratory
15 JJ Thomson Avenue, Cambridge CB3 OFD, UK
alk23@cl.cam.ac.uk, ejb@cl.cam.ac.uk
Abstract
Lexical-semantic verb classifications have
proved useful in supporting various natural lan-
guage processing (NLP) tasks. The largest and
the most widely deployed classification in En-
glish is Levin?s (1993) taxonomy of verbs and
their classes. While this resource is attrac-
tive in being extensive enough for some NLP
use, it is not comprehensive. In this paper, we
present a substantial extension to Levin?s tax-
onomy which incorporates 57 novel classes for
verbs not covered (comprehensively) by Levin.
We also introduce 106 novel diathesis alterna-
tions, created as a side product of constructing
the new classes. We demonstrate the utility of
our novel classes by using them to support au-
tomatic subcategorization acquisition and show
that the resulting extended classification has
extensive coverage over the English verb lex-
icon.
1 Introduction
Lexical-semantic classes which aim to capture the close
relationship between the syntax and semantics of verbs
have attracted considerable interest in both linguistics and
computational linguistics (e.g. (Pinker, 1989; Jackendoff,
1990; Levin, 1993; Dorr, 1997; Dang et al, 1998; Merlo
and Stevenson, 2001)). Such classes can capture general-
izations over a range of (cross-)linguistic properties, and
can therefore be used as a valuable means of reducing
redundancy in the lexicon and for filling gaps in lexical
knowledge.
Verb classes have proved useful in various (multilin-
gual) natural language processing (NLP) tasks and ap-
plications, such as computational lexicography (Kipper
et al, 2000), language generation (Stede, 1998), ma-
chine translation (Dorr, 1997), word sense disambigua-
tion (Prescher et al, 2000), document classification (Kla-
vans and Kan, 1998), and subcategorization acquisition
(Korhonen, 2002). Fundamentally, such classes define
the mapping from surface realization of arguments to
predicate-argument structure and are therefore a critical
component of any NLP system which needs to recover
predicate-argument structure. In many operational con-
texts, lexical information must be acquired from small
application- and/or domain-specific corpora. The predic-
tive power of classes can help compensate for lack of suf-
ficient data fully exemplifying the behaviour of relevant
words, through use of back-off smoothing or similar tech-
niques.
Although several classifications are now available for
English verbs (e.g. (Pinker, 1989; Jackendoff, 1990;
Levin, 1993)), they are all restricted to certain class
types and many of them have few exemplars with each
class. For example, the largest and the most widely de-
ployed classification in English, Levin?s (1993) taxon-
omy, mainly deals with verbs taking noun and preposi-
tional phrase complements, and does not provide large
numbers of exemplars of the classes. The fact that no
comprehensive classification is available limits the use-
fulness of the classes for practical NLP.
Some experiments have been reported recently which
indicate that it should be possible, in the future, to au-
tomatically supplement extant classifications with novel
verb classes and member verbs from corpus data (Brew
and Schulte im Walde, 2002; Merlo and Stevenson, 2001;
Korhonen et al, 2003). While the automatic approach
will avoid the expensive overhead of manual classifica-
tion, the very development of the technology capable of
large-scale automatic classification will require access to
a target classification and gold standard exemplification
of it more extensive than that available currently.
In this paper, we address these problems by introduc-
ing a substantial extension to Levin?s classification which
incorporates 57 novel classes for verbs not covered (com-
prehensively) by Levin. These classes, many of them
drawn initially from linguistic resources, were created
semi-automatically by looking for diathesis alternations
shared by candidate verbs. 106 new alternations not cov-
ered by Levin were identified for this work. We demon-
strate the usefulness of our novel classes by using them
to improve the performance of our extant subcategoriza-
tion acquisition system. We show that the resulting ex-
tended classification has good coverage over the English
verb lexicon. Discussion is provided on how the classifi-
cation could be further refined and extended in the future,
and integrated as part of Levin?s extant taxonomy.
We discuss Levin?s classification and its extensions in
section 2. Section 3 describes the process of creating the
new verb classes. Section 4 reports the experimental eval-
uation and section 5 discusses further work. Conclusions
are drawn in section 6.
2 Levin?s Classification
Levin?s classification (Levin, 1993) provides a summary
of the variety of theoretical research done on lexical-
semantic verb classification over the past decades. In
this classification, verbs which display the same or simi-
lar set of diathesis alternations in the realization of their
argument structure are assumed to share certain meaning
components and are organized into a semantically coher-
ent class. Although alternations are chosen as the primary
means for identifying verb classes, additional properties
related to subcategorization, morphology and extended
meanings of verbs are taken into account as well.
For instance, the Levin class of ?Break Verbs? (class
45.1), which refers to actions that bring about a change
in the material integrity of some entity, is characterized
by its participation (1-3) or non-participation (4-6) in the
following alternations and other constructions (7-8):
1. Causative/inchoative alternation:
Tony broke the window   The window broke
2. Middle alternation:
Tony broke the window   The window broke easily
3. Instrument subject alternation:
Tony broke the window with the hammer   The hammer
broke the window
4. *With/against alternation:
Tony broke the cup against the wall   *Tony broke the
wall with the cup
5. *Conative alternation:
Tony broke the window   *Tony broke at the window
6. *Body-Part possessor ascension alternation:
*Tony broke herself on the arm   Tony broke her arm
7. Unintentional interpretation available (some verbs):
Reflexive object: *Tony broke himself
Body-part object: Tony broke his finger
8. Resultative phrase:
Tony broke the piggy bank open, Tony broke the glass to
pieces
Levin?s taxonomy provides a classification of 3,024
verbs (4,186 senses) into 48 broad and 192 fine-grained
classes according to their participation in 79 alternations
involving NP and PP complements.
Some extensions have recently been proposed to
this resource. Dang et al (1998) have supplemented
the taxonomy with intersective classes: special classes
for verbs which share membership of more than one
Levin class because of regular polysemy. Bonnie Dorr
(University of Maryland) has provided a reformulated
and extended version of Levin?s classification in her LCS
database (http://www.umiacs.umd.edu/  bonnie/verbs-
English.lcs). This resource groups 4,432 verbs (11,000
senses) into 466 Levin-based and 26 novel classes.
The latter are Levin classes refined according to verbal
telicity patterns (Olsen et al, 1997), while the former
are additional classes for non-Levin verbs which do not
fall into any of the Levin classes due to their distinctive
syntactic behaviour (Dorr, 1997).
As a result of this work, the taxonomy has gained con-
siderably in depth, but not to the same extent in breadth.
Verbs taking ADJP, ADVP, ADL, particle, predicative,
control and sentential complements are still largely ex-
cluded, except where they show interesting behaviour
with respect to NP and PP complementation. As many
of these verbs are highly frequent in language, NLP ap-
plications utilizing lexical-semantic classes would bene-
fit greatly from a linguistic resource which provides ad-
equate classification of their senses. When extending
Levin?s classification with new classes, we particularly
focussed on these verbs.
3 Creating Novel Classes
Levin?s original taxonomy was created by
1. selecting a set of diathesis alternations from linguis-
tic resources,
2. classifying a large number of verbs according to
their participation in these alternations,
3. grouping the verbs into semantic classes based on
their participation in sets of alternations.
We adopted a different, faster approach. This involved
1. composing a set of diathesis alternations for verbs
not covered comprehensively by Levin,
2. selecting a set of candidate lexical-semantic classes
for these verbs from linguistic resources,
3. examining whether (sub)sets of verbs in each candi-
date class could be related to each other via alterna-
tions and thus warrant creation of a new class.
In what follows, we will describe these steps in detail.
3.1 Novel Diathesis Alternations
When constructing novel diathesis alternations, we took
as a starting point the subcategorization classification
of Briscoe (2000). This fairly comprehensive classifica-
tion incorporates 163 different subcategorization frames
(SCFs), a superset of those listed in the ANLT (Boguraev
et al, 1987) and COMLEX Syntax dictionaries (Grishman
et al, 1994). The SCFs define mappings from surface
arguments to predicate-argument structure for bounded
dependency constructions, but abstract over specific par-
ticles and prepositions, as these can be trivially instanti-
ated when the a frame is associated with a specific verb.
As most diathesis alternations are only semi-predictable
on a verb-by-verb basis, a distinct SCF is defined for every
such construction, and thus all alternations can be repre-
sented as mappings between such SCFs.
We considered possible alternations between pairs of
SCFs in this classification, focusing in particular on those
SCFs not covered by Levin. The identification of alterna-
tions was done manually, using criteria similar to Levin?s:
the SCFs alternating should preserve the sense in ques-
tion, or modify it systematically.
106 new alternations were discovered using this
method and grouped into different, partly overlapping
categories. Table 1 shows some example alternations and
their corresponding categories. The alternating patterns
are indicated using an arrow (  ). The SCFs are marked
using number codes whose detailed description can be
found in (Briscoe, 2000) (e.g. SCF 53. refers to the COM-
LEX subcategorization class NP-TO-INF-OC).
3.2 Candidate Lexical-Semantic Classes
Starting off from set of candidate classes accelerated the
work considerably as it enabled building on extant lin-
guistic research. Although a number of studies are avail-
able on verb classes not covered by Levin, many of these
assume a classification system completely different to
that of Levin?s, and/or incorporate sense distinctions too
fine-grained for easy integrations with Levin?s classifica-
tion. We therefore restricted our scope to a few classifi-
cations of a suitable style and granularity:
3.2.1 The LCS Database
The LCS database includes 26 classes for verbs which
could not be mapped into any of the Levin classes due
to their distinctive syntactic behaviour. These classes
were originally created by an automatic verb classifica-
tion algorithm described in (Dorr, 1997). Although they
appear semantically meaningful, their syntactic-semantic
properties have not been systematically studied in terms
of diathesis alternations, and therefore re-examination is
warranted.
3.2.2 Rudanko?s Classification
Rudanko (1996, 2000) provides a semantically moti-
vated classification for verbs taking various types of sen-
tential complements (including predicative and control
constructions). His relatively fine-grained classes, orga-
nized into sets of independent taxonomies, have been cre-
ated in a manner similar to Levin?s. We took 43 of Run-
danko?s verb classes for consideration.
3.2.3 Sager?s Classification
Sager (1981) presents a small classification consisting
of 13 classes, which groups verbs (mostly) on the basis
of their syntactic alternations. While semantic properties
are largely ignored, many of the classes appear distinctive
also in terms of semantics.
3.2.4 Levin?s Classification
At least 20 (broad) Levin classes involve verb senses
which take sentential complements. Because full treat-
ment of these senses requires considering sentential com-
plementation, we re-evaluated these classes using our
method.
3.3 Method for Creating Classes
Each candidate class was evaluated as follows:
1. We extracted from its class description (where one
was available) and/or from the COMLEX Syntax dic-
tionary (Grishman et al, 1994) all the SCFs taken by
its member verbs.
2. We extracted from Levin?s taxonomy and from our
novel list of 106 alternations all the alternations
where these SCFs were involved.
3. Where one or several alternations where found
which captured the sense in question, and where the
minimum of two member verbs were identified, a
new verb class was created.
Steps 1-2 were done automatically and step 3 manu-
ally. Identifying relevant alternations helped to identify
additional SCFs, which in turn often led to the discov-
ery of additional alternations. The SCFs and alternations
discovered in this way were used to create the syntactic-
semantic description of each novel class.
For those candidate classes which had an insufficient
number of member verbs, new members were searched
for in WordNet (Miller, 1990). Although WordNet clas-
sifies verbs on a purely semantic basis, the syntactic reg-
ularities studied by Levin are to some extent reflected
Category Example Alternations Alternating SCFs
Equi I advised Mary to go  I advised Mary 53  24
He helped her bake the cake  He helped bake the cake 33  142
Raising Julie strikes me as foolish  Julie strikes me as a fool 143  29
He appeared to her to be ill  It appeared to her that he was ill 99  12
Category He failed in attempting to climb  He failed in the climb 63  87
switches I promised Mary to go  I promised Mary that I will go 54  52
PP deletion Phil explained to him how to do it  Phil explained how to do it 90  17
He contracted with him for the man to go  He contracted for the man to go 88  15
P/C deletion I prefer for her to do it  I prefer her to do it 15  53
They asked about what to do  They asked what to do 73  116
Table 1: Examples of new alternations
by semantic relatedness as it is represented by Word-
Net?s particular structure (e.g. (Fellbaum, 1999)). New
member verbs were frequently found among the syn-
onyms, troponyms, hypernyms, coordinate terms and/or
antonyms of the extant member verbs.
For example, using this method, we gave the following
description to one of the candidate classes of Rudanko
(1996), which he describes syntactically with the single
SCF 63 (see the below list) and semantically by stating
that verbs in this class (e.g. succeed, manage, fail) have
approximate meaning1 ?perform the act of? or ?carry out
the activity of?:
20. SUCCEED VERBS
SCF 22: John succeeded
SCF 87: John succeeded in the climb
SCF 63: John succeeded in attempting the climb
SCF 112: John succeeded to climb
Alternating SCFs: 22  87, 87  63, 22  112
Some of the candidate classes, particularly those of
Rudanko, proved too fine-grained to be helpful for a
Levin type of classification, and were either combined
with other classes or excluded from consideration. Some
other classes, particularly the large ones in the LCS
database, proved too coarse-grained after our method was
applied, and were split down to subclasses.
For example, the LCS class of Coerce Verbs (002) was
divided into four subclasses according to the particular
syntactic-semantic properties of the subsets of its mem-
ber verbs. One of these subclasses was created for verbs
such as force, induce, and seduce, which share the ap-
1Rudanko does not assign unique labels to his classes, and
the descriptions he gives - when taken out of the context - cannot
be used to uniquely identify the meaning involved in a specific
class. For details of this class, see his description in (Rudanko,
1996) page 28.
proximate meaning of ?urge or force (a person) to an ac-
tion?. The sense gives rise to object equi SCFs and alter-
nations:
2. FORCE VERBS
SCF 24: John forced him
SCF 40: John forced him into coming
SCF 49: John forced him into it
SCF 53: John forced him to come
Alternating SCFs: 24  53, 40  49, 49  24
Another subclass was created for verbs such as order
and require, which share the approximate meaning of ?di-
rect somebody to do something?. These verbs take object
raising SCFs and alternations:
3. ORDER VERBS
SCF 57: John ordered him to be nice
SCF 104: John ordered that he should be nice
SCF 106: John ordered that he be nice
Alternating SCFs: 57  104, 104  106
New subclasses were also created for those Levin
classes which did not adequately account for the varia-
tion among their member verbs. For example, a new class
was created for those 37. Verbs of Communication which
have an approximate meaning of ?make a proposal? (e.g.
suggest, recommend, propose). These verbs take a rather
distinct set of SCFs and alternations, which differ from
those taken by other communication verbs. This class
is somewhat similar in meaning to Levin?s 37.9 Advise
Verbs. In fact, a subset of the verbs in 37.9 (e.g. ad-
vise, instruct) participate in alternations prototypical to
this class (e.g. 104  106) but not, for example, in the
ones involving PPs (e.g. 103  116).
47. SUGGEST VERBS
SCF 16: John suggested how she could do it
SCF 17: John suggested how to do it
SCF 24: John suggested it
SCF 49: John suggested it to her
SCF 89: John suggested to her how she could do it
SCF 90: John suggested to her how to do it
SCF 97: John suggested to her that she would do it
SCF 98: John suggested to her that she do it
SCF 101: John suggested to her what she could do
SCF 103: John suggested to her what to do
SCF 104: John suggested that she could do it
SCF 106: John suggested that she do it
SCF 114: John suggested what she could do
SCF 116: John suggested what to do
Alternating SCFs: 16  17, 24  49, 89  16,
90  17, 97  104, 98  106, 101  114,
103  116, 104  106
Our work resulted in accepting, rejecting, combining
and refining the 102 candidate classes and - as a by-
product - identifying 5 new classes not included in any
of the resources we used. In the end, 57 new verb classes
were formed, each associated with 2-45 member verbs.
Those Levin or Dorr classes which were examined but
found distinctive enough as they stand are not included
in this count. However, their possible subclasses are, as
well as any of the classes adapted from the resources of
Rudanko or Sager. The new classes are listed in table 2,
along with example verbs.
4 Evaluation
4.1 Task-Based Evaluation
We performed an experiment in the context of automatic
SCF acquisition to investigate whether the new classes
can be used to support an important NLP task. The task is
to associate classes to specific verbs along with an es-
timate of the conditional probability of a SCF given a
specific verb. The resulting valency or subcategorization
lexicon can be used by a (statistical) parser to recover
predicate-argument structure.
Our test data consisted of a total of 35 verbs from 12
new verb classes. The classes were chosen at random,
subject to the constraint that their member verbs were fre-
quent enough in corpus data. A minimum of 300 corpus
occurrences per verb is required to yield a reliable SCF
distribution for a polysemic verb with multiple SCFs (Ko-
rhonen, 2002). We took a sample of 20 million words of
the British National Corpus (BNC) (Leech, 1992) and ex-
tracted all sentences containing an occurrence of one of
the test verbs. After the extraction process, we retained
Class Example Verbs
1. URGE ask, persuade
2. FORCE manipulate, pressure
3. ORDER command, require
4. WANT need, want
5. TRY attempt, try
6. WISH hope, expect
7. ENFORCE impose, risk
8. ALLOW allow, permit
9. ADMIT include, welcome
10. CONSUME spend, waste
11. PAY pay, spend
12. FORBID prohibit, ban
13. REFRAIN abstain, refrain
14. RELY bet, count
15. CONVERT convert, switch
16. SHIFT resort, return
17. ALLOW allow, permit
18. HELP aid, assist
19. COOPERATE collaborate, work
20. SUCCEED fail, manage
21. NEGLECT omit, fail
22. LIMIT restrict, restrain
23. APPROVE accept, object
24. ENQUIRE ask, consult
25. CONFESS acknowledge, reveal
26. INDICATE demonstrate, imply
27. DEDICATE devote, commit
28. FREE cure, relieve
29. SUSPECT accuse, condemn
30. WITHDRAW retreat, retire
31. COPE handle, deal
32. DISCOVER hear, learn
33. MIX pair, mix
34. CORRELATE coincide, alternate
35. CONSIDER imagine, remember
36. SEE notice, feel
37. LOVE like, hate
38. FOCUS focus, concentrate
39. CARE mind, worry
40. DISCUSS debate, argue
41. BATTLE fight, communicate
42. SETTLE agree, contract
43. SHOW demonstrate, quote
44. ALLOW allow, permit
45. EXPLAIN write, read
46. LECTURE comment, remark
47. SUGGEST propose, recommend
48. OCCUR happen, occur
49. MATTER count, weight
50. AVOID miss, boycott
51. HESITATE loiter, hesitate
52. BEGIN continue, resume
53. STOP terminate, finish
54. NEGLECT overlook, neglect
55. CHARGE commit, charge
56. REACH arrive, hit
57. ADOPT assume, adopt
Table 2: New Verb Classes
1000 citations, on average, for each verb.
Our method for SCF acquisition (Korho-
nen, 2002) involves first using the system of
Briscoe and Carroll (1997) to acquire a putative SCF dis-
tribution for each test verb from corpus data. This system
employs a robust statistical parser (Briscoe and Carroll,
2002) which yields complete though shallow parses from
the PoS tagged data. The parse contexts around verbs
are passed to a comprehensive SCF classifier, which
selects one of the 163 SCFs. The SCF distribution is then
smoothed with the back-off distribution corresponding
to the semantic class of the predominant sense of a verb.
Although many of the test verbs are polysemic, we relied
on the knowledge that the majority of English verbs have
a single predominating sense in balanced corpus data
(Korhonen and Preiss, 2003).
The back-off estimates were obtained by the following
method:
(i) A few individual verbs were chosen from a new
verb class whose predominant sense according to the
WordNet frequency data belongs to this class,
(ii) SCF distributions were built for these verbs by man-
ually analysing c. 300 occurrences of each verb in
the BNC,
(iii) the resulting SCF distributions were merged.
An empirically-determined threshold was finally set on
the probability estimates from smoothing to reject noisy
SCFs caused by errors during the statistical parsing phase.
This method for SCF acquisition is highly sensitive to
the accuracy of the lexical-semantic classes. Where a
class adequately predicts the syntactic behaviour of the
predominant sense of a test verb, significant improvement
is seen in SCF acquisition, as accurate back-off estimates
help to correct the acquired SCF distribution and deal
with sparse data. Incorrect class assignments or choice
of classes can, however, degrade performance.
The SCFs were evaluated against manually analysed
corpus data. This was obtained by annotating a maximum
of 300 occurrences for each test verb in the BNC data. We
calculated type precision (the percentage of SCF types
that the system proposes which are correct), type recall
(the percentage of SCF types in the gold standard that the
system proposes) and F  -measure2. To investigate how
well the novel classes help to deal with sparse data, we
recorded the total number of SCFs missing in the distri-
butions, i.e. false negatives which did not even occur in
the unthresholded distributions and were, therefore, never
hypothesized by the parser and classifier. We also com-
pared the similarity between the acquired unthresholded
2 
	 ffProceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 33?40,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
I will shoot your shopping down and you can shoot all my tins
Automatic Lexical Acquisition from the CHILDES Database
Paula Buttery and Anna Korhonen
RCEAL, University of Cambridge
9 West Road, Cambridge, CB3 9DB, UK
pjb48, alk23@cam.ac.uk
Abstract
Empirical data regarding the syntactic com-
plexity of children?s speech is important for
theories of language acquisition. Currently
much of this data is absent in the annotated
versions of the CHILDES database. In this
perliminary study, we show that a state-of-
the-art subcategorization acquisition system of
Preiss et al (2007) can be used to extract large-
scale subcategorization (frequency) informa-
tion from the (i) child and (ii) child-directed
speech within the CHILDES database without
any domain-specific tuning. We demonstrate
that the acquired information is sufficiently ac-
curate to confirm and extend previously re-
ported research findings. We also report quali-
tative results which can be used to further im-
prove parsing and lexical acquisition technol-
ogy for child language data in the future.
1 Introduction
Large empirical data containing children?s speech are
the key to developing and evaluating different theo-
ries of child language acquisition (CLA). Particularly
important are data related to syntactic complexity of
child language since considerable evidence suggests
that syntactic information plays a central role during
language acquisition, e.g. (Lenneberg, 1967; Naigles,
1990; Fisher et al, 1994).
The standard corpus in the study of CLA is the
CHILDES database (MacWhinney, 2000)1 which pro-
vides 300MB of transcript data of interactions be-
1See http://childes.psy.cmu.edu for details.
tween children and parents over 25 human languages.
CHILDES is currently available in raw, part-of-speech-
tagged and lemmatized formats. However, adequate
investigation of syntactic complexity requires deeper
annotations related to e.g. syntactic parses, subcatego-
rization frames (SCFs), lexical classes and predicate-
argument structures.
Although manual syntactic annotation is possible,
it is extremely costly. The alternative is to use natu-
ral language processing (NLP) techniques for annota-
tion. Automatic techniques are now viable, cost effec-
tive and, although not completely error-free, are suffi-
ciently accurate to yield annotations useful for linguis-
tic purposes. They also gather important qualitative
and quantitative information, which is difficult for hu-
mans to obtain, as a side-effect of the acquisition pro-
cess.
For instance, state-of-the-art statistical parsers,
e.g. (Charniak, 2000; Briscoe et al, 2006), have wide
coverage and yield grammatical representations capa-
ble of supporting various applications (e.g. summa-
rization, information extraction). In addition, lexi-
cal information (e.g. subcategorization, lexical classes)
can now be acquired automatically from parsed
data (McCarthy and Carroll, 2003; Schulte im Walde,
2006; Preiss et al, 2007). This information comple-
ments the basic grammatical analysis and provides ac-
cess to the underlying predicate-argument structure.
Containing considerable ellipsis and error, spoken
child language can be challenging for current NLP
techniques which are typically optimized for written
adult language. Yet Sagae et al (2005) have recently
demonstrated that existing statistical parsing tech-
niques can be usefully modified to analyse CHILDES
33
with promising accuracy. Although further improve-
ments are still required for optimal accuracy, this re-
search has opened up the exciting possibility of auto-
matic grammatical annotation of the entire CHILDES
database in the future.
However, no work has yet been conducted on au-
tomatic acquisition of lexical information from child
speech. The only automatic lexical acquisition study
involving CHILDES that we are aware of is that of
Buttery and Korhonen (2005). The study involved
extracting subcategorization information from (some
of) the adult (child-directed) speech in the database,
and showing that this information differs from that ex-
tracted from the spoken part of the British National
Corpus (BNC) (Burnard, 1995).
In this paper, we investigate whether state-of-the-
art subcategorization acquisition technology can be
used?without any domain-specific tuning?to obtain
large-scale verb subcategorization frequency informa-
tion from CHILDES which is accurate enough to show
differences and similarities between child and adult
speech, and thus be able to provide support for syn-
tactic complexity studies in CLA.
We use the new system of Preiss et al (2007) to
extract SCF frequency data from the (i) child and
(ii) child-directed speech within CHILDES. We show
that the acquired information is sufficiently accu-
rate to confirm and extend previously reported SCF
(dis)similarities between the two types of data. In par-
ticular, we demonstrate that children and adults have
different preferences for certain types of verbs, and
that these preferences seem to influence the way chil-
dren acquire subcategorization. In addition, we report
qualitative results which can be used to further im-
prove parsing and lexical acquisition technology for
spoken child language data in the future.
2 Subcategorization Acquisition System
We used for subcategorization acquisition the new sys-
tem of Preiss, Briscoe and Korhonen (2007) which
is essentially a much improved and extended version
of Briscoe and Carroll?s (1997) system. It incorpo-
rates 168 SCF distinctions, a superset of those found
in the COMLEX Syntax (Grishman et al, 1994) and
ANLT (Boguraev et al, 1987) dictionaries. Currently,
SCFs abstract over specific lexically governed parti-
cles and prepositions and specific predicate selectional
preferences but include some derived semi-predictable
bounded dependency constructions, such as particle
and dative movement?this will be revised in future
versions of the SCF system.
The system tokenizes, tags, lemmatizes and parses
input sentences using the recent (second) release of
the RASP (Robust Accurate Statistical Parsing) system
(Briscoe et al, 2006) which parses arbitrary English
text with state-of-the-art levels of accuracy. SCFs are
extracted from the grammatical relations (GRs) output
of the parser using a rule-based classifier. This clas-
sifier operates by exploiting the close correspondence
between the dependency relationships which the GRs
embody and the head-complement structure which
subcategorization acquisition attempts to recover. Lex-
ical entries of extracted SCFs are constructed for each
word in the corpus data. Finally, the entries may be
optionally filtered to obtain a more accurate lexicon.
This is done by setting empirically determined thresh-
olds on the relative frequencies of SCFs.
When evaluated on cross-domain corpora contain-
ing mainly adult language, this system achieves 68.9
F-measure2 in detecting SCF types?a result which
compares favourably to those reported with other com-
parable SCF acquisition systems.
3 Data
The English (British and American) sections of the
CHILDES database (MacWhinney, 2000) were used to
create two corpora: 1) CHILD and 2) CDS. Both cor-
pora contained c. 1 million utterances which were se-
lected from the data after some utterances contain-
ing un-transcribable sections were removed. Speak-
ers were identified using speaker-id codes within the
CHAT transcriptions of the data:3 CHILD contained
the utterances of speakers identified as target children;
CDS contained input from speakers identified as par-
ents/caretakers. The mean utterance length (measured
in words) in CHILD and CDS were 3.48 and 4.61, re-
spectively. The mean age of the child speaker in CHILD
is around 3 years 6 months.4
2See Section 4 for details of F-measure.
3CHAT is the transcription and coding format used by all the
transcriptions within CHILDES.
4The complete age range is from 1 year and 1 month up to 7
years.
34
3.1 Test Verbs and SCF Lexicons
We selected a set of 161 verbs for experimentation.
The words were selected at random, subject to the con-
straint that a sufficient number of SCFs would be ex-
tracted (> 100) from both corpora to facilitate max-
imally useful comparisons. All sentences containing
an occurrence of one of the test verbs were extracted
from the two corpora and fed into the SCF acquisition
system described earlier in section 2.
In some of our experiments the two lexicons were
compared against the VALEX lexicon (Korhonen et al,
2006)?a large subcategorization lexicon for English
which was acquired automatically from several cross-
domain corpora (containing both written and spoken
language). VALEX includes SCF and frequency infor-
mation for 6,397 English verbs. We employed the
most accurate version of the lexicon here (87.3 F-
measure)?this lexicon was obtained by selecting high
frequency SCFs and supplementing them with lower
frequency SCFs from manually built lexicons.
4 Analysis
4.1 Methods for Analysis
The similarity between verb and SCF distributions in
the lexicons was examined. To maintain a robust anal-
ysis in the presence of noise, multiple similarity mea-
sures were used to compare the verb and SCF distri-
butions (Korhonen and Krymolowski, 2002). In the
following p = (p
i
) and q = (q
i
) where p
i
and q
i
are
the probabilities associated with SCF
i
in distributions
(lexicons) P and Q:
? Intersection (IS) - the intersection of non-zero probability
SCFs in p and q;
? Spearman rank correlation (RC) - lies in the range [1; 1], with
values near 0 denoting a low degree of association and val-
ues near -1 and 1 denoting strong association;
? Kullback-Leibler (KL) distance - a measure of the additional
information needed to describe p using q, KL is always ? 0
and = 0 only when p ? q;
The SCFs distributions acquired from the corpora for
the chosen words were evaluated against: (i) a gold
standard SCF lexicon created by merging the SCFs in
the COMLEX and ANLT syntax dictionaries?this en-
abled us to determine the accuracy of the acquired
SCFs; (ii) another acquired SCF lexicon (as if it were
a gold standard)?this enabled us to determine simi-
larity of SCF types between two lexicons. In each case
Verb CHILD CDS
go 1 1
want 2 2
get 3 3
know 4 4
put 5 6
see 6 5
come 7 10
like 8 7
make 9 11
say 10 8
take 11 13
eat 12 14
play 13 15
need 14 16
look 15 12
fall 16 22
sit 17 21
think 18 9
break 19 27
give 20 17
Table 1: Ranks of the 20 most frequent verbs in CHILD
and in CDS
we recorded the number of true positives (TPs), correct
SCFs, false positives (FPs), incorrect SCFs, and false
negatives (FNs), correct SCFs not in the gold standard.
Using these counts, we calculated type precision
(the percentage of SCF types in the acquired lexicon
which are correct), type recall (the percentage of SCF
types in the gold standard that are in the lexicon) and
F-measure:
F =
2 ? precision ? recall
precision + recall
(1)
4.2 Verb Analysis
Before conducting the SCF comparisons we first com-
pared (i) our 161 test verbs and (ii) all the 1212
common verbs and their frequencies in CHILD and
CDS using the Spearman rank correlation (RC) and
the Kullback-Leibler distance (KL). The result was
a strong correlation between the 161 test verbs (RC =
0.920 ? 0.0791, KL = 0.05) as well as between all the
1212 verbs (RC = 0.851 ? 0.0287, KL = 0.07) in the
two corpora.
These figures suggest that the child-directed speech
(which is less diverse in general than speech between
adults, see e.g. the experiments of Buttery and Ko-
rhonen (2005)) contains a very similar distribution of
verbs to child speech. This is to be expected since the
35
corpora essentially contain separate halves of the same
interactions.
However, our large-scale frequency data makes it
possible to investigate the cause for the apparently
small differences in the distributions. We did this by
examining the strength of correlation throughout the
ranking. We compared the ranks of the individual
verbs and discovered that the most frequent verbs in
the two corpora have indeed very similar ranks. Ta-
ble 1 lists the 20 most frequent verbs in CHILD (starting
from the highest ranked verb) and shows their ranks
in CDS. As illustrated in the table, the top 4 verbs
are identical in the two corpora (go, want, get, know)
while the top 15 are very similar (including many ac-
tion verbs e.g. put, look, sit, eat, and play).
Yet some of the lower ranked verbs turned out to
have large rank differences between the two corpora.
Two such relatively highly ranked verbs are included
in the table?think which has a notably higher rank
in CDS than in CHILD, and break which has a higher
rank in CHILD than in CDS. Many other similar cases
were found in particular among the medium and low
frequency verbs in the two corpora.
To obtain a better picture of this, we calculated for
each verb its rank difference between CHILD vs. CDS.
Table 2 lists 40 verbs with substantial rank differences
between the two corpora. The first column shows
verbs which have higher ranks in CHILD than in CDS,
and the second column shows verbs with higher ranks
in CDS than in CHILD. We can see e.g. that children
tend to prefer verbs such as shoot, die and kill while
adults prefer verbs such as remember, send and learn.
To investigate whether these differences in pref-
erences are random or motivated in some manner,
we classified the verbs with the largest differences
in ranks (>10) into appropriate Levin-style lexical-
semantic classes (Levin, 1993) according to their pre-
dominant senses in the two corpora.5 We discovered
that the most frequent classes among the verbs that
children prefer are HIT (e.g. bump, hit, kick), BREAK
(e.g. crash, break, rip), HURT (e.g. hurt, burn, bite)
and MOTION (e.g. fly, jump, run) verbs. Overall, many
of the preferred verbs (regardless of the class) express
negative actions or feelings (e.g. shoot, die, scare,
hate).
5This classification was done manually to obtain a reliable re-
sult.
CHILD CDS
shoot tie remember hope
hate wish send suppose
die cut learn bet
write crash wipe kiss
use kick pay smell
bump scare feed guess
win step ask change
lock burn feel set
fight stand listen stand
jump care wait wonder
Table 2: 20 verbs ranked higher in (i) child speech and
(ii) child-directed speech.
In contrast, adults have a preference for verbs from
classes expressing cognitive processes (e.g. remember,
suppose, think, wonder, guess, believe, hope, learn) or
those that can be related to the education of children,
e.g. the WIPE verbs wash, wipe and brush and the PER-
FORMANCE verbs draw, dance and sing. In contrast to
children, adults prefer verbs which express positive ac-
tions and feelings (e.g. share, help, love, kiss).
It is commonly reported that child CLA is moti-
vated by a wish to communicate desires and emo-
tions, e.g. (Pinker, 1994), but a relative preference
in child speech over child-directed speech for certain
verb types or verbs expressing negative actions and
feelings has not been explicitly shown on such a scale
before. While this issue requires further investigation,
our findings already demonstrate the value of using
large scale corpora in producing novel data and hy-
potheses for research in CLA.
4.3 SCF Analysis
4.3.1 Quantitative SCF Comparison
The average number of SCFs taken by studied verbs
in the two corpora proved quite similar. In unfil-
tered SCF distributions, verbs in CDS took on average
a larger number of SCFs (29) than those in CHILD (24),
but in the lexicons filtered for accuracy the numbers
were identical (8?10, depending on the filtering thresh-
old applied). The intersection between the CHILD /
CDS SCFs and those in the VALEX lexicon was around
0.5, indicating that the two lexicons included only
50% of the SCFs in the lexicon extracted from general
(cross-domain) adult language corpora. Recall against
VALEXwas consequently low (between 48% and 68%
depending on the filtering threshold) but precision was
around 50-60% for both CHILDES and CDS lexicons
36
Measures Unfilt. Filt.
Precision (%) 82.9 88.7
Recall (%) 69.3 44.5
F-measure 75.5 59.2
IS 0.73 0.62
RC 0.69 0.72
KL 0.33 0.46
Table 3: Average results when SCF distributions in
CHILD and CDS are compared against each other.
(also depending on the filtering threshold), which is
a relatively good result for the challenging CHILDES
data. However, it should be remembered that with this
type of data it would not be expected for the SCF sys-
tem to achieve as high precision and recall as it would
on, for instance, adult written text and that the missing
SCFs and/or misclassified SCFs are likely to provide us
with the most interesting information.
As expected, there were differences between the
SCF distributions in the two lexicons. Table 3 shows
the results when the CHILD and CDS lexicons are com-
pared against each other (i.e. using the CDS as a gold
standard). The comparison was done using both the
unfiltered and filtered (using relative frequency thresh-
old of 0.004) versions of the lexicons. The similarity
in SCF types is 75.5 according to F-measure in the un-
filtered lexicons and 59.2 in filtered ones.6
4.3.2 Qualitative SCF Comparison
Our qualitative analysis of SCFs in the two corpora
revealed reasons for the differences. Table 4 lists the
10 most frequent SCFs in CHILD (starting from the
highest ranked SCF), along with their ranks in CDS
and VALEX. The top 3 SCFs (NP, INTRANSITIVE and
PP frames) are ranked quite similarly in all the cor-
pora. Looking at the top 10 SCFs, CHILD appears,
as expected, more similar to CDS than with VALEX,
but large differences can be detected in lower ranked
frames.
To identify those frames, we calculated for each SCF
its difference in rank between CHILD vs. CDS. Table 5
exemplifies some of the SCFs with the largest rank
differences. Many of these concern frames involving
sentential complementation. Children use more fre-
6The fact that the unfiltered lexicons appear so much more sim-
ilar suggests that some of the similarity is due to similarity in in-
correct SCFs (many of which are low in frequency, i.e. fall under
the threshold).
quently than adults SCFs involving THAT and HOW
complementation, while adults have a preference for
SCFs involving WHETHER, ING and IF complementa-
tion.
Although we have not yet looked at SCF differences
across ages, these discoveries are in line with previous
findings, e.g. (Brown, 1973), which indicate that chil-
dren master the sentential complementation SCFs pre-
ferred by adults (in our experiment) fairly late in the
acquisition process. With a mean utterance length for
CHILD at 3.48, we would expect to see relatively few of
these frames in the CHILD corpus?and consequently
a preference for the simpler THAT constructions.
4.4 The Impact of Verb Type Preferences on SCF
Differences
Given the new research findings reported in Sec-
tion 4.2 (i.e. the discovery that children and adults have
different preferences for many medium-low frequency
verbs) we investigated whether verb type preferences
play a role in SCF differences between the two corpora.
We chose for experimentation 10 verbs from 3 groups:
1. Group 1 ? verbs with similar ranks in CHILD and CDS: bring,
find, give, know, need, put, see, show, tell, want
2. Group 2 ? verbs with higher ranks in CDS: ask, feel, guess,
help, learn, like, pull, remember, start, think
3. Group 3 ? verbs with higher ranks in CHILD: break, die,
forget, hate, hit, jump, scare, shoot, burn, wish
The test verbs were selected randomly, subject to
the constraint that their absolute frequencies in the two
corpora were similar.7 We first correlated the unfil-
tered SCF distributions of each test verb in the two cor-
pora against each other and calculated the similarity in
the SCF types using the F-measure. We then evaluated
for each group, the accuracy of SCFs in unfiltered dis-
tributions against our gold standard (see Section 4.1).
Because the gold standard was too ambitious in terms
of recall, we only calculated the precision figures: the
average number of TP and FP SCFs taken by test verbs.
The results are included in Table 6. Verbs in Group
1 show the best SCF type correlation (84.7 F-measure)
between the two corpora although they are the rich-
est in terms of subcategorization (they take the highest
number of SCFs out of the three groups). The SCF cor-
relation is clearly lower in Groups 2 and 3, although
7This requirement was necessary because frequency may influ-
ence subcategorization acquisition performance.
37
SCF Example sentence CHILD CDS VALEX
NP I love rabbits 1 1 1
INTRANS I sleep with a pillow and blanket 2 2 2
PP He can jump over the fence 3 4 3
PART I can?t give up 4 7 9
TO-INF-SC I want to play with something else 5 3 6
PART-NP/NP-PART He looked it up 6 6 7
NP-NP Ask her all these questions 7 5 18
NP-INF-OC Why don?t you help her put the blocks in the can ? 8 9 60
INTR-RECIP So the kitten and the dog won?t fight 9 8 48
NP-PP He put his breakfast in the bin 10 10 4
Table 4: 10 most frequent SCFs in CHILD, along with their ranks in CDS and VALEX.
SCF Example sentence
CHILD MP I win twelve hundred dollars
INF-AC You can help me wash the dishes
PP-HOW-S He explained to her how she did it
HOW-TO-INF Daddy can you tell me how to spell Christmas carols?
NP-S He did not tell me that it was gonna cost me five dollars
CDS ING-PP Stop throwing a tantrum
NP-AS-NP I sent him as a messenger
NP-WH-S I?ll tell you whether you can take it off
IT WHS, SUBTYPE IF How would you like it if she pulled your hair?
NP-PP-PP He turned it from a disaster into a victory
Table 5: Typical SCFs with higher ranks in (i) CHILD and (ii) CDS.
Measures Group1 Group2 Group3
SCF similarity F-measure 84.7 72.17 75.60
SCF accuracy TPs CDS 12 11 7
TPs CHILD 10 9 8
FPs CDS 36 29 13
FPs CHILD 32 18 15
Table 6: Average results for 3 groups when (i) unfil-
tered SCF distributions in CHILD and CDS are com-
pared against each other (SCF similarity) and when (ii)
the SCFs in the distributions are evaluated against a
gold standard (SCF accuracy).
the verbs in these groups take fewer SCFs. Interest-
ingly, Group 3 is the only group where children pro-
duce more TPs and FPs on average than adults do, i.e.
both correct and incorrect SCFs which are not exem-
plified in the adult speech. The frequency effects con-
trolled, the reason for these differences is likely to lie
in the differing relative preferences children and adults
have for verbs in groups 2 and 3, which we think may
impact the richness of their language.
4.5 Further Analysis of TP and FP Differences
We looked further at the interesting TP and FP differ-
ences in Group 3 to investigate whether they tell us
something about (i) how children learn SCFs (via both
TPs and FPs), and (ii) how the parsing / SCF extraction
system could be improved for CHILDES data in the fu-
ture (via the FPs).
We first made a quantitative analysis of the rela-
tive difference in TPs and FPs for all the SCFs in both
corpora. The major finding of this high level anal-
ysis was a significantly high FP rate for some ING
frames (e.g. PART-ING-SC, ING-NP-OMIT, NP-ING-
OC) within CHILD (e.g. ?car going hit?, ?I hurt hand
moving?). This agrees with many previous studies,
e.g. (Brown, 1973), which have shown that children
overextend and incorrectly use the ?ing? morpheme
during early acquisition.
A qualitative analysis of the verbs from Group 3 was
then carried out, looking for the following scenarios:
? SCF is a FP in both CHILD and CDS - either i) the
gold standard is incomplete, or ii) there is error in
the parser/subcategorization system with respect to the
CHILDES domain.
? SCF is a TP in CDS and not present in CHILD - children have
not acquired the frame despite exposure to it (perhaps it is
complicated to acquire).
? SCF is a TP in CHILD but not present in CDS - adults are
not using the frame but the children have acquired it. This
indicates that either i) children are acquiring the frame from
elsewhere in their environment (perhaps from a television),
38
NP-INF NP-NP
INTRANS 
ADJP 
PART 
NP 
PP 
PART-NP 
PART-NP-PP PART-PP
PP-PP PP-BASE 
NP-S NP-PP NP-ADJP 
NP-NP-up 
Figure 1: SCFs obtained for the verb shoot
or ii) there is a misuse of the verb?s semantic class in child
speech.
? SCF is a FP in CHILD but not present in CDS - children should
not have been exposed to this frame but they have acquired
it. This indicates either i) a misuse of the verb?s semantic
class, or ii) error in the parsing/subcategorization technology
with respect to the child-speech domain.
These scenarios are illustrated in Figure 1 which
graphically depicts the differences in TPs and FPs for
the verb shoot. The SCFs have been arranged in a
complexity hierarchy where complexity is defined in
terms of increasing argument structure.8 SCFs found
within our ANLT-COMLEX gold standard lexicon for
shoot are indicated in bold-face. A right-angled rect-
angle drawn around a SCF indicates that the frame
is present in CHILD?a solid line indicating a strong
presence (relative frequency > 0.010) and a dotted
line indicating a weak presence (relative frequency >
0.005). Rounded-edge rectangles represent the pres-
ence of SCFs within CDS similarly. For example, the
frame NP represents a TP in both CHILD and CDS and
the frame NP-NP represents a FP within CHILD.
With reference to Figure 1, we notice that all of
the SCFs present in CHILD are directly connected
within the hierarchy and there is a tendency for weakly
present SCFs to inherit from those strongly present. A
possible explanation for this is that children are ex-
ploring SCFs?trying out frames that are slightly more
complex than those already acquired (for a learning
8For instance, the intransitive frame INTRANS is less complex
than the transitive frame NP, which in turn is less complex than the
di-transitive frame NP-NP. For a detailed description of all SCFs
see (Korhonen, 2002).
algorithm that exploits such a hypothesis in general
see (Buttery, 2006)).
The SCF NP-NP is strongly present in CHILD de-
spite being a FP. Inspection of the associated utter-
ances reveals that some instances NP-NP are legitimate
but so uncommon in adult language that they are omit-
ted from the gold-standard (e.g. ?can i shoot us all to
pieces?. However, other instances demonstrate a mis-
understanding of the semantic class of the verb; there
is possible confusion with the semantic class of send
or throw (e.g. ?i shoot him home?).
The frame NP-INF is a FP in both corpora and a fre-
quent FP in CHILD. Inspection of the associated utter-
ances flags up a parsing problem. Frame NP-INF can
be illustrated by the sentences ?he helped her bake the
cake? or ?he made her sing?, however, within CHILD
the NP-INF has been acquired from utterances such
as ?i want ta shoot him?. The RASP parser has mis-
tagged the word ?ta? leading to a misclassification
by the SCF extraction system. This problem could be
solved by augmenting RASP?s current grammar with a
lexical entry specifying ?ta? as an alternative to infini-
tival ?to?.
In summary, our analysis of TP and FP differ-
ences has confirmed previous studies regarding the
nature of child speech (the over-extension of the
?ing? morpheme). It has also demonstrated that
TP/FP analysis can be a useful diagnostic for pars-
ing/subcategorization extraction problems within a
new data domain. Further, we suggest that analysis
of FPs can provide empirical data regarding the man-
ner in which children learn the semantic classes of
39
verbs (a matter that has been much debated e.g. (Levin,
1993), (Brooks and Tomasello, 1999)).
5 Conclusion
We have reported the first experiment for automatically
acquiring verbal subcategorization from both child and
child-directed parts of the CHILDES database. Our re-
sults show that a state-of-the-art subcategorization ac-
quisition system yields useful results on challenging
child language data even without any domain-specific
tuning. It produces data which is accurate enough
to confirm and extend several previous research find-
ings in CLA. We explore the discovery that children
and adults have different relative preferences for cer-
tain verb types, and that these preferences influence
the way children acquire subcategorization. Our work
demonstrates the value of using NLP technology to an-
notate child language data, particularly where manual
annotations are not readily available for research use.
Our pilot study yielded useful information which will
help us further improve both parsing and lexical ac-
quisition performance on spoken/child language data.
In the future, we plan to optimize the technology so
that it can produce higher quality data for investiga-
tion of syntactic complexity in this domain. Using the
improved technology we plan to then conduct a more
thorough investigation of the interesting CLA topics
discovered in this study?first concentrating on SCF
differences in child speech across age ranges.
References
B. Boguraev, J. Carroll, E. J. Briscoe, D. Carter, and C. Grover.
1987. The derivation of a grammatically-indexed lexicon from
the Longman Dictionary of Contemporary English. In Proc. of
the 25th Annual Meeting of ACL, pages 193?200, Stanford, CA.
E Briscoe and J Carroll. 1997. Automatic extraction of subcatego-
rization from corpora. In 5th ACL Conference on Applied Nat-
ural Language Processing, pages 356?363, Washington, DC.
ACL.
E. J. Briscoe, J. Carroll, and R. Watson. 2006. The second re-
lease of the rasp system. In Proc. of the COLING/ACL 2006
Interactive Presentation Sessions, Sydney, Australia.
P Brooks and M Tomasello. 1999. Young children learn to pro-
duce passives with nonce verbs. Developmental Psychology,
35:29?44.
R Brown. 1973. A first Language: the early stages. Harvard
University Press, Cambridge, MA.
L. Burnard, 1995. The BNC Users Reference Guide. British Na-
tional Corpus Consortium, Oxford, May.
P. Buttery and A. Korhonen. 2005. Large-scale analysis of verb
subcategorization differences between child directed speech
and adult speech. In Proceedings of the Interdisciplinary Work-
shop on the Identification and Representation of Verb Features
and Verb Classes, Saarbrucken, Germany.
P Buttery. 2006. Computational Models for First Language Ac-
quisition. Ph.D. thesis, University of Cambridge.
E. Charniak. 2000. A maximum-entropy-inspired parser. In Pro-
ceedings of the 1st Meeting of the North American Chapter of
the Association for Computational Linguistics, Seattle, WA.
C. Fisher, G. Hall, S. Rakowitz, and L. Gleitman. 1994. When
it is better to receive than to give: syntactic and conceptual
constraints on vocabulary growth. Lingua, 92(1?4):333?375,
April.
R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX Syn-
tax: Building a Computational Lexicon. In Proc. of COLING,
Kyoto.
A. Korhonen and Y. Krymolowski. 2002. On the Robustness of
Entropy-Based Similarity Measures in Evaluation of Subcate-
gorization Acquisition Systems. In Proc. of the 6th CoNLL,
pages 91?97, Taipei, Taiwan.
A. Korhonen, Y. Krymolowski, and E. J. Briscoe. 2006. A large
subcategorization lexicon for natural language processing ap-
plications. In Proc. of the 5th LREC, Genova, Italy.
A Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis,
University of Cambridge. Thesis published as Technical Report
UCAM-CL-TR-530.
E Lenneberg. 1967. Biological Foundations of Language. Wiley
Press, New York, NY.
B Levin. 1993. English Verb Classes and Alternations. Chicago
University Press, Chicago, IL.
B. MacWhinney. 2000. The CHILDES Project: Tools for Analyz-
ing Talk. Lawrence Erlbaum, Mahwah, NJ, 3rd edition.
D. McCarthy and J. Carroll. 2003. Disambiguating nouns, verbs,
and adjectives using automatically acquired selectional prefer-
ences. Computational Linguistics, 29(4).
L Naigles. 1990. Children use syntax to learn verb meanings.
Journal of Child Language, 17:357?374.
S Pinker. 1994. The Language Instinct: How the Mind Creates
Language. Harper Collins, New York, NY.
J. Preiss, E. J. Briscoe, and A. Korhonen. 2007. A system for
large-scale acquisition of verbal, nominal and adjectival sub-
categorization frames from corpora. In Proceedings of the 45th
Annual Meeting of ACL, Prague, Czech Republic. To appear.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Automatic mea-
surement of syntactic development in child langugage. In Pro-
ceedings of the 42nd Meeting of the Association for Computa-
tional Linguistics, Ann Arbor, Michigan.
S. Schulte im Walde. 2006. Experiments on the automatic induc-
tion of german semantic verb classes. Computational Linguis-
tics, 32(2):159?194.
40
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 74?82,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Unsupervised and Constrained Dirichlet Process Mixture Models for Verb
Clustering
Andreas Vlachos
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
av308l@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
alk23@cl.cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
Cambridge CB2 1PZ, UK
zoubin@eng.cam.ac.uk
Abstract
In this work, we apply Dirichlet Process
Mixture Models (DPMMs) to a learning
task in natural language processing (NLP):
lexical-semantic verb clustering. We thor-
oughly evaluate a method of guiding DP-
MMs towards a particular clustering so-
lution using pairwise constraints. The
quantitative and qualitative evaluation per-
formed highlights the benefits of both
standard and constrained DPMMs com-
pared to previously used approaches. In
addition, it sheds light on the use of evalu-
ation measures and their practical applica-
tion.
1 Introduction
Bayesian non-parametric models have received a
lot of attention in the machine learning commu-
nity. These models have the attractive property
that the number of components used to model
the data is not fixed in advance but is actually
determined by the model and the data. This
property is particularly interesting for NLP where
many tasks are aimed at discovering novel, pre-
viously unknown information in corpora. Recent
work has applied Bayesian non-parametric mod-
els to anaphora resolution (Haghighi and Klein,
2007), lexical acquisition (Goldwater, 2007) and
language modeling (Teh, 2006) with good results.
Recently, Vlachos et al (2008) applied the ba-
sic models of this class, Dirichlet Process Mix-
ture Models (DPMMs) (Neal, 2000), to a typical
learning task in NLP: lexical-semantic verb clus-
tering. The task involves discovering classes of
verbs similar in terms of their syntactic-semantic
properties (e.g. MOTION class for travel, walk,
run, etc.). Such classes can provide important
support for other NLP tasks, such as word sense
disambiguation, parsing and semantic role label-
ing (Dang, 2004; Swier and Stevenson, 2004).
Although some fixed classifications are available
(e.g. VerbNet (Kipper-Schuler, 2005)) these are
not comprehensive and are inadequate for specific
domains (Korhonen et al, 2006b).
Unlike the clustering algorithms applied to this
task before, DPMMs do not require the number of
clusters as input. This is important because even
if the number of classes in a particular task was
known (e.g. in the context of a carefully controlled
experiment), a particular dataset may not contain
instances for all the classes. Moreover, each class
is not necessarily contained in one cluster exclu-
sively, since the target classes are defined manu-
ally without taking into account the feature rep-
resentation used. The fact that DPMMs do not
require the number of target clusters in advance,
renders them promising for the many NLP tasks
where clustering is used for learning purposes.
While the results of Vlachos et al (2008) are
promising, the use of a clustering approach which
discovers the number of clusters in data presents
a new challenge to existing evaluation measures.
In this work, we investigate optimal evaluation
for such approaches, using the dataset and the ba-
sic method of Vlachos et al as a starting point.
We review the applicability of existing evalua-
tion measures and propose a modified version of
the newly introduced V-measure (Rosenberg and
Hirschberg, 2007). We complement the quanti-
tative evaluation with thorough qualitative assess-
ment, for which we introduce a method to summa-
rize samples obtained from a clustering algorithm.
In preliminary work by Vlachos et al (2008),
a constrained version of DPMMs which takes ad-
vantage of must-link and cannot-link pairwise con-
straints was introduced. It was demonstrated how
such constraines can guide the clustering solution
towards some prior intuition or considerations rel-
evant to the specific NLP application in mind. We
explain the inference algorithm for the constrained
DPMM in greater detail and evaluate quantita-
74
tively the contribution of each constraint type of
independently, complementing it with qualitative
analysis. The latter demonstrates how the pairwise
constraints added affects instances beyond those
involved directly. Finally, we discuss how the un-
supervised and the constrained version of DPMMs
can be used in a real-world setup.
The results from our comprehensive evaluation
show that both versions of DPMMs are capable
of learning novel information not in the gold stan-
dard, and that the constrained version is more ac-
curate than a previous verb clustering approach
which requires setting the number of clusters in
advance and is therefore less realistic.
2 Unsupervised clustering with DPMMs
With DPMMs, as with other Bayesian non-
parametric models, the number of mixture compo-
nents is not fixed in advance, but is determined by
the model and the data. The parameters of each
component are generated by a Dirichlet Process
(DP) which can be seen as a distribution over the
parameters of other distributions. In turn, each in-
stance is generated by the chosen component given
the parameters defined in the previous step:
G|?,G0 ? DP (?,G0)
?i|G ? G (1)
xi|?i ? F (?i)
In Eq. 1, G0 and G are probability distributions
over the component parameters (?), and ? > 0 is
the concentration parameter which determines the
variance of the Dirichlet process. We can think
of G as a randomly drawn probability distribution
with meanG0. Intuitively, the larger ? is, the more
similar G will be to G0. Instance xi is generated
by distribution F , parameterized by ?i. The graph-
ical model is depicted in Figure 1.
The prior probability of assigning an instance
to a particular component is proportionate to the
number of instances already assigned to it (n?i,z).
In other words, DPMMs exhibit the ?rich get
richer? property. In addition, the probability that
a new cluster is created is dependent on the con-
centration parameter ?. A popular metaphor to de-
scribe DPMMs which exhibits an equivalent clus-
tering property is the Chinese Restaurant Process
(CRP). Customers (instances) arrive at a Chinese
restaurant which has an infinite number of tables
(components). Each customer sits at one of the ta-
bles that is either occupied or vacant with popular
tables attracting more customers.
Figure 1: Graphical representation of DPMMs.
In this work, the distribution used to model the
components is the multinomial and the prior used
is the Dirichlet distribution (F and G0 in Eq. 1).
The conjugacy between them allows for the ana-
lytic integration over the component parameters.
Following Neal (2000), the component assign-
ments zi are sampled using the following scheme:
P (zi = z|z?i, xi) ?
p(zi = z|z?i)DirM(xi|zi = z, x?i,z, ?) (2)
In Eq. 2DirM is the Dirichlet-Multinomial distri-
bution, ? are the parameters of the Dirichlet prior
G0 and x?i,z are the instances assigned already to
component z (none if we are sampling the prob-
ability of assignment to a new component). This
sampling scheme is possible due to the fact that the
instances in the model are exchangeable, i.e. the
order in which they are generated is not relevant.
In terms of the CRP metaphor, we consider each
instance xi as the last customer to arrive and he
chooses to sit together with other customers at an
existing table or to sit at a new table. Following
Navarro et al (2006) who used the same model to
analyze individual differences, we sample the con-
centration parameter ? using the inverse Gamma
distribution as a prior.
3 Evaluation measures
The evaluation of unsupervised clustering against
a gold standard is not straightforward because the
clusters found are not explicitly labelled. Formally
defined, an unsupervised clustering algorithm par-
titions a set of instances X = {xi|i = 1, ..., N}
into a set of clusters K = {kj |j = 1, ..., |K|}.
The standard approach to evaluate the quality of
the clusters is to use an external gold standard in
which the instances are partitioned into a set of
75
classes C = {cl|l = 1, ..., |C|}. Given this, the
goal is to find a partitioning of the instances K
that is as close as possible to the gold standard C.
Most work on verb clustering has used the F-
measure or the Rand Index (RI) (Rand, 1971)
for evaluation, which rely on counting pairwise
links between instances. However, Rosenberg and
Hirschberg (2007) pointed out that F-measure as-
sumes (the missing) mapping between cl and kj .
In practice, RI values concentrate in a small inter-
val near 100% (Meila?, 2007).
Rosenberg & Hirschberg (2007) proposed an
information-theoretic metric: V-measure. V-
measure is the harmonic mean of homogeneity
and completeness which evaluate the quality of the
clustering in a complementary way. Homogeneity
assesses the degree to which each cluster contains
instances from a single class of C. This is com-
puted as the conditional entropy of the class dis-
tribution of the gold standard given the clustering
discovered by the algorithm, H(C|K), normal-
ized by the entropy of the class distribution in the
gold standard, H(C). Completeness assesses the
degree to which each class is contained in a single
cluster. This is computed as the conditional en-
tropy of the cluster distribution discovered by the
algorithm given the class, H(K|C), normalized
by the entropy of the cluster distribution, H(K).
In both cases, we subtract the resulting ratios from
1 to associate higher scores with better solutions:
h = 1?
H(C|K)
H(C)
c = 1?
H(K|C)
H(K)
V? =
(1 + ?) ? h ? c
(? ? h) + c
(3)
The parameter ? in Eq. 3 regulates the balance
between homogeneity and completeness. Rosen-
berg & Hirschberg set it to 1 in order to obtain the
harmonic mean of these qualities. They also note
that V-measure favors clustering solutions with a
large number of clusters (large |K|), since such so-
lutions can achieve very high homogeneity while
maintaining reasonable completeness. This ef-
fect is more prominent when a dataset includes a
small number of instaces for gold standard classes.
While increasing |K| does not guarantee an in-
crease in V-measure (splitting homogeneous clus-
ters would reduce completeness without improv-
ing homogeneity), it is easier to achieve higher
scores when more clusters are produced.
Another relevant measure is the Variation of In-
formation (VI) (Meila?, 2007). Like V-measure,
it assesses homogeneity and completeness using
the quantitiesH(C|K) andH(K|C) respectively,
however it simply adds them up to obtain a final
result (higher scores are worse). It is also a metric,
i.e. VI scores can be added, subtracted, etc, since
the quantities involved are measured in bits. How-
ever, it can be observed that if |C| and |K| are very
different then the terms H(C|K) and H(K|C)
will not necessarily be in the same range. In par-
ticular, if |K|  |C| then H(K|C) (and V I) will
be low. In addition, VI scores are not normalized
and therefore their interpretation is difficult.
Both V-measure and VI have important advan-
tages over RI and F-measure: they do not assume
a mapping between classes and clusters and their
scores depend only on the relative sizes of the clus-
ters. However, V-measure and VI can be mislead-
ing if the number of clusters found (|K|) is sub-
stantially different than the number of gold stan-
dard classes (|C|). In order to ameliorate this, we
suggest to take advantage of the ? parameter in
Eq. 3 in order to balance homogeneity and com-
pleteness. More specifically, setting ? = |K|/|C|
assigns more weight to completeness than to ho-
mogeneity in case |K| > |C| since the former is
harder to achieve and the latter is easier when the
clustering solution has more clusters than the gold
standard has classes. The opposite occurs when
|K| < |C|. In case |K| = |C| the score is the
same as the original V-measure. Achieving 100%
score according to any of these measures requires
correct prediction of the number of clusters.
In this work, we evaluate our results using the
three measures described above (V-measure, VI,
V-beta). We complement this evaluation with
qualitative evaluation which assesses the poten-
tial of DPMMs to discover novel information that
might not be included in the gold standard.
4 Experiments
To perform lexical-semantic verb clustering we
used the dataset of Sun et al (2008). It contains
204 verbs belonging to 17 fine-grained classes in
Levin?s (1993) taxonomy so that each class con-
tains 12 verbs. The classes and their verbs were
selected randomly. The features for each verb are
its subcategorization frames (SCFs) and associ-
ated frequencies in corpus data, which capture the
76
DPMM Sun et al
no. of clusters 37.79 17
homogeneity 60.23% 57.57%
completeness 55.82% 60.19%
V-measure 57.94% 58.85%
V-beta 57.11% 58.85%
VI (bits) 3.5746 3.3598
Table 1: Clustering performances.
syntactic context in which the verb occurs. SCFs
were extracted from the publicly available VALEX
lexicon (Korhonen et al, 2006a). VALEX was ac-
quired automatically using a domain-independent
statistical parsing toolkit, RASP (Briscoe and Car-
roll, 2002), and a classifier which identifies verbal
SCFs. As a consequence, it includes some noise
due to standard text processing and parsing errors
and due to the subtlety of argument-adjunct dis-
tinction. In our experiments, we used the SCFs
obtained from VALEX1, parameterized for the
prepositional frame, which had the best perfor-
mance in the experiments of Sun et al (2008).
The feature sets based on verbal SCFs are very
sparse and the counts vary over a large range of
values. This can be problematic for generative
models like DPMMs, since a few dominant fea-
tures can mislead the model. To reduce the spar-
sity, we applied non-negative matrix factorization
(NMF) (Lin, 2007) which decomposes the dataset
in two dense matrices with non-negative values. It
has proven useful in a variety of tasks, e.g. infor-
mation retrieval (Xu et al, 2003) and image pro-
cessing (Lee and Seung, 1999).
We use a symmetric Dirichlet prior with param-
eters of 1 (? in Equation 2). The number of di-
mensions obtained using NMF was 35. We run
the Gibbs sampler 5 times, using 100 iterations for
burn-in and draw 20 samples from each run with
5 iterations lag between samples. Table 1 shows
the average performances. The DPMM discov-
ers 37.79 verb clusters on average with its perfor-
mance ranging between 53% and 58% depending
on the evaluation measure used. Homogeneity is
4.5% higher than completeness, which is expected
since the number of classes in the gold standard is
17. The fact that the DPMM discovers more than
twice the number of classes is reflected in the dif-
ference between the V-measure and V-beta, the lat-
ter being lower. In the same table, we show the re-
sults of Sun et al (2008), who used pairwise clus-
tering (PC) (Puzicha et al, 2000) which involves
determining the number of clusters in advance.
The performance of the DPMM is 1%-3% lower
than that of Sun et al As expected, the differ-
ence in V-measure is smaller since the DPMM
discovers a larger number of clusters, while for
VI it is larger. The slightly better performance
of PC can be attributed to two factors. First,
the (correct) number of clusters is given as in-
put to the PC algorithm and not discovered like
by the DPMM. Secondly, PC uses the similarities
between the instances to perform the clustering,
while the DPMM attempts to find the parameters
of the process that generated the data, which is a
different and typically a harder task. In addition,
the DPMM has two clear advantages which we il-
lustrate in the following sections: it can be used to
discover novel information and it can be modified
to incorporate intuitive human supervision.
5 Qualitative evaluation
The gold standard employed in this work (Sun et
al., 2008) is not fully accurate or comprehensive.
It classifies verbs according to their predominant
senses in the fairly small SemCor data. Individ-
ual classes are relatively coarse-grained in terms
of syntactic-semantic analysis1 and they capture
some of the meaning components only. In addi-
tion, the gold standard does not capture the se-
mantic relatedness of distinct classes. In fact, the
main goal of clustering is to improve such exist-
ing classifications with novel information and to
create classifications for new domains. We per-
formed qualitative analysis to investigate the ex-
tent to which the DPMM meets this goal.
We prepared the data for qualitative analysis as
follows: We represented each clustering sample
as a linking matrix between the instances of the
dataset and measured the frequency of each pair
of instances occurring in the same cluster. We
constructed a partial clustering of the instances
using only those links that occur with frequency
higher than a threshold prob link. Singleton clus-
ters were formed by considering instances that
are not linked with any other instances more fre-
quently than a threshold prob single. The lower
the prob link threshold, the larger the clusters will
be, since more instances get linked. Note that in-
cluding more links in the solution can either in-
1Many original Levin classes have been manually refined
in VerbNet.
77
crease the number of clusters when instances in-
volved were not linked otherwise, or decrease it
when linking instances that already belong to other
clusters. The higher the prob single threshold,
the more instances will end up as singletons. By
adjusting these two thresholds we can affect the
coverage of the analysis. This approach was cho-
sen because it enables to conduct qualitative analy-
sis of data relevant to most clustering samples and
irrespective of individual samples. It can also be
useful in order to use the output of the clustering
algorithm as a component in a pipeline which re-
quires a single result rather than multiple samples.
Using this method, we generated data sets for
qualitative analysis using 4 sets of values for
prob link and prob single, respectively: (99%,
1%), (95%, 5%), (90%, 10%) and (85%, 15%).
Table 1 shows the number of a) verbs, b) clusters
(2 or more instances) and c) singletons in each
resulting data set, along with the percentage and
size of the clusters which represent 1, 2, or mul-
tiple gold standard classes. As expected, higher
threshold values produce high precision clusters
for a smaller set of verbs (e.g. (99%,1%) pro-
duces 5 singletons and assigns 70 verbs to 20 clus-
ters, 55% of which represent a single gold stan-
dard class), while less extreme threshold values
yield higher recall clusters for a larger set of verbs
(e.g. (85%,15%) produces 10 singletons and as-
signs 140 verbs to 25 clusters, 20% of which con-
tain verbs from several gold standard classes).
We conducted the qualitative analysis by com-
paring the four data sets against the gold standard,
SCF distributions, and WordNet (Fellbaum, 1998)
senses for each test verb. We first analysed the
5-10 singletons in data sets and discovered that
while 3 of the verbs resist classification because
of syntactic idiosyncrasy (e.g. unite takes intransi-
tive SCFs with frequency higher than other mem-
bers of class 22.2), the majority of them (7) end
up in singletons for valid semantic reasons: taking
several frequent WordNet senses they are ?too pol-
ysemous? to be realistically clustered according to
their predominant sense (e.g. get and look).
We then examined the clusters, and discovered
that even in the data set created with the lowest
prob link threshold of 85%, almost half of the
?errors? are in fact novel semantic patterns discov-
ered by clustering. Many of these could be new
sub-classes of existing gold standard classes. For
example, looking at the 13 high accuracy clusters
which correspond to a single gold standard class
each, they only represent 9 gold standard classes
because as many as 4 classes been divided into
two clusters, suggesting that the gold standard is
too coarse-grained. Interestingly, each such sub-
division seems semantically justified (e.g. the 11.1
PUT verbs bury and immerse appear in a differ-
ent cluster than the semantically slightly different
place and situate).
In addition, the DPMM discovers semantically
similar gold standard classes. For example, in the
data set created with the prob link threshold of
99%, 6 of the clusters include members from 2
different gold standard classes. 2 occur due to
syntactic idiosyncrasy, but the majority (4) oc-
cur because of true semantic relatedness (e.g. the
clustering relates 22.2 AMALGAMATE and 36.1
CORRESPOND classes which share similar mean-
ing components). Similarly, in the data set pro-
duced by the prob link threshold of 85%, one
of the largest clusters includes 26 verbs from 5
gold standard classes. The majority of them be-
long to 3 classes which are related by the meaning
component of ?motion?: 43.1 LIGHT EMISSION,
47.3 MODES OF BEING INVOLVING MOTION, and
51.3.2 RUN verbs:
? class 22.2 AMALGAMATE: overlap
? class 36.1 CORRESPOND: banter, concur, dissent, hag-
gle
? class 43.1 LIGHT EMISSION: flare, flicker, gleam, glis-
ten, glow, shine, sparkle
? class 47.3 MODES OF BEING INVOLVING MOTION:
falter, flutter, quiver, swirl, wobble
? class 51.3.2 RUN: fly, gallop, glide, jog, march, stroll,
swim, travel, trot
Thus many of the singletons and the clusters
in the different outputs capture finer or coarser-
grained lexical-semantic differences than those
captured in the gold standard. It is encouraging
that this happens despite us focussing on a rela-
tively small set of 204 verbs and 17 classes only.
6 Constrained DPMMs
While the ability to discover novel information is
attractive in NLP, in many cases it is also desir-
able to influence the solution with respect to some
prior intuition or consideration relevant to the ap-
plication in mind. For example, while discover-
ing finer-grained classes than those included in the
gold standard is useful for some applications, oth-
ers may benefit from a coarser clustering or a clus-
tering that reveals a specific aspect of the dataset.
78
% and size of clusters containing
THR verbs clusters singletons 1 class 2 classes multiple classes
99%,1% 70 20 5 55% (3.0) 30% (2.8) 15% (4.5)
95%,5% 104 25 9 40% (3.7) 44% (2.8) 16% (6.8)
90%,10% 128 28 9 46% (3.4) 39% (2.5) 14% (11.0)
85%,15% 140 25 10 44% (3.7) 28% (3.3) 20% (13.0)
Table 2: An overview of the data sets generated for qualitative analysis
Preliminary work by Vlachos et al (2008) intro-
duced a constrained version of DPMMs that en-
ables human supervision to guide the clustering
solution when needed. We model the human su-
pervision as pairwise constraints over instances,
following Wagstaff & Cardie (2000): given a pair
of instances, they are either linked together (must-
link) or not (cannot-link). For example, charge
and run should form a must-link if the aim is
to cluster 51.3 MOTION verbs together, but they
should form a cannot-link if we are interested in
54.5 BILL verbs. In the discussion and the experi-
ments that follow, we assume that all links are con-
sistent with each other. This information can be
obtained by asking human experts to label links,
or by extracting it from extant lexical resources.
Specifying the relations between the instances re-
sults in a partial labeling of the instances. Such
labeling is likely to be re-usable, since relations
between the instances are likely to be useful for a
wider range of tasks which might not have identi-
cal labels but could still have similar relations.
In order to incorporate the constraints in the
DPMM, we modify the underlying generative pro-
cess to take them into account. In particular must-
linked instances are generated by the same com-
ponent and cannot-linked instances always by dif-
ferent ones. In terms of the CRP metaphor, cus-
tomers connected with must-links arrive at the
restaurant together and choose a table jointly, re-
specting their cannot-links with other customers.
They get seated at the same table successively one
after the other. Customers without must-links with
others choose tables avoiding their cannot-links.
In order to sample the component assignments
according to this model, we restrict the Gibbs sam-
pler to take them into account using the sampling
scheme of Fig. 2. First we identify linked-groups
of instances, taking into account transitivity2. We
then sample the component assignments only from
distributions that respect the links provided. More
2If A is linked to B and B to C, then A is linked to C.
specifically, for each instance that does not belong
to a linked-group, we restrict the sampler to choose
components that do not contain instances cannot-
linked with it. For instances in a linked-group, we
sample their assignment jointly, again taking into
account their cannot-links. This is performed by
adding each instance of the linked-group succes-
sively to the same component. In Fig. 2, Ci are the
cannot-links for instance(s) i, ` are the indices of
the instances in a linked-group, and z<i and x<i
are the assignments and the instances of a linked-
group that have been assigned to a component be-
fore instance i.
Input: data X , must-linksM, cannot-links C
linked groups = find linked groups(X ,M)
Initialize Z according toM, C
for i not in linked groups
for z = 1 to |Z|+ 1
if x?i,z ? Ci = ?
P (zi = z|z?i, xi) (Eq. 2)
else
P (zi = z|z?i, xi) = 0
Sample from P (zi)
for ` in linked groups
for z = 1 to |Z|+ 1
if x?`,z ? C` = ?
Set P (z` = z|z?`, x`) = 1
for i in `
P (z`= z|z?`, x`)? =
P (zi = z|z?`, x?`,z, z<i, x<i)
else
P (z` = z|z?`, x`) = 0
Sample from P (z`)
Figure 2: Gibbs sampler incorporating must-links
and cannot-links.
7 Experiments using constraints
To investigate the impact of pairwise constraints
on clustering by the DPMM, we conduct exper-
79
iments in which the links are sampled randomly
from the gold standard. The number of links var-
ied from 10 to 50 and the random choice was re-
peated 5 times without checking for redundancy
due to transitivity. All the other experimental set-
tings are identical to those in Section 4. Follow-
ing Wagstaff & Cardie (2000), in Table 3 we show
the impact of each link type independently (la-
beled ?must? and ?cannot? accordingly), as well
as when mixed in equal proportions (?mix?).
Adding randomly selected pairwise links is ben-
eficial. In particular, must-links improve the clus-
tering rapidly. Incorporating 50 must-links im-
proves the performance by 7-8% according to the
evaluation measures. In addition, it reduces the
average number of clusters by approximately 4.
The cannot-links are rather ineffective, which is
expected as the clustering discovered by the un-
supervised DPMM is more fine-grained than the
gold standard. For the same reason, it is more
likely that the randomly selected cannot-links are
already discovered by the DPMM and are thus re-
dundant. Wagstaff & Cardie also noted that the
impact of the two types of links tends to vary
across data sets. Nevertheless, a minor improve-
ment is observed in terms of homogeneity. The
balanced mix improves the performance, but less
rapidly than the must-links.
In order to assess how the links added help the
DPMM learn other links we use the Constrained
Rand Index (CRI), which is a modification of the
Rand Index that takes into account only the pair-
wise decisions that are not dictated by the con-
straints added (Wagstaff and Cardie, 2000; Klein
et al, 2002). We evaluate the constrained DPMM
with CRI (Table 3, bottom right graph) and our re-
sults show that the improvements obtained using
pairwise constraints are due to learning links be-
yond the ones enforced.
In a real-world setting, obtaining the mixed set
of links is equivalent to asking a human expert to
give examples of verbs that should be clustered to-
gether or not. Such information could be extracted
from a lexical resource (e.g. ontology). Alterna-
tively, the DPMM could be run without any con-
straints first and if a human expert judges the clus-
tering too coarse (or fine) then cannot-links (or
must-links) could help, since they can adapt the
clustering rapidly. When 20 randomly selected
must-links are integrated, the DPMM reaches or
exceeds the performance of PC used by Sun et
al. (2008) according to all the evaluation mea-
sures. We also argue that it is more realistic to
guide the clustering algorithm using pairwise con-
straints than by defining the number of clusters in
advance. Instead of using pairwise constraints to
affect the clustering solution, one could alter the
parameters for the Dirichlet prior G0 (Eq. 1) or
experiment with varying concentration parameter
values. However, it is difficult to predict in ad-
vance the exact effect such changes would have in
the solution discovered.
Finally, we conducted qualitative analysis of the
samples obtained constraining the DPMM with 10
randomly selected must-links. We first prepared
the data according to the method described in Sec-
tion 5, using prob link and prob single thresh-
olds of 99% and 1% respectively. This resulted in
26 clusters and one singleton for 79 verbs. Recall
that without constraining the DPMM these thresh-
olds produced 20 clusters and 5 singletons for 70
verbs. 49 verbs are shared in both outputs, while
the average cluster size is similar.
The resulting clusters are highly accurate. As
many as 16 (i.e. 62%) of them represent a sin-
gle gold standard class, 7 of which contain (only)
the pairs of must-linked verbs. Interestingly, only
11 out of 17 gold standard classes are exempli-
fied among the 16 clusters, with 5 classes sub-
divided into finer-grained classes. Each of these
sub-divisions seems semantically fully motivated
(e.g. 30.3 PEER verbs were subdivided so that
peep and peek were assigned to a different cluster
than the semantically different gaze, glance and
stare) and 4 of them can be directly attributed to
the use of must-links.
From the 6 clusters that contained members
from two different gold standard classes, the ma-
jority (5) make sense as well. 3 of these contain
members of must-link pairs together with verbs
from semantically related classes (e.g. 37.7 SAY
and 40.2 NONVERBAL EXPRESSION classes). 3 of
the clusters that contain members of several gold
standard classes include must-link pairs as well.
In two cases must-links have helped to bring to-
gether verbs which belong to the same class (e.g.
the members of the must-link pair broaden-freeze
which represent 45.4 CHANGE OF STATE class ap-
pear now in the same cluster with other class mem-
bers dampen, soften and sharpen). Thus, DP-
MMs prove useful in learning novel information
taking into account pairwise constraints. Only 4
80
 60
 61
 62
 63
 64
 65
 66
 67
 68
 0  10  20  30  40  50
Ho
m
og
en
eit
y
mix
must
cannot
 55
 56
 57
 58
 59
 60
 61
 62
 63
 0  10  20  30  40  50
Co
m
ple
te
ne
ss
mix
must
cannot
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 0  10  20  30  40  50
V-
m
ea
su
re
mix
must
cannot
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 0  10  20  30  40  50
V-
be
ta
mix
must
cannot
 2.9
 3
 3.1
 3.2
 3.3
 3.4
 3.5
 3.6
 3.7
 0  10  20  30  40  50
VI
mix
must
cannot
 90
 90.2
 90.4
 90.6
 90.8
 91
 91.2
 91.4
 91.6
 91.8
 0  10  20  30  40  50
CR
I
mix
must
cannot
Table 3: Performance of constrained DPMMs incorporating pairwise links.
(i.e. 15%) of the clusters in the output examined
are not meaningful (mostly due to the mismatch
between the syntax and semantics of verbs).
8 Related work
Previous work on unsupervised verb clustering
used algorithms that require the number of clus-
ters as input e.g. PC, Information Bottleneck (Ko-
rhonen et al, 2006b) and spectral clustering (Brew
and Schulte im Walde, 2002). In terms of apply-
ing non-parametric Bayesian approaches to NLP,
Haghighi and Klein (2007) evaluated the cluster-
ing properties of DPMMs by performing anaphora
resolution with good results.
There is a large body of work on semi-
supervised learning (SSL), but relatively little
work has been done on incorporating some form
of supervision in clustering. It is important to note
that the pairwise links used in this work consti-
tute a weak form of supervision since they cannot
be used to infer class labels which are required for
SSL. However, the opposite can be done. Wagstaff
& Cardie (2000) employed must-links and cannot-
links to constrain the COBWEB algorithm, while
Klein et al (2002) applied them to complete-link
hierarchical agglomerative clustering. The latter
also studied how the added links affect instances
not directly involved in them.
It can be argued that one could use clustering
algorithms that require the number of clusters to
be known in advance to discover interesting sub-
classes such as those discovered by the DPMMs.
However, this would normally require multiple
runs and manual inspection of the results, while
DPMMs discover them automatically. Apart from
the fact that fixing the number of clusters in ad-
vance restricts the discovery of novel information
in the data, such algorithms cannot take full ad-
vantage of the pairwise constraints, since the latter
are likely to change the number of clusters.
9 Conclusions - Future Work
In this work, following Vlachos et al (2008) we
explored the application of DPMMs to the task of
verb clustering. We modified V-measure (Rosen-
berg and Hirschberg, 2007) to deal more appro-
priately with the varying number of clusters dis-
covered by DPMMs and presented a method of
agregating the generated samples which allows for
qualitative evaluation. The quantitative and qual-
itative evaluation demonstrated that they achieve
performance comparable with that of previous
work and in addition discover novel information in
the data. Furthermore, we evaluated the incorpo-
ration of constraints to guide the DPMM obtaining
promising results and we discussed their applica-
tion in a real-world setup.
The results obtained encourage the application
of DPMMs and non-parametric Bayesian methods
to other NLP tasks. We plan to extend our ex-
periments to larger datasets and further domains.
While the improvements achieved using randomly
selected pairwise constraints were promising, an
active constraint selection scheme as in Klein et
al. (2002) could increase their impact. Finally,
an extrinsic evaluation of the clustering provided
by DPMMs in the context of an NLP application
would be informative on their practical potential.
81
Acknowledgments
We are grateful to Diarmuid O? Se?aghdha and Jur-
gen Van Gael for helpful discussions.
References
Chris Brew and Sabine Schulte im Walde. 2002. Spec-
tral Clustering for German Verbs. In Proceedings of
the 2002 Conference on Empirical Methods in Nat-
ural Language Processing, pages 117?124.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504.
Hoa Trang Dang. 2004. Investigations into the role
of lexical semantics in word sense disambiguation.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Sharon J. Goldwater. 2007. Nonparametric bayesian
models of lexical acquisition. Ph.D. thesis, Brown
University, Providence, RI, USA.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855, Prague, Czech Republic.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Dan Klein, Sepandar Kamvar, and Chris Manning.
2002. From instance-level constraints to space-level
constraints: Making the most of prior knowledge in
data clustering. In Proceedings of the Nineteenth In-
ternational Conference on Machine Learning.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006a. A large subcategorization lexicon for natural
language processing applications. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2006b. Automatic classification of verbs in
biomedical texts. In Proceedings of the COLING-
ACL, pages 345?352.
Daniel D. Lee and Sebastian H. Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788?791, October.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago.
Chih-Jen Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural Compua-
tion, 19(10):2756?2779.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,
and Michael D. Lee. 2006. Modeling individual dif-
ferences using dirichlet processes. Journal of Math-
ematical Psychology, 50(2):101?122, April.
Radford M. Neal. 2000. Markov Chain Sam-
pling Methods for Dirichlet Process Mixture Mod-
els. Journal of Computational and Graphical Statis-
tics, 9(2):249?265, June.
Jan Puzicha, Thomas Hofmann, and Joachim Buh-
mann. 2000. A theory of proximity based clus-
tering: Structure detection by optimization. Pattern
Recognition, 33(4):617?634.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66(336):846?850.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL, pages 410?420, Prague, Czech Republic.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the 9th International Conference
on Intelligent Text Processing and Computational
Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of COLING-ACL, pages 985?992, Syd-
ney, Australia.
Andreas Vlachos, Zoubin Ghahramani, and Anna Ko-
rhonen. 2008. Dirichlet process mixture models for
verb clustering. In Proceedings of the ICML work-
shop on Prior Knowledge for Text and Language.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning, pages 1103?1110, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Docu-
ment clustering based on non-negative matrix factor-
ization. In Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in informaion retrieval, pages 267?273,
New York, NY, USA. ACM Press.
82
Proceedings of the Workshop on BioNLP, pages 108?116,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
User-Driven Development of Text Mining Resources for Cancer Risk
Assessment
Lin Sun, Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0GD, UK
ls418,alk23@cl.cam.ac.uk
Ilona Silins, Ulla Stenius
Institute of Environmental Medicine
Karolinska Institutet
S-17177, Stockholm
Sweden
ilona.silins,ulla.stenius@ki.se
Abstract
One of the most neglected areas of biomed-
ical Text Mining (TM) is the development
of systems based on carefully assessed user
needs. We investigate the needs of an im-
portant task yet to be tackled by TM ? Can-
cer Risk Assessment (CRA) ? and take the
first step towards the development of TM for
the task: identifying and organizing the sci-
entific evidence required for CRA in a taxon-
omy. The taxonomy is based on expert annota-
tion of 1297 MEDLINE abstracts. We report
promising results with inter-annotator agree-
ment tests and automatic classification experi-
ments, and a user test which demonstrates that
the resources we have built are well-defined,
accurate, and applicable to a real-world CRA
scenario. We discuss extending and refining
the taxonomy further via manual and machine
learning approaches, and the subsequent steps
required to develop TM for the needs of CRA.
1 Introduction
Biomedical Text Mining (TM) has become increas-
ingly popular due to the pressing need to provide
access to the tremendous body of texts available
in biomedical sciences. Considerable progress has
been made in the development of basic resources
(e.g. ontologies, annotated corpora) and techniques
(e.g. Information Retrieval (IR), Information Ex-
traction (IE)) in this area, and research has began
to focus on increasingly challenging tasks, e.g. sum-
marization and the discovery of novel information in
biomedical literature (Hunter and Cohen 2006, Ana-
niadou et al 2006, Zweigenbaum et al 2007).
In recent past, there has been an increasing de-
mand for research which is driven by actual user
needs rather than technical developments (Zweigen-
baum et al 2007). Shared tasks (e.g. BioCreative
and the TREC Genomics track) targeting the work-
flow of biomedical researchers have appeared along
with studies exploring the TM needs of specific tasks
(Karamanis et al 2008, Demaine et al 2006). How-
ever, the understanding of user needs is still one of
the neglected areas of BIO-TM, and further user-
centered evaluations and systems grounded in real-
life tasks are required to determine which tools and
services are useful (Cohen et al 2008).
We investigate the user needs of a challenging
task yet to be tackled by TM but identified as an
important potential application for it (Lewin et al
2008): Cancer Risk Assessment (CRA). Over the
past years, CRA has become increasingly important
as the link between environmental chemicals and
cancer has become evident. It involves examining
published evidence to determine the relationship be-
tween exposure to a chemical and the likelihood of
developing cancer from that exposure (EPA, 2005).
Performed manually by experts in health related in-
stitutions worldwide, CRA requires searching, lo-
cating and interpreting information in biomedical
journal articles. It can be extremely time-consuming
because the data for a single carcinogen may be scat-
tered across thousands of articles.
Given the exponentially growing volume of
biomedical literature and the rapid development of
molecular biology techniques, the task is now get-
ting too challenging to manage via manual means.
From the perspective of BIO-TM, CRA is an excel-
lent example of real-world task which could greatly
benefit from a dedicated TM tool. However, the de-
velopment of a truly useful tool requires careful in-
vestigation of risk assessors needs.
108
This paper reports our investigation of the user
needs of CRA and the creation of basic TM re-
sources for the task. Expanding on our preliminary
experiments (Lewin et al 2008), we present a taxon-
omy which specifies the scientific evidence needed
for CRA at the level of detail required for TM. The
taxonomy is based on expert annotation of a corpus
of 1297 MEDLINE abstracts. We report promis-
ing results with inter-annotator agreement tests, au-
tomatic classification of corpus data into taxonomy
classes, and a user test in a near real-world CRA
scenario which shows that the taxonomy is highly
accurate and useful for practical CRA. We discuss
refining and extending it further via manual and ma-
chine learning approaches, and the subsequent steps
required to develop TM for the needs of CRA.
2 User Needs of Cancer Risk Assessment
We interviewed 14 experienced risk assessors work-
ing for a number of authorities in Sweden1 asking
a range of questions related to different aspects of
their work. The risk assessors described the follow-
ing steps of CRA: (1) identifying the journal articles
relevant for CRA of the chemical in question, (2)
identifying the scientific evidence in these articles
which help to determine whether/how the chemical
causes cancer, (3) classifying and analysing the re-
sulting (partly conflicting) evidence to build the tox-
icological profile for the chemical, and (4) prepar-
ing the risk assessment report. These steps are con-
ducted manually, relying only on standard literature
search engines (e.g. PubMed) and word processors.
The average time required for CRA of a single
chemical was reported to be two years when done
(as usual) on a part time basis. Risk assessors were
unanimous about the need to increase productivity
to meet the current CRA demand. They reported
that locating and classifying the scientific evidence
in literature is the most time consuming part of their
work and that a tool capable of assisting it and ensur-
ing that all the potentially relevant evidence is found
would be particularly helpful.
It became clear that a prerequisite for the devel-
opment of such a tool would be an extensive spec-
ification of the scientific evidence used for CRA.
1Institute of Environmental Medicine at Karolinska Insti-
tutet, Swedish Chemical Inspectorate, Scientific Committee on
Occupational Exposure Limits (EU), Swedish Criteria Group.
This evidence ? which forms the basis of all the
subsequent steps of CRA ? is described in the
guideline documents of major international CRA
agencies, e.g. European Chemicals Agency (ECHA,
2008) and the United States Environmental Protec-
tion Agency (EPA, 2005). However, although these
documents constitute the main reference material in
CRA, they cover the main types of evidence only,
do not specify the evidence at the level of detail
required for comprehensive data gathering, and are
not updated regularly (i.e. do not incorporate the lat-
est developments in biomedical sciences). The risk
assessors admitted that rather than relying on these
documents, they rely on their experience and expert
knowledge when looking for the evidence. We de-
cided that our starting point should be to compose
a more adequate specification of the scientific evi-
dence needed for CRA.
3 Cancer Risk Assessment Taxonomy
We recruited three experienced risk assessors to help
construct the resources described in sections below:
(i) a representative corpus of CRA literature for
parts of hazard identification (i.e. the assessment of
whether a chemical is capable of causing cancer),
(ii) a tool for expert annotation of the corpus, (iii) an
annotated corpus, and (iv) a taxonomy which classi-
fies and organizes the scientific evidence discovered
in the corpus.
3.1 CRA corpus
Various human, animal (in vivo), cellular (in vitro)
and other mechanistic data provide evidence for haz-
ard identification and the assessment of the Mode of
Action (MOA) (i.e. the sequence of key events that
result in cancer formation, e.g. mutagenesis and in-
creased cell proliferation) in CRA. The experts se-
lected eight chemicals which are (i) well-researched
using a range of scientific tests and (ii) represent the
two most frequently used MOAs ? genotoxic and
non-genotoxic2 . 15 journals were identified which
are used frequently for CRA and jointly provide a
good coverage of relevant scientific evidence (e.g.
Cancer Research, Chemico-biological Interaction,
Mutagenesis, Toxicological Sciences). From these
2Chemicals acting by a genotoxic MOA interact with DNA,
while chemicals acting by a nongenotoxic MOA induce cancer
without interfering directly with DNA.
109
Figure 1: Screenshot of the annotation tool
journals, all the PubMed abstracts from 1998-2008
which include one of the 8 chemicals were down-
loaded. The resulting corpus of 1297 abstracts is
distributed per chemical as shown in Table 1.
3.2 Annotation tool
Risk assessors typically (i) read each abstract re-
trieved by PubMed to determine its relevance for
CRA, and (ii) classify each relevant abstract based
on the type of evidence it provides for CRA. We ex-
tended the tool designed for expert annotation of ab-
stracts in our earlier work (Lewin et al 2008) so that
imitates this process as closely as possible.
The tool provides two types of functionality. The
first enables the experts to classify abstracts as rele-
vant, irrelevant or unsure. The second enables them
to annotate such keywords (words or phrases) in ab-
stracts and their titles which indicate the scientific
evidence relevant for the task. Keyword annotation
was chosen because the experts found it intuitive, it
did not require linguistic training, and it specifies the
scientific evidence more precisely than larger spans
of text.
Initially a very shallow taxonomy (including only
human, animal, and cellular data) and the two types
of MOA was integrated inside the tool. This was
gradually extended as the annotation progressed.
The tool permits annotating any number of relevant
keywords in the abstracts, attaching them to any
class in the taxonomy, and classifying the same text
in more than one way. It was implemented inside the
familiar Mozilla Firefox browser using its extension
facility. A screenshot illustrating the tool is provided
in Figure 1.
3.3 Annotation
Given a set of initial guidelines agreed by the ex-
perts, one of the experts annotated a subset of the
corpus, the other two evaluated the result, disagree-
ments were then discussed, and the guidelines were
improved where needed. This process (crucial for
maintaining quality) was repeated several times.
The guidelines described below are the final result
of this work.
3.3.1 Relevance annotation
An abstract is classified as (i) relevant when it (or
its title) contains evidence relevant for CRA and (ii)
irrelevant when it (or its title) contains no evidence
or contains ?negative? evidence (e.g. diseases or
endpoints unrelated to cancer). Abstracts containing
vague, conflicting or complex evidence (e.g. stud-
ies on chemicals in complex mixtures) or evidence
whose association with cancer is currently unclear
were dealt on case by case basis. All the potentially
relevant abstracts were included for further assess-
ment as not to lose data valuable for CRA.
The experts annotated the 1297 abstracts in the
corpus. 89.4% were classified as relevant, 10.1% as
irrelevant, and 0.5% as unsure. We used the Kappa
statistics (Cohen 1960) to measure inter-annotator
agreement on unseen data which two experts an-
notated independently. 208 abstracts were selected
randomly from the 15 journals and from 16 jour-
nals likely to be irrelevant for CRA. The latter were
included to make the task harder as the proportion
of relevant abstracts was high in our corpus. Our
Kappa result is 0.68 ? a figure which indicates sub-
stantial agreement (Landis and G.Koch 1977).
The experts disagreed on 24 (11.5% of the) ab-
stracts. Half of the disagreements are due to one
of the annotators failing to notice relevant evidence.
Such cases are likely to decrease when annotators
gain more experience. The other half are caused by
vague or conflicting evidence. Many of these could
be addressed by further development of guidelines.
3.3.2 Keyword annotation
Keyword annotation focussed on the types of sci-
entific evidence experts typically look for in CRA:
carcinogenic activity (human, animal, cellular, and
other mechanistic data), Mode of Action (MOA)
(data for a specific MOA type ? genotoxic or non-
110
Chemical Retrieved Relevant
1,3-butadiene 195 187
phenobarbital 270 240
diethylnitrosamine 221 214
diethylstilbestrol 145 110
benzoapyrene 201 192
fumonisin 80 70
chloroform 96 84
styrene 162 132
Total 1297 1164
Table 1: Total of abstracts per chemical
genotoxic), and relevant parts of toxicokinetics (e.g.
metabolic activation). The experts annotated the
keywords which they considered as the most impor-
tant and which jointly identify the types of scientific
data offered by the abstract. They focussed on new
(rather than previously published) data on the chem-
ical in question.
All the 1164 abstracts deemed relevant were an-
notated. A total of 1742 unique keywords were
identified, both simple nouns and complex nomi-
nals / phrases. Figure 1 shows an example of an
annotated abstract where the keyword chromoso-
mal aberrations is identified as evidence for geno-
toxic MOA. Since the experts were not required to
annotate every relevant keyword, calculating inter-
annotator agreement was not meaningful. However,
the keyword annotation was evaluated jointly with
taxonomy classification (the following section).
3.4 The taxonomy and the resulting corpus
During keyword annotation, the initial taxonomy
was extended and refined with new classes and class
members. The resulting taxonomy relies solely on
expert knowledge. Experts were merely advised
on the main principles of taxonomy creation: the
classes should be conceptually coherent and their hi-
erarchical organization should be in terms of coher-
ent sub- and superordinate relations.
The taxonomy contains three top level classes:
1) Carcinogenic activity (CA), 2) Mode of Action
(MOA) and 3) Toxicokinetics (TOX). 1) and 2) are
organized by TYPE-OF relations (leukemia is a type
of carcinogenic evidence) and 3) by PART-OF rela-
tions (biodegradation is a part of Metabolism). Each
top level class divides into sub-classes. Figure 2
shows CA taxonomy with three keyword examples
per class. The taxonomy has 48 classes in total; half
of them under CA. Table 6 shows the total number
of abstracts and keywords per class: 82.4% of the
abstracts include keywords for CA, and 50.3% and
28.1% for MOA and TOX, respectively.
We calculated inter-annotator agreement for as-
signing abstracts to taxonomy classes. For each of
the 8 chemicals, 10 abstracts were randomly cho-
sen from the 15 journals. The average agreement
between two annotators is the highest with CA and
MOA (78%) and the lowest with TOX (62%). The
overall agreement is 76%. This result is good, par-
ticularly considering the high number of classes and
the chance agreement of 1.5%. The disagreements
are mostly due to one of the experts annotating as
many keywords as possible, and the other one an-
notating only the ones that classify each abstract as
precisely as possible. This was not a serious prob-
lem for us, but it demonstrates the importance of de-
tailed guidelines. Also, some of the classes were too
imprecise to yield unique distinctions. Future work
should focus on refining them further.
4 Automatic classification
To examine whether the classification created by ex-
perts provides a good representation of the corpus
data and is machine learnable, we conducted a se-
ries of abstract classification experiments.
4.1 Methods
4.1.1 Feature extraction
The first step of text categorization (TC) is to
transform documents into a feature vector represen-
tation. We experimented with two document rep-
resentation techniques. The first one is the sim-
ple ?bag of words? approach (BOW) which consid-
ers each word in the document as a separate feature.
BOW was evaluated using three methods which have
proved useful in previous TC work: (i) stemming
(using the Porter (1980) stemmer) which removes
affixes from words, (ii) the TFIDF weighting (Kib-
riya et al 2004), and (iii) stop word removal.
The second technique is the recent ?bag of sub-
strings? (BOS) method by (Wang et al 2008) which
considers the whole abstract as a string and extracts
from it all the length p substrings without affix re-
moval. BOS has proved promising in biomedical
TC (Han et al 2006, Wang et al 2008) and un-
like a traditional grammatical stemmer, does not re-
111
Figure 2: Taxonomy of Carcinogenic Activity
quire domain tuning for optimal performance. Be-
cause BOS generates substrings with fixed length p,
a word shorter than p?2 can get obscured by its con-
text3. For example, ?mice? would be transformed to
? mice a?, ? mice b?, . . . , which is less informative
than the original word form. Therefore, we enriched
BOS features with word forms shorter than p? 2.
4.1.2 Feature selection
We employed two feature selection methods for
dimensionality reduction. The first is Information
Gain (IG) which has proved useful in TC (Yang
and Pedersen 1997). Given a feature?s distribu-
tion X and class label distribution Y , IG(X) =
H(Y ) ? H(Y |X), H(X) is the entropy of X. The
second method fscore optimises the number of fea-
tures (N ). Features are first ranked using the simple
fscore criterion (Chen and Lin 2006), and N is se-
lected based on the performance of the SVM classi-
fier using the N features.
4.1.3 Classification
Three classifiers were used: Naive Multino-
mial Bayesian (NMB), Complement Naive Bayesian
(CNB) (Rennie and Karger 2003) and Linear Sup-
port Vector Machines (L-SVM) (Vapnik 1995).
NMB is a widely used classifier in TC (Kib-
riya et al 2004). It selects the class C with
the maximum probability given the document d:
argmaxc Pr(C)?w?d Pr(X = w|C). Pr(C) can
3Minus 2 because of space characters.
be estimated from the frequency of documents in C .
Pr(X = w|C) is estimated as the fraction of tokens
in documents of class C that contain w.
CNB extends NMB by addressing the problems
it has e.g. with imbalanced data and weight
magnitude error. The class c of a document
is: argmaxc[logp(?c)??i filogNc?i+?iNc?+? ]. Nc?i is the
number of times term i occurs in classes other than
c. ? and ?i are the smoothing parameters. p(?c) is
the prior distribution of class c.
L-SVM is the basic type of SVM which pro-
duces a hyperplane that separates two-class samples
with a maximum margin. It handles high dimen-
sional data efficiently, and has shown to perform
well in TC (Yang and Liu 1999). Given the data
set X = (x1, y1), . . . , (xn, yn) yi ? {?1,+1},
L-SVM requires a solution w to the following un-
constrained optimisation problem: min(12wTw +
C?ni=1 max(1 ? yiwTxi, 0)2. Cost parameter C
was estimated within range 22,. . . , 25 on training
data using cross validation. The C of the posi-
tive class was weighted by class population ratio
r = negative populationpositive population .
4.1.4 Evaluation
We used the standard measures of recall (R), pre-
cision (P) and F measure (F) for evaluation. These
are defined as follows:
R = TPTP+FN P = TPTP+FP F = 2?R?PR+P
Our random baseline is P+N+P+ .
112
P+/N : positive/negative population TP: truth positive; FN: false negative, FP: false positive
4.2 Experimental evaluation
4.2.1 Data
Our data was the expert annotated CRA corpus.
4.2.2 Document preprocessing
We first evaluated the BOW preprocessing tech-
nique with and without the use of (i) the Porter
(1980) stemmer, (ii) TFIDF, (iii) stop word removal,
and (iv) their combinations. The evaluation was
done in the context of the binary relevance classifica-
tion of abstracts (not in the context of the main tax-
onomic classification task to avoid overfitting pre-
processing techniques to the taxonomy). Only (iii)
improved all the classifiers and was thus adopted
for the main experiments. The poor performance
of (i) demonstrates that a standard stemmer is not
optimal for our data. As highlighted by (Han et al
2006, Wang et al 2008), semantically related bio-
logical terms sharing the same stem are not always
reducible to the stem form.
4.2.3 Feature selection
We evaluated the feature selection methods on
two taxonomy classes: the most balanced class ?An-
imal study? (positive/negative 1:1.4) and an imbal-
anced class ?Adducts? (positive/negative 1:6.5). IG
was used for the fixed N setting and fscore for the
dynamic N setting. Each combination of classifiers
(NMB/CNB/SVM), document representations (BOW,
BOS) and settings for N (dynamic, . . . , 83098) was
evaluated. The results show that the dynamic setting
yields consistent improvement on all the setups (al-
though the impact on SVM?s is not big). Also the
optimal N varies by the data and the classifier. Thus,
we used the dynamic feature selection in the taxo-
nomic classification.
4.2.4 Taxonomic classification
Experimental setup We ran two sets of experi-
ments on the corpus, using 1) BOW and 2) BOS for
feature extraction. Without feature selection, BOW
had c. 9000 features and BOS c. 83000. Features
were selected using fscore. For each class with
more than 20 abstracts (37 in total)4, three ?one
4The classes with less than 20 abstracts may have less than
2 positive abstracts in each fold of 10 fold CV, which is not
Method Feature Set P R F
NMB BOW 0.59 0.75 0.66
NMB BOS 0.62 0.82 0.70
CNB BOW 0.52 0.74 0.60
CNB BOS 0.57 0.76 0.64
SVM BOW 0.68 0.76 0.71
SVM BOS 0.71 0.77 0.74
Table 2: Performance of classifiers with BOS/BOW
Class Method P R F
CA NMB 0.94 0.89 0.91
CA CNB 0.92 0.94 0.93
CA SVM 0.93 0.93 0.93
MOA NMB 0.88 0.81 0.84
MOA CNB 0.84 0.82 0.83
MOA SVM 0.92 0.80 0.86
TOX NMB 0.66 0.83 0.74
TOX CNB 0.70 0.80 0.75
TOX SVM 0.76 0.79 0.78
Table 3: Result for the top level classes
against other? classifiers (NMB, CNB and L-SVM)
were trained and tested using 10-fold cross valida-
tion.
Results Table 2 shows the average performance
for the whole taxonomy. The performance of BOS
is better than that of BOW according to all the three
measures. On average, BOS outperforms BOW by
4% in P and F, and 3% in R. SVM yields the best
overall P and F (0.71 and 0.74) with BOS. Surpris-
ingly, NMB outperforms CNB with all the settings.
NMB yields the best overall R with BOS (0.82) but
its P is notably lower than that of SVM.
Table 3 shows the average P, R and F for the top
level classes using the best performing feature set
BOS with the three classifiers. CA has the best F
(0.93). Its positive population is the highest (posi-
tive/negative: 5:1). TOX with a lower positive pop-
ulation (1:2.6) has still good F (0.78). R and P are
balanced with an average difference of 0.06.
Table 4 shows the distribution of F across the
taxonomy. There is a clear correlation between
representative for the class population.
No. of abstracts(f) Classes F Random
f > 300 9 0.80 0.38
100 < f ? 300 12 0.73 0.13
20 < f ? 100 16 0.68 0.04
Table 4: Mean F and random baseline for taxonomic
classes in three frequency ranges.
113
frequency and performance: the average F de-
creases with descending frequency range, revealing
increased classification difficulty. Classes with more
than 300 abstracts have the highest average F (0.80
with standard deviation (SD) 0.08). Classes with
20-100 abstracts have the average F 0.68 (SD 0.11),
which is lower but still fairly good. No class has F
lower than 0.46, which is much higher than the av-
erage random baseline of 0.11.
5 User Test
A user test was carried out to examine the practical
usefulness of the automatic classification in a near
real-world scenario. The L-SVM+BOS classifier was
applied to the PubMed abstract data (from 1998-
2008) of five unseen chemicals representing geno-
toxic (geno) and non-genotoxic (non) MOAs (see
table 5). The results were displayed to two experts
in a friendly web interface. The experts were in-
vited to imagine that they have submitted a query to
a system, the system has returned the classification
of relevant abstracts for each chemical, and the task
is to judge whether it is correct. The top 500 BOS
features per class were shown to aid the judgement.
Results were evaluated using precision (P) (re-
call could not be calculated as not all of the positive
polulation was known). Table 5 shows the average
P for chemicals and top level classes. The results
are impressive: the only chemical with P lower than
0.90 is polychlorinated biphenyls (PCB). As PCB
has a well-known neuro-behavioural effect, the data
includes many abstracts irrelevant for CRA. Most
other errors are due to the lack of training data for
low frequency classes. For example, the CRA cor-
pus had only 27 abstracts in ?DNA repair (damage)?
class, while the new corpus has many abstracts on
DNA damage some of which are irrelevant for CRA.
The experts found the tool easy to use and felt
that if such a tool was available to support real-world
CRA, it could significantly increase their productiv-
ity and also lead to more consistent and thorough
CRA. Such a wide range of scientific evidence is dif-
ficult to gather via manual means, and chemical car-
cinogenesis is such a complex process that even the
most experienced risk assessor is incapable of mem-
orizing the full range of relevant evidence without
the support of a thorough specification / taxonomy.
Name MOA ? P
Aflatoxin B1 geno 189 0.95
Benzene geno 461 0.99
PCB non 761 0.89
Tamoxifen non 382 0.96
TCDD non 641 0.96
Class P
CA 0.94
MOA 0.95
TOX 0.99
Table 5: Chemicals and the results of the user test
6 Conclusion and Future Work
The results of our inter-annotator agreement tests,
automatic classification experiments and the user
test demonstrate that the taxonomy created by risk
assessors is accurate, well-defined, and can be use-
ful in a real-world CRA scenario. This is particu-
larly encouraging considering that the taxonomy is
based on biomedical annotation. As highlighted by
(Kim et al 2008), expert annotation is more chal-
lenging and prone to inter-annotator disagreement
than better-constrained linguistic annotation. We
believe that we obtained promising results because
we worked in collaboration with risk assessors and
developed technology which imitates their current
practices as closely as possible.
Most related work focuses on binary classifica-
tion, e.g. BioCreative II had a subtask (Krallinger
et al 2008) on the relevance classification of ab-
stracts for protein interactions. The few works
that have attempted multi-classification include e.g.
that of Aphinyanaphongs et al (2005) who applied
NMB, SVM and AdaBoost to classify abstracts of
internal medicine into four categories, and that of
Han et al (2006) who used BOS and NMB/L-SVM to
classify abstracts in five categories of protein post-
translational modifications.
In the future, we plan to refine the taxonomy fur-
ther by careful analysis of keyword types found in
the data and the taxonomic relationships defined by
experts. This will help to transform the taxonomy
into a better-developed knowledge resource. We
also need to extend the taxonomy. Although our
results show that the current taxonomy provides a
good basis for the classification of CRA literature,
it is not comprehensive: more data is required espe-
cially for low frequency classes, and the taxonomy
needs to be extended to cover more specific MOA
types (e.g. further subtypes of non-genotoxic chem-
icals).
The taxonomy can be extended by manual annota-
114
Change in F ? Classes Abstracts of class
20-100 100 - 200 200 - 1100
?F > 1% 16 (43%) 75% 33% 8%
|?F | ? 1% 15 (41%) 6% 44% 75%
?F < ?1% 6 (16%) 19% 33% 17%
Table 6: F gain(?F ) of MeSH compared to BOS
Class ? F
Carcinogenic activity 1068 92.8
Human study/epidemiology 190 77.7
Animal study 629 80.2
Cell experiments 319 78.5
Study on microorganisms 44 85.2
Mode of Action 653 85.5
Genotoxic 421 89.1
Nongenotoxic 324 76.3
Toxicokinetics 356 77.7
Absorption, . . . ,excretion 113 69.8
Metabolism 268 76.4
Toxicokinetic modeling 31 84.6
Table 7: ? abstracts and F of level 1,2 classes.
tion, supplementing it with additional information in
knowledge resources and/or by automatic methods.
One knowledge resource potentially useful is the
Medical Subject Headings (MeSH) taxonomy (Nel-
son et al 2002) which classifies PubMed abstracts
according to manually defined terms. We performed
a small experiment to investigate the usefulness of
MeSH for supplementing our current classification.
MeSH terms were first retrieved for each abstract us-
ing EFetch (NCBI 2005) and then appended to the
BOS feature vector. Best features were then selected
using fscore and classified using L-SVM. The fig-
ures in table 6 show that the results improved sig-
nificantly for 43% of the low frequency classes. Al-
though this demonstrates the potential usefulness of
additional resources, given the rapidly evolving na-
ture of CRA data, the best approach long term is
to develop technology for automatic updating of the
taxonomy from literature. Given the basic resources
we have constructed, the development of such tech-
nology is now realistic and can be done using unsu-
pervised or semi-supervised machine learning tech-
niques, e.g. (Cohen and Hersh 2005, Blaschko and
Gretton 2009).
The automatic classification could be improved
by the use of more sophisticated features extracted
using NLP tools that have been tuned for biomedi-
cal texts, such as parsers, e.g. (Tsuruoka et al 2005),
and named entity recognizers, e.g. (Corbett et al
2007), and exploiting resources such as the BioLex-
ion (Sasaki et al 2008).
Our long term goal is to develop a TM tool
specifically designed for CRA. Some tools have re-
cently been built to assist other critical activities of
biomedicine (e.g. literature curation for genetics).
A few of them have been evaluated for their practi-
cal usefulness in a real-world scenario (Karamanis
et al 2008, Demaine et al 2006). Such tools and
evaluations act as an important proof of concept for
biomedical TM and help to develop technology for
the needs of practical applications.
According to the interviews we conducted (Sec-
tion 2), a tool capable of identifying, ranking and
classifying articles based on the evidence they con-
tain, displaying the results to experts, and assisting
also in subsequent steps of CRA would be particu-
larly welcome. Such a tool, if developed in close
collaboration with users, could significantly increase
the productivity of CRA and enable risk assessors
to concentrate on what they are best at: the expert
judgement.
Acknowledgements Our work was funded by the
Royal Society (UK), the Medical Research Council
(G0601766) (UK) and the Swedish Council for Working
Life and Social Research (Sweden). LS was supported
by a Dorothy Hodgkin Postgraduate Award (UK). We
would like to thank Ian Lewin for his assistance at the
early stages of this work and for providing the first ver-
sion of the annotation tool. We are also grateful to Johan
Hogberg for supporting the annotation and the taxonomy
construction work.
References
Sophia Ananiadou, Douglas B. Kell, and Jun ichi Tsujii.
Text mining and its potential applications in systems
biology. Trends in Biotechnology, 24(12), 2006.
Y. Aphinyanaphongs, I. Tsamardinos, A. Statnikov,
D. Hardin, and C.F. Aliferis. Text categorization
models for high-quality article retrieval in internal
medicine. JAMIA, 12(2), 2005.
Matthew Blaschko and Arthur Gretton. Learning tax-
onomies by dependence maximization. In 22rd NIPS,
2009.
Yi-Wei Chen and Chih-Jen Lin. Combining SVMs with
various feature selection strategies. In Feature extrac-
tion, foundations and applications. 2006.
Aaron M. Cohen and William R. Hersh. A survey of
115
current work in biomedical text mining. Briefings in
Bioinformatics, 6(1), 2005.
Jacob Cohen. A coefficient of agreement for nominal
scales. Educ. Psychol. Meas., 20(1), 1960.
K. Bretonnel Cohen, Hong Yu, Philip E. Bourne, and
Lynette Hirschman. Translating biology:text mining
tools that work. In PSB, 2008.
Peter Corbett, Colin Batchelor, and Simone Teufel. An-
notation of chemical named entities. In Proceedings of
the ACL, 2007.
Jeffrey Demaine, Joel Martin, Lynn Wei, and Berry
de Bruijn. Litminer: integration of library services
within a bio-informatics application. Biomedical Dig-
ital Libraries, 3(1), 2006.
ECHA, 2008. Guidance on Information Requirements
and Chemical Safety Assessment. European Chemicals
Agency, 2008.
Bo Han, Zoran Obradovic, Zhang zhi Hu, Cathy H. Wu,
and Slobodan Vucetic. Substring selection for biomed-
ical document classification. Bioinformatics, 22, 2006.
Lawrence Hunter and K. Bretonnel Cohen. Biomedical
language processing: What?s beyond pubmed? Mol
Cell, 21(5), 2006.
N. Karamanis, R. Seal, I. Lewin, P. McQuilton, A. Vla-
chos, C. Gasperin, R. Drysdale, and T. Briscoe. Nat-
ural language processing in aid of flybase curators.
BMC Bioinformatics, 9(1), 2008.
Ashraf M. Kibriya, Eibe Frank, Bernhard Pfahringer, and
Geoffrey Holmes. Multinomial naive bayes for text
categorization revisited. In Australian Conference on
AI, volume 3339, 2004.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. Cor-
pus annotation for mining biomedical events from lter-
ature. BMC Bioinformatics, 9, 2008.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, and Alfonso Valencia. Overview of the
protein-protein interaction annotation extraction task
of biocreative ii. Genome Biology, 2008.
J.Richard Landis and Gary G.Koch. The measurement of
observer agreement for categorical data. Biometrics,
33(1), 1977.
Ian Lewin, Ilona Silins, Anna Korhonen, Johan Hogberg,
and Ulla Stenius. A new challenge for text mining:
Cancer risk assessment. In Proceedings of the ISMB
BioLINK Special Interest Group on Text Data Mining.,
2008.
NCBI. Efetch entrez utility, 2005. URL
http://www.ncbi.nlm.nih.gov/entrez/
query/static/efetch_help.html.
Sturart J. Nelson, Tammy Powell, and Besty L.
Humphreys. The Unified Medical Language System
(UMLS) Project. In Encyclopedia of Library and In-
formation Science, pages 369?378. Marcel Dekker,
2002.
M. F. Porter. An algorithm for suffix stripping. Program,
14(3):130?137, 1980.
Jason D. M. Rennie and David Karger. Tackling the poor
assumptions of naive bayes text classifiers. In In Pro-
ceedings of the 20th ICML, 2003.
Y. Sasaki, S. Montemagni, P. Pezik, D. Rebholz-
Schuhmann, J. McNaught, and S. Ananiadou. BioLex-
icon: A Lexical Resource for the Biology Domain.
2008.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. Developing a Robust Part-
of-Speech Tagger for Biomedical Text. 3746, 2005.
EPA, 2005. Guidelines for carcinogen risk as-
sessment. U.S. Environmental Protection Agency,
2005. URL http://www.epa.gov/iris/
cancer032505.pdf.
Vladimir N. Vapnik. The nature of statistical learning
theory. New York, NY, USA, 1995.
Hongning Wang, Minlie Huang, Shilin Ding, and Xi-
aoyan Zhu. Exploiting and integrating rich features
for biological literature classification. BMC Bioinfor-
matics, 9(Suppl 3), 2008.
Yiming Yang and Xin Liu. A re-examination of text cate-
gorization methods. In Proceedings of the 22nd SIGIR,
New York, NY, USA, 1999.
Yiming Yang and Jan O. Pedersen. A comparative study
on feature selection in text categorization. 1997.
Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,
and Kevin B. Cohen. Frontiers of biomedical text min-
ing: current progress. Brief Bioinform, 8(5), 2007.
116
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 689?697,
Beijing, August 2010
Exploring variation across biomedical subdomains
Tom Lippincott and Diarmuid O? Se?aghdha and Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
{tl318,do242,ls418,alk23}@cam.ac.uk
Abstract
Previous research has demonstrated the
importance of handling differences be-
tween domains such as ?newswire? and
?biomedicine? when porting NLP systems
from one domain to another. In this paper
we identify the related issue of subdomain
variation, i.e., differences between subsets
of a domain that might be expected to be-
have homogeneously. Using a large corpus
of research articles, we explore how subdo-
mains of biomedicine vary across a variety
of linguistic dimensions and discover that
there is rich variation. We conclude that
an awareness of such variation is necessary
when deploying NLP systems for use in
single or multiple subdomains.
1 Introduction
One of the most noticeable trends in the past
decade of Natural Language Processing (NLP) re-
search has been the deployment of language pro-
cessing technology to meet the information re-
trieval and extraction needs of scientists in other
disciplines. This meeting of fields has proven mu-
tually beneficial: scientists increasingly rely on
automated tools to help them cope with the expo-
nentially expanding body of publications in their
field, while NLP researchers have been spurred to
address new conceptual problems in theirs. Among
the fundamental advances from the NLP perspec-
tive has been the realisation that tools which per-
form well on textual data from one source may fail
to do so on another unless they are tailored to the
new source in some way. This has led to signifi-
cant interest in the idea of contrasting domains and
the concomitant problem of domain adaptation,
as well as the production of manually annotated
domain-specific corpora.1
One definition of domain variation associates
it with differences in the underlying probability
distributions from which different sets of data are
drawn (Daume? III and Marcu, 2006). The concept
also mirrors the notion of variation across thematic
subjects and the corpus-linguistic notions of reg-
ister and genre (Biber, 1988). In addition to the
differences in vocabulary that one would expect
to observe, domains can vary in many linguistic
variables that affect NLP systems. The scientific
domain which has received the most attention (and
is the focus of this paper) is the biomedical domain.
Notable examples of corpus construction projects
for the biomedical domain are PennBioIE (Kulick
et al, 2004) and GENIA (Kim et al, 2003). These
corpora have been used to develop systems for a
range of processing tasks, from entity recognition
(Jin et al, 2006) to parsing (Hara et al, 2005) to
coreference resolution (Nguyen and Kim, 2008).
An implicit assumption in much previous work
on biomedical NLP has been that particular subdo-
mains of biomedical literature ? typically molec-
ular biology ? can be used as a model of biomed-
ical language in general. For example, GENIA
consists of abstracts dealing with a specific set
of subjects in molecular biology, while PennBioIE
covers abstracts in two specialised domains, cancer
genomics and the behaviour of a particular class
of enzymes. This assumption of representative-
ness is understandable because linguistic annota-
tion is labour-intensive and it may not be worth-
while to produce annotated corpora for multiple
subdomains within a single discipline if there is lit-
1A workshop dedicated to domain adaptation is collocated
with ACL 2010.
689
tle task-relevant variation across those subdomains.
However, such conclusions should not be made
before studying the actual degree of difference be-
tween the subdomains of interest.
One of the principal goals of this paper is to map
how the concept of ?biomedical language?, often
construed as a monolithic entity, is composed of
diverse patterns of behaviour at more fine-grained
topical levels. Hence we study linguistic variation
in a broad biomedical corpus of abstracts and full
papers, the PMC Open Access Subset.2 We select
a range of lexical and structural phenomena for
quantitative investigation. The results indicate that
common subdomains for resource development are
not representative of biomedical text in general and
furthermore that different linguistic features often
partition the subdomains in quite different ways.
2 Related Work
A number of researchers have explored the dif-
ferences between non-technical and scientific lan-
guage. Biber and Gray (2010) describe two
distinctive syntactic characteristics of academic
writing which set it apart from general English.
Firstly, in academic writing additional information
is most commonly integrated by pre- and post-
modification of phrases rather than by the addi-
tion of extra clauses. Secondly, academic writing
places greater demands on the reader by omitting
non-essential information, through the frequent
use of passivisation, nominalisation and noun com-
pounding. Biber and Gray also show that these ten-
dencies towards ?less elaborate and less explicit?
language have become more pronounced in recent
history.
We now turn to corpus studies that focus on
biomedical writing. Verspoor et al (2009) use
measurements of lexical and structural variation
to demonstrate that Open Access and subscription-
based journal articles in a specific domain (mouse
genomics) are sufficiently similar that research on
the former can be taken as representative of the lat-
ter. While their primary goal is different from ours
and they do not consider variation across multiple
domains, they do compare their mouse genomics
corpus with small reference corpora drawn from
2http://www.ncbi.nlm.nih.gov/pmc/
about/openftlist.html
newswire and general biomedical sources. This
analysis unsurprisingly finds differences between
the domain and newswire corpora across many
linguistic dimensions; more interestingly for our
purposes, the comparison of domain text to the
broader biomedical superdomain shows a more
complex picture with similarities in some aspects
(e.g., passivisation and negation) and dissimilari-
ties in others (e.g., sentence length, semantic fea-
tures).
Friedman et al (2002) document the ?sublan-
guages? associated with two biomedical domains:
clinical reports and molecular biology articles.
They set out restricted ontologies and frequent co-
occurrence templates for the two domains and dis-
cuss the similarities and differences between them,
but they do not perform any quantitative analysis.
Other researchers have focused on specific phe-
nomena, rather than cataloguing a broad scope
of variation. Cohen et al (2008) carry out a de-
tailed analysis of argument realisation with respect
to verbs and nominalisations, using the GENIA
and PennBioIE corpora. Nguyen and Kim (2008)
compare the behaviour of anaphoric pronouns in
newswire and biomedical corpora; they improve
the performance of a pronoun resolver by incorpo-
rating their observations, thus demonstrating the
importance of capturing domain-specific phenom-
ena. Nguyen and Kim?s findings are discussed in
more detail in Section 5.4 below.
3 Subdomains in the OpenPMC Corpus
The Open Access Subset of PubMed (OpenPMC)
is the largest publicly available corpus of full-text
articles in the biomedical domain. OpenPMC is
comprised of 169,338 articles drawn from 1233
medical journals, totalling approximately 400 mil-
lion words. The NIH maintains a one-to-many
mapping from journals to 122 subject areas (NIH,
2009b). This covers about 400 of the OpenPMC
journals, but these account for over 70% of the
database by byte size and word count. Journals are
assigned up to five subject areas with the majority
assigned one (69%) or two (26%) subjects. In this
paper we adopt the OpenPMC subject areas (e.g.
?Pulmonary Medicine?, ?Genetics?, ?Psychiatry?)
as the basis for subdomain comparison.
690
0 10 20 30 40Word count (millions)
Ethics
Complementary Therapies
Education
Obstetrics
Pharmacology
Geriatrics
Gastroenterology
Pediatrics
Veterinary Medicine
Biomedical Engineering
Psychiatry
Embryology
Genetics, Medical
Ophthalmology
Vascular Diseases
Botany
Virology
Endocrinology
Pulmonary Medicine
Physiology
Tropical Medicine
Critical Care
Rheumatology
Cell Biology
Communicable Diseases
Science
Neurology
Biotechnology
Medicine
Microbiology
Environmental Health
Public Health
Biochemistry
Molecular Biology
Neoplasms
Medical Informatics
Genetics
Figure 1: OpenPMC word count by subdomain,
dark colouring indicates data assigned single sub-
domain, each lighter shade indicates an additional
overlapping subdomain
4 Methodology
4.1 Data selection and preprocessing
An important initial question was how to treat data
with multiple classifications: we only consider
journals assigned a single subdomain, to avoid
the added complexity of interactions in data from
overlapping subdomains. To ensure sufficient data
for comparing a variety of linguistic features, we
discard the subdomains with less than one mil-
lion words meeting the single-subdomain criterion.
After review, we also drop the ?Biology? subdo-
main, which appears to function as a catch-all for
many loosely related areas. Figure 1 shows the
distribution of data across the subjects we use, by
word-count, with lighter-coloured areas represent-
ing data that is assigned multiple subjects. These
subjects provide a convenient starting point for di-
viding the corpus into subdomains (hereafter, ?sub-
domain? will be used rather than ?subject?). We
also add a reference subdomain, ?Newswire?, com-
posed of a 6 million word random sample from the
English Gigaword corpus (Graff et al, 2005). The
final data set has a total of 39 subdomains.
Articles in the OpenPMC corpus are formatted
according to a standard XML tag set (NIH, 2009a).
We first convert each article to plain text, ignoring
?non-content? elements such as tables and formulas,
and split the result into sentences, aggregating the
results by subdomain.
4.2 Feature extraction
We investigate subdomain variation in our cor-
pus across a range of lexical, syntactic, sentential
and discourse features. The corpus is lemmatised,
tagged and parsed using the C&C pipeline (Cur-
ran et al, 2007) with the adapted part-of-speech
and lexical category tagging models produced by
Rimell and Clark (2009) for biomedical parsing.
From this output we count occurrences of noun,
verb, adjective and adverb lemmas, part-of-speech
(POS) tags, grammatical relations (GRs), chunks,
and lexical categories. The lemma features are
Zipfian-distributed items from an open class, so
we have experimented with filtering low-frequency
items at various thresholds to reduce noise and
improve processing speed. The other feature sets
can be viewed as closed classes, where filtering is
unnecessary.
Since verbs are central to the meaning and struc-
ture of sentences, we consider their special behav-
ior by constructing features for each verb?s dis-
tribution over other grammatical properties. Sev-
eral grammatical properties are captured by pairing
each verb with its POS (indicating e.g. tense, such
as present, past, and present participle). Voice is de-
termined from additional annotation output by the
C&C parser. Table 1 shows the POS-distribution
for the verb ?restrict?, in two subdomains from
the corpus. Finally, we record distributions over
verb subcategorization frames (SCFs) taken by
each verb, and over the GRs it participates in.
691
Subdomain VB VBG VBN VBP VBZ
Medical Informatics .35 .29 .06 .09 .21
Cell Biology .14 .43 .05 .10 .29
Table 1: Distribution over POS tags for verb ?re-
strict?, in two subdomains
SCFs were extracted using a system of Preiss et al
(2007).
To facilitate a more robust and interpretable anal-
ysis of vocabulary differences, we estimate a ?topic
model? of the corpus with Latent Dirichlet Analy-
sis (Blei et al, 2003) using the MALLET toolkit.3
As preprocessing we divide the corpus into arti-
cles, removing stopwords and words shorter than
3 characters. The Gibbs sampling procedure is
parameterised to induce 100 topics, each giving a
coherent cluster of related words learned from the
data, and to run for 1000 iterations. We collate the
predicted distribution over topics for each article
in a subdomain, weighted by article wordcount, to
produce a topic distribution for the subdomain.
4.3 Measurements of divergence
Our goal is to illustrate the presence or absence
of differences between the feature sets, and to do
so we calculated the Jensen-Shannon divergence
and the Pearson correlation. Jensen-Shannon diver-
gence is a finite symmetric measurement of the di-
vergence between probability distributions, while
Pearson correlation quantifies the linear relation-
ship between two real-valued samples.
The count-features are weighted, for a given
subdomain, by the feature?s log-likelihood be-
tween the subdomain?s data and the rest of the
corpus. Log-likelihood has been shown to perform
well when comparing counts of potentially low-
frequency features (Rayson and Garside, 2000)
such as found in Zipfian-distributed data. This
serves to place more weight in the comparison on
items that are distinctive of the subdomain with
respect to the entire corpus.
While the count-features are treated as a single
distribution for the purposes of JSD, the verbwise-
features are composed of many distributions, one
for each verb lemma. Our approach is to com-
bine the JSD of the verbs, weighted by the log-
3http://mallet.cs.umass.edu
likelihood of the verb lemma between the two
subdomains in question, and normalize the dis-
tances to the interval [0, 1]. Using the lemma?s log-
likelihood assumes that, when a verb?s distribution
behaves differently in a subdomain, its frequency
changes as well.
We present the results as dendrograms and
heat maps. Dendrograms are tree structures that
illustrate the results of hierarchical clustering.
We perform hierarchical clustering on the inter-
subdomain divergences for each set of features.
The algorithm begins with each instance (in our
case, subdomains) as a singleton cluster, and re-
peatedly joins the two most similar clusters until
all the data is clustered together. The order of these
merges is recorded as a tree structure that can be
visualized as a dendrogram in which the length of
a branch represents the distance between its child
nodes. Similarity between clusters is calculated us-
ing average distance between all members, known
as ?average linking?.
Heat maps show the pairwise calculation of
a metric in a grid of squares, where square
(x, y) is shaded according to the value of
metric(subx, suby). For our measurements of
JSD, black represents 0 (i.e. identical distributions)
and white represents the metric?s theoretical maxi-
mum of 1. We also inscribe the actual value inside
each square. Dendrograms are tree structures that
illustrate the hierarchical clustering procedure de-
scribed above. The dendrograms present all 39
subdomains, while for readability the heatmaps
present 12 subdomains selected for representative-
ness.
5 Results
Different thresholds for filtering low-frequency
terms had little effect on the divergence measures,
and served mainly to improve processing time. We
therefore report results using a cutoff of 150 occur-
rences (over the entire 234 million word data set)
and log-likelihood weights. The results of Pearson
correlation and JSD show similar trends, and due
to its specific design for comparing distributions
we only report the latter.
692
5.1 Vocabulary and lexical features
Differences in vocabulary are what first comes to
mind when describing subdomains. Word features
are fundamental components for systems such as
POS taggers and lexicalised parsers; one therefore
expects that these systems will be affected by vari-
ation in lexical distributions. Figure 2a uses JSD
calculated on each subdomain?s distribution over
100 LDA-induced topics to compare vocabulary
distributions. Subdomains related to molecular
biology (Genetics, Molecular Biology) show the
smallest divergences, an interesting fact since these
are heavily used in building resources for BioNLP.
The dendrogram shows a rough division into ?pub-
lic policy?, ?patient-centric?, ?applied? and ?mi-
croscopic? subdomains, with the distance between
unrelated subdomains such as Biochemistry and
Pediatrics almost as large as their respective differ-
ences from Newswire.
We omit figures for variation over noun, verb
and adjective lemmas due to space restrictions; in
general, these correlate with the variation in LDA
topics though there are some differences. Figure 2b
shows JSD calculated on distributions over adverb
lemmas. Part of the variation is due to character-
istic markers of scientific argument (?therefore?,
?significantly?, ?statistically?). A more interesting
factor is the coining of domain-specific adverbs,
an example of the tendency in scientific text to use
complex lexical items and premodifiers rather than
additional clauses. This also has the effect of mov-
ing subdomain-specific objects and processes from
verbs and nouns to adverbs. This behavior seems
non-continuous, in that subdomains either make
heavy, or almost no, use of it: for example, Pedi-
atrics has no subdomain-specific items among the
its ten top adverbs by log-likelihood, while Neo-
plasms has ?histologically?, ?immunohistochemi-
cally? and ?subcutaneously?. These information-
dense terms could prove useful for tasks like auto-
matic curation of subdomain vocabularies, where
they imply relationships between their components,
the items they modify, etc.
5.2 Verb distributional behavior
Modelling verb behavior is important for both syn-
tactic (Collins, 2003) and semantic (Korhonen et
al., 2008) processing, and subdomains are known
to conscript verbs into specific roles that change the
distributions of their syntactic properties (Roland
and Jurafsky, 1998). The four properties we con-
sidered verbs? distributions over (SCF, POS, GR
and voice) produced similar inter-subdomain JSD
values. Figure 2c demonstrates how verbs differ
between subdomains with respect to SCFs. For
example, while the Pediatrics subdomain uses the
verb ?govern? in a single SCF among its 12 pos-
sibilities, the Genetics subdomain distributes its
usage over 7 of them. Two subdomains may both
use ?restrict? with high frequency (e.g. Molecular
Biology and Ethics), but with different frequency
distributions over SCFs.
5.3 Syntax
It is difficult to measure syntactic complexity accu-
rately without access to a hand-annotated treebank,
but it is well-known that sentence length corre-
lates strongly with processing difficulty (Collins,
1996). The first column of Table 2 gives average
sentence lengths (excluding punctuation and ?sen-
tences? of fewer than three words) for selected
domains. All standard errors are < 0.1. It is clear
that all biomedical subdomains typically use longer
sentences than newswire, though there is also vari-
ation within biomedicine, from an average length
of 27 words in Molecular Biology to 24.5 words
in Pediatrics.
?Packaging? information in complex pre- and/or
post-modified noun phrases is a characteristic fea-
ture of academic writing (Biber and Gray, 2010).
This increases the information density of a sen-
tence but brings with it syntactic and semantic
ambiguities. For example, the difficulty of resolv-
ing the internal structure of noun-noun compounds
and strings of prepositional phrases has been the fo-
cus of ongoing research in NLP; these phenomena
have also been identified as significant challenges
in biomedical language processing (Rosario and
Hearst, 2001; Schuman and Bergler, 2006). The
second and third columns of Table 2 present aver-
age lengths for full noun phrases, defined as every
word dominated by a head noun in the grammat-
ical relation graph for a sentence, and for base
nominals, defined as nouns plus premodifying ad-
jectives and nouns only. All standard errors are
? 0.01. Newswire text uses the simplest noun
693
(a) LDA-induced distribution over topics
(b) Adverb lemma frequencies
(c) Verb distributions over subcategorization frames
Figure 2: Subdomain variation plotted as heat maps and dendrograms
694
Sentence length Full NP length Base nominal length
Mol. Biology 27.0 Biochemistry 4.03 Biochemistry 1.85
Genetics 26.6 Genetics 3.90 Neoplasms 1.85
Cell Biology 26.3 Critical Care 3.86 Mol. Biology 1.84
Ethics 26.2 Neoplasms 3.85 Genetics 1.83
PMC Average 25.9 PMC Average 3.85 PMC Average 1.80
Biochemistry 25.8 Pediatrics 3.84 Cell Biology 1.80
Neoplasms 25.5 Med. Informatics 3.84 Critical Care 1.80
Psychiatry 25.3 Comm. Diseases 3.81 Med. Informatics 1.78
Critical Care 25.0 Therapeutics 3.80 Comm. Diseases 1.78
Therapeutics 24.9 Mol. Biology 3.79 Therapeutics 1.75
Comm. Diseases 24.9 Psychiatry 3.77 Psychiatry 1.75
Med. Informatics 24.6 Ethics 3.69 Pediatrics 1.73
Pediatrics 24.6 Cell Biology 3.55 Ethics 1.65
Newswire 19.1 Newswire 3.18 Newswire 1.60
Table 2: Average sentence, NP and base nominal lengths across domains
phrase structures; there is notable variation across
PMC domains. Full NP and base nominal lengths
do not always correlate; for example, Cell Biol-
ogy uses relatively long base NPs (nominalisations
and multitoken names in particular) but relatively
simple full NP structures.
5.4 Coreference
Resolving coreferential terms is a crucial and chal-
lenging task when extracting information from
texts in any domain. Nguyen and Kim (2008)
compare the use of pronouns in the newswire
and biomedical domains, using the GENIA cor-
pus as representative of the latter. Among the dif-
ferences observed between the domains were the
absence of any personal pronouns other than third-
person neuter pronouns in the GENIA corpus, and
a greater proportion of demonstrative pronouns in
GENIA than in the ACE or MUC newswire cor-
pora. Corroborating the importance of domain
modelling, Nguyen and Kim demonstrate that tai-
loring a pronoun resolution system to specific prop-
erties of the biomedical domain improves perfor-
mance.
As our corpus is not annotated for coreference
we restrict our attention to types that are reliably
coreferential: masculine/feminine personal pro-
nouns (he, she and case variations), neuter personal
pronouns (they, it and variations) and definite NPs
with demonstrative determiners such as this and
that. To filter out pleonastic pronouns we used a
combination of the C+C parser?s pleonasm tag and
heuristics based on Lappin and Leass (1994). To
filter out the most common class of non-anaphoric
demonstrative NPs we simply discarded any match-
ing the pattern this. . . paper|study|article.
Table 3 presents statistics for selected types of
coreferential noun phrases in a number of domains.
The results generally agree with the findings of
Nguyen and Kim (2008): biomedical text is on
average 200 times less likely than news text to
use gendered pronouns and twice as likely to use
anaphoric definite noun phrases. At the domain
level, however, there is clear variation within the
biomedical corpus. In contrast to Nguyen and
Kim?s observations about GENIA some domains
do make non-negligible use of gendered pronouns,
most notably Ethics (usually to refer to other schol-
ars) and domains such as Psychiatry and Pediatrics
where studies of actual patients are common. All
biomedical domains use demonstrative NPs more
frequently than newswire and only one (Ethics)
matches newswire for frequent use of neuter 3rd-
person pronouns.
6 Conclusion
In this paper we have explored the phenomenon
of linguistic variation at a finer-grained level than
previous NLP research, focusing on subdomains
695
Pronouns (neuter, 3rd) Pronouns (non-neuter, 3rd) Demonstrative NPs
Ethics 0.0658 Newswire 0.0591 Genetics 0.0275
Newswire 0.0607 Ethics 0.0037 Med. Informatics 0.0263
Therapeutics 0.0354 Pediatrics 0.0015 Biochemistry 0.0263
Med. Informatics 0.0346 Psychiatry 0.0009 Ethics 0.0260
Psychiatry 0.0342 Comm. Diseases 0.0009 Mol. Biology 0.0251
Pediatrics 0.0308 Therapeutics 0.0005 PMC Average 0.0226
PMC Average 0.0284 PMC Average 0.0005 Cell Biology 0.0210
Genetics 0.0275 Critical Care 0.0004 Comm. Diseases 0.0207
Critical Care 0.0272 Neoplasms 0.0002 Neoplasms 0.0205
Mol. Biology 0.0258 Med. Informatics 0.0002 Psychiatry 0.0201
Biochemistry 0.0251 Genetics 0.0001 Critical Care 0.0201
Neoplasms 0.0227 Mol. Biology 2.5? 10?5 Therapeutics 0.0192
Cell Biology 0.0217 Biochemistry 2.0? 10?5 Pediatrics 0.0191
Comm. Diseases 0.0213 Cell Biology 1.5? 10?5 Newswire 0.0118
Table 3: Frequency of coreferential types (proportion of all NPs) across domains
rather than traditional domains such as ?newswire?
and ?biomedicine?. We have identified patterns of
variation across dimensions of vocabulary, syntax
and discourse that are known to be of importance
for NLP applications. While the magnitude of vari-
ation between subdomains is unsurprisingly less
pronounced than between coarser domains, sub-
domain variation clearly does exist and should be
taken into account when considering the generalis-
ability of systems trained and evaluated on specific
subdomains, for example molecular biology.
Future work includes directly evaluating the ef-
fect of subdomain variation on practical tasks, in-
vestigating further dimensions of variation such
as nominalisation usage and learning alternative
subdomain taxonomies directly from the corpus
text. Ultimately, we expect that a more nuanced
understanding of subdomain effects will have tan-
gible benefits for many applications of scientific
language processing.
Acknowledgements
This work was supported by EPSRC grant
EP/G051070/1, the Royal Society (AK) and a
Dorothy Hodgkin Postgraduate Award (LS).
References
Biber, Douglas and Bethany Gray. 2010. Challeng-
ing stereotypes about academic writing: Complex-
ity, elaboration, explicitness. Journal of English for
Academic Purposes, 9(1):2?20.
Biber, Douglas. 1988. Variation Across Speech and
Writing. Cambridge University Press, Cambridge.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Cohen, K. Bretonnel, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9):e3158.
Collins, Michael John. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of ACL-96, Santa Cruz, CA.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Curran, James, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&C
and Boxer. In Proceedings of the ACL-07 Demo and
Poster Sessions, Prague, Czech Republic.
Daume? III, Hal and Daniel Marcu. 2006. Domain
adaptation for statistical classifiers. Journal of Ar-
tificial Intelligence Research, 26:101?126.
Friedman, Carol, Pauline Kraa, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35(4):222?235.
Graff, David, Junbo Kong, Ke Chen, and Kazuaki
Maeda, 2005. English Gigaword Corpus, 2nd Edi-
tion. Linguistic Data Consortium.
696
Hara, Tadayoshi, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Pro-
ceedings of IJCNLP-05, Jeju Island, South Korea.
Jin, Yang, Ryan T. McDonald, Kevin Lerman, Mark A.
Mandel, Steven Carroll, Mark Y. Liberman, Fer-
nando C. Pereira, Raymond S. Winters, and Peter S.
White. 2006. Automated recognition of malignancy
mentions in biomedical literature. BMC Bioinfor-
matics, 7:492.
Kim, J.-D., T. Ohta, Y. Tateisi, and J. Tsujii. 2003.
GENIA corpus - a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(Suppl. 1):i180?
i182.
Korhonen, Anna, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classifica-
tion of verbs in biomedical texts. In Proceedings
of COLING-08, Manchester, UK.
Kulick, Seth, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In Proceedings of the HLT-NAACL-04
Workshop on Linking Biological Literature, Ontolo-
gies and Databases, Boston, MA.
Lappin, Shalom and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?561.
Nguyen, Ngan L.T. and Jin-Dong Kim. 2008. Explor-
ing domain differences for the design of a pronoun
resolution system for biomedical text. In Proceed-
ings of COLING-08, Manchester, UK.
NIH. 2009a. Journal publishing tag set.
http://dtd.nlm.nih.gov/publishing/.
NIH. 2009b. National library of
medicine: Journal subject terms.
http://wwwcf.nlm.nih.gov/serials/journals/index.cfm.
Preiss, Judita, E.J. Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of ACL-07, Prague, Czech
Republic.
Rayson, Paul and Roger Garside. 2000. Comparing
corpora using frequency profiling. In Proceedings
of the ACL-00 Workshop on Comparing Corpora,
Hong Kong.
Rimell, Laura and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Roland, Douglas and Daniel Jurafsky. 1998. How
verb subcategorization frequencies are affected by
corpus choice. In Proceedings of COLING-ACL-98,
Montreal, Canada.
Rosario, Barbara and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via
a domain-specific lexical hierarchy. In Proceedings
of EMNLP-01, Pittsburgh, PA.
Schuman, Jonathan and Sabine Bergler. 2006. Post-
nominal prepositional phrase attachment in pro-
teomics. In Proceedings of the HLT-NAACL-06
BioNLP Workshop on Linking Natural Language
and Biology, New York, NY.
Verspoor, Karin, K Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of tradi-
tional and Open Access scientific journals are simi-
lar. BMC Bioinformatics, 10:183.
697
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1002?1010,
Beijing, August 2010
Metaphor Identification Using Verb and Noun Clustering
Ekaterina Shutova, Lin Sun and Anna Korhonen
Computer Laboratory, University of Cambridge
es407,ls418,alk23@cam.ac.uk
Abstract
We present a novel approach to auto-
matic metaphor identification in unre-
stricted text. Starting from a small seed set
of manually annotated metaphorical ex-
pressions, the system is capable of har-
vesting a large number of metaphors of
similar syntactic structure from a corpus.
Our method is distinguished from previ-
ous work in that it does not employ any
hand-crafted knowledge, other than the
initial seed set, but, in contrast, captures
metaphoricity by means of verb and noun
clustering. Being the first to employ un-
supervised methods for metaphor identifi-
cation, our system operates with the pre-
cision of 0.79.
1 Introduction
Besides enriching our thought and communica-
tion with novel imagery, the phenomenon of
metaphor also plays a crucial structural role in our
use of language. Metaphors arise when one con-
cept is viewed in terms of the properties of the
other. Below are some examples of metaphor.
(1) How can I kill a process? (Martin, 1988)
(2) Inflation has eaten up all my savings. (Lakoff
and Johnson, 1980)
(3) He shot down all of my arguments. (Lakoff
and Johnson, 1980)
(4) And then my heart with pleasure fills,
And dances with the daffodils.1
In metaphorical expressions seemingly unrelated
features of one concept are associated with an-
other concept. In the computer science metaphor
1?I wandered lonely as a cloud?, William Wordsworth,
1804.
in (1) the computational process is viewed as
something alive and, therefore, its forced termi-
nation is associated with the act of killing. Lakoff
and Johnson (1980) explain metaphor as a system-
atic association, or a mapping, between two con-
cepts or conceptual domains: the source and the
target. The metaphor in (3) exemplifies a map-
ping of a concept of argument to that of war. The
argument, which is the target concept, is viewed
in terms of a battle (or a war), the source concept.
The existence of such a link allows us to talk about
arguments using the war terminology, thus giving
rise to a number of metaphors.
Characteristic to all areas of human activity
(from poetic to ordinary to scientific) and, thus,
to all types of discourse, metaphor becomes an
important problem for natural language process-
ing (NLP). In order to estimate the frequency of
the phenomenon, Shutova and Teufel (2010) con-
ducted a corpus study on a subset of the British
National Corpus (BNC) (Burnard, 2007) repre-
senting various genres. They manually anno-
tated metaphorical expressions in this data and
found that 241 out of 761 sentences contained a
metaphor, whereby in 164 phrases metaphoricity
was introduced by a verb. Due to such a high fre-
quency of their use, a system capable of recog-
nizing and interpreting metaphorical expressions
in unrestricted text would become an invaluable
component of any semantics-oriented NLP appli-
cation.
Automatic processing of metaphor can be
clearly divided into two subtasks: metaphor
identification (distinguishing between literal and
metaphorical language in text) and metaphor
interpretation (identifying the intended literal
meaning of a metaphorical expression). Both of
them have been repeatedly attempted in NLP.
To date the most influential account of
metaphor identification is that of Wilks (1978).
1002
According to Wilks, metaphors represent a viola-
tion of selectional restrictions in a given context.
Consider the following example.
(5) My car drinks gasoline. (Wilks, 1978)
The verb drink normally takes an animate subject
and a liquid object. Therefore, drink taking a car
as a subject is an anomaly, which may as well in-
dicate metaphorical use of drink.
This approach was automated by Fass (1991)
in his met* system. However, Fass himself in-
dicated a problem with the method: it detects
any kind of non-literalness or anomaly in lan-
guage (metaphors, metonymies and others), i.e.,
it overgenerates with respect to metaphor. The
techniques met* uses to differentiate between
those are mainly based on hand-coded knowledge,
which implies a number of limitations. In a sim-
ilar manner manually created knowledge in the
form of WordNet (Fellbaum, 1998) is employed
by the system of Krishnakumaran and Zhu (2007),
which essentially differentiates between highly
lexicalized metaphors included in WordNet, and
novel metaphorical senses.
Alternative approaches (Gedigan et al, 2006)
search for metaphors of a specific domain defined
a priori (e.g. MOTION metaphors) in a specific
type of discourse (e.g. Wall Street Journal). In
contrast, the scope of our experiments is the whole
of the British National Corpus (BNC) (Burnard,
2007) and the domain of the expressions we iden-
tify is unrestricted. However, our technique is also
distinguished from the systems of Fass (1991) and
Krishnakumaran and Zhu (2007) in that it does
not rely on any hand-crafted knowledge, but rather
captures metaphoricity in an unsupervised way by
means of verb and noun clustering.
The motivation behind the use of clustering
methods for metaphor identification task lies in
the nature of metaphorical reasoning based on as-
sociation. Compare, for example, the target con-
cepts of marriage and political regime. Having
quite distinct meanings, both of them are cogni-
tively mapped to the source domain of mecha-
nism, which shows itself in the following exam-
ples:
(6) Our relationship is not really working.
(7) Diana and Charles did not succeed in mend-
ing their marriage.
(8) The wheels of Stalin?s regime were well oiled
and already turning.
We expect that such relatedness of distinct tar-
get concepts should manifest itself in the exam-
ples of language use, i.e. target concepts that are
associated with the same source concept should
appear in similar lexico-syntactic environments.
Thus, clustering concepts using grammatical rela-
tions (GRs) and lexical features would allow us to
capture their relatedness by association and har-
vest a large number of metaphorical expressions
beyond our seed set. For example, the sentence
in (6) being part of the seed set should enable the
system to identify metaphors in both (7) and (8).
In summary, our system (1) starts from a seed
set of metaphorical expressions exemplifying a
range of source?target domain mappings; (2) per-
forms unsupervised noun clustering in order to
harvest various target concepts associated with the
same source domain; (3) by means of unsuper-
vised verb clustering creates a source domain verb
lexicon; (4) searches the BNC for metaphorical
expressions describing the target domain concepts
using the verbs from the source domain lexicon.
We tested our system starting with a collection
of metaphorical expressions representing verb-
subject and verb-object constructions, where the
verb is used metaphorically. We evaluated the pre-
cision of metaphor identification with the help of
human judges. In addition to this we compared
our system to a baseline built upon WordNet,
whereby we demonstrated that our method goes
far beyond synonymy and captures metaphors not
directly related to any of those seen in the seed set.
2 Experimental Data
2.1 Seed Phrases
We used the dataset of Shutova (2010) as a seed
set. Shutova (2010) annotated metaphorical ex-
pressions in a subset of the BNC sampling vari-
ous genres: literature, newspaper/journal articles,
essays on politics, international relations and his-
tory, radio broadcast (transcribed speech). The
dataset consists of 62 phrases that are single-word
1003
metaphors representing verb-subject and verb-
object relations, where a verb is used metaphor-
ically. The seed phrases include e.g. stir ex-
citement, reflect enthusiasm, accelerate change,
grasp theory, cast doubt, suppress memory, throw
remark (verb - direct object constructions) and
campaign surged, factor shaped [..], tension
mounted, ideology embraces, changes operated,
approach focuses, example illustrates (subject -
verb constructions).
2.2 Corpus
The search space for metaphor identification was
the British National Corpus (BNC) that was
parsed using the RASP parser of Briscoe et al
(2006). We used the grammatical relations out-
put of RASP for BNC created by Andersen et al
(2008). The system searched the corpus for the
source and target domain vocabulary within a par-
ticular grammatical relation (verb-object or verb-
subject).
3 Method
Starting from a small seed set of metaphorical ex-
pressions, the system implicitly captures the as-
sociations that underly their production and com-
prehension. It generalizes over these associations
by means of unsupervised verb and noun clus-
tering. The obtained clusters then represent po-
tential source and target concepts between which
metaphorical associations hold. The knowledge
of such associations is then used to annotate
metaphoricity in a large corpus.
3.1 Clustering Motivation
Abstract concepts that are associated with the
same source domain are often related to each
other on an intuitive and rather structural level,
but their meanings, however, are not necessarily
synonymous or even semantically close. The re-
sults of previous research on corpus-based lexi-
cal semantics suggest that the linguistic environ-
ment in which a lexical item occurs can shed light
on its meaning. A number of works have shown
that it is possible to automatically induce seman-
tic word classes from corpus data via clustering of
contextual cues (Pereira et al, 1993; Lin, 1998;
Schulte im Walde, 2006). The consensus is that
the lexical items exposing similar behavior in a
large body of text most likely have the same mean-
ing. However, the concepts of marriage and po-
litical regime, that are also observed in similar
lexico-syntactic environments, albeit having quite
distinct meanings are likewise assigned by such
methods to the same cluster. In contrast to con-
crete concepts, such as tea, water, coffee, beer,
drink, liquid, that are clustered together due to
meaning similarity, abstract concepts tend to be
clustered together by association with the same
source domain. It is the presence of this associ-
ation that explains the fact that they share com-
mon contexts. We exploit this idea for identifi-
cation of new target domains associated with the
same source domain. We then use unsupervised
verb clustering to collect source domain vocab-
ulary, which in turn allows us to harvest a large
number of new metaphorical expressions.
3.2 Verb and Noun Clustering
Since Levin (1993) published her classification,
there have been a number of attempts to automati-
cally classify verbs into semantic classes using su-
pervised and unsupervised approaches (Lin, 1998;
Brew and Schulte im Walde, 2002; Korhonen et
al., 2003; Schulte im Walde, 2006; Joanis et al,
2008; Sun and Korhonen, 2009). Similar methods
were also applied to acquisition of noun classes
from corpus data (Rooth et al, 1999; Pantel and
Lin, 2002; Bergsma et al, 2008).
We adopt a recent verb clustering approach of
Sun and Korhonen (2009), who used rich syntac-
tic and semantic features extracted using a shallow
parser and a clustering method suitable for the re-
sulting high dimensional feature space. When Sun
and Korhonen evaluated their approach on 204
verbs from 17 Levin classes, they obtained 80.4
F-measure (which is high in particular for an un-
supervised approach). We apply this approach to a
much larger set of 1610 verbs: all the verb forms
appearing in VerbNet (Kipper et al, 2006) with
the exception of highly infrequent ones. In addi-
tion, we adapt the approach to noun clustering.
3.2.1 Feature Extraction
Our verb dataset is a subset of VerbNet com-
piled as follows. For all the verbs in VerbNet we
1004
extracted their occurrences (up to 10,000) from
the raw corpus data collected originally by Korho-
nen et al (2006) for construction of VALEX lexi-
con. Only the verbs found in this data more than
150 times were included in the experiment.
For verb clustering, we adopted the best per-
forming features of Sun and Korhonen (2009):
automatically acquired verb subcategorization
frames (SCFs) parameterized by their selectional
preferences (SPs). We obtained these features us-
ing the SCF acquisition system of Preiss et al
(2007). The system tags and parses corpus data
using the RASP parser and extracts SCFs from the
resulting GRs using a rule-based classifier which
identifies 168 SCF types for English verbs. It pro-
duces a lexical entry for each verb and SCF com-
bination occurring in corpus data. We obtained
SPs by clustering argument heads appearing in the
subject and object slots of verbs in the resulting
lexicon.
Our noun dataset consists of 2000 most fre-
quent nouns in the BNC. Following previous
works on semantic noun classification (Pantel and
Lin, 2002; Bergsma et al, 2008), we used GRs as
features for noun clustering. We employed all the
argument heads and verb lemmas appearing in the
subject, direct object and indirect object relations
in the RASP-parsed BNC.
The feature vectors were first constructed from
the corpus counts, and subsequently normalized
by the sum of the feature values before applying
clustering.
3.2.2 Clustering Algorithm
We use spectral clustering (SPEC) for both
verbs and nouns. This technique has proved to be
effective in previous verb clustering works (Brew
and Schulte im Walde, 2002; Sun and Korhonen,
2009) and in related NLP tasks involving high di-
mensional data (Chen et al, 2006). We use the
MNCut algorithm for SPEC which has a wide ap-
plicability and a clear probabilistic interpretation
(Meila and Shi, 2001).
The task is to group a given set of words W =
{wn}Nn=1 into a disjoint partition of K classes.
SPEC takes a similarity matrix as input. We
construct it using the Jensen-Shannon divergence
(JSD) as a measure. The JSD between two feature
vectors w and w? is djsd(w, w?) = 12D(w||m) +1
2D(w?||m) where D is the Kullback-Leibler di-vergence, and m is the average of the w and w?.
The similarity matrix S is constructed where
Sij = exp(?djsd(w, w?)). In SPEC, the simi-
larities Sij are viewed as weights on the edges
ij of a graph G over W . The similarity matrix
S is thus the adjacency matrix for G. The de-
gree of a vertex i is di = ?Nj=1 Sij . A cut be-
tween two partitions A and A? is defined to be
Cut(A, A?) =?m?A,n?A? Smn.
The similarity matrix S is then transformed into
a stochastic matrix P .
P = D?1S (1)
The degree matrix D is a diagonal matrix where
Dii = di.
It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constants2 with respect to a partition I? and their
eigenvalues are not zero, then I? minimizes the
multiway normalized cut (MNCut):
MNCut(I) = K ??Kk=1 Cut(Ik,Ik)Cut(Ik,I)
Pmn can be interpreted as the transition probabil-
ity between the vertexes m, n. The criterion can
thus be expressed as MNCut(I) = ?Kk=1(1 ?
P (Ik ? Ik|Ik)) (Meila, 2001), which is the sum
of transition probabilities across different clusters.
This criterion finds the partition where random
walks are most likely to happen within the same
cluster. In practice, the leading eigenvectors of
P are not piecewise constants. However, we can
extract the partition by finding the approximately
equal elements in the eigenvectors using a cluster-
ing algorithm, such as K-Means.
Since SPEC has elements of randomness, we ran
the algorithm multiple times and the partition that
minimizes the distortion (the distances to cluster
centroid) is reported. Some of the clusters ob-
tained as a result of applying the algorithm to our
noun and verb datasets are demonstrated in Fig-
ures 1 and 2 respectively. The noun clusters rep-
resent target concepts that we expect to be asso-
ciated with the same source concept (some sug-
gested source concepts are given in Figure 1, al-
though the system only captures those implicitly).
2An eigenvector v is piecewise constant with respect to I
if v(i) = v(j)?i, j ? Ik and k ? 1, 2...K
1005
Source: MECHANISM
Target Cluster: consensus relation tradition partnership
resistance foundation alliance friendship contact reserve
unity link peace bond myth identity hierarchy relation-
ship connection balance marriage democracy defense
faith empire distinction coalition regime division
Source: STORY; JOURNEY
Target Cluster: politics practice trading reading occupa-
tion profession sport pursuit affair career thinking life
Source: LOCATION; CONTAINER
Target Cluster: lifetime quarter period century succes-
sion stage generation decade phase interval future
Source: LIVING BEING; END
Target Cluster: defeat fall death tragedy loss collapse de-
cline disaster destruction fate
Figure 1: Clustered target concepts
Source Cluster: sparkle glow widen flash flare gleam
darken narrow flicker shine blaze bulge
Source Cluster: gulp drain stir empty pour sip spill swal-
low drink pollute seep flow drip purify ooze pump bub-
ble splash ripple simmer boil tread
Source Cluster: polish clean scrape scrub soak
Source Cluster: kick hurl push fling throw pull drag haul
Source Cluster: rise fall shrink drop double fluctuate
dwindle decline plunge decrease soar tumble surge spiral
boom
Figure 2: Clustered verbs (source domains)
The verb clusters contain coherent lists of source
domain vocabulary.
3.3 Selectional Preference Strength Filter
Following Wilks (1978), we take metaphor to rep-
resent a violation of selectional restrictions. How-
ever, not all verbs have an equally strong capacity
to constrain their arguments, e.g. remember, ac-
cept, choose etc. are weak in that respect. We
suggest that for this reason not all the verbs would
be equally prone to metaphoricity, but only the
ones exhibiting strong selectional preferences. We
test this hypothesis experimentally and expect that
placing this criterion would enable us to filter out
a number of candidate expressions, that are less
likely to be used metaphorically.
We automatically acquired selectional pref-
erence distributions for Verb-Subject and
Verb-Object relations from the BNC parsed
by RASP. We first clustered 2000 most frequent
nouns in the BNC into 200 clusters using SPEC
as described in the previous section. The ob-
tained clusters formed our selectional preference
classes. We adopted the selectional preference
measure proposed by Resnik (1993) and success-
fully applied to a number of tasks in NLP includ-
ing word sense disambiguation (Resnik, 1997).
Resnik models selectional preference of a verb in
probabilistic terms as the difference between the
posterior distribution of noun classes in a partic-
ular relation with the verb and their prior distri-
bution in that syntactic position regardless of the
identity of the predicate. He quantifies this dif-
ference using the relative entropy (or Kullback-
Leibler distance), defining the selectional prefer-
ence strength (SPS) as follows.
SR(v) = D(P (c|v)||P (c)) =
?
c
P (c|v) log P (c|v)P (c) ,
(2)
where P (c) is the prior probability of the noun
class, P (c|v) is the posterior probability of the
noun class given the verb and R is the gram-
matical relation in question. SPS measures how
strongly the predicate constrains its arguments.
We use this measure to filter out the verbs with
weak selectional preferences. The optimal SPS
threshold was set experimentally on a small held-
out dataset and approximates to 1.32. We ex-
cluded expressions containing the verbs with pref-
erence strength below this threshold from the set
of candidate metaphors.
4 Evaluation and Discussion
In order to prove that our metaphor identification
method generalizes well over the seed set and goes
far beyond synonymy, we compared its output to
that of a baseline taking WordNet synsets to repre-
sent source and target domains. We evaluated the
quality of metaphor tagging in terms of precision
with the help of human judges.
4.1 Comparison against WordNet Baseline
The baseline system was implemented using syn-
onymy information from WordNet to expand on
the seed set. Assuming all the synonyms of the
verbs and nouns in seed expressions to represent
the source and target vocabularies respectively,
the system searches for phrases composed of lex-
ical items belonging to those vocabularies. For
example, given a seed expression stir excitement,
the baseline finds phrases such as arouse fervour,
1006
stimulate agitation, stir turmoil etc. However, it is
not able to generalize over the concepts to broad
semantic classes, e.g. it does not find other feel-
ings such as rage, fear, anger, pleasure etc., which
is necessary to fully characterize the target do-
main. The same deficiency of the baseline system
manifests itself in the source domain vocabulary:
the system has only the knowledge of direct syn-
onyms of stir, as opposed to other verbs charac-
teristic to the domain of liquids, e.g. pour, flow,
boil etc., successfully identified by means of clus-
tering.
To compare the coverage achieved by unsuper-
vised clustering to that of the baseline in quanti-
tative terms, we estimated the number of Word-
Net synsets, i.d. different word senses, in the
metaphorical expressions captured by the two sys-
tems. We found that the baseline system covers
only 13% of the data identified using clustering
and does not go beyond the concepts present in
the seed set. In contrast, most metaphors tagged
by our method are novel and represent a con-
siderably wider range of meanings, e.g. given
the seed metaphors stir excitement, throw remark,
cast doubt the system identifies previously unseen
expressions swallow anger, hurl comment, spark
enthusiasm etc. as metaphorical.
4.2 Comparison with Human Judgements
In order to access the quality of metaphor identifi-
cation by both systems we used the help of human
annotators. The annotators were presented with
a set of randomly sampled sentences containing
metaphorical expressions as annotated by the sys-
tem and by the baseline. They were asked to mark
the tagged expressions that were metaphorical in
their judgement as correct.
The annotators were encouraged to rely on their
own intuition of metaphor. However, we also pro-
vided some guidance in the form of the following
definition of metaphor3:
1. For each verb establish its meaning in con-
text and try to imagine a more basic meaning
of this verb on other contexts. Basic mean-
ings normally are: (1) more concrete; (2) re-
3taken from the annotation procedure of Shutova and
Teufel (2010) that is in turn partly based on the work of Prag-
glejaz Group (2007).
CKM 391 Time and time again he would stare at the
ground, hand on hip, if he thought he had received a bad
call, and then swallow his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when
he found the comms system down, but Tammuz was
nearly hysterical by this stage.
AMA 349 We will halt the reduction in NHS services
for long-term care and community health services which
support elderly and disabled patients at home.
ADK 634 Catch their interest and spark their enthu-
siasm so that they begin to see the product?s potential.
K2W 1771 The committee heard today that gangs regu-
larly hurled abusive comments at local people, making
an unacceptable level of noise and leaving litter behind
them.
Figure 3: Sentences tagged by the system
(metaphors in bold)
lated to bodily action; (3) more precise (as
opposed to vague); (4) historically older.
2. If you can establish the basic meaning that
is distinct from the meaning of the verb in
this context, the verb is likely to be used
metaphorically.
We had 5 volunteer annotators who were all na-
tive speakers of English and had no or sparse lin-
guistic knowledge. Their agreement on the task
was 0.63 in terms of ? (Siegel and Castellan,
1988), whereby the main source of disagreement
was the presence of highly lexicalized metaphors,
e.g. verbs such as adopt, convey, decline etc.
We then evaluated the system performance against
their judgements in terms of precision. Precision
measures the proportion of metaphorical expres-
sions that were tagged correctly among the ones
that were tagged. We considered the expressions
tagged as metaphorical by at least three annota-
tors to be correct. As a result our system identi-
fies metaphor with the precision of 0.79, whereas
the baseline only attains 0.44. Some examples of
sentences annotated by the system are shown in
Figure 3.
Such a striking discrepancy between the per-
formance levels of the clustering approach and
the baseline can be explained by the fact that a
large number of metaphorical senses are included
in WordNet. This means that in WordNet synsets
source domain verbs are mixed with more abstract
terms. For example, the metaphorical sense of
shape in shape opinion is part of the synset (de-
1007
termine, shape, mold, influence, regulate). This
results in the baseline system tagging literal ex-
pressions as metaphorical, erroneously assuming
that the verbs from the synset belong to the source
domain.
The main source of confusion in the output of
our clustering method was the conventionality of
some metaphorical expressions, e.g. hold views,
adopt traditions, tackle a problem. The system
is capable of tracing metaphorical etymology of
conventional phrases, but their senses are highly
lexicalized. This lexicalization is reflected in the
data and affects clustering in that conventional
metaphors are sometimes clustered together with
literally used terms, e.g. tackle a problem and re-
solve a problem, which may suggest that the lat-
ter are metaphorical. It should be noted, however,
that such errors are rare.
Since there is no large metaphor-annotated cor-
pus available, it was impossible for us to reli-
ably evaluate the recall of the system. How-
ever, the system identified a total number of 4456
metaphorical expressions in the BNC starting with
a seed set of only 62, which is a promising result.
5 Related Work
One of the first attempts to identify and inter-
pret metaphorical expressions in text automati-
cally is the approach of Fass (1991). Fass devel-
oped a system called met*, capable of discrimi-
nating between literalness, metonymy, metaphor
and anomaly. It does this in three stages. First,
literalness is distinguished from non-literalness
using selectional preference violation as an in-
dicator. In the case that non-literalness is de-
tected, the respective phrase is tested for be-
ing a metonymic relation using hand-coded pat-
terns (such as CONTAINER-for-CONTENT). If
the system fails to recognize metonymy, it pro-
ceeds to search the knowledge base for a rele-
vant analogy in order to discriminate metaphor-
ical relations from anomalous ones. E.g., the
sentence in (5) would be represented in this
framework as (car,drink,gasoline), which does
not satisfy the preference (animal,drink,liquid),
as car is not a hyponym of animal. met*
then searches its knowledge base for a triple
containing a hypernym of both the actual ar-
gument and the desired argument and finds
(thing,use,energy source), which represents the
metaphorical interpretation.
Birke and Sarkar (2006) present a sen-
tence clustering approach for non-literal lan-
guage recognition implemented in the TroFi sys-
tem (Trope Finder). This idea originates from
a similarity-based word sense disambiguation
method developed by Karov and Edelman (1998).
The method employs a set of seed sentences,
where the senses are annotated, computes simi-
larity between the sentence containing the word
to be disambiguated and all of the seed sentences
and selects the sense corresponding to the anno-
tation in the most similar seed sentences. Birke
and Sarkar (2006) adapt this algorithm to perform
a two-way classification: literal vs. non-literal,
and they do not clearly define the kinds of tropes
they aim to discover. They attain a performance
of 53.8% in terms of f-score.
The method of Gedigan et al (2006) discrimi-
nates between literal and metaphorical use. They
trained a maximum entropy classifier for this pur-
pose. They obtained their data by extracting the
lexical items whose frames are related to MO-
TION and CURE from FrameNet (Fillmore et al,
2003). Then they searched the PropBank Wall
Street Journal corpus (Kingsbury and Palmer,
2002) for sentences containing such lexical items
and annotated them with respect to metaphoric-
ity. They used PropBank annotation (arguments
and their semantic types) as features to train the
classifier and report an accuracy of 95.12%. This
result is, however, only a little higher than the per-
formance of the naive baseline assigning majority
class to all instances (92.90%). These numbers
can be explained by the fact that 92.00% of the
verbs of MOTION and CURE in the Wall Street
Journal corpus are used metaphorically, thus mak-
ing the dataset unbalanced with respect to the tar-
get categories and the task notably easier.
Both Birke and Sarkar (2006) and Gedigan et
al. (2006) focus only on metaphors expressed by
a verb. As opposed to that the approach of Kr-
ishnakumaran and Zhu (2007) deals with verbs,
nouns and adjectives as parts of speech. They
use hyponymy relation in WordNet and word bi-
gram counts to predict metaphors at the sentence
1008
level. Given an IS-A metaphor (e.g. The world is
a stage4) they verify if the two nouns involved are
in hyponymy relation in WordNet, and if this is
not the case then this sentence is tagged as con-
taining a metaphor. Along with this they con-
sider expressions containing a verb or an adjec-
tive used metaphorically (e.g. He planted good
ideas in their minds or He has a fertile imagi-
nation). Hereby they calculate bigram probabil-
ities of verb-noun and adjective-noun pairs (in-
cluding the hyponyms/hypernyms of the noun in
question). If the combination is not observed in
the data with sufficient frequency, the system tags
the sentence containing it as metaphorical. This
idea is a modification of the selectional prefer-
ence view of Wilks. However, by using bigram
counts over verb-noun pairs as opposed to verb-
object relations extracted from parsed text Kr-
ishnakumaran and Zhu (2007) loose a great deal
of information. The authors evaluated their sys-
tem on a set of example sentences compiled from
the Master Metaphor List (Lakoff et al, 1991),
whereby highly conventionalized metaphors (they
call them dead metaphors) are taken to be neg-
ative examples. Thus, they do not deal with lit-
eral examples as such: essentially, the distinc-
tion they are making is between the senses in-
cluded in WordNet, even if they are conventional
metaphors, and those not included in WordNet.
6 Conclusions and Future Directions
We presented a novel approach to metaphor iden-
tification in unrestricted text using unsupervised
methods. Starting from a limited set of metaphor-
ical seeds, the system is capable of capturing the
regularities behind their production and annotat-
ing a much greater number and wider range of
previously unseen metaphors in the BNC.
Our system is the first of its kind and it is capa-
ble of identifying metaphorical expressions with a
high precision (0.79). By comparing its coverage
to that of a WordNet baseline, we proved that our
method goes far beyond synonymy and general-
izes well over the source and target domains. Al-
though at this stage we tested our system on verb-
subject and verb-object metaphors only, we are
4William Shakespeare
convinced that the described identification tech-
niques can be similarly applied to a wider range
of syntactic constructions. Extending the system
to deal with more parts of speech and types of
phrases is part of our future work.
One possible limitation of our approach is that
it is seed-dependent, which makes the recall of the
system questionable. Thus, another important fu-
ture research avenue is the creation of a more di-
verse seed set. We expect that a set of expres-
sions representative of the whole variety of com-
mon metaphorical mappings, already described in
linguistics literature, would enable the system to
attain a very broad coverage of the corpus. Mas-
ter Metaphor List (Lakoff et al, 1991) and other
existing metaphor resources could be a sensible
starting point on a route to such a dataset.
Acknowledgments
We are very grateful to our anonymous reviewers
for their useful feedback on this work and the vol-
unteer annotators for their interest, time and help.
This research is funded by generosity of Cam-
bridge Overseas Trust (Katia Shutova), Dorothy
Hodgkin Postgraduate Award (Lin Sun) and the
Royal Society, UK (Anna Korhonen).
References
Andersen, O. E., J. Nioche, E. Briscoe, and J. Carroll.
2008. The BNC parsed with RASP4UIMA. In Pro-
ceedings of LREC 2008, Marrakech, Morocco.
Bergsma, S., D. Lin, and R. Goebel. 2008. Discrimi-
native learning of selectional preference from unla-
beled text. In Proceedings of the EMNLP.
Birke, J. and A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonlit-
eral language. In In Proceedings of EACL-06, pages
329?336.
Brew, C. and S. Schulte im Walde. 2002. Spectral
clustering for German verbs. In Proceedings of
EMNLP.
Briscoe, E., J. Carroll, and R. Watson. 2006. The sec-
ond release of the rasp system. In Proceedings of
the COLING/ACL on Interactive presentation ses-
sions, pages 77?80.
Burnard, L. 2007. Reference Guide for the British
National Corpus (XML Edition).
1009
Chen, J., D. Ji, C. Lim Tan, and Z. Niu. 2006. Un- Lin, D. 1998. Automatic retrieval and clustering of
supervised relation disambiguation using spectral similar words. In Proceedings of the 17th inter-
clustering. In Proceedings of COLING/ACL. national conference on Computational linguistics,
pages 768?774.Fass, D. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa- Martin, J. H. 1988. Representing regularities in the
tional Linguistics, 17(1):49?90. metaphoric lexicon. In Proceedings of the 12th con-
ference on Computational linguistics, pages 396?Fellbaum, C., editor. 1998. WordNet: An Electronic 401.Lexical Database (ISBN: 0-262-06197-X). MIT
Press, first edition. Meila, M. and J. Shi. 2001. A random walks view of
spectral segmentation. In AISTATS.Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International Meila, M. 2001. The multicut lemma. Technical re-
Journal of Lexicography, 16(3):235?250. port, University of Washington.
Gedigan, M., J. Bryant, S. Narayanan, and B. Ciric. Pantel, P. and D. Lin. 2002. Discovering word
2006. Catching metaphors. In In Proceedings of the senses from text. In Proceedings of the eighth ACM
3rd Workshop on Scalable Natural Language Un- SIGKDD international conference on Knowledge
derstanding, pages 41?48, New York. discovery and data mining, pages 613?619. ACM.
Joanis, E., S. Stevenson, and D. James. 2008. A gen- Pereira, F., N. Tishby, and L. Lee. 1993. Distribu-
eral feature space for automatic verb classification. tional clustering of English words. In Proceedings
Natural Language Engineering, 14(3):337?367. of ACL-93, pages 183?190, Morristown, NJ, USA.
Karov, Y. and S. Edelman. 1998. Similarity-based Pragglejaz Group. 2007. MIP: A method for iden-
word sense disambiguation. Computational Lin- tifying metaphorically used words in discourse.
guistics, 24(1):41?59. Metaphor and Symbol, 22:1?39.
Kingsbury, P. and M. Palmer. 2002. From TreeBank Preiss, J., T. Briscoe, and A. Korhonen. 2007. A sys-
to PropBank. In Proceedings of LREC-2002, Gran tem for large-scale acquisition of verbal, nominal
Canaria, Canary Islands, Spain. and adjectival subcategorization frames from cor-
pora. In Proceedings of ACL-2007, volume 45, pageKipper, K., A. Korhonen, N. Ryant, and M. Palmer. 912.2006. Extensive classifications of English verbs.
In Proceedings of the 12th EURALEX International Resnik, P. 1993. Selection and Information: A Class-
Congress. based Approach to Lexical Relationships. Ph.D.
thesis, Philadelphia, PA, USA.Korhonen, A., Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame dis- Resnik, P. 1997. Selectional preference and sense dis-
tributions semantically. In Proceedings of ACL ambiguation. In ACL SIGLEX Workshop on Tag-
2003, Sapporo,Japan. ging Text with Lexical Semantics, Washington, D.C.
Korhonen, A., Y. Krymolowski, and T. Briscoe. 2006. Rooth, M., S. Riezler, D. Prescher, G. Carroll, and
A large subcategorization lexicon for natural lan- F. Beil. 1999. Inducing a semantically annotated
guage processing applications. In Proceedings of lexicon via EM-based clustering. In Proceedings of
LREC 2006. ACL 99, pages 104?111.
Krishnakumaran, S. and X. Zhu. 2007. Hunting elu- Schulte im Walde, S. 2006. Experiments on the au-
sive metaphors using lexical resources. In Proceed- tomatic induction of German semantic verb classes.
ings of the Workshop on Computational Approaches Computational Linguistics, 32(2):159?194.
to Figurative Language, pages 13?20, Rochester,
NY. Shutova, E. and S. Teufel. 2010. Metaphor corpusannotated for source - target domain mappings. In
Lakoff, G. and M. Johnson. 1980. Metaphors We Live Proceedings of LREC 2010, Malta.
By. University of Chicago Press, Chicago. Shutova, E. 2010. Automatic metaphor interpretation
Lakoff, G., J. Espenson, and A. Schwartz. 1991. The as a paraphrasing task. In Proceedings of NAACL
master metaphor list. Technical report, University 2010, Los Angeles, USA.
of California at Berkeley. Siegel, S. and N. J. Castellan. 1988. Nonparametric
Levin, B. 1993. English Verb Classes and Alterna- statistics for the behavioral sciences. McGraw-Hill
tions. University of Chicago Press, Chicago. Book Company, New York, USA.
1010
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1056?1064,
Beijing, August 2010
Investigating the cross-linguistic potential of VerbNet -style classification
Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
ls418,alk23@cl.cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS
thierry.poibeau@ens.fr
Ce?dric Messiant
LIPN, UMR7030
CNRS & U. Paris 13
cedric.messiant@lipn.fr
Abstract
Verb classes which integrate a wide range
of linguistic properties (Levin, 1993) have
proved useful for natural language pro-
cessing (NLP) applications. However,
the real-world use of these classes has
been limited because for most languages,
no resources similar to VerbNet (Kipper-
Schuler, 2005) are available. We apply
a verb clustering approach developed for
English to French ? a language for which
no such experiment has been conducted
yet. Our investigation shows that not only
the general methodology but also the best
performing features are transferable be-
tween the languages, making it possible
to learn useful VerbNet style classes for
French automatically without language-
specific tuning.
1 Introduction
A number of verb classifications have been built to
support natural language processing (NLP) tasks
(Grishman et al, 1994; Miller, 1995; Baker et al,
1998; Palmer et al, 2005; Kipper-Schuler, 2005;
Hovy et al, 2006). These include both syntactic
and semantic classifications, as well as ones which
integrate aspects of both. Classifications which in-
tegrate a wide range of linguistic properties can
be particularly useful for NLP applications suffer-
ing from data sparseness. One such classification
is VerbNet (Kipper-Schuler, 2005). Building on
the taxonomy of Levin (1993), VerbNet groups
verbs (e.g. deliver, post, dispatch) into classes
(e.g. SEND) on the basis of their shared mean-
ing components and syntactic behaviour, identi-
fied in terms of meaning preserving diathesis al-
ternations. Such classes can be identified across
the entire lexicon, and they may also apply across
languages, since their meaning components are
said to be cross-linguistically applicable (Jack-
endoff, 1990).
Offering a powerful tool for generalization, ab-
straction and prediction, VerbNet classes have
been used to support many important NLP
tasks, including e.g. computational lexicography,
parsing, word sense disambiguation, semantic
role labeling, information extraction, question-
answering, and machine translation (Swier and
Stevenson, 2004; Dang, 2004; Shi and Mihalcea,
2005; Abend et al, 2008). However, to date their
exploitation has been limited because for most
languages, no Levin style classification is avail-
able.
Since manual classification is costly (Kipper
et al, 2008) automatic approaches have been pro-
posed recently which could be used to learn novel
classifications in a cost-effective manner (Joanis
et al, 2008; Li and Brew, 2008; O? Se?aghdha
and Copestake, 2008; Vlachos et al, 2009; Sun
and Korhonen, 2009). However, most work on
Levin type classification has focussed on English.
Large-scale research on other languages such as
German (Schulte im Walde, 2006) and Japanese
(Suzuki and Fukumoto, 2009) has focussed on se-
mantic classification. Although the two classifica-
tion systems have shared properties, studies com-
paring the overlap between VerbNet and WordNet
(Miller, 1995) have reported that the mapping is
only partial and many to many due to fine-grained
nature of classes based on synonymy (Shi and Mi-
halcea, 2005; Abend et al, 2008).
Only few studies have been conducted on Levin
style classification for languages other than En-
glish. In their experiment involving 59 verbs and
three classes, Merlo et al (2002) applied a su-
pervised approach developed for English to Ital-
ian, obtaining high accuracy (86.3%). In an-
other experiment with 60 verbs and three classes,
1056
they showed that features extracted from Chinese
translations of English verbs can improve English
classification. These results are promising, but
those from a later experiment by Ferrer (2004)
are not. Ferrer applied a clustering approach de-
veloped for English to Spanish, and evaluated it
against the manual classification of Va?zquez et al
(2000), constructed using criteria similar (but not
identical) to Levin?s. This experiment involving
514 verbs and 31 classes produced results only
slightly better than the random baseline.
In this paper, we investigate the cross-linguistic
potential of Levin style classification further. In
past years, verb classification techniques ? in par-
ticular unsupervised ones ? have improved con-
siderably, making investigations for a new lan-
guage more feasible. We take a recent verb clus-
tering approach developed for English (Sun and
Korhonen, 2009) and apply it to French ? a ma-
jor language for which no such experiment has
been conducted yet. Basic NLP resources (cor-
pora, taggers, parsers and subcategorization ac-
quisition systems) are now sufficiently developed
for this language for the application of a state-of-
the-art verb clustering approach to be realistic.
Our investigation reveals similarities between
the English and French classifications, support-
ing the linguistic hypothesis (Jackendoff, 1990)
and the earlier result of Merlo et al (2002)
that Levin classes have a strong cross-linguistic
basis. Not only the general methodology but
also best performing features are transferable be-
tween the languages, making it possible to learn
useful classes for French automatically without
language-specific tuning.
2 French Gold Standard
The development of an automatic verb classifi-
cation approach requires at least an initial gold
standard. Some syntactic (Gross, 1975) and se-
mantic (Vossen, 1998) verb classifications exist
for French, along with ones which integrate as-
pects of both (Saint-Dizier, 1998). Since none of
these resources offer classes similar to Levins?,
we followed the idea of Merlo et al (2002) and
translated a number of Levin classes from English
to French. As our aim was to to investigate the
cross-linguistic applicability of classes, we took
an English gold standard which has been used to
evaluate several recent clustering works ? that of
Sun et al (2008). This resource includes 17 fine-
grained Levin classes. Each class has 12 member
verbs whose predominant sense in English (ac-
cording to WordNet) belongs to that class.
Member verbs were first translated to French.
Where several relevant translations were identi-
fied, each of them was considered. For each can-
didate verb, subcategorization frames (SCFs) were
identified and diathesis alternations were consid-
ered using the criteria of Levin (1993): alterna-
tions must result in the same or extended verb
sense. Only verbs sharing diathesis alternations
were kept in the class.
For example, the gold standard class 31.1
AMUSE includes the following English verbs:
stimulate, threaten, shock, confuse, upset, over-
whelm, scare, disappoint, delight, exhaust, in-
timidate and frighten. Relevant French transla-
tions were identified for all of them: abattre,
accabler, briser, de?primer, consterner, ane?antir,
e?puiser, exte?nuer, e?craser, ennuyer, e?reinter, inon-
der. The majority of these verbs take similar SCFs
and diathesis alternations, e.g. Cette affaire e?crase
Marie (de chagrin), Marie est e?crase?e par le cha-
grin, Le chagrin e?crase Marie. However, stim-
uler (stimulate) and menacer (threaten) do not,
and they were therefore removed.
40% of translations were discarded from
classes because they did not share the same aler-
nations. The final version of the gold stan-
dard (shown in table 1) includes 171 verbs in 16
classes. Each class is named according to the
original Levin class. The smallest class (30.3) in-
cludes 7 verbs and the largest (37.3) 16. The aver-
age number of verbs per class is 10.7.
3 Verb Clustering
We performed an experiment where we
? took a French corpus and a SCF lexicon au-
tomatically extracted from that corpus,
? extracted from these resources a range of fea-
tures (lexical, syntactic and semantic) ? a
representative sample of those employed in
recent English experiments,
1057
Class No Class Verbs
9.1 PUT accrocher, de?poser, mettre, placer, re?partir, re?inte?grer, empiler, emporter, enfermer,
inse?rer, installer
10.1 REMOVE o?ter, enlever, retirer, supprimer, retrancher, de?barrasser, soustraire, de?compter, e?liminer
11.1 SEND envoyer, lancer, transmettre, adresser, porter, expe?dier, transporter, jeter, renvoyer, livrer
13.5.1 GET acheter, prendre, saisir, re?server, conserver, garder, pre?server, maintenir, retenir, louer,
affre?ter
18.1 HIT cogner, heurter, battre, frapper, fouetter, taper, rosser, brutaliser, e?reinter, maltraiter,
corriger,
22.2 AMALGAMATE incorporer, associer, re?unir, me?langer, me?ler, unir, assembler, combiner, lier, fusionner
29.2 CHARACTERIZE appre?hender, concevoir, conside?rer, de?crire, de?finir, de?peindre, de?signer, envisager,
identifier, montrer, percevoir, repre?senter, ressentir
30.3 PEER regarder, e?couter, examiner, conside?rer, voir, scruter, de?visager
31.1 AMUSE abattre, accabler, briser, de?primer, consterner, ane?antir, e?puiser, exte?nuer, e?craser, en-
nuyer, e?reinter, inonder,
36.1 CORRESPOND coope?rer, participer, collaborer, concourir, contribuer, prendre part, s?associer, travaille
37.3 MANNER OF
SPEAKING
ra?ler, gronder, crier, ronchonner, grogner, bougonner, maugre?er, rouspe?ter, grommeler,
larmoyer, ge?mir, geindre, hurler, gueuler, brailler, chuchoter
37.7 SAY dire, re?ve?ler, de?clarer, signaler, indiquer, montrer, annoncer, re?pondre, affirmer, certifier,
re?pliquer
43.1 LIGHT EMIS-
SION
briller, e?tinceler, flamboyer, luire, resplendir, pe?tiller, rutiler, rayonner., scintiller
45.4 CHANGE OF
STATE
me?langer, fusionner, consolider, renforcer, fortifier, adoucir, polir, atte?nuer, tempe?rer,
pe?trir, fac?onner, former
47.3 MODES OF BE-
ING
trembler, fre?mir, osciller, vaciller, vibrer, tressaillir, frissonner, palpiter, gre?siller, trem-
bloter, palpiter
51.3.2 RUN voyager, aller, se promener, errer, circuler, se de?placer, courir, bouger, naviguer, passer
Table 1: A Levin style gold standard for French
? clustered the features using a method which
has proved promising in both English and
German experiments: spectral clustering,
? evaluated the clusters both quantitatively (us-
ing the gold standard) and qualitatively,
? and compared the performance to that re-
cently obtained for English in order to gain
a better understanding of the cross-linguistic
and language-specific properties of verb clas-
sification
This work is described in the subsequent sections.
3.1 Data: the LexSchem Lexicon
We extracted the features for clustering from
LexSchem (Messiant et al, 2008). This large sub-
categorization lexicon provides SCF frequency in-
formation for 3,297 French verbs. It was acquired
fully automatically from Le Monde newspaper
corpus (200M words from years 1991-2000) us-
ing ASSCI ? a recent subcategorization acquisi-
tion system for French (Messiant, 2008). Systems
similar to ASSCI have been used in recent verb
classification works e.g. (Schulte im Walde, 2006;
Li and Brew, 2008; Sun and Korhonen, 2009).
Like these other systems, ASSCI takes raw corpus
data as input. The data is first tagged and lemma-
tized using the Tree-Tagger and then parsed us-
ing Syntex (Bourigault et al, 2005). Syntex is
a shallow parser which employs a combination
of statistics and heuristics to identify grammati-
cal relations (GRs) in sentences. ASSCI considers
GRs where the target verbs occur and constructs
SCFs from nominal, prepositional and adjectival
phrases, and infinitival and subordinate clauses.
When a verb has no dependency, its SCF is con-
sidered as intransitive. ASSCI assumes no pre-
defined list of SCFs but almost any combination
of permitted constructions can appear as a candi-
date SCF. The number of automatically generated
SCF types in LexSchem is 336.
Many candidate SCFs are noisy due to process-
ing errors and the difficulty of argument-adjunct
distinction. Most SCF systems assume that true
arguments occur in argument positions more fre-
quently than adjuncts. Many systems also inte-
grate filters for removing noise from system out-
put. When LexSchem was evaluated after filter-
1058
ing its F-measure was 69 ? which is similar to
that of other current SCF systems (Messiant et al,
2008) We used the unfiltered version of the lexi-
con because English experiments have shown that
information about adjuncts can help verb cluster-
ing (Sun et al, 2008).
4 Features
Lexical entries in LexSchem provide a variety of
material for verb clustering. Using this material,
we constructed a range of features for experimen-
tation. The first three include basic information
about SCFs:
F1: SCFs and their relative frequencies with indi-
vidual verbs. SCFs abstract over particles and
prepositions.
F2: F1, with SCFs parameterized for the tense
(the POS tag) of the verb.
F3: F2, with SCFs parameterized for prepositions
(PP).
The following six features include informa-
tion about the lexical context (co-occurrences)
of verbs. We adopt the best method of Li and
Brew (2008) where collocations (COs) are ex-
tracted from the window of words immediately
preceding and following a lemmatized verb. Stop
words are removed prior to extraction.
F4, F6, F8: COs are extracted from the window
of 4, 6 and 8 words, respectively. The relative
word position is ignored.
F5, F7, F9: F4, F6 and F8 with the relative word
position recorded.
The next four features include information
about lexical preferences (LP) of verbs in argu-
ment head positions of specific GRs associated
with the verb:
F10: LP(PREP): the type and frequency of prepo-
sitions in the preposition (PREP) relation.
F11: LP(SUBJ): the type and frequency of nouns
in the subject (SUBJ) relation.
F12: LP(IOBJ): the type and frequency of nouns
in the object (OBJ) and indirect object (IOBJ)
relation.
F13: LP(ALL): the combination of F10-F13.
The final two features refine SCF features with
LPs and semantic information about verb selec-
tional preferences (SP):
F14-F16: F1-F3 parameterized for LPs.
F17: F3 refined with SPs.
We adopt a fully unsupervised approach to SP
acquisition using the method of Sun and Korho-
nen (2009), with the difference that we determine
the optimal number of SP clusters automatically
following Zelnik-Manor and Perona (2004). The
method is introduced in the following section. The
approach involves (i) taking the GRs (SUBJ, OBJ,
IOBJ) associated with verbs, (ii) extracting all the
argument heads in these GRs, and (iii) clustering
the resulting N most frequent argument heads into
M classes. The empirically determined N 200
was used. The method produced 40 SP clusters.
5 Clustering Methods
Spectral clustering (SPEC) has proved promising
in previous verb clustering experiments (Brew
and Schulte im Walde, 2002; Sun and Korho-
nen, 2009) and other similar NLP tasks involv-
ing high dimensional feature space (Chen et al,
2006). Following Sun and Korhonen (2009) we
used the MNCut spectral clustering (Meila and
Shi, 2001) which has a wide applicability and
a clear probabilistic interpretation (von Luxburg,
2007; Verma and Meila, 2005). However, we ex-
tended the method to determine the optimal num-
ber of clusters automatically using the technique
proposed by (Zelnik-Manor and Perona, 2004).
Clustering groups a given set of verbs V =
{vn}Nn=1 into a disjoint partition of K classes.
SPEC takes a similarity matrix as input. All our
features can be viewed as probabilistic distribu-
tions because the combination of different fea-
tures is performed via parameterization. Thus we
use the Jensen-Shannon divergence (JSD) to con-
struct the similarity matrix. The JSD between
1059
two feature vectors v and v? is djsd(v, v?) =
1
2D(v||m)+ 12D(v?||m) where D is the Kullback-Leibler divergence, and m is the average of the v
and v?.
The similarity matrix W is constructed where
Wij = exp(?djsd(v, v?)). In SPEC, the simi-
larities Wij are viewed as the connection weight
ij of a graph G over V . The similarity matrix
W is thus the adjacency matrix for G. The de-
gree of a vertex i is di = ?Nj=1 wij . A cut be-
tween two partitions A and A? is defined to be
Cut(A,A?) =?m?A,n?A? Wmn.
The similarity matrix W is normalized into a
stochastic matrix P .
P = D?1W (1)
The degree matrix D is a diagonal matrix where
Dii = di.
It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constant1 with respect to a partition I? and their
eigenvalues are not zero, then I? minimizes the
multiway normalized cut(MNCut):
MNCut(I) = K ??Kk=1 Cut(Ik,Ik)Cut(Ik,I)
Pmn can be interpreted as the transition proba-
bility between vertices m,n. The criterion can
thus be expressed as MNCut(I) = ?Kk=1(1 ?
P (Ik ? Ik|Ik)) (Meila, 2001), which is the sum
of transition probabilities across different clusters.
This criterion finds the partition where the random
walks are most likely to happen within the same
cluster. In practice, the leading eigenvectors of P
are not piecewise constant. But we can extract the
partition by finding the approximately equal ele-
ments in the eigenvectors using a clustering algo-
rithm like K-Means.
As the value of K is not known beforehand, we
use Zelnik-Manor and Perona (2004)?s method to
estimate it. This method finds the optimal value
by minimizing a cost function based on the eigen-
vector structure of W .
Like Brew and Schulte im Walde (2002), we
compare SPEC against a K-Means baseline. We
used the Matlab implementation with euclidean
distance as the distance measure.
1The eigenvector v is piecewise constant with respect to
I if v(i) = v(j)?i, j ? Ik and k ? 1, 2...K
6 Experimental Evaluation
6.1 Data and Pre-processing
The SCF-based features (F1-F3 and F14-F17)
were extracted directly from LexSchem. The CO
(F4-F9) and LP features (F10-F13) were extracted
from the raw and parsed corpus sentences, respec-
tively, which were used for creating the lexicon.
Features that only appeared once were removed.
Feature vectors were normalized by the sum of the
feature values before clustering. Since our clus-
tering algorithms have an element of randomness,
we repeated clustering multiple times. We report
the results that minimize the distortion (the dis-
tance to cluster centroid).
6.2 Evaluation Measures
We employ the same measures for evaluation as
previously employed e.g. by O? Se?aghdha and
Copestake (2008) and Sun and Korhonen (2009).
The first measure is modified purity (mPUR) ?
a global measure which evaluates the mean preci-
sion of clusters. Each cluster is associated with its
prevalent class. The number of verbs in a cluster
K that take this class is denoted by nprevalent(K).
Verbs that do not take it are considered as errors.
Clusters where nprevalent(K) = 1 are disregarded
as not to introduce a bias towards singletons:
mPUR =
?
nprevalent(ki)>2
nprevalent(ki)
number of verbs
The second measure is weighted class accuracy
(ACC): the proportion of members of dominant
clusters DOM-CLUSTi within all classes ci.
ACC =
?C
i=1 verbs in DOM-CLUSTi
number of verbs
mPUR and ACC can be seen as a measure of pre-
cision(P) and recall(R) respectively. We calculate
F measure as the harmonic mean of P and R:
F = 2 ? mPUR ? ACCmPUR + ACC
The random baseline (BL) is calculated as fol-
lows: BL = 1/number of classes
7 Evaluation
7.1 Quantitative Evaluation
In our first experiment, we evaluated 116 verbs ?
those which appeared in LexSchem the minimum
1060
of 150 times. We did this because English exper-
iments had shown that due to the Zipfian nature
of SCF distributions, 150 corpus occurrences are
typically needed to obtain a sufficient number of
frames for clustering (Sun et al, 2008).
Table 2 shows F-measure results for all the fea-
tures. The 4th column of the table shows, for com-
parison, the results of Sun and Korhonen (2009)
obtained for English when they used the same fea-
tures as us, clustered them using SPEC, and evalu-
ated them against the English version of our gold
standard, also using F-measure2.
As expected, SPEC (the 2nd column) outper-
forms K-Means (the 3rd column). Looking at the
basic SCF features F1-F3, we can see that they per-
form significantly better than the BL method. F3
performs the best among the three features both
in French (50.6 F) and in English (63.3 F). We
therefore use F3 as the SCF feature in F14-F17
(the same was done for English).
In French, most CO features (F4-F9) outper-
form SCF features. The best result is obtained
with F7: 55.1 F. This is clearly better than the
best SCF result 50.6 (F3). This result is interesting
since SCFs correspond better than COs with fea-
tures used in manual Levin classification. Also,
SCFs perform considerably better than COs in the
English experiment (we only have the result for F4
available, but it is considerably lower than the re-
sult for F3). However, earlier English studies have
reported contradictory results (e.g. Li and Brew
(2008) showed that CO performs better than SCF
in supervised verb classification), indicating that
the role of CO features in verb classification re-
quires further investigation.
Looking at the LP features, F13 produces the
best F (52.7) for French which is slightly better
than the best SCF result for the language. Also
in English, F13 performs the best in this feature
group and yields a higher result than the best SCF-
based feature F3.
Parameterizing the best SCF feature F3 with LPs
(F14-16) and SPs (F17) yields better performance
2Note that the results for the two languages are not mu-
tually comparable due to differences in test sets, data sizes,
and feature extraction systems (see Section 8 for discussion).
The results for English are included so that we can compare
the relative performance of individual features in the two lan-
guages in question.
in French. F15 and F17 have the F of 54.5 and
54.6, respectively. These results are so close to
the result of the best CO feature F7 (55.1 ? which
is the highest result in this experiment) that the
differences are not statistically significant. In En-
glish, the results of F14-F17 are similarly good;
however, only F17 beats the already high perfor-
mance of F13.
On the basis of this experiment, it is difficult to
tell whether shallow CO features or more sophisti-
cated SCF-based features are better for French. In
the English experiment sophisticated features per-
formed better (the SCF-SP feature was the best).
However, the English experiment employed a
much larger dataset. These more sophisticated
features may suffer from data sparseness in our
French experiment since although we required the
minimum of 150 occurrences per verb, verb clus-
tering performance tends to improve when more
data is available, and given the fine-grained nature
of LexShem SCFs it is likely that more data is re-
quired for optimal performance.
We therefore performed another experiment
with French on the full set of 147 verbs, using
SPEC, where we investigated the effect of instance
filtering on the performance of the best features
from each feature group: F3, F7, F13 and F17.
The results shown in Table 3 reveal that the perfor-
mance of the features remains fairly similar until
the instance threshold of 1000. When 2000 occur-
rences per verb are used, the differences become
clearer, until at the threshold of 4000, it is obvious
that the most sophisticated SCF-SP feature F17 is
by far the best feature for French (65.4 F) and the
SCF feature F3 the second best (60.5 F). The CO-
feature F7 and the LP feature F13 are not nearly as
good (53.4 and 51.0 F).
Although the results at different thresholds are
not comparable due to the different number of
verbs and classes (see columns 2-3), the results
for features at the same threshold are. Those re-
sults suggest that when 2000 or more occurrences
per verb are used, most features perform like they
performed for English in the experiment of Sun
and Korhonen (2009), with CO being the least in-
formative3 and SCF-SP being the most informa-
3However, it is worth noting that CO is not a useless fea-
ture. As table 3 shows, when 150 or fewer occurrences are
1061
SPEC K Eng.
BL 6.7 6.7 6.7
F1 SCF 42.4 39.3 57.8
F2 SCF(POS) 45.9 40.3 46.7
F3 SCF(PP) 50.6 36.9 63.3
F4 CO(4) 50.3 38.2 40.9
F5 CO(4+loc) 48.8 26.3 -
F6 CO(6) 52.7 29.2 -
F7 CO(6+loc) 55.1 33.8 -
F8 CO(8) 54.2 36.4 -
F9 CO(8+loc) 54.6 37.2 -
F10 LP(PREP) 35.5 32.8 49.0
F11 LP(SUBJ) 33.7 23.6 -
F12 LP(OBJ) 50.1 33.3 -
F13 LP(ALL) 52.7 40.1 74.6
F14 SCF+LP(SUBJ) 50.3 40.1 71.7
F15 SCF+LP(OBJ) 54.5 35.6 74.0
F16 SCF+LP(SUBJ+OBJ) 53.4 36.2 73.0
F17 SCF+SP 54.6 39.8 80.4
Table 2: Results for all the features for French
(SPEC and K-means) and English (SPEC)
THR Verbs Cls F3 F7 F13 F17
0 147 15 43.7 57.5 43.3 50.1
50 137 15 47.9 56.1 44.8 49.1
100 125 15 49.2 54.3 44.8 49.5
150 116 15 50.6 55.1 52.7 54.6
200 110 15 54.9 52.9 49.7 52.5
400 96 15 52.7 52.9 43.9 53.2
1000 71 15 51.4 54.0 44.8 54.5
2000 59 12 52.3 45.9 42.7 53.5
3000 51 12 55.7 49.0 46.8 59.2
4000 43 10 60.5 53.4 51.0 65.4
Table 3: The effect of verb frequency
tive feature. The only exception is the LP feature
which performed better than CO in English.
7.2 Qualitative Evaluation
We conducted qualitative analysis of the clusters
for French: those created using SPEC with F17
and F3. Verbs in the gold standard classes 29.2,
36.1, 37.3, 37.7 and 47.3 (Table 1) performed
particularly well, with the majority of member
verbs found in the same cluster. These verbs
are ideal for clustering because they have distinc-
tive syntactic-semantic characteristics. For exam-
ple, verbs in 29.2 CHARACTERIZE class (e.g. con-
cevoir, conside?rer, de?peindre) not only have a very
specific meaning but they also take high frequency
SCFs involving the preposition comme (Eng. as)
available for a verb, CO outperforms all the other features in
French, compensating for data sparseness.
which is not typical to many other classes. Inter-
estingly, Levin classes 29.2, 36.1, 37.3, and 37.7
were among the best performing classes also in
the supervised verb classification experiment of
Sun et al (2008) because these classes have dis-
tinctive characteristics also in English.
The benefit of sophisticated features which
integrate also semantic (SP) information (F17)
is particularly evident for classes with non-
distinctive syntactic characteristics. For example,
the intransitive verbs in 43.1 LIGHT EMISSION
class (e.g. briller, e?tinceler, flamboyer) are diffi-
cult to cluster based on syntax only, but semantic
features work because the verbs pose strong SPs
on their subjects (entities capable of light emis-
sion). In the experiment of Sun et al (2008), 43.1
was the worst performing class, possibly because
no semantic features were used in the experiment.
The most frequent source of error is syntac-
tic idiosyncracy. This is particularly evident
for classes 10.1 REMOVE and 45.4 CHANGE OF
STATE. Although verbs in these classes can take
similar SCFs and alternations, only some of them
are frequent in data. For example, the SCF o?ter X
a` Y is frequent for verbs in 10.1, but not o?ter X
de Y. Although class 10.1 did not suffer from this
problem in the English experiment of Sun et al
(2008), class 45.4 did. Class 45.4 performs par-
ticularly bad in French also because its member
verbs are low in frequency.
Some errors are due to polysemy, caused partly
by the fact that the French version of the gold stan-
dard was not controlled for this factor. Some verbs
have their predominant senses in classes which are
missing in the gold standard, e.g. the most fre-
quent sense of retenir is memorize, not keep as in
the gold standard class 13.5.1. GET.
Finally, some errors are not true errors but
demonstrate the capability of clustering to learn
novel information. For example, the CHANGE
OF STATE class 45.4 includes many antonyms
(e.g. weaken vs. strenghten). Clustering (us-
ing F17) separates these antonyms, so that verbs
adoucir, atte?nuer and tempe?rer appear in one clus-
ter and consolider and renforcer in another. Al-
though these verbs share the same alternations,
their SPs are different. The opposite effect can be
observed when clustering maps together classes
1062
which are semantically and syntactically related
(e.g. 36.1 CORRESPOND and 37.7 SPEAK). Such
classes are distinct in Levin and VerbNet, al-
though should ideally be related. Cases such as
these show the potential of clustering in discover-
ing novel valuable information in data.
8 Discussion and Conclusion
When sufficient corpus data is available, there is
a strong correlation between the types of features
which perform the best in English and French.
When the best features are used, many individ-
ual Levin classes have similar performance in the
two languages. Due to differences in data sets
direct comparison of performance figures for En-
glish and French is not possible. When consid-
ering the general level of performance, our best
performance for French (65.4 F) is lower than the
best performance for English in the experiment of
Sun and Korhonen (2009). However, it does com-
pare favourably to the performance of other state-
of-the-art (even supervised) English systems (Joa-
nis et al, 2008; Li and Brew, 2008; O? Se?aghdha
and Copestake, 2008; Vlachos et al, 2009). This
is impressive considering that we experimented
with a fully unsupervised approach originally de-
veloped for another language.
When aiming to improve performance further,
employing larger data is critical. Most recent ex-
periments on English have employed bigger data
sets, and unlike us, some of them have only con-
sidered the predominant senses of medium-high
frequency verbs. As seen in section 7.1, such dif-
ferences in data can have significant impact on
performance. However, parser and feature ex-
traction performance can also play a big role in
overall accuracy, and should therefore be inves-
tigated further (Sun and Korhonen, 2009). The
relatively low performance of basic LP features
in French suggests that at least some of the cur-
rent errors are due to parsing. Future research
should investigate the source of error at different
stages of processing. In addition, it would be in-
teresting to investigate whether language-specific
tuning (e.g. using language specific features such
as auxiliary classes) can further improve perfor-
mance on French.
Earlier works most closely related to ours are
those of Merlo et al (2002) and Ferrer (2004).
Our results contrast with those of Ferrer who
showed that a clustering approach does not trans-
fer well from English to Spanish. However, she
used basic SCF and named entity features only,
and a clustering algorithm less suitable for high
dimensional data. Like us, Merlo et al (2002) cre-
ated a gold standard by translating Levin classes
to another language (Italian). They also applied a
method developed for English to Italian, and re-
ported good overall performance using features
developed for English. Although the experiment
was small (focussing on three classes and a few
features only) and involved supervised classifica-
tion, the results agree with ours.
These experiments support the linguistic hy-
pothesis that Levin style classification can be
cross-linguistically applicable. A clustering tech-
nique such as the one presented here could be used
as a tool for investigating whether classifications
are similar across a wider range of more diverse
languages. From the NLP perspective, the fact that
an unsupervised technique developed for one lan-
guage can be applied to another language with-
out the need for substantial tuning means that au-
tomatic techniques could be used to hypothesise
useful Levin style classes for further languages.
This, in turn, could facilitate the creation of mul-
tilingual VerbNets in the future.
9 Acknowledgement
Our work was funded by the Royal Society Uni-
versity Research Fellowship (AK), the Dorothy
Hodgkin Postgraduate Award (LS), the EPSRC
grants EP/F030061/1 and EP/G051070/1 (UK)
and the EU FP7 project ?PANACEA?.
References
Omri Abend, Roi Reichart, and Ari Rappoport. A
supervised algorithm for verb disambiguation into
VerbNet classes. In Proc. of COLING, pages 9?16,
2008.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
The Berkeley FrameNet Project. In COLING-ACL,
pages 86?90, 1998.
Didier Bourigault, Marie-Paule Jacques, Ce?cile Fabre,
Ce?cile Fre?rot, and Sylwia Ozdowska. Syntex,
analyseur syntaxique de corpus. In Actes des
1063
12e`mes journe?es sur le Traitement Automatique des
Langues Naturelles, 2005.
Chris Brew and Sabine Schulte im Walde. Spectral
clustering for German verbs. In Proc. of EMNLP,
pages 117?124, 2002.
Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and
Zheng-Yu Niu. Unsupervised relation disambigua-
tion using spectral clustering. In Proc. of COL-
ING/ACL, pages 89?96, 2006.
Hoa Trang Dang. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. PhD
thesis, CIS, University of Pennsylvania, 2004.
Eva Esteve Ferrer. Towards a semantic classification of
Spanish verbs based on subcategorisation informa-
tion. In Proc. of ACL Student Research Workshop,
2004.
Ralph Grishman, Catherine Macleod, and Adam Mey-
ers. Comlex syntax: building a computational lexi-
con. In Proc. of COLING, pages 268?272, 1994.
Maurice Gross. Me?thodes en syntaxe. Hermann, Paris,
1975.
Eduard Hovy, Mitch Marcus, Martha Palmer,
L. Ramshaw, and R. Weischedel. Ontonotes: The
90% solution. In HLT/NAACL, 2006.
Ray Jackendoff. Semantic Structures. The MIT Press,
Cambridge, MA, 1990.
Eric Joanis, Suzanne Stevenson, and David James. A
general feature space for automatic verb classifica-
tion. Nat. Lang. Eng., 14(3):337?367, 2008.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. A large-scale classification of En-
glish verbs. Language Resources and Evaluation,
42:21?40, 2008.
Karin Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. University of Pennsyl-
vania, PA, 2005.
Beth. Levin. English verb classes and alternations: A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Fea-
tures for Automatic Verb Classification. In Proc. of
ACL, pages 434?442, 2008.
Marina. Meila. The multicut lemma. Technical report,
University of Washington, 2001.
Marina Meila and Jianbo Shi. A random walks view of
spectral segmentation. In AISTATS, 2001.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. A multilingual paradigm for auto-
matic verb classification. In Proc. of ACL, 2002.
Ce?dric Messiant. ASSCI : A subcategorization frames
acquisition system for French. In Proc. of ACL Stu-
dent Research Workshop, pages 55?60, 2008.
Ce?dric Messiant, Thierry Poibeau, and Anna Korho-
nen. LexSchem: a Large Subcategorization Lexicon
for French Verbs. In Proc. of LREC, 2008.
George A. Miller. WordNet: a lexical database for En-
glish. Communications of the ACM, 1995.
Diarmuid O? Se?aghdha and Ann Copestake. Semantic
classification with distributional kernels. In Proc. of
COLING, pages 649?656, 2008.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 3(1):71?
106, 2005.
Patrick Saint-Dizier. Verb Semantic Classes Based on
?alternations? and WordNet-like criteria . In P. Saint-
Dizier, editor, Predicative Forms in Natural lan-
guage and lexical Knowledge Bases , pages 247?
279. Kluwer Academic, 1998.
Sabine Schulte im Walde. Experiments on the Auto-
matic Induction of German Semantic Verb Classes.
Computational Linguistics, 2006.
Lei Shi and Rada Mihalcea. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proc. of CICLing, pages
100?111, 2005.
Lin Sun and Anna Korhonen. Improving verb cluster-
ing with automatically acquired selectional prefer-
ences. In Proc. of EMNLP, pages 638?647, 2009.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
Verb class discovery from rich syntactic data. LNCS,
4919:16, 2008.
Yoshimi Suzuki and Fumiyo Fukumoto. Classify-
ing Japanese Polysemous Verbs based on Fuzzy C-
means Clustering. In Proc. of TextGraphs-4, pages
32?40, 2009.
Robert Swier and Suzanne Stevenson. Unsupervised
semantic role labelling. In Proc. of EMNLP, 2004.
Gloria Va?zquez, Ana Ferna?ndez, Irene Castello?n, and
M. Antonia Mart??. Clasificacio?n verbal: Alternan-
cias de dia?tesis. In Quaderns de Sintagma. Univer-
sitat de Lleida, 2000.
Deepak Verma and Marina Meila. A comparison of
spectral clustering algorithms. Technical report, De-
partment of CSE University of Washington Seattle,
2005.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. Unsupervised and Constrained Dirich-
let Process Mixture Models for Verb Clustering. In
Proc. of the Workshop on on GEMS, pages 74?82,
2009.
Ulrike von Luxburg. A tutorial on spectral clustering.
STAT COMPUT, 17:395 ? 416, 2007.
Piek Vossen. EuroWordNet: A Multilingual Database
with Lexical Semantic Networks. Kluwer Academic
Publishers, Dordrecht, 1998.
Lihi Zelnik-Manor and Pietro Perona. Self-tuning
spectral clustering. NIPS, 17(1601-1608):16, 2004.
1064
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 76?80, Dublin, Ireland, August 23-29 2014.
CRAB 2.0: A text mining tool for supporting literature review in chemical
cancer risk assessment
Yufan Guo
1
, Diarmuid
?
O S
?
eaghdha
1
, Ilona Silins
2
, Lin Sun
1
,
Johan H
?
ogberg
2
, Ulla Stenius
2
, Anna Korhonen
1
1
Computer Laboratory, University of Cambridge, UK
2
Institute of Environmental Medicine, Karolinska Institutet, Stockholm, Sweden
Abstract
Chemical cancer risk assessment is a literature-dependent task which could greatly benefit from
text mining support. In this paper we describe CRAB ? the first publicly available tool for
supporting the risk assessment workflow. CRAB, currently at version 2.0, facilitates the gathering
of relevant literature via PubMed queries as well as semantic classification, statistical analysis and
efficient study of the literature. The tool is freely available as an in-browser application.
1 Introduction
Biomedical text mining addresses the great need to access information in the growing body of literature
in biomedical sciences. Prior research has produced useful tools for supporting practical tasks such as
literature curation and development of semantic databases, among others (Chapman and Cohen, 2009;
Harmston et al., 2010; Simpson and Demner-Fushman, 2012; McDonald and Kelly, 2012). In this paper
we describe a tool we have built to aid literature exploration for the task of chemical risk assessment
(CRA). The need for assessment of chemical hazards, exposures and their corresponding health risks is
growing, as many countries have tightened up their chemical safety rules. CRA work requires thorough
review of available scientific data for each chemical under inspection, much of which can be found in
scientific literature (EPA, 2005). Since the scientific data is highly varied and well-studied chemicals
may have tens of thousands of publications (e.g. to date PubMed contains 23,665 articles mentioning
phenobarbital), the task can be extremely time consuming when conducted via conventional means
(Korhonen et al., 2009). As a result, there is interest among the CRA community in text mining tools that
can aid and streamline the literature review process.
We have developed CRAB, an online system that supports the entire process of literature review for
cancer risk assessors. It is the first and only NLP system that serves this need. CRAB contains three main
components:
1. Literature search with PubMed integration
2. Semantic classification of abstracts with summary visualisation
3. Literature browsing with markup of information structure
These components are described further in Section 2 below. Version 2.0 of CRAB is freely available as an
in-browser application; see Section 4 for access information.
2 System description
2.1 Literature search
The first step for the user is to retrieve a collection of scientific articles relevant to their need, e.g., all
articles with abstracts that contain the name of a given chemical. The CRAB 2.0 search page (Figure
1) allows the user to directly query the MEDLINE database of biomedical abstracts. The search query
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
76
Figure 1: The CRAB 2.0 search interface
(a) Taxonomy view
gene
ral(wo
rds re
lated 
to RA
)
hum
an s
tudy
/epid
emio
logy
tumo
r rela
ted
mo
rpho
logic
al ef
fect 
on ti
ssue
/orga
n
bioc
hem
ical/c
ellbio
logic
al ef
fects
biom
arke
rs
polym
orph
ism
anim
al st
udy
stud
y len
gth
2?ye
ar ca
ncer
 bioa
ssay
shor
t and
  me
dium tumo
rs
pren
eopl
astic
 lesio
ns
mo
rpho
logic
al ef
fect 
on ti
ssue
/orga
n
bioc
hem
ical/c
ellbio
logic
al ef
fects
biom
arke
r
type
 of a
nima
l
gene
ticall
y mo
dified
 anim
als
cell 
expe
rime
nts 
bioc
hem
ical/c
ellbio
logic
al ef
fects
subc
ellula
r sys
tems
 
stud
y on
 mic
roorg
anism
s
rev
iew 
artic
le, s
umm
ary
Scientific Evidence
% ab
strac
ts
0
20
40
60
80
100
(b) Histogram view
Figure 2: The CRAB 2.0 classification component
is sent, and the results received, using the E-Utilities web service provided by the National Center
for Biotechnology Information.
1
This query interface supports PubMed Advanced Search, facilitating
complex Boolean queries.
2.2 Semantic classification
The document collection returned by the PubMed web service is passed in XML format to a semantic
classifier that annotates each abstract with 42 binary labels indicating the presence/absence of concepts
relevant to CRA. These concepts are organised hierarchically in two main taxonomies: (1) kinds of
scientific evidence used for CRA (e.g., human studies, animal studies, cell experiments, biochemical/cell
biological effects); (2) the carcinogenic modes of action indicated by the evidence (e.g., genotoxic,
nongenotoxic/indirect genotoxic, cell death, inflammation, angiogenesis). The underlying classifier is a
support vector machine (SVM) trained on a dataset of 3,078 manually annotated abstracts. Features used
by the SVM include lexical n-grams, character n-grams and MeSH concepts. For more details on the
concept taxonomies, training corpus and classifier see Korhonen et al. (2012).
1
http://www.ncbi.nlm.nih.gov/books/NBK25501/
77
Figure 3: The CRAB 2.0 information structure component
Once each abstract in the retrieved collection has been classified, the user is presented with a summary
of counts for each concept (Figure 2a). In a user study, risk assessors found this summary very useful for
obtaining a broad overview of the literature, identifying groups of chemicals with similar toxicological
profiles and identifying data gaps (Korhonen et al., 2012). The user can also request a histogram
visualisation (Figure 2b), which is produced through a call to the statistical software R.
2
2.3 Literature browsing
The risk assessment workflow involves close reading of relevant abstracts to identify specific information
about methods, experimental details, results and conclusions. While it is not feasible to automate this
process, we have shown that automatic markup and visualisation of abstracts? information structure
can accelerate it considerably (Guo et al., 2011). The model of information structure incorporated in
CRAB 2.0 is based on argumentative zoning (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel, 2010),
whereby the text of a scientific abstract (or article) is segmented into blocks of sentences that carry a
specific rhetorical function and combine to communicate the argument the authors wish to convey to
the reader. The markup scheme used in our system labels each sentence with one of seven categories:
background, objective, method, result, conclusion, related work and future work (Guo et al., 2010). The
CRAB system incorporates preprocessing (lemmatisation, POS tagging, parsing) with the C&C toolkit
3
and information structure markup with an SVM classifier that labels sentences according to a combination
of lexical, syntactic and discourse features (Guo et al., 2011). The classifier has been trained on an
annotated dataset of 1,000 CRA abstracts (Guo et al., 2010).
The automatic information structure markup is used to support browsing of the set of abstracts assigned
a label of interest by the semantic classifier; e.g., the user can inspect all abstracts labelled genotoxic
(Figure 3). Each information structure category is highlighted in a different colour and the user can select
a single category to focus on. To our knowledge, CRAB 2.0 is the first publicly available online tool that
provides information structure analysis of biomedical literature.
3 Evaluation
Intrinsic cross-validation evaluations of the semantic taxonomy classifier and information structure
classifier show high performance: 0.78 macro-averaged F-score (Korhonen et al., 2012) and 0.88 accuracy
(Guo et al., 2011), respectively. Furthermore, user-based evaluation in the context of real-life CRA has
2
http://www.r-project.org/
3
http://svn.ask.it.usyd.edu.au/trac/candc
78
produced promising results. (Korhonen et al., 2012) showed that the concept distributions produced by
our classifier confirmed known properties of chemicals without human input. Guo et al. (2011) found that
integrating information structure visualisation in abstract browsing helped risk assessors to find relevant
information in abstracts 7-8% more quickly.
4 Use
CRAB 2.0 is freely available as an in-browser application at http://omotesando-e.cl.cam.
ac.uk/CRAB/request.html. New users can register an id and password to allow them to store
and retrieve data from previous sessions. Alternatively, they can use an anonymous guest account (id
guest@coling, password guest@coling).
5 Conclusion
We have presented Version 2.0 of CRAB, the first NLP tool for supporting the workflow of literature
review for cancer risk assessment. CRAB meets a real, specialised need and is already being used to
improve the efficiency of CRA work. Although currently focused on cancer, CRAB can be easily adapted
to other health risks provided with the appropriate taxonomy and annotated data for machine learning. In
the future, the tool can be developed further in various ways, e.g. to support submissions in other formats
than PubMed XML; to take into account journal impact factors, number of citations and cross references
to better organize the literature; and to offer enriched statistical analysis of classified literature.
Acknowledgements
This work was supported by the Royal Society, Vinnova and the Swedish Research Council.
References
Wendy W. Chapman and K. Bretonnel Cohen. 2009. Current issues in biomedical text mining and natural language
processing. Journal of Biomedical Informatics, 42(5):757?759.
EPA. 2005. Guidelines for carcinogen risk assessment. US Environmental Protection Agency.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins, Lin Sun, and Ulla Stenius. 2010. Identifying the informa-
tion structure of scientific abstracts: An investigation of three different schemes. In Proceedings of BioNLP-10,
Uppsala, Sweden.
Yufan Guo, Anna Korhonen, Ilona Silins, and Ulla Stenius. 2011. Weakly supervised learning of information
structure of scientific abstracts: Is it accurate enough to benefit real-world tasks in biomedicine? Bioinformatics,
27(22):3179?3185.
Nathan Harmston, Wendy Filsell, and Michael P.H. Stumpf. 2010. What the papers say: Text mining for genomics
and systems biology. Human Genomics, 5(1):17?29.
Anna Korhonen, Ilona Silins, Lin Sun, and Ulla Stenius. 2009. The first step in the development of text min-
ing technology for cancer risk assessment: Identifying and organizing scientific evidence in risk assessment
literature. BMC Bioinformatics, 10:303.
Anna Korhonen, Diarmuid
?
O S?eaghdha, Ilona Silins, Lin Sun, Johan H?ogberg, and Ulla Stenius. 2012. Text
mining for literature review and knowledge discovery in cancer risk assessment and research. PLoS ONE,
7(4):e33427.
Diane McDonald and Ursula Kelly. 2012. The value and benefit of text mining to UK further and higher education.
Report 811, JISC.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel Collier. 2006. Zone analysis in biology articles as a basis
for information extraction. International Journal of Medical Informatics, 75(6):468?487.
Matthew S. Simpson and Dina Demner-Fushman. 2012. Biomedical text mining: A survey of recent progress. In
Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data. Springer.
79
Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
Simone Teufel. 2010. The Structure of Scientific Articles: Applications to Citation Indexing and Summarization.
CSLI Publications, Stanford, CA.
80
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273?283,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Weakly-supervised Approach to Argumentative Zoning of Scientific
Documents
Yufan Guo
Computer Laboratory
University of Cambridge, UK
yg244@cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge, UK
alk23@cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS, France
thierry.poibeau@ens.fr
Abstract
Argumentative Zoning (AZ) ? analysis of the
argumentative structure of a scientific paper ?
has proved useful for a number of informa-
tion access tasks. Current approaches to AZ
rely on supervised machine learning (ML).
Requiring large amounts of annotated data,
these approaches are expensive to develop and
port to different domains and tasks. A poten-
tial solution to this problem is to use weakly-
supervised ML instead. We investigate the
performance of four weakly-supervised clas-
sifiers on scientific abstract data annotated for
multiple AZ classes. Our best classifier based
on the combination of active learning and self-
training outperforms our best supervised clas-
sifier, yielding a high accuracy of 81% when
using just 10% of the labeled data. This re-
sult suggests that weakly-supervised learning
could be employed to improve the practical
applicability and portability of AZ across dif-
ferent information access tasks.
1 Introduction
Many practical tasks require accessing specific types
of information in scientific literature. For example,
a reader of scientific literature may be looking for
information about the objective of the study in ques-
tion, the methods used in the study, the results ob-
tained, or the conclusions drawn by authors. Sim-
ilarly, many Natural Language Processing (NLP)
tasks focus on the extraction of specific types of in-
formation in documents only.
To date, a number of approaches have been pro-
posed for sentence-based classification of scien-
tific literature according to categories of information
structure (or discourse, rhetorical, argumentative or
conceptual structure, depending on the framework
in question). Some of these classify sentences ac-
cording to typical section names seen in scientific
documents (Lin et al, 2006; Hirohata et al, 2008),
while others are based e.g. on argumentative zones
(Teufel and Moens, 2002; Mizuta et al, 2006; Teufel
et al, 2009), qualitative dimensions (Shatkay et al,
2008) or conceptual structure (Liakata et al, 2010)
of documents.
The best of current approaches have yielded
promising results and proved useful for information
retrieval, information extraction and summarization
tasks (Teufel and Moens, 2002; Mizuta et al, 2006;
Tbahriti et al, 2006; Ruch et al, 2007). How-
ever, relying on fully supervised machine learning
(ML) and a large body of annotated data, existing
approaches are expensive to develop and port to dif-
ferent scientific domains and tasks.
A potential solution to this bottleneck is to de-
velop techniques based on weakly-supervised ML.
Relying on a small amount of labeled data and
a large pool of unlabeled data, weakly-supervised
techniques (e.g. semi-supervision, active learning,
co/tri-training, self-training) aim to keep the advan-
tages of fully supervised approaches. They have
been applied to a wide range of NLP tasks, includ-
ing named-entity recognition, question answering,
information extraction, text classification and many
others (Abney, 2008), yielding performance levels
similar or equivalent to those of fully supervised
techniques.
To the best of our knowledge, such techniques
273
have not yet been applied to the analysis of infor-
mation structure of scientific documents by afore-
mentioned approaches. Recent experiments have
demonstrated the usefulness of weakly-supervised
learning for classifying discourse relations in scien-
tific texts, e.g. (Hernault et al, 2011). However, fo-
cusing on local (rather than global) structure of doc-
uments and being much more fine-grained in nature,
this related task differs from ours considerably.
In this paper, we investigate the potential of
weakly-supervised learning for Argumentative Zon-
ing (AZ) of scientific abstracts. AZ is an approach to
information structure which provides an analysis of
the rhetorical progression of the scientific argument
in a document (Teufel and Moens, 2002). It has
been used to analyze scientific texts in various disci-
plines ? including computational linguistics (Teufel
and Moens, 2002), law, (Hachey and Grover, 2006),
biology (Mizuta et al, 2006) and chemistry (Teufel
et al, 2009) ? and has proved useful for NLP tasks
such as summarization (Teufel and Moens, 2002).
Although the basic scheme is said to be discipline-
independent (Teufel et al, 2009), its application to
different domains has resulted in various modifica-
tions and laborious annotation exercises. This sug-
gests that a weakly-supervised approach would be
more practical than a fully supervised one for the
real-world application of AZ.
Taking two supervised classifiers as a comparison
point ? Support Vector Machines (SVM) and Con-
ditional Random Fields (CRF) ? we investigate the
performance of four weakly-supervised classifiers
on the AZ task: two based on semi-supervised learn-
ing (transductive SVM and semi-supervised CRF)
and two on active learning (Active SVM alone and
in combination with self-training).
The results are promising. Our best weakly-
supervised classifier (Active SVM with self-
training) outperforms the best supervised classifier
(SVM), yielding high accuracy of 81% when using
just 10% of the labeled data. When using just one
third of the labeled data, it performs equally well as
a fully supervised SVM which uses 100% of the la-
beled data. Our investigation suggests that weakly-
supervised learning could be employed to improve
the practical applicability and portability of AZ to
different information access tasks.
2 Data
We used in our experiments the recent dataset of
(Guo et al, 2010). Guo et al (2010) provide a cor-
pus of 1000 biomedical abstracts (consisting of 7985
sentences and 225785 words) annotated according
to three schemes of information structure ? those
based on section names (Hirohata et al, 2008), AZ
(Mizuta et al, 2006) and Core Scientific Concepts
(CoreSC) (Liakata et al, 2010). We focus here on
AZ only, because it subsumes all the categories of
the simple section name -based scheme, and accord-
ing to the inter-annotator agreement and ML experi-
ments reported by Guo et al (2010) it performs bet-
ter on this data than the fairly fine-grained CoreSC
scheme.
AZ is a scheme which provides an analysis of
the rhetorical progression of the scientific argument,
following the knowledge claims made by authors.
(Teufel and Moens, 2002) introduced AZ and ap-
plied it first to computational linguistics papers.
(Hachey and Grover, 2006) applied the scheme later
to legal texts and (Mizuta et al, 2006) modified it for
biology papers. More recently, (Teufel et al, 2009)
introduced a refined version of AZ and applied it to
chemistry papers.
The biomedical dataset of (Guo et al, 2010) has
been annotated according to the version of AZ de-
veloped for biology papers (Mizuta et al, 2006)
(with only minor modifications concerning zone
names). Seven categories of this scheme (out of the
10 possible) actually appear in abstracts and in the
resulting corpus. These are shown and explained
in Table 1. For example, the Method zone (METH)
is for sentences which describe a way of doing re-
search, esp. according to a defined and regular
plan; a special form of procedure or characteristic
set of procedures employed in a field of study as a
mode of investigation and inquiry.
An example of a biomedical abstract annotated
according to AZ is shown in Figure 1, with different
zones highlighted in different colors. For example,
the RES zone is highlighted in lemon green.
Table 2 shows the distribution of sentences per
scheme category in the corpus: Results (RES) is
by far the most frequent zone (accounting for 40%
of the corpus), while Background (BKG), Objective
(OBJ), Method (METH) and Conclusion (CON) cover
274
Table 1: Categories of AZ appearing in the corpus of (Guo et al, 2010)
Category Abbr. Definition
Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.
Objective OBJ A thing aimed at or sought, a target or goal
Method METH A way of doing research, esp. according to a defined and regular plan; a special form
of procedure or characteristic set of procedures employed in a field of study as a mode
of investigation and inquiry
Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula,
etc. obtained by calculation
Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction,
induction; a proposition deduced by reasoning from other propositions; the result of
a discussion, or examination of a question, final determination, decision, resolution,
final arrangement or agreement
Related work REL A comparison between the current work and the related work
Future work FUT The work that needs to be done in the future
Figure 1: An example of an annotated abstract
B
ut
ad
ie
ne
(B
D
)m
et
ab
ol
is
m
sh
ow
s
ge
nd
er
,s
pe
ci
es
an
d
co
nc
en
tr
at
io
n
de
pe
nd
en
cy
,m
ak
in
g
th
e
ex
tr
ap
ol
at
io
n
of
an
im
al
re
su
lts
to
hu
m
an
s
co
m
pl
ex
.B
D
is
m
et
ab
ol
iz
ed
m
ai
nl
y
by
cy
to
ch
ro
m
e
P4
50
2E
1
to
th
re
e
ep
ox
id
es
,1
,2
-e
po
xy
-3
-b
ut
en
e
(E
B
),
1,
2;
3,
4-
di
ep
ox
yb
ut
an
e
(D
E
B
)a
nd
1,
2-
ep
ox
y-
bu
ta
ne
di
ol
(E
B
-d
io
l).
Fo
ra
cc
ur
at
e
ris
k
as
se
ss
m
en
ti
ti
s
im
po
rt
an
tt
o
el
uc
id
at
e
sp
ec
ie
s
di
ff
er
en
ce
s
in
th
e
in
te
rn
al
fo
rm
at
io
n
of
th
e
in
di
vi
du
al
ep
ox
id
es
in
or
de
rt
o
as
si
gn
th
e
re
la
tiv
e
ris
ks
as
so
ci
at
ed
w
ith
th
ei
rd
iff
er
en
tm
ut
ag
en
ic
po
te
nc
ie
s.
A
na
ly
si
s
of
N
-t
er
m
in
al
gl
ob
in
ad
du
ct
s
is
a
co
m
m
on
ap
pr
oa
ch
fo
rm
on
ito
rin
g
th
e
in
te
rn
al
fo
rm
at
io
n
of
B
D
de
riv
ed
ep
ox
id
es
.O
ur
lo
ng
te
rm
st
ra
te
gy
is
to
de
ve
lo
p
an
L
C
-M
S/
M
S
m
et
ho
d
fo
rs
im
ul
ta
ne
ou
s
de
te
ct
io
n
of
al
lt
hr
ee
B
D
he
m
og
lo
bi
n
ad
du
ct
s.
Th
is
ap
pr
oa
ch
is
m
od
el
ed
af
te
rt
he
re
ce
nt
ly
re
po
rt
ed
im
m
un
oa
ff
in
ity
L
C
-M
S/
M
S
m
et
ho
d
fo
rt
he
cy
cl
ic
N
,N
-(
2,
3-
di
hy
dr
ox
y-
1,
4-
bu
ta
dy
il)
-v
al
in
e
(p
yr
-V
al
,d
er
iv
ed
fr
om
D
E
B
).
W
e
re
po
rt
he
re
in
th
e
an
al
ys
is
of
th
e
E
B
-d
er
iv
ed
2-
hy
dr
ox
yl
-3
-b
ut
en
yl
-v
al
in
e
pe
pt
id
e
(H
B
-V
al
).
Th
e
pr
oc
ed
ur
e
ut
ili
ze
s
tr
yp
si
n
hy
dr
ol
ys
is
of
gl
ob
in
an
d
im
m
un
oa
ff
in
ity
(I
A
)p
ur
ifi
ca
tio
n
of
al
ky
la
te
d
he
pt
ap
ep
tid
es
.Q
ua
nt
ita
tio
n
is
ba
se
d
on
L
C
-M
S/
M
S
m
on
ito
rin
g
of
th
e
tr
an
si
tio
n
fr
om
th
e
si
ng
ly
ch
ar
ge
d
m
ol
ec
ul
ar
io
n
of
H
B
-V
al
(1
-7
)t
o
th
e
a(
1)
fr
ag
m
en
t.
H
um
an
H
B
-V
al
(1
-1
1)
w
as
sy
nt
he
si
ze
d
an
d
us
ed
fo
ra
nt
ib
od
y
pr
od
uc
tio
n.
A
s
in
te
rn
al
st
an
da
rd
,t
he
la
be
le
d
ra
t-
[(
13
)C
(5
)(
15
)N
]-
V
al
(1
-1
1)
w
as
pr
ep
ar
ed
th
ro
ug
h
di
re
ct
al
ky
la
tio
n
of
th
e
co
rr
es
po
nd
in
g
pe
pt
id
e
w
ith
E
B
.S
ta
nd
ar
ds
w
er
e
ch
ar
ac
te
riz
ed
an
d
qu
an
tif
ie
d
by
L
C
-M
S/
M
S
an
d
L
C
-U
V
.T
he
m
et
ho
d
w
as
va
lid
at
ed
w
ith
di
ff
er
en
ta
m
ou
nt
s
of
hu
m
an
H
B
-V
al
st
an
da
rd
.T
he
re
co
ve
ry
w
as
>7
5%
an
d
co
ef
fic
ie
nt
of
va
ria
tio
n
<2
5%
.T
he
L
O
Q
w
as
se
tt
o
10
0
fm
ol
/in
je
ct
io
n.
Fo
ra
pr
oo
fo
fp
rin
ci
pa
le
xp
er
im
en
t,
gl
ob
in
sa
m
pl
es
fr
om
m
al
e
an
d
fe
m
al
e
ra
ts
ex
po
se
d
to
10
00
pp
m
B
D
fo
r9
0
da
ys
w
er
e
an
al
yz
ed
.T
he
am
ou
nt
s
of
H
B
-V
al
pr
es
en
tw
er
e
26
8.
2+
/-
56
an
d
35
0+
/-
70
pm
ol
/g
(m
ea
n+
/-
S.
D
.)
fo
rm
al
es
an
d
fe
m
al
es
,r
es
pe
ct
iv
el
y.
N
o
H
B
-V
al
w
as
de
te
ct
ed
in
co
nt
ro
ls
.T
he
se
da
ta
ar
e
m
uc
h
lo
w
er
co
m
pa
re
d
to
pr
ev
io
us
ly
re
po
rt
ed
va
lu
es
m
ea
su
re
d
by
G
C
-M
S/
M
S.
Th
e
di
ff
er
en
ce
m
ay
be
du
e
hi
gh
er
sp
ec
ifi
ci
ty
of
th
e
L
C
-M
S/
M
S
m
et
ho
d
to
th
e
N
-t
er
m
in
al
pe
pt
id
e
fr
om
th
e
al
ph
a-
ch
ai
n
ve
rs
us
de
riv
at
iz
at
io
n
of
bo
th
al
ph
a-
an
d
be
ta
-c
ha
in
by
E
dm
an
de
gr
ad
at
io
n,
an
d
po
ss
ib
le
in
st
ab
ili
ty
of
H
B
-V
al
ad
du
ct
s
du
rin
g
lo
ng
te
rm
st
or
ag
e
(a
bo
ut
10
ye
ar
s)
be
tw
ee
n
th
e
an
al
ys
es
.T
he
se
di
ff
er
en
ce
s
w
ill
be
re
so
lv
ed
by
ex
am
in
in
g
re
ce
nt
ly
co
lle
ct
ed
sa
m
pl
es
,u
si
ng
th
e
sa
m
e
in
te
rn
al
st
an
da
rd
fo
rp
ar
al
le
la
na
ly
si
s
by
G
C
-M
S/
M
S
an
d
L
C
-M
S/
M
S.
B
as
ed
on
ou
re
xp
er
ie
nc
e
w
ith
py
r-
V
al
ad
du
ct
as
sa
y
w
e
an
tic
ip
at
e
th
at
th
is
as
sa
y
w
ill
be
su
ita
bl
e
fo
r
ev
al
ua
tio
n
of
H
B
-V
al
in
m
ul
tip
le
sp
ec
ie
s.
Ba
ck
gr
ou
nd
O
bj
ec
tiv
e
M
et
ho
d
Re
su
lt
Co
nc
lu
si
on
Re
la
te
d 
w
or
k
Fu
tu
re
 w
or
k
Table 2: Distribution of sentences in the AZ-annotated
corpus
BKG OBJ METH RES CON REL FUT
Word 36828 23493 41544 89538 30752 2456 1174
Sentence 1429 674 1473 3185 1082 95 47
Sentence 18% 8% 18% 40% 14% 1% 1%
8-18% of the corpus each. Two categories are very
low in frequency, only covering 1% of the corpus
each: Related work (REL) and Future work (FUT).
Guo et al (2010) report the inter-annotator agree-
ment between their three annotators: one linguist,
one computational linguist and one domain expert.
According to Cohen?s kappa (Cohen, 1960) the
agreement is relatively high: ? = 0.85.
3 Automatic identification of AZ
3.1 Features and feature extraction
Guo et al (2010) used a variety of features in
their fully supervised ML experiments on different
schemes of information structure. Since their fea-
ture types cover the best performing feature types in
earlier works e.g. (Teufel and Moens, 2002; Lin et
al., 2006; Mullen et al, 2005; Hirohata et al, 2008;
Merity et al, 2009) we re-implemented and used
them in our experiment1. However, being aware
of the fact that some of these features may not be
optimal for weakly-supervised learning (i.e. when
learning from smaller data), we evaluate their per-
formance and suitability for the task later in sec-
tion 4.3.
? Location. Zones tend to appear in typical po-
sitions in abstracts. Each abstract was there-
1The only exception is the history feature which was left out
because it cannot be applied to all of our methods
275
fore divided into ten parts (1-10, measured by
the number of words), and the location was de-
fined by the parts where the sentence begins
and ends.
? Word. All the words in the corpus.
? Bi-gram. Any combination of two adjacent
words in the corpus.
? Verb. All the verbs in the corpus.
? Verb Class. 60 verb classes appearing in
biomedical journal articles.
? Part-of-Speech ? POS. The POS tag of each
verb in the corpus.
? Grammatical Relation ? GR. Subject (nc-
subj), direct object (dobj), indirect object (iobj)
and second object (obj2) relations in the cor-
pus. e.g. (ncsubj observed 14 difference 5
obj). The value of this feature equals 1 if it
occurs in a particular sentence (and 0 if not).
? Subj and Obj. The subjects and objects ap-
pearing with any verbs in the corpus (extracted
from above GRs).
? Voice. The voice of verbs (active or passive) in
the corpus.
These features were extracted from the corpus us-
ing a number of tools. A tokenizer was used to detect
the boundaries of sentences and to separate punctu-
ation from adjacent words e.g. in complex biomed-
ical terms such as 2-amino-3,8-diethylimidazo[4,5-
f]quinoxaline. The C&C tools (Curran et al, 2007)
trained on biomedical literature were employed for
POS tagging, lemmatization and parsing. The
lemma output was used for creating Word, Bi-gram
and Verb features. The GR output was used for cre-
ating the GR, Subj, Obj and Voice features. The
?obj? marker in a subject relation indicates passive
voice (e.g. (ncsubj observed 14 difference 5 obj)).
The verb classes were acquired automatically from
the corpus using the unsupervised spectral cluster-
ing method of (Sun and Korhonen, 2009). To con-
trol the number of features we lemmatized the lexi-
cal items for all the features, and removed the words
and GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences.
3.2 Machine learning methods
Support Vector Machines (SVM) and Conditional
Random Fields (CRF) have proved the best perform-
ing fully supervised methods in most recent works
on information structure, e.g. (Teufel and Moens,
2002; Mullen et al, 2005; Hirohata et al, 2008; Guo
et al, 2010). We therefore implemented these meth-
ods as well as weakly supervised variations of them:
active SVM with and without self-training, transduc-
tive SVM and semi-supervised CRF.
3.2.1 Supervised methods
SVM constructs hyperplanes in a multidimen-
sional space to separate data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w ? x ? b = 0, where w is its normal vec-
tor. We want to maximize the distance from the hy-
perplane to the data points, or the distance between
two parallel hyperplanes each of which separates the
data. The parallel hyperplanes can be written as:
w ? x ? b = 1 and w ? x ? b = ?1, and the dis-
tance between them is 2|w| . The problem reduces to:
Minimize |w| (in w, b)
Subject to
w ? x? b ? 1 for x of one class,
w ? x? b ? ?1 for x of the other,
which can be solved by using the SMO algorithm
(Platt, 1999b). We used Weka software (Hall et al,
2009) (employing its linear kernel) for SVM experi-
ments.
CRF is an undirected graphical model which de-
fines a probability distribution over the hidden states
(e.g. label sequences) given the observations. The
probability of a label sequence y given an observa-
tion sequence x can be written as:
p(y|x, ?) = 1Z(x)exp(
?
j ?jFj(y, x)),
where Fj(y, x) is a real-valued feature function of
the states and the observations; ?j is the weight of
Fj , and Z(x) is a normalization factor. The ? pa-
rameters can be learned using the L-BFGS algorithm
(Nocedal, 1980). We used Mallet software (McCal-
lum, 2002) for CRF experiments.
3.2.2 Weakly-supervised methods
Active SVM (ASVM) starts with a small amount of
labeled data, and iteratively chooses a proportion of
276
unlabeled data for which SVM has less confidence
to be labeled (the labels can be restored from the
original corpus) and used in the next round of learn-
ing, i.e. active learning. Query strategies based on
the structure of SVM are frequently employed (Tong
and Koller, 2001; Novak et al, 2006). For exam-
ple, it is often assumed that the data points close to
the separating hyperplane are those that the SVM is
uncertain about. Unlike these methods, our learn-
ing algorithm compares the posterior probabilities
of the best estimate given each unlabeled instance,
and queries those with the lowest probabilities for
the next round of learning. The probabilities can be
obtained by fitting a Sigmoid after the standard SVM
(Platt, 1999a), and combined using a pairwise cou-
pling algorithm (Hastie and Tibshirani, 1998) in the
multi-class case. We used the SVM linear kernel in
Weka for classification, and the -M flag in Weka for
calculating the posterior probabilities.
Active SVM with self-training (ASSVM) is an ex-
tension of ASVM where each round of training has
two steps: (i) training on the labeled, and testing
on the unlabeled data, and querying; (ii) training on
both labeled and unlabeled/machine-labeled data by
using the estimates from step (i). The idea of ASSVM
is to make the best use of the labeled data, and to
make the most use of the unlabeled data.
Transductive SVM (TSVM) is an extension of
SVM which takes advantage of both labeled and un-
labeled data (Vapnik, 1998). Similar to SVM, the
problem is defined as:
Minimize |w| (in w, b, y(u))
Subject to
y(l)(w ? x(l) ? b) ? 1,
y(u)(w ? x(u) ? b) ? 1 ,
y(u) ? {?1, 1},
where x(u) is unlabeled data and y(u) the estimate
of its label. The problem can be solved by using
the CCCP algorithm (Collobert et al, 2006). We
used UniverSVM software (Sinz, 2011) for TSVM
experiments.
Semi-supervised CRF (SSCRF) can be imple-
mented with entropy regularization (ER). It ex-
tends the objective function on Labeled data?
L log p(y(l)|x(l), ?) with an additional term?
U
?
Y p(y|x(u), ?) log p(y|x(u), ?) to minimize
the conditional entropy of the model?s predictions on
Unlabeled data (Jiao et al, 2006; Mann and Mccal-
lum, 2007). We used Mallet software (McCallum,
2002) for SSCRF experiments.
4 Experimental evaluation
4.1 Evaluation methods
We evaluated the ML results in terms of accuracy,
precision, recall, and F-measure against manual AZ
annotations in the corpus:
acc = no. of correctly classified sentencestotal no. ofsentences in the corpus
p = no. of sentences correctly identified as Classitotal no. of sentences identified as Classi
r = no. of sentences correctly identified as Classitotal no. of sentences in Classi
f = 2?p?rp+r
We used 10-fold cross validation for all the meth-
ods to avoid the possible bias introduced by rely-
ing on any particular split of the data. More specif-
ically, the data was randomly assigned to ten folds
of roughly the same size. Each fold was used once
as test data and the remaining nine folds as training
data. The results were then averaged.
Following (Dietterich, 1998), we used McNe-
mar?s test (McNemar, 1947) to measure the statisti-
cal significance between the results of different ML
methods. The chosen significance level was .05.
4.2 Results
Table 3 shows the results for the four weakly-
supervised and two supervised methods when 10%
of the training data (i.e. ?700 sentences) has been
labeled. We can see that ASSVM is the best perform-
ing method with an accuracy of 81% and the macro
Table 3: Results when using 10% of the labeled data
Acc. F-score
MF BKG OBJ METH RES CON REL FUT
SVM .77 .74 .84 .68 .71 .82 .64 - -
CRF .70 .65 .75 .46 .48 .78 .76 - -
ASVM .80 .75 .88 .56 .68 .87 .78 .33
ASSVM .81 .76 .86 .56 .76 .88 .76 - -
TSVM .76 .73 .84 .61 .71 .79 .71 - -
SSCRF .73 .67 .76 .48 .52 .81 .78 - -
MF: Macro F-score of the five high frequency categories:
BKG, OBJ, METH, RES, CON.
277
Figure 2: Learning curve for different methods when using 0-100% of the labeled data
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Labeled
A
cc
u
ra
cy
SVM CRF ASVM ASSVM TSVM SSCRF
Figure 3: Area under learning curves at different intervals
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SVM CRF ASVM ASSVM TSVM SSCRF
Ar
ea
(0,10%] [10%,20%] [20%,40%] [40%,100%]
F-score of .76 (the macro F-score is calculated for
the 5 scheme categories which are found by all the
methods). ASVM performs nearly as well, with an
accuracy of 80% and F-score of .75. Both methods
outperform supervised SVM with a statistically sig-
nificant difference (p < .001).
TSVM is the lowest performing SVM-based
method. Yielding an accuracy of 76% and F-score
of .73 its performance is lower than that of the super-
vised SVM. However, it does outperform both CRF-
based methods. SSCRF performs better than CRF
with 3% higher accuracy and .02 higher F-score.
The difference in accuracy is statistically significant
(p < .001).
Only one method (ASVM) identifies six out of the
seven possible categories. Other methods identify
five categories. The 1-2 missing categories are very
low in frequency (accounting for 1% of the corpus
data each, see table 2). Looking at the results for
other categories, they seem to reflect the amount of
corpus data available for each category (Table 2),
with RES (Results) being the highest and OBJ (Ob-
jective) the lowest performing category with most
methods. Interestingly, the only method that per-
forms relatively well on OBJ is the supervised SVM.
The best method ASSVM outperforms other meth-
ods most clearly on METH (Method) category. Al-
though METH is a high frequency category (account-
ing for 18% of the corpus data) other methods tend
to confuse it with OBJ, presumably because a single
sentence may contain elements of both (e.g. scien-
tists may describe some of their method when de-
scribing the objective of the study).
Figure 2 shows the learning curve of different
methods (in terms of accuracy) when the percentage
of the labeled data (in the training set) ranges from 0
to 100%. ASSVM outperforms other methods, reach-
ing its best performance of 88% accuracy when us-
ing ?40% of the labeled data. Indeed when using
33% of the labeled data, it performs already equally
well as fully-supervised SVM using 100% of the la-
beled data. The advantage of ASSVM over ASVM
(the second best method) is clear especially when
20-40% of the labeled data is used. SVM and TSVM
tend to perform quite similarly with each other when
more than 25% of the labeled data is used, but when
less data is available, SVM performs better. Look-
ing at the CRF-based methods, SSCRF outperforms
CRF in particular when 10-25% of the labeled data
is used. However, neither of them reaches the per-
formance level of SVM-based methods.
Figure 3 shows the area under the learning curves
(by the trapezoidal rule) at different intervals, which
gives a reasonable approximation to the overall per-
formance of different methods. The area under
ASSVM is the largest at each of the four intervals,
with a value of .08 at (0,10%], .07 at [10%,20%],
278
.20 at [20%, 40%] and .50 at [40%,100%]. The dif-
ference between supervised and weakly-supervised
methods is more significant at (0, 20%] than at
[20%,100%].
4.3 Further analysis of the features
As explained in section 3.1, we employed in our
experiments a collection of features which had per-
formed well in previous supervised AZ experiments.
We conducted further analysis to investigate which
of these features are the most (and the least) useful
for weakly-supervised learning. We took our best
performing method ASSVM and conducted leave-
one-out analysis of the features with 10% of the la-
beled data. The results are shown in Table 4.
Table 4: Leaving one feature out results for ASSVM when
using 10% of the labeled data
Acc. F-score
MF BKG OBJ METH RES CON REL FUT
Location .73 .67 .67 .55 .62 .85 .65 - -
Word .80 .78 .87 .70 .74 .85 .72 - -
Bigram .81 .75 .83 .57 .71 .87 .78 .33 -
Verb .81 .79 .84 .77 .73 .87 .75 - -
VC .79 .75 .86 .62 .72 .84 .70 - -
POS .74 .70 .66 .65 .66 .82 .73 - -
GR .79 .75 .83 .67 .69 .84 .72 - -
Subj .80 .76 .87 .65 .73 .85 .72 - -
Obj .80 .78 .84 .75 .70 .85 .75 - -
Voice .78 .75 .88 .70 .71 .83 .62 - -
? .81 .76 .86 .56 .76 .88 .76 - -
MF: Macro F-score of the five high frequency categories:
BKG, OBJ, METH, RES, CON.
?: Employing all the features.
We can see that the Location feature is by far the
most useful feature for ASSVM. The performance
drops 8% in accuracy and .09 in F-score in the ab-
sence of this feature. Location is particularly im-
portant for BKG (which nearly always appears in the
same location: in the beginning of an abstract) and is
highly useful for METH and CON as well. Removing
POS has almost equally strong effect, in particular
on BKG and METH, suggesting that verb tense is par-
ticularly useful for distinguishing these categories.
Also Voice, Verb class and GR contribute to gen-
eral performance, especially to accuracy. Voice is
particularly important for CON, which differs from
other categories in the sense that it is marked by fre-
quent usage of active voice. Verb class is helpful for
METH, RES and CON while GR is helpful for all high
frequency categories.
Among the least helpful features are those which
suffer from sparse data problems, including e.g.
Word, Bi-gram, and Verb. They perform particularly
badly when applied to low frequency zones. How-
ever, this is not the case when using fully-supervised
methods (i.e. 100% of the labeled data), suggest-
ing that a good performance in fully supervised ex-
periments does not necessarily translate into a good
performance in weakly-supervised experiments, and
that careful feature analysis and selection is impor-
tant when aiming to optimize the performance when
learning from sparse data.
5 Discussion
In our experiments, the majority of weakly-
supervised methods outperformed their correspond-
ing supervised methods when using just 10% of
the labeled data. The SVM-based methods per-
formed better than the CRF-based ones (regardless of
whether they were weakly or fully supervised). Guo
et al (2010) made a similar discovery when com-
paring fully supervised versions of SVM and CRF.
Our best performing weakly-supervised methods
were those based on active learning. Making a good
use of both labeled and unlabeled data, active learn-
ing combined with self-training (ASSVM) proved to
be the most useful method. Given 10% of the la-
beled data, ASSVM obtained an accuracy of 81% and
F-score of .76, outperforming the best supervised
method SVM with a statistically significant differ-
ence. It reached its top performance (88% accuracy)
when using 40% of the labeled data, and performed
equally well as fully supervised SVM (i.e. 100% of
the labeled data) when using just one third of the la-
beled data.
This result is in line with the results of many
other text classification works where active learn-
ing (alone or in combination with other techniques
such as self-training) has proved similarly useful,
e.g. (Lewis and Gale, 1994; Tong and Koller, 2002;
Brinker, 2006; Novak et al, 2006; Esuli and Sebas-
tiani, 2009; Yang et al, 2009).
While active learning iteratively explores the
unknown aspects of the unlabeled data, semi-
supervised learning attempts to make the best use
279
of what it already knows about the data. In our ex-
periments, semi-supervised methods (TSVM and SS-
CRF) did not perform equally well as active learning
? TSVM even produced a lower accuracy than SVM
with the same amount of labeled data ? although
these methods have gained success in related works.
We therefore looked into related works using
TSVM, e.g. (Chapelle and Zien, 2005), and discov-
ered that our dataset is much higher in dimensional-
ity than those employed in many other works. High
dimensional data is more sensitive, and therefore
fine-tuning with unlabeled data may cause a big de-
viation. We also looked into related works using
SSCRF, in particular the work of (Jiao et al, 2006)
who used the same SSCRF as the one we used in our
experiments. Jiao et al (2006) employed a much
larger data set than we did ? one including 5448 la-
beled instances (in 3 classes) and 5210-25145 unla-
beled instances. Given more labeled and unlabeled
data per class we might be able to obtain better per-
formance using SSCRF also on our task. However,
given the high cost of obtaining labeled data meth-
ods not needing it are preferable.
6 Conclusions and future work
Our experiments show that weakly-supervised
learning can be used to identify AZ in scientific
documents with good accuracy when only a limited
amount of labeled data is available. This is helpful
thinking of the real-world application and porting of
the approach to different tasks and domains. To the
best of our knowledge, no previous work has been
done on weakly-supervised learning of information
structure according to schemes of the type we have
focused on (Teufel and Moens, 2002; Mizuta et al,
2006; Lin et al, 2006; Hirohata et al, 2008; Shatkay
et al, 2008; Liakata et al, 2010).
Recently, some work has been done on the related
task of classification of discourse relations in sci-
entific texts: (Hernault et al, 2011) used structural
learning (Ando and Zhang, 2005) for this task. They
obtained 30-60% accuracy on the RST Discourse
Treebank (including 41 relation types) when using
100-10000 labeled and 100000 unlabeled instances.
The accuracy was 20-60% when using the labeled
data only. However, although related, the task of
discourse relation classification differs substantially
from our task in that it focuses on local discourse re-
lations while our task focuses on the global structure
of the scientific document.
In the future, we plan to improve and extend this
work in several directions. First, the approach to
active learning could be improved in various ways.
The query strategy we employed (uncertainty sam-
pling) is a relatively straightforward method which
only considers the best estimate for each unlabeled
instance, disregarding other estimates that may con-
tain useful information. In the future, we plan to
experiment with more sophisticated strategies, e.g.
the margin sampling algorithm by (Scheffer et al,
2001) and the query-by-committee (QBC) algorithm
by (Seung et al, 1992). In addition, there are al-
gorithms designed for reducing the redundancy in
queries which may be worth investigating (Hoi et al,
2006).
Also, (Hoi et al, 2006) shows that Logistic Re-
gression (LR) outperforms SVM when used with ac-
tive learning, yielding higher F-score on the Reuters-
21578 data set (binary classification, 10,788 docu-
ments in total, 100 of them labeled). It would be
interesting to explore whether supervised methods
other than SVM are optimal for active learning when
applied to our task.
Secondly, we plan to investigate other semi-
supervised methods, for example, the Expectation-
Maximization (EM) algorithm. (Lanquillon, 2000)
has shown that EM SVM performs better than super-
vised and transductive SVM on a text classification
task when applied to the dataset of 20 Newsgroups
(20 classes, 4000 documents for testing, 10000 un-
labeled ones), yielding up to ?10% higher accu-
racy when 200-5000 labeled documents are used for
training.
In addition, other combinations of weakly-
supervised methods might be worth looking into,
such as EM+active learning (McCallum and Nigam,
1998) and co-training+EM+active learning (Muslea
et al, 2002), which have proved promising in related
text classification works.
Besides looking for optimal ML strategies, we
plan to look for optimal features for the task. Our
feature analysis showed that not all the features
which had proved promising in fully supervised ex-
periments were equally promising when applied to
weakly-supervised learning from smaller data. We
280
plan to look into ways of reducing the sparse data
problem in features, e.g. by classifying not only
verbs but also other word classes into semantically-
motivated categories.
One the key motivations for developing a weakly-
supervised approach is to facilitate easy porting of
schemes such as AZ to new tasks and domains. Re-
cent research shows that active learning in a target
domain can leverage information from a different
but related (source) domain (Rai et al, 2010). Mak-
ing use of existing annotated datasets in biology,
chemistry, computational linguistics and law (Teufel
and Moens, 2002; Mizuta et al, 2006; Hachey
and Grover, 2006; Teufel et al, 2009) we will ex-
plore optimal ways of combining weakly-supervised
learning with domain-adaptation.
The work presented in this paper has focused on
the abstracts annotated according to the AZ scheme.
In the future, we plan to investigate the usefulness
of weakly-supervised learning for identifying other
schemes of information structure, e.g. (Lin et al,
2006; Hirohata et al, 2008; Shatkay et al, 2008;
Liakata et al, 2010), and not only in scientific ab-
stracts but also in full journal papers which typically
exemplify a larger set of scheme categories.
Finally, an important avenue of future research
is to evaluate the usefulness of weakly-supervised
identification of information structure for NLP tasks
such as summarization and information extraction
(Tbahriti et al, 2006; Ruch et al, 2007), and for
practical tasks such as manual review of scientific
papers for research purposes (Guo et al, 2010).
Acknowledgments
The work reported in this paper was funded by the
Royal Society (UK). YG was funded by the Cam-
bridge International Scholarship.
References
Steven Abney. 2008. Semi-supervised learning for com-
putational linguistics. Chapman & Hall / CRC.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res., 6:1817?
1853.
Klaus Brinker. 2006. On active learning in multi-label
classification. In From Data and Information Analysis
to Knowledge Engineering, pages 206?213.
Olivier Chapelle and Alexander Zien. 2005. Semi-
supervised classification by low density separation.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20(1):37?46.
Ronan Collobert, Fabian Sinz, Jason Weston, and Le?on
Bottou. 2006. Trading convexity for scalability. In
Proceedings of the 23rd international conference on
Machine learning.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&c and boxer. In Pro-
ceedings of the ACL 2007 Demonstrations Session.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Comput., 10:1895?1923.
Andrea Esuli and Fabrizio Sebastiani. 2009. Active
learning strategies for multi-label text classification.
In Proceedings of the 31th European Conference on
IR Research on Advances in Information Retrieval.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins
Karolinska, Lin Sun, and Ulla Stenius. 2010. Identi-
fying the information structure of scientific abstracts:
an investigation of three different schemes. In Pro-
ceedings of the 2010 Workshop on Biomedical Natural
Language Processing.
Ben Hachey and Claire Grover. 2006. Extractive sum-
marisation of legal texts. Artif. Intell. Law, 14:305?
345.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18.
T. Hastie and R. Tibshirani. 1998. Classification by pair-
wise coupling. Advances in Neural Information Pro-
cessing Systems, 10.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Semi-supervised discourse relation
classification with structural learning. In CICLing (1).
K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka.
2008. Identifying sections in scientific abstracts us-
ing conditional random fields. In Proceedings of 3rd
International Joint Conference on Natural Language
Processing.
Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006.
Large-scale text categorization by batch mode active
learning. In Proceedings of the 15th international con-
ference on World Wide Web.
F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL.
Carsten Lanquillon. 2000. Learning from labeled and
unlabeled documents: A comparative study on semi-
supervised text classification. In Proceedings of the
281
4th European Conference on Principles of Data Min-
ing and Knowledge Discovery.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batche-
lor. 2010. Corpora for the conceptualisation and zon-
ing of scientific papers. In Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?10).
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings of
BioNLP-06.
G. S. Mann and A. Mccallum. 2007. Efficient compu-
tation of entropy gradient for semi-supervised condi-
tional random fields. In HLT-NAACL.
Andrew McCallum and Kamal Nigam. 1998. Employ-
ing em and pool-based active learning for text classi-
fication. In Proceedings of the Fifteenth International
Conference on Machine Learning.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference Between Correlated Proportions or
Percentages. Psychometrika, 12(2):153?157.
S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate
argumentative zoning with maximum entropy models.
In Proceedings of the 2009 Workshop on Text and Ci-
tation Analysis for Scholarly Digital Libraries.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006.
Zone analysis in biology articles as a basis for in-
formation extraction. International Journal of Med-
ical Informatics on Natural Language Processing in
Biomedicine and Its Applications, 75(6):468?487.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A base-
line feature set for learning rhetorical zones using full
articles in the biomedical domain. Natural language
processing and text mining, 7(1):52?58.
Ion Muslea, Steven Minton, and Craig A. Knoblock.
2002. Active + semi-supervised learning = robust
multi-view learning. In Proceedings of the Nineteenth
International Conference on Machine Learning.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices
with Limited Storage. Mathematics of Computation,
35(151):773?782.
Bla Novak, Dunja Mladeni, and Marko Grobelnik. 2006.
Text classification with active learning. In From Data
and Information Analysis to Knowledge Engineering,
pages 398?405.
J. C. Platt. 1999a. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classiers,
pages 61?74.
John C. Platt. 1999b. Using analytic qp and sparseness
to speed training of support vector machines. In Pro-
ceedings of the 1998 conference on Advances in neural
information processing systems II.
Piyush Rai, Avishek Saha, Hal Daume?, III, and Suresh
Venkatasubramanian. 2010. Domain adaptation
meets active learning. In Proceedings of the NAACL
HLT 2010 Workshop on Active Learning for Natural
Language Processing.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. Int J Med Inform, 76(2-3):195?200.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for informa-
tion extraction. In Proceedings of the 4th International
Conference on Advances in Intelligent Data Analysis.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the fifth an-
nual workshop on Computational learning theory.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008.
Multi-dimensional classification of biomedical text:
Toward automated, practical provision of high-utility
text to diverse users. Bioinformatics, 24(18):2086?
2093.
F. Sinz, 2011. UniverSVM Support Vector Ma-
chine with Large Scale CCCP Functionality.
http://www.kyb.mpg.de/bs/people/fabee/universvm.html.
L. Sun and A. Korhonen. 2009. Improving verb cluster-
ing with automatically acquired selectional preference.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75(6):488?495.
S. Teufel and M. Moens. 2002. Summarizing scien-
tific articles: Experiments with relevance and rhetor-
ical status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning: Ev-
idence from chemistry and computational linguistics.
In Proceedings of EMNLP.
S. Tong and D. Koller. 2001. Support vector machine
active learning with applications to text classification.
Journal of Machine Learning Research, 2:45?66.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. J. Mach. Learn. Res., 2:45?66.
282
V. N. Vapnik. 1998. Statistical learning theory. Wiley,
New York.
Bishan Yang, Jian-Tao Sun, Tengjiao Wang, and Zheng
Chen. 2009. Effective multi-label active learning for
text classification. In Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining.
283
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012?1022,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Latent Vector Weighting for Word Meaning in Context
Tim Van de Cruys
RCEAL
University of Cambridge
tv234@cam.ac.uk
Thierry Poibeau
LaTTiCe, UMR8094
CNRS & ENS
thierry.poibeau@ens.fr
Anna Korhonen
Computer Laboratory & RCEAL
University of Cambridge
alk23@cam.ac.uk
Abstract
This paper presents a novel method for the com-
putation of word meaning in context. We make
use of a factorization model in which words, to-
gether with their window-based context words
and their dependency relations, are linked to
latent dimensions. The factorization model al-
lows us to determine which dimensions are im-
portant for a particular context, and adapt the
dependency-based feature vector of the word
accordingly. The evaluation on a lexical substi-
tution task ? carried out for both English and
French ? indicates that our approach is able to
reach better results than state-of-the-art meth-
ods in lexical substitution, while at the same
time providing more accurate meaning repre-
sentations.
1 Introduction
According to the distributional hypothesis of meaning
(Harris, 1954), words that occur in similar contexts
tend to be semantically similar. In the spirit of this by
now well-known adage, numerous algorithms have
sprouted up that try to capture the semantics of words
by looking at their distribution in texts, and compar-
ing those distributions in a vector space model.
Up till now, the majority of computational ap-
proaches to semantic similarity represent the mean-
ing of a word as the aggregate of the word?s contexts,
and hence do not differentiate between the different
senses of a word. The meaning of a word, however, is
largely dependent on the particular context in which
it appears. Take for example the word work in sen-
tences (1) and (2).
(1) The painter?s recent work is a classic example
of art brut.
(2) Equal pay for equal work!
The meaning of work is quite different in both sen-
tences. In sentence (1), work refers to the product of a
creative act, viz. a painting. In sentence (2), it refers
to labour carried out as a source of income. The
NLP community?s standard answer to the ambiguity
problem has always been some flavour of word sense
disambiguation (WSD), which in its standard form
boils down to choosing the best-possible fit from a
pre-defined sense inventory. In recent years, it has
become clear that this is in fact a very hard task to
solve for computers and humans alike (Ide and Wilks,
2006; Erk et al, 2009; Erk, 2010).
With these findings in mind, researchers have
started looking at different methods to tackle lan-
guage?s ambiguity, ranging from coarser-grained
sense inventories (Hovy et al, 2006) and graded
sense assignment (Erk and McCarthy, 2009), over
word sense induction (Schu?tze, 1998; Pantel and Lin,
2002; Agirre et al, 2006), to the computation of indi-
vidual word meaning in context (Erk and Pado?, 2008;
Thater et al, 2010; Dinu and Lapata, 2010). This
research inscribes itself in the same line of thought,
in which the meaning disambiguation of a word is
not just the assignment of a pre-defined sense; in-
stead, the original meaning representation of a word
is adapted ?on the fly?, according to ? and specifi-
cally tailored for ? the particular context in which
it appears. To be able to do so, we build a factor-
ization model in which words, together with their
window-based context words and their dependency
1012
relations, are linked to latent dimensions. The factor-
ization model allows us to determine which dimen-
sions are important for a particular context, and adapt
the dependency-based feature vector of the word ac-
cordingly. The evaluation on a lexical substitution
task ? carried out for both English and French ? indi-
cates that our method is able to reach better results
than state-of-the-art methods in lexical substitution,
while at the same time providing more accurate mean-
ing representations.
The remainder of this paper is organized as follows.
In section 2, we present some earlier work that is
related to the research presented here. Section 3
describes the methodology of our method, focusing
on the factorization model, and the computation of
meaning in context. Section 4 presents a thorough
evaluation on a lexical substitution task, both for
English and French. The last section then draws
conclusions, and presents a number of topics that
deserve further exploration.
2 Related work
One of the best known computational models of se-
mantic similarity is latent semantic analysis ? LSA
(Landauer and Dumais, 1997; Landauer et al, 1998).
In LSA, a term-document matrix is created, that con-
tains the frequency of each word in a particular doc-
ument. This matrix is then decomposed into three
other matrices with a mathematical factorization tech-
nique called singular value decomposition (SVD).
The most important dimensions that come out of the
SVD are said to represent latent semantic dimensions,
according to which nouns and documents can be rep-
resented more efficiently. Our model also applies
a factorization technique (albeit a different one) in
order to find a reduced semantic space.
The nature of a word?s context is a determining
factor in the kind of the semantic similarity that is in-
duced. A broad context window (e.g. a paragraph or
document) yields broad, topical similarity, whereas
a small context window yields tight, synonym-like
similarity. This has lead a number of researchers
(e.g. Lin (1998)) to use the dependency relations that
a particular word takes part in as context features.
An overview of dependency-based semantic space
models is given in Pado? and Lapata (2007).
A number of researchers have exploited the no-
tion of context to differentiate between the different
senses of a word in an unsupervised way (a task la-
beled word sense induction or WSI). Schu?tze (1998)
proposed a context-clustering approach, in which
context vectors are created for the different instances
of a particular word, and those contexts are grouped
into a number of clusters, representing the different
senses of the word. The context vectors are rep-
resented as second-order co-occurrences (i.e. the
contexts of the target word are similar if the words
they in turn co-occur with are similar). Van de Cruys
(2008) proposed a model for sense induction based
on latent semantic dimensions. Using a factorization
technique based on non-negative matrix factorization,
the model induces a latent semantic space according
to which both dependency features and broad con-
textual features are classified. Using the latent space,
the model is able to discriminate between different
word senses. Our approach makes use of a simi-
lar factorization model, but we extend the approach
with a probabilistic framework that is able to adapt
the original vector according to the context of the
instance.
Recently, a number of models emerged that aim
to model the individual meaning of words in context.
Erk and Pado? (2008, 2009) make use of selectional
preferences to express the meaning of a word in con-
text; the meaning of a word in the presence of an
argument is computed by multiplying the word?s vec-
tor with a vector that captures the inverse selectional
preferences of the argument. Thater et al (2009) and
Thater et al (2010) extend the approach based on se-
lectional preferences by incorporating second-order
co-occurrences in their model; their model allows
first-order co-occurrences to act as a filter upon the
second-order vector space, which allows for the com-
putation of meaning in context.
Erk and Pado? (2010) propose an exemplar-based
approach, in which the meaning of a word in context
is represented by the activated exemplars that are
most similar to it. And Mitchell and Lapata (2008)
propose a model for vector composition, focusing on
the different functions that might be used to combine
the constituent vectors. Their results indicate that
a model based on pointwise multiplication achieves
better results than models based on vector addition.
Finally, Dinu and Lapata (2010) propose a proba-
bilistic framework that models the meaning of words
1013
as a probability distribution over latent dimensions
(?senses?). Contextualized meaning is then mod-
eled as a change in the original sense distribution.
The model presented in this paper bears some resem-
blances to their approach; however, while their ap-
proach computes the contextualized meaning directly
within the latent space, our model exploits the latent
space to determine the features that are important
for a particular context, and adapt the original (out-
of-context) dependency-based feature vector of the
target word accordingly. This allows for a more pre-
cise and more distinct computation of word meaning
in context. Secondly, Dinu and Lapata use window-
based context features to build their latent model,
while our approach combines both window-based
and dependency-based features.
3 Methodology
3.1 Non-negative Matrix Factorization
Our model uses non-negative matrix factorization
(Lee and Seung, 2000) in order to find latent dimen-
sions. There are a number of reasons to prefer NMF
over the better known singular value decomposition
used in LSA. First of all, NMF allows us to mini-
mize the Kullback-Leibler divergence as an objec-
tive function, whereas SVD minimizes the Euclidean
distance. The Kullback-Leibler divergence is better
suited for language phenomena. Minimizing the Eu-
clidean distance requires normally distributed data,
and language phenomena are typically not normally
distributed. Secondly, the non-negative nature of
the factorization ensures that only additive and no
subtractive relations are allowed. This proves partic-
ularly useful for the extraction of semantic dimen-
sions, so that the NMF model is able to extract much
more clear-cut dimensions than an SVD model. And
thirdly, the non-negative property allows the resulting
model to be interpreted probabilistically, which is not
straightforward with an SVD factorization.
The key idea is that a non-negative matrix A is
factorized into two other non-negative matrices, W
and H
Ai?j ?Wi?kHk?j (1)
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler di-
vergence as an objective function, we want to find the
matrices W and H for which the Kullback-Leibler
divergence between A and WH (the multiplication
of W and H) is the smallest. This factorization is
carried out through the iterative application of update
rules. Matrices W and H are randomly initialized,
and the rules in 2 and 3 are iteratively applied ? alter-
nating between them. In each iteration, each vector is
adequately normalized, so that all dimension values
sum to 1.
Ha? ? Ha?
?
iWia
Ai?
(WH)i??
kWka
(2)
Wia ?Wia
?
?Ha?
Ai?
(WH)i??
vHav
(3)
3.2 Combining syntax and context words
Using an extension of non-negative matrix factor-
ization (Van de Cruys, 2008), it is possible to
jointly induce latent factors for three different modes:
words, their window-based context words, and their
dependency-based context features. As input to
the algorithm, three matrices are constructed that
capture the pairwise co-occurrence frequencies for
the different modes. The first matrix contains co-
occurrence frequencies of words cross-classified
by dependency-based features, the second matrix
contains co-occurrence frequencies of words cross-
classified by words that appear in the word?s context
window, and the third matrix contains co-occurrence
frequencies of dependency-based features cross-
classified by co-occurring context words. NMF is
then applied to the three matrices, and the separate
factorizations are interleaved (i.e. the results of the
former factorization are used to initialize the factor-
ization of the next matrix). A graphical represen-
tation of the interleaved factorization algorithm is
given in figure 1.
When the factorization is finished, the three dif-
ferent modes (words, window-based context words
and dependency-based features) are all represented
according to a limited number of latent factors.
1014
= xW H
= xV G
= xU F
j
i
s
k
i
j
kAwords xdependency relations
B
words xcontext words
Ccontext words xdependency relations
k k
k
k
i
j
i
s
j
s s
Figure 1: A graphical representation of the interleaved
NMF
The factorization that comes out of the NMF model
can be interpreted probabilistically (Gaussier and
Goutte, 2005; Ding et al, 2008). More specifically,
we can transform the factorization into a standard
latent variable model of the form
p(wi, dj) =
K?
z=1
p(z)p(wi|z)p(dj |z) (4)
by introducing two K ?K diagonal scaling matrices
X and Y, such that Xkk = ?iWik and Ykk =?
jHkj . The factorization WH can then be rewritten
as
WH = (WX?1X)(YY?1H)
= (WX?1)(XY)(Y?1H)
(5)
such that WX?1 represents p(wi|z), (Y?1H)T rep-
resents p(dj |z), and XY represents p(z). Using
Bayes? theorem, it is now straightforward to deter-
mine p(z|wi) and p(z|dj).
p(z|wi) =
p(wi|z)p(z)
p(wi)
(6)
p(z|dj) =
p(dj |z)p(z)
p(dj)
(7)
3.3 Meaning in Context
3.3.1 Overview
Using the results of the factorization model de-
scribed above, we can now adapt a word?s feature vec-
tor according to the context in which it appears. Intu-
itively, the contextual features of the word (i.e. the
window-based context words or dependency-based
context features) pinpoint the important semantic di-
mensions of the particular instance, creating a proba-
bility distribution over latent factors. For a number of
context words of a particular instance, we determine
the probability distribution over latent factors given
the context, p(z|C), as the average of the context
words? probability distributions over latent factors
(equation 8).
p(z|C) =
?
wi?C p(z|wi)
|C| (8)
The probability distribution over latent factors
given a number of dependency-based context features
can be computed in a similar fashion, replacing wi
with dj . Additionally, this step allows us to combine
both windows-based context words and dependency-
based context features in order to determine the latent
probability distribution (e.g. by taking a linear com-
bination).
The resulting probability distribution over latent
factors can be interpreted as a semantic fingerprint of
the passage in which the target word appears. Using
this fingerprint, we can now determine a new prob-
ability distribution over dependency features given
the context.
p(d|C) = p(z|C)p(d|z) (9)
The last step is to weight the original probability
vector of the word according to the probability vector
of the dependency features given the word?s context,
by taking the pointwise multiplication of probability
vectors p(d|wi) and p(d|C).
p(d|wi, C) = p(d|wi) ? p(d|C) (10)
Note that this final step is a crucial one in our ap-
proach. We do not just build a model based on latent
factors, but we use the latent factors to determine
which of the features in the original word vector are
the salient ones given a particular context. This al-
lows us to compute an accurate adaptation of the
original word vector in context.
Also note the resemblance to Mitchell and Lap-
ata?s best scoring vector composition model which,
likewise, uses pointwise multiplication. However,
1015
the model presented here has two advantages. First
of all, it allows to take multiple context features into
account, each of which contributes to the probability
distribution over latent factors. Secondly, the target
word and its features do not need to live in the same
vector space (i.e. they do not need to be defined ac-
cording to the same features), as the connections and
the appropriate weightings are determined through
the latent model.
3.3.2 Example
Let us exemplify the procedure with an example.
Say we want to compute the distributionally similar
words to the noun record in the context of example
sentences (3) and (4).
(3) Jack is listening to a record.
(4) Jill updated the record.
First, we extract the context features for both in-
stances, in this case C1 = {listen?1prep(to)} for sen-
tence (3), and C2 = {update?1obj} for sentence (4).1Next, we compute p(z|C1) and p(z|C2) ? the proba-
bility distributions over latent factors given the con-
text ? by averaging over the latent probability dis-
tributions of the individual context features.2 Using
these probability distributions over latent factors, we
can now determine the probability of each depen-
dency feature given the different contexts ? p(d|C1)
and p(d|C2).
The former step yields a general probability dis-
tribution over dependency features that tells us how
likely a particular dependency feature is given the
context that our target word appears in. Our last step
is now to weight the original probability vector of
the target word (the aggregate of dependency-based
context features over all contexts of the target word)
according to the new distribution given the context
in which the target word appears. For the first sen-
tence, features associated with the music sense of
record (or more specifically, the dependency features
associated with latent factors that are related to the
feature {listen?1prep(to)}) will be emphasized, while
1In this example we use dependency features, but the compu-
tations are similar for window-based context words.
2In this case, the sets of context features contain only one
item, so the average probability distribution of the sets is just the
latent probability distribution of their respective item.
features associated with unrelated latent factors are
leveled out. For the second sentence, features that
are associated with the administrative sense of record
(dependency features associated with latent factors
that are related to the feature {update?1obj}) are em-phasized, while unrelated features are played down.
We can now return to our original matrix A and
compute the top similar words for the two adapted
vectors of record given the different contexts, which
yields the results presented below.
1. recordN , C1: album, song, recording, track, cd
2. recordN , C2: file, datum, document, database,
list
4 Evaluation
In this section, we present a thorough evaluation of
the method described above, and compare it with
related methods for meaning computation in context.
In order to test the applicability of the method to
multiple languages, we present evaluation results for
both English and French.
4.1 Datasets
For English, we make use of the SEMEVAL 2007 En-
glish Lexical Substitution task (McCarthy and Nav-
igli, 2007; McCarthy and Navigli, 2009). The task?s
goal is to find suitable substitutes for a target word in
a particular context. The complete data set contains
200 target words (about 50 for each part of speech,
viz. nouns, verbs, adjectives, and adverbs). Each
target word occurs in 10 different sentences, which
yields a total of 2000 sentences. Five annotators pro-
vided suitable substitutes for each target word in the
different contexts.
For French, we developed a small-scale lexical sub-
stitution task ourselves, closely following the guide-
lines of the original English task. We manually se-
lected 10 ambiguous French nouns, and for each noun
we selected 10 different sentences from the FRWaC
corpus (Baroni et al, 2009). Four different native
French speakers were then asked to provide suitable
substitutes for the nouns in context.3
3The task is provided as supplementary material to this paper;
it is also available from the first author?s website.
1016
4.2 Implementational details
The model for English has been trained on part of the
UKWaC corpus (Baroni et al, 2009), covering about
500M words. The corpus has been part of speech
tagged and lemmatized with Stanford Part-Of-Speech
Tagger (Toutanova and Manning, 2000; Toutanova
et al, 2003), and parsed with MaltParser (Nivre et
al., 2006) trained on sections 2-21 of the Wall Street
Journal section of the Penn Treebank extended with
about 4000 questions from the QuestionBank4, so
that dependency triples could be extracted. The sen-
tences of the English lexical substitution task have
been tagged, lemmatized and parsed in the same way.
The model for French has been trained on the French
version of Wikipedia (? 100M words), parsed with
the FRMG parser (Villemonte de La Clergerie, 2010)
for French.
For English, we built different models for each
part of speech (nouns, verbs, adjectives and adverbs),
which yields four models in total. For each model, the
matrices needed for our interleaved NMF factoriza-
tion are extracted from the corpus. The noun model,
for example, was built using 5K nouns, 80K depen-
dency relations, and 2K context words5 (excluding
stop words) with highest frequency in the training
set, which yields matrices of 5K nouns ? 80K de-
pendency relations, 5K nouns ? 2K context words,
and 80K dependency relations ? 2K context words.
The models for the three other parts of speech were
constructed in a similar vein. For French, we only
constructed a model for nouns, as our lexical substi-
tution task for French is limited to this part of speech.
The interleaved NMF model was carried out using
K = 600 (the number of factorized dimensions in
the model), and applying 100 iterations.6 The inter-
leaved NMF algorithm was implemented in Matlab;
the preprocessing scripts and scripts for vector com-
putation in context were written in Python. Cosine
was used as a similarity measure.
4http://maltparser.org/mco/english_
parser/engmalt.html
5We used a fairly large, paragraph-like window of four sen-
tences.
6We experimented with different values (in the range 300?
1500) for K, but the models did not seem to improve much
beyond K = 600; hence, we stuck with 600 factors, due to
speed and memory advantages of a lower number of factors.
4.3 Measures
Up till now, most researchers have interpreted the
lexical substitution task as a ranking problem, in
which the possible substitutes are given beforehand
and the goal is to rank the substitutes according to
their suitability in a particular context, so that sound
substitutes are given a higher rank than their non-
suitable counterparts. This means that all possible
substitutes for a given target word (extracted from the
gold standard) are lumped together, and the system
then has to produce a ranking for the complete set of
substitutes.
We also adopt this approach in our evaluation
framework, but we complement it with the original
evaluation measures of the lexical substitution task,
in which the system is not given a list of possible sub-
stitutes beforehand, but has to come up with the suit-
able candidates itself. This is a much harder task, but
we believe that such an approach is more compelling
in assessing the system?s ability to induce a proper
meaning representation for word usage in context.
We coin the former approach paraphrase ranking,
and the latter one paraphrase induction. In the next
paragraphs, we will describe the actual evaluation
measures that have been used for both approaches.
Paraphrase ranking Following Dinu and Lapata
(2010), we compare the ranking produced by our
model with the gold standard ranking using Kendall?s
?b (which is adjusted for ties). For reasons of com-
parison, we also compute general average precision
(GAP, Kishida (2005)), which was used by Erk and
Pado? (2010) and Thater et al (2010) to evaluate their
rankings. Differences between models are tested for
significance using stratified shuffling (Yeh, 2000),
using a standard number of 10000 iterations.
We compare the results for paraphrase ranking to
two different baselines. The first baseline is a ran-
dom one, in which the gold standard is compared
to an arbitrary ranking. The second baseline is a
dependency-based vector space model that does not
take the context of the particular instance into ac-
count (and thus returns the same ranking for each
instance of the target word). This is a fairly competi-
tive baseline, as noted by other researchers (Erk and
Pado?, 2008; Thater et al, 2009; Dinu and Lapata,
2010).
1017
Paraphrase induction To evaluate the system?s
ability to come up with suitable substitutes from
scratch, we use the measures designed to evaluate
systems that took part in the original English lexical
substitution task (McCarthy and Navigli, 2007). Two
different measures were used, which were coined
best and out-of-ten (oot). The strict best measure
allows the system to give as many candidate substi-
tutes as it considers appropriate, but the credit for
each correct substitute is divided by the total number
of guesses. Recall is then calculated as the average
annotator response frequency of substitutes found by
the system over all items T.
Rbest =
?
s?M?G f(s)
|M | ??s?G f(s)
(11)
whereM is the system?s candidate list7,G is the gold-
standard data, and f(s) is the annotator response
frequency of the candidate.
The out-of-ten measure is more liberal; it allows
the system to give up to ten substitutes, and the credit
for each correct substitute is not divided by the total
number of guesses. The more liberal measure was
introduced to account for the fact that the lexical
substitution task?s gold standard is susceptible to a
considerate amount of variation, and there is only a
limited number of annotators.
P10 =
?
s?M?G f(s)?
s?G f(s)
(12)
where M is the system?s list of 10 candidates, and
G and f(s) are the same as above. Because we only
use the best guess with Rbest, the two measures are
exactly the same except for the number of candidates
M .
4.4 Results
4.4.1 English
Table 1 presents the paraphrase ranking results of
our approach, comparing them to the two baselines
and to a number of previous approaches to meaning
computation in context.
The first two models represent our baselines. The
first baseline is the random baseline, where the can-
didate substitutes are ranked randomly (?b close to
7In our evaluations, we calculate best using the system?s best
guess only, so the candidate list contains only one item.
model ?b GAP
random -0.61 29.98
vectordep 16.57 45.08
EP09 ? 32.2 H
EP10 ? 39.9 H
TFP ? 45.94H
DL 16.56 41.68
NMFcontext 20.64?? 47.60??
NMFdep 22.49?? 48.97??
NMFc+d 22.59?? 49.02??
Table 1: Kendall?s ?b and GAP paraphrase ranking scores
for the English lexical substitution task. Scores marked
with ?H? are copied from the authors? respective papers.
Scores marked with ???? are statistically significant with
p < 0.01 compared to the second baseline.
zero indicates that there is no correlation). The sec-
ond baseline is a standard dependency-based vector
space model, which yields the same ranking for all
instances of a target word. Note that the second base-
line is a rather competitive one.
The next four models represent previous ap-
proaches to meaning computation in context. EP09
is Erk and Pado?s (2009) selectional preference ap-
proach; EP10 is Erk and Pado?s (2010) exemplar-
based approach; TFP stands for Thater et al?s (2010)
approach; and DL is Dinu and Lapata?s (2010) latent
modeling approach. The results are reproduced from
their respective papers, except for Dinu and Lapata?s
approach, which we reimplemented ourselves.8 Note
that the reproduced results (EP09, EP10 and TFP) are
not entirely comparable, because the authors only use
a subset of the lexical substitution task.
The last three models are instantiations of our ap-
proach: NMFcontext is a model that uses window-
based context features, NMFdep is a model that uses
dependency-based context features, and NMFc+d is
a model that uses a linear combination of window-
based and dependency-based context features, giving
equal weight to both.
The three instantiations of our approach reach bet-
ter results than all previous approaches. Moreover,
our approach is the only one able to significantly
8The original paper reports a slightly lower ?b of 16.01 for
their best scoring model.
1018
beat our second (competitive) baseline of a stan-
dard dependency-based vector model. Comparing
our three instantiations, the model that combines
window-based context and dependency-based con-
text scores best, closely followed by the dependency-
based model. The model that only uses window-
based context gets the lowest score of the three, but
is still fairly competitive compared to the previous
approaches. The differences between the models are
statistically significant (p < 0.01), except for the
difference between NMFdep and NMFc+d.
model n v a r
vectordep 15.85 11.68 16.71 25.29
NMFcontext 20.58 16.24 21.00 27.22
NMFdep 21.96 17.33 24.57 28.16
NMFc+d 22.68 17.47 23.84 28.66
Table 2: Kendall?s ?b paraphrase ranking scores for the
English lexical substitution task across different parts of
speech
Table 2 shows the performance of the three model
instantiations on paraphrase ranking across different
parts of speech. The results largely confirm tenden-
cies reported by other researchers (cfr. Dinu and
Lapata (2010)), viz. that verbs are the most difficult,
followed by nouns and adjectives. These parts of
speech also benefit the most from the use of a contex-
tualized model. Adverbs are easier, but there is less
to be gained from using contextualized models.
model Rbest P10
vectordep 8.78 30.21
DL 1.06 7.59
KU 20.65 46.15
IRST2 20.33 68.90
NMFcontext 8.81 30.49
NMFdep 7.73 26.92
NMFc+d 8.96 29.26
Table 3: Rbest and P10 paraphrase induction scores for
the English lexical substitution task
Table 3 shows the performance of the different
models on the paraphrase induction task. Note
once again that our baseline vectordep ? a simple
dependency-based vector space model ? is a highly
competitive one. NMFcontext and NMFc+d are able to
reach marginally better results, but the differences are
not statistically significant. However, all of our mod-
els are able to reach much better results than Dinu
and Lapata?s approach. The results indicate that our
approach, after vector adaptation in context, is still
able to provide accurate similarity calculations across
the complete word space. While other algorithms are
able to rank candidate substitutes at the expense of
accurate similarity calculations, our approach is able
to do both. This is one of the important advantages
of our approach.
For reasons of comparison, we also included the
scores of the best performing models that partici-
pated in the SEMEVAL 2007 lexical substitution task
(KU (Yuret, 2007) and IRST2 (Giuliano et al, 2007),
which got the best scores for Rbest and P10, respec-
tively). These models reach better scores compared
to our models. Note, however, that all participants
of the SEMEVAL 2007 lexical substitution task relied
on a predefined sense inventory (i.e. WordNet, or
a machine readable thesaurus). Our system, on the
other hand, induces paraphrases in a fully unsuper-
vised way. To our knowledge, this is the first time a
fully unsupervised system is tested on the paraphrase
induction task.
model n v a r
vectordep 31.66 23.53 29.91 38.43
NMFcontext 33.73?? 25.21? 28.58 36.45
NMFdep 31.40 25.97?? 20.56 31.48
NMFc+d 33.37? 25.99?? 24.20 35.81
Table 4: P10 paraphrase induction scores for the English
lexical substitution task across different parts of speech.
Scores marked with ???? and ??? are statistically significant
with respectively p < 0.01 and p < 0.05 compared to the
baseline.
Table 4 presents the results for paraphrase induc-
tion (oot) across the different parts of speech. The
results indicate that paraphrase induction works best
for nouns and verbs, with statistically significant im-
provements over the baseline. The differences among
the models themselves are not significant. Adjectives
and adverbs yield lower scores, indicating that their
1019
contextualization yields less precise vectors for mean-
ing computation. Note, however, that the NMFcontext
model is still quite apt for meaning computation,
yielding results that are only slightly lower than the
dependency-based vector space model.
4.4.2 French
This section presents the results on the French lex-
ical substitution task. Table 5 presents the results for
paraphrase ranking, while table 6 shows the models?
performance on the paraphrase induction task.
model Kendall?s ?b GAP
vectordep 7.79 36.46
DL 17.99 41.73
NMFcontext 18.63 44.96
NMFdep 17.15 44.66
NMFc+d 18.40 43.14
Table 5: Kendall?s ?b and GAP paraphrase ranking scores
for the French lexical substitution task
The results for paraphrase ranking in French (ta-
ble 5) show similar tendencies as the results for En-
glish: all of our models are able to improve signifi-
cantly over the dependency-based vector space base-
line. Note, however, thar our models generally score
a bit lower compared to the English results. This drop
in performance is not present for Dinu and Lapata?s
model. The difference might be due to the differ-
ence in corpora size: for the method to operate at full
power, we need to make a good estimate of the co-
occurrences of three modes (words, window-based
context words and dependency-based features), and
thus our methods requires a significant amount of
data. Nevertheless, our approach still yields the best
results, with NMFcontext as the best scoring model.
Finally, the results for paraphrase induction in
French (table 6) interestingly show a significant and
large improvement over the baseline. The improve-
ments indicate once again that the models are able
to carry out precise similarity computations over the
whole word space, while at the same time providing
an adequately adapted contextualized meaning vector.
Dinu and Lapata?s model, which performs similarity
calculations in the latent space, is not able to provide
accurate word vectors, and thus perform worse at the
paraphrase induction task.
model Rbest P10
vectordep 6.38 24.43
DL 0.50 5.34
NMFcontext 10.71 31.42
NMFdep 9.65 28.52
NMFc+d 10.64 35.32
Table 6: Rbest and P10 paraphrase induction scores for
the French lexical substitution task
5 Conclusion
In this paper, we presented a novel method for the
modeling of word meaning in context. We make use
of a factorization model based on non-negative ma-
trix factorization, in which words, together with their
window-based context words and their dependency
relations, are linked to latent dimensions. The factor-
ization model allows us to determine which particular
dimensions are important for a target word in a partic-
ular context. A key feature of the algorithm is that we
adapt the original dependency-based feature vector
of the target word through the latent semantic space.
By doing so, our model is able to make accurate simi-
larity calculations for word meaning in context across
the whole word space. Our evaluation shows that the
approach presented here is able to improve upon the
state-of-the art performance on paraphrase ranking.
Moreover, our approach scores well for both para-
phrase ranking and paraphrase induction, whereas
previous approaches only seem capable of improving
performance on the former task at the expense of the
latter.
During our research, a number of topics surfaced
that we consider worth exploring in the future. First
of all, we would like to further investigate the opti-
mal configuration for combining window-based and
dependency-based contexts. At the moment, the per-
formance of the combined model does not yield a
uniform picture. The results might improve further
if window-based context and dependency-based con-
text are combined in an optimal way. Secondly, we
would like to subject our approach to further evalu-
ation, in particular on a number of different evalua-
tion tasks, such as semantic compositionality. And
thirdly, we would like to transfer the general idea
of the approach presented in this paper to a tensor-
1020
based framework (which is able to capture the multi-
way co-occurrences of words, together with their
window-based and dependency-based context fea-
tures, in a natural way) and investigate whether such
a framework proves beneficial for the modeling of
word meaning in context.
Acknowledgements
The work reported in this paper was funded by
the Isaac Newton Trust (Cambridge, UK), the
EU FP7 project ?PANACEA?, the EPSRC grant
EP/G051070/1 and the Royal Society (UK).
References
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle, and
Aitor Soroa. 2006. Two graph-based algorithms for
state-of-the-art wsd. In Proceedings of the Empirical
Methods in Natural Language Processing (EMNLP)
Conference, pages 585?593, Sydney, Australia.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evaluation,
43(3):209?226.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 440?449, Suntec, Singapore.
Katrin Erk and Sebastian Pado?. 2008. A structured vector
space model for word meaning in context. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 897?906, Waikiki,
Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics, pages
57?65, Athens, Greece.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings of
the ACL 2010 Conference Short Papers, pages 92?97,
Uppsala, Sweden.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009.
Investigations on word senses and word usages. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 10?18.
Katrin Erk. 2010. What is word meaning, really? (and
how can distributional models help us describe it?). In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 17?26.
Eric Gaussier and Cyril Goutte. 2005. Relation between
PLSA and NMF and implications. In Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 601?602, Salvador, Brazil.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings of
the Fourth International Workshop on Semantic Evalu-
ations, pages 145?148.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
57?60, New York, New York, USA.
Nancy Ide and Yorick Wilks. 2006. Making Sense About
Sense. In Word Sense Disambiguation: Algorithms
And Applications, chapter 3. Springer, Dordrecht.
Kazuaki Kishida. 2005. Property of average precision and
its generalization: An examination of evaluation indi-
cator for information retrieval experiments. Technical
report, National Institute of Informatics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211?240.
Thomas Landauer, Peter Foltz, and Darrell Laham. 1998.
An Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:295?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms
for non-negative matrix factorization. In Advances in
Neural Information Processing Systems 13, pages 556?
562.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics (COLING-ACL98), Volume 2, pages 768?
774, Montreal, Quebec, Canada.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In
1021
Proceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 48?53.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language resources and
evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216?
2219.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 613?619, Edmonton, Alberta, Canada.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009.
Ranking paraphrases in context. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 948?957, Uppsala, Sweden.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceedings
of HLT-NAACL 2003, pages 252?259.
Tim Van de Cruys. 2008. Using three way data for word
sense discrimination. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 929?936, Manchester.
Eric Villemonte de La Clergerie. 2010. Building factor-
ized TAGs with meta-grammars. In Proceedings of
the 10th International Conference on Tree Adjoining
Grammars and Related Formalisms (TAG+10), pages
111?118, New Haven, Connecticut, USA.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947?953, Saarbru?cken, Germany.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the Fourth International
Workshop on Semantic Evaluations, pages 207?213.
1022
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023?1033,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Hierarchical Verb Clustering Using Graph Factorization
Lin Sun and Anna Korhonen
University of Cambridge, Computer Laboratory
15 JJ Thomson Avenue, Cambridge CB3 0GD, UK
ls418,alk23@cl.cam.ac.uk
Abstract
Most previous research on verb clustering has
focussed on acquiring flat classifications from
corpus data, although many manually built
classifications are taxonomic in nature. Also
Natural Language Processing (NLP) applica-
tions benefit from taxonomic classifications
because they vary in terms of the granularity
they require from a classification. We intro-
duce a new clustering method called Hierar-
chical Graph Factorization Clustering (HGFC)
and extend it so that it is optimal for the task.
Our results show that HGFC outperforms the
frequently used agglomerative clustering on a
hierarchical test set extracted from VerbNet,
and that it yields state-of-the-art performance
also on a flat test set. We demonstrate how the
method can be used to acquire novel classifi-
cations as well as to extend existing ones on
the basis of some prior knowledge about the
classification.
1 Introduction
A variety of verb classifications have been built to
support NLP tasks. These include syntactic and se-
mantic classifications, as well as ones which in-
tegrate aspects of both (Grishman et al, 1994;
Miller, 1995; Baker et al, 1998; Palmer et al, 2005;
Kipper, 2005; Hovy et al, 2006). Classifications
which integrate a wide range of linguistic proper-
ties can be particularly useful for tasks suffering
from data sparseness. One such classification is
the taxonomy of English verbs proposed by Levin
(1993) which is based on shared (morpho-)syntactic
and semantic properties of verbs. Levin?s taxon-
omy or its extended version in VerbNet (Kipper,
2005) has proved helpful for various NLP applica-
tion tasks, including e.g. parsing, word sense disam-
biguation, semantic role labeling, information ex-
traction, question-answering, and machine transla-
tion (Swier and Stevenson, 2004; Dang, 2004; Shi
and Mihalcea, 2005; Zapirain et al, 2008).
Because verbs change their meaning and be-
haviour across domains, it is important to be able
to tune existing classifications as well to build novel
ones in a cost-effective manner, when required. In
recent years, a variety of approaches have been pro-
posed for automatic induction of Levin style classes
from corpus data which could be used for this pur-
pose (Schulte im Walde, 2006; Joanis et al, 2008;
Sun et al, 2008; Li and Brew, 2008; Korhonen
et al, 2008; O? Se?aghdha and Copestake, 2008; Vla-
chos et al, 2009). The best of such approaches
have yielded promising results. However, they have
mostly focussed on acquiring and evaluating flat
classifications. Levin?s classification is not flat, but
taxonomic in nature, which is practical for NLP pur-
poses since applications differ in terms of the gran-
ularity they require from a classification.
In this paper, we experiment with hierarchical
Levin-style clustering. We adopt as our baseline
method a well-known hierarchical method ? ag-
glomerative clustering (AGG) ? which has been pre-
viously used to acquire flat Levin-style classifica-
tions (Stevenson and Joanis, 2003) as well as hierar-
chical verb classifications not based on Levin (Fer-
rer, 2004; Schulte im Walde, 2008). The method has
also been popular in the related task of noun clus-
1023
tering (Ushioda, 1996; Matsuo et al, 2006; Bassiou
and Kotropoulos, 2011).
We introduce then a new method called Hierar-
chical Graph Factorization Clustering (HGFC) (Yu
et al, 2006). This graph-based, probabilistic cluster-
ing algorithm has some clear advantages over AGG
(e.g. it delays the decision on a verb?s cluster mem-
bership at any level until a full graph is available,
minimising the problem of error propagation) and it
has been shown to perform better than several other
hierarchical clustering methods in recent compar-
isons (Yu et al, 2006). The method has been applied
to the identification of social network communities
(Lin et al, 2008), but has not been used (to the best
of our knowledge) in NLP before.
We modify HGFC with a new tree extraction al-
gorithm which ensures a more consistent result, and
we propose two novel extensions to it. The first is a
method for automatically determining the tree struc-
ture (i.e. number of clusters to be produced for each
level of the hierarchy). This avoids the need to pre-
determine the number of clusters manually. The sec-
ond is addition of soft constraints to guide the clus-
tering performance (Vlachos et al, 2009). This is
useful for situations where a partial (e.g. a flat) verb
classification is available and the goal is to extend it.
Adopting a set of lexical and syntactic features
which have performed well in previous works, we
compare the performance of the two methods on test
sets extracted from Levin and VerbNet. When eval-
uated on a flat clustering task, HGFC outperforms
AGG and performs very similarly with the best flat
clustering method reported on the same test set (Sun
and Korhonen, 2009). When evaluated on a hierar-
chical task, HGFC performs considerably better than
AGG at all levels of gold standard classification. The
constrained version of HGFC performs the best, as
expected, demonstrating the usefulness of soft con-
straints for extending partial classifications.
Our qualitative analysis shows that HGFC is ca-
pable of detecting novel information not included in
our gold standards. The unconstrained version can
be used to acquire novel classifications from scratch
while the constrained version can be used to extend
existing ones with additional class members, classes
and levels of hierarchy.
2 Target classification and test sets
The taxonomy of Levin (1993) groups English verbs
(e.g. break, fracture, rip) into classes (e.g. 45.1
Break verbs) on the basis of their shared mean-
ing components and (morpho-)syntactic behaviour,
defined in terms of diathesis alternations (e.g. the
causative/inchoative alternation, where an NP frame
alternates with an intransitive frame: Tony broke the
window ? The window broke). It classifies over
3000 verbs in 57 top level classes, some of which
divide further into subclasses. The extended version
of the taxonomy in VerbNet (Kipper, 2005) classifies
5757 verbs. Its 5 level taxonomy includes 101 top
level and 369 subclasses. We used three gold stan-
dards (and corresponding test sets) extracted from
these resources in our experiments:
T1: The first gold standard is a flat gold standard
which includes 13 classes appearing in Levin?s orig-
inal taxonomy (Stevenson and Joanis, 2003). We in-
cluded this small gold standard in our experiments
so that we could compare the flat version of our
method against previously published methods. Fol-
lowing Stevenson and Joanis (2003), we selected 20
verbs from each class which occur at least 100 times
in our corpus. This gave us 260 verbs in total.
T2: The second gold standard is a large, hi-
erarchical gold standard which we extracted from
VerbNet as follows: 1) We removed all the verbs
that have less than 1000 occurrences in our cor-
pus. 2) In order to minimise the problem of pol-
ysemy, we assigned each verb to the class which,
according to VerbNet, corresponds to its predomi-
nant sense in WordNet (Miller, 1995). 3) In order
to minimise the sparse data problem with very fine-
grained classes, we converted the resulting classifi-
cation into a 3-level representation so that the classes
at the 4th and 5th level were combined. For exam-
ple, the sub-classes of Declare verbs (numbered as
29.4.1.1.{1,2,3}) were combined into 29.4.1. 4) The
classes that have fewer than 5 members were dis-
carded. The total number of verb senses in the re-
sulting gold standard is 1750, which is 33.2% of the
verbs in VerbNet. T2 has 51 top level, 117 second
level, and 133 third level classes.
T3: The third gold standard is a subset of T2
where singular classes (top level classes which do
not divide into subclasses) are removed. This gold
1024
standard was constructed to enable proper evalua-
tion of the constrained version of HGFC (introduced
in the following section) where we want to com-
pare the impact of constraints across several levels
of classification. T3 provides classification of 357
verbs into 11 top level, 14 second level, and 32 third
level classes.
For each verb appearing in T1-T3, we extracted
all the occurrences (up to 10,000) from the British
National Corpus (Leech, 1992) and North American
News Text Corpus (Graff, 1995).
3 Method
3.1 Features and feature extraction
Previous works on Levin style verb classification
have investigated optimal features for this task
(Stevenson and Joanis, 2003; Li and Brew, 2008;
Sun and Korhonen, 2009)). We adopt for our exper-
iments a set of features which have performed well
in recent verb clustering works:
A: Subcategorization frames (SCFs) and their rela-
tive frequencies with individual verbs.
B: A with SCFs parameterized for prepositions.
C: B with SCFs parameterized for subjects appear-
ing in grammatical relations associated with the
verb in parsed data.
D: B with SCFs parameterized for objects appear-
ing in grammatical relations associated with the
verb in parsed data.
These features are purely syntactic. Although
semantic features ? verb selectional preferences ?
proved the best (when used in combination with syn-
tactic features) in the recent work of Sun and Ko-
rhonen (2009), we left such features for future work
because we noticed that different levels of classifi-
cation are likely to require semantic features at dif-
ferent granularities.
We extracted the syntactic features using the sys-
tem of Preiss et al (2007). The system tags, lemma-
tizes and parses corpus data using the RASP (Robust
Accurate Statistical Parsing toolkit (Briscoe et al,
2006)), and on the basis of the resulting grammat-
ical relations, assigns each occurrence of a verb as
a member of one of the 168 verbal SCFs. We pa-
rameterized the SCFs as described above using the
information provided by the system.
3.2 Clustering
We introduce the agglomerative clustering (AGG)
and Hierarchical Graph Factorization Clustering
(HGFC) methods in the following two subsec-
tions, respectively. The subsequent two subsections
present our extensions to HGFC: (i) automatically
determining the cluster structure and (ii) adding soft
constraints to guide clustering performance.
3.2.1 Agglomerative clustering
AGG is a method which treats each verb as a
singleton cluster and then successively merges two
closest clusters until all the clusters have been
merged into one. We used the SciPy?s imple-
mentation (Oliphant, 2007) of the algorithm. The
cluster distance is measured using linkage criteria.
We experimented with four commonly used link-
age criteria: Single, Average, Complete and Ward?s
(Ward Jr., 1963). Ward?s criterion performed the
best and was used in all the experiments in this pa-
per. It measures the increase in variance after two
clusters are merged. The output of AGG tends to
have excessive number of levels. Cut-based meth-
ods (Wu and Leahy, 1993; Shi and Malik, 2000) are
frequently applied to extract a simplified view. We
followed previous verb clustering works and cut the
AGG hierarchy manually.
AGG suffers from two problems. The first is er-
ror propagation. When a verb is misclassified at a
lower level, the error propagates to all the upper lev-
els. The second is local pairwise merging, i.e. the
fact that only two clusters can be combined at any
level. For example, in order to group clusters rep-
resenting Levin classes 9.1, 9.2 and 9.3 into a sin-
gle cluster representing class 9, the method has to
produce intermediate clusters, e.g. 9.{1,2} and 9.3.
Such clusters do not always have a semantic inter-
pretation. Although they can be removed using a
cut-based method, this requires a pre-defined cut-off
value which is difficult to set (Stevenson and Joanis,
2003). In addition, a significant amount of informa-
tion is lost in pair-wise clustering. In the above ex-
ample, only the clusters 9.{1,2} and 9.3 are consid-
ered, while alternative clusters 9.{1,3} and 9.2 are
ignored. Ideally, information about all the possible
intermediate clusters should be aggregated, but this
is intractable in practice.
1025
3.2.2 Hierarchical Graph Factorization
Clustering
Our new method HGFC derives a probabilistic bi-
partite graph from the similarity matrix (Yu et al,
2006). The local and global clustering structures are
learned via the random walk properties of the graph.
The method does not suffer from the above prob-
lems with AGG. Firstly, there is no error propagation
because the decision on a verb?s membership at any
level is delayed until the full bipartite graph is avail-
able and until a tree structure can be extracted from
it by aggregating probabilistic information from all
the levels. Secondly, the bipartite graph enables the
construction of a hierarchical structure without any
intermediate classes. For example, we can group
classes 9.{1,2,3} directly into class 9.
We use HGFC with the distributional similarity
measure Jensen-Shannon Divergence (djs(v, v?)).
Given a set of verbs, V = {vn}Nn=1, we
compute a similarity matrix W where Wij =
exp(?djs(v1, v2)). W can be encoded by a undi-
rected graph G (Figure 1(a)), where the verbs are
mapped to vertices and the Wij is the edge weight
between vertices i and j.
The graph G and the cluster structure can be rep-
resented by a bipartite graph K(V,U). V are the
vertices onG. U = {up}mp=1 represent the hiddenm
clusters. For example, looking at Figure 1(b), V on
G can be grouped into three clusters u1, u2 and u3.
The matrix B denotes the n ?m adjacency matrix,
with bip being the connection weight between the
vertex vi and the cluster up. Thus, B represents the
connections between clusters at an upper and lower
level of clustering. A flat clustering algorithm can
be induced by computing B.
The bipartite graph K also induces a similarity
(W ?) between vi and vj : w?ij =
?m
p=1
bipbjp
?p =
(B??1BT )ij where ? = diag(?1, ..., ?m). There-
fore,B can be found by approximating the similarity
matrix W of G using W ? derived from K. Given a
distance function ? between two similarity matrices,
B approximates W by minimizing the cost function
?(W,B??1BT ). The coupling between B and ? is
removed by setting H = B??1:
min
H,?
?(W,H?HT ), s.t.
n?
i=1
hip = 1 (1)
We use the divergence distance: ?(X,Y ) =?
ij(xij log
xij
yij ?xij+yij). Yu et al (2006) showedthat this cost function is non-increasing under the
update rule:
h?ip ? hip
?
j
wij
(H?HT )ij
?phjp s.t.
?
i
h?ip = 1 (2)
??p ? ?p
?
j
wij
(H?HT )ij
hiphjp s.t.
?
p
??p =
?
ij
wij (3)
wij can be interpreted as the probability of the di-
rect transition between vi and vj : wij = p(vi, vj),
when?ij wij = 1. bip can be interpreted as:
p(up, uq) = p(up)p(up|uq) =
n?
i=1
bipbiq
di
= (BTD?1B)pq (4)
D = diag(d1, ..., dn) where di =
m?
p=0
bip
p(up, uq) is the similarity between the clusters. It
takes into account of a weighted average of contri-
butions from all the data. This is different from the
linkage method where only the data from two clus-
ters are considered.
Given the cluster similarity p(up, uq), we can con-
struct a new graphG1 (Figure 1(d)) with the clusters
U as vertices. The cluster algorithm can be applied
again (Figure 1(e)). This process can go on itera-
tively, leading to a hierarchical graph.
Algorithm 1 HGFC algorithm (Yu et al, 2006)
Require: N verbs V , number of clusters ml for L levels
Compute the similarity matrix W0 from V
Build the graph G0 from W0 , and m0 ? n
for l = 1, 2 to L do
FactorizeGl?1 to obtain bipartite graph Kl with the
adjacency matrix Bl (eq. 1, 2 and 3)
Build a graph Gl with similarity matrix Wl =
BTl D?1l Bl according to equation 4end for
return BL, BL?1...B1
Additional steps need to be performed in order to
extract a tree from the hierarchical graph. Yu et al
(2006) performs the extraction via a propagation of
probabilities from the bottom level clusters. For a
verb vi, the probability of assigning it to cluster v(l)p
at level l is given by:
1026
v1
v6
v2
v4
v3
v5
v7 v8v9
(a)
v1
v7
v6
v9
v8
v2v3v4v5
u1
u2
u3
(b)
u3
u1
u2
v1
v6
v2
v4
v3
v5
v7 v8v9
(c)
u1 u2
u3
(d)
v1
v7
v6
v9
v8
v2v3v4v5
u1
u2
u3
q1
q2
(e)
Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters
on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1
p(v(l)p |vi) =
?
Vl?1
...
?
V1
p(v(l)p |v(l?1))...p(v(1)|vi)
= (D(?1)1 B1D?12 B2D?13 B3...D?1l Bl)ip (5)
This method might not extract a consistent tree
structure, because the cluster membership at the
lower level does not constrain the upper level mem-
bership. This prevented us from extracting a Levin
style hierarchical classification in our initial experi-
ments. For example, where two verbs were grouped
together at a lower level, they could belong to sepa-
rate clusters at an upper level. We therefore propose
a new tree extraction algorithm (Algorithm 2).
The new algorithm starts from the top level bipar-
tite graph, and generates consistent labels for each
level by taking into account of the tree constraints
set at upper levels.
Algorithm 2 Tree extraction algorithm for HGFC
Require: Given N , (Bl,ml) on each level for L levels
On the top level L, collect the labels TL (eq. 5)
Define C to be a (mL?1 ?mL) zero matrix, Cij ? 1,
where i, j = arg maxi,j{BLij}for l = L? 1 to 1 do
for i = 1 to N do
Compute p(vlp|vi) for each cluster p (eq. 5)
tli = argmaxp{p(vlp|vi)|p = 1...ml, Cptl+1i 6= 0}end for
Redefine C to be a (ml?1?ml) zero matrix, Cij ?
1, where i, j = arg maxi,j{Blij}end for
return Tree consistent labels TL, TL?1...T 1
3.2.3 Automatically determining the number of
clusters for HGFC
HGFC needs the number of levels and clusters at
each level as input. However, this information is not
always available (e.g. when the goal is to actually
learn this information automatically). We therefore
propose a method for inferring the cluster structure
from data. As shown in figure 1, a similarity ma-
trix W models one-hop transitions that follow the
links from vertices to neighbors. A walker can also
go to other vertices via multi-hop transitions. Ac-
cording to the chain rule of the Markov process, the
multi-hop transitions indicate a decaying similarity
function on the graph (Yu et al, 2006). After t tran-
sitions, the similarity matrix (Wt) becomes:
Wt = Wt?1D?10 W0
Yu et al (2006) proved the correspondence be-
tween the HGFC levels (l) and the random walk time:
t = 2l?1. So the vertices at level l induce a sim-
ilarity matrix of verbs after t-hop transitions. The
decaying similarity function captures the different
scales of clustering structure in the data (Azran and
Ghahramani, 2006b). The upper levels would have
a smaller number of clusters which represent a more
global structure. After several levels, all the verbs
are expected to be grouped into one cluster. The
number of levels and clusters at each level can thus
be learned automatically.
We therefore propose a method that uses the de-
caying similarity function to learn the hierarchical
clustering structure. One simple modification to al-
gorithm 1 is to set the number of clusters at level l
1027
(ml) to be ml?1 ? 1. m is denoted as the number
of clusters that have at least one member according
to eq. 5. We start by treating each verb as a cluster
at the bottom level. The algorithm stops when all
the data points are merged into one cluster. The in-
creasingly decaying similarity causes many clusters
to have 0 members especially at lower levels, which
are pruned in the tree extraction.
3.2.4 Adding constraints to HGFC
The basic version of HGFC makes no prior as-
sumptions about the classification. It is useful
for learning novel verb classifications from scratch.
However, when wishing to extend an existing clas-
sification (e.g. VerbNet) it may be desirable to guide
the clustering performance on the basis of informa-
tion that is already known. We propose a constrained
version of HGFC which makes uses of labels at the
bottom level to learn upper level classifications. We
do this by adding soft constraints to clustering, fol-
lowing Vlachos et al (2009).
We modify the similarity matrix W as follows: If
two verbs have different labels (li 6= lj), the simi-
larity between them is decreased by a factor a, and
a < 1. We set a to 0.5 in the experiments. The re-
sulting tree is generally consistent with the original
classification. The influence of the underlying data
(domain or features) is reduced according to a.
4 Experimental evaluation
We applied the clustering methods introduced in
section 3 to the test sets described in section 2 and
evaluated them both quantitatively and qualitatively,
as described in the subsequent sections.
4.1 Evaluation methods
We used class based accuracy (ACC) and adjusted
rand index (Radj) to evaluate the results on the flat
test set T1 (see section 2 for details of T1-T3).
ACC is the proportion of members of dominant
clusters DOM-CLUSTi within all classes ci.
ACC =
?C
i=1 verbs in DOM-CLUSTi
number of verbs
The formula of Radj is (Hubert and Arabie, 1985):
Radj =
?
i,j
(nij
2
)
??i
(ni?
2
)?
j
(n?j
2
)
/
(n
2
)
1
2 [
?
i
(ni?
2
)
+
?
j
(n?j
2
)
]??i
(ni?
2
)?
j
(n?j
2
)
/
(n
2
)
where nij is the size of the intersection between
class i and cluster j.
We used normalized mutual information (NMI)
and F-Score (F) to evaluate hierarchical clustering
results on T2 and T3. NMI measures the amount of
statistical information shared by two random vari-
ables representing the clustering result and the gold-
standard labels. Given random variables A and B:
NMI(A,B) = I(A;B)[H(A) +H(B)]/2
I(A,B) =
?
k
?
j
|(vk ? cj |
N log
N |vk ? cj |
|vk||cj |
where |vk ? cj | is the number of shared member-
ship between cluster vk and gold-standard class cj .
The normalized variant of mutual information (MI)
enables the comparison of clustering with different
cluster numbers (Manning et al, 2008).
F is the harmonic mean of precision (P) and re-
call (R). P is calculated using modified purity ? a
global measure which evaluates the mean precision
of clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by nprevalent(K).
mPUR =
?
nprevalent(ki)>2
nprevalent(ki)
number of verbs
R is calculated using ACC.
F = 2 ?mPUR ? ACCmPUR + ACC
F is not suitable for comparing results with dif-
ferent cluster numbers (Rosenberg and Hirschberg,
2007). Therefore, we only report NMI when the
number of classes in clustering and gold-standard is
substantially different.
Finally, we supplemented quantitative evaluation
with qualitative evaluation of clusters produced by
different methods.
4.2 Quantitative evaluation
We first evaluated AGG and the basic (uncon-
strained) HGFC on the small flat test set T1. The
main purpose of this evaluation was to compare the
results of our methods against previously published
results on the same test set. The number of clus-
ters (K) and levels (L) were inferred automatically
for HGFC as described in section 3.2.3. However, to
1028
make the results comparable with previously pub-
lished ones, we cut the resulting hierarchy at the
level of closest match (12 clusters) to the K (13) in
the gold-standard. For AGG, we cut the hierarchy at
13 clusters.
Method ACC Radj
HGFC 41.2 17.4
AGG (reproduced) 32.7 9.9
AGG (Stevenson and Joanis (2003) 31.0 9.0
Table 1: Comparison against Stevenson and Joanis
(2003)?s result on T1 (using similar features).
Table 1 shows our results and the results of
Stevenson and Joanis (2003) on T1 when employing
AGG using Ward as the linkage criterion. In this ex-
periment, we used the same feature set as Stevenson
and Joanis (2003) (set B, see section 3.1) and were
therefore able to reproduce their AGG result with a
difference smaller than 2%. When using this simple
feature set, HGFC outperforms the best performing
AGG clearly: 8.5% in ACC and 7.3% in Radj .
We also compared HGFC against the best reported
clustering method on T1 to date ? that of spectral
clustering by Sun and Korhonen (2009). We used
the feature sets C and D which are similar to the
features (SCF parameterized by lexical prefences) in
their experiments. HGFC obtains F of 49.93% on T1
which is 5% lower than the result of Sun and Ko-
rhonen (2009). The difference comes from the tree
consistency requirement. When the HGFC is forced
to produce a flat clustering (a one level tree only), it
achieves the F of 52.55% which is very close to the
performance of spectral clustering.
We then evaluated our methods on the hierarchi-
cal test sets T2 and T3. In the first set of experi-
ments, we pre-defined the tree structure for HGFC
by setting L to 3 and K at each level to be the K
in the hierarchical gold standard. The hierarchy pro-
duced by AGG was cut into 3 levels according to Ks
in the gold standard. This enabled direct evaluation
of the results against the 3 level gold standards using
both NMI and F.
The results are reported in tables 2 and 3. In these
tables, Nc is the number of clusters in HGFC cluster-
ing while Nl is the number of classes in the gold
standard (the two do not always correspond per-
fectly because a few clusters have zero members).
Nc Nl
HGFC
unconstrained
AGG
NMI F NMI F
130 133 57.31 36.65 54.22 32.62
114 117 54.67 37.96 51.35 32.44
50 51 37.75 40.00 32.61 32.78
Table 2: Performance on T2 using a pre-defined tree
structure.
Nc Nl
HGFC
unconstrained
HGFC
constrained
AGG
NMI F NMI F NMI F
31 32 51.65 42.01 91.47 92.07 49.70 40.30
15 14 42.75 47.70 82.16 82.80 39.19 43.69
11 11 38.91 51.17 71.69 75.00 34.88 44.80
Table 3: Performance on T3 using a pre-defined tree
structure.
Table 2 compares the results of the unconstrained
version of HGFC against those of AGG on our largest
test set T2. As with T1, HGFC outperforms AGG
clearly. The benefit can now be seen at 3 different
levels of hierarchy. On average, the HGFC outper-
forms AGG 3.5% in NMI and 4.8% in F. The dif-
ference between the methods becomes clearer when
moving towards the upper levels of the hierarchy.
Table 3 shows the results of both unconstrained
and constrained versions of HGFC and those of
AGG on the test set T3 (where singular classes are
removed to enable proper evaluation of the con-
strained method). The results are generally gener-
ally better on this test set than on T2 ? which is to be
expected since T3 is a refined subset of T21.
Recall that the constrained version of HGFC learns
the upper levels of classification on the basis of soft
constraints set at the bottom level, as described ear-
lier in section 3.2.4. As a consequence, NMI and F
are both greater than 90% at the bottom level and
the results at the top level are notably lower because
the impact of the constraints degrades the further
away one moves from the bottom level. Yet, the rela-
tively high result across all levels shows that the con-
strained version of HGFC can be employed a useful
method to extend the hierarchical structure of known
classifications.
1NMI is higher on T2, however, because NMI has a higher
baseline for larger number of clusters (Vinh et al, 2009). NMI
is not ideal for comparing the results of T2 and T3.
1029
T2 T3
Nc Nl HGFC Nc Nl HGFC
148 133 53.26 64 32 54.91
97 117 49.85 35 32 50.83
46 51 33.55 20 14 44.02
19 51 25.80 10 14 34.41
9 51 19.17 6 11 32.27
3 51 13.06
Table 4: NMI of unconstrained HGFC when trees for T2
and T3 are inferred automatically.
Finally, Table 4 shows the results for the uncon-
strained HGFC on T2 and and T3 when the tree struc-
ture is not pre-defined but inferred automatically as
described in section 3.2.3. 6 levels are learned for
T2 and 5 for T3. The number of clusters produced
ranges from 3 to 148 for T2 and from 6 to 64 for
T3. We can see that the automatically detected clus-
ter numbers distribute evenly across different levels.
The scale of the clustering structure is more com-
plete here than in the gold standards.
In the table, Nc indicates the number of clusters
in the inferred tree, while Nl indicates the closest
match to the number of classes in the gold stan-
dard. This evaluation is not fully reliable because
the match between the gold standard and the cluster-
ing is poor at some levels of hierarchy. However, it
is encouraging to see that the results do not drop dra-
matically until the match between the two is really
poor.
4.3 Qualitative evaluation
To gain a better insight into the performance of
HGFC, we conducted further qualitative analysis of
the clusters the two versions of this method pro-
duced for T3. We focussed on the top level of 11
clusters (in the evaluation against the hierarchical
gold standard, see table 3) as the impact of soft con-
straints is the weakest for the constrained method at
this level.
As expected, the constrained HGFC kept many in-
dividual verbs belonging to same Verbnet subclass
together (e.g. verbs enjoy, hate, disdain, regret, love,
despise, detest, dislike, fear for the class 31.2.1) so
that most clusters simply group lower level classes
and their members together. Three nearly clean clus-
ters were produced which only include sub-classes
of the same class (e.g. 31.2.0 and 31.2.1 which both
belong to 31.2 Admire verbs). However, the remain-
ing 8 clusters group together sub-classes (and their
members) belonging to unrelated parent classes. In-
terestingly, 6 of these make both syntactic and se-
mantic sense. For example, several such 37.7 Say
verbs and 29.5 Conjencture verbs are found together
which share the meaning of communication and
which take similar sentential complements.
In contrast, none of the clusters produced by
the unconstrained HGFC represent a single VerbNet
class. The majority represent a high number of
classes and fewer members per class. Yet many of
the clusters make syntactic and semantic sense. A
good example is a cluster which includes member
verbs from 9.7 Spray/Load verbs, 21.2 Carve verbs,
51.3.1 Roll verbs, and 10.4 Wipe verbs. The verbs
included in this cluster share the meaning of specific
type of motion and show similar syntactic behaviour.
Thorough Levin style investigation of especially
the unconstrained method would require looking at
shared diathesis alternations between cluster mem-
bers. We left this for future work. However,
the analysis we conducted confirmed that the con-
strained method could indeed be used for extend-
ing known classifications, while the unconstrained
method is more suitable for acquiring novel classi-
fications from scratch. The errors in clusters pro-
duced by both methods were mostly due to syntactic
idiosyncracy and the lack of semantic information in
clustering. We plan to address the latter problem in
our future work.
5 Discussion and conclusion
We have introduced a new graph-based method ?
HGFC ? to hierarchical verb clustering which avoids
some of the problems (e.g. error propagation, pair-
wise cluster merging) reported with the frequently
used AGG method. We modified HGFC so that it can
be used to automatically determine the tree struc-
ture for clustering, and proposed two extensions to
it which make it even more suitable for our task. The
first involves automatically determining the number
of clusters to be produced, which is useful when
this is not known in advance. The second involves
adding soft constraints to guide the clustering per-
formance, which is useful when aiming to extend
existing classification.
1030
The results reported in the previous section are
promising. On a flat test set (T1), the unconstrained
version of HGFC outperforms AGG and performs
very similarly with the best current flat clustering
method (spectral clustering) evaluated on the same
dataset. On the hierarchical test sets (T2 and T3),
the unconstrained and constrained versions of HGFC
outperform AGG clearly at all levels of classification.
The constrained version of HGFC detects the missing
hierarchy from the existing gold standards with high
accuracy. When the number of clusters and levels
is learned automatically, the unconstrained method
produces a multi-level hierarchy. Our evaluation
against a 3-level gold standard shows that such a hi-
erarchy is fairly accurate. Finally, the results from
our qualitative evaluation show that both constrained
and unconstrained versions of HGFC are capable of
learning valuable novel information not included in
the gold standards.
The previous work on Levin style verb classifica-
tion has mostly focussed on flat classifications us-
ing methods suitable for flat clustering (Schulte im
Walde, 2006; Joanis et al, 2008; Sun et al, 2008; Li
and Brew, 2008; Korhonen et al, 2008; O? Se?aghdha
and Copestake, 2008; Vlachos et al, 2009). How-
ever, some works have employed hierarchical clus-
tering as a method to infer flat clustering.
For example, Schulte im Walde and Brew (2002)
employed AGG to initialize the KMeans clustering
for German verbs. This gave better results than
random initialization. Stevenson and Joanis (2003)
used AGG for flat clustering on T1. They cut the hi-
erarchy at the number of classes in the gold standard
and found that it is difficult to automatically deter-
mine a good cut-off. Our evaluation in the previous
section shows that HGFC outperforms their imple-
mentation of AGG.
AGG was also used by Ferrer (2004) who per-
formed hierarchical clustering of 514 Spanish verbs.
The results were evaluated against a hierarchical
gold standard resembling that of Levin?s classifi-
cation in English (Va?zquez et al, 2000). Radj of
0.07 was reported for a 15-way classification which
is comparable to the result of Stevenson and Joanis
(2003).
Hierarchical clustering has also been performed
for the related task of semantic verb classification.
For example, Basili et al (1993) identified the prob-
lems of AGG, and applied a conceptual clustering al-
gorithm (Fisher, 1987) to Italian verbs. They used
semi-automatically acquired semantic roles and the
concept types as features. No quantitative results
were reported. The qualitative evaluation shows that
the resulting clusters are very fine-grained.
Schulte im Walde (2008) performed hierarchical
clustering of German verbs using human verb asso-
ciation as features and AGG as a method. They fo-
cussed on two small collections of 56 and 104 verbs
and evaluated the result against flat gold standard
extracted from GermaNet (Kunze and Lemnitzer,
2002) and German FrameNet (Erk et al, 2003), re-
spectively. They reported F of 62.69% for the 56
verbs, and F of 34.68% for the 104 verbs.
In the future, we plan to extend this research line
in several directions. First, we will try to deter-
mine optimal features for different levels of clus-
tering. For example, the general syntactic features
(e.g. SCF) may perform the best at top levels of a hi-
erarchy while more specific or refined features (e.g.
SCF+pp) may be optimal at lower levels. We also
plan to investigate incorporating semantic features,
like verb selectional preferences, in our feature set.
It is likely that different levels of clustering require
more or less specific selectional preferences. One
way to obtain the latter is hierarchical clustering of
relevant noun data.
In addition, we plan to apply the unconstrained
HGFC to specific domains to investigate its capabil-
ity to learn novel, previously unknown classifica-
tions. As for the constrained version of HGFC, we
will conduct a larger scale experiment on the Verb-
Net data to investigate what kind of upper level hi-
erarchy it can propose for this resource (which cur-
rently has over 100 top level classes).
Finally, we plan to compare HGFC to other hier-
archical clustering methods that are relatively new
to NLP but have proved promising in other fields,
including Bayesian Hierarchical Clustering (Heller
and Ghahramani, 2005; Teh et al, 2008) and the
method of Azran and Ghahramani (2006a) based on
spectral clustering.
6 Acknowledgement
Our work was funded by the Royal Society Uni-
versity Research Fellowship (AK), the Dorothy
Hodgkin Postgraduate Award (LS), the EPSRC
1031
grants EP/F030061/1 and EP/G051070/1 (UK) and
the EU FP7 project ?PANACEA?.
References
Arik Azran and Zoubin Ghahramani. A new approach
to data driven clustering. In Proceedings of the 23rd
international conference on Machine learning, ICML
?06, pages 57?64, New York, NY, USA, 2006a. ISBN
1-59593-383-2.
Arik Azran and Zoubin Ghahramani. Spectral methods
for automatic multiscale data clustering. In Proceed-
ings of the 2006 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition-Volume
1, pages 190?197. IEEE Computer Society Washing-
ton, DC, USA, 2006b.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
The berkeley framenet project. In In COLING-ACL,
pages 86?90, 1998.
Roberto. Basili, Maria Teresa Pazienza, and Paola Ve-
lardi. Hierarchical clustering of verbs. In Proceedings
of the Workshop on Acquisition of Lexical Knowledge
from Text, 1993.
Nikoletta Bassiou and Constantine Kotropoulos. Long
distance bigram models applied to word clustering.
Pattern Recogn., 44:145?158, January 2011. ISSN
0031-3203.
Ted Briscoe, John Carroll, and Rebecca Watson. The
second release of the rasp system. In Proceedings
of the COLING/ACL on Interactive presentation ses-
sions, 2006.
Hoa Trang Dang. Investigations into the Role of Lexical
Semantics in Word Sense Disambiguation. PhD thesis,
CIS, University of Pennsylvania, 2004.
Katrin Erk, Andrea Kowalski, Sebastian Pado?, and Man-
fred Pinkal. Towards a resource for lexical semantics:
a large german corpus with extensive semantic anno-
tation. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics - Volume
1, ACL ?03, pages 537?544, Stroudsburg, PA, USA,
2003. Association for Computational Linguistics.
Eva Esteve Ferrer. Towards a semantic classification
of spanish verbs based on subcategorisation informa-
tion. In Proceedings of the ACL 2004 workshop on
Student research, ACLstudent ?04, Stroudsburg, PA,
USA, 2004. Association for Computational Linguis-
tics.
Douglas H. Fisher. Knowledge acquisition via incremen-
tal conceptual clustering. Machine Learning, 2:139?
172, 1987. ISSN 0885-6125.
David Graff. North american news text corpus. Linguistic
Data Consortium, 1995.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
Comlex syntax: Building a computational lexicon. In
COLING, pages 268?272, 1994.
Katherine A. Heller and Zoubin Ghahramani. Bayesian
hierarchical clustering. In Proceedings of the 22nd
international conference on Machine learning, pages
297?304. ACM, 2005. ISBN 1595931805.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. Ontonotes: the
90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, NAACL-Short ?06,
pages 57?60, Stroudsburg, PA, USA, 2006. Associa-
tion for Computational Linguistics.
Lawrence Hubert and Phipps Arabie. Comparing par-
titions. Journal of Classification, 2:193?218, 1985.
ISSN 0176-4268.
Eric Joanis, Suzanne Stevenson, and David James. A
general feature space for automatic verb classification.
Natural Language Engineering, 14(3):337?367, 2008.
Karin Kipper. VerbNet: A broad-coverage, comprehen-
sive verb lexicon. 2005.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
The Choice of Features for Classification of Verbs in
Biomedical Texts. In Proceedings of COLING, 2008.
Claudia Kunze and Lothar Lemnitzer. GermaNet-
representation, visualization, application. In Proceed-
ings of LREC, 2002.
Geoffrey Leech. 100 million words of english: the
british national corpus. Language Research, 28(1):1?
13, 1992.
Beth. Levin. English verb classes and alternations: A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Features
for Automatic Verb Classification. In Proceedings of
ACL, 2008.
Yu-Ru Lin, Yun Chi, Shenghuo Zhu, Hari Sundaram, and
Belle L. Tseng. Facetnet: a framework for analyz-
ing communities and their evolutions in dynamic net-
works. In Proceeding of the 17th international confer-
ence on World Wide Web, pages 685?694, New York,
NY, USA, 2008. ACM.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA,
2008. ISBN 0521865719, 9780521865715.
Yutaka Matsuo, Takeshi Sakaki, Ko?ki Uchiyama, and
Mitsuru Ishizuka. Graph-based word clustering using
a web search engine. In Proceedings of the EMNLP,
pages 542?550, 2006.
1032
George A. Miller. WordNet: a lexical database for En-
glish. Communications of the ACM, 38(11):39?41,
1995.
Travis E. Oliphant. Python for scientific computing.
Computing in Science and Engineering, 9:10?20,
2007. ISSN 1521-9615.
Diarmuid O? Se?aghdha and Ann Copestake. Semantic
classification with distributional kernels. In Proceed-
ings of COLING, 2008.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106, 2005.
Judita Preiss, Ted Briscoe, and Anna Korhonen. A sys-
tem for large-scale acquisition of verbal, nominal and
adjectival subcategorization frames from corpora. In
Proceedings of ACL, pages 912?919, 2007.
Andrew Rosenberg and Julia Hirschberg. V-measure: A
conditional entropy-based external cluster evaluation
measure. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
2007.
Sabine Schulte im Walde. Experiments on the automatic
induction of german semantic verb classes. Computa-
tional Linguistics, 32(2), 2006.
Sabine Schulte im Walde. Human associations and the
choice of features for semantic verb classification.
Research on Language and Computation, 6:79?111,
2008. ISSN 1570-7075.
Sabine Schulte im Walde and Chris Brew. Inducing ger-
man semantic verb classes from purely syntactic sub-
categorisation information. In Proceedings of ACL,
pages 223?230, 2002.
Jianbo Shi and Jitendra Malik. Normalized cuts and im-
age segmentation. IEEE Transactions on pattern anal-
ysis and machine intelligence, 22(8):888?905, 2000.
Lei Shi and Rada Mihalcea. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proceedings of CICLING,
2005.
Suzanne Stevenson and Eric Joanis. Semi-supervised
verb class discovery using noisy features. In Proceed-
ings of HLT-NAACL 2003, pages 71?78, 2003.
Lin Sun and Anna Korhonen. Improving verb clustering
with automatically acquired selectional preferences. In
Proceedings of the EMNLP 2009, 2009.
Lin Sun, Anna Korhonen, and Yuval Krymolowski. Verb
class discovery from rich syntactic data. Lecture Notes
in Computer Science, 4919:16, 2008.
Robert Swier and Suzanne Stevenson. Unsupervised
semantic role labelling. In Proceedings of EMNLP,
pages 95?102, 2004.
Yee Whye Teh, Hal Daume? III, and Daniel Roy. Bayesian
agglomerative clustering with coalescents. In Ad-
vances in Neural Information Processing Systems, vol-
ume 20, 2008.
Akira Ushioda. Hierarchical clustering of words. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 1159?1162. Association
for Computational Linguistics, 1996.
Gloria Va?zquez, Ana Ferna?ndez-Montraveta, and
M. Anto`nia Mart??. Clasificacio?n verbal:(alternancias
de dia?tesis). Universitat de Lleida, 2000. ISBN
8484090671.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. Infor-
mation theoretic measures for clusterings comparison:
is a correction for chance necessary? In ICML ?09:
Proceedings of the 26th Annual International Confer-
ence on Machine Learning, pages 1073?1080, New
York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-
1.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. Unsupervised and constrained dirichlet process
mixture models for verb clustering. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 74?82, 2009.
Joe H. Ward Jr. Hierarchical grouping to optimize an ob-
jective function. Journal of the American statistical as-
sociation, 58(301):236?244, 1963. ISSN 0162-1459.
Zhenyu Wu and Richard Leahy. An optimal graph the-
oretic approach to data clustering: Theory and its ap-
plication to image segmentation. IEEE transactions
on pattern analysis and machine intelligence, pages
1101?1113, 1993. ISSN 0162-8828.
Kai Yu, Shipeng Yu, and Volker Tresp. Soft clustering on
graphs. Advances in Neural Information Processing
Systems, 18:1553, 2006.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. Ro-
bustness and generalization of role sets: PropBank vs.
VerbNet. In Proceedings of ACL-08: HLT, pages 550?
558, 2008.
1033
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1047?1057,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Probabilistic models of similarity in syntactic context
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
United Kingdom
do242@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
United Kingdom
Anna.Korhonen@cl.cam.ac.uk
Abstract
This paper investigates novel methods for in-
corporating syntactic information in proba-
bilistic latent variable models of lexical choice
and contextual similarity. The resulting mod-
els capture the effects of context on the inter-
pretation of a word and in particular its effect
on the appropriateness of replacing that word
with a potentially related one. Evaluating our
techniques on two datasets, we report perfor-
mance above the prior state of the art for esti-
mating sentence similarity and ranking lexical
substitutes.
1 Introduction
Distributional models of lexical semantics, which
assume that aspects of a word?s meaning can be re-
lated to the contexts in which that word is typically
used, have a long history in Natural Language Pro-
cessing (Spa?rck Jones, 1964; Harper, 1965). Such
models still constitute one of the most popular ap-
proaches to lexical semantics, with many proven ap-
plications. Much work in distributional semantics
treats words as non-contextualised units; the models
that are constructed can answer questions such as
?how similar are the words body and corpse?? but
do not capture the way the syntactic context in which
a word appears can affect its interpretation. Re-
cent developments (Mitchell and Lapata, 2008; Erk
and Pado?, 2008; Thater et al, 2010; Grefenstette et
al., 2011) have aimed to address compositionality of
meaning in terms of distributional semantics, lead-
ing to new kinds of questions such as ?how similar
are the usages of the words body and corpse in the
phrase the body/corpse deliberated the motion. . . ??
and ?how similar are the phrases the body deliber-
ated the motion and the corpse rotted??. In this pa-
per we focus on answering questions of the former
type and investigate models that describe the effect
of syntactic context on the meaning of a single word.
The work described in this paper uses probabilis-
tic latent variable models to describe patterns of syn-
tactic interaction, building on the selectional prefer-
ence models of O? Se?aghdha (2010) and Ritter et al
(2010) and the lexical substitution models of Dinu
and Lapata (2010). We propose novel methods for
incorporating information about syntactic context in
models of lexical choice, yielding a probabilistic
analogue to dependency-based models of contextual
similarity. Our models attain state-of-the-art per-
formance on two evaluation datasets: a set of sen-
tence similarity judgements collected by Mitchell
and Lapata (2008) and the dataset of the English
Lexical Substitution Task (McCarthy and Navigli,
2009). In view of the well-established effectiveness
of dependency-based distributional semantics and of
probabilistic frameworks for semantic inference, we
expect that our approach will prove to be of value in
a wide range of application settings.
2 Related work
The literature on distributional semantics is vast; in
this section we focus on outlining the research that is
most directly related to capturing effects of context
and compositionality.1 Mitchell and Lapata (2008)
1The interested reader is referred to Pado? and Lapata (2007)
and Turney and Pantel (2010) for a general overview.
1047
follow Kintsch (2001) in observing that most dis-
tributional approaches to meaning at the phrase or
sentence level assume that the contribution of syn-
tactic structure can be ignored and the meaning of a
phrase is simply the commutative sum of the mean-
ings of its constituent words. As Mitchell and Lap-
ata argue, this assumption clearly leads to an impov-
erished model of semantics. Mitchell and Lapata in-
vestigate a number of simple methods for combining
distributional word vectors, concluding that point-
wise multiplication best corresponds to the effects
of syntactic interaction.
Erk and Pado? (2008) introduce the concept of a
structured vector space in which each word is as-
sociated with a set of selectional preference vec-
tors corresponding to different syntactic dependen-
cies. Thater et al (2010) develop this geometric ap-
proach further using a space of second-order distri-
butional vectors that represent the words typically
co-occurring with the contexts in which a word typi-
cally appears. The primary concern of these authors
is to model the effect of context on word meaning;
the work we present in this paper uses similar intu-
itions in a probabilistic modelling framework.
A parallel strand of research seeks to represent
the meaning of larger compositional structures us-
ing matrix and tensor algebra (Smolensky, 1990;
Rudolph and Giesbrecht, 2010; Baroni and Zampar-
elli, 2010; Grefenstette et al, 2011). This nascent
approach holds the promise of providing a much
richer notion of context than is currently exploited
in semantic applications.
Probabilistic latent variable frameworks for gen-
eralising about contextual behaviour (in the form
of verb-noun selectional preferences) were proposed
by Pereira et al (1993) and Rooth et al (1999). La-
tent variable models are also conceptually similar
to non-probabilistic dimensionality reduction tech-
niques such as Latent Semantic Analysis (Landauer
and Dumais, 1997). More recently, O? Se?aghdha
(2010) and Ritter et al (2010) reformulated Rooth et
al.?s approach in a Bayesian framework using mod-
els related to Latent Dirichlet Allocation (Blei et al,
2003), demonstrating that this ?topic modelling? ar-
chitecture is a very good fit for capturing selectional
preferences. Reisinger and Mooney (2010) inves-
tigate nonparametric Bayesian models for teasing
apart the context distributions of polysemous words.
As described in Section 3 below, Dinu and Lapata
(2010) propose an LDA-based model for lexical sub-
stitution; the techniques presented in this paper can
be viewed as a generalisation of theirs. Topic models
have also been applied to other classes of semantic
task, for example word sense disambiguation (Li et
al., 2010), word sense induction (Brody and Lapata,
2009) and modelling human judgements of semantic
association (Griffiths et al, 2007).
3 Models
3.1 Latent variable context models
In this paper we consider generative models of lex-
ical choice that assign a probability to a particular
word appearing in a given linguistic context. In par-
ticular, we follow recent work (Dinu and Lapata,
2010; O? Se?aghdha, 2010; Ritter et al, 2010) in as-
suming a latent variable model that associates con-
texts with distributions over a shared set of variables
and associates each variable with a distribution over
the vocabulary of word types:
P (w|c) =
?
z?Z
P (w|z)P (z|c) (1)
The set of latent variables Z is typically much
smaller than the vocabulary size; this induces a (soft)
clustering of the vocabulary. Latent Dirichlet Allo-
cation (Blei et al, 2003) is a powerful method for
learning such models from a text corpus in an unsu-
pervised way; LDA was originally applied to doc-
ument modelling, but it has recently been shown to
be very effective at inducing models for a variety of
semantic tasks (see Section 2).
Given the latent variable framework in (1) we can
develop a generative model of paraphrasing a word
o with another word n in a particular context c:
PC?T (n|o, c) =
?
z
P (n|z)P (z|o, c) (2)
P (z|o, c) = P (o|z)P (z|c)?
z? P (o|z?)P (z?|c)
(3)
In words, the probability P (n|o, c) is the probability
that n would be generated given the latent variable
distribution associated with seeing o in context c;
this latter distribution P (z|o, c) can be derived using
Bayes? rule and the assumption P (o|z, c) = P (o|z).
1048
Given a set of contexts C in which an instance o ap-
pears (e.g., it may be both the subject of a verb and
modified by an adjective), (2) and (3) become:
PC?T (n|o, C) =
?
z
P (n|z)P (z|o, C) (4)
P (z|o, C) = P (o|z)P (z|C)?
z? P (o|z?)P (z?|C)
(5)
P (z|C) =
?
c?C P (z|c)?
z?
?
c?C P (z?|c)
(6)
Equation (6) can be viewed as defining a ?product
of experts? model (Hinton, 2002). Dinu and Lapata
(2010) also use a similar formulation to (5), except
that P (z|o, C) is factorised over P (z|o, C) rather
than just P (z|C):
PDL10(z|o, C) =
?
c?C
P (o|z)P (z|c)?
z? P (o|z?)P (z?|c)
(7)
In Section 5 below, we find that using (5) rather than
(7) gives better results.
The model described above (henceforth C ? T )
models the dependence of a target word on its con-
text. An alternative perspective is to model the de-
pendence of a set of contexts on a target word, i.e.,
we induce a model
P (c|w) =
?
z
P (c|z)P (z|w) (8)
Making certain assumptions, a formula for P (n|o, c)
can be derived from (8):
PT?C(n|o, c) =
P (c|o, n)P (n|o)
P (c|o) (9)
P (c|o, n) =
?
z
P (c|z)P (z|o, n)
P (z|o, n) = P (z|o)P (z|n)?
z? P (z?|o)P (z?|n)
(10)
P (c|o) =
?
z
P (c|z)P (z|o) (11)
P (n|o) = 1/V (12)
The assumption of a uniform prior P (n|o) on the
choice of a paraphrase n for o is clearly not appro-
priate from a language modelling perspective (one
could imagine an alternative P (n) based on corpus
frequency), but in the context of measuring semantic
similarity it serves well. The T ? C model for a set
of contexts C is:
PT?C(n|o, C) =
P (C|o, n)P (n|o)
P (C|o) (13)
P (C|o, n) =
?
z
P (z|o, n)
?
c?C
P (c|z) (14)
P (C|o) =
?
z
P (z|o)
?
c?C
P (c|z) (15)
P (z|o, C) = P (z|o)P (C|o)?
z? P (z?|o)P (C|o)
(16)
With appropriate priors chosen for the distribu-
tions over words and latent variables, P (n|o, C) is
a fully generative model of lexical substitution. A
non-generative alternative is one that estimates the
similarity of the latent variable distributions associ-
ated with seeing n and o in context C. The princi-
ple that similarity between topic distributions corre-
sponds to semantic similarity is well-known in doc-
ument modelling and was proposed in the context
of lexical substitution by Dinu and Lapata (2010).
In terms of the equations presented above, we could
compare the distributions P (z|o, C) with P (z|n,C)
using equations (5) or (16). However, Thater et
al. (2010) and Dinu and Lapata (2010) both ob-
serve that contextualising both o and n can degrade
performance; in view of this we actually compare
P (z|o, C) with P (z|n) and make the further simpli-
fying assumption that P (z|n) ? P (n|z). The sim-
ilarity measure we adopt is the Bhattacharyya coef-
ficient, which is a natural measure of similarity be-
tween probability distributions and is closely related
to the Hellinger distance used in previous work on
topic modelling (Blei and Lafferty, 2007):
simbhatt(Px(z), Py(z)) =
?
z
?
Px(z)Py(z) (17)
This measure takes values between 0 and 1.
In this paper we train LDA models of P (w|c) and
P (c|w). In the former case, the analogy to document
modelling is that each context type plays the role of
a ?document? consisting of all the words observed
in that context in a corpus; for P (c|w) the roles are
reversed. The models are trained by Gibbs sampling
using the efficient procedure of Yao et al (2009).
The empirical estimates for distributions over words
and latent variables are derived from the assignment
1049
of topics over the training corpus in a single sam-
pling state. For example, to model P (w|c) we cal-
culate:
P (w|z) = fzw + ?fz? +N?
(18)
P (z|c) = fzc + ?zf?c +
?
z? ?z?
(19)
where fzw is the number of words of type w as-
signed topic z, fzc is the number of times z is associ-
ated with context c, fz? and f?c are the marginal topic
and context counts respectively, N is the number of
word types and ? and ? parameterise the Dirichlet
prior distributions over P (z|c) and P (w|z). Follow-
ing the recommendations of Wallach et al (2009)
we use asymmetric ? and symmetric ?; rather than
using fixed values for these hyperparameters we es-
timate them from data in the course of LDA train-
ing using an EM-like method.2 We use standard set-
tings for the number of training iterations (1000), the
length of the burnin period before hyperparameter
estimation begins (200 iterations) and the frequency
of hyperparameter estimation (50 iterations).
3.2 Context types
We have not yet defined what the contexts c look
like. In vector space models of semantics it is
common to distinguish between window-based and
dependency-based models (Pado? and Lapata, 2007);
one can make the same distinction for probabilis-
tic context models. A broad generalisation is that
window-based models capture semantic association
(e.g. referee is associated with football), while
dependency models capture a finer-grained notion
of similarity (referee is similar to umpire but not
to football). Dinu and Lapata (2010) propose a
window-based model of lexical substitution; the set
of contexts in which a word appears is the set of
surrounding words within a prespecified ?window
size?. In this paper we also investigate dependency-
based context sets derived from syntactic structure.
Given a sentence such as
2We use the estimation methods provided by the MAL-
LET toolkit, available from http://mallet.cs.umass.
edu/.
The:d executive:j body:n
n:ncmod:j
OO decided:v
v:ncsubj:n
 . . .
the set C of dependency contexts for the noun body
is {executive:j:ncmod?1:n, decide:v:ncsubj:n},
where ncmod?1 denotes that body stands in an in-
verse non-clausal modifier relation to executive (we
assume that nouns are the heads of their adjectival
modifiers).
4 Experiment 1: Similarity in context
4.1 Data
Mitchell and Lapata (2008) collected human judge-
ments of semantic similarity for pairs of short sen-
tences, where the sentences in a pair share the same
subject but different verbs. For example, the sales
slumped and the sales declined should be judged as
very similar while the shoulders slumped and the
shoulders declined should be judged as less similar.
The resulting dataset (henceforth ML08) consists of
120 such pairs using 15 verbs, balanced across high
and low expected similarity. 60 subjects rated the
data using a scale of 1?7; Mitchell and Lapata cal-
culate average interannotator correlation to be 0.40
(using Spearman?s ?). Both Mitchell and Lapata
and Erk and Pado? (2008) split the data into a devel-
opment portion and a test portion, the development
portion consisting of the judgements of six annota-
tors; in order to compare our results with previous
research we use the same data split. To evaluate per-
formance, the predictions made by a model are com-
pared to the judgements of each annotator in turn
(using ?) and the resulting per-annotator ? values are
averaged.
4.2 Models
All models were trained on the written section of the
British National Corpus (around 90 million words),
parsed with RASP (Briscoe et al, 2006). The BNC
was also used by Mitchell and Lapata (2008) and
Erk and Pado? (2008); as the ML08 dataset was com-
piled using words appearing more than 50 times in
the BNC, there are no coverage problems caused
by data sparsity. We trained LDA models for the
grammatical relations v:ncsubj:n and n:ncsubj?1:v
1050
Model PARA SIM
No optimisation
C ? T 0.24 0.34
T ? C 0.36 0.39
T ? C 0.33 0.39
Optimised on dev
C ? T 0.24 0.35
T ? C 0.41 0.41
T ? C 0.37 0.41
Erk and Pado? (2008) Mult 0.24SVS 0.27
Table 1: Performance (average ?) on the ML08 test
set
and used these to create predictors of type C ? T
and T ? C, respectively. For each predictor, we
trained five runs with 100 topics for 1000 iterations
and averaged the predictions produced from their fi-
nal states. We investigate both the generative para-
phrasing model (PARA) and the method of compar-
ing topic distributions (SIM). For both PARA and
SIM we present results using each predictor type on
its own as well as a combination of both types (T ?
C); for PARA the contributions of the types are mul-
tiplied and for SIM they are averaged.3 One poten-
tial complication is that the PARA model is trained
to predict P (n|c, o), which might not be comparable
across different combinations of subject c and verb
o. Using P (n|c, o) as a proxy for the desired joint
distribution P (n, c, o) is tantamount to assuming a
uniform distribution P (c, o), which can be defended
on the basis that the choice of subject noun and ref-
erence verb is not directly relevant to the task. As
shown by the results below, this assumption seems
to work reasonably well in practice.
As well as reporting correlations for straightfor-
ward averages of each set of five runs, we also inves-
tigate whether the development data can be used to
select an optimal subset of runs. This is done by sim-
ply evaluating every possible subset of 1?5 runs on
the development data and picking the best-scoring
subset.
4.3 Results
Table 1 presents the results of the PARA and SIM
predictors on the ML08 dataset. The best results
3This configuration seems the most intuitive; averaging
PARA predictors and multiplying SIM also give good results.
previously reported for this dataset were given by
Erk and Pado? (2008), who measured average ? val-
ues of 0.24 for a vector multiplication method and
0.27 for their structured vector space (SVS) syn-
tactic disambiguation method. Even without using
the development set to select models, performance is
well above the previous state of the art for all predic-
tors except PARAC?T . Model selection on the de-
velopment data brings average ? up to 0.41, which is
comparable to the human ?ceiling? of 0.40 measured
by Mitchell and Lapata. In all cases the T ? C pre-
dictors outperform C ? T : models that associate
target words with distributions over context clusters
are superior to those that associate contexts with dis-
tributions over target words.
Figure 1 plots the beneficial effect of averaging
over multiple runs; as the number of runs n is in-
creased, the average performance over all combi-
nations of n predictors chosen from the set of five
T ? C and five C ? T runs is observed to in-
crease monotonically. Figure 1 also shows that the
model selection procedure is very effective at se-
lecting the optimal combination of models; develop-
ment set performance is a reliable indicator of test
set performance.
5 Experiment 2: Lexical substitution
5.1 Data
The English Lexical Substitution task, run as part
of the SemEval-1 competition, required participants
to propose good substitutes for a set of target words
in various sentential contexts (McCarthy and Nav-
igli, 2009). Table 2 shows two example sentences
and the substitutes appearing in the gold standard,
ranked by the number of human annotators who pro-
posed each substitute. The dataset contains a total of
2,010 annotated sentences with 205 distinct target
words across four parts of speech (noun, verb, ad-
jective, adverb). In line with previous work on con-
textual disambiguation, we focus here on the subtask
of ranking attested substitutes rather than proposing
them from an unrestricted vocabulary. To this end,
a candidate set is constructed for each target word
from all the substitutes proposed for that word in all
sentences in the dataset.
The data contains a number of multiword para-
phrases such as rush at; as our models (like most
1051
1 2 3 4 5
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(a) PARA: Target ? Context
1 2 3 4 5
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(b) PARA: Context ? Target
2 3 4 5 6 7 8 9 10
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(c) PARA: Target ? Context
1 2 3 4 5
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(d) SIM: Target ? Context
1 2 3 4 5
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(e) SIM: Context ? Target
2 3 4 5 6 7 8 9 10
0.1
0.2
0.3
0.4
0.5
No. of predictors
?
(f) SIM: Target ? Context
Figure 1: Performance on the ML08 test set with different predictor types and different numbers of LDA
runs per predictor type; the solid line tracks the average performance, the dashed line shows the performance
of the predictor combination that scores best on the development set.
Realizing immediately that strangers have come, attack (5), rush at (1)the animals charge them and the horses began to fight.
Commission is the amount charged to execute a trade. levy (2), impose (1), take (1), demand (1)
Table 2: Examples for the verb charge from the English Lexical Substitution Task
current models of distributional semantics) do not
represent multiword expressions, we remove such
paraphrases and discard the 17 sentences which have
only multiword substitutes in the gold standard.4
There are also 7 sentences for which the gold stan-
dard contains no substitutes. This leaves a total of
1986 sentences. These sentences were lemmatised
and parsed with RASP.
Previous authors have partitioned the dataset in
various ways. Erk and Pado? (2008) use only a sub-
set of the data where the target is a noun headed
by a verb or a verb heading a noun. Thater et al
4Thater et al (2010) and Dinu and Lapata (2010) similarly
remove multiword paraphrases (Georgiana Dinu, p.c.).
(2010) discard sentences which their parser cannot
parse and paraphrases absent from their training cor-
pus and then optimise the parameters of their model
through four-fold cross-validation. Here we aim for
complete coverage on the dataset and do not perform
any parameter tuning. We use two measures to eval-
uate performance: Generalised Averaged Precision
(Kishida, 2005) and Kendall?s ?b rank correlation
coefficient, which were used for this task by Thater
et al (2010) and Dinu and Lapata (2010), respec-
tively. Generalised Averaged Precision (GAP) is
a precision-like measure for evaluating ranked pre-
dictions against a gold standard. ?b is a variant of
Kendall?s ? that is appropriate for data containing
tied ranks. We do not use the ?precision out of ten?
1052
COORDINATION:
Cats and
c:conj:n
OO
c:conj:n
OOdogs run
v:ncsubj:n

? Cats and dogsOO
n:and:n
OO run

v:ncsubj:n

PREDICATION:
The cat is
v:ncsubj:n
OO
v:xcomp:j
fierce ? The cat
n:ncmod:j
is fierce
PREPOSITIONS:
The cat
n:ncmod:i
in
i:dobj:n
OOthe hat ? The cat
n:prep in:n
in the hat
Table 3: Dependency graph preprocessing
measure that was used in the original Lexical Substi-
tution Task; this measure assigns credit for the pro-
portion of the first 10 proposed paraphrases that are
present in the gold standard and in the context of
ranking attested substitutes it is unclear how to ob-
tain non-trivial results for target words with 10 or
fewer possible substitutes. We calculate statistical
significance of performance differences using strati-
fied shuffling (Yeh, 2000).5
5.2 Models
We apply the models developed in Section 3.1 to the
Lexical Substitution Task dataset using dependency-
and window-based context information. Here we
only use the SIM predictor type. PARA did not give
satisfactory results; in particular, it tended to rank
common words highly in most contexts.6
As before we compiled training data by extracting
target-context cooccurrences from a text corpus. In
addition to the parsed BNC described above we used
a corpus of Wikipedia text consisting of over 45 mil-
lion sentences (almost 1 billion words) parsed using
the fast Combinatory Categorial Grammar (CCG)
parser described by Clark et al (2009). The depen-
5We use the software package available at http://www.
nlpado.de/?sebastian/sigf.html.
6Favouring more general words may indeed make sense in
some paraphrasing tasks (Nulty and Costello, 2010).
dency representation produced by this parser is inter-
operable with the RASP dependency format. In or-
der to focus our models on semantically discrimina-
tive information and make inference more tractable
we ignored all parts of speech other than nouns,
verbs, adjectives, prepositions and adverbs. Stop-
words and words of fewer than three characters were
removed. We also removed the very frequent but se-
mantically weak lemmas be and have.
We compare two classes of context models: mod-
els learned from window-based contexts and models
learned from syntactic dependency contexts. For the
syntactic models we extracted all dependencies and
inverse dependencies between lemmas of the afore-
mentioned POS types; in order to maximise the ex-
traction yield, the dependency graph for each sen-
tence was preprocessed using the transformations
shown in Table 3. For the window-based context
model we follow Dinu and Lapata (2010) in treating
each word within five words of a target as a member
of its context set.
It proved necessary to subsample the corpora in
order to make LDA training tractable, especially for
the window-based model where the training set of
context-target counts is extremely dense (each in-
stance of a word in the corpus contributes up to
10 context instances). For the window-based data,
we divided each context-target count by a factor of
5 and a factor of 70 for the BNC and Wikipedia
corpora respectively, rounding fractional counts to
the closest integer. The choice of 70 for scaling
Wikipedia counts is adopted from Dinu and Lap-
ata (2010), who used the same factor for the com-
parably sized English Gigaword corpus. As the de-
pendency data is an order of magnitude smaller we
downsampled the Wikipedia counts by 5 and left the
BNC counts untouched. Finally, we created a larger
corpus by combining the counts from the BNC and
Wikipedia datasets. Type and token counts for the
BNC and combined corpora are given in Table 4.
We trained three LDA predictors for each corpus:
a window-based predictor (W5), a Context ? Tar-
get predictor (C ? T ) and a Target ? Context
predictor (T ? C). For W5 the sets of types and
contexts should be symmetrical (in practice there
is some discrepancy due to preprocessing artefacts).
ForC ? T , individual models were trained for each
of the four target parts of speech; in each case the set
1053
BNC BNC+Wikipedia
Tokens Types Contexts Tokens Types Contexts
Nouns 18723082 122999 316237 54145216 106448 514257
Verbs 7893462 18494 57528 20082658 16673 82580
Adjectives 4385788 73684 37163 11536424 88488 57531
Adverbs 1976837 7124 14867 3017936 4056 18510
Window5 28329238 88265 102792 42828094 139640 143443
Table 4: Type and token counts for the BNC and downsampled BNC+Wikipedia corpora
BNC BNC + Wikipedia
GAP ?b Coverage GAP ?b Coverage
W5 44.5 0.17 100.0 44.8 0.17 100.0
C ? T 43.2 0.16 86.4 48.7 0.21 86.5
T ? C 47.2 0.21 86.4 49.3 0.22 86.5
T ? C 45.7 0.20 86.4 49.1 0.23 86.5
W5 + C ? T 46.0 0.18 100.0 48.7 0.21 100.0
W5 + T ? C 48.6 0.21 100.0 49.3 0.22 100.0
W5 + T ? C 48.1 0.20 100.0 49.5 0.23 100.0
Table 5: Results on the English Lexical Substitution Task dataset; boldface denotes best performance at full
coverage for each corpus
of types is the vocabulary for that part of speech and
the set of contexts is the set of dependencies taking
those types as dependents. For T ? C we again
train four models; the sets of types and contexts are
reversed. For the both corpora we trained models
with Z = {600, 800, 1000, 1200} topics; for each
setting of Z we ran five estimation runs. Each in-
dividual prediction of similarity between P (z|C, o)
and P (z|n) is made by averaging over the predic-
tions of all runs and over all settings of Z. Choosing
a single setting of Z does not degrade performance
significantly; however, averaging over settings is a
convenient way to avoid having to pick a specific
value.
We also investigate combinations of predictor
types, once again produced by averaging: we com-
bine C ? T with C ? T (T ? C) and combine
each of these three models with W5.
5.3 Results
Table 5 presents the results attained by our mod-
els on the Lexical Substitution Task data. The
dependency-based models have imperfect coverage
(86% of the data); they can make no prediction when
no syntactic context is provided for a target, per-
haps as a result of parsing error. The window-based
models have perfect coverage, but score noticeably
lower. By combining dependency- and window-
based models we can reach high performance with
perfect coverage. All combinations outperform the
corresponding W5 results to a statistically signifi-
cant degree (p < 0.01). Performance at full cov-
erage is already very good (GAP= 48.6, ?b = 0.21)
on the BNC corpus, but the best results are attained
by W5 + T ? C trained on the combined corpus
(GAP= 49.5, ?b = 0.23). The results for the W5
model trained on BNC data is comparable to that
trained on the combined corpus; however the syntac-
tic models show a clear benefit from the less sparse
dependency data in the combined training corpus.
As remarked in Section 3.1, Dinu and Lap-
ata (2010) use a slightly different formulation of
P (z|C, o). Using the window-based context model
our formulation (5) outperforms (7) for both training
corpora; the Dinu and Lapata (2010) version scores
GAP = 41.5, ?b = 0.15 for the BNC corpus and
GAP = 42.0, ?b = 0.15 for the combined corpus.
The advantage of our formulation is statistically sig-
nificant for all evaluation measures.
1054
Nouns Verbs Adjectives Adverbs Overall
GAP ?b GAP ?b GAP ?b GAP ?b GAP ?b
W5 46.0 0.16 38.9 0.14 44.0 0.18 54.0 0.22 44.8 0.17
W5 + T ? C 50.7 0.22 45.1 0.20 48.8 0.24 55.9 0.24 49.5 0.23
Thater et al (2010) (Model 1) 46.4 ? 45.9 ? 39.4 ? 48.2 ? 44.6 ?
Thater et al (2010) (Model 2) 42.5 ? ? ? 43.2 ? 51.4 ? ? ?
Dinu and Lapata (2010) (LDA) ? 0.16 ? 0.14 ? 0.17 ? 0.21 ? 0.16
Dinu and Lapata (2010) (NMF) ? 0.15 ? 0.14 ? 0.16 ? 0.26 ? 0.16
Table 6: Performance by part of speech
Table 6 gives a breakdown of performance by tar-
get part of speech for the BNC+Wikipedia-trained
W5 and W5 + T ? C models, as well as figures
provided by previous researchers.7 W5 + T ? C
outperforms W5 on all parts of speech using both
evaluation metrics. As remarked above, previous re-
searchers have used the corpus in slightly different
ways; we believe that the results of Dinu and Lapata
(2010) are fully comparable, while those of Thater et
al. (2010) were attained on a slightly smaller dataset
with parameters set through cross-validation. The
results for W5 + T ? C outperform all of Dinu
and Lapata?s per-POS and overall results except for
a slightly superior score on adverbs attained by their
NMF model (?b = 0.26 compared to 0.24). Turn-
ing to Thater et al, we report higher scores for ev-
ery POS with the exception of the verbs where their
Model 1 achieves 45.9 GAP compared to 45.1; the
overall average for W5 + T ? C is substantially
higher at 49.5 compared to 44.6. On balance, we
suggest that our models do have an advantage over
the current state of the art for lexical substitution.
6 Conclusion
In this paper we have proposed novel methods for
modelling the effect of context on lexical mean-
ing, demonstrating that information about syntactic
context and textual proximity can fruitfully be inte-
grated to produce state-of-the-art models of lexical
choice. We have demonstrated the effectiveness of
our techniques on two datasets but they are poten-
tially applicable to a range of applications where se-
mantic disambiguation is required. In future work,
7The overall average GAP for Thater et al (2010) does not
appear in their paper but can be calculated from the score and
number of instances listed for each POS.
we intend to adapt our approach for word sense dis-
ambiguation as well as related domain-specific tasks
such as gene name normalisation (Morgan et al,
2008). A further, more speculative direction for fu-
ture research is to investigate more richly structured
models of context, for example capturing correla-
tions between words in a text within a framework
similar to the Correlated Topic Model of Blei and
Lafferty (2007) or more explicitly modelling poly-
semy effects as in Reisinger and Mooney (2010).
Acknowledgements
We are grateful to the EMNLP reviewers for their
helpful comments. This research was supported by
EPSRC grant EP/G051070/1.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
10), Cambridge, MA.
David M. Blei and John D. Lafferty. 2007. A correlated
topic model of science. The Annals of Applied Statis-
tics, 1(1):17?35.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions,
Sydney, Australia.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of EACL-09, Athens,
Greece.
1055
Stephen Clark, Ann Copestake, James R. Curran, Yue
Zhang, Aurelie Herbelot, James Haggerty, Byung-Gyu
Ahn, Curt Van Wyk, Jessika Roesner, Jonathan Kum-
merfeld, and Tim Dawborn. 2009. Large-scale syn-
tactic processing: Parsing the web. Technical report,
Final Report of the 2009 JHU CLSP Workshop.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP-10), Cambridge,MA.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-08),
Honolulu, HI.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011. Con-
crete sentence spaces for compositional distributional
models of meaning. In Proceedings of the 9th In-
ternational Conference on Computational Semantics
(IWCS-11), Oxford, UK.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211?244.
Kenneth E. Harper. 1965. Measurement of similarity be-
tween nouns. In Proceedings of the 1965 International
Conference on Computational Linguistics (COLING-
65), New York, NY.
Geoffrey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14(8):1771?1800.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Kazuaki Kishida. 2005. Property of average precision
and its generalisation: An examination of evaluation
indicator for information retrieval experiments. Tech-
nical Report NII-2005-014E, National Institute of In-
formatics, Tokyo, Japan.
Thomas K Landauer and Susan T Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), Uppsala, Sweden.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics (ACL-08), Columbus, OH.
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,
Aaron M Cohen, Juliane Fluck, Patrick Ruch, Anna
Divoli, Katrin Fundel, Robert Leaman, Jo?rg Haken-
berg, Chengjie Sun, Heng hui Liu, Rafael Torres,
Michael Krauthammer, William W Lau, Hongfang
Liu, Chun-Nan Hsu, Martijn Schuemie, K. Bretonnel
Cohen, and Lynette Hirschman. 2008. Overview of
BioCreative II gene normalization. Genome Biology,
9(Suppl 2).
Paul Nulty and Fintan Costello. 2010. UCD-PN: Select-
ing general paraphrases using conditional probability.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval-2), Uppsala, Sweden.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL-10), Uppsala, Sweden.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st Annual Meeting of the Association
for Computational Linguistics, Columbus, OH.
Joseph Reisinger and Raymond Mooney. 2010. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-10),
Cambridge,MA.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional prefer-
ences. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-
10), Uppsala, Sweden.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-99), College
Park, MD.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-10), Uppsala,
Sweden.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1?
2):159?216.
Karen Spa?rck Jones. 1964. Synonymy and Semantic
Classification. Ph.D. thesis, University of Cambridge.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
1056
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL-10), Uppsala, Swe-
den.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
Hanna Wallach, David Mimno, and Andrew McCallum.
2009. Rethinking LDA: Why priors matter. In Pro-
ceedings of NIPS-09, Vancouver, BC.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Proceedings
of the 15th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-09),
Paris, France.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics
(COLING-00), Saarbru?cken, Germany.
1057
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255?265,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning Abstract Concept Embeddings from Multi-Modal Data:
Since You Probably Can?t See What I Mean
Felix Hill
Computer Laboratory
University of Cambridge
felix.hill@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
anna.korhonen@cl.cam.ac.uk
Abstract
Models that acquire semantic represen-
tations from both linguistic and percep-
tual input are of interest to researchers
in NLP because of the obvious parallels
with human language learning. Perfor-
mance advantages of the multi-modal ap-
proach over language-only models have
been clearly established when models are
required to learn concrete noun concepts.
However, such concepts are comparatively
rare in everyday language. In this work,
we present a new means of extending
the scope of multi-modal models to more
commonly-occurring abstract lexical con-
cepts via an approach that learns multi-
modal embeddings. Our architecture out-
performs previous approaches in combin-
ing input from distinct modalities, and
propagates perceptual information on con-
crete concepts to abstract concepts more
effectively than alternatives. We discuss
the implications of our results both for op-
timizing the performance of multi-modal
models and for theories of abstract con-
ceptual representation.
1 Introduction
Multi-modal models that learn semantic represen-
tations from both language and information about
the perceptible properties of concepts were orig-
inally motivated by parallels with human word
learning (Andrews et al., 2009) and evidence that
many concepts are grounded in perception (Barsa-
lou and Wiemer-Hastings, 2005). The perceptual
information in such models is generally mined di-
rectly from images (Feng and Lapata, 2010; Bruni
et al., 2012) or from data collected in psychologi-
cal studies (Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013).
By exploiting the additional information en-
coded in perceptual input, multi-modal models
can outperform language-only models on a range
of semantic NLP tasks, including modelling sim-
ilarity (Bruni et al., 2014; Kiela et al., 2014) and
free association (Silberer and Lapata, 2012), pre-
dicting compositionality (Roller and Schulte im
Walde, 2013) and concept categorization (Silberer
and Lapata, 2014). However, to date, these pre-
vious approaches to multi-modal concept learning
focus on concrete words such as cat or dog, rather
than abstract concepts, such as curiosity or loyalty.
However, differences between abstract and con-
crete processing and representation (Paivio, 1991;
Hill et al., 2013; Kiela et al., 2014) suggest that
conclusions about concrete concept learning may
not necessarily hold in the general case. In this pa-
per, we therefore focus on multi-modal models for
learning both abstract and concrete concepts.
Although concrete concepts might seem more
basic or fundamental, the vast majority of open-
class, meaning-bearing words in everyday lan-
guage are in fact abstract. 72% of the noun or
verb tokens in the British National Corpus (Leech
et al., 1994) are rated by human judges
1
as more
abstract than the noun war, for instance, a con-
cept many would already consider to be quite
abstract. Moreover, abstract concepts by defi-
nition encode higher-level (more general) princi-
ples than concrete concepts, which typically re-
side naturally in a single semantic category or do-
main (Crutch and Warrington, 2005). It is there-
fore likely that abstract representations may prove
highly applicable for multi-task, multi-domain or
transfer learning models, which aim to acquire
?general-purpose? conceptual knowledge without
reference to a specific objective or task (Collobert
and Weston, 2008; Mesnil et al., 2012).
In a recent paper, Hill et al. (2014) investigate
whether the multi-modal models cited above are
1
Contributors to the USF dataset (Nelson et al., 2004).
255
effective for learning concepts other than concrete
nouns. They observe that representations of cer-
tain abstract concepts can indeed be enhanced in
multi-modal models by combining perceptual and
linguistic input with an information propagation
step. Hill et al. (2014) propose ridge regression as
an alternative to the nearest-neighbour averaging
proposed by Johns and Jones (2012) for such prop-
agation, and show that it is more robust to changes
in the type of concept to be learned. However, both
methods are somewhat inelegant, in that they learn
separate linguistic and ?pseudo-perceptual? repre-
sentations, which must be combined via a separate
information combination step. Moreover, for the
majority of abstract concepts, the best performing
multi-modal model employing these techniques
remains less effective than conventional text-only
representation learning model.
Motivated by these observations, we introduce
an architecture for learning both abstract and con-
crete representations that generalizes the skipgram
model of Mikolov et al. (2013) from text-based to
multi-modal learning. Aspects of the model de-
sign are influenced by considering the process of
human language learning. The model moderates
the training input to include more perceptual infor-
mation about commonly-occurring concrete con-
cepts and less information about rarer concepts.
Moreover, it integrates the processes of combin-
ing perceptual and linguistic input and propagat-
ing information from concrete to abstract concepts
into a single representation update process based
on back-propagation.
We train our model on running-text language
and two sources of perceptual descriptors for con-
crete nouns: the ESPGame dataset of annotated
images (Von Ahn and Dabbish, 2004) and the
CSLB set of concept property norms (Devereux
et al., 2013). We find that our model combines in-
formation from the different modalities more ef-
fectively than previous methods, resulting in an
improved ability to model the USF free associa-
tion gold standard (Nelson et al., 2004) for con-
crete nouns. In addition, the architecture propa-
gates the extra-linguistic input for concrete nouns
to improve representations of abstract concepts
more effectively than alternative methods. While
this propagation can effectively extend the advan-
tage of the multi-modal approach to many more
concepts than simple concrete nouns, we observe
that the benefit of adding perceptual input appears
to decrease as target concepts become more ab-
stract. Indeed, for the most abstract concepts of
all, language-only models still provide the most
effective learning mechanism.
Finally, we investigate the optimum quantity
and type of perceptual input for such models. Be-
tween the most concrete concepts, which can be
effectively represented directly in the perceptual
modality, and the most abstract concepts, which
cannot, we identify a set of concepts that cannot
be represented effectively directly in the percep-
tual modality, but still benefit from perceptual in-
put propagated in the model via concrete concepts.
The motivation in designing our model and ex-
periments is both practical and theoretical. Taken
together, the empirical observations we present are
potentially important for optimizing the learning
of representations of concrete and abstract con-
cepts in multi-modal models. In addition, they of-
fer a degree of insight into the poorly understood
issue of how abstract concepts may be encoded in
human memory.
2 Model Design
Before describing how our multi-modal architec-
ture encodes and integrates perceptual informa-
tion, we first describe the underlying corpus-based
representation learning model.
Language-only Model Our multi-modal archi-
tecture builds on the continuous log-linear skip-
gram language model proposed by Mikolov et
al. (2013). This model learns lexical representa-
tions in a similar way to neural-probabilistic lan-
guage models (NPLM) but without a non-linear
hidden layer, a simplification that facilitates the
efficient learning of large vocabularies of dense
representations, generally referred to as embed-
dings (Turian et al., 2010). Embeddings learned
by the model achieve state-of-the-art performance
on several evaluations including sentence comple-
tion and analogy modelling (Mikolov et al., 2013).
For each word type w in the vocabulary V , the
model learns both a ?target-embedding? r
w
? R
d
and a ?context-embedding? r?
w
? R
d
such that,
given a target word, its ability to predict nearby
context words is maximized. The probability of
seeing context word c given target w is defined as:
p(c|w) =
e
r?
c
?r
w
?
v?V
e
r?
v
?r
w
256
  
w
n
Target Representation
Score: p(c|w)
Context Representations  Information Source
w
n+2
pw
n+2
w
n+1
pw
n+1
w
n-1
pw
n-1
w
n-2
pw
n-2
Linguistic
Text8 Corpus
Perceptual
P
ESP  
P
CSBL
Figure 1: Our multi-modal model architecture. Light boxes are elements of the original Mikolov et
al. (2013) model. For target words w
n
in the domain of P (concrete concepts), the model updates its
representations based on corpus context wordsw
n?i
, then on words p
w
n?i
in perceptual pseudo-sentences.
For w
n
not in the domain of P (abstract concepts), updates are based solely on the w
n?i
.
The model learns from a set of target-word,
context-word pairs, extracted from a corpus of
sentences as follows. In a given sentence S (of
length N ), for each position n ? N , each word
w
n
is treated in turn as a target word. An inte-
ger t(n) is then sampled from a uniform distribu-
tion on {1, . . . k}, where k > 0 is a predefined
maximum context-window parameter. The pair to-
kens {(w
n
, w
n+j
) : ?t(n) ? j ? t(n), w
i
? S}
are then appended to the training data. Thus, tar-
get/context training pairs are such that (i) only
words within a k-window of the target are selected
as context words for that target, and (ii) words
closer to the target are more likely to be selected
than those further away.
The training objective is then to maximize the
sum of the log probabilities T across of all such
examples from S and across all sentences in the
corpus, where T is defined as follows:
T =
1
N
N
?
n=1
?
?t(n)?j?t(n),j 6=0
log(p(w
n+j
|w
n
))
The model free parameters (target-embeddings
and context-embeddings of dimension d for each
word in the corpus with frequency above a certain
threshold f ) are updated according to stochastic
gradient descent and backpropation, with learning
rate controlled by Adagrad (Duchi et al., 2011).
For efficiency, the output layer is encoded as a
hierarchical softmax function based on a binary
Huffman tree (Morin and Bengio, 2005).
As with other distributional architectures, the
model captures conceptual semantics by exploit-
ing the fact that words appearing in similar lin-
guistic contexts are likely to have similar mean-
ings. Informally, the model adjusts its embeddings
to increase the ?probability? of seeing the language
in the training corpus. Since this probability in-
creases with the p(c|w), and the p(c|w) increase
with the dot product r?
c
? r
w
, the updates have the
effect of moving each target-embedding incremen-
tally ?closer? to the context-embeddings of its col-
locates. In the target-embedding space, this results
in embeddings of concept words that regularly oc-
cur in similar contexts moving closer together.
Multi-modal Extension We extend the Mikolov
et al. (2013) architecture via a simple means of in-
troducing perceptual information that aligns with
human language learning. Based on the assump-
tion that frequency in domain-general linguistic
corpora correlates with the likelihood of ?experi-
encing? a concept in the world (Bybee and Hop-
per, 2001; Chater and Manning, 2006), perceptual
information is introduced to the model whenever
designated concrete concepts are encountered in
the running-text linguistic input. This has the ef-
fect of introducing more perceptual input for com-
monly experienced concrete concepts and less in-
put for rarer concrete concepts.
To implement this process, perceptual informa-
tion is extracted from external sources and en-
coded in an associative array P, which maps (typ-
ically concrete) words w to bags of perceptual fea-
tures b(w). The construction of this array depends
on the perceptual information source; the process
for our chosen sources is detailed in Section 2.1.
Training our model begins as before on running-
text. When a sentence S
m
containing a word w in
the domain of P is encountered, the model finishes
training on S
m
and begins learning from a per-
ceptual pseudo-sentence
?
S
m
(w).
?
S
m
(w) is con-
structed by alternating the token w with a fea-
257
?S(crocodile) = Crocodile legs crocodile teeth crocodile
teeth crocodile scales crocodile green crocodile.
?
S(screwdriver) = Screwdriver handle screwdriver flat
screwdriver long screwdriver handle screwdriver head.
Figure 2: Example pseudo-sentences generated by
our model.
ture sampled at random from b(w) until
?
S
m
(w)
is the same length as S
m
(see Figure 2). Because
we want the ensuing perceptual learning process
to focus on how w relates to its perceptual prop-
erties (rather than how those properties relate to
each other), we insert multiple instances of w into
?
S
m
(w). This ensures that the majority of train-
ing cases derived from
?
S
m
(w) are instances of (w,
feature) rather than (feature, feature) pairs. Once
training on
?
S
m
(w) is complete, the model reverts
to the next ?genuine? (linguistic) sentence S
m+1
,
and the process continues. Thus, when a concrete
concept is encountered in the corpus, its embed-
ding is first updated based on language (moved in-
crementally closer to concepts appearing in sim-
ilar linguistic contexts), and then on perception
(moved incrementally closer to concepts with the
same or similar perceptual features).
For greater flexibility, we introduce a parameter
? reflecting the raw quantity of perceptual infor-
mation relative to linguistic input. When ? = 2,
two pseudo-sentences are generated and inserted
for every corpus occurrence of a token from the
domain of P. For non-integral ?, the number of
sentences inserted is b?c, and a further sentence is
added with probability ?? b?c.
In all experiments reported in the following sec-
tions we set the window size parameter k = 5 and
the minimum frequency parameter f = 3, which
guarantees that the model learns embeddings for
all concepts in our evaluation sets. While the
model learns both target and context-embeddings
for each word in the vocabulary, we conduct our
experiments with the target embeddings only. We
set the dimension parameter d = 300 as this pro-
duces high quality embeddings in the language-
only case (Mikolov et al., 2013).
2.1 Information Sources
We construct the associative array of perceptual
information P from two sources typical of those
used for multi-modal semantic models.
ESPGame Dataset The ESP-Game dataset
(ESP) (Von Ahn and Dabbish, 2004) consists of
100,000 images, each annotated with a list of lex-
ical concepts that appear in that image.
For any concept w identified in an ESP im-
age, we construct a corresponding bag of features
b(w). For each ESP image I that contains w, we
append the other concept tokens identified in I to
b(w). Thus, the more frequently a concept co-
occurs with w in images, the more its correspond-
ing lexical token occurs in b(w). The array P
ESP
in this case then consists of the (w,b(w)) pairs.
CSLB Property Norms The Centre for Speech,
Language and the Brain norms (CSLB) (Devereux
et al., 2013) is a recently-released dataset contain-
ing semantic properties for 638 concrete concepts
produced by human annotators. The CSLB dataset
was compiled in the same way as the McRae et
al. (2005) property norms used widely in multi-
modal models (Silberer and Lapata, 2012; Roller
and Schulte im Walde, 2013); we use CSLB be-
cause it contains more concepts. For each concept,
the proportion of the 30 annotators that produced
a given feature can also be employed as a measure
of the strength of that feature.
When encoding the CSLB data in P, we first
map properties to lexical forms (e.g. is green
becomes green). By directly identifying percep-
tual features and linguistic forms in this way,
we treat features observed in the perceptual data
as (sub)concepts to be acquired via the same
multi-modal input streams and stored in the same
domain-general memory as the evaluation con-
cepts. This design decision in fact corresponds
to a view of cognition that is sometimes disputed
(Fodor, 1983). In future studies we hope to com-
pare the present approach to architectures with
domain-specific conceptual memories.
For each concept w in CSLB, we then con-
struct a feature bag b(w) by appending lexical
forms to b(w) such that the count of each fea-
ture word is equal to the strength of that feature
for w. Thus, when features are sampled from
b(w) to create pseudo-sentences (as detailed pre-
viously) the probability of a feature word occur-
ring in a sentence reflects feature strength. The
array P
CSLB
then consists of all (w,b(w)) pairs.
Linguistic Input The linguistic input to all
models is the 400m word Text8 Corpus
2
of
2
From http://mattmahoney.net/dc/textdata.html
258
ESPGame CSLB
Image 1 Image 2 Crocodile Screwdriver
red wreck has 4 legs (7) has handle (28)
chihuaua cyan has tail (18) has head (5)
eyes man has jaw (7) is long (9)
little crash has scales (8) is plastic (18)
ear accident has teeth (20) is metal (28)
nose street is green (10)
small is large (10)
Table 1: Concepts identified in images in the ESP
Game (left) and features produced for concepts by
human annotators in the CSLB dataset (with fea-
ture strength, max=30).
Concept 1 Concept 2 Assoc.
abdomen (6.83) stomach (6.04) 0.566
throw (4.05) ball (6.08) 0.234
hope (1.18) glory (3.53) 0.192
egg (5.79) milk (6.66) 0.012
Table 2: Example concept pairs (with mean con-
creteness rating) and free-association scores from
the USF dataset.
Wikipedia text, split into sentences and with punc-
tuation removed.
2.2 Evaluation
We evaluate the quality of representations by how
well they reflect free association scores, an em-
pirical measure of cognitive conceptual proxim-
ity. The University of South Florida Norms
(USF) (Nelson et al., 2004) contain free associa-
tion scores for over 40,000 concept pairs, and have
been widely used in NLP to evaluate semantic rep-
resentations (Andrews et al., 2009; Feng and La-
pata, 2010; Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013). Each concept that we
extract from the USF database has also been rated
for conceptual concreteness on a Likert scale of
1-7 by at least 10 human annotators. Following
previous studies (Huang et al., 2012; Silberer and
Lapata, 2012), we measure the (Spearman ?) cor-
relation between association scores and the cosine
similarity of vector representations.
We create separate abstract and concrete con-
cept lists by ranking the USF concepts accord-
ing to concreteness and sampling at random from
the first and fourth quartiles respectively. We also
introduce a complementary noun/verb dichotomy,
Concept Type List Pairs Examples
concrete nouns 541 1418 yacht, cup
abstract nouns 100 295 fear, respect
all nouns 666 1815 fear, cup
concrete verbs 50 66 kiss, launch
abstract verbs 50 127 differ, obey
all verbs 100 221 kiss, obey
Table 3: Details the subsets of USF data used in
our evaluations, downloadable from our website.
on the intuition that information propagation may
occur differently from noun to noun or from noun
to verb (because of their distinct structural rela-
tionships in sentences). POS-tags are not assigned
as part of the USF data, so we draw the noun/verb
distinction based on the majority POS-tag of USF
concepts in the lemmatized British National Cor-
pus (Leech et al., 1994). The abstract/concrete
and noun/verb dichotomies yield four distinct con-
cept lists. For consistency, the concrete noun list
is filtered so that each concrete noun concept w
has a perceptual representation b(w) in both P
ESP
and P
CSLB
. For the four resulting concept lists
C (concrete/abstract, noun/verb), a correspond-
ing set of evaluation pairs {(w
1
, w
2
) ? USF :
w
1
, w
2
? C} is extracted (see Table 3 for details).
3 Results and Discussion
Our experiments were designed to answer four
questions, outlined in the following subsec-
tions: (1) Which model architectures perform best
at combining information pertinent to multiple
modalities when such information exists explicitly
(as common for concrete concepts)? (2) Which
model architectures best propagate perceptual in-
formation to concepts for which it does not exist
explicitly (as is common for abstract concepts)?
(3) Is it preferable to include all of the perceptual
input that can be obtained from a given source, or
to filter this input stream in some way? (4) How
much perceptual vs. linguistic input is optimal for
learning various concept types?
3.1 Combining information sources
To evaluate our approach as a method of in-
formation combination we compared its perfor-
mance on the concrete noun evaluation set against
three alternative methods. The first alternative
is simple concatenation of these perceptual vec-
tors with linguistic vectors embeddings learned
259
by the Mikolov et al. (2013) model on the Text8
Corpus. In the second alternative (proposed
for multi-modal models by Silberer and Lapata
(2012)), canonical correlation analysis (CCA)
(Hardoon et al., 2004) was applied to the vec-
tors of both modalities. CCA yields reduced-
dimensionality representations that preserve un-
derlying inter-modal correlations, which are then
concatenated. The final alternative, proposed by
Bruni et al. (2014) involves applying Singular
Value Decomposition (SVD) to the matrix of con-
catenated multi-modal representations, yielding
smoothed representations.
3
When implementing the concatenation, CCA
and SVD methods, we first encoded the percep-
tual input directly into sparse feature vectors, with
coordinates for each of the 2726 features in CSLB
and for each of the 100,000 images in ESP. This
sparse encoding matches the approach taken by
Silberer and Lapata (2012), for CCA and concate-
nation, and by Hill et al. (2014) for the ridge re-
gression method of propagation (see below).
We compare these alternatives to our proposed
model with ? = 1. In The CSLB and ESP models,
all training pseudo-sentences are generated from
the arrays P
CSLB
and P
ESP
respectively. In the
models classed as CSLB&ESP, a random choice
between P
CSLB
and P
ESP
is made every time
perceptual input is included (so that the overall
quantity of perceptual information is the same).
As shown in Figure 2 (left side), the embed-
dings learned by our model achieve a higher cor-
relation with the USF data than simple concatena-
tion, CCA and SVD regardless of perceptual input
source. With the optimal perceptual source (ESP
only), for instance, the correlation is 11% higher
that the next best alternative method, CCA.
One possible factor behind this improvement
is that, in our model, the learned representations
fully integrate the two modalities, whereas for
both CCA and the concatenation method each rep-
resentation feature (whether of reduced dimension
or not) corresponds to a particular modality. This
deeper integration may help our architecture to
overcome the challenges inherent in information
combination such as inter-modality differences in
information content and representation sparsity. It
is also important to note that Bruni et al. (2014) ap-
3
CCA was implemented using the CCA package in
R. SVD was implemented using SVDLIBC (http://
tedlab.mit.edu/
?
dr/SVDLIBC/), with truncation
factor k = 1024 as per (Bruni et al., 2014).
plied their SVD method with comparatively dense
perceptual representations extracted from images,
whereas our dataset-based perceptual vectors were
sparsely-encoded.
3.2 Propagating input to abstract concepts
To test the process of information propagation in
our model, we evaluated the learned embeddings
of more abstract concepts. We compared our
approach with two recently-proposed alternative
methods for inferring perceptual features when ex-
plicit perceptual information is unavailable.
Johns and Jones In the method of Johns and
Jones (2012), pseudo-perceptual representations
for target concepts without a perceptual repre-
sentations (uni-modal concepts) are inferred as a
weighted average of the perceptual representations
of concepts that do have such a representation (bi-
modal concepts).
In the first step of their two-step method, for
each uni-modal concept k, a quasi-perceptual rep-
resentation is computed as an average of the
perceptual representations of bi-modal concepts,
weighted by the proximity between each of these
concepts and k
k
p
=
?
c?
?
C
S(k
l
, c
l
)
?
? c
p
where
?
C is the set of bi-modal concepts, c
p
and k
p
are the perceptual representations for c and k re-
spectively, and c
l
and k
l
the linguistic representa-
tions. The exponent parameter ? reflects the learn-
ing rate.
In step two, the initial quasi-perceptual repre-
sentations are inferred for a second time, but with
the weighted average calculated over the percep-
tual or initial quasi-perceptual representations of
all other words, not just those that were originally
bi-modal. As with Johns and Jones (2012), we set
the learning rate parameter ? to be 3 in the first
step and 13 in the second.
Ridge Regression An alternative, proposed for
the present purpose by Hill et al. (2014), uses ridge
regression (Myers, 1990). Ridge regression is a
variant of least squares regression in which a reg-
ularization term is added to the training objective
to favor solutions with certain properties.
For bi-modal concepts of dimension n
p
, we ap-
ply ridge regression to learn n
p
linear functions
260
fi
: R
n
l
? R that map the linguistic represen-
tations (of dimension n
l
) to a particular percep-
tual feature i. These functions are then applied
together to map the linguistic representations of
uni-modal concepts to full quasi-perceptual repre-
sentations.
Following Hill et al. (2014), we take the Euclid-
ian l
2
norm of the inferred parameter vector as the
regularization term. This ensures that the regres-
sion favors lower coefficients and a smoother so-
lution function, which should provide better gen-
eralization performance than simple linear regres-
sion. The objective for learning the f
i
is then to
minimize
?aX ? Y
i
?
2
2
+ ?a?
2
2
where a is the vector of regression coefficients, X
is a matrix of linguistic representations and Y
i
a
vector of the perceptual feature i for the set of bi-
modal concepts.
Comparisons We applied the Johns and Jones
method and ridge regression starting from linguis-
tic embeddings acquired by the Mikolov et al.
(2013) model on the Text8 Corpus, and concate-
nated the resulting pseudo-perceptual and linguis-
tic representations. As with the implementation
of our model, the perceptual input for these alter-
native models was limited to concrete nouns (i.e.
concrete nouns were the only bi-modal concepts
in the models).
Figure 3 (right side) shows the propagation per-
formance of the three models. While the corre-
lations overall may seem somewhat low, this is
a consequence of the difficulty of modelling the
USF data. In fact, the performance of both the
language-only model and our multi-modal exten-
sion across the concept types (from 0.18 to 0.36) is
equal to or higher than previous models evaluated
on the same data (Feng and Lapata, 2010; Silberer
and Lapata, 2012; Silberer et al., 2013).
For learning representations of concrete verbs,
our approach achieves a 69% increase in perfor-
mance over the next best alternative. The perfor-
mance of the model on abstract verbs is marginally
inferior to Johns and Jones? method. Neverthe-
less, the clear advantage for concrete verbs makes
our model the best choice for learning represen-
tations of verbs in general, as shown by perfor-
mance on the set all verbs, which also includes
mixed abstract-concrete pairs.
Our model is also marginally inferior to alterna-
tive approaches in learning representations of ab-
stract nouns. However, in this case, no method
improves on the linguistic-only baseline. It is
possible that perceptual information is simply so
removed from the core semantics of these con-
cepts that they are best acquired via the linguis-
tic medium alone, regardless of learning mecha-
nism. The moderately inferior performance of our
method in such cases is likely caused by its greater
inherent inter-modal dependence compared with
methods that simply concatenate uni-modal rep-
resentations. When the perceptual signal is of
low quality, this greater inter-modal dependence
allows the linguistic signal to be obscured.
The trade-off, however, is generally higher-
quality representations when the perceptual signal
is stronger, exemplified by the fact that our pro-
posed approach outperforms alternatives on pairs
generated from both abstract and concrete nouns
(all nouns). Indeed, the low performance of the
Johns and Jones method on all nouns is strik-
ing given that: (a) It performs best on abstract
nouns (? = .282), and (b) For concrete nouns it
reverts to simple concatenation, which also per-
forms comparatively well (? = .249). The poor
performance of the Jobns and Jones method on
all nouns must therefore derive its comparisons
of mixed abstract-concrete or concrete-abstract
pairs. This suggests that the pseudo-perceptual
representations inferred by this method for ab-
stract concepts method may not be compatible
with the directly-encoded perceptual representa-
tions of concrete concepts, rendering the compar-
ison computation between items of differing con-
creteness inaccurate.
3.3 Direct representation vs. propagation
Although property norm datasets such as the
CSLB data typically consist of perceptual fea-
ture information for concrete nouns only, image-
based datasets such as ESP do contain informa-
tion on more abstract concepts, which was omit-
ted from the previous experiments. Indeed, im-
age banks such as Google Images contain millions
of photographs portraying quite abstract concepts,
such as love or war. On the other hand, encod-
ings or descriptions of abstract concepts are gen-
erally more subjective and less reliable than those
of concrete concepts (Wiemer-Hastings and Xu,
2005). We therefore investigated whether or not
it is preferable to include this additional informa-
tion as model input or to restrict perceptual input
261
0.203
0.22
0.15
0.239
0.259 0.271 0.256
0.301
0.249 0.24 0.231
0.296
0.0
0.1
0.2
0.3
0.4
CSLB ESP CSLB & ESPConcrete nouns ? information combination
Cor
rela
tion
Combination Method
Vector ConcatenationCCASVDOur Model (?=1)
0.282
0.265 0.25
0.07
0.236
0.364
0.06
0.116
0.197
0.177 0.172 0.175 0.167 0.175
0.225
0.0
0.1
0.2
0.3
0.4
abstract nouns all nouns concrete verbs abstract verbs all verbsMore abstract concepts ? information propagation (CSLB & ESP
Cor
rela
tion
Propagation Method
Johns and JonesRidge RegressionOur Model (?=1)
Figure 3: The proposed approach compared with other methods of information combination (left) and
propagation. Dashed lines indicate language-only model baseline. For brevity we include both perceptual
input sources ESP and CSLB when comparing means of propagation; results with individual information
sources were similar.
to concrete nouns as previously.
Of our evaluation sets, it was possible to con-
struct from ESP (and add to P
ESP
) representa-
tions for all of the concrete verbs, and for ap-
proximately half of the abstract verbs and abstract
nouns. Figure 4 (top), shows the performance of
a our model trained on all available perceptual in-
put versus the model in which the perceptual input
was restricted to concrete nouns.
The results reflect a clear manifestation of the
abstract/concrete distinction. Concrete verbs be-
have similarly to concrete nouns, in that they can
be effectively represented directly from perceptual
information sources. The information encoded in
these representations is beneficial to the model and
increases performance. In contrast, constructing
?perceptual? representations of abstract verbs and
abstract nouns directly from perceptual informa-
tion sources is clearly counter-productive (to the
extent that performance also degrades on the com-
bined sets all nouns and all verbs). It appears in
these cases that the perceptual input acts to ob-
scure or contradict the otherwise useful signal in-
ferred from the corpus.
As shown in the previous section, the inclusion
of any form of perceptual input inhibits the learn-
ing of abstract nouns. However, this is not the case
for abstract verbs. Our model learns higher qual-
ity representations of abstract verbs if perceptual
input is restricted to concrete nouns than if no per-
ceptual input is included at all and when percep-
tual input is included for both concrete nouns and
abstract verbs. This supports the idea of a grad-
ual scale of concreteness: The most concrete con-
cepts can be effectively represented directly in the
perceptual modality; somewhat more abstract con-
cepts cannot be represented directly in the percep-
tual modality, but have representations that are im-
proved by propagating perceptual input from con-
crete concepts via language; and the most abstract
concepts are best acquired via language alone.
3.4 Source and quantity of perceptual input
For different concept types, we tested the effect of
varying the proportion of perceptual to linguistic
input (the parameter ?). Perceptual input was re-
stricted to concrete nouns as in Sections 3.1-3.2.
As shown in Figure 4, performance on concrete
nouns improves (albeit to a decreasing degree) as
? increases. When learning concrete noun rep-
resentations, linguistic input is apparently redun-
dant if perceptual input is of sufficient quality and
quantity. For the other concept types, in each case
there is an optimal value for ? in the range .5?2,
above which perceptual input obscures the linguis-
tic signal and performance degrades. The prox-
imity of these optima to 1 suggests that for op-
timal learning, when a concrete concept is experi-
enced approximately equal weight should be given
to available perceptual and linguistic information.
4 Conclusions
Motivated by the notable prevalence of abstract
concepts in everyday language, and their likely
importance to flexible, general-purpose represen-
tation learning, we have investigated how abstract
and concrete representations can be acquired by
multi-modal models. In doing so, we presented a
simple and easy-to-implement architecture for ac-
quiring semantic representations of both types of
262
0.1
0.2
0.3
0 1 2 3 4 5?
Cor
rela
tion
Concrete Nouns
0.1
0.2
0.3
0 1 2 3 4 5?
Abstract Nouns
0.1
0.2
0.3
0 1 2 3 4 5?
Concrete Verbs
0.1
0.2
0.3
0 1 2 3 4 5?
Perceptual Input
CSLB
ESP
CSLB & 
 ESP
Text?only
Abstract Verbs
0.267 0.295
0.136
0.249
0.335 0.364 0.337
0.176
0.087
0.166 0.201
0.225
0.0
0.1
0.2
0.3
0.4
concrete 
 nouns
abstract 
 nouns
all nouns concrete 
 verbs abstract  verbs all verbsConcept Type
Cor
rela
tion
Perceptual Information Source Direct representation Propagation
Our Model ? = 1
Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual in-
formation where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of
increasing ? on correlation with USF pairs (Spearman ?) for each concept type. Horizontal dashed lines
indicate language-only model baseline.
concept from linguistic and perceptual input.
While neuro-probabilistic language models
have been applied to the problem of multi-modal
representation learning previously (Srivastava and
Salakhutdinov, 2012; Wu et al., 2013; Silberer and
Lapata, 2014) our model and experiments develop
this work in several important ways. First, we ad-
dress the problem of learning abstract concepts.
By isolating concepts of different concreteness
and part-of-speech in our evaluation sets, and sep-
arating the processes of information combination
and propagation, we demonstrate that the multi-
modal approach is indeed effective for some, but
perhaps not all, abstract concepts. In addition, our
model introduces a clear parallel with human lan-
guage learning. Perceptual input is introduced pre-
cisely when concrete concepts are ?experienced?
by the model in the corpus text, much like a lan-
guage learner experiencing concrete entities via
sensory perception.
Taken together, our findings indicate the utility
of distinguishing three concept types when learn-
ing representations in the multi-modal setting.
Type I Concepts that can be effectively repre-
sented directly in the perceptual modality. For
such concepts, generally concrete nouns or con-
crete verbs, our proposed approach provides a sim-
ple means of combining perceptual and linguistic
input. The resulting multi-modal representations
are of higher quality than those learned via other
approaches, resulting in a performance improve-
ment of over 10% in modelling free association.
Type II Concepts, including abstract verbs, that
cannot be effectively represented directly in the
perceptual modality, but whose representations
can be improved by joint learning from linguis-
tic input and perceptual information about related
concepts. Our model can effectively propagate
perceptual input (exploiting the relations inferred
from the linguistic input) from Type I concepts to
enhance the representations of Type II concepts
above the language-only baseline. Because of the
frequency of abstract concepts, such propagation
extends the benefit of the multi-modal approach to
a far wider range of language than models based
solely in the concrete domain.
Type III Concepts that are more effectively
learned via language-only models than multi-
modal models, such as abstract nouns. Neither
263
our proposed approach nor alternative propagation
methods achieve an improvement in representa-
tion quality for these concepts over the language-
only baseline. Of course, it is an empirical ques-
tion whether a multi-modal approach could ever
enhance the representation learning of these con-
cepts, one with potential implications for cognitive
theories of grounding (a topic of much debate in
psychology (Grafton, 2009; Barsalou, 2010)).
Additionally, we investigated the optimum type
and quantity of perceptual input for learning con-
cepts of different types. We showed that too much
perceptual input can result in degraded represen-
tations. For concepts of type I and II, the op-
timal quantity resulted from setting ? = 1; i.e.
whenever a concrete concept was encountered, the
model learned from an equal number of language-
based and perception-based examples. While we
make no formal claims here, such observations
may ultimately provide insight into human lan-
guage learning and semantic memory.
In future we will address the question of
whether Type III concepts can ever be enhanced
via multi-modal learning, and investigate multi-
modal models that optimally learn concepts of
each type. This may involve filtering the percep-
tual input stream for concepts according to con-
creteness, and possibly more elaborate model ar-
chitectures that facilitate distinct representational
frameworks for abstract and concrete concepts.
Acknowledgements
Thanks to the Royal Society and St John?s College
for supporting this research, and to Yoshua Bengio
and Diarmuid
?
O S?eaghdha for helpful discussions.
References
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463.
Lawrence W Barsalou and Katja Wiemer-Hastings.
2005. Situating abstract concepts. Grounding Cog-
nition: The Role of Perception and Action in Mem-
ory, Language, and Thought, pages 129?163.
Lawrence W Barsalou. 2010. Grounded cognition:
past, present, and future. Topics in Cognitive Sci-
ence, 2(4):716?724.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
Joan L Bybee and Paul J Hopper. 2001. Frequency and
the Emergence of Linguistic Structure, volume 45.
John Benjamins Publishing.
Nick Chater and Christopher D Manning. 2006. Prob-
abilistic models of language processing and acquisi-
tion. Trends in Cognitive Sciences, 10(7):335?344.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160?167. ACM.
Sebastian J Crutch and Elizabeth K Warrington. 2005.
Abstract and concrete concepts have structurally
different representational frameworks. Brain,
128(3):615?627.
Barry J Devereux, Lorraine K Tyler, Jeroen Geertzen,
and Billi Randall. 2013. The centre for speech, lan-
guage and the brain (cslb) concept property norms.
Behavior Research Methods, pages 1?9.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99. Asso-
ciation for Computational Linguistics.
Jerry A Fodor. 1983. The modularity of mind: An
essay on faculty psychology. MIT press.
Scott T Grafton. 2009. Embodied cognition and the
simulation of action to understand others. Annals of
the New York Academy of Sciences, 1156(1):97?117.
David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639?2664.
Felix Hill, Anna Korhonen, and Christian Bentz.
2013. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive Science.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Multi-modal models for abstract and concrete con-
cept semantics. Transactions of the Association for
Computational Linguistics.
264
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Brendan T Johns and Michael N Jones. 2012. Per-
ceptual inference through global lexical similarity.
Topics in Cognitive Science, 4(1):103?120.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of the annual meeting of the
Association for Computational Linguistics. ACL.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the British National
Corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547?
559.
Gr?egoire Mesnil, Yann Dauphin, Xavier Glorot, Salah
Rifai, Yoshua Bengio, Ian J Goodfellow, Erick
Lavoie, Xavier Muller, Guillaume Desjardins, David
Warde-Farley, et al. 2012. Unsupervised and trans-
fer learning challenge: a deep learning approach.
Journal of Machine Learning Research-Proceedings
Track, 27:97?110.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international Workshop on Arti-
ficial Intelligence and Statistics, pages 246?252.
Raymond H Myers. 1990. Classical and Modern
Regression with Applications, volume 2. Duxbury
Press Belmont, CA.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, & Com-
puters, 36(3):402?407.
Allan Paivio. 1991. Dual coding theory: Retrospect
and current status. Canadian Journal of Psychology,
45(3):255.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433. As-
sociation for Computational Linguistics.
Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of the annual meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with vi-
sual attributes. In Proceedings of the 51th Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria, August.
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In NIPS, pages 2231?2239.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on human factors in computing
systems, pages 319?326. ACM.
Katja Wiemer-Hastings and Xu Xu. 2005. Content
differences for abstract and concrete concepts. Cog-
nitive Science, 29(5):719?736.
Pengcheng Wu, Steven CH Hoi, Hao Xia, Peilin Zhao,
Dayong Wang, and Chunyan Miao. 2013. On-
line multimodal deep similarity learning with ap-
plication to image retrieval. In Proceedings of the
21st ACM International Conference on Multimedia,
pages 153?162. ACM.
265
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 278?289,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
An Unsupervised Model for Instance Level Subcategorization Acquisition
Simon Baker
Computer Laboratory
University of Cambridge
sb895@cam.ac.uk
Roi Reichart
Technion, IIT
Haifa, Israel
roiri@ie.technion.ac.il
Anna Korhonen
Computer Laboratory
University of Cambridge
alk23@cam.ac.uk
Abstract
Most existing systems for subcategoriza-
tion frame (SCF) acquisition rely on su-
pervised parsing and infer SCF distribu-
tions at type, rather than instance level.
These systems suffer from poor portability
across domains and their benefit for NLP
tasks that involve sentence-level process-
ing is limited. We propose a new unsuper-
vised, Markov Random Field-based model
for SCF acquisition which is designed
to address these problems. The system
relies on supervised POS tagging rather
than parsing, and is capable of learning
SCFs at instance level. We perform eval-
uation against gold standard data which
shows that our system outperforms several
supervised and type-level SCF baselines.
We also conduct task-based evaluation in
the context of verb similarity prediction,
demonstrating that a vector space model
based on our SCFs substantially outper-
forms a lexical model and a model based
on a supervised parser
1
.
1 Introduction
Subcategorization frame (SCF) acquisition in-
volves identifying the arguments of a predicate
and generalizing about its syntactic frames,
where each frame specifies the syntactic type and
number of arguments permitted by the predicate.
For example, in sentences (1)-(3) the verb distin-
guish takes three different frames, the difference
between which is not evident when considering
the phrase structure categorization:
(1) Direct Transitive: [They]NP [distin-
guished]VP [the mast]NP [of [ships on the
horizon ]NP ]PP .
1
The verb similarity dataset used for the evaluation of our
model is publicly available at ie.technion.ac.il/?roiri/.
(2) Indirect Transitive: [They]NP [distin-
guished]VP [between [me and you]ADVP ]PP .
(3) Ditransitive: [They]NP [distinguished]VP
[him]NP [from [the other boys]NP ]PP.
As SCFs describe the syntactic realization of
the verbal predicate-argument structure, they are
highly valuable for a variety of NLP tasks. For
example, verb subcategorization information has
proven useful for tasks such as parsing (Carroll
and Fang, 2004; Arun and Keller, 2005; Cholakov
and van Noord, 2010), semantic role labeling
(Bharati et al., 2005; Moschitti and Basili, 2005),
verb clustering, (Schulte im Walde, 2006; Sun
and Korhonen, 2011) and machine translation (hye
Han et al., 2000; Haji?c et al., 2002; Weller et al.,
2013).
SCF induction is challenging. The argument-
adjunct distinction is difficult even for humans,
and is further complicated by the fact that both ar-
guments and adjuncts can appear frequently in po-
tential argument head positions (Korhonen et al.,
2000). SCFs are also highly sensitive to domain
variation so that both the frames themselves and
their probabilities vary depending on the meaning
and behavior of predicates in the domain in ques-
tion (e.g. (Roland and Jurafsky, 1998; Lippincott
et al., 2010; Rimell et al., 2013), Section 4).
Because of the strong impact of domain vari-
ation, SCF information is best acquired automat-
ically. Existing data-driven SCF induction sys-
tems, however, do not port well between do-
mains. Most existing systems rely on hand-
written rules (Briscoe and Carroll, 1997; Korho-
nen, 2002; Preiss et al., 2007) or simple co-
occurrence statistics (O?Donovan et al., 2005;
Chesley and Salmon-Alt, 2006; Ienco et al., 2008;
Messiant et al., 2008; Lenci et al., 2008; Al-
tamirano and Alonso i Alemany, 2010; Kawa-
hara and Kurohashi, 2010) applied to the gram-
matical dependency output of supervised statisti-
cal parsers. Even the handful of recent systems
278
that use modern machine learning techniques (De-
bowski, 2009; Lippincott et al., 2012; Van de
Cruys et al., 2012; Reichart and Korhonen, 2013)
use supervised parsers to pre-process the data
2
.
Supervised parsers are notoriously sensitive to
domain variation (Lease and Charniak, 2005). As
annotation of data for each new domain is un-
realistic, current SCF systems suffer from poor
portability. This problem is compounded for
the many systems that employ manually devel-
oped SCF rules because rules are inherently ig-
norant to domain-specific preferences. The few
SCF studies that focused on specific domains (e.g.
biomedicine) have reported poor performance due
to these reasons (Rimell et al., 2013).
Another limitation of most current SCF systems
is that they produce a type-level SCF lexicon (i.e.
a lexicon which lists, for a given predicate, dif-
ferent SCF types with their relative frequencies).
Such a lexicon provides a useful high-level pro-
file of the syntactic behavior of the predicate in
question, but is less useful for downstream NLP
tasks (e.g. information extraction, parsing, ma-
chine translation) that involve sentence processing
and can therefore benefit from SCF information
at instance level. Sentences (1)-(3) demonstrate
this limitation - a prior distribution over the pos-
sible syntactic frames of distinguish provides only
a weak signal to a sentence level NLP application
that needs to infer the verbal argument structure of
its input sentences.
We propose a new unsupervised model for SCF
induction which addresses these problems with
existing systems. Our model does not use a parser
or hand-written rules, only a part-of-speech (POS)
tagger is utilizes in order to produce features for
machine learning. While POS taggers are also
sensitive to domain variation, they can be adapted
to domains more easily than parsers because they
require much smaller amounts of annotated data
(Lease and Charniak, 2005; Ringger et al., 2007).
However, as we demonstrate in our experiments,
domain adaptation of POS tagging may not even
be necessary to obtain good results on the SCF ac-
quisition task.
Our model, based on the Markov Random Field
(MRF) framework, performs instance-based SCF
learning. It encodes syntactic similarities among
verb instances across different verb types (derived
2
(Lippincott et al., 2012) does not use a parser, but the
syntactic frames induced by the system do not capture sets of
arguments for verbs, so are not SCFs in a traditional sense.
from a lexical and POS-based feature representa-
tion of verb instances) as well as prior beliefs on
the tendencies of specific instances of the same
verb type to take the same SCF.
We evaluate our model against corpora anno-
tated with verb instance SCFs (Quochi et al.,
2012). In addition, following the Levin verb
clustering tradition (Levin, 1993) which ties verb
meanings with their syntactic properties, we eval-
uate the semantic predictive power of our clusters.
In the former evaluation, our model outperforms a
number of strong baselines, including supervised
and type-level ones, achieving an accuracy of up
to 69.2%. In the latter evaluation a vector space
model that utilized our induced SCFs substantially
outperforms the output of a type-level SCF system
that uses the fully trained Stanford parser.
2 Previous Work
Several SCF acquisition systems are available for
English (O?Donovan et al., 2005; Preiss et al.,
2007; Lippincott et al., 2012; Van de Cruys et
al., 2012; Reichart and Korhonen, 2013) and other
languages, including French (Messiant, 2008),
Italian (Lenci et al., 2008), Turkish (Uzun et al.,
2008), Japanese (Kawahara and Kurohashi, 2010)
and Chinese (Han et al., 2008). The promi-
nent input to these systems are grammatical re-
lations (GRs) which express binary dependen-
cies between words (e.g. direct and indirect ob-
jects, various types of complements and conjunc-
tions). These are generated by some parsers (e.g.
(Briscoe et al., 2006)) and can be extracted from
the output of others (De-Marneffe et al., 2006).
Two representative systems for English are the
Cambridge system (Preiss et al., 2007) and the
BioLexicon system which was used to acquire a
substantial lexicon for biomedicine (Venturi et al.,
2009). These systems extract GRs at the verb in-
stance level from the output of a parser: the RASP
general-language unlexicalized parser
3
(Briscoe et
al., 2006) and the lexicalized Enju parser tuned to
the biomedical domain (Miyao and Tsujii, 2005),
respectively. They generate potential SCFs by
mapping GRs to a predefined SCF inventory us-
ing a set of manually developed rules (the Cam-
bridge system) or by simply considering the sets
of GRs including verbs in question as potential
SCFs (BioLexicon). Finally, a type level lexicon
3
A so-called unlexicalized parser is a parser trained with-
out explicit SCF annotations.
279
is built through noisy frame filtering (based on
frequencies or on external resources and annota-
tions), which aims to remove errors from parsing
and argument-adjunct distinction. Clearly, these
systems require extensive manual work: a-priori
definition of an SCF inventory and rules, manu-
ally annotated sentences for training a supervised
parser, SCF annotations for parser lexicalization,
and manually developed resources for optimal fil-
tering.
A number of recent works have applied mod-
ern machine learning techniques to SCF induc-
tion, including point-wise co-occurrence of ar-
guments (Debowski, 2009), a Bayesian network
model (Lippincott et al., 2012), multi-way tensor
factorization (Van de Cruys et al., 2012) and De-
terminantal Point Processes (DPPs) -based clus-
tering (Reichart and Korhonen, 2013). However,
all of these systems induce type-level SCF lexi-
cons and, except from the system of (Lippincott et
al., 2012) that is not capable of learning traditional
SCFs, they all rely on supervised parsers.
Our new system differs from previous ones in
a number of respects. First, in contrast to most
previous systems, our system provides SCF anal-
ysis for each verb instance in its sentential con-
text, yielding more precise SCF information for
systems benefiting from instance-based analysis.
Secondly, it addresses SCF induction as an unsu-
pervised clustering problem, avoiding the use of
supervised parsing or any of the sources of man-
ual supervision used in previous works. Our sys-
tem relies on POS tags - however, we show that it
is not necessary to train a tagger with in-domain
data to obtain good performance on this task, and
therefore our approach provides a more domain-
independent solution to SCF acquisition.
We employ POS-tagging instead of unsuper-
vised parsing for two main reasons. First, while
a major progress has been made on unsupervised
parsing (e.g. (Cohen and Smith, 2009; Berg-
Kirkpatrick et al., 2010)), the performance is still
considerably behind that of supervised parsing.
For example, the state-of-the-art discriminative
model of (Berg-Kirkpatrick et al., 2010) achieves
only 63% directed arc accuracy for WSJ sentences
of up to 10 words, compared to more than 95%
obtained with supervised parsers. Second, current
unsupervised parsers produce unlabeled structures
which are substantially less useful for SCF acqui-
sition than labeled structures produced by super-
vised parsers (e.g. grammatical relations).
Finally, a number of recent works addressed re-
lated tasks such as argument role clustering for
SRL (Lang and Lapata, 2011a; Lang and Lapata,
2011b; Titvo and Klementiev, 2012) in an unsu-
pervised manner. While these works differ from
ours in the task (clustering arguments rather than
verbs) and the level of supervision (applying a su-
pervised parser), like us they analyze the verb ar-
gument structure at the instance level.
3 Model
We address SCF induction as an unsupervised
verb instance clustering problem. Given a set of
plain sentences, our algorithm aims to cluster the
verb instances in its input into syntactic clusters
that strongly correlate with SCFs. In this sec-
tion we introduce a Markov Random Field (MRF)
model for this task: Section 3.1 describes our
model?s structure, components and objective; Sec-
tion 3.2 describes the model potentials and the
knowledge they encode; and Section 3.3 describes
how clusters are induced from the model.
3.1 Model Structure
We implement our model in the MRF framework
(Koller and Friedman, 2009). This enables us to
encode the two main sources of information that
govern SCF selection in verb instances: (1) At
the sentential context, the verbal syntactic frame
is encoded through syntactic features. Verb in-
stances with similar feature representations should
therefore take the same syntactic frame; and (2)
At the global context, per verb type SCF distribu-
tions tend to be Zipfian (Korhonen et al., 2000).
Instances of the same verb type should therefore
be biased to take the same syntactic frame.
Given a collection of plain input sentences, we
denote the number of verb instances in the col-
lection with n, and the number of data-dependent
equivalence classes (ECs) with K (see below for
their definition), and define an undirected graphi-
cal model (MRF), G = (V,E, L). We define the
vertex set as V = X ?C, with X = {x
1
, . . . , x
n
}
consisting of one vertex for every verb instance in
the input collection, and C = {c
1
. . . c
K
} consist-
ing of one vertex for each data-dependent EC. The
set of labels used by the model, L, corresponds to
the syntactic frames taken by the verbs in the in-
put data. The edge set E is defined through the
model?s potentials that are described below.
280
We encode information in the model through
three main sets of potentials: one set of single-
ton potentials - defined over individual model ver-
texes, and two sets of pairwise potentials - defined
between pairs of vertexes. The first set consists of
a singleton potential for each vertex in the model.
Reflecting the Zipfian distribution of SCFs across
the instances of the same verb type, these poten-
tials encourage the model to assign such verb in-
stances to the same frame (cluster). The infor-
mation encoded in these potentials is induced via
a pre-processing clustering step. The second set
consists of a pairwise potential for each pair of ver-
texes x
i
, x
j
? X - that is, for each verb instance
pair in the input, across verb types. These poten-
tials encode the belief, computed as feature-based
similarity (see below), that their verb instance ar-
guments implement the same SCF.
Finally, potentials from the last set bias the
model to assign the same SCF to high cardinal-
ity sets of cross-type verb instances based on their
syntactic context. While these are pairwise poten-
tials defined between verb instance vertexes (X)
and EC vertexes (C), they are designed so that
they bias the assignment of all verb instance ver-
texes that are connected to the same EC vertex to-
wards the same frame assignment (l ? L). The
two types of pairwise potentials complement each
other by modeling syntactic similarities among
verb instance pairs, as well as among higher cardi-
nality verb instance sets.
The resulted maximum aposteriori problem
(MAP) takes the following form:
MAP (V ) = argmax
x,c?V
n?
i=1
?
i
(x
i
) +
n?
i=1
n?
j=1
?
i,j
(x
i
, x
j
)+
n?
i=1
K?
j=1
?
i,j
(x
i
, c
j
) ? I(x
i
? EC
j
) +
K?
i=1
K?
j=1
?
i,j
(c
i
, c
j
)
where the predicate I(x
i
? EC
j
) returns 1 if
the i-th verb instance belongs the j-th equivalence
class and 0 otherwise. The ? pairwise potentials
defined between EC vertexes are very simple po-
tentials designed to promise different assignments
for each pair of EC vertexes. They do so by assign-
ing a ?? score to assignments where their argu-
ment vertexes take the same frame and a 0 other-
wise. In the rest of this section we do not get back
to this simple set of potentials.
A graphical illustration of the model is given
in Figure 1. Note that we could have selected a
richer model structure, for example, by defining
a similarity potential over all verb instance ver-
texes that share an equivalence class. However, as
the figure demonstrates, even the structure of the
pruned version of our model (see Section 3.3) usu-
ally contains cycles, which makes inference NP-
hard (Shimony, 1994). Our design choices aim to
balance between the expressivity of the model and
the complexity of inference. In Section 3.3 we de-
scribe the LP relaxation algorithm we use for in-
ference.
C1 C2
Figure 1: A graphical illustration of our model
(after pruning, see Sec. 3.3) for twenty verb in-
stances (|X| = 20), each represented with a black
vertex, and two equivalence classes (ECs), each
represented with a gray vertex (|C| = 2). Solid
lines represent edges (and ?
i,j
pairwise potentials)
between verb instance vertexes. Dashed lines rep-
resent edges between verb instance vertexes and
EC vertexes (?
i,j
pairwise potentials) or between
EC vertexes (?
i,j
pairwise potentials) .
3.2 Potentials and Encoded Knowledge
Pairwise Syntactic Similarity Potentials. The
pairwise syntactic similarity potentials are defined
for each pair of verb instance vertexes, x
i
, x
j
? X .
They are designed to encourage the model to as-
sign verb instances with similar fine-grained fea-
ture representations to the same frame (l ? L)
and verb instances with dissimilar representations
to different frames. For this aim, for every verb
pair i, j with feature representation vectors v
i
, v
j
and verb instance vertexes x
i
, x
j
? X , we define
the following potential function:
?
i,j
(x
i
= l
1
, x
j
= l
2
) =
{
?(v
i
, v
j
) if l
1
= l
2
0 otherwise
}
Where l
1
, l
2
? L are label pairs and ? is a verb
instance similarity function. Below we describe
the feature representation and the ? function.
The verb instance feature representation is de-
fined through the following process. For each
281
word instance in the input sentences we first build
a basic feature representation (see below). Then,
for each verb instance we construct a final fea-
ture representation defined to be the concatena-
tion of that verb?s basic feature representation with
the basic representations of the words in a size
2 window around the represented verb. The fi-
nal feature representation for the i-th verb in-
stance in our dataset is therefore defined to be
v
i
= [w
?2
, w
?1
, vb
i
, w
+1
, w
+2
], where w
?k
and
w
+k
are the basic feature representations of the
words in distance ?k or +k from the i-th verb in-
stance in its sentence, and vb
i
is the basic feature
representation of that verb instance.
Our basic feature representation is inspired
from the feature representation of the MST parser
(McDonald et al., 2005) except that in the parser
the features represent a directed edge in the com-
plete directed graph defined over the words in a
sentence that is to be parsed, while our features are
generated for word n-grams. Particularly, our fea-
ture set is a concatenation of two sets derived from
the MST set described in Table 1 of (McDonald et
al., 2005) in the following way: (1) In both sets the
parent word in the parser?s set is replaced with the
represented word; (2) In one set every child word
in the parser?s set is replaced by the word to the
left of the represented word and in the other set it
is replaced by the word to its right. This choice of
features allows us to take advantage of a provably
useful syntactic feature representation without the
application of any parse tree annotation or parser.
We compute the similarity between the syntac-
tic environments of two verb instances, i, j, using
the following equation:
?(v
i
, v
j
) = W ? cos(v
i
, v
j
)? S
Where W is a hyperparameter designed to bias
verb instances of the same verb type towards the
same frame. Practically, W was tuned to be 3 for
instances of the same type, and 1 otherwise
4
.
While the cosine function is the standard mea-
sure of similarity between two vectors, its val-
ues are in the [0, 1] range. In the MRF modeling
framework, however, we must encode a negative
pairwise potential value between two vertexes in
order to encourage the model to assign different
labels (frames) to them. We therefore added the
positive hyperparameter S which was tuned, with-
4
All hyperparameters that require gold-standard annota-
tion for tuning, were tuned using held-out data (Section 4).
out access to gold standard manual annotations, so
that there is an even number of negative and pos-
itive pairwise syntactic similarity potentials after
the model is pruned (see Section 3.3)
5
.
Type Level Singleton Potentials. The goal of
these potentials is to bias verb instances of the
same type to be assigned to the same syntactic
frame while still keeping the instance based nature
of our algorithm. For this aim, we applied Algo-
rithm 1 for pre-clustering of the verb instances and
encoded the induced clusters into the local poten-
tials of the corresponding x ? X vertexes. For
every x ? X the singleton potential is therefore
defined to be:
?
i
(x
i
= l) =
{
F ? max? if l is induced by Algorithm 1
0 otherwise
}
where max? is the maximum ? score across all
verb instance pairs in the model and F = 0.2 is a
hyperparamter.
Algorithm 1 has two hyperparameters: T and
M , the first is a similarity cut-off value used to de-
termine the initial set of clusters, while the second
is used to determine whether two clusters are simi-
lar enough to be merged. We tuned these hyperpa-
rameters, without manually annotated data, so that
the number of clusters induced by this algorithm
will be equal to the number of gold standard SCFs.
T was tuned so that the first part of the algorithm
generates an excessive number of clusters, and M
was then tuned so that these clusters are merged to
the desired number of clusters.
The ? function, used to measure the similar-
ity between two verbs, is designed to bias the in-
stances of the same verb type to have a higher sim-
ilarity score. Algorithm 1 therefore tends to assign
such instances to the same cluster. In our experi-
ments that was always the case for this algorithm.
High Cardinality Verb Sets Potentials. This
set of potentials aims to bias larger sets of verb
instances to share the same SCF. It is inspired by
(Rush et al., 2012) who demonstrated, that syn-
tactic structures that appear at the same syntac-
tic context, in terms of the surrounding POS tags,
tend to manifest similar syntactic behavior. While
they demonstrated the usefulness of their method
for dependency parsing and POS tagging, we im-
plement it for higher level SCFs.
We identified syntactic contexts that imply simi-
lar SCFs for verb instances appearing inside them.
5
The values in practice are S = 0.43 for labour legislation
and S = 0.38 for environment.
282
Algorithm 1 Verb instance pre-clustering algo-
rithm.
?
? is the average ? score between the mem-
bers of its cluster arguments. T and M are hyper-
parametes tuned without access to gold standard
data.
Require: K = ?
for all x ? X do
for all k ? K do
for all u ? k do
if ?(v
x
, v
u
) > T then
k = k ? {x}
Go to next x
end if
end for
end for
k1 = {x}
K = K ? k1
end for
for all k
1
, k
2
? K: k
1
6= k
2
do
if
?
?(k
1
, k
2
) > M then
Merge (k
1
, k
2
)
end if
end for
Contexts are characterized by the coarse POS tag
to the left and to the right of the verb instance.
While the number of context sets is bounded only
by the number of frames our model is designed
to induce, in practice we found that defining two
equivalence sets led to the best performance gain,
and the sets we used are presented in Table 1.
In order to encode this information into our
MRF, each set of syntactic contexts is associated
with an equivalence class (EC) vertex c ? C and
the verb instance vertexes of all verbs that appear
in a context from that set are connected with an
edge to c. The pairwise potential between a vertex
x ? X and its equivalence class is defined to be:
?
i,j
(x
i
= l
1
, c
j
= l
2
) =
{
U if l
1
= l
2
0 otherwise
}
U = 10 is a hyperparameter that strongly biases x
vertexes to get the same SCF as their EC vertex.
3.3 Verb Cluster Induction
In this section we describe how we induce verb
instance clusters from our model. This process
is based on the following three steps: (1) Graph
pruning; (2) Induction of an Ensemble of approx-
imate MAP inference solutions in the resulted
graphical model; and, (3) Induction of a final clus-
tering solution based on the ensemble created at
step 2. Below we explain the necessity of each of
these steps and provide the algorithmic details.
EC-1 EC-2
Left Right Left Right
, D V T
N D R T
V . N D
R D R N
Table 1: POS contexts indicative for the syntactic
frame of the verb instance they surround. D: de-
terminer, N: noun, V: verb, T: the preposition ?to?
(which has its own POS tag in the WSJ POS tag set
which we use), R: adverb. EC-1 and EC-2 stand
for the first and second equivalence class respec-
tively. In addition, the following contexts where
associated with both ECs: (T,D), (T,N), (N,N)
and (V, I) where I stands for a preposition.
Graph Pruning. The edge set of our model
consists of an edge for every pair of verb in-
stance vertexes and of the edges that connect verb
instance vertexes and equivalence class vertexes.
This results in a large tree-width graph which sub-
stantially complicates MRF inference. To alleviate
this we prune all edges with a positive score lower
than p
+
and all edges with a negative score higher
than p
?
, where p
+
and p
?
are manually tuned hy-
perparametes
6
.
MAP Inference. For most reasonable values of
p
+
and p
?
our graph still contains cycles even af-
ter it is pruned, which makes inference NP-hard
(Shimony, 1994). Yet, thanks to our choice of an
edge-factorized model, there are various approxi-
mate inference algorithms suitable for our case.
We applied the message passing algorithm for
linear-programming (LP) relaxation of the MAP
assignment (MPLP, (Sontag et al., 2008)). LP re-
laxation algorithms for the MAP problem define
an upper bound on the original objective which
takes the form of a linear program. Consequently,
a minimum of this upper bound can be found us-
ing standard LP solvers or, more efficiently, using
specialized message passing algorithms (Yanover
et al., 2006). The MPLP algorithm described in
(Sontag et al., 2008) is appealing in that it itera-
tively computes tighter upper bounds on the MAP
objective (for details see their paper).
Cluster Ensemble Generation and a Final
Solution. As our MAP objective is non-convex,
6
The values used in practice are p
+
= 0.28, p
?
= ?0.17
for the labour legislation dataset, and p
+
= 0.25, p
?
=
?0.20 for the environment set.
283
the convergent point of an optimization algorithm
applied to it is highly sensitive to its initializa-
tion. To avoid convergence to arbitrary local max-
ima which may be of poor quality, we turn to a
perturbation protocol where we repeatedly intro-
duce random noise to the MRF?s potential func-
tions and then compute the approximate MAP so-
lution of the resulted model using the MPLP algo-
rithm. Noising was done by adding an  term to
the lambda values described in section 3.2
7
. This
protocol results in a set of cluster (label) assign-
ments for the involved verb instances, which we
treat as an ensemble of experts from which a final,
high quality, solution is to be induced.
The basic idea in ensemble learning is that if
several experts independently cluster together two
verb instances, our belief that these verbs belong
in the same cluster should increase. (Reichart et
al., 2012) implemented this idea through the k-
way normalized cut clustering algorithm (Yu and
Shi, 2003). Its input is an undirected graph
?
G =
(
?
V ,
?
E,
?
W ) where
?
V is the set of vertexes,
?
E is
the set of edges and
?
W is a non-negative and sym-
metric edge weight matrix. To apply this model
to our task, we construct the input graph
?
G from
the labelings (frame assignments) contained in the
ensemble. The graph vertexes
?
V correspond to the
verb instances and the (i, j)-th entry of the matrix
?
W is the number of ensemble members that assign
the same label to the i-th and j-th verb instances.
For A,B ?
?
V define:
links(A,B) =
?
i?A,j?B
?
W (i, j)
Using this definition, the normalized link ratio
of A and B is defined to be:
NormLinkRatio(A,B) =
links(A,B)
links(A,
?
V )
The k-way normalized cut problem is to mini-
mize the links that leave a cluster relative to the
total weight of the cluster. Denote the set of clus-
terings of
?
V that consist of k clusters by
?
C =
{c?
1
, . . . c?
t
} and the j-th cluster of the i-th cluster-
7
 was accepted by first sampling a number in the [0, 1]
range using the Java psuodorandom generator and then scal-
ing it to 1% of cos(v
i
, v
j
). This value was tuned, without
access to gold standard manual annotations, so that there is
an even number of negative and positive pairwise syntactic
similarity potentials after the model is pruned (Section 3.3).
ing by c?
ij
. Then
c
?
= argmin
c?
i
?
?
C
k
?
j=1
NormLinkRatio(c?
ij
,
?
V ? c?
ij
)
The algorithm of (Yu and Shi, 2003) solves this
problem very efficiently as it avoids the heavy
eigenvalues and eigenvectors computations re-
quired by traditional approaches.
4 Experiments and Results
Our model is unique compared to existing systems
in two respects. First, it does not utilize supervi-
sion in the form of either a supervised syntactic
parser and/or manually crafted SCF rules. Conse-
quently, it induces unnamed frames (clusters) that
are not directly comparable to the named frames
induced by previous systems. Second, it induces
syntactic frames at the verb instance, rather than
type, level. Evaluation, and especially comparison
to previous work, is therefore challenging.
We therefore evaluate our system in two ways.
First, we compare its output, as well as the output
of a number of clustering baselines, to the gold
standard annotation of corpora from two differ-
ent domains (the only publicly available ones with
instance level SCF annotation, to the best of our
knowledge). Second, in order to compare the out-
put of our system to a rule-based SCF system that
utilizes a supervised syntactic parser, we turn to
a task-based evaluation. We aim to predict the
degree of similarity between verb pairs and, fol-
lowing (Pado and Lapata, 2007) , we do so using
a syntactic-based vector space model (VSM). We
construct three VSMs - (a) one that derives fea-
tures from our clusters; (b) one whose features
come from the output of a state-of-the-art verb
type level, rule based, SCF system (Reichart and
Korhonen, 2013) that uses a modern parser (Klein
and Manning, 2003); and (c) a standard lexical
VSM. Below we show that our system compares
favorably in both evaluations.
Data. We experimented with two datasets taken
from different domains: labor legislation and en-
vironment (Quochi et al., 2012). These datasets
were created through web crawling followed by
domain filtering. Each sentence in both datasets
may contain multiple verbs but only one target
verb has been manually annotated with a SCF.
The labour legislation domain dataset contains
4415 annotated verb instances (and hence also
284
sentences) of 117 types, and the environmental
domain dataset contains 4503 annotated verb in-
stances of 116 types. In both datasets no verb type
accounts for more than 4% of the instances and
only up to 35 verb types account for 1% of the
instances or more. The lexical difference between
the corpora is substantial: they share only 42 anno-
tated verb types in total, of which only 2 verb types
(responsible for 4.1% and 5.2% of the instances in
the environment and labor legislation domains re-
spectively) belong to the 20 most frequent types
(responsible for 37.9% and 46.85% of the verb in-
stances in the respective domains) of each corpus.
The 29 members of the SCF inventory are de-
tailed in (Quochi et al., 2012). Table 2, presenting
the distribution of the 5 highest frequency frames
in each corpus, demonstrates that, in addition to
the significant lexical difference, the corpora differ
to some extent in their syntactic properties. This is
reflected by the substantially different frequencies
of the ?dobj:iobj-prep:su? and ?dobj:su? frames.
As a pre-processing step we first POS tagged
the datasets with the Stanford tagger (Toutanova
et al., 2003) trained on the standard POS training
sections of the WSJ PennTreebank corpus.
4.1 Evaluation Against SCF Gold Standard
Experimental Protocol The computational com-
plexity of our algorithm does not allow us to run it
on thousands of verb instances in a feasible time.
We therefore repeatedly sampled 5% of the sen-
tences from each dataset, ran our algorithm as well
as the baselines (see below) and report the average
performance of each method. The number of rep-
etitions was 40 and samples were drawn from a
uniform distribution while still promising that the
distribution of gold standard SCFs in each sam-
ple is identical to their distribution in the entire
dataset. Before running this protocol, 5% of each
corpus was kept as held-out data on which hyper-
parameter tuning was performed.
EvaluationMeasures and Baselines. We com-
pare our system?s output to instance-level gold
standard annotation. We use standard measures
for clustering evaluation, one measure from each
of the two leading measure types: the V measure
(Rosenberg and Hirschberg, 2007), which is an in-
formation theoretic measure, and greedy many-to-
one accuracy, which is a mapping-based measure.
For the latter, each induced cluster is first mapped
to the gold SCF frame that annotates the highest
number of verb instances this induced cluster also
annotates and then a standard instance-level accu-
racy score is computed (see, e.g., (Reichart and
Rappoport, 2009)). Both measures scale from 100
(perfect match with gold standard) to 0 (no match).
As mentioned above, comparing the perfor-
mance of our system with respect to a gold stan-
dard to the performance of previous type-level
systems that used hand-crafted rules and/or su-
pervised syntactic parsers would be challenging.
We therefore compare our model to the follow-
ing baselines: (a) The most frequent class (MFC)
baseline which assigns all verb instances with the
SCF that is the most frequent one in the gold stan-
dard annotation of the data; (b) The Random base-
line which simply assigns every verb instance with
a randomly selected SCF; (c) Algorithm 1 of sec-
tion 3.2 which generates unsupervised verb in-
stance clustering such that verb instances of the
same type are assigned to the same cluster; and
(d) Finally, we also compare our model against
versions where everything is kept fixed, except a
subset of potentials which is omitted. This enables
us to study the intricacies of our model and the rel-
ative importance of its components. For all mod-
els, the number of induced clusters is equal to the
number of SCFs in the gold standard.
Results Table 3 presents the results, demon-
strating that our full model substantially outper-
forms all baselines. For the first two simple heuris-
tic baselines (MFC and Random) the margin is
higher than 20% for both the greedy M-1 mapping
measure and the V measure. Note tat the V score
of the MFC baseline is 0 by definition, as it as-
signs all items to the same cluster. The poor per-
formance of these simple baselines is an indication
of the difficulty of our task.
Recall that the type level clustering induced by
Algorithm 1 is the main source of type level in-
formation our model utilizes (through its single-
ton potentials). The comparison to the output of
this algorithm (the Type Pre-clustering baseline)
therefore shows the quality of the instance level
refinement our model provides. As seen in table 3,
our model outperforms this baseline by 6.9% for
the M-1 measure and 5.2% for the V measure.
In order to compare our model to its compo-
nents we exclude either the EC potentials (? and
?) only (Model - EC), or the EC and the singleton
potentials (?
i
, Model - EC - Type pre-clustering).
The results show that our model gains much more
285
Environment Labour Legislation
SCF Frequency SCF Frequency
dobj:su 46% dobj:su 39%
su 9% dobj:iobj-prep:su 15%
iobj-prep:su 8% su 10%
dobj:iobj-prep:su 6% su:xcompto-vbare 8%
su:xcompto-vbare 6% iobj-prep:su 7%
Table 2: Top 5 most frequent SCFs for the Environment and Labour Legislation datasets used in our
experiments.
Environment Labour Legislation
M-1 V M-1 V
Full Model 66.4 57.3 69.2 55.6
Baselines
MFC 46.2 0 39.4 0
Random 34.6 28.1 36.5 27.8
Type Pre-clustering 60.1 52.1 62.3 51.4
Model Components
Model - EC 64.9 56.2 67.4 54.6
Model - EC - Type pre-clustering 48.3 48.9 45.7 44.7
Table 3: Results for our full model, the baselines (Type Pre-clustering: the pre-clustering algorithm
(Algorithm 1 of section 3.2), MFC: the most frequent class (SCF) in the gold standard annotation and
Random: random SCF assignment) and the model components. The full model outperforms all other
models across measures and datasets.
from the type level information encoded through
the singleton potentials than from the EC poten-
tials. Yet, EC potentials do lead to an improvement
of up to 1.5% in M-1 and up to 1.1% in V and are
therefore responsible for up to 26.1% and 21.2%
of the improvement over the type pre-clustering
baseline in terms of M-1 and V, respectively.
4.2 Task Based Evaluation
We next evaluate our model in the context of vec-
tor space modeling for verb similarity prediction
(Turney and Pantel, 2010). Since most previous
word similarity works used noun datasets, we con-
structed a new verb pair dataset, following the pro-
tocol used in the collection of the wordSimilarity-
353 dataset (Finkelstein et al., 2002).
Our dataset consists of 143 verb pairs, con-
structed from 122 unique verb lemma types. The
participating verbs appear ? 10 times in the con-
catenation of the labour legislation and the envi-
ronment datasets. Only pairs of verbs that were
considered at least remotely similar by human
judges (independent of those that provided the
similarity scores) were included. A similarity
score between 1 and 10 was assigned to each pair
by 10 native English speaking annotators and were
then averaged in order to get a unique pair score.
Our first baseline is a standard VSM based on
lexical collocations. In this model features corre-
spond to the number of collocations inside a size
2 window of the represented verb with each of the
5000 most frequent nouns in the Google n-gram
corpus (Goldberg and Orwant, 2013). Since our
corpora are limited in size, we use the collocation
counts from the Google corpus.
We used our model to generate a vector repre-
sentation of each verb in the following way. We
run the model 5000 times, each time over a set of
verbs consisting of one instance of each of the 122
verb types participating in the verb similarity set.
The output of each such run is transformed to a
binary vector for each participating verb, where
all coordinates are assigned the value of 0, ex-
cept from the one that corresponds to the cluster to
which the verb was assigned which has the value
of 1. The final vector representation is a concate-
nation of the 5000 binary vectors. Note that for
this task we did not use the graph cut algorithm to
generate a final clustering from the multiple MRF
286
runs. Instead we concatenated the output of all
these runs into one feature representation that fa-
cilitates similarity prediction. For our model we
estimated the verb pair similarity using the Tani-
mato similarity score for binary vectors:
T (X,Y ) =
?
i
X
i
? Y
i
?
i
x
i
? Y
i
For the baseline model, where the features are
collocation counts, we used the standard cosine
similarity.
Our second baseline is identical to our model,
except that: (a) the data is parsed with the Stan-
ford parser (version 3.3.0, (Klein and Manning,
2003)) which was trained with sections 2-21 of the
WSJ corpus; (b) the phrase structure output of the
parser is transformed to the CoNLL dependency
format using the official CoNLL 2007 conversion
script (Johansson and Nugues, 2007); and then (c)
the SCF of each verb instance is inferred using the
rule-based system used by (Reichart and Korho-
nen, 2013). The vector space representation for
each verb is then created using the process we de-
scribed for our model and the same holds for vec-
tor comparison. This baseline allows direct com-
parison of frames induced by our SCF model with
those derived from a supervised parser?s output.
We computed the Pearson correlation between
the scores of each of the models and the human
scores. The results demonstrate the superiority
of our model in predicting verb similarity: the
correlation of our model with the human scores
is 0.642 while the correlation of the lexical col-
location baseline is 0.522 and that of the super-
vised parser baseline is only 0.266. The results
indicate that in addition to their good alignment
with SCFs, our clusters are also highly useful for
verb meaning representation. This is in line with
the verb clustering theory of the Levin tradition
(Levin, 1993) which ties verb meaning with their
syntactic properties. We consider this an intrigu-
ing direction of future work.
5 Conclusions
We presented an MRF-based unsupervised model
for SCF acquisition which produces verb instance
level SCFs as output. As opposed to previous sys-
tems for the task, our model uses only a POS tag-
ger, avoiding the need for a statistical parser or
manually crafted rules. The model is particularly
valuable for NLP tasks benefiting from SCFs that
are applied across text domains, and for the many
tasks that involve sentence-level processing.
Our results show that the accuracy of the model
is promising, both when compared against gold
standard annotations and when evaluated in the
context of a task. In the future we intend to im-
prove our model by encoding additional informa-
tion in it. We will also adapt it to a multilingual
setup, aiming to model a wide range of languages.
Acknowledgments
The first author is supported by the Common-
wealth Scholarship Commission (CSC) and the
Cambridge Trust.
References
Ivana Romina Altamirano and Laura Alonso i Ale-
many. 2010. IRASubcat, a highly customizable,
language independent tool for the acquisition of ver-
bal subcategorization information from corpus. In
Proceedings of the NAACL 2010 Workshop on Com-
putational Approaches to Languages of the Ameri-
cas.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
french. In Proceedings of ACL-05.
Taylor Berg-Kirkpatrick, Alexander Bouchard-Cote,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL-HLT-10.
Akshar Bharati, Sriram Venkatapathy, and Prashanth
Reddy. 2005. Inferring semantic roles using sub-
categorization frames and maximum entropy model.
In Proceedings of CoNLL-05.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of ANLP-97.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceed-
ings of ACL-COLING-06.
John Carroll and Alex Fang. 2004. The automatic ac-
quisition of verb subcategorisations and their impact
on the performance of an HPSG parser. In Proceed-
ings of IJCNLP-04.
Paula Chesley and Susanne Salmon-Alt. 2006. Au-
tomatic extraction of subcategorization frames for
french. In Proceedings of LREC-06.
Kostadin Cholakov and Gertjan van Noord. 2010. Us-
ing unknown word techniques to learn known words.
In Proceedings of EMNLP-10.
287
Shay Cohen and Noah Smith. 2009. Shared logistic
normal distributions for soft parameter tying in un-
supervised grammar induction. In Proceedings of
NAACL-HLT-09.
Marie-Catherine De-Marneffe, Bill Maccartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC-06.
Lukasz Debowski. 2009. Valence extraction using EM
selection and co-occurrence matrices. Proceedins of
LREC-09.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ei-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20:116?131.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus
of english books. In Proceedings of (*SEM)-13. As-
sociation for Computational Linguistics.
Jan Haji?c, Martin mejrek, Bonnie Dorr, Yuan Ding, Ja-
son Eisner, Daniel Gildea, Terry Koo, Kristen Par-
ton, Gerald Penn, Dragomir Radev, and Owen Ram-
bow. 2002. Natural language generation in the con-
text of machine translation. Technical report, Cen-
ter for Language and Speech Processing, Johns Hop-
kins University, Baltimore. Summer Workshop Final
Report.
Chung hye Han, Benoit Lavoie, Martha Palmer, Owen
Rambow, Richard Kittredge, Tanya Korelsky, and
Myunghee Kim. 2000. Handling structural diver-
gences and recovering dropped arguments in a ko-
rean/english machine translation system. In Pro-
ceedings of the AMTA-00.
Dino Ienco, Serena Villata, and Cristina Bosco. 2008.
Automatic extraction of subcategorization frames
for italian. In Proceedings of LREC-08.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of NODALIDA-07.
Daisuke Kawahara and Sadao Kurohashi. 2010. Ac-
quiring reliable predicate-argument structures from
raw corpora for case frame compilation. In Proceed-
ings of LREC-10.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-03.
Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. The
MIT Press.
Anna Korhonen, Genevieve Gorrell, and Diana Mc-
Carthy. 2000. Statistical filtering and subcate-
gorization frame acquisition. In Proceedings of
EMNLP-00.
Anna Korhonen. 2002. Semantically motivated sub-
categorization acquisition. In Proceedings of the
ACL-02 workshop on Unsupervised lexical acquisi-
tion.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In Proceedings of ACL-11.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
Proceedings of EMNLP-11.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In Proceedings of IJCNLP-
05.
Alessandro Lenci, Barbara Mcgillivray, Simonetta
Montemagni, and Vito Pirrelli. 2008. Unsupervised
acquisition of verb subcategorization frames from
shallow-parsed corpora. In Proceedings of LREC-
08.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. Chicago, IL.
Tom Lippincott, Anna Korhonen, and Diarmuid Os-
eaghdha. 2010. Exploring subdomain variation in
biomedical language. BMC Bioinformatics.
Tom Lippincott, Aanna Korhonen, and Diarmuid Os-
eaghdha. 2012. Learning syntactic verb frames us-
ing graphical models. In Proceedings of ACL-12.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-05.
Cedric Messiant, Anna Korhonen, and Thierry
Poibeau. 2008. LexSchem: A large subcategoriza-
tion lexicon for French verbs. In Proceedings of
LREC-08.
Cedric Messiant. 2008. A subcategorization acquis-
tion system for french verbs. In Proceedings of
ACL08-SRW.
Yusuke Miyao and Junichi Tsujii. 2005. Probabilistic
disambiguaton models for wide-coverage hpsg pars-
ing. In Proceedings of ACL-05.
Alessandro Moschitti and Roberto Basili. 2005. Verb
subcategorization kernels for automatic semantic la-
beling. In Proceedings of the ACL-SIGLEX Work-
shop on Deep Lexical Acquisition.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the penn-ii and penn-iii treebanks. Computational
Linguistics, 31:328?365.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33:161?199.
288
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of ACL-07.
Valeria Quochi, Francesca Frontini, Roberto Bartolini,
Olivier Hamon, Marc Poch, Muntsa Padr, Nuria Bel,
Gregor Thurmair, Antonio Toral, and Amir Kam-
ram. 2012. Third evaluation report. evaluation of
panacea v3 and produced resources. Technical re-
port.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through dpp-based verb cluster-
ing. In Proceedings of ACL-13.
Roi Reichart and Ari Rappoport. 2009. The nvi
clustering evaluation measure. In Proceedings of
CoNLL-09.
Roi Reichart, Gal Elidan, and Ari Rappoport. 2012. A
diverse dirichlet process ensemble for unsupervised
induction of syntactic categories. In Proceedings of
COLING-12.
Laura Rimell, Thomas Lippincott, Karin Verspoor, He-
len Johnson, and Anna Korhonen. 2013. Acqui-
sition and evaluation of verb subcategorization re-
sources for biomedicine. Journal of Biomedical In-
formatics, 46:228?237.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learning
for part-of-speech tagging: Accelerating corpus an-
notation. In Proceedings of the ACL-07 Linguistic
Annotation Workshop.
Douglas Roland and Daniel Jurafsky. 1998. subcate-
gorization frequencies are affected by corpus choice.
In Proceedings of ACL-98.
Andrew Rosenberg and Julia Hirschberg. 2007. V
measure: a conditional entropybased external cluster
evaluation measure. In Proceedings of EMNLP-07.
Alexander Rush, Roi Reichart, Michael Collins, and
Amir Globerson. 2012. Improved parsing and pos
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP-12.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of german semantic verb
classes. Computational Linguistics, 32(2):159?194.
Solomon Shimony. 1994. Finding the maps for belief
networks is np-hard. Artificial Intelligence, 68:399?
310.
David Sontag, Talya Meltzer, Amir Globerson, Tommi
Jaakkola, and Yair Weiss. 2008. Tightening lp re-
laxations for map using message passing. In Pro-
ceedings of UAI-08.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP-11.
Ivan Titvo and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of EMNLP-12.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of NAACL-03.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37:141?188.
Tim Van de Cruys, Laura Rimell, Thierry Poibeau, and
Anna Korhonen. 2012. Multi-way tensor factor-
ization for unsupervised lexical acquisition. In Pro-
ceedings of COLING-12.
Giulia Venturi, Simonetta Montemagni, Simone
Marchi, Yutaka Sasaki, Paul Thompson, John Mc-
Naught, and Sophia Ananiadou. 2009. Bootstrap-
ping a verb lexicon for biomedical information ex-
traction. Computational Linguistics and Intelligent
Text Processing, 5449:137?148.
Marion Weller, Alexander Fraser, and Sabine Schulte
im Walde. 2013. Using subcategorization knowl-
edge to improve case prediction for translation to
german. In Proceedings of ACL-13.
Chen Yanover, Talya Meltzer, and Yair Weiss. 2006.
Linear programming relazations and belief pro-
pogataion an empitical study. JMLR Special Issue
on Machine Learning and Large Scale Optimization.
Stella Yu and Jianbo Shi. 2003. Multiclass spectral
clustering. In Proceedings of ICCV-13.
289
Statistical Metaphor Processing
Ekaterina Shutova?
University of Cambridge
Simone Teufel?
University of Cambridge
Anna Korhonen?
University of Cambridge
Metaphor is highly frequent in language, which makes its computational processing indis-
pensable for real-world NLP applications addressing semantic tasks. Previous approaches to
metaphor modeling rely on task-specific hand-coded knowledge and operate on a limited domain
or a subset of phenomena. We present the first integrated open-domain statistical model of
metaphor processing in unrestricted text. Our method first identifies metaphorical expressions
in running text and then paraphrases them with their literal paraphrases. Such a text-to-text
model of metaphor interpretation is compatible with other NLP applications that can benefit
from metaphor resolution. Our approach is minimally supervised, relies on the state-of-the-art
parsing and lexical acquisition technologies (distributional clustering and selectional preference
induction), and operates with a high accuracy.
1. Introduction
Our production and comprehension of language is a multi-layered computational
process. Humans carry out high-level semantic tasks effortlessly by subconsciously
using a vast inventory of complex linguistic devices, while simultaneously integrating
their background knowledge, to reason about reality. An ideal computational model
of language understanding would also be capable of performing such high-level se-
mantic tasks. With the rapid advances in statistical natural language processing (NLP)
and computational lexical semantics, increasingly complex semantic tasks can now
be addressed. Tasks that have received much attention so far include, for example,
word sense disambiguation (WSD), supervised and unsupervised lexical classification,
selectional preference induction, and semantic role labeling. In this article, we take a
step further and show that state-of-the-art statistical NLP and computational lexical
semantic techniques can be used to successfully model complexmeaning transfers, such
as metaphor.
? Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue, Cambridge CB3 0FD, UK.
E-mail: {Ekaterina.Shutova, Simone.Teufel, Anna.Korhonen}@cl.cam.ac.uk.
Submission received: 28 July 2011; revised submission received: 21 April 2012; accepted for publication:
31 May 2012.
doi:10.1162/COLI a 00124
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 2
Metaphors arise when one concept is viewed in terms of the properties of another.
Humans often use metaphor to describe abstract concepts through reference to more
concrete or physical experiences. Some examples of metaphor include the following.
(1) How can I kill a process? (Martin 1988)
(2) Hillary brushed aside the accusations.
(3) I investedmyself fully in this research.
(4) And then my heart with pleasure fills,
And danceswith the daffodils.
(?I wandered lonely as a cloud,? William Wordsworth, 1804)
Metaphorical expressions may take a great variety of forms, ranging from conventional
metaphors, which we produce and comprehend every day, for example, those in Exam-
ples (1)?(3), to poetic and novel ones, such as Example (4). In metaphorical expressions,
seemingly unrelated features of one concept are attributed to another concept. In Ex-
ample (1), a computational process is viewed as something alive and, therefore, its forced
termination is associated with the act of killing. In Example (2) Hillary is not literally
cleaning the space by sweeping accusations. Instead, the accusations lose their validity
in that situation, in other words Hillary rejects them. The verbs brush aside and reject both
entail the resulting disappearance of their object, which is the shared salient property
that makes it possible for this analogy to be lexically expressed as a metaphor.
Characteristic of all areas of human activity (from poetic to ordinary to scientific)
and thus of all types of discourse, metaphor becomes an important problem for NLP.
As Shutova and Teufel (2010) have shown in an empirical study, the use of conventional
metaphor is ubiquitous in natural language text (according to their data, on average
every third sentence in general-domain text contains a metaphorical expression). This
makes metaphor processing essential for automatic text understanding. For example,
an NLP application which is unaware that a ?leaked report? is a ?disclosed report?
and not, for example, a ?wet report,? would fail further semantic processing of the
piece of discourse in which this phrase appears. A system capable of recognizing and
interpreting metaphorical expressions in unrestricted text would become an invaluable
component of any real-world NLP application that needs to access semantics (e.g., infor-
mation retrieval [IR], machine translation [MT], question answering [QA], information
extraction [IE], and opinion mining).
So far, these applications have not used any metaphor processing techniques and
thus often fail to interpret metaphorical data correctly. Consider an example from MT.
Figure 1 shows metaphor translation from English into Russian by a state-of-the-art
statistical MT system (Google Translate1). For both sentences the MT system produces
literal translations of metaphorical terms in English, rather than their literal interpreta-
tions. This results in otherwise grammatical sentences being semantically infelicitous,
poorly formed, and barely understandable to a native speaker of Russian. The meaning
of stir in Figure 1 (1) and spill in Figure 1 (2) would normally be realized in Russian only
via their literal interpretation in the given context (provoke and tell), as shown under
CORRECT TRANSLATION in Figure 1. A metaphor processing component could help to
1 http://translate.google.com/.
302
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 1
Examples of metaphor translation.
avoid such errors. We conducted a pilot study of the importance of metaphor for MT,
by running an English-to-Russian MT system (Google Translate) on the sentences from
the data set of Shutova (2010) containing single-word verb metaphors. We found that
27 out of 62 sentences (44%) were translated incorrectly due to metaphoricity. Due to
the high frequency of metaphor in text according to corpus studies, such a high level of
error becomes important for MT.
Examples where metaphor understanding is crucial can also be found in opinion
mining, that is, detection of the speaker?s attitude to what is said and to the topic.
Consider the following sentences.
(5) a. Government loosened its strangle-hold on business. (Narayanan 1999)
b. Government deregulated business. (Narayanan 1999)
Both sentences describe the same fact. The use of the metaphor loosened strangle-hold in
Example (5a) suggests that the speaker opposes government control of economy, how-
ever, whereas Example (5b) does not imply this. One can infer the speaker?s negative
attitude via the presence of a negativeword strangle-hold. Ametaphor processing system
would establish the correct meaning of Example (5a) and thus discover the actual fact
towards which the speaker has a negative attitude.
Because metaphor understanding requires resolving non-literal meanings via ana-
logical comparisons, the development of a complete and computationally practical
account of this phenomenon is a challenging and complex task. Despite the impor-
tance of metaphor for NLP systems dealing with semantic interpretation, its automatic
processing has received little attention in contemporary NLP, and is far from being a
solved problem. The majority of computational approaches to metaphor still exploit
ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They
often rely on task-specific hand-coded knowledge (Martin 1990; Fass 1991; Narayanan
1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al 2007)
and reduce the task to reasoning about a limited domain or a subset of phenomena
(Gedigian et al 2006; Krishnakumaran and Zhu 2007). So far there has been no robust
statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein
andManning 2003; Briscoe, Carroll, andWatson 2006; Clark and Curran 2007), however,
as well as recent work on computational lexical semantics (Schulte im Walde 2006;
303
Computational Linguistics Volume 39, Number 2
Mitchell and Lapata 2008; Davidov, Reichart, and Rappoport 2009; Erk and McCarthy
2009; Sun and Korhonen 2009; Abend and Rappoport 2010; O? Se?aghdha 2010) open up
many avenues for the creation of such a system. This is the niche the presented work is
intending to fill.
1.1 What Is Metaphor?
Metaphor has traditionally been viewed as an artistic device that lends vividness and
distinction to an author?s style. This view was first challenged by Lakoff and Johnson
(1980), who claimed that it is a productive phenomenon that operates at the level of
mental processes. According to Lakoff and Johnson, metaphor is thus not merely a
property of language (i.e., a linguistic phenomenon), but rather a property of thought
(i.e., a cognitive phenomenon). This view was subsequently adopted and extended
by a multitude of approaches (Grady 1997; Narayanan 1997; Fauconnier and Turner
2002; Feldman 2006; Pinker 2007) and the term conceptual metaphor was coined to
describe it.
The view postulates that metaphor is not limited to similarity-based meaning ex-
tensions of individual words, but rather involves reconceptualization of a whole area
of experience in terms of another. Thus metaphor always involves two concepts or
conceptual domains: the target (also called the topic or tenor in the linguistics literature)
and the source (also called the vehicle). Consider Examples (6) and (7).
(6) He shot down all of my arguments. (Lakoff and Johnson 1980)
(7) He attacked every weak point in my argument. (Lakoff and Johnson 1980)
According to Lakoff and Johnson, a mapping of the concept of argument to that of war is
used in both Examples (6) and (7). The argument, which is the target concept, is viewed
in terms of a battle (or awar), the source concept. The existence of such a link allows us to
talk about arguments using war terminology, thus giving rise to a number of metaphors.
Conceptual metaphor, or source?target domain mapping, is thus a generalization over
a set of individual metaphorical expressions that covers multiple cases in which ways
of reasoning about the source domain systematically correspond to ways of reasoning
about the target.
Conceptual metaphor manifests itself in natural language in the form of linguistic
metaphor (or metaphorical expressions) in a variety of ways. The most common types
of linguistic metaphor are lexical metaphor (i.e., metaphor at the level of a single
word sense, as in the Examples (1)?(4)), multi-word metaphorical expressions (e.g.,
?whether we go on pilgrimagewith Raleigh or put out to seawith Tennyson?), or extended
metaphor, that spans over longer discourse fragments.
Lexical metaphor is by far the most frequent type. In the presence of a certain
conceptual metaphor individual words can be used in entirely novel contexts, which
results in the formation of new meanings. Consider the following example.
(8) How can we build a ?Knowledge economy? if research is handcuffed? (Barque
and Chaumartin 2009)
In this sentence the physical verb handcuff is used with an abstract object research
and its meaning adapts accordingly. Metaphor is a productive phenomenon (i.e., its
304
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
novel examples continue to emerge in language). A large number of metaphorical
expressions, however, become conventionalized (e.g., ?I cannot grasp his way of think-
ing?). Although metaphorical in nature, their meanings are deeply entrenched in every-
day use, and are thus cognitively treated as literal terms. Both novel and conventional
metaphors are important for text processing, hence our work is concerned with both
types. Fixed non-compositional idiomatic expressions (e.g., kick the bucket, rock the boat,
put a damper on), however, are left aside, because the mechanisms of their formation are
no longer productive in modern language and, as such, they are of little interest for the
design of a generalizable computational model of metaphor.
Extended metaphor refers to the use of metaphor at the discourse level. A famous
example of extended metaphor can be found in William Shakespeare?s play As You Like
It, where he first compares the world to a stage and then in the following discourse
describes its inhabitants as players. Extended metaphor often appears in literature
in the form of an allegory or a parable, whereby a whole story from one domain is
metaphorically transferred onto another in order to highlight certain attributes of the
subject or teach a moral lesson.
1.2 Computational Modeling of Metaphor
In this article we focus on lexical metaphor and the computational modeling thereof.
From an NLP viewpoint, not all metaphorical expressions are equally important. A
metaphorical expression is interesting for computational modeling if its metaphorical
sense is significantly distinct from its original literal sense and cannot be interpreted
directly (e.g., by existing word sense disambiguation techniques using a predefined
sense inventory). The identification of highly conventionalized metaphors (e.g., the
verb impress, whose meaning originally stems from printing) are not of interest for NLP
tasks, because their metaphorical senses have long been dominant in language and their
original literal senses may no longer be used. A number of conventionalizedmetaphors,
however, require explicit interpretation in order to be understood by computer (e.g.,
?cast doubt,? ?polish the thesis,? ?catch a disease?), as do all novel metaphors. Thus
we are concerned with both novel and conventional metaphors, but only consider the
cases whereby the literal and metaphorical senses of the word are in clear opposition in
common use in contemporary language.
Automatic processing of metaphor can be divided into two subtasks: metaphor
identification, or recognition (distinguishing between literal and metaphorical language
in text); and metaphor interpretation (identifying the intended literal meaning of a
metaphorical expression). An ideal metaphor processing system should address both
of these tasks and provide useful information to support semantic interpretation in
real-world NLP applications. In order to be directly applicable to other NLP systems,
it should satisfy the following criteria:
 Provide a representation of metaphor interpretation that can be easily
integrated with other NLP systems: This criterion places constraints
on how the metaphor processing task should be defined. The most
universally applicable metaphor interpretation would be in the text-to-text
form. This means that a metaphor processing system would take raw text
as input and provide a more literal text as output, in which metaphors
are interpreted.
305
Computational Linguistics Volume 39, Number 2
 Operate on unrestricted running text: In order to be useful for real-world
NLP the system needs to be capable of processing real-world data. Rather
than only dealing with individual carefully selected clear-cut examples,
the system should be fully implemented and tested on free naturally
occurring text.
 Be open-domain: The system needs to cover all domains, genres, and
topics. Thus it should not rely on any domain-specific information or focus
on individual types of instances (e.g., a hand-chosen limited set of
source-target domain mappings).
 Be unsupervised or minimally supervised: To be easily adaptable to new
domains, the system needs to be unsupervised or minimally supervised.
This means it should not use any task-specific (i.e., metaphor-specific)
hand-coded knowledge. The only acceptable exception might be a
multi-purpose general-domain lexicon that is already in existence and
does not need to be created in a costly manner, although it would be an
advantage if no such resource is required.
 Cover all syntactic constructions: To be robust, the system needs to be
able to deal with metaphors represented by all word classes and syntactic
constructions.
In this article, we address both the metaphor identification and interpretation
tasks, resulting in the first integrated domain-independent corpus-based computational
model of metaphor. The method is designed with the listed criteria in mind. It takes
unrestricted text as input and produces textual output. Metaphor identification and
interpretation modules, based on the algorithms of Shutova, Sun, and Korhonen (2010)
and Shutova (2010), are first evaluated independently, and then combined and evalu-
ated together as an integrated system. All components of the method are in principle
applicable to all part-of-speech classes and syntactic constructions. In the current ex-
periments, however, we tested the system only on single-word metaphors expressed by
a verb. Verbs are frequent in language and central to conceptual metaphor. Cameron
(2003) conducted a corpus study of the use of metaphor in educational discourse for all
parts of speech. She found that verbs account for around 50% of the data, the rest being
shared by nouns, adjectives, adverbs, copula constructions, and multi-word metaphors.
This suggests that verb metaphors provide a reliable testbed for both linguistic and
computational experiments. Restricting the scope to verbs is a methodological step
aimed at testing the main principles of the proposed approach in a well-defined setting.
We would, however, expect the presented methods to scale to other parts of speech
and to a wide range of syntactic constructions, because they rely on techniques from
computational lexical semantics that have been shown to be effective in modeling not
only verb meanings, but also those of nouns and adjectives.
As opposed to previous approaches that modeled metaphorical reasoning starting
from a hand-crafted description and applying it to explain the data, we aim to design
a statistical model that captures regular patterns of metaphoricity in a large corpus and
thus generalizes to unseen examples. Compared to labor-intensive manual efforts, this
approach is more robust and, being nearly unsupervised, cost-effective. In contrast to
previous statistical approaches, which addressed metaphors of a specific topic or did
not consider linguistic metaphor at all (e.g., Mason 2004), the proposed method covers
all metaphors in principle, can be applied to unrestricted text, and can be adapted to
different domains and genres.
306
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Our first experiment is concerned with the identification of metaphorical expres-
sions in unrestricted text. Starting from a small set of metaphorical expressions, the
system learns the analogies involved in their production in aminimally supervised way.
It generalizes over the exemplified analogies by means of verb and noun clustering
(i.e., the identification of groups of similar concepts). This generalization allows it to
recognize previously unseen metaphorical expressions in text. Consider the following
examples:
(9) All of this stirred an uncontrollable excitement in her.
(10) Time and time again he would stare at the ground, hand on hip, and then
swallow his anger and play tennis.
Having once seen the metaphor ?stir excitement? in Example (9) the metaphor identifi-
cation system successfully concludes that ?swallow anger? in Example (10) is also used
metaphorically.
The identified metaphors then need to be interpreted. Ideally, a metaphor interpre-
tation task should be aimed at producing a representation of metaphor understanding
that can be directly embedded into other NLP applications that could benefit frommeta-
phor resolution. We define metaphor interpretation as a paraphrasing task and build
a system that discovers literal meanings of metaphorical expressions in text and pro-
duces their literal paraphrases. For example, for metaphors in Examples (11a) and (12a)
the system produces the paraphrases in Examples (11b) and (12b), respectively.
(11) a. All of this stirred an uncontrollable excitement in her.
b. All of this provoked an uncontrollable excitement in her.
(12) a. a carelessly leaked report
b. a carelessly disclosed report
The paraphrases for metaphorical expressions are acquired in a data-driven manner
from a large corpus. Literal paraphrases are then identified using a selectional prefer-
ence model.
This article first surveys the relevant theoretical and computational work on meta-
phor, then describes the design of the identification and paraphrasingmodules and their
independent evaluation, and concludes with the evaluation of the integrated text-to-text
metaphor processing system. The evaluations were carried out with the aid of human
subjects. In the case of identification, the subjects were asked to judge whether a system-
annotated phrase is a metaphor. In case of paraphrasing, they had to decide whether
the system-produced paraphrase for the metaphorical expression is correct and literal
in the given context. In addition, we created a metaphor paraphrasing gold standard
by asking human subjects (not previously exposed to system output) to produce their
own literal paraphrases for metaphorical verbs. The system paraphrasing was then also
evaluated against this gold standard.
2. Theoretical and Computational Background
2.1 Metaphor and Polysemy
Theorists of metaphor distinguish between two kinds of metaphorical language: novel
(or poetic) metaphors (i.e., those that are imaginative), and conventionalized metaphors
307
Computational Linguistics Volume 39, Number 2
(i.e., those that are used as a part of an ordinary discourse). According to Nunberg
(1987), all metaphors emerge as novel, but over time they become part of general usage
and their rhetorical effect vanishes, resulting in conventionalized metaphors. Following
Orwell (1946), Nunberg calls such metaphors ?dead? and claims that they are not
psychologically distinct from literally used terms. The scheme described by Nunberg
demonstrates how metaphorical associations capture patterns governing polysemy,
namely, the capacity of aword to havemultiplemeanings. Over time some of the aspects
of the target domain are added to the meaning of a term in the source domain, resulting
in a (metaphorical) sense extension of this term. Copestake and Briscoe (1995) discuss
sense extension mainly based on metonymic examples and model the phenomenon
using lexical rules encoding metonymic patterns. They also suggest that similar mecha-
nisms can be used to account for metaphorical processes. According to Copestake and
Briscoe, the conceptual mappings encoded in the sense extension rules would define
the limits to the possible shifts in meaning.
General-domain lexical resources often include information about metaphorical
word senses, although unsystematically and without any accompanying semantic an-
notation. For example, WordNet2 (Fellbaum 1998) contains the comprehension sense
of grasp, defined as ?get the meaning of something,? and the reading sense of skim,
defined as ?read superficially.? A great deal of metaphorical senses are absent from
the current version of WordNet, however. A number of researchers have advocated the
necessity of systematic inclusion and mark-up of metaphorical senses in such general-
domain lexical resources (Alonge and Castelli 2003; Lo?nneker and Eilts 2004) and claim
that this would be beneficial for the computational modeling of metaphor. Metaphor
processing systems could then either use this knowledge or be evaluated against it.
Lo?nneker (2004) mapped the senses from EuroWordNet3 to the Hamburg Metaphor
Database (Lo?nneker 2004; Reining and Lo?nneker-Rodman 2007) containing examples
of metaphorical expressions in German and French. Currently no explicit information
about metaphor is integrated into WordNet for English, however.
Although consistent inclusion in WordNet is in principle possible for conventional
metaphorical senses, it is not viable for novel contextual sense alternations. Because
metaphor is a productive phenomenon, all possible cases of contextual meaning alter-
nations it results in cannot be described via simple sense enumeration (Pustejovsky
1995). Computational metaphor processing therefore cannot be approached using the
standard word sense disambiguation paradigm, whereby the contextual use of a word
is classified according to an existing sense inventory. The metaphor interpretation task
is inherently more complex and requires generation of new and often uncommon
meanings of the metaphorical term based on the context.
2.2 Theoretical Views on Metaphor
The following views on metaphor are prominent in linguistics and philosophy: the
comparison view (e.g., the Structure-Mapping Theory of Gentner [1983]), the interaction
view (Black 1962; Hesse 1966), the selectional restrictions violation view (Wilks 1975,
1978), and conceptual metaphor theory (CMT) (Lakoff and Johnson 1980). All of these
2 http://wordnet.princeton.edu/.
3 EuroWordNet is a multilingual database containing WordNets for several European languages (Dutch,
Italian, Spanish, German, French, Czech, and Estonian). The WordNets are structured in the same way
as the Princeton WordNet for English. URL: http://www.illc.uva.nl/EuroWordNet/.
308
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
approaches share the idea of an interconceptual mapping that underlies the production
of metaphorical expressions. Gentner?s Structure-Mapping Theory postulates that the
ground for metaphor lies in similar properties and relations shared by the two con-
cepts (the target and the source). Tourangeau and Sternberg (1982), however, criticize
this view by noting that ?everything has some feature or category that it shares with
everything else, but we cannot combine just any two things in metaphor? (Tourangeau
and Sternberg 1982, page 226). The interaction view focuses on the surprise and novelty
that metaphor introduces. Its proponents claim that the source concept (or domain) rep-
resents a template for seeing the target concept in an entirely new way. The conceptual
metaphor theory of Lakoff and Johnson (1980) takes this idea much further by stating
that metaphor operates at the level of thought rather than at the level of language, and
that it is based on a set of cognitive mappings between source and target domains. Thus
Lakoff and Johnson put the emphasis on the structural aspect of metaphor, rather than
its decorative function in language that dominated the preceding theories. The selec-
tional restrictions violation view of Wilks (1978) concerns manifestation of metaphor in
language. Wilks suggests that metaphor represents a violation of combinatory norms
in the linguistic context and that metaphorical expressions can be detected via such
violation.
2.2.1 Conceptual Metaphor Theory. Examples (6) and (7) provided a good illustration of
CMT. Lakoff and Johnson explain them via the conceptual metaphor ARGUMENT IS
WAR, which is systematically reflected in language in a variety of expressions.
(13) Your claims are indefensible. (Lakoff and Johnson 1980)
(14) I demolished his argument. (Lakoff and Johnson 1980)
(15) I?ve never won an argument with him. (Lakoff and Johnson 1980)
(16) You disagree? Okay, shoot! (Lakoff and Johnson 1980)
According to CMT, we conceptualize and structure arguments in terms of battle, which
systematically influences the way we talk about arguments within our culture. In
other words, the conceptual structure behind battle (i.e., that one can shoot, demolish,
devise a strategy, win, and so on), is metaphorically transferred onto the domain of
argument.
Manifestations of conceptual metaphor are ubiquitous in language and communi-
cation. Here are a few other examples of common metaphorical mappings.
 TIME IS MONEY (e.g., ?That flat tire costme an hour?)
 IDEAS ARE PHYSICAL OBJECTS (e.g., ?I cannot grasp his way of
thinking?)
 LINGUISTIC EXPRESSIONS ARE CONTAINERS (e.g., ?I would not
be able to put all my feelings intowords?)
 EMOTIONS ARE VEHICLES (e.g., ?[...] she was transportedwith
pleasure?)
 FEELINGS ARE LIQUIDS (e.g., ?[...] all of this stirred an unfathomable
excitement in her?)
309
Computational Linguistics Volume 39, Number 2
 LIFE IS A JOURNEY (e.g., ?He arrived at the end of his life with very little
emotional baggage?)
CMT produced a significant resonance in the fields of philosophy, linguistics, cogni-
tive science, and artificial intelligence, including NLP. It inspired novel research (Martin
1990, 1994; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman andNarayanan 2004;
Mason 2004; Martin 2006; Agerri et al 2007), but was also criticized for the lack of
consistency and empirical verification (Murphy 1996; Shalizi 2003; Pinker 2007). The
sole evidence with which Lakoff and Johnson (1980) supported their theory was a set of
carefully selected examples. Such examples, albeit clearly illustrating the main tenets of
the theory, are not representative. They cannot possibly capture the whole spectrum
of metaphorical expressions, and thus do not provide evidence that the theory can
adequately explain the majority of metaphors in real-world texts. Aiming to verify the
latter, Shutova and Teufel (2010) conducted a corpus-based analysis of conceptual meta-
phor in the data from the British National Corpus (BNC) (Burnard 2007). In their study
three independent participants annotated both linguistic metaphors and the underlying
source?target domain mappings. Their results show that although the annotators reach
some overall agreement on the annotation of interconceptual mappings, they experi-
enced a number of difficulties, one of which was the problem of finding the right level
of abstraction for the source and target domain categories. The difficulties in category
assignment for conceptual metaphor suggest that it is hard to consistently assign explicit
labels to source and target domains, even though the interconceptual associations exist
in some sense and are intuitive to humans.
2.2.2 Selectional Restrictions Violation View. Lakoff and Johnson do not discuss howmeta-
phors can be recognized in linguistic data. To date, the most influential account of this
issue is that ofWilks (1975, 1978). According toWilks, metaphors represent a violation of
selectional restrictions (or preferences) in a given context. Selectional restrictions are the
semantic constraints that a predicate places onto its arguments. Consider the following
example.
(17) a. My aunt always drinks her tea on the terrace.
b. My car drinks gasoline. (Wilks 1978)
The verb drink normally requires a grammatical subject of type ANIMATE and a gram-
matical object of type LIQUID, as in Example (17a). Therefore, drink taking a car as a
subject in (17b) is an anomaly, which, according to Wilks, indicates a metaphorical use
of drink.
Although Wilks?s idea inspired a number of computational experiments on meta-
phor recognition (Fass and Wilks 1983; Fass 1991; Krishnakumaran and Zhu 2007), it
is important to note that in practice this approach has a number of limitations. Firstly,
there are other kinds of non-literalness or anomaly in language that cause a violation of
semantic norm, such as metonymies. Thus the method would overgenerate. Secondly,
there are kinds of metaphor that do not represent a violation of selectional restrictions
(i.e., the approach may also undergenerate). This would happen, for example, when
highly conventionalized metaphorical word senses are more frequent than the original
literal senses. Due to their frequency, selectional preference distributions of such words
in real-world data would be skewed towards the metaphorical senses (e.g., capturemay
select for ideas rather than captives according to the data). As a result, no selectional
preferences violation can be detected in the use of such verbs. Another case where the
310
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
method does not apply is copula constructions, such as ?All the world?s a stage.? And
finally, the method does not take into account the fact that interpretation (of metaphor
as well as other linguistic phenomena) is always context-dependent. For example,
the phrase ?All men are animals? uttered by a biology professor or a feminist would
have entirely different interpretations, the latter clearly metaphorical, but without any
violation of selectional restrictions.
2.3 Computational Approaches to Metaphor
2.3.1 Automatic Metaphor Recognition. One of the first attempts to automatically identify
and interpret metaphorical expressions in text is the approach of Fass (1991). It origi-
nates in the idea of Wilks (1978) and utilizes hand-coded knowledge. Fass developed a
system called met*, which is capable of discriminating between literalness, metonymy,
metaphor, and anomaly. It does this in three stages. First, literalness is distinguished
from non-literalness using selectional preference violation as an indicator. In the case
that non-literalness is detected, the respective phrase is tested for being metonymic
using hand-coded patterns (such as CONTAINER-FOR-CONTENT). If the system fails
to recognize metonymy, it proceeds to search the knowledge base for a relevant analogy
in order to discriminate metaphorical relations from anomalous ones. For example, the
sentence in Example (17b) would be represented in this framework as (car,drink,gasoline),
which does not satisfy the preference (animal,drink,liquid), as car is not a hyponym of
animal. met* then searches its knowledge base for a triple containing a hypernym of
both the actual argument and the desired argument and finds (thing,use,energy source),
which represents the metaphorical interpretation.
Goatly (1997) identifies a set of linguistic cues, namely, lexical patterns indicating
the presence of a metaphorical expression in running text, such as metaphorically speak-
ing, utterly, completely, so to speak, and literally. This approach, however, is likely to find
only a small proportion ofmetaphorical expressions, as the vast majority of them appear
without any signaling context. We conducted a corpus study in order to investigate the
effectiveness of linguistic cues asmetaphor indicators. For each cue suggested by Goatly
(1997), we randomly sampled 50 sentences from the BNC containing it and manually
annotated them for metaphoricity. The results are presented in Table 1. The average
precision (i.e., the proportion of identified expressions that were metaphorical) of the
linguistic cue method according to these data is 0.40, which suggests that the set of
metaphors that this method generates contains a great deal of noise. Thus the cues are
unlikely to be sufficient for metaphor extraction on their own, but together with some
additional filters, they could contribute to a more complex system.
The work of Peters and Peters (2000) concentrates on detecting figurative language
in lexical resources. They mine WordNet (Fellbaum 1998) for examples of systematic
Table 1
Corpus statistics for linguistic cues.
Cue BNC frequency Sample size Metaphors Precision
?metaphorically speaking? 7 7 5 0.71
?literally? 1,936 50 13 0.26
?figurative? 125 50 9 0.18
?utterly? 1,251 50 16 0.32
?completely? 8,339 50 13 0.26
?so to speak? 353 49 35 0.71
311
Computational Linguistics Volume 39, Number 2
polysemy, which allows them to capture metonymic and metaphorical relations. Their
system searches for nodes that are relatively high in the WordNet hierarchy (i.e., are
relatively general) and that share a set of commonword forms among their descendants.
Peters and Peters found that such nodes often happen to be in a metonymic (e.g.,
publisher ? publication) or a metaphorical (e.g., theory ? supporting structure) relation.
The CorMet system (Mason 2004) is the first attempt at discovering source?
target domain mappings automatically. It does this by finding systematic variations in
domain-specific selectional preferences, which are inferred from texts on the Web. For
example, Mason collects texts from the LAB domain and the FINANCE domain, in both
of which pourwould be a characteristic verb. In the LAB domain pour has a strong selec-
tional preference for objects of type liquid, whereas in the FINANCE domain it selects for
money. From this Mason?s system infers the domain mapping FINANCE ? LAB and the
concept mappingMONEY IS LIQUID. He compares the output of his system against the
Master Metaphor List (MML; Lakoff, Espenson, and Schwartz 1991) and reports a per-
formance of 77% in terms of accuracy (i.e., proportion of correctly induced mappings).
Birke and Sarkar (2006) present a sentence clustering approach for non-literal lan-
guage recognition, implemented in the TroFi system (Trope Finder). The idea behind
their system originates from a similarity-based word sense disambiguation method
developed by Karov and Edelman (1998). The latter uses a set of seed sentences anno-
tated with respect to word sense. The system computes similarity between the sentence
containing the word to be disambiguated and all of the seed sentences and selects the
sense corresponding to the annotation in the most similar seed sentences. Birke and
Sarkar adapt this algorithm to perform a two-way classification (literal vs. non-literal),
not aiming to distinguish between specific kinds of tropes. An example for the verb
pour in their database is shown in Figure 2. They attain a performance of 0.54 in terms
of F-measure (van Rijsbergen 1979).
Themethod of Gedigian et al (2006) discriminates between literal andmetaphorical
use. The authors trained a maximum entropy classifier for this purpose. They col-
lected their data using FrameNet (Fillmore, Johnson, and Petruck 2003) and PropBank
(Kingsbury and Palmer 2002) annotations. FrameNet is a lexical resource for English
containing information on words? semantic and syntactic combinatory possibilities, or
valencies, in each of their senses. PropBank is a corpus annotated with verbal propo-
sitions and their arguments. Gedigian et al (2006) extracted the lexical items whose
frames are related to MOTION and CURE from FrameNet, then searched the PropBank
Wall Street Journal corpus (Kingsbury and Palmer 2002) for sentences containing such
lexical items and annotated them with respect to metaphoricity. For example, the verb
run in the sentence ?Texas Air has run into difficulty? was annotated as metaphorical,
and in ?I was doing the laundry and nearly broke my neck running upstairs to see?
as literal. Gedigian et al used PropBank annotation (arguments and their semantic
pour
*nonliteral cluster*
wsj04:7878 N As manufacturers get bigger, they are likely to pour more money into the battle for shelf
space, raising the ante for new players.
wsj25:3283 N Salsa and rap music pour out of the windows.
wsj06:300 U Investors hungering for safety and high yields are pouring record sums into single-
premium, interest-earning annuities.
*literal cluster*
wsj59:3286 L Custom demands that cognac be poured from a freshly opened bottle.
Figure 2
An example of the data of Birke and Sarkar (2006).
312
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
types) as features to train the classifier, and report an accuracy of 95.12%. This result is,
however, only 2.22 percentage points higher than the performance of the naive baseline
assigning majority class to all instances (92.90%). Such high performance of their system
can be explained by the fact that 92.90% of the verbs ofMOTION and CURE in their data
are used metaphorically, thus making the data set unbalanced with respect to target
categories and making the task easier.
Both Birke and Sarkar (2006) and Gedigian et al (2006) focus only on metaphors
expressed by a verb. The approach of Krishnakumaran and Zhu (2007) additionally
covers metaphors expressed by nouns and adjectives. Krishnakumaran and Zhu use
hyponymy relation in WordNet and word bigram counts to predict metaphors at a
sentence level. Given a metaphor in copula constructions, or an IS-A metaphor (e.g., the
famous quote by William Shakespeare ?All the world?s a stage?) they verify if the two
nouns involved are in hyponymy relation inWordNet, otherwise this sentence is tagged
as containing a metaphor. They also treat expressions containing a verb or an adjective
used metaphorically (e.g., ?He planted good ideas in their minds? or ?He has a fertile
imagination?). For those cases, they calculate bigram probabilities of verb?noun and
adjective?noun pairs (including the hyponyms/hypernyms of the noun in question). If
the combination is not observed in the data with sufficient frequency, the system tags
the sentence as metaphorical. This idea is a modification of the selectional preference
view of Wilks, although applied at the bigram level. Alternatively, one could extract
verb?object relations from parsed text. Compared to the latter, Krishnakumaran and
Zhu (2007) lose a great deal of information. The authors evaluated their system on a
set of example sentences compiled from the Master Metaphor List, whereby highly con-
ventionalized metaphors are taken to be negative examples. Thus they do not deal with
literal examples as such. Essentially, the distinction Krishnakumaran and Zhu are mak-
ing is between the senses included inWordNet, even if they are conventional metaphors
(e.g., ?capture an idea?), and those not included in WordNet (e.g., ?planted good ideas?).
2.3.2 Automatic Metaphor Interpretation. One of the first computational accounts of meta-
phor interpretation is that of Martin (1990). In his metaphor interpretation, denotation
and acquisition system (MIDAS), Martin models the hierarchical organization of con-
ventional metaphors. The main assumption underlying this approach is that more spe-
cific conventional metaphors (e.g., COMPUTATIONAL PROCESS viewed as a LIVING
BEING in ?How can I kill a process??) descend from more general ones (e.g., PROCESS
[general, as a sequence of events] is a LIVINGBEING). Given an example of ametaphor-
ical expression, MIDAS searches its database for a corresponding conceptual metaphor
that would explain the anomaly. If it does not find any, it abstracts from the example to
more general concepts and repeats the search. If a suitable general metaphor is found,
it creates a new mapping for its descendant, a more specific metaphor, based on this
example. This is also how novel conceptual metaphors are acquired by the system.
The metaphors are then organized into a resource called MetaBank (Martin 1994). The
knowledge is represented in MetaBank in the form of metaphor maps (Martin 1988)
containing detailed information about source-target concept mappings and empirically
derived examples. MIDAS has been integrated with Unix Consultant, a system that
answers users? questions about Unix. The system first tries to find a literal answer to the
question. If it is not able to, it calls MIDAS, which detects metaphorical expressions via
selectional preference violation and searches its database for a metaphor explaining the
anomaly in the question.
Another cohort of approaches aims to perform inference about entities and events
in the source and target domains for the purpose of metaphor interpretation. These
313
Computational Linguistics Volume 39, Number 2
include the KARMA system (Narayanan 1997, 1999; Feldman and Narayanan 2004)
and the ATT-Meta project (Barnden and Lee 2002; Agerri et al 2007). Within both
systems the authors developed a metaphor-based reasoning framework in accordance
with CMT. The reasoning process relies on manually coded knowledge about the world
and operates mainly in the source domain. The results are then projected onto the target
domain using the conceptual mapping representation. The ATT-Meta project concerns
metaphorical and metonymic description of mental states; and reasoning about mental
states is performed using first order logic. Their system, however, does not take natural
language sentences as input, but hand-coded logical expressions that are representa-
tions of small discourse fragments. KARMA in turn deals with a broad range of abstract
actions and events and takes parsed text as input.
Veale and Hao (2008) derive a ?fluid knowledge representation for metaphor inter-
pretation and generation? called Talking Points. Talking Points is a set of characteristics
of concepts belonging to source and target domains and related facts about the world
which are acquired automatically from WordNet and from the Web. Talking Points are
then organized in Slipnet, a framework that allows for a number of insertions, deletions,
and substitutions in definitions of such characteristics in order to establish a connection
between the target and the source concepts. This work builds on the idea of slippage in
knowledge representation for understanding analogies in abstract domains (Hofstadter
and Mitchell 1994; Hofstadter 1995). The following is an example demonstrating how
slippage operates to explain the metaphorMake-up is a Western burqa.
Make-up =>
? typically worn by women
? expected to be worn by women
?must be worn by women
?must be worn by Muslim women
Burqa <=
By doing insertions and substitutions, the system arrives from the definition ?typi-
cally worn by women? to that of ?must be worn byMuslimwomen.? Thus it establishes
a link between the concepts of make-up and burqa. Veale and Hao, however, did not
evaluate to what extent their system is able to interpret metaphorical expressions in
real-world text.
The next sections of the paper are devoted to our own experiments on metaphor
identification and interpretation.
3. Metaphor Identification Method and Experiments
The first task for metaphor processing within NLP is its identification in text. As dis-
cussed earlier, previous approaches to this problem either utilize hand-coded knowl-
edge (Fass 1991; Krishnakumaran and Zhu 2007) or reduce the task to searching for
metaphors of a specific domain defined a priori (e.g., MOTIONmetaphors) in a specific
type of discourse (e.g., the Wall Street Journal [Gedigian et al 2006]). In contrast, the
search space in our experiments is the entire BNC and the domain of the expressions
identified is unrestricted. In addition, the developed technique does not rely on any
hand-crafted lexical or world knowledge, but rather captures metaphoricity by means
of verb and noun clustering in a data-driven manner.
Themotivation behind the use of clusteringmethods for themetaphor identification
task lies in CMT. The patterns of conceptual metaphor (e.g., FEELINGS ARE LIQUIDS)
314
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
always operate on semantic classes, that is, groups of related concepts, defined by
Lakoff and Johnson as conceptual domains (FEELINGS include love, anger, hatred, etc.;
LIQUIDS include water, tea, petrol, beer, etc.). Thus modeling metaphorical mechanisms
in accordance with CMT would involve capturing such semantic classes automatically.
Previous research on corpus-based lexical semantics has shown that it is possible to
automatically induce semantic word classes from corpus data via clustering of contex-
tual cues (Pereira, Tishby, and Lee 1993; Lin 1998; Schulte im Walde 2006). The current
consensus is that the lexical items showing similar behavior in a large body of text most
likely have related meanings.
The second reason for the use of unsupervised and weakly supervised methods
is suggested by the results of corpus-based studies of conceptual metaphor. The anal-
ysis of conceptual mappings in unrestricted text, conducted by Shutova and Teufel
(2010), although confirming some aspects of CMT, uncovered a number of fundamental
difficulties. One of these is the choice of the level of abstraction and granularity of
categories (i.e., labels for source and target domains). This suggests that it is hard
to define a comprehensive inventory of labels for source and target domains. Thus a
computational model of metaphorical associations should not rely on explicit domain
labels. Unsupervised methods allow us to recover patterns in data without assigning
any explicit labels to concepts, and thus to model interconceptual mappings implicitly.
The method behind our metaphor identification system relies on distributional
clustering. Noun clustering, specifically, is central to the approach. It is traditionally
assumed that noun clusters produced using distributional clustering contain concepts
that are similar to each other. This is true only in part, however. There exist two types
of concepts: concrete, those concepts denoting physical entities or physical experiences
(e.g., chair, apple, house, rain) and abstract, those concepts that do not physically exist
at any particular time or place, but rather exist as a type of thing or as an idea (e.g.,
justice, love, democracy). It is the abstract concepts that tend to be described metaphori-
cally, rather than concrete concepts. Humans use metaphor attempting to gain a better
understanding of an abstract concept by comparing it to their physical experiences. As
a result, abstract concepts expose different distributional behavior in a corpus. This
in turn affects the application of clustering techniques and the obtained clusters for
concrete and abstract concepts would be structured differently. Consider the example in
Figure 3. The figure shows a cluster containing concrete concepts (on the right) that are
various kinds of mechanisms; a cluster containing verbs co-occurring with mechanisms
in the corpus (at the bottom); and a cluster containing abstract concepts (on the left)
that tend to co-occur with these verbs. Such abstract concepts, albeit having quite
distinct meanings (e.g., marriage and democracy), are observed in similar lexico-syntactic
environments. This is due to the fact that they are systematically used metaphorically
with the verbs from the domain of MECHANISM. Hence, they are automatically
assigned to the same cluster. The following examples illustrate this phenomenon in
textual data.
(18) Our relationship is not really working.
(19) Diana and Charles did not succeed in mending their marriage.
(20) The wheels of Stalin?s regime were well oiled and already turning.
Such a structure of the abstract clusters can be explained by the fact that relationships,
marriages, collaborations, and political systems are all cognitively mapped to the same
315
Computational Linguistics Volume 39, Number 2
Figure 3
Cluster of target concepts associated with MECHANISM.
source domain of MECHANISM. In contrast to concrete concepts, such as tea, water,
coffee, beer, drink, liquid, that are clustered together when they have similar meanings,
abstract concepts tend to be clustered together if they are associated with the same
source domain.We define this phenomenon as clustering by association and it becomes
central to the system design. The expectation is that clustering by association would
allow the harvesting of new target domains that are associated with the same source
domain, and thus identify new metaphors.
The metaphor identification system starts from a small set of seed metaphorical
expressions, that is, annotatedmetaphors (such as those in Examples (18) or (19)), which
serve as training data. Note that seed annotation only concerns linguistic metaphors;
metaphorical mappings are not annotated. The system then (1) creates source domains
describing these examples by means of verb clustering (such as the verb cluster in
Figure 3); (2) identifies new target domains associated with the same source domain by
means of noun clustering (see, e.g., ABSTRACT cluster in Figure 3), and (3) establishes a
link between the source and the target clusters based on the seed examples.
Thus the system captures metaphorical associations implicitly. It generalizes over
the associated domains by means of verb and noun clustering. The obtained clusters
then represent source and target concepts between which metaphorical associations
hold. The knowledge of such associations is then used to identify new metaphorical
expressions in a large corpus.
In addition to this, we build a selectional preference?based metaphor filter. This
idea stems from the view of Wilks (1978), but is, however, a modification of it. The
filter assumes that the verbs exhibiting weak selectional preferences, namely, verbs co-
occurring with any argument class in linguistic data (remember, influence, etc.) generally
have no or only weak potential for being a metaphor. It has been previously shown
that it is possible to quantify verb selectional preferences on the basis of corpus data,
using, for example, a measure defined by Resnik (1993). Once the candidate metaphors
are identified in the corpus using clustering methods, those displaying weak selectional
preferences can be filtered out.
Figures 4 and 5 depict the metaphor identification pipeline: first, the identifica-
tion of metaphorical associations and then that of metaphorical expressions in text. In
316
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 4
Learning metaphorical associations by means of verb and noun clustering and using the seed set.
summary, the system (1) starts from a seed set of metaphorical expressions exemplifying
a range of source?target domain mappings; (2) performs noun clustering in order to
harvest various target concepts associated with the same source domain; (3) creates a
source domain verb lexicon by means of verb clustering; (4) searches the corpus for
metaphorical expressions describing the target domain concepts using the verbs from
the source domain lexicon; and (5) filters out the candidates exposing weak selectional
preference strength as non-metaphorical.
Figure 5
Identification of new metaphorical expressions in text.
317
Computational Linguistics Volume 39, Number 2
3.1 Experimental Data
The identification system takes a list of seed phrases as input. Seed phrases contain
manually annotated linguistic metaphors. The system generalizes from these linguistic
metaphors to the respective conceptual metaphors by means of clustering. This gen-
eralization is then used to harvest a large number of new metaphorical expressions in
unseen text. Thus the data needed for the identification experiment consist of a seed set,
data sets of verbs and nouns that are subsequently clustered, and an evaluation corpus.
3.1.1 Metaphor Corpus and Seed Phrases. The data to test the identification module were
extracted from the metaphor corpus created by Shutova and Teufel (2010). Their corpus
is a subset of the BNC (Burnard 2007) and, as such, it provides a suitable platform
for testing the metaphor processing system on real-world general-domain expressions
in contemporary English. Our data set consists of verb?subject and verb?direct object
metaphorical expressions. In order to avoid extra noise, we enforced some additional
selection criteria. All phrases were included unless they fell in one of the following
categories:
 Phrases where the subject or object referent is unknown (e.g., containing
pronouns such as in ?in which they [changes] operated?) or represented
by a named entity (e.g., ?Then Hillary leapt into the conversation?).
These cases were excluded from the data set because their processing
would involve the use of additional modules for coreference resolution
and named entity recognition, which in turn may introduce additional
errors into the system.
 Phrases whose metaphorical meaning is realized solely in passive
constructions (e.g., ?sociologists have been inclined to [..]?). These cases
were excluded because for many such examples it was hard for humans
to produce a literal paraphrase realized in the form of the same syntactic
construction. Thus their paraphrasing was deemed to be an unfairly
hard task for the system.
 Multiword metaphors (e.g., ?whether we go on pilgrimage with Raleigh or
put out to seawith Tennyson?). The current system is designed to identify
and paraphrase single-word, lexical metaphors. In the future the system
needs to be modified to process multiword metaphorical expressions;
this is, however, outside the scope of the current experiments.
The resulting data set consists of 62 phrases that are different single-word metaphors
representing verb?subject and verb?direct object relations, where a verb is used meta-
phorically. The phrases include, for instance, ?stir excitement,? ?reflect enthusiasm,?
?grasp theory,? ?cast doubt,? ?suppressmemory,? ?throw remark? (verb?direct object con-
structions); and ?campaign surged,? ?factor shaped [...],? ?tension mounted,? ?ideology
embraces,? ?example illustrates? (subject?verb constructions). This data set was used as
a seed set in the identification experiments. The phrases in the data set were manually
annotated for grammatical relations.
3.1.2 Verb and Noun Data Sets. The noun data set used for clustering consists of the
2,000 most frequent nouns in the BNC. The 2,000 most frequent nouns cover most
common target categories and their linguistic realizations. BNC represents a suitable
318
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
source for such nouns because the corpus is balanced with respect to genre, style,
and theme.
The verb data set is a subset of VerbNet (Kipper et al 2006). VerbNet is the largest
resource for general-domain verbs organized into semantic classes as proposed by Levin
(1993). The data set includes all the verbs in VerbNet with the exception of highly
infrequent ones. The frequency of the verbs was estimated from the data collected by
Korhonen, Krymolowski, and Briscoe (2006) for the construction of the VALEX lexicon,
which to date is one of the largest automatically created verb resources. The verbs from
VerbNet that appear less than 150 times in this data were excluded. The resulting data
set consists of 1,610 general-domain verbs.
3.1.3 Evaluation Corpus. The evaluation data for metaphor identification was the BNC
parsed by the RASP parser (Briscoe, Carroll, and Watson 2006). We used the gram-
matical relation (GR) output of RASP for the BNC created by Andersen et al (2008).
The system searched the corpus for the source and target domain vocabulary within a
particular grammatical relation (verb?direct object or verb?subject).
3.2 Method
The main components of the method include (1) distributional clustering of verbs
and nouns, (2) search through the parsed corpus, and (3) selectional preference-based
filtering.
3.2.1 Verb and Noun Clustering Method. The metaphor identification system relies on
the clustering method of Sun and Korhonen (2009). They use a rich set of syntactic
and semantic features (GRs, verb subcategorization frames [SCFs], and selectional
preferences) and spectral clustering, a method particularly suitable for the resulting
high dimensional feature space. This algorithm has proved to be effective in previous
verb clustering experiments (Brew and Schulte im Walde 2002) and in other NLP tasks
involving high dimensional data (Chen et al 2006).
Spectral clustering partitions objects relying on their similarity matrix. Given a set
of data points, the similarity matrix records similarities between all pairs of points. The
system of Sun and Korhonen (2009) constructs similarity matrices using the Jensen-
Shannon divergence as a measure. Jensen-Shannon divergence between two feature
vectors wi and wj is defined as follows:
JSD(wi,wj) =
1
2
D(wi||m)+ 12D(wj||m) (1)
where D is the Kullback-Leibler divergence, and m is the average of the wi and wj.
Spectral clustering can be viewed in abstract terms as the partitioning of a graph
G over a set of words W. The weights on the edges of G are the similarities Sij. The
similarity matrix S thus represents the adjacency matrix for G. The clustering problem
is then defined as identifying the optimal partition, or cut, of the graph into clusters,
such that the intra-cluster weights are high and the inter-cluster weights are low. The
system of Sun and Korhonen (2009) uses the MNCut algorithm of Meila and Shi (2001)
for this purpose.
Sun and Korhonen (2009) evaluated their clustering approach on 204 verbs from
17 Levin classes and obtained an F-measure of 80.4, which is the state-of-the-art
319
Computational Linguistics Volume 39, Number 2
performance level. The metaphor identification system uses the method of Sun and
Korhonen to cluster both verbs and nouns (separately), however, significantly extending
its coverage to unrestricted general-domain data and applying the method to a con-
siderably larger data set of 1,610 verbs.
3.2.2 Feature Extraction and Clustering Experiments. For verb clustering, the best perform-
ing features from Sun and Korhonen (2009) were adopted. These include automatically
acquired verb SCFs parameterized by their selectional preferences. These features were
obtained using the SCF acquisition system of Preiss, Briscoe, and Korhonen (2007). The
system tags and parses corpus data using the RASP parser (Briscoe, Carroll, andWatson
2006) and extracts SCFs from the produced grammatical relations using a rule-based
classifier which identifies 168 SCF types for English verbs. It produces a lexical entry
for each verb and SCF combination occurring in corpus data. The selectional preference
classes were obtained by clustering nominal arguments appearing in the subject and
object slots of verbs in the resulting lexicon.
Following previous works on semantic noun classification (Pantel and Lin 2002;
Bergsma, Lin, and Goebel 2008), grammatical relations were used as features for noun
clustering. More specifically, the frequencies of nouns and verb lemmas appearing in
the subject, direct object, and indirect object relations in the RASP-parsed BNC were
included in the feature vectors. For example, the feature vector for bananawould contain
the following entries: {eat-dobj n1, fry-dobj n2, sell-dobj n3,..., eat with-iobj ni,
look at-iobj ni+1,..., rot-subj nk, grow-subj nk+1,...}.
We experimented with different clustering granularities, subjectively examined the
obtained clusters, and determined that the number of clusters set to 200 is the most
suitable setting for both nouns and verbs in our task. This was done by means of qual-
itative analysis of the clusters as representations of source and target domains?that is,
by judging how complete and homogeneous the verb clusters were as lists of potential
source domain vocabulary and howmany new target domains associated with the same
source domain were found correctly in the noun clusters. This analysis was performed
on a randomly selected set of 10 clusters taken from different granularity settings and
none of the seed expressions were used for it. Examples of such clusters are shown in
Figures 6 (nouns) and 7 (verbs), respectively. The noun clusters represent target concepts
associated with the same source concept (some suggested source concepts are given in
Figure 6, although the system only captures those implicitly). The verb clusters contain
lists of source domain vocabulary.
3.2.3 Corpus Search.Once the clusters have been obtained, the system proceeds to search
the corpus for source and target domain terms within verb?object (both direct and
indirect) and verb?subject relations. For each seed expression, a cluster is retrieved for
the verb to form the source concept, and a cluster is retrieved for the noun to form a list
of target concepts. The retrieved verb and noun clusters are then linked, and such links
represent metaphorical associations. The system then classifies grammatical relations in
the corpus as metaphorical if the lexical items in the grammatical relation appear in the
linked source (verb) and target (noun) clusters. This search is performed on the BNC
parsed by RASP. Consider the following example sentence extracted from the BNC (the
BNC text ID is given in brackets, followed by the hypothetical conceptual metaphor):
(21) Few would deny that in the nineteenth century change was greatly accelerated.
(ACA) ? CHANGE IS MOTION
320
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Source: MECHANISM
Target Cluster: consensus relation tradition partnership resistance foundation alliance friendship con-
tact reserve unity link peace bond myth identity hierarchy relationship connection balance marriage
democracy defense faith empire distinction coalition regime division
Source: PHYSICAL OBJECT; LIVING BEING; STRUCTURE
Target Cluster: view conception theory concept ideal belief doctrine logic hypothesis interpretation
proposition thesis assumption idea argument ideology conclusion principle notion philosophy
Source: STORY; JOURNEY
Target Cluster: politics practice trading reading occupation profession sport pursuit affair career think-
ing life
Source: LIQUID
Target Cluster: disappointment rage concern desire hostility excitement anxiety passion doubt panic
delight anger fear curiosity shock terror surprise pride happiness pain enthusiasm alarm hope memory
love satisfaction sympathy spirit frustration impulse instinct warmth beauty ambition thought guilt
emotion sensation horror feeling laughter suspicion pleasure
Source: LIVING BEING; END
Target Cluster: defeat fall death tragedy loss collapse decline disaster destruction fate
Figure 6
Clustered nouns (the associated source domain labels are suggested by the authors for clarity;
the system does not assign any labels, but models source and target domains implicitly).
Source Cluster: sparkle glow widen flash flare gleam darken narrow flicker shine blaze bulge
Source Cluster: gulp drain stir empty pour sip spill swallow drink pollute seep flow drip purify ooze
pump bubble splash ripple simmer boil tread
Source Cluster: polish clean scrape scrub soak
Source Cluster: kick hurl push fling throw pull drag haul
Source Cluster: rise fall shrink drop double fluctuate dwindle decline plunge decrease soar tumble
surge spiral boom
Source Cluster: initiate inhibit aid halt trace track speed obstruct impede accelerate slow stimulate
hinder block
Source Cluster: work escape fight head ride fly arrive travel come run go slip move
Figure 7
Clustered verbs.
The relevant GRs identified by the parser are presented in Figure 8. The relation between
the verb accelerate and its semantic object change in Example (21) is expressed in the
passive voice and is, therefore, tagged by RASP as an ncsubj GR. Because this GR con-
tains terminology from associated source (MOTION) and target (CHANGE) domains,
it is marked as metaphorical and so is the term accelerate, which belongs to the source
domain of MOTION.
3.2.4 Selectional Preference Strength Filter. In the previous step a set of candidate verb
metaphors and the associated grammatical relations were extracted from the BNC.
These now need to be filtered based on selectional preference strength. To do this, we
Figure 8
Grammatical relations output for metaphorical expressions.
321
Computational Linguistics Volume 39, Number 2
automatically acquire selectional preference distributions for verb?subject and verb?
direct object relations from the RASP-parsed BNC. The noun clusters obtained using
Sun and Korhonen?s method as described earlier form the selectional preference classes.
To quantify selectional preferences, we adopt the selectional preference strength (SPS)
measure of Resnik (1993). Resnik models selectional preferences of a verb in proba-
bilistic terms as the difference between the posterior distribution of noun classes in a
particular relation with the verb and their prior distribution in that syntactic position
irrespective of the identity of the verb. He quantifies this difference using the Kullback-
Leibler divergence and defines selectional preference strength as follows:
SR(v) = D(P(c|v)||P(c)) =
?
c
P(c|v) log
P(c|v)
P(c) (2)
where P(c) is the prior probability of the noun class, P(c|v) is the posterior probability
of the noun class given the verb, and R is the grammatical relation in question. In order
to quantify how well a particular argument class fits the verb, Resnik defines another
measure called selectional association:
AR(v, c) =
1
SR(v)
P(c|v) log
P(c|v)
P(c)
(3)
which stands for the contribution of a particular argument class to the overall selectional
preference strength of a verb.
The probabilities P(c|v) and P(c) were estimated from the corpus data as follows:
P(c|v) =
f (v, c)
?
k f (v, ck)
(4)
P(c) =
f (c)
?
k f (ck)
(5)
where f (v, c) is the number of times the predicate v co-occurs with the argument class c
in the relation R, and f (c) is the number of times the argument class occurs in the relation
R regardless of the identity of the predicate.
Thus for each verb, its SPS can be calculated for specific grammatical relations.
This measure was used to filter out the verbs with weak selectional preferences. The
expectation is that such verbs are unlikely to be used metaphorically. The optimal
selectional preference strength threshold was set experimentally for both verb?subject
and verb?object relations on a small held-out data set (via qualitative analysis of the
data). It approximates to 1.32. The system excludes expressions containing the verbs
with preference strength below this threshold from the set of candidate metaphors.
Examples of verbs with weak and strong direct object SPs are shown in Tables 2 and
3, respectively. Given the SPS threshold of 1.32, the filter discards 31% of candidate
expressions initially identified in the corpus.
3.3 Evaluation
In order to show that the described metaphor identification method generalizes well
over the seed set and that it operates beyond synonymy, its output was compared to
322
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 2
Verbs with weak direct object SPs.
SPS Verb
1.3175 undo
1.3160 bud
1.3143 deplore
1.3138 seal
1.3131 slide
1.3126 omit
1.3118 reject
1.3097 augment
1.3094 frustrate
1.3087 restrict
1.3082 employ
1.3081 highlight
1.3081 correspond
1.3056 dab
1.3053 assist
1.3043 neglect
...
Table 3
Verbs with strong direct object SPs.
SPS Verb SPS Verb
...
3.0810 aggravate 2.9434 coop
3.0692 dispose 2.9326 hobble
3.0536 rim 2.9285 paper
3.0504 deteriorate 2.9212 sip
3.0372 mourn ...
3.0365 tread 1.7889 schedule
3.0348 cadge 1.7867 cheat
3.0254 intersperse 1.7860 update
3.0225 activate 1.7840 belt
3.0085 predominate 1.7835 roar
3.0033 lope 1.7824 intensify
2.9957 bone 1.7811 read
2.9955 pummel 1.7805 unnerve
2.9868 disapprove 1.7776 arrive
2.9838 hoover 1.7775 publish
2.9824 beam 1.7775 reason
2.9807 amble 1.7774 bond
2.9760 diversify 1.7770 issue
2.9759 mantle 1.7760 verify
2.9730 pulverize 1.7734 vomit
2.9604 skim 1.7728 impose
2.9539 slam 1.7726 phone
2.9523 archive 1.7723 purify
2.9504 grease ...
323
Computational Linguistics Volume 39, Number 2
that of a baseline using WordNet. In the baseline system, WordNet synsets represent
source and target domains. The quality of metaphor identification for both the system
and the baseline was evaluated in terms of precision with the aid of human judges.
To compare the coverage of the system to that of the baseline in quantitative terms we
assessed how broadly they expand on the seed set. To do this, we estimated the number
of word senses captured by the two systems and the proportion of identified metaphors
that are not synonymous with any of those seen in the seed set, according to WordNet.
This type of evaluation assesses how well clustering methods are suited to identify new
metaphors not directly related to those in the seed set.
3.3.1 Comparison with WordNet Baseline. The baseline system was implemented using
synonymy information from WordNet to expand on the seed set. Source and target
domain vocabularies were thus represented as sets of synonyms of verbs and nouns in
seed expressions. The baseline system then searched the corpus for phrases composed
of lexical items belonging to those vocabularies. For example, given a seed expression
?stir excitement,? the baseline finds phrases such as ?arouse fervour, stimulate agitation,
stir turmoil,? and so forth. It is not able to generalize over the concepts to broad
semantic classes, however?for example, it does not find other FEELINGS such as
rage, fear, anger, pleasure. This, however, is necessary to fully characterize the target
domain. Similarly, in the source domain, the system only has access to direct synonyms
of stir, rather than to other verbs characteristic of the domain of LIQUIDS (pour, flow,
boil, etc.).
To compare the coverage achieved by the system using clustering to that of the
baseline in quantitative terms, we estimated the number of WordNet synsets, that
is, different word senses, in the metaphorical expressions captured by the two sys-
tems. We found that the baseline system covers only 13% of the data identified using
clustering. This is due to the fact that it does not reach beyond the concepts present
in the seed set. In contrast, most metaphors tagged by the clustering method (87%)
are non-synonymous to those in the seed set and some of them are novel. Together,
these metaphors represent a considerably wider range of meanings. Given the seed
metaphors ?stir excitement, throw remark, cast doubt,? the system identifies previously
unseen expressions ?swallow anger, hurl comment, spark enthusiasm,? and so on, as
metaphorical. Tables 4 and 5 show examples of how the system and the baseline expand
on the seed set, respectively. Full sentences containing metaphors annotated by the
system are shown in Figure 9. Twenty-one percent of the expressions identified by the
system do not have their correspondingmetaphorical senses included inWordNet, such
as ?spark enthusiasm?; the remaining 79% are, however, more common conventional
metaphors. Starting with a seed set of only 62 examples, the system expands signif-
icantly on the seed set and identifies a total of 4,456 metaphorical expressions in the
BNC. This suggests that the method has the potential to attain a broad coverage of the
corpus given a large and representative seed set.
3.3.2 Evaluation Against Human Judgments. In order to assess the quality of metaphor
identification by both systems, their output was assessed by human judgments. For
this purpose, we randomly sampled sentences containing metaphorical expressions as
annotated by the system and by the baseline and asked human annotators to decide
whether these were metaphorical or not.
Participants Five volunteers participated in the experiment. They were all native
speakers of English and had no formal training in linguistics.
324
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 4
Examples of seed set expansion by the system.
Seed phrase Harvested metaphors BNC frequency
reflect concern (V-O): reflect concern 78
reflect interest 74
reflect commitment 26
reflect preference 22
reflect wish 17
reflect determination 12
reflect intention 8
reflect willingness 4
reflect sympathy 3
reflect loyalty 2
disclose interest 10
disclose intention 3
disclose concern 2
disclose sympathy 1
disclose commitment 1
disguise interest 6
disguise intention 3
disguise determination 2
obscure interest 1
obscure determination 1
cast doubt (V-O): cast doubt 197
cast fear 3
cast suspicion 2
catch feeling 3
catch suspicion 2
catch enthusiasm 1
catch emotion 1
spark fear 10
spark enthusiasm 3
spark passion 1
spark feeling 1
campaign surged (S-V): campaign surged 1
charity boomed 1
effort decreased 1
expedition doubled 1
effort doubled 1
campaign shrank 1
campaign soared 1
drive spiraled 1
Materials The subjects were presented with a set of 78 randomly sampled sentences
annotated by the two systems. Fifty percent of the data set were the sentences annotated
by the identification system and the remaining 50%were annotated by the baseline; and
the sentences were randomized. The annotation was done electronically in Microsoft
Word. An example of annotated sentences is given in Figure 10.
Task and guidelines The subjects were asked to mark which of the expressions were
metaphorical in their judgment. The participants were encouraged to rely on their
own intuition of what a metaphor is in the annotation process. Additional guidance,
325
Computational Linguistics Volume 39, Number 2
Table 5
Examples of seed set expansion by the baseline.
Seed phrase Harvested metaphors BNC frequency
reflect concern (V-O): reflect concern 78
ponder business 1
ponder headache 1
reflect business 4
reflect care 2
reflect fear 19
reflect worry 3
cast doubt (V-O): cast doubt 197
cast question 11
couch question 1
drop question 2
frame question 21
purge doubt 2
put doubt 12
put question 151
range question 1
roll question 1
shed doubt 2
stray question 1
throw doubt 35
throw question 17
throw uncertainty 1
campaign surged (S-V): campaign surged 1
campaign soared 1
however, in the form of the following definition of metaphor (Pragglejaz Group 2007)
was also provided:
1. For each verb establish its meaning in context and try to imagine a more
basic meaning of this verb in other contexts. Basic meanings normally are:
(1) more concrete; (2) related to bodily action; (3) more precise (as opposed
to vague); (4) historically older.
2. If you can establish a basic meaning that is distinct from the meaning of
the verb in this context, the verb is likely to be used metaphorically.
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then swallow his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AMA 349Wewill halt the reduction in NHS services for long-term care and community health services
which support elderly and disabled patients at home.
ADK 634 Catch their interest and spark their enthusiasm so that they begin to see the product?s
potential.
K2W 1771 The committee heard today that gangs regularly hurled abusive comments at local people,
making an unacceptable level of noise and leaving litter behind them.
Figure 9
Sentences tagged by the system (metaphors in bold).
326
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 10
Evaluation of metaphor identification.
Interannotator agreement Reliability was measured at ? = 0.63 (n = 2,N = 78, k = 5).
The data suggest that the main source of disagreement between the annotators was the
presence of conventional metaphors (e.g., verbs such as adopt, convey, decline).
Results The system performance was then evaluated against the elicited judgments in
terms of precision. The system output was compared to the gold standard constructed
by merging the judgments, whereby the expressions tagged as metaphorical by at least
three annotators were considered to be correct. This resulted in P = 0.79, with the
baseline attaining P = 0.44. In addition, the system tagging was compared to that of
each annotator pairwise, yielding an average P = 0.74 for the system and P = 0.41 for
the baseline.
In order to compare system performance to the human ceiling, pairwise agreement
was additionally calculated in terms of precision between the majority gold standard
and each judge. This corresponds to an average of P = 0.94.
To show that the system performance is significantly different from that of the base-
line, we annotated additional 150 instances identified by both systems for correctness
and conducted a one-tailed t-test for independent samples. The difference is statistically
significant with t = 4.11 (df = 148, p < 0.0005).
3.4 Discussion
We have shown that the method leads to a considerable expansion on the seed set and
operates with high precision?namely, it produces high quality annotations, and iden-
tifies fully novel metaphorical expressions relying only on the knowledge of source?
target domain mappings that it learns automatically. By comparing its coverage to that
of a WordNet baseline, we showed that the method reaches beyond synonymy and
generalizes well over the source and target domains.
The observed discrepancy in precision between the clustering approach and the
baseline can be explained by the fact that a large number of metaphorical senses are
included in WordNet. This means that in WordNet synsets source domain verbs appear
together with more abstract terms. For instance, the metaphorical sense of shape in
the phrase ?shape opinion? is part of the synset ?(determine, shape, mold, influence,
regulate).? This results in the low precision of the baseline system, because it tags literal
expressions (e.g., influence opinion) as metaphorical, assuming that all verbs from the
synset belong to the source domain.
327
Computational Linguistics Volume 39, Number 2
To perform a more comprehensive error analysis, we examined a larger subset of
the metaphorical expressions identified by the system (200 sentences, equally covering
verb?subject and verb?object constructions). System precision against the additional
judgments by one of the authors was measured at 76% (48 instances were tagged
incorrectly according to the judgments). The classification of system errors by type is
presented in Table 6. Precision errors in the output of the system were also concentrated
around the problem of conventionality of some metaphorical verbs, such as those in
?hold views, adopt traditions, tackle a problem.? This conventionality is reflected in the
data in that such verbs are frequently used in their ?metaphorical? contexts. As a result,
they are clustered together with literally used terms. For instance, the verb tackle is
found in a cluster with solve, resolve, handle, confront, face, and so forth. This results in
the system tagging ?resolve a problem? as metaphorical if it has previously seen ?tackle
a problem.?
A number of system errors affecting its precision are also due to cases of general
polysemy and homonymy of both verbs and nouns. For example, the noun passage
can mean both ?the act of passing from one state or place to the next? and ?a section
of text; particularly a section of medium length,? as defined in WordNet. Sun and
Korhonen?s (2009) method performs hard clustering, that is, it does not distinguish
between different word senses. Hence the noun passage occurred in only one cluster,
containing concepts like thought, word, sentence, expression, reference, address, description,
and so on. This cluster models the ?textual? meaning of passage. As a result of sense
ambiguity within the cluster, given the seed phrase ?she blocked the thought,? the system
tags such expressions as ?block passage,? ?impede passage,? ?obstruct passage,? and
?speed passage? as metaphorical.
The errors that may cause low recall of the system are of a different nature. Whereas
noun clustering considerably expands the seed set by identifying new associated tar-
get concepts (e.g., given the seed metaphor ?sell soul? it identifies ?sell skin? and
?launch pulse? as metaphorical), the verb clusters sometimes miss a certain proportion
of source domain vocabulary. For instance, given the seed metaphor ?example illus-
trates,? the system identifies the following expressions: ?history illustrates,? ?episode
illustrates,? ?tale illustrates,? ?combination illustrates,? ?event illustrates,? and so forth. It
does not, however, capture obvious verb-based expansions, such as ?episode portrays,?
present in the BNC. This is one of the problems that could lead to a lower recall
of the system.
Nevertheless, in many cases the system benefits not only from dissimilar concepts
within the noun clusters used to detect new target domains, but also from dissim-
ilar concepts in the verb clusters. Verb clusters produced automatically relying on
Table 6
Common system errors by type.
Source of error Subject?Verb Verb?Object Totals
Metaphor conventionality 7 14 21
General polysemy 9 6 15
Verb clustering 4 5 9
Noun clustering 2 1 3
SP filter 0 0 0
Totals 22 26 48
328
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
contextual features may contain lexical items with distinct, or even opposite meanings
(e.g., throw and catch, take off and land). They tend to belong to the same semantic
domain, however (e.g., verbs of dealing with LIQUIDS, verbs describing a FIGHT) It is
the diversity of verb meanings within the domain cluster that allows the generalization
from a limited number of seed expressions to a broader spectrum of previously unseen
and novel metaphors, non-synonymous with those in the seed set.
The fact that the approach is seed-dependent is one of its possible limitations,
affecting the coverage of the system. Wide coverage is essential for the practical use
of the system. At this stage, however, it was impossible for us to reliably measure the
recall of the system, because there is no large corpus annotated for metaphor available.
In addition, because the current system was only tested with very few seeds (again,
due to the lack of metaphor-annotated data), we expect the current overall recall of the
system to be relatively low. In order to obtain a full coverage of the corpus, a large and
representative seed set is necessary. Although it is hard to capture the whole variety of
metaphorical language in a limited set of examples, it is possible to compile a seed set
representative of all common source?target domain mappings. The learning capabilities
of the system can then be used to expand on those to the whole range of conventional
and novel metaphorical mappings and expressions. In addition, because the precision
of the system was measured on the data set produced by expanding individual seed
expressions, we would expect the expansion of other, new seed expressions to yield a
comparable quality of annotations. Incorporating new seed expressions is thus likely to
result in increasing recall without a significant loss in precision.
The current system harvests a large and relatively clean set of metaphorical
expressions from the corpus. These annotations could provide a new platform for the
development and testing of other metaphor systems.
4. Metaphor Interpretation Method and Experiments
As is the case in metaphor identification, the majority of existing approaches to meta-
phor interpretation also rely on task-specific hand-coded knowledge (Martin 1990; Fass
1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004;
Agerri et al 2007) and produce interpretations in a non-textual format (Veale and Hao
2008). The ultimate objective of automatic metaphor processing, however, is a type
of interpretation that can be directly embedded into other systems to enhance their
performance. We thus define metaphor interpretation as a paraphrasing task and build
a system that automatically derives literal paraphrases for metaphorical expressions in
unrestricted text. Our method is also distinguished from previous work in that it does
not rely on any hand-crafted knowledge aboutmetaphor, but in contrast is corpus-based
and uses automatically induced selectional preferences.
The metaphor paraphrasing task can be divided into two subtasks: (1) generating
paraphrases, that is, other ways of expressing the same meaning in a given context,
and (2) discriminating between literal and metaphorical paraphrases. Consequently,
the proposed approach is theoretically grounded in two ideas underlying each of these
subtasks:
 The meaning of a word in context emerges through interaction with the
meaning of the words surrounding it. This assumption is widely accepted
in lexical semantics theory (Pustejovsky 1995; Hanks and Pustejovsky
2005) and has been exploited for lexical acquisition (Schulte im Walde
2006; Sun and Korhonen 2009). It suggests that the context itself imposes
329
Computational Linguistics Volume 39, Number 2
certain semantic restrictions on the words which can occur within it.
Given a large amount of linguistic data, it is possible to model these
semantic restrictions in probabilistic terms (Lapata 2001). This can be
done by deriving a ranking scheme for possible paraphrases that fit or
do not fit in a specific context based on word co-occurrence evidence.
This is how initial paraphrases are generated within the metaphor
paraphrasing module.
 Literalness can be detected via strong selectional preference. This idea
is a mirror-image of the selectional preference violation view of Wilks
(1978), who suggested that a violation of selectional preferences indicates
a metaphor. The key information that selectional preferences provide is
whether there is an association between the predicate and its potential
argument and how strong that association is. A literal paraphrase
would normally come from the target domain (e.g., ?understand the
explanation?) and be strongly associated with the target concept, whereas
a metaphorical paraphrase would belong to the source domain (e.g.,
?grasp the explanation?) and be associated with the concepts from this
source domain more strongly than with the target concept. Hence we
use a selectional preference model to measure the semantic fit of the
generated paraphrases into the given context as opposed to all other
contexts. The highest semantic fit then indicates the most literal
paraphrase.
Thus the context-based probabilistic model is used for paraphrase generation and
the selectional preference model for literalness detection. The key difference between
the two models is that the former favors the paraphrases that co-occur with the words
in the context more frequently than other paraphrases do, and the latter favors the
paraphrases that co-occur with the words from the context more frequently than with
any other lexical items in the corpus. This is the main intuition behind our approach.
The system thus incorporates the following components:
 a context-based probabilistic model that acquires paraphrases for
metaphorical expressions from a large corpus;
 a WordNet similarity component that filters out the irrelevant
paraphrases based on their similarity to the metaphorical term (similarity
is defined as sharing a common hypernym within three levels in the
WordNet hierarchy);
 a selectional preference model that discriminates literal paraphrases from
the metaphorical ones. It re-ranks the paraphrases, de-emphasizing the
metaphorical ones and emphasizing the literal ones.
In addition, the system disambiguates the sense of the paraphrases using the
WordNet inventory of senses. The context-based model together with the WordNet
filter constitute a metaphor paraphrasing baseline. By comparing the final system to
this baseline, we demonstrate that simple context-based substitution, even supplied by
extensive knowledge contained in lexical resources, is not sufficient for metaphor inter-
pretation and that a selectional preference model is needed to establish the literalness of
the paraphrases.
330
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
This section first provides an overview of paraphrasing and lexical substitution
and relates these tasks to the problem of metaphor interpretation. It then describes
the experimental data used to develop and test the paraphrasing system and the
method itself, and finally, concludes with the system evaluation and the presentation
of results.
4.1 Paraphrasing and Lexical Substitution
Paraphrasing can be viewed as a text-to-text generation problem, whereby a new piece
of text is produced conveying the same meaning as the original text. Paraphrasing can
be carried out at multiple levels (sentence-, phrase-, and word-levels), and may involve
both syntactic and lexical transformations. Paraphrasing by replacing individual words
in a sentence is known as lexical substitution (McCarthy 2002). Because, in this article,
we address the phenomenon of metaphor at a single-word level, our task is close in
nature to lexical substitution. The task of lexical substitution originates from word
sense disambiguation (WSD). The key difference between the two is that whereas WSD
makes use of a predefined sense-inventory to characterize the meaning of a word in
context, lexical substitution is aimed at automatic induction of meanings. Thus the goal
of lexical substitution is to generate the set of semantically valid substitutes for the
word. Consider the following sentences from Preiss, Coonce, and Baker (2009).
(22) His parents felt that he was a bright boy.
(23) Our sun is a bright star.
Bright in Example (22) can be replaced by the word intelligent. The same replacement
in the context of Example (23) will not produce an appropriate sentence. A lexical
substitution system needs to (1) find a set of candidate synonyms for the word and
(2) select the candidate that matches the context of the word best.
Both sentence- or phrase-level paraphrasing and lexical substitution find a wide
range of applications in NLP. These include summarization (Knight and Marcu 2000;
Zhou et al 2006), information extraction (Shinyama and Sekine 2003), machine trans-
lation (Kurohashi 2001; Callison-Burch, Koehn, and Osborne 2006), text simplification
(Carroll et al 1999), question answering (McKeown 1979; Lin and Pantel 2001) and
textual entailment (Sekine et al 2007). Consequently, there has been a plethora of
NLP approaches to paraphrasing (McKeown 1979; Meteer and Shaked 1988; Dras 1999;
Barzilay and McKeown 2001; Lin and Pantel 2001; Barzilay and Lee 2003; Bolshakov
and Gelbukh 2004; Quirk, Brockett, and Dolan 2004; Kauchak and Barzilay 2006; Zhao
et al 2009; Kok and Brockett 2010) and lexical substitution (McCarthy and Navigli 2007,
2009; Erk and Pado? 2009; Preiss, Coonce, and Baker 2009; Toral 2009; McCarthy, Keller,
and Navigli 2010).
Among paraphrasing methods one can distinguish (1) rule-based approaches,
which rely on a set of hand-crafted (McKeown 1979; Zong, Zhang, and Yamamoto 2001)
or automatically learned (Lin and Pantel 2001; Barzilay and Lee 2003; Zhao et al 2008)
paraphrasing patterns; (2) thesaurus-based approaches, which generate paraphrases
by substituting words in the sentence by their synonyms (Bolshakov and Gelbukh
2004; Kauchak and Barzilay 2006); (3) natural language generation?based approaches
(Kozlowski, McCoy, and Vijay-Shanker 2003; Power and Scott 2005), which transform
a sentence into its semantic representation and generate a new sentence from it; and
(4) SMT-based methods (Quirk, Brockett, and Dolan 2004), operating as monolingual
331
Computational Linguistics Volume 39, Number 2
MT. A number of approaches to lexical substitution rely on manually constructed
thesauri to find sets of candidate synonyms (McCarthy and Navigli 2007), whereas
others address the task in a fully unsupervised fashion. In order to derive and rank
candidate substitutes, the latter systems make use of distributional similarity measures
(Pucci et al 2009; McCarthy, Keller, and Navigli 2010), vector space models of word
meaning (De Cao and Basili 2009; Erk and Pado? 2009) or statistical learning techniques,
such as hidden Markov models and n-grams (Preiss, Coonce, and Baker 2009).
The metaphor interpretation task is different from the WSD task, because it is
impossible to predefine a set of senses of metaphorical words, in particular for novel
metaphors. Instead, the correct substitute for the metaphorical term needs to be gen-
erated in a data-driven manner, as for lexical substitution. The metaphor paraphrasing
task, however, also differs from lexical substitution in the following two ways. Firstly,
a suitable substitute needs to be used literally in the target context, or at least more
conventionally than the original word. Secondly, by definition, the substitution is not
required to be a synonym of the metaphorical word. Moreover, for our task this is not
even desired, because there is the danger that synonymous paraphrasing may result
in another metaphorical expression, rather than the literal interpretation of the original
one. Metaphor paraphrasing therefore presents an additional challenge in comparison
to lexical substitution, namely, that of discriminating between literal and metaphorical
substitutes. This second, harder, and not previously addressed task is the main focus
of the work presented in this section. The remainder of the section is devoted to the
description of the metaphor paraphrasing experiment.
4.2 Experimental Data
The paraphrasing system is first tested individually on a set of metaphorical expres-
sions extracted from a manually annotated metaphor corpus of Shutova and Teufel
(2010). This is the same data set as the one used for seeding the identification module
(see Section 3.1.1 for description). Because the paraphrasing evaluation described in
this section is conducted independently from the identification experiment, and no
part of the paraphrasing system relies on the output of the identification system and
vice versa, the use of the same data set does not give any unfair advantage to the
systems. In the later experiment (Section 5) when the identification and paraphrasing
system are evaluated jointly, again the same seed set will be used for identification;
paraphrasing, however, will be performed on the output of the identification system
(i.e., the new identified metaphors) and both the identified metaphors and their para-
phrases will be evaluated by human judges not used in the previous and the current
experiments.
4.3 Method
The system takes phrases containing annotated single-word metaphors as input; where
a verb is used metaphorically, its context is used literally. It generates a list of possible
paraphrases of the verb that can occur in the same context and ranks them according
to their likelihood, as derived from the corpus. It then identifies shared features of the
paraphrases and themetaphorical verb using theWordNet hierarchy and removes unre-
lated concepts. It then identifies the literal paraphrases among the remaining candidates
based on the verb?s automatically induced selectional preferences and the properties of
the context.
332
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
4.3.1 Context-based Paraphrase Ranking Model. Terms replacing the metaphorical verb v
will be called its interpretations i. We model the likelihood L of a particular paraphrase
as a joint probability of the following events: the interpretation i co-occurring with the
other lexical items from its context w1, ...,wN in syntactic relations r1, ..., rN, respectively.
Li = P(i, (w1, r1), (w2, r2), ..., (wN, rN )) (6)
where w1, ...,wN and r1, ..., rN represent the fixed context of the term used metaphori-
cally in the sentence. In the system output, the context w1, ...,wN will be preserved, and
the verb v will be replaced by the interpretation i.
We assume statistical independence between the relations of the terms in a phrase.
For instance, for a verb that stands in a relation with both a subject and an object, the
verb?subject and verb?direct object relations are considered to be independent events
within the model. The likelihood of an interpretation is then calculated as follows:
P(i, (w1, r1), (w2, r2), ..., (wN, rN )) = P(i) ? P((w1, r1)|i) ? ... ? P((wN, rN )|i) (7)
The probabilities can be calculated using maximum likelihood estimation
P(i) =
f (i)
?
k f (ik)
(8)
P(wn, rn|i) =
f (wn, rn, i)
f (i)
(9)
where f (i) is the frequency of the interpretation irrespective of its arguments,
?
k f (ik) is
the number of times its part of speech class is attested in the corpus, and f (wn, rn, i) is
the number of times the interpretation co-occurs with context word wn in relation rn. By
performing appropriate substitutions into Equation (7) one obtains
P(i, (w1, r1), (w2, r2), ..., (wN, rN )) =
f (i)
?
k f (ik)
?
f (w1, r1, i)
f (i)
? ... ?
f (wN, rN, i)
f (i)
=
?N
n=1 f (wn, rn, i)
( f (i))N?1 ?
?
k f (ik)
(10)
This model is then used to rank the possible replacements of the term used meta-
phorically in the fixed context according to the data. The parameters of the model were
estimated from the RASP-parsed BNC using the grammatical relations output created
by Andersen et al (2008).
4.3.2 WordNet Filter. The context-based model described in Section 4.3.1 overgenerates
and hence there is a need to further narrow down the results. It is acknowledged in the
linguistics community that metaphor is, to a great extent, based on similarity between
the concepts involved (Gentner et al 2001). We exploit this fact to refine paraphrasing.
After obtaining the initial list of possible substitutes for the metaphorical term, the
system filters out the terms whose meanings do not share any common properties
with that of the metaphorical term. Consider the computer science metaphor ?kill a
process,? which stands for ?terminate a process.? The basic sense of kill implies an end
333
Computational Linguistics Volume 39, Number 2
Table 7
The list of paraphrases with the initial ranking (correct paraphrases are underlined).
Log-likelihood Replacement
Verb?DirectObject
hold back truth:
?13.09 contain
?14.15 conceal
?14.62 suppress
?15.13 hold
?16.23 keep
?16.24 defend
stir excitement:
?14.28 create
?14.84 provoke
?15.53 make
?15.53 elicit
?15.53 arouse
?16.23 stimulate
?16.23 raise
?16.23 excite
?16.23 conjure
leak report:
?11.78 reveal
?12.59 issue
?13.18 disclose
?13.28 emerge
?14.84 expose
?16.23 discover
Subject?Verb
campaign surge:
?13.01 run
?15.53 improve
?16.23 soar
?16.23 lift
or termination of life. Thus termination is the shared element of the metaphorical verb
and its literal interpretation.
Such an overlap of properties can be identified using the hyponymy relations in the
WordNet taxonomy. Within the initial list of paraphrases, the system selects the terms
that are hypernyms of the metaphorical term, or share a common hypernym with it. To
maximize the accuracy, we restrict the hypernym search to a depth of three levels in the
taxomomy. Table 7 shows the filtered lists of paraphrases for some of the test phrases,
together with their log-likelihood. Selecting the highest ranked paraphrase from this list
as a literal interpretation will serve as a baseline.
4.3.3 Re-ranking Based on Selectional Preferences. The lists which were generated contain
some irrelevant paraphrases (e.g., ?contain the truth? for ?hold back the truth?) and
some paraphrases where the substitute itself is metaphorically used (e.g., ?suppress the
334
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
truth?). As the task is to identify the literal interpretation, however, the system should
remove these.
One way of dealing with both problems simultaneously is to use selectional prefer-
ences of the verbs. Verbs used metaphorically are likely to demonstrate semantic pref-
erence for the source domain, e.g., suppress would select for MOVEMENTS (political)
rather than IDEAS, or TRUTH (the target domain), whereas the ones used literally for
the target domain (e.g., conceal) would select for TRUTH. Selecting the verbs whose
preferences the noun in the metaphorical expression matches best should allow filtering
out non-literalness, as well as unrelated terms.
We automatically acquired selectional preference distributions of the verbs in the
paraphrase lists (for verb?subject and verb?direct object relations) from the RASP-
parsed BNC. As in the identification experiment, we derived selectional preference
classes by clustering the 2,000 most frequent nouns in the BNC into 200 clusters us-
ing Sun and Korhonen?s (2009) algorithm. In order to quantify how well a particular
argument class fits the verb, we adopted the selectional association measure proposed
by Resnik (1993), identical to the one we used within the selectional preference-based
filter for metaphor identification, as described in Section 3.2.4. To remind the reader,
selectional association is defined as follows:
AR(v, c) =
1
SR(v)
P(c|v) log
P(c|v)
P(c)
(11)
where P(c) is the prior probability of the noun class, P(c|v) is the posterior probability
of the noun class given the verb, and SR is the overall selectional preference strength of
the verb in the grammatical relation R.
We use selectional association as a measure of semantic fitness (i.e., literalness) of
the paraphrases. The paraphrases are re-ranked based on their selectional association
with the noun in the context. Those paraphrases that are not well suited or used meta-
phorically are dispreferred within this ranking. The new ranking is shown in Table 8.
The expectation is that the paraphrase in the first rank (i.e., the verb with which the
noun in the context has the highest association) represents a literal interpretation.
4.4 Evaluation and Discussion
As in the case of identification, the paraphrasing system was tested on verb?subject and
verb?direct object metaphorical expressions. These were extracted from the manually
annotated metaphor corpus of Shutova and Teufel (2010), as described in Section 3.1.1.
We compared the output of the final selectional-preference based system to that of the
WordNet filter acting as a baseline. We evaluated the quality of paraphrasing with the
help of human judges in two different experimental settings. The first setting involved
direct judgments of system output by humans. In the second setting, the subjects did
not have access to system output and had to provide their own literal paraphrases for
the metaphorical expressions in the data set. The system was then evaluated against
human judgments in Setting 1 and a paraphrasing gold standard created by merging
annotations in Setting 2.
4.4.1 Setting 1: Direct Judgment of System Output. The subjects were presented with a
set of sentences containing metaphorical expressions and the top-ranked paraphrases
produced by the system and by the baseline, randomized. They were asked to mark as
335
Computational Linguistics Volume 39, Number 2
Table 8
Paraphrases re-ranked by SP model (correct paraphrases are underlined).
Association Replacement
Verb?DirectObject
hold back truth:
0.1161 conceal
0.0214 keep
0.0070 suppress
0.0022 contain
0.0018 defend
0.0006 hold
stir excitement:
0.0696 provoke
0.0245 elicit
0.0194 arouse
0.0061 conjure
0.0028 create
0.0001 stimulate
? 0 raise
? 0 make
? 0 excite
leak report:
0.1492 disclose
0.1463 discover
0.0674 reveal
0.0597 issue
? 0 emerge
? 0 expose
Subject?Verb
campaign surge:
0.0086 improve
0.0009 run
? 0 soar
? 0 lift
correct the paraphrases that have the same meaning as the term used metaphorically if
they are used literally in the given context.
Subjects Seven volunteers participated in the experiment. They were all native
speakers of English (one bilingual) and had little or no linguistics expertise.
Interannotator agreement The reliability was measured at ? = 0.62 (n = 2,
N = 95, k = 7).
System evaluation against judgments We then evaluated the system performance
against the subjects? judgments in terms of Precision at Rank 1, P(1). Precision at Rank
(1) measures the proportion of correct literal interpretations among the paraphrases
in rank 1. The results are shown in Table 9. The system identifies literal paraphrases
with a P(1) = 0.81 and the baseline with a P(1) = 0.55. We then conducted a one-tailed
Sign test (Siegel and Castellan 1988) that showed that this difference in performance is
statistically significant (N = 15, x = 1, p < 0.001).
336
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 9
System and baseline P(1) and MAP.
Relation System P(1) Baseline P(1) System MAP Baseline MAP
Verb?DirectObject 0.79 0.52 0.60 0.54
Verb?Subject 0.83 0.57 0.66 0.57
Average 0.81 0.55 0.62 0.56
4.4.2 Setting 2: Creation of a Paraphrasing Gold Standard. The subjects were presented with
a set of sentences containing metaphorical expressions and asked to write down all suit-
able literal paraphrases for the highlighted metaphorical verbs that they could think of.
Subjects Five volunteer subjects who were different from the ones used in the pre-
vious setting participated in this experiment. They were all native speakers of English
and some of them had a linguistics background (postgraduate-level degree in English).
Gold Standard The elicited paraphrases combined together can be interpreted
as a gold standard. For instance, the gold standard for the phrase ?brushed aside the
accusations? consists of the verbs rejected, ignored, disregarded, dismissed, overlooked, and
discarded.
System evaluation by gold standard comparison The system output was com-
pared against the gold standard using mean average precision (MAP) as a measure.
MAP is defined as follows:
MAP = 1
M
M
?
j=1
1
Nj
Nj
?
i=1
Pji (12)
where M is the number of metaphorical expressions, Nj is the number of correct para-
phrases for the metaphorical expression j, Pji is the precision at each correct paraphrase
(the number of correct paraphrases among the top i ranks). First, average precision
is estimated for individual metaphorical expressions, and then the mean is computed
across the data set. This measure allows one to assess ranking quality beyond rank 1,
as well as the recall of the system. As compared with the gold standard, MAP of the
paraphrasing system is 0.62 and that of the baseline is 0.56, as shown in Table 9.
4.4.3 Discussion. Given that the metaphor paraphrasing task is open-ended, any gold
standard elicited on the basis of it cannot be exhaustive. Some of the correct paraphrases
may not occur to subjects during the experiment. As an example, for the phrase ?stir
excitement? most subjects suggested only one paraphrase ?create excitement,? which is
found in rank 3, suggesting an average precision of 0.33 for this phrase. The top ranks of
the system output are occupied by provoke and stimulate, however, which are intuitively
correct, more precise paraphrases, despite none of the subjects having thought of them.
Such examples contribute to the fact that the system?s MAP is significantly lower than
its precision at rank 1, because a number of correct paraphrases proposed by the system
are not included in the gold standard.
The selectional preference-based re-ranking yields a considerable improvement in
precision at rank 1 (26%) over the baseline. This component is also responsible for some
errors of the system, however. One of the potential limitations of selectional preference-
based approaches to metaphor paraphrasing is the presence of verbs exhibiting weak
337
Computational Linguistics Volume 39, Number 2
selectional preferences. This means that these verbs are not strongly associated with
any of their argument classes. As noted in Section 3, such verbs tend to be used
literally, and are therefore suitable paraphrases. Our selectional preference model de-
emphasizes them, however, and, as a result, they are not selected as literal paraphrases
despite matching the context. This type of error is exemplified by the phrase ?mend
marriage.? For this phrase, the system ranking overruns the correct top suggestion
of the baseline, ?improve marriage,? and outputs ?repair marriage? as the most likely
literal interpretation, although it is in fact a metaphorical use. This is likely to be due to
the fact that improve exposes a moderate selectional preference strength.
Table 10 provides frequencies of the common errors of the system by type. The
most common type of error is triggered by the conventionality of certain metaphorical
verbs. Because they frequently co-occur with the target noun class in the corpus, they
receive a high association score with that noun class. This results in a high ranking
of conventional metaphorical paraphrases. Examples of top-ranked metaphorical para-
phrases include ?confront a question? for ?tackle a question,? ?repairmarriage? for ?mend
marriage,? ?example pictures? for ?example illustrates.?
These errors concern non-literalness of the produced paraphrases. A less frequently
occurring error was paraphrasing with a verb that has a different meaning. One such
example was the metaphorical expression ?tensionmounted,? for which the system pro-
duced a paraphrase ?tension lifted,? which has the opposite meaning. This error is likely
to have been triggered by the WordNet filter, whereby one of the senses of lift would
have a common hypernym with the metaphorical verb mount. This results in lift not
being discarded by the filter, and subsequently ranked top due to the conventionality of
the expression ?tension lifted.?
Another important issue that the paraphrase analysis brought to the foreground
is the influence of wider context on metaphorical interpretation. The current system
processes only the information contained within the GR of interest, discarding the
rest of the context. For some cases, however, this is not sufficient and the analysis
of a wider context is necessary. For instance, given the phrase ?scientists focus? the
system produces a paraphrase ?scientists think,? rather than the more likely paraphrase
?scientists study.? Such ambiguity of focus could potentially be resolved by taking its
wider context into account. The context-based paraphrase ranking model described in
Section 4.3.1 allows for the incorporation of multiple relations of the metaphorical verb
in the sentence.
Although the paraphrasing system uses hand-coded lexical knowledge from
WordNet, it is important to note that metaphor paraphrasing is not restricted to
metaphorical senses included in WordNet. Even if a metaphorical sense is absent from
WordNet, the system can still identify its correct literal paraphrase relying on the
Table 10
Common system errors by type.
Source of error Subject?Verb Verb?Object Totals
Metaphor conventionality 0 5 5
General polysemy/WordNet filter 1 1 2
SP re-ranking 0 1 1
Lack of context 1 1 2
Totals 2 8 10
338
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
hyponymy relation and similarity between concepts, as described in Section 4.3.2. For
example, the metaphorical sense of handcuff in ?research is handcuffed? is not included
in Wordnet, although the system correctly identifies its paraphrase confine (?research is
confined?).
5. Evaluation of Integrated System
Up to now, the identification and the paraphrasing systemswere evaluated individually
as modules. To determine to which extent the presented systems are applicable within
NLP, we then ran the two systems together in a pipeline and evaluated the accuracy of
the resulting text-to-text metaphor processing. First, the metaphor identification system
was applied to naturally occurring text taken from the BNC and then the metaphorical
expressions identified in those texts were paraphrased by the paraphrasing system.
Some of the expressions identified and paraphrased by the integrated system are shown
in Figure 11. The system output was compared against human judgments in two
phases. In phase 1, a small sample of sentences containing metaphors identified and
paraphrased by the system was judged by multiple judges. In phase 2, a larger sample
of phrases was judged by only one judge (one of the authors of this article). Agreement
of the judgments of the latter with the other judges was measured on the data from
phase 1.
Because our goal was to evaluate both the accuracy of the integrated system and
its usability by other NLP tasks, we assessed its performance in a two-fold fashion.
Instances where metaphors were both correctly identified and paraphrased by the
system were considered strictly correct, as they show that the system fully achieved
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then swallow his anger and play tennis.
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then suppress his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AD9 3205 He tried to hide the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AMA 349Wewill halt the reduction in NHS services for long-term care and community health services
which support elderly and disabled patients at home.
AMA 349 We will prevent the reduction in NHS services for long-term care and community health
services which support elderly and disabled patients at home.
J7F 77 An economist would frame this question in terms of a cost-benefit analysis: the maximization of
returns for the minimum amount of effort injected.
J7F 77 An economist would phrase this question in terms of a cost-benefit analysis: the maximization
of returns for the minimum amount of effort injected.
EEC 1362 In it, Younger stressed the need for additional alternatives to custodial sentences, which had
been implicit in the decision to ask the Council to undertake the enquiry.
EEC 1362 In it, Younger stressed the need for additional alternatives to custodial sentences, which had
been implicit in the decision to ask the Council to initiate the enquiry.
A1F 24 Moreover, Mr Kinnock brushed aside the suggestion that he needed a big idea or unique selling
point to challenge the appeal of Thatcherism.
A1F 24 Moreover, Mr Kinnock dismissed the suggestion that he needed a big idea or unique selling
point to challenge the appeal of Thatcherism.
Figure 11
Metaphors identified (first sentences) and paraphrased (second sentences) by the system.
339
Computational Linguistics Volume 39, Number 2
its goals. Instances where the paraphrasing retained the meaning and resulted in a
literal paraphrase (including the cases where the identification module tagged a literal
expression as a metaphor) were considered correct lenient. The intuition behind this
evaluation setting is that correct paraphrasing of literal expressions by other literal
expressions, albeit not demonstrating the positive contribution of metaphor processing,
does not lead to any errors in system output and thus does not hamper the overall
usability of the integrated system.
5.1 Phase 1: Small Sample, Multiple Judges
Three volunteer subjects participated in the experiment. They were all native speakers
of English and had no formal training in linguistics.
Materials and task Subjects were presented with a set of sentences containing
metaphorical expressions identified by the system and their paraphrases, as shown
in Figure 12. There were 35 such sentences in the sample. They were asked to do the
following:
1. Compare the sentences, decide whether the highlighted expressions have
the same meaning, and record this in the box provided;
2. Decide whether the verbs in both sentences are used metaphorically or
literally and tick the respective boxes.
For the second task, the same definition of metaphor as in the identification evaluation
(cf. Section 3.3.2) was provided for guidance.
Interannotator agreement The reliability of annotations was evaluated indepen-
dently for judgments on similarity of paraphrases and their literalness. The inter-
annotator agreement on the task of distinguishing metaphoricity from literalness was
measured at ? = 0.53 (n = 2,N = 70, k = 3). On the paraphrase (i.e., meaning retention)
task, reliability was measured at ? = 0.63 (n = 2,N = 35, k = 3).
System performance We then evaluated the integrated system performance
against the subjects? judgments in terms of accuracy (both strictly correct and correct
Figure 12
Evaluation of metaphor identification and paraphrasing.
340
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 11
Integrated system performance.
Tagging case Acceptability Percentage
Correct paraphrase: metaphorical? literal ? 53.8
Correct paraphrase: literal? literal ? 13.5
Correct paraphrase: literal?metaphorical ? 0.5
Correct paraphrase: metaphorical?metaphorical ? 10.7
Incorrect paraphrase ? 21.5
lenient). Strictly correct accuracy in this task measures the proportion of metaphors
both identified and paraphrased correctly in the given set of sentences. Correct lenient
accuracy, which demonstrates applicability of the system, is represented by the overall
proportion of paraphrases that retained their meaning and resulted in a literal para-
phrase (i.e., including literal paraphrasing of literal expressions in original sentences).
Human judgments were merged into a majority gold standard, which consists of those
instances that were considered correct (i.e., identified metaphor correctly paraphrased
by the system) by at least two judges. Compared to this majority gold standard, the
integrated system operates with a strictly correct accuracy of 0.66 and correct lenient
accuracy of 0.71. The average human agreement with the majority gold standard in
terms of accuracy is 0.80 on the literalness judgments and 0.89 on the meaning retention
judgments.
5.2 Phase 2: Larger Sample, One Judge
The systemwas also evaluated on a larger sample of automatically annotatedmetaphor-
ical expressions (600 sentences) using one person?s judgments produced following
the procedure from phase 1. We measured how far these judgments agree with the
judges used in phase 1. The agreement on meaning retention was measured at ? = 0.59
(n = 2,N = 35, k = 4) and that on the literalness of paraphrases at ? = 0.54 (n = 2,N =
70, k = 4).
On this larger data set, the system achieved an accuracy of 0.54 (strictly correct) and
0.67 (correct lenient). The proportions of different tagging cases are shown in Table 11.
The table also shows the acceptability of tagging cases. Acceptability indicates whether
or not this type of system paraphrasing would cause an error when hypothetically
integrated with an external NLP application. Cases where the system produces correct
literal paraphrases for metaphorical expressions identified in the text would benefit
another NLP application, whereas cases where literal expressions are correctly para-
phrased by other literal expressions are considered neutral. Both such cases are deemed
acceptable, because they increase or preserve literalness of the text. All other tagging
cases introduce errors, thus they are marked as unacceptable. Examples of different
tagging cases are shown in Table 12.
The accuracy of metaphor-to-literal paraphrasing (0.54) indicates the level of in-
formative contribution of the system, and the overall accuracy of correct paraphrasing
resulting in a literal expression (0.67) represents the level of its acceptability within NLP.
5.3 Discussion and Error Analysis
The results of integrated system evaluation suggest that the system is capable of pro-
viding useful information about metaphor for an external text processing application
341
Computational Linguistics Volume 39, Number 2
Table 12
Examples of different tagging cases.
Tagging case Examples
Correct paraphrase: met? lit throw an idea? express an idea
Correct paraphrase: lit? lit adopt a recommendation? accept a recommendation
Correct paraphrase: lit?met arouse memory? awakenmemory
Correct paraphrase: met?met work killed him?work exhausted him
with a reasonable accuracy (0.67). It may, however, also introduce errors in the text by
incorrect paraphrasing, as well as by producing metaphorical paraphrases. If the latter
errors are rare (0.5%), the errors of the former type are sufficiently frequent (21.5%) to
make the metaphor system less desirable for use in NLP. It is therefore important to
address such errors.
Table 13 shows the contribution of the individual system components to the overall
error. The identification system tags 28% of all instances incorrectly (170). This yields a
component performance of 72%. This result is slightly lower than that obtained in its
individual evaluation in a setting with multiple judges (79%). This can be explained
by the fact that the integrated system was evaluated by one judge only, rather than
using a majority gold standard. When compared with the judgments of each annotator
pairwise the system precision was measured at 74% (cf. Section 3.3.2). Some of the
literal instances tagged as ametaphor by the identification component are then correctly
paraphrased with a literal expression by the paraphrasing component. Such cases do
not change the meaning of the text, and hence are considered acceptable. The resulting
contribution of the identification component to the overall error of the integrated system
is thus 15%.
As Table 13 shows, the paraphrasing component failed in 32% of all cases
(196 instances out of 600 were paraphrased incorrectly). As mentioned previously, this
error can be further split into paraphrasing without meaning retention (21.5%) and
metaphorical paraphrasing (11%). Both of these error types are unacceptable and lead
to lower performance of the integrated system. This error rate is also higher than that
of the paraphrasing system when evaluated individually on a manually created data
set (19%). The reasons for incorrect paraphrasing by the integrated system are manifold
Table 13
System errors by component. Three categories are cases where the identification model
incorrectly tagged a literal expression as metaphoric (false negatives from this module
were not measured). The remaining two categories are for paraphrase errors on correctly
identified metaphors.
Type of error Identification Paraphrasing
Correct paraphrase: lit? lit 81 0
Correct paraphrase: lit?met 3 3
Correct paraphrase: met?met ? 64
Incorrect paraphrase for literal 86 86
Incorrect paraphrase for metaphor ? 43
Totals 170 196
342
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
and concern both themetaphor identification and paraphrasing components. One of the
central problems stems from the initial tagging of literal expressions as metaphorical by
the identification system. The paraphrasing system is not designed with literal-to-literal
paraphrasing in mind. When it receives literal expressions which have been incorrectly
identified as input, it searches for a more literal paraphrase for them. Not all literally
used words have suitable substitutes in the given context, however. For instance,
the literal expression ?approve conclusion? is incorrectly paraphrased as ?evaluate
conclusion.?
Similar errors occur when metaphorical expressions do not have any single-word
literal paraphrases, for example, ?country functions according to...?. This is, however,
a more fundamental problem for metaphor paraphrasing as a task. In such cases, the
system, nonetheless, attempts to produce a substitute with approximately the same
meaning, which often leads to either metaphorical or incorrect paraphrasing. For in-
stance, ?country functions? is paraphrased by ?country runs,? with suggestions with
lower rank being ?country works? and ?country operates.?
Some errors that occur at the paraphrasing level are also due to the general
word sense ambiguity of certain verbs or nouns. Consider the following paraphras-
ing example, where Example (24a) shows an automatically identified metaphor and
Example (24b) its system-derived paraphrase:
(24) a. B71 852 Craig Packer and Anne Pusey of the University of Chicago have
continued to follow the life and loves of these Tanzanian lions.
b. B71 852 Craig Packer and Anne Pusey of the University of Chicago have
continued to succeed the life and loves of these Tanzanian lions.
This error results from the fact that the verb succeed has a high selectional preference for
life in one of its senses (?attain success or reach a desired goal?) and is similar to follow
in WordNet in another of its senses (?be the successor [of]?). The system merges these
two senses in one, resulting in an incorrect paraphrase.
One automatically identified example exhibited interaction of metaphor with
metonymy at the interpretation level. In the phrase ?break word,? the verb break is used
metaphorically (although conventionally) and the noun word is a metonym standing
for promise. This affected paraphrasing in that the system searched for verbs denoting
actions that could be done with words, rather than promises, and suggested the para-
phrase ?interrupt word(s).? This paraphrase is interpretable in the context of a person
giving a speech, but not in the context of a person giving a promise. This was the only
case of metonymy in the analyzed data, however.
Another issue that the evaluation on a larger data set revealed is the limitations of
the WordNet filter used in the paraphrasing system. Despite being a wide-coverage
general-domain database, WordNet does not include information about all possible
relations that exist between particular word senses. This means that some of the correct
paraphrases suggested by the context-based model get discarded by the WordNet filter
due to missing information in WordNet. For instance, the system produces no para-
phrase for the metaphors ?hurl comment,? ?spark enthusiasm,? and ?magnify thought?
that it correctly identified. This problemmotivates the exploration of possibleWordNet-
free solutions for similarity detection in the metaphor paraphrasing task. The system
could either rely entirely on such a solution, or back off to it in cases when theWordNet-
based system fails.
Table 14 provides a summary of system errors by type. The most common errors
are caused by metaphor conventionality resulting in metaphorical paraphrasing (e.g.,
343
Computational Linguistics Volume 39, Number 2
Table 14
Errors of the paraphrasing component by type.
Source of error Met?Met Lit?Met Incorr. for Lit Incorr. for Met Total
No literal paraphrase exists 11 0 5 2 18
Metaphor conventionality 53 3 0 0 56
General polysemy 0 0 13 10 23
WordNet filter 0 0 21 21 42
SP re-ranking 0 0 41 7 48
Lack of context 0 0 6 2 8
Interaction with metonymy 0 0 0 1 1
Totals 64 3 86 43 196
?swallow anger? suppress anger,? ?work killed him? work exhausted him?), followed
by the WordNet filter? and general polysemy?related errors (e.g. ?follow lives ?
succeed lives?), resulting in incorrect paraphrasing or the system not producing any
paraphrase at all. Metaphor paraphrasing by another conventional metaphor instead
of a literal expression is undesirable, although it may still be useful if the paraphrases
are more lexicalized than the original expression. The word sense ambiguity? and
WordNet-based errors are more problematic, however, and need to be addressed in the
future. SP re-ranking is responsible for the majority of incorrect paraphrasing of literal
expressions. This may be due to the fact that the model is ignorant of the meaning
retention aspect, but rather favors the paraphrases that are used literally (albeit
incorrectly) in the given context. This shows that when building an integrated system,
it is necessary to adapt the metaphor paraphrasing module to be able to also handle
literal expressions, because the identification module is likely to produce at least some
of them.
5.4 Comparison to the CorMet System
It is hard to directly compare the performance of the presented system to the other
recent approaches to metaphor, because all of these approaches assume different task
definitions, and hence use data sets and evaluation techniques of their own. Among the
data-driven methods, however, the closest in nature to ours is Mason?s (2004) CorMet
system. Mason?s system does not perform metaphor interpretation or identification of
metaphorical expressions in text, but rather focuses on the detection of metaphorical
links between distant domains. Our system also involves such detection. Whereas
Mason relies on domain-specific selectional preferences for this purpose, however, our
system uses information about verb subcategorization, as well as general selectional
preferences, to perform distributional clustering of verbs and nouns and then link the
clusters based on metaphorical seeds. Another fundamental difference is that whereas
CorMet assigns explicit domain labels, our system models source and target domains
implicitly. In the evaluation of the CorMet system, the acquired metaphorical mappings
are compared to those in the manually created Master Metaphor List demonstrating the
accuracy of 77%. In our system, on the contrary, metaphor acquisition is evaluated via
extraction of naturally occurring metaphorical expressions, achieving a performance of
79% in terms of precision. In order to compare the new mapping acquisition ability
344
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
of our system to that of CorMet, however, we performed an additional analysis of the
mappings hypothesized by our noun clusters in relation to those in the MML. It was not
possible to compare the new mappings discovered by our system to the MML directly
as was done in Mason?s experiments, because in our approach source domains are
represented by clusters of their characteristic verbs. The analysis of the noun clusters
with respect to expansion of the the seed mappings taken from the MML, however,
allowed us to evaluate the mapping acquisition by our system in terms of both precision
and recall. The goal was to confirm our hypothesis that abstract concepts get clustered
together if they are associated with the same source domain and to evaluate the quality
of the newly acquired mappings.
To do this, we randomly selected 10 target domain categories described in the
MML and manually extracted all corresponding mappings (42 mappings in total).
The categories included SOCIETY, IDEA, LIFE, OPPORTUNITY, CHANGE, LOVE,
DIFFICULTY, CREATION, RELATIONSHIP, and COMPETITION. For the concept of
OPPORTUNITY, for example, three mappings were present in the MML: OPPORTU-
NITIES ARE PHYSICAL OBJECTS, OPPORTUNITIES ARE MOVING ENTITIES, and
OPPORTUNITIES ARE OPEN PATHS, whereas for the concept of COMPETITION the
list describes only two mappings: COMPETITION IS A RACE, COMPETITION IS A
WAR.
We then extracted the system-produced clusters containing the selected 10 target
concepts. Examples of the mappings and the corresponding clusters are shown in
Figure 13. Our goal was to verify whether other concepts in the cluster containing the
target concept are associated with the source domains given in the mappings. Each
member of these clusters was analyzed for possible association with the respective
source domains. For each concept in a cluster, we verified that it is associated with
the respective source domain by finding a corresponding metaphorical expression and
annotating the concepts accordingly. The degree of association of the members of the
clusters with a given source domain was evaluated in terms of precision on the set
of hypothesized mappings. The precision of the cluster?s association with the source
Conceptual mapping: RELATIONSHIP IS A MECHANISM (VEHICLE)
Cluster: consensus relation tradition partnership resistance foundation alliance friendship con-
tact reserve unity link peace bond myth identity hierarchy relationship connection balance
marriage democracy defense faith empire distinction coalition regime division
Conceptual mapping: LIFE IS A JOURNEY
Cluster: politics practice trading reading occupation profession sport pursuit affair career
thinking life
Conceptual mapping: SOCIETY is a (HUMAN) BODY
Cluster: class population nation state country family generation trade profession household
kingdom business industry economy market enterprise world community institution society
sector
Conceptual mapping: DIFFICULTY is DIFFICULTY IN MOVING (OBSTACLE); PHYSICAL
HARDNESS
Cluster: threat crisis risk problem poverty obstacle dilemma challenge prospect danger dis-
crimination barrier difficulty shortage
Conceptual mapping: OPPORTUNITY is a PHYSICAL OBJECT; MOVING ENTITY; OPEN
PATH
Cluster: incentive attraction scope remedy chance choice solution option perspective range
possibility contrast opportunity selection alternative focus
Figure 13
Noun clusters.
345
Computational Linguistics Volume 39, Number 2
concept was calculated as a proportion of the associated concepts in it. Based on these
results we computed the average precision (AP) as follows:
AP = 1
M
M
?
j=1
#associated concepts in cluster cj
|cj|
(13)
whereM is the number of hypothesizedmappings and cj is the cluster of target concepts
corresponding to mapping j.
The annotation was carried out by one of the authors and its average precision
is 0.82. This confirms the hypothesis of clustering by association and shows that our
method favorably compares to Mason?s system. This is only an approximate compari-
son, however. Direct comparison of metaphor acquisition by the two systems was not
possible, as they produce the output in different formats and, as mentioned earlier, our
system models conceptual mappings implicitly, both within the noun clusters, as well
as by linking them to the verb clusters.
We then additionally evaluated the recall of mapping acquisition by our system
against the MML. For each selected MML mapping, we manually extracted all alter-
native target concepts associated with the source domain in the mapping from the
MML. For example, in case of LIFE IS A JOURNEY we identified all target concepts
associated with JOURNEY according to the MML and extracted them. These included
LIFE, CAREER, LOVE, and CHANGE. We then verified whether the relevant system-
produced noun clusters contained these concepts. The recall was then calculated as a
proportion of the concepts in this list within one cluster. For example, the concepts LIFE
and CAREER are found in the same cluster, but not LOVE and CHANGE. The overall
recall of mapping acquisition was measured at 0.50.
These results show that the system is able to discover a large number of metaphor-
ical connections in the data with high precision. Although the evaluation against the
Master Metaphor List is subjective, it suggests that the use of statistical data-driven
methods in general, and distributional clustering in particular, is a promising direction
for computational modeling of metaphor.
6. Conclusion and Future Directions
The 1980s and 1990s provided us with a wealth of ideas on the structure and mecha-
nisms of metaphor. The computational approaches formulated back then are still highly
influential, although their use of task-specific hand-coded knowledge is becoming in-
creasingly less popular. The last decade witnessed a significant technological leap in
natural language computation, whereby manually crafted rules gradually gave way
to more robust corpus-based statistical methods. This is also the case for metaphor
research. In this article, we presented the first integrated statistical system for metaphor
processing in unrestricted text. Our method is distinguished from previous work in that
it does not rely on anymetaphor-specific hand-coded knowledge (besides the seed set in
the identification experiments), operates on open-domain text, and produces interpreta-
tions in textual format. The system, consisting of independent metaphor identification
and paraphrasing modules, operates with a high precision (0.79 for identification, 0.81
for paraphrasing, and 0.67 as an integrated system). Although the system has been
tested only on verb?subject and verb?object metaphors at this stage, the described iden-
tification and paraphrasing methods should be similarly applicable to a wider range
of syntactic constructions. This expectation rests on the fact that both distributional
346
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
clustering and selectional preference induction techniques have been shown to model
the meanings of a range of word classes (Hatzivassiloglou and McKeown 1993; Boleda
Torrent and Alonso i Alemany 2003; Brockmann and Lapata 2003; Zapirain, Agirre,
and Ma`rquez 2009). Extending the system to deal with metaphors represented by other
word classes and constructions as well as multi-word metaphors is part of future work.
Such an extension of the identification system would require the creation of a
seed set exemplifying more syntactic constructions and the corpus search over fur-
ther grammatical relations (e.g., verb?prepositional phrase [PP] complement relations:
?Hillary leapt in the conversation;? adjectival modifier?noun relations ?slippery mind,
deep unease, heavy loss;? noun?PP complement relations: ?a fraction of self-control, a
foot of a mountain;? verb?VP complement relations: ?aching to begin the day;? and
copula constructions: ?Death is the sorry end of the human story, not a mysterious
prelude to a new one?). Besides noun and verb clustering, it would also be necessary
to perform clustering of adjectives and adverbs. Clusters of verbs, adjectives, adverbs,
and concrete nouns would then represent source domains within the model. The data
study of Shutova and Teufel (2010) suggested that it is sometimes difficult to choose the
optimal level of abstraction of domain categories that would generalize well over the
data. Although the system does not explicitly assign any domain labels, its domain
representation is still restricted by the fixed level of generality of source concepts,
defined by the chosen cluster granularity. To relax this constraint, one could attempt
to automatically optimize cluster granularity to fit the data more accurately and to
ensure that the generated clusters explain themetaphorical expressions in the data more
comprehensively. A hierarchical clustering algorithm, such as that of Yu, Yu, and Tresp
(2006) or Sun andKorhonen (2011), could be used for this purpose. Besides this, it would
be desirable to be able to generalize metaphorical associations learned from one type
of syntactic construction across all syntactic constructions, without providing explicit
seed examples for the latter. For instance, given the seed phrase ?stir excitement,?
representing the conceptual mapping FEELINGS ARE LIQUIDS, the system should be
able to discover not only that phrases such as ?swallow anger? are metaphorical, but
that phrases such as ?ocean of happiness? are as well.
The extension of the paraphrasing system to other syntactic constructions would
involve the extraction of further grammatical relations from the corpus, such as those
listed herein, and their incorporation into the context-based paraphrase selectionmodel.
Extending both the identification system and the paraphrasing system would require
the application of the selectional preference model to other word classes. Although
Resnik?s selectional association measure has been used to model selectional preferences
of verbs for their nominal arguments, it is in principle a generalizable measure of word
association. Information-theoretic word association measures (e.g., mutual information
[Church and Hanks 1990]) have been continuously successfully applied to a range of
syntactic constructions in a number of NLP tasks (Hoang, Kim, and Kan 2009; Baldwin
and Kim 2010). This suggests that applying a distributional association measure, such
as the one proposed by Resnik, to other part-of-speech classes should still result in a
realistic model of semantic fitness, which in our terms corresponds to a measure of
?literalness? of the paraphrases.
In addition, the selectional preference model can be improved by using an SP
acquisition algorithm that can handle word sense ambiguity (e.g., Rooth et al 1999;
O? Se?aghdha 2010; Reisinger and Mooney 2010). The current approach relies on SP
classes produced by hard clustering and fails to accurately model word senses of gener-
ally polysemous words. This resulted in a number of errors in metaphor paraphrasing
and it therefore needs to be addressed in the future.
347
Computational Linguistics Volume 39, Number 2
The current version of the metaphor paraphrasing system still relies on some hand-
coded knowledge in the form of WordNet. WordNet has been criticized for a lack of
consistency, high granularity of senses, and negligence with respect to some important
semantic relations (Lenat, Miller, and Yokoi 1995). In addition, WordNet is a general-
domain resource, which is less suitable if one wanted to apply the system to domain-
specific data. For all of these reasons it would be preferable to develop a WordNet-free
fully automated approach to metaphor resolution. Vector space models of word mean-
ing (Erk 2009; Rudolph and Giesbrecht 2010; Van de Cruys, Poibeau, and Korhonen
2011) might provide a solution, as they have proved efficient in general paraphrasing
and lexical substitution settings (Erk and Pado? 2009). The feature similarity component
of the paraphrasing system that is currently based on WordNet could be replaced by
such a model.
Another crucial problem that needs to be addressed is the coverage of the iden-
tification system. To enable high usability of the system it is necessary to perform
high-recall processing. One way to improve the coverage is the creation of a larger,
more diverse seed set. Although it is hardly possible to describe the whole variety of
metaphorical language, it is possible to compile a set representative of (1) all most com-
mon source?target domain mappings and (2) all types of syntactic constructions that
exhibit metaphoricity. The existing metaphor resources, primarily the Master Metaphor
List (Lakoff, Espenson, and Schwartz 1991), and examples from the linguistic literature
about metaphor, could be a sensible starting point on a route to such a data set. Having
a diverse seed set should enable the identification system to attain a broad coverage of
the corpus.
The proposed text-to-text representation of metaphor processing is directly trans-
ferable to other NLP tasks and applications that could benefit from the inclusion of
a metaphor processing component. Overall, our results suggest that the system can
provide useful and accurate information about metaphor to other NLP tasks relying
on lexical semantics. In order to prove its usefulness for external applications, however,
an extrinsic task-based evaluation is outstanding. In the future, we intend to integrate
metaphor processing with NLP applications, exemplified by MT and opinion mining,
in order to demonstrate the contribution of this pervasive yet rarely addressed phe-
nomenon to natural language semantics.
Acknowledgments
We would like to thank the volunteer
annotators for their help in the evaluations,
as well as the Cambridge Overseas Trust
(UK), EU FP-7 PANACEA project, and the
Royal Society (UK), who funded our work.
References
Abend, Omri and Ari Rappoport. 2010.
Fully unsupervised core-adjunct argument
classification. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 226?236,
Uppsala.
Agerri, Rodrigo, John Barnden, Mark Lee,
and Alan Wallington. 2007. Metaphor,
inference and domain-independent
mappings. In Proceedings of RANLP-2007,
pages 17?23, Borovets.
Alonge, Antonietta and Margherita Castelli.
2003. Encoding information on metaphoric
expressions in WordNet-like resources.
In Proceedings of the ACL 2003 Workshop
on Lexicon and Figurative Language,
pages 10?17, Sapporo.
Andersen, Oistein, Julien Nioche, Ted
Briscoe, and John Carroll. 2008. The
BNC parsed with RASP4UIMA. In
Proceedings of LREC 2008, pages 865?869,
Marrakech.
Baldwin, Timothy and Su Nam Kim. 2010.
Multiword expressions. In N. Indurkhya
and F. J. Damerau, editors, Handbook of
Natural Language Processing, Second Edition.
CRC Press, Taylor and Francis Group,
Boca Raton, FL, pages 267?292.
348
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Barnden, John and Mark Lee. 2002. An
artificial intelligence approach to
metaphor understanding. Theoria et
Historia Scientiarum, 6(1):399?412.
Barque, Lucie and Franc?ois-Re?gis
Chaumartin. 2009. LDV Forum, 24(2):5?18.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: an unsupervised
approach using multiple-sequence
alignment. In Proceedings of the 2003
Conference of the North American Chapter of
the Association for Computational Linguistics
on Human Language Technology - Volume 1,
NAACL ?03, pages 16?23, Edmonton.
Barzilay, Regina and Kathryn McKeown.
2001. Extracting paraphrases from a
parallel corpus. In Proceedings of the
39th Annual Meeting on Association for
Computational Linguistics, ACL ?01,
pages 50?57, Toulouse.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
EMNLP ?08, pages 59?68, Honolulu, HI.
Birke, Julia and Anoop Sarkar. 2006.
A clustering approach for the nearly
unsupervised recognition of nonliteral
language. In Proceedings of EACL-06,
pages 329?336, Trento.
Black, Max. 1962.Models and Metaphors.
Cornell University Press, Ithaca, NY.
Boleda Torrent, Gemma and Laura Alonso i
Alemany. 2003. Clustering adjectives for
class acquisition. In Proceedings of the Tenth
Conference of the European Chapter of the
Association for Computational Linguistics -
Volume 2, EACL ?03, pages 9?16, Budapest.
Bolshakov, Igor and Alexander Gelbukh.
2004. Synonymous paraphrasing using
Wordnet and Internet. In Proceedings of the
9th International Conference on Applications
of Natural Language to Information Systems,
NLDB 2004, pages 312?323, Alicante.
Brew, Chris and Sabine Schulte im Walde.
2002. Spectral clustering for German verbs.
In Proceedings of EMNLP, pages 117?124,
Philadelphia, PA.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release of
the RASP system. In Proceedings of the
COLING/ACL on Interactive Presentation
Sessions, pages 77?80, Sydney.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the Tenth
Conference of the European Chapter of the
Association for Computational Linguistics -
Volume 1, EACL ?03, pages 27?34,
Budapest.
Burnard, Lou. 2007. Reference Guide for the
British National Corpus (XML Edition).
Available at http://www.natcorp.
ox.ac.uk/docs/URG.
Callison-Burch, Chris, Philipp Koehn, and
Miles Osborne. 2006. Improved statistical
machine translation using paraphrases. In
Proceedings of NAACL, HLT-NAACL ?06,
pages 17?24, New York, NY.
Cameron, Lynne. 2003.Metaphor in
Educational Discourse. Continuum, London.
Carroll, John, Guido Minnen, Darren Pearce,
Yvonne Canning, Siobhan Devlin, and
John Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings
of the 9th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL), pages 269?270, Bergen.
Chen, Jinxiu, Donghong Ji, Chew Lim Tan,
and Zhengyu Niu. 2006. Unsupervised
relation disambiguation using spectral
clustering. In Proceedings of the
COLING/ACL, pages 89?96, Sydney.
Church, Kenneth and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Clark, Stephen and James Curran. 2007.
Wide-coverage efficient statistical
parsing with CCG and log-linear models.
Computational Linguistics, 33(4):493?552.
Copestake, Ann and Ted Briscoe. 1995.
Semi-productive polysemy and sense
extension. Journal of Semantics, 12:15?67.
Davidov, Dmitry, Roi Reichart, and Ari
Rappoport. 2009. Superior and efficient
fully unsupervised pattern-based concept
acquisition using an unsupervised parser.
In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning,
CoNLL ?09, pages 48?56, Boulder, CO.
De Cao, Diego and Roberto Basili.
2009. Combining distributional and
paradigmatic information in a lexical
substitution task. In Proceedings of
EVALITA Workshop, 11th Congress of
Italian Association for Artificial Intelligence,
Reggie Emilia.
Dras, Mark. 1999. Tree Adjoining Grammar
and the Reluctant Paraphrasing of Text. Ph.D.
thesis, Macquarie University, Australia.
Erk, Katrin. 2009. Representing words as
regions in vector space. In Proceedings of
the Thirteenth Conference on Computational
Natural Language Learning, pages 57?65,
Boulder, CO.
349
Computational Linguistics Volume 39, Number 2
Erk, Katrin and Diana McCarthy. 2009.
Graded word sense assignment. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 440?449, Edinburgh.
Erk, Katrin and Sebastian Pado?. 2009.
Paraphrase assessment in structured
vector space: exploring parameters and
datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language
Semantics, pages 57?65, Athens.
Fass, Dan. 1991. met*: A method for
discriminating metonymy and metaphor
by computer. Computational Linguistics,
17(1):49?90.
Fass, Dan and Yorick Wilks. 1983. Preference
semantics, ill-formedness, and metaphor.
Computational Linguistics, 9(3-4):178?187.
Fauconnier, Gilles and Mark Turner. 2002.
The Way We Think: Conceptual Blending and
the Mind?s Hidden Complexities. Basic Books,
New York, NY.
Feldman, Jerome. 2006. From Molecule to
Metaphor: A Neural Theory of Language.
The MIT Press, Cambridge, MA.
Feldman, Jerome and Srini Narayanan.
2004. Embodied meaning in a neural
theory of language. Brain and Language,
89(2):385?392.
Fellbaum, Christiane, editor. 1998.
WordNet: An Electronic Lexical Database
(ISBN: 0-262-06197-X). MIT Press,
Cambridge, MA.
Fillmore, Charles, Christopher Johnson,
and Miriam Petruck. 2003. Background
to FrameNet. International Journal of
Lexicography, 16(3):235?250.
Gedigian, Matt, John Bryant, Srini
Narayanan, and Branimir Ciric. 2006.
Catching metaphors. In Proceedings
of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48,
New York, NY.
Gentner, Dedre. 1983. Structure mapping:
A theoretical framework for analogy.
Cognitive Science, 7:155?170.
Gentner, Dedre, Brian Bowdle, Phillip Wolff,
and Consuelo Boronat. 2001. Metaphor is
like analogy. In D. Gentner, K. J. Holyoak,
and B. N. Kokinov, editors, The Analogical
Mind: Perspectives from Cognitive Science.
MIT Press, Cambridge, MA,
pages 199?253.
Goatly, Andrew. 1997. The Language of
Metaphors. Routledge, London.
Grady, Joe. 1997. Foundations of Meaning:
Primary Metaphors and Primary Scenes.
Ph.D. thesis, University of California
at Berkeley.
Hanks, Patrick and James Pustejovsky. 2005.
A pattern dictionary for natural language
processing. Revue Franc?aise de linguistique
applique?e, 10(2):63?82.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1993. Towards the automatic
identification of adjectival scales:
Clustering adjectives according to
meaning. In Proceedings of the 31st Annual
Meeting of the Association for Computational
Linguistics, ACL ?93, pages 172?182,
Columbus, OH.
Hesse, Mary. 1966.Models and Analogies in
Science. Notre Dame University Press,
Notre Dame, IN.
Hoang, Hung Huu, Su Nam Kim, and
Min-Yen Kan. 2009. A re-examination of
lexical association measures. In Proceedings
of the Workshop on Multiword Expressions,
pages 31?39, Singapore.
Hofstadter, Douglas. 1995. Fluid Concepts and
Creative Analogies: Computer Models of the
Fundamental Mechanisms of Thought.
HarperCollins Publishers, London.
Hofstadter, Douglas and Melanie Mitchell.
1994. The Copycat Project: A model of
mental fluidity and analogy-making. In
K. J. Holyoak and J. A. Barnden, editors,
Advances in Connectionist and Neural
Computation Theory. Ablex, New York, NY.
Karov, Yael and Shimon Edelman.
1998. Similarity-based word sense
disambiguation. Computational
Linguistics, 24(1):41?59.
Kauchak, David and Regina Barzilay. 2006.
Paraphrasing for automatic evaluation. In
Proceedings of the Main Conference on Human
Language Technology, Conference of the North
American Chapter of the Association of
Computational Linguistics, HLT-NAACL
?06, pages 455?462, New York, NY.
Kingsbury, Paul and Martha Palmer. 2002.
From TreeBank to PropBank. In Proceedings
of LREC-2002, pages 1989?1993,
Gran Canaria, Canary Islands.
Kipper, Karin, Anna Korhonen, Neville
Ryant, and Martha Palmer. 2006.
Extensive classifications of English verbs.
In Proceedings of the 12th EURALEX
International Congress, pages 1?15, Torino.
Klein, Dan and Christopher Manning.
2003. Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics, pages 423?430, Sapporo.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?step one:
Sentence compression. In Proceedings of the
Seventeenth National Conference on Artificial
350
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Intelligence and Twelfth Conference on
Innovative Applications of Artificial
Intelligence, pages 703?710, Austin, TX.
Kok, Stanley and Chris Brockett. 2010.
Hitting the right paraphrases in good time.
In Human Language Technologies: The 2010
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, HLT ?10, pages 145?153,
Los Angeles, CA.
Korhonen, Anna, Yuval Krymolowski,
and Ted Briscoe. 2006. A large
subcategorization lexicon for natural
language processing applications.
In Proceedings of LREC 2006,
pages 1015?1020, Genoa.
Kozlowski, Raymond, Kathleen F.
McCoy, and K. Vijay-Shanker. 2003.
Generation of single-sentence
paraphrases from predicate/argument
structure using lexico-grammatical
resources. In Proceedings of the Second
International Workshop on Paraphrasing -
Volume 16, PARAPHRASE ?03,
pages 1?8, Sapporo.
Krishnakumaran, Saisuresh and Xiaojin Zhu.
2007. Hunting elusive metaphors using
lexical resources. In Proceedings of the
Workshop on Computational Approaches to
Figurative Language, pages 13?20,
Rochester, NY.
Kurohashi, Sadao. 2001. SENSEVAL-2
Japanese translation task. In Proceedings of
the SENSEVAL-2 Workshop, pages 37?44,
Toulouse.
Lakoff, George, Jane Espenson, and Alan
Schwartz. 1991. The master metaphor list.
Technical report, University of California
at Berkeley.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Lapata, Mirella. 2001. The Acquisition
and Modeling of Lexical Knowledge: A
Corpus-Based Investigation of Systematic
Polysemy. Ph.D. thesis, University of
Edinburgh.
Lenat, Doug, George Miller, and Toshio
Yokoi. 1995. CYC, WordNet, and EDR:
Critiques and responses. Commun.
ACM, 38(11):45?48.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago, IL.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistics, pages 768?774,
Montreal.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7:343?360.
Lo?nneker, Birte. 2004. Lexical databases as
resources for linguistic creativity: Focus on
metaphor. In Proceedings of the LREC 2004
Workshop on Language Resources for
Linguistic Creativity, pages 9?16, Lisbon.
Lo?nneker, Birte and Carina Eilts. 2004. A
current resource and future perspectives
for enriching Wordnets with metaphor
information. In Proceedings of the Second
International WordNet Conference?GWC
2004, pages 157?162, Brno.
Martin, James. 1988. Representing
regularities in the metaphoric lexicon.
In Proceedings of the 12th Conference on
Computational Linguistics, pages 396?401,
Budapest.
Martin, James. 1990. A Computational Model of
Metaphor Interpretation. Academic Press
Professional, Inc., San Diego, CA.
Martin, James. 1994. Metabank: A
knowledge-base of metaphoric language
conventions. Computational Intelligence,
10:134?149.
Martin, James. 2006. A corpus-based
analysis of context effects on metaphor
comprehension. In A. Stefanowitsch and
S. T. Gries, editors, Corpus-Based Approaches
to Metaphor and Metonymy. Mouton de
Gruyter, Berlin, pages 214?236.
Mason, Zachary. 2004. Cormet: A
computational, corpus-based conventional
metaphor extraction system. Computational
Linguistics, 30(1):23?44.
McCarthy, Diana. 2002. Lexical
substitution as a task for WSD evaluation.
In Proceedings of the ACL-02 Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions - Volume 8, WSD ?02,
pages 109?115, Philadelphia, PA.
McCarthy, Diana, Bill Keller, and
Roberto Navigli. 2010. Getting synonym
candidates from raw data in the English
lexical substitution task. In Proceedings of
the 14th EURALEX International Congress,
Leeuwarden.
McCarthy, Diana and Roberto Navigli.
2007. Semeval-2007 task 10: English
lexical substitution task. In Proceedings
of the 4th Workshop on Semantic
Evaluations (SemEval-2007), pages 48?53,
Prague.
McCarthy, Diana and Roberto Navigli. 2009.
The English lexical substitution task.
Language Resources and Evaluation,
43(2):139?159.
351
Computational Linguistics Volume 39, Number 2
McKeown, Kathleen. 1979. Paraphrasing
using given and new information in a
question-answer system. In Proceedings
of the 17th Annual Meeting of the Association
for Computational Linguistics, ACL ?79,
pages 67?72, La Jolla, CA.
Meila, Marina and Jianbo Shi. 2001.
A random walks view of spectral
segmentation. In AISTATS, Key West, FL.
Meteer, Marie and Varda Shaked. 1988.
Strategies for effective paraphrasing.
In Proceedings of the 12th Conference on
Computational Linguistics - Volume 2,
COLING ?88, pages 431?436, Budapest.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL,
pages 236?244, Columbus, OH.
Murphy, Gregory. 1996. On metaphoric
representation. Cognition, 60:173?204.
Narayanan, Srini. 1997. Knowledge-based
Action Representations for Metaphor and
Aspect (KARMA). Ph.D. thesis, University
of California at Berkeley.
Narayanan, Srini. 1999. Moving right along:
A computational model of metaphoric
reasoning about events. In Proceedings of
AAAI 99, pages 121?128, Orlando, FL.
Nunberg, Geoffrey. 1987. Poetic and prosaic
metaphors. In Proceedings of the 1987
Workshop on Theoretical Issues in Natural
Language Processing, pages 198?201,
Stroudsburg, PA.
O? Se?aghdha, Diarmuid. 2010. Latent
variable models of selectional preference.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 435?444, Uppsala.
Orwell, George. 1946. Politics and the
English Language. Horizon, 13(76):252?265.
Pantel, Patrick and Dekang Lin. 2002.
Discovering word senses from text. In
Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 613?619,
Edmonton.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of ACL-93,
pages 183?190, Morristown, NJ.
Peters, Wim and Ivonne Peters. 2000.
Lexicalised systematic polysemy in
Wordnet. In Proceedings of LREC 2000,
Athens.
Pinker, Stephen. 2007. The Stuff of Thought:
Language as a Window into Human Nature.
Viking Adult, New York, NY.
Power, Richard and Donia Scott. 2005.
Automatic generation of large-scale
paraphrases. In Proceedings of IWP,
pages 73?79.
Pragglejaz Group. 2007. MIP: A method
for identifying metaphorically used
words in discourse.Metaphor and Symbol,
22:1?39.
Preiss, Judita, Ted Briscoe, and Anna
Korhonen. 2007. A system for large-scale
acquisition of verbal, nominal and
adjectival subcategorization frames from
corpora. In Proceedings of ACL-2007,
volume 45, page 912, Prague.
Preiss, Judita, Andrew Coonce, and
Brittany Baker. 2009. HMMs, GRs, and
n-grams as lexical substitution techniques:
are they portable to other languages?
In Proceedings of the Workshop on Natural
Language Processing Methods and Corpora
in Translation, Lexicography, and Language
Learning, MCTLLL ?09, pages 21?27,
Borovets.
Pucci, Dario, Marco Baroni, Franco Cutugno,
and Alessandro Lenci. 2009. Unsupervised
lexical substitution with a word space
model. In Proceedings of the EVALITA
Workshop, 11th Congress of Italian
Association for Artificial Intelligence,
Reggie Emilia.
Pustejovsky, James. 1995. The Generative
Lexicon.MIT Press, Cambridge, MA.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation.
In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language
Processing, pages 142?149, Barcelona.
Reining, Astrid and Birte Lo?nneker-Rodman.
2007. Corpus-driven metaphor harvesting.
In Proceedings of the HLT/NAACL-07
Workshop on Computational Approaches
to Figurative Language, pages 5?12,
Rochester, NY.
Reisinger, Joseph and Raymond Mooney.
2010. A mixture model with sharing for
lexical semantics. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10,
pages 1173?1182, Cambridge, MA.
Resnik, Philip. 1993. Selection and
Information: A Class-based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings
of ACL 99, pages 104?111, Maryland.
Rudolph, Sebastian and Eugenie Giesbrecht.
2010. Compositional matrix-space
352
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
models of language. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 907?916,
Uppsala.
Schulte im Walde, Sabine. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32(2):159?194.
Sekine, Satoshi, Kentaro Inui, Ido Dagan,
Bill Dolan, Danilo Giampiccolo, and
Bernardo Magnini, editors. 2007.
Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
Prague.
Shalizi, Cosma. 2003. Analogy and metaphor.
Available at http://masi.cscs.lsa.
umich.edu/?crshalizi/notabene.
Shinyama, Yusuke and Satoshi Sekine. 2003.
Paraphrase acquisition for information
extraction. In Proceedings of the Second
International Workshop on Paraphrasing -
Volume 16, PARAPHRASE ?03,
pages 65?71, Sapporo.
Shutova, Ekaterina. 2010. Automatic
metaphor interpretation as a paraphrasing
task. In Proceedings of NAACL 2010,
pages 1029?1037, Los Angeles, CA.
Shutova, Ekaterina, Lin Sun, and Anna
Korhonen. 2010. Metaphor identification
using verb and noun clustering.
In Proceedings of COLING 2010,
pages 1,002?1,010, Beijing.
Shutova, Ekaterina and Simone Teufel.
2010. Metaphor corpus annotated for
source?target domain mappings.
In Proceedings of LREC 2010,
pages 3,255?3,261, Malta.
Siegel, Sidney and N. John Castellan. 1988.
Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill Book Company,
New York, NY.
Sun, Lin and Anna Korhonen. 2009.
Improving verb clustering with
automatically acquired selectional
preferences. In Proceedings of
EMNLP 2009, pages 638?647, Singapore.
Sun, Lin and Anna Korhonen. 2011.
Hierarchical verb clustering using graph
factorization. In Proceedings of EMNLP,
pages 1,023?1,033, Edinburgh.
Toral, Antonio. 2009. The lexical substitution
task at EVALITA 2009. In Proceedings of
EVALITA Workshop, 11th Congress of Italian
Association for Artificial Intelligence,
Regio Emilia.
Tourangeau, Roger and Robert Sternberg.
1982. Understanding and appreciating
metaphors. Cognition, 11:203?244.
Van de Cruys, Tim, Thierry Poibeau, and
Anna Korhonen. 2011. Latent vector
weighting for word meaning in
context. In Proceedings of EMNLP,
pages 1,012?1,022, Edinburgh.
van Rijsbergen, Keith. 1979. Information
Retrieval, 2nd edition. Butterworths,
London.
Veale, Tony and Yanfen Hao. 2008.
A fluid knowledge representation for
understanding and generating creative
metaphors. In Proceedings of COLING 2008,
pages 945?952, Manchester.
Wilks, Yorick. 1975. A preferential
pattern-seeking semantics for natural
language inference. Artificial Intelligence,
6:53?74.
Wilks, Yorick. 1978. Making preferences more
active. Artificial Intelligence, 11(3):197?223.
Yu, K., S. Yu, and V. Tresp. 2006. Soft
clustering on graphs. NIPS,
pages 1553?1561, Vancouver.
Zapirain, Ben?at, Eneko Agirre, and Llu??s
Ma`rquez. 2009. Generalizing over lexical
features: selectional preferences for
semantic role classification. In Proceedings
of the ACL-IJCNLP 2009 Conference Short
Papers, pages 73?76, Singapore.
Zhao, S., H. Wang, T. Liu, and S. Li. 2008.
Pivot approach for extracting paraphrase
patterns from bilingual corpora.
In Proceedings of ACL-08:HLT,
pages 780?788, Columbus, OH.
Zhao, Shiqi, Xiang Lan, Ting Liu, and
Sheng Li. 2009. Application-driven
statistical paraphrase generation. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP:
Volume 2, ACL ?09, pages 834?842, Suntec.
Zhou, Liang, Chin-Yew Lin, Dragos Stefan
Munteanu, and Eduard H. Hovy. 2006.
PARAEVAL: Using paraphrases to
evaluate summaries automatically.
In Proceedings of HLT-NAACL,
pages 447?454, New York, NY.
Zong, Chengqing, Yujie Zhang, and
Kazuhide Yamamoto. 2001. Approach to
spoken Chinese paraphrasing based on
feature extraction. In Proceedings of NLPRS,
pages 551?556, Tokyo.
353

Probabilistic Distributional Semantics with
Latent Variable Models
Diarmuid O? Se?aghdha?
University of Cambridge, UK
Anna Korhonen?
University of Cambridge, UK
We describe a probabilistic framework for acquiring selectional preferences of linguistic predi-
cates and for using the acquired representations to model the effects of context on word meaning.
Our framework uses Bayesian latent-variable models inspired by, and extending, the well-known
Latent Dirichlet Allocation (LDA) model of topical structure in documents; when applied to
predicate?argument data, topic models automatically induce semantic classes of arguments and
assign each predicate a distribution over those classes. We consider LDA and a number of
extensions to the model and evaluate them on a variety of semantic prediction tasks, demon-
strating that our approach attains state-of-the-art performance. More generally, we argue that
probabilistic methods provide an effective and flexible methodology for distributional semantics.
1. Introduction
Computational models of lexical semantics attempt to represent aspects of word mean-
ing. For example, a model of the meaning of dog may capture the facts that dogs are
animals, that they bark and chase cats, that they are often kept as pets, and so on. Word
meaning is a fundamental component of the way language works: Sentences (and larger
structures) consist of words, and their meaning is derived in part from the contributions
of their constituent words? lexical meanings. At the same time, words instantiate a
mapping between conceptual ?world knowledge? and knowledge of language.
The relationship between the meanings of an individual word and the larger
linguistic structure in which it appears is not unidirectional; while the word contributes
to the meaning of the structure, the structure also clarifies the meaning of the word.
Taken on its own a word may be vague or ambiguous, in the senses of Zwicky and
Sadock (1975); even when the word?s meaning is relatively clear it may still admit
specification of additional details that affect its interpretation (e.g., what color/breed
was the dog?). This specification comes through context, which consists of both
linguistic and extralinguistic factors but shows a strong effect of the immediate lexical
and syntactic environment?the other words surrounding the word of interest and
their syntactic relations to it.
? 15 JJ Thomson Avenue, Cambridge, CB3 0FD, United Kingdom.
E-mail: Diarmuid.O?Seaghdha@cl.cam.ac.uk.
Submission received: 20 December 2012; revised version received: 14 July 2013; accepted for publication:
7 October 2013
doi:10.1162/COLI a 00194
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
These diverse concerns motivate lexical semantic modeling as an important task
for all computational systems that must tackle problems of meaning. In this article
we develop a framework for modeling word meaning and how it is modulated by
contextual effects.1 Our models are distributional in the sense that their parameters
are learned from observed co-occurrences between words and contexts in corpus data.
More specifically, they are probabilistic models that associate latent variables with
automatically induced classes of distributional behavior and associate each word with
a probability distribution over those classes. This has a natural interpretation as a
model of selectional preference, the semantic phenomenon by which predicates such
as verbs or adjectives more plausibly combine with some classes of arguments than
with others. It also has an interpretation as a disambiguation model: The different latent
variable values correspond to different aspects of meaning and a word?s distribution
over those values can be modified by information coming from the context it appears
in. We present a number of specific models within this framework and demonstrate that
they can give state-of-the-art performance on tasks requiring models of preference and
disambiguation. More generally, we illustrate that probabilistic modeling is an effective
general-purpose framework for distributional semantics and a useful alternative to the
popular vector-space framework.
The main contributions of the article are as follows:
 We describe the probabilistic approach to distributional semantics,
showing how it can be applied as generally as the vector-space approach.
 We present three novel probabilistic selectional preference models and
show that they outperform a variety of previously proposed models on a
plausibility-based evaluation.
 Furthermore, the representations learned by these models correspond to
semantic classes that are useful for modeling the effect of context on
semantic similarity and disambiguation.
Section 2 presents background on distributional semantics and an overview of prior
work on selectional preference learning and on modeling contextual effects. Section 3
introduces the probabilistic latent-variable approach and details the models we use.
Section 4 presents our experimental results on four data sets. Section 5 concludes and
sketches promising research directions for the future.
2. Background and Related Work
2.1 Distributional Semantics
The distributional approach to semantics is often traced back to the so-called ?distri-
butional hypothesis? put forward by mid-century linguists such as Zellig Harris and
J.R. Frith:
If we consider words or morphemes A and B to be more different in meaning than A
and C, then we will often find that the distributions of A and B are more different than
the distributions of A and C. (Harris 1954)
1 We build on previous work published in O? Se?aghdha (2010) and O? Se?aghdha and Korhonen (2011),
adding new models and evaluation experiments as well as a comprehensive exposition. In Section 4
we indicate which experimental results have previously been reported.
588
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
You shall know a word by the company it keeps. (Frith 1957)
In Natural Language Processing (NLP), the term distributional semantics encompasses
a broad range of methods that identify the semantic properties of a word or other
linguistic unit with its patterns of co-occurrence in a corpus of textual data. The
potential for learning semantic knowledge from text was recognized very early in
the development of NLP (Spa?rck Jones 1964; Cordier 1965; Harper 1965), but it is with
the technological developments of the past twenty years that this data-driven approach
to semantics has become dominant. Distributional approaches may use a representation
based on vector spaces, on graphs, or (like this article) on probabilistic models, but
they all share the common property of estimating their parameters from empirically
observed co-occurrences.
The basic unit of distributional semantics is the co-occurrence: an observation of
a word appearing in a particular context. The definition is a general one: We may
be interested in all kinds of words, or only a particular subset of the vocabulary;
we may define the context of interest to be a document, a fixed-size window around
a nearby word, or a syntactic dependency arc incident to a nearby word. Given a
data set of co-occurrence observations we can extract an indexed set of co-occurrence
counts fw for each word of interest w; each entry fwc counts the number of times that
w was observed in context c. Alternatively, we can extract an indexed set fc for each
context.
The vector-space approach is the best-known methodology for distributional
semantics; under this conception fw is treated as a vector in R|C|, where C is the
vocabulary of contexts. As such, fw is amenable to computations known from lin-
ear algebra. We can compare co-occurrence vectors for different words with a simi-
larity function such as the cosine measure or a dissimilarity function such as
Euclidean distance; we can cluster neighboring vectors; we can project a matrix
of co-occurrence counts onto a low-dimensional subspace; and so on. This is per-
haps the most popular approach to distributional semantics and there are many
good general overviews covering the possibilities and applications of the vector space
model (Curran 2003; Weeds and Weir 2005; Pado? and Lapata 2007; Turney and Pantel
2010).
Although it is natural to view the aggregate of co-occurrence counts for a word
as constituting a vector, it is equally natural to view it as defining a probability distri-
bution. When normalized to have unit sum, fw parameterizes a discrete distribution
giving the conditional probability of observing a particular context given that we
observe w. The contents of the vector-space modeler?s toolkit generally have prob-
abilistic analogs: similarity and dissimilarity can be computed using measures from
information theory such as the Kullback?Leibler or Jensen?Shannon divergences (Lee
1999); the effects of clustering and dimensionality reduction can be achieved through
the use of latent variable models (see Section 3.2.2). Additionally, Bayesian priors on
parameter distributions provide a flexible toolbox for performing regularization and
incorporating prior information in learning. A further advantage of the probabilistic
framework is that it is often straightforward to extend existing models to account
for additional structure in the data, or to tie together parameters for shared statis-
tical strength, while maintaining guarantees of well-normalized behavior thanks to
the laws of probability. In this article we focus on selectional preference learning and
contextual disambiguation but we believe that the probabilistic approach exempli-
fied here can fruitfully be applied in any scenario involving distributional semantic
modeling.
589
Computational Linguistics Volume 40, Number 3
2.2 Selectional Preferences
2.2.1 Motivation. A fundamental concept in linguistic knowledge is the predicate, by
which we mean a word or other symbol that combines with one or more arguments
to produce a composite representation with a composite meaning (by the principle of
compositionality). The archetypal predicate is a verb; for example, transitive drink takes
two noun arguments as subject and object, with which it combines to form a basic
sentence. However, the concept is a general one, encompassing other word classes as
well as more abstract items such as semantic relations (Yao et al. 2011), semantic frames
(Erk, Pado?, and Pado? 2010), and inference rules (Pantel et al. 2007). The asymmetric
distinction between predicate and argument is analogous to that between context and
word in the more general distributional framework.
It is intuitive that a particular predicate will be more compatible with some semantic
argument classes than with others. For example, the subject of drink is typically an
animate entity (human or animal) and the object of drink is typically a beverage. The
subject of eat is also typically an animate entity but its object is typically a foodstuff.
The noun modified by the adjective tasty is also typically a foodstuff, whereas the
noun modified by informative is an information-bearing object. This intuition can be
formalized in terms of a predicate?s selectional preference: a function that assigns a
numerical score to a combination of a predicate and one or more arguments according
to the semantic plausibility of that combination. This score may be a probability, a rank,
a real value, or a binary value; in the last case, the usual term is selectional restriction.
Models of selectional preference aim to capture conceptual knowledge that all
language users are assumed to have. Speakers of English can readily identify that
examples such as the following are semantically infelicitous despite being syntactically
well-formed:
1. The beer drank the man.
2. Quadruplicity drinks procrastination. (Russell 1940)
3. Colorless green ideas sleep furiously. (Chomsky 1957)
4. The paint is silent. (Katz and Fodor 1963)
Psycholinguistic experiments have shown that the time course of human sentence
processing is sensitive to predicate?argument plausibility (Altmann and Kamide 1999;
Rayner et al. 2004; Bicknell et al. 2010): Reading times are faster when participants are
presented with plausible combinations than when they are presented with implausible
combinations. It has also been proposed that selectional preference violations are cues
that trigger metaphorical interpretation. Wilks (1978) gives the example My car drinks
gasoline, which must be understood non-literally since car strongly violates the subject
preference of drink and gasoline is also an unlikely candidate for something to drink.
In NLP, one motivation for modeling predicate?argument plausibility is to
investigate whether this aspect of human conceptual knowledge can be learned
automatically from text corpora. If the predictions of a computational model correlate
with judgments collected from human behavioral data, the assumption is that the
model itself shares some properties with human linguistic knowledge and is in some
sense a ?good? semantic model. More practically, NLP researchers have shown that
selectional preference knowledge is useful for downstream applications, including
metaphor detection (Shutova 2010), identification of non-compositional multiword
590
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
expressions (McCarthy, Venkatapathy, and Joshi 2007), semantic role labeling (Gildea
and Jurafsky 2002; Zapirain, Agirre, and Ma`rquez 2009; Zapirain et al. 2010), word
sense disambiguation (McCarthy and Carroll 2003), and parsing (Zhou et al. 2011).
2.2.2 The ?Counting? Approach. The simplest way to estimate the plausibility of a
predicate?argument combination from a corpus is to count the number of times that
combination appears, on the assumptions that frequency correlates with plausibility
and that given enough data the resulting estimates will be relatively accurate. For exam-
ple, Keller and Lapata (2003) estimate predicate?argument plausibilities by submitting
appropriate queries to a Web search engine and counting the number of ?hits? returned.
To estimate the frequency with which the verb drink takes beer as a direct object,
Keller and Lapata?s method uses the query <drink|drinks|drank|drunk|drinking a|the|?
beer|beers>; to estimate the frequency with which tasty modifies pizza the query is simply
<tasty pizza|pizzas>. Where desired, these joint frequency counts can be normalized by
unigram hit counts to estimate conditional probabilities such as P(pizza|tasty).
The main advantages of this approach are its simplicity and its ability to exploit
massive corpora of raw text. On the other hand, it is hindered by the facts that only
shallow processing is possible and that even in a Web-scale corpus the probability esti-
mates for rare combinations will not be accurate. At the time of writing, Google returns
zero hits for the query <draughtsman|draughtsmen whistle|whistles|whistled|whistling>
and 1,570 hits for <onion|onions whistle|whistles|whistled|whistling>, suggesting the im-
plausible conclusion that an onion is far more likely to whistle than a draughtsman.2
Zhou et al. (2011) modify the Web query approach to better capture statistical
association by using pointwise mutual information (PMI) rather than raw co-occurrence
frequency to quantify selectional preference:
PMI(p, a) = log
P(p, a)
P(p)P(a)
(1)
The role of the PMI transformation is to correct for the effect of unigram frequency: A
common word may co-occur often with another word just because it is a common word
rather than because there is a semantic association between them. However, it does not
provide a way to overcome the problem of inaccurate counts for low-probability co-
occurrences. Zhou et al.?s goal is to incorporate selectional preference features into a
parsing model and they do not perform any evaluation of the semantic quality of the
resulting predictions.
2.2.3 Similarity-Based Smoothing Methods. During the 1990s, research on language mod-
eling led to the development of various ?smoothing? methods for overcoming the
data sparsity problem that inevitably arises when estimating co-occurrence counts from
finite corpora (Chen and Goodman 1999). The general goal of smoothing algorithms
is to alter the distributional profile of observed counts to better match the known
statistical properties of linguistic data (e.g., that language exhibits power-law behavior).
Some also incorporate semantic information on the assumption that meaning guides the
distribution of words in a text.
2 The analogous example given by O? Se?aghdha (2010) relates to the plausibility of a manservant or a
carrot laughing; Google no longer returns zero hits for <a|the manservant|manservants|menservants
laugh|laughs|laughed> but a frequency-based estimate still puts the probability of a carrot laughing at
200 times that of a manservant laughing (1,680 hits against 81 hits).
591
Computational Linguistics Volume 40, Number 3
One such class of methods is based on similarity-based smoothing, by which
one can extrapolate from observed co-occurrences by implementing the distributional
hypothesis: ?similar? words will have similar distributional properties. A general form
for similarity-based co-occurrence estimates is
P(w2|w1) =
?
w3?S(w1,w2)
sim(w2, w3)
?
w??S(w1,w2) sim(w2, w
?)
P(w3|w1) (2)
sim can be an arbitrarily chosen similarity function; Dagan, Lee, and Pereira (1999)
investigate a number of options. S(w1, w2) is a set of comparison words that may depend
on w1 or w2, or neither: Essen and Steinbiss (1992) use the entire vocabulary, whereas
Dagan, Lee, and Pereira use a fixed number of the most similar words to w2, provided
their similarity value is above a threshold t.
While originally proposed for language modeling?the task of estimating the
probability of a sequence of words?these methods require only trivial alteration to
estimate co-occurrence probabilities for predicates and arguments, as was noted early
on by Grishman and Sterling (1993) and Dagan, Lee, and Pereira (1999). Erk (2007)
and Erk, Pado?, and Pado? (2010) build on this prior work to develop an ?exemplar-
based? selectional preference model called EPP. In the EPP model, the set of comparison
words is the set of words observed for the predicate p in the training corpus, denoted
Seenargs(p):
SelprefEPP(a|p) =
?
a??Seenargs(p)
weight(a?|p)sim(a?, a)
?
a???Seenargs(p) weight(a
??
|p)
(3)
The co-occurrence strength weight(a|p) may simply be normalized co-occurrence fre-
quency; alternatively a statistical association measure such as pointwise mutual in-
formation may be used. As before, sim(a, a?) may be any similarity measure defined
on members of A. One advantage of this and other similarity-based models is that
the corpus used to estimate similarity need not be the same as that used to estimate
predicate?argument co-occurrence, which is useful when the corpus labeled with these
co-occurrences is small (e.g., a corpus labeled with FrameNet frames).
2.2.4 Discriminative Models. Bergsma, Lin, and Goebel (2008) cast selectional preference
acquisition as a supervised learning problem to which a discriminatively trained classi-
fier such as a Support Vector Machine (SVM) can be applied. To produce training data
for a predicate, they pair ?positive? arguments that were observed for that predicate
in the training corpus and have an association with that predicate above a speci-
fied threshold (measured by mutual information) with randomly selected ?negative?
arguments of similar frequency that do not occur with the predicate or fall below
the association threshold. Given this training data, a classifier can be trained in a
standard way to predict a positive or negative score for unseen predicate?argument
pairs.
An advantage of this approach is that arbitrary sets of features can be used to
represent the training and testing items. Bergsma, Lin, and Goebel include conditional
probabilities P(a|p) for all predicates the candidate argument co-occurs with, typo-
graphic features of the argument itself (e.g., whether it is capitalized, or contains digits),
lists of named entities, and precompiled semantic classes.
592
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
2.2.5 WordNet-Based Models. An alternative approach to preference learning models the
argument distribution for a predicate as a distribution over semantic classes provided
by a predefined lexical resource. The most popular such resource is the WordNet lexical
hierarchy (Fellbaum 1998), which provides semantic classes and hypernymic structures
for nouns, verbs, adjectives, and adverbs.3 Incorporating knowledge about the WordNet
taxonomy structure in a preference model enables the use of graph-based regularization
techniques to complement distributional information, while also expanding the cov-
erage of the model to types that are not encountered in the training corpus. On the
other hand, taxonomy-based methods build in an assumption that the lexical hierarchy
chosen is the universally ?correct? one and they will not perform as well when faced
with data that violates the hierarchy or contains unknown words. A further issue faced
by these models is that the resources they rely on require significant effort to create and
will not always be available to model data in a new language or a new domain.
Resnik (1993) proposes a measure of associational strength between a predicate and
WordNet classes based on the empirical distribution of words of each class (and their
hyponyms) in a corpus. Abney and Light (1999) conceptualize the process of generating
an argument for a predicate in terms of a Markovian random walk from the hierarchy?s
root to a leaf node and choosing the word associated with that leaf node. Ciaramita
and Johnson (2000) likewise treat WordNet as defining the structure of a probabilistic
graphical model, in this case a Bayesian network. Li and Abe (1998) and Clark and Weir
(2002) both describe models in which a predicate ?cuts? the hierarchy at an appropriate
level of generalization, such that all classes below the cut are considered appropriate
arguments (whether observed in data or not) and all classes above the cut are considered
inappropriate.
In this article we focus on purely distributional models that do not rely on
manually constructed lexical resources; therefore we do not revisit the models
described in this section subsequently, except as a basis for empirical comparison.
O? Se?aghdha and Korhonen (2012) do investigate a number of Bayesian preference
models that incorporate WordNet classes and structure, finding that such models
outperform previously proposed WordNet-based models and perform comparably to
the distributional Bayesian models presented here.
2.3 Measuring Similarity in Context
2.3.1 Motivation. A fundamental idea in semantics is that the meaning of a word is
disambiguated and modulated by the context in which it appears. The word body clearly
has a different sense in each of the following text fragments:
1. Depending on the present position of the planetary body in its orbital path, . . .
2. The executive body decided. . .
3. The human body is intriguing in all its forms.
In a standard word sense disambiguation experiment, the task is to map instances of
ambiguous words onto senses from a manually compiled inventory such as WordNet.
An alternative experimental method is to have a system rate the suitability of replacing
an ambiguous word with an alternative word that is synonymous or semantically
3 WordNet also contains many other kinds of semantic relations besides hypernymy but these are not
typically used for selectional preference modeling.
593
Computational Linguistics Volume 40, Number 3
similar in some contexts but not others. For example, committee is a reasonable
substitute for body in fragment 2 but less reasonable in fragment 1. An evaluation of
semantic models based on this principle was run as the English Lexical Substitution
Task in SemEval 2007 (McCarthy and Navigli 2009). The annotated data from the
Lexical Substitution Task have been used by numerous researchers to evaluate models
of lexical choice; see Section 4.5 for further details.
In this section we formalize the problem of predicting the similarity or substitutabil-
ity of a pair of words wo, ws in a given context C = {(r1, w1), (r2, w2), . . . , (rn, wn)}. When
the task is substitution, wo is the original word and ws is the candidate substitute. Our
general approach is to compute a representation Rep(wo|C) for wo in context C and
compare it with Rep(ws), our representation for wn:
sim(wo, ws|C) = sim(Rep(wo|C), Rep(ws)) (4)
where sim is a suitable similarity function for comparing the representations. This
general framework leaves open the question of what kind of representation we use for
Rep(wo|C) and Rep(ws); in Section 2.3.2 we describe representations based on vector-
space semantics and in Section 3.5 we describe representations based on latent-variable
models.
A complementary perspective on the disambiguatory power of context models is
provided by research on semantic composition, namely, how the syntactic effect of
a grammar rule is accompanied by a combinatory semantic effect. In this view, the
goal is to represent the combination of a context and an in-context word, not just to
represent the word given the context. The co-occurrence models described in this article
are not designed to scale up and provide a representation for complex syntactic struc-
tures,4 but they are applicable to evaluation scenarios that involve representing binary
co-occurrences.
2.3.2 Vector-Space Models. As described in Section 2.1, the vector-space approach to
distributional semantics casts word meanings as vectors of real numbers and uses linear
algebra operations to compare and combine these vectors. A word w is represented by
a vector vw that models aspects of its distribution in the training corpus; the elements
of this vector may be co-occurrence counts (in which case it is the same as the frequency
vector fw) or, more typically, some transformation of the raw counts.
Mitchell and Lapata (2008, 2010) present a very general vector-space framework
in which to consider the problem of combining the semantic representations of co-
occurring words. Given pre-computed word vectors vw, vw? , their combination p is pro-
vided by a function g that may also depend on syntax R and background knowledge K:
p = g(vw, vw? , R, K) (5)
Mitchell and Lapata investigate a number of functions that instantiate Equation (5),
finding that elementwise multiplication is a simple and consistently effective choice:
pi = vwi ? vw?i (6)
4 cf. Grefenstette and Sadrzadeh (2011), Socher et al. (2011)
594
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
The motivation for this ?disambiguation by multiplication? is that lexical vectors are
sparse and the multiplication operation has the effect of sending entries not supported
in both vw and vw? towards zero while boosting entries that have high weights in both
vectors.
The elementwise multiplication approach assumes that all word vectors are in
the same space. For a syntactic co-occurrence model, this is often not the case: The
contexts for a verb and a noun may have no dependency labels in common and hence
multiplying their vectors will not give useful results. Erk and Pado? (2008) propose a
structured vector space approach in which each word w is associated with a set of
?expectation? vectors Rw, indexed by dependency label, in addition to its standard co-
occurrence vector vw. The expectation vector Rw(r) for word w and dependency label r
is an average over co-occurrence vectors for seen arguments of w and r in the training
corpus:
Rw(r) =
?
w?: f (w,r,w?)>0
f (w, r, w?) ? vw? (7)
Whereas a standard selectional preference model addresses the question ?which words
are probable as arguments of predicate (w, r)??, the expectation vector (7) addresses
the question ?what does a typical co-occurrence vector for an argument of the pred-
icate (w, r) look like??. To disambiguate the semantics of word w in the context of a
predicate (w?, r?), Erk and Pado? combine the expectation vector Rw? (r?) with the word
vector vw:
vw|r?,w? = Rw? (r
?) ? vw (8)
Thater, Fu?rstenau, and Pinkal (2010, 2011) have built on the idea of using syntactic
vector spaces for disambiguation. The model of Thater, Fu?rstenau, and Pinkal (2011),
which is simpler and better-performing, sets the representation of w in the context of
(r?, w?) to be
vw|r?,w? =
?
w??,r??
?w?,r?,w??,r?? ? weight(w
??, r??, w) ? er??,w?? (9)
where ? quantifies the compatibility of the observed predicate (w?, r?) with the smooth-
ing predicate (w??, r??), weight quantifies the co-occurrence strength between (w??, r??)
and w, and er??,w?? is a basis vector for (w??, r??). This is a general formulation admit-
ting various choices of ? and weight; the optimal configuration is found to be as
follows:
?w?,r?,w??,r?? =
{
sim(vw? , vw?? ) if r? = r??
0 otherwise (10)
weight(w??, r??, w) = PMI((w??, r??), w) (11)
This is conceptually very similar to the EPP selectional preference model (3) of Erk,
Pado?, and Pado? (2010); each entry in the vector vw|r?,w? is a similarity-smoothed estimate
of the preference of (w?, r?) for w. EPP uses seen arguments of (w?, r?) for smoothing,
whereas Thater, Fu?rstenau, and Pinkal (2011) take a complementary approach and
595
Computational Linguistics Volume 40, Number 3
smooth with seen predicates for w. In order to combine the disambiguatory effects of
multiple predicates, a sum over contextualized vectors is taken:
vw|(r1,w1 ),(r2 ,w2 ),...,(rn,wn ) =
n
?
i
vw|ri,wi (12)
All the models described in this section provide a way of relating a word?s standard
co-occurrence vector to a vector representation of the word?s meaning in context. This
allows us to calculate the similarity between two in-context words or between a word
and an in-context word using standard vector similarity measures such as the cosine. In
applications where the task is to judge the appropriateness of substituting a word ws for
an observed word wo in context C = {(r1, w1), (r2, w2), . . . , (rn, wn)}, a common approach
is to compute the similarity between the contextualized vector vwo|(r1,w1 ),(r2,w2 ),...,(rn,wn )
and the uncontextualized word vector vws . It has been demonstrated empirically that
this approach yields better performance than contextualizing both vectors before the
similarity computation.
3. Probabilistic Latent Variable Models for Lexical Semantics
3.1 Notation and Terminology
We define a co-occurrence as a pair (c, w), where c is a context belonging to the vo-
cabulary of contexts C and w is a word belonging to the word vocabulary W .5 Unless
otherwise stated, the contexts considered in this article are head-lexicalized dependency
edges c = (r, wh) where r ? R is the grammatical relation and wh ? W is the head lemma.
We notate grammatical relations as ph:label:pd, where ph is the head word?s part of
speech, pd is the dependent word?s part of speech, and label is the dependency label.6
We use a coarse set of part-of-speech tags: n (noun), v (verb), j (adjective), r (adverb).
The dependency labels are the grammatical relations used by the RASP system (Briscoe
2006; Briscoe, Carroll, and Watson 2006), though in principle any dependency formalism
could be used. The assumption that predicates correspond to head-lexicalized depen-
dency edges means that they have arity one.
Given a parsed sentence, each word w in the sentence has a syntactic context set
C comprising all the dependency edges incident to w. In the sentence fragment The
executive body decided. . . , the word body has two incident edges:
The:d executive:j body:n
n:ncmod:j

decided:v
v:ncsubj:n

. . .
5 When specifically discussing selectional preferences, we will also use the terms predicate and argument
to describe a co-occurrence pair; when restricted to syntactic predicates, the former term is synonymous
with our definition of context.
6 Strictly speaking, w and wh are drawn from subsets of W that are licensed by r when r is a syntactic
relation, that is, they must have parts of speech pd and ph , respectively. Our models assume a fixed
argument vocabulary, so we can partition the training data according to part of speech; the models are
agnostic regarding the predicate vocabulary as these are subsumed by the context vocabulary. In the
interest of parsimony we leave this detail implicit in our notation.
596
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
The context set for body is C = {(j:ncmod?1:n,executive), (v:ncsubj:n,decide)}, where
(v:ncsubj:n,decide) indicates that body is the subject of decide and (j:ncmod?1:n,executive)
denotes that it stands in an inverse non-clausal modifier relation to executive (we assume
that nouns are the heads of their adjectival modifiers).
To estimate our preference models we will rely on co-occurrence counts extracted
from a corpus of observations O. Each observation is a co-occurrence of a predicate and
an argument. The set of observations for context c is denoted O(c). The co-occurrence
frequency of context c and word w is denoted by fcw, and the total co-occurrence
frequency of c by fc =
?
w?W fcw.
3.2 Modeling Assumptions
3.2.1 Bayesian Modeling. The Bayesian approach to probabilistic modeling (Gelman et al.
2003) is characterized by (1) the use of prior distributions over model parameters to
encode the modeler?s expectations about the values they will take; and (2) the explicit
quantification of uncertainty by maintaining posterior distributions over parameters
rather than point estimates.7
As is common in NLP, the data we are interested in modeling are drawn from a
discrete sample space (e.g., the vocabulary of words or a set of semantic classes). This
leads to the use of a categorical or multinomial distribution for the data likelihood. This
distribution is parameterized by a unit-sum vector ? with length |K| where K is the
sample space. The probability that an observation o takes value k is then:
o ? Multinomial(?) (13)
P(o = k|?) = ?k (14)
The value of ? must typically be learned from data. The maximum likelihood estimate
(MLE) sets ?k proportional to the number of times k was observed in a set of observa-
tions O, where each observation oi ? K:
?
MLE
k =
fk
|O|
(15)
Although simple, such an approach has significant limitations. Because a linguistic
vocabulary contains a large number of items that individually have low probability
but together account for considerable total probability mass, even a large corpus is
unlikely to give accurate estimates for low-probability types (Evert 2004). Items that
do not appear in the training data will be assigned zero probability of appearing in
unseen data, which is rarely if ever a valid assumption. Sparsity increases further when
the sample space contains composite items (e.g., context-words pairs).
The standard approach to dealing with the shortcomings of MLE estimation in
language modeling is to ?smooth? the distribution by taking probability mass from
frequent types and giving it to infrequent types. The Bayesian approach to smoothing
is to place an appropriate prior on ? and apply Bayes? Theorem:
P(?|O) =
P(O|?)P(?)
?
P(O|?)P(?)d?
(16)
7 However, the second point is often relaxed in application contexts where the posterior mean is used for
inference (e.g., Section 3.4.2).
597
Computational Linguistics Volume 40, Number 3
A standard choice for the prior distribution over the parameters of a discrete distribu-
tion is the Dirichlet distribution:
? ? Dirichlet(?) (17)
P(?|?) =
?(
?
k ?k)
?
k ?(?k)
?
k
?
?k?1
k (18)
Here, ? is a |K|-length vector where each ?k > 0. One effect of the Dirichlet prior is that
setting the sum
?
k ?k to a small value will encode the expectation that the parameter
vector ? is likely to distribute its mass more sparsely. The Dirichlet distribution is a
conjugate prior for multinomial and categorical likelihoods, in the sense that the poste-
rior distribution P(?|O) in Equation (16) is also a Dirichlet distribution when P(O|?) is
multinomial or categorical and P(?) is Dirichlet:
? ? Dirichlet(fO ??) (19)
where ? indicates elementwise addition of the observed count vector fO to the Dirichlet
parameter vector ?. Furthermore, the conjugacy property allows us to do a number of
important computations in an efficient way. In many applications we are interested in
predicting the distribution over values K for a ?new? observation given a set of prior
observations O while retaining our uncertainty about the model parameters. We can
average over possible values of ?, weighted according to their probability P(?|O,?) by
?integrating out? the parameter and still retain a simple closed-form expression for the
posterior predictive distribution:
P(o
|O|+1 = k|O,?) =
?
P(o
|O|+1 = k|?)P(?|O,?)d? (20)
=
fk + ?k
|O|+
?
k? ?k?
(21)
Expression (21) is central to the implementation of collapsed Gibbs samplers for
Bayesian models such as latent Dirichlet allocation (Section 3.3). For mathematical
details of these derivations, see Heinrich (2009).
Other priors commonly used for discrete distributions in NLP include the Dirichlet
process and the Pitman?Yor process (Goldwater, Griffiths, and Johnson 2011). The
Dirichlet process provides similar behavior to the Dirichlet distribution prior but is
?non-parametric? in the sense of varying the size of its support according to the data;
in the context of mixture modeling, a Dirichlet process prior allows the number of
mixture components to be learned rather than fixed in advance. The Pitman?Yor process
is a generalization of the Dirichlet process that is better suited to learning power-
law distributions. This makes it particularly suitable for language modeling where the
Dirichlet distribution or Dirichlet process would not produce a long enough tail due to
their preference for sparsity (Teh 2006). On the other hand, Dirichlet-like behavior may
be preferable in semantic modeling, where we expect, for example, predicate?class and
class?argument distributions to be sparse.
3.2.2 The Latent Variable Assumption. In probabilistic modeling, latent variables are
random variables whose values are not provided by the input data. As a result, their
598
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
values must be inferred at the same time as the model parameters on the basis of the
training data and model structure. The latent variable concept is a very general one that
is used across a wide range of probabilistic frameworks, from hidden Markov models
to neural networks. One important application is in mixture models, where the data
likelihood is assumed to have the following form:
P(x) =
?
z
P(x|z)P(z) (22)
Here the latent variables z index mixture components, each of which is associated
with a distribution over observations x, and the resulting likelihood is an average of
the component distributions weighted by the mixing weights P(z). The set of possible
values for z is the set of components Z. When |Z| is small relative to the size of the
training data, this model has a clustering effect in the sense that the distribution learned
for P(x|z) is informed by all datapoints assigned to component z.
In a model of two-way co-occurrences each observation consists of two discrete
variables c and w, drawn from vocabularies C and W , respectively.
P(w|c) =
?
z
P(w|z)P(z|c) (23)
The idea of compressing the observed co-occurrence data through a small layer of
latent variables shares the same basic motivations as other, not necessarily probabilistic,
dimensionality reduction techniques such as Latent Semantic Analysis or Non-negative
Matrix Factorization. An advantage of probabilistic models is their flexibility, both in
terms of learning methods and model structures. For example, the models considered
in this article can potentially be extended to multi-way co-occurrences and to hierarchi-
cally defined contexts that cannot easily be expressed in frameworks that require the
input to be a |C| ? |W| co-occurrence matrix.
To the best of our knowledge, latent variable models were first applied to co-
occurrence data in the context of noun clustering by Pereira, Tishby, and Lee (1993).
They suggest a factorization of a noun n?s distribution over verbs v as
P(v|n) =
?
z
P(v|z)P(z|n) (24)
which is equivalent to Equation (23) when we take n as the predicate and v as the
argument, in effect defining an inverse selectional preference model. Pereira, Tishby,
& Lee also observe that given certain assumptions Equation (24) can be written more
symmetrically as
P(v, n) =
?
z
P(v|z)P(n|z)P(z) (25)
The distributions P(v|z), P(n|z), and P(z) are estimated by an optimization procedure
based on Maximum Entropy. Rooth et al. (1999) propose a much simpler Expectation
Maximization (EM) procedure for estimating the parameters of Equation (25).
599
Computational Linguistics Volume 40, Number 3
3.3 Bayesian Models for Binary Co-occurrences
Combining the latent variable co-occurrence model (23) with the use of Dirichlet priors
naturally leads to Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003). Often
described as a ?topic model,? LDA is a model of document content that assumes each
document is generated from a mixture of multinomial distributions or ?topics.? Topics
are shared across documents and correspond to thematically coherent patterns of word
usage. For example, one topic may assign high probability to the words finance, fund,
bank, and invest, whereas another topic may assign high probability to the words football,
goal, referee, and header. LDA has proven to be a very successful model with many
applications and extensions, and the topic modeling framework remains an area of
active research in machine learning.
Although originally conceived for modeling document content, LDA can be ap-
plied to any kind of discrete binary co-occurrence data. The original application of
LDA is essentially a latent-variable model of document?word co-occurrence. Adapting
LDA for selectional preference modeling was suggested independently by O? Se?aghdha
(2010) and Ritter, Mausam, and Etzioni (2010). Conceptually the shift is straightforward
and intuitive: Documents become contexts and words become argument words. The
selectional preference probability P(w|c) is modeled as
P(w|c) =
?
z
P(z|c)P(w|z) (26)
Figure 1 sketches the ?generative story? according to which LDA generates
arguments for predicates and also presents a plate diagram indicating the dependencies
between variables in the model. Table 1 illustrates the semantic representation induced
by a 600-topic LDA model trained on predicate?noun co-occurrences extracted from
the British National Corpus (for more details of this training data, see Section 4.1). The
?semantic classes? are actually distributions over all nouns in the vocabulary rather
than a hard partitioning; therefore we present the eight most probable words for each.
We also present the contexts most frequently associated with each class. Whereas a
for topic z ? {1 . . . |Z|} do
(Draw a distribution over words)
?z ? Dirichlet(?)
end for
for context c ? {1 . . . |C|} do
(Draw a distribution over classes)
?c ? Dirichlet(?)
for observation oi ? O(c) do
(Draw a class)
zi ? Multinomial(?c)
(Draw a word)
wi ? Multinomial(?zi )
end for
end for
?
?
z
w
?
?
O(c)
C
Z
Figure 1
Generative story and plate diagram for LDA; descriptive comments (in parentheses) precede
each sampling step.
600
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
Table 1
Sample semantic classes learned by an LDA syntactic co-occurrence model with |Z| = 600
trained on BNC co-occurrences.
Class 1
Words: attack, raid, assault, campaign, operation, incident, bombing
Object of: launch, carry, follow, suffer, lead, mount, plan, condemn
Subject of: happen, come, begin, cause, continue, take, follow
Modifies: raid, furnace, shelter, victim, rifle, warning, aircraft
Modified by: heart, bomb, air, terrorist, indecent, latest, further, bombing
Prepositional: on home, on house, by force, target for, hospital after, die after
Class 2
Words: line, axis, section, circle, path, track, arrow, curve
Object of: draw, follow, cross, dot, break, trace, use, build, cut
Subject of: divide, run, represent, follow, indicate, show, join, connect
Modifies: manager, number, drawing, management, element, treatment
Modified by: straight, railway, long, cell, main, front, production, product
Prepositional: on map, by line, for year, line by, point on, in fig, angle to
Class 3
Words: test, examination, check, testing, exam, scan, assessment, sample
Object of: pass, carry, use, fail, perform, make, sit, write, apply
Subject of: show, reveal, confirm, prove, consist, come, take, detect, provide
Modifies: result, examination, score, case, ban, question, board, paper, kit
Modified by: blood, medical, final, routine, breath, fitness, driving, beta
Prepositional: subject to, at end, success in, on part, performance on
Class 4
Words: university, college, school, polytechnic, institute, institution, library
Object of: enter, attend, leave, visit, become, found, involve, close, grant
Subject of: offer, study, make, become, develop, win, establish, undertake
Modifies: college, student, library, course, degree, department, school
Modified by: university, open, technical, city, education, state, technology
Prepositional: student at, course at, study at, lecture at, year at
Class 5
Words: fund, reserve, eyebrow, revenue, awareness, conservation, alarm
Object of: raise, set, use, provide, establish, allocate, administer, create
Subject of: raise, rise, shoot, lift, help, remain, set, cover, hold
Modifies: manager, asset, raiser, statement, management, commissioner
Modified by: nature, pension, international, monetary, national, social, trust
Prepositional: for nature, contribution to, for investment, for development
topic model trained on document?word co-occurrences will find topics that reflect
broad thematic commonalities, the model trained on syntactic co-occurrences finds
semantic classes that capture a much tighter sense of similarity: Words assigned high
probability in the same topic tend to refer to entities that have similar properties, that
perform similar actions, and have similar actions performed on them. Thus Class 1 is
represented by attack, raid, assault, campaign, and so on, forming a coherent semantic
grouping. Classes 2, 3, and 4 correspond to groups of tests, geometric objects, and
public/educational institutions, respectively. Class 5 has been selected to illustrate a
potential pitfall of using syntactic co-occurrences for semantic class induction: fund,
revenue, eyebrow, and awareness hardly belong together as a coherent conceptual class.
The reason, it seems, is that they are all entities that can be (and in the corpus, are)
raised. This class has also conflated different (but related) senses of reserve and as a
result the modifier nature is often associated with it.
An alternative approach is suggested by the model used by Pereira, Tishby, and
Lee (1993) and Rooth et al. (1999) that is formalized in Equation (25). This model can
601
Computational Linguistics Volume 40, Number 3
(Draw a distribution over topics)
? ? Dirichlet(?)
for topic z ? {1 . . . |Z|} do
(Draw a distribution over words)
?z ? Dirichlet(?)
(Draw a distribution over contexts)
?z ? Dirichlet(?)
end for
for observation oi ? O do
(Draw a topic)
zi ? Multinomial(?)
(Draw a word)
wi ? Multinomial(?zi )
(Draw a context)
ci ? Multinomial(?zi )
end for
?
?
z
wc
?
?
?
?
O ZZ
Figure 2
Generative story and plate diagram for ROOTH-LDA.
be ?Bayesianized? by placing Dirichlet priors on the component distributions; adapting
Equation (25) to our notation, the resulting joint distribution over contexts and words is
P(c, w) =
?
z
P(c|z)P(w|z)P(z) (27)
The generative story and plate diagram for this model, which was called ROOTH-LDA
in O? Se?aghdha (2010), are given in Figure 2. Whereas LDA induces classes of arguments,
ROOTH-LDA induces classes of predicate?argument interactions. Table 2 illustrates
some classes learned by ROOTH-LDA from BNC verb?object co-occurrences. One class
shows that a cost, number, risk, or expenditure can plausibly be increased, reduced, cut, or
involved; another shows that a house, building, home, or station can be built, left, visited, or
used. As with LDA, there are some over-generalizations; the fact that an eye or mouth can
be opened, closed, or shut does not necessarily entail that it can be locked or unlocked.
For many predicates, the best description of their argument distributions is one that
accounts for general semantic regularities and idiosyncratic lexical patterns. This sug-
gests the idea of combining a distribution over semantic classes and a predicate-specific
Table 2
Sample semantic classes learned by a Rooth-LDA model with |Z| = 100 trained on BNC
verb?object co-occurrences.
Class 1 Class 2 Class 3 Class 4
increase cost open door build house spend time
reduce number close eye leave building work day
cut risk shut mouth visit home wait year
involve expenditure lock window use station come hour
control demand slam gate enter church waste night
estimate pressure unlock shop include school take week
limit rate keep fire see plant remember month
cover power round book run office end life
602
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
distribution over arguments. One way of doing this is through the model depicted
in Figure 3, which we call LEX-LDA; this model defines the selectional preference
probability P(w|c) as
P(w|c) = ?cPlex(w|c) + (1 ? ?c )Pclass(w|c) (28)
= ?cPlex(w|c) + (1 ? ?c )
?
z
P(w|z)P(z|c) (29)
where ?c is a value between 0 and 1 that can be interpreted as a measure of argument
lexicalization or as the probability that an observation for context c is drawn from the
lexical distribution Plex or the class-based distribution Pclass. Pclass has the same form as
the LDA preference model. The value of ?c will vary across predicates according to how
well their argument preference can be fit by the class-based models; a predicate with
high ?c will have idiosyncratic argument patterns that are best learned by observing
that predicate?s co-occurrences in isolation. In many cases this may reflect idiomatic or
non-compositional usages, though it is also to be expected that ?c will correlate with
frequency; given sufficient data for a context, smoothing becomes less important. As
an example we trained the LEX-LDA model on BNC verb-object co-occurrences and
estimated posterior mean values for ?c for all verbs occurring more than 100 times and
taking at least 10 different object argument types. The verbs with highest and lowest
values are listed in Table 3. Although almost anything can be discussed or highlighted,
for topic z ? {1 . . . |Z|} do
(Draw a distribution over words)
?z ? Dirichlet(?)
end for
for context c ? {1 . . . |C|} do
(Draw a distribution over topics)
?c ? Dirichlet(?)
(Draw a distribution over words)
?c ? Dirichlet(?)
(Draw a lexicalization probability)
?c ? Beta(?0, ?1)
for observation oi ? O(c) do
(Draw a lexicalization indicator)
si ? Bernoulli(?c )
if si = 0 then
(Draw a topic)
zi ? Multinomial(?c)
(Draw a word)
wi ? Multinomial(?zi )
else
(Draw a word)
wi ? Multinomial(?c)
end if
end for
end for
?
?
z s
w
?
?
?
?
?
?1?0
O(c)
C
Z
Figure 3
Generative story and plate diagram for LEX-LDA.
603
Computational Linguistics Volume 40, Number 3
Table 3
BNC verbs with lowest and highest estimated lexicalization values ?c for their object arguments,
as well as the arguments with highest Plex(w|c) for high-lexicalization verbs.
Lowest ?c Highest ?c Top lexicalized arguments
discuss 1.2 ? 10?4 pose 0.872 problem, threat, question, challenge, risk
highlight 4.6 ? 10?4 wreak 0.864 havoc, vengeance, revenge, damage
consume 5.4 ? 10?4 adjourn 0.857 case, hearing, meeting, inquest, trial
emphasize 5.8 ? 10?4 reap 0.857 benefit, rewards, harvest, advantage
assert 6.5 ? 10?4 exert 0.851 influence, pressure, effect, control, force
contrast 6.5 ? 10?4 retrace 0.847 step, route, footstep, path, journey
obscure 6.8 ? 10?4 solve 0.847 problem, mystery, equation, crisis, case
document 6.8 ? 10?4 sip 0.839 coffee, tea, drink, wine, champagne
debate 6.9 ? 10?4 answer 0.826 question, call, phone, door, query
safeguard 8.0 ? 10?4 incur 0.823 cost, expense, loss, expenditure, liability
verbs such as pose and wreak have very lexicalized argument preferences. The semantic
classes learned by LEX-LDA are broadly comparable to those learned by LDA, though
it is less likely to mix classes on the basis of a single argument lexicalization; whereas
the LDA class in row 5 of Table 1 is distracted by the high-frequency collocations
nature reserve and raise eyebrow, LEX-LDA models trained on the same data can explain
these through lexicalization effects and separate out body parts, conservation areas, and
investments in different classes.
3.4 Parameter and Hyperparameter Learning
3.4.1 Learning Methods. A variety of methods are available for parameter learning in
Bayesian models. The two standard approaches are variational inference, in which
an approximation to the true distribution over parameters is estimated exactly, and
sampling, in which convergence to the true posterior is guaranteed in theory but rarely
verifiable in practice. In some cases the choice of approach is guided by the model, but
often it is a matter of personal preference; for LDA, there is evidence that equivalent
levels of performance can be achieved through variational learning and sampling given
appropriate parameterization (Asuncion et al. 2009). In this article we use learning
methods based on Gibbs sampling, following Griffiths and Steyvers (2004). The basic
idea of Gibbs sampling is to iterate through the corpus one observation at a time,
updating the latent variable value for each observation according to the conditional
probability distribution determined by the current observed and latent variable values
for all other observations. Because the likelihoods are multinomials with Dirichlet
priors, we can integrate out their parameters using Equation (21).
For LDA, the conditional probability that the latent variable for the ith observation
is assigned value z is computed as
P(zi = z|z
?i, ci, wi) ? ( fzci + ?z)
fzwi + ?
fz + |W|?
(30)
where z?i is the set of current assignments for all observations other than the ith, fz
is the number of observations in that set assigned latent variable z, fzci is the number
of observations with context ci assigned latent variable z, and fzwi is the number of
observations with word wi assigned latent variable z.
604
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
For ROOTH-LDA we make a similar calculation:
P(zi = z|z
?i, ci, wi) ? ( fz + ?z)
fzwi + ?
fz + |W|?
fzci + ?
fz + |C|?
(31)
For LEX-LDA the lexicalization variables si must also be sampled for each token.
We ?block? the sampling for zi and si to improve convergence. The Gibbs sampling
distribution is
P(si = 0, zi = z|z
?i, s?i, ci, wi) ? ( fci,s=0 + ?0)
fzci + ?z
fci,s=0 +
?
z? ?z?
fzwi + ?
fz + |W|?
(32)
P(si = 1, zi = ?|z
?i, s?i, ci, wi) ? ( fci,s=1 + ?1)
fciwi,s=1 + ?
fci,s=1 + |W|?
(33)
P(si = 0, zi = ?|z
?i, s?i, ci, wi) = 0 (34)
P(si = 1, zi = ?|z
?i, s?i, ci, wi) = 0 (35)
where ? indicates that no topic is assigned. The fact that topics are not assigned for all
tokens means that LEX-LDA is less useful in situations that require representational
power they afford?for example, the contextual similarity paradigm described in
Section 3.5.
A naive implementation of the sampler will take time linear in the number of topics
and the number of observations to complete one iteration. Yao, Mimno, and McCallum
(2009) present a new sampling algorithm for LDA that yields a considerable speedup by
reformulating Equation (30) to allow caching of intermediate values and an intelligent
sorting of topics so that in many cases only a small number of topics need be iterated
though before assigning a topic to an observation. In this article we use Yao, Mimno,
& McCallum?s algorithm for LDA, as well as a transformation of the ROOTH-LDA and
LEX-LDA samplers that can be derived in an analogous fashion.
3.4.2 Inference. As noted previously, the Gibbs sampling procedure is guaranteed to
converge to the true posterior after a finite number of iterations; however, this number
is unknown and it is difficult to detect convergence. In practice, we run the sampler
for a hopefully sufficient number of iterations and perform inference based on the
final sampling state (assignments of all z and s variables) and/or a set of intermediate
sampling states.
In the case of the LDA model, the selectional preference probability P(w|c) is
estimated using posterior mean estimates of ?c and ?z:
P(w|c) =
?
z
P(z|c)P(w|z) (36)
P(z|c) =
fzc + ?z
fc +
?
z? ?z?
(37)
P(w|z) =
fzw + ?
fz + |W|?
(38)
605
Computational Linguistics Volume 40, Number 3
For ROOTH-LDA, the joint probability P(c, w) is given by
P(c, w) =
?
z
P(c|z)P(w|z)P(z) (39)
P(z) =
fz + ?z
|O|+
?
z? ?z?
(40)
P(w|z) =
fzw + ?
fz + |W|?
(41)
P(c|z) =
fzc + ?
fz + |C|?
(42)
For LEX-LDA, P(w|c) is given by
P(w|c) = P(? = 1|c)Plex(w|c) + P(? = 0|c)Pclass(w|c) (43)
P(? = 1|c) =
fc,s=1 + ?1
fc + ?0 + ?1
(44)
P(? = 0|c) = 1 ? P(? = 1|c) (45)
Plex(w|c) =
fwc,s=1 + ?
fc,s=1 + |W|?
(46)
Pclass(w|c) =
?
z
P(z|c)P(w|z) (47)
P(z|c) =
fzc + ?z
fc,s=0 +
?
z? ?z?
(48)
P(w|z) =
fzw + ?
fz + |W|?
(49)
Given a sequence or chain of sampling states S1, . . . , Sn, we can predict a value for
P(w|c) or P(c, w) using these equations and the set of latent variable assignments at a
single state Si. As the sampler is initialized randomly and will take time to find a good
area of the search space, it is standard to wait until a number of iterations have passed
before using any samples for prediction. States S1, . . . , Sb from this burn-in period are
discarded.
For predictive stability it can be beneficial to average over predictions computed
from more than one sampling state; for example, we can produce an averaged estimate
of P(w|c) from a set of states S:
P(w|c) = 1
|S|
?
Si?S
PSi (w|c) (50)
It is also possible to average over states drawn from multiple chains. However,
averaging of any kind can only be performed on quantities whose interpretation does
not depend on the sampling state itself. For example, we cannot average over estimates
of P(z1|c) drawn from different samples as the topic called z1 in one iteration is not
identical to the topic called z1 in another; even within the same chain, the meaning of
a topic will often change gradually from state to state.
606
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
3.4.3 Choosing |Z|. In the ?parametric? latent variable models used here the number
of topics or semantic classes, |Z|, must be fixed in advance. This brings significant
efficiency advantages but also the problem of choosing an appropriate value for |Z|. The
more classes a model has, the greater its capacity to capture fine distinctions between
entities. However, this finer granularity inevitably comes at a cost of reduced general-
ization. One approach is to choose a value that works well on training or development
data before evaluating held-out test items. Results in lexical semantics are often reported
over the entirety of a data set, meaning that if we wish to compare those results we
cannot hold out any portion. If the method is relatively insensitive to the parameter it
may be sufficient to choose a default value. Rooth et al. (1999) suggest cross-validating
on the training data likelihood (and not on the ultimate evaluation measure). An alter-
native solution is to average the predictions of models trained with different choices
of |Z|; this avoids the need to pick a default and can give better results than any one
value as it integrates contributions at different levels of granularity. As mentioned in
Section 3.4.2 we must take care when averaging predictions to compute with quan-
tities that do not rely on topic identity?for example, estimates of P(a|p) can safely be
combined whereas estimates of P(z1|p) cannot.
3.4.4 Hyperparameter Estimation. Although the likelihood parameters can be integrated
out, the parameters for the Dirichlet and Beta priors (often referred to as ?hyperparame-
ters?) cannot and must be specified either manually or automatically. The value of these
parameters affects the sparsity of the learned posterior distributions. Furthermore, the
use of an asymmetric prior (where not all its parameters have equal value) implements
an assumption that some observation values are more likely than others before any
observations have been made. Wallach, Mimno, and McCallum (2009) demonstrate that
the parameterization of the Dirichlet priors in an LDA model has a material effect
on performance, recommending in conclusion a symmetric prior on the ?emission?
likelihood P(w|z) and an asymmetric prior on the document topic likelihoods P(z|d). In
this article we follow these recommendations and, like Wallach, Mimno, and McCallum,
we optimize the relevant hyperparameters using a fixed point iteration to maximize
the log evidence (Minka 2003; Wallach 2008).
3.5 Measuring Similarity in Context with Latent-Variable Models
The representation induced by latent variable selectional preference models also allows
us to capture the disambiguatory effect of context. Given an observation of a word in
a context, we can infer the most probable semantic classes to appear in that context
and we can also infer the probability that a class generated the observed word. We
can also estimate the probability that the semantic classes suggested by the observation
would have licensed an alternative word. Taken together, these can be used to estimate
in-context semantic similarity. The fundamental intuitions are similar to those behind
the vector-space models in Section 2.3.2, but once again we are viewing them from the
perspective of probabilistic modeling.
The basic idea is that we identify the similarity between an observed term wo and an
alternative term ws in context C with the similarity between the probability distribution
over latent variables associated with wo and C and the probability distribution over
latent variables associated with ws:
sim(wo, ws|C) = sim(P(z|wo, C), P(z|ws)) (51)
607
Computational Linguistics Volume 40, Number 3
This assumes that we can associate a distribution over the same set of latent variables
with each context item c ? C. As noted in Section 2.3.2, previous research has found that
conditioning the representation of both the observed term and the candidate substitute
on the context gives worse performance than conditioning the observed term alone; we
also found a similar effect. Dinu and Lapata (2010) present a specific version of this
framework, using a window-based definition of context and the assumption that the
similarity given a set of contexts is the product of the similarity value for each context:
simDL10(wo, ws|C) =
?
c?C
sim(P(z|wo, c), P(z|ws)) (52)
In this article we generalize to syntactic as well as window-based contexts and also
derive a well-motivated approach to incorporating multiple contexts inside the prob-
ability model; in Section 4.5 we show that both innovations contribute to improved
performance on a lexical substitution data set.
The distributions we use for prediction are as follows. Given an LDA latent variable
preference model that generates words given a context, it is straightforward to calculate
the distribution over latent variables conditioned on an observed context?word pair:
PC?T(z|wo, c) =
P(wo|z)P(z|c)
?
z? P(wo|z
?)P(z?|c)
(53)
Given a set of multiple contexts C, each of which has an opinion about the distribution
over latent variables, this becomes
P(z|wo, C) =
P(wo|z)P(z|C)
?
z? P(wo|z
?)P(z?|C)
(54)
P(z|C) =
?
c?C P(z|c)
?
z?
?
c?C P(z
?
|c)
(55)
The uncontextualized distribution P(z|ws) is not given directly by the LDA model. It
can be estimated from relative frequencies in the Gibbs sampling state; we use an
unsmoothed estimate.8 We denote this model C ? T to note that the target word is
generated given the context.
Where the context?word relationship is asymmetric (as in the case of syntactic
dependency contexts), we can alternatively learn a model that generates contexts given
a target word; we denote this model T ? C:
PT?C(z|wo, c) =
P(z|wo)P(c|z)
?
z? P(z
?
|wo)P(c|z?)
(56)
Again, we can generalize to non-singleton context sets:
P(z|wo, C) =
P(z|wo)P(C|z)
?
z? P(z
?
|wo)P(C|z?)
(57)
8 In the notation of Section 3.4, this estimate is given by fzwsfws
.
608
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
where
P(C|z) =
?
c?C
P(c|z) (58)
Equation (57) has the form of a ?product of experts? model (Hinton 2002), though
unlike many applications of such models we train the experts independently and thus
avoid additional complexity in the learning process. The uncontextualized distribution
P(z|ws) is an explicit component of the T ? C model.
An analogous definition of similarity can be derived for the ROOTH-LDA model.
Here there is no asymmetry as the context and target are generated jointly. The distri-
bution over topics for a context c and target word wo is given by
PROOTH-LDA (z|wo, c) =
P(wo, c|z)P(z)
?
z? P(wo, c|z
?)P(z?)
(59)
while calculating the uncontextualized distribution P(z|ws) requires summing over the
set of possible contexts C?:
PROOTH-LDA (z|ws) =
P(z)
?
c??C? P(ws, c|z)
?
z? P(z
?)
?
c??C? P(ws, c|z
?)
(60)
Because the interaction classes learned by ROOTH-LDA are specific to a relation type,
this model is less applicable than LDA to problems that involve a rich context set C.
Finally, we must choose a measure of similarity between probability distributions.
The information theory literature has provided many such measures; in this article we
use the Bhattacharyya coefficient (Bhattacharyya 1943):
simbhatt(Px(z), Py(z)) =
?
z
?
Px(z)Py(z) (61)
One could alternatively use similarities derived from probabilistic divergences such
as the Jensen?Shannon Divergence or the L1 distance (Lee 1999; O? Se?aghdha and
Copestake 2008).
3.6 Related Work
As related earlier, non-Bayesian mixture or latent-variable approaches to co-occurrence
modeling were proposed by Pereira, Tishby, and Lee (1993) and Rooth et al. (1999).
Blitzer, Globerson, and Pereira (2005) describe a co-occurrence model based on a
different kind of distributed latent-variable architecture similar to that used in the
literature on neural language models. Brody and Lapata (2009) use the clustering effects
of LDA to perform word sense induction. Vlachos, Korhonen, and Ghahramani (2009)
use non-parametric Bayesian methods to cluster verbs according to their co-occurrences
with subcategorization frames. Reisinger and Mooney (2010, 2011) have also
investigated Bayesian methods for lexical semantics in a spirit similar to that adopted
here. Reisinger and Mooney (2010) describe a ?tiered clustering? model that, like LEX-
LDA, mixes a cluster-based preference model with a predicate-specific distribution over
609
Computational Linguistics Volume 40, Number 3
words; however, their model does not encourage sharing of classes between different
predicates. Reisinger and Mooney (2011) propose a very interesting variant of the latent-
variable approach in which different kinds of contextual behavior can be explained
by different ?views,? each of which has its own distribution over latent variables; this
model can give more interpretable classes than LDA for higher settings of |Z|.
Some extensions of the LDA topic model incorporate local as well as document
context to explain lexical choice. Griffiths et al. (2004) combine LDA and a hidden
Markov model (HMM) in a single model structure, allowing each word to be drawn
from either the document?s topic distribution or a latent HMM state conditioned on the
preceding word?s state; Moon, Erk, and Baldridge (2010) show that combining HMM
and LDA components can improve unsupervised part-of-speech induction. Wallach
(2006) also seeks to capture the influence of the preceding word, while at the same time
generating every word from inside the LDA model; this is achieved by conditioning
the distribution over words on the preceding word type as well as on the chosen
topic. Boyd-Graber and Blei (2008) propose a ?syntactic topic model? that makes topic
selection conditional on both the document?s topic distribution and on the topic of the
word?s parent in a dependency tree. Although these models do represent a form of
local context, they either use a very restrictive one-word window or a notion of syntax
that ignores lexical or dependency-label effects; for example, knowing that the head of
a noun is a verb is far less informative than knowing that the noun is the direct object
of eat.
More generally, there is a connection between the models developed here and
latent-variable models used for parsing (e.g., Petrov et al. 2006). In such models
each latent state corresponds to a ?splitting? of a part-of-speech label so as to pro-
duce a finer-grained grammar and tease out intricacies of word?rule ?co-occurrence.?
Finkel, Grenager, and Manning (2007) and Liang et al. (2007) propose a non-parametric
Bayesian treatment of state splitting. This is very similar to the motivation behind an
LDA-style selectional preference model. One difference is that the parsing model must
explain the parse tree structure as well as the choice of lexical items; another is that in the
selectional preference models described here each head?dependent relation is treated
as an independent observation (though this could be changed). These differences allow
our selectional preference models to be trained efficiently on large corpora and, by fo-
cusing on lexical choice rather than syntax, to home in on purely semantic information.
Titov and Klementiev (2011) extend the idea of latent-variable distributional modeling
to do ?unsupervised semantic parsing? and reason about classes of semantically similar
lexicalized syntactic fragments.
4. Experiments
4.1 Training Corpora
In our experiments we use two training corpora:
BNC the written component of the British National Corpus,9 comprising around
90 million words. The corpus was tagged for part of speech, lemmatized, and
parsed with the RASP toolkit (Briscoe, Carroll, and Watson 2006).
9 http://www.natcorp.ox.ac.uk/.
610
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
COORDINATION:
Cats and
c:conj:n

c:conj:n

dogs run
v:ncsubj:n

?
Cats and dogs

n:and:n

run

v:ncsubj:n

PREDICATION:
The cat is
v:ncsubj:n

v:xcomp:j

fierce
?
The cat
n:ncmod:j

is fierce
PREPOSITIONS:
The cat
n:ncmod:i

in
i:dobj:n

the hat
?
The cat
n:prep in:n

in the hat
Figure 4
Dependency graph preprocessing.
WIKI a Wikipedia dump of over 45 million sentences (almost 1 billion words) tagged,
lemmatized, and parsed with the C+C toolkit10 and the fast CCG parser described
by Clark et al. (2009).
Although two different parsers were used, they both have the ability to output gram-
matical relations in the RASP format and hence they are interoperable for our purposes
as downstream users. This allows us to construct a combined corpus by simply concate-
nating the BNC and WIKI corpora.
In order to train our selectional preference models, we extracted word?context
observations from the parsed corpora. Prior to extraction, the dependency graph for
each sentence was transformed using the preprocessing steps illustrated in Figure 4.
We then filtered for semantically discriminative information by ignoring all words with
part of speech other than common noun, verb, adjective, and adverb. We also ignored
instances of the verbs be and have and discarded all words containing non-alphabetic
characters and all words with fewer than three characters.11
As mentioned in Section 2.1, the distributional semantics framework admits flex-
ibility in how the practitioner defines the context of a word w. We investigate two
possibilities in this article:
Syn The context of w is determined by the syntactic relations r and words w? incident
to it in the sentence?s parse tree, as illustrated in Section 3.1.
10 http://svn.ask.it.usyd.edu.au/trac/candc.
11 An exception was made for the word PC as it appears in the Keller and Lapata (2003) data set used
for evaluation.
611
Computational Linguistics Volume 40, Number 3
Win5 The context of w is determined by the words appearing within a window of five
words on either side of it. There are no relation labels, so there is essentially just
one relation r to consider.
Training topic models on a data set with very large ?documents? leads to tractability
issues. The window-based approach is particularly susceptible to an explosion in the
number of extracted contexts, as each token in the data can contribute 2 ? W word?
context observations, where W is the window size. We reduced the data by applying
a simple downsampling technique to the training corpora. For the WIKI/Syn corpus,
all word?context counts were divided by 5 and rounded to the nearest integer. For
the WIKI/Win5 corpus we divided all counts by 70; this number was suggested by
Dinu and Lapata (2010), who used the same ratio for downsampling the similarly sized
English Gigaword Corpus. Being an order of magnitude smaller, the BNC required
less pruning; we divided all counts in the BNC/Win5 by 5 and left the BNC/Syn
corpus unaltered. Type/token statistics for the resulting sets of observations are given
in Table 4.
4.2 Evaluating Selectional Preference Models
Various approaches have been suggested in the literature for evaluating selectional
preference models. One popular method is ?pseudo-disambiguation,? in which a sys-
tem must distinguish between actually occurring and randomly generated predicate?
argument combinations (Pereira, Tishby, and Lee 1993; Chambers and Jurafsky 2010).
In a similar vein, probabilistic topic models are often evaluated by measuring the
probability they assign to held-out data; held-out likelihood has also been used for
evaluation in a task involving selectional preferences (Schulte im Walde et al. 2008).
These two approaches take a ?language modeling? approach in which model quality
is identified with the ability to predict the distribution of co-occurrences in unseen text.
Although this metric should certainly correlate with the semantic quality of the model, it
may also be affected by frequency and other idiosyncratic aspects of language use unless
tightly controlled. In the context of document topic modeling, Chang et al. (2009) find
that a model can have better predictive performance on held-out data while inducing
topics that human subjects judge to be less semantically coherent.
In this article we choose to evaluate models by comparing system predictions
with semantic judgments elicited from human subjects. These judgments take various
forms. In Section 4.3 we use judgments of how plausible it is that a given predicate
takes a given word as its argument. In Section 4.4 we use judgments of similarity
Table 4
Type and token counts for the BNC and BNC+WIKI corpora.
BNC BNC+WIKI
Tokens Types Contexts Tokens Types Contexts
Nouns 18,723,082 122,999 316,237 54,145,216 106,448 514,257
Verbs 7,893,462 18,494 57,528 20,082,658 16,673 82,580
Adjectives 4,385,788 73,684 37,163 11,536,424 88,488 57,531
Adverbs 1,976,837 7,124 14,867 3,017,936 4,056 18,510
Window5 28,329,238 88,265 102,792 42,828,094 139,640 143,443
612
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
between pairs of predicate?argument combinations. In Section 4.5 we use judgments
of substitutability for a target word as disambiguated by its sentential context. Taken
together, these different experimental designs provide a multifaceted analysis of model
quality.
4.3 Predicate?Argument Plausibility
4.3.1 Data. For the plausibility-based evaluation we use a data set of human judgments
collected by Keller and Lapata (2003). This comprises data for three grammatical re-
lations: verb?object, adjective?noun, and noun?noun modification. For each relation,
30 predicates were selected; each predicate was paired with three noun arguments
from different predicate?argument frequency bands in the BNC as well as three noun
arguments that were not observed for that predicate in the BNC. In this way two
subsets (Seen and Unseen) of 90 items each were assembled for each predicate. Human
plausibility judgments were elicited from a large number of subjects; these numeri-
cal judgments were then normalized, log-transformed, and averaged in a Magnitude
Estimation procedure.
Predicate Seen Unseen
dredge channel 0.1875 legend ?0.3221
dredge canal 0.2388 sheet ?0.2486
dredge rubbish ?0.1999 survivor ?0.2077
Following Keller and Lapata (2003), we evaluate our models by measuring
the correlation between system predictions and the human judgments. Keller and
Lapata use Pearson?s correlation coefficient r; we additionally use Spearman?s rank
correlation coefficient ? for a non-parametric evaluation. Each system prediction is
log-transformed before calculating the correlation to improve the linear fit to the gold
standard.
4.3.2 Methods. We evaluate the LDA, ROOTH-LDA, and LEX-LDA latent-variable pref-
erence models, trained on predicate?argument pairs (c, w) extracted from the BNC.
We use a default setting |Z| = 100 for the number of classes; in our experiments we
have observed that our Bayesian models are relatively robust to the choice of |Z|. We
average predictions of the joint probability P(c, w) over three independent samples, each
of which is obtained by sampling P(c, w) every 50 iterations after a burn-in period of
200 iterations. ROOTH-LDA gives joint probabilities by definition (25), but LDA and
LEX-LDA are defined in terms of conditional probabilities (24). There are two options
for training these models:
P ? A: Model the distribution P(w|c) over arguments for each predicate.
A ? P: Model the distribution P(c|w) over predicates for each argument.
As the descriptions suggest, the definition of ?predicate? and ?argument? is arbitrary;
it is equally valid to talk of the selectional preference of a noun for verbs taking it as
a direct object as it is to talk of the preference of a verb for nouns taking it as a direct
object. We expect both configurations to perform comparably on average, though there
613
Computational Linguistics Volume 40, Number 3
may be linguistic or conceptual reasons why one configuration is better than the other
for specific classes of co-occurrence.
To convert conditional probabilities to joint probabilities we multiply by a relative-
frequency (MLE) estimate of the probability of the conditioning term:
PP?A = P(w|c)P(c) (62)
PA?P = P(c|w)P(w) (63)
As well as evaluating P ? A and A ? P implementations of LDA and LEX-LDA,
we can evaluate a combined model P ? A that simply averages the two sets of
predictions; this removes the arbitrariness involved in choosing one direction or the
other.
For comparison, we report the performance figures given by Keller and Lapata
for their search-engine method using AltaVista and Google12 as well as a number of
alternative methods that we have reimplemented and trained on identical data:
BNC (MLE) A maximum-likelihood estimate proportional to the co-occurrence fre-
quency f (c, w) in the parsed BNC.
BNC (KN) BNC relative frequencies smoothed with modified Kneser-Ney (Chen and
Goodman 1999).
Resnik The WordNet-based association strength of Resnik (1993). We used WordNet
version 2.1 as the method requires multiple roots in the hierarchy for good
performance.
Clark/Weir The WordNet-based method of Clark and Weir (2002), using WordNet 3.0.
This method requires that a significance threshold ? and significance test be
chosen; we investigated a variety of settings and report performance for ? = 0.9
and Pearson?s ?2 test, as this combination consistently gave the best results.
Rooth-EM Rooth et al. (1999)?s latent-variable model without priors, trained with EM.
As for the Bayesian models, we average the predictions over three iterations. This
method is very sensitive to the number of classes; as proposed by Rooth et al.,
we choose the number of classes from the range (20, 25, . . . , 50) through 5-fold
cross-validation on a held-out log-likelihood measure.
EPP The vector-space method of Erk, Pado?, and Pado? (2010), as described in Sec-
tion 2.2.3. We used the cosine similarity measure for smoothing as it performed
well in Erk, Pado?, & Pado??s experiments.
Disc A discriminative model inspired by Bergsma, Lin, and Goebel (2008) (see Sec-
tion 2.2.4). In order to get true probabilistic predictions, we used a logistic regres-
sion classifier with L1 regularization rather than a Support Vector Machine.13
We train one classifier per predicate in the Keller and Lapata data set. Following
Bergsma, Lin, and Goebel, we generate pseudonegative instances for each
predicate by sampling noun arguments that either do not co-occur with it or have
a negative PMI association. Again following Bergsma, Lin, and Goebel, we use
a ratio of two pseudonegative instances for each positive instance and require
pseudonegative arguments to be in the same frequency quintile as the matched
12 Keller and Lapata only report Pearson?s r correlations; as we do not have their per-item predictions we
cannot calculate Spearman?s ? correlations or statistical significance scores.
13 We used the logistic regression implementation provided by LIBLINEAR (Fan et al. 2008), available at
http://www.csie.ntu.edu.tw/~cjlin/liblinear.
614
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
observed argument. The features used for each data instance, corresponding
to an argument, are: the conditional probability of the argument co-occurring
with each predicate in the training data; and string-based features capturing the
length and initial and final character n-grams of the argument word.14 We also
investigate whether our LDA model can be used to provide additional features
for the discriminative model, by giving the index of the most probable class
maxz P(z|c, w); results for this system are labeled Disc+LDA.
In order to test statistical significance of performance differences we use a test for
correlated correlation coefficients proposed by Meng, Rosenthal, and Rubin (1992). This
is more appropriate than a standard test for independent correlation coefficients as it
takes into account the strength of correlation between two sets of system outputs as
well as each output?s correlation with the gold standard. Essentially, if the two sets of
system outputs are correlated there is less chance that their difference will be deemed
significant. As we have no a priori reason to believe that one model will perform better
than another, all tests are two-tailed.
4.3.3 Results. Results on the Keller and Lapata (2003) plausibility data set are presented
in Table 5.15 For common combinations (the Seen data) it is clear that relative corpus
frequency is a reliable indicator of plausibility, especially when Web-scale resources are
available. The BNC MLE estimate outperforms the best selectional preference model
on three out of six Seen evaluations, and the AltaVista and Google estimates from
Keller and Lapata (2003) outperforms the best selectional preference model on every
applicable Seen evaluation. For the rarer Unseen combinations, however, MLE esti-
mates are not sufficient and the latent-variable selectional preference models frequently
outperform even the Web-based predictions. The results for BNC(KN) improve on the
MLE estimates for the Unseen data but do not match the models that have a semantic
component.
It is clear from Table 5 that the new Bayesian latent-variable models outperform
the previously proposed selectional preference models under almost every evaluation.
Among the latent-variable models there is no one clear winner, and small differences
in performance are as likely to arise through random sampling variation as through
qualitative differences between models. That said, ROOTH-LDA and LEX-LDA do score
higher than LDA in a majority of cases. As expected, the bidirectional P ? A models
tend to perform at around the midpoint of the P ? A and A ? P models, though they
can also exceed both; this suggests that they are a good choice when there is no intuitive
reason to choose one direction over the other.
Table 6 aggregates comparisons for all combinations of the six data sets and two
evaluation measures. As before, all the Bayesian latent-variable models achieve a
roughly similar level of performance, consistently outperforming the models selected
from the literature and frequently reaching statistical significance (p < 0.05). These
results confirm that LDA-style models can be considered the current state of the art
for selectional preference modeling.
14 Bergsma, Lin, and Goebel (2008) also use features extracted from gazetteers. However, they observe that
additional features only give a small improvement over co-occurrence features alone. We do not use such
features here but hypothesize that the improvement would be even smaller in our experiments as the
data do not contain proper nouns.
15 Results for LDAP?A and ROOTH-LDA were previously published in O? Se?aghdha (2010).
615
Computational Linguistics Volume 40, Number 3
Table 5
Results (Pearson r and Spearman ? correlations) on Keller and Lapata?s (2003) plausibility data.
Asterisks denote performance figures that are taken from the source paper; all other figures are
drawn from our own (re)implementation trained on identical data.
Verb?object Noun?noun Adjective?noun
Seen Unseen Seen Unseen Seen Unseen
r ? r ? r ? r ? r ? r ?
AltaVista* .641 ? .551 ? .700 ? .578 ? .650 ? .480 ?
Google* .624 ? .520 ? .692 ? .595 ? .641 ? .473 ?
BNC (MLE) .620 .614 .196 .222 .544 .604 .114 .125 .543 .622 .135 .102
BNC (KN) .615 .614 .327 .350 .543 .594 .485 .523 .510 .619 .179 .173
Resnik .384 .473 .469 .470 .242 .187 .152 .037 .309 .388 .311 .280
Clark/Weir .489 .546 .312 .365 .441 .521 .543 .576 .440 .476 .271 .242
ROOTH-EM .455 .487 .479 .520 .503 .491 .586 .625 .514 .463 .395 .355
EPP .541 .562 .403 .436 .382 .465 .377 .398 .401 .400 .260 .195
Disc .318 .318 .376 .354 .331 .294 .258 .250 .188 .274 .303 .327
Disc+LDA .328 .338 .473 .476 .308 .285 .266 .292 .228 .308 .333 .368
LDAP?A .504 .541 .558 .603 .615 .641 .636 .666 .594 .558 .468 .459
LDAA?P .514 .555 .448 .469 .623 .652 .648 .688 .547 .583 .465 .458
LDAP?A .513 .546 .530 .542 .619 .645 .653 .697 .593 .570 .467 .445
ROOTH-LDA .520 .548 .564 .605 .607 .622 .691 .722 .575 .599 .501 .469
LEX-LDAP?A .570 .600 .601 .662 .511 .537 .677 .706 .600 .627 .465 .451
LEX-LDAA?P .568 .572 .523 .542 .532 .568 .659 .703 .545 .623 .513 .477
LEX-LDAP?A .575 .589 .560 .599 .553 .563 .669 .698 .572 .629 .517 .497
Human* .604 ? .640 ? .641 ? .570 ? .630 ? .550 ?
Table 6
Aggregate comparisons for the Keller and Lapata (2003) plausibility data set between
latent-variable models (rows) and previously proposed selectional preference models (columns).
Cell entries give the number of evaluations (out of 12) in which the latent-variable model
outperformed the alternative method and the number in which the improvement was
statistically significant.
Resnik Clark/Weir ROOTH-EM EPP Disc
LDAP?A 12/5 11/8 12/6 10/5 12/4
LDAA?P 10/4 12/8 10/5 10/3 12/4
LDAP?A 12/4 11/9 12/6 10/5 12/5
ROOTH-LDA 12/6 12/7 12/7 10/5 12/5
LEX-LDAP?A 12/5 11/8 12/6 12/5 12/5
LEX-LDAA?P 10/4 12/8 10/5 12/5 12/6
LEX-LDAP?A 12/4 11/9 12/6 12/5 12/5
616
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
One way of performing error analysis for a given result is to decompose the cor-
relation coefficient into a sum of per-item ?pseudo-coefficients.? For Pearson?s r, the
contribution for the ith item is
ri =
(xi ? ?x)(yi ? ?y)
?
?
j(xj ? ?x)
2
?
?
j(yj ? ?y)
2
(64)
Spearman?s ? is equivalent to the r correlation between ranks and so a similar quantity
can be computed. Table 7 illustrates the items with highest and lowest contributions
for one evaluation (Spearman?s ? on the Keller and Lapata Unseen data set). We have
attempted to identify general factors that predict the difficulty of an item by measuring
rank correlation between the per-item pseudo-coefficients and various corpus statis-
tics. However, it has proven difficult to isolate reliable patterns. One finding is that
arguments with high corpus frequency tend to incur larger errors for the P ? A latent-
variable models and ROOTH-LDA, whereas predicates with high corpus frequency tend
to incur smaller errors; with the A ? P the effect is lessened but not reversed, suggesting
that part of the effect may be inherent in the data set rather than in the prediction model.
4.4 Predicate?Argument Similarity
4.4.1 Data. Mitchell and Lapata (2008, 2010) collected human judgments of similarity
between pairs of predicates and arguments corresponding to minimal sentences.
Mitchell and Lapata?s explicit aim was to facilitate evaluation of general semantic
compositionality models but their data sets are also suitable for evaluating predicate?
argument representations.
Mitchell and Lapata (2008) used the BNC to extract 4 attested subject nouns for each
of 15 verbs, yielding 60 reference combinations. Each verb?noun tuple was matched
with two verbs that are synonyms of the reference verb in some contexts but not in
Table 7
Most- and least-accurately predicted items for the LDAP?A models using per-item Spearman?s ?
pseudo-coefficients on the unseen data set, with gold and predicted rank values.
Item ri Gold Pred Item ri Gold Pred
influence worker 0.030 3 2 spend life ?0.012 63 1
originate miner 0.029 89 86 rank pc ?0.012 21 75
undergo container 0.027 90 83 deduct stage ?0.011 79 25
litter surface 0.027 7 3 sponsor embassy ?0.010 23 73
injure pilot 0.026 2 9 spend error ?0.007 80 30
desk tomato 0.028 87 87 guitar conviction ?0.012 82 25
pupil morale 0.026 3 9 towel fee ?0.011 11 65
landlord committee 0.025 12 1 workshop victim ?0.007 18 60
restoration specialist 0.025 1 12 opera recommendation ?0.006 6 54
cable manager 0.024 7 8 valuation afternoon ?0.005 70 32
superb character 0.032 2 1 tremendous newspaper ?0.014 13 72
scientific document 0.032 1 2 continuous clinic ?0.012 75 21
valid silk 0.031 89 89 lazy promoter ?0.012 24 79
naughty protocol 0.026 84 87 unfair coalition ?0.012 20 73
exciting can 0.026 87 84 lazy shadow ?0.010 74 24
617
Computational Linguistics Volume 40, Number 3
Table 8
Sample items from the Mitchell and Lapata (2008) data set.
shoulder slump 6, 7, 5, 5, 6, 5, 5, 7, 5, 5, 7, 5, 6, 6, 5, 6, 6, 6, 7, 5,
7, 6, 6, 5, 5, 5, 5, 6, 6, 7, 7, 7, 7, 7shoulder slouch
shoulder slump 2, 5, 4, 4, 3, 3, 2, 3, 2, 1, 3, 3, 6, 5, 3, 2, 1, 1, 1, 7,
4, 4, 6, 3, 5, 6shoulder decline
Table 9
Sample items from the Mitchell and Lapata (2010) data set.
stress importance 6, 7, 7, 5, 5, 7, 7, 7, 6, 5, 6, 7, 3, 7, 7, 6, 7, 7emphasize need
ask man 3, 1, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1stretch arm
football club 7, 6, 7, 6, 6, 5, 5, 3, 6, 6, 4, 5, 4, 6, 2, 7, 5, 5league match
education course 7, 7, 5, 5, 7, 5, 5, 7, 7, 4, 6, 2, 5, 6, 6, 7, 7, 4training program
others. In this way, Mitchell and Lapata created a data set of 120 pairs of predicate?
argument combinations. Similarity judgments were obtained from human subjects for
each pair on a Likert scale of 1?7. Examples of the resulting data items are given in
Table 8. Mitchell and Lapata use six subjects? ratings as a development data set for
setting model parameters and the remaining 54 subjects? ratings for testing. In this
article we use the same split.
Mitchell and Lapata (2010) adopt a similar approach to data collection with the dif-
ference that instead of keeping arguments constant across combinations in a pair, both
predicates and arguments vary across comparand combinations. They also consider a
range of grammatical relations: verb?object, adjective?noun, and noun?noun modifica-
tion. Human subjects rated similarity between predicate?argument combinations on a
1?7 scale as before; examples are given in Table 9. Inspection of the data suggests that
the subjects? annotation may conflate semantic similarity and relatedness; for example,
football club and league match are often given a high similarity score. Mitchell and Lapata
again split the data into development and testing sections, the former comprising 54
subjects? ratings and the latter comprising 108 subjects? ratings.
Turney (2012) reports, on the basis of personal communication, that Mitchell and
Lapata (2010) used an involved evaluation procedure that is not described in their
original paper; for each grammatical relation, the annotators are partitioned in three
groups and the Spearman?s ? correlation computed for each group is combined by av-
eraging.16 The analogous approach for the Mitchell and Lapata (2008) data set calculates
a single ? value by pairing of each annotator-item score with the system prediction for
the appropriate item. Let s be the sequence of system predictions for |I| items and ya
16 We do not compare against the system of Turney (2012) as Turney uses a different experimental design
based on partitioning by phrases rather than annotators.
618
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
be the scores assigned by annotator a ? A to those |I| items. Then the ?concatenated?
correlation ?cat is calculated as follows:17
ycat = (ya1i1 , ya1i2 , . . . , ya1i
|I|
, ya2i1 . . . , ya
|A|i|I| ) (65)
scat = (s1, s2, . . . , s
|I|, s1, . . . , s|I|) (66)
?cat = ?(scat, ycat) (67)
The length of the ycat and scat sequences is equal to the total number of annotator-item
scores. For the Mitchell and Lapata (2010) data set, a ?cat value is calculated for each
of the three annotator groups and these are then averaged. As Turney observes, this
approach seems to have the effect of underestimating model quality relative to the inter-
annotator agreement figure, which is calculated as average intersubject correlation.
Therefore, in addition to Mitchell and Lapata?s ?cat evaluation, we also perform an
evaluation that computes the average correlation ?ave between the system output and
each individual annotator:
?ave =
1
|A|
?
a?A
?(s, ya) (68)
4.4.2 Models. For the Mitchell and Lapata (2008) data set we train the following models
on the BNC corpus:
LDA An LDA selectional preference model of verb?subject co-occurrence with simi-
larity computed as described in Section 3.5. Similarity predictions sim(n, o|c) are
averaged over five runs. We consider three models of context?target interaction,
which in this case corresponds to verb?subject interaction:
LDAC?T Target generation is conditioned on the context, as in equation (53).
LDAT?C Context generation is conditioned on the target, as in equation (56).
LDAC?T An average of the predictions made by LDAC?T and LDAT?C.
As before, we consider a default setting of |Z| = 100. As well as presenting results
for an average over all predictors we investigate whether the choice of predictors
can be optimized by using the development data to select the best subset of
predictors.
Mult Pointwise multiplication (6) using Win5 co-occurrences.
We also compare against the best figures reported in previous studies; these also used
the BNC for training and so should be directly comparable:
M+L08 The best-performing system of Mitchell and Lapata (2008), combining an addi-
tive and a multiplicative model and using window-based co-occurrences.
SVS The best-performing system of Erk and Pado? (2008); the Structured Vector Space
model (8), parameterized to use window-based co-occurrences and raising the
expectation vector values (7) to the 20th power (this parameter was optimized
on the development data).
17 In practice the sequence of items is not the same for every annotator and the sequence of predictions s
must be changed accordingly.
619
Computational Linguistics Volume 40, Number 3
Table 10
Results (?ave averaged across annotators) for the Mitchell and Lapata (2008) similarity data set.
Model No Optimization Optimized on Dev
|Z| = 100
LDAC?T 0.34 0.35
LDAT?C 0.39 0.41
LDAC?T 0.39 0.41
Mult 0.15 ?
Human* 0.40 ?
For the Mitchell and Lapata (2010) data set we train the following models, again on
the BNC corpus:
ROOTH-LDA/Syn A ROOTH-LDA model trained on the appropriate set of syntactic
co-occurrences (verb?object, noun?noun modification, or adjective?noun), with
the topic distribution calculated as in Equation (59).
LDA/Win5 An LDA model trained on the Win5 window-based co-occurrences. Be-
cause all observations are modeled using the same latent classes, the distributions
P(z|o, c) (Equation (53)) for each word in the pair can be combined by taking a
normalized product.
Combined This model averages the similarity prediction of the ROOTH-LDA/Syn and
LDA/Win5 models.
Mult Pointwise multiplication (6) using Win5 co-occurrences.
We report results for an average over all predictors as well as for the subset that per-
forms best on the development data. We also list results that were reported by Mitchell
and Lapata:
M+L10/Mult A multiplicative model (6) using a vector space based on window co-
occurrences in the BNC.
M+L10/Best The best result for each grammatical relation from any of the semantic
spaces and combination methods tested by Mitchell and Lapata. Some of these
methods require parameters to be set through optimization on the development
set.18
4.4.3 Results. Results for the Mitchell and Lapata (2008) data set are presented in
Tables 10 and 11.19 The LDA preference models clearly outperform the previous state
of the art of ?cat = 0.27 (Erk and Pado? 2008), with the best simple average of predictors
scoring ?cat = 0.38, ?ave = 0.41, and the best optimized combination scoring ?cat = 0.39,
?ave = 0.41. This is comparable to the average level of agreement between human judges
estimated by Mitchell and Lapata?s to be ?ave = 0.40. Optimizing on the development
data consistently gave better performance than averaging over all predictors, though
in most cases the differences are small.
18 Ultimately, however, none of the combination methods needing optimization outperform the
parameter-free methods in Mitchell and Lapata?s results.
19 The results in Table 10 were previously published in O? Se?aghdha and Korhonen (2011).
620
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
Table 11
Results (?cat) for the Mitchell and Lapata (2008) similarity data set.
Model No Optimization Optimized on Dev
|Z| = 100
LDAC?T 0.28 0.32
LDAT?C 0.38 0.39
LDAC?T 0.33 0.38
Mult 0.13 ?
SVS* 0.27 ?
M+L08* 0.19 ?
Human* 0.40 ?
Table 12
Results (?ave averaged across annotators) for the Mitchell and Lapata (2010) similarity data set.
Model No Optimization Optimized on Dev
V?Obj N?N Adj?N V?Obj N?N Adj?N
LDA/Win5 0.41 0.56 0.46 0.42 0.58 0.49
ROOTH-LDA/Syn 0.42 0.46 0.51 0.42 0.47 0.52
Combined 0.44 0.56 0.53 0.46 0.58 0.55
Mult/Win5 0.34 0.33 0.34 ? ? ?
Human* 0.55 0.49 0.52 ? ? ?
Results for the Mitchell and Lapata (2010) data set are presented in Tables 12 and
Table 13.20 Again the latent-variable models perform well, comfortably outperforming
the Mult baseline, and with just one exception the Combined models surpass Mitchell
and Lapata?s reported results. Combining the syntactic co-occurrence model ROOTH-
LDA/Syn and the window-based model LDA/Win5 consistently gives the best perfor-
mance, suggesting that the human ratings in this data set are sensitive to both strict
similarity and a looser sense of relatedness. As Turney (2012) observes, the average-?cat-
per-group approach of Mitchell and Lapata leads to lower performance figures than
averaging across annotators; with the latter approach (Table 12) the ?ave correlation
values approach the level of human interannotator agreement for two of the three
relations: noun?noun and adjective?noun modification.
4.5 Lexical Substitution
4.5.1 Data. The data set for the English Lexical Substitution Task (McCarthy and Navigli
2009) consists of 2,010 sentences sourced from Web pages. Each sentence features one of
205 distinct target words that may be nouns, verbs, adjectives, or adverbs. The sentences
have been annotated by human judges to suggest semantically acceptable substitutes
for their target words. Table 14 gives example sentences and annotations for the target
verb charge. For the original shared task the data was divided into development and test
20 These results were not previously published.
621
Computational Linguistics Volume 40, Number 3
Table 13
Results (?cat averaged across groups) for the Mitchell and Lapata (2010) similarity data set.
Model No Optimization Optimized on Dev
V?Obj N?N Adj?N V?Obj N?N Adj?N
LDA/Win5 0.37 0.51 0.42 0.37 0.53 0.44
ROOTH-LDA/Syn 0.37 0.42 0.45 0.37 0.43 0.47
Combined 0.39 0.51 0.47 0.41 0.53 0.48
Mult/Win5 0.31 0.30 0.30 ? ? ?
M+L10/Mult* 0.37 0.49 0.46 ? ? ?
M+L10/Best* 0.40 0.49 0.46 0.40 0.49 0.46
Human* 0.55 0.49 0.52 ? ? ?
Table 14
Example sentences for the verb charge from the English Lexical Substitution Task.
Commission is the amount charged to execute a trade.
levy (2), impose (1), take (1), demand (1)
Annual fees are charged on a pro-rata basis to correspond with the standardized renewal date
in December.
levy (2), require (1), impose (1), demand (1)
Meanwhile, George begins obsessive plans for his funeral. . . George, suspicious, charges to
her room to confront them.
run (2), rush (2), storm (1), dash (1)
Realizing immediately that strangers have come, the animals charge them and the horses
began to fight.
attack (5), rush at (1)
sections; in this article we follow subsequent work using parameter-free models and
use the whole data set for testing.
The gold standard substitute annotations contain a number of multiword terms
such as rush at and generate electricity. As it is impossible for a standard lexical distri-
butional model to reason about such terms, we remove these substitutes from the gold
standard.21 We remove entirely the 17 sentences that have only multiword substitutes
in the gold standard, as well as 7 sentences for which no gold annotations are provided.
This leaves 1,986 sentences.
The original Lexical Substitution Task asked systems to propose substitutes from
an unrestricted English vocabulary, though in practice all participants used lexical
resources to constrain the set of substitutes considered. Most subsequent researchers
using the Lexical Substitution data to evaluate models of contextual meaning have
adopted a slightly different experimental design, in which systems are asked to rank
only the list of attested substitutes for the target word in each sentence. For example,
21 Thater, Fu?rstenau, and Pinkal (2010, 2011) and Dinu and Lapata (2010) similarly remove multiword
paraphrases (Georgiana Dinu, p.c.).
622
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
the list of substitute candidates for an instance of charge is the union of the substitute
lists in the gold standard for every sentence containing charge as a target word: levy,
impose, take, demand, require, impose, run, rush, storm, dash, attack,. . . . Evaluation of system
predictions for a given sentence then involves comparing the ranking produced by the
system with the implicit ranking produced by annotators, assuming that any candidates
not attested for the sentence appear with frequency 0 at the bottom of the ranking.
Dinu and Lapata (2010) use Kendall?s ?b, a standard rank correlation measure that is
appropriate for data containing tied ranks. Thater, Fu?rstenau, and Pinkal (2010, 2011)
use Generalized Average Precision (GAP), a precision-like measure originally proposed
by Kishida (2005) for information retrieval:
GAP =
?n
i=1 I(xi)
?i
k=1 xk
i
?R
j=1 I(yj)
?j
l=1 yl
j
(69)
where x1, . . . , xn are the ranked candidate scores provided by the system, y1, . . . , yR are
the ranked scores in the gold standard and I(x) is an indicator function with value 1 if
x > 0 and 0 otherwise.
In this article we report both ?b and GAP scores, calculated individually for each
sentence and averaged. The open-vocabulary design of the original Lexical Substitution
Task facilitated the use of other evaluation measures such as ?precision out of ten?: the
proportion of the first 10 words in a system?s ranked substitute list that are contained
in the gold standard annotation for that sentence. This measure is not appropriate
in the constrained-vocabulary scenario considered here; when there are fewer than
10 candidate substitutes for a target word, the precision will always be 1.
4.5.2 Models. We apply both window-based and syntactic models of similarity in context
to the lexical substitution data set; we expect the latter to give more accurate predictions
but to have incomplete coverage when a test sentence is not fully and correctly parsed
or when the test lexical items were not seen in the appropriate contexts in training.22 We
therefore also average the predictions of the two model types in the hope of attaining
superior performance with full coverage.
The models we train on the BNC and combined BNC + WIKI corpora are as follows:
Win5 An LDA model using 5-word-window contexts (so |C| ? 10) and similarity
P(z|o, C) computed according to Equation (54).
C ? T An LDA model using syntactic co-occurrences with similarity computed accord-
ing to Equation (54).
T ? C An LDA model using syntactic co-occurrences with similarity computed accord-
ing to Equation (57).
T ? C A model averaging the predictions of the C ? T and T ? C models.
Win5 + C ? T, Win5 + T ? C, Win5 + T ? C A model averaging the predictions of
Win5 and the appropriate syntactic model.
TFP11 The vector-space model of Thater, Fu?rstenau, and Pinkal (2011). We report fig-
ures with and without backoff to lexical similarity between target and substitute
words in the absence of a syntax-based prediction.
22 An LDA model cannot make an informative prediction of P(z|o, C) if word o was never seen entering
into at least one (unlexicalized) syntactic relation in C. Other syntactic models such as that of Thater,
Fu?rstenau, and Pinkal (2011) face analogous restrictions.
623
Computational Linguistics Volume 40, Number 3
Table 15
Results on the English Lexical Substitution Task data set; boldface denotes best performance at
full coverage for each corpus.
BNC BNC + Wikipedia
GAP ?b %Coverage GAP ?b %Coverage
Win5 44.5 0.17 100.0 44.8 0.17 100.0
C ? T 46.8 0.20 86.4 48.7 0.21 86.5
T ? C 47.2 0.21 86.4 49.3 0.22 86.5
T ? C 48.2 0.22 86.4 49.1 0.23 86.5
Win5 + C ? T 46.0 0.18 100.0 48.7 0.21 100.0
Win5 + T ? C 48.6 0.21 100.0 49.3 0.22 100.0
Win5 + T ? C 48.1 0.20 100.0 49.5 0.23 100.0
Baseline:
No Context 43.8 0.16 100.0 43.7 0.15 100.0
No Similarity 39.7 0.14 100.0 40.3 0.14 100.0
TFP11:
No Backoff 46.8 0.20 84.8 47.7 0.22 84.9
+Backoff 46.4 0.19 98.1 47.3 0.21 98.2
We also consider two baseline LDA models:
No Context A model that ranks substitutes n by computing the Bhattacharyya similar-
ity between their topic distributions P(z|n) and the target word topic distribution
P(z|o).
No Similarity A model that ranks substitutes n by their context-conditioned probabil-
ity P(n|C) only; this is essentially a language-modeling approach using syntactic
?bigrams.?
We report baseline results for the T ? C syntactic model, but performance is similar
with other co-occurrence types.
Predictions for the LDA models are averaged over five runs for each setting of |Z|
in the range {600, 800, 1000, 1200}. In order to test statistical significance of differences
between models we use stratified shuffling (Yeh 2000).23
4.5.3 Results. Table 15 presents results on the Lexical Substitution Task data set.24 As
expected, the window-based LDA models attain good coverage but worse performance
than the syntactic models. The combined model Win5 + T ? C trained on BNC+WIKI
gives the best scores (GAP = 49.5, ?b = 0.23). Every combined model gives a statistically
significant improvement (p < 0.01) over the corresponding window-based Win5 model.
Our TFP11 reimplementation of Thater, Fu?rstenau, and Pinkal (2011) has slightly less
than complete coverage, and performs worse than almost all combined LDA models.
To compute statistical significance we only use the sentences for which TFP11 made
predictions; for both the BNC and BNC+WIKI corpora, the Win5 + T ? C model
23 We use the software package provided by Sebastian Pado? at
http://www.nlpado.de/~sebastian/sigf.html.
24 Results for the LDA models were reported in O? Se?aghdha and Korhonen (2011).
624
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
gives a statistically significant (p < 0.05) improvement over TFP11 for both GAP and
?b, while Win5 + T ? C gives a significant improvement for GAP and ?b on the BNC
training corpus. The no-context and no-similarity baselines are clearly worse than the
full models; this difference is statistically significant (p < 0.01) for both training corpora
and all models.
Table 16 breaks performance down across the four parts of speech used in the
data set. Verbs appear to present the most difficult substitution questions and also
demonstrate the greatest beneficial effect of adding syntactic disambiguation to the
basic Win5 model. The full Win5 + T ? C outperforms our reimplementation of Thater,
Fu?rstenau, and Pinkal (2011) on all parts of speech for the GAP statistic and on verbs and
adjectives for ?b, scoring a tie on nouns and adverbs. Table 16 also lists results reported
by Dinu and Lapata (2010) and Thater, Fu?rstenau, and Pinkal (2010, 2011) for their
models trained on the English Gigaword Corpus. This corpus is of comparable size to
the BNC+WIKI corpus, but we note that the results reported by Thater, Fu?rstenau, and
Pinkal (2011) are better than those attained by our reimplementation, suggesting that
uncontrolled factors such as choice of corpus, parser, or dependency representation may
be responsible. Thater, Fu?rstenau, and Pinkal?s (2011) results remain the best reported
for this data set; our Win5 + T ? C results are better than Dinu and Lapata (2010) and
Thater, Fu?rstenau, and Pinkal (2010) in this uncontrolled setting.
5. Conclusion
In this article we have shown that the probabilistic latent-variable framework provides
a flexible and effective toolbox for distributional modeling of lexical meaning and gives
state-of-the-art results on a number of semantic prediction tasks. One useful feature of
this framework is that it induces a representation of semantic classes at the same time
as it learns about selectional preference distributions. This can be viewed as a kind of
coarse-grained sense induction or as a kind of concept induction. We have demonstrated
that reasoning about these classes leads to an accurate method for calculating semantic
similarity in context. By applying our models we attain state-of-the-art performance
on a range of evaluations involving plausibility prediction, in-context similarity, and
Table 16
Performance by part of speech, with additional results from Thater, Fu?rstenau, and Pinkal (2010,
2011) and Dinu and Lapata (2010).
Nouns Verbs Adjectives Adverbs Overall
GAP ?b GAP ?b GAP ?b GAP ?b GAP ?b
Win5/BNC+WIKI 46.0 0.16 38.9 0.14 44.0 0.18 54.0 0.22 44.8 0.17
Win5 + T ? C 50.7 0.22 45.1 0.20 48.8 0.24 55.9 0.24 49.5 0.23
TFP11 (+Backoff) 48.9 0.22 42.5 0.17 46.0 0.22 55.2 0.24 47.3 0.21
TFP10* (Model 1) 46.4 ? 45.9 ? 39.4 ? 48.2 ? 44.6 ?
TFP10* (Model 2) 42.5 ? ? ? 43.2 ? 51.4 ? ? ?
TFP11* (+Backoff) 52.9 ? 48.8 ? 51.1 ? 55.3 ? 51.7 ?
DL10* (LDA) ? 0.16 ? 0.14 ? 0.17 ? 0.21 ? 0.16
DL10* (NMF) ? 0.15 ? 0.14 ? 0.16 ? 0.26 ? 0.16
625
Computational Linguistics Volume 40, Number 3
lexical substitution. The three models we have investigated?LDA, ROOTH-LDA and
LEX-LDA?all perform at a similar level for predicting plausibility, but in other cases
the representation induced by one model may be more suitable than the others.
In future work, we anticipate that the same intuitions may lead to similarity accu-
rate methods for other tasks where disambiguation is required; an obvious candidate
would be traditional word sense disambiguation, perhaps in combination with the
probabilistic WordNet-based preference models of O? Se?aghdha and Korhonen (2012).
More generally, we expect that latent-variable models will prove useful in applications
where other selectional preference models have been applied, for example, metaphor
interpretation and semantic role labeling.
A second route for future work is to enrich the semantic representations that are
learned by the model. As previously mentioned, probabilistic generative models are
modular in the sense that they can be integrated in larger models. Bayesian methods
for learning tree structures could be applied to learn taxonomies of semantic classes
(Blei, Griffiths, and Jordan 2010; Blundell, Teh, and Heller 2010). Borrowing ideas
from Bayesian hierarchical language modeling (Teh 2006), one could build a model of
selectional preference and disambiguation in the context of arbitrarily long dependency
paths, relaxing our current assumption that only the immediate neighbors of a target
word affect its meaning. Our class-based preference model also suggests an approach
to identifying regular polysemy alternation by finding class co-occurrences that repeat
across words, offering a fully data-driven alternative to polysemy models based on
WordNet (Boleda, Pado?, and Utt 2012). In principle, any structure that can be reasoned
about probabilistically, from syntax trees to coreference chains or semantic relations, can
be coupled with a selectional preference model to incorporate disambiguation or lexical
smoothing in a task-oriented architecture.
Acknowledgments
The work in this article was funded by the
EPSRC (grant EP/G051070/1) and by the
Royal Society. We are grateful to Frank Keller
and Mirella Lapata for sharing their data set
of plausibility judgments; to Georgiana
Dinu, Karl Moritz Hermann, Jeff Mitchell,
Sebastian Pado?, and Andreas Vlachos for
offering information and advice; and to the
anonymous Computational Linguistics
reviewers, whose suggestions have
substantially improved the quality of this
article.
References
Abney, Steven and Marc Light. 1999. Hiding
a semantic hierarchy in a Markov model.
In Proceedings of the ACL-99 Workshop on
Unsupervised Learning in NLP, pages 1?8,
College Park, MD.
Altmann, Gerry T. M. and Yuki Kamide.
1999. Incremental interpretation of
verbs: Restricting the domain of
subsequent reference. Cognition,
73(3):247?264.
Asuncion, Arthur, Max Welling, Padhraic
Smyth, and Yee Whye Teh. 2009. On
smoothing and inference for topic models.
In Proceedings of the 25th Conference on
Uncertainty in Artificial Intelligence
(UAI-09), pages 27?34, Montreal.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preferences from unlabeled
text. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language
Processing (EMNLP-08), pages 59?68,
Honolulu, HI.
Bhattacharyya, A. 1943. On a measure
of divergence between two statistical
populations defined by their probability
distributions. Bulletin of the Calcutta
Mathematical Society, 35:99?110.
Bicknell, Klinton, Jeffrey L. Elman, Mary
Hare, Ken McRae, and Marta Kutas. 2010.
Effects of event knowledge in processing
verbal arguments. Journal of Memory and
Language, 63(4):489?505.
Blei, David M., Thomas L. Griffiths, and
Michael I. Jordan. 2010. The nested
Chinese restaurant process and Bayesian
nonparametric inference of topic
hierarchies. Journal of the ACM, 57(2):1?30.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
626
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
Journal of Machine Learning Research,
3:993?1,022.
Blitzer, John, Amir Globerson, and
Fernando Pereira. 2005. Distributed latent
variable models of lexical co-occurrences.
In Proceedings of the 10th International
Workshop on Artificial Intelligence and
Statistics (AISTATS-05), pages 25?32,
Barbados.
Blundell, Charles, Yee Whye Teh, and
Katherine A. Heller. 2010. Bayesian
rose trees. In Proceedings of the 26th
Conference on Uncertainty in Artificial
Intelligence (UAI-10), pages 65?72,
Catalina Island, CA.
Boleda, Gemma, Sebastian Pado?, and
Jason Utt. 2012. Regular polysemy:
A distributional model. In Proceedings
of *SEM-12, pages 151?160, Montreal.
Boyd-Graber, Jordan and David M. Blei.
2008. Syntactic topic models. In Proceedings
of the 22nd Annual Conference on Neural
Information Processing Systems (NIPS-08),
pages 185?192, Vancouver.
Briscoe, Ted. 2006. An introduction to tag
sequence grammars and the RASP
system parser. Technical Report 662,
Computer Laboratory, University of
Cambridge.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release of
the RASP system. In Proceedings of the
ACL-06 Interactive Presentation Sessions,
pages 77?80, Sydney.
Brody, Samuel and Mirella Lapata. 2009.
Bayesian word sense induction.
In Proceedings of EACL-09, pages 103?111,
Athens.
Chambers, Nathanael and Dan Jurafsky.
2010. Improving the use of pseudo-words
for evaluating selectional preferences.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics (ACL-10), pages 445?453,
Uppsala.
Chang, Jonathan, Jordan Boyd-Graber, Sean
Gerrish, Chong Wang, and David M. Blei.
2009. Reading tea leaves: How humans
interpret topic models. In Proceedings
of the 23rd Annual Conference on Neural
Information Processing Systems (NIPS-09),
pages 288?296, Vancouver.
Chen, Stanley F. and Joshua Goodman.
1999. An empirical study of smoothing
techniques for language modeling.
Computer Speech and Language,
13(4):359?393.
Chomsky, Noam. 1957. Syntactic Structures.
Mouton de Gruyter, Berlin.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference
with Bayesian networks. In Proceedings
of the 18th International Conference on
Computational Linguistics (COLING-00),
pages 187?193, Saarbru?cken.
Clark, Stephen, Ann Copestake, James R.
Curran, Yue Zhang, Aurelie Herbelot,
James Haggerty, Byung-Gyu Ahn,
Curt Van Wyk, Jessika Roesner, Jonathan
Kummerfeld, and Tim Dawborn. 2009.
Large-scale syntactic processing: Parsing
the Web. Technical report, Final Report
of the 2009 JHU CLSP Workshop.
Clark, Stephen and David Weir. 2002.
Class-based probability estimation using
a semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Cordier, Brigitte. 1965. Factor-analysis
of correspondences. In Proceedings of
the 1965 International Conference on
Computational Linguistics (COLING-65),
New York, NY.
Curran, James. 2003. From Distributional
to Semantic Similarity. Ph.D. thesis,
School of Informatics, University of
Edinburgh.
Dagan, Ido, Lillian Lee, and Fernando C. N.
Pereira. 1999. Similarity-based models of
word co-occurrence probabilities. Machine
Learning, 34(1):34?69.
Dinu, Georgiana and Mirella Lapata. 2010.
Measuring distributional similarity
in context. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing (EMNLP-10),
pages 1,162?1,172, Cambridge, MA.
Erk, Katrin. 2007. A simple, similarity-
based model for selectional preferences.
In Proceedings of the 45th Annual Meeting
of the Association for Computational
Linguistics (ACL-07), pages 216?223,
Prague.
Erk, Katrin and Sebastian Pado?. 2008.
A structured vector space model for word
meaning in context. In Proceedings of the
2008 Conference on Empirical Methods in
Natural Language Processing (EMNLP-08),
pages 897?906, Honolulu, HI.
Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.
2010. A flexible, corpus-driven model of
regular and inverse selectional preferences.
Computational Linguistics, 36(4):723?763.
Essen, Ute and Volker Steinbiss. 1992.
Co-occurrence smoothing for stochastic
language modeling. In Proceedings of the
1992 IEEE International Conference on
Acoustics, Speech, and Signal Processing
627
Computational Linguistics Volume 40, Number 3
(ICASSP-92), pages 161?164,
San Francisco, CA.
Evert, Stefan. 2004. The Statistics of Word
Co-occurrences: Word Pairs and Collocations.
Ph.D. thesis, Institut fu?r maschinelle
Sprachverarbeitung, Universita?t
Stuttgart.
Fan, Ron-En, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008.
LIBLINEAR: A library for large linear
classification. Journal of Machine Learning
Research, 9:1,871?1,874.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Finkel, Jenny Rose, Trond Grenager, and
Christopher D. Manning. 2007. The infinite
tree. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics (ACL-07), pages 272?279,
Prague.
Frith, J. R. 1957. A Synopsis of Linguistic
Theory 1930-1955. In Studies in Linguistic
Analysis. Oxford Philological Society,
Oxford.
Gelman, Andrew, John B. Carlin, Hal S.
Stern, and Donald B. Rubin. 2003. Bayesian
Data Analysis. Chapman and Hall/
CRC, Boca Raton, FL, 2nd edition.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Goldwater, Sharon, Thomas L. Griffiths,
and Mark Johnson. 2011. Producing
power-law distributions and damping
word frequencies with two-stage
language models. Journal of Machine
Learning Research, 12:2,335?2,382.
Grefenstette, Edward and Mehrnoosh
Sadrzadeh. 2011. Experimental support for
a categorical compositional distributional
model of meaning. In Proceedings
of EMNLP-11, pages 1,394?1,404,
Edinburgh, UK.
Griffiths, Thomas L. and Mark Steyvers.
2004. Finding scientific topics. Proceedings
of the National Academy of Sciences,
101(Suppl. 1):5,228?5,235.
Griffiths, Thomas L., Mark Steyvers,
David M. Blei, and Joshua B. Tenenbaum.
2004. Integrating topics and syntax.
In Proceedings of the 18th Annual
Conference on Neural Information Processing
Systems (NIPS-04), pages 537?544,
Vancouver.
Grishman, Ralph and John Sterling. 1993.
Smoothing of automatically generated
selectional constraints. In Proceedings of
the ARPA Human Language Technology
Workshop (HLT-93), pages 254?259,
Plainsboro, NJ.
Harper, Kenneth E. 1965. Measurement of
similarity between nouns. In Proceedings
of the 1965 International Conference on
Computational Linguistics (COLING-65),
New York, NY.
Harris, Zellig. 1954. Distributional structure.
Word, 10(23):146?162.
Heinrich, Gregor. 2009. Parameter estimation
for text analysis. Technical report,
Fraunhofer IGD.
Hinton, Geoffrey E. 2002. Training products
of experts by minimizing contrastive
divergence. Neural Computation,
14(8):1,771?1,800.
Katz, Jerrold J. and Jerry A. Fodor. 1963. The
stucture of a semantic theory. Language,
39(2):170?210.
Keller, Frank and Mirella Lapata. 2003. Using
the Web to obtain frequencies for unseen
bigrams. Computational Linguistics,
29(3):459?484.
Kishida, Kazuaki. 2005. Property of average
precision and its generalization: An
examination of evaluation indicator
for information retrieval experiments.
Technical Report NII-2005-014E, National
Institute of Informatics, Tokyo, Japan.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-99), pages 25?32,
College Park, MD.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Liang, Percy, Slav Petrov, Michael Jordan,
and Dan Klein. 2007. The infinite PCFG
using hierarchical Dirichlet processes.
In Proceedings of the 2007 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-07), pages 688?697,
Prague.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639?654.
McCarthy, Diana and Roberto Navigli.
2009. The English lexical substitution
task. Language Resources and Evaluation,
43(2):139?159.
McCarthy, Diana, Sriram Venkatapathy,
and Aravind K. Joshi. 2007. Detecting
compositionality of verb-object
combinations using selectional
preferences. In Proceedings of the 2007 Joint
628
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL-07), pages 369?379,
Prague.
Meng, Xiao-Li, Robert Rosenthal, and
Donald B. Rubin. 1992. Comparing
correlated correlation coefficients.
Psychological Bulletin, 111(1):172?175.
Minka, Thomas P. 2003. Estimating a
Dirichlet distribution. Available at
http://research.microsoft.com/en-us/
um/people/minka/papers/dirichlet/.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of the
46th Annual Meeting of the Association
for Computational Linguistics (ACL-08),
pages 236?244, Columbus, OH.
Mitchell, Jeff and Mirella Lapata. 2010.
Composition in distributional models
of semantics. Cognitive Science,
34(8):1,388?1,429.
Moon, Taesun, Katrin Erk, and Jason
Baldridge. 2010. Crouching Dirichlet,
hidden Markov model: Unsupervised POS
tagging with context local tag generation.
In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-10), pages 196?206,
Cambridge, MA.
O? Se?aghdha, Diarmuid. 2010. Latent
variable models of selectional preference.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics (ACL-10), pages 435?444,
Uppsala.
O? Se?aghdha, Diarmuid and Ann Copestake.
2008. Semantic classification with
distributional kernels. In Proceedings
of the 22nd International Conference on
Computational Linguistics (COLING-08),
pages 649?655, Manchester.
O? Se?aghdha, Diarmuid and Anna Korhonen.
2011. Probabilistic models of similarity in
syntactic context. In Proceedings of the
2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP-11),
pages 1,047?1,057, Edinburgh.
O? Se?aghdha, Diarmuid and Anna Korhonen.
2012. Modelling selectional preferences in
a lexical hierarchy. In Proceedings of the 1st
Joint Conference on Lexical and Computational
Semantics (*SEM-12), pages 170?179,
Montreal.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and Eduard
Hovy. 2007. ISP: Learning inferential
selectional preferences. In Proceedings of
NAACL-07, pages 564?571, Rochester, NY.
Pereira, Fernando, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of
English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics (ACL-93),
pages 183?190, Columbus, OH.
Petrov, Slav, Leon Barrett, Romain Thibaux,
and Dan Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics
(COLING-ACL-06), pages 433?440, Sydney.
Rayner, Keith, Tessa Warren, Barbara J.
Juhasz, and Simon P. Liversedge. 2004. The
effect of plausibility on eye movements in
reading. Journal of Experimental Psychology:
Learning Memory and Cognition,
30(6):1,290?1,301.
Reisinger, Joseph and Raymond Mooney.
2010. A mixture model with sharing for
lexical semantics. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing (EMNLP-10),
pages 1,173?1,182, Cambridge, MA.
Reisinger, Joseph and Raymond Mooney.
2011. Cross-cutting models of lexical
semantics. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing (EMNLP-11),
pages 1,405?1,415, Edinburgh.
Resnik, Philip. 1993. Selection and Information:
A Class-Based Approach to Lexical
Relationships. Ph.D. thesis, University of
Pennsylvania.
Ritter, Alan, Mausam, and Oren Etzioni.
2010. A latent Dirichlet allocation method
for selectional preferences. In Proceedings of
the 48th Annual Meeting of the Association
for Computational Linguistics (ACL-10),
pages 424?434, Uppsala.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association
for Computational Linguistics (ACL-99),
pages 104?111, College Park, MD.
Russell, Bertrand. 1940. An Inquiry into
Meaning and Truth. George Allen and
Unwin, London.
Schulte im Walde, Sabine, Christian Hying,
Christian Scheible, and Helmut Schmid.
629
Computational Linguistics Volume 40, Number 3
2008. Combining EM training and the
MDL principle for an automatic
verb classification incorporating
selectional preferences. In Proceedings
of ACL-08: HLT, pages 496?504,
Columbus, OH.
Shutova, Ekaterina. 2010. Automatic
metaphor interpretation as a paraphrasing
task. In Proceedings of Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics
(NAACL-HLT-10), pages 1,029?1,037,
Los Angeles, CA.
Socher, Richard, Eric H. Huang, Jeffrey
Pennington, Andrew Y. Ng, and
Christopher D. Manning. 2011. Dynamic
pooling and unfolding recursive
autoencoders for paraphrase detection.
In Proceedings of the 25th Annual
Conference on Neural Information Processing
Systems (NIPS-11), pages 801?809,
Granada.
Spa?rck Jones, Karen. 1964. Synonymy and
Semantic Classification. Ph.D. thesis,
University of Cambridge.
Teh, Yee Whye. 2006. A hierarchical Bayesian
language model based on Pitman-Yor
processes. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics
(COLING-ACL-06), pages 985?992,
Sydney.
Thater, Stefan, Hagen Fu?rstenau, and
Manfred Pinkal. 2010. Contextualizing
semantic representations using
syntactically enriched vector models.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics (ACL-10), pages 948?957,
Uppsala.
Thater, Stefan, Hagen Fu?rstenau, and
Manfred Pinkal. 2011. Word meaning in
context: A simple and effective vector
model. In Proceedings of the 5th International
Joint Conference on Natural Language
Processing (IJCNLP-11), pages 1,134?1,143,
Hyderabad.
Titov, Ivan and Alexandre Klementiev.
2011. A Bayesian model for unsupervised
semantic parsing. In Proceedings of the
49th Annual Meeting of the Association
for Computational Linguistics,
pages 1,445?1,455, Portland, OR.
Turney, Peter D. 2012. Domain and function:
A dual-space model of semantic relations
and compositions. Journal of Artificial
Intelligence Research, 44:533?585.
Turney, Peter D. and Patrick Pantel. 2010.
From frequency to meaning: Vector
space models of semantics. Journal
of Artificial Intelligence Research,
37:141?188.
Vlachos, Andreas, Anna Korhonen, and
Zoubin Ghahramani. 2009. Unsupervised
and constrained Dirichlet process mixture
models for verb clustering. In Proceedings
of the EACL-09 Workshop on Geometrical
Models of Natural Language Semantics
(GEMS-09), pages 74?82, Athens.
Wallach, Hanna, David Mimno, and
Andrew McCallum. 2009. Rethinking
LDA: Why priors matter. In Proceedings
of the 23rd Annual Conference on Neural
Information Processing Systems(NIPS-09),
pages 1,973?1,981, Vancouver.
Wallach, Hanna M. 2006. Topic modeling:
Beyond bag-of-words. In Proceedings of the
23rd International Conference on Machine
Learning (ICML-06), pages 977?984,
Pittsburgh, PA.
Wallach, Hanna M. 2008. Structured topic
models for language. Ph.D. thesis,
University of Cambridge.
Weeds, Julie and David Weir. 2005.
Co-occurrence retrieval: A flexible
framework for lexical distributional
similarity. Computational Linguistics,
31(4):439?476.
Wilks, Yorick. 1978. Making preferences
more active. Artificial Intelligence,
11:197?225.
Yao, Limin, Aria Haghighi, Sebastian Riedel,
and Andrew McCallum. 2011. Structured
relation discovery using generative
models. In Proceedings of EMNLP-11,
pages 1,456?1,466, Edinburgh.
Yao, Limin, David Mimno, and Andrew
McCallum. 2009. Efficient methods for
topic model inference on streaming
document collections. In Proceedings of the
15th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining
(KDD-09), pages 937?946, Paris.
Yeh, Alexander. 2000. More accurate
tests for the statistical significance of
result differences. In Proceedings of the
18th Conference on Computational
Linguistics (COLING-00), pages 947?953,
Saarbru?cken.
Zapirain, Ben?at, Eneko Agirre, and Llu??s
Ma`rquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
630
O? Se?aghdha and Korhonen Probabilistic Distributional Semantics
Processing of the AFNLP (ACL-IJCNLP-09),
pages 73?76, Singapore.
Zapirain, Ben?at, Eneko Agirre, Llu??s
Ma`rquez, and Mihai Surdeanu. 2010.
Improving semantic role classification
with selectional preferences. In Proceedings
of Human Language Technologies: The 2010
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL-HLT-10),
pages 373?376, Los Angeles, CA.
Zhou, Guangyou, Jun Zhao, Kang Liu,
and Li Cai. 2011. Exploiting Web-derived
selectional preference to improve
statistical dependency parsing. In
Proceedings of ACL-11, pages 1,556?1,565,
Portland, OR.
Zwicky, Arnold M. and Jerrold M. Sadock.
1975. Ambiguity tests and how to fail
them. In John P. Kimball, editor, Syntax
and Semantics 4. Academic Press,
New York, NY.
631

Proceedings of NAACL-HLT 2013, pages 928?937,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Information Structure Analysis of Scientific Documents Through
Discourse and Lexical Constraints
Yufan Guo
University of Cambridge, UK
yg244@cam.ac.uk
Roi Reichart
University of Cambridge, UK
rr439@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Abstract
Inferring the information structure of scien-
tific documents is useful for many down-
stream applications. Existing feature-based
machine learning approaches to this task re-
quire substantial training data and suffer from
limited performance. Our idea is to guide
feature-based models with declarative domain
knowledge encoded as posterior distribution
constraints. We explore a rich set of discourse
and lexical constraints which we incorporate
through the Generalized Expectation (GE) cri-
terion. Our constrained model improves the
performance of existing fully and lightly su-
pervised models. Even a fully unsupervised
version of this model outperforms lightly su-
pervised feature-based models, showing that
our approach can be useful even when no la-
beled data is available.
1 Introduction
Techniques that enable automatic analysis of the in-
formation structure of scientific articles can help sci-
entists identify information of interest in the grow-
ing volume of scientific literature. For example,
classification of sentences according to argumenta-
tive zones (AZ) ? an information structure scheme
that is applicable across scientific domains (Teufel
et al, 2009) ? can support information retrieval, in-
formation extraction and summarization (Teufel and
Moens, 2002; Tbahriti et al, 2006; Ruch et al,
2007; Liakata et al, 2012; Contractor et al, 2012).
Previous work on sentence-based classification of
scientific literature according to categories of infor-
mation structure has mostly used feature-based ma-
chine learning, such as Support Vector Machines
(SVM) and Conditional Random Fields (CRF) (e.g.
(Teufel and Moens, 2002; Lin et al, 2006; Hiro-
hata et al, 2008; Shatkay et al, 2008; Guo et al,
2010; Liakata et al, 2012)). Unfortunately, the per-
formance of these methods is rather limited, as indi-
cated e.g. by the relatively low numbers reported by
Liakata et al (2012) in biochemistry and chemistry
with per-class F-scores ranging from .18 to .76.
We propose a novel approach to this task in which
traditional feature-based models are augmented with
explicit declarative expert and domain knowledge,
and apply it to sentence-based AZ. We explore two
sources of declarative knowledge for our task - dis-
course and lexical. One way to utilize discourse
knowledge is to guide the model predictions by en-
coding a desired predicted class (i.e. information
category) distribution in a given position in the doc-
ument. Consider, for example, sentence (1) from the
first paragraph of the Discussion section in a paper:
(1) In time, this will prove to be most suitable for
detailed analysis of the role of these hormones in
mammary cancer development.
Although the future tense and cue phrases such as
?in time? can indicate that authors are discussing fu-
ture work (i.e. the ?Future work? class in the AZ
scheme), in this case they refer to their own contri-
bution (i.e. the ?Conclusion? class in AZ). As most
authors discuss their own contribution in the begin-
ning of the Discussion section and future directions
in the end, encoding the desired class distribution as
a function of the position in this section can guide
the model to the right decision.
Likewise, lexical knowledge can guide the model
928
through predicted class distributions for sentences
that contain specific vocabulary. Consider, for ex-
ample, sentence (2):
(2) The values calculated for lungs include the
presumed DNA adduct of BA and might thus be
slightly overestimated.
The verb ?calculated? usually indicates the
?Method? class, but, when accompanied by the
modal verb ?might?, it is more likely to imply that
authors are interpreting their own results (i.e. the
?Conclusion? class in AZ). This can be explicitly
encoded in the model through a target distribution
for sentences containing certain modal verbs.
Recent work has shown that explicit declaration
of domain and expert knowledge can be highly use-
ful for structured NLP tasks such as parsing, POS
tagging and information extraction (Chang et al,
2007; Mann and McCallum, 2008; Ganchev et al,
2010). These works have encoded expert knowledge
through constraints, with different frameworks dif-
fering in the type of constraints and the inference
and learning algorithms used. We build on the Gen-
eralized Expectation (GE) framework (Mann and
McCallum, 2007) which encodes expert knowledge
through a preference (i.e. soft) constraints for pa-
rameter settings for which the predicted label distri-
bution matches a target distribution.
In order to integrate domain knowledge with a
features-based model, we develop a simple taxon-
omy of constraints (i.e. desired class distributions)
and employ a top-down classification algorithm on
top of a Maximum Entropy Model augmented with
GE constraints. This algorithm enables us to break
the multi-class prediction into a pipeline of consecu-
tive, simpler predictions which can be better assisted
by the encoded knowledge.
We experiment in the biological domain with the
eight-category AZ scheme (Table 1) adapted from
(Mizuta et al, 2006) and described in (Contractor
et al, 2012). The results show that our constrained
model substantially outperforms a baseline uncon-
strained Maximum Entropy Model. While this type
of constrained models have previously improved
the feature-based model performance mostly in the
weakly supervised and domain adaptation scenarios
(e.g. (Mann and McCallum, 2007; Mann and Mc-
Callum, 2008; Ganchev et al, 2010)), we demon-
strate substantial gains both when the Maximum En-
Table 1: The AZ categories included in the categorization
scheme of this paper.
Zone Definition
Background (BKG) the background of the study
Problem (PROB) the research problem
Method (METH) the methods used
Result (RES) the results achieved
Conclusion (CON) the authors? conclusions
Connection (CN) work consistent with the current work
Difference (DIFF) work inconsistent with the current work
Future work (FUT) the potential future direction of the research
tropy Model is fully trained and when its training
data is sparse. This demonstrates the importance of
expert knowledge for our task and supports our mod-
eling decision that combines feature-based methods
with domain knowledge encoded via constraints.
2 Previous work
Information structure analysis The information
structure of scientific documents (e.g. journal ar-
ticles, abstracts, essays) can be analyzed in terms
of patterns of topics, functions or relations observed
in multi-sentence scientific text. Computational ap-
proaches have mainly focused on analysis based
on argumentative zones (Teufel and Moens, 2002;
Mizuta et al, 2006; Hachey and Grover, 2006;
Teufel et al, 2009), discourse structure (Burstein et
al., 2003; Webber et al, 2011), qualitative dimen-
sions (Shatkay et al, 2008), scientific claims (Blake,
2009), scientific concepts (Liakata et al, 2010) and
information status (Markert et al, 2012).
Most existing methods for analyzing scientific
text according to information structure use full
supervision in the form of thousands of manu-
ally annotated sentences (Teufel and Moens, 2002;
Burstein et al, 2003; Mizuta et al, 2006; Shatkay
et al, 2008; Guo et al, 2010; Liakata et al, 2012;
Markert et al, 2012). Because manual annotation is
prohibitively expensive, approaches based on light
supervision are now emerging for the task, including
those based on active learning and self-training (Guo
et al, 2011) and unsupervised methods (Varga et al,
2012; Reichart and Korhonen, 2012). Unfortunately,
these approaches do not reach the performance level
of fully supervised models, let alne exceed it. Our
novel method addresses this problem.
Declarative knowledge and constraints Previ-
ous work has shown that incorporating declara-
tive constraints into feature-based machine learning
929
models works well in many NLP tasks (Chang et
al., 2007; Mann and McCallum, 2008; Druck et al,
2008; Bellare et al, 2009; Ganchev et al, 2010).
Such constraints can be used in a semi-supervised or
unsupervised fashion. For example, (Mann and Mc-
Callum, 2008) shows that using CRF in conjunction
with auxiliary constraints on unlabeled data signif-
icantly outperforms traditional CRF in information
extraction, and (Druck et al, 2008) shows that using
declarative constraints alone for unsupervised learn-
ing achieves good results in text classification. We
show that declarative constraints can be highly use-
ful for the identification of information structure of
scientific documents. In contrast with most previous
works, we show that such constraints can improve
the performance of a fully supervised model. The
constraints are particularly helpful for identifying
low-frequency information categories, but still yield
high performance on high-frequency categories.
3 Maximum-Entropy Estimation and
Generalized Expectation (GE)
In this section we describe the Generalized Expecta-
tion method for declarative knowledge encoding.
Maximum Entropy (ME) The idea of General-
ized Expectation (Dud??k, 2007; Mann and McCal-
lum, 2008; Druck et al, 2008) stems from the prin-
ciple of maximum entropy (Jaynes, 1957; Pietra and
Pietra, 1993) which raises the following constrained
optimization problem:
max
p
H(?)
subject to Ep[f(?)] = Ep?[f(?)]
p(?) ? 0
?
p(?) = 1, (1)
where p?(?) is the empirical distribution, p(?) is a
probability distribution in the model and H(?) is the
corresponding information entropy, f(?) is a collec-
tion of feature functions, and Ep[f(?)] and Ep?[f(?)]
are the expectations of f with respect to p(?) and
p?(?). An example of p(?) could be a conditional
probability distribution p(y|x), and H(?) could be
a conditional entropy H(y|x). The optimal p(y|x)
will take on an exponential form:
p?(y|x) =
1
Z?
exp(? ? f(x, y)), (2)
where ? is the Lagrange multipliers in the corre-
sponding unconstrained objective function, and Z?
is the partition function. The dual problem be-
comes maximizing the conditional log-likelihood of
labeled data L (Berger et al, 1996):
max
?
?
(xi,yi)?L
log(p?(yi|xi)), (3)
which is usually known as a Log-linear or Maximum
Entropy Model (MaxEnt).
ME with Generalized Expectation The objec-
tive function and the constraints on expectations in
(1) can be generalized to:
max
?
?
?
x
p?(x)D(p?(y|x)||p0(y|x))
? g(Ep?(x)[Ep?(y|x)[f(x, y)|x]]), (4)
where D(p?||p0) is the divergence from p? to a base
distribution p0, and g(?) is a constraint/penalty func-
tion that takes empirical evidence Ep?(x,y)[f(x, y)] as
a reference point (Pietra and Pietra, 1993; Chen et
al., 2000; Dud??k, 2007). Note that a special case of
this is MaxEnt where p0 is set to be a uniform distri-
bution, D(?) to be the KL divergence, and g(?) to be
an equality constraint.
The constraint g(?) can be set in a relaxed manner:
?
k
1
2?2k
(Ep?(x)[Ep?(y|x)[fk(x, y)|x]]? Ep?(x,y)[fk(x, y)])
2,
which is the logarithm of a Gaussian distribution
centered at the reference values with a diagonal co-
variance matrix (Pietra and Pietra, 1993), and the
dual problem will become a regularized MaxEnt
with a Gaussian prior (?k = 0, ?2k =
1
?2k
) over the
parameters:
max
?
?
(xi,yi)?L
log(p?(yi|xi))?
?
k
?2k
2?2k
(5)
Such a model can be further extended to include
expert knowledge or auxiliary constraints on unla-
beled data U (Mann and McCallum, 2008; Druck et
al., 2008; Bellare et al, 2009):
max
?
?
(xi,yi)?L
log(p?(yi|xi))?
?
k
?2k
2?2k
? ?g?(Ep?(y|x)[f
?(x, y)]) (6)
where f?(?) is a collection of auxiliary feature func-
tions on U , g?(?) is a constraint function that takes
expert/declarative knowledge Ep?(y|x)[f
?(x, y)] as a
reference point, and ? is the weight of the auxiliary
GE term.
930
The auxiliary constraint g?(?) can take on many
forms and the one we used in this work is an L2
penalty function (Dud??k, 2007). We trained the
model with L-BFGS (Nocedal, 1980) in supervised,
semi-supervised and unsupervised fashions on la-
beled and/or unlabeled data, using the Mallet soft-
ware (McCallum, 2002).
4 Incorporating Expert Knowledge into
GE constraints
We defined the auxiliary feature functions ? the ex-
pert knowledge on unlabeled data as1:
f?k (x, y) = 1(xk,yk)(x, y),
such that Ep?(y|x)[fk(x, y)] = p
?(yk|xk), (7)
where 1(xk,yk)(x, y) is an indicator function, and
p?(yk|xk) is a conditional probability specified in
the form of
p?(yk|xk) ? [ak, bk] (8)
by experts. In particular, we took
p?(yk|xk) =
?
?
?
ak if p?(yk|xk) < a
bk if p?(yk|xk) > b
p?(yk|xk) if a ? p?(yk|xk) ? b
(9)
as the reference point when calculating g?(?).
We defined two types of constraints: those based
on discourse properties such as the location of a sen-
tence in a particular section or paragraph, and those
based on lexical properties such as citations, refer-
ences to tables and figures, word lists, tenses, and
so on. Note that the word lists actually contain both
lexical and semantic information.
To make an efficient use of the declarative knowl-
edge we build a taxonomy of information structure
categories centered around the distinction between
categories that describe the authors? OWN work and
those that describe OTHER work (see Section 5). In
practice, our model labels every sentence with an
AZ category augmented by one of the two cate-
gories, OWN or OTHER. In evaluation we consider
only the standard AZ categories which are part of
the annotation scheme of (Contractor et al, 2012).
1Accordingly, Ep?(y|x)[fk(x, y)] = p?(yk|xk)
Table 2: Discourse and lexical constraints for identifying infor-
mation categories at different levels of the information structure
taxonomy.
(a) OWN / OTHER
OWN Discourse
(1) Target(last part of paragraph) = 1
(2) Target(last part of section) = 1
Lexical
(3) Target(tables/figures) ? 1
(4) ?x ? {w|w?we} Target(x) = 1
? ?y ? {w|w?previous} Target(y) = 0
(5) ?x ? {w|w?thus} Target(x) = 1
OTHER Lexical
(6) Target(cite) = 1
(7) Target(cite) > 1
(8) Backward(cite) = 1
? ?x ? {w|w?in addition} Target(x) = 1
(b) PROB / METH / RES / CON / FUT
PROB Discourse
(1) Target(last part in section) = 1
Lexical
(2) ?x ? {w|w?aim} Target(x) = 1
(3) ?x ? {w|w?question} Target(x) = 1
(4) ?x ? {w|w?investigate} Target(x) = 1
METH Lexical
(5) ?x ? {w|w?{use,method}} Target(x) = 1
RES Lexical
(6) Target(tables/figures) ? 1
(7) ?x ? {w|w?observe} Target(x) = 1
CON Lexical
(8) Target(cite) ? 1
(9) ?x ? {w|w?conclude} Target(x) = 1
(10) ?x ? {w|w?{suggest, thus, because, likely}}
Target(x) = 1
FUT Discourse
(11) Target(first part in section) = 1
(12) Target(last part in section) = 1
? ?x ? {w|w?{will,need,future}} Target(x) = 1
Lexical
(13) ?x {w|w?will,future} Target(x) = 1
(14) Target(present continuous tense) = 1
(c) BKG / CN / DIFF
BKG Discourse
(1) Target(first part in paragraph) = 1
(2) Target(first part in section) = 1
Lexical
(3) ?x ? {w|w?we} Target(x) = 1
? ?y ? {w|w?previous} Target(y) = 0
(4) Forward(cite) = 1
? ?x ? {w|w?{consistent,inconsistent,than}}
(Target(x) = 0 ? Forward(x) = 0)
CN Lexical
(5) ?x ? {w|w?consistent} Target(x) = 1
(6) ?x ? {w|w?consistent} Forward(x) = 1
DIFF Lexical
(7) ?x ? {w|w?inconsistent} Target(x) = 1
(8) ?x ? {w|w?inconsistent} Forward(x) = 1
(9) ?x ? {w|w?{inconsistent,than,however}}
Forward(x) = 1 ? ?y ? {w|w?we} Forward(y) = 1
? ?z ? {w|w?previous}} Forward(z) = 0
931
Table 3: The lexical sets used as properties in the constraints.
Cue Synonyms
we our, present study
previous previously, recent, recently
thus therefore
aim objective, goal, purpose
question hypothesis, ?
investigate explore, study, test, examine, evaluate, assess, deter-
mine, characterize, analyze, report, present
use employ
method algorithm, assay
observe see, find, show
conclude conclusion, summarize, summary
suggest illustrate, demonstrate, imply, indicate, confirm, re-
flect, support, prove, reveal
because result from, attribute to
likely probable, probably, possible, possibly, may, could
need remain
future further
consistent match, agree, support, in line, in agreement, similar,
same, analogous
inconsistent conflicting, conflict, contrast, contrary, differ, differ-
ent, difference
than compare
however other hand, although, though, but
The constraints in Table 2(a) refer to the top level
of this taxonomy: distinction between the authors?
own work and the work of others, and the constraints
in Tables 2(b)-(c) refer to the bottom level of the tax-
onomy: distinction between AZ categories related to
the authors? own work (Table 2(b)) and other?s work
(Table 2(c)).
The first and second columns in each table refer
to the y and x variables in Equation (8), respectively.
The functions Target(?), Forward(?) and Backward(?)
refer to the property value for the target, next and
preceding sentence, respectively. If their value is 1
then the property holds for the respective sentence,
if it is 0, the property does not hold. In some cases
the value of such functions can be greater than 1,
meaning that the property appears multiple times in
the sentence. Terms of the form {w|w?{wi}} refer
to any word/bi-grams that have the same sense aswi,
where the actual word set we use with every example
word in Table 2 is described in Table 3.
For example, take constraints (1) and (4) in Table
2(a). The former is a standard discourse constraint
that refers to the probability that the target sentence
describes the authors? own work given that it appears
in the last of the ten parts in the paragraph. The lat-
ter is a standard lexical constraint that refers to the
probability that a sentence presents other people?s
work given that it contains any words in {we, our,
present study} and that it doesn?t contain any words
Figure 1: The constraint taxonomy for top-down modeling.
INFO [Table 2(a)]
OWN [Table 2(b)]
PROB METH RES CON FUT
OTHER [Table 2(c)]
BKG CN DIFF
in {previous, previously, recent, recently}. Our con-
straint set further includes constraints that combine
both types of information. For example, constraint
(12) in Table 2(b) refers to the probability that a sen-
tence discusses future work given that it appears in
the last of the ten parts of the section (discourse) and
that it contains at least one word in {will, future, fur-
ther, need, remain} (lexical).
5 Top-Down Model
An interesting property of our task and domain is
that the available expert knowledge does not directly
support the distinctions between AZ categories, but
it does provide valuable indirect guidance. For ex-
ample, the number of citations in a sentence is only
useful for separating the authors? work from other
people?s work, but not for further fine grained dis-
tinctions between zone categories. Moreover, those
constraints that are useful for making fine grained
distinctions between AZ categories are usually use-
ful only for a particular subset of the categories only.
For example, all the constraints in Table 2(b) are
conditioned on the assumption that the sentence de-
scribes the authors? own work.
To make the best use of the domain knowledge,
we developed a simple constraint taxonomy, and ap-
ply a top-down classification approach which uti-
lizes it. The taxonomy is presented in Figure 1. For
classification we trained three MaxEnt models aug-
mented with GE constraints: one for distinguishing
between OWN and OTHER2, one for distinguishing
between the AZ categories under the OWN auxiliary
category and one for distinguishing between the AZ
categories under the OTHER auxiliary category. At
test time we first apply the first classifier and based
on its prediction we apply either the classifier that
distinguishes between OWN categories or the one
that distinguishes between OTHER categories.
2For the training of this model, each training data AZ cate-
gory is mapped to its respective auxiliary class.
932
6 Experiments
Data We used the full paper corpus used by Contrac-
tor et al (2012) which contains 8171 sentences from
50 biomedical journal articles. The corpus is anno-
tated according to the AZ scheme described in Table
1. AZ describes the logical structure, scientific argu-
mentation and intellectual attribution of a scientific
paper. It was originally introduced by Teufel and
Moens (2002) and applied to computational linguis-
tics papers, and later adapted to other domains such
as biology (Mizuta et al, 2006) ? which we used in
this work ? and chemistry (Teufel et al, 2009).
Table 4 shows the AZ class distribution in full ar-
ticles as well as in individual sections. Since section
names vary across scientific articles, we grouped
similar sections before calculating the statistics (e.g.
Discussion and Conclusions sections were grouped
under Discussion). We can see that although there is
a major category in each section (e.g. CON in Dis-
cussion), up to 36.5% of the sentences in each sec-
tion still belong to other categories.
Features We extracted the following features
from each sentence and used them in the feature-
based classifiers: (1) Discourse features: location in
the article/section/paragraph. For this feature each
text batch was divided to ten equal size parts and the
corresponding feature value identifies the relevant
part; (2) Lexical features: number of citations and
references to tables and figures (0, 1, or more), word,
bi-gram, verb, and verb class (obtained by spectral
clustering (Sun and Korhonen, 2009)); (3) Syntac-
tic features: tense and voice (POS tags of main and
auxiliary verbs), grammatical relation, subject and
object. The lexical and the syntactic features were
extracted for the represented sentence as well as for
its surrounding sentences. We used the C&C POS
tagger and parser (Curran et al, 2007) for extract-
ing the lexical and the syntactic features. Note that
all the information encoded into our constraints is
also encoded in the features and is thus available to
the feature-based model. This enables us to properly
evaluate the impact of our modeling decision which
augments a feature-based model with constraints.
Baselines We compared our model against four
baselines, two with full supervision: Support Vec-
tor Machines (SVM) and Maximum Entropy Mod-
els (MaxEnt), and two with light supervision: Trans-
Table 4: Class distribution (shown in percentages) in articles
and their individual sections in the AZ-annotated corpus.
BKG PROB METH RES CON CN DIFF FUT
Article 16.9 2.8 34.8 17.9 22.3 4.3 0.8 0.2
Introduction 74.8 13.2 5.4 0.6 5.9 0.1 - -
Methods 0.5 0.2 97.5 1.4 0.2 0.2 0.1 -
Results 4.0 2.1 11.7 68.9 12.1 1.1 0.1 -
Discussion 16.9 1.1 0.7 1.5 63.5 13.3 2.4 0.7
Table 5: Performance of baselines on the Discussion section.
BKG PROB METH RES CON CN DIFF FUT
Full supervision
SVM .56 0 0 0 .84 .35 0 0
MaxEnt .55 .08 0 0 .84 .38 0 0
Light supervision with 150 labeled sentence
SVM .26 0 0 0 .80 .05 0 0
TSVM .25 .04 .04 .03 .33 14 .06 .02
MaxEnt .25 0 0 0 .80 .10 0 0
MaxEnt+ER .23 0 0 0 .80 .07 0 0
ductive SVM (TSVM) and semi-supervised Max-
Ent based on Entropy Regularization (ER) (Vapnik,
1998; Jiao et al, 2006). SVM and MaxEnt have
proved successful in information structure analysis
(e.g. (Merity et al, 2009; Guo et al, 2011)) but,
to the best of our knowledge, their semi-supervised
versions have not been used for AZ of full articles.
Parameter tuning The boundaries of the ref-
erence probabilities (ak and bk in Equation (8))
were defined and optimized on the development data
which consists of one third of the corpus. We con-
sidered six types of boundaries: Fairly High for
1, High for [0.9,1), Medium High for [0.5,0.9),
Medium Low for [0.1,0.5), Low for [0,0.1), and
Fairly Low for 0.
Evaluation We evaluated the precision, recall and
F-score for each category, using a standard ten-fold
cross-validation scheme. The models were tested on
each of the ten folds and trained on the rest of them,
and the results were averaged across the ten folds.
7 Results
We report results at two levels of granularity. We
first provide detailed results for the Discussion sec-
tion which should be, as is clearly evident from Ta-
ble 4, the most difficult section for AZ prediction as
only 63.5% of its sentences take its most dominant
class (CON). As we show below, this is also where
our constrained model is most effective. We then
show the advantages of our model for other sections.
Results for the Discussion section To get a bet-
933
Table 6: Discussion section performance of MaxEnt, MaxEnt+GE and a MaxEnt+GE model that does not include our top-down
classification scheme. Results are presented for different amounts of labeled training data. The MaxEnt+GE (Top-down) model
outperforms the MaxEnt in 44 out of 48 cases, and MaxEnt+GE (Flat) in 39 out of 48 cases.
MaxEnt MaxEnt + GE (Top-down) MaxEnt + GE (Flat)
50 100 150 500 1000 Full 50 100 150 500 1000 Full 50 100 150 500 1000 Full
BKG .10 .26 .25 .44 .48 .55 .49 .49 .48 .52 .55 .57 .35 .37 .37 .46 .51 .53
PROB 0 0 0 0 0 0 .38 .16 .29 .13 .30 .41 .38 .23 .19 .39 .38 .33
METH 0 0 0 0 0 0 .17 .22 .37 .35 .50 .39 .16 .17 .21 .24 .32 .29
RES 0 0 0 0 0 0 .18 .24 .58 0 0 .46 .13 .05 .21 .31 .25 .34
CON .79 .80 .80 .83 .83 .84 .77 .78 .82 .83 .84 .84 .63 .66 .68 .74 .78 .78
CN .02 .04 .10 .24 .34 .38 .29 .31 .33 .35 .40 .39 .21 .21 .24 .26 .30 .32
DIFF 0 0 0 0 0 0 .26 .25 .25 .19 .24 .21 .14 .16 .15 .14 .18 .17
FUT 0 0 0 0 0 0 .35 .38 .31 .25 .35 .31 .36 .36 .39 .33 .25 .37
Figure 2: Performance of the MaxEnt and MaxEnt+GE models on the Introduction (left), Methods (middle) and Results (right)
sections. The MaxEnt+GE model is superior.
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
 
 
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
 
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 Table 7: Discussion section performance of the MaxEnt, Max-
Ent+GE and unsupervised GE models when the former two are
trained with 150 labeled sentences. Unsupervised GE outper-
forms the standard MaxEnt model for all categories except for
CON ? the major c tegory of the section. The result pattern for
the other sections are very similar.
MaxEnt MaxEnt + GE Unsup GE
P R F P R F P R F
BKG .38 .19 .25 .49 .48 .48 .49 .44 .46
PROB 0 0 0 .38 .23 .29 .28 .38 .32
METH 0 0 0 .29 .50 .37 .08 .56 .14
RES 0 0 0 .68 .51 .58 .08 .51 .14
CON .69 .96 .80 .81 .84 .82 .74 .69 .71
CN .35 .06 .10 .39 .29 .33 .40 .13 .20
DIFF 0 0 0 .21 .30 .25 .12 .13 .12
FUT 0 0 0 .24 .44 .31 .26 .61 .36
ter understanding of the nature of the challenge we
face, Table 5 shows the F-scores of fully- and semi-
supervised SVM and MaxEnt on the Discussion sec-
tion. The dominant zone category CON, which ac-
counts for 63.5% of the section sentences, has the
highest F-scores for all methods and scenarios. Most
of the methods also identify the second and the third
most frequent zones BKG and CN, but with relatively
lower F-scores. Other low-frequency categories can
hardly be identified by any of the methods regardless
of the amount of labeled data available for training.
Note that the compared models perform quite sim-
ilarly. We therefore use the MaxEnt model, which
Table 8: Analysis of the impact of the different constraint types
for the lightly supervised and the fully supervised cases. Results
are presented for the Discussion section. Using only the lexical
constraints is generally preferable in the fully supervised case.
Combining the different constraint types is preferable for the
lightly supervised case.
Discourse Lexical Discourse+Lexical
150 Full 150 Full 150 Full
BKG .29 .55 .46 .58 .48 .57
PROB 0 0 .37 .40 .29 .41
METH 0 .11 .29 .35 .37 .39
RES 0 .06 .32 .47 .58 .46
CON .81 .84 .80 .84 .82 .84
CN .12 .34 .35 .42 .33 .39
DIFF 0 0 .21 .21 .25 .21
FUT 0 0 0 .29 .31 .31
is most naturally augmented with GE constraints, as
the baseline unconstrained model.
When adding the GE constraints we observe a
substantial performance gain, in both the fully and
the lightly supervised cases, especially for the low-
frequency categories. Table 6 presents the F-scores
of MaxEnt with and without GE constraints (?Max-
Ent+GE (Top-down)? and ?MaxEnt?) in the light
and full supervision scenarios. Incorporating GE
into MaxEnt results in a substantial F-score im-
provement for all AZ categories except for the ma-
jor category CON for which the performance is kept
very similar. In total, MaxEnt+GE (Top-down) is
934
better in 44 out of the 48 cases presented in the table.
Importantly, the constrained model provides sub-
stantial improvements for both the relatively high-
frequency classes (BKG and CN which together label
30.2% of the sentences) and for the low-frequency
classes (which together label 6.4% of the sentences).
The table also clearly demonstrates the impact of
our tree-based top-down classification scheme, by
comparing the Top-down version of MaxEnt+GE
to the standard ?Flat? version. In 39 out of 48
cases, the Top-down model performs better. In some
cases, especially for high-frequency categories and
when the amount of training data increases, un-
constrained MaxEnt even outperforms the flat Max-
Ent+GE model. The results presented in the rest of
the paper for the MaxEnt+GE model therefore refer
to its Top-down version.
All sections We next turn to the performance of
our model on the three other sections. Our exper-
iments show that augmenting the MaxEnt model
with domain knowledge constraints improves per-
formance for all the categories (either low or high
frequency), except the major section category, and
keep the performance for the latter on the same level.
Figure 2 demonstrates this pattern for the lightly su-
pervised case with 150 training sentences but the
same pattern applies to all other amounts of training
data, including the fully supervised case. Naturally,
we cannot demonstrate all these cases due to space
limitations. The result patterns are very similar to
those presented above for the Discussion section.
Unsupervised GE We next explore the quality of
the domain knowledge constraints when used in iso-
lation from a feature-based model. The objective
function of this model is identical to Equation (6)
except that the first (likelihood) term is omitted. Our
experiments reveal that this unsupervised GE model
outperforms standard MaxEnt for all the categories
except the major category of the section, when up
to 150 training sentences are used. Table 7 demon-
strates this for the Discussion section. This pattern
holds for the other scientific article sections. Even
when more than 150 labeled sentences are used, the
unsupervised model better detects the low frequency
categories (i.e. those that label less than 10% of
the sentences) for all sections. These results provide
strong evidence for the usefulness of our constraints
even when they are used with no labeled data.
Model component analysis We finally analyze
the impact of the different types of constraints on
the performance of our model. Table 8 presents the
Discussion section performance of the constrained
model with only one or the full set of constraints.
Interestingly, when the feature-based model is fully
trained the application of the lexical constraints
alone results in a very similar performance to the
application of the full set of lexical and discourse
constraints. It is only in the lightly supervised case
where the full set of constraints is required and re-
sults in the best performing model.
8 Conclusions and Future Work
We have explored the application of posterior dis-
course and lexical constraints for the analysis of the
information structure of scientific documents. Our
results are strong. Our constrained model outper-
forms standard feature-based models by a large mar-
gin in both the fully and the lightly supervised cases.
Even an unsupervised model based on these con-
straints provides substantial gains over feature-based
models for most AZ categories.
We provide a detailed analysis of the results
which reveals a number of interesting properties of
our model which may be useful for future research.
First, the constrained model significantly outper-
forms its unconstrained counterpart for low-medium
frequency categories while keeping the performance
on the major section category very similar to that of
the baseline model. Improved modeling of the major
category is one direction for future research. Sec-
ond, our full constraint set is most beneficial in the
lightly supervised case while the lexical constraints
alone yield equally good performance in the fully
supervised case. This calls for better understand-
ing of the role of discourse constraints for our task
as well as for the design of additional constraints
that can enhance the model performance either in
combination with the existing constraints or when
separately applied to the task. Finally, we demon-
strated that our top-down tree classification scheme
provides a substantial portion of our model?s impact.
A clear direction for future research is the design of
more fine-grained constraint taxonomies which can
enable efficient usage of other constraint types and
can result in further improvements in performance.
935
References
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
UAI ?09, pages 43?50, Arlington, Virginia, United
States. AUAI Press.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.,
22(1):39?71.
Catherine Blake. 2009. Beyond genes, proteins, and
abstracts: Identifying scientific claims from full-text
biomedical articles. J Biomed Inform, 43(2):173?89.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the write stuff: Automatic identification of
discourse structure in student essays. IEEE Intelligent
Systems, 18(1):32?39.
M.W. Chang, L. Ratinovc, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
Stanley F. Chen, Ronald Rosenfeld, and Associate Mem-
ber. 2000. A survey of smoothing techniques for me
models. IEEE Transactions on Speech and Audio Pro-
cessing, 8:37?50.
Danish Contractor, Yufan Guo, and Anna Korhonen.
2012. Using argumentative zones for extractive sum-
marization of scientific articles. In COLING.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguisti-
cally motivated large-scale nlp with c&c and boxer. In
Proceedings of the ACL 2007 Demonstrations Session,
pages 33?36.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 595?602.
Miroslav Dud??k. 2007. Maximum entropy density
estimation and modeling geographic distributions of
species. Ph.D. thesis.
K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins
Karolinska, Lin Sun, and Ulla Stenius. 2010. Identi-
fying the information structure of scientific abstracts:
an investigation of three different schemes. In Pro-
ceedings of BioNLP, pages 99?107.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 273?283.
Ben Hachey and Claire Grover. 2006. Extractive sum-
marisation of legal texts. Artif. Intell. Law, 14:305?
345.
K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka.
2008. Identifying sections in scientific abstracts us-
ing conditional random fields. In Proceedings of 3rd
International Joint Conference on Natural Language
Processing, pages 381?388.
E. T. Jaynes. 1957. Information Theory and Statistical
Mechanics. Physical Review Online Archive (Prola),
106(4):620?630.
F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL, pages 209?216.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.
2010. Corpora for the conceptualisation and zoning of
scientific papers. In Proceedings of LREC?10.
Maria Liakata, Shyamasree Saha, Simon Dobnik, Colin
Batchelor, and Dietrich Rebholz-Schuhmann. 2012.
Automatic recognition of conceptualisation zones in
scientific articles and two life science applications.
Bioinformatics, 28:991?1000.
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings of
BioNLP-06, pages 65?72.
G. Mann and A. McCallum. 2007. Simple, robust, scal-
able semi-supervised learning via expectation regular-
ization. In ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of condi-
tional random fields. In ACL.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of ACL 2012, pages 795?804.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate
argumentative zoning with maximum entropy models.
In Proceedings of the 2009 Workshop on Text and Ci-
tation Analysis for Scholarly Digital Libraries, pages
19?26.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006.
Zone analysis in biology articles as a basis for in-
formation extraction. International Journal of Med-
ical Informatics on Natural Language Processing in
Biomedicine and Its Applications, 75(6):468?487.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices
with Limited Storage. Mathematics of Computation,
35(151):773?782.
936
S. Della Pietra and V. Della Pietra. 1993. Statistical mod-
eling by me. Technical report, IBM.
Roi Reichart and Anna Korhonen. 2012. Document and
corpus level inference for unsupervised and transduc-
tive learning of information structure of scientic docu-
ments. In Proceedings of COLING 2012.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. Int J Med Inform, 76(2-3):195?200.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008.
Multi-dimensional classification of biomedical text:
Toward automated, practical provision of high-utility
text to diverse users. Bioinformatics, 24(18):2086?
2093.
L. Sun and A. Korhonen. 2009. Improving verb cluster-
ing with automatically acquired selectional preference.
In Proceedings of EMNLP, pages 638?647.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75(6):488?495.
S. Teufel and M. Moens. 2002. Summarizing scien-
tific articles: Experiments with relevance and rhetor-
ical status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards discipline-independent argumentative zoning:
Evidence from chemistry and computational linguis-
tics. In EMNLP.
V. N. Vapnik. 1998. Statistical learning theory. Wiley,
New York.
Andrea Varga, Daniel Preotiuc-Pietro, and Fabio
Ciravegna. 2012. Unsupervised document zone iden-
tification using probabilistic graphical models. In
Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12).
B. Webber, M. Egg, and V. Kordoni. 2011. Discourse
structure and language technology. Natural Language
Engineering, 18:437?490.
937
Proceedings of NAACL-HLT 2013, pages 1142?1151,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Tensor-based Factorization Model of Semantic Compositionality
Tim Van de Cruys
IRIT ? UMR 5505
CNRS
Toulouse, France
tim.vandecruys@irit.fr
Thierry Poibeau?
LaTTiCe ? UMR 8094
CNRS & ENS
Paris, France
thierry.poibeau@ens.fr
Anna Korhonen
Computer Laboratory & DTAL?
University of Cambridge
United Kingdom
anna.korhonen@cl.cam.ac.uk
Abstract
In this paper, we present a novel method for the
computation of compositionality within a distri-
butional framework. The key idea is that com-
positionality is modeled as a multi-way interac-
tion between latent factors, which are automat-
ically constructed from corpus data. We use
our method to model the composition of sub-
ject verb object triples. The method consists
of two steps. First, we compute a latent factor
model for nouns from standard co-occurrence
data. Next, the latent factors are used to induce
a latent model of three-way subject verb object
interactions. Our model has been evaluated on
a similarity task for transitive phrases, in which
it exceeds the state of the art.
1 Introduction
In the course of the last two decades, significant
progress has been made with regard to the automatic
extraction of lexical semantic knowledge from large-
scale text corpora. Most work relies on the distribu-
tional hypothesis of meaning (Harris, 1954), which
states that words that appear within the same contexts
tend to be semantically similar. A large number of
researchers have taken this dictum to heart, giving
rise to a plethora of algorithms that try to capture
the semantics of words by looking at their distribu-
tion in text. Up till now, however, most work on the
automatic acquisition of semantics only deals with
individual words. The modeling of meaning beyond
the level of individual words ? i.e. the combination
of words into larger units ? is to a large degree left
unexplored.
The principle of compositionality, often attributed
to Frege, is the principle that states that the meaning
of a complex expression is a function of the meaning
of its parts and the way those parts are (syntactically)
combined (Frege, 1892). It is the fundamental prin-
ciple that allows language users to understand the
meaning of sentences they have never heard before,
by constructing the meaning of the complex expres-
sion from the meanings of the individual words. Re-
cently, a number of researchers have tried to reconcile
the framework of distributional semantics with the
principle of compositionality (Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Coecke et al,
2010; Socher et al, 2012). However, the absolute
gains of the systems remain a bit unclear, and a sim-
ple method of composition ? vector multiplication ?
often seems to produce the best results (Blacoe and
Lapata, 2012).
In this paper, we present a novel method for the
joint composition of a verb with its subject and di-
rect object. The key idea is that compositionality is
modeled as a multi-way interaction between latent
factors, which are automatically constructed from
corpus data. In order to adequately model the multi-
way interaction between a verb and its subject and
objects, a significant part of our method relies on
tensor algebra. Additionally, our method makes use
of a factorization model appropriate for tensors.
The remainder of the paper is structured as follows.
In section 2, we give an overview of previous work
that is relevant to the task of computing composition-
ality within a distributional framework. Section 3
presents a detailed description of our method, in-
cluding an overview of the necessary mathematical
1142
machinery. Section 4 illustrates our method with a
number of detailed examples. Section 5 presents a
quantitative evaluation, and compares our method
to other models of distributional compositionality.
Section 6, then, concludes and lays out a number of
directions for future work.
2 Previous Work
In recent years, a number of methods have been de-
veloped that try to capture compositional phenomena
within a distributional framework. One of the first
approaches to tackle compositional phenomena in a
systematic way is Mitchell and Lapata?s (2008) ap-
proach. They explore a number of different models
for vector composition, of which vector addition (the
sum of each feature) and vector multiplication (the
elementwise multiplication of each feature) are the
most important. They evaluate their models on a
noun-verb phrase similarity task, and find that the
multiplicative model yields the best results, along
with a weighted combination of the additive and mul-
tiplicative model.
Baroni and Zamparelli (2010) present a method
for the composition of adjectives and nouns. In their
model, an adjective is a linear function of one vector
(the noun vector) to another vector (the vector for the
adjective-noun pair). The linear transformation for a
particular adjective is represented by a matrix, and
is learned automatically from a corpus, using partial
least-squares regression.
Coecke et al (2010) present an abstract theoreti-
cal framework in which a sentence vector is a func-
tion of the Kronecker product of its word vectors,
which allows for greater interaction between the dif-
ferent word features. A number of instantiations of
the framework are tested experimentally in Grefen-
stette and Sadrzadeh (2011a) and Grefenstette and
Sadrzadeh (2011b). The key idea is that relational
words (e.g. adjectives or verbs) have a rich (multi-
dimensional) structure that acts as a filter on their
arguments. Our model uses an intuition similar to
theirs.
Socher et al (2012) present a model for composi-
tionality based on recursive neural networks. Each
node in a parse tree is assigned both a vector and
a matrix; the vector captures the actual meaning of
the constituent, while the matrix models the way
it changes the meaning of neighbouring words and
phrases.
Closely related to the work on compositionality
is research on the computation of word meaning in
context. Erk and Pado? (2008, 2009) make use of
selectional preferences to express the meaning of a
word in context; the meaning of a word in the pres-
ence of an argument is computed by multiplying the
word?s vector with a vector that captures the inverse
selectional preferences of the argument. Thater et
al. (2009, 2010) extend the approach based on se-
lectional preferences by incorporating second-order
co-occurrences in their model. And Dinu and La-
pata (2010) propose a probabilistic framework that
models the meaning of words as a probability distri-
bution over latent factors. This allows them to model
contextualized meaning as a change in the original
sense distribution. Dinu and Lapata use non-negative
matrix factorization (NMF) to induce latent factors.
Similar to their work, our model uses NMF ? albeit
in a slightly different configuration ? as a first step
towards our final factorization model.
In general, latent models have proven to be useful
for the modeling of word meaning. One of the best
known latent models of semantics is Latent Seman-
tic Analysis (Landauer and Dumais, 1997), which
uses singular value decomposition in order to auto-
matically induce latent factors from term-document
matrices. Another well known latent model of mean-
ing, which takes a generative approach, is Latent
Dirichlet Allocation (Blei et al, 2003).
Tensor factorization has been used before for the
modeling of natural language. Giesbrecht (2010)
describes a tensor factorization model for the con-
struction of a distributional model that is sensitive to
word order. And Van de Cruys (2010) uses a tensor
factorization model in order to construct a three-way
selectional preference model of verbs, subjects, and
objects. Our underlying tensor factorization ? Tucker
decomposition ? is the same as Giesbrecht?s; and
similar to Van de Cruys (2010), we construct a la-
tent model of verb, subject, and object interactions.
The way our model is constructed, however, is sig-
nificantly different. The former research does not
use any syntactic information for the construction
of the tensor, while the latter makes use of a more
restricted tensor factorization model, viz. parallel
factor analysis (Harshman and Lundy, 1994).
1143
The idea of modeling compositionality by means
of tensor (Kronecker) product has been proposed
in the literature before (Clark and Pulman, 2007;
Coecke et al, 2010). However, the method presented
here is the first that tries to capture compositional
phenomena by exploiting the multi-way interactions
between latent factors, induced by a suitable tensor
factorization model.
3 Methodology
3.1 Mathematical preliminaries
The methodology presented in this paper requires
a number of concepts and mathematical operations
from tensor algebra, which are briefly reviewed in
this section. The interested reader is referred to Kolda
and Bader (2009) for a more thorough introduction
to tensor algebra (including an overview of various
factorization methods).
A tensor is a multidimensional array; it is the gen-
eralization of a matrix to more than two dimensions,
or modes. Whereas matrices are only able to cap-
ture two-way co-occurrences, tensors are able to cap-
ture multi-way co-occurrences.1 Following prevail-
ing convention, tensors are represented by boldface
Euler script notation (X), matrices by boldface capi-
tal letters (X), vectors by boldface lower case letters
(x), and scalars by italic letters (x).
The n-mode product of a tensor X ? RI1?I2?...?IN
with a matrix U ? RJ?In is denoted by X?n U, and
is defined elementwise as
(X?n U)i1...in?1 jin+1...iN =
In
?
in=1
xi1i2...iN u jin (1)
The Kronecker product of matrices A ? RI?J and
B?RK?L is denoted by A?B. The result is a matrix
of size (IK)? (JL), and is defined by
A?B =
?
?
?
?
?
a11B a12B ? ? ? a1JB
a21B a22B ? ? ? a2JB
...
...
. . .
...
aI1B aI2B . . . aIJB
?
?
?
?
?
(2)
1In this research, we limit ourselves to three-way co-
occurrences of verbs, subject, and objects, modelled using a
three-mode tensor.
A special case of the Kronecker product is the
outer product of two vectors a ? RI and b ? RJ , de-
noted a?b. The result is a matrix A ? RI?J obtained
by multiplying each element of a with each element
of b.
Finally, the Hadamard product, denoted A ?B,
is the elementwise multiplication of two matrices
A ? RI?J and B ? RI?J , which produces a matrix
that is equally of size I? J.
3.2 The construction of latent noun factors
The first step of our method consists in the construc-
tion of a latent factor model for nouns, based on their
context words. For this purpose, we make use of non-
negative matrix factorization (Lee and Seung, 2000).
Non-negative matrix factorization (NMF) minimizes
an objective function ? in our case the Kullback-
Leibler (KL) divergence ? between an original matrix
VI?J and WI?KHK?J (the matrix multiplication of
matrices W and H) subject to the constraint that all
values in the three matrices be non-negative. Param-
eter K is set  I,J so that a reduction is obtained
over the original data. The factorization model is
represented graphically in figure 1.
= xV W H
k
k
noun
s
context words
noun
s
context words
Figure 1: Graphical representation of NMF
NMF can be computed fairly straightforwardly,
alternating between the two iterative update rules
represented in equations 3 and 4. The update rules
are guaranteed to converge to a local minimum in the
KL divergence.
Ha? ?Ha?
?i Wia
Vi?
(WH)i?
?k Wka
(3)
Wia?Wia
?? Ha?
Vi?
(WH)i?
?v Hav
(4)
3.3 Modeling multi-way interactions
In our second step, we construct a multi-way interac-
tion model for subject verb object (svo) triples, based
1144
on the latent factors induced in the first step. Our
latent interaction model is inspired by a tensor factor-
ization model called Tucker decomposition (Tucker,
1966), although our own model instantiation differs
significantly. In order to explain our method, we
first revisit Tucker decomposition, and subsequently
explain how our model is constructed.
3.3.1 Tucker decomposition
Tucker decomposition is a multilinear generaliza-
tion of the well-known singular value decomposition,
used in Latent Semantic Analysis. It is also known as
higher order singular value decomposition (HOSVD,
De Lathauwer et al (2000)). In Tucker decomposi-
tion, a tensor is decomposed into a core tensor, multi-
plied by a matrix along each mode. For a three-mode
tensor X ? RI?J?L, the model is defined as
X = G?1 A?2 B?3 C (5)
=
P
?
p=1
Q
?
q=1
R
?
r=1
gpqrap ?bq ? cr (6)
Setting P,Q,R I,J,L, the core tensor G repre-
sents a compressed, latent version of the original ten-
sor X; matrices A ?RI?P, B ?RJ?Q, and C ?RL?R
represent the latent factors for each mode, while
G ? RP?Q?R indicates the level of interaction be-
tween the different latent factors. Figure 2 shows a
graphical representation of Tucker decomposition.2
subjects
verb
s
object
s
=
object
s
k
k
k
verb
s
subjects
k
k
k
Figure 2: A graphical representation of Tucker decompo-
sition
2where P = Q = R = K, i.e. the same number of latent factors
K is used for each mode
3.3.2 Reconstructing a Tucker model from
two-way factors
Computing the Tucker decomposition of a tensor
is rather costly in terms of time and memory require-
ments. Moreover, the decomposition is not unique:
the core tensor G can be modified without affecting
the model?s fit by applying the inverse modification
to the factor matrices. These two drawbacks led us
to consider an alternative method for the construc-
tion of the Tucker model. Specifically, we consider
the factor matrices as given (as the output from our
first step), and proceed to compute the core tensor G.
Additionally, we do not use a latent representation
for the first mode, which means that the first mode is
represented by its original instances.
Our model can be straightforwardly applied to lan-
guage data. The core tensor G models the latent
interactions between verbs, subject, and objects. G
is computed by applying the n-mode product to the
appropriate mode of the original tensor (equation 7),
G=X?2 WT ?3 WT (7)
where XV?N?N is our original data tensor, consisting
of the weighted co-occurrence frequencies of svo
triples (extracted from corpus data), and WN?K is
our latent factor matrix for nouns. Note that we do
not use a latent representation for the verb mode. To
be able to efficiently compute the similarity of verbs
(both within and outside of compositional phrases),
only the subject and object mode are represented by
latent factors, while the verb mode is represented
by its original instances. This means that our core
tensor G will be of size V ?K?K.3 A graphical
representation is given in figure 3.
Note that both tensor X and factor matrices W are
non-negative, which means our core tensor G will
also be non-negative.
3.4 The composition of svo triples
In order to compute the composition of a particular
subject verb object triple ?s,v,o?, we first extract the
appropriate subject vector ws and object vector wo
(both of length K) from our factor matrix W, and
3It is straightforward to also construct a latent factor model
for verbs using NMF, and include it in the construction of our
core tensor; we believe such a model might have interesting
applications, but we save this as an exploration for future work.
1145
subjects
verb
s
object
s
=
object
s
k
k
verb
s
subjectskk
Figure 3: A graphical representation of our model instan-
tiation without the latent verb mode
compute the outer product of both vectors, resulting
in a matrix Y of size K?K.
Y = ws ?wo (8)
Our second and final step is then to weight the
original verb matrix Gv of latent interactions (the
appropriate verb slice of tensor G) with matrix Y,
containing the latent interactions of the specific sub-
ject and object. This is carried out by taking the
Hadamard product of Gv and Y.
Z = Gv ?Y (9)
4 Example
In this section, we present a number of example com-
putations that clarify how our model is able to capture
compositionality. All examples come from actual cor-
pus data, and are computed in a fully automatic and
unsupervised way.
Consider the following two sentences:
(1) The athlete runs a race.
(2) The user runs a command.
Both sentences contain the verb run, but they rep-
resent clearly different actions. When we compute
the composition of both instances of run with their
respective subject and object, we want our model to
show this difference.
To compute the compositional representation of
sentences (1) and (2), we proceed as follows. First,
we extract the latent vectors for subject and object
(wathlete and wrace for the first sentence, wuser and
wcommand for the second sentence) from matrix W.
Next, we compute the outer product of subject and
object ? wathlete ?wrace and wuser ?wcommand ? which
yields matrices Y?athlete,race? and Y?user,command?. By
virtue of the outer product, the matrices Y ? of size
K?K ? represent the level of interaction between the
latent factors of the subject and the latent factors of
the object. We can inspect these interactions by look-
ing up the factor pairs (i.e. matrix cells) with the high-
est values in the matrices Y. Table 1 presents the fac-
tor pairs with highest value for matrix Y?athlete,race?;
table 2 represents the factor pairs with highest value
for matrix Y?user,command?. In order to render the fac-
tors interpretable, we include the three most salient
words for the various factors (i.e. the words with the
highest value for a particular factor).
The examples in tables 1 and 2 give an impression
of the effect of the outer product: semantic features
of the subject combine with semantic features of the
object, indicating the extent to which these features
interact within the expression. In table 1, we notice
that animacy features (28, 195) and a sport feature
(25) combine with a ?sport event? feature (119). In
table 2, we see that similar animacy features (40,
195) and technological features (7, 45) combine with
another technological feature (89).
Similarly, we can inspect the latent interactions of
the verb run, which are represented in the tensor slice
Grun. Note that this matrix contains the verb seman-
tics computed over the complete corpus. The most
salient factor interactions for Grun are represented in
table 3.
Table 3 illustrates that different senses of the verb
run are represented within the matrix Grun. The first
two factor pairs hint at the ?organize? sense of the
verb (run a seminar). The third factor pair repre-
sents the ?transport? sense of the verb (the bus runs
every hour).4 And the fourth factor pair represents
the ?execute? or ?deploy? sense of run (run Linux,
run a computer program). Note that we only show
the factor pairs with the highest value; matrix G con-
tains a value for each pairwise combination of the
latent factors, effectively representing a rich latent
semantics for the verb in question.
The last step is to take the Hadamard product of
matrices Y with verb matrix G, which yields our final
4Obviously, hour is not an object of the verb, but due to
parsing errors it is thus represented.
1146
factors subject object value
?195,119? people (.008), child (.008), adolescent (.007) cup (.007), championship (.006), final (.005) .007
?25,119? hockey (.007), poker (.007), tennis (.006) cup (.007), championship (.006), final (.005) .004
?90,119? professionalism (.007), teamwork (.007), confi-
dence (.006)
cup (.007), championship (.006), final (.005) .003
?28,119? they (.004), pupil (.003), participant (.003) cup (.007), championship (.006), final (.005) .003
Table 1: Factor pairs with highest value for matrix Y?athlete,race?
factors subject object value
?7,89? password (.009), login (.007), username (.007) filename (.007), null (.006), integer (.006) .010
?40,89? anyone (.004), reader (.004), anybody (.003) filename (.007), null (.006), integer (.006) .007
?195,89? people (.008), child (.008), adolescent (.007) filename (.007), null (.006), integer (.006) .006
?45,89? website (.004), Click (.003), site (.003) filename (.007), null (.006), integer (.006) .006
Table 2: Factor pairs with highest value for matrix Y?user,command?
matrices, Zrun,?athlete,race? and Zrun,?user,command?. The
Hadamard product will act as a bidirectional filter
on the semantics of both the verb and its subject
and object: interactions of semantic features that are
present in both matrix Y and G will be highlighted,
while the other interactions are played down. The
result is a representation of the verb?s semantics tuned
to its particular subject-object combination. Note that
this final step can be viewed as an instance of function
application (Baroni and Zamparelli, 2010). Also
note the similarity to Grefenstette and Sadrzadeh?s
(2011a,2011b) approach, who equally make use of
the elementwise matrix product in order to weight
the semantics of the verb.
We can now go back to our original tensor G, and
compute the most similar verbs (i.e. the most similar
tensor slices) for our newly computed matrices Z.5
If we do this for matrix Zrun,?athlete,race?, our model
comes up with verbs finish (.29), attend (.27), and
win (.25). If, instead, we compute the most similar
verbs for Zrun,?user,command?, our model yields execute
(.42), modify (.40), invoke (.39).
Finally, note that the design of our model natu-
rally takes into account word order. Consider the
following two sentences:
(3) man damages car
(4) car damages man
5Similarity is calculated by measuring the cosine of the vec-
torized and normalized representation of the verb matrices.
Both sentences contain the exact same words, but the
process of damaging described in sentences (3) and
(4) is of a rather different nature. Our model is able
to take this difference into account: if we compute
Zdamage,?man,car? following sentence (3), our model
yields crash (.43), drive (.35), ride (.35) as most sim-
ilar verbs. If we do the same for Zdamage,?car,man? fol-
lowing sentence (4), our model instead yields scare
(.26), kill (.23), hurt (.23).
5 Evaluation
5.1 Methodology
In order to evaluate the performance of our tensor-
based factorization model of compositionality, we
make use of the sentence similarity task for transi-
tive sentences, defined in Grefenstette and Sadrzadeh
(2011a). This is an extension of the similarity task
for compositional models developed by Mitchell and
Lapata (2008), and constructed according to the same
guidelines. The dataset contains 2500 similarity
judgements, provided by 25 participants, and is pub-
licly available.6
The data consists of transitive verbs, each paired
with both a subject and an object noun ? thus form-
ing a small transitive sentence. Additionally, a ?land-
mark? verb is provided. The idea is to compose both
the target verb and the landmark verb with subject
and noun, in order to form two small compositional
6http://www.cs.ox.ac.uk/activities/
CompDistMeaning/GS2011data.txt
1147
factors subject object value
?128,181? Mathematics (.004), Science (.004), Economics
(.004)
course (.005), tutorial (.005), seminar (.005) .058
?293,181? organization (.007), association (.007), federa-
tion (.006)
course (.005), tutorial (.005), seminar (.005) .053
?60,140? rail (.011), bus (.009), ferry (.008) third (.004), decade (.004), hour (.004) .038
?268,268? API (.008), Apache (.007), Unix (.007) API (.008), Apache (.007), Unix (.007) .038
Table 3: Factor combinations for Grun
phrases. The system is then required to come up with
a suitable similarity score for these phrases. The cor-
relation of the model?s judgements with human judge-
ments (scored 1?7) is then calculated using Spear-
man?s ? . Two examples of the task are provided in
table 4.
p target subject object landmark sim
19 meet system criterion visit 1
21 write student name spell 6
Table 4: Two example judgements from the phrase simi-
larity task defined by Grefenstette and Sadrzadeh (2011a)
Grefenstette and Sadrzadeh (2011a) seem to cal-
culate the similarity score contextualizing both the
target verb and the landmark verb. Another possibil-
ity is to contextualize only the target verb, and com-
pute the similarity score with the non-contextualized
landmark verb. In our view, the latter option pro-
vides a better assessment of the model?s similar-
ity judgements, since contextualizing low-similarity
landmarks often yields non-sensical phrases (e.g. sys-
tem visits criterion). We provide scores for both
contextualized and non-contextualized landmarks.
We compare our results to a number of different
models. The first is Mitchell and Lapata?s (2008)
model, which computes the elementwise vector mul-
tiplication of verb, subject and object. The second
is Grefenstette and Sadrzadeh?s (2011b) best scoring
model instantiation of the categorical distributional
compositional model (Coecke et al, 2010). This
model computes the outer product of the subject and
object vector, the outer product of the verb vector
with itself, and finally the elementwise product of
both results. It yields the best score on the transitive
sentence similarity task reported to date.
As a baseline, we compute the non-contextualized
similarity score for target verb and landmark. The up-
per bound is provided by Grefenstette and Sadrzadeh
(2011a), based on interannotator agreement.
5.2 Implementational details
All models have been constructed using the UKWAC
corpus (Baroni et al, 2009), a 2 billion word corpus
automatically harvested from the web. From this data,
we accumulate the input matrix V for our first NMF
step. We use the 10K most frequent nouns, cross-
classified by the 2K most frequent context words.7
Matrix V is weighted using pointwise mutual infor-
mation (PMI, Church and Hanks (1990)).
A parsed version of the corpus is available, which
has been parsed with MaltParser (Nivre et al, 2006).
We use this version in order to extract our svo triples.
From these triples, we construct our tensor X, using
1K verbs ? 10K subjects ? 10K objects. Note once
again that the subject and object instances in the sec-
ond step are exactly the same as the noun instances
in the first step. Tensor X has been weighted using a
three-way extension of PMI, following equation 10
(Van de Cruys, 2011).
pmi3(x,y,z) = log
p(x,y,z)
p(x)p(y)p(z)
(10)
We set K = 300 as our number of latent factors.
The value was chosen as a trade-off between a model
that is both rich enough, and does not require an
excessive amount of memory (for the modeling of
the core tensor). The algorithm runs fairly effi-
ciently. Each NMF step is computed in a matter of
seconds, with convergence after 50?100 iterations.
The construction of the core tensor is somewhat more
7We use a context window of 5 words, both before and after
the target word; a stop list was used to filter out grammatical
function words.
1148
evolved, but does not exceed a wall time of 30 min-
utes. Results have been computed on a machine with
Intel Xeon 2.93Ghz CPU and 32GB of RAM.
5.3 Results
The results of the various models are presented in ta-
ble 5; multiplicative represents Mitchell and Lapata?s
(2008) multiplicative model, categorical represents
Grefenstette and Sadrzadeh?s (2011b) model, and
latent represents the model presented in this paper.
model contextualized non-contextualized
baseline .23
multiplicative .32 .34
categorical .32 .35
latent .32 .37
upper bound .62
Table 5: Results of the different compositionality models
on the phrase similarity task
In the contextualized version of the similarity task
(in which the landmark is combined with subject
and object), all three models obtain the same result
(.32). However, in the non-contextualized version
(in which only the target verb is combined with sub-
ject and object), the models differ in performance.
These differences are statistically significant.8 As
mentioned before, we believe the non-contextualized
version of the task gives a better impression of the
systems? ability to capture compositionality. The
contextualization of the landmark verb often yields
non-sensical combinations, such as system visits crite-
rion. We therefore deem it preferable to compute the
similarity of the target verb in composition (system
meets criterion) to the non-contextualized semantics
of the landmark verb (visit).
Note that the scores presented in this evalua-
tion (including the baseline score) are significantly
higher than the scores presented in Grefenstette and
Sadrzadeh (2011b). This is not surprising, since the
corpus we use ? UKWAC ? is an order of magni-
tude larger than the corpus used in their research ?
the British National Corpus (BNC). Presumably, the
scores are also favoured by our weighting measure.
8 p < 0.01; model differences have been tested using stratified
shuffling (Yeh, 2000).
In our experience, PMI performs better than weight-
ing with conditional probabilities.9
6 Conclusion
In this paper, we presented a novel method for the
computation of compositionality within a distribu-
tional framework. The key idea is that composition-
ality is modeled as a multi-way interaction between
latent factors, which are automatically constructed
from corpus data. We used our method to model
the composition of subject verb object combinations.
The method consists of two steps. First, we com-
pute a latent factor model for nouns from standard
co-occurrence data. Next, the latent factors are used
to induce a latent model of three-way subject verb
object interactions, represented by a core tensor. Our
model has been evaluated on a similarity task for tran-
sitive phrases, in which it matches and even exceeds
the state of the art.
We conclude with a number of future work issues.
First of all, we would like to extend our framework in
order to incorporate more compositional phenomena.
Our current model is designed to deal with the latent
modeling of subject verb object combinations. We
would like to investigate how other compositional
phenomena might fit within our latent interaction
framework, and how our model is able to tackle the
computation of compositionality across a differing
number of modes.
Secondly, we would like to further explore the
possibilities of our model in which all three modes
are represented by latent factors. The instantiation
of our model presented in this paper has two latent
modes, using the original instances of the verb mode
in order to efficiently compute verb similarity. We
think a full-blown latent interaction model might
prove to have interesting applications in a number of
NLP tasks, such as the paraphrasing of compositional
expressions.
Finally, we would like to test our method using a
number of different evaluation frameworks. We think
tasks of similarity judgement have their merits, but in
a way are also somewhat limited. In our opinion, re-
search on the modeling of compositional phenomena
within a distributional framework would substantially
9Contrary to the findings of Mitchell and Lapata (2008), who
report a high correlation with human similarity judgements.
1149
benefit from new evaluation frameworks. In particu-
lar, we think of a lexical substitution or paraphrasing
task along the lines of McCarthy and Navigli (2009),
but specifically aimed at the assessment of composi-
tional phenomena.
Acknowledgements
Tim Van de Cruys and Thierry Poibeau are supported
by the Centre National de la Recherche Scientifique
(CNRS, France), Anna Korhonen is supported by the
Royal Society (UK).
References
Brett W. Bader, Tamara G. Kolda, et al 2012. Matlab ten-
sor toolbox version 2.5. http://www.sandia.gov/
~tgkolda/TensorToolbox/.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1183?1193, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evaluation,
43(3):209?226.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic compo-
sition. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 546?556, Jeju Island, Korea, July. Association
for Computational Linguistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information & lexicography.
Computational Linguistics, 16(1):22?29.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In Pro-
ceedings of the AAAI Spring Symposium on Quantum
Interaction, pages 52?55.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributed model of meaning. Lambek Festschrift, Lin-
guistic Analysis, vol. 36, 36.
Lieven De Lathauwer, Bart De Moor, and Joseph Vande-
walle. 2000. A multilinear singular value decomposi-
tion. SIAM Journal on Matrix Analysis and Applica-
tions, 21(4):1253?1278.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October.
Katrin Erk and Sebastian Pado?. 2008. A structured vector
space model for word meaning in context. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 897?906, Waikiki,
Hawaii, USA.
Katrin Erk and Sebastian Pado?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics, pages
57?65, Athens, Greece.
Gottlob Frege. 1892. U?ber Sinn und Bedeutung.
Zeitschrift fu?r Philosophie und philosophische Kritik,
100:25?50.
Eugenie Giesbrecht. 2010. Towards a matrix-based dis-
tributional model of meaning. In Proceedings of the
NAACL HLT 2010 Student Research Workshop, pages
23?28. Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 1394?1404, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b.
Experimenting with transitive verbs in a discocat. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
62?66, Edinburgh, UK, July. Association for Computa-
tional Linguistics.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Richard A Harshman and Margaret E Lundy. 1994.
Parafac: Parallel factor analysis. Computational Statis-
tics & Data Analysis, 18(1):39?72.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Tamara G. Kolda and Jimeng Sun. 2008. Scalable tensor
decompositions for multi-aspect data mining. In ICDM
2008: Proceedings of the 8th IEEE International Con-
ference on Data Mining, pages 363?372, December.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis the-
1150
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211?240.
Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms
for non-negative matrix factorization. In Advances in
Neural Information Processing Systems 13, pages 556?
562.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language resources and
evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216?
2219.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings
of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Nat-
ural Language Learning, pages 1201?1211, Jeju Island,
Korea, July. Association for Computational Linguistics.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009.
Ranking paraphrases in context. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
44?47, Suntec, Singapore.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 948?957, Uppsala, Sweden.
Ledyard R. Tucker. 1966. Some mathematical notes on
three-mode factor analysis. Psychometrika, 31(3):279?
311.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induction.
Natural Language Engineering, 16(4):417?437.
Tim Van de Cruys. 2011. Two multivariate generaliza-
tions of pointwise mutual information. In Proceedings
of the Workshop on Distributional Semantics and Com-
positionality, pages 16?20, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947?953, Saarbru?cken, Germany.
1151
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 420?429,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning Syntactic Verb Frames Using Graphical Models
Thomas Lippincott
University of Cambridge
Computer Laboratory
United Kingdom
tl318@cam.ac.uk
Diarmuid O? Se?aghdha
University of Cambridge
Computer Laboratory
United Kingdom
do242@cam.ac.uk
Anna Korhonen
University of Cambridge
Computer Laboratory
United Kingdom
alk23@cam.ac.uk
Abstract
We present a novel approach for building
verb subcategorization lexicons using a simple
graphical model. In contrast to previous meth-
ods, we show how the model can be trained
without parsed input or a predefined subcate-
gorization frame inventory. Our method out-
performs the state-of-the-art on a verb clus-
tering task, and is easily trained on arbitrary
domains. This quantitative evaluation is com-
plemented by a qualitative discussion of verbs
and their frames. We discuss the advantages of
graphical models for this task, in particular the
ease of integrating semantic information about
verbs and arguments in a principled fashion.
We conclude with future work to augment the
approach.
1 Introduction
Subcategorization frames (SCFs) give a compact de-
scription of a verb?s syntactic preferences. These
two sentences have the same sequence of lexical
syntactic categories (VP-NP-SCOMP), but the first
is a simple transitive (?X understood Y?), while the
second is a ditransitive with a sentential complement
(?X persuaded Y that Z?):
1. Kim (VP understood (NP the evidence
(SCOMP that Sandy was present)))
2. Kim (VP persuaded (NP the judge) (SCOMP
that Sandy was present))
An SCF lexicon would indicate that ?persuade?
is likely to take a direct object and sentential com-
plement (NP-SCOMP), while ?understand? is more
likely to take just a direct object (NP). A compre-
hensive lexicon would also include semantic infor-
mation about selectional preferences (or restrictions)
on argument heads of verbs, diathesis alternations
(i.e. semantically-motivated alternations between
pairs of SCFs) and a mapping from surface frames
to the underlying predicate-argument structure. In-
formation about verb subcategorization is useful for
tasks like information extraction (Cohen and Hunter,
2006; Rupp et al, 2010), verb clustering (Korho-
nen et al, 2006b; Merlo and Stevenson, 2001) and
parsing (Carroll et al, 1998). In general, tasks that
depend on predicate-argument structure can benefit
from a high-quality SCF lexicon (Surdeanu et al,
2003).
Large, manually-constructed SCF lexicons
mostly target general language (Boguraev and
Briscoe, 1987; Grishman et al, 1994). However,
in many domains verbs exhibit different syntactic
behavior (Roland and Jurafsky, 1998; Lippincott
et al, 2010). For example, the verb ?develop?
has specific usages in newswire, biomedicine and
engineering that dramatically change its probability
distribution over SCFs. In a few domains like
biomedicine, the need for focused SCF lexicons
has led to manually-built resources (Bodenreider,
2004). Such resources, however, are costly, prone to
human error, and in domains where new lexical and
syntactic constructs are frequently coined, quickly
become obsolete (Cohen and Hunter, 2006). Data-
driven methods for SCF acquisition can alleviate
420
these problems by building lexicons tailored to
new domains with less manual effort, and higher
coverage and scalability.
Unfortunately, high quality SCF lexicons are dif-
ficult to build automatically. The argument-adjunct
distinction is challenging even for humans, many
SCFs have no reliable cues in data, and some SCFs
(e.g. those involving control such as type raising)
rely on semantic distinctions. As SCFs follow a Zip-
fian distribution (Korhonen et al, 2000), many gen-
uine frames are also low in frequency. State-of-the-
art methods for building data-driven SCF lexicons
typically rely on parsed input (see section 2). How-
ever, the treebanks necessary for training a high-
accuracy parsing model are expensive to build for
new domains. Moreover, while parsing may aid the
detection of some frames, many experiments have
also reported SCF errors due to noise from parsing
(Korhonen et al, 2006a; Preiss et al, 2007).
Finally, many SCF acquisition methods operate
with predefined SCF inventories. This subscribes to
a single (often language or domain-specific) inter-
pretation of subcategorization a priori, and ignores
the ongoing debate on how this interpretation should
be tailored to new domains and applications, such as
the more prominent role of adjuncts in information
extraction (Cohen and Hunter, 2006).
In this paper, we describe and evaluate a novel
probabilistic data-driven method for SCF acquisi-
tion aimed at addressing some of the problems with
current approaches. In our model, a Bayesian net-
work describes how verbs choose their arguments
in terms of a small number of frames, which are
represented as distributions over syntactic relation-
ships. First, we show that by allowing the infer-
ence process to automatically define a probabilistic
SCF inventory, we outperform systems with hand-
crafted rules and inventories, using identical syntac-
tic features. Second, by replacing the syntactic fea-
tures with an approximation based on POS tags, we
achieve state-of-the-art performance without relying
on error-prone unlexicalized or domain-specific lex-
icalized parsers. Third, we highlight a key advantage
of our method compared to previous approaches: the
ease of integrating and performing joint inference of
additional syntactic and semantic information. We
describe how we plan to exploit this in our future
research.
2 Previous work
Many state-of-the-art SCF acquisition systems take
grammatical relations (GRs) as input. GRs ex-
press binary dependencies between lexical items,
and many parsers produce them as output, with
some variation in inventory (Briscoe et al, 2006;
De Marneffe et al, 2006). For example, a subject-
relation like ?ncsubj(HEAD, DEPENDENT)? ex-
presses the fact that the lexical item referred to by
HEAD (such as a present-tense verb) has the lexi-
cal item referred to by DEPENDENT as its subject
(such as a singular noun). GR inventories include
direct and indirect objects, complements, conjunc-
tions, among other relations. The dependency rela-
tionships included in GRs correspond closely to the
head-complement structure of SCFs, which is why
they are the natural choice for SCF acquisition.
There are several SCF lexicons for general lan-
guage, such as ANLT (Boguraev and Briscoe, 1987)
and COMLEX (Grishman et al, 1994), that depend
on manual work. VALEX (Preiss et al, 2007) pro-
vides SCF distributions for 6,397 verbs acquired
from a parsed general language corpus via a system
that relies on hand-crafted rules. There are also re-
sources which provide information about both syn-
tactic and semantic properties of verbs: VerbNet
(Kipper et al, 2008) draws on several hand-built
and semi-automatic sources to link the syntax and
semantics of 5,726 verbs. FrameNet (Baker et al,
1998) provides semantic frames and annotated ex-
ample sentences for 4,186 verbs. PropBank (Palmer
et al, 2005) is a corpus where each verb is annotated
for its arguments and their semantic roles, covering
a total of 4,592 verbs.
There are many language-specific SCF acquisi-
tion systems, e.g. for French (Messiant, 2008),
Italian (Lenci et al, 2008), Turkish (Han et al,
2008) and Chinese (Han et al, 2008). These typ-
ically rely on language-specific knowledge, either
directly through heuristics, or indirectly through
parsing models trained on treebanks. Furthermore,
some require labeled training instances for super-
vised (Uzun et al, 2008) or semi-supervised (Han
et al, 2008) learning algorithms.
Two state-of-the-art data-driven systems for En-
glish verbs are those that produced VALEX, Preiss et
al. (2007), and the BioLexicon (Venturi et al, 2009).
421
The Preiss system extracts a verb instance?s GRs us-
ing the Rasp general-language unlexicalized parser
(Briscoe et al, 2006) as input, and based on hand-
crafted rules, maps verb instances to a predefined
inventory of 168 SCFs. Filtering is then performed
to remove noisy frames, with methods ranging from
a simple single threshold to SCF-specific hypothesis
tests based on external verb classes and SCF inven-
tories. The BioLexicon system extracts each verb in-
stance?s GRs using the lexicalized Enju parser tuned
to the biomedical domain (Miyao, 2005). Each
unique GR-set considered a potential SCF, and an
experimentally-determined threshold is used to fil-
ter low-frequency SCFs.
Note that both methods require extensive man-
ual work: the Preiss system involves the a priori
definition of the SCF inventory, careful construc-
tion of matching rules, and an unlexicalized pars-
ing model. The BioLexicon system induces its SCF
inventory automatically, but requires a lexicalized
parsing model, rendering it more sensitive to domain
variation. Both rely on a filtering stage that depends
on external resources and/or gold standards to select
top-performing thresholds. Our method, by contrast,
does not use a predefined SCF inventory, and can
perform well without parsed input.
Graphical models have been increasingly popu-
lar for a variety of tasks such as distributional se-
mantics (Blei et al, 2003) and unsupervised POS
tagging (Finkel et al, 2007), and sampling methods
allow efficient estimation of full joint distributions
(Neal, 1993). The potential for joint inference of
complementary information, such as syntactic verb
and semantic argument classes, has a clear and in-
terpretable way forward, in contrast to the pipelined
methods described above. This was demonstrated in
Andrew et al (2004), where a Bayesian model was
used to jointly induce syntactic and semantic classes
for verbs, although that study relied on manually
annotated data and a predefined SCF inventory and
MLE. More recently, Abend and Rappoport (2010)
trained ensemble classifiers to perform argument-
adjunct disambiguation of PP complements, a task
closely related to SCF acquisition. Their study em-
ployed unsupervised POS tagging and parsing, and
measures of selectional preference and argument
structure as complementary features for the classi-
fier.
Finally, our task-based evaluation, verb clustering
with Levin (1993)?s alternation classes as the gold
standard, was previously conducted by Joanis and
Stevenson (2003), Korhonen et al (2008) and Sun
and Korhonen (2009).
3 Methodology
In this section we describe the basic components of
our study: feature sets, graphical model, inference,
and evaluation.
3.1 Input and feature sets
We tested several feature sets either based on, or
approximating, the concept of grammatical relation
described in section 2. Our method is agnostic re-
garding the exact definition of GR, and for example
could use the Stanford inventory (De Marneffe et al,
2006) or even an entirely different lexico-syntactic
formalism like CCG supertags (Curran et al, 2007).
In this paper, we distinguish ?true GRs? (tGRs), pro-
duced by a parser, and ?pseudo GRs? (pGRs), a
POS-based approximation, and employ subscripts to
further specify the variations described below. Our
input has been parsed into Rasp-style tGRs (Briscoe
et al, 2006), which facilitates comparison with pre-
vious work based on the same data set.
We?ll use a simple example sentence to illustrate
how our feature sets are extracted from CONLL-
formatted data (Nivre et al, 2007). The CONLL
format is a common language for comparing output
from dependency parsers: each lexical item has an
index, lemma, POS tag, tGR in which it is the de-
pendent, and index to the corresponding head. Table
1 shows the relevant fields for the sentence ?We run
training programmes in Romania and other coun-
tries?.
We define the feature set for a verb occurrence as
the counts of each GR the verb participates in. Table
2 shows the three variations we tested: the simple
tGR type, with parameterization for the POS tags
of head and dependent, and with closed-class POS
tags (determiners, pronouns and prepositions) lexi-
calized. In addition, we tested the effect of limiting
the features to subject, object and complement tGRs,
indicated by adding the subscript ?lim?, for a total of
six tGR-based feature sets.
While ideally tGRs would give full informa-
422
Index Lemma POS Head tGR
1 we PPIS2 2 ncsubj
2 run VV0 0
3 training NN1 4 ncmod
4 programme NN2 2 dobj
5 in II 4 ncmod
6 romania NP1 7 conj
7 and CC 5 dobj
8 other JB 9 ncmod
9 country NN2 7 conj
Table 1: Simplified CONLL format for example sen-
tence ?We run training programmes in Romania and
other countries?. Head=0 indicates the token is the
root.
Name Features
tGR ncsubj dobj
tGRparam ncsubj(VV0,PPIS2) dobj(VV0,NN2)
tGRparam,lex ncsubj(VV0,PPIS2-we) dobj(VV0,NN2)
Table 2: True-GR features for example sentence:
note there are also tGR?,lim versions of each that
only consider subjects, objects and complements
and are not shown.
tion about the verb?s syntactic relationship to other
words, in practice parsers make (possibly prema-
ture) decisions, such as deciding that ?in? modifies
?programme?, and not ?run? in our example sen-
tence. An unlexicalized parser cannot distinguish
these based just on POS tags, while a lexicalized
parser requires a large treebank. We therefore define
pseudo-GRs (pGRs), which consider each (distance,
POS) pair within a given window of the verb to be
a potential tGR. Table 3 shows the pGR features for
the test sentence using a window of three. As with
tGRs, the closed-class tags can be lexicalized, but
there are no corresponding feature sets for param
(since they are already built from POS tags) or lim
(since there is no similar rule-based approach).
Name Features
pGR -1(PPIS2) 1(NN1) 2(NN2) 3(II)
pGRlex -1(PPIS2-we) 1(NN1) 2(NN2) 3(II-in)
Table 3: Pseudo-GR features for example sentence
with window=3
Whichever feature set is used, an instance is sim-
ply the count of each GR?s occurrences. We extract
instances for the 385 verbs in the union of our two
gold standards from the VALEX lexicon?s data set,
which was used in previous studies (Sun and Korho-
nen, 2009; Preiss et al, 2007) and facilitates com-
parison with that resource. This data set is drawn
from five general-language corpora parsed by Rasp,
and provides, on average, 7,000 instances per verb.
3.2 SCF extraction
Our graphical modeling approach uses the Bayesian
network shown in Figure 1. Its generative story
is as follows: when a verb is instantiated, an SCF
is chosen according to a verb-specific multinomial.
Then, the number and type of syntactic arguments
(GRs) are chosen from two SCF-specific multino-
mials. These three multinomials are modeled with
uniform Dirichlet priors and corresponding hyper-
parameters ?, ? and ?. The model is trained via
collapsed Gibbs sampling, where the probability of
assigning a particular SCF s to an instance of verb v
with GRs (gr1 . . . grn) is the product
P (s|V erb = v,GRs = gr1 . . . grn) =
P (SCF = s|V erb = v)?
P (N = n|SCF = s)?
?
i=1:n
P (GR = gri|SCF = s)
The three terms, given the hyper-parameters and
conjugate-prior relationship between Dirichlet and
Multinomial distributions, can be expressed in terms
of current assignments of s to verb v ( csv ), s to
GR-count n ( csn ) and s to GR ( csg ), the corre-
sponding totals ( cv, cs ), the dimensionality of the
distributions ( |SCF |, |N | and |G| ) and the hyper-
parameters ?, ? and ?:
P (SCF = s|V erb = v) = (csv+?)/(cv+|SCF |?)
P (N = n|SCF = s) = (csn + ?)/(cs + |N |?)
P (GR = gri|SCF = s) = (csgri +?)/(cs + |G|?)
Note that N , the possible GR-count for an in-
stance, is usually constant for pGRs ( 2 ? window
), unless the verb is close to the start or end of the
sentence.
423
? // V erbxSCF
&&
V erbi

i ? I
SCFi //

Ni
||
SCFxNoo ?oo
GRi SCFxGRoo ?oo
Figure 1: Our simple graphical model reflecting subcategorization. Double-circles indicate an observed
value, arrows indicate conditional dependency. What constitutes a ?GR? depends on the feature set being
used.
We chose our hyper-parameters ? = ? = ? = .02
to reflect the characteristic sparseness of the phe-
nomena (i.e. verbs tend to take a small number of
SCFs, which in turn are limited to a small number
of realizations). For the pGRs we used a window
of 5 tokens: a verb?s arguments will fall within a
small window in the majority of cases, so there is
diminished return in expanding the window at the
cost of increased noise. Finally, we set our SCF
count to 40, about twice the size of the strictly syn-
tactic general-language gold standard we describe in
section 3.3. This overestimation allows some flex-
ibility for the model to define its inventory based
on the data; any supernumerary frames will act as
?junk frames? that are rarely assigned and hence
will have little influence. We run Gibbs sampling
for 1000 iterations, and average the final 100 sam-
ples to estimate the posteriors P (SCF |V erb) and
P (GR|SCF ). Variance between adjacent states?
estimates of P (SCF |V erb) indicates that the sam-
pling typically converges after about 100-200 itera-
tions.1
3.3 Evaluation
Quantitative: cluster gold standard
Evaluating the output of unsupervised methods is
not straightforward: discrete, expert-defined cate-
gories (like many SCF inventories) are unlikely to
line up perfectly with data-driven, probabilistic out-
put. Even if they do, finding a mapping between
them is a problem of its own (Meila, 2003).
1Full source code for this work is available at http://cl.
cam.ac.uk/?tl318/files/subcat.tgz
Our goal is to define a fair quantitative compari-
son between arbitrary SCF lexicons. An SCF lexi-
con makes two claims: first, that it defines a reason-
able SCF inventory. Second, that for each verb, it
has an accurate distribution over that inventory. We
therefore compare the lexicons based on their per-
formance on a task that a good SCF lexicon should
be useful for: clustering verbs into lexical-semantic
classes. Our gold standard is from (Sun and Korho-
nen, 2009), where 200 verbs were assigned to 17
classes based on their alternation patterns (Levin,
1993). Previous work (Schulte im Walde, 2009;
Sun and Korhonen, 2009) has demonstrated that the
quality of an SCF lexicon?s inventory and probabil-
ity estimates corresponds to its predictive power for
membership in such alternation classes.
To compare the performance of our feature sets,
we chose the simple and familiar K-Means cluster-
ing algorithm (Hartigan and Wong, 1979). The in-
stances are the verbs? SCF distributions, and we se-
lect the number of clusters by the Silhouette vali-
dation technique (Rousseeuw, 1987). The clusters
are then compared to the gold standard clusters with
the purity-based F-Score from Sun and Korhonen
(2009) and the more familiar Adjusted Rand Index
(Hubert and Arabie, 1985). Our main point of com-
parison is the VALEX lexicon of SCF distributions,
whose scores we report alongside ours.
Qualitative: manual gold standard
We also want to see how our results line up with
a traditional linguistic view of subcategorization,
but this requires digging into the unsupervised out-
424
put and associating anonymous probabilistic objects
with established categories. We therefore present
sample output in three ways: first, we show the
clustering output from our top-performing method.
Second, we plot the probability mass over GRs for
two anonymous SCFs that correspond to recogniz-
able traditional SCFs, and one that demonstrates un-
expected behavior. Third, we compared the out-
put for several verbs to a coarsened version of the
manually-annotated gold standard used to evaluate
VALEX (Preiss et al, 2007). We collapsed the orig-
inal inventory of 168 SCFs to 18 purely syntactic
SCFs based on their characteristic GRs and removed
frames that depend on semantic distinctions, leav-
ing the detection of finer-grained and semantically-
based frames for future work.
4 Results
4.1 Verb clustering
We evaluated SCF lexicons based on the eight fea-
ture sets described in section 3.1, as well as the
VALEX SCF lexicon described in section 2. Table 4
shows the performance of the lexicons in ascending
order.
Method Pur. F-score Adj. Rand
tGR .24 .02
tGRlim .27 .02
pGRlex .32 .09
tGRlim,param .35 .08
pGR .35 .10
VALEX .36 .10
tGRparam,lex .37 .10
tGRparam .39 .12
tGRlim,param,lex .44 .12
Table 4: Task-based evaluation of lexicons acquired
with each of the eight feature types, and the state-of-
the-art rule-based VALEX lexicon.
These results lead to several conclusions: first,
training our model on tGRs outperforms pGRs and
VALEX. Since the parser that produced them is
known to perform well on general language (Briscoe
et al, 2006), the tGRs are of high quality: it makes
sense that reverting to the pGRs is unnecessary in
this case. The interesting point is the major perfor-
mance gain over VALEX, which uses the same tGR
features along with expert-developed rules and in-
ventory.
Second, we achieve performance comparable to
VALEX using pGRs with a narrow window width.
Since POS tagging is more reliable and robust across
domains than parsing, retraining on new domains
will not suffer the effects of a mismatched parsing
model (Lippincott et al, 2010). It is therefore pos-
sible to use this method to build large-scale lexicons
for any new domain with sufficient data.
Third, lexicalizing the closed-class POS tags in-
troduces semantic information outside the scope
of the alternation-based definition of subcatego-
rization. For example, subdividing the indefinite
pronoun tag ?PN1? into ?PN1-anyone? and ?PN1-
anything? gives information about the animacy of
the verb?s arguments. Our results show this degrades
performance for both pGR and tGR features, unless
the latter are limited to tGRs traditionally thought to
be relevant for the task.
4.2 Qualitative analysis
Table 5 shows clusters produced by our top-scoring
method, GRparam,lex,lim. Some clusters are imme-
diately intelligible at the semantic level and corre-
spond closely to the lexical-semantic classes found
in Levin (1993). For example, clusters 1, 6, and 14
include member verbs of Levin?s SAY, PEER and
AMUSE classes, respectively. Some clusters are
based on broader semantic distinctions (e.g. cluster
2 which groups together verbs related to locations)
while others relate semantic classes purely based
on their syntactic similarity (e.g. the verbs in clus-
ter 17 share strong preference for ?to? preposition).
The syntactic-semantic nature of the clusters reflects
the multimodal nature of verbs and illustrates why a
comprehensive subcategorization lexicon should not
be limited to syntactic frames. This phenomenon is
also encouraging for future work to tease apart and
simultaneously exploit several verbal aspects via ad-
ditional latent structure in the model.
An SCF?s distribution over features can reveal its
place in the traditional definition of subcategoriza-
tion. Figure 2 shows the high-probability (>.02)
tGRs for one SCF: the large mass centered on di-
rect object tGRs indicates this approximates the no-
tion of ?transitive?. Looking at the verbs most likely
to take this SCF (?stimulate?, ?conserve?) confirms
425
1 exclaim, murmur, mutter, reply, retort, say,
sigh, whisper
2 bang, knock, snoop, swim, teeter
3 flicker, multiply, overlap, shine
4 batter, charter, compromise, overwhelm,
regard, sway, treat
5 abolish, broaden, conserve, deepen, eradi-
cate, remove, sharpen, shorten, stimulate,
strengthen, unify
6 gaze, glance, look, peer, sneer, squint, stare
7 coincide, commiserate, concur, flirt, inter-
act
8 grin, smile, wiggle
9 confuse, diagnose, march
10 mate, melt, swirl
11 frown, jog, stutter
12 chuckle, mumble, shout
13 announce, envisage, mention, report, state
14 frighten, intimidate, scare, shock, upset
15 bash, falter, snarl, wail, weaken
16 cooperate, eject, respond, transmit
17 affiliate, compare, contrast, correlate, for-
ward, mail, ship
Table 5: Clusters (of size >2 and <20) produced
using tGRparam,lex,lim
this. Figure 3 shows a complement-taking SCF,
which is far rarer than simple transitive but also
clearly induced by our model.
The induced SCF inventory also has some redun-
dancy, such as additional transitive frames beside
figure 2, and frames with poor probability estimates.
Most of these issues can be traced to our simplifying
assumption that each tGR is drawn independently
w.r.t. an instance?s other tGRs. For example, if an
SCF gives any weight to indirect objects, it gives
non-zero probability to an instance with only indi-
rect objects, an impossible case. This can lead to
skewed probability estimates: since some tGRs can
occur multiple times in a given instance (e.g. in-
direct objects and prepositional phrases) the model
may find it reasonable to create an SCF with all
probability focused on that tGR, ignoring all oth-
ers, such as in figure 4. We conclude that our inde-
pendence assumption was too strong, and the model
would benefit from defining more structure within
Figure 2: The SCF corresponding to transitive has
most probability centered on dobj (e.g. stimulate,
conserve, deepen, eradicate, broaden)
Figure 3: The SCF corresponding to verbs taking
complements has more probability on xcomp and
ccomp (e.g. believe, state, agree, understand, men-
tion)
instances.
The full tables necessary to compare verb SCF
distributions from our output with the manual gold
standard are prohibited by space, but a few exam-
ples reinforce the analysis above. The verbs ?load?
and ?fill? show particularly high usage of ditransi-
tive SCFs in the gold standard. In our inventory, this
is reflected in high usage of an SCF with probabil-
ity centered on indirect objects, but due to the inde-
pendence assumptions the frame has a correspond-
ing low probability on subjects and direct objects,
despite the fact that these necessarily occur along
with any indirect object. The verbs ?acquire? and
?buy? demonstrate both a strength of our approach
and a weakness of using parsed input: both verbs
426
Figure 4: This SCF is dominated by indirect objects
and complements, catering to verbs that may take
several such tGRs, at the expense of subjects
show high probability of simple transitive in our
output and the gold standard. However, the Rasp
parser often conflates indirect objects and preposi-
tional phrases due to its unlexicalized model. While
our system correctly gives high probability to ditran-
sitive for both verbs, it inherits this confusion and
over-estimates ?acquire??s probability mass for the
frame. This is an example of how bad decisions
made by the parser cannot be fixed by the graphi-
cal model, and an area where pGR features have an
advantage.
5 Conclusions and future work
Our study reached two important conclusions: first,
given the same data as input, an unsupervised prob-
abilistic model can outperform a hand-crafted rule-
based SCF extractor with a predefined inventory.
We achieve better results with far less effort than
previous approaches by allowing the data to gov-
ern the definition of frames while estimating the
verb-specific distributions in a fully Bayesian man-
ner. Second, simply treating POS tags within a
small window of the verb as pseudo-GRs produces
state-of-the-art results without the need for a pars-
ing model. This is particularly encouraging when
building resources for new domains, where com-
plex models fail to generalize. In fact, by integrat-
ing results from unsupervised POS tagging (Teichert
and Daume? III, 2009) we could render this approach
fully domain- and language-independent.
We did not dwell on issues related to choosing
our hyper-parameters or latent class count. Both of
these can be accomplished with additional sampling
methods: hyper-parameters of Dirichlet priors can
be estimated via slice sampling (Heinrich, 2009),
and their dimensionality via Dirichlet Process priors
(Heinrich, 2011). This could help address the redun-
dancy we find in the induced SCF inventory, with the
potential SCFs growing to accommodate the data.
Our initial attempt at applying graphical models
to subcategorization also suggested several ways to
extend and improve the method. First, the indepen-
dence assumptions between GRs in a given instance
turned out to be too strong. To address this, we could
give instances internal structure to capture condi-
tional probability between generated GRs. Second,
our results showed the conflation of several verbal
aspects, most notably the syntactic and semantic.
In a sense this is encouraging, as it motivates our
most exciting future work: augmenting this simple
model to explicitly capture complementary infor-
mation such as distributional semantics (Blei et al,
2003), diathesis alternations (McCarthy, 2000) and
selectional preferences (O? Se?aghdha, 2010). This
study targeted high-frequency verbs, but the use of
syntactic and semantic classes would also help with
data sparsity down the road. These extensions would
also call for a more comprehensive evaluation, aver-
aging over several tasks, such as clustering by se-
mantics, syntax, alternations and selectional prefer-
ences.
In concrete terms, we plan to introduce latent vari-
ables corresponding to syntactic, semantic and alter-
nation classes, that will determine a verb?s syntac-
tic arguments, their semantic realization (i.e. selec-
tional preferences), and possible predicate-argument
structures. By combining the syntactic classes with
unsupervised POS tagging (Teichert and Daume? III,
2009) and the selectional preferences with distribu-
tional semantics (O? Se?aghdha, 2010), we hope to
produce more accurate results on these complemen-
tary tasks while avoiding the use of any supervised
learning. Finally, a fundamental advantage of a data-
driven, parse-free method is that it can be easily
trained for new domains. We next plan to test our
method on a new domain, such as biomedical text,
where verbs are known to take on distinct syntactic
behavior (Lippincott et al, 2010).
427
6 Acknowledgements
The work in this paper was funded by the Royal So-
ciety, (UK), EPSRC (UK) grant EP/G051070/1 and
EU grant 7FP-ITC-248064. We are grateful to Lin
Sun and Laura Rimell for the use of their cluster-
ing and subcategorization gold standards, and the
ACL reviewers for their helpful comments and sug-
gestions.
References
Omri Abend and Ari Rappoport. 2010. Fully unsuper-
vised core-adjunct argument classification. In ACL
?10.
Galen Andrew, Trond Grenager, and Christopher Man-
ning. 2004. Verb sense and subcategorization: us-
ing joint inference to improve performance on com-
plementary tasks. EMNLP ?04.
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In COLING ACL ?98.
David Blei, Andrew Ng, Michael Jordan, and John Laf-
ferty. 2003. Latent dirichlet alocation. Journal of
Machine Learning Research.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical termi-
nology. Nucleic Acids Research, 32.
Bran Boguraev and Ted Briscoe. 1987. Large lexicons
for natural language processing. Computational Lin-
guistics, 13.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions.
John Carroll, Guido Minnen, and Ted Briscoe. 1998.
Can subcategorisation probabilities help a statistical
parser? In The 6th ACL/SIGDAT Workshop on Very
Large Corpora.
K Bretonnel Cohen and Lawrence Hunter. 2006. A
critical review of PASBio?s argument structures for
biomedical verbs. BMC Bioinformatics, 7.
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-Scale NLP with C&C and
Boxer. In ACL ?07.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC ?06.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2007. The infinite tree. In ACL ?07.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. Comlex syntax: building a computational lexi-
con. In COLING ?94.
Xiwu Han, Chengguo Lv, and Tiejun Zhao. 2008.
Weakly supervised SVM for Chinese-English cross-
lingual subcategorization lexicon acquisition. In The
11th Joint Conference on Information Science.
J.A. Hartigan and M.A. Wong. 1979. Algorithm AS 136:
A K-Means clustering algorithm. Journal of the Royal
Statistical Society. Series C (Applied Statistics).
Gregor Heinrich. 2009. Parameter estimation for text
analysis. Technical report, Fraunhofer IGD.
428
Gregor Heinrich. 2011. Infinite LDA implementing the
HDP with minimum code complexity. Technical re-
port, arbylon.net.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
partitions. Journal of Classification, 2.
Eric Joanis and Suzanne Stevenson. 2003. A general fea-
ture space for automatic verb classification. In EACL
?03.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
English verbs. In LREC ?08.
Anna Korhonen, Genevieve Gorrell, and Diana Mc-
Carthy. 2000. Statistical filtering and subcategoriza-
tion frame acquisition. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006a. A large subcategorization lexicon for natural
language processing applications. In LREC ?06.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2006b. Automatic classification of verbs in biomedi-
cal texts. In ACL ?06.
Anna Korhonen, Yuval Krymolowski, and Nigel Collier.
2008. The choice of features for classification of verbs
in biomedical texts. In COLING ?08.
Ro Lenci, Barbara Mcgillivray, Simonetta Montemagni,
and Vito Pirrelli. 2008. Unsupervised acquisition
of verb subcategorization frames from shallow-parsed
corpora. In LREC ?08.
Beth Levin. 1993. English Verb Classes and Alternation:
A Preliminary Investigation. University of Chicago
Press, Chicago, IL.
Thomas Lippincott, Anna Korhonen, and Diarmuid O?
Se?aghdha. 2010. Exploring subdomain variation in
biomedical language. BMC Bioinformatics.
Diana McCarthy. 2000. Using semantic preferences to
identify verbal participation in role switching alterna-
tions. In NAACL ?00.
Marina Meila. 2003. Comparing clusterings by the Vari-
ation of Information. In COLT.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions of
argument structure. Computational Linguistics.
Ce?dric Messiant. 2008. A subcategorization acquisition
system for French verbs. In ACL HLT ?08 Student Re-
search Workshop.
Yusuke Miyao. 2005. Probabilistic disambiguation mod-
els for wide-coverage HPSG parsing. In ACL ?05.
Radford M. Neal. 1993. Probabilistic inference using
markov chain Monte Carlo methods. Technical report,
University of Toronto.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In The CoNLL Shared Task Session
of EMNLP-CoNLL 2007.
Diarmuid O? Se?aghdha. 2010. Latent variable models of
selectional preference. In ACL ?10.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: an annotated corpus of
semantic roles. Computational Linguistics.
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007. A
system for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from corpora.
In ACL ?07.
Douglas Roland and Daniel Jurafsky. 1998. How verb
subcategorization frequencies are affected by corpus
choice. In ACL ?98.
Peter Rousseeuw. 1987. Silhouettes: a graphical aid
to the interpretation and validation of cluster analysis.
Journal of Computational and Applied Mathematics.
C.J. Rupp, Paul Thompson, William Black, and John Mc-
Naught. 2010. A specialised verb lexicon as the ba-
sis of fact extraction in the biomedical domain. In In-
terdisciplinary Workshop on Verbs: The Identification
and Representation of Verb Features.
Sabine Schulte im Walde. 2009. The induction of verb
frames and verb classes from corpora. In Corpus
Linguistics. An International Handbook. Mouton de
Gruyter.
Lin Sun and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired selectional
preferences. In EMNLP?09.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In ACL ?03.
Adam R. Teichert and Hal Daume? III. 2009. Unsuper-
vised part of speech tagging without a lexicon. In
NIPS Workshop on Grammar Induction, Representa-
tion of Language and Language Learning.
E. Uzun, Y. Klaslan, H.V. Agun, and E. Uar. 2008.
Web-based acquisition of subcategorization frames for
Turkish. In The Eighth International Conference on
Artificial Intelligence and Soft Computing.
Giulia Venturi, Simonetta Montemagni, Simone Marchi,
Yutaka Sasaki, Paul Thompson, John McNaught, and
Sophia Ananiadou. 2009. Bootstrapping a verb lex-
icon for biomedical information extraction. In Com-
putational Linguistics and Intelligent Text Processing.
Springer Berlin / Heidelberg.
429
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 862?872,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improved Lexical Acquisition through DPP-based Verb Clustering
Roi Reichart
University of Cambridge, UK
rr439@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Abstract
Subcategorization frames (SCFs), selec-
tional preferences (SPs) and verb classes
capture related aspects of the predicate-
argument structure. We present the first
unified framework for unsupervised learn-
ing of these three types of information.
We show how to utilize Determinantal
Point Processes (DPPs), elegant proba-
bilistic models that are defined over the
possible subsets of a given dataset and
give higher probability mass to high qual-
ity and diverse subsets, for clustering. Our
novel clustering algorithm constructs a
joint SCF-DPP DPP kernel matrix and uti-
lizes the efficient sampling algorithms of
DPPs to cluster together verbs with sim-
ilar SCFs and SPs. We evaluate the in-
duced clusters in the context of the three
tasks and show results that are superior to
strong baselines for each 1.
1 Introduction
Verb classes (VCs), subcategorization frames
(SCFs) and selectional preferences (SPs) capture
different aspects of predicate-argument structure.
SCFs describe the syntactic realization of verbal
predicate-argument structure, SPs capture the se-
mantic preferences verbs have for their arguments
and VCs in the Levin (1993) tradition provide a
shared level of abstraction for verbs that share
many aspects of their syntactic and semantic be-
havior.
These three of types of information have proved
useful for Natural Language Processing (NLP)
1The source code of the clustering algorithms and evalu-
ation is submitted with this paper and will be made publicly
available upon acceptance of the paper.
tasks which require information about predicate-
argument structure, including parsing (Shi and Mi-
halcea, 2005; Cholakov and van Noord, 2010;
Zhou et al, 2011), semantic role labeling (Swier
and Stevenson, 2004; Dang, 2004; Bharati et al,
2005; Moschitti and Basili, 2005; zap, 2008; Zapi-
rain et al, 2009), and word sense disambiguation
(Dang, 2004; Thater et al, 2010; O? Se?aghdha and
Korhonen, 2011), among many others.
Because lexical information is highly sensitive
to domain variation, approaches that can identify
VCs, SCFs and SPs in corpora have become in-
creasingly popular, e.g. (O?Donovan et al, 2005;
Schulte im Walde, 2006; Erk, 2007; Preiss et al,
2007; Van de Cruys, 2009; Reisinger and Mooney,
2011; Sun and Korhonen, 2011; Lippincott et al,
2012).
The task of SCF induction involves identifying
the arguments of a verb lemma and generalizing
about the frames (i.e. SCFs) taken by the verb,
where each frame includes a number of arguments
and their syntactic types. For example, in (1),
the verb ?show? takes the frame SUBJ-DOBJ-
CCOMP (subject, direct object, and clausal
complement).
(1) [A number of SCF acquisition papers]SUBJ
[show]VERB [their readers]DOBJ [which fea-
tures are most valuable for the acquisition
process]CCOMP.
SP induction involves identifying and classify-
ing the lexical items in a given argument slot. In
sentence (2), for example, the verb ?show? takes
the frame SUBJ-DOBJ. The direct object in this
frame is likely to be inanimate.
(2) [Most SCF and SP acquisition papers]SUBJ,
862
[show]VERB [no evidence to the usefulness of
joint learning leaning for these tasks]DOBJ.
Finally, VC induction involves clustering to-
gether verbs with similar meaning, reflected in
similar SCFs and SPs. For example, ?show? in the
above examples could get clustered together with
?demonstrate? and ?indicate?.
Because these challenging tasks capture com-
plementary information about predicate argument
structure, they should be able to inform and sup-
port each other. Recently, researchers have be-
gun to investigate the benefits of their joint learn-
ing. Schulte im Walde et al (2008) integrated SCF
and VC acquisition and used it for WordNet-based
SP classification. O? Se?aghdha (2010) presented a
?dual-topic? model for SPs that induces also verb
clusters. Both works reported SP evaluation with
promising results. Lippincott et al (2012) pre-
sented a joint model for inducing simple syntac-
tic frames and VCs. They reported high accuracy
results on VCs. de Cruys et al (2012) introduced
a joint model for SCF and SP acquisition. They
evaluated both the SCFs and SPs, obtaining rea-
sonable result on both tasks.
In this paper, we present the first unified frame-
work for unsupervised learning of the three types
of information - SCFs, SPs and VCs. Our frame-
work is based on Determinantal Point Processes
(DPPs, (Kulesza, 2012; Kulesza and Taskar,
2012c)), elegant probabilistic models that are de-
fined over the possible subsets of a given dataset
and give higher probability mass to high quality
and diverse subsets.
We first show how individual-task DPP kernel
matrices can be naturally combined to construct a
joint kernel. We use this to construct a joint SCF-
SP kernel. We then introduce a novel clustering
algorithm based on iterative DPP sampling which
can (contrary to other probabilistic frameworks
such as Markov random fields) be performed both
accurately and efficiently. When defined over the
joint SCF and SP kernel, this new algorithm can
be used to induce VCs that are valuable for both
tasks.
We also contribute by evaluating the value of
the clusters induced by our model for the acquisi-
tion of the three information types. Our evaluation
against a well-known VC gold standard shows that
our clustering model outperforms the state-of-the-
art verb clustering algorithm of Sun and Korhonen
(2009), in our setup where no manually created
SCF or SP data is available. Our evaluation against
a well-known SCF gold standard and in the con-
text of SP disambiguation tasks shows results that
are superior to strong baselines, demonstrating the
benefit our approach.
2 Previous Work
SCF acquisition Most current works induce SCFs
from the output of an unlexicalized parser (i.e.
a parser trained without SCF annotations) using
hand-written rules (Briscoe and Carroll, 1997; Ko-
rhonen, 2002; Preiss et al, 2007) or grammatical
relation (GR) co-occurrence statistics (O?Donovan
et al, 2005; Chesley and Salmon-Alt, 2006; Ienco
et al, 2008; Messiant et al, 2008; Lenci et al,
2008; Altamirano and Alonso i Alemany, 2010;
Kawahara and Kurohashi, 2010).
Only a handful of SCF induction works are
unsupervised. Carroll and Rooth (1996) applied
an EM-based approach to a context-free grammar
based model, Dkebowski (2009) used point-wise
co-occurrence of arguments in parsed Polish data
and Lippincott et al (2012) presented a Bayesian
network model for syntactic frame induction that
identifies SPs on argument types. However, the
frames induced by Lippincott et al (2012) do not
capture sets of arguments for verbs so are far sim-
pler than traditional SCFs.
Current approaches to SCF acquisition suffer
from lack of semantic information which is needed
to guide the purely syntax-driven acquisition pro-
cess. Previous works have showed the benefit of
hand-coded semantic information in SCF acquisi-
tion (Korhonen, 2002). We will address this prob-
lem in an unsupervised way: our approach is to
consider SCFs together with semantic SPs through
VCs which generalize over syntactically and se-
mantically similar verbs.
SP acquisition Considerable research has been
conducted on SP acquisition, with a variety of
unsupervised models proposed for this task that
use no hand-crafted information during training.
The latter approaches include latent variable mod-
els (O? Se?aghdha, 2010; Ritter and Etzioni, 2010;
Reisinger and Mooney, 2011), distributional sim-
ilarity methods (Bhagat et al, 2007; Basili et
al., 2007; Erk, 2007) and methods based on
non-negative tensor factorization (Van de Cruys,
2009). These works use a variety of linguistic fea-
tures in the acquisition process but none of them
863
integrates the three information types covered in
our work.
Verb clustering A variety of VC approaches
have been proposed in the literature. These in-
clude syntactic, semantic and mixed syntactic-
semantic classifications (Grishman et al, 1994;
Miller, 1995; Baker et al, 1998; Palmer et al,
2005; Schuler, 2006; Hovy et al, 2006). We fo-
cus on Levin style classes (Levin, 1993) which
are defined in terms of diathesis alternations and
capture generalizations over a range of syntactic
and semantic properties. Previous unsupervised
VC acquisition approaches clustered a variety of
linguistic features using different (e.g. K-means
and spectral) algorithms (Schulte im Walde, 2006;
Joanis et al, 2008; Sun et al, 2008; Li and Brew,
2008; Korhonen et al, 2008; Sun and Korhonen,
2009; Vlachos et al, 2009; Sun and Korhonen,
2011). The linguistic features included SCFs and
SPs, but these were induced separately and then
feeded as features to the clustering algorithm. Our
framework combines together SCF-motivated and
SP-motivated kernel matrices , and uses the joint
kernel to induce verb clusters which are likely to
be highly relevant for both tasks. Importantly, no
manual or automatic system for SCF or SP acqui-
sition has been utilized when constructing the ker-
nel matrices, we only consider features extracted
from the output of an unlexicalized parser. Our ap-
proach hence provides a framework for acquiring
valuable information for the three tasks together.
Joint Modeling A small number of works have
recently investigated joint approaches to SCFs,
SPs and VCs. Each of them has addressed only
a subset of the tasks and all but one have eval-
uated the performance in the context of one task
only. O? Se?aghdha (2010) presented a ?dual-topic?
model for SPs that induces VCs, reporting evalua-
tion of SPs only. Lippincott et al (2012) presented
a Bayesian network model for syntactic frame
(rather than full SCF) induction that induces VCs.
Only VCs are evaluated. de Cruys et al (2012)
presented a joint unsupervised model of SCF and
SP acquisition based on non-negative tensor fac-
torization. Both SCFs and SPs were evaluated. Fi-
nally, the model of Schulte im Walde et al (2008)
addresses the three types of information but SP
parameters are estimated with a WordNet based
method and only the SPs are evaluated. Although
evaluation of these recent joint models has been
partial, the results have been encouraging and fur-
ther motivate the development of a framework that
acquires the three types of information together.
3 The Unified Framework
In this section we present our unified framework.
Our idea is to utilize DPPs for verb clustering that
informs both SCF and SP acquisition. DPPs define
a probability distribution over the possible subsets
of a given set. These models assign higher prob-
ability mass to subsets that are both high quality
and diverse.
Our novel clustering algorithm makes use of
three DPP properties that are appealing for our
purpose: (1) The existence of efficient sam-
pling algorithms for these models, which enable
tractable sampling of high quality and diverse verb
subsets; (2) Such verb subsets form natural high
quality seeds for hierarchical clustering; and (3)
Given individual-task DPP kernel matrices there
are various simple and natural ways to combine
them into a new DPP kernel matrix.
Individual task DPP kernels represent (i) the
quality of a data point (verb) as its average feature-
based similarity with the other points in the data
set and (ii) the divergence between a pair of points
as the inverse similarity between them. For dif-
ferent tasks, different feature sets are used for the
kernel construction. The high quality and diverse
subsets sampled from the DPP model are consid-
ered good cluster seeds as they are likely to be rel-
atively uniformly spread and to provide good cov-
erage of the data set. The algorithm induces an
hierarchical clustering, which is particularly suit-
able for semantic tasks, where a set of clusters that
share a parent consists of pure members (i.e. most
of the points in each cluster member belong to the
same gold cluster) and together provide good cov-
erage of the verb space.
After a brief description of the Determinantal
Point Processes (DPP) framework (Section 3.1),
we discuss the construction of the joint DPP ker-
nel, given a kernel for each individual task, In sec-
tion 3.3 we present the DPP-Cluster clustering al-
gorithm.
3.1 Determinantal Point Processes
Determinantal point processes (DPPs) are elegant
probabilistic models of repulsion that offer effi-
cient and exact algorithms for sampling, marginal-
ization, conditioning, and other inference tasks.
Recently (Kulesza, 2012; Kulesza and Taskar,
864
2012c) introduced them to the machine learning
community and demonstrated their usefulness for
a variety of tasks including document summariza-
tion, image search, modeling non-overlapping hu-
man poses in images and video and automati-
cally building timelines of important news stories
(Kulesza and Taskar, 2010; Kulesza and Taskar,
2012a; Gillenwater et al, 2012; Kulesza and
Taskar, 2012b). Below we provide a brief descrip-
tion of the framework, a comprehensive survey
can be found in (Kulesza and Taskar, 2012c).
Given a set of items Y = {y1, . . . , yN}, a DPP
P defines a probability measure on the set of all
subsets of Y , 2Y . Kulesza and Taskar (2012c) re-
stricted their discussion of DDPs to L-ensembles,
where the probability of a subsetY ? Y is defined
through a positive semi-definite matrix L indexed
by the elements of Y:
PL(Y = Y ) =
det(LY )?
Y?Y det(LY )
= det(LY )det(L+ I)
(1)
Where I is the N ? N identity matrix and
det(L?) = 1. Since L is positive semi-definite, it
can be decomposed to L = BTB. This allows the
construction of an intuitively interpretable model
where each column Bi is the product of a quality
term qi ? R+ and a vector of (normalized) diver-
sity features ?i ? RD, ||?i|| = 1. In this model,
qi measures an inherent quality of the i ? th item
in Y while ?Ti ?j ? [?1, 1] is a similarity measure
between items i and j. With this representation we
can write:
Lij = qi?Ti ?jqj (2)
Sij = ?Ti ?j =
Lij?
LiiLjj
(3)
PL(Y = Y ) ? (
?
i?Y
q2i )det(SY ) (4)
It can be shown that the first term in equation 4 in-
creases with the quality of the selected items, and
the second term increases with their diversity. As
a consequence, this distribution places most of its
weight on sets that are both high quality and di-
verse.
Although the number of possible realizations of
Y is exponential in N , many inference procedures
can be performed accurately and efficiently (i.e.
in polynomial time which is very short in prac-
tice). In particular, sampling, which NP-hard for
alternative models such as Markov Random Fields
(MRFs), is efficient, theoretically and practically,
for DPPs.
3.2 Constructing a Joint Kernel Matrix
DPPs are particularly suitable for joint modeling
as they come with various simple and intuitive
ways to combine individual model kernel matrices
into a joint kernel. This stems from the fact that
every positive-semidefinite matrix forms a legal
DPP kernel (equation 1). Given individual model
DPP kernels, we would therefore like to combine
them into a positive-semidefinite matrix.
While there are various ways to construct a
positive-semidefinite matrix from two positive-
semidefinite matrices ? for example, by taking
their sum ? in this work we are motivated by the
product of experts approach (Hinton, 2002), rea-
soning that high quality assignments according to
a product of models have to be of high quality ac-
cording to each individual model, and sick for a
product combination. 2
In practice we construct the joint kernel in the
following way. We build on the aforementioned
property that a matrix L is positive semi-definite
iff L = BTB. Given two DPPs, PL1 defined by
L1 = AT1A1 and PL2 defined by L2 = AT2A2, we
construct the joint kernel L12:
L12 = L1L2L2L1 = CTC (5)
Where C = AT2A2AT1A1 and CT =
AT1A1AT2A2.
3.3 Clustering Algorithm
Algorithm (1) and Figure (1) provide a pseudo-
code of the algorithm and an example output. Be-
low is a detailed description.
Features Our algorithm builds two DPP ker-
nel matrices (the GenKernelMatrix function),
in which the rows and columns correspond to the
verbs in the data set, such that the (i, j)-th entry
corresponds to verbs number i and j. Following
equations 2 and 3 one matrix is built for SCF and
one for SP, and they are then combined into the
2Note that we do not take a product of the individual mod-
els but only of their kernel matrices. Yet, if we construct the
joint matrix by a multiplication then it follows from a simple
generalization of the Cauchy-Binet formula that its principle
minors, which define the subset probabilities (equation 1), are
a sum of multiplications of the principle minors of the indi-
vidual model kernels. Still, we do not have guarantees that
our choice of kernel combination is the right one. We leave
this for future research.
865
joint kernel matrix (the GenJointMat function)
following equation 5. Each kernel matrix requires
a proper feature representation ? and quality score
q.
In both kernels we represent a verb by the
counts of the grammatical relations (GRs) it par-
ticipates in. In the SCF kernel a GR is represented
by the GR type and the POS tags of the verb and
its arguments. In the SP kernels the GRs are rep-
resented by the POS tags of the verb and its ar-
guments as well as by the argument head word.
Based on this feature representation, the similarity
(opposite divergence) is encoded to the model by
equation 3 as the dot product between the normal-
ized feature vectors. The quality score qi of the
i-th verb is the average similarity of this verb with
the other verbs in the dataset.
Cluster set construction In its while loop, the
algorithm iteratively generates fixed-size cluster
sets such that each data point belongs to exactly
one cluster in one set. These cluster sets form
the leaf level of the tree in Figure (1). It does
so by extracting the T highest probability K-point
samples from a set of M subsets, each of which
sampled from the joint DPP model, and cluster-
ing them by the cluster procedure. The sampling
is done by the K-DPP sampling process ((Kulesza
and Taskar, 2012c), page 62) 3.
The cluster procedure first seeds a K-cluster
set with the highest probability sample. Then, it
gradually extends the clusters by iteratively map-
ping the samples, in decreasing order of probabil-
ity, to the existing clusters (them1Mapping func-
tion). Mapping is done by attaching every point
in the mapped subset to its closet cluster, where
the distance between a point and the cluster is the
maximum over the distances between the point
and each of the points in the cluster. The map-
ping is many-to-one, that is, multiple points in the
subset can be assigned to the same cluster.
Based on the DPP properties, the higher the
probability of a sampled subset, the more likely it
is to consist of distinct points that provide a good
coverage of the verb set. By iteratively extending
the clusters with high probability subsets, we thus
expect each cluster set to consist of clusters that
demonstrate these properties.
3K-DPP is a DPP conditioned on the sample size. As
shown in ((Kulesza and Taskar, 2012c), Section 2.4.3) this
conditional distribution is also a DPP. We could have obtained
samples of size K by sampling the DPP and rejecting sam-
ples of other sizes but this would have been slower.
SET 1-2-3-4 (45,K)
SET 1-2 (23,K)
SET1 (12,K) SET2 (11,K)
SET3-4(22,K)
SET 3 (12,K) SET4 (10,K)
Figure 1: An example output hierarchy of DPP-
Cluster for a set of 45 data points. Each set is
augmented with the number of points (left num-
ber) and clusters (right number) it includes. The
iterative DPP-samples clustering (the While loop)
generates the lowest level of the tree, by dividing
the data set into cluster sets, each of which con-
sists of K clusters. Each point in the data set be-
longs to exactly one cluster in exactly one set. The
agglomerative clustering then iteratively combines
cluster sets such that in each iteration two sets are
combined to one set with K clusters.
Agglomerative Clustering Finally, the
AgglomerativeClustering function builds a
hierarchy of cluster sets, by iteratively combining
cluster set pairs. In each iteration it computes the
similarity between any such pair, defined to be the
lowest similarity between their cluster members,
which is in turn defined to be the lowest cosine
similarity between their point members. The most
similar cluster sets are combined such that each
of the clusters in one set is mapped to its most
similar cluster in the other set. In this step the
algorithm generates data partitions at different
granularity levels from finest (from the iterative
sampling step) to the coarsest set (generated by
the last agglomerative clustering iteration and
consisting of exactly K clusters). This property is
useful as the optimal level of generalization may
be task dependent.
4 Evaluation
Data sets and gold standards We evaluated the
SCFs and verb clusters on gold standard datasets.
We based our set of the largest available joint set
for SCFs and VCs - that of (de Cruys et al, 2012).
It provides SCF annotations for 183 verbs (an av-
erage of 12.3 SCF types per verb) obtained by
annotating 250 corpus occurrences per verb with
the SCF types of (de Cruys et al, 2012). The
verbs represent a range of Levin classes at the top
level of the hierarchy in VerbNet (Kipper-Schuler,
2005). Where a verb has more than one Verb-
Net class, we assign it to the one supported by the
highest number of member verbs. To ensure suf-
866
|C| = 20, 21.6 |C| = 40, 41 |C| = 60, 58.6 |C| = 69, 77.6 |C| = 89, 97.4
Model R P F R P F R P F R P F R P F
DPP-cluster 93.1 17.3 29.3 77.9 25.4 38.3 63 31.9 42.3 43.8 33.6 38.1 34.4 40.6 37.2
AC 67 17.8 28.2 46.6 24 31.7 40.5 29.4 34 33 34.9 33.9 24.7 41.1 30.9
SC 32.1 27.5 29.6 26.6 35.9 30.6 23.7 41.5 30.2 22.8 43.6 29.9 21.6 48.7 29.9
Table 1: Verb clustering evaluation for the last five iterations of our DPP-cluster model and the baseline
agglomerative clustering algorithm (AC, see text for its description), and for the spectral clustering (SC)
algorithm of (Sun and Korhonen, 2009) with the same number of clusters induced by DPP-cluster. |C| is
the number of clusters for DPP-cluster and SC (first number) and for AC (second number). The F-score
performance of DPP-cluster is superior in 4 out of 5 cases.
Arg. per verb P (DPP) P(AC) P (B) P (NF) R (DPP) R (AC) R (B) R(NF) ERR DPP ERR AC ERR B
? 200 (133 verbs) 27.3 23.7 27.3 23.1 9.9 7.6 8 11.3 3.4 0.16 1.55
? 600 (205 verbs) 26.5 25 27.3 22.6 14.8 11.5 11.9 16.6 2.3 0.50 1.1
? 1000 (238 verbs) 24.6 23.6 25.6 21.1 17.5 13.8 14.7 19.8 1.6 0.42 0.95
Table 2: Performance of the Corpus Statistics SP baseline (non-filtered, NF) as well as for three filtering
methods: frequency based (filter-baseline, B), DPP-cluster based (DPP) and AC cluster based (AC). P
(method) and R (method) present the precision and recall of the method respectively. The error reduc-
tion ratio (ERR) is the ratio between the reduction in precision error achieved by each method and the
increase in recall error (each method is compared to the NF baseline). Ratio greater than 1 means that
the reduction in precision error is larger than the increase in recall error (see text for exact definition).
DPP based filtering provides substantially better ratio.
ficient representation of each class, we collected
from VerbNet the verbs for which at least one of
the possible classes is represented in the 183 verbs
set by at least one and at most seven verbs. This
yielded 101 additional verbs which we added to
the gold standard with the initial 183 verbs.
We parsed the BNC corpus with the RASP
parser (Briscoe et al, 2006) and used it for feature
extraction. Since 176 out of the 183 initial verbs
are represented in this corpus, our final gold stan-
dard consists of 34 classes containing 277 verbs,
of which 176 have SCF gold standard and has been
evaluated for this task. We set the parameters of
our algorithm on an held-out data, consisting of
different verbs than those used in our experiments,
to be M = 10000, K = 20 and T = 10.
Clustering Evaluation We first evaluate the
quality of the clusters induced by our algorithm
(DPP-cluster) compared to the gold standard VCs
(table 1). To evaluate the importance of the DPP
component, we compare to the performance of a
version of our algorithm where everything is kept
fixed except from the sampling which is done from
a uniform distribution rather than from the DPP
joint kernel (this model is denoted in the table
with AC for agglomerative clustering) 4. We also
compare to the state-of-the-art spectral clustering
method of Sun and Korhonen (2009) where our
4Importantly, the kernel matrix L used in the agglomera-
tive clustering process is also used by AC.
kernel matrix is used for the distance between data
points (SC) 5.
We evaluated the unified cluster set induced in
each iteration of our algorithm and of the AC base-
line and induced the same number of clusters as in
each iteration of our algorithm using the SC base-
line. Since the number of clusters in each iteration
is not an argument for our algorithm or for the AC
baseline, the number of clusters slightly differ be-
tween the two. The AC and SC baseline results
were averaged over 5 and 100 runs respectively.
DPP-cluster has produced identical output across
runs.
The table demonstrates the superiority of the
DPP-cluster model. For four out of five conditions
its F-score performance outperforms the baselines
by 4.2-8.3%. Moreover, in all conditions its recall
performances are substantially higher than those
of the baselines (by 9.7-26.1%). Note that DPP-
cluster runs for 17 iterations while the AC baseline
performs only 6. We therefore evaluated only the
last 5 iterations of each model 6.
SCF evaluation For this evaluation, we first
built a baseline SCF lexicon based on the parsed
5Sun and Korhonen (2009) report better results than those
we report for their algorithm (on a different data set). Note,
however, that they used the output of a rule-based SCF sys-
tem as a source of features, as opposed to our unsupervised
approach.
6For the additional comparable iteration the result pattern
is very similar to the (C = 89, 97.4) case in the table, and is
not presented due to space limitations.
867
Algorithm 1 The DPP-cluster clustering algo-
rithm. K is the size of the sampled subsets, M is
the number of subsets sampled at each iteration, Y
is the verb set, T is the number of most probable
samples to be used in each iteration
Algorithm DPP-cluster :
Arguments: K,M,Y ,T
Return: cluster sets S = {S1, . . . Sn}
i? 1
S ? ?
while Y 6= ? do
(L1, S1)? GenKernelMatrix(Y, SCF )
(L2, S2)? GenKernelMatrix(Y, SP )
(L12, S12)? GenJointMat(L1, L2)
samples? sampleDpp(L,K,M)
topSamples? exTop(samples, T )
Si ? cluster(topSamples, L)
Y ? Y ? elements(Si)
S ? S ? Si
i? i+ 1
end while
AgglomerativeClustering(S)
???????????????????
???
Function cluster :
Arguments: topSamples,L
Return: S
S ? ?, topSample? ?
i? 1
while (topSample ? elements(S) = ?) do
topSample? topSamples(i)
S ? m1Mapping(topSample, S)
i? i+ 1
if (i > size(topSamples)) then
return S
end if
end while
BNC corpus. We do this by gathering the GR com-
binations for each of the verbs in our gold stan-
dard, assuming they are frames and gathering their
frequencies. Note that this corpus statistics base-
line is a very strong baseline that performs very
similarly to (de Cruys et al, 2012), the best unsu-
pervised SCF model we are aware of, when run on
their dataset 7.
As shown in table 3 the corpus statistics base-
line achieves high recall (84%) at the cost of
low precision (52.5%) (similar pattern has been
7personal communication with the authors.
demonstrated for the system of de Cruys et al
(2012)). On the other extreme, two other com-
monly used baselines strongly prefer precision.
These are the Most Frequent SCF (O?Donovan et
al., 2005) which uniformly assigns to all verbs the
two most frequent SCFs in general language, tran-
sitive (SUBJ-DOBJ) and intransitive (SUBJ) (and
results in poor F-score), and a filtering that re-
moves frames with low corpus frequencies (which
results in low recall even when trying to provide
the maximum recall for a given precision level).
The task we address is therefore to improve the
precision of the corpus statistics baseline in a way
that does not substantially harm the F-score.
To remedy this imbalance, we apply a cluster
based filtering method on top of the maximum-
recall frequency filter. This filter excludes a candi-
date frame from a verb?s lexicon only if it meets
the frequency filter criterion and appears in no
more than N other members of the cluster of the
verb in question. The filter utilizes the clustering
produced by the seventh to last iteration of DPP-
cluster that contains seven clusters with approxi-
mately 30 members each. Such clustering should
provide a good generalization level for the task.
We report results for moderate as well as ag-
gressive filtering (N = 3 andN = 7 respectively).
Table 3 clearly demonstrates that cluster based fil-
tering (DPP-cluster and AC) is the only method
that provides a good balance between the recall
and the precision of the SCF lexicon. Moreover,
the lexicon induced by this method includes a sub-
stantially higher number of frames per verb com-
pared to the other filtering methods. While both
AC and DPP-cluster still prefer recall to precision,
DPP-cluster does so to a smaller extent 8. This
clearly demonstrates that the clustering serves to
provide SCF acquisition with semantic informa-
tion needed for improved performance.
SP evaluation We explore a variant of the
pseudo-disambiguation task of Rooth et al (1999)
which has been applied to SP acquisition by a
number of recent papers (e.g. (de Cruys et al,
2012)). Rooth et al (1999) proposed to judge
which of two verbs v and v? is more likely to take a
given noun n as its argument. In their experiments
the model has to choose between a pair (v, n) that
8We show results for the maximum recall frequency fil-
tering with precision equals to 80 or 90. When the frequency
threshold is further reduced from 0.03, the same result pat-
tern hold. We do not give a detailed description due to space
limitations.
868
Corpus Statistics: [P = 52.5, R = 84, F = 64.6, AF = 12.3]
Most Frequent SCF: [P = 86.7, R = 22.5, F = 35.8, AF = 2]
Clustering Moderate Clustering Aggressive
Maximum Recall Frequency Threshold Model P R F AF P R F AF
threshold = 0.03, Prec. > 80 DPP-cluster 60.8 68.3 64.3 8.7 64.1 64.2 64.2 7.7
[P=88.7,R=52.4,F=65.9,AF=4.5] AC 58 73.2 64.6 9.7 61.3 68.9 64.7 8.6
threshold = 0.05, Prec. > 90 DPP-cluster 60.1 64.6 62.3 8.7 63.3 59.3 61.3 7.2
[P=92.3,R=44.4,F=59.9,AF=3.7] AC 57.5 70.6 63.2 9.4 60.7 65.4 62.7 8.3
Table 3: SCF Results for the DPP-cluster model compared to the Corpus Statistics baseline, Most Fre-
quent SCF baseline, maximum-recall frequency thresholding with the maximum threshold values that
keep precision above 80 (threshold = 0.03) and above 90 (threshold = 0.05), and the AC clustering base-
line. AF is the average number of frames per verb. All methods except from cluster based filtering
(DPP-cluster and AC) induce lexicons with strong recall/precision imbalance. Cluster based fil-
tering keeps a larger number of frames in the lexicon compared to the frequency thresholding
baseline, while keeping similar F-score levels. DPP-cluster provides better recall/precision balance
than AC.
appears only in the test corpus and a pair (v?, n)
that appears neither in the test nor in the training
corpus. Note, however, that this test only evaluates
the capability of a model to distinguish a correct
unseen verb-argument pair from an incorrect one,
but not its capability to identify erroneous pairs
when no alternative pair is presented. This last
property can strongly affect the precision of the
model.
We therefore propose to measure both aspects
of the SP task by computing both the recall and the
precision between the list of possible arguments a
verb can take according to the model and the cor-
responding test corpus list 9.
We evaluate the value of our clustering for SP
acquisition in the particularly challenging scenario
of domain adaptation. For each of the verbs in
our set we induce a list of possible noun direct ob-
jects from the BNC corpus and an equivalent list
from the North American News Text (NANT) cor-
pus. Following previous work (e.g. (de Cruys et
al., 2012)) arguments are identified using a parser
(RASP in our case). Using the verb clusters we
create a filtered version of the BNC argument lex-
icon which includes in the noun argument list of
a verb only those nouns that appear in the BNC
as arguments of that verb and of one of its cluster
members. For each verb we then compare the fil-
tered as well as the non-filtered BNC induced lex-
icon to the NANT lexicon by computing the aver-
age recall and precision between the argument lists
9In principle these measures can take into account the
probability assigned by the model to each argument and the
corresponding test corpus frequency. In this work we com-
pute probability-ignorant scores and keep more sophisticated
evaluations for future research.
and then report the average scores across all verbs.
We compare to a baseline which maintains only
noun arguments that appear at least twice in BNC
10. As a final measure of performance we compute
the ratio between the reduction in precision error
(i.e. pmodel?pbaseline100?pbaseline ) and the increase in recall er-
ror ( rbaseline?rmodel100?rmodel ).Table 2 presents the results for verbs with up to
200, 600 and 1000 noun arguments in the training
data. In all cases, the relative error reduction of the
DPP cluster filter is substantially higher than that
of the frequency baseline. Note that for this task
the baseline AC clusters are of low quality which
is reflects by an error reduction ratio of up to 0.5.
5 Conclusions and Future Work
In this paper we have presented the first unified
framework for the induction of verb clusters, sub-
categorization frames and selectional preferences
from corpus data. Our key idea is to cluster to-
gether verbs with similar SCFs and SPs and to use
the resulting clusters for SCF and SP induction. To
implement our idea we presented a novel method
which involves constructing a product DPP model
for SCFs and SPs and introduced a new algorithm
that utilizes the efficient DPP sampling algorithms
to cluster together verbs with similar SCFs and
SPs. The induced clusters performed well in eval-
uation against a VerbNet -based gold standard and
proved useful in improving the quality of SCFs
and SPs over strong baselines.
Our results demonstrate the benefits of a uni-
fied framework for acquiring lexical informa-
10we experimented with other threshold values for this
baseline but the recall in those case becomes very low.
869
tion about different aspects of verbal predicate-
argument structure. Not only the acquisition of
different types information (syntactic and seman-
tic) can support and inform each other, but also a
unified framework can be useful for NLP tasks and
applications which require rich information about
predicate-argument structure. In future work we
plan to apply our approach on larger scale data
sets and gold standards and to evaluate it in differ-
ent domains, languages and in the context of NLP
tasks such as syntactic parsing and SRL.
In addition, in our current framework SCF and
SP information is used for clustering which is in
turn used to improve SCF and SP quality. At this
stage no further information flows from the SCF
and SP models to the clustering model. A natural
extension of our unified framework is to construct
a joint model in which the predictions for all three
tasks inform each other at all stages of the predic-
tion process.
Acknowledgements
The work in this paper was funded by the Royal
Society University Research Fellowship (UK).
References
Ivana Romina Altamirano and Laura Alonso i Ale-
many. 2010. IRASubcat, a highly customizable,
language independent tool for the acquisition of ver-
bal subcategorization information from corpus. In
Proceedings of the NAACL HLT 2010 Young Inves-
tigators Workshop on Computational Approaches to
Languages of the Americas.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In COLING-
ACL-98.
Roberto Basili, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
RANLP 2007, Borovets, Bulgaria.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In EMNLP-07, page
161170, Prague, Czech Republic.
Akshar Bharati, Sriram Venkatapathy, and Prashanth
Reddy. 2005. Inferring semantic roles using sub-
categorization frames and maximum entropy model.
In CoNLL-05.
Ted Briscoe and John Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In ANLP-
97.
E.J. Briscoe, J. Carroll, and R. Watson. 2006. The
second relsease of the rasp system. In COLING/ACL
interactive presentation session.
Glenn Carroll and Mats Rooth. 1996. Valence induc-
tion with a head-lexicalized pcfg. In EMNLP-96.
Paula Chesley and Susanne Salmon-Alt. 2006. Au-
tomatic extraction of subcategorization frames for
french. In LREC-06.
Kostadin Cholakov and Gertjan van Noord. 2010. Us-
ing unknown word techniques to learn known words.
In EMNLP-10.
Hoa Trang Dang. 2004. Investigations into the Role of
Lexical Semantics in Word Sense Disambiguation.
Ph.D. thesis, CIS, University of Pennsylvania.
Tim Van de Cruys, Laura Rimell, Thierry Poibeau,
and Anna Korhonen. 2012. Multi-way tensor fac-
torization for unsupervised lexical acquisition. In
COLING-12.
Lukasz Dkebowski. 2009. Valence extraction us-
ing EM selection and co-occurrence matrices. Lan-
guage resources and evaluation, 43(4):301?327.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In ACL 2007, Prague,
Czech Republic.
J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Dis-
covering diverse and salient threads in document
collections. In EMNLP-12.
Ralph Grishman, Catherine Macleod, and Adam Mey-
ers. 1994. Comlex syntax: Building a computa-
tional lexicon. In COLNIG-94.
G.E. Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural Compu-
tation, 14:1771?1800.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Porceedings 0f NAACL-HLT-06
short papers.
Dino Ienco, Serena Villata, and Cristina Bosco. 2008.
Automatic extraction of subcategorization frames
for italian. In LREC-08.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic verb
classification. Natural Language Engineering.
Daisuke Kawahara and Sadao Kurohashi. 2010. Ac-
quiring reliable predicate-argument structures from
raw corpora for case frame compilation. In LREC-
10.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, June.
870
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classifica-
tion of verbs in biomedical texts. In Proceddings
of COLING-08.
Anna Korhonen. 2002. Semantically motivated
subcategorization acquisition. In Proceedings of
the ACL-02 workshop on Unsupervised lexical
acquisition-Volume 9.
A. Kulesza and B. Taskar. 2010. Structured determi-
nantal point processes. In NIPS-10.
A. Kulesza and B. Taskar. 2012a. k-dpps: fixed-size
determinantal point processes. In ICML-11.
A. Kulesza and B. Taskar. 2012b. Learning determi-
nantal point processes. In UAI-12.
Alex Kulesza and Ben Taskar. 2012c. Determi-
nantal point processes for machine learning. In
arXiv:1207.6083.
A. Kulesza. 2012. Learning with determinantal point
processes. Ph.D. thesis, CIS, University of Pennsyl-
vania.
Alessandro Lenci, Barbara McGillivray, Simonetta
Montemagni, and Vito Pirrelli. 2008. Unsuper-
vised acquisition of verb subcategorization frames
from shallow-parsed corpora. In LREC-08.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. Chicago, IL.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In ACL-
08.
Tom Lippincott, Anna Korhonen, and Diarmuid O?
Se?aghdha. 2012. Learning syntactic verb frames
using graphical models. In ACL-12, Jeju, Korea.
Ce?dric Messiant, Anna Korhonen, and Thierry
Poibeau. 2008. LexSchem: A large subcategoriza-
tion lexicon for French verbs. In LREC-08.
George A. Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Alessandro Moschitti and Roberto Basili. 2005. Verb
subcategorization kernels for automatic semantic la-
beling. In Proceedings of the ACL-SIGLEX Work-
shop on Deep Lexical Acquisition.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the penn-ii and penn-iii treebanks. Computational
Linguistics, 31:328?365.
Diarmuid O? Se?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In EMNLP-11, Edinburgh, UK.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In ACL-10, Uppsala, Swe-
den.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In ACL-07.
Joseph Reisinger and Raymond Mooney. 2011. Cross-
cutting models of lexical semantics. In EMNLP-11,
Edinburgh, UK.
Alan Ritter and Oren Etzioni. 2010. A latent dirich-
let alocation method for selectional preferences. In
ACL-10.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
ACL-99.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
S. Schulte im Walde, C. Hying, C. Scheible, and
H. Schmid. 2008. Combining EM training and the
MDL principle for an automatic verb classification
incorporating selectional preferences. In ACL-08,
pages 496?504.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of german semantic verb
classes. Computational Linguistics, 32(2):159?194.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In CICLING-05.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In EMNLP-09, Singapore.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In EMNLP-11.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
Lecture Notes in Computer Science, 4919(16).
Robert Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In EMNLP-04.
Stefan Thater, Hagen Furstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In ACL-
10, Uppsala, Sweden.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction.
In Proceedings of the workshop on Geometric Mod-
els for Natural Language Semantics (GEMS).
871
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics.
2008. Robustness and generalization of role sets:
PropBank vs. VerbNet.
Benat Zapirain, Eneko Agirre, and Lluis Marquex.
2009. Generalizing over lexical features: Selec-
tional preferences for semantic role classification. In
ACL-IJCNLP-09, Singapore.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
ACL-11, Portland, OR.
872
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736?741,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Diathesis alternation approximation for verb clustering
Lin Sun
Greedy Intelligence Ltd
Hangzhou, China
lin.sun@greedyint.com
Diana McCarthy and Anna Korhonen
DTAL and Computer Laboratory
University of Cambridge
Cambridge, UK
diana@dianamccarthy.co.uk
alk23@cam.ac.uk
Abstract
Although diathesis alternations have been
used as features for manual verb clas-
sification, and there is recent work on
incorporating such features in computa-
tional models of human language acquisi-
tion, work on large scale verb classifica-
tion has yet to examine the potential for
using diathesis alternations as input fea-
tures to the clustering process. This pa-
per proposes a method for approximating
diathesis alternation behaviour in corpus
data and shows, using a state-of-the-art
verb clustering system, that features based
on alternation approximation outperform
those based on independent subcategoriza-
tion frames. Our alternation-based ap-
proach is particularly adept at leveraging
information from less frequent data.
1 Introduction
Diathesis alternations (DAs) are regular alterna-
tions of the syntactic expression of verbal argu-
ments, sometimes accompanied by a change in
meaning. For example, The man broke the win-
dow ? The window broke. The syntactic phe-
nomena are triggered by the underlying semantics
of the participating verbs. Levin (1993)?s seminal
book provides a manual inventory both of DAs and
verb classes where membership is determined ac-
cording to participation in these alternations. For
example, most of the COOK verbs (e.g. bake,
cook, fry . . . ) can all take various DAs, such as
the causative alternation, middle alternation and
instrument subject alternation.
In computational linguistics, work inspired by
Levin?s classification has exploited the link be-
tween syntax and semantics for producing clas-
sifications of verbs. Such classifications are use-
ful for a wide variety of purposes such as se-
mantic role labelling (Gildea and Jurafsky, 2002),
predicting unseen syntax (Parisien and Steven-
son, 2010), argument zoning (Guo et al, 2011)
and metaphor identification (Shutova et al, 2010).
While Levin?s classification can be extended man-
ually (Kipper-Schuler, 2005), a large body of re-
search has developed methods for automatic verb
classification since such methods can be applied
easily to other domains and languages.
Existing work on automatic classification relies
largely on syntactic features such as subcatego-
rization frames (SCF)s (Schulte im Walde, 2006;
Sun and Korhonen, 2011; Vlachos et al, 2009;
Brew and Schulte im Walde, 2002). There has also
been some success incorporating selectional pref-
erences (Sun and Korhonen, 2009).
Few have attempted to use, or approximate,
diathesis features directly for verb classification
although manual classifications have relied on
them heavily, and there has been related work on
identifying the DAs themselves automatically us-
ing SCF and semantic information (Resnik, 1993;
McCarthy and Korhonen, 1998; Lapata, 1999;
McCarthy, 2000; Tsang and Stevenson, 2004).
Exceptions to this include Merlo and Stevenson
(2001), Joanis et al (2008) and Parisien and
Stevenson (2010, 2011). Merlo and Stevenson
(2001) used cues such as passive voice, animacy
and syntactic frames coupled with the overlap
of lexical fillers between the alternating slots to
predict a 3-way classification (unergative, unac-
cusative and object-drop). Joanis et al (2008)
used similar features to classify verbs on a much
larger scale. They classify up to 496 verbs us-
ing 11 different classifications each having be-
tween 2 and 14 classes. Parisien and Steven-
son (2010, 2011) used hierarchical Bayesian mod-
els on slot frequency data obtained from child-
directed speech parsed with a dependency parser
to model acquisition of SCF, alternations and ul-
timately verb classes which provided predictions
for unseen syntactic behaviour of class members.
736
Frame Example sentence Freq
NP+PPon Jessica sprayed paint on the wall 40
NP+PPwith Jessica sprayed the wall with paint 30
PPwith *The wall sprayed with paint 0
PPon Jessica sprayed paint on the wall 30
Table 1: Example frames for verb spray
In this paper, like Sun and Korhonen (2009);
Joanis et al (2008) we seek to automatically clas-
sify verbs into a broad range of classes. Like Joa-
nis et al, we include evidence of DA, but we do
not manually select features attributed to specific
alternations but rather experiment with syntactic
evidence for alternation approximation. We use
the verb clustering system presented in Sun and
Korhonen (2009) because it achieves state-of-the-
art results on several datasets, including those of
Joanis et al, even without the additional boost in
performance from the selectional preference data.
We are interested in the improvement that can be
achieved to verb clustering using approximations
for DAs, rather than the DA per se. As such we
make the simple assumption that if a pair of SCFs
tends to occur with the same verbs, we have a po-
tential occurrence of DA. Although this approx-
imation can give rise to false positives (pairs of
frames that co-occur frequently but are not DA)
we are nevertheless interested in investigating its
potential usefulness for verb classification. One
attractive aspect of this method is that it does not
require a pre-defined list of possible alternations.
2 Diathesis Alternation Approximation
A DA can be approximated by a pair of SCFs.
We parameterize frames involving prepositional
phrases with the preposition. Example SCFs for
the verb ?spray? are shown in Table 1. The feature
value of a single frame feature is the frequency
of the SCF. Given two frames fv(i), fv(j) of a
verb v, they can be transformed into a feature pair
(fv(i), fv(j)) as an approximation to a DA. The
feature value of the DA feature (fv(i), fv(j)) is ap-
proximated by the joint probability of the pair of
frames p(fv(i), fv(j)|v), obtained by integrating
all the possible DAs. The key assumption is that
the joint probability of two SCFs has a strong cor-
relation with a DA on the grounds that the DA gives
rise to both SCFs in the pair. We use the DA feature
(fv(i), fv(j)) with its value p(fv(i), fv(j)|v) as a
new feature for verb clustering. As a comparison
point, we can ignore the DA and make a frame in-
dependence assumption. The joint probability is
decomposed as:
p(fv(i), fv(j)|v)? , p(fv(i)|v) ? p(fv(j)|v) (1)
We assume that SCFs are dependent as they are
generated by the underlying meaning components
(Levin and Hovav, 2006). The frame dependency
is represented by a simple graphical model in fig-
ure 1.
Figure 1: Graphical model for the joint probability of pairs of
frames. v represents a verb, a represents a DA and f repre-
sents a specific frame in total of M possible frames
In the data, the verb (v) and frames (f ) are ob-
served, and any underlying alternation (a) is hid-
den. The aim is to approximate but not to detect a
DA, so a is summed out:
p(fv(i), fv(j)|v) =
?
a
p(fv(i), fv(j)|a) ? p(a|v)
(2)
In order to evaluate this sum, we use a relaxation
1: the sum in equation 1 is replaced with the max-
imum (max). This is a reasonable relaxation, as a
pair of frames rarely participates in more than one
type of a DA.
p(fv(i), fv(j)|v) ? max(p(fv(i), fv(j)|a)?p(a|v))
(3)
The second relaxation further relaxes the first one
by replacing the max with the least upper bound
(sup): If fv(i) occurs a times, fv(j) occurs b times
and b < a, the number of times that a DA occurs
between fv(i) and fv(j) must be smaller or equal
to b.
p(fv(i), fv(j)|v) ? sup{p(fv(i), fv(j)|a)} ? sup{p(a|v)}
(4)
sup{p(fv(i), fv(j)|a)} = Z?1 ?min(fv(i), fv(j))
sup{p(a|v)} = 1
Z =
?
m
?
n
min(fv(m), fv(n))
1A relaxation is used in mathematical optimization for re-
laxing the strict requirement, by either substituting it with an
easier requirement or dropping it completely.
737
Frame pair Possible DA Frequency
NP+PPon NP+PPwith Locative 30
NP+PPon PPwith Causative(with) 0
NP+PPon PPon Causative(on) 30
NP+PPwith PPwith ? 0
NP+PPwith PPon ? 30
PPwith PPon ? 0
NP+PPon NP+PPon - 40
NP+PPwith NP+PPwith - 30
PPwith PPwith - 0
PPon PPon - 30
Table 2: Example frame pair features for spray
So we end up with a simple form:
p(fv(i), fv(j)|v) ? Z?1 ?min(fv(i), fv(j)) (5)
The equation is intuitive: If fv(i) occurs 40 times
and fv(j) 30 times, the DA between fv(i) and
fv(j) ? 30 times. This upper bound value is used
as the feature value of the DA feature. The original
feature vector f of dimension M is transformed
into M2 dimensions feature vector f? . Table 2
shows the transformed feature space for spray.
The feature space matches our expectation well:
valid DAs have a value greater than 0 and invalid
DAs have a value of 0.
3 Experiments
We evaluated this model by performing verb clus-
tering experiments using three feature sets:
F1: SCF parameterized with preposition. Exam-
ples are shown in Table 1.
F2: The frame pair features built from F1 with
the frame independence assumption (equa-
tion 1). This feature is not a DA feature as
it ignores the inter-dependency of the frames.
F3: The frame pair features (DAs) built from
F1 with the frame dependency assumption
(equation 4). This is the DA feature which
considers the correlation of the two frames
which are generated from the alternation.
F3 implicitly includes F1, as a frame can pair
with itself. 2 In the example in Table 2, the frame
pair ?PP(on) PP(on)? will always have the same
value as the ?PP(on)? frame in F1.
We extracted the SCFs using the system of
Preiss et al (2007) which classifies each corpus
2We did this so that F3 included the SCF features as well
as the DA approximation features. It would be possible in
future work to exclude the pairs involving identical frames,
thereby relying solely on the DA approximations, and com-
pare performance with the results obtained here.
occurrence of a verb as a member of one of the 168
SCFs on the basis of grammatical relations iden-
tified by the RASP (Briscoe et al, 2006) parser.
We experimented with two datasets that have been
used in prior work on verb clustering: the test sets
7-11 (3-14 classes) in Joanis et al (2008), and the
17 classes set in Sun et al (2008).
We used the spectral clustering (SPEC) method
and settings as in Sun and Korhonen (2009) but
adopted the Bhattacharyya kernel (Jebara and
Kondor, 2003) to improve the computational effi-
ciency of the approach given the high dimension-
ality of the quadratic feature space.
wb(v, v?) =
D?
d=1
(vdv?d)1/2 (6)
The mean-filed bound of the Bhattacharyya kernel
is very similar to the KL divergence kernel (Jebara
et al, 2004) which is frequently used in verb clus-
tering experiments (Korhonen et al, 2003; Sun
and Korhonen, 2009).
To further reduce computational complexity, we
restricted our scope to the more frequent features.
In the experiment described in this section we used
the 50 most frequent features for the 3-6 way clas-
sifications (Joanis et al?s test set 7-9) and 100 fea-
tures for the 7-17 way classifications. In the next
section, we will demonstrate that F3 outperforms
F1 regardless of the feature number setting. The
features are normalized to sum 1.
The clustering results are evaluated using F-
Measure as in Sun and Korhonen (2009) which
provides the harmonic mean of precision (P ) and
recall (R)
P is calculated using modified purity ? a global
measure which evaluates the mean precision of
clusters. Each cluster (ki ? K) is associated
with the gold-standard class to which the major-
ity of its members belong. The number of verbs
in a cluster (ki) that take this class is denoted by
nprevalent(ki).
P =
?
ki?K:nprevalent(ki)>2
nprevalent(ki)
|verbs|
R is calculated using weighted class accuracy:
the proportion of members of the dominant cluster
DOM-CLUSTi within each of the gold-standard
classes ci ? C.
738
Datasets
Joanis et al Sun et al7 8 9 10 11
F1 54.54 49.97 35.77 46.61 38.81 60.03
F2 50.00 49.50 32.79 54.13 40.61 64.00
F3 56.36 53.79 52.90 66.32 50.97 69.62
Table 3: Results when using F3 (DA), F2 (pair of independent
frames) and F1 (single frame) features with Bhattacharyya
kernel on Joanis et al and Sun et al datasets
R =
?|C|
i=1 |verbs in DOM-CLUSTi|
|verbs|
The results are shown in Table 3. The result of
F2 is lower than that of F3, and even lower than
that of F1 for 3-6 way classification. This indi-
cates that the frame independence assumption is
a poor assumption. F3 yields substantially better
result than F2 and F1. The result of F3 is 6.4%
higher than the result (F=63.28) reported in Sun
and Korhonen (2009) using the F1 feature.
This experiment shows, on two datasets, that DA
features are clearly more effective than the frame
features for verb clustering, even when relaxations
are used.
4 Analysis of Feature Frequency
A further experiment was carried out using F1 and
F3 on Joanis et al (2008)?s test sets 10 and 11.
The frequency ranked features were added to the
clustering one at a time, starting from the most
frequent one. The results are shown in figure 2.
F3 outperforms F1 clearly on all the feature num-
ber settings. After adding some highly frequent
frames (22 for test set 10 and 67 for test set 11),
the performance for F1 is not further improved.
The performance of F3, in contrast, is improved
for almost all (including the mid-range frequency)
frames, although to a lesser degree for low fre-
quency frames.
5 Related work
Parisien and Stevenson (2010) introduced a hier-
archical Bayesian model capable of learning verb
alternations and constructions from syntactic in-
put. The focus was on modelling and explaining
the child alternation acquisition rather than on au-
tomatic verb classification. Therefore, no quanti-
tative evaluation of the clustering is reported, and
the number of verbs under the novel verb gen-
eralization test is relatively small. Parisien and
1 22 1000.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
1 20 1.53416
347
3476
34?
34?6
34.
34.6
346
3466
??? ? ?? ? ????? ?1?1??
Figure 2: Comparison between frame features (F1) and DA
features (F3) with different feature number settings. DA fea-
tures clearly outperform frame features. The top figure is the
result on test set 10 (8 ways). The bottom figure is the result
on test set 11 (14 ways). The x axis is the number of features.
The y axis is the F-Measure result.
Stevenson (2011) extended this work by adding
semantic features.
Parisien and Stevenson?s (2010) model 2 has a
similar structure to the graphic model in figure 1.
A fundamental difference is that we explicitly use
a probability distribution over alternations (pair of
frames) to represent a verb, whereas they represent
a verb by a distribution over the observed frames
similar to Vlachos et al (2009) ?s approach. Also
the parameters in their model were inferred by
Gibbs sampling whereas we avoided this inference
step by using relaxation.
6 Conclusion and Future work
We have demonstrated the merits of using DAs for
verb clustering compared to the SCF data from
which they are derived on standard verb classi-
fication datasets and when integrated in a state-
of-the-art verb clustering system. We have also
demonstrated that the performance of frame fea-
tures is dominated by the high frequency frames.
In contrast, the DA features enable the mid-range
frequency frames to further improve the perfor-
mance.
739
In the future, we plan to evaluate the perfor-
mance of DA features in a larger scale experiment.
Due to the high dimensionality of the transformed
feature space (quadratic of the original feature
space), we will need to improve the computational
efficiency further, e.g. via use of an unsupervised
dimensionality reduction technique Zhao and Liu
(2007). Moreover, we plan to use Bayesian in-
ference as in Vlachos et al (2009); Parisien and
Stevenson (2010, 2011) to infer the actual param-
eter values and avoid the relaxation.
Finally, we plan to supplement the DA feature
with evidence from the slot fillers of the alternat-
ing slots, in the spirit of earlier work (McCarthy,
2000; Merlo and Stevenson, 2001; Joanis et al,
2008). Unlike these previous works, we will use
selectional preferences to generalize the argument
heads but will do so using preferences from dis-
tributional data (Sun and Korhonen, 2009) rather
than WordNet, and use all argument head data in
all frames. We envisage using maximum average
distributional similarity of the argument heads in
any potentially alternating slots in a pair of co-
occurring frames as a feature, just as we currently
use the frequency of the less frequent co-occurring
frame.
Acknowledgement
Our work was funded by the Royal Society
University Research Fellowship (AK) and the
Dorothy Hodgkin Postgraduate Award (LS).
References
C. Brew and S. Schulte im Walde. Spectral clus-
tering for German verbs. In Proceedings of
EMNLP, 2002.
E. Briscoe, J. Carroll, and R. Watson. The second
release of the RASP system. In Proceedings
of the COLING/ACL on Interactive presentation
sessions, pages 77?80, 2006.
D. Gildea and D. Jurafsky. Automatic labeling
of semantic roles. Computational Linguistics,
28(3):245?288, 2002.
Y. Guo, A. Korhonen, and T. Poibeau. A
weakly-supervised approach to argumentative
zoning of scientific documents. In Proceedings
of EMNLP, pages 273?283, Stroudsburg, PA,
USA, 2011. ACL.
T. Jebara and R. Kondor. Bhattacharyya and ex-
pected likelihood kernels. In Learning Theory
and Kernel Machines: 16th Annual Conference
on Learning Theory and 7th Kernel Workshop,
page 57. Springer, 2003.
T. Jebara, R. Kondor, and A. Howard. Probability
product kernels. The Journal of Machine Learn-
ing Research, 5:819?844, 2004.
E. Joanis, S. Stevenson, and D. James. A general
feature space for automatic verb classification.
Natural Language Engineering, 2008.
K. Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. PhD thesis, Com-
puter and Information Science Dept., University
of Pennsylvania, Philadelphia, PA, June 2005.
A. Korhonen, Y. Krymolowski, and Z. Marx.
Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of
ACL, pages 64?71, Morristown, NJ, USA,
2003. ACL.
M. Lapata. Acquiring lexical generalizations from
corpora: A case study for diathesis alternations.
In Proceedings of ACL, pages 397?404. ACL
Morristown, NJ, USA, 1999.
B. Levin and M. Hovav. Argument realiza-
tion. Computational Linguistics, 32(3):447?
450, 2006.
B. Levin. English Verb Classes and Alterna-
tions: a preliminary investigation. University
of Chicago Press, Chicago and London, 1993.
D. McCarthy and A. Korhonen. Detecting verbal
participation in diathesis alternations. In Pro-
ceedings of ACL, volume 36, pages 1493?1495.
ACL, 1998.
D. McCarthy. Using semantic preferences to iden-
tify verbal participation in role switching alter-
nations. In Proceedings of NAACL, pages 256?
263. Morgan Kaufmann Publishers Inc. San
Francisco, CA, USA, 2000.
P. Merlo and S. Stevenson. Automatic verb clas-
sification based on statistical distributions of ar-
gument structure. Computational Linguistics,
27(3):373?408, 2001.
C. Parisien and S. Stevenson. Learning verb al-
ternations in a usage-based Bayesian model. In
Proceedings of the 32nd annual meeting of the
Cognitive Science Society, 2010.
C. Parisien and S. Stevenson. Generalizing be-
tween form and meaning using learned verb
classes. In Proceedings of the 33rd Annual
Meeting of the Cognitive Science Society, 2011.
740
J. Preiss, T. Briscoe, and A. Korhonen. A system
for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from
corpora. In Proceedings of ACL, volume 45,
page 912, 2007.
P. Resnik. Selection and Information: A Class-
Based Approach to Lexical Relationships. PhD
thesis, University of Pennsylvania, 1993.
S. Schulte im Walde. Experiments on the au-
tomatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?
194, 2006.
E. Shutova, L. Sun, and A. Korhonen. Metaphor
identification using verb and noun clustering.
In Proceedings of COLING, pages 1002?1010.
ACL, 2010.
L. Sun and A. Korhonen. Improving verb clus-
tering with automatically acquired selectional
preferences. In Proceedings of EMNLP, pages
638?647, 2009.
L. Sun and A. Korhonen. Hierarchical verb clus-
tering using graph factorization. In Proceedings
of EMNLP, pages 1023?1033, Edinburgh, Scot-
land, UK., July 2011. ACL.
L. Sun, A. Korhonen, and Y. Krymolowski. Verb
class discovery from rich syntactic data. Lecture
Notes in Computer Science, 4919:16, 2008.
V. Tsang and S. Stevenson. Using selectional
profile distance to detect verb alternations. In
HLT/NAACL 2004 Workshop on Computational
Lexical Semantics, 2004.
A. Vlachos, A. Korhonen, and Z. Ghahramani.
Unsupervised and constrained dirichlet process
mixture models for verb clustering. In Proceed-
ings of the Workshop on Geometrical Models
of Natural Language Semantics, pages 74?82,
2009.
Z. Zhao and H. Liu. Spectral feature selection
for supervised and unsupervised learning. In
Proceedings of ICML, pages 1151?1157, New
York, NY, USA, 2007. ACM.
741
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 725?731,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Concreteness and Subjectivity as Dimensions of Lexical Meaning
Felix Hill
Computer Laboratory
Cambridge University
felix.hill@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
Cambridge University
anna.korhonen@cl.cam.ac.uk
Abstract
We quantify the lexical subjectivity of ad-
jectives using a corpus-based method, and
show for the first time that it correlates
with noun concreteness in large corpora.
These cognitive dimensions together influ-
ence how word meanings combine, and
we exploit this fact to achieve performance
improvements on the semantic classifica-
tion of adjective-noun pairs.
1 Introduction
Concreteness, the degree to which language has
a perceptible physical referent, and subjectivity,
the extent to which linguistic meaning depends on
the perspective of the speaker, are well established
cognitive and linguistic notions. Recent results
suggest that they could also be useful knowledge
for natural language processing systems that aim
to extract and represent the meaning of language.
Insight into concreteness can help systems to
classify adjective-noun pairs according to their se-
mantics. In the non-literal expressions kill the pro-
cess or black comedy, a verb or adjective that oc-
curs with a concrete argument in literal phrases
takes an abstract argument. Turney et al (2011)
present a supervised model that exploits this effect
to correctly classify 79% of adjective-noun pairs
as having literal or non-literal meaning.
Subjectivity analysis has already proved highly
applicable to a range of NLP applications, includ-
ing sentiment analysis, information extraction and
text categorization (Pang and Lee, 2004; Riloff
and Wiebe, 2003). For such applications, subjec-
tivity is analyzed at the phrasal or document level.
However, recent work has highlighted the applica-
tion of subjectivity analysis to lexical semantics,
for instance to the tasks of disambiguating words
according to their usage or sense (Wiebe and Mi-
halcea, 2006; Banea et al, 2014).
The importance of concreteness to NLP systems
is likely to grow with the emergence of multi-
modal semantic models (Bruni et al, 2012; Roller
and Schulte im Walde, 2013). Such models, which
learn representations from both linguistic and per-
ceptual input, outperform text-only models on a
range of evaluations. However, while multi-modal
models acquire richer representations of concrete
concepts, their ability to represent abstract con-
cepts can be weaker than text-only models (Hill et
al., 2013). A principled treatment of concreteness
is thus likely to be important if the multi-modal
approach is to prove effective on a wider range of
concepts. In a similar vein, interest in subjectiv-
ity analysis is set to grow with interest in extract-
ing sentiment and opinion from the web and social
media (Benson et al, 2011). Moreover, given that
humans seem to exploit both concreteness (Paivio,
1990) and subjectivity (Canestrelli et al, 2013)
clues when processing language, it is likely that
the same clues should benefit computational mod-
els aiming to replicate human-level performance
in this area.
In this paper, we show how concreteness and
subjectivity can be applied together to produce
performance improvements on two classification
problems: distinguishing literal and non-literal
adjective-noun pairs (Turney et al, 2011), and
classifying the modification type exhibited by
such pairs (Boleda et al, 2012). We describe an
unsupervised corpus-based method to quantify ad-
jective subjectivity, and show that it effectively
predicts the labels of a hand-coded subjectivity
lexicon. Further, we show for the first time that
adjective subjectivity correlates with noun con-
creteness in large corpora. In addition, we anal-
yse the effect of noun concreteness and adjective
subjectivity on meaning combination, illustrating
how the interaction of these dimensions enables
the accurate classification of adjective-noun pairs
according to their semantics. We conclude by dis-
725
cussing other potential applications of concrete-
ness and subjectivity to NLP.
2 Dimensions of meaning
Concreteness A large and growing body of em-
pirical evidence indicates clear differences be-
tween concrete concepts, such as donut or hot-
dog and abstract concepts, such as guilt or obesity.
Concrete words are more easily learned, remem-
bered and processed than abstract words (Paivio,
1991), while differences in brain activity (Binder
et al, 2005) and cognitive representation (Hill et
al., 2013) have also been observed. In linguistic
conctructions, concreteness appears to influence
compound and phrasal semantics (Traugott, 1985;
Bowdle and Gentner, 2005; Turney et al, 2011).
Together with the practical applications outlined in
Section 1, these facts indicate the potential value
of concreteness for models aiming to replicate hu-
man performance in language processing tasks.
While automatic methods have been proposed
for the quantification of lexical concreteness, they
each rely on dictionaries or similar hand-coded
resources (Kwong, 2008; Turney et al, 2011).
We instead extract scores from a recently released
dataset of lexical concepts rated on a 1-5 scale for
concreteness by 20 annotators in a crowdsourcing
experiment (Brysbaert et al, 2013).
1
Subjectivity Subjectivity is the degree to which
language is interpretable independently of the
speaker?s perspective (Langacker, 2002). For ex-
ample, the utterance he sits across the table is
more subjective than he sits opposite Sam as its
truth depends on the speaker?s position. Language
may also be subjective because it conveys evalua-
tions or opinions (Mihalcea et al, 2007).
Computational applications of subjectivity, in-
cluding sentiment analysis and information ex-
traction, have focused largely on phrase or doc-
ument meaning.
2
In contrast, here we present six
corpus-based features designed to quantify the lex-
ical subjectivity of adjectives. The features Com-
parability and Modifiability are identified as pre-
dictors of subjectivity by Wiebe (2000). The re-
mainder are motivated by corpus studies and/or
observations from the theoretical literature.
3
1
Available at http://crr.ugent.be/archives/1330.
2
See e.g. (Wiebe and Riloff, 2011).
3
Several of the features here were applied by Hill (2012),
to the task of ordering multiple-modifier strings.
Adverbiability: Quirk et al (1985) theorizes that
subjective adjectives tend to develop derived ad-
verbial forms, whereas more objective adjectives
do not. We thus define adverbiability as the fre-
quency of derived adverbial forms relative to the
frequency of their base form, e.g.
?
hotly
?
hot+
?
hotly
Comparability: Wiebe (2000) oberve that grad-
able are more likely to be subjective. Following
Wiebe, we note that the existence of comparative
forms for an adjective are indicative of gradabil-
ity. We thus define comparability as the frequency
of comparative or superlative forms relative to the
frequency of the base form, e.g.
?
hotter +
?
hottest
?
hot+
?
hotter +
?
hottest
LeftTendency: Adamson (2000) proposes that
more subjective adjectives typically occur furthest
from the noun in multiple-modifier strings such as
(hot crossed buns). We consequently extract the
LeftTendency of our adjectives, defined as the fre-
quency of occurrence as the leftmost of two ad-
jectives as a proportion of the overall frequency of
occurrence in multiple-modifier strings.
Modifiability: Another characteristic of gradable
adjectives noted by Wiebe (2000) is that they ad-
mit degree modifiers (very/quite delicious). We
therefore extract the relative frequency of occur-
rence with one of a hand-coded list of English de-
gree modifiers.
Predicativity: Bolinger (1967) proposed that sub-
jective adjectives occur in predicative construc-
tions (the cake is sweet), rather than attribu-
tive constructions (the German capital) more fre-
quently than objective adjectives. We therefore ex-
tract the relative frequency of occurrence in such
constructions.
Non-nominality: Many adjectives also function
as nouns (sweet cake vs. (boiled sweet). Un-
like nouns, many adjectives are inherently subjec-
tive, and the number of adjectives in texts corre-
lates with human judgements of their subjectivity
(Hatzivassiloglou and Wiebe, 2000). We there-
fore extract the frequency with which concepts are
tagged as adjectives relative to as nouns, on the
726
assumption that ?pure? adjectives are on average
more subjective than nominal-style adjectives.
Concreteness meets Subjectivity Demonstra-
ble commonalities in how different people per-
ceive the physical world suggest that concrete lan-
guage may be more objective than abstract lan-
guage (Langacker, 1997). Intuitively, adjectives
ascribing physical properties (wooden shed) are
more objective than those conveying abstract traits
(suspicious man). Indeed, in certain cases the
original, apparently objective, senses of polyse-
mous adjectives are not modifiable (very wooden
shed?), while their more abstract sense extensions
are (very wooden personality).
Motivated by these observations, in the follow-
ing sections we test two hypotheses. (1) Subjec-
tive / objective adjectives are more likely to mod-
ify abstract / concrete nouns respectively. (2) Sub-
jectivity and concreteness can predict aspects of
how adjective and noun concepts combine.
3 Analysis
In addressing (1), we extracted the 2,000 highest-
frequency nouns from the Brysbaert et al (2013)
concreteness dataset. We denote by CONC(n)
the mean concreteness rating for noun n. For the
24,908 adjectives that occur in some adjective-
noun pair with one of these nouns in the British
National Corpus (BNC) (Leech et al, 1994), we
extracted subjectivity features from the Google
Books Corpus (Goldberg and Orwant, 2013).
Since each of the six features takes values on [0, 1],
we define the overall subjectivity of an adjective a
with feature vector s
a
= [s
a
1
. . . s
a
6
] as
SUBJ(a) =
6
?
i=1
s
a
i
.
To verify the quality of our subjectivity features,
we measured their performance as predictors in a
logistic regression classifying the 3,250 adjectives
labelled as subjective or not in the Wilson et al
(2005) lexicon.
4
The combination of all features
produced an overall classifiction accuracy of 79%.
The performance of individual features as predic-
tors in isolation is shown in Figure 1 (top).
We first tested the relationship between con-
creteness and subjectivity with a correlation anal-
ysis over noun concepts. For each noun n we de-
4
Available at http://mpqa.cs.pitt.edu/
79.1
74.2
72.8
70.1
69.4
71.2
73.7
Combined
Adverbiability
Modifiability
Predicativity
Comparability
LeftTendency
NonNominality
0 20 40 60 80Feature Prediction Performance (% correct)
?0.1
0.0
0.1
0.2
0.3
0.4
2 4 6CONC (noun concreteness)S
UB
J (ad
jecti
ve s
ubje
ctivi
ty)
Figure 1: Top: Performance of features in
predicting subjectivity labels from the Wilson
et al (2005) lexicon. Bottom: Concreteness-
subjectivity correlation in adj-noun pairs.
a SUBJ(a) n CONC(n)
flashy 1.98 umbrella 5
honest 1.63 flask 5
good 1.59 automobile 5
Siberian 6.9? 10
?
4 virtue 1.49
interglacial 6.3? 10
?
4 pride 1.46
Soviet 1.9? 10
?
4 hope 1.18
Table 1: The most and least subjective adjectives
and most and least concrete nouns in our data.
fined its subjectivity profile as the mean of the sub-
jectivity vectors of its modifying adjectives
SUBJpfl(n) =
1
|A
n
|
?
a?A
n
s
a
where the bag A
n
contains an adjective a for each
occurrence of the pair (a, n) in the BNC. As hy-
pothesized, CONC(n) was a significant predictor
of the magnitude of the subjectivity profile (Pear-
son r = ?0.421, p < 0.01). This effect is illus-
trated in Figure 1 (bottom).
To explore the relationship between concrete-
ness, subjectivity and meaning, we plotted the
20,000 highest frequency (a, n) pairs in the BNC
in the CONC-SUBJ semantic space (Figure 2,
top). In addition, to examine the effect of con-
creteness alone on adjective-noun semantics, we
727
(a, n) ? (a, n) ?
white hope 4.61 mature attitude 4.05
fresh hope 4.34 injured pride 4.03
male pride 4.28 black mood 3.99
wild hope 4.06 white spirit 3.93
Table 2: The eight pairs with highest ? =
ExpCONC(a)? CONC(n) in our data.
defined a new adjective feature
ExpCONC(a) =
1
|N
a
|
?
n?N
a
CONC(n)
where the bag N
a
contains noun n for each occur-
rence of the pair (a, n) in the BNC. Figure 2 (bot-
tom) illustrates the the CONC-ExpCONC space.
In both spaces, the extremities reflect particular
meaning combination types. Pairs in the bottom-
left region of the CONC-SUBJ space (objective
adjectives with abstract nouns, such as green pol-
itics) seem to exhibit a non-literal, or at least non
prototypical modification type. In contrast, for
pairs in the objective+concrete corner, the adjec-
tives appear to perform a classifying or categoriz-
ing function (baptist minister).
In the CONC-ExpCONC space, on the diag-
onal, where noun-concreteness is ?as expected?,
pairings appear to combine literally. Away from
the diagonal, meaning composition is less pre-
dictable. In the top-left, where ExpCONC is
greater than CONC, the combinations are almost
all non-literal, as shown in Table 2.
In this section we have outlined a set of corpus
features that, taken together, enable effective ap-
proximation of adjective subjectivity. The results
of our analyses also demonstrate a clear interac-
tion between subjectivity and concreteness scores
for nouns attributed by human raters. Specifi-
cally, objective adjectives are more likely to mod-
ify concrete nouns and subjective adjectives are
more likely to modify abstract nouns. Qualita-
tive investigations further suggest the interaction
between these dimensions to be useful in the se-
mantic characterization of adjective-noun pairs, a
proposition we test formally in the next section.
4 Evaluation
We evaluate the potential of our adjective subjec-
tivity features, together with noun concreteness,
to predict adjective-noun semantics, based on two
existing classification tasks.
middle:way
mass:democracy
green:politics
hardy:soullyric:fantasy
soviet:attitude
public:moneypop:music
relative:peacelow:intelligence
expensive:carserious:condition
honest:man
baptist:ministerhindu:temple
delicious:food
sincere:persondutiful:son
?1
0
1
2 4 6CONC (noun concreteness)
SUB
J (ad
jectiv
e s
ubje
ctivit
y)
easy:meat
naked:truthtired:mind
chief:objectivehot:issue fresh:air
blue:sky
public:information local:prisonfuture:home
expeditionary:force
regular:contactfunny:shape
unsettling:effect
diplomatic:bag
powdered:milk
salty:water
2
3
4
5
6
7
2 4 6CONC (noun concreteness)Ex
pCO
NC (a
djecti
ve ex
pect
ed c
oncr
eten
ess)
Figure 2: Adjective-noun pairs in two semantic
spaces. Selected pairs are labelled for illustration,
italics indicate non-literal meaning combinations.
4.1 Non-literal Composition Task
To evaluate their model of lexical concreteness,
Turney et al (2011) developed a list of 100 com-
mon adjective-noun pairs classified as either deno-
tative (used literally) or connotative (non-literal)
by five annotators. Using an identical supervised
learning procedure to Turney et al (logistic re-
gression with 10-fold cross-validation), we test
whether our lexical representations based on sub-
jectivity and concreteness convey sufficient infor-
mation to perform the same classification.
4.2 Modification-type Classification
Boleda et al (2012) introduce a set of 370
adjective-noun pairs grouped into modification
types by human judges. Because a red car is
both a car and red, the pair is classed as intersec-
tive, whereas dark humour, which is not literally
dark, is classed as subsective. To create a distinct
but analogous binary categorization problem to the
composition task, we filtered out pairs not unani-
mously allocated to either class. We again aim to
classify the remaining 211 intersective and 93 sub-
sective pairs with a logistic regression.
728
Feature type Composition Modification
Baseline 55.0 69.4
Concreteness 83.0 72.7
Subjectivity 64.0 70.4
Combined 85.0 75.0
Turney et al 79.0 -
Table 3: Prediction accuracy (%) of models with
different features on the two tasks. The baseline
method allocates all test pairs to the majority class.
4.3 Results
Models were trained with concreteness features
(CONC and ExpCONC), subjectivity features
(SUBJ and SUBJpfl) and the combination of both
types (Combined). The performance of each
model is presented in Table 3, along with a base-
line score reflecting the strategy of allocating all
pairs to the largest class.
On the non-literal composition task, the con-
creteness (83.0) and combined (85.0) models out-
perform that of Turney et al (79.0). The concrete-
ness model performance represents further confir-
mation of the link between concreteness and com-
position. The improvement of this model over
Turney et al (2011) is perhaps to be expected,
since our model exploits the wide scope of the
new Brysbaert et al (2013) crowdsourced data
whereas Turney et al infer concreteness scores
from a smaller training set. Notably, our combined
model improves on the concreteness-only model,
confirming that the interaction of concreteness and
subjectivity provides additional information perti-
nent to meaning composition.
The modification-type task has no performance
benchmark since Boleda et al (2012) do not use
their data for classification. Although all models
improve on the majority-class baseline, the com-
bined model was again most effective. Additive
improvement over the baseline in each case was
lower than for the composition task, which may
reflect the greater subtlety inherent in the sub-
sective/intersective classification. Indeed, inter-
annotator agreement for this goldstandard (Co-
hen?s ? = 0.87) was lower than for the composi-
tion task (0.95), implying a less cognitively salient
distinction.
5 Conclusion
We have shown that objective adjectives are most
likely to modify concrete nouns, and that non-
literal combinations can emerge when this princi-
ple is violated (Section 3). Indeed, the occurrence
of an adjective with a more abstract noun than
those it typically modifies is a strikingly consistent
indicator of metaphoricity (Table 2). In addition,
we showed that both concreteness and subjectivity
improve the automatic classification of adjective-
noun pairs according to compositionality or mod-
ification type (Section 4). Importantly, a classifier
with both subjectivity and concreteness features
outperforms concreteness-only classifiers, includ-
ing those proposed in previous work.
The results underline the relevance of both sub-
jectivity and concreteness to lexical and phrasal
semantics, and their application to language pro-
cessing tasks. We hypothesize that concreteness
and subjectivity are fundamental to human lan-
guage processing because language is precisely
the conveyance of information about the world
from one party to another. In decoding this sig-
nal, it is clearly informative to understand to what
extent the information refers directly to the world,
and also to what extent it reports a fact versus an
opinion. We believe these dimensions will ulti-
mately prove essential for computational systems
aiming to replicate human performance in inter-
preting language. As the results suggest, they may
be particularly important for capturing the intrica-
cies of semantic composition and thus extending
representations beyond the lexeme.
Of course, two dimensions alone are not suf-
ficient to reflect all of the subtleties of adjective
and noun semantics. For instance, our model clas-
sifies white spirit, a transparent cleaning product,
as non-literal, since the lexical concreteness score
does not allow for strong noun polysemy. Further,
it makes no allowance for wider sentential context,
which can be an important clue to modification
type in such cases.
We aim to address these limitations in future
work by integrating subjectivity and concreteness
with conventially acquired semantic representa-
tions, and, ultimately, models that integrate input
corresponding to the perceptual modalities.
6 Acknowledgements
The authors are supported by St John?s College,
Cambridge and The Royal Society.
729
References
Sylvia Adamson. 2000. A lovely little example. In
Olga Fischer, Annette Rosenbach, and Deiter Stein,
editors, Pathways of change: Grammaticalization in
English. John Benjamins.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2014. Sense-level subjectivity in a multilingual set-
ting. Computer Speech & Language, 28(1):7?19.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
Jeffrey R Binder, Chris F Westbury, Kristen A McK-
iernan, Edward T Possing, and David A Medler.
2005. Distinct brain systems for processing concrete
and abstract concepts. Journal of Cognitive Neuro-
science, 17(6):905?917.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First-order vs. higher-
order modification in distributional semantics. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1223?1233. Association for Computational Linguis-
tics.
Dwight Bolinger. 1967. Adjectives in english: attribu-
tion and predication. Lingua, 18:1?34.
Brian F Bowdle and Dedre Gentner. 2005. The career
of metaphor. Psychological review, 112(1):193.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known English word lemmas. Be-
havior research methods, pages 1?8.
Anneloes R Canestrelli, Willem M Mak, and Ted JM
Sanders. 2013. Causal connectives in discourse
processing: How differences in subjectivity are re-
flected in eye movements. Language and Cognitive
Processes, 28(9):1394?1413.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics, Associa-
tion for Computational Linguistics, pages 241?247.
Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Janyce M Wiebe. 2000.
Effects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of the 18th
conference on Computational linguistics-Volume 1,
pages 299?305. Association for Computational Lin-
guistics.
Felix Hill, Douwe Kiela, and Anna Korhonen. 2013.
Concreteness and corpora: A theoretical and prac-
tical analysis. ACL 2013 Workshop on Cognitive
Modelling and Computational Linguistics, CMCL
2013, page 75.
Felix Hill. 2012. Beauty before age? Applying sub-
jectivity to automatic english adjective ordering. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies:
Student Research Workshop, pages 11?16. Associa-
tion for Computational Linguistics.
Oi Yee Kwong. 2008. A preliminary study on the im-
pact of lexical concreteness on word sense disam-
biguation. In PACLIC, pages 235?244.
Ronald W Langacker. 1997. Consciousness, construal
and subjectivity. Language structure, discourse and
the access to consciousness. Advances in Conscious-
ness Research. John Benjamins, pages 49?57.
Ronald W Langacker. 2002. Deixis and subjectiv-
ity. In Frank Brisard, editor, Grounding: The epis-
temic footing of deixis and reference, pages 1?28.
De Gruyter.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of ACL, pages 622?628. As-
sociation for Computational Linguistics.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Annual Meeting of
the Association for Computational Linguistics, vol-
ume 45, page 976.
Allan Paivio. 1990. Mental Representations: A Dual
Coding Approach. Oxford University Press.
Allan Paivio. 1991. Dual coding theory: Retrospect
and current status. Canadian Journal of Psychol-
ogy/Revue canadienne de psychologie, 45(3):255.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Randolph Quirk, David Crystal, and Pearson Educa-
tion. 1985. A Comprehensive Grammar of the
English Language, volume 397. Cambridge Univ
Press.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the 2003 conference on Empirical methods in
730
natural language processing, pages 105?112. Asso-
ciation for Computational Linguistics.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA. Association for Computational
Linguistics.
Elizabeth C Traugott. 1985. On regularity in semantic
change. Journal of literary semantics, 14(3):155?
173.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empiri-
cal Methods in Natural Language Processing, pages
680?690.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 1065?1072. Asso-
ciation for Computational Linguistics.
Janyce Wiebe and Ellen Riloff. 2011. Finding mutual
benefit between subjectivity analysis and informa-
tion extraction. Affective Computing, IEEE Trans-
actions on, 2(4):175?191.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In AAAI/IAAI, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing, pages
347?354. Association for Computational Linguis-
tics.
731
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835?841,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving Multi-Modal Representations Using Image Dispersion:
Why Less is Sometimes More
Douwe Kiela*, Felix Hill*, Anna Korhonen and Stephen Clark
University of Cambridge
Computer Laboratory
{douwe.kiela|felix.hill|anna.korhonen|stephen.clark}@cl.cam.ac.uk
Abstract
Models that learn semantic representations
from both linguistic and perceptual in-
put outperform text-only models in many
contexts and better reflect human concept
acquisition. However, experiments sug-
gest that while the inclusion of perceptual
input improves representations of certain
concepts, it degrades the representations
of others. We propose an unsupervised
method to determine whether to include
perceptual input for a concept, and show
that it significantly improves the ability of
multi-modal models to learn and represent
word meanings. The method relies solely
on image data, and can be applied to a va-
riety of other NLP tasks.
1 Introduction
Multi-modal models that learn semantic concept
representations from both linguistic and percep-
tual input were originally motivated by parallels
with human concept acquisition, and evidence that
many concepts are grounded in the perceptual sys-
tem (Barsalou et al, 2003). Such models extract
information about the perceptible characteristics
of words from data collected in property norming
experiments (Roller and Schulte im Walde, 2013;
Silberer and Lapata, 2012) or directly from ?raw?
data sources such as images (Feng and Lapata,
2010; Bruni et al, 2012). This input is combined
with information from linguistic corpora to pro-
duce enhanced representations of concept mean-
ing. Multi-modal models outperform language-
only models on a range of tasks, including mod-
elling conceptual association and predicting com-
positionality (Bruni et al, 2012; Silberer and Lap-
ata, 2012; Roller and Schulte im Walde, 2013).
Despite these results, the advantage of multi-
modal over linguistic-only models has only been
demonstrated on concrete concepts, such as
chocolate or cheeseburger, as opposed to abstract
concepts such as such as guilt or obesity. Indeed,
experiments indicate that while the addition of
perceptual input is generally beneficial for repre-
sentations of concrete concepts (Hill et al, 2013a;
Bruni et al, 2014), it can in fact be detrimental
to representations of abstract concepts (Hill et al,
2013a). Further, while the theoretical importance
of the perceptual modalities to concrete represen-
tations is well known, evidence suggests this is not
the case for more abstract concepts (Paivio, 1990;
Hill et al, 2013b). Indeed, perhaps the most influ-
ential characterization of the abstract/concrete dis-
tinction, the Dual Coding Theory (Paivio, 1990),
posits that concrete representations are encoded
in both the linguistic and perceptual modalities
whereas abstract concepts are encoded only in the
linguistic modality.
Existing multi-modal architectures generally
extract and process all the information from their
specified sources of perceptual input. Since per-
ceptual data sources typically contain information
about both abstract and concrete concepts, such in-
formation is included for both concept types. The
potential effect of this design decision on perfor-
mance is significant because the vast majority of
meaning-bearing words in everyday language cor-
respond to abstract concepts. For instance, 72% of
word tokens in the British National Corpus (Leech
et al, 1994) were rated by contributors to the Uni-
versity of South Florida dataset (USF) (Nelson et
al., 2004) as more abstract than the noun war, a
concept that many would consider quite abstract.
In light of these considerations, we propose
a novel algorithm for approximating conceptual
concreteness. Multi-modal models in which per-
ceptual input is filtered according to our algorithm
learn higher-quality semantic representations than
previous approaches, resulting in a significant per-
formance improvement of up to 17% in captur-
835
ing the semantic similarity of concepts. Further,
our algorithm constitutes the first means of quan-
tifying conceptual concreteness that does not rely
on labor-intensive experimental studies or annota-
tors. Finally, we demonstrate the application of
this unsupervised concreteness metric to the se-
mantic classification of adjective-noun pairs, an
existing NLP task to which concreteness data has
proved valuable previously.
2 Experimental Approach
Our experiments focus on multi-modal models
that extract their perceptual input automatically
from images. Image-based models more natu-
rally mirror the process of human concept acquisi-
tion than those whose input derives from exper-
imental datasets or expert annotation. They are
also more scalable since high-quality tagged im-
ages are freely available in several web-scale im-
age datasets.
We use Google Images as our image source,
and extract the first n image results for each con-
cept word. It has been shown that images from
Google yield higher-quality representations than
comparable sources such as Flickr (Bergsma and
Goebel, 2011). Other potential sources, such as
ImageNet (Deng et al, 2009) or the ESP Game
Dataset (Von Ahn and Dabbish, 2004), either do
not contain images for abstract concepts or do not
contain sufficient images for the concepts in our
evaluation sets.
2.1 Image Dispersion-Based Filtering
Following the motivation outlined in Section 1, we
aim to distinguish visual input corresponding to
concrete concepts from visual input correspond-
ing to abstract concepts. Our algorithm is moti-
vated by the intuition that the diversity of images
returned for a particular concept depends on its
concreteness (see Figure 1). Specifically, we an-
ticipate greater congruence or similarity among a
set of images for, say, elephant than among im-
ages for happiness. By exploiting this connection,
the method approximates the concreteness of con-
cepts, and provides a basis to filter the correspond-
ing perceptual information.
Formally, we propose a measure, image disper-
sion d of a concept word w, defined as the aver-
age pairwise cosine distance between all the image
representations { ~w
1
. . . ~w
n
} in the set of images
for that concept:
Figure 1: Example images for a concrete (elephant
? little diversity, low dispersion) and an abstract
concept (happiness ? greater diversity, high dis-
persion).
Figure 2: Computation of PHOW descriptors us-
ing dense SIFT for levels l = 0 to l = 2 and the
corresponding histogram representations (Bosch
et al, 2007).
d(w) =
1
2n(n? 1)
?
i<j?n
1?
~w
i
? ~w
j
| ~w
i
|| ~w
j
|
(1)
We use an average pairwise distance-based met-
ric because this emphasizes the total variation
more than e.g. the mean distance from the cen-
troid. In all experiments we set n = 50.
Generating Visual Representations Visual
vector representations for each image were ob-
tained using the well-known bag of visual words
(BoVW) approach (Sivic and Zisserman, 2003).
BoVW obtains a vector representation for an
836
image by mapping each of its local descriptors
to a cluster histogram using a standard clustering
algorithm such as k-means.
Previous NLP-related work uses SIFT (Feng
and Lapata, 2010; Bruni et al, 2012) or SURF
(Roller and Schulte im Walde, 2013) descriptors
for identifying points of interest in an image,
quantified by 128-dimensional local descriptors.
We apply Pyramid Histogram Of visual Words
(PHOW) descriptors, which are particularly well-
suited for object categorization, a key component
of image similarity and thus dispersion (Bosch et
al., 2007). PHOW is roughly equivalent to run-
ning SIFT on a dense grid of locations at a fixed
scale and orientation and at multiple scales (see
Fig 2), but is both more efficient and more accu-
rate than regular (dense) SIFT approaches (Bosch
et al, 2007). We resize the images in our dataset
to 100x100 pixels and compute PHOW descriptors
using VLFeat (Vedaldi and Fulkerson, 2008).
The descriptors for the images were subse-
quently clustered using mini-batch k-means (Scul-
ley, 2010) with k = 50 to obtain histograms of
visual words, yielding 50-dimensional visual vec-
tors for each of the images.
Generating Linguistic Representations We
extract continuous vector representations (also of
50 dimensions) for concepts using the continu-
ous log-linear skipgram model of Mikolov et al
(2013a), trained on the 100M word British Na-
tional Corpus (Leech et al, 1994). This model
learns high quality lexical semantic representa-
tions based on the distributional properties of
words in text, and has been shown to outperform
simple distributional models on applications such
as semantic composition and analogical mapping
(Mikolov et al, 2013b).
2.2 Evaluation Gold-standards
We evaluate models by measuring the Spearman
correlation of model output with two well-known
gold-standards reflecting semantic proximity ? a
standard measure for evaluating the quality of rep-
resentations (see e.g. Agirre et al (2009)).
To test the ability of our model to capture
concept similarity, we measure correlations with
WordSim353 (Finkelstein et al, 2001), a selec-
tion of 353 concept pairs together with a similar-
ity rating provided by human annotators. Word-
Sim has been used as a benchmark for distribu-
tional semantic models in numerous studies (see
e.g. (Huang et al, 2012; Bruni et al, 2012)).
As a complementary gold-standard, we use the
University of South Florida Norms (USF) (Nelson
et al, 2004). This dataset contains scores for free
association, an experimental measure of cognitive
association, between over 40,000 concept pairs.
The USF norms have been used in many previous
studies to evaluate semantic representations (An-
drews et al, 2009; Feng and Lapata, 2010; Sil-
berer and Lapata, 2012; Roller and Schulte im
Walde, 2013). The USF evaluation set is partic-
ularly appropriate in the present context because
concepts in the dataset are also rated for concep-
tual concreteness by at least 10 human annotators.
We create a representative evaluation set of USF
pairs as follows. We randomly sample 100 con-
cepts from the upper quartile and 100 concepts
from the lower quartile of a list of all USF con-
cepts ranked by concreteness. We denote these
sets C, for concrete, and A for abstract respec-
tively. We then extract all pairs (w
1
, w
2
) in the
USF dataset such that bothw
1
andw
2
are inA?C.
This yields an evaluation set of 903 pairs, of which
304 are such that w
1
, w
2
? C and 317 are such
that w
1
, w
2
? A.
The images used in our experiments and
the evaluation gold-standards can be down-
loaded from http://www.cl.cam.ac.uk/
?
dk427/dispersion.html.
3 Improving Multi-Modal
Representations
We apply image dispersion-based filtering as fol-
lows: if both concepts in an evaluation pair have
an image dispersion below a given threshold, both
the linguistic and the visual representations are in-
cluded. If not, in accordance with the Dual Cod-
ing Theory of human concept processing (Paivio,
1990), only the linguistic representation is used.
For both datasets, we set the threshold as the
median image dispersion, although performance
could in principle be improved by adjusting this
parameter. We compare dispersion filtered rep-
resentations with linguistic, perceptual and stan-
dard multi-modal representations (concatenated
linguistic and perceptual representations). Sim-
ilarity between concept pairs is calculated using
cosine similarity.
As Figure 3 shows, dispersion-filtered multi-
modal representations significantly outperform
837
0.145
0.532
0.477
0.542
0.189
0.229
0.203
0.247
0.0
0.1
0.2
0.3
0.4
0.5
Similarity ? WordSim 353 Free association ? USF (903)Evaluation Set
Cor
rela
tion
Model Representations
Linguistic onlyImage onlyStandard multi?modalDispersion filtered
Figure 3: Performance of conventional multi-
modal (visual input included for all concepts) vs.
image dispersion-based filtering models (visual in-
put only for concepts classified as concrete) on the
two evaluation gold-standards.
standard multi-modal representations on both
evaluation datasets. We observe a 17% increase in
Spearman correlation on WordSim353 and a 22%
increase on the USF norms. Based on the corre-
lation comparison method of Steiger (1980), both
represent significant improvements (WordSim353,
t = 2.42, p < 0.05; USF, t = 1.86, p < 0.1). In
both cases, models with the dispersion-based filter
also outperform the purely linguistic model, which
is not the case for other multi-modal approaches
that evaluate on WordSim353 (e.g. Bruni et al
(2012)).
4 Concreteness and Image Dispersion
The filtering approach described thus far improves
multi-modal representations because image dis-
persion provides a means to distinguish concrete
concepts from more abstract concepts. Since re-
search has demonstrated the applicability of con-
creteness to a range of other NLP tasks (Turney et
al., 2011; Kwong, 2008), it is important to exam-
ine the connection between image dispersion and
concreteness in more detail.
4.1 Quantifying Concreteness
To evaluate the effectiveness of image dispersion
as a proxy for concreteness we evaluated our al-
gorithm on a binary classification task based on
the set of 100 concrete and 100 abstract concepts
A?C introduced in Section 2. By classifying con-
0.184
0.257
0.29
0.054
0.189
0.167
0.0
0.1
0.2
0.3
0.4
'concrete' pairs (304) 'abstract' pairs (317)Concept Type
Cor
rela
tion
Representation Modality
LinguisticVisualLinguistic+Visual
Figure 4: Visual input is valuable for representing
concepts that are classified as concrete by the im-
age dispersion algorithm, but not so for concepts
classified as abstract. All correlations are with the
USF gold-standard.
cepts with image dispersion below the median as
concrete and concepts above this threshold as ab-
stract we achieved an abstract-concrete prediction
accuracy of 81%.
While well-understood intuitively, concreteness is
not a formally defined notion. Quantities such as
the USF concreteness score depend on the sub-
jective judgement of raters and the particular an-
notation guidelines. According to the Dual Cod-
ing Theory, however, concrete concepts are pre-
cisely those with a salient perceptual representa-
tion. As illustrated in Figure 4, our binary clas-
sification conforms to this characterization. The
importance of the visual modality is significantly
greater when evaluating on pairs for which both
concepts are classified as concrete than on pairs of
two abstract concepts.
Image dispersion is also an effective predic-
tor of concreteness on samples for which the ab-
stract/concrete distinction is less clear. On a differ-
ent set of 200 concepts extracted by random sam-
pling from the USF dataset stratified by concrete-
ness rating (including concepts across the con-
creteness spectrum), we observed a high correla-
tion between abstractness and dispersion (Spear-
man ? = 0.61, p < 0.001). On this more diverse
sample, which reflects the range of concepts typi-
cally found in linguistic corpora, image dispersion
is a particularly useful diagnostic for identifying
838
Concept Image Dispersion Conc. (USF)
shirt .488 6.05
bed .495 5.91
knife .560 6.08
dress .578 6.59
car .580 6.35
ego 1.000 1.93
nonsense .999 1.90
memory .999 1.78
potential .997 1.90
know .996 2.70
Table 1: Concepts with highest and lowest image
dispersion scores in our evaluation set, and con-
creteness ratings from the USF dataset.
the very abstract or very concrete concepts. As
Table 1 illustrates, the concepts with the lowest
dispersion in this sample are, without exception,
highly concrete, and the concepts of highest dis-
persion are clearly very abstract.
It should be noted that all previous approaches
to the automatic measurement of concreteness rely
on annotator ratings, dictionaries or manually-
constructed resources. Kwong (2008) proposes
a method based on the presence of hard-coded
phrasal features in dictionary entries correspond-
ing to each concept. By contrast, S?anchez et al
(2011) present an approach based on the position
of word senses corresponding to each concept in
the WordNet ontology (Fellbaum, 1999). Turney
et al (2011) propose a method that extends a large
set of concreteness ratings similar to those in the
USF dataset. The Turney et al algorithm quanti-
fies the concreteness of concepts that lack such a
rating based on their proximity to rated concepts
in a semantic vector space. In contrast to each of
these approaches, the image dispersion approach
requires no hand-coded resources. It is therefore
more scalable, and instantly applicable to a wide
range of languages.
4.2 Classifying Adjective-Noun Pairs
Finally, we explored whether image dispersion
can be applied to specific NLP tasks as an effec-
tive proxy for concreteness. Turney et al (2011)
showed that concreteness is applicable to the clas-
sification of adjective-noun modification as either
literal or non-literal. By applying a logistic regres-
sion with noun concreteness as the predictor vari-
able, Turney et al achieved a classification accu-
racy of 79% on this task. This model relies on sig-
nificant supervision in the form of over 4,000 hu-
man lexical concreteness ratings.
1
Applying im-
age dispersion in place of concreteness in an iden-
tical classifier on the same dataset, our entirely un-
supervised approach achieves an accuracy of 63%.
This is a notable improvement on the largest-class
baseline of 55%.
5 Conclusions
We presented a novel method, image dispersion-
based filtering, that improves multi-modal repre-
sentations by approximating conceptual concrete-
ness from images and filtering model input. The
results clearly show that including more percep-
tual input in multi-modal models is not always bet-
ter. Motivated by this fact, our approach provides
an intuitive and straightforward metric to deter-
mine whether or not to include such information.
In addition to improving multi-modal represen-
tations, we have shown the applicability of the im-
age dispersion metric to several other tasks. To
our knowledge, our algorithm constitutes the first
unsupervised method for quantifying conceptual
concreteness as applied to NLP, although it does,
of course, rely on the Google Images retrieval al-
gorithm. Moreover, we presented a method to
classify adjective-noun pairs according to modi-
fication type that exploits the link between image
dispersion and concreteness. It is striking that this
apparently linguistic problem can be addressed
solely using the raw data encoded in images.
In future work, we will investigate the precise
quantity of perceptual information to be included
for best performance, as well as the optimal filter-
ing threshold. In addition, we will explore whether
the application of image data, and the interaction
between images and language, can yield improve-
ments on other tasks in semantic processing and
representation.
Acknowledgments
DK is supported by EPSRC grant EP/I037512/1.
FH is supported by St John?s College, Cambridge.
AK is supported by The Royal Society. SC is sup-
ported by ERC Starting Grant DisCoTex (306920)
and EPSRC grant EP/I037512/1. We thank the
anonymous reviewers for their helpful comments.
1
The MRC Psycholinguistics concreteness ratings (Colt-
heart, 1981) used by Turney et al (2011) are a subset of those
included in the USF dataset.
839
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 19?27, Boulder, Colorado.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological review, 116(3):463.
Lawrence W Barsalou, W Kyle Simmons, Aron K Bar-
bey, and Christine D Wilson. 2003. Grounding
conceptual knowledge in modality-specific systems.
Trends in cognitive sciences, 7(2):84?91.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
RANLP, pages 399?405.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image classification using random forests and
ferns. In Proceedings of ICCV.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
Max Coltheart. 1981. The MRC psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497?505.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248?255. IEEE.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99. Asso-
ciation for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406?
414. ACM.
Felix Hill, Douwe Kiela, and Anna Korhonen. 2013a.
Concreteness and corpora: A theoretical and practi-
cal analysis. CMCL 2013.
Felix Hill, Anna Korhonen, and Christian Bentz.
2013b. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive science, 38(1).
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Oi Yee Kwong. 2008. A preliminary study on the im-
pact of lexical concreteness on word sense disam-
biguation. In PACLIC, pages 235?244.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, & Com-
puters, 36(3):402?407.
Allan Paivio. 1990. Mental representations: A dual
coding approach. Oxford University Press.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
David S?anchez, Montserrat Batet, and David Isern.
2011. Ontology-based information content compu-
tation. Knowledge-Based Systems, 24(2):297?303.
D Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th international conference on
World wide web, pages 1177?1178. ACM.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
840
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433. As-
sociation for Computational Linguistics.
J. Sivic and A. Zisserman. 2003. Video Google: a text
retrieval approach to object matching in videos. In
Proceedings of the Ninth IEEE International Con-
ference on Computer Vision, volume 2, pages 1470?
1477, Oct.
James H Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empiri-
cal Methods in Natural Language Processing, pages
680?690.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319?326. ACM.
841
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170?179,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Modelling selectional preferences in a lexical hierarchy
Diarmuid O? Se?aghdha
Computer Laboratory
University of Cambridge
Cambridge, UK
do242@cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge, UK
Anna.Korhonen@cl.cam.ac.uk
Abstract
This paper describes Bayesian selectional
preference models that incorporate knowledge
from a lexical hierarchy such as WordNet. In-
spired by previous work on modelling with
WordNet, these approaches are based either on
?cutting? the hierarchy at an appropriate level
of generalisation or on a ?walking? model that
selects a path from the root to a leaf. In
an evaluation comparing against human plau-
sibility judgements, we show that the mod-
els presented here outperform previously pro-
posed comparable WordNet-based models, are
competitive with state-of-the-art selectional
preference models and are particularly well-
suited to estimating plausibility for items that
were not seen in training.
1 Introduction
The concept of selectional preference captures the
intuitive fact that predicates in language have a bet-
ter semantic ?fit? for certain arguments than oth-
ers. For example, the direct object argument slot
of the verb eat is more plausibly filled by a type
of food (I ate a pizza) than by a type of vehicle (I
ate a car), while the subject slot of the verb laugh
is more plausibly filled by a person than by a veg-
etable. Human language users? knowledge about
selectional preferences has been implicated in anal-
yses of metaphor processing (Wilks, 1978) and in
psycholinguistic studies of comprehension (Rayner
et al, 2004). In Natural Language Processing, au-
tomatically acquired preference models have been
shown to aid a number of tasks, including semantic
role labelling (Zapirain et al, 2009), parsing (Zhou
et al, 2011) and lexical disambiguation (Thater et
al., 2010; O? Se?aghdha and Korhonen, 2011).
It is tempting to assume that with a large enough
corpus, preference learning reduces to a simple lan-
guage modelling task that can be solved by counting
predicate-argument co-occurrences. Indeed, Keller
and Lapata (2003) show that relatively good perfor-
mance at plausibility estimation can be attained by
submitting queries to a Web search engine. How-
ever, there are many scenarios where this approach
is insufficient: for languages and language domains
where Web-scale data is unavailable, for predicate
types (e.g., inference rules or semantic roles) that
cannot be retrieved by keyword search and for ap-
plications where accurate models of rarer words are
required. O? Se?aghdha (2010) shows that the Web-
based approach is reliably outperformed by more
complex models trained on smaller corpora for less
frequent predicate-argument combinations. Models
that induce a level of semantic representation, such
as probabilistic latent variable models, have a further
advantage in that they can provide rich structured in-
formation for downstream tasks such as lexical dis-
ambiguation (O? Se?aghdha and Korhonen, 2011) and
semantic relation mining (Yao et al, 2011).
Recent research has investigated the potential
of Bayesian probabilistic models such as Latent
Dirichlet Allocation (LDA) for modelling selec-
tional preferences (O? Se?aghdha, 2010; Ritter et al,
2010; Reisinger and Mooney, 2011). These mod-
els are flexible and robust, yielding superior perfor-
mance compared to previous approaches. In this
paper we present a preliminary study of analogous
170
models that make use of a lexical hierarchy (in our
case the WordNet hierarchy). We describe two broad
classes of probabilistic models over WordNet and
how they can be implemented in a Bayesian frame-
work. The two main potential advantages of in-
corporating WordNet information are: (a) improved
predictions about rare and out-of-vocabulary argu-
ments; (b) the ability to perform syntactic word
sense disambiguation with a principled probabilistic
model and without the need for an additional step
that heuristically maps latent variables onto Word-
Net senses. Focussing here on (a), we demon-
strate that our models attain better performance than
previously-proposed WordNet-based methods on a
plausibility estimation task and are particularly well-
suited to estimating plausibility for arguments that
were not seen in training and for which LDA cannot
make useful predictions.
2 Background and Related Work
The WordNet lexical hierarchy (Fellbaum, 1998)
is one of the most-used resources in NLP, finding
many applications in both the definition of tasks (e.g.
the SENSEVAL/SemEval word sense disambigua-
tion tasks) and in the construction of systems. The
idea of using WordNet to define selectional prefer-
ences was first implemented by Resnik (1993), who
proposed a measure of associational strength be-
tween a semantic class s and a predicate p corre-
sponding to a relation type r:
A(s, p, r) =
1
?
P (s|p, r) log2
P (s|p, r)
P (s|r)
(1)
where ? is a normalisation term. This measure cap-
tures the degree to which the probability of seeing
s given the predicate p differs from the prior proba-
bility of s. Given that we are often interested in the
preference of p for a word w rather than a class and
words generally map onto multiple classes, Resnik
suggests calculating A(s, p, r) for all classes that
could potentially be expressed by w and predicting
the maximal value.
Cut-based models assume that modelling the se-
lectional preference of a predicate involves finding
the right ?level of generalisation? in the WordNet
hierarchy. For example, the direct object slot of
the verb eat can be associated with the subhierarchy
rooted at the synset food#n#1, as all hyponyms of
that synset are assumed to be edible and the imme-
diate hypernym of the synset, substance#n#1, is too
general given that many substances are rarely eaten.1
This leads to the notion of ?cutting? the hierarchy at
one or more positions (Li and Abe, 1998). The mod-
elling task then becomes that of finding the cuts that
are maximally general without overgeneralising. Li
and Abe (1998) propose a model in which the appro-
priate cut c is selected according to the Minimum
Description Length principle; this principle explic-
itly accounts for the trade-off between generalisa-
tion and accuracy by minimising a sum of model de-
scription length and data description length. The
probability of a predicate p taking as its argument
an synset s is modelled as:
Pla(s|p, r) = P (s|cs,p,r)P (c|p) (2)
where cs,p,r is the portion of the cut learned for p
that dominates s. The distribution P (s|cs,p,r) is held
to be uniform over all synsets dominated by cs,p,r,
while P (c|p) is given by a maximum likelihood es-
timate.
Clark and Weir (2002) present a model that, while
not explicitly described as cut-based, likewise seeks
to find the right level of generalisation for an obser-
vation. In this case, the hypernym at which to ?cut?
is chosen by a chi-squared test: if the aggregate pref-
erence of p for classes in the subhierarchy rooted at c
differs significantly from the individual preferences
of p for the immediate children of c, the hierarchy is
cut below c. The probability of p taking a synset s
as its argument is given by:
Pcw(s|p, r) =
P (p|cs,p,r, r)
P (s|r)
P (p|r)
?
s??S P (p|cs?,p,r, r)
P (s?|r)
P (p|r)
(3)
where cs,p,r is the root node of the subhierarchy con-
taining s that was selected for p.
An alternative approach to modelling with Word-
Net uses its hierarchical structure to define a Markov
model with transitions from senses to senses and
from senses to words. The intuition here is that each
observation is generated by a ?walk? from the root
of the hierarchy to a leaf node and emitting the word
1In this paper we use WordNet version 3.0, except where
stated otherwise.
171
corresponding to the leaf. Abney and Light (1999)
proposed such a model for selectional preferences,
trained via EM, but failed to achieve competitive
performance on a pseudodisambiguation task.
The models described above have subsequently
been used in many different studies. For exam-
ple: McCarthy and Carroll (2003) use Li and Abe?s
method in a word sense disambiguation setting;
Schulte im Walde et al (2008) use their MDL ap-
proach as part of a system for syntactic and seman-
tic subcategorisation frame learning; Shutova (2010)
deploys Resnik?s method for metaphor interpreta-
tion. Brockmann and Lapata (2003) report a com-
parative evaluation in which the methods of Resnik
and Clark and Weir outpeform Li and Abe?s method
on a plausibility estimation task.
Much recent work on preference learning has fo-
cused on purely distributional methods that do not
use a predefined hierarchy but learn to make general-
isations about predicates and arguments from corpus
observations alone. These methods can be vector-
based (Erk et al, 2010; Thater et al, 2010), dis-
criminative (Bergsma et al, 2008) or probabilistic
(O? Se?aghdha, 2010; Ritter et al, 2010; Reisinger
and Mooney, 2011). In the probabilistic category,
Bayesian models based on the ?topic modelling?
framework (Blei et al, 2003b) have been shown to
achieve state-of-the-art performance in a number of
evaluation settings; the models considered in this pa-
per are also related to this framework.
In machine learning, researchers have proposed
a variety of topic modelling methods where the la-
tent variables are arranged in a hierarchical structure
(Blei et al, 2003a; Mimno et al, 2007). In con-
trast to the present work, these models use a rel-
atively shallow hierarchy (e.g., 3 levels) and any
hierarchy node can in principle emit any vocabu-
lary item; they thus provide a poor match for our
goal of modelling over WordNet. Boyd-Graber et
al. (2007) describe a topic model that is directly in-
fluenced by Abney and Light?s Markov model ap-
proach; this model (LDAWN) is described further in
Section 3.3 below. Reisinger and Pas?ca (2009) in-
vestigate Bayesian methods for attaching attributes
harvested from the Web at an appropriate level in
the WordNet hierarchy; this task is related in spirit
to the preference learning task.
3 Probabilistic modelling over WordNet
3.1 Notation
We assume that we have a lexical hierarchy in the
form of a directed acyclic graph G = (S,E) where
each node (or synset) s ? S is associated with a
set of words Wn belonging to a large vocabulary V .
Each edge e ? E leads from a node n to its children
(or hyponyms) Chn. As G is a DAG, a node may
have more than one parent but there are no cycles.
The ultimate goal is to learn a distribution over the
argument vocabulary V for each predicate p in a set
P , through observing predicate-argument pairs. A
predicate is understood to correspond to a pairing of
a lexical item v and a relation type r, for example
p = (eat, direct object). The list of observations
for a predicate p is denoted by Observations(p).
3.2 Cut-based models
Model 1 Generative story for WN-CUT
for cut c ? {1 . . . |C|} do
?c ?Multinomial(?c)
end for
for predicate p ? {1 . . . |P |} do
?p ? Dirichlet(?)
for argument instance i ? Observations(p)
do
ci ?Multinomial(?p)
wi ?Multinomial(?ci)
end for
end for
The first model we consider, WN-CUT, is directly
inspired by Li and Abe?s model (2). Each predicate
p is associated with a distribution over ?cuts?, i.e.,
complete subgraphs of G rooted at a single node
and containing all nodes dominated by the root. It
follows that the number of possible cuts is the same
as the number of synsets. Each cut c is associated
with a non-uniform distribution over the set of words
Wc that can be generated by the synsets contained
in c. As well as the use of a non-uniform emis-
sion distribution and the placing of Dirichlet priors
on the multinomial over cuts, a significant differ-
ence from Li and Abe?s model is that overlapping
cuts are permitted (indeed, every cut has non-zero
probability given a predicate). For example, the
172
model may learn that the direct object slot of eat
gives high probability to the cut rooted at food#n#1
but also that the cut rooted at substance#n#1 has
non-negligible probability, higher than that assigned
to phenomenon#n#1. It follows that the estimated
probability of p taking argument w takes into ac-
count all possible cuts, weighted by their probability
given p.
The generative story for WN-CUT is given in Al-
gorithm 1; this describes the assumptions made by
the model about how a corpus of observations is gen-
erated. The probability of predicate p taking argu-
ment w is defined as (4); an empirical posterior esti-
mate of this quantity can be computed from a Gibbs
sampling state via (5):
P (w|p) =
?
c
P (c|p)P (w|c) (4)
?
?
c
fcp + ?
f?p + |C|?
fwc + ?
f?c + |Wc|?
(5)
where fcw is the number of observations contain-
ing argument w that have been assigned cut c, fcp
is the number of observations containing predicate
p that have been assigned cut c and fc?, f?p are the
marginal counts for cut c and predicate p, respec-
tively. The two terms that are multiplied in (4) play
complementary roles analogous to those of the two
description lengths in Li and Abe?s MDL formula-
tion; P (c|p) will prefer to reuse more general cuts,
while P (w|c) will prefer more specific cuts with a
smaller associated argument vocabulary.
As the number of words |Wc| that can be emitted
by a cut |c| varies according to the size of the sub-
hierarchy under the cut, the proportion of probability
mass accorded to the likelihood and the prior in (5)
is not constant. An alternative formulation is to keep
the distribution of mass between likelihood and prior
constant but vary the value of the individual ?c pa-
rameters according to cut size. Experiments suggest
that this alternative does not differ in performance.
The second cut-based model, WN-CUT-TOPICS,
extends WN-CUT by adding two extra layers of la-
tent variables. Firstly, the choice of cut is condi-
tional on a ?topic? variable z rather than directly
conditioned on the predicate; when the topic vocab-
ulary Z is much smaller than the cut vocabulary C,
this has the effect of clustering the cuts. Secondly,
Model 2 Generative story for WN-CUT-TOPICS
for topic z ? {1 . . . |Z|} do
?z ? Dirichlet(?)
end for
for cut c ? {1 . . . |C|} do
?c ? Dirichlet(?c)
end for
for synset s ? {1 . . . |S|} do
?s ? Dirichlet(?s)
end for
for predicate p ? {1 . . . |P |} do
?p ? Dirichlet(?)
for argument instance i ? Observations(p)
do
zi ?Multinomial(?p)
ci ?Multinomial(?z)
si ?Multinomial(?c)
wi ?Multinomial(?s)
end for
end for
instead of immediately drawing a word once a cut
has been chosen, the model first draws a synset s
and then draws a word from the vocabularyWs asso-
ciated with that synset. This has two advantages; it
directly disambiguates each observation to a specific
synset rather than to a region of the hierarchy and it
should also improve plausibility predictions for rare
synonyms of common arguments. The generative
story for WN-CUT-TOPICS is given in Algorithm 2;
the distribution over arguments for p is given in (6)
and the corresponding posterior estimate in (7):
P (w|p) =
?
z
P (z|p)
?
c
P (c|z)
?
s
P (s|c)P (w|s)
(6)
?
?
z
fzp + ?z
f?p +
?
z? ?z?
?
c
fcz + ?
f?z + |C|?
?
?
s
fsc + ?
f?c + |Sc|?
fws + ?
f?s + |Ws|?
(7)
As before, fzp, fcz , fsc and fws are the re-
spective co-occurrence counts of topics/predicates,
cuts/topics, synsets/cuts and words/synsets in the
sampling state and f?p, f?z , f?c and f?s are the cor-
responding marginal counts.
173
Since WN-CUT and WN-CUT-TOPICS are con-
structed from multinomials with Dirichlet priors,
it is relatively straightforward to train them by
collapsed Gibbs sampling (Griffiths and Steyvers,
2004), an iterative method whereby each latent vari-
able in the model is stochastically updated accord-
ing to the distribution given by conditioning on the
current latent variable assignments of all other to-
kens. In the case of WN-CUT, this amounts to up-
dating the cut assignment ci for each token in turn.
For WN-CUT-TOPICS there are three variables to
update; ci and si must be updated simultaneously,
but zi can be updated independently for the bene-
fit of efficiency. Although WordNet contains 82,115
noun synsets, updates for ci and si can be computed
very efficiently, as there are typically few possible
synsets for a given word type and few possible cuts
for a given synset (the maximum synset depth is 19).
The hyperparameters for the various Dirichlet pri-
ors are also reestimated in the course of learning; the
values of these hyperparameters control the degree
of sparsity preferred by the model. The ?top-level?
hyperparameters ? in WN-CUT and ? in WN-CUT-
TOPICS are estimated using a fixed-point iteration
proposed by Wallach (2008); the other hyperparam-
eters are learned by slice sampling (Neal, 2003).
3.3 Walk-based models
Abney and Light (1999) proposed an approach to
selectional preference learning in which arguments
are generated for predicates by following a path
? = (l1, . . . , l|?|) from the root of the hierarchy to a
leaf node and emitting the corresponding word. The
path is chosen according to a Markov model with
transition probabilities specific to each predicate. In
this model, each leaf node is associated with a sin-
gle word; the synsets associated with that word are
the immediate parent nodes of the leaf. Abney and
Light found that their model did not match the per-
formance of Resnik?s (1993) simpler method. We
have had a similar lack of success with a Bayesian
version of this model, which we do not describe fur-
ther here.
Boyd-Graber et al (2007) describe a related topic
model, LDAWN, for word sense disambiguation
that adds an intermediate layer of latent variables
Z on which the Markov model parameters are con-
ditioned. In their application, each document in a
Model 3 Generative story for LDAWN
for topic z ? {1 . . . |Z|} do
for synset s ? {1 . . . |S|} do
Draw transition probabilities ?z,s ?
Dirichlet(??s)
end for
end for
for predicate p ? {1 . . . |P |} do
?p ? Dirichlet(?)
for argument instance i ? Observations(p)
do
zi ?Multinomial(?p)
Create a path starting at the root synset ?0:
while not at a leaf node do
?t+1 ?Multinomial(?zi,?t)
end while
Emit the word at the leaf as wi
end for
end for
corpus is associated with a distribution over topics
and each topic is associated with a distribution over
paths. The clustering effect of the topic layer allows
the documents to ?share? information and hence al-
leviate problems due to sparsity. By analogy to Ab-
ney and Light, it is a short and intuitive step to ap-
ply LDAWN to selectional preference learning. The
generative story for LDAWN is given in Algorithm
3; the probability model for P (w|p) is defined by (8)
and the posterior estimate is (9):
P (w|p) =
?
z
P (z|p)
?
?
1[?? w]P (?|z) (8)
?
?
z
fzp + ?z
f?p +
?
z? ?z?
?
?
1[?? w]?
|?|?1?
i=1
fz,li?li+1 + ??li?li+1
fz,li?? + ?
(9)
where 1[? ? w] = 1 when the path ? leads to leaf
node w and has value 0 otherwise. Following Boyd-
Graber et al the Dirichlet priors on the transition
probabilities are parameterised by the product of a
strength parameter ? and a distribution ?s, the latter
being fixed according to relative corpus frequencies
to ?guide? the model towards more fruitful paths.
Gibbs sampling updates for LDAWN are given in
Boyd-Graber et al (2007). As before, we reestimate
174
SEEN:
staff morale 0.4889
team morale 0.5945
issue morale 0.0595
UNSEEN:
pupil morale 0.4318
minute morale -0.0352
snow morale -0.2748
Table 1: Extract from the noun-noun section of Keller and
Lapata?s (2003) dataset, with human plausibility scores
the hyperparameters during learning; ? is estimated
by Wallach?s fixed-point iteration and ? is estimated
by slice sampling.
4 Experiments
4.1 Experimental procedure
We evaluate our methods by comparing their predic-
tions to human judgements of predicate-argument
plausibility. This is a standard approach to se-
lectional preference evaluation (Keller and Lapata,
2003; Brockmann and Lapata, 2003; O? Se?aghdha,
2010) and arguably yields a better appraisal of a
model?s intrinsic semantic quality than other eval-
uations such as pseudo-disambiguation or held-out
likelihood prediction.2 We use a set of plau-
sibility judgements collected by Keller and Lap-
ata (2003). This dataset comprises 180 predicate-
argument combinations for each of three syntactic
relations: verb-object, noun-noun modification and
adjective-noun modification. The data for each re-
lation is divided into a ?seen? portion containing
90 combinations that were observed in the British
National Corpus and an ?unseen? portion contain-
ing 90 combinations that do not appear (though
the predicates and arguments do appear separately).
Plausibility judgements were elicited from a large
group of human subjects, then normalised and log-
transformed. Table 1 gives a representative illus-
tration of the data. Following the evaluation in O?
Se?aghdha (2010), with which we wish to compare,
we use Pearson r and Spearman ? correlation coef-
ficients as performance measures.
All models were trained on the 90-million word
2For a related argument in the context of topic model evalu-
ation, see Chang et al (2009).
written component of the British National Cor-
pus,3 lemmatised, POS-tagged and parsed with the
RASP toolkit (Briscoe et al, 2006). We removed
predicates occurring with just one argument type
and all tokens containing non-alphabetic characters.
The resulting datasets consist of 3,587,172 verb-
object observations (7,954 predicate types, 80,107
argument types), 3,732,470 noun-noun observations
(68,303 predicate types, 105,425 argument types)
and 3,843,346 adjective-noun observations (29,975
predicate types, 62,595 argument types).
All the Bayesian models were trained by Gibbs
sampling, as outlined above. For each model we run
three sampling chains for 1,000 iterations and aver-
age the plausibility predictions for each to produce a
final prediction P (w|p) for each predicate-argument
item. As the evaluation demands an estimate of the
joint probability P (w, p) we multiply the predicted
P (w|p) by a predicate probability P (p|r) estimated
from relative corpus frequencies. In training we use
a burn-in period of 200 iterations, after which hyper-
parameters are reestimated and P (p|r) predictions
are sampled every 50 iterations. All probability es-
timates are log-transformed to match the gold stan-
dard judgements.
In order to compare against previously proposed
selectional preference approaches based on Word-
Net we also reimplemented the methods that per-
formed best in the evaluation of Brockmann and
Lapata (2003): Resnik (1993) and Clark and Weir
(2002). For Resnik?s model we used WordNet 2.1
rather than WordNet 3.0 as the former has multi-
ple roots, a property that turns out to be necessary
for good performance. Clark and Weir?s method
requires that the user specify a significance thresh-
old ? to be used in deciding where to cut; to give
it the best possible chance we tested with a range
of values (0.05, 0.3, 0.6, 0.9) and report results for
the best-performing setting, which consistently was
? = 0.9. One can also use different statistical hy-
pothesis tests; again we choose the test giving the
best results, which was Pearson?s chi-squared test.
As this method produces a probability estimate con-
ditioned on the predicate p we multiply by a MLE
estimate of P (p|r) and log-transform the result.
3http://www.natcorp.ox.ac.uk/
175
eat food#n#1, aliment#n#1, entity#n#1, solid#n#1, food#n#2
drink fluid#n#1, liquid#n#1, entity#n#1, alcohol#n#1, beverage#n#1
appoint individual#n#1, entity#n#1, chief#n#1, being#n#2, expert#n#1
publish abstract entity#n#1, piece of writing#n#1, communication#n#2, publication#n#1
Table 2: Most probable cuts learned by WN-CUT for the object argument of selected verbs
Verb-object Noun-noun Adjective-noun
Seen Unseen Seen Unseen Seen Unseen
r ? r ? r ? r ? r ? r ?
WN-CUT .593 .582 .514 .571 .550 .584 .564 .590 .561 .618 .453 .439
WN-CUT-100 .500 .529 .575 .630 .619 .639 .662 .706 .537 .510 .464 .431
WN-CUT-200 .538 .546 .557 .608 .595 .632 .639 .669 .585 .587 .435 .431
LDAWN-100 .497 .538 .558 .594 .605 .619 .635 .633 .549 .545 .459 .462
LDAWN-200 .546 .562 .508 .548 .610 .654 .526 .568 .578 .583 .453 .450
Resnik .384 .473 .469 .470 .242 .187 .152 .037 .309 .388 .311 .280
Clark/Weir .489 .546 .312 .365 .441 .521 .543 .576 .440 .476 .271 .242
BNC (MLE) .620 .614 .196 .222 .544 .604 .114 .125 .543 .622 .135 .102
LDA .504 .541 .558 .603 .615 .641 .636 .666 .594 .558 .468 .459
Table 3: Results (Pearson r and Spearman ? correlations) on Keller and Lapata?s (2003) plausibility data; underlining
denotes the best-performing WordNet-based model, boldface denotes the overall best performance
4.2 Results
Table 2 demonstrates the top cuts learned by the
WN-CUT model from the verb-object training data
for a selection of verbs. Table 3 gives quanti-
tative results for the WordNet-based models un-
der consideration, as well as results reported by O?
Se?aghdha (2010) for a purely distributional LDA
model with 100 topics and a Maximum Likelihood
Estimate model learned from the BNC. In general,
the Bayesian WordNet-based models outperform the
models of Resnik and Clark and Weir, and are com-
petitive with the state-of-the-art LDA results. To
test the statistical significance of performance differ-
ences we use the test proposed by Meng et al (1992)
for comparing correlated correlations, i.e., correla-
tion scores with a shared gold standard. The dif-
ferences between Bayesian WordNet models are not
significant (p > 0.05, two-tailed) for any dataset or
evaluation measure. However, all Bayesian mod-
els improve significantly over Resnik?s and Clark
and Weir?s models for multiple conditions. Perhaps
surprisingly, the relatively simple WN-CUT model
scores the greatest number of significant improve-
ments over both Resnik (7 out of 12 conditions)
and Clark and Weir (8 out of 12), though the other
Bayesian models do follow close behind. This may
suggest that the incorporation of WordNet structure
into the model in itself provides much of the cluster-
ing benefit provided by an additional layer of ?topic?
latent variables.4
In order to test the ability of the WordNet-based
models to make predictions about arguments that
are absent from the training vocabulary, we created
an artificial out-of-vocabulary dataset by removing
each of the Keller and Lapata argument words from
the input corpus and retraining. An LDA selectional
preference model will completely fail here, but we
hope that the WordNet models can still make rela-
tively accurate predictions by leveraging the addi-
tional lexical knowledge provided by the hierarchy.
For example, if one knows that a tomatillo is classed
as a vegetable in WordNet, one can predict a rel-
atively high probability that it can be eaten, even
though the word tomatillo does not appear in the
BNC.
As a baseline we use a BNC-trained model that
4An alternative hypothesis is that samplers for the more
complex models take longer to ?mix?. We have run some exper-
iments with 5,000 iterations but did not observe an improvement
in performance.
176
Verb-object Noun-noun Adjective-noun
Seen Unseen Seen Unseen Seen Unseen
r ? r ? r ? r ? r ? r ?
WN-CUT .334 .326 .518 .569 .252 .212 .254 .274 .451 .397 .471 .458
WN-CUT-100 .308 .357 .459 .489 .223 .207 .126 .074 .285 .264 .234 .226
WN-CUT-200 .273 .321 .452 .482 .192 .174 .115 .053 .266 .212 .220 .214
LDAWN-100 .223 .235 .410 .391 .259 .220 .132 .138 .016 .037 .264 .254
LDAWN-200 .291 .285 .392 .379 .240 .163 .118 .131 .041 .078 .209 .212
Resnik .203 .341 .472 .497 .054 -.054 .184 .089 .353 .393 .333 .365
Clark/Weir .222 .287 .201 .235 .225 .162 .279 .304 .313 .202 .190 .148
BNC .206 .224 .276 .240 .256 .240 .223 .225 .088 .103 .220 .231
Table 4: Forced-OOV results (Pearson r and Spearman ? correlations) on Keller and Lapata?s (2003) plausibility data
predicts P (w, p) proportional to the MLE predicate
probability P (p); a distributional LDA model will
make essentially the same prediction. Clark and
Weir?s method does not have full coverage; if no
sense s of an argument appears in the data then
P (s|p) is zero for all senses and the resulting pre-
diction is zero, which cannot be log-transformed.
To sidestep this issue, unseen senses are assigned a
pseudofrequency of 0.1. Results for this ?forced-
OOV? task are presented in Table 4. WN-CUT
proves the most adept at generalising to unseen ar-
guments, attaining the best performance on 7 of 12
dataset/evaluation conditions and a statistically sig-
nificant improvement over the baseline on 6. We ob-
serve that estimating the plausibility of unseen ar-
guments for noun-noun modifiers is particularly dif-
ficult. One obvious explanation is that the training
data for this relation has fewer tokens per predi-
cate, making it more difficult to learn their prefer-
ences. A second, more hypothetical, explanation is
that the ontological structure of WordNet is a rela-
tively poor fit for the preferences of nominal modi-
fiers; it is well-known that almost any pair of nouns
can combine to produce a minimally plausible noun-
noun compound (Downing, 1977) and it may be that
this behaviour is ill-suited by the assumption that
preferences are sparse distributions over regions of
WordNet.
5 Conclusion
In this paper we have presented a range of
Bayesian selectional preference models that incor-
porate knowledge about the structure of a lexical hi-
erarchy. One motivation for this work was to test
the hypothesis that such knowledge can be helpful
in constructing robust models that can handle rare
and unseen arguments. To this end we have re-
ported a plausibility-based evaluation in which our
models outperform previously proposed WordNet-
based preference models and make sensible predic-
tions for out-of-vocabulary items. A second motiva-
tion, which we intend to explore in future work, is
to apply our models in the context of a word sense
disambiguation task. Previous studies have demon-
strated the effectiveness of distributional Bayesian
selectional preference models for predicting lexical
substitutes (O? Se?aghdha and Korhonen, 2011) but
these models lack a principled way to map a word
onto its most likely WordNet sense. The methods
presented in this paper offer a promising solution to
this issue. Another potential research direction is in-
tegration of semantic relation extraction algorithms
with WordNet or other lexical resources, along the
lines of Pennacchiotti and Pantel (2006) and Van
Durme et al (2009).
Acknowledgements
The work in this paper was funded by the EP-
SRC (UK) grant EP/G051070/1, EU grant 7FP-ITC-
248064 and the Royal Society, (UK).
References
Steven Abney and Marc Light. 1999. Hiding a semantic
hierarchy in a Markov model. In Proceedings of the
ACL-99 Workshop on Unsupervised Learning in NLP,
College Park, MD.
177
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preferences
from unlabeled text. In Proceedings of EMNLP-08,
Honolulu, HI.
David M. Blei, Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2003a. Hierarchical topic
models and the nested Chinese Restaurant Process. In
Proceedings of NIPS-03, Vancouver, BC.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003b. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of EMNLP-CoNLL-07, Prague, Czech Re-
public.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions,
Sydney, Australia.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional pref-
erence acquisition. In Proceedings of EACL-03, Bu-
dapest, Hungary.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of NIPS-09, Vancouver, BC.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2), 187?206.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Frank Keller and Mirella Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4):639?654.
Xiao-Li Meng, Robert Rosenthal, and Donald B. Rubin.
1992. Comparing correlated correlation coefficients.
Psychological Bulletin, 111(1):172?175.
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with Pachinko alloca-
tion. In Proceedings of ICML-07, Corvallis, OR.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31(3):705?767.
Diarmuid O? Se?aghdha and Anna Korhonen. 2011. Prob-
abilistic models of similarity in syntactic context. In
Proceedings of EMNLP-11, Edinburgh, UK.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL-10,
Uppsala, Sweden.
Marco Pennacchiotti and Patrick Pantel. 2006. Ontolo-
gizing semantic relations. In Proceedings of COLING-
ACL-06, Sydney, Australia.
Keith Rayner, Tessa Warren, Barbara J. Juhasz, and Si-
mon P. Liversedge. 2004. The effect of plausibil-
ity on eye movements in reading. Journal of Experi-
mental Psychology: Learning Memory and Cognition,
30(6):1290?1301.
Joseph Reisinger and Raymond Mooney. 2011. Cross-
cutting models of lexical semantics. In Proceedings of
EMNLP-11, Edinburgh, UK.
Joseph Reisinger and Marius Pas?ca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of ACL-IJCNLP-09, Suntec, Singapore.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional prefer-
ences. In Proceedings ACL-10, Uppsala, Sweden.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining EM
training and the MDL principle for an automatic verb
classification incorporating selectional preferences. In
Proceedings of ACL-08:HLT, Columbus, OH.
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings of
NAACL-HLT-10, Los Angeles, CA.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL-10, Uppsala, Sweden.
Benjamin Van Durme, Philip Michalak, and Lenhart K.
Schubert. 2009. Deriving generalized knowledge
from corpora using WordNet abstraction. In Proceed-
ings of EACL-09, Athens, Greece.
Hanna Wallach. 2008. Structured Topic Models for Lan-
guage. Ph.D. thesis, University of Cambridge.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11:197?225.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
178
generative models. In Proceedings of EMNLP-11, Ed-
inburgh, UK.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. 2009.
Generalizing over lexical features: Selectional prefer-
ences for semantic role classification. In Proceedings
of ACL-IJCNLP-09, Singapore.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of ACL-11, Portland, OR.
179
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 61?69,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Acquiring Human-like Feature-Based
Conceptual Representations from Corpora
Colin Kelly
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
colin.kelly
@cl.cam.ac.uk
Barry Devereux
Centre for Speech,
Language, and the Brain
University of Cambridge
Cambridge, CB2 3EB, UK
barry@csl.psychol.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
anna.korhonen
@cl.cam.ac.uk
Abstract
The automatic acquisition of feature-based
conceptual representations from text corpora
can be challenging, given the unconstrained
nature of human-generated features. We
examine large-scale extraction of concept-
relation-feature triples and the utility of syn-
tactic, semantic, and encyclopedic informa-
tion in guiding this complex task. Meth-
ods traditionally employed do not investi-
gate the full range of triples occurring in
human-generated norms (e.g. flute produce
sound), rather targeting concept-feature pairs
(e.g. flute ? sound) or triples involving specific
relations (e.g. is-a, part-of ). We introduce
a novel method that extracts candidate triples
(e.g. deer have antlers, flute produce sound)
from parsed data and re-ranks them using se-
mantic information. We apply this technique
to Wikipedia and the British National Corpus
and assess its accuracy in a variety of ways.
Our work demonstrates the utility of external
knowledge in guiding feature extraction, and
suggests a number of avenues for future work.
1 Introduction
In the cognitive sciences, theories about how con-
crete concepts such as ELEPHANT are represented in
the mind have often adopted a distributed, feature-
based model of conceptual knowledge (e.g. Ran-
dall et al (2004), Tyler et al (2000)). According
to such accounts, conceptual representations consist
of patterns of activation over sets of interconnected
semantic feature nodes (e.g. has eyes, has ears,
is large). To test these theories empirically, cogni-
tive psychologists require an accurate estimate of the
kinds of knowledge that people are likely to repre-
sent in such a system. To date, the most important
sources of such knowledge are property-norming
studies, where a large number of participants write
down lists of features for concepts. For example,
McRae et al (2005) collected a set of norms list-
ing features for 541 concrete concepts. In that study,
the features listed by different participants were nor-
malised by mapping different feature descriptions
with identical meanings to the same feature label.1
Table 1 gives the ten most frequent normed features
for two concepts in the norms.
elephant banana
Relation Feature Relation Feature
is large is yellow
has a trunk is a fruit
is an animal is edible
is grey is soft
lives in Africa grows on trees
has ears eaten by peeling
has tusks - grows
has legs eaten by monkeys
has four legs is long
has large ears tastes good
Table 1: Sample triples from McRae Norms
However, property norm data have certain weak-
nesses (these have been widely discussed; e.g. Mur-
phy (2002), McRae et al (2005)). One issue is
that participants tend to under-report features that
are present in many of the concepts in a given cat-
egory (McRae et al, 2005; Murphy, 2002). For ex-
ample, for the concept ELEPHANT, participants list
salient features like has trunk, but not less salient
features such as breathes air, even though presum-
ably all McRae et al?s participants knew that ele-
phants breathe air. Although the largest collection
1For example, for CAR, ?used for transportation? and
?people use it for transportation? were mapped to the same
used for transportation feature.
61
of norms lists features for over 500 concepts, the
relatively small size of property norm sets still gives
cause for concern. Larger sets of norms would be
useful to psycholinguists; however, large-scale prop-
erty norming studies are time-consuming and costly.
In NLP, researchers have developed methods for
extracting and classifying generic relationships from
data, e.g. Pantel and Pennacchiotti (2008), Davidov
and Rappoport (2008a, 2008b). In recent years,
researchers have also begun to develop methods
which can automatically extract feature norm-like
representations from corpora, e.g. Almuhareb and
Poesio (2005), Barbu (2008), Baroni et al (2009).
The automatic approach is capable of gathering
large-scale distributional data, and furthermore it is
cost-effective. Corpora contain natural-language in-
stances of words denoting concepts and their fea-
tures, and therefore serve as ideal material for fea-
ture generation tasks. However, current methods
are restricted to specific relations between concepts
and their features, or target concept-feature pairs
only. For example, Almuhareb and Poesio (2005)
proposed a method based on manually developed
lexico-syntactic patterns that extracts information
about attributes and values of concepts. They used
these syntactic patterns and two grammatical rela-
tions to create descriptions of nouns consisting of
vector entries and evaluated their approach based
on how well their vector descriptions clustered con-
cepts. This method performed well, but targeted
is-a and part-of relations only. Barbu (2008) com-
bined manually defined linguistic patterns with a co-
occurrence based method to extract features involv-
ing six classes of relations. He then split learning
for the property classes into two distinct paradigms.
One used a pattern-based approach (four classes)
with a seeded pattern-learning algorithm. The other
measured strength of association between the con-
cept and referring adjectives and verbs (two classes).
His pattern-based approach worked well for proper-
ties in the superordinate class, had reasonable recall
for stuff and location classes, but zero recall for part
class. His approach for the other two classes used
various association measures which he summed to
establish an overall score for potential properties.
The recent Strudel model (Baroni et al, 2009) re-
lies on more general linguistic patterns, ?connector
patterns?, consisting of sequences of part-of-speech
(POS) tags to look for candidate feature terms near
a target concept. The method assumes that ?the va-
riety of patterns connecting a concept and a poten-
tial property is a good indicator of the presence of
a true semantic link?. Thus, properties are scored
based on the count of distinct patterns connecting
them to a concept. When evaluated against the ESS-
LLI dataset (Baroni et al (2008); see section 3.1),
Strudel yields a precision of 23.9% ? this figure is
the best state-of-the-art result for unconstrained ac-
quisition of concept-feature pairs.
It seems unlikely that further development of the
shallow connector patterns will significantly im-
prove accuracy, as these already broadly cover most
POS sequences that are concept-feature connectors.
Because of the difficult nature of the task, we believe
that extraction of more accurate representations ne-
cessitates additional linguistic and world knowl-
edge. Furthermore, the utility of Strudel is limited
because it only produces concept-feature pairs, and
not concept-relation-feature triples similar to those
in human generated norms (although the distribution
of the connector patterns for a extracted pair does of-
fer clues about the broad class of semantic relation
that holds between concept and feature).
In this paper, we explore issues of both method-
ology and evaluation that arise when attempting
unconstrained, large-scale extraction of concept-
relation-feature triples in corpus data. Extracting
such human-like features is difficult, and we do not
anticipate a high level of accuracy in these early ex-
periments. We examine the utility of three types
of external knowledge in guiding feature extrac-
tion: syntactic, semantic and encyclopedic. We
build three automatically parsed corpora, two from
Wikipedia and one from the British National Cor-
pus. We introduce a method that (i) extracts concept-
relation-feature triples from grammatical depen-
dency paths produced by a parser and (ii) uses prob-
abilistic information about semantic classes of fea-
tures and concepts to re-rank the candidate triples
before filtering them. We then assess the accuracy
of our model using several different methods, and
demonstrate that external knowledge can help guide
the extraction of human-like features. Finally, we
highlight issues in both methodology and evaluation
that are important for further progress in this area of
research.
62
2 Extraction Method
2.1 Corpora
We used Wikipedia to investigate the usefulness of
world knowledge for our task. Almost all con-
cepts in the McRae norms have their own Wikipedia
articles, and the articles often include facts simi-
lar to those elicited in norming studies.2 Extrane-
ous data were removed from the articles (e.g. in-
foboxes, bibliographies) to create a plaintext version
of each article. The 1.84 million articles were then
compiled into two subcorpora. The first of these
(Wiki500) consists of the Wikipedia articles corre-
sponding to each of the McRae concepts. It con-
tains c. 500 articles (1.1 million words). The sec-
ond subcorpus is comprised of those articles where
the title is fewer than five words long and contains
one of the McRae concept words.3 This corpus,
called Wiki110K, holds 109,648 plaintext articles
(36.5 million words).
We also employ the 100-million word British Na-
tional Corpus (BNC) (Leech et al, 1994) which con-
tains written (90%) and spoken (10%) English. It
was designed to represent a broad cross-section of
modern British English. This corpus provides an in-
teresting contrast with Wikipedia, since we assume
that any features contained in such a wide-ranging
corpus would be presented in an incidental fashion
rather than explicitly. The BNC may contain use-
ful features which are encoded in everyday speech
and text but not in Wikipedia, perhaps due to their
ambiguity for encyclopedic purposes, or due to their
non-scientific but rather common-sense nature. For
example, eaten by monkeys is listed as a feature of
BANANA in the McRae norms, but the word monkey
does not appear in the Wikipedia banana article.
2.2 Candidate feature extraction
Using a modified, British English version of the
published norms, we recoded them to a uniform
concept-relation-feature representation suitable for
our experiments ? it is triples of this form that we
aim to extract. Our method for extracting concept-
2e.g. The article Elephant describes how elephants are large,
are mammals, and live in Africa.
3This was done in order to avoid articles on very specific
topics which are unlikely to contain basic information about the
target concept.
relation-feature triples consists of two main stages.
In the first stage, we extract large sets of candidate
concept-relation-feature triples for each target con-
cept from parsed corpus data. In the second stage,
we re-rank and filter these triples with the intention
of retaining only those triples which are likely to be
true semantic features.
In the first stage, the corpora are parsed using the
Robust Accurate Statistical Parsing (RASP) system
(Briscoe et al, 2006). For each sentence in the cor-
pora, this yields the most probable analysis returned
by the parser in the form of a set of grammatical
relations (GRs). The GR sets for each sentence con-
taining the target concept noun are then retrieved
from the corpus. These GRs form an undirected
acyclic graph, whose nodes are labelled with words
in the sentence and their POS, and whose edges are
labelled with the GR types linking the nodes to-
gether. Using this graph we generate all possible
paths which are rooted at our target concept node
using a breadth-first search.
We then examine whether any of these paths
match prototypical feature-relation GR structures
according to our manually-generated rules. The
rules were created by first extracting features from
the McRae norms for a small subset of the concepts
and extracting those sentences from the Wiki500
corpus which contained both concept and feature
terms. For each sentence, we then examined each
path through the graph (containing the GRs and POS
tags) linking the concept, the feature, and all inter-
mediate terms, and (providing no other rule already
generated the concept-relation-feature triple) manu-
ally generated a rule based on each path.
For example, the sentence There are also aprons
that will cover the sleeves should yield the triple
apron cover sleeve. We examine the tree structure
of the sentence rooted at the concept (apron):
apron+s:17_NN2
cmod-that cover:34_VV0
L--- dobj sleeve+s:44_NN2
L--- det the:40_AT
L--- aux will:29_VM
cmod-that cover:34_VV0
xcomp be+:8_VBR
L--- ncmod also:12_RR
L--- ncsubj There:2_EX
Here, the relation is relatively simple ? we merely
63
create a rule which requires that the relation is a verb
(i.e. has a V POS tag), the feature has an NN tag and
that there is a dobj GR linking the feature to the
concept. Our rules are effectively a constraint on (a)
which paths should be followed through the tree, and
(b) which items in that path should be noted in our
concept-relation-feature triple. By creating several
such rules and applying them to a large number of
sentences, we extract potential features and relations
for our concepts.
We avoided specifying too many POS tags and
GRs in rules since this could have resulted in too
few matching paths. In the above example, we could
have required also a cmod-that relation linking the
feature and concept ? but this would have excluded
sentences like the apron covered the sleeves. Con-
versely, we avoided making our rules too permis-
sive. For example, eliminating the dobj requirement
would have yielded the triple apron be steel from the
sentence the apron hooks were steel.
The application of this method to a number of
concepts in the Wiki500 corpus yielded 15 rules
which we employed in our experiments. We extract
triples using both singular and plural occurrences of
both the concept term and the feature term. We show
the first three of our rules in Table 2. The first stage
of our method uses the 15 rules to extract a very
large number of candidate triples from corpus data.
Rule: relation of concept has a VVN tag, feature
has a NN tag and they are linked by an xcomp
GR
S: This is an anchor which relies solely on be-
ing a heavy weight.
T: anchor be weight
Rule: relation of concept is a verb, feature is an ad-
jective and they are linked by an xcomp GR
S: Sliced apples turn brown with exposure to
air due to the conversion of natural pheno-
lic substances into melanin upon exposure to
oxygen.
T: apple turn brown
Rule: feature of concept has a VV0 tag, relation is
a verb and they are linked by an aux GR
S: Grassy bottoms may be good holding, but
only if the anchor can penetrate the foliage.
T: anchor can penetrate
Table 2: Three sample rules for a given concept, with
example sentence (S) and corresponding triple (T).
2.3 Re-ranking based on semantic information
The second stage of our method evaluates the quality
of the extracted candidates using semantic informa-
tion, with the aim of filtering out the poor quality
features generated in the first stage. We would ex-
pect the number of times a triple is extracted for a
given concept to be proportional to the likelihood
that the triple represents a true feature of that con-
cept. However, production frequency alone is not a
sufficient indicator of quality, because concept terms
can produce unexpected candidate feature terms.4
One may attempt to address this issue by intro-
ducing semantic categories. In other words, the
probability of a feature being part of a concept?s
representation is dependent on the semantic cate-
gory to which the concept belongs (for example,
used for-cutting would be expected to have low
probability for animal concepts). We analysed the
norms to quantify this type of semantic information
with the aim of identifying higher-order structure in
the distribution of semantic classes for features and
concepts. The overarching goal was to determine
whether this information can indeed improve the ac-
curacy of feature extraction.
In formal terms, we assume that there is a 2-
dimensional probability distribution over concept
and feature classes, P(C,F), where C is a concept
class (e.g. Apparel) and F is a feature class (e.g.
Materials). Knowing this distribution provides us
with a means of assessing how likely it is that a can-
didate feature f is true for a concept c, assuming that
we know that c ? C and f ? F . The McRae norms
may be considered to be a sample drawn from this
distribution, if the concept and feature terms appear-
ing in the norms can be assigned to suitable concept
and feature classes. These classes were identified
by way of clustering. The reranking step employed
the McRae norms so we could establish an upper
bound for the semantic analysis, although we could
also use other knowledge resources, e.g. the Open
Mind Common Sense database (Singh et al, 2002).
2.3.1 Clustering
We utilised Lin?s similarity measure (1998) for
our similarity metric, employing WordNet (Fell-
4For example, one of the extracted triples for TIGER is tiger
have squadron because of the RAF squadron called the Tigers.
64
k-means
banjo biscuit blackbird
bat cup ox
beehive kettle peacock
birch sailboat prawn
bookcase shoe prune
NMF
ashtray bouquet eel
bayonet cabinet grapefruit
cape card guppy
cat cellar moose
catfish chandelier otter
Hierarchical
Fruit/Veg Apparel Instruments
apple apron accordion
avocado armour bagpipes
banana belt banjo
beehive blouse cello
blueberry boot clarinet
Table 3: First five elements alphabetically from three
sample clusters for the three clustering methods.
baum, 1998) as the basis for calculating similarity.
This metric is suitable for our task as we would
like to generate appropriate superordinate classes for
which we can calculate distributional statistics. We
could merely cluster on the most frequent sense of
concept and feature words in WordNet, but the most
frequent sense in WordNet may not correspond to
the intended sense in our feature norm data.5 So we
consider also other senses of words in WordNet by
employing a manually-annotated list to choose the
correct sense in WordNet. This is only possible for
concept clustering since we don?t possess a manual
WordNet sense annotation for the 7000 McRae fea-
tures; for the feature clustering, we simply use the
most frequent sense in WordNet.
The concepts and feature-head terms appearing
in the recoded norms were each clustered indepen-
dently into 50 clusters using three methods: hi-
erarchical clustering, k-means clustering and non-
negative matrix factorization (NMF). We show the
first five alphabetical elements from three of the
clusters produced by our clustering methods in Table
3. The hierarchical clustering seems to be producing
5e.g. the first and second most frequent definitions of kite
refer to a slang meaning for the word cheque ? only the third
most frequent meaning refers to kite as a toy, which most people
would understand to be its predominant sense.
Hierarchical Clustering
Plant Parts Materials Activities
berry cotton annoying
bush fibre listening
core nylon music
plant silk showing
seed spandex looking
Table 4: Example members of feature clusters for hierar-
chical clustering.
Fruit/Veg Apparel Instruments
Plant Parts 0.144 0.037 0.008
Materials 0.006 0.148 0.008
Activities 0.009 0.074 0.161
Table 5: P(F |C) for C ? {Fruit/Veg, Apparel, Instru-
ments} and F ? {Plant Parts, Materials, Activities}
the most intuitive clusters.
We calculated the conditional probability P(F |C)
of a feature cluster given a concept cluster using the
data in the McRae norms. Table 5 gives the condi-
tional probability for each of the three feature clus-
ters given each of the three concept clusters that
were presented in Tables 3 and 4 for hierarchical
clustering. For example, P(Materials|Apparel) is
higher than P(Materials|Fruit/Veg): given a concept
in the Apparel cluster the probability of a Materials
feature is relatively high whereas given a concept in
the Fruit/Veg cluster the probability of a Materials
feature is low. The cluster analysis therefore sup-
ports our hypothesis that the likelihood of a partic-
ular feature for a particular concept is dependent on
the semantic categories that both belong to.
2.3.2 Reranking
We investigated whether this distributional semantic
information could be used to improve the quality of
the candidate triples, by using the conditional prob-
abilities of the appropriate feature cluster given the
concept cluster as a weighting factor. To obtain the
probabilities for a triple, we first find the clusters that
the concept and feature-head words belong to. If the
feature-head word of the extracted triple appears in
the norms, its cluster membership is drawn directly
from there; if not, we assign the feature-head to the
feature cluster with which it has the highest average
similarity.6 Having determined the concept and fea-
6We use average-linkage for hiearchical and k-means clus-
tering, and mean cosine similarity for NMF.
65
ture clusters for the triple, we reweight its raw cor-
pus occurrence frequency by multiplying it by the
conditional probability. In this way, incorrect triples
that occur frequently in the data are downgraded and
more plausible triples have their ranking boosted.
2.3.3 Baseline model
We also implemented as a baseline a co-occurrence-
based model, based on the ?SVD? model de-
scribed by Baroni and colleagues (Baroni and Lenci,
2008; Baroni et al, 2009) ? it is a simple, word-
association method, not tailored to extracting fea-
tures. A context-word-by-target-word frequency co-
occurrence matrix was constructed for both corpora,
with a sentence-sized window. Context words and
target words were defined to be the 5,000 and 10,000
most frequent content words in the corpus respec-
tively. The target words were supplemented with
the concept words from the recoded norms. The
co-occurrence matrix was reduced to 150 dimen-
sions by singular value decomposition, and cosine
similarity between pairs of target words was calcu-
lated. The 200 most similar target words to each
concept acted as the feature-head terms extracted by
this model.
3 Experimental Evaluation
3.1 Methods of Evaluation
We considered a number of methods for evaluating
the quality of the extracted feature triples. One pos-
sibility would be to calculate precision and recall
for the extracted triples with respect to the McRae
norms ?gold standard?. However, direct comparison
with the recoded norms is problematic, since there
may be extracted features which are semantically
equivalent to a triple in the norms but possessing a
different lexical form.7
Since semantically identical features can be lex-
ically different, we followed the approach taken in
the ESSLLI 2008 Workshop on semantic models
(Baroni et al, 2008). The gold standard for the ESS-
LLI task was the top 10 features for 44 of the McRae
concepts. For each concept-feature pair an expan-
sion set was generated containing synonyms of the
7For example, avocado have stone appears in the recoded
norms whilst avocado contain pit is extracted by our method;
direct comparison of these two triples results in avocado con-
tain pit being incorrectly marked as an error.
feature terms appearing in the norms. For example,
the feature lives on water was expanded to the set
{aquatic, lake, ocean, river, sea, water}.
We would expect to find in corpus data correct
features that do not appear in our ?gold standard?
(e.g. breathes air is listed for WHALE but for no
other animal). We therefore aim to attain high re-
call when evaluating against the ESSLLI set (since
ideally all features in the norms should be extracted)
but we are somewhat less concerned about achieving
high precision (since extracted features that are not
in the norms may still be correct, e.g. breathes air
for TIGER). To evaluate the ability of our model
to generate such novel features, we also conducted
a manual evaluation of the highest-ranked extracted
features that did not appear in the norms.
Extraction set Corpus Prec. Recall
SVD Baseline
Wiki500 0.0235 0.4712
Wiki110K 0.0140 0.2798
BNC 0.0131 0.2621
Method -
unfiltered
Wiki500 0.0242 0.6515
Wiki110K 0.0039 0.8944
BNC 0.0042 0.8813
Method - top 20
(unweighted)
Wiki500 0.1159 0.2326
Wiki110K 0.0761 0.1523
BNC 0.0841 0.1692
Method - top 20
(hierarchical
clustering)
Wiki500 0.1693 0.3394
Wiki110K 0.1733 0.3553
BNC 0.1943 0.3896
Method - top 20
(k-means
clustering)
Wiki500 0.1159 0.2323
Wiki110K 0.1000 0.2008
BNC 0.1216 0.2442
Method - top 20
(NMF
clustering)
Wiki500 0.1375 0.2755
Wiki110K 0.1409 0.2826
BNC 0.1500 0.3010
Table 6: Results when matching on features only.
3.2 Evaluation
Previous large-scale models of feature extraction
have been evaluated on pairs rather than triples e.g.
Baroni et al (2009). Table 6 presents the results
of our method when we evaluate using the feature-
head term alone (i.e. in calculating precision and re-
call we disregard the relation verb and require only
a match between the feature-head terms in the ex-
tracted triples and the recoded norms). Results for
six sets of extractions are presented. The first set
is the set of features extracted by the SVD baseline.
66
The second set of extracted triples consists of the
full set of triples extracted by our method, prior to
the reweighting stage. ?Top 20 unweighted? gives
the results when all but the top 20 most frequently
extracted triples for each concept are filtered out.
Note that the filtering criteria here is raw extraction
frequency, without reweighting by conditional prob-
abilities. ?Top 20 (clustering type)? are the corre-
sponding results when the features are weighted by
the conditional probability factors (derived from our
three clustering methods) prior to filtering; that is,
using the top 20 reranked features. The effective-
ness of using the semantic class-based analysis data
in our method can be assessed by comparing the fil-
tered results with and without feature weighting.
For the baseline implementation, the results are
better when we use the smaller Wiki500 corpus
compared to the larger Wiki110K corpus. This is
not surprising, since the smaller corpus contains
only those articles which correspond to the concepts
found in the norms. This smaller corpus thus min-
imises noise due to phenomena such as word poly-
semy which are more apparent in the larger corpus.
The results for the baseline model and the unfil-
tered method are quite similar for the Wiki500 cor-
pus, whilst the results for the unfiltered method us-
ing the Wiki110K corpus give the maximum recall
achieved by our method; 89.4% of the features are
extracted, although this figure is closely followed by
that of the BNC at 88.1%. As the unfiltered method
is deliberately greedy, a large number of features are
being extracted and therefore precision is low.
Extraction set Corpus Prec. Recall
Method - top 20
(hierarchical
clustering)
Wiki500 0.1011 0.2028
Wiki110K 0.1102 0.2210
BNC 0.0955 0.1917
Table 7: Results for our best method when matching on
features and relations.
For the results of the filtered method, where all
but the top 20 of features were discarded, we see the
benefit of reranking, with the reranked frequencies
for all three clustering types yielding much higher
precision and recall scores than the unweighted
method. Our best performance is achieved using the
BNC and hierarchical clustering, where we obtain
19.4% precision and 38.9% recall. Thus both gen-
eral and encyclopedic corpus data prove useful for
the task. An interesting question is whether these
two data types offer different, complementary fea-
ture types for the task. We discuss this point further
in section 3.3.
Using exactly the same gold standard, Baroni et
al. (2009) obtained precision of 23.9%. However,
this result is not directly comparable with ours, since
we define precision over the whole set of extracted
features while Baroni et al considered the top 10
extracted features only.
The innovation of our method is that it uses infor-
mation about the GR-graph of the sentence to also
extract the relation which appears in the path link-
ing the concept and feature terms in the sentence,
which is not possible in a purely co-occurrence-
based model. We therefore also evaluated the ex-
tracted triples using the full relation + feature-head
pair (i.e. both the feature and the relation verb have
to be correct). The results for our best method are
shown in Table 7. Unsurprisingly, because this task
is more difficult, precision and recall are reduced.
However, since we enforce no constraints on what
the relation may be and since we do not have ex-
panded synonym sets for our relations (as we do for
our features) it is actually impressive to have both
the exact relation verb and feature matching with the
recoded norms almost one in every five times. To our
knowledge, our work is the first to try to compare ex-
tracted features to the full relation and feature norm
parts of the triple.
3.3 Qualitative analysis
Since a key aim of our work is to learn novel features
in corpus data, we also performed a qualitative eval-
uation of the extracted features and relations. This
analysis revealed that many of the errors were not
true errors but potentially valid triples missing from
the gold standard. Table 8 shows the top 10 features
for two concepts extracted by our best method from
the Wiki500 corpus and the BNC corpus. We la-
bel those features that are correct according to the
norms as Correct (C), those which do not appear in
our norms but we believe to be plausible as Plausi-
ble (P), and those that do not appear in the norms
and are also implausible as Incorrect (I). We can see
that our method has detected several plausible fea-
tures not appearing in the norms (and thus our gold
standard), e.g. swan have chick and screwdriver be
67
swan
Wiki500 BNC
be bird C have number I
be black P have water C
have chick P have lake C
have plumage C be bird C
have feather C be white C
restrict water C have neck C
be mute P be wild P
eat grass P have duck I
turn elisa I have song I
have neck C have pair I
screwdriver
Wiki500 BNC
use handle C have tool C
have blade P have end P
use tool C have blade P
remedy problem P have hand I
have size P be sharp P
have head C have bit P
rotate end P have arm I
have plastic P be large P
achieve goal I be sonic P
have hand I have range P
Table 8: Top 10 returned features and relations for swan
and screwdriver.
sharp. Indeed, it could be argued that some ?incor-
rect? features (e.g. screwdriver achieve goal) could
be considered to be at least broadly accurate. We
recognise that the ideal evaluation for our method
would involve having human participants assess the
extracted features for a diverse cross-section of our
concepts, but this is beyond the scope of this paper.
When considering the top 20 features extracted
using our best method applied to the Wiki500 cor-
pus versus the BNC corpus, the overlap of features
is relatively low at 22.73%. When one also takes the
extracted relations into account, this figure descends
to 6.45%. It is clear that relatively distinct groups of
features are being extracted from the encyclopedic
and general corpus data. Future work could investi-
gate combining these for improved performance e.g.
using the intersection of the best features from the
BNC and Wiki110k corpora to improve precision
and the union to improve recall.
4 Discussion
This paper examined large-scale, unconstrained ac-
quisition of human-like feature norms from corpus
data. Our work was not limited to only a subset
of concepts, relation types or concept-feature pairs.
Rather, we investigated concepts, features and rela-
tions in conjunction, and extracted property norm-
like concept-relation-feature triples.
Our investigation shows that external knowledge
is highly useful in guiding this challenging task. En-
cyclopedic information proved useful for feature ex-
traction: although our Wikipedia corpora are consid-
erably smaller than the BNC, they performed almost
equally well. We also demonstrated the benefits of
employing syntactic information in feature extrac-
tion: our base extraction method operating on parsed
data outperforms the co-occurrence-based baseline
and permits us to extract relation verbs. This un-
derscores the usefulness of parsing for semantically
meaningful feature extraction. This is consistent
with recent work in the field of computational lex-
ical semantics, although GR data has not previously
been successfully applied to feature extraction.
We showed that semantic information about co-
occurring concept and feature clusters can be used
to enhance feature acquisition. We employed the
McRae norms for our analysis, however we could
also employ other knowledge resources and cluster
relation verbs using recent methods, e.g. Sun and
Korhonen (2009), Vlachos et al (2009).
Our paper has also investigated methods of eval-
uation, which is a critical but difficult issue for fea-
ture extraction. Most recent approaches have been
evaluated against the ESSLLI sub-set of the McRae
norms which expands the set of features in the norms
with their synonyms. Yet even expansion sets like
the ESSLLI norms do not facilitate adequate eval-
uation because they are not complete in the sense
that there are true features which are not included
in the norms. Our qualitative analysis shows that
many of the errors against the recoded norms are
in fact correct or plausible features. Future work
can aim for larger-scale qualitative evaluation using
multiple judges as well as investigating other task-
based evaluations. For example, we have demon-
strated that our automatically-acquired feature rep-
resentations can make predictions about fMRI activ-
ity associated with concept stimuli that are as pow-
erful as those produced by a manually-selected set
of features (Devereux et al, 2010).
68
Acknowledgments
This research was supported by EPSRC grant
EP/F030061/1 and the Royal Society University
Research Fellowship, UK. We are grateful to McRae and
colleagues for making their norms publicly available,
and to the anonymous reviewers for their input.
References
Abdulrahman Almuhareb and Massimo Poesio. 2005.
Concept learning and categorization from the web. In
Proceedings of the 27th Annual Meeting of the Cogni-
tive Science Society, pages 103?108.
Eduard Barbu. 2008. Combining methods to learn
feature-norm-like concept descriptions. In Proceed-
ings of the ESSLLI Workshop on Distributional Lexical
Semantics, pages 9?16.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of Lin-
guistics, 20(1):55?88.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. ESSLLI 2008 Workshop on Distributional
Lexical Semantics.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2009. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, pages 1?33.
Edward J. Briscoe, John Carroll, and Rebecca Wat-
son. 2006. The second release of the RASP sys-
tem. In Proceedings of the Interactive Demo Session
of COLING/ACL-06, pages 77?80.
D. Davidov and A. Rappoport. 2008a. Classification of
semantic relationships between nominals using pattern
clusters. ACL.08.
D. Davidov and A. Rappoport. 2008b. Unsupervised
discovery of generic relationships using pattern clus-
ters and its evaluation by automatically generated SAT
analogy questions. ACL.08.
Barry Devereux, Colin Kelly, and Anna Korhonen. 2010.
Using fmri activation to conceptual stimuli to evalu-
ate methods for extracting conceptual representations
from corpora. In Proceedings of the NAACL-HLT
Workshop on Computational Neurolinguistics.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
G. Leech, R. Garside, and M. Bryant. 1994. CLAWS4:
the tagging of the British National Corpus. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 1, pages 622?628.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML?98, pages 296?
304.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37:547?559.
Gregory Murphy. 2002. The big book of concepts. The
MIT Press, Cambridge, MA.
Patrick Pantel and Marco Pennacchiotti. 2008. Automat-
ically harvesting and ontologizing semantic relations.
In Paul Buitelaar and Philipp Cimiano, editors, Ontol-
ogy learning and population. IOS press.
Billi Randall, Helen E. Moss, Jennifer M. Rodd, Mike
Greer, and Lorraine K. Tyler. 2004. Distinctive-
ness and correlation in conceptual structure: Behav-
ioral and computational studies. Journal of Experi-
mental Psychology: Learning, Memory & Cognition,
30(2):393?406.
P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins,
and W. Li Zhu. 2002. Open Mind Common
Sense: Knowledge acquisition from the general pub-
lic. On the Move to Meaningful Internet Systems 2002:
CoopIS, DOA, and ODBASE, pages 1223?1237.
Lin Sun and Anna Korhonen. 2009. Improving Verb
Clustering with Automatically Acquired Selectional
Preferences. Empirical Methods on Natural Language
Processing.
L. K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained dirichlet
process mixture models for verb clustering. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics, pages 74?82, Athens,
Greece.
69
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 70?78,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using fMRI activation to conceptual stimuli to evaluate methods for
extracting conceptual representations from corpora
Barry Devereux
Centre for Speech, Language and the Brain
Department of Experimental Psychology
University of Cambridge
barry@csl.psychol.cam.ac.uk
Colin Kelly & Anna Korhonen
Computer Laboratory
University of Cambridge
{ck329,alk23}@cam.ac.uk
Abstract
We present a series of methods for deriv-
ing conceptual representations from corpora
and investigate the usefulness of the fMRI
data and machine learning methodology of
Mitchell et al (2008) as a basis for evaluat-
ing the different models. Within this frame-
work, the quality of a semantic model is quan-
tified by its ability to predict the fMRI ac-
tivation associated with conceptual stimuli.
Mitchell et al used a manually-acquired set of
verbs as the basis for their semantic model; in
this paper, we also consider automatically ac-
quired feature-norm-like semantic representa-
tions. These models make different assump-
tions about the kinds of information avail-
able in corpora that is relevant to represent-
ing conceptual knowledge. Our results in-
dicate that automatically-acquired representa-
tions can make equally powerful predictions
about the brain activity associated with the
stimuli.
1 Introduction
Mitchell et al (2008) presented a novel approach for
predicting human brain activity associated with con-
ceptual stimuli. This approach represents a useful
development for interdisciplinary researchers inter-
ested in lexical semantics, for several reasons. Most
broadly, it is useful in testing the hypothesis that
distributional properties of words in corpora can re-
veal important information about the meanings of
words. A strong version of this hypothesis (i.e. that
children in part learn the meaning of concrete con-
cept words from co-occurring words in discourse
that they are exposed to) has formed the basis of
one class of probabilistic cognitive models of con-
ceptual representation (Andrews et al, 2005; An-
drews et al, 2009; Steyvers, 2010). Furthermore
this approach is useful for testing hypotheses about
the kind of co-occurring information that is useful
for representing conceptual semantics. In Mitchell
et al?s work (2008), for example, they adopt the po-
sition that the meaning of concrete concepts is en-
coded in the brain with information associated with
basic sensory and motor activities (such as actions
involving changes to spatial relationships and phys-
ical actions performed on objects).
At a more technical level, Mitchell et al?s fMRI
activation data1 give researchers developing feature-
based models of conceptual representation an im-
portant benchmark for evaluation. For these re-
searchers, a key problem is the lack of a reason-
able ?gold standard? against which the quality of the
representations generated by a computational model
may be evaluated. Previous research has adopted
two main approaches to evaluation. Firstly, some
models ? especially those aiming to extract repre-
sentations composed of psychologically meaningful
semantic feature units, such as Baroni et al (2009)
? have been evaluated against features gathered in
large scale property norming studies (e.g. McRae
et al (2005)).2 By comparing the system output
against features elicited by people, this kind of eval-
1fMRI data measures changes in oxygen concentrations in
the brain. These changes are tied to cognitive processes.
2In property norming studies, a group of human subjects are
asked to cite features which come to mind for a given concept.
These features are compiled by frequency (with a minimum fre-
quency cut-off) to generate a list of features for each concept.
70
uation aims to test the psychological validity of com-
putational methods. Furthermore, it allows a fine-
grained analysis of performance, for example by re-
vealing the classes of features (part-of, taxonomic,
etc) which a given model is particularly good at ex-
tracting (Baroni et al, 2008).
However, property norms come with important
caveats. One problem is that they tend to over-
represent informative or salient information about
concepts whilst under-representing other kinds of
features. For example, participants report that
camels have humps, but not that camels have hearts,
even though all participants are likely to have both
pieces of information accessible in their representa-
tion of the concept CAMEL. If a model is successful
in extracting these less salient features, there is no
way of evaluating their correctness using property
norms. A related issue is that participants can only
report verbalizable features, which may not repre-
sent the total sum of their conceptual knowledge
(Murphy, 2002; McRae et al, 2005).
A second problem with using property norms as
the basis of evaluation is that there is often no direct
lexical match between feature terms appearing in the
system output and the norms. Feature norms are typ-
ically normalized such that near-synonymous prop-
erties (e.g. is endangered, is an endangered species,
is almost extinct, etc., for WHALE) given by differ-
ent participants are mapped to the same feature la-
bel (e.g. is endangered). As a consequence, a model
may correctly extract endangered for WHALE, but
other lexical forms of the same feature will not
match any feature in the norms. One solution to this
is to create an expansion set for each feature which
includes its synonyms (Baroni et al, 2008). How-
ever, this is only a partial solution because lexical
variation in features is not limited to synonyms.
A second approach to evaluating semantic mod-
els uses classification or similarity data. For exam-
ple, Andrews et al (2009) evaluated their models by
calculating cosine similarity scores between seman-
tic representations and using these similarity scores
to predict behavioral data which are contingent on
the semantic similarity between pairs of concepts
(e.g. lexical substitution errors, semantic priming
latencies, word-association norms, etc). Although
this approach is psychologically motivated, it evalu-
ates a set of extracted features more indirectly than
comparison with norm data. In computational lin-
guistics, a similarly indirect evaluation method is to
cluster the extracted representations. This approach
avoids the difficulties in evaluating individual fea-
tures; however it only allows consideration along
one dimension of the data, namely the similarity be-
tween pairs of concepts.
fMRI data such as the Mitchell et al (2008)
dataset offers an advancement over both of these
evaluation techniques. Unlike, for example, prop-
erty norming data, fMRI data offers direct insight
into how the brain is functioning in response to given
stimuli. Its multidimensional nature makes it eas-
ier to inspect what aspects of meaning a particular
model is performing strongly or weakly on, and al-
lows for better control of experimental variation. Fi-
nally, it avoids the two major issues associated with
property norms, which we outlined above.
This paper is structured as follows. In the next
section, we briefly describe the models which we
used to extract conceptual representations for the 60
concepts in the Mitchell et al (2008) dataset. In
Section 3, we outline our experimental objectives,
and the framework we adopt for testing our seman-
tic models. In Section 4, we present the results of
our evaluation, which indicate above chance perfor-
mance for each of the models. Finally, we exam-
ine the differences between models by investigating
for which concepts prediction of the fMRI activity is
poorest, and discuss these differences with respect to
the differing assumptions made by the methods.
2 Semantic models
We consider four different semantic models in this
paper, which are described briefly below. These
models were selected as we were interested in the
various kinds of knowledge (part-of-speech, syntac-
tic, and semantic) in corpora available to the extrac-
tion process, and the extent to which the use of these
types of knowledge can affect the quality of the ex-
tracted conceptual representations.
2.1 Mitchell verb-based semantic model
The first semantic model we considered was that
of Mitchell et al (2008). This model assumes that
sensory-motor information is an important aspect of
conceptual representation, and that the information
71
relevant to a target concept?s representation can be
estimated from the concept word?s frequency of co-
occurrence with 25 sensory-motor verbs (eat, ma-
nipulate, push, etc) in a very large corpus. Our reim-
plementation of this method used the co-occurrence
statistics provided by Mitchell et al3 which were
extracted from the Google n-gram corpus consisting
of 1 trillion words of web text.
2.2 SVD model
Secondly, we implemented a co-occurrence-based
Singular Value Decomposition (SVD) model based
on the one described by Baroni and colleagues (Ba-
roni and Lenci, 2008; Baroni et al, 2009). This
model combines aspects of both the HAL (Landauer
et al, 1998) and LSA (Lund and Burgess, 1996)
models in constructing representations for words
based on their co-occurrences in texts. A word-
by-word co-occurrence matrix was constructed for
our corpus, storing how often each target word co-
occurred with each context word. The set of context
words consisted of the 5,000 most frequent content
words (i.e. words not occurring in a stop-list of func-
tion words) appearing in the corpus. The set of target
words consisted of the 60 concept terms appearing
in the fMRI dataset, supplemented with the 10,000
most frequent content words in the corpus (with the
exception of the top 10 most frequent words). For
calculating co-occurrence frequency between target
and context words, the context window was defined
by sentence boundaries: two words were considered
to co-occur if they appeared in the same sentence4.
Following Baroni and Lenci (2008), the dimen-
sionality of the target-word ? context-word co-
occurrence matrix was reduced to 150 columns by
singular value decomposition. That is, the singu-
lar value decomposition of the co-occurrence matrix
was computed and the 150 left singular vectors that
accounted for most of the variance, multiplied by the
corresponding singular values, were used as the 150-
dimensional representation of each target term. Sim-
3http://www.cs.cmu.edu/?tom/science2008/
semanticFeatureVectors.html
4In Baroni et al?s implementation a context window of 5
(Baroni and Lenci, 2008) or 20 (Baroni et al, 2009) words
either side of the target word was used instead; we chose a
sentence-based context window as it is analogous to the context
used in our experimental method (described in the following
section).
ilarity between pairs of target words was calculated
as the cosine between their vectors, and for each of
the 60 concept words in the experimental stimuli we
chose the 200 most similar target words to act as the
feature terms extracted by the model. The corpus
used with this model was the British National Cor-
pus (BNC) (Leech et al, 1994).
2.3 Novel extraction method
Finally we implemented a novel extraction method,
which aims to extract property-norm-like, psy-
chologically meaningful features from corpus data
(Kelly et al, 2010). The method aims to extract se-
mantically unconstrained feature triples of the form
concept-relation-feature , where feature is a feature
(either noun or adjective) of the target concept and
relation is a verb representing the semantic relation-
ship between them. Examples of extracted triples
include: swan be white, swan have neck and screw-
driver be tool. The model uses a corpus parsed for
grammatical relations (GRs) using Robust Accurate
Statistical Parsing (RASP) (Briscoe et al, 2006).
For each sentence containing a target concept, the
set of GRs for that sentence are examined to test
whether they match manually-created rules. These
rules include prototypical feature-relation GR struc-
tures connecting elements of the sentence and rep-
resent dependency patterns which encode potential
semantic relationships between the concept and can-
didate feature terms occurring in the sentence. A
large set of candidate triples are extracted by ap-
plying these rules to each sentence in the corpus
containing a target concept, and the triples for each
concept are ranked by their frequency of extraction.
In the second stage of the method, the extracted
triples are reweighted on the basis of probabilistic
high-level semantic information obtained from hu-
man property norm data. This subsequent stage has
the effect of increasing the weight associated with
more high-quality features and downgrading lower-
quality features. The extraction method is described
more fully in Kelly et al (2010). For this method
we also used the BNC. The top 200 triples ranked
by frequency (i.e. unweighted) and the top 200 fea-
tures after reweighting with the semantic data were
used in our experiments.
72
3 Experiment
As mentioned above, we are primarily interested in
using the fMRI data to evaluate the quality of the
different methods for extracting conceptual repre-
sentations from corpora (rather than being interested
in investigating methods for predicting fMRI activa-
tion). We make no attempt to build on the method
described by Mitchell et al (2008), although there
are likely to be many interesting avenues through
which that method could be extended.5 We therefore
followed the Mitchell et al methodology as closely
as possible, using the same multiple regression train-
ing and leave-two-out cross-validation paradigms as
presented in their paper and supporting online ma-
terial. The only parameter that we varied was the
extraction method (and corpus) that was used to gen-
erate the feature-vectors associated with the 60 con-
cepts that were used during the training phase. The
quality of the predictions generated for the concepts
using each semantic model can therefore be adopted
as an index of model performance.
The Mitchell et al method uses co-occurrence
with a specific set of 25 manually selected verbs
(eat, push, etc) that are the same for each concept.
This results in 25-dimensional feature vectors for in-
put into training. However, for both the SVD model
and our triple extraction models there are no a pri-
ori constraints on the number of unique features that
can be extracted for the concepts. For these mod-
els, we selected the top 200 features associated with
each concept; therefore, across all 60 concepts in the
Mitchell et al dataset, there are thousands of unique
features extracted which are used in the concepts?
representations. To ensure that the linear regres-
sion model for each method would be fitted using
the same number of free parameters during training
(thereby maximizing the comparability of the dif-
ferent methods), we reduced the dimensionality of
the generated feature spaces for the SVD method
and the two triple-extraction methods using Prin-
cipal Components Analysis (PCA). The concept ?
feature extraction frequency matrices for the three
models were submitted to PCA, and the first 25 com-
ponents (i.e. those components which best charac-
5For example, the method currently makes the simplifying
assumption that the activity in neighbouring voxels is indepen-
dent.
Triples (weighted) SVD
PCA1 PCA2 PCA1 PCA2
Highest-valued concepts
horse house coat butterfly
cat apartment skirt cow
cow dog shirt ant
dog igloo pants bee
beetle car dress lettuce
Lowest-valued concepts
knife pants car desk
door coat watch arm
hammer dress horse chair
saw skirt dog knife
chisel shirt fly leg
Table 1: Highest- and lowest-valued concepts for the
first two components for the SVD and weighted triple-
extraction methods.
terized the variance of the original features) for each
model were selected. In the case of the SVD model,
these 25 dimensions explained 77.7% of the vari-
ance in the original 3,061-dimensional vectors. For
our unweighted extraction method, the 25 extracted
components explained 63.0% of the original 5,525
dimensions; for the weighted method the compo-
nents explained 71.5% of the original 6,567 dimen-
sions.
It is interesting to consider the kind of seman-
tic information that is being captured by the resul-
tant PCA components. In particular, the compo-
nents appear to capture meaningful distinctions be-
tween stimuli. For example, the first PCA com-
ponent for our weighted triple extraction method
can be interpreted as the concepts? degree of ?an-
imalness? (animal stimuli have high values on this
component). Table 1 presents the five highest and
lowest-valued concepts for the first two components
for the SVD model and the weighted triple extrac-
tion model. Concepts which overlap with respect
to a specific set of semantic properties tend to have
high or low values on a given dimension, indicating
that that component is capturing a specific cluster of
co-occurring semantic features. For example, PCA1
for SVD can be interpreted as ?has features associ-
ated with clothing?.
Therefore, a key difference between the Michell
73
Method Feature Type POS Syntax Semantics
Mitchell 25 verbs no no no
SVD tuples (content-words) yes no no
triple-extraction method (unweighted) feature-triples yes yes no
triple-extraction method (weighted) feature-triples yes yes yes
Table 2: Comparison of the information available to each model.
et al model and our models is that while Mitchell
et al posit that certain sensory-motor function verbs
can act as important features of concepts, our models
instead place more importance on intrinsic semantic
features.
Finally, Table 2 gives a summary comparison of
the different models, in terms of whether or not each
uses part of speech (POS) data, syntactic informa-
tion (i.e. GRs), and semantic filtering (Section 2.3).
It should be noted that the BNC corpus (used with
the SVD model and our triple-extraction method) is
10,000 times smaller than the corpus from which
the Mitchell et al feature vectors are derived. As
such the semantic representations we extract with
our method need to make better use of the data avail-
able in the corpus if they are to compete with the
verb-based features used by Mitchell et al?s method.
4 Results
The accuracy for each of the four methods was eval-
uated using a leave-two-out validation paradigm.
There are 1,770 possible pairs of concepts that can
be drawn from the set of 60 concept stimuli. Train-
ing was performed separately for each participant
and for each of the 1,770 held-out pairs. Given
a particular participant and held-out pair, for each
voxel v we fit the activation at that voxel to the set
of 58 training items with multiple linear regression,
using as predictor variables the elements of the 25-
dimensional feature vectors associated with each of
the 58 concepts. Training therefore yields a set of
25 ?-coefficients, which can be used to generate a
prediction for the activation yv of voxel v for the
held-out word w using the equation
ypredv =
25?
i=1
?v,ifi,w (1)
where fi,w is the ith element of the feature vector for
word w (see Mitchell et al (2008) for details). Over
all voxels, this method gives a prediction for the ac-
tivation with respect to the held-out word w which
can then be compared to the observed activation for
that stimulus.
Rather than comparing the activity between pre-
dicted and observed images using all voxels, we
compared images using only the 500 most stable
voxels for each participant. For each participant, the
500 most stable voxels were the voxels which gave
the most consistent pattern of activation across the
six presentations of all 60 stimuli (see Mitchell et al
(2008) for details).
The top row of Figure 1 presents the learned co-
efficients for one feature dimension for each of the
four semantic models considered in our experiments
(for these images, all voxels rather then the 500
most stable voxels are used). For the Mitchell et al
method, the coefficients presented correspond to the
verb eat; for the other models the feature is the PCA
component that explained the most variance in the
original representations. We also present the pre-
dicted images for the concepts CELERY and AIR-
PLANE, calculated on the coefficients learned over
the remaining 58 concepts. Importantly, for the
Mitchell et al method (column (a)), the learned co-
efficients for eat and the predicted images for CEL-
ERY and AIRPLANE agree with those reported by
Mitchell et al (2008, Figure 2 & online supplemen-
tary material6).
We calculated similarity between predicted and
observed images using both cosine and Pearson cor-
relation and the 500 most stable voxels; we report
the results using Pearson correlation here as this
measure consistently gave slightly better accuracies
6http://www.cs.cmu.edu/?tom/science2008/
featureSignaturesP1.html
74
(a) ?eat? (b) PCA1/clothes (c) PCA1/clothes (d) PCA1/animals
(a) celery (b) celery (c) celery (d) celery
(a) airplane (b) airplane (c) airplane (d) airplane
Figure 1: Learned coefficients on a selected feature dimension (top row) and predicted activation for CELERY (middle
row) and AIRPLANE (bottom row) for four semantic models: (a) Mitchell et al (2008), (b) SVD (c) triple extraction
method (unweighted), and (d) triple extraction method (weighted). Warmer colours indicate higher values (i.e. larger
?-coefficients for the feature dimensions and higher predicted activation for the concepts). PCA components have been
given intuitive labels indicating the kind of information described by that component (see Table 1). As in Figure 2 of
Mitchell et al (2008), the figure shows just one slice in the horizontal plane (z = -12 in MNI space) for one participant
(P1). The predicted images for CELERY and AIRPLANE were generated from the feature coefficients learned on the
other 58 concepts using each of the four models; the corresponding observed images for CELERY and AIRPLANE can
be found in Mitchell et al (2008) Figure 2 B.
75
Method P1 P2 P3 P4 P5 P6 P7 P8 P9 Mean
Mitchell et al (2008) 0.84 0.83 0.76 0.81 0.79 0.66 0.73 0.64 0.68 0.75
SVD 0.82 0.67 0.79 0.83 0.74 0.64 0.64 0.70 0.75 0.73
Triple-extraction (unweighted) 0.82 0.71 0.79 0.80 0.70 0.69 0.65 0.53 0.78 0.72
Triple-extraction (weighted) 0.82 0.72 0.76 0.83 0.73 0.65 0.68 0.51 0.76 0.72
Table 3: Accuracy results for the four semantic models.
for each of the four models (the results are very simi-
lar using the cosine measure). Following Mitchell et
al. (2008; supplementary material), a match score
for each held out pair w1 and w2 was calculated
as the sum of the similarities between the correctly
aligned predicted and observed images:
a = sim(wpred1 , w
obs
1 ) + sim(w
pred
2 , w
obs
2 ) (2)
Similarly a mismatch score was calculated as
b = sim(wpred1 , w
obs
2 ) + sim(w
pred
2 , w
obs
1 ) (3)
Cases where the match score is greater than the mis-
match score (i.e. a > b) count as successes for the
model (i.e. the model correctly identifies the two
predicted images). Otherwise there is a failure by
the model (i.e. the model identifies the observed im-
age for w1 as being w2 and vice-versa).
Table 3 presents the results of the leave-two-
out cross-validation evaluation, giving the propor-
tion (across all 1,770 pairs) of predicted images for
the held-out pairs that were correctly matched to
the observed images.7 The original Mitchell et al
(2008) model has the best mean performance, al-
though across the nine participants, there is no sig-
nificant difference in accuracy between any of the
models (|t(8)| < 1.49, p > 0.17, for all pairwise
paired t-tests between Mitchell et al (2008), SVD,
and weighted triple extraction).
That there is no difference between the perfor-
mance of the Mitchell et al (2008), SVD and triple
7Our results for the Mitchell et al (2008) method are simi-
lar, though not identical, to those reported in that paper (where
the reported mean accuracy across all participants is 0.77, using
cosine similarity). Our implementation of the method for select-
ing the 500 most stable voxels yields slightly different voxels
from those obtained by Mitchell et al (2008; see supplemen-
tary material). In any case, the same set of 500 voxels for each
participant were used for generating the results of each model
presented here, and so we do not believe that this discrepancy
affects comparison of the different models.
extraction methods is surprising, given the different
kinds of information that are available to the dif-
ferent models. In particular, the models that auto-
matically acquire very general and semantically un-
constrained feature-based representations perform
as well as the model which uses a set of manually-
selected sensory-motor verbs, even though the rep-
resentations generated for these models are derived
from 10,000 times less corpus data.
As mentioned in our introduction, an advantage of
evaluating against the fMRI dataset is that this multi-
dimensional data allows us to investigate strengths
and weaknesses of different models in a way which
is not possible using similarity or clustering-based
evaluation. As a very simple investigation of spe-
cific differences in model performance, we present
in Table 4 the pairs of concepts for which each of the
models performs most poorly on. The Mitchell et al
(2008) method appears to do poorly on pairs of con-
cepts where a constituent word can be ambiguous
with respect to its part-of-speech (e.g. SAW, BEAR).
This is not surprising, given that part-of-speech data
is not available in the Google n-gram corpus used
with this method. The performance of the Mitchell
et al method might therefore be improved signifi-
cantly by applying heuristics to the n-gram data to
make inferences about the correct part-of-speech of
instances of words like SAW and BEAR. For the SVD
and weighted triple extraction methods, which both
use the BNC corpus, there is some evidence that
the models are performing poorly for relatively low
frequency words8 (e.g. CHISEL), words which are
semantically ambiguous as nouns (e.g. ARM), and
pairs which are semantically similar (e.g. SPOON &
KNIFE). This suggests that the SVD and triple ex-
traction methods may perform better with a larger
and more diverse corpus.
8AIRPLANE is relatively low frequency in the BNC; it may
be more sensible to use the word AEROPLANE with a British
corpus.
76
Mitchell et al SVD Triple Extraction (weighted)
Pair Nr. Pair Nr. Pair Nr.
bear saw 0 cup airplane 0 dresser chimney 0
bell carrot 0 cup lettuce 0 airplane chisel 0
bell saw 0 horse beetle 0 airplane hand 0
knife bear 0 chisel arm 0 airplane tomato 0
cup saw 1 hammer arm 1 spoon chisel 0
bear tomato 1 dresser arch 1 spoon knife 0
Table 4: Leave-out pairs for which each model performs least accurately, across the nine participants. Nr. = the number
of participants for which this leave-out pair was correctly matched.
5 Conclusion
The fMRI dataset and training and evaluation
methodology presented by Mitchell et al (2008)
gives researchers an interesting new framework with
which to evaluate the quality of feature-based con-
ceptual representations extracted from corpora. This
framework avoids some of the problems inherent in
evaluating extracted representations against a ?gold
standard? based on participant-generated property
norms. It also provides a rich multi-dimensional
dataset through which the strengths and weaknesses
of extraction methods can be identified.
We have applied this evaluation framework to
four feature extraction methods which use different
sources of information available in corpora to extract
conceptual representations. Surprisingly, in spite of
their major differences, we did not find any signifi-
cant difference in performance between the models.
This finding has interesting theoretical implica-
tions, given that previous research has suggested
that aspects of meaning defined by sensory-motor
verbs may have a somewhat distinctive role to play
in predicting the fMRI activation associated with
conceptual stimuli (Mitchell et al, 2008). Our re-
sults suggest that general feature-based representa-
tions of concepts, which place no a priori distinc-
tion on sensory-motor properties, may be equally
capable of predicting activation to conceptual stim-
uli. This highlights the potential for the Mitchell
et al method to be used to inform both distributed
and sensory-motor accounts of conceptual represen-
tation (e.g. McRae et al (1997), Cree et al (2006),
Tyler et al (2000), Tyler & Moss (2001), Moss et
al. (2007), Martin & Chao (2001)), as well as pro-
viding a benchmark with which to assess semantic
model development. In a similar vein, Murphy et
al. (2009) used a dependency-parsed corpus yielding
verb co-occurrence statistics to predict EEG9 activa-
tion patterns with significant accuracy.
The training and evaluation framework presented
by Mitchell et al (2008) represents just one point
in a large space of possibilities for using computa-
tional modelling to predict human brain activity as-
sociated with conceptual stimuli. In these initial ex-
periments, we have chosen to follow the Mitchell
et al approach as closely as possible, in order to
maximize comparability with their results. In future
work, we aim to investigate other methods for train-
ing and evaluation, other corpora and other sources
of imaging data. Furthermore, we aim to use the
evaluation results from such work to inform the de-
velopment of our extraction method.
Acknowledgments
Our work was funded by the EPSRC grant
EP/F030061/1, and the Royal Society University
Research Fellowship, UK. We thank Mitchell et
al. (2008) and McRae et al (2005) for making their
data publically available.
References
Mark Andrews, G. Vigliocco, and D. Vinson. 2005.
Integrating attributional and distributional informa-
tion in a probabilistic model of meaning representa-
tion. In Timo Honkela et al, editor, Proceedings of
AKRR?05, International and Interdisciplinary Confer-
ence on Adaptive Knowledge Representation and Rea-
9EEG measures voltages induced by neuronal firing across
the human scalp.
77
soning, pages 15?25, Espoo, Finland: Helsinki Uni-
versity of Technology.
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic representations. Psychological Re-
view, 116(3):463?498.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. From context to mean-
ing: Distributional models of the lexicon in linguis-
tics and cognitive science (Special issue of the Italian
Journal of Linguistics), 20(1):55?88.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. ESSLLI 2008 Workshop on Distributional
Lexical Semantics.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2009. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, pages 1?33.
E. Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the Interactive Demo Session of COLING/ACL-
06, pages 77?80.
George S. Cree, Chris McNorgan, and Ken McRae.
2006. Distinctive features hold a privileged status
in the computation of word meaning: Implications
for theories of semantic memory. Journal of Experi-
mental Psychology. Learning, Memory, and Cognition,
32(4):643?58.
Colin Kelly, Barry Devereux, and Anna Korhonen. 2010.
Acquiring human-like feature-based conceptual repre-
sentations from corpora. In Brian Murphy, Kai min
Kevin Chang, and Anna Korhonen, editors, Proceed-
ings of the NAACL-HLT Workshop on Computational
Neurolinguistics, Los Angeles, USA.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
G. Leech, R. Garside, and M. Bryant. 1994. CLAWS4:
the tagging of the British National Corpus. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 1, pages 622?628. Association for
Computational Linguistics.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28(2):203?208.
Alex Martin and Linda L. Chao. 2001. Semantic mem-
ory and the brain: structure and processes. Current
Opinion in Neurobiology, 11(2):194?201.
Ken McRae, Virginia R. de Sa, and Mark S. Seidenberg.
1997. On the nature and scope of featural representa-
tions of word meaning. Journal of Experimental Psy-
chology: General, 126(2):99?130.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37:547?559.
Tom M. Mitchell, Svetlana V. Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L. Malave, Robert A.
Mason, and Marcel A. Just. 2008. Predicting human
brain activity associated with the meanings of nouns.
Science, 320(5880):1191?1195.
Helen E. Moss, Lorraine K. Tyler, and Kirsten I. Taylor.
2007. Conceptual structure. In M. Gareth Gaskell, ed-
itor, The Oxford handbook of psycholinguistics, pages
217?234. Oxford University Press, Oxford, UK.
B. Murphy, M. Baroni, and M. Poesio. 2009. Eeg re-
sponds to conceptual stimuli and corpus semantics.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2009),
pages 619?627, East Stroudsburg, PA.
Gregory Murphy. 2002. The big book of concepts. The
MIT Press, Cambridge, MA.
Mark Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234?243.
Lorraine K. Tyler and Helen E. Moss. 2001. Towards a
distributed account of conceptual knowledge. Trends
in Cognitive Sciences, 5(6):244?252.
L. K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
78
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 99?107,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Identifying the Information Structure of Scientific Abstracts: An
Investigation of Three Different Schemes
Yufan Guo
University of Cambridge, UK
yg244@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Maria Liakata
Aberystwyth University, UK
mal@aber.ac.uk
Ilona Silins
Karolinska Institutet, SWEDEN
Ilona.Silins@ki.se
Lin Sun
University of Cambridge, UK
ls418@cam.ac.uk
Ulla Stenius
Karolinska Institutet, SWEDEN
Ulla.Stenius@ki.se
Abstract
Many practical tasks require accessing
specific types of information in scientific
literature; e.g. information about the ob-
jective, methods, results or conclusions of
the study in question. Several schemes
have been developed to characterize such
information in full journal papers. Yet
many tasks focus on abstracts instead. We
take three schemes of different type and
granularity (those based on section names,
argumentative zones and conceptual struc-
ture of documents) and investigate their
applicability to biomedical abstracts. We
show that even for the finest-grained of
these schemes, the majority of categories
appear in abstracts and can be identified
relatively reliably using machine learning.
We discuss the impact of our results and
the need for subsequent task-based evalu-
ation of the schemes.
1 Introduction
Scientific abstracts tend to be very similar in terms
of their information structure. For example, many
abstracts provide some background information
before defining the precise objective of the study,
and the conclusions are typically preceded by the
description of the results obtained.
Many readers of scientific abstracts are inter-
ested in specific types of information only, e.g.
the general background of the study, the methods
used in the study, or the results obtained. Accord-
ingly, many text mining tasks focus on the ex-
traction of information from certain parts of ab-
stracts only. Therefore classification of abstracts
(or full articles) according to the categories of in-
formation structure can support both the manual
study of scientific literature as well as its auto-
matic analysis, e.g. information extraction, sum-
marization and information retrieval (Teufel and
Moens, 2002; Mizuta et al, 2005; Tbahriti et al,
2006; Ruch et al, 2007).
To date, a number of different schemes and
techniques have been proposed for sentence-based
classification of scientific literature according to
information structure, e.g. (Teufel and Moens,
2002; Mizuta et al, 2005; Lin et al, 2006; Hi-
rohata et al, 2008; Teufel et al, 2009; Shatkay
et al, 2008; Liakata et al, 2010). Some of the
schemes are coarse-grained and merely classify
sentences according to typical section names seen
in scientific documents (Lin et al, 2006; Hirohata
et al, 2008). Others are finer-grained and based
e.g. on argumentative zones (Teufel and Moens,
2002; Mizuta et al, 2005; Teufel et al, 2009),
qualitative dimensions (Shatkay et al, 2008) or
conceptual structure (Liakata et al, 2010) of doc-
uments.
The majority of such schemes have been de-
veloped for full scientific journal articles which
are richer in information and also considered to
be more in need of the definition of information
structure (Lin, 2009). However, many practical
tasks currently focus on abstracts. As a distilled
summary of key information in full articles, ab-
stracts may exhibit an entirely different distribu-
tion of scheme categories than full articles. For
tasks involving abstracts, it would be useful to
know which schemes are applicable to abstracts
and which can be automatically identified in them
with reasonable accuracy.
In this paper, we will compare the applicabil-
ity of three different schemes ? those based on
section names, argumentative zones and concep-
tual structure of documents ? to a collection of
biomedical abstracts used for cancer risk assess-
ment (CRA). CRA is an example of a real-world
task which could greatly benefit from knowledge
about the information structure of abstracts since
cancer risk assessors look for a variety of infor-
mation in them ranging from specific methods to
99
results concerning different chemicals (Korhonen
et al, 2009). We report work on the annotation
of CRA abstracts according to each scheme and
investigate the schemes in terms of their distri-
bution, mutual overlap, and the success of iden-
tifying them automatically using machine learn-
ing. Our investigation provides an initial idea of
the practical usefulness of the schemes for tasks
involving abstracts. We discuss the impact of our
results and the further task-based evaluation which
we intend to conduct in the context of CRA.
2 The three schemes
We investigate three different schemes ? those
based on Section Names (S1), Argumentative
Zones (S2) and Core Scientific Concepts (S3):
S1: The first scheme differs from the others in the
sense that it is actually designed for abstracts. It
is based on section names found in some scientific
abstracts. We use the 4-way classification from
(Hirohata et al, 2008) where abstracts are divided
into objective, method, results and conclusions.
Table 1 provides a short description of each cate-
gory for this and other schemes (see also this table
for any category abbreviations used in this paper).
S2: The second scheme is based on Argumenta-
tive Zoning (AZ) of documents. The idea of AZ
is to follow the knowledge claims made by au-
thors. Teufel and Moens (2002) introduced AZ
and applied it to computational linguistics papers.
Mizuta et al (2005) modified the scheme for biol-
ogy papers. More recently, Teufel et al (2009) in-
troduced a refined version of AZ and applied it to
chemistry papers. As these schemes are too fine-
grained for abstracts (some of the categories do
not appear in abstracts at all), we adopt a reduced
version of AZ which integrates seven categories
from (Teufel and Moens, 2002) and (Mizuta et al,
2005) - those which actually appear in abstracts.
S3: The third scheme is concept-driven and
ontology-motivated (Liakata et al, 2010). It treats
scientific papers as humanly-readable representa-
tions of scientific investigations and seeks to re-
trieve the structure of the investigation from the
paper as generic high-level Core Scientific Con-
cepts (CoreSC). The CoreSC is a 3-layer annota-
tion scheme but we only consider the first layer
in the current work. The second layer pertains to
properties of the categories (e.g. ?advantage? vs.
?disadvantage? of METH, ?new? vs. ?old? METH
or OBJT). Such level of granularity is rare in ab-
stracts. The 3rd layer involves coreference iden-
tification between the same instances of each cat-
egory, which is also not of concern in abstracts.
With eleven categories, S3 is the most fine-grained
of our schemes. CoreSC has been previously ap-
plied to chemistry papers (Liakata et al, 2010,
2009).
3 Data: cancer risk assessment abstracts
We used as our data the corpus of CRA ab-
stracts described in (Korhonen et al, 2009) which
contains MedLine abstracts from different sub-
domains of biomedicine. The abstracts were se-
lected so that they provide rich information about
various scientific data (human, animal and cellu-
lar) used for CRA. We selected 1000 abstracts (in
random) from this corpus. The resulting data in-
cludes 7,985 sentences and 225,785 words in total.
4 Annotation of abstracts
Annotation guidelines. We used the guidelines of
Liakata for S3 (Liakata and Soldatova, 2008), and
developed the guidelines for S1 and S2 (15 pages
each). The guidelines define the unit (a sentence)
and the categories of annotation and provide ad-
vice for conflict resolution (e.g. which categories
to prefer when two or several are possible within
the same sentence), as well as examples of anno-
tated abstracts.
Annotation tool. We modified the annotation tool
of Korhonen et al (2009) so that it could be used to
annotate abstracts according to the schemes. This
tool was originally developed for the annotation of
CRA abstracts according to the scientific evidence
they contain. The tool works as a Firefox plug-in.
Figure 1 shows an example of an abstract anno-
tated according to the three schemes.
Description of annotation. Using the guidelines
and the tool, the CRA corpus was annotated ac-
cording to each of the schemes. The annotation
proceeded scheme by scheme, independently, so
that annotations of one scheme were not based on
any of the other two. One annotator (a computa-
tional linguist) annotated all the abstracts accord-
ing to the three schemes, starting from the coarse-
grained S1, then proceeding to S2 and finally to
the finest-grained S3. It took 45, 50 and 90 hours
in total for S1, S2 and S3, respectively.
The resulting corpus. Table 2 shows the distri-
bution of sentences per scheme category in the re-
sulting corpus.
100
Table 1: The Three Schemes
S1 Objective OBJ The background and the aim of the research
Method METH The way to achieve the goal
Result RES The principle findings
Conclusion CON Analysis, discussion and the main conclusions
S2 Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.
Objective OBJ A thing aimed at or sought, a target or goal
Method METH A way of doing research, esp. according to a defined and regular plan; a special form of proce-
dure or characteristic set of procedures employed in a field of study as a mode of investigation
and inquiry
Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc. obtained
by calculation
Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction, induc-
tion; a proposition deduced by reasoning from other propositions; the result of a discussion,
or examination of a question, final determination, decision, resolution, final arrangement or
agreement
Related work REL A comparison between the current work and the related work
Future work FUT The work that needs to be done in the future
S3 Hypothesis HYP A statement that has not been yet confirmed rather than a factual statement
Motivation MOT The reason for carrying out the investigation
Background BKG Description of generally accepted background knowledge and previous work
Goal GOAL The target state of the investigation where intended discoveries are made
Object OBJT An entity which is a product or main theme of the investigation
Experiment EXP Experiment details
Model MOD A statement about a theoretical model or framework
Method METH The means by which the authors seek to achieve a goal of the investigation
Observation OBS The data/phenomena recorded within an investigation
Result RES Factual statements about the outputs of an investigation
Conclusion CON Statements inferred from observations and results, relating to research hypothesis
Inter-annotator agreement. We measured the
inter-annotator agreement on 300 abstracts (i.e. a
third of the corpus) using three annotators (one lin-
guist, one expert in CRA, and the computational
linguist who annotated all the corpus). Accord-
ing to Cohen?s Kappa (Cohen, 1960), the inter-
annotator agreement for S1, S2, and S3 was ? =
0.84, ? = 0.85, and ? = 0.50, respectively. Ac-
cording to (Landis and Koch, 1977), the agree-
ment 0.81-1.00 is perfect and 0.41-0.60 is mod-
erate. Our results indicate that S1 and S2 are
the easiest schemes for the annotators and S3 the
most challenging. This is not surprising as S3 is
the scheme with the finest granularity. Its reliable
identification may require a longer period of train-
ing and possibly improved guidelines. Moreover,
previous annotation efforts using S3 have used do-
main experts for annotation (Liakata et al, 2009,
2010). In our case the domain expert and the lin-
guist agreed the most on S3 (? = 0.60). For S1
and S2 the best agreement was between the lin-
guist and the computational linguist (? = 0.87 and
? = 0.88, respectively).
Table 2: Distribution of sentences in the scheme-
annotated CRA corpus
S1 OBJ METH RES CON
61483 39163 89575 35564 Words
2145 1396 3203 1241 Sentences
27% 17% 40% 16% Sentences
S2 BKG OBJ METH RES CON REL FUT
36828 23493 41544 89538 30752 2456 1174 Words
1429 674 1473 3185 1082 95 47 Sentences
18% 8% 18% 40% 14% 1% 1% Sentences
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
2676 4277 28028 10612 15894 22444 1157 17982 17402 75951 29362 Words
99 172 1088 294 474 805 41 637 744 2582 1049 Sentences
1% 2% 14% 4% 6% 10% 1% 8% 9% 32% 13% Sentences
5 Comparison of the schemes in terms of
annotations
The three schemes we have used to annotate ab-
stracts were developed independently and have
separate guidelines. Thus, even though they seem
to have some categories in common (e.g. METH,
RES, CON) this does not necessarily guarantee that
the latter cover the same information across all
three schemes. We therefore wanted to investigate
the relation between the schemes and the extent of
overlap or complementarity between them.
We used the annotations obtained with each
scheme to create three contingency matrices for
pairwise comparison. We calculated the chi-
squared Pearson statistic, the chi-squared like-
101
Figure 1: An example of an abstract annotated ac-
cording to the three schemes
S1
S2
S3
lihood ratio, the contingency coefficient and
Cramer?s V (Table 3)1, all of which showed a def-
inite correlation between rows and columns for the
pairwise comparison of all three schemes.
However, none of the above measures give an
indication of the differential association between
schemes, i.e. whether it goes both directions and
to what extent. For this reason we calculated the
Goodman-Kruskal lambda L statistic (Siegel and
Castellan, 1988), which gives us the reduction in
error for predicting the categories of one annota-
tion scheme, if we know the categories assigned
according to the other. When using the categories
of S1 as the independent variables, we obtained a
lambda of over 0.72 which suggests a 72% reduc-
tion in error in predicting S2 categories and 47% in
1These are association measures for r x c tables. We used
the implementation in the vcd package of R (http://www.r-
project.org/).
predicting S3 categories. With S2 categories being
the independent variables, we obtained a reduction
in error of 88% when predicting S1 and 55% when
predicting S3 categories. The lower lambdas for
predicting S3 are hardly surprising as S3 has 11
categories as opposed to 4 and 7 for S1 and S2 re-
spectively. S3 on the other hand has strong predic-
tive power in predicting the categories of S1 and
S2 with lambdas of 0.86 and 0.84 respectively. In
terms of association, S1 and S2 seem to be more
strongly associated, followed by S1 and S3 and
then S2 and S3.
We were then interested in the correspondence
between the actual categories of the three schemes,
which is visualized in Figure 2. Looking at the
categories of S1, OBJ maps mostly to BKG and OBJ
in S2 (with a small percentage in METH and REL).
S1 OBJ maps to BKG, GOAL, HYP, MOT and OBJT
in S3 (with a small percentage in METH and MOD).
S1 METH maps to METH in S2 (with a small per-
centage in S2 OBJ) while it maps to EXP, METH
and MOD in S3 (with a small percentage in GOAL
and OBJT). S1 RES covers S2 RES and 40% REL,
whereas in S3 it covers RES, OBS and 20% MOD.
S1 CON covers S2 CON, FUT, 45% REL and a small
percentage of RES. In terms of the S2 vs S3 com-
parison, S2 BKG maps to S3 BKG, HYP, MOT and a
small percentage of OBJT and MOD. S2 CON maps
to S3 CON, with a small percentage in RES, OBS
and HYP. S2 FUT maps entirely to S3 CON. S2
METH maps to S3 METH, EXP, MOD, 20% OBJT
and a small percentage of GOAL. S2 OBJ maps
to S3 GOAL and OBJT, with 15% HYP, MOD and
MOT and a small percentage in METH. S2 REL
spans across S3 CON, RES, MOT and OBJT, albeit
in very small percentages. Finally, S2 RES maps to
S3 RES and OBS, with 25% in MOD and small per-
centages in METH, CON, OBJT. Thus, it appears
that each category in S1 maps to a couple of cate-
gories in S2 and several in S3, which in turn seem
to elaborate on the S2 categories.
Based on the above analysis of the categories,
it is reasonable to assume a subsumption relation
between the categories of the type S1 > S2 >
S3, with REL cutting across several of the S3 cat-
egories and FUT branching off S3 CON. This is
an interesting and exciting outcome given that the
three different schemes have such a different ori-
gin.
102
Table 3: Association measures between schemes S1, S2, S3
S1 vs S2 S1 vs S3 S2 vs S3
X2 df P X2 df P X2 df P
Likelihood Ratio 5577.1 18 0 5363.6 30 0 6293.4 60 0
Pearson 6613.0 18 0 6371.0 30 0 8554.7 60 0
Contingency Coeff 0.842 0.837 0.871
Cramer?s V 0.901 0.885 0.725
Figure 2: Pairwise interpretation of categories of
one scheme in terms of the categories of the other.
6 Automatic identification of information
structure
6.1 Features
The first step in automatic identification of infor-
mation structure is feature extraction. We chose
a number of general purpose features suitable for
all the three schemes. With the exception of our
novel verb class feature, the features are similar to
those employed in related works, e.g. (Teufel and
Moens, 2002; Mullen et al, 2005; Hirohata et al,
2008):
History. There are typical patterns in the infor-
mation structure, e.g. RES tends to be followed
by CON rather than by BKG. Therefore, we used
the category assigned to the previous sentence as
a feature.
Location. Categories tend to appear in typical po-
sitions in a document, e.g. BKG occurs often in the
beginning and CON at the end of the abstract. We
divided each abstract into ten equal parts (1-10),
measured by the number of words, and defined the
location (of a sentence) feature by the parts where
the sentence begins and ends.
Word. Like many text classification tasks, we em-
ployed all the words in the corpus as features.
Bi-gram. We considered each bi-gram (combina-
tion of two word features) as a feature.
Verb. Verbs are central to the meaning of sen-
tences, and can vary from one category to another.
For example, experiment is frequent in METH and
conclude in CON. Previous works have used the
matrix verb of each sentence as a feature. Because
the matrix verb is not the only meaningful verb,
we used all the verbs instead.
Verb Class. Because individual verbs can result in
sparse data problems, we also experimented with a
novel feature: verb class (e.g. the class of EXPERI-
MENT verbs for verbs such as measure and inject).
We obtained 60 classes by clustering verbs appear-
ing in full cancer risk assessment articles using the
approach of Sun and Korhonen (2009).
POS. Tense tends to vary from one category to an-
other, e.g. past is common in RES and past partici-
103
ple in CON. We used the part-of-speech (POS) tag
of each verb assigned by the C&C tagger (Curran
et al, 2007) as a feature.
GR. Structural information about heads and de-
pendents has proved useful in text classification.
We used grammatical relations (GRs) returned by
the C&C parser as features. They consist of a
named relation, a head and a dependent, and pos-
sibly extra parameters depending on the relation
involved, e.g. (dobj investigate mouse). We cre-
ated features for each subject (ncsubj), direct ob-
ject (dobj), indirect object (iobj) and second object
(obj2) relation in the corpus.
Subj and Obj. As some GR features may suf-
fer from data sparsity, we collected all the subjects
and objects (appearing with any verbs) from GRs
and used them as features.
Voice. There may be a correspondence between
the active and passive voice and categories (e.g.
passive is frequent in METH). We therefore used
voice as a feature.
6.2 Methods
We used Naive Bayes (NB) and Support Vector
Machines (SVM) for classification. NB is a sim-
ple and fast method while SVM has yielded high
performance in many text classification tasks.
NB applies Bayes? rule and Maximum Like-
lihood estimation with strong independence as-
sumptions. It aims to select the class c with maxi-
mum probability given the feature set F :
argmaxc P (c|F )=argmaxc
P (c)?P (F |c)
P (F )
=argmaxc P (c)?P (F |c)
=argmaxc P (c)?
?
f?F P (f |c)
SVM constructs hyperplanes in a multidimen-
sional space that separates data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w ? x? b = 0, where w is the normal vector
to the hyperplane. We want to maximize the dis-
tance from the hyperplane to the data points, or the
distance between two parallel hyperplanes each of
which separates the data. The parallel hyperplanes
can be written as:
w?x?b = 1 andw?x?b = ?1, and the distance
between the two is 2|w| . The problem reduces to:
Minimize |w|
Subject to w ? xi ? b ? 1 for xi of one class,
and w ? xi ? b ? ?1 for xi of the other.
7 Experimental evaluation
7.1 Preprocessing
We developed a tokenizer to detect the bound-
aries of sentences and to perform basic tokenisa-
tion, such as separating punctuation from adjacent
words e.g. in tricky biomedical terms such as 2-
amino-3,8-diethylimidazo[4,5-f]quinoxaline. We
used the C&C tools (Curran et al, 2007) for POS
tagging, lemmatization and parsing. The lemma
output was used for extracting Word, Bi-gram and
Verb features. The parser produced GRs for each
sentence from which we extracted the GR, Subj,
Obj and Voice features. We only considered the
GRs relating to verbs. The ?obj? marker in a sub-
ject relation indicates a verb in passive voice (e.g.
(ncsubj observed 14 difference 5 obj)). To control
the number of features we removed the words and
GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences, and lemmatized the
lexical items for all the features.
7.2 Evaluation methods
We used Weka (Witten, 2008) for the classifica-
tion, employing its NB and SVM linear kernel. The
results were measured in terms of accuracy (the
percentage of correctly classified sentences), pre-
cision, recall, and F-Measure. We used 10-fold
cross validation to avoid the possible bias intro-
duced by relying on any one particular split of the
data. The data were randomly divided into ten
parts of approximately the same size. Each indi-
vidual part was retained as test data and the re-
maining nine parts were used as training data. The
process was repeated ten times with each part used
once as the test data. The resulting ten estimates
were then combined to give a final score. We
compare our classifiers against a baseline method
based on random sampling of category labels from
training data and their assignment to sentences on
the basis of their observed distribution.
7.3 Results
Table 4 shows F-measure results when using each
individual feature alone, and Table 5 when using
all the features but the individual feature in ques-
tion. In these two tables, we only report the results
for SVM which performed considerably better than
NB. Although we have results for most scheme
categories, the results for some are missing due to
the lack of sufficient training data (see Table 2), or
due to a small feature set (e.g. History alone).
104
Table 4: F-Measure results when using each in-
dividual feature alone
a b c d e f g h i j k
S1 OBJ .39 .83 .71 .69 .52 .45 .45 .45 .54 .39 -
METH - .47 .81 .74 .63 .49 - .46 .03 .42 .51
RES - .76 .85 .86 .76 .70 .72 .69 .70 .68 .54
CON - .72 .70 .65 .63 .53 .49 .57 .68 .20 -
S2 BKG .26 .73 .69 .67 .45 .38 .56 .33 .33 .29 -
OBJ - .13 .72 .68 .54 .63 - .49 .48 .20 -
METH - .50 .81 .72 .64 .47 - .47 .03 .42 .51
RES - .76 .85 .87 .76 .72 .72 .70 .69 .68 .54
CON - .70 .73 .71 .62 .51 .40 .61 .67 .23 -
REL - - - - - - - - - - -
FUT - - - - - - - - - - -
S3 HYP - - - - .67 - - - - - -
MOT .18 .57 .70 .49 .39 .13 .36 .33 .30 .40 -
BKG - - .54 .40 .21 - - .11 .06 .06 -
GOAL - - .53 .33 .22 - .19 .31 - .25 -
OBJT - - .73 .63 .60 .10 - .26 .32 - -
EXP - .22 .63 .46 .33 .30 - .31 .07 .44 .25
MOD - - - - - - - - - - -
METH - - .82 .61 .39 .39 - .50 - .37 -
OBS - .59 .75 .71 .63 .56 .56 .54 .48 .52 .47
RES - - .87 .73 .41 .34 - .38 .24 .35 -
CON - .74 .68 .65 .65 .50 .48 .49 .55 .21 -
a-k: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR,
Subj, Obj, Voice
Looking at individual features alone, Word,
Bi-gram and Verb perform the best for all the
schemes, and History and Voice perform the worst.
In fact History performs very well on the training
data, but for the test data we can only use esti-
mates rather than the actual labels. The Voice fea-
ture works only for RES and METH for S1 and S2,
and for OBS for S3. This feature is probably only
meaningful for some of the categories.
When using all but one of the features, S1 and
S2 suffer the most from the absence of Location,
while S3 from the absence of Word/POS. Verb
Class on its own performs worse than Verb, how-
ever when combined with other features it per-
forms better: leave-Verb-out outperforms leave-
Verb Class-out.
After comparing the various combinations of
features, we found that the best selection of fea-
tures was all but the Verb for all the schemes. Ta-
ble 6 shows the results for the baseline (BL), and
the best results for NB and SVM. NB and SVM per-
form clearly better than BL for all the schemes.
The results for SVM are the best. NB yields the
highest performance with S1. Being sensitive to
sparse data, it does not perform equally well on S2
and S3 which have a higher number of categories,
some of which are low in frequency (see Table 2).
For S1, SVM finds all the four scheme categories
with the accuracy of 89%. F-measure is 90 for
OBJ, RES and CON and 81 for METH. For S2,
the classifier finds six of the seven categories, with
the accuracy of 90% and the average F-measure of
Table 5: F-Measure results using all the features and
all but one of the features
ALL A B C D E F G H I J K
S1 OBJ .90 .89 .87 .92 .90 .90 .91 .91 .91 .92 .91 .88
METH .80 .81 .80 .80 .79 .81 .79 .80 .80 .80 .81 .81
RES .88 .90 .88 .90 .88 .90 .88 .88 .88 .89 .89 .90
CON .86 .85 .82 .87 .88 .90 .90 .88 .89 .88 .88 .90
S2 BKG .91 .94 .90 .90 .93 .94 .94 .91 .93 .94 .92 .94
OBJ .72 .78 .84 .78 .83 .88 .84 .81 .83 .84 .78 .83
METH .81 .83 .80 .81 .80 .85 .80 .78 .81 .81 .82 .83
RES .88 .90 .88 .89 .88 .91 .89 .89 .90 .90 .90 .89
CON .84 .83 .77 .83 .86 .88 .86 .87 .88 .89 .88 .81
REL - - - - - - - - - - - -
FUT - 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
S3 HYP - - - - - - - - - - - -
MOT .82 .84 .80 .76 .82 .82 .83 .78 .83 .83 .83 .83
BKG .59 .60 .60 .54 .67 .62 .62 .59 .61 .61 .62 .61
GOAL .62 .67 .67 .62 .71 .62 .67 .43 .67 .67 .67 .62
OBJT .88 .85 .83 .74 .83 .85 .83 .74 .83 .83 .83 .85
EXP .72 .68 .72 .53 .65 .70 .72 .73 .74 .74 .72 .68
MOD - - - - - - - - - - - -
METH .87 .86 .87 .66 .85 .89 .87 .88 .86 .86 .87 .86
OBS .82 .81 .84 .72 .80 .82 .81 .80 .82 .82 .81 .81
RES .87 .87 .88 .74 .87 .86 .87 .86 .87 .87 .87 .88
CON .88 .88 .82 .88 .83 .87 .87 .84 .87 .88 .87 .86
A-K: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR, Subj,
Obj, Voice
We have 1.0 for FUT in S2 probably because the size of the training data is
just right, and the model doesn?t overfit the data. We make this assumption
because we have 1.0 for almost all the categories in the training data, but only
for FUT on the test data.
Table 6: Baseline and best NB and SVM results
Acc. F-Measure
S1 OBJ METH RES CON
BL .29 .23 .23 .39 .18
NB .82 .85 .75 .85 .71
SVM .89 .90 .81 .90 .90
Acc. F-Measure
S2 BKG OBJ METH RES CON REL FUT
BL .25 .13 .08 .22 .40 .13 - -
NB .76 .79 .25 .70 .83 .66 - -
SVM .90 .94 .88 .85 .91 .88 - 1.0
Acc. F-Measure
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
BL .15 - .10 .06 .04 .06 .11 - .13 .24 .15 .17
NB .53 - .56 - - - .30 - .32 .61 .59 .62
SVM .81 - .82 .62 .62 .85 .70 - .89 .82 .86 .87
91 for the six categories. As with S2, METH has
the lowest performance (at 85 F-measure). The
one missing category (REL) appears in our abstract
data with very low frequency (see Table 2).
For S3, SVM uncovers as many as nine of the
11 categories with accuracy of 81%. Six cate-
gories perform well, with F-measure higher than
80. EXP, BKG and GOAL have F-measure of 70,
62 and 62, respectively. Like the missing cate-
gories HYP and MOD, GOAL is very low in fre-
quency. The lower performance of the higher fre-
quency EXP and BKG is probably due to low pre-
cision in distinguishing between EXP and METH,
and BKG and other categories, respectively.
105
8 Discussion and conclusions
The results from our corpus annotation (see Ta-
ble 2) show that for the coarse-grained S1, all the
four categories appear frequently in biomedical
abstracts (this is not surprising because S1 was ac-
tually designed for abstracts). All of them can be
identified using machine learning. For S2 and S3,
the majority of categories appear in abstracts with
high enough frequency that we can conclude that
also these two schemes are applicable to abstracts.
For S2 we identified six categories using machine
learning, and for S3 as many as nine, indicating
that automatic identification of the schemes in ab-
stracts is realistic.
Our analysis in section 5 showed that there is
a subsumption relation between the categories of
the schemes. S2 and S3 provide finer-grained in-
formation about the information structure of ab-
stracts than S1, even with their 2-3 low frequency
(or missing) categories. They can be useful for
practical tasks requiring such information. For ex-
ample, considering S3, there may be tasks where
one needs to distinguish between EXP, MOD and
METH, between HYP, MOT and GOAL, or between
OBS and RES.
Ultimately, the optimal scheme will depend on
the level of detail required by the application at
hand. Therefore, in the future, we plan to conduct
task-based evaluation of the schemes in the con-
text of CRA and to evaluate the usefulness of S1-
S3 for tasks cancer risk assessors perform on ab-
stracts (Korhonen et al, 2009). Now that we have
annotated the CRA corpus for S1-S3 and have a
machine learning approach available, we are in an
excellent position to conduct this evaluation.
A key question for real-world tasks is the level
of machine learning performance required. We
plan to investigate this in the context of our task-
based evaluation. Although we employed fairly
standard text classification methodology in our ex-
periments, we obtained high performance for S1
and S2. Due to the higher number of categories
(and less training data for each of them), the over-
all performance was not equally impressive for S3
(although still quite high at 81% accuracy).
Hirohata et al (2008) have showed that the
amount of training data can have a big impact
on our task. They used c. 50,000 Medline ab-
stracts annotated (by the authors of the Medline
abstracts) as training data for S1. When using a
small set of standard text classification features
and Conditional Random Fields (CRF) (Lafferty
et al, 2001) for classification, they obtained 95.5%
per-sentence accuracy on 1000 abstracts. How-
ever, when only 1000 abstracts were used for train-
ing the accuracy was considerably worse; their re-
ported per-abstract accuracy dropped from 68.8%
to less than 50%. Although it would be difficult to
obtain similarly huge training data for S2 and S3,
this result suggests that one key to improved per-
formance is larger training data, and this is what
we plan to explore especially for S3.
In addition we plan to improve our method. We
showed that our schemes partly overlap and that
similar features and methods tend to perform the
best / worst for each of the schemes. It is therefore
unlikely that considerable scheme specific tuning
will be necessary. However, we plan to develop
our features further and to make better use of the
sequential nature of information structure. Cur-
rently this is only represented as the History fea-
ture, which provides a narrow window view to the
category of the previous sentence. Also we plan to
compare SVM against methods such as CRF and
Maximum Entropy which have proved successful
in recent related works (Hirohata et al, 2008; Mer-
ity et al, 2009). The resulting models will be eval-
uated both directly and in the context of CRA to
provide an indication of their practical usefulness
for real-world tasks.
Acknowledgments
The work reported in this paper was funded by the
Royal Society (UK), the Swedish Research Coun-
cil, FAS (Sweden), and JISC (UK) which is fund-
ing the SAPIENT Automation project. YG was
funded by the Cambridge International Scholar-
ship.
106
References
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&c and boxer. In
Proceedings of the ACL 2007 Demonstrations Ses-
sion, pages 33?36.
K. Hirohata, N. Okazaki, S. Ananiadou, and
M. Ishizuka. 2008. Identifying sections in scien-
tific abstracts using conditional random fields. In
Proc. of 3rd International Joint Conference on Nat-
ural Language Processing.
A. Korhonen, L. Sun, I. Silins, and U. Stenius. 2009.
The first step in the development of text mining tech-
nology for cancer risk assessment: Identifying and
organizing scientific evidence in risk assessment lit-
erature. BMC Bioinformatics, 10:303.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditionl random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
J. R. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33:159?174.
M. Liakata and L.N. Soldatova. 2008. Guide-
lines for the annotation of general scientific con-
cepts. Aberystwyth University, JISC Project Report
http://ie-repository.jisc.ac.uk/88/.
M. Liakata, Claire Q, and L.N. Soldatova. 2009. Se-
mantic annotation of papers: Interface & enrichment
tool (sapient). In Proceedings of BioNLP-09, pages
193?200, Boulder, Colorado.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batch-
elor. 2010. Corpora for the conceptualisation and
zoning of scientific papers. To appear in the 7th In-
ternational Conference on Language Resources and
Evaluation.
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings
of BioNLP-06, pages 65?72, New York, USA.
J. Lin. 2009. Is searching full text more effective than
searching abstracts? BMC Bioinformatics, 10:46.
S. Merity, T. Murphy, and J. R. Curran. 2009. Ac-
curate argumentative zoning with maximum entropy
models. In Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital
Libraries, pages 19?26. Association for Computa-
tional Linguistics.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2005. Zone analysis in biology articles as a basis
for information extraction. International Journal of
Medical Informatics on Natural Language Process-
ing in Biomedicine and Its Applications.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A baseline
feature set for learning rhetorical zones using full ar-
ticles in the biomedical domain. Natural language
processing and text mining, 7:52?58.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007.
Using argumentation to extract key sentences from
biomedical abstracts. Int J Med Inform, 76:195?
200.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur.
2008. Multi-dimensional classification of biomed-
ical text: Toward automated, practical provision of
high-utility text to diverse users. Bioinformatics,
18:2086?2093.
S. Siegel and N. J. Jr. Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill, Berkeley, CA, 2nd edition.
L. Sun and A. Korhonen. 2009. Improving verb clus-
tering with automatically acquired selectional pref-
erence. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75:488?495.
S. Teufel and M. Moens. 2002. Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning:
Evidence from chemistry and computational linguis-
tics. In Proc. of EMNLP.
I. H. Witten, 2008. Data mining: practical machine
learning tools and techniques with Java Implemen-
tations. http://www.cs.waikato.ac.nz/ml/weka/.
107
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 11?20,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Semi-supervised learning for automatic conceptual property extraction
Colin Kelly
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
colin.kelly
@cl.cam.ac.uk
Barry Devereux
Centre for Speech,
Language, and the Brain
University of Cambridge
Cambridge, CB2 3EB, UK
barry@csl.psychol.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
anna.korhonen
@cl.cam.ac.uk
Abstract
For a given concrete noun concept, humans
are usually able to cite properties (e.g., ele-
phant is animal, car has wheels) of that con-
cept; cognitive psychologists have theorised
that such properties are fundamental to un-
derstanding the abstract mental representation
of concepts in the brain. Consequently, the
ability to automatically extract such properties
would be of enormous benefit to the field of
experimental psychology. This paper investi-
gates the use of semi-supervised learning and
support vector machines to automatically ex-
tract concept-relation-feature triples from two
large corpora (Wikipedia and UKWAC) for
concrete noun concepts. Previous approaches
have relied on manually-generated rules and
hand-crafted resources such as WordNet; our
method requires neither yet achieves bet-
ter performance than these prior approaches,
measured both by comparison with a property
norm-derived gold standard as well as direct
human evaluation. Our technique performs
particularly well on extracting features rele-
vant to a given concept, and suggests a number
of promising areas for future focus.
1 Introduction
The representation of concrete concepts (e.g., car,
banana, spanner) in the human brain has long been
an important area of investigation for cognitive psy-
chologists. Recent theories of this mental repre-
sentation have proposed a componential, property-
based and distributed model of conceptual knowl-
edge (e.g., Farah and McClelland (1991), Randall et
al. (2004), Tyler et al (2000)).
In order to empirically test these cognitive the-
ories, researchers have moved towards employing
real-world knowledge in their experiments. This
knowledge has usually been procured from human-
derived lists of properties taken from property norm-
ing studies (Garrard et al, 2001; McRae et al,
2005). In such studies, human participants are
asked to describe and note properties of a given
concept (e.g., has shell for turtle). Synonymous
responses are grouped together as a single prop-
erty and those meeting a certain minimum response-
frequency threshold are taken as valid properties.
The most wide-ranging study to date was that con-
ducted by McRae et al (2005): some sample prop-
erties from this set are in Table 1.
As others have noted (Murphy, 2002; McRae et
al., 2005), property norming studies are prone to a
number of deficiencies. One such weakness is the
incongruity of shared properties across even highly-
related concepts: human respondents exhibit a lack
of consistency when listing properties that are com-
mon to many similar concepts. For example, while
has legs is listed as a property of crocodile in the
McRae norms, it is absent as a property of alliga-
tor. A related issue is the non-comprehensive nature
of the generated norms ? although they may cover
the most salient properties for a given concept, they
are unlikely to comprise all of a concept?s properties
(e.g., has heart does not appear as a property of any
of the 92 animal concepts).
Our research aims to use NLP techniques to cre-
ate a system able to emulate the output of such
studies, and overcome some of the aforementioned
weaknesses. Our proposed system begins by search-
ing dependency-parsed corpora for those sentences
containing concept and feature terms which are
also found in a McRae norm-derived training set
of properties. For these sentences, the system
generates grammatical relation/part-of-speech struc-
tural attributes and applies support vector machines
(SVMs) to learn sets of attributes likely to indicate
the instantiation of a property in a sentence. These
11
turtle bowl
has a shell 25 is round 19
lays eggs 16 used for eating 12
swims 15 used for soup 11
is green 14 used for food 11
lives in water 14 used for liquids 10
is slow 13 used for eating cereal 10
an animal 11 made of plastic 8
walks 10 used for holding things 7
walks slowly 10 is curved 7
has 4 legs 9 found in kitchens 7
Table 1: Top ten properties from McRae norms with pro-
duction frequencies for turtle and bowl.
learned patterns of salient attributes are finally ap-
plied to a corpus to derive new properties for unseen
concepts.
Our task is a challenging one: the properties we
seek are extremely diverse in their form. They range
from the simple (e.g., banana is yellow) to the com-
plex (e.g., bayonet found at the end of a gun). Al-
though the properties can broadly be divided into
a number of categories (encyclopedic, taxonomic,
functional, etc) there is not a great deal of regular-
ity in the nature of the properties a given noun will
likely possess: it is highly concept-dependent.
Furthermore, we hope to derive these properties
from corpora, with the assumption that these prop-
erties will manifest themselves therein. Indeed, An-
drews et al (2005) discuss a theory of human knowl-
edge which relies on a combination of both dis-
tributional (i.e., derived from spoken and written
language) and experiential data (i.e., that derived
from our interactions with the real world), claiming
that the necessary contribution of each data-type for
a comprehensive human semantic representation is
non-trivial. Finally, there are difficulties associated
with evaluating our system?s output directly against
a set of human-generated property norms: we dis-
cuss these in further detail later.
Given their provenance, the properties found in
property norms are free-form. To simplify our task
we apply a more rigid representation to the proper-
ties we already have and to those we aim to seek. We
delineate each property into a concept relation fea-
ture triple (see Section 2.2) and our task becomes
one of finding valid relation feature pairs given a par-
ticular concept. This recoding renders our task more
well-defined and makes evaluation of our method
reptile1NNS
include2VBP
species0IN
of3IN
turtle5NN
dobj
five1DT
ncmod ncmod
dobj
marine0NNP
ncmod ncsubj
Figure 1: C&C-derived GR-POS graph for the sentence
Marine reptiles include five species of turtle.
more comparable to previous and related work.
Having framed our task in this way, there is an
obvious parallel with relation extraction: both ne-
cessitate the selection/classification of relationships
between individual entities (in our case, between
concept and feature). Hearst (1992) was the first
to propose a pattern-based approach to this task us-
ing lexico-syntactic patterns to automatically extract
hyponyms and this technique has frequently been
used for ontology learning. For example, Pantel and
Pennacchiotti (2008) linked instantiations of a set of
semantic relations into existing semantic ontologies
and Davidov et al (2007) employed seed concepts
from a given semantic class to discover relations
shared by concepts in that class.
Our task is more complex than classic relation ex-
traction for two main reasons: 1) the relations which
we aim to extract are not limited to a small set of
just a few well-defined relations (e.g., is-a and part-
of) nor to the relations of a specific semantic class
(e.g., capital-is for countries). Indeed the relations
can be as many and diverse as the concepts them-
selves (e.g., each concept could possess a unique
and distinguishing relation and feature). 2) We are
attempting to simultaneously extract two pieces of
information: features of the concept and those fea-
tures? defining relationship with the concept, but
only those relations and features which would be
classified as ?common-sense?, something which is
easy for humans to recognise but difficult (if not im-
possible) to describe rigorously or formally.
There has recently been work on the automatic ex-
12
traction of binary relations that scale to a web cor-
pus, for example the ReVerb (Etzioni et al, 2011)
and WOE (Wu and Weld, 2010) systems. These
systems are designed to extract legitimate relations
from a given sentence. In contrast, our aim is to cap-
ture more general relationships which are ?common-
sense?; just because an extracted relation is correct
in a given context does not automatically make it
true in general. Previous reasoned approaches to our
task have taken their lead from Hearst and her suc-
cessors, employing manually-created rulesets to ex-
tract such properties from corpora (e.g., Baroni et al
(2009), Devereux et al (2010), and our comparison
system (Kelly et al, 2010)). Baroni et al extract re-
lational information in the form of ?type-sketches?,
which give an approximate, implicit description of
the relationship whereas we are aiming to extract
explicit relations between the target concept and its
corresponding features. Devereux et al and Kelly
et al have attempted this, but both employ WordNet
(Fellbaum, 1998) to extract semantic relatedness in-
formation.
We use semi-supervised learning as it offers a
flexible technique of harnessing small amounts of
labelled data to derive information from unlabelled
datasets/corpora and allows us to guide the extrac-
tion towards our desired ?common-sense? output.
We chose SVMs as they have been used for a va-
riety of tasks in NLP (e.g., Joachims et al (1998),
Gime?nez and Marquez (2004)). We will demon-
strate that our system?s performance exceeds that of
Kelly et al (2010) and Etzioni et al (2011). It is, as
far as we are aware, the first work to employ semi-
supervised learning for this task.
2 Method
We will use SVMs to learn lexico-syntactic pat-
terns in our corpora corresponding to known prop-
erties in order to find new ones. Training an SVM
requires a labelled training set. To generate this
set we harness our already-known concepts/features
(and their relationships) from the McRae norms to
find instantiations of said relationships within our
corpora. We use parsed sentence information from
our corpora to create a set of attributes describing
each relationship, our learning patterns. In doing
so, we are assuming that across sentences in our
corpora containing a concept/feature pair found in
the McRae norms, there will be a set of consistent
lexico-syntactic patterns which indicate the same re-
lationship as that linking the pair in the norms.
Thus we iterate over our chosen corpora, parsing
each concept-containing sentence to yield grammat-
ical relation (GR) and part-of-speech (POS) infor-
mation from which we can create a GR-POS graph
relating the two. Then for each triple, we find any/all
paths through the graph which link the concept to its
feature and use the corresponding relation to label
this path. We collect descriptive information about
the path in the form of attributes describing it (e.g.,
path nodes, labels, length) to create a training pattern
specific to that concept relation feature triple and
sentence. It is these lists of attributes (and their rela-
tion labels) which we employ as the labelled training
set and as input for our SVM.
2.1 Corpora
We employ two corpora for our experiments:
Wikipedia and the UKWAC corpus (Ferraresi et al,
2008). These are both publicly available and web-
based: the former a source of encyclopedic infor-
mation and the latter a source of general text. Our
Wikipedia corpus is based on a Sep 2009 version
of English-language Wikipedia and contains around
1.84 million articles (>1bn words). Our UKWAC
corpus is an English-language corpus (>2bn words)
obtained by crawling the .uk internet domain.
2.2 Training data
Our experiments use a British-English version of
the McRae norms (see Taylor et al (2011) for de-
tails). We needed to recode the free-form McRae
properties into relation-classes and features which
would be usable for our learning algorithm. As
we will be matching the features from these prop-
erties with individual words in the training corpus
it was essential that the features we generated con-
tained only one lemmatised word. In contrast, the
relations were merely labels for the relationship de-
scribed (they did not need to occur in the sentences
we were training from) and therefore needed only
to be single-string relations. This allowed preposi-
tional verbs as distinct relations, something which
has not been attempted in previous work yet can be
semantically significant (e.g., the relations used-in,
used-for and used-by have dissimilar meanings).
We applied the following sequential multi-step
13
process to our set of free-form properties to distill
them to triples of the form concept relation feature,
where relation can be a multi-word string and feature
is a single word:
1. Translation of implicit properties to their correct re-
lations (e.g., pig an animal ? pig is an animal).
2. Removal of indefinite and definite articles.
3. Behavioural properties become ?does? properties
(e.g., turtle beh eats ? turtle does eats).
4. Negative properties given their own relation classes
(e.g., turkey does cannot fly ? turkey doesnt fly).
5. All numbers are translated to named cardinals (e.g.,
spider has 8 legs ? spider has eight legs).
6. Some of the norms already contained synonymous
terms: these were split into separate triples for each
synonym (e.g., pepper tastes hot/spicy ? pepper
tastes hot and pepper tastes spicy).
7. Prepositional verbs were translated to one-word,
hyphenated strings (e.g., made of ? made-of ).
8. Properties with present participles as the penulti-
mate word were split into one including the verb as
the feature and one including it in the relation (e.g.,
envelope used for sending letters ? envelope used-
for-sending letters and envelope used-for sending).
9. Any remaining multi-word properties were split
with the first term after the concept acting as the
relation (e.g., bull has ring in its nose ? bull has
ring, bull has in, bull has its and bull has nose).
10. All remaining stop-words were removed; properties
ending in stop-words (e.g., bull has in and bull has
its) were removed completely.
This yielded 7,518 property-triples with 254 distinct
relations and an average of 14.7 triples per concept.
2.3 Parsing
We parsed both corpora using the C&C parser
(Clark and Curran, 2007) as we employ both GR
and POS information in our learning method. To ac-
celerate this stage, we process only sentences con-
taining a form (e.g., singular/plural) of one of our
training/testing concepts. We lemmatise each word
using the WordNet NLTK lemmatiser (Bird, 2006).
Parsing our corpora yields around 10Gb and 12Gb
of data for UKWAC and Wikipedia respectively.
The C&C dependency parse output contains, for
a given sentence, a set of GRs forming an acyclic
graph whose nodes correspond to words from the
sentence, with each node also labelled with the POS
of that word. Thus the GR-POS graph interrelates all
lexical, POS and GR information for the entire sen-
tence. It is therefore possible to construct a GR-POS
graph rooted at our target term (the concept in ques-
tion), with POS-labelled words as nodes, and edges
labelled with GRs linking the nodes to one another.
An example graph can be seen in Figure 1.
2.4 Support vector machines
We use SVMs (Cortes and Vapnik, 1995) for our
experiments as they have been widely used in NLP
and their properties are well-understood, showing
good performance on classification tasks (Meyer et
al., 2003). In their canonical form, SVMs are non-
probabilistic binary linear classifiers which take a set
of input data and predict, for each given input, which
of two possible classes it corresponds to.
There are more than two possible relation-labels
to learn for our input patterns, so ours is a multi-class
classification task. For our experiments we use the
SVM Light Multiclass (v. 2.20) software (Joachims,
1999) which applies the fixed-point SVM algorithm
described by Crammer and Singer (2002) to solve
multi-class problem instances. Joachims? software
has been widely used to implement SVMs (Vinok-
ourov et al, 2003; Godbole et al, 2002).
2.5 Attribute selection
Previous techniques for our task have made use of
lexical, syntactic and semantic information. We are
deliberately avoiding the use of manually-created
semantic resources, so we rely only on lexical and
syntactic attributes for our learning stage (i.e., the
GR-POS paths described earlier).
A table of all the categories of attributes we ex-
tract for each GR-POS path are in Table 2.4, together
with attributes from the path linking turtle and reptile
in our example sentence (see Figure 1).
We ran our experiments with two vector-types
which we call our ?verb-augmented? and our ?non-
augmented? vector-types. The sets are identical ex-
cept the verb-augmented vector-type will also con-
tain an additional attribute category containing an
attribute for every instance of a relation verb (i.e.,
a verb which is found in our training set of relations,
e.g., become, cause, taste, use, have and so on) in
the lexical path. We do this to ascertain whether this
additional verb-information might be more informa-
tive to our system when learning relations (which
tend to be composed of verbs).
14
Attribute category Example attribute(s)
GR path-length LEN
lemmatised anchor node LEM=turtle
POS of anchor node POS=NN
GR path labels GR1=dobjR
from anchor GR2=ncmodR
(indexed) GR3=dobjR
GR4=ncsubjN
GR path labels GR1=ncsubjR
from target GR2=dobjN
(indexed) GR3=ncmodN
GR4=dobjN
POS of path nodes POS1=IN
from anchor POS2=NNS
(indexed) POS3=VBP
POS4=NNS
POS of path nodes POS1=NNS
from target POS2=VBP
(indexed) POS3=NNS
POS4=IN
lemmatised path nodes LEM=include
(bag of words) LEM=species
LEM=of
POS of all path nodes POS=IN
(set) POS=NNS
POS=VBP
Relation verbs N/A
GR path labels GR=dobjR
(set) GR=ncmodN
GR=ncsubjN
lemmatised target node LEM=reptile
POS of target node POS=NNS
Table 2: An example vector for an instance of the
relation-label is. The attributes are distinguished from
one another by their attribute category. Relation verbs
only appear in the verb-augmented vector-type and no
such verbs appear in our example sentence, so this cat-
egory of attribute is empty. All attributes in the table will
receive the value 1.0 except the LEN attribute which will
have the value 0.2 (the reciprocal of the path length, 5).
We considered allocating a ?no-rel? relation la-
bel to those sets of attributes corresponding to paths
through the GR-POS graph which did not link the
concept to a feature found in our training data;
however our initial experiments indicated the SVM
model would assign every pattern we tested to the
?no-rel? relation. Therefore we used only positive
instances in our training pattern data.
We cycle through all training concepts/features,
finding sentences containing both. For each such
sentence, our system generates the attributes from
the GR-POS path linking the concept to the fea-
ture (the linking-path) to create a pattern for that
pair, in the form of a relation-labelled vector con-
taining real-valued attributes. The system assigns
1.0 to all attributes occurring in a given path
and the LEN value receives the reciprocal of the
path-length.1 Each linking-path is collected into a
relation-labelled, sparse vector in this manner. In
the larger UKWAC corpus this corresponds to over
29 million unique attributes across all found linking-
paths (this figure corresponds to the dimensionality
of our vectors). We then pass all vectors to the learn-
ing module2 of SVM Light to generate a learned
model across all training concepts.
2.6 Extracting candidate patterns
Having trained our model, we must now find po-
tential features and relations for our test concepts
in our corpora. We again only examine sentences
which contain at least one of our test concepts. Fur-
thermore, to avoid a combinatorial explosion of pos-
sible paths rooted at those concepts we only permit
as candidates those paths whose anchor node is a
singular or plural noun and whose target node is ei-
ther a singular/plural noun or adjective. This filter-
ing corresponds to choosing patterns containing one
of the three most frequent anchor node POS tags
(NN, NNS and NNP) and target node POS tags (NN,
JJ and NNS) found during our training stage. These
candidate patterns constitute 92.6% and 87.7% of
all the vectors, respectively, from our training set
of patterns (on the UKWAC corpus). This pattern
pre-selection allows us to immediately ignore paths
which, despite being rooted at a test concept, are un-
likely to contain property norm-like information.
2.7 Generating and ranking triples
We next classified our test concepts? candidate
patterns using the learned model. SVM Light as-
signs each pattern a relation-class from the training
set and outputs the values of the decision functions
from the learned model when applied to that par-
ticular pattern. The sign of these values indicates
the binary decision function choice, and their mag-
nitude acts as a measure of confidence. We wanted
those vectors which the model was most confident
in across all decision functions, so we took the sum
of the absolute values of the decision values to gen-
erate a pattern score for each vector/relation-label.
1All other possible attributes are assigned the value 0.0.
2Using a regularisation parameter (C) value of 1.0 and de-
fault parameters otherwise.
15
Vector-type Corpus ?LL ?PMI ?SVM Prec. Recall F
Ignoring relation.
Non-augmented
Wikipedia 0.3 0.00 1.00 0.2214 0.3197 0.2564
UKWAC 0.10 0.05 0.60 0.2279 0.3330 0.2664
UKWAC-Wikipedia 0.35 0.00 0.75 0.2422 0.3533 0.2829
Verb-augmented
Wikipedia 0.20 0.00 0.65 0.2217 0.3202 0.2568
UKWAC 0.30 0.00 0.95 0.2326 0.3400 0.2720
UKWAC-Wikipedia 0.40 0.05 1.00 0.2444 0.3577 0.2859
With relation.
Non-augmented
Wikipedia 0.05 0.00 1.00 0.1199 0.1732 0.1394
UKWAC 0.05 0.00 1.00 0.1126 0.1633 0.1312
UKWAC-Wikipedia 0.05 0.00 0.65 0.1241 0.1808 0.1449
Verb-augmented
Wikipedia 0.05 0.00 1.00 0.1215 0.1747 0.1410
UKWAC 0.05 0.00 1.00 0.1190 0.1724 0.1387
UKWAC-Wikipedia 0.05 0.00 0.70 0.1281 0.1860 0.1494
Table 3: Parameter estimation both with and without relation, using our augmented and non-augmented vector-types
and across our two corpora and the combined corpora set.
From these patterns we derived an output set of
triples where the concept and feature of a triple cor-
responded to the anchor and target nodes of its pat-
tern and the relation corresponded to the pattern?s
relation-label. Identical triples from differing pat-
terns had their pattern scores summed to give a final
?SVM score? for that triple.
2.8 Calculating triple scores
A brief qualitative evaluation of our system?s out-
put indicates that although the higher-ranked (by
SVM score) features and relations were, for the most
part, quite sensible, there were some obvious output
errors (e.g., non-dictionary strings or verbs appear-
ing as features). Therefore we restricted our fea-
tures to those which appear as nouns or adjectives in
WordNet and excluded features containing an NLTK
(Bird, 2006) corpus stop-word. Despite these exclu-
sions, some general (and therefore less informative)
relation/feature combinations (e.g., is good, is new)
were still ranking highly. To mitigate this, we ex-
tract both log-likelihood (LL) and pointwise mutual
information (PMI) scores for each concept/feature
pair to assess the relative saliency of each extracted
feature, with a view to downweighting common but
less interesting features. To speed up this and later
stages, we calculate both statistics for the top 1,000
triples extracted for each concept only.
PMI was proposed by Church and Hanks (1990)
to estimate word association. We will use it to mea-
sure the strength of association between a concept
and its feature. We hope that emphasising concept-
feature pairs with high mutual information will ren-
der our triples more relevant/informative.
We also employ the LL measure across our set of
concept-feature pairs. Proposed by Dunning (1993),
LL is a measure of the distribution of linguistic phe-
nomena in texts and has been used to contrast the
relative corpus frequencies of words. Our aim is to
highlight features which are particularly distinctive
for a given concept, and hence likely to be features
of that concept alone.
We calculate an overall score for a triple, t, by a
weighted combination of the triple?s SVM, PMI and
LL scores using the following formula:
score(t) = ?PMI?PMI(t)+?LL?LL(t)+?SVM?SVM(t)
where the PMI, SVM and LL scores are normalised
so they are in the range [0, 1]. The relative ? weights
thus give an estimate of the three measures? impor-
tance relative to one another and allows us to gauge
which combination of these scores is optimal.
2.9 Datasets
We also wanted to ascertain the extent to which
the output from both our corpora could be combined
to improve results, balancing the encyclopedic but
somewhat specific nature of Wikipedia with the gen-
erality and breadth of the UKWAC corpus. We com-
bined the output by summing individual SVM scores
of each triple from both corpora to yield a combined
SVM score. PMI and LL scores were then calcu-
lated as usual from this combined set of triples.
3 Experimental Evaluation
3.1 Evaluation methodology
We employ ten-fold cross-validation to ascertain
optimal SVM, LL and PMI ? parameters for our fi-
nal system. We exclude 44 concepts from our set of
16
Relation Prec. Recall F
Kelly et al Without 0.1943 0.3896 0.2592With 0.1102 0.2210 0.1471
ReVerb Without 0.1142 0.2258 0.1514With 0.0431 0.0864 0.0576
Our method Without 0.2417 0.4847 0.3225With 0.1238 0.2493 0.1654
Table 4: Our best scores on the ESSLLI set compared to
Kelly et al (2010) and the ReVerb system (Etzioni et al,
2011). Our results are from the verb-augmented vector-
type, using the combined UKWAC-Wikipedia corpus and
using the ? parameters highlighted in Table 3.
510 to use in our final system testing and split the
remaining 466 concepts randomly and evenly into
10 folds. We apply the training steps above to nine
of the folds, generating predictions for the single
held-out fold. We repeat this for all ten folds, yield-
ing relations and features with SVM, LL and PMI
scores for our full set of 466 training concepts on
the UKWAC, Wikipedia and combined corpora.
We varied the ? values from our scoring equa-
tion in the range [0,1] (interval 0.05) and com-
pared the top twenty triples for each concept directly
against the held-out training set. The best F-scores
and their corresponding ? values (evaluating on full
triples and concept-feature pairs alone) are in Ta-
ble 3. We can see that our best results employ the
verb-augmented vector-type and the combined cor-
pus, with a best F-score of 0.2859 when ignoring
the relation term and 0.1494 when including it in the
evaluation. The main difference between these two
results is the relative contribution of the reweighting
factors: the SVM score is the most important over-
all, but the LL and PMI scores come into play when
evaluating without the relation. This could be ex-
plained by the fact that the PMI and LL scores do
not use any relation terms in their calculations.
3.2 Quantitative evaluation
The unseen subset of the McRae norms is a set
of human-generated common-sense properties with
which our extracted properties can be compared.
However, an issue with the McRae norms is that
semantically identical properties can be represented
by lexically different triples. This problem was ac-
knowledged by Baroni et al (2008) who created
a synonym-expanded set of properties for 44 con-
cepts (selected evenly across six semantic classes;
the 44 concepts we excluded for testing) to par-
Judge Judge
turtle A B bowl A B
is green c c is large p p
is small c c used for food c c
is species c c used for mixing c c
is marine c c used for storing food c c
used for sea r r used for storing soup r r
is animal c c is ceramic c c
is many p c is small p p
has shell c c used for storing cereal r r
is large c p used for storing spoon r r
is reptile c c used for storing sugar p c
Table 5: Our judges? assessments of the correctness of the
top ten relation/feature pairs for two concepts extracted
from our best system.
tially solve it. This expansion set comprises the con-
cepts? top ten properties from the McRae norms with
semi-automatically generated synonyms for each of
the ten distinct features. For example, the triple
turtle has shell was expanded to also include tur-
tle has shield and turtle has carapace.
We use the two best systems (i.e., including and
excluding the relation; highlighted in Table 3) to
generate two sets of top twenty output triples for
our 44 concepts. We then calculate precision, re-
call and F-scores for each against our synonym-
expanded set.3 Using this expanded set alows us
to compare our work with that of Kelly et al (2010).
We also compare with the top twenty output of the
Reverb system Etzioni et al (2011) using their pub-
licly available relations derived from the ClueWeb09
corpus, employing their normalized triples ranked
by frequency. All sets of results are in Table 4. We
note that even though Kelly et al optimised their
algorithm on the ESSLLI set to yield a theoretical
best-possible score?we are evaluating ?blind??our
performance still shows an advance on theirs: the
improvement on both sets when comparing the pop-
ulation of F-scores across all 44 concepts is statisti-
cally significant at the 0.5% level.4
3.3 Human evaluation
The above does not quite offer the full picture:
unlike the features, the relations are not synonym-
expanded. Furthermore, it is possible that there
3We note that we are incorporating an upper bound for pre-
cision of 0.500 by comparing with only the top ten properties.
4Paired t-tests. ?With relation?: t = 3.524, d.f.= 43, p =
0.0010. ?Without relation?: t = 3.503, d.f.= 43, p = 0.0011.
17
Relation A B ? Agreements
With c / p 146 161 0.7421 261 (87%)
r / w 153 138
Without c / p 226 235 0.5792 255 (85%)
r / w 74 65
Table 6: Inter-annotator agreement for our best system,
both including and excluding the relation.
are correct properties being generated which simply
don?t appear in the ESSLLI evaluation set.
In order to address these concerns, we also per-
formed a human evaluation on 15 of our concepts.5
We asked two native English-speaking judges to de-
cide whether a given triple was correct,6 plausible,7
wrong but related,8 or wrong.9 We executed the
human evaluation on our two best systems (as de-
scribed above). As there were shared triples and
concept-feature pairs across the two output sets,
each triple and pair was evaluated only once. The
judges were aware of the purposes of the study but
were blind to the source sets. Some example judge-
ments are in Table 5.
The agreement results across all 15 concepts to-
gether with their ? coefficients (Cohen, 1960) are
in Table 6. In our evaluation we conflated the cor-
rect/plausible and wrong but related/wrong cate-
gories (see also Kelly et al (2010) and Devereux et
al. (2010)). We did this because of the subjective na-
ture of the judgements, and because we are seeking
properties which are indeed correct or at least plausi-
ble. These results indicate that our system is extract-
ing correct or plausible triples 51.1% of the time (ris-
ing to 76.8% when considering features only). They
also demonstrate a marked discrepancy between the
results for our two evaluations, reflecting the neces-
sity of human evaluation when assessing our partic-
ular task.
4 Discussion
In this paper we have shown that semi-supervised
learning techniques can automatically learn lexico-
5The 44 evaluation concepts had been separated into super-
ordinate categories for unrelated psycholinguistic research and
we selected our 15 proportionally and at random from these su-
perordinate categories.
6A correct, valid, feature.
7A triple which is plausible but only in a specific set of cir-
cumstances or a feature which was correct but very general.
8The triple is incorrect but there existed some sort of rela-
tionship between the concept and relation and/or feature.
9When the triple is simply wrong.
syntactic patterns indicative of property norm-like
relations and features. Using these patterns, our
system can extract relevant and accurate properties
from any parsed corpus and allows for multi-word
relation labels, allowing greater semantic precision.
As already mentioned, the work of Baroni et
al. (2009) is relevant to our own. Their approach
achieves a precision score of 0.239 on the top ten
returned features evaluated against the ESSLLI set:
our best system offers precision of 0.370 on the same
evaluation. Moreover, Baroni et al do not explicitly
derive relation terms. We better the performance of
a comparable system (Kelly et al, 2010), even when
evaluating against an unseen set of concepts, and our
system does not use manually-generated rules or se-
mantic information. Furthermore, human evaluation
shows over half of our extracted properties are cor-
rect/plausible.
For future work, we have already mentioned that
we are ignoring a large amount of potentially in-
structive training data, specifically those GR-POS
paths in our corpus which don?t terminate on one of
our training features, as well as those paths through
sentences containing one of our concepts but none
of our training features. It might therefore be worth-
while investigating the use of this ?negative? infor-
mation. Another potential avenue for exploration
would be the expansion of the learning vector-types.
Although we already use a significant number of
learning attributes (an average of 37.9 per training
pattern), we could include more: there may be addi-
tional information not directly on the GR-POS path
linking a concept and feature (e.g., nodes adjacent
to said path) which might be indicative of their re-
lationship. We would also consider using active-
learning, introducing a feedback loop and human-
annotation to better distinguish between relations
which our algorithm tends to classify incorrectly.
For example, we could supplement input pattern
data with disambiguating POS-GR graphs, drawing
a distinction between valid and non-valid relations.
Finally, our system could also be evaluated in the
context of a psycholinguistic experiment. For exam-
ple, we could use our system output to predict con-
cept similarity by using our extracted triples to cre-
ate vector representations of each concept, calculat-
ing the distance between those vectors and compar-
ing these similarity ratings with human judgements.
18
Acknowledgements
This research was supported by EPSRC grant
EP/F030061/1. We are grateful to McRae and col-
leagues for making their norms publicly available,
and to the anonymous reviewers for their helpful in-
put.
References
M. Andrews, G. Vigliocco, and D. Vinson. 2005.
Integrating attributional and distributional informa-
tion in a probabilistic model of meaning representa-
tion. In Timo Honkela et al, editor, Proceedings of
AKRR?05, International and Interdisciplinary Confer-
ence on Adaptive Knowledge Representation and Rea-
soning, pages 15?25, Espoo, Finland: Helsinki Uni-
versity of Technology.
M. Baroni, S. Evert, and A. Lenci, editors. 2008. ESSLLI
2008 Workshop on Distributional Lexical Semantics.
M. Baroni, B. Murphy, Barbu E., and Poesio M. 2009.
Strudel: A corpus-based semantic model based on
properties and types. Cognitive Science, pages 1?33.
S. Bird. 2006. NLTK: The natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, pages 69?72. Association for Com-
putational Linguistics.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
S. Clark and J.R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4):493?552.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and psychological measurement.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273?297.
K. Crammer and Y. Singer. 2002. On the algorithmic
implementation of multiclass kernel-based vector ma-
chines. The Journal of Machine Learning Research,
2:265?292.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Annual Meeting-Association
For Computational Linguistics, volume 45, page 232.
B. Devereux, N. Pilkington, T. Poibeau, and A. Korho-
nen. 2010. Towards unrestricted, large-scale acquisi-
tion of feature-based conceptual representations from
corpus data. Research on Language & Computation,
pages 1?34.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational linguistics,
19(1):61?74.
O. Etzioni, A. Fader, J. Christensen, S. Soderland, and
M.T. Center. 2011. Open information extraction: The
second generation. In Twenty-Second International
Joint Conference on Artificial Intelligence.
M.J. Farah and J.L. McClelland. 1991. A computational
model of semantic memory impairment: Modality
specificity and emergent category specificity. Journal
of Experimental Psychology: General, 120(4):339?
357.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini.
2008. Introducing and evaluating ukwac, a very large
web-derived corpus of english. In Proceedings of the
4th Web as Corpus Workshop (WAC-4) Can we beat
Google, pages 47?54.
P. Garrard, M.A.L. Ralph, J.R. Hodges, and K. Patterson.
2001. Prototypicality, distinctiveness, and intercorre-
lation: Analyses of the semantic attributes of living
and nonliving concepts. Cognitive Neuropsychology,
18(2):125?174.
J. Gime?nez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector ma-
chines. In In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Citeseer.
S. Godbole, S. Sarawagi, and S. Chakrabarti. 2002. Scal-
ing multi-class support vector machines using inter-
class confusion. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 513?518. ACM.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics-Volume 2,
pages 539?545. Association for Computational Lin-
guistics.
T. Joachims, C. Nedellec, and C. Rouveirol. 1998. Text
categorization with support vector machines: learning
with many relevant. In Machine Learning: ECML-
98 10th European Conference on Machine Learning,
Chemnitz, Germany, pages 137?142. Springer.
T. Joachims. 1999. Svmlight: Support vector machine.
SVM-Light Support Vector Machine http://svmlight.
joachims. org/, University of Dortmund, 19.
C. Kelly, B. Devereux, and A. Korhonen. 2010. Ac-
quiring human-like feature-based conceptual represen-
tations from corpora. In First Workshop on Computa-
tional Neurolinguistics, page 61. Citeseer.
K. McRae, G.S. Cree, M.S. Seidenberg, and C. McNor-
gan. 2005. Semantic feature production norms for
a large set of living and nonliving things. Behav-
ioral Research Methods, Instruments, and Computers,
37:547?559.
19
D. Meyer, F. Leisch, and K. Hornik. 2003. The support
vector machine under test. Neurocomputing, 55(1-
2):169?186.
G. Murphy. 2002. The big book of concepts. The MIT
Press, Cambridge, MA.
P. Pantel and M. Pennacchiotti. 2008. Automatically har-
vesting and ontologizing semantic relations. In Pro-
ceeding of the 2008 conference on Ontology Learning
and Population: Bridging the Gap between Text and
Knowledge, pages 171?195. IOS Press.
B. Randall, H.E. Moss, J.M. Rodd, M. Greer, and L.K.
Tyler. 2004. Distinctiveness and correlation in con-
ceptual structure: Behavioral and computational stud-
ies. Journal of Experimental Psychology Learning
Memory and Cognition, 30(2):393?406.
K.I. Taylor, B.J. Devereux, K. Acres, B. Randall, and
L.K. Tyler. 2011. Contrasting effects of feature-based
statistics on the categorisation and basic-level identifi-
cation of visual objects. Cognition.
L.K. Tyler, H.E. Moss, M.R. Durrant-Peatfield, and J.P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
A. Vinokourov, J. Shawe-Taylor, and N. Cristianini.
2003. Inferring a semantic representation of text via
cross-language correlation analysis. Advances in neu-
ral information processing systems, 15:1473?1480.
F. Wu and D.S. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118?127. Association for Computational
Linguistics.
20
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 75?83,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Concreteness and Corpora: A Theoretical and Practical Analysis  
 
Felix Hill 
Computer Laboratory 
University of Cambridge 
fh295@cam.ac.uk 
Douwe Kiela 
Computer Laboratory 
University of Cambridge 
dlk427@cam.ac.uk 
Anna Korhonen 
Computer Laboratory 
University of Cambridge 
alk23@cam.ac.uk 
 
  
Abstract 
An increasing body of empirical evidence 
suggests that concreteness is a fundamental 
dimension of semantic representation. By im-
plementing both a vector space model and a 
Latent Dirichlet Allocation (LDA) Model, we 
explore the extent to which concreteness is re-
flected in the distributional patterns in corpora.  
In one experiment, we show that that vector 
space models can be tailored to better model 
semantic domains of particular degrees of 
concreteness.   In a second experiment, we 
show that the quality of the representations of 
abstract words in LDA models can be im-
proved by supplementing the training data 
with information on the physical properties of 
concrete concepts.  We conclude by discussing 
the implications for computational systems 
and also for how concrete and abstract con-
cepts are represented in the mind 
1 Introduction 
A growing body of theoretical evidence empha-
sizes the importance of concreteness to semantic 
representations.  This fact has not been widely 
exploited in NLP systems, despite its clear theo-
retical relevance to tasks such as word-sense in-
duction and compositionality modeling.  In this 
paper, we take a first step towards integrating 
concreteness into NLP by testing the extent to 
which it is reflected by the superficial (distribu-
tional) patterns in corpora.  The motivation is 
both theoretical and practical: We consider the 
implications for the development of computa-
tional systems and also for how concrete and ab-
stract concepts are represented in the human 
mind.  Experimenting with two popular methods 
of extracting lexical representations from text, 
we show both that these approaches are sensitive 
to concreteness and that their performance can be 
improved by adapting their implementation to 
the concreteness of the domain of application.  In 
addition, our findings offer varying degrees of 
support to several recent proposals about concep-
tual representation.   
In the following section we review recent 
theoretical and practical work. In Section 3 we 
explore the extent to which concreteness is re-
flected by Vector-Space Models of meaning 
(VSMs), and in Section 4 we conduct a similar 
analysis for (Bayesian) Latent Dirichlet Alloca-
tion (LDA) models.   We conclude, in Section 5, 
by discussing practical and theoretical implica-
tions.     
2 Related work 
2.1 Concreteness 
Empirical evidence indicates important cognitive 
differences between abstract concepts, such as 
guilt or obesity, and concrete concepts, such as 
chocolate or cheeseburger.  It has been shown 
that concrete concepts are more easily learned 
and remembered than abstract concepts, and that 
language referring to concrete concepts is more 
easily processed (Schwanenflugel, 1991).  There 
are cases of brain damage in which either ab-
stract or concrete concepts appear to be specifi-
cally impaired (Warrington, 1975), and function-
al magnetic resonance imaging (fMRI) studies 
implicate overlapping but partly distinct neural 
systems in the processing of the two concept 
types (Binder et al, 2005).  Further, there is in-
creasing evidence that concrete concepts are 
represented via intrinsic properties whereas ab-
stract representations encode extrinsic relations 
to other concepts (Hill et al, in press). However, 
while these studies together suggest that con-
creteness is fundamental to human conceptual 
representation, much remains to be understood 
about the precise cognitive basis of the ab-
stract/concrete distinction.  Indeed, the majority 
of theoretically motivated studies of conceptual 
representation focus on concrete domains, and 
75
comparatively little has been established empiri-
cally about abstract concepts. 
Despite this support for the cognitive impor-
tance of concreteness, its application to computa-
tional semantics has been limited to date.  One 
possible reason for this is the difficulty in mea-
suring lexical concreteness using corpora alone 
(Kwong, 2008).  Turney et al (2011) overcome 
this hurdle by applying a semi-supervised me-
thod to quantify noun concreteness.  Using this 
data, they show that a disparity in the concrete-
ness between elements of a construction can faci-
litate metaphor identification. For instance, in the 
expressions kill the process or black comedy, a 
verb or adjective that generally occurs with a 
concrete argument takes an abstract argument. 
Turney et al show that a supervised classifier 
can exploit this effect to correctly identify 79% 
of adjective-noun and verb-object constructions 
as literal or metaphorical.  Although these results 
are clearly promising, to our knowledge Turney 
et al?s paper is unique in integrating corpus-
based methods and concreteness in NLP systems.   
1.2 Association / similarity 
A proposed distinction between abstract and 
concrete concepts that is particularly important 
for the present work relates to the semantic rela-
tions association and (semantic) similarity (see 
e.g. Crutch et al 2009; Resnik, 1995). The dif-
ference between these relations is exemplified by 
the concept pairs {car, petrol} and {car, van}.  
Car is said to be (semantically) similar to van, 
and associated with (but not similar to) petrol.  
Intuitively, the basis for the similarity of car and 
bike may be their common physical features 
(wheels) or the fact that they fall within a clearly 
definable category (modes of transport).  In con-
trast, the basis for the association between car 
and petrol may be that they are often found to-
gether or the clear functional relationship be-
tween them.  The two relations are neither mu-
tually exclusive nor independent; bike and car 
are related to some degree by both association 
and similarity.  
Based on fresults of behavioral experiments, 
Crutch et al (2009) make the following proposal 
concerning how association and similarity inte-
ract with concreteness: 
 
(C) The conceptual organization of abstract con-
cepts is governed by association, whereas the 
organization of concrete concepts is governed by 
similarity.   
 
Crutch et al?s hypothesis derives from experi-
ments in which participants selected the odd-one-
out from lists of five words appearing on a 
screen. The lists comprised either concrete or 
abstract words (based on ratings of six infor-
mants) connected either by similarity (e.g. dog, 
wolf, fox etc.; theft, robbery, stealing etc.) or 
association (dog, bone, collar etc.; theft, law, vic-
tim etc.), with an unrelated odd-one-out item in 
each list. Controlling for frequency and position, 
subjects were both significantly faster and more 
accurate if the related words were either abstract 
and associated or concrete and similar. These 
results support (C) on the basis that decision 
times are faster when the related items form a 
more coherent group, rendering the odd-one out 
more salient.  Hill et al (in press) tested the same 
hypothesis on a larger scale, analyzing over 
18,000 concept pairs scored by human annotators 
for concreteness as well as the strength of associ-
ation between them.  They found a moderate in-
teraction between concreteness and the correla-
tion between association strength and similarity 
(as measured using WordNet), but concluded 
that the strength of the effect was not sufficiently 
strong to either confirm or refute (C). 
Against this backdrop, the present work ex-
amines how association, similarity and concrete-
ness are reflected in LDA models and, first, 
VSMs.  In both cases we test Hypothesis (C) and 
related theoretical proposals, and discuss whether 
these findings can lead to better performing se-
mantic models.   
3 Vector Space Models 
Vector space models (VSMs) are perhaps the 
most common general method of extracting se-
mantic representations from corpora (Sahlgren, 
2006; Turney & Pantel, 2010).  Words are 
represented in VSMs as points in a (geometric) 
vector space. The dimensions of the space cor-
respond to the model features, which in the sim-
plest case are high frequency words from the 
corpus.  In such models, the position of a word 
representation along a given feature dimension 
depends on how often that word occurs within a 
specified proximity to tokens of the feature word 
in the corpus.  The exact proximity required is an 
important parameter for model implementation, 
and is referred to as the context window.  Finally, 
the degree to which two word representations are 
related can be calculated as some function of the 
distance between the corresponding points in the 
semantic space.   
76
3.1 Motivation 
VSMs are well established as a method of quan-
tifying relations between word concepts and have 
achieved impressive performance in related NLP 
tasks (Sahlgren, 2006; Turney & Pantel, 2010).  
In these studies, however, it is not always clear 
exactly which semantic relation is best reflected 
by the implemented models.  Indeed, research 
has shown that by changing certain parameter 
settings in the standard VSM architecture, mod-
els can be adapted to better reflect one relation 
type or another.  Specifically, models with 
smaller context windows are reportedly better at 
reflecting similarity, whereas models with larger 
windows better reflect association. (Agirre et al, 
2009; Peirsman et al, 2008) 
Our experiments in this section aim first to 
corroborate these findings by testing how models 
of varying context window sizes perform on em-
pirical data of both association and similarity.  
We then test if this effect differentially affects 
performance on concrete and abstract words.   
3.2 Method  
We employ a conventional VSM design, extract-
ing representations from the (unlemmatised) 
British National Corpus (Leech et al, 1994) with 
stopwords removed.   In the vector representation 
of each noun, our dimension features are the 
50,000 most frequently occurring (non-
stopword) words in the corpus.    We experiment 
with window sizes of three, five and nine (one, 
two and four words either side of the noun, 
counting stopwords).  Finally, we apply point-
wise mutual information (PMI) weighting of our 
co-occurrence frequencies, and measure similari-
ty between weighted noun vectors by the cosine 
of the angle between them in the vector space.    
To evaluate modeling of association, we use 
the University of South Florida (USF) Free-
association Norms (Nelson & McEvoy, 2012).  
The USF data consist of over 5,000 words paired 
with their free associates.  To elicit free asso-
ciates, more than 6,000 participants were pre-
sented with cue words and asked to ?write the 
first word that comes to mind that is meaningful-
ly related or strongly associated to the presented 
word?.  For a cue word c and an associate a, the 
forward association strength (association) from 
c to a is the proportion of participants who pro-
duced a when presented with c.  association is 
thus a measure of the strength of an associate 
relative to other associates of that cue.  The USF 
data is well suited to our purpose because many 
cues and associates in the data have a concrete-
ness score, taken from either the norms of Paivio, 
Yuille and Madigan (1968) or Toglia and Battig 
(1978).  In both cases contributors were asked to 
rate words based on a scale of 1 (very abstract) to 
7 (very concrete).1  We extracted the all 2,230 
nouns from the USF data for which concreteness 
scores were known, yielding a total of 15,195 
noun-noun pairs together with concreteness and 
association values.   
Although some empirical word-similarity da-
tasets are publically available, they contain few if 
any abstract words (Finkelstein et al, 2002; Ru-
benstein & Goodenough, 1965).  Therefore to 
evaluate similarity modeling, we use Wu-Palmer 
Similarity (similarity) (Wu & Palmer, 1994), a 
word similarity metric based on the position of 
the senses of two words in the WordNet taxono-
my (Felbaum, 1998).  similarity can be applied to 
both abstract and concrete nouns and achieves a 
high correlation, with human similarity judg-
ments (Wu & Palmer, 1994).2     
3.3 Results 
In line with previous studies, we observed that 
VSMs with smaller window sizes were better 
able to predict similarity.  The model with win-
dow size 3 achieves a higher correlation with 
similarity (Spearman rank rs  = -0.29) than the 
model with window size 9 (rs  = -0.25).  Howev-
er, the converse effect for association was not 
observed: Model correlation with association 
was approximately constant over all window siz-
es.  These effects are illustrated in Fig. 1.  
 
                                                 
1Although concreteness is well understood intuitively, it 
lacks a universally accepted definition.  It is often described 
in terms of reference to sensory experience (Paivio et al, 
1968), but also connected to specificity; rose is often consi-
dered more concrete than flora.  The present work does not 
address this ambiguity.     
2 similarity achieves a Pearson correlation of r  = .80 on 
the  30 concrete word pairs in the Miller & Charles (1991) 
data.   
0.123 0.125
0.12
0.286
0.29
0.241
0.0
0.1
0.2
0.3
3 5 9
Window
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
77
Figure 1:  Spearman correlations between VSM out-
put and association and similarity for different win-
dow sizes. 
 
In addressing the theoretical Hypothesis (C) we 
focused on the output of our VSM of window 
size five, although the same trends were ob-
served over all three models.  Over all 18,195 
noun-noun pairs the correlation between the 
model output and association was significant (rs  
= 0.13, p < 0.001) but notably lower than the cor-
relation with similarity (rs  = -0.29, p < 0.001).  
To investigate the effect of concreteness, we 
ranked each pair in our sample by the total con-
creteness of both nouns, and restricted our analy-
sis to the 1000 most concrete and 1000 most ab-
stract pairs.  The models captured association 
better over the abstract pairs than concrete con-
cepts, but reflected similarity better over the con-
crete concepts.  The strength of this effect is illu-
strated in Fig. 2.   
 
 
Figure 2: Spearman correlation values between VSM 
output and similarity and association over subsets of 
concrete and abstract pairs. 
 
Given that small window sizes are optimal for 
modeling similarity, and that WSMs appear to 
model similarity better over concrete concepts 
than over abstract concepts, we explored whether 
different window sizes were optimal for either 
abstract or concrete word pairs. When comparing 
the model output to association, no interaction 
between window size and concreteness was ob-
served.  However, there was a notable interaction 
when considering performance in modeling simi-
larity.  As illustrated in Fig. 3, performance on 
concrete word pairs is better for smaller window 
sizes, whereas with abstract word pairs a larger 
window size is preferable.   
 
 
Figure 3:  Spearman correlation values between VSM 
output and similarity and association for different 
window sizes over abstract and concrete word pair 
subsets 
3.4 Conclusion 
Our results corroborate the body of VSM re-
search that reports better performance from small 
window sizes in modeling similarity.  A likely 
explanation for this finding is that similarity is a 
paradigmatic relation: Two similar entities can 
be plausibly exchanged in most linguistic con-
texts.  Small context windows emphasize prox-
imity, which loosely reflects structural relation-
ships such as verb-object, ensuring that paradig-
matically related entities score highly.  Models 
with larger context windows cannot discern pa-
radigmatically and syntagmatically related enti-
ties in this way.  The performance of our models 
on the association dataset did not support the 
converse conclusion that larger window sizes 
perform better.  Overall, each of the three models 
was notably better at capturing similarity than 
association.  This suggests that the core architec-
ture of WSMs is not well suited to modeling as-
sociation.  Indeed, ?first order? models that di-
rectly measure word co-occurrences, rather than 
connecting them via features, seem to perform 
better at this task (Chaudhari et al, 2011).  This 
fact is consistent with the view that association is 
a more basic or fundamental semantic relation 
from which other more structured relations are 
derived.  
The fact that the USF association data re-
flects the instinctive first response of participants 
when presented with a cue word is important for 
interpreting the results with respect to Hypothe-
sis (C).  Our findings suggest that VSMs are bet-
ter able to model this data for abstract word pairs 
than for concrete word pairs.  This is consistent 
with the idea that language fundamentally deter-
mines which abstract concepts come to be asso-
ciated or connected in the mind.  Conversely, the 
0.125
0.108
0.215
0.297
0.0
0.1
0.2
0.3
Association Similarity
Relation Type
C
o
r
r
e
l
a
t
i
o
n
Abstract
Concrete
0.202
0.272
0.223
0.254
0.14
0.086
0.104
0.099
0.0
0.1
0.2
3 - Similarity 9 - Similarity 3 - Assoc. 9 - Assoc.
Window size - Relation type
C
o
r
r
e
l
a
t
i
o
n
Abstract
Concrete
78
fact that the model reflects associations between 
concrete words less well suggests that the impor-
tance of extra-linguistic information is lower for 
connecting concrete concepts in this instinctive 
way.  Indeed, it seems plausible that the process 
by which concrete concepts become associated 
involves visualization or some other form of per-
ceptual reconstruction. Consistent with Hypothe-
sis (C), this reconstruction, which is not possible 
for abstract concepts, would naturally reflect si-
milarity to a greater extent than linguistic context 
alone.   
Finally, when modeling similarity, the ad-
vantage of a small window increases as the 
words become more concrete.  Similarity be-
tween concrete concepts is fundamental to cogni-
tive theories involving the well studied notions 
of prototype and categorization (Rosch, 1975; 
Rogers & McClelland, 2003).   In contrast, the 
computation of abstract similarity is intuitively a 
more complex cognitive operation.  Although the 
accurate quantification of abstract similarity may 
be beyond existing corpus-based methods, our 
results suggest that a larger context window 
could in fact be marginally preferable should 
VSMs be applied to this task.   
Overall, our findings show that the design of 
VSMs can be tailored to reflect particular seman-
tic relations and that this in turn can affect their 
performance on different semantic domains, par-
ticularly with respect to concreteness.  In the 
next section, we investigate whether the same 
conclusions should apply to a different class of 
distributional model.        
4 Latent Dirichlet Allocation Models 
LDA models are trained on corpora that are di-
vided into sections (typically documents), ex-
ploiting the principle that words appearing in the 
same document are likely to have similar mean-
ings.  In an LDA model, the sections are viewed 
as having been generated by random sampling 
from unknown latent dimensions, which are 
represented as probability distributions (Dirichlet 
distributions) over words.  Each document can 
then be represented by a probability distribution 
over these dimensions, and by considering the 
meaning of the dimensions, the meaning of the 
document can be effectively characterized.  More 
importantly, because each latent dimension clus-
ters words of a similar meaning, the output of 
such models can be exploited to provide high 
quality lexical representations (Griffiths et al, 
2007).  Such a word representation encodes the 
extent to which each of the latent dimensions 
influences the meaning of that word, and takes 
the form of a probability distribution over these 
dimensions.  The degree to which two words are 
related can then be approximated by any function 
that measures the similarity or difference be-
tween distributions.        
4.1 Motivation 
In recent work, Andrews et al (2009) explore 
ways in which LSA models can be modified to 
improve the quality of their lexical representa-
tions.  They propose that concepts are acquired 
via two distinct information sources: experiential 
data ? the perceptible properties of objects, and 
distributional data ? the superficial patterns of 
language.  To test this hypothesis, Andrews et al 
construct three different LDA models, one 
trained on experiential data, one trained in the 
conventional manner on running text, and one 
trained on the same text but with the experiential 
data appended.  They evaluate the quality of the 
lexical representations in the three models by 
calculating the Kulback-Leibler divergence be-
tween the representation distributions to measure 
how closely related two words are (Kullback & 
Leibler, 1951).  When this data was compared 
with the USF association data, the combined 
model performed better than the corpus-based 
model, which in turn performed better than the 
features-only model.  Andrews et al concluded 
that both experiential and distributional data are 
necessary for the acquisition of good quality lex-
ical representations. 
     As well as suggesting a way to improve the 
performance of LDA models on NLP tasks by 
supplementing the training data, the approach 
taken by Andrews et al may be useful for better 
understanding the nature of the abstract/concrete 
distinction.  In recent work, Hill et al (in press) 
present empirical evidence that concrete con-
cepts are represented in terms of intrinsic fea-
tures or properties whereas abstract concepts are 
represented in terms of connections to other 
(concrete and abstract) concepts.  For example, 
the features [legs], [tail], [fur], [barks] are all 
central aspects of the concrete representation of 
dog, whereas the representation of the abstract 
concept love encodes connections to other con-
cepts such as heart, rose, commitment and hap-
piness etc.  If a feature-based representation is 
understood to be constructed from physical or 
perceptible properties (which themselves may be 
basic or fundamental concrete representations), 
Hill et al?s characterization of concreteness can 
be summarized as follows:  
79
 
(H) Concreteness correlates with the degree to 
which conceptual representations are feature-
based 
 
Because such differences in representation struc-
ture would in turn entail differences in the com-
putation of similarity, (H) is closely related to a 
proposal of Markman and Stilwell (2001; see 
also Gentner & Markman, 2007):  
 
(M) Computing similarity among concrete con-
cepts involves a feature-comparison operation, 
whereas similarity between abstract concepts is 
a structural, analogy-like, comparison.   
 
The findings of Andrews et al do not address 
(H) or (M) directly, for two reasons. Firstly, they 
evaluate their model on a set that includes no 
abstract concepts.  Secondly, they compare their 
model output to association data without testing 
how well it reflects similarity.  In this section we 
therefore reconstruct the Andrews models and 
evaluate how well they reflect both association 
and similarity across a larger set of abstract and 
concrete concepts.   
4.2  Method/materials 
We reconstruct two of the three models devel-
oped by Andrews et al (2009), excluding the 
features-only model because of the present focus 
on corpus-based approaches.  However, while 
the experiential data applied in the Andrews et 
al. combined model was that collected by Vig-
liocco et al (2004), we use the publicly available 
McRae feature production norms (McRae et al, 
2005).  The McRae data consist of 541 concrete 
noun concepts together with features for each 
elicited from 725 participants.  In the data collec-
tion, feature was understood in a very loose 
sense, so that participants were asked to list both 
physical and functional properties of the nouns in 
addition to encyclopedic facts.  However, for the 
present work, we filter out those features that 
were not perceptual properties using McRae et 
al.?s feature classes, leaving a total of 1,285 fea-
ture types, such as [has_claws] and 
[made_of_brass].  The importance of each fea-
ture to the representation of a given concept is 
reflected by the proportion of participants who 
named that feature in the elicitation experiment.  
For each noun concept we therefore extract a 
corresponding probability distribution over fea-
tures. 
The model design and inference are identical 
to those applied by Andrews et al  Our distribu-
tional model contains 250 latent dimensions and 
was trained using a Gibbs Sampling algorithm on 
approximately 7,500 sections of the BNC with 
stopwords removed.3  The combined model con-
tains 350 latent dimensions, and was trained on 
the same BNC data.  However, for each instance 
of one of the 541 McRae concept words, a fea-
ture is drawn at random from the probability dis-
tribution corresponding to that word and ap-
pended to the training data.  The latent dimen-
sions in the combined model therefore corres-
pond to probability distributions both over words 
and over features. This leads to an important dif-
ference between how words come to be related in 
the distributional model and in the combined 
model.  Both models infer connections between 
words by virtue of their occurrence either in the 
same document or in pairs of documents for 
which the same latent dimensions are prominent.  
In the distributional model, it is the words in a 
document that determines which latent dimen-
sions are ultimately prominent, whereas the in 
combined model it is both the words and the fea-
tures in that document.  Therefore, in the com-
bined model, two words can come to be related 
because they occur not only in documents whose 
words are related, but also in documents whose 
features are related.  For words in the McRae 
data, this has the effect of strengthening the rela-
tionship between words with common features.  
More interestingly, because it alters which latent 
dimensions are most prominent for each docu-
ment, it should also influence the relationship 
between words not in the McRae data.   
We evaluate the performance of our models in 
reflecting free association (association) and simi-
larity (similarity).  To obtain test items we rank 
the 18,195 noun-noun pairs from the USF data 
by the product of the two (BNC) word frequen-
cies and select the 5,000 highest frequency pairs.   
4.3 Results 
As expected, the correlation of the combined 
model output with association was greater than 
the correlation of the distributional model output.  
Notably, however, as illustrated in Fig. 4, we 
observed far greater differences between the 
combined and the distributional models when 
comparing to similarity.  Over all noun pairs, the 
addition of features in the combined model im-
                                                 
3 Code for model implementation was taken from Mark 
Andrews : http://www.mjandrews.net/code/index.html  
80
proved the correlation with similarity from 
Spearman rs  =  0.09  to  rs  =  0.15.   
 
Figure 4:  Spearman correlations between distribu-
tional and combined model outputs, similarity and 
association  
 
In order to address Hypothesis (C) (Section 2.2), 
we analyzed the output of the combined model 
on subsets of the 1000 most abstract and concrete 
word pairs in our data as before.  Perhaps surpri-
singly, as shown in Fig. 5, when comparing with 
similarity, the model performed better over ab-
stract pairs, whereas when comparing with asso-
ciation the model performed better over concrete 
pairs.  However, when these concrete pairs were 
restricted to those for which at least one of the 
two words was in the McRae data, and hence to 
which features had been appended in the corpus, 
the ability of the model to reflect similarity in-
creased significantly.          
 
Figure 5:  Spearman correlations between combined 
model output and similarity and association on differ-
ent word pair subsets  
 
Finally, to address hypotheses (H) and (M) we 
compared the previous analysis of the combined 
model output to the equivalent output from the 
distributional model.  Surprisingly, as shown in 
Fig. 6, the ability of the model to reflect associa-
tion over abstract pairs seemed to reduce with the 
addition of features to the training data.  Never-
theless, in all other cases the combined model 
outperformed the distributional model.  Interes-
tingly, the combined model advantage when 
comparing with similarity was roughly the same 
over both abstract and concrete pairs.  However, 
when these pairs contained at least one word 
from the McRae data, the combined model was 
indeed significantly better at modeling similarity, 
consistent with Hypotheses (M) and (H). 
 
Figure 6:  Comparison between distributional 
model and combined model output correlations with 
similarity and association over different word pair 
subsets 
4.4 Conclusion 
Our findings corroborate the main conclusion of 
Andrews et al, that the addition of experiential 
data improves the performance of the LDA mod-
el in reflecting association.  However, they also 
indicate that the advantage of feature-based LDA 
models is far more significant when the objective 
is to model similarity. 
 The findings are also consistent with, if 
not suggestive of, the theoretical hypotheses (H) 
and (M).  Clearly, the property features in the 
combined model training data enable it to better 
model both similarity and association between 
those concepts to which the features correspond.  
However, this benefit is greater when modeling 
similarity than when modeling association.  This 
suggests that the similarity operation is indeed 
based on features to a greater extent than associa-
tion.  Moreover, this effect is far greater for the 
concrete words for which the features were add-
ed than over the other words pairs we tested.  
Whilst this is not a sound test of hypothesis (H) 
(no attempt was made to add ?features? of ab-
stract concepts to the model), it is certainly con-
sistent with the idea that features or properties 
are a more important aspect of concrete represen-
tations than of abstract representations. 
0.13
0.09
0.14
0.15
0.00
0.05
0.10
0.15
Distributional Combined
Model type
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
0.01
0.16
0.2
0.08
0.14
0.36
0.0
0.1
0.2
0.3
Abstract Concrete McRae
Word pair category
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
0.03
0.093
0.15
0.018
0.11
0.025
0.01
0.16
0.2
0.08
0.14
0.36
0.0
0.1
0.2
0.3
Abstract Concrete 
 Distributional model
McRae Abstract_ Concrete_
 Combined model
McRae_
Word pair category
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
81
Perhaps the most interesting aspect of the 
combined model is how the addition of feature 
information in the training data for certain words 
influences performance on words for which fea-
tures were not added.  In this case, our findings 
suggest that the benefit when modeling similarity 
is marginally greater than when modeling associ-
ation, an observation consistent with Hypothesis 
(M).  A less expected observation is that, be-
tween words for which features were not added, 
the advantage of the combined model over the 
distributional model in modeling similarity was 
equal if not greater for abstract than for concrete 
concepts.  We hypothesize that this is because 
abstract representations naturally inherit any re-
liance on feature information from the concrete 
concepts with which they participate.  In con-
trast, highly concrete representations do not en-
code relations to other concepts and therefore 
cannot inherit relevant feature information in the 
same way.  Under this interpretation, the con-
crete information from the McRae words would 
propagate more naturally to abstract concepts 
than to other concrete concepts.  As a result, the 
highest quality representations in the combined 
model would be those of the McRae words, fol-
lowed by those of the abstract concepts to which 
they closely relate.    
5 Discussion  
This study has investigated how concreteness is 
reflected in the distributional patterns found in 
running text corpora. Our results add to the body 
of evidence that abstract and concrete concepts 
are represented differently in the mind.  The fact 
that VSMs with small windows are particularly 
adept at modeling relations between concrete 
concepts supports the view that similarity go-
verns the conceptual organization of concrete 
concepts to a greater extent than for abstract con-
cepts.  Further, the performance of our LSA 
models on different tasks and across different 
word pairs is consistent with the idea that con-
crete representations are built around features, 
whereas abstract concepts are not.  
More practically, we have demonstrated that 
vector space models can be tailored to reflect 
either similarity or association by adjusting the 
size of the context window.  This in turn indi-
cates a way in which VSMs might be optimized 
to either abstract or concrete domains.  Our expe-
riments with Latent Dirichlet Allocation corrobo-
rate a recent proposal that appending training 
data with perceptible feature or property infor-
mation for a subset of concrete nouns can signif-
icantly improve the quality of the model?s lexical 
representations.  As expected, this effect was 
particularly salient for representations of words 
for which features were appended to the training 
data.  However, the results show that this infor-
mation can propagate to words for which fea-
tures were not appended, in particular to abstract 
words.   
The fact that certain perceptible aspects of 
meaning are not exhaustively reflected in linguis-
tic data is a potentially critical obstacle for cor-
pus-based semantic models.  Our findings sug-
gest that existing machine learning techniques 
may be able to overcome this by adding the re-
quired information for words that refer to con-
crete entities and allowing this information to 
propagate to other elements of language.  In fu-
ture work we aim to investigate specifically 
whether this hypothesis holds for particular parts 
of speech.  For example, we would hypothesize 
that verbs inherit a good degree of their meaning 
from their prototypical nominal arguments.     
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J. 
Pasca, K,. & Soroa,A. 2009. A Study on Similarity 
and Relatedness Using Distributional and WordNet-
based Approaches. In Proceedings of NAACL-HLT 
2009. 
Andrews, M., Vigliocco, G. & Vinson, D. 2009. 
Integrating experiential and distributional data to 
learn semantic represenations. Psychological Review, 
116(3), 463-498. 
Barsalou, L. 1999. Perceptual symbol systems. Be-
havioral and Brain Sciences, 22, 577-609. 
Binder, J., Westbury, C., McKiernan, K., Possing, 
E., & Medler, D. 2005. Distinct brain systems for 
processing concrete and abstract concepts. Journal of 
Cognitive Neuroscience 17(6), 905-917. 
Chaudhari, D., Damani, O., & Laxman, S. 2011. 
Lexical Co-occurrence, Statistical Significance, and 
Word Association. EMNLP 2011, 1058-1068. 
Crutch, S., Connell, S., & Warrington, E. 2009. 
The different representational frameworks underpin-
ning abstract and concrete knowledge: evidence from 
odd-one-out judgments. Quarterly Journal of Experi-
mental Psychology, 62(7), 1377-1388. 
Felbaum, C. 1998. WordNet: An Electronic Lexical 
Database. Cambridge, MA: MIT Press. 
Finkelstein, L., Gabrilovich, Matias, Rivlin, Solan, 
Wolfman & Ruppin. 2002. Placing Search in Context: 
The Concept Revisited. ACM Transactions on Infor-
mation Systems, 20(1):116-131. 
Gentner, D., & Markman, A. 1997. Structure map-
ping in analogy and similarity. American Psycholo-
gist, 52. 45-56. 
82
Griffiths, T., Steyvers, M., & Tenembaum, J. 2007. 
Topics in semantic representation. Psychological Re-
view, 114 (2), 211-244. 
Hill, F., Korhonen, A., & Bentz, C. A quantitative 
empricial analysis of the abstract/concrete distinction. 
Cognitive Science. In press.  
Kullback, S., & Leibler, R.A. 1951. On Informa-
tion and Sufficiency. Annals of Mathematical Statis-
tics 22 (1): 79?86. 
Kwong, O, Y. 2008. A Preliminary study on induc-
ing lexical concreteness from dictionary definitions. 
22nd Pacific Asia Conference on Language, In-
formation and Computation, 235?244. 
Leech, G., Garside, R. & Bryant, R. 1994. Claws4: 
The tagging of the British National Corpus. COL-ING 
94, Lancaster: UK. 
Markman, A, & Stilwell, C. 2001. Role-governed 
categories. Journal of Theoretical and Experimental 
Artificial Intelligence, 13, 329-358. 
McRae, K., Cree, G. S., Seidenberg, M. S., & 
McNorgan, C. 2005. Semantic feature production 
norms for a large set of living and nonliving things. 
Behavior Research Methods, 37, 547-559 
Miller, G., & Charles, W. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive 
Processes, 6(1). 
Nelson, D., & McEvoy, C. 2012. The University of 
South Florida Word Association, Rhyme and Word 
Fragment Norms. Retrieved online from: 
http://web.usf.edu/FreeAssociation/Intro.html. 
Paivio, A., Yuille, J., & Madigan, S. 1968. Con-
creteness, imagery, and meaningfulness values for 
925 nouns. Journal of Experimental Psychology Mo-
nograph Supplement, 76(1, Pt. 2).  
Peirsman, y., Heylen, K. & Geeraerts, D. 2008. 
Size Matters. Tight and Loose Context Definitions in 
English Word Space Models. In Proceedings of the 
ESSLLI Workshop on Distributional Lexical Seman-
tics, Hamburg, Germany 
Resnik, P. 1995. Using Information Content to 
Evaluate Semantic Similarity in a Taxonomy. Pro-
ceedings of IJCAI-95. 
Rogers, T., & McLelland, J. 2003. Semantic Cog-
nition. Cambridge, Mass: MIT Press. 
Rosch, E. 1975. Cognitive representations of se-
mantic categories. Journal of Experimental Psycholo-
gy: General, 104(3), (September 1975), pp. 192?233. 
Rubenstein, H., & Goodenough, J. 1965. Contex-
tual correlates of synonymy. Communications of the 
ACM 8(10), 627-633. 
Sahlgren, M. 2006. The Word-Space Model: Using 
distributional analysis to represent syntagmatic and 
paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, De-
partment of Linguistics, Stockholm University.  
Schwanenflugel, P. 1991.  Why are abstract con-
cepts hard to understand? In P.  Schwanenflugel.  
The psychology of word meanings (pp.  223-250).  
Hillsdale, NJ: Erlbaum. 
Toglia, M., & Battig, W. 1978. Handbook of se-
mantic word norms. Hillsdale, N.J: Erlbaum. 
Turney, P, & Pantel, P. 2010. From frequency to 
meaning: Vector space models of semantics. Journal 
of Artificial Intelligence Research (JAIR), 37, 141-
188. 
Turney,P., Neuman, Y., Assaf,.D, Cohen, Y. 2011. 
Literal and Metaphorical Sense Identification through 
Concrete and Abstract Context. EMNLP 2011: 680-
690 
Vigliocco, G., Vinson, D. P., Lewis, W., & Garrett, 
M. F. 2004. Reprssenting the meanings of object and 
action words: The featural and unitary semantic space 
hypothesis. Cognitive Psychology, 48, 422?488. 
Warrington, E. (1975). The selective impairment of 
semantic memory. Quarterly Journal of Experimental 
Psychology 27(4), 635-657. 
Wu, Z., Palmer, M. 1994. Verb semantics and lexi-
cal selection. In: Proceedings of the 32nd Annual 
Meeting of the Associations for Computational Lin-
guistics. 133?138. 
 
 
83
