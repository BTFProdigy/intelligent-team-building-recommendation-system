Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 67?74,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
A Character n-gram Based Approach for Improved Recall
in Indian Language NER
Praneeth M Shishtla
praneethms
@students.iiit.ac.in
Prasad Pingali
pvvpr
@iiit.ac.in
Vasudeva Varma
vv@iiit.ac.in
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
Abstract
Named Entity Recognition (NER) is the
task of identifying and classifying all proper
nouns in a document as person names, or-
ganization names, location names, date &
time expressions and miscellaneous. Previ-
ous work (Cucerzan and Yarowsky, 1999)
was done using the complete words as fea-
tures which suffers from a low recall prob-
lem. Character n-gram based approach
(Klein et al, 2003) using generative mod-
els, was experimented on English language
and it proved to be useful over the word
based models. Applying the same technique
on Indian Languages, we experimented with
Conditional Random Fields (CRFs), a dis-
criminative model, and evaluated our sys-
tem on two Indian Languages Telugu and
Hindi. The character n-gram based models
showed considerable improvement over the
word based models. This paper describes the
features used and experiments to increase
the recall of Named Entity Recognition Sys-
tems which is also language independent.
1 Introduction
The objective of NER is to classify all tokens in a
text document into predefined classes such as per-
son, organization, location, miscellaneous. NER is
a precursor to many language processing tasks. The
creation of a subtask for NER in Message Under-
standing Conference (MUC) (Chinchor, 1997) re-
flects the importance of NER in Information Extrac-
tion (IE). NER also finds aplication in question an-
swering systems (Toral et al, 2005; Molla et al,
2006), and machine translation (Babych and Hart-
ley, 2003). NER is an essential subtask in organizing
and retrieving biomedical information (Tsai, 2006).
NER can be treated as a two step process
? identification of proper nouns.
? classification of these identified proper nouns.
Challenges in named entity recognition.
Many named entities (NEs) occur rarely in corpus
if at all.
Ambiguity of NEs. Ex Washington can be a per-
son?s name or location.
There are many ways of mentioning the same
NE. Ex: Mahatma Gandhi, M.K.Gandhi, Mohandas
Karamchand Gandhi, Gandhi all refer to the same
person. New Jersey, NJ both refer to the same loca-
tion.
In English, the problem of identifying NEs is solved
to some extent by using the capitalization feature.
Most of the named entities begin with a capital let-
ter which is a discriminating feature for classifying a
token as named entity. In addition to the above chal-
lenges, the complexity of Indian Languages pose
few more problems. In case of Indian languages
there is no concept of capitalization. Ex: The per-
son name Y.S.R (in english) is represented as ysr in
the Indian Languages.
Agglutinative property of the Indian Languages
makes the identification more difficult. For exam-
ple: hyderabad, hyderabad ki, hyderabadki, hyder-
abadlo, hyderabad ni, hyderabad ko etc .. all refer
to the place Hyderabad. where lo, ki, ni are all post-
postion markers in Telugu and ko is a postposition
67
marker in Hindi.
There are many ways of representing acronyms.
The letters in acronyms could be the English alpha-
bet or the native alphabet. Ex: B.J.P and BaJaPa
both are acronyms of Bharatiya Janata Party. In-
dian Languages lack particular standard for forming
acronyms.
Due to these wide variations and the agglutina-
tive nature of Indian languages, probabilistic graph-
ical models result in very less recall. If we are able
to identify the presence of a named entity with a
fairly good amount of accuracy, classification then
can be done efficiently. But, when the machine fails
to identify the presence of named entities, there is
no chance of entity classification because we miss
many of the named entities (less recall which results
in less F-measure,F?=1). So we focus mainly on the
ways to improve the recall of the system. Also, In-
dian Languages have a relatively free word order,
i.e. the words (named entities) can occupy any place
in the sentence. This change in the word position is
compensated using case markers.
2 Related Work & Our Contributions
The state-of-art techniques for Indic lan-
guages(Telugu and Hindi) use word based models
which suffer from low recall, use gazetteers and
are language dependent. As such there is no
NER system for Telugu. Previously (Klein et al,
2003) experimented with character-level models
for English using character based HMM which is
a generative model. We experimented using the
discriminative model for English, Hindi and Telugu.
? We propose an approach that increases the re-
call of Indic languages (even the agglutinative
languages).
? The model is language independent as none of
the language resources is needed.
3 Problem Statement
3.1 NER as sequence labelling task
Named entity recognition (NER) can be modelled
as a sequence labelling task (Lafferty et al, 2001).
Given an input sequence of words W n1 = w1w2w3
...wn, the NER task is to construct a label sequence
Ln1 = l1l2l3 ...ln , where label li either belongs to
the set of predefined classes for named entities or
is none (representing words which are not proper
nouns). The general label sequence ln1 has the high-
est probability of occuring for the word sequence
W n1 among all possible label sequences, that is
?Ln1 = argmax {Pr (Ln1 | W n1 ) }
3.2 Tagging Scheme
We followed the IOB tagging scheme (Ramshaw
and Marcus, 1995) for all the three languages (En-
glish, Hindi and Telugu). In this scheme each line
contains a word at the beginning followed by its
tag. The tag encodes the type of named entity
and whether the word is in the beginning or inside
the NE. Empty lines represent sentence (document)
boundaries. An example of the IOB tagging scheme
is given in Table 1.
Words tagged with O are outside of named entities
Token Named Entity Tag
Dr. B-PER
Talcott I-PER
led O
a O
team O
of O
researchers O
from O
the O
National B-ORG
Cancer I-ORG
Institute I-ORG
Table 1: IOB tagging scheme.
and the I-XXX tag is used for words inside a named
entity of type XXX. Whenever two entities of type
XXX are immediately next to each other, the first
word of the second entity will be tagged B-XXX in
order to show that it starts another entity. This tag-
ging scheme is the IOB scheme originally put for-
ward by Ramshaw and Marcus (Ramshaw and Mar-
cus, 1995).
4 Conditional Random Fields
Conditional Random Fields (CRFs) (Wallach, 2004)
are undirected graphical models used to calculate
68
the conditional probability of values on designated
output nodes given the values assigned to other des-
ignated input nodes. In the special case in which
the output nodes of the graphical model are linked
by edges in a linear chain, CRFs make a first-order
Markov independence assumption, and thus can be
understood as conditionally-trained Finite State Ma-
chines (FSMs).
Let o = ? O1,O2,...OT ? be some observed input
data sequence, such as a sequence of words in text
in a document, (the values on n input nodes of the
graphical model). Let S be a set of Finite State Ma-
chine (FSM) states, each of which is associated with
a label, l ? L .
Let s = ? s1,s2,... sT ,? be some sequence of states,(the
values on T output nodes). By the Hammersley-
Clifford theorem CRFs define the conditional prob-
ability of a state sequence given an input sequence
to be
P(s|o) = 1
Zo
? exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
where Zo is a normalization factor over all state
sequences, is an arbitrary feature function over its ar-
guments, and ?k is a learned weight for each feature
function. A feature function may, for example, be
defined to have value 0 or 1. Higher ? weights make
their corresponding FSM transitions more likely.
CRFs define the conditional probability of a la-
bel sequence based on total probability over the state
sequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is the
sequence of labels corresponding to the labels of the
states in sequence s. Note that the normalization fac-
tor, Zo, (also known in statistical physics as the parti-
tion function) is the sum of the scores of all possible
state sequences,
Zo = ?
s?ST
?exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
and that the number of state sequences is expo-
nential in the input sequence length, T. In arbitrarily-
structured CRFs, calculating the partition function in
closed form is intractable, and approximation meth-
ods such as Gibbs sampling, or loopy belief propa-
gation must be used.
5 Features
There are many types of features used in NER sys-
tems.
Many systems use binary features i.e. the
word-internal features, which indicate the presence
or absence of particular property in the word.
(Mikheev, 1997; Wacholder et al, 1997; Bikel et
al., 1997). Following are examples of commonly
used binary features: All-Caps (IBM), internal
capitalization (eBay), initial capital (Abdul Kalam),
uncapitalized word (can), 2-digit number (83, 73),
4-digit number (1983, 2007), all digits (8, 28, 1273)
etc. The features that correspond to the capitaliza-
tion are not applicable to Indian languages. Also,
we have not used any of the binary features in any
of our models.
Dictionaries: Dictionaries are used to check if a
part of the named entity is present in the dictionary.
These dictionaries are called as gazetteers. The
problem with the Indian languages is that there are
no proper gazetteers in Indian languages.
Lexical features like a sliding window
[w?2,w?1,wo,w1,w2] are used to create a lexi-
cal history view. Prefix and suffix tries were also
used previously (Cucerzan and Yarowsky, 1999).
Linguistics features like Part Of Speech, Chunk,
etc are also used. In our approach we don?t use any
of these language specific (linguistic) information.
5.1 Our Features
In our experiments, we considered and character n-
grams (ASCII characters) as tokens.
For example for the word Vivekananda, the 4-gram
model would result in 8 tokens namely Vive, ivek,
veka, ekan, kana, anan, nand and anda. If our cur-
rent token (w0) is kana
Feature Example
current token: w0 kana
previous 3 tokens: w?3,w?2,w?1 ivek,veka,ekan
next 3 tokens: w1,w2,w3 anan,nand,anda
compound feature: w0 w1 kanaanan
compound feature: w?1 w0 ekankana
In Indian Languages suffixes and other inflections
get attached to the words increasing the length of the
word and reducing the number of occurences of that
word in the entire corpus. The character n-grams
69
can capture these variations. The compound features
also help in capturing such variations. The sliding
window feature helps in guessing the class of the en-
tity using the context. In total 9 features were used
in training and testing. All the features are languge
independent and no binary features are used.
6 Experimental Setup
6.1 Corpus
We conducted the experiments on three languages
namely Telugu, Hindi and English. We collected the
Telugu corpus from Eenadu, a telugu daily news-
paper. The topics included politics, health and
medicine, sports, education, general issues etc. The
annotated corpus had 45714 tokens, out of which
4709 were named entities. We collected the English
corpus from the Wall Street Journal (WSJ) news ar-
ticles. The corpus had 45870 tokens out of which
4287 were named entities. And we collected the
hindi corpus from various sources. The topics in the
corpus included social sciences, biological sciences,
financial articles, religion, etc. The hindi corpus is
not a news corpus. The corpus had 45380 tokens out
of which 3140 were named entities. We evaluated
the hand-annotated corpus once to check for any er-
rors.
6.2 Experiments
We conducted various experiments on Telugu and
Hindi. Also, to verify the correctness of our model
for other languages, we have conducted some ex-
periments on English data also. In this section we
describe the various experiments conducted on the
Telugu, Hindi and English data sets.
We show the average performance of the system
in terms of precision, recall and F-measure for Tel-
ugu, Hindi and English in Table 6 and then for the
impact of training data size on performance of the
system in Table 7 (Telugu), Table 8 (English) and
Table 9 (Hindi). Here, precision measures the num-
ber of correct Named Entities (NEs) in the machine
tagged file over the total number of NEs in the ma-
chine tagged file and the recall measures the number
of correct NEs in the machine tagged file over the to-
tal number of NEs in the golden standard file while
F-measure is the weighted harmonic mean of preci-
sion and recall:
F =
(? 2 + 1) RP
? 2R + P
with
? 2 = 1
where P is Precision, R is Recall and F is F-measure.
Precision Recall F?=1
words 89.66% 29.21% 44.07
n=2 77.36% 46.07% 57.75
n=3 85.45% 52.81% 65.28
n=4 79.63% 48.31% 60.14
n=5 74.47% 39.33% 51.47
n=6 76.32% 32.58% 45.67
Table 2: Precision,Recall and F?=1 measure for Date
& Time expressions in Telugu.
Precision Recall F?=1
words 83.65% 28.71% 42.75
n=2 80.29% 36.30% 50
n=3 78.26% 35.64% 48.98
n=4 81.03% 31.02% 44.87
n=5 75.42% 29.37% 42.28
n=6 53.21% 27.39% 36.17
Table 3: Precision,Recall & F?=1 measure values for
location names in Telugu.
Precision Recall F?=1
words 51.11% 18.70% 27.38
n=2 53.41% 38.21% 44.55
n=3 69.35% 34.96% 46.49
n=4 69.35% 34.96% 46.49
n=5 55.00% 26.83% 36.07
n=6 50.98% 21.14% 29.89
Table 4: Precision,Recall and F?=1 measure values
for organisation names in Telugu.
Table:6 shows the average precison(P),recall(R)
and F-measure(F) values for NEs in Telugu.
Tables 2 to 5 show the P,R,F values for the indi-
vidual categories of NEs in Telugu. Interestingly,
70
Precision Recall F?=1
words 57.32% 18.65% 28.14
n=2 55.77% 34.52% 42.65
n=3 61.04% 37.30% 46.31
n=4 56.92% 29.37% 38.74
n=5 60.50% 28.57% 38.81
n=6 54.21% 23.02% 32.31
Table 5: Precision,Recall and F?=1 measure values
for Person names in Telugu.
though we have not used any of the features per-
taining to years and numbers we have acheived an
appreciable F-measure of 65.28 for date & time ex-
pressions.
In each table the model with the highest F-
measure is higlighted in bold. And, the tri-gram
model performed best in most of the cases except
with locations where bi-gram model performed well.
But, even the tri-gram model (F?=1=48.98) per-
formed close to the bi-gram model ((F?=1=50).
For Hindi, the recall of the n-gram models(Table
6) is more than the word based models but the
amount of increase in recall and F-measure is less.
On examining, we found that the average number of
named entities in the Hindi data were quite less. This
is because the articles for hindi were taken from gen-
eral articles. Whereas in case of English and Telugu,
the corpus was collected from news articles, which
had more probability of having new and more named
entities, which can occur in a similar repeating pat-
tern.
The character n-gram approach showed consider-
able improvement in recall and F-measure (with a
drop in precision) in Telugu and Hindi, which are
agglutinative in nature. In Telugu, there is a differ-
ence of 14.19 and 14.02 in recall and F-measure re-
spectively between the word based model and the
best performing n-gram model (n=3) of size 3. In
Hindi, there is a difference of 2.34 and 2.33 in re-
call and F-measure respectively between the word
based model and the best performing n-gram model
(n=5). Even in case of non-agglutinative language
like English there is a considerable improvement of
1.48 and 1.91 in recall and F-measure respectively
between the word based model and best performing
n-gram model (n=2) of size 2.
In almost all the cases the character based models
performed better in terms of recall and F-measure
than the word based models.
We also experimented changing the training data
size keeping the testing data size unchanged for Tel-
ugu(Table 7) and English(Table 8) and Hindi(Table
9). From Table 7:All the models (words,character
n-gram models) are able to learn as we increase the
training data size. And the recall of the character
n-gram models is considerably more than recall of
the word based model. Also the 3-gram model per-
formed well in almost all the runs. The rate of learn-
ing is more in case of 30K.
From Table 8, in all the runs, the bi-gram char-
acter model constantly performed the best. Also
interestingly the model is able to achieve a least
F-measure of 44.75 with just 10K words of train-
ing data. But, in case of Telugu,(Table 7) an F-
measure of 44+ was reached with training data of
size 35K i.e the learning rate for english is more for
less amount of data. This is due to the reason that
Telugu (Entropy=15.625 bits per character) (Bharati
et al, 1998) is comparitively a high entropy lan-
guage than English (Brown and Pietra, 1992). How-
ever for Hindi, the relative jump in the performance
(compared to Telugu and English)is less. Even the
entropy of Hindi (Entorpy=11.088) (Bharati et al,
1998) is more than English. This is also observed
from the table (Table 10). The numbers in the sec-
ond, third and fourth columns are the number of fea-
tures for English,Telugu and Hindi respectively.
English Telugu Hindi
words 29145 320260 685032
n=2 27707 267340 647109
n=3 45580 680720 1403352
n=4 64284 1162320 1830438
n=5 65248 1359980 1735614
n=6 57297 1278790 1433322
Table 10: Number of features calculated in the word
based model for English,Telugu and Hindi.
7 Conclusion & Future Work
The character based n-gram approach worked bet-
ter than the word based approach even with agglu-
tinative languages. A considerably good NER for
71
Language English Telugu Hindi
Precision Recall F?=1 Precision Recall F?=1 Precision Recall F?=1
Words 92.42% 47.29% 62.56 70.38% 23.83% 35.6 51.66% 36.45% 42.74
n=2 81.21% 68.77% 74.47 65.67% 37.11% 47.42 37.30% 36.06% 36.67
n=3 88.37% 62.45% 73.18 71.39% 38.02% 49.62 54.89% 37.23% 44.37
n=4 93.17% 59.19% 72.39 70.17% 33.07% 44.96 54.67% 37.62% 44.57
n=5 90.71% 58.30% 70.98 66.57% 29.82% 41.19 53.78% 38.79% 45.07
n=6 91.03% 56.14% 69.45 55.68% 25.52% 35 51.79% 36.65% 42.92
Table 6: Average Precision, Recall and F?=1 measure for English, Telugu and Hindi ?n? indicates the number
of n-gram characters
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 58.04 8.46 14.77 56.54 14.06 22.52 67.90 21.48 32.64 71.03 23.31 35.1
n=2 53.81 13.80 21.97 60.31 25.52 35.86 63.68 31.51 42.16 65.16 35.55 46
n=3 68.07 14.71 24.2 64.71 24.35 35.38 70.22 32.55 44.48 71.79 37.11 48.93
n=4 71.23 13.54 22.76 63.42 21.22 31.8 68.14 28.12 39.82 68.16 31.77 43.34
n=4 69.92 11.20 19.3 61.20 19.92 30.06 63.90 26.04 37 66.96 29.30 40.76
n=6 52.38 8.59 14.77 52.70 16.54 25.17 56.13 22.66 32.28 55.16 24.35 33.79
Table 7: Effect of training data size on Average Precision,Recall and F?=1 measure for Telugu.
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 81.84 30.79 44.75 86.54 40.93 55.57 89.04 45.95 60.62 89.80 46.35 61.14
n=2 71.49 42.00 52.92 74.80 58.40 65.59 75.46 61.03 67.49 76.63 61.87 68.46
n=3 76.09 28.85 41.84 81.15 50.03 61.9 81.31 54.28 65.11 82.18 56.84 67.2
n=4 83.42 25.75 39.36 83.35 42.93 56.67 88.01 48.70 62.7 87.40 50.25 63.81
n=5 81.95 25.64 39.06 84.48 41.00 55.21 86.81 44.47 58.81 88.07 47.43 61.66
n=6 79.24 26.89 40.16 83.31 38.18 52.36 89.34 42.88 57.95 87.71 44.32 58.88
Table 8: Effect of training data size on Average Precision,Recall and F?=1 measure for English.
Size 10K 20K 30K 35K
Model P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1 P(%) R(%) F?=1
words 43.13 30.60 35.80 47.97 34.50 40.14 48.67 35.67 41.17 51.92 36.84 43.10
n=2 39.29 30.41 34.29 40.73 34.70 37.47 37.58 36.26 36.90 37.91 36.06 36.96
n=3 48.17 33.33 39.40 50.56 35.28 41.56 47.72 36.65 41.46 50.68 36.06 42.14
n=4 49.18 35.09 40.96 49.21 36.26 41.75 52.14 35.67 42.36 54.87 38.40 45.18
n=5 41.08 34.11 37.27 41.93 33.92 37.50 48.72 37.23 42.21 53.12 39.77 45.48
n=6 41.43 31.58 35.84 44.59 33.72 38.40 46.35 35.87 40.44 50.67 36.84 42.66
Table 9: Effect of training data size on Average Precision,Recall and F?=1 measure for Hindi.
English can be built with less amount of data when
we use character based models and for high entropy
languages large amount of training data is necessary
to build a considerably good NER. We are able to
achieve an F-measure (49.62 for Telugu and 45.07
for Hindi) even without any extra features like regu-
72
lar expressions and gazetteer information. The char-
acter based n-gram models have worked well even
with the discriminative models. A total of 9 features
were used in training and testing. We have not used
any of the language dependent resources and any bi-
nary features. To improve the efficiency of the sys-
tem we plan to experiment with language specific
resources like Part Of Speech (POS) Taggers, Chun-
kers, Morphological analyzers.. etc and also include
some regular expressions and gazetteer information.
References
Bogdan Babych and Anthony Hartley. 2003. Improv-
ing machine translation quality with automatic named
entity recognition.
Akshar Bharati, Prakash Rao K, Rajeev Sangal, and
S.M.Bendre. 1998. Basic statistical analysis of cor-
pus and cross comparison among corpora. Tech-
nical report, International Institute of Information
Technology-Hyderabad(IIIT-H).
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder.
Peter E Brown and Vincent J. Della Pietra. 1992. An
estimate of an upper bound for the entropy of english.
Nancy Chinchor. 1997. Muc-7 named entity task defini-
tion. Technical Report Version 3.5, Science Applica-
tions International Corporation, Fairfax, Virginia.
S. Cucerzan and D. Yarowsky. 1999. Language indepen-
dent named entity recognition combining morphologi-
cal and contextual evidence.
D. Klein, J. Smarr, H. Nguyen, and C. Manning. 2003.
Named entity recognition with character-level models.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrei Mikheev. 1997. Automatic rule induction
for unknown-word guessing. Comput. Linguist.,
23(3):405?423.
Diego Molla, Menno van Zaanen, and Daniel Smith.
2006. Named entity recognition for question answer-
ing. In Proceedings of Australasian Language Tech-
nology Workshop 2006, Sydney, Australia.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question answering
using named entity recognition. In Proceedings of the
10th NLDB congress, Lecture notes in Computer Sci-
ence, Alicante, Spain. Springer-Verlag.
Richard Tzong-Han Tsai. 2006. A hybrid approach to
biomedical named entity recognition and semantic role
labeling. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 243?246, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text.
Hanna M. Wallach. 2004. Conditional random fields: An
introduction. Technical Report MS-CIS-04-21, Uni-
versity of Pennsylvania, Department of Computer and
Information Science, University of Pennsylvania.
73
74
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 105?110,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Experiments in Telugu NER: A Conditional Random Field Approach
Praneeth M Shishtla, Karthik Gali, Prasad Pingali and Vasudeva Varma
{praneethms,karthikg}@students.iiit.ac.in,{pvvpr,vv}@iiit.ac.in
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
Abstract
Named Entity Recognition(NER) is the task
of identifying and classifying tokens in a
text document into predefined set of classes.
In this paper we show our experiments
with various feature combinations for Tel-
ugu NER. We also observed that the prefix
and suffix information helps a lot in find-
ing the class of the token. We also show
the effect of the training data on the perfor-
mance of the system. The best performing
model gave an F?=1 measure of 44.91. The
language independent features gave an F?=1
measure of 44.89 which is close to F?=1
measure obtained even by including the lan-
guage dependent features.
1 Introduction
The objective of NER is to identify and classify all
tokens in a text document into predefined classes
such as person, organization, location, miscella-
neous. The Named Entity information in a document
is used in many of the language processing tasks.
NER was created as a subtask in Message Under-
standing Conference (MUC) (Chinchor, 1997). This
reflects the importance of NER in the area of Infor-
mation Extraction (IE). NER has many applications
in the areas of Natural Language Processing, Infor-
mation Extraction, Information Retrieval and speech
processing. NER is also used in question answer-
ing systems (Toral et al, 2005; Molla et al, 2006),
and machine translation systems (Babych and Hart-
ley, 2003). It is also a subtask in organizing and re-
trieving biomedical information (Tsai, 2006).
The process of NER consists of two steps
? identification of boundaries of proper nouns.
? classification of these identified proper nouns.
The Named Entities(NEs) should be correctly iden-
tified for their boundaries and later correctly classi-
fied into their class. Recognizing NEs in an English
document can be done easily with a good amount
of accuracy(using the capitalization feature). Indian
Languages are very much different from the English
like languages.
Some challenges in named entity recognition that
are found across various languages are: Many
named entities(NEs) occur rarely in the corpus i.e
they belong to the open class of nouns. Ambiguity
of NEs. Ex Washington can be a person?s name or a
place name. There are many ways of mentioning the
same Named Entity(NE). In case of person names,
Ex: Abdul Kalam, A.P.J.Kalam, Kalam refer to the
same person. And, in case of place names Waran-
gal, WGL both refer to the same location. Named
Entities mostly have initial capital letters. This dis-
criminating feature of NEs can be used to solve the
problem to some extent in English.
Indian Languages have some additional chal-
lenges: We discuss the challenges that are specific
to Telugu. Absence of capitalization. Ex: The con-
densed form of the person name S.R.Shastry is writ-
ten as S.R.S in English and is represented as srs in
Telugu. Agglutinative property of the Indian Lan-
guages makes the identification more difficult. Ag-
glutinative languages such as Turkish or Finnish,
Telugu etc. differ from languages like English in
105
the way lexical forms are generated. Words are
formed by productive affixations of derivational and
inflectional suffixes to roots or stems. For example:
warangal, warangal ki, warangalki, warangallo,
warangal ni etc .. all refer to the place Waran-
gal. where lo, ki, ni are all postpostion markers
in Telugu. All the postpositions get added to the
stem hyderabad. There are many ways of represent-
ing acronyms. The letters in acronyms could be the
English alphabet or the native alphabet. Ex: B.J.P
and BaJaPa both are acronyms of Bharatiya Janata
Party. Telugu has a relatively free word order when
compared with English. The morpohology of Tel-
ugu is very complex. The Named Entity Recogni-
tion algorithm must be able handle most of these
above variations which otherwise are not found in
languages like English. There are not rich and robust
tools for the Indian Languages. For Telugu, though
a Part Of Speech(POS) Tagger for Telugu, is avail-
able, the accuracy is less when compared to English
and Hindi.
2 Problem Statement
NER as sequence labelling task
Named entity recognition (NER) can be modelled
as a sequence labelling task (Lafferty et al, 2001).
Given an input sequence of words W n1 = w1w2w3
...wn, the NER task is to construct a label sequence
Ln1 = l1l2l3 ...ln , where label li either belongs to
the set of predefined classes for named entities or
is none(representing words which are not proper
nouns). The general label sequence ln1 has the high-
est probability of occuring for the word sequence
W n1 among all possible label sequences, that is
L?n1 = argmax {Pr (L
n
1 |W
n
1 ) }
3 Conditional Random Fields
Conditional Random Fields (CRFs) (Wallach, 2004)
are undirected graphical models used to calculate the
conditional probability of values on designated out-
put nodes given values assigned to other designated
input nodes. In the special case in which the output
nodes of the graphical model are linked by edges in a
linear chain, CRFs make a first-order Markov inde-
pendence assumption, and thus can be understood as
conditionally-trained finite state machines(FSMs).
Let o = ? O1,O2,...OT ? be some observed input
data sequence, such as a sequence of words in text
in a document,(the values on n input nodes of the
graphical model). Let S be a set of FSM states, each
of which is associated with a label, l ?L .
Let s = ? s1,s2,... sT ,? be some sequence of states,(the
values on T output nodes). By the Hammersley-
Clifford theorem CRFs define the conditional prob-
ability of a state sequence given an input sequence
to be
P(s|o) =
1
Zo
? exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
where Zo is a normalization factor over all state
sequences, is an arbitrary feature function over its ar-
guments, and ?k is a learned weight for each feature
function. A feature function may, for example, be
defined to have value 0 or 1. Higher ? weights make
their corresponding FSM transitions more likely.
CRFs define the conditional probability of a la-
bel sequence based on total probability over the state
sequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is the
sequence of labels corresponding to the labels of the
states in sequence s. Note that the normalization fac-
tor, Zo, (also known in statistical physics as the parti-
tion function) is the sum of the scores of all possible
state sequences,
Zo = ?
s?ST
?exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
and that the number of state sequences is expo-
nential in the input sequence length,T. In arbitrarily-
structure CRFs, calculating the partition function in
closed form is intractable, and approximation meth-
ods such as Gibbs sampling, or loopy belief propa-
gation must be used.
4 Features
There are many types of features used in general
NER systems. Many systems use binary features
i.e. the word-internal features, which indicate the
presence or absence of particular property in the
word. (Mikheev, 1997; Wacholder et al, 1997;
Bikel et al, 1997). Following are examples of
binary features commonly used. All-Caps (IBM),
Internal capitalization (eBay), initial capital (Abdul
Kalam), uncapitalized word (can), 2-digit number
106
(83, 28), 4-digit number (1273, 1984), all digits (8,
31, 1228) etc. The features that correspond to the
capitalization are not applicable to Telugu. We have
not used any binary features in our experiments.
Gazetteers are used to check if a part of the
named entity is present in the gazetteers. We don?t
have proper gazetteers for Telugu.
Lexical features like a sliding window
[w?2,w?1,wo,w1,w2] are used to create a lexi-
cal history view. Prefix and suffix tries were also
used previously(Cucerzan and Yarowsky,1999).
Linguistics features like Part Of Speech, Chunk,
etc are also used.
4.1 Our Features
We donot have a highly accurate Part Of
Speech(POS) tagger. In order to obtain some
POS and chunk information, we ran a POS Tagger
and chunker for telugu (PVS and G, 2007) on the
data. And from that, we used the following features
in our experiments.
Language Independent Features
current token: w0
previous 3 tokens: w?3,w?2,w?1
next 3 tokens: w1,w2,w3
compound feature:w0 w1
compound feature:w?1 w0
prefixes (len=1,2,3,4) of w0: pre0
suffixes (len=1,2,3,4) of w0: su f0
Language Dependent Features
POS of current word: POS0
Chunk of current word: Chunk0
Each feature is capable of providing some infor-
mation about the NE.
The word window helps in using the context in-
formation while guessing the tag of the token. The
prefix and suffix feature to some extent help in cap-
turing the variations that may occur due to aggluti-
nation.
The POS tag feature gives a hint whether the word
is a proper noun. When this is a proper noun it has
a chance of being a NE. The chunk feature helps in
finding the boundary of the NE.
In Indian Languages suffixes and other inflections
get attached to the words increasing the length of the
word and reducing the number of occurences of that
word in the entire corpus. The character n-grams can
capture these variations.
5 Experimental Setup
5.1 Corpus
We conducted the experiments on the developement
data released as a part of NER for South and South-
East Asian Languages (NERSSEAL) Competetion.
The corpus in total consisted of 64026 tokens out
of which 10894 were Named Entities(NEs). We di-
vided the corpus into training and testing sets. The
training set consisted of 46068 tokens out of which
8485 were NEs. The testing set consisted of 17951
tokens out of which 2407 were NEs. The tagset as
mentioned in the release, was based on AUKBC?s
ENAMEX,TIMEX and NAMEX, has the follow-
ing tags: NEP (Person), NED (Designation), NEO
(Organization), NEA (Abbreviation), NEB (Brand),
NETP (Title-Person), NETO (Title-Object), NEL
(Location), NETI (Time), NEN (Number), NEM
(Measure) & NETE (Terms).
5.2 Tagging Scheme
The corpus is tagged using the IOB tagging scheme
(Ramshaw and Marcus, 1995). In this scheme each
line contains a word at the beginning followed by
its tag. The tag encodes the type of named entity
and whether the word is in the beginning or inside
the NE. Empty lines represent sentence(document)
boundaries. An example is given in table 1.
Words tagged with O are outside of named en-
tities and the I-XXX tag is used for words inside a
named entity of type XXX. Whenever two entities
of type XXX are immediately next to each other,
the first word of the second entity will be tagged B-
XXX in order to show that it starts another entity.
This tagging scheme is the IOB scheme originally
put forward by Ramshaw and Marcus (1995).
5.3 Experiments
To evaluate the performance of our Named Entity
Recognizer, we used three standard metrics namely
precision, recall and f-measure. Precision measures
the number of correct Named Entities(NEs) in the
107
Token Named Entity Tag
Swami B-NEP
Vivekananda I-NEP
was O
born O
on O
January B-NETI
, I-NETI
12 I-NETI
in O
Calcutta B-NEL
. O
Table 1: IOB tagging scheme.
machine tagged file over the total number of NEs in
the machine tagged file and the recall measures the
number of correct NEs in the machine tagged file
over the total number of NEs in the golden standard
file while F-measure is the weighted harmonic mean
of precision and recall:
F =
(
? 2+1
)
RP
? 2R+P
with
? = 1
where P is Precision, R is Recall and F is F-measure.
W?n+n: A word window :w?n, w?n+1, .., w?1, w0,
w1, .., wn?1, wn.
POSn: POS nth token.
Chn: Chunk of nth token.
pren: Prefix information of nth token. (prefix
length=1,2,3,4)
su fn: Suffix information of nth token. (suffix
length=1,2,3,4)
The more the features, the better is the perfor-
mance. The inclusion of the word window, prefix
and suffix features have increased the F?=1 mea-
sure significantly. Whenever the suffix feature is
included, the performance of the system increased.
This shows that the system is able to caputure those
agglutinative language variations. We also have ex-
perimented changing the training data size. While
varying the training data size, we have tested the
performance on the same amount of testing data of
17951 tokens.
6 Conclusion & Future Work
The inclusion of prefix and suffix feature helps in
improving the F?=1 measure (also recall) of the sys-
tem. As the size of the training data is increased,
the F?=1 measure is increased. Even without the
language specific information the system is able to
perform well. The suffix feature helped improve the
recall. This is due to the fact that the POS tagger
also uses the same features in predicting the POS
tags. Prefix, suffix and word are three non-linguistic
features that resulted in good performance. We plan
to experiment with the character n-gram approach
(Klein et al, 2003) and include gazetteer informa-
tion.
References
Bogdan Babych and Anthony Hartley. 2003. Improv-
ing machine translation quality with automatic named
entity recognition. In Proceedings of Seventh Inter-
national EAMT Workshop on MT and other language
technology tools, Budapest, Hungary.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
the fifth conference on Applied natural language pro-
cessing, pages 194?201, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Nancy Chinchor. 1997. Muc-7 named entity task defini-
tion. Technical Report Version 3.5, Science Applica-
tions International Corporation, Fairfax, Virginia.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003, pages 180?183, Morristown, NJ,
USA. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrei Mikheev. 1997. Automatic rule induction
for unknown-word guessing. Comput. Linguist.,
23(3):405?423.
108
Features Precision Recall F?=1
Ch0 51.41% 9.19% 15.59
POS0 46.32% 9.52% 15.80
POS0.Ch0 46.63% 9.69% 16.05
W?3+3.Ch0 59.08% 19.50% 29.32
W?3+3.POS0 58.43% 19.61% 29.36
Ch0.pren 53.97% 24.76% 33.95
POS0.pren 53.94% 24.93% 34.10
POS0.Ch0.pren 53.94% 25.32% 34.46
POS0.su fn 47.51% 29.36% 36.29
POS0.Ch0.su fn 48.02% 29.24% 36.35
Ch0.su fn 48.55% 29.13% 36.41
W?3+3.POS0.pren 62.98% 27.45% 38.24
W?3+3.POS0.Ch0.pren 62.95% 27.51% 38.28
W?3+3.Ch0.pren 62.88% 27.62% 38.38
W?3+3.POS0.su fn 60.09% 30.53% 40.49
W?3+3.POS0.Ch0.su fn 59.93% 30.59% 40.50
W?3+3.Ch0.su fn 61.18% 30.81% 40.98
POS0.Ch0.pren.su fn 57.83% 34.57% 43.27
POS0.pren.su fn 57.41% 34.73% 43.28
Ch0.pren.su fn 57.80% 34.68% 43.35
W?3+3.Ch0.pren.su fn 64.12% 34.34% 44.73
W?3+3.POS0.pren.su fn 64.56% 34.29% 44.79
W?3+3.POS0.Ch0.pren.su fn 64.07% 34.57% 44.91
Table 2: Average Precision,Recall and F?=1 measure for different language dependent feature combinations.
Features Precision Recall F?=1
w 57.05% 20.62% 30.29
pre 53.65% 23.87% 33.04
suf 47.75% 29.19% 36.23
w.pre 63.08% 27.56% 38.36
w.suf 60.93% 30.76% 40.88
pre.suf 57.94% 34.96% 43.61
w.pre.suf 64.80% 34.34% 44.89
Table 3: Average Precision,Recall and F?=1 measure for different language independent feature combina-
tions.
Diego Molla, Menno van Zaanen, and Daniel Smith.
2006. Named entity recognition for question answer-
ing. In Proceedings of Australasian Language Tech-
nology Workshop 2006, Sydney, Australia.
Avinesh PVS and Karthik G. 2007. Part-of-speech tag-
ging and chunking using conditional random fields and
transformation based learning. In In Proceedings of
SPSAL-2007 Workshop.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question answering
using named entity recognition. In Proceedings of the
10th NLDB congress, Lecture notes in Computer Sci-
ence, Alicante, Spain. Springer-Verlag.
109
Number of Words Precision Recall F?=1
2500 51.37% 9.47% 15.99
5000 64.74% 11.93% 20.15
7500 61.32% 13.50% 22.13
10000 66.88% 23.31% 34.57
12500 63.42% 27.39% 38.26
15000 63.55% 31.26% 41.91
17500 60.58% 30.64% 40.70
20000 58.32% 30.03% 39.64
22500 57.72% 29.75% 39.26
25000 59.33% 29.92% 39.78
27500 60.91% 30.03% 40.23
30000 62.77% 30.42% 40.98
32500 62.66% 30.64% 41.16
35000 62.08% 30.81% 41.18
37500 61.02% 30.87% 41.00
40000 61.60% 31.09% 41.33
42500 62.12% 32.44% 42.62
45000 62.70% 32.77% 43.05
47500 63.20% 32.72% 43.12
50000 64.29% 34.29% 44.72
Table 4: The effect of training data size on the performance of the NER.
Richard Tzong-Han Tsai. 2006. A hybrid approach to
biomedical named entity recognition and semantic role
labeling. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 243?246, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Nina Wacholder, Yael Ravin, and Misook Choi. 1997.
Disambiguation of proper names in text. In Proceed-
ings of the fifth conference on Applied natural lan-
guage processing, pages 202?208, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
HannaM.Wallach. 2004. Conditional random fields: An
introduction. Technical Report MS-CIS-04-21, Uni-
versity of Pennsylvania, Department of Computer and
Information Science, University of Pennsylvania.
110
Statistical Transliteration for Cross Langauge Information Retrieval using
HMM alignment and CRF
Surya Ganesh, Sree Harsha
LTRC, IIIT
Hyderabad, India
suryag,sreeharshay@students.iiit.net
Prasad Pingali, Vasudeva Varma
LTRC, IIIT
Hyderabad, India
pvvpr,vv@iiit.net
Abstract
In this paper we present a statistical translit-
eration technique that is language indepen-
dent. This technique uses Hidden Markov
Model (HMM) alignment and Conditional
Random Fields (CRF), a discriminative
model. HMM alignment maximizes the
probability of the observed (source, target)
word pairs using the expectation maximiza-
tion algorithm and then the character level
alignments (n-gram) are set to maximum
posterior predictions of the model. CRF
has efficient training and decoding processes
which is conditioned on both source and
target languages and produces globally op-
timal solutions. We apply this technique
for Hindi-English transliteration task. The
results show that our technique perfoms
better than the existing transliteration sys-
tem which uses HMM alignment and con-
ditional probabilities derived from counting
the alignments.
1 Introduction
In cross language information retrieval (CLIR)
a user issues a query in one language to search
a document collection in a different language.
Out of Vocabulary (OOV) words are problematic
in CLIR. These words are a common source of
errors in CLIR. Most of the query terms are OOV
words like named entities, numbers, acronyms and
technical terms. These words are seldom found in
Bilingual dictionaries used for translation. These
words can be the most important words in the query.
These words need to be transcribed into document
language when query and document languages
do not share common alphabet. The practice of
transcribing a word or text written in one language
into another language is called transliteration.
A source language word can have more than
one valid transliteration in target language. For
example for the Hindi word below four different
transliterations are possible .
- gautam, gautham, gowtam, gowtham
Therefore, in a CLIR context, it becomes im-
portant to generate all possible transliterations to
retrieve documents containing any of the given
forms.
Most current transliteration systems use a gen-
erative model for transliteration such as freely
available GIZA++1 (Och and Ney , 2000),an im-
plementation of the IBM alignment models (Brown
et al, 1993). These systems use GIZA++ (which
uses HMM alignment) to get character level
alignments (n-gram) from word aligned data. The
transliteration system was built by counting up the
alignments and converting the counts to conditional
probabilities. The readers are strongly encouraged
to refer to (Nasreen and Larkey , 2003) to have a
detailed understanding of this technique.
In this paper, we present a simple statistical
technique for transliteration. This technique
uses HMM alignment and Conditional Random
Fields (Hanna , 2004) a discriminative model.
Based on this technique desired number of translit-
erations are generated for a given source language
word. We also describe the Hindi-English transliter-
ation system built by us. However there is nothing
particular to both these languages in the system.
We evaluate the transliteration system on a test
set of proper names from Hindi-English parallel
transliterated word lists. We compare the efficiency
of this system with the system that was developed
using HMMs (Hidden Markov Models) only.
1http://www.fjoch.com/GIZA++.html
2 Previous work
Earlier work in the field of Hindi CLIR was done
by Jaleel and Larkey (Larkey et al, 2003). They did
this based on their work in English-Arabic transliter-
ation for cross language Information retrieval (Nas-
reen and Larkey , 2003). Their approach was
based on HMM using GIZA++ (Och and Ney ,
2000). Prior work in Arabic-English translitera-
tion for machine translation purpose was done by
Arababi (Arbabi et al, 1994). They developed a hy-
brid neural network and knowledge-based system to
generate multiple English spellings for Arabic per-
son names. Knight and Graehl (Knight and Graehl
, 1997) developed a five stage statistical model to
do back transliteration, that is, recover the original
English name from its transliteration into Japanese
Katakana. Stalls and Knight (Stalls and Knight ,
1998) adapted this approach for back translitera-
tion from Arabic to English of English names. Al-
Onaizan and Knight (Onaizan and Knight , 2002)
have produced a simpler Arabic/English translitera-
tor and evaluates how well their system can match a
source spelling. Their work includes an evaluation
of the transliterations in terms of their reasonable-
ness according to human judges. None of these stud-
ies measures their performance on a retrieval task or
on other NLP tasks. Fujii and Ishikawa (Fujii and
Ishikawa , 2001) describe a transliteration system
for English-Japanese cross language IR that requires
some linguistic knowledge. They evaluate the ef-
fectiveness of their system on an English-Japanese
cross language IR task.
3 Problem Description
The problem can be stated formally as a se-
quence labelling problem from one language al-
phabet to other. Consider a source language word
x1x2..xi..xN where each xi is treated as a word
in the observation sequence. Let the equivalent
target language orthography of the same word be
y1y2..yi..yN where each yi is treated as a label in
the label sequence. The task here is to generate a
valid target language word (label suquence) for the
source language word (observation sequence).
x1 ?????? y1
x2 ?????? y2
. ??????- .
. ??????- .
. ??????- .
xN ?????? yN
Here the valid target language alphabet(yi) for a
source language alphabet(xi) in the input source
language word may depend on various factors like
1. The source language alphabet in the input
word.
2. The context(alphabets) surrounding source lan-
guage alphabet(xi) in the input word.
3. The context(alphabets) surrounding target lan-
guage alphabet(yi) in the desired output word.
4 Transliteration using HMM alignment
and CRF
Our approach for transliteration is divided into
two phases. The first phase induces character
alignments over a word-aligned bilingual corpus,
and the second phase uses some statistics over the
alignments to transliterate the source language word
and generate the desired number of target language
words.
The selected statistical model for translitera-
tion is based on HMM alignment and CRF. HMM
alignment maximizes the probability of the observed
(source, target) word pairs using the expectation
maximization algorithm. After the maximization
process is complete, the character level alignments
(n-gram) are set to maximum posterior predictions
of the model. This alignment is used to get char-
acter level alignment (n-gram) of source and target
language words. From the character level alignment
obtained we compare each source language charac-
ter (n-gram) to a word and its corresponding target
language character (n-gram) to a label. Conditional
random fields (CRFs) are a probabilistic framework
for labeling and segmenting sequential data. We use
CRFs to generate target language word (similar to
label sequence) from source language word (similar
to observation sequence).
CRFs are undirected graphical models which
define a conditional distribution over a label
sequence given an observation sequence. We
define CRFs as conditional probability distributions
P (Y |X) of target language words given source
language words. The probability of a particular
target language word Y given source language word
X is the normalized product of potential functions
each of the form
e(
?
j
?jtj(Yi?1,Yi,X,i))+(
?
k
?ksk(Yi,X,i))
where tj(Yi?1, Yi, X, i) is a transition feature
function of the entire source language word and the
target language characters (n-gram) at positions i
and i? 1 in the target language word; sk(Yi, X, i) is
a state feature function of the target language word
at position i and the source language word; and ?j
and ?k are parameters to be estimated from training
data.
Fj(Y,X) =
n?
i=1
fj(Yi?1, Yi, X, i)
where each fj(Yi?1, Yi, X, i) is either a state
function s(Yi?1, Yi, X, i) or a transition function
t(Yi?1, Yi, X, i). This allows the probability of a tar-
get language word Y given a source language word
X to be written as
P (Y |X,?) = (
1
Z(X)
)e(
?
?jFj(Y,X))
Z(X) is a normalization factor.
The parameters of the CRF are usually estimated
from a fully observed training data {(x(k), y(k))}.
The product of the above equation over all training
words, as a function of the parameters ?, is known
as the likelihood, denoted by p({y(k)}|{x(k)}, ?).
Maximum likelihood training chooses parameter
values such that the logarithm of the likelihood,
known as the log-likelihood, is maximized. For a
CRF, the log-likelihood is given by
L(?) =
?
k
[log
1
Z(x(k))
+
?
j
?jFj(y
(k), x(k))]
This function is concave, guaranteeing con-
vergence to the global maximum. Maximum
likelihood parameters must be identified using
an iterative technique such as iterative scal-
ing (Berger , 1997) (Darroch and Ratcliff, 1972)
or gradient-based methods (Wallach , 2002).
Finally after training the model using CRF we gen-
erate desired number of transliterations for a given
source language word.
5 Hindi - English Transliteration system
The whole model has three important phases. Two
of them are off-line processes and the other is a run
time process. The two off-line phases are prepro-
cessing the parallel corpora and training the model
using CRF++2. CRF++ is a simple, customizable,
and open source implementation of Conditional
Random Fields (CRFs) for segmenting/labeling se-
quential data. The on-line phase involves generat-
ing desired number of transliterations for the given
Hindi word (UTF-8 encoded).
5.1 Preprocessing
The training file is converted into a format required
by CRF++. The sequence of steps in preprocessing
are
1. Both Hindi and English words were prefixed
with a begin symbol B and suffixed with an end
symbol E which correspond to start and end
states. English words were converted to lower
case.
2. The training words were segmented in to uni-
grams and the English-Hindi word pairs were
aligned using GIZA++, with English as the
source language and Hindi as target language.
3. The instances in which GIZA++ aligned a se-
quence of English characters to a single Hindi
unicode character were counted. The 50 most
frequent of these character sequences were
added to English symbol inventory. There were
hardly any instances in which a sequence of
Hindi unicode characters were aligned to a sin-
gle English character. So, in our model we con-
sider Hindi unicode characters, NULL, En-
glish unigrams and English n-grams.
4. The English training words were re segmented
based on the new symbol inventory, i.e., if
2http://crfpp.sourceforge.net/
a character was a part of an n-gram, it was
grouped with the other characters in the n-
gram. If not, it was rendered separately.
GIZA++ was used to align the above Hindi
and English training word pairs, with Hindi
as source language and English as target lan-
guage.
These four steps are performed to get the char-
acter level alignment (n-grams) for each source
and target language training words.
5. The alignment file from the GIZA++ output
is used to generate training file as required by
CRF++ to work. In the training file a Hindi uni-
code character aligned to a English uni-gram or
n-gram is called a token. Each token must be
represented in one line, with the columns sepa-
rated by white space (spaces or tabular charac-
ters).Each token should have equal number of
columns.
5.2 Training Phase
The preprocessing phase converts the corpus into
CRF++ input file format. This file is used to
train the CRF model. The training requires a tem-
plate file which specifies the features to be selected
by the model. The training is done using Lim-
ited memory Broyden-Fletcher-Goldfarb-Shannon
method(LBFGS) (Liu and Nocedal, 1989) which
uses quasi-newton algorithm for large scale numer-
ical optimization problem. We used Hindi unicode
characters as features for our model and a window
size of 5.
5.3 Transliteration
The list of Hindi words that need to be translit-
erated is taken. These words are converted into
CRF++ test file format and transliterated using the
trained model which gives the top n probable En-
glish words. CRF++ uses forward Viterbi and back-
ward A* search whose combination produce the ex-
act n-best results.
6 Evaluation
We evaluate the two transliteration systems for
Hindi - English that use HMM alignment and CRF
with the system that uses HMM only in two ways. In
first evaluation method we compare transliteration
accuracies of the two systems using in-corpus (train-
ing data) and out of corpus words. In second method
we compare CLIR performance of the two systems
using Cross Language Evaluation Forum (CLEF)
2007 ad-hoc bilingual track (Hindi-English) docu-
ments in English language and 50 topics in Hindi
Language. The evaluation document set consists of
news articles and reports from Los Angeles Times
of 2002. A set of 50 topics representing the informa-
tion need were given in Hindi. A set of human rele-
vance judgements for these topics were generated by
assessors at CLEF. These relevance judgements are
binary relevance judgements and are decided by a
human assessor after reviewing a set of pooled doc-
uments using the relevant document pooling tech-
nique. The system evaluation framework is similar
to the Craneld style system evaluations and the mea-
sures are similar to those used in TREC3.
6.1 Transliteration accuracy
We trained the model on 30,000 words containing
Indian city names, Indian family names, Male first
names and last names, Female first names and last
names. We compare this model with the HMM
model trained on same training data. We tested both
the models using in-corpus (training data) and out
of corpus words. The out of corpus words consist of
both Indian and foreign place names, person names.
We evaluate both the models by considering top 5,
10, 15 and 20 transliterations. Accuracy was calcu-
lated using the following equation below
Accuracy =
C
N
? 100
C - Number of test words with the correct transliter-
ation appeared in the desired number (5, 10, 15, 20,
25) of transliterations.
N - Total number of test words.
The results for 30,000 in-corpus words and 1,000
out of corpus words are shown in the table 1
and table 2 respectively. In below tables 1 & 2
HMM model refers to the system developed using
HMM alignment and conditional probabilities de-
rived from counting the alignments, HMM & CRF
model refers to the system developed using HMM
3Text Retrieval Conferences, http://trec.nist.gov
Model Top 5 Top 10 Top 15 Top 20 Top 25
HMM 74.2 78.7 81.1 82.1 83.0
HMM & CRF 76.5 83.6 86.5 88.9 89.7
Table 1: Transliteration accuracy of the two systems for in-corpus words.
Model Top 5 Top 10 Top 15 Top 20 Top 25
HMM 69.3 74.3 77.8 80.5 81.3
HMM & CRF 72.1 79.9 83.5 85.6 86.5
Table 2: Transliteration accuracy of the two systems for out of corpus words.
alignment and CRF for generating top n translitera-
tions.
CRF models for Named entity recognition, POS
tagging etc. have efficiency in high nineties when
tested on training data. Here the efficiency (Table 1)
is low due to the use of HMM alignment in GIZA++.
We observe that there is a good improvement in
the efficiency of the system with the increase in the
number of transliterations up to some extent(20) and
after that there is no significant improvement in the
efficiency with the increase in the number of translit-
erations.
During testing, the efficiency was calculated by con-
sidering only one of the correct transliterations pos-
sible for a given Hindi word. If we consider all the
correct transliterations the efficiency will be much
more.
The results clearly show that CRF model per-
forms better than HMM model for Hindi to English
transliteration.
6.2 CLIR Evaluation
In this section we evaluate the transliterations pro-
duced by the two systems in CLIR task, the task for
which these transliteration systems were developed.
We tested the systems on the CLEF 2007 documents
and 50 topics. The topics which contain named enti-
ties are few in number; there were around 15 topics
with them. These topics were used for evaluation of
both the systems.
We developed a basic CLIR system which per-
forms the following steps
1. Tokenizes the Hindi query and removes stop
words.
2. Performs query translation; each Hindi word is
looked up in a Hindi - English dictionary and
all the English meanings for the Hindi word
were added to the translated query and for the
words which were not found in the dictionary,
top 20 transliterations generated by one of the
systems are added to the query.
3. Retrieves relevant documents by giving trans-
lated query to CLEF documents.
We present standard IR evaluation metrics such as
precision, mean average precision(MAP) etc.. in the
table 3 below for the two systems.
The above results show a small improvement in
different IR metrics for the system developed using
HMM alignment and CRF when compared to the
other system. The difference in metrics between the
systems is low because the number of topics tested
and the number of named entities in the tested topics
is low.
7 Future Work
The selected statistical model for transliteration is
based on HMM alignment and CRF. This alignment
model is used to get character level alignment (n-
gram) of source and target language words. The
alignment model uses IBM models, such as Model
4, that resort to heuristic search techniques to ap-
proximate forward-backward and Viterbi inference,
which sacrifice optimality for tractability. So, we
plan to use discriminative model CRF for character
level alignment (Phil and Trevor , 2006) of source
and target language words. The behaviour of the
other discrminative models such as Maximum En-
tropy models etc., towards the transliteration task
Model P10 tot rel tot rel ret MAP bpref
HMM 0.3308 13000 3493 0.1347 0.2687
HMM & CRF 0.4154 13000 3687 0.1499 0.2836
Table 3: IR Evaluation of the two systems.
also needs to be verified.
8 Conclusion
We demonstrated a statistical transliteration sys-
tem using HMM alignment and CRF for CLIR that
works better than using HMMs alone. The following
are our important observations.
1. With the increase in number of output target
language words for a given source language
word the efficiency of the system increases.
2. The difference between efficiencies for top n
and n-5 where n > 5; is decreasing on increas-
ing the n value.
References
A. L. Berger. 1997. The improved iterative scaling algo-
rithm: A gentle introduction.
Al-Onaizan Y, Knight K. 2002. Machine transla-
tion of names in Arabic text. Proceedings of the ACL
conference workshop on computational approaches to
Semitic languages.
Arababi Mansur, Scott M. Fischthal, Vincent C. Cheng,
and Elizabeth Bar. 1994. Algorithms for Arabic name
transliteration. IBM Journal of research and Develop-
ment.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large-scale optimization, Math.
Programming 45 (1989), pp. 503?528.
Fujii Atsushi and Tetsuya Ishikawa. 2001.
Japanese/English Cross-Language Information
Retrieval: Exploration of Query Translation and
Transliteration. Computers and the Humanities,
Vol.35, No.4, pp.389-420.
H. M. Wallach. 2002. Efficient training of condi-
tional random fields. Masters thesis, University of Ed-
inburgh.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction.
J. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathe-
matical Statistics, 43:14701480.
Knight Kevin and Graehl Jonathan. 1997. Machine
transliteration. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics, pp. 128-135. Morgan Kaufmann.
Larkey, Connell,AbdulJaleel. 2003. Hindi CLIR in
Thirty Days.
Nasreen Abdul Jaleel and Leah S. Larkey. 2003. Sta-
tistical Transliteration for English-Arabic Cross Lan-
guage Information Retrieval.
Och Franz Josef and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proc. of the 38th Annual
Meeting of the Association for Computational Linguis-
tics, pp. 440-447, Hong Kong, China.
P. F. Brown, S. A. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263-311.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
Stalls Bonnie Glover and Kevin Knight. 1998. Translat-
ing names and technical terms in Arabic text.
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 46?52,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Sentence Position revisited:
A robust light-weight Update Summarization ?baseline? Algorithm
Rahul Katragadda
rahul k@research.iiit.ac.in
Prasad Pingali
pvvpr@iiit.ac.in
Language Technologies Research Center
IIIT Hyderabad
Vasudeva Varma
vv@iiit.ac.in
Abstract
In this paper, we describe a sentence po-
sition based summarizer that is built based
on a sentence position policy, created from
the evaluation testbed of recent summariza-
tion tasks at Document Understanding Con-
ferences (DUC). We show that the summa-
rizer thus built is able to outperform most sys-
tems participating in task focused summariza-
tion evaluations at Text Analysis Conferences
(TAC) 2008. Our experiments also show that
such a method would perform better at pro-
ducing short summaries (upto 100 words) than
longer summaries. Further, we discuss the
baselines traditionally used for summarization
evaluation and suggest the revival of an old
baseline to suit the current summarization task
at TAC: the Update Summarization task.
1 Introduction
Document summarization received a lot of atten-
tion since an early work by Luhn (1958). Statis-
tical information derived from word frequency and
distribution was used by the machine to compute
a relative measure of significance, first for individ-
ual words and then for sentences. Later, Edmund-
son (1969) introduced four clues for identifying sig-
nificant words (topics) in a text. Among them title
and location are related to position methods, while
the other two are presence of cue words and high
frequency content words. Edmundson assigned pos-
itive weights to sentences according to their ordinal
position in the text, giving more weight to the first
sentence in the first paragraph and last sentence in
the last paragraph.
Position of a sentence in a document or the po-
sition of a word in a sentence give good clues to-
wards importance of the sentence or word respec-
tively. Such features are called locational features,
and a sentence position feature deals with presence
of key sentences at specific locations in the text.
Sentence Position has been well studied in summa-
rization research since its inception, early in Ed-
mundson?s work (1969). Earlier, Baxendale (1958)
investigated a sample of 200 paragraphs to deter-
mine where the important words are most likely to
be found. He concluded that in 85% of the para-
graphs, the first sentence was a topic sentence and in
7% of the paragraphs, the final one.
Recent advances in machine learning have been
adapted to summarization problem through the years
and locational features have been consistently used
to identify salience of a sentence. Some represen-
tative work in ?learning? sentence extraction would
include training a binary classifier (Kupiec et al,
1995), training a Markov model (Conroy et al,
2004), training a CRF (Shen et al, 2007), and learn-
ing pairwise-ranking of sentences (Toutanova et al,
2007).
In recent years, at the Document Understand-
ing Conferences (DUC1), Text Summarization re-
search evolved through task focused evaluations
ranging from ?generic single-document summariza-
tion? to ?query-focused multi-document summariza-
tion (QFMDS)?. The QFMDS task models the real-
world complex question answering task wherein,
given a topic and a set of 25 relevant documents, the
1http://duc.nist.gov/
46
task is to synthesize a fluent, well-organized 250-
word summary of the documents that answers the
question(s) in the topic statement. Recent focus
in the community has been towards query-focused
update-summarization task at DUC and the Text
Analysis Conference (TAC2). The update task was to
produce short (~100 words) multi-document update
summaries of newswire articles under the assump-
tion that the user has already read a set of earlier
articles. The purpose of each update summary will
be to inform the reader of new information about a
particular topic.
The rest of the paper is organized as follows. In
Section 2, we describe a Sub-optimal Position Pol-
icy (SPP) based on Pyramid Annotated Data, then
we derive a simple algorithm for summarization
based on the SPP in Section 3, and show evaluation
results. Next, in Section 4, we explain the current
baselines and evaluation for Multi-Document Sum-
marization and finally in Section 5, we discuss the
need for an older baseline in the current context of
the short summary task of update summarization.
2 Sub-Optimal Sentence Position Policy
Given a large text collection and a way to approxi-
mate the relevance for a reasonably large subset of
sentences, we could identify significant positional
attributes for the genre of the collection. Our ex-
periments are based on the work described in (Lin
and Hovy, 1997), whose experiments using the Ziff-
Davis corpus gave great insights on the selective
power of the position method.
2.1 Sentence Position Yield and Optimal
Position Policy (OPP)
Lin and Hovy (1997) provide an empirical validation
for the position hypothesis. They describe a method
of deriving an Optimal Position Policy for a collec-
tion of texts within a genre, as long as a small set
of topic keywords is defined for each text. They de-
fined sentence yield (strength of relevance) of a sen-
tence based on the mention of topic keywords in the
sentence.
The positional yield is defined as the average sen-
tence yield for that position in the document. They
2http://www.nist.gov/tac/
computed the yield of each sentence position in each
document by counting the number of different key-
words contained in the respective sentence in each
document, and averaging over all documents. An
Optimal Position Policy (OPP) is derived based on
the decreasing values of positional yield.
Their experiments grounded on the assumption
that abstract is an ideal representation of central
topic(s) of a text. For their evaluations, they used
the abstract to compare whether the sentences found
based on their Optimal Position Policy are indeed a
good selection. They used precision-recall measures
to establish those findings.
At our disposal we had data from pyramid eval-
uations that provided sentences and their mapping
to any content units in the gold standard summaries.
The annotations in the data provide a unique prop-
erty that each sentence can derive for itself a score
for relevance.
2.2 Documents
There are a wide variety of document types across
genre. In our case of newswire collection we have
identified two primary types of documents: small
document and large document. This distinction is
made based on the total sentences in the document.
All documents that have the number of sentences
above a threshold should be considered large. We
experimented on thresholds varying from 10 to 35
sentences and figured out that documents? distribu-
tion into the two categories was acceptable when
threshold-ed at 20 sentences. This decision is also
well supported by the fact that the last sentences of
a document were more important than the others in
the middle (Baxendale, 1958).
Sentence Position Yield (SPY) is obtained sep-
arately for both types of documents. For a small
document, sentence positions have values from 1
through 20. Meanwhile, for a large document we
compute SPY for position 1 through 20, then the last
15 sentences labeled 136 through 150 and ?any other
sentence? is labeled 100. It can be seen in figure 3
that sentences that do not come from leading or trail-
ing part of large documents do not contribute much
content to the summaries.
47
Figure 1: A sample mapping of SCU annotation to source document sentences. An excerpt from mapping of topic
D0701A of DUC 2007 QF-MDS task.
Figure 2: Sentence Position Yield for small documents.
2.3 Pyramid Data
Summary content units, referred as SCUs hereafter,
are semantically motivated, sub-sentential units that
are variable in length but not bigger than a sentential
clause. SCUs emerge from annotation of a collec-
tion of human summaries for the same input. They
are identified by noting information that is repeated
across summaries, whether the repetition is as small
as a modifier of a noun phrase or as large as a clause.
The weight an SCU obtains is directly proportional
to the number of reference summaries that support
that piece of information. The evaluation method
that is based on overlapping SCUs in human and
automatic summaries is described in the Pyramid
method (Nenkova et al, 2007).
The University of Ottawa has organized the pyra-
mid annotation data such that for some of the sen-
tences in the original document collection (those
that were picked by systems participating in pyra-
mid evaluation), a list of corresponding content units
is known (Copeck et al, 2006). We used this data to
identify locations in a document from where most
sentences were being picked, and which of those lo-
cations were being most content responsive to the
query.
A sample of SCU mapping is shown in figure 1.
Three sentences are seen in the figure among which
two have been annotated with system IDs and SCU
weights wherever applicable. The first sentence has
not been picked by any of the summarizers partici-
pating in Pyramid Evaluations, hence it is unknown
if the sentence would have contributed to any SCU.
The second sentence was picked by 8 summarizers
and that sentence contributed to an SCU of weight
3. The third sentence in the example was picked
by one summarizer, however, it did not contribute
to any SCU. This example shows all the three types
of sentences available in the corpus: unknown sam-
ples, positive samples and negative samples.
For each SCU, a weight is associated in pyramid
annotations. Thus a sentential score could be de-
fined as sum of weights of all the contributing SCUs
of the sentence. For an unknown sample and a neg-
ative sample, sentential score is 0. For example, in
the second sentence in figure 1 the score is 3, con-
tributed by a single SCU.While the same for the first
and third sentences is 0.
For each sentence position the sentential score is
averaged over all documents, which we call Sen-
tence Position Yield. SPY for small and large doc-
uments is shown in figures 2 and 3. Based on these
values for various positions, a simple Position Pol-
48
Figure 3: Sentence Position Yield for large documents
icy was framed as shown below. A position policy is
an ordered set consisting of elements in the order of
most importance. Within a subset, each sub-element
is equally important and treated likewise.
{s1, S1, {s2, S2, s3} , {S3, s4, s5, s6, s7, s8, s20} ,
{S4, s9} . . . }
In the above position policy, sentences from small
documents and large documents are represented by
si and Sj respectively.
The position policy described above provides an
ordering of ranked sentence positions based on a
very accurate ?relevance? annotations on sentences.
However, there is a large subset of sentences that are
not annotated with either positive or negative rele-
vance judgment. Hence, the policy derived is based
on a high-precision low-recall corpus3 for sentence
relevance. If all the sentences were annotated with
such judgements, the policy could have been differ-
ent. For this reason we call the above derived policy,
a Sub-optimal Position Policy (SPP).
3 SPP as an algorithm
The goal of creating a position policy was to identify
its effectiveness as a summarization algorithm. The
3DUC 2005 and 2006 data has been used for learning the
SPP. In further experiments in section 3, DUC 2007 and TAC
2008 data have been used as test data.
above simple heuristic was easily incorporated as an
algorithm based on simple scoring for each distinct
set in the policy. For instance, based on the policy
above, all s1 get the highest weight followed by next
best weight to all S1 and so on.
As it can be observed, only the first sentence of
each document could end up comprising the sum-
mary. This is okay, till we don?t get redundant infor-
mation in the summary. Hence we also used a sim-
ple unigram match based redundancy measure that
doesn?t allow a sentence if it matches any of the al-
ready selected sentences in at least 40% of content
words in it. We also dis-allow sentences greater than
25 content words.
We applied the above algorithm to generate multi-
document summaries for various tasks. We have ap-
plied it to Query-Focused Multi-Document Summa-
rization (QF-MDS) task of DUC 2007 and Query-
Focused Update Summarization task of TAC 2008.
3.1 Query-Focused Multi-Document
Summarization
The query-focused multi-document summarization
task at DUC models the real world complex ques-
tion answering task. Given a topic and a set of 25
relevant documents, this task is to synthesize a flu-
ent, well-organized 250 word summary of the docu-
ments that answers the question(s) in the topic state-
49
ment/narration.
The summaries from the above algorithm for the
QF-MDS were evaluated based on ROUGE met-
rics (Lin, 2004). The average4 recall scores are re-
ported for ROUGE-2 and ROUGE-SU4 in Table 1.
Also reported are the performance of the top per-
forming system and the official baseline(s). This al-
gorithm performed worse than most systems partic-
ipating in the task that year and performed better5
than only the ?first x words? baseline and 3 other sys-
tems.
system ROUGE-2 ROUGE-SU4
?first x words? baseline 0.06039 0.10507
?generic? baseline 0.09382 0.14641
SPP algorithm 0.06913 0.12492
system 15 (top system) 0.12448 0.17711
Table 1: ROUGE 2, SU4 Recall scores for two base-
lines, the SPP algorithm and a top performing system
at Query-Focused Multi-Document Summarization task,
DUC 2007.
3.2 Update Summarization Task
The update summarization task is to produce short
(~100 words) multi-document update summaries of
newswire articles under the assumption that the user
has already read a set of earlier articles. The initial
document set is called cluster A and the next set of
articles are called cluster B. For cluster A, a query-
focused multi-document summary is expected. The
purpose of each ?update summary? (summary of
cluster B) will be to inform the reader of new in-
formation about a particular topic. Summaries from
the above algorithm for the Query Focused Up-
date Summarization task were evaluated based on
ROUGE metrics. This algorithm performed surpris-
ingly better at this task when compared to QF-MDS.
The rouge scores suggest that this algorithm is well
above the median for cluster A and among the top 5
systems for cluster B.
It must be noted that consistent performance
across clusters (both A and B) shows the robustness
of the ?SPP algorithm? at the update summarization
task. Also, it is evident that such an algorithm is
computationally simple and light-weight.
4Averaged over all the 45 topics of DUC 2007 dataset.
5Better in a statistical sense, based on 95% confidence inter-
vals of the two systems? evaluation based on ROUGE-2.
These surprisingly high scores on ROUGE met-
rics prompted us to evaluate the summaries based on
Pyramid Evaluation (Nenkova et al, 2007). Pyramid
evaluation provides a more semantic approach to
evaluation of content based on SCUs as discussed in
Section 2.3. The average6 modified pyramid scores
of cluster A and cluster B summaries is shown in
Table 2, along with the average recall scores for
ROUGE-2, ROUGE-SU4 scores. The pyramid eval-
uation7 suggests that this algorithm performs better
than all other automated systems at TAC 2008. Ta-
ble 3 shows the average performance (across clus-
ters) of ?first x words? baseline, SPP algorithm and
two top performing systems (System ID=43 and
ID=11). System 43 was adjudged best system based
on ROUGE metrics, and system 11 was top per-
former based on pyramid evaluations at TAC 2008.
ROUGE-2 ROUGE-SU4 pyramid
cluster A 0.08987 0.1213 0.3432
cluster B 0.09319 0.1283 0.3576
Table 2: Cluster wise ROUGE 2, SU4 Recall scores and
modified Pyramid Scores for SPP algorithm at the Update
Summarization task.
3.3 Discussion
It is interesting to observe that the algorithm that
performs very poorly at QF-MDS, does very well
in the Update Summarization task. A possible ex-
planation for such behavior could be based on sum-
mary length. For a 250 word summary in the QF-
MDS task, human summaries might provide a de-
scriptive answer to the query that includes informa-
tion nuggets accompanied by background informa-
tion. Indeed, it has been earlier reported that humans
appreciate receiving more information than just the
answer to the query, whenever possible (Lin et al,
2003; Bosma, 2005).
Whereas, in the case of Update Summarization
task the summary length is only 100 words. In such
a short length humans need to trade-off between an-
swer sentences and supporting sentences, and usu-
ally answers are preferred. And since our method
6Averaged over all the 48 topics of TAC 2008 dataset.
7Pyramid Annotation were done by a volunteer who also
volunteered for annotations during DUC 2007.
50
system ROUGE-2 ROUGE-SU4 pyramid
?first x words? baseline 0.05896 0.09327 0.166
SPP algorithm 0.09153 0.1245 0.3504
System 43 (top in ROUGE) 0.10395 0.13646 0.289
System 11 (top in pyramid) 0.08858 0.12484 0.336
Table 3: Average ROUGE 2, SU4 Recall scores and modified Pyramid Scores for baseline, SPP algorithm and two top
performing systems at TAC 2008.
identifies sentences that are known to be contribut-
ing towards the needed answers, it performs better
at the shorter version of the task.
Another possible explanation is that as a shorter
summary length is required, the task of choosing the
most important information becomes more difficult
and no approach works well consistently. Also, it
has often been noted that this baseline is indeed quite
strong for this genre, due to the journalistic conven-
tion for putting the most important part of an article
in the initial paragraphs.
4 Baselines in Summarization Tasks
Over the years, as summarization research followed
trends from generic single-document summariza-
tion, to generic multi-document summarization, to
focused multi-document summarization there were
two major baselines that stayed throughout the eval-
uations. Those two baselines are:
1. First N words of the document (or of the most re-
cent document).
2. First sentence from each document in chronological
order until the length requirement is reached.
The first baseline was in place ever since the first
evaluation of generic single document summariza-
tion took place in DUC 2001. For multi-document
summarization, first N words of the most recent
document (chronologically) was chosen as the base-
line 1. In the recent summarization evaluations at
Text Analysis Conference (TAC 2008), where up-
date summarization was evaluated; baseline 1 still
persists. This baseline performs pretty poorly at con-
tent evaluations based on all manual and automatic
metrics. However, since it doesn?t disturb the orig-
inal flow and ordering of a document, linguistically
these summaries are the best. Indeed it outperforms
all the automated systems based on linguistic quality
evaluations.
The second baseline had been used occasionally
with multi-document summarization from 2001 to
2004 with both generic multi-document summariza-
tion and focused multi-document summarization. In
2001 only one system significantly outperformed the
baseline 2 (Nenkova, 2005). In 2003 QF-MDS how-
ever, only one system outperformed the baseline 2
above, while in 2004 at the same task, no system
significantly outperforms the baseline. This baseline
as can be seen, over the years has been pretty much
untouched by systems based on content evaluation.
However, the linguistic aspects of summary quality
would be compromised in such a summary.
Currently, for the Update Summarization task at
TAC 2008, NIST?s baseline is the baseline 1 (?first x
words? baseline). And all systems (except one) per-
form better than the baseline in all forms of content
evaluation. Since the task is to generate 100 word
summaries (short summaries), based on past experi-
ences, there is no doubt that baseline 2 would per-
form well.
It is interesting to observe that baseline 2 is a close
approximation to the ?SPP algorithm? described in
this paper. There are two main differences that we
draw between ?baseline 2? and SPP algorithm. First,
?baseline 2? picks only the first sentence in each
document, while ?SPP algorithm? could pick other
sentences in an order described by the position pol-
icy. Second, ?baseline 2? puts no restriction on re-
dundancy, thus due to journalistic conventions entire
summary might be comprised of the same ?informa-
tion nuggets?, wasting the minimal real-estate avail-
able (~100 words). On the other hand, in our ?SPP
algorithm? we consider a simple unigram-overlap
measure to identify redundant information in sen-
tence pairs that avoids redundant nuggets in the final
summary.
51
5 Discussion and Conclusion
Baselines 1 and 2 mentioned above, could together
act as a balancing mechanism to compare for lin-
guistic quality and responsive content in the sum-
mary. The availability of a stronger content respon-
sive summary as a baseline would enable steady
progress in the field. While all the linguistically
motivated systems would compare themselves with
baseline 1, the summary content motivated systems
would compare with the stronger baseline 2 and get
better than it.
Over the years to come, the usage of ?baseline 1?
doesn?t help in understanding whether there has
been significant improvement in the field. This is be-
cause almost every simple algorithm beats the base-
line performance. Having a better baseline, like the
one based on the position hypothesis, would raise
the bar for systems participating in coming years,
and tracking progress of the field over the years is
easier.In this paper, we derived a method to identify a
?sub-optimal position policy? based on pyramid an-
notation data, that were previously unavailable. We
also distinguish small and large documents to obtain
the position policy. We described the Sub-optimal
Sentence Position Policy (SPP) based on pyramid
annotation data and implemented the SPP as an al-
gorithm to show that a position policy thus formed
is a good representative of the genre and thus per-
forms way above median performance. We further
describe the baselines used in summarization evalu-
ation and discuss the need to bring back baseline 2
(or the ?SPP algorithm?) as an official baseline for
update summarization task.
Ultimately, as Lin and Hovy (1997) suggest, the
position method can only take us certain distance. It
has a limited power of resolution (the sentence) and
its limited method of identification (the position in a
text). Which is why we intend to use it as a baseline.
Currently, as we can see the algorithm generates a
generic summary, it doesn?t consider the topic or
query to generate a query-focused summary. In fu-
ture we plan to extend the SPP algorithm with some
basic method for bringing in relevance.
References
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature ? an experiment. IBM Journal of Re-
search and Development, 2(Non-topical Issue).
Wauter Bosma. 2005. Extending answers using dis-
course structures. In Horacio Saggion and J. L. Minel,
editors, RANLP workshop on Crossing Barriers in Text
summarization Research, pages 2?9. Incoma Ltd.
John M. Conroy, Judith D. Schlesinger, Jade Goldstein,
and Dianne P. O?leary. 2004. Left-brain/right-brain
multi-document summarization. In the proceedings of
Document Understanding Conference (DUC) 2004.
Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy,
D Kipp, Vivi Nastase, and Stan Szpakowicz. 2006.
Leveraging duc. In proceedings of DUC 2006.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. In Journal of the ACM, volume 16, pages
264?285. ACM.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In the proceedings
of ACM SIGIR?95, pages 68?73. ACM.
Chin-Yew Lin and Eduard Hovy. 1997. Identifying top-
ics by position. In Proceedings of the fifth conference
on Applied natural language processing, pages 283?
290. ACL.
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,
David Huynh, Boris Katz, and David R. Karger. 2003.
The role of context in question answering systems. In
the proceedings of CHI?04. ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In the proceedings of ACL
Workshop on Text Summarization Branches Out. ACL.
H.P. Luhn. 1958. The automatic creation of literature ab-
stracts. In IBM Journal of Research and Development,
Vol. 2, No. 2, pp. 159-165, April 1958.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. In ACM Trans. Speech Lang. Process., vol-
ume 4, New York, NY, USA. ACM.
Ani Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document under-
standing conference. In Manuela M. Veloso and Sub-
barao Kambhampati, editors, AAAI, pages 1436?1441.
AAAI Press / The MIT Press.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng
Chen. 2007. Document summarization using condi-
tional random fields. In the proceedings of IJCAI ?07.,
pages 2862?2867. IJCAI.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamundi, Hisami Suzuki, and Lucy Van-
derwende. 2007. The pythy summarization system:
Microsoft research at duc 2007. In the proceedings of
Document Understanding Conference 2007.
52
 
 
 
Hindi and Telugu to English CLIR using Query Expansion 
 
Prasad Pingali and Vasudeva Varma 
 
Abstract 
 
This paper presents the experiments of Language Technologies Research 
Centre (LTRC) as part of their participation in CLEF2 2007 Indian language 
to English ad-hoc cross language document retrieval task. In this paper we 
discuss our Hindi and Telugu to English CLIR system and the experiments 
using CLEF 2007 dataset. We used a variant of TFIDF algorithm in 
combination with a bilingual lexicon for query translation. We also explored 
the role of a document summary in fielded queries and two different boolean 
formulations of query translations. We find that a hybrid boolean 
formulation using a combination of boolean AND and boolean OR operators 
improves ranking of documents. We also find that simple disjunctive 
combination of translated query keywords results in maximum recall. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
