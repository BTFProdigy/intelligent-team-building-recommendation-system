Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the SIGDIAL 2013 Conference, pages 193?202,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Verbal indicators of psychological distress in interactive dialogue with a
virtual human
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum,
Stefan Scherer, Albert (Skip) Rizzo, Louis-Philippe Morency
University of Southern California, Institute for Creative Technologies
Playa Vista, CA
devault@ict.usc.edu
Abstract
We explore the presence of indicators of
psychological distress in the linguistic be-
havior of subjects in a corpus of semi-
structured virtual human interviews. At
the level of aggregate dialogue-level fea-
tures, we identify several significant dif-
ferences between subjects with depres-
sion and PTSD when compared to non-
distressed subjects. At a more fine-grained
level, we show that significant differences
can also be found among features that
represent subject behavior during specific
moments in the dialogues. Finally, we
present statistical classification results that
suggest the potential for automatic assess-
ment of psychological distress in individ-
ual interactions with a virtual human dia-
logue system.
1 Introduction
One of the first steps toward dealing with psy-
chological disorders such as depression and PTSD
is diagnosing the problem. However, there is of-
ten a shortage of trained health care professionals,
or of access to those professionals, especially for
certain segments of the population such as mili-
tary personnel and veterans (Johnson et al, 2007).
One possible partial remedy is to use virtual hu-
man characters to do a preliminary triage screen-
ing, so that mental healthcare providers can focus
their attention on those who are most likely to need
help. The virtual human would engage an indi-
vidual in an interview and analyze some of their
behavioral characteristics. In addition to serving
a triage function, this automated interview could
produce valuable information to help the health-
care provider make their expert diagnosis.
In this paper, we investigate whether features
in the linguistic behavior of participants in a con-
versation with a virtual human could be used
for recognizing psychological distress. We focus
specifically on indicators of depression and post-
traumatic stress disorder (PTSD) in the verbal be-
havior of participants in a Wizard-of-Oz corpus.
The results and analysis presented here are part
of a broader effort to create an automated, interac-
tive virtual human dialogue system that can detect
indicators of psychological distress in the multi-
modal communicative behavior of its users. Re-
alizing this vision requires a careful and strate-
gic design of the virtual human?s dialogue behav-
ior, and in concert with the system?s behavior, the
identification of robust ?indicator? features in the
verbal and nonverbal responses of human intervie-
wees. These indicators should be specific behavior
patterns that are empirically correlated with spe-
cific psychological disorders, and that can inform
a triage screening process or facilitate the diagno-
sis or treatment performed by a clinician.
In this paper, we report on several kinds of such
indicators we have observed in a corpus of 43
Wizard-of-Oz interactions collected with our pro-
totype virtual human, Ellie, pictured in Figure 1.
We begin in Section 2 with a brief discussion of
background and related work on the communica-
tive behavior associated with psychological dis-
tress. In Section 3, we describe our Wizard-of-Oz
data set. Section 4 presents an analysis of indicator
features we have explored in this data set, identi-
fying several significant differences between sub-
jects with depression and PTSD when compared
to non-distressed subjects. In Section 5 we present
statistical classification results that suggest the po-
tential for automatic assessment of psychological
distress based on individual interactions with a vir-
tual human dialogue system. We conclude in Sec-
tion 6.
2 Background and Related Work
There has been a range of psychological and clin-
ical research that has identified differences in the
193
Figure 1: Ellie.
communicative behavior of patients with specific
psychological disorders such as depression. In this
section, we briefly summarize some closely re-
lated work.
Most work has observed the behavior of patients
in human-human interactions, such as clinical in-
terviews and doctor-patient interactions. PTSD is
generally less well studied than depression.
Examples of the kinds of differences that have
been observed in non-verbal behavior include dif-
ferences in rates of mutual gaze and other gaze
patterns, downward angling of the head, mouth
movements, frowns, amount of gesturing, fidget-
ing, emotional expressivity, and voice quality; see
Scherer et al (2013) for a recent review.
In terms of verbal behavior, our exploration of
features here is guided by several previous obser-
vations in the literature. Cohn and colleagues have
identified increased speaker-switch durations and
decreased variability of vocal fundamental fre-
quency as indicators of depression, and have ex-
plored the use of these features for classification
(Cohn et al, 2009). That work studied these fea-
tures in human-human clinical interviews, rather
than in virtual human interactions as reported here.
In clinical studies, acute depression has been as-
sociated with decreased speech, slow speech, de-
lays in delivery, and long silent pauses (Hall et al,
1995). Aggregate differences in lexical frequen-
cies have also been observed. For example, in
written essays, Rude et al (2004) observed that
depressed participants used more negatively va-
lenced words and used the first-person pronoun ?I?
more frequently than never-depressed individuals.
Heeman et al (2010) observed differences in chil-
dren with autism in how long they pause before
speaking and in their use of fillers, acknowledg-
ments, and discourse markers. Some of these fea-
tures are similar to those studied here, but looked
at children communicating with clinicians rather
than a virtual human dialogue system.
Recent work on machine classification has
demonstrated the ability to discriminate between
schizophrenic patients and healthy controls based
on transcriptions of spoken narratives (Hong et al,
2012), and to predict patient adherence to med-
ical treatment from word-level features of dia-
logue transcripts (Howes et al, 2012). Automatic
speech recognition and word alignment has also
been shown to give good results in scoring narra-
tive recall tests for identification of cognitive im-
pairment (Prud?hommeaux and Roark, 2011; Lehr
et al, 2012).
3 Data Set
In this section, we introduce the Wizard-of-Oz
data set that forms the basis for this paper. In
this virtual human dialogue system, the charac-
ter Ellie depicted in Figure 1 carries out a semi-
structured interview with a single user. The sys-
tem was designed after a careful analysis of a
set of face-to-face interviews in the same do-
main. The face-to-face interviews make up the
large human-human Distress Assessment Inter-
view Corpus (DAIC) that is described in Scherer
et al (2013). Drawing on observations of inter-
viewer behavior in the face-to-face dialogues, El-
lie was designed to serve as an interviewer who
is also a good listener, providing empathetic re-
sponses, backchannels, and continuation prompts
to elicit more extended replies to specific ques-
tions. The data set used in this paper is the re-
sult of a set of 43 Wizard-of-Oz interactions where
the virtual human interacts verbally and nonver-
bally in a semi-structured manner with a partici-
pant. Excerpts from the transcripts of two interac-
tions in this Wizard-of-Oz data set are provided in
the appendix in Figure 5.1
3.1 Procedure
The participants were recruited via Craigslist and
were recorded at the USC Institute for Creative
1A sample demonstration video of an interaction be-
tween the virtual agent and a human actor can be seen here:
http://www.youtube.com/watch?v=ejczMs6b1Q4
194
Technologies. In total 64 participants interacted
with the virtual human. All participants who met
requirements (i.e. age greater than 18, and ad-
equate eyesight) were accepted. In this paper,
we focus on a subset of 43 of these participants
who were told that they would be interacting with
an automated system. (The other participants,
which we exclude from our analysis, were aware
that they were interacting with a human-controlled
system.) The mean age of the 43 participants in
our data set was 36.6 years, with 23 males and 20
females.
We adhered to the following procedure for data
collection: After a short explanation of the study
and giving consent, participants completed a series
of questionnaires. These questionnaires included
the PTSD Checklist-Civilian version (PCL-C) and
the Patient Health Questionnaire, depression mod-
ule (PHQ-9) (Scherer et al, 2013) along with other
questions. Then participants engage in an inter-
view with the virtual human, Ellie. After the di-
alogue concludes, participants are then debriefed
(i.e. the wizard control is revealed), paid $25 to
$35, and escorted out.
The interaction between the participants and El-
lie was designed as follows: Ellie explains the pur-
pose of the interaction and that she will ask a series
of questions. She then tries to build rapport with
the participant in the beginning of the interaction
with a series of casual questions about Los Ange-
les. Then the main interview begins, including a
range of questions such as:
What would you say are some of your
best qualities?
What are some things that usually put
you in a good mood?
Do you have disturbing thoughts?
What are some things that make you re-
ally mad?
How old were you when you enlisted?
What did you study at school?
Ellie?s behavior was controlled by two human
?wizards? in a separate room, who used a graph-
ical user interface to select Ellie?s nonverbal be-
havior (e.g. head nods, smiles, back-channels)
and verbal utterances (including the interview
questions, verbal back-channels, and empathy re-
sponses). This Wizard-of-Oz setup allows us to
prove the utility of the protocol and collect training
data for the eventual fully automatic interaction.
The speech for each question was pre-recorded us-
ing an amateur voice actress (who was also one of
the wizards). The virtual human?s performance of
these utterances is animated using the SmartBody
animation system (Thiebaux et al, 2008).
3.2 Condition Assessment
The PHQ-9 and PCL-C scales provide researchers
with guidelines on how to assess the participants?
conditions based on the responses. Among the
43 participants, 13 scored above 10 on the PHQ-
9, which corresponds to moderate depression and
above (Kroenke et al, 2001). We consider these
13 participants as positive for depression in this
study. 20 participants scored positive for PTSD,
following the PCL-C classification. The two pos-
itive conditions overlap strongly, as the evalu-
ated measurements PHQ-9 and PCL-C correlate
strongly (Pearson?s r > 0.8, as reported in Scherer
et al (2013)).
4 Feature Analysis
4.1 Transcription and timing of speech
We have a set D = {d1, ..., d43} of 43 dialogues.
The user utterances in each dialogue were tran-
scribed using ELAN (Wittenburg et al, 2006),
with start and end timestamps for each utterance.2
At each pause of 300ms or longer in the user?s
speech, a new transcription segment was started.
The resulting speech segments were subsequently
reviewed and corrected for accuracy.
For each dialogue di ? D, this process resulted
in a sequence of user speech segments. We repre-
sent each segment as a tuple ?s, e, t?, where s and e
are the starting and ending timestamps in seconds,
and t is the manual text transcription of the corre-
sponding audio segment. The system speech seg-
ments, including their starting and ending times-
tamps and verbatim transcripts of system utter-
ances, were recovered from the system log files.
To explore aggregate statistical features based
on user turn-taking behavior in the dialogues, we
employ a simple approach to identifying turns
within the dialogues. First, all user and system
speech segments are sorted in increasing order of
2ELAN is a tool that supports annotation of
video and audio, from the Max Planck Insti-
tute for Psycholinguistics, The Language Archive,
Nijmegen, The Netherlands. It is available at
http://tla.mpi.nl/tools/tla-tools/elan/.
195
Segment level features
(a) mean speaking rate of each user segment
(b) mean onset time of first segment in each user turn
(c) mean onset time of non-first segments in user turns
(d) mean length of user segments
(e) mean minimum valence in user segments
(f) mean mean valence in user segments
(g) mean maximum valence in user segments
(h) mean number of filled pauses in user segments
(i) mean filled pause rate in user segments
Dialogue level features
(j) total number of user segments
(k) total length of all user segments
Figure 2: List of context-independent features.
their starting timestamps. All consecutive seg-
ments with the same speaker are then designated
as constituting a single turn. While this simple
scheme does not provide a detailed treatment of
relevant phenomena such as overlapping speech,
backchannels, and the interactive process of ne-
gotiating the turn in dialogue (Yang and Heeman,
2010), it provides a conceptually simple model for
the definition of features for aggregate statistical
analysis.
4.2 Context-independent feature analysis
We begin by analyzing a set of shallow features
which we describe as context-independent, as they
apply to user speech segments independently of
what the system has recently said. Most of these
are features that apply to many or all user speech
segments. We describe our context-independent
features in Section 4.2.1, and present our results
for these features in Section 4.2.2.
4.2.1 Context-independent features
We summarize our context-independent features
in Figure 2.
Speaking rate and onset times Based on previ-
ous clinical observations related to slowed speech
and increased onset time for depressed individuals
(Section 2), we defined features for speaking rate
and onset time of user speech segments.
We quantify the speaking rate of a user speech
segment ?s, e, t?, where t = ?w1, ..., wN ?, as
N/(e ? s). Feature (a) is the mean value of
this feature across all user speech segments within
each dialogue.
Onset time is calculated using the notion of user
turns. For each user turn, we extracted the first
user speech segment in the turn fu = ?su, eu, tu?,
and the most recent system speech segment ls =
?ss, es, ts?. We define the onset time of such a first
user segment as su ? es, and for each dialogue,
feature (b) is the intra-dialogue mean of these on-
set times.
In order to also quantify pause length between
user speech segments within a turn, we define fea-
ture (c), a similar feature that measures the mean
onset time between non-first user speech segments
within a user turn in relation to the preceding user
speech segment.
Length of user segments As one way to quan-
tify the amount of speech, feature (d) reports the
mean length of all user speech segments within a
dialogue (measured in words).
Valence features for user speech Features (e)-
(g) are meant to explore the idea that distressed
users might use more negative or less positive vo-
cabulary than non-distressed subjects. As an ex-
ploratory approach to this topic, we used Senti-
WordNet 3.0 (Baccianella and Sebastiani, 2010),
a lexical sentiment dictionary, to assign valence
to individual words spoken by users in our study.
The dictionary contains approximately 117,000
entries. In general, each word w may appear in
multiple entries, corresponding to different parts
of speech and word senses. To assign a single va-
lence score v(w) to each word in the dictionary, in
our features we compute the average score across
all parts of speech and word senses:
v(w) =
?
e?E(w) PosScoree(w)?NegScoree(w)
|E(w)|
where E(w) is the set of entries for the word w,
PosScoree(w) is the positive score for w in entry
e, and NegScoree(w) is the negative score for w
in entry e. This is similar to the ?averaging across
senses? method described in Taboada et al (2011).
We use several different measures of the va-
lence of each speech segment with transcript t =
?w1, ..., wn?. We compute the min, mean, and max
valence of each transcript:
minimum valence of t = minwi?t v(wi)
mean valence of t = 1n
?
wi?t v(wi)
maximum valence of t = maxwi?t v(wi)
Features (e)-(f) then are intra-dialogue mean
196
values for these three segment-level valence mea-
sures.
Filled pauses Another feature that we explored
is the presence of filled pauses in user speech seg-
ments. To do so, we counted the number of times
any of the tokens uh, um, uhh, umm, mm, or mmm
appeared in each speech segment. For each dia-
logue, feature (h) is the mean number of these to-
kens per user speech segment. In order to account
for the varying length of speech segments, we also
normalize the raw token counts in each segment
by dividing them by the length of the segment, to
produce a filled pause rate for the segment. Fea-
ture (i) is the mean value of the filled pause rate
for all speech segments in the dialogue.
Dialogue level features We also included two
dialogue level measures of how ?talkative? the
user is. Feature (j) is the total number of user
speech segments throughout the dialogue. Feature
(k) is the total length (in words) of all speech seg-
ments throughout the dialogue.
Standard deviation features For the classifica-
tion experiments reported in Section 5, we also in-
cluded a standard deviation variant of each of the
features (a)-(i) in Figure 2. These variants are de-
fined as the intra-dialogue standard deviation of
the underlying value, rather than the mean. We
discuss examples of standard deviation features
further in Section 5.
4.2.2 Results for context-independent
features
We summarize the observed significant effects in
our Wizard-of-Oz corpus in Table 1.
Onset time We report our findings for individu-
als with and without depression and PTSD for fea-
ture (b) in Table 1 and in Figure 3. The units are
seconds. While an increase in onset time for in-
dividuals with depression has previously been ob-
served in human-human interaction (Cohn et al,
2009; Hall et al, 1995), here we show that this
effect transfers to interactions between individuals
with depression and virtual humans. We find that
mean onset time is significantly increased for indi-
viduals with depression in their interactions with
our virtual human Ellie (p = 0.018, Wilcoxon
rank sum test).
Additionally, while to our knowledge onset time
for individuals with PTSD has not been reported,
we also found a significant increase in onset time
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
No depr.
??
Depr.
?
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
?PTSD
?
PTSD
?
Figure 3: Onset time.
for individuals with PTSD (p = 0.019, Wilcoxon
rank sum test).
Filled pauses We report our findings for individ-
uals with and without depression and PTSD under
feature (h) in Table 1 and in Figure 4. We observed
a significant reduction in this feature for both in-
dividuals with depression (p = 0.012, Wilcoxon
rank sum test) and PTSD (p = 0.014, Wilcoxon
rank sum test). We believe this may be related
to the trend we observed toward shorter speech
segments from distressed individuals (though this
trend did not reach significance). There is a pos-
itive correlation, ? = 0.55 (p = 0.0001), be-
tween mean segment length (d) and mean number
of filled pauses in segments (h).
Other features We did not observe significant
differences in the values of the other context-
independent features in Figure 2.
4.3 Context-dependent features
Our data set alows us to zoom in and look at
specific contextual moments in the dialogues, and
observe how users respond to specific Ellie ques-
tions. As an example, one of Ellie?s utterances,
which has system ID happy lasttime, is:
happy lasttime = Tell me about the last
time you felt really happy.
In our data set of 43 dialogues, this question was
asked in 42 dialogues, including 12 users positive
for depression and 19 users positive for PTSD.
197
Feature Depression (13 yes, 30 no) PTSD (20 yes, 23 no)
(b) mean onset time of first
segment in each user turn
?
Depr.: 1.72 (0.89)
No Depr.: 1.08 (0.56)
p = 0.018
?
PTSD.: 1.56 (0.80)
No PTSD.: 1.03 (0.57)
p = 0.019
(h) mean number of filled pauses
in user segments
?
Depr.: 0.32 (0.19)
No Depr.: 0.48 (0.23)
p = 0.012
?
PTSD: 0.36 (0.24)
No PTSD: 0.49 (0.21)
p = 0.014
Table 1: Results for context-independent features. For each feature and condition, we provide the mean
(standard deviation) for individuals with and without the condition. P-values for individual Wilcoxon
rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased feature values
for positive individuals. A down arrow (?) indicates a significant trend toward decreased feature values
for positive individuals.
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
No depr.
?
Depr.
?
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
?PTSD PTSD
?
Figure 4: Number of filled pauses per speech seg-
ment.
This question is one of 95 topic setting utter-
ances in Ellie?s repertoire. (Ellie has additional
utterances that serve as continuation prompts,
backchannels, and empathy responses, which can
be used as a topic is discussed.)
To define context-dependent features, we asso-
ciate with each user segment the most recent of
Ellie?s topic setting utterances that has occurred in
the dialogue. We then focus our analysis on those
user segments and turns that follow specific topic
setting utterances. In Table 2, we present some ex-
amples of our findings for context-dependent fea-
tures for happy lasttime.3
3While we provide significance test results here at the p <
0.05 level, it should be noted that because of the large number
of context-dependent features that may be defined in a small
corpus such as ours, we report these merely as observations in
our corpus. We do not claim that these results transfer beyond
The contextual feature labeled (g?) in Table 2 is
the mean of the maximum valence feature across
all segments for which happy lasttime is the most
recent topic setting system utterance. We provide
a full example of this feature calculation in Fig-
ure 5 in the appendix.
As the figure shows, we find that users with
both PTSD and depression show a sharp reduc-
tion in the mean maximum valence in their speech
segments that respond to this question. This sug-
gests that in these virtual human interactions, this
question plays a useful role in eliciting differen-
tial responses from subjects with these psycholog-
ical disorders. We observed three additional ques-
tions which showed a significant difference in the
mean maximum valence feature. One example is
the question, How would your best friend describe
you?.
With feature (b?) in Table 2, we find an in-
creased onset time in responses to this question for
subjects with depression.4 Feature (d?) shows that
subjects with PTSD exhibit shorter speech seg-
ments in their responses to this question.
We observed a range of findings of this sort for
various combinations of Ellie?s topic setting utter-
ances and specific context-dependent features. In
future work, we would like to study the optimal
combinations of context-dependent questions that
yield the most information about the user?s distress
status.
this data set.
4In comparing Table 2 with Table 1, this question seems
to induce a higher mean onset time for distressed users than
the average system utterance does. This does not seem to be
the case for non-distressed users.
198
Feature Depression (12 yes, 30 no) PTSD (19 yes, 23 no)
(g?) mean maximum valence
in user segments following
happy lasttime
?
Depr.: 0.15 (0.07)
No Depr.: 0.26 (0.12)
p = 0.003
?
PTSD: 0.16 (0.08)
No PTSD: 0.28 (0.11)
p = 0.0003
(b?) mean onset time of first
segments in user turns
following happy lasttime
?
Depr.: 2.64 (2.70)
No Depr.: 0.94 (1.80)
p = 0.030
n.s.
PTSD: 2.18 (2.48)
No PTSD: 0.80 (1.76)
p = 0.077
(d?) mean length of user
segments following
happy lasttime
n.s.
Depr.: 5.95 (1.80)
No Depr.: 10.03 (6.99)
p = 0.077
?
PTSD: 6.82 (5.12)
No PTSD: 10.55 (6.68)
p = 0.012
Table 2: Example results for context-dependent features. For each feature and condition, we provide
the mean (standard deviation) for individuals with and without the condition. P-values for individual
Wilcoxon rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased
feature values for positive individuals. A down arrow (?) indicates a significant trend toward decreased
feature values for positive individuals.
5 Classification Results
In this section, we present some suggestive clas-
sification results for our data set. We construct
three binary classifiers that use the kinds of fea-
tures described in Section 4 to predict the pres-
ence of three conditions: PTSD, depression, and
distress. For the third condition, we define dis-
tress to be present if and only if PTSD, depres-
sion, or both are present. Such a notion of distress
that collapses distinctions between disorders may
be the most appropriate type of classification for a
potential application in which distressed users of
any type are prioritized for access to health care
professionals (who will make a more informed as-
sessment of specific conditions).
For each individual dialogue, each of the three
classifiers emits a single binary label. We train
and evaluate the classifiers in a 10-fold cross-
validation using Weka (Hall et al, 2009).
While our data set of 43 dialogues is perhaps
of a typical size for a study of a research proto-
type dialogue system, it remains very small from
a machine learning perspective. We report here
two kinds of results that help provide perspective
on the prospects for classification of these condi-
tions. The first kind looks at classification based
on all the context-independent features described
in Section 4.2.1. The second looks at classifica-
tion based on individual features from this set.
In the first set of experiments, we trained a
Na??ve Bayes classifier for each condition using
all the context-independent features. We present
our results in Table 3, comparing each classifier to
a baseline that always predicts the majority class
(i.e. no condition for PTSD, no condition for de-
pression, and with condition for distress).
We note first that the trained classifiers all out-
perform the baseline in terms of weighted F-score,
weighted precision, weighted recall, and accuracy.
The accuracy improvement over baseline is sub-
stantial for PTSD (20.9% absolute improvement)
and distress (23.2% absolute improvement). The
accuracy improvement over baseline is more mod-
est for depression (2.3% absolute). We believe
one factor in the relative difficulty of classifying
depression more accurately is the relatively small
number of depressed participants in our study
(13).
While it has been shown in prior work (Cohn et
al., 2009) that depression can be classified above
baseline performance using features observed in
clinical human-human interactions, here we have
shown that classification above baseline perfor-
mance is possible in interactions between human
participants and a virtual human dialogue system.
Further, we have shown classification results for
PTSD and distress as well as depression.
We tried incorporating context-dependent fea-
tures, and also unigram features, but found that
neither improved performance. We believe our
data set is too small for effective training with
these very large extended feature sets.
199
Disorder Model Weighted F-score Weighted Precision Weighted Recall Accuracy
PTSD Na??ve Bayes 0.738 0.754 0.744 74.4%
Majority Baseline 0.373 0.286 0.535 53.5%
Depression Na??ve Bayes 0.721 0.721 0.721 72.1%
Majority Baseline 0.574 0.487 0.698 69.8%
Distress Na??ve Bayes 0.743 0.750 0.744 74.4%
Majority Baseline 0.347 0.262 0.512 51.2%
Table 3: Classification results.
In our second set of experiments, we sought to
gain understanding of which features were pro-
viding the greatest value to classification perfor-
mance. We therefore retrained Na??ve Bayes classi-
fiers using only one feature at a time. We summa-
rize here some of the highest performing features.
For depression, we found that the feature stan-
dard deviation in onset time of first segment in
each user turn yielded very strong performance
by itself. In our corpus, we observed that de-
pressed individuals show a greater standard devia-
tion in the onset time of their responses to Ellie?s
questions (p = 0.024, Wilcoxon rank sum test).
The value of this feature in classification comple-
ments the clinical finding that depressed individu-
als show greater onset times in their responses to
interview questions (Cohn et al, 2009).
For distress, we found that the feature mean
maximum valence in user segments was the most
valuable. We discussed findings for a context-
dependent version of this feature in Section 4.3.
This finding for distress can be related to previ-
ous observations that individuals with depression
use more negatively valenced words (Rude et al,
2004).
For PTSD, we found that the feature mean num-
ber of filled pauses in user segments was among
the most informative.
Based on our observation of classification per-
formance using individual features, we believe
there remains much room for improvement in fea-
ture selection and training. A larger data set would
enable feature selection approaches that use held
out data, and would likely result in both increased
performance and deeper insights into the most
valuable combination of features for classification.
6 Conclusion
In this paper, we have explored the presence of in-
dicators of psychological distress in the linguistic
behavior of subjects in a corpus of semi-structured
virtual human interviews. In our data set, we
have identified several significant differences be-
tween subjects with depression and PTSD when
compared to non-distressed subjects. Drawing on
these features, we have presented statistical classi-
fication results that suggest the potential for auto-
matic assessment of psychological distress in indi-
vidual interactions with a virtual human dialogue
system.
Acknowledgments
This work is supported by DARPA under con-
tract (W911NF-04-D-0005) and the U.S. Army
Research, Development, and Engineering Com-
mand. The content does not necessarily reflect the
position or the policy of the Government, and no
official endorsement should be inferred.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Jeffery F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara Tejera
Padilla, Feng Zhou, and Fernando De la Torre.
2009. Detecting depression from facial actions and
vocal prosody. In Affective Computing and Intelli-
gent Interaction (ACII), September.
Judith A. Hall, Jinni A. Harrigan, and Robert Rosen-
thal. 1995. Nonverbal behavior in clinician-patient
interaction. Applied and Preventive Psychology,
4(1):21 ? 37.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Peter A Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois Black, and Jan Van Santen. 2010. Autism and
200
interactional aspects of dialogue. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 249?252.
Association for Computational Linguistics.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal differences in autobiographical narratives from
schizophrenic patients and healthy controls. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 37?
47, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 79?83, Seoul,
South Korea, July. Association for Computational
Linguistics.
Shannon J Johnson, Michelle D Sherman, Jeanne S
Hoffman, Larry C James, Patti L Johnson, John E
Lochman, Thomas N Magee, David Riggs, Jes-
sica Henderson Daniel, Ronald S Palomares, et al
2007. The psychological needs of US military ser-
vice members and their families: A preliminary re-
port. American Psychological Association Presi-
dential Task Force on Military Deployment Services
for Youth, Families and Service Members.
Kurt Kroenke, Robert L. Spitzer, and Janet B. W.
Williams. 2001. The phq-9. Journal of General
Internal Medicine, 16(9):606?613.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. In Interspeech 2012: 13th Annual Con-
ference of the International Speech Communication
Association, Portland, Oregon, September.
Emily Prud?hommeaux and Brian Roark. 2011. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Interspeech 2011: 12th An-
nual Conference of the International Speech Com-
munication Association, pages 3021?3024, Flo-
rence, Italy, August.
Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In IEEE Conference on Automatic
Face and Gesture Recognition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Marcus Thiebaux, Stacy Marsella, Andrew N. Mar-
shall, and Marcelo Kallmann. 2008. Smartbody:
behavior realization for embodied conversational
agents. In Proceedings of the 7th international
joint conference on Autonomous agents and multi-
agent systems - Volume 1, AAMAS ?08, pages 151?
158, Richland, SC. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Peter Wittenburg, Hennie Brugman, Albert Russel,
Alex Klassmann, and Han Sloetjes. 2006. Elan: a
professional framework for multimodality research.
In Proceedings of LREC 2006, Fifth International
Conference on Language Resources and Evaluation.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue. Computer Speech &
Language, 24(2):175 ? 189.
201
Appendix A. Wizard-of-Oz Dialogue Excerpts
Example user with PTSD and depression Example non-distressed user
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.014 when i play with my dog
Ellie when was the last time
that happened?
User 0.000 this morning
Ellie aw
Ellie tell me more about that
User 0.312 i don?t know i just
User 0.010 whenever i get
User 0.312 super bored or i just
need some time to
User 0.312 for myself i just play
with the dog
Ellie that?s good
User 0.000 yeah
0.1371 = mean
maximum valence in
user segments following
happy lasttime
Ellie (ideal weekendC) tell
me how you spend your
ideal weekend
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.562 um the last time i felt
really happy was
User 0.000 hm
User 0.000 today
Ellie tell me more about that
User 0.688 uh just from the moment
i woke up it was a
beautiful sunny day
User -0.062 i
User 0.565 went to see some friends
we had a good time
went to school
User 0.565 had some good grades
on some papers um
wrote a good essay
User 0.292 feel pretty accomplished
and
User 0.312 i feel like my day is just
User 0.565 a good day
Ellie that?s so good to hear
0.3487 = mean
maximum valence in
user segments following
happy lasttime
Ellie (BF describe) how
would your best friend
describe you?
Figure 5: Examples of maximum valence feature.
202
