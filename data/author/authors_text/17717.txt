Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 48?57,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Lemmatization and Morphosyntactic Tagging of Croatian and Serbian
Z?eljko Agic?? Nikola Ljubes?ic?? Danijela Merkler?
?Department of Information and Communication Sciences
?Department of Linguistics
Faculty of Humanities and Social Sciences, University of Zagreb
Ivana Luc?ic?a 3, 10000 Zagreb, Croatia
zagic@ffzg.hr nljubesi@ffzg.hr dmerkler@ffzg.hr
Abstract
We investigate state-of-the-art statistical
models for lemmatization and morphosyn-
tactic tagging of Croatian and Serbian.
The models stem from a new manually
annotated SETIMES.HR corpus of Croa-
tian, based on the SETimes parallel cor-
pus. We train models on Croatian text
and evaluate them on samples of Croat-
ian and Serbian from the SETimes corpus
and the two Wikipedias. Lemmatization
accuracy for the two languages reaches
97.87% and 96.30%, while full morphosyn-
tactic tagging accuracy using a 600-tag
tagset peaks at 87.72% and 85.56%, respec-
tively. Part of speech tagging accuracies
reach 97.13% and 96.46%. Results indicate
that more complex methods of Croatian-to-
Serbian annotation projection are not re-
quired on such dataset sizes for these par-
ticular tasks. The SETIMES.HR corpus, its
resulting models and test sets are all made
freely available.
1 Introduction
Part of speech tagging (POS tagging) is an natu-
ral language processing task in which words are
annotated with the corresponding grammatical cate-
gories ? parts of speech: verb, noun, adjective, pro-
noun, etc. ? in a given context. It is also frequently
called morphosyntactic tagging (MSD tagging, i.e.,
tagging with morphosyntactic descriptions), espe-
cially when addressing highly inflected languages,
for which the tagging process often includes as-
signing additional subcategories to words, such as
gender and case for nouns or tense and person for
verbs. POS/MSD tagging is a well-known task and
an important preprocessing step in natural language
processing. It is often preceded or followed by
lemmatization ? the process of mapping inflected
word forms to corresponding base forms or lemmas.
State of the art in POS/MSD tagging and lemma-
tization across languages is generally achieved ?
both in terms of per token accuracy and speed and
robustness ? by statistical methods, which involve
training annotation models on manually annotated
corpora.
In this paper, we investigate the possibility of uti-
lizing statistical models trained on corpora of Croa-
tian in lemmatization and MSD tagging of Croatian
and Serbian. We present a new manually annotated
corpus of Croatian ? the SETIMES.HR corpus. We
test a number of lemmatizers and MSD taggers on
Croatian and Serbian test sets from two different
domains and consider options of annotation trans-
fer between the two languages. We also outline a
first version of the Multext East v5 tagset and three
usable reductions of this tagset. Special emphasis
is given to rapid resource development and public
availability of our research. Thus, the SETIMES.HR
corpus, the test sets and the best lemmatization and
MSD tagging models are made publicly available.1
In the following section, we discuss related work
on lemmatization and tagging of Croatian and Ser-
bian. We then present the SETIMES.HR corpus and
the test sets, selected lemmatizers and morphosyn-
tactic taggers and the experimental method. Finally
we provide a discussion of the evaluation results
and indicate future work directions.
2 Related work
The task of tagging English sentences with parts of
speech is generally considered a closed issue. This
is due to the fact that, over the course of the past 11
years, from (Brants, 2000) to (S?gaard, 2011), the
current state of the art in tagging English has im-
proved by 1.04 ? to 97.50% in terms of per token
accuracy. This is, however, not the case for lan-
guages with richer morphology and free sentence
1http://nlp.ffzg.hr/resources/models/
48
word order, such as Croatian and Serbian.
Current state of the art for statistical MSD tag-
ging of Croatian is reported at 86.05% (Agic? et
al., 2008). It involves a hidden Markov model tri-
gram tagger CroTag, trained on the Croatia Weekly
100 thousand wordform (100 kw) subcorpus of
Croatian newspaper text from Croatian National
Corpus (Tadic?, 2009), manually MSD-tagged and
lemmatized using the Multext East v3 tagset (MTE
v3) (Erjavec, 2004) and Croatian Lemmatization
Server (Tadic?, 2005) for guided annotation. The
tagger is not publicly available. Just recently, the
Croatia Weekly corpus has been made publicly
available through META-SHARE.2 Another line of
research reports on a prototype constraint grammar
tagger for Croatian (Peradin and S?najder, 2012),
which scores at 86.36% using a MTE-based tagset.
This tagger is also not publicly available as it is in
prototype stage and it currently does not analyze
out-of-vocabulary word forms. The top score for
lemmatizing Croatian text is reported at 96.96%
by combining CroTag and Croatian Morphological
Lexicon (Agic? et al, 2009). The lemmatizer is not
publicly available.
Lemmatization and tagging of Serbian text
was recently addressed in (Gesmundo and
Samardz?ic?, 2012a; Gesmundo and Samardz?ic?,
2012b). It involves BTagger, a combined bidirec-
tional tagger-lemmatizer tool which implements a
lemmatization-as-tagging paradigm. Models are
trained on the Serbian Multext East 1984 corpus,
they are publicly available3 under a permissive li-
cense, reaching overall accuracies of 97.72% for
lemmatization and 86.65% for MSD tagging. It
should be noted, however, that BTagger evaluation
in terms of spatial and temporal complexity was not
documented and that the results provided for Ser-
bian are obtained on specific in-domain data, i.e.,
a corpus of fiction and are thus not directly com-
parable to, e.g., results for Croatian on the Croatia
Weekly newspaper corpus.
Other lines of research in Serbian lemmatization
and tagging exists. Delic? et al (2009) deals with
transformation-based tagging of Serbian text, but
it does not provide state-of-the-art results or freely
available resources. Rule-based approaches to pro-
cessing Serbian using NooJ 4 and similar linguistic
development environments have been thoroughly
2http://metashare.elda.org/
3https://github.com/agesmundo/BTagger
4http://www.nooj4nlp.net/
explored (Vitas et al, 2003). Several resources rel-
evant for Serbian lemmatization and tagging are
provided to the public. The Serbian version of
Jules Verne 60 kw manually lemmatized and MTE-
tagged corpus implements a small deviation from
MTE v4 and deals with specific fictional closed-
vocabulary data. SrpLemKor is a 3.7 Mw corpus of
Serbian newspaper text, automatically lemmatized
and POS-tagged using TreeTagger (Schmid, 1995)
with a tagset of 16 POS tags. A morphological dic-
tionary of 85 thousand Serbian lemmas with sligtly
deviated MTE v4 tagset is available through NooJ.
Public availability of these resources is enabled
through META-SHARE, with somewhat more re-
strictive licensing that involves non-commercial
use in all cases and for some of them it also im-
poses no redistribution.
Related work on lemmatizer and tagger compar-
ison exists for many languages. Restraining the
search to closely related Slavic languages, exten-
sive work in this domain has been done for Bul-
garian (Georgiev et al, 2012), Czech (Spoustova?
et al, 2007) and Slovene (Erjavec and Dz?eroski,
2004; Rupnik et al, 2008). For Croatian, prelim-
inary work on tagger evaluation for tagger voting
has been conducted (Agic? et al, 2010).
3 SETIMES.HR corpus
SETIMES.HR is a new manually lemmatized and
MSD-tagged corpus of Croatian. It is built on top
of the SETimes parallel newspaper corpus involv-
ing 10 languages from the SEE region,5 Croatian
and Serbian included. This initial dataset selection
was deliberate in terms of enabling us with possibil-
ity of cross-lingual annotation projection and other
cross-lingual experiments. SETIMES.HR was anno-
tated by experts using the Croatian Lemmatization
Server (HML)6 (Tadic?, 2005) to facilitate the pro-
cess. We made a number of changes to the initial
annotation provided by human annotators. Namely,
HML provides MSD tags using an undocumented
alteration of the initial MTE tagset, which we cor-
rected to conform entirely to the MTE v4 standard
(Erjavec, 2012). Also, for certain lemmas HML
provides lemmatization with morphosemantic cues
encoded by lemma numbering ? e.g. biti1 (en. to
be) and biti2 (en. to beat) ? which we omitted as
they are used only in the process of generating the
morphological lexicon (Tadic? and Fulgosi, 2003)
5http://www.nljubesic.net/resources/corpora/setimes/
6http://hml.ffzg.hr
49
Corpus Sent?s Tokens Types Lemmas
SETIMES.HR 4 016 89 785 18 089 8 930
set.test.hr 100 2 297 1 270 991
set.test.sr 100 2 320 1 251 981
wiki.test.hr 100 1 887 1 027 802
wiki.test.sr 100 1 953 1 055 795
Table 1: Stats for SETIMES.HR and test sets
and are thus not required for purposes of lemmati-
zation and MSD tagging. We make the resulting 90
kw SETIMES.HR corpus, along with the four test
sets, publicly available under the CC-BY-SA-3.0
license.7 Corpus stats are given in Table 1.
For purposes of this experiment, we propose an
alteration of the baseline MTE v4 tagset in form
of a first version for the MTE v5 standard.8 The
biggest changes in the new version are participal ad-
jectives and adverbs moving from the verbal subset
? which was very complex in v4 ? to the adjectival
and adverbial subsets. Additionally, acronyms are
moved from the abbreviation subset to the noun
subset. A general shrinking of the length of many
tags was performed as well because from v4 on-
wards the MTE standard does not require one tagset
for all languages in the standard. We also suggest
three reductions of the suggested MTE v5 tagset:
1. without adjective definiteness (v5r1),
2. without common (Nc) vs. proper (Np) distinc-
tion for nouns (v5r2) and
3. without both (v5r3).
Adjectival definiteness is a category which is easy
to implement in a morphological lexicon, but is
very hard to distinguish in context as many of its
variants are homographs. We question the distinc-
tion between common and proper nouns as well
since they are contextually very hard to discrimi-
nate. On the other hand, some foreign proper nouns
are inflected by specific paradigms and suffix tries
used on unknown words could profit from this dis-
tinction. Stats for the MTE v5 and the reduced
tagset versions in comparison with the baseline
MTE v4 tagset version of SETIMES.HR are given
in Table 2. They reflect the design choices we
made: MTE v5 has a comparable amount of tags
as MTE v4, gaining additional tags in the adjective
subset, but losing tags in the verb and abbreviation
subsets, while the reductions subsequently lower
the overall MSD tag count.
7http://creativecommons.org/licenses/by-sa/3.0/
8http://nl.ijs.si/ME/V5/msd/html/
set.test wiki.test
Tagset SETIMES.HR hr sr hr sr
MTE v4 660 235 236 188 192
MTE v5 663 233 234 192 195
MTE v5r1 618 213 216 176 180
MTE v5r2 634 216 217 178 181
MTE v5r3 589 196 199 162 166
Table 2: Tagset variation in tag counts
4 Experiment setup
In this section, we define specific experiment goals
and the experiment design. We also present the
datasets and tools used in the experiment.
4.1 Objectives
The principal goal of this experiment is to provide
prospective users with freely available ? download-
able, retrainable and usable, both for research pur-
poses and for commercial use ? state-of-the-art
lemmatization and tagging modules for Croatian
and Serbian. An additional goal of our experi-
ment is to inspect lemmatization and tagging tools
available under permissive licenses and give an
overview regarding their accuracy and time com-
plexity when used on languages of morphological
complexity such as Croatian and Serbian.
Regarding the previously discussed constraints
on existing corpora and tools for Croatian and Ser-
bian tagging and lemmatization, our objective im-
plies exclusive usage of the SETIMES.HR corpus in
the experiment.9 Since SETIMES.HR is part of the
SETimes parallel corpus which, among other lan-
guages, includes both Croatian and Serbian, manu-
ally annotated SETIMES.HR text has a freely avail-
able Serbian equivalent. Our first course of action
was thus to train a number of taggers and lemma-
tizers on SETIMES.HR and test it on Croatian and
Serbian held out text to verify state-of-the-art ac-
curacy on Croatian text and to observe whether
the expected decline in accuracy on Serbian text is
substantial or not.
In case of substantial decrease in accuracy for
lemmatizing and tagging Serbian using Croatian
models, we designed multiple schemes for project-
ing annotation from SETIMES.HR to its Serbian
9Considering corpora of Croatian and Serbian stated in
related work, we chose not to use non-MTE resources and
corpora of fiction as an experiment basis. Importance of en-
coding the full set of morphological features from the MTE
tagset is illustrated by its benefits for dependency parsing of
Croatian (Agic? and Merkler, 2013).
50
equivalent from the SETimes parallel corpus. The
general directions for identifying the bitext sub-
set for annotation projection were using parallel
sentences which have the highest longest common
subsequence or using statistical machine transla-
tion to produce Serbian sentences with minimum
difference to the Croatian counterpart. Projecting
tags on a bitext of high similarity would include
heuristics of annotating the variation with the same
morphosyntactic category if the variation was one
token long or annotating it with the existing model
for tagging if the variation was longer than that.
Lemmatization of the single-token variation would
be reapplied if the token ending in both languages
was identical while other cases would be annotated
with the existing lemmatization model.
4.2 Experiment workflow
We do four batches of experiments:
1. to identify the best available tool and underly-
ing paradigm for lemmatization and tagging
of both languages by observing overall accu-
racy and execution time,
2. to establish the need for annotation projec-
tion from Croatian SETIMES.HR corpus to its
Serbian counterpart,
3. to select the best of the proposed MTE-based
tagsets for both tasks and
4. to provide in-depth evaluation of the selected
top-performing lemmatizer and tagger on both
languages by using the top-performing tagset.
In the first experiment batch, we test the tools only
on Croatian data from SETimes. The second batch
establishes the need for ? or needlessness of ? an-
notation projection for improved processing of Ser-
bian text by testing the tools selected in the first
batch on both languages. The in-depth evaluation
of the third and fourth experiment batch includes,
for both languages and all test sets, observing the
influence of tagset selection to overall accuracy and
investigating tool performance in more detail. We
measure precision, recall and F1 scores for selected
parts of speech and inspect lemmatization and tag-
ging confusion matrices for detailed analysis and
possible prediction of tool operation in real-world
language processing environments.
We aim for the experiment to serve as underly-
ing documentation for enabling prospective users
in implementing more complex natural language
processing systems for Croatian and Serbian by us-
ing these resources. Additionally, the overview of
the usability of tools available is informative for re-
searchers developing basic language technologies
for other languages. We test statistical significance
of observed differences in our results by using the
approximate randomization test.
4.3 Datasets
All models are trained on SETIMES.HR. To at
least partially avoid the possible pitfall of exclu-
sive in-domain testing, we define two test sets for
each language. The first test set consists of 100
Croatian-Serbian parallel sentence pairs taken by
random sampling from the relative complement
of the SETimes parallel corpus and SETIMES.HR.
The second test set is taken from the Croatian and
Serbian Wikipedia by manually selecting 20 match-
ing Wikipedia articles and manually extracting 100
approximate sentence pairs. We chose manual over
random sampling from Wikipedia to account for
the fact that a certain number of articles is virtu-
ally identical between the two Wikipedias due to
language similarity and mutual copying between
Wikipedia users. All four test sets were manually
annotated using the same procedure that was used
for SETIMES.HR. The stats are given in Table 1. In
addition, we have verified the difference between
language test sets by measuring lexical coverage
using HML as a high-coverage morphological lex-
icon of Croatian. For the Croatian SETimes and
Wikipedia samples, we detected 5.2% and 3.9%
out-of-vocabulary word forms and 11.40% and
8.86% were observed for the corresponding Ser-
bian samples, supporting well-foundedness of the
test sets in terms of maintaining the differences
between the two languages.
4.4 Lemmatizers and taggers
As lemmatizers and taggers with permissive licens-
ing schemes and documented cross-lingual state-of-
the-art performance have become largely available,
we chose not to implement our own but to obtain a
set of tools and test them using our data, i.e., train
them on the SETIMES.HR corpus and test them
on Croatian and Serbian SETimes and Wikipedia
test samples. We selected the tools on the basis of
availability and underlying stochastic paradigms as
to identify the best tools and best paradigms.
We tested hidden Markov model trigram taggers
HunPos10 (Hala?csy et al, 2007) and lemmatization-
capable PurePos11 (Orosz and Nova?k, 2012),
10https://code.google.com/p/hunpos/
11https://github.com/ppke-nlpg/purepos
51
Tool Lem. MSD Train (sec) Test (sec)
BTagger 96.22 86.63 24 864.47 87.01
CST 97.78 ? 1.80 0.03
+ lex 97.04 ? 1.87 0.12
HunPos ? 87.11 1.10 0.11
+ lex ? 84.81 10.79 0.45
PurePos 74.40 86.63 5.49 4.42
SVMTool ? 84.99 1 897.08 3.28
TreeTagger 90.51 85.07 7.49 0.19
+ lex 94.12 87.01 17.48 0.31
Table 3: Preliminary evaluation
lemmatization-capable decision-tree-based Tree-
Tagger12 (Schmid, 1995), support vector machine
tagger SVMTool13 (Gime?nez and Ma`rquez, 2004)
and CST?s14 data-driven rule-based lemmatizer (In-
gason et al, 2008). Keeping in mind the previously
mentioned state-of-the-art scores on Serbian 1984
corpus and statistical lemmatization capability, we
also tested BTagger (Gesmundo and Samardz?ic?,
2012a; Gesmundo and Samardz?ic?, 2012b). Since
some lemmatizers and taggers are capable of using
an external morphological lexicon, we used a MTE
v5r1 version of Apertium?s lexicon of Croatian15
(Peradin and Tyers, 2012) where applicable.16 All
tools are well-documented and successfully applied
across languages, as indicated in related work.
5 Results and discussion
A discussion of the experiment results follows in
the next four subsections. Each subsection repre-
sents one batch of experiments. First we select the
best lemmatizer and tagger, next we check for a
need of annotation projection to the Serbian corpus,
then the best MTE-based tagset using the best tool
combination. Finally we provide a more detailed
insight into the results of the top-performing pair
of selected tools and tagset.
5.1 Tool selection
Results of the first experimental batch, consisting
of testing the selected set of lemmatizers and tag-
gers on the MTE v5r1 version of Croatian SETimes
test set, are given in Table 3. In terms of lemmati-
12http://www.cis.uni-muenchen.de/ schmid/tools/TreeTagger/
13http://www.lsi.upc.edu/ nlp/SVMTool/
14http://cst.dk/online/lemmatiser/uk/
15http://www.apertium.org/
16As with already existing Croatian annotated corpora,
HML is not fully MTE compliant. For future work, we might
utilize a compliant version in our experiment and resulting
models, being that its coverage is generally greater than the
one of Apertium?s lexicon due to size difference.
set.test wiki.test
POS hr sr hr sr
HunPos 97.04 95.47 94.25 96.46
+ lex 96.60 95.09 94.62 95.58
MSD
HunPos 87.11 85.00 80.83 82.74
+ lex 84.81 81.59 78.49 79.20
Table 4: Overall tagging accuracy with and without
the inflectional lexicon
set.test wiki.test
Model hr sr hr sr
CST 97.78 95.95 96.59 96.30
+ lex 97.04 95.52 96.38 96.61
Table 5: Overall lemmatization accuracy with and
without the inflectional lexicon
zation and tagging accuracy as well as processing
speed in both training and testing, the top perform-
ing tools are CST lemmatizer and HunPos tagger.
Thus, we chose these two for further investigation
in the following batches of experiments. It should
be noted that, even though its performance is com-
parable to the one of CST and HunPos, BTagger
was not chosen for the other batches primarily be-
cause of its temporal complexity, as it is orders of
magnitude higher than for the selected tools. Given
that lemmatization and tagging are considered pre-
requisites for further processing of text tata, the
data itself often being fed to these modules in large
quantities (e.g., web corpora), we insist on the sig-
nificance of temporal complexity in tool selection.
The other results are comparable with previous re-
search in tagging Croatian. Where applicable, we
tried assisting the tools by providing Apertium?s
lexicon as an optional input for improved lemma-
tization and tagging. Only TreeTagger lemmatiza-
tion and tagging benefited from lexicon inclusion.
However, it should be noted that TreeTagger imple-
ments a very simple approach to lemmatization, as
it only performs dictionary matching and does not
lemmatize unknown words. Inclusion of a larger
lexicon such as HML might be more beneficial for
all the tools.
5.2 Annotation projection
HunPos tagging accuracy on all Croatian and Ser-
bian test sets for both POS only and full MSD is
given in Table 4 for the default variant and for the
52
Tagset set.test wiki.test
POS hr sr hr sr
MTE v4 96.08 94.61 93.96 95.85
MTE v5 97.04 95.52 94.30 96.40
MTE v5r1 97.04 95.47 94.25 96.46
MTE v5r2 97.00 95.60 94.20 96.30
MTE v5r3 97.13 95.56 94.09 96.15
MSD
MTE v4 86.24 83.45 80.45 81.98
MTE v5 86.77 84.48 80.46 82.43
MTE v5r1 87.11 85.00 80.83 82.74
MTE v5r2 87.11 84.96 81.20 82.38
MRE v5r3 87.72 85.56 81.52 82.79
Table 6: HunPos POS and MSD tagging accuracy
for all tagsets
set.test wiki.test
Tagset hr sr hr sr
MTE v4 97.78 95.82 96.66 96.11
MTE v5 97.82 95.86 96.81 96.30
MTE v5r1 97.78 95.95 96.59 96.30
MTE v5r2 97.87 95.99 96.75 96.20
MTE v5r3 97.74 95.99 96.54 96.20
Table 7: CST lemmatization accuracy for all tagsets
one using Apertium?s lexicon. These results serve
as the first decision point regarding the need for
Croatian-to-Serbian annotation projection, the sec-
ond one being the lemmatization scores in Table
5. Here we observed an unsubstantial decrease
in POS and MSD tagging between Croatian and
Serbian test sets ? the observed difference is, in
fact, more substantial across domains than across
languages. Overall, Croatian and Serbian scores
differ less than 3%. Results for Serbian Wikipedia
sample are even consistently better than for Croa-
tian Wikipedia, emphasizing domain significance
over language difference. The tagger does not ben-
efit from the inclusion of the inflectional lexicon
in POS tagging and it even incurs a substantial 2%
to 4% penalty in MSD tagging. Since such obser-
vations were not made while including the lexicon
with the TreeTagger tool ? which implements the
simplest form of dictionary lemmatization ? we
performed a small results analysis and noticed an
unnaturally high percentage of categories that are
as expected present in the lexicon, but very rare in
the training corpus (like the vocative case) point-
ing to a na??ve implementation of the procedure.
Thus we chose not to use the lexicon in further
observations. Lack of more substantial differences
Tagsets v5 v5r1 v5r2 v5r3
v4 0.268 <0.05 <0.05 <0.01
v5 / <0.01 <0.05 <0.01
v5r1 / / 0.877 <0.05
v5r2 / / / <0.01
Table 8: Statistical significance of differences in
full MSD tagging between tagsets (p-values using
approximate randomization)
in tagging scores between Croatian and Serbian
for this specific test scenario implied no need for
annotation projection.
This is further supported by overall lemmatiza-
tion scores in Table 5. Even with the observed
lexical differences between the languages, as we
indicated in the description of the test sets by mea-
suring lexical coverage using HML, the learned
CST lemmatizer rules are more robust consider-
ing language alteration than the trigram tagging
model of HunPos. Lemmatization accuracy stays
in the margins of approximately 97%?1% for both
languages. Average accuracy on Croatian is less
than 2% higher than for Serbian and the domain
patterns observed for tagging are also observed for
lemmatization. Benefits of an inflectional lexicon
for lemmatization are minor, if any, which can be
followed back to the small size of the lexicon and
high quality of the CST lemmatizer. On the con-
trary, TreeTagger?s simple lemmatization does gain
four points by using the lexicon, but it initially
performs seven points worse than CST.
5.3 Tagset selection
Tables 6 and 7 show the influence of tagset de-
sign on tagging and lemmatization accuracy. They
are accompanied by Table 8, i.e., results of testing
statistical significance of differences between the
tagsets in the task of full MSD tagging from Table 6.
Statistical significance is calculated with all test
sets merged into one. Differences in lemmatization
accuracy are virtually non-existent regarding the
tagset choice. Full MSD tagging follows the usual
pattern of inverse proportionality between tagset
size and overall accuracy. It should be noted that
MTE v5 accuracy is not significantly higher than
MTE v4 accuracy (p = 0.268), but we consider the
new tagset to be easier to use for humans since its
tags are shortened by removing placehodlers for
features used in other MTE languages. Consider-
ing that only tagging accuracy using the MTE v5r3
tagset is significantly better than tagging using all
53
Croatian Serbian
POS P R F1 P R F1
Adj 94.33 90.14 92.19 94.34 93.98 94.16
66.80 63.83 65.28 66.79 66.54 66.66
Adv 84.56 82.73 83.63 82.57 73.77 77.92
84.56 82.73 83.63 82.57 73.77 77.92
Conj 95.29 93.82 94.55 97.92 95.29 96.59
94.12 92.66 93.38 96.89 94.28 95.57
Noun 95.70 96.34 96.02 95.42 96.59 96.00
76.78 77.30 77.04 75.38 76.30 75.84
Num 94.57 97.75 96.13 96.51 93.26 94.86
91.30 94.38 92.81 94.19 91.01 92.57
Prep 98.10 99.72 98.90 98.45 98.70 98.57
95.93 97.52 96.72 94.30 94.55 94.42
Pron 95.97 97.54 96.75 95.78 97.42 96.59
81.85 83.20 82.52 81.43 82.83 82.12
Verb 95.88 98.07 96.96 95.23 95.72 95.47
93.81 95.96 94.87 93.36 93.84 93.60
Table 9: Precision (P), recall (R) and F1 score for
POS only (1st column) and full MSD (2nd column)
on Croatian and Serbian
other suggested tagsets, we chose this tagset and
tagging model for further observation of lemmatiza-
tion and tagging properties in the remainder of the
paper. Still, in this section, we present the results
on all tagsets to serve as underlying documentation
of the observed differences, mainly because of the
fact that only MTE v4 is officially supported at
this moment and MTE v5 is a newly-introduced
prototype that displays better performance in this
specific experiment.
5.4 In-depth analysis
In Table 9 we merge SETimes and Wikipedia test
sets by language and provide POS and MSD tag-
ging precision, recall and F1 score for selected
Croatian and Serbian parts of speech. In terms of
POS only, the most difficult-to-tag part of speech
is the adverb, followed by the adjective in both
Croatian and Serbian. The other categories are
consistently POS-tagged with an F1 score of ap-
proximately 95% or higher. The decrease for ad-
verbs and adjectives is somewhat more evident in
precision than in recall and the POS confusion ma-
trix for both languages, given in Table 10, shows
that these two parts of speech are often mistaken
for each other by the tagger. Regarding full MSD
tagging using the MTE v5r3 tagset, for both lan-
guages, the lowest F1 scores are observed for ad-
jectives (approximately 66%), nouns (76%) and
pronouns (82%). This is most likely due to the fact
that these parts of speech have the largest tagset
subsets, making it easier for the tagger to get con-
fused.17 Performance for other parts of speech is
satisfactory, especially for verbs, keeping in mind,
e.g., possible subsequent dependency parsing of the
two languages. The absolute difference between
POS and MSD tagging score is most substantial
for adjectives (approximately 27%), indicating that
certain MSD features might be triggering the de-
crease. This is partially supported by our tagset
design investigation as dropping adjective definite-
ness atribute yielded substantial overall tagging
accuracy increase when compared with the tagsets
in which this attribute is still encoded.
In Table 10 we provide a part of speech confu-
sion matrix for Croatian and Serbian on test sets
merged by language. In Croatian test sets, the most
frequent confusions are those between adjectives
and nouns (28.9%), nouns and verbs (14.5%), ad-
jectives and adverbs (11.6%) and nouns and ad-
verbs (6.9%). In Serbian text, the tagger most fre-
quently confuses nouns ? for adjectives (21.1%),
verbs (20%) and adverbs (16%). Merging the test
sets by language mostly evens out the tagging dif-
ferences as there is a total of 173 MSD confusions
in Croatian test sets and only 3 more, i.e., 175 in
the Serbian test sets.
POS scores for both languages neared the level
of human error in our experiment. Keeping that
in mind, upon observing the confusion instances
themselves, we spotted a confusion between adjec-
tives and nouns (e.g. names of countries (Hrvatska
(en. Croatia, Croatian)), homographic forms
(strana (en. foreign, side), svet (en. world, holy))
and confusion between adjectives and adverbs. Ad-
verbs and prepositions are sometimes confused
with nouns, especially for nouns in instrumental
case (e.g. godinama (en. year, yearly), tijekom
(en. duration, during)). Conjuctions are at times
incorrently tagged because various words can have
a conjuctional function, most frequently pronouns
and adverbs: s?to (en. what), kako (en. how), kada
(en. when). Interestingly, there is some confusion
between nouns and verbs in Wikipedia test sets,
while in SETimes test sets there are almost none.
This confusion arises from the homographic forms
? e.g. mora (en. must, seas) ? or from nouns with
17There are 589 MTE v5r3 tags in SETIMES.HR. Out of
these, 164 are used for tagging adjectives, 42 for nouns and
268 for pronouns, thus accounting for 80.47% of the tagset.
There are also 50 verb tags.
54
POS Abbr Adj Adv Conj Noun Num Part Prep Pron Res Verb
Abbr 0 0 0 1 3 0 0 0 0 0
Adj 0 20 0 50 0 1 0 3 1 4
Adv 0 10 9 12 0 0 2 0 0 2
Conj 0 0 5 2 0 5 5 7 0 0
Noun 0 37 28 0 4 0 1 5 7 25
Num 2 4 0 0 2 0 0 0 0 0
Part 0 0 0 3 0 0 0 0 0 3
Prep 0 0 2 3 2 0 1 0 0 0
Pron 0 2 1 9 3 0 1 0 0 1
Res 0 0 1 0 4 0 0 2 0 0
Verb 0 9 4 0 35 1 2 1 0 1
Table 10: POS confusion matrix for Croatian (top right) and Serbian (bottom left)
Figure 1: Learning curves for Croatian and Serbian lemmatization and tagging
suffixes -la and -lo, which are used for denoting
participles in feminine and neuter gender, or with
suffix -ti, which is also a suffix for infinitive.
Most MSD tag confusions arise from the fact that
the same suffix can denote different cases in dif-
ferent declensions. We observed confused number
and gender category (mostly in adjectives in mas-
culine and neuter gender), but the most frequent
confusion occurs for accusative forms in masculine
gender, which have different suffixes when they de-
note animacy (suffix is the same as in the genitive
case: pobjednika (en. winner), kandidata (en. can-
didate)) and when they denote inanimacy (suffix
is the same as in the nominative case: metak (en.
bullet), bubnjar (en. drummer)).
In lemmatization, as in POS tagging, errors are
generally very infrequent. Some occur with adjec-
tives, when an assigned lemma represents a definite
form of an adjective, instead of an indefinitive form
(and less frequently vice versa). Besides, adjec-
tives are sometimes confused with adverbs (e.g.,
target lemma is znac?ajno (en. significantly), but
the lemma znac?ajan (en. significant) is assigned,
and vice versa). Other less frequent examples in-
clude cases in which the assigned lemma is not in
its canonical form, but a case other than the nomi-
native case, or when the assigned lemma is a word
stem. A small number of errors also occurs due
to slight differences in Croatian and Serbian word-
forms, e.g., when a Serbian nominative form is not
a nominative form in Croatian (planeta as Serbian
nominative and Croatian genitive, planet being the
Croatian nominative).
Figure 1 provides lemmatization, POS and MSD
tagging learning curves for both languages on
merged test sets. Apart from the slight difference
in lemmatization scores in favor of Croatian, the
learning curves and overall scores on merged test
sets are virtually identical. The easiest task to learn
is lemmatization while the most complex one is
applying MSD.
6 Conclusions and future work
In this paper, we have addressed the issue of lemma-
tization and morphosyntactic tagging of two gener-
ally under-resourced languages, Croatian and Ser-
bian. Our goal was to provide the general public
with freely available language resources and state-
55
of-the-art models for lemmatization and tagging of
these two languages in terms of accuracy, robust-
ness and speed. We also aimed at using lemmati-
zation and tagging as a platform for implicit com-
parison of the two languages in natural language
processing terms, as to provide partial insight to
how difficult and lossy ? or, more desireably, how
easy and straightforward ? would it be to port lin-
guistic resources and language processing tools
from one language to another.
While developing the models, we completed a
series of experiments. We used the Croatian text
from the freely available SETimes parallel corpus
to create a new manually lemmatized and mor-
phosyntactically tagged corpus of Croatian ? the
SETIMES.HR corpus. Beside the Multext East v4
morphosyntactic tagset specification for Croatian
which was used for initial corpus annotation, we
designed and implemented a first version of the
Multext East v5 tagset and its three reductions and
applied these to SETIMES.HR. Using SETimes
and Wikipedia as starting point resources, we cre-
ated two gold standard test sets for each language
in order to test existing state-of-the-art lemmatiz-
ers and taggers. We ran preliminary tests on a
number of tools to select CST lemmatizer and
HunPos tagger as tools of choice considering ob-
served accuracy, training time and text processing
time. In an in-depth evaluation of these tools, we
obtained peak overall lemmatization accuracy of
97.87% and 96.30% for Croatian and Serbian and
full morphosyntactic tagging accuracy of 87.72%
and 85.56%, with basic part of speech tagging ac-
curacy at 97.13% and 96.46%. In this specific test
scenario and with this specific training set, we have
shown the differences in results between Croatian
and Serbian not to be significant enough to justify
an effort in more elaborate strategy of adapting
Croatian models to Serbian data ? simply training
the models on Croatian text from SETIMES.HR
corpus and using them on Serbian text provided
state-of-the-art results in lemmatization and tag-
ging, while maintaining and even topping previ-
ously documented state of the art for Croatian.
The SETIMES.HR corpus, Croatian and Serbian
test sets and top-performing lemmatization and tag-
ging models are publicly available and freely down-
loadable18 under the CC-BY-SA-3.0 license.
Our future work plans include both enlarging
and enhancing SETIMES.HR. The presented learn-
18http://nlp.ffzg.hr/resources/models/
ing curves show significant room for improvement
by annotating additional data. The dataset aleady
serves as a basis for the SETIMES.HR treebank of
Croatian (Agic? and Merkler, 2013), implementing
a novel dependency syntactic formalism and en-
abling experiments with joint dependency parsing
of Croatian and Serbian. Should dependency pars-
ing experiments show the need for more elaborate
language adaptation strategies, we will most likely
implement them also on the level of lemmas and
morphosyntactic tags before addressing syntactic
issues. This will possibly be helped by statistical
machine translation between Croatian and Serbian
to enhance bitext similarity and empower projec-
tion strategies. An effort could be made to adapt
existing Croatian and Serbian resources and subse-
quently to attempt achieving better lemmatization
and tagging performance by combining these with
SETIMES.HR. We will use the models presented in
this paper to annotate the web corpora of Croatian
and Serbian (Ljubes?ic? and Erjavec, 2011) ? hrWaC
and srWaC.
Acknowledgement
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 un-
der grant agreement n? PIAP-GA-2012-324414
(project Abu-MaTran).
References
Z?eljko Agic? and Danijela Merkler. 2013. Three Syn-
tactic Formalisms for Data-Driven Dependency Pars-
ing of Croatian. In Text, Speech and Dialogue. Lec-
ture Notes in Computer Science. Springer.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2008. Improving Part-of-Speech Tagging Accuracy
for Croatian by Morphological Analysis. Informat-
ica, 32(4):445?451.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2009. Evaluating Full Lemmatization of Croatian
Texts. In Recent Advances in Intelligent Information
Systems, pages 175?184. Exit Warsaw.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2010. Tagger Voting Improves Morphosyntactic
Tagging Accuracy on Croatian Texts. In Proceed-
ings of ITI, pages 61?66.
Thorsten Brants. 2000. TnT: A Statistical Part-of-
Speech Tagger. In Proceedings of ANLP, pages 224?
231.
56
Vlado Delic?, Milan Sec?ujski, and Aleksandar Ku-
pusinac. 2009. Transformation-Based Part-of-
Speech Tagging for Serbian Language. In Proceed-
ings of CIMMACS.
Tomaz? Erjavec and Sas?o Dz?eroski. 2004. Machine
Learning of Morphosyntactic Structure: Lemmatiz-
ing Unknown Slovene Words. Applied Artificial In-
telligence, 18:17?41.
Tomaz? Erjavec. 2004. MULTEXT-East Version 3:
Multilingual Morphosyntactic Specifications, Lexi-
cons and Corpora. In Proceedings of LREC.
Tomaz? Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic Resources for Central and Eastern European
Languages. Language Resources and Evaluation,
46(1):131?142.
Georgi Georgiev, Valentin Zhikov, Kiril Simov, Petya
Osenova, and Preslav Nakov. 2012. Feature-Rich
Part-of-speech Tagging for Morphologically Com-
plex Languages: Application to Bulgarian. In Pro-
ceedings of EACL, pages 492?502.
Andrea Gesmundo and Tanja Samardz?ic?. 2012a. Lem-
matisation as a Tagging Task. In Proceedings of
ACL.
Andrea Gesmundo and Tanja Samardz?ic?. 2012b. Lem-
matising Serbian as Category Tagging with Bidirec-
tional Sequence Classification. In Proceedings of
LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS Tagger Generator Based on Support
Vector Machines. In Proceedings of LREC.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.
2007. HunPos: An Open Source Trigram Tagger.
In Proceedings of ACL, pages 209?212.
Anton Karl Ingason, Sigru?n Helgado?ttir, Hrafn Lofts-
son, and Eir??kur Ro?gnvaldsson. 2008. A Mixed
Method Lemmatization Algorithm Using a Hierar-
chy of Linguistic Identities (HOLI). In Proceedings
of GoTAL, pages 205?216.
Nikola Ljubes?ic? and Tomaz? Erjavec. 2011. hrWaC
and slWaC: Compiling Web Corpora for Croatian
and Slovene. In Text, Speech and Dialogue, pages
395?402. Springer.
Gyo?rgy Orosz and Attila Nova?k. 2012. PurePos ?
An Open Source Disambiguator. In Proceedings of
NLPCS.
Hrvoje Peradin and Jan S?najder. 2012. Towards a
Constraint Grammar Based Morphological Tagger
for Croatian. In Text, Speech and Dialogue, pages
174?182. Springer.
Hrvoje Peradin and Francis M. Tyers. 2012. A Rule-
Based Machine Translation System from Serbo-
Croatian to Macedonian. In Proceedings of
FREERBMT12, pages 55?65.
Jan Rupnik, Miha Grc?ar, and Tomaz? Erjavec. 2008.
Improving Morphosyntactic Tagging of Slovene
Language Through Meta-Tagging. Informatica,
32(4):437?444.
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging With an Application to German. In
Proceedings of ACL SIGDAT Workshop.
Anders S?gaard. 2011. Semi-Supervised Condensed
Nearest Neighbor for Part-of-Speech Tagging. In
Proceedings of ACL-HLT, pages 48?52.
Drahom??ra ?johanka? Spoustova?, Jan Hajic?, Jan
Votrubec, Pavel Krbec, and Pavel Kve?ton?. 2007.
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech. In Proceedings
of BSNLP, pages 67?74.
Marko Tadic? and Sanja Fulgosi. 2003. Building the
Croatian Morphological Lexicon. In Proceedings of
EACL 2003 Workshop on Morphological Processing
of Slavic Languages, pages 41?46.
Marko Tadic?. 2005. Croatian Lemmatization Server.
Southern Journal of Linguistics, 29(1):206?217.
Marko Tadic?. 2009. New Version of the Croatian Na-
tional Corpus. After Half a Century of Slavonic Nat-
ural Language Processing, pages 199?205.
Dus?ko Vitas, Cvetana Krstev, Ivan Obradovic?,
Ljubomir Popovic?, and Gordana Pavlovic?-Laz?etic?.
2003. An Overview of Resources and Basic Tools
for Processing of Serbian Written Texts. In Pro-
ceedings of the Workshop on Balkan Language Re-
sources, 1st Balkan Conference in Informatics.
57
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 22?33,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Parsing Croatian and Serbian by Using Croatian Dependency Treebanks
Z?eljko Agic?? Danijela Merkler? Das?a Berovic??
?Department of Information and Communication Sciences
?Department of Linguistics
Faculty of Humanities and Social Sciences, University of Zagreb
Ivana Luc?ic?a 3, 10000 Zagreb, Croatia
zagic@ffzg.hr dmerkler@ffzg.hr dberovic@ffzg.hr
Abstract
We investigate statistical dependency parsing
of two closely related languages, Croatian and
Serbian. As these two morphologically com-
plex languages of relaxed word order are gen-
erally under-resourced ? with the topic of de-
pendency parsing still largely unaddressed, es-
pecially for Serbian ? we make use of the
two available dependency treebanks of Croa-
tian to produce state-of-the-art parsing models
for both languages. We observe parsing accu-
racy on four test sets from two domains. We
give insight into overall parser performance
for Croatian and Serbian, impact of prepro-
cessing for lemmas and morphosyntactic tags
and influence of selected morphosyntactic fea-
tures on parsing accuracy.
1 Introduction
Croatian and Serbian are very closely related South
Slavic languages with complex morphology and rel-
atively free word order. They are mutually intel-
ligible with one another, as well as with Bosnian
and Montenegrin, amounting for more than 20 mil-
lion native speakers.1 Regarding language technol-
ogy support, they are considered to be generally
under-resourced. More specifically, while a cor-
pus of research on processing Croatian and Ser-
bian on the morphosyntactic and shallow syntactic
1Bekavac et al (2008) provide a corpus-based comparison
of Bosnian, Croatian and Serbian, observing similarities and
differences in morphology, syntax and semantics. For further
insight regarding Croatian and Serbian morphosyntax, see the
respective contemporary grammars (Silic? and Pranjkovic?, 2005;
Stanojc?ic? and Popovic?, 2008).
layer does exist (Tadic? et al, 2012; Vitas et al,
2012), approaches to full syntactic analysis of the
two languages were up to this point very sparse
and very recent (Agic? and Merkler, 2013). As lin-
guistic tradition supports dependency-based syntac-
tic formalisms for the two languages (Bo?hmova? et
al., 2003; Tadic?, 2007), it should be noted that
they have not participated in the previous collabo-
rative research efforts in dependency parsing, such
as the CoNLL shared tasks (Buchholz and Marsi,
2006; Nivre et al, 2007). Furthermore, regardless of
the specific research topic, the communities dealing
with natural language processing of Croatian, Ser-
bian and other closely related languages from their
group are still to reach the common level of aware-
ness with respect to public availability of their re-
search. Contributions to availability of Croatian and
Serbian resources have once again been very few
and recent (Tadic? and Varadi, 2012), especially for
free culture licensing.
Through the line of research we propose here,2 we
seek to provide state-of-the-art in dependency pars-
ing for both Croatian and Serbian. In this first group
of experiments, we build on the fact of their close
relatedness by using the two Croatian treebanks ?
Croatian Dependency Treebank (Tadic?, 2007) and
SETIMES.HR Treebank (Agic? and Merkler, 2013) ?
to build unified parsing models and evaluate them
across the languages and domains. As we deal with
highly inflectional languages, we also investigate
the influence of morphological preprocessing and
morphosyntactic feature selection on parsing perfor-
2This work was partly financed by the EU FP7 STREP
project XLike (FP7-288342).
22
mance. We aim to use this first inquiry as a decision
point regarding further advancements in resource in-
terchangeability in terms of, e.g., annotation projec-
tion (Yarowsky et al, 2001) and domain adaptation
(S?gaard, 2013). Availability is highly emphasized,
as we provide our resources and models to the public
under the CC-BY-SA-3.0 license.3 We stress the es-
sential role of free culture licensing in enabling and
maturing NLP for under-resourced languages.
In the following section, we give an overview of
related work in computational processing of Croat-
ian and Serbian morphology and syntax. Further, we
define the experiment objectives and describe the re-
sources and experiment workflow. We elaborate on
the obtained results and conclude by sketching pos-
sible future research plans.
2 Related work
Two overviews of current state of language technol-
ogy development have appeared just recently for the
two languages we investigate in this paper.
The Croatian overview (Tadic? et al, 2012) states
that a few underperforming shallow parsing proto-
types for Croatian do exist (Vuc?kovic? et al, 2008),
while deep parsing is left completely unaddressed.
In contrast, it indicates that the more basic re-
sources ? manually annotated corpora, inflectional
lexicons, lemmatizers, morphosyntactic and named
entity taggers ? are of higher quality and availability.
Most of these are available through META-SHARE
(Tadic? and Varadi, 2012). However, in terms of
mandatory preprocessing for dependency parsing, to
the best of our knowledge, the only freely avail-
able and standard-compliant lemmatization, part-of-
speech (POS) or morphosyntactic (MSD) tagging
resources are those by (Agic? et al, 2013).4 Their
elaboration contains a more substantial overview of
preprocessing. Relevant to our research, these mod-
els provide the state of the art in preprocessing for
both Croatian and Serbian.
Croatian Dependency Treebank (HOBS) project
was initiated by (Tadic?, 2007). However, its suffi-
ciency in size increase, followed by the first experi-
ments with dependency parsing of Croatian, did not
appear soon enough to be included in the CoNLL
3http://creativecommons.org/licenses/by-sa/3.0/
4http://nlp.ffzg.hr/resources/models/tagging/
shared tasks and the overview of (Tadic? et al,
2012). Preliminary experiments in transition-based
(Berovic? et al, 2012) and graph-based parsing have
been augmented by a hybrid approach which in-
cluded integrating a graph-based parser (Hall, 2007)
and a valency lexicon (Agic?, 2012). Due to uncov-
ered partial inadequacies of the HOBS formalism
at describing certain syntactic properties of Croa-
tian, a new line of research was initiated, aiming
at creating a more simplistic dependency-based for-
malism for data-driven parsing of Croatian (Agic?
and Merkler, 2013). It provided a new freely avail-
able dependency treebank, the SETIMES.HR Tree-
bank, and derived state-of-the-art dependency pars-
ing models.5 On the downside, SETIMES.HR is a
prototype with currently less than 2 500 sentences
and a documented need for addressing certain an-
notation challenges, such as consistent annotation
of complex predicates, an issue that was previously
observed and partially resolved in HOBS as well
(Berovic? et al, 2012).
The overview of Serbian language technologies
(Vitas et al, 2012) explicitly denotes a satisfac-
tory development level for Serbian preprocessing
based on large electronic dictionaries, manually an-
notated corpora and hand-crafted transducer gram-
mars. These are available through META-SHARE,
even if mostly coupled with restrictive licensing.
Further, the overview lists some preliminary re-
search in shallow syntactic analysis, while it clearly
states that the absence of a formalised syntax of Ser-
bian restricts the development of syntactically anno-
tated corpora and thus hinders the research in full
parsing of Serbian, making the creation of a syntac-
tic formalism for Serbian a very urgent task.
Similar to Croatian, research in Serbian shallow
parsing deals exclusively with the manual design of
rule-based modules (Nenadic?, 2000; Nenadic? et al,
2003; Vitas et al, 2003) in linguistic development
enviroments such as Intex and NooJ (Silberztein,
2004). We also inquired into a case study on the pos-
sibilities of resource transfer from English to Ser-
bian (Martinovic?, 2008), only to conclude that it
does not provide any empirical results. Hence, to
the best of our knowledge, no experiments in de-
pendency treebank construction and data-driven de-
5http://nlp.ffzg.hr/resources/corpora/setimes-hr/
23
pendency parsing ? or, for that matter, any other ap-
proaches to deep syntactic modeling and processing
? currently exist for Serbian.
3 Experiment setup
In this section, we present the experimental setup by
which we aim at subsequently addressing the pre-
viously outlined issues with dependency parsing of
Croatian and Serbian. We define our goals, describe
the utilized resources and lay out the workflow.
3.1 Objectives
We identify the main issues unaddressed by previous
research in Croatian and Serbian syntactic process-
ing and use these to define our research objectives.
They are listed here as follows.
1. No empirical research was conducted in de-
pendency parsing of Serbian. Even if this fact
was justified by the lack of applied research in
creating formalisms targeted exclusively at de-
scribing syntactic properties of Serbian, we fol-
low the underspecification approach that was
successfully implemented in HOBS for Croa-
tian. Namely, as the Prague Dependency Tree-
bank (PDT) formalism for Czech (Bo?hmova?
et al, 2003) was altogether ported to Croatian
by simply using the PDT annotation manual
for annotating Croatian sentences due to mi-
nor differences in syntactic structure between
Croatian and Czech, we reflect this to the
even greater similarity between Croatian and
Serbian on all levels of linguistic description.
Hence, we use Croatian data to parse Serbian
and to serve as a baseline in Serbian parsing.
2. Using Croatian syntactic models for parsing
Serbian text serves to establish the need for ad-
vanced approaches to porting resources among
languages, such as annotation projection.
3. The best dependency parsing models for Croa-
tian are created and tested using a small pro-
totype treebank. SETIMES.HR currently pro-
vides state of the art in Croatian dependency
parsing. To serve our experiment, we enlarge
it by 50% by following the annotation guide-
lines (Merkler et al, 2013) and provide its new
version to the public.
4. Previous experiments were conducted by ten-
fold cross-validation on treebank data. This is
a standard approach to dependency parser eval-
uation, especially in under-resourced environ-
ments. In this setting, observations are posi-
tively biased by text domain and phrase trans-
fer due to randomization. We seek to partially
account for these effects by designing a set of
language- and domain-aware test samples. By
these we also target at establishing the need for
domain adaptation for parsing.
5. No research was done in investigating the ef-
fects of preprocessing and linguistic feature se-
lection to dependency parsing for these lan-
guages. As these are highly inflectional, hav-
ing very large morphosyntactic tagsets, we seek
to inspect the impact of preprocessing choices
on their dependency parsing. There is am-
ple research on the effect preprocessing has
on dependency parsing (Goldberg and Elhadad,
2009; Mohamed, 2011) and on joint morpho-
logical and syntactic processing (Bohnet and
Nivre, 2012), but none of it included any of the
South Slavic languages.
3.2 Workflow
We define three batches of experiments to meet the
research objectives:
1. to select the best Croatian dependency formal-
ism with respect to its overall parsing accuracy
on Croatian and Serbian ? with an emphasis
on the most important syntactic categories that
match across formalisms ? and incidentally to
establish the need for annotation projection,
2. to inspect the impact of state-of-the-art auto-
matic preprocessing on dependency parsing of
both languages and
3. to establish the importance of specific Croat-
ian and Serbian morphosyntactic features of the
most frequent parts of speech in modeling syn-
tactic fenomena for dependency parsing.
In the first batch, we use HOBS in two instances
and SETIMES.HR to create parsing models and test
them on Croatian and Serbian test samples. Draw-
ing from previous research, we use a standard non-
projective graph-based MSTParser generator with
second-order features (McDonald et al, 2006), as
this setting favors Croatian (Agic?, 2012) and re-
24
lated languages such as Czech and Slovene (Buch-
holz and Marsi, 2006). We are aware of the exis-
tence of novel dependency parsers that implement
approaches to handling non-local dependencies and
outperform MSTParser on a set of languages, such
as (Bohnet and Nivre, 2012). They are not included
here due to temporal constraints and the fact that
we were provided with prebuilt MSTParser models
for the HOBS instances and needed to ensure their
comparability with SETIMES.HR. As we mainly
deal with the concept of resource sharing between
closely related languages, we assign a more elabo-
rated parser selection for future research.
For the second batch, we redo the experiments
from the first batch in a realistic scenario regard-
ing preprocessing. We use the publicly available
state-of-the-art tagging and lemmatization models
for Croatian and Serbian (Agic? et al, 2013) instead
of manual annotation to observe the incurred ef-
fects. We do both batches for all three formalisms
(two HOBS instances and SETIMES.HR) and pro-
vide learning curves.
The third batch of experiments deals with ob-
serving the impact of certain morphosyntactic fea-
tures by removing them from training and test data.
We inspect all features involved in subspecification
of adjectives, nouns and verbs in compliance with
the Multext East specification (Erjavec, 2012), i.e.,
MTE v5 as its fifth release.6
In all batches, we observe labeled (LAS) and un-
labeled (UAS) attachment scores. We use approxi-
mate randomization for statistical significance test-
ing where applicable and meaningful.
3.3 Treebanks
Two Croatian dependency treebanks are used in this
experiment: HOBS (Tadic?, 2007) and SETIMES.HR
(Agic? and Merkler, 2013).
HOBS is available in two instances or implemen-
tations. The first one closely follows the PDT anno-
tation guidelines (Bo?hmova? et al, 2003) with several
adaptations of predicate annotation (Berovic? et al,
2012). The second one introduces a set of additional
syntactic tags used for the introduction and subclas-
sification of subordinate clauses. It also alters the
head attachment rules for subordinating conjunc-
6http://nl.ijs.si/ME/V5/msd/html/
Features HOBS HOBS + Sub SETIMES.HR
Sentences 4 626 4 626 3 853
Tokens 117 369 117 369 86 991
Types 25 038 25 038 17 723
Lemmas 12 388 12 388 8 773
MSD tags 914 911 662
Syn. tags 27 (70) 28 (81) 15
Table 1: Basic treebank statistics. Syntactic tag counts
are given for the basic and the full tagset (the latter inside
brackets) for the two HOBS treebanks.
set.test wiki.test
Features hr sr hr sr
Sentences 100 100 100 100
Tokens 2 285 2 308 1 878 1 947
Types 1 265 1 246 1 027 1 055
Lemmas 989 979 803 797
MSD tags
MTE v4 tags 236 237 189 193
MTE v5 tags 233 234 192 195
Syntactic tags
HOBS 22(37) 23(37) 22(41) 22(44)
HOBS + Sub 22(46) 24(49) 23(49) 22(50)
SETIMES.HR 15 15 15 15
Table 2: Basic statistics for the four test sets. Morphosyn-
tactic and syntactic tag counts are given with respect to
the formalism used.
tions. This addition enabled consistency in predicate
annotation in clauses and an increase in dependency
parsing accuracy (Agic? and Merkler, 2013), while
taking a turn away from the PDT guidelines and to-
wards specifics of Croatian syntax. In the paper,
we refer to this instance of HOBS as HOBS + Sub.
Both of them are based on Croatian newspaper text
and manually preprocessed. They implement a mor-
phosyntactic tagset based on, but slightly deviated
from MTE v4 (Erjavec, 2012). HOBS is available
from META-SHARE for research purposes, but its
syntactic tags are stripped from this version. HOBS
+ Sub is not publicly available. Both have been
made available to us in whole for conducting this
experiment, along with prebuilt MSTParser models
compatible with our experimental settings.
SETIMES.HR is based on Croatian newspaper text
25
from the SETimes parallel corpus.7 It implements a
simplistic new formalism (Merkler et al, 2013) tar-
geting and reaching increased dependency parsing
performance while maintaining the information on
the main syntactic categories and compliance with
the general guidelines for HOBS for these categories
(Agic? and Merkler, 2013). It is also manually pre-
processed, but using the newer MTE v5 morphosyn-
tactic tagset. SETIMES.HR is fully compliant with
this tagset. As mentioned, it is freely available for all
purposes. With this in mind, following the annota-
tion guidelines, we have expanded its 2 500 sentence
prototype by introducing 1 365 new sentences.
Treebank statistics are given in Table 1. HOBS
treebanks are larger than SETIMES.HR by approxi-
mately 800 sentences, i.e., 30 thousand tokens (30
kw). The morphosyntactic tagsets also differ, favor-
ing SETIMES.HR and MTE v5 by 250 tags if we are
to consider the smaller tagset as better in terms of
the expressivity vs. preprocessing accuracy balanc-
ing. Syntactic tagset of SETIMES.HR has only 15
tags. Tag counts for HOBS treebanks are given by
two figures: the first one represents the basic tagset,
while the second one includes the subclassification
tags. For example, a coordinated predicate is anno-
tated as Pred using the basic tagset and as Pred Co
in the full tagset. Here, we use only the basic tagset.
As we anticipated given the properties of Croat-
ian syntax, non-projectivity is amply present in both
treebanks. Approximately 2% of all dependency re-
lations and more than 20% of all sentences are non-
projective, supporting our parser selection.
As the three treebanks ? HOBS, HOBS + Sub and
SETIMES.HR? formally do implement different ap-
proaches to syntactic modeling, issues may be raised
regarding the comparability of dependency parsing
scores. However, since HOBS and HOBS + Sub are
both based on the PDT formalism and SETIMES.HR
implements a simplistic formalism that is still based
on the PDT and HOBS annotation guidelines and
syntactic tagset reduction (Merkler et al, 2013), we
consider the comparison to be valid. Moreover, all
three formalisms encode the main Croatian syntactic
categories by closely following the general guide-
lines for describing the Croatian syntax (Silic? and
Pranjkovic?, 2005), thus indicating that comparisons
7http://opus.lingfil.uu.se/SETIMES2.php
for the main syntactic categories ? such as predi-
cates, subjects, objects, prepositional and adverbial
phrases ? should hold true for the task of depen-
dency parsing irregardless of the formal differences
between the models.
3.4 Test sets
The publicly available test sets are obtained from an
experiment in lemmatization and tagging of Croat-
ian and Serbian (Agic? et al, 2013). They were avail-
able in MTE v4 and v5. As HOBS uses the former
and SETIMES.HR the latter tagset, they were well-
suited for our experiment. We syntactically anno-
tated the test sets threefold, i.e., by using the HOBS,
HOBS + Sub and SETIMES.HR formalisms. There
are four test samples: Croatian and Serbian paral-
lel sentences from newspaper sources (set.test) and
Wikipedia (wiki.test). Their suitability for testing
models on closely related languages was thoroughly
elaborated by (Agic? et al, 2013), where their dif-
ferences were measured by using inflectional lexi-
cons of Croatian and Serbian and were found to be
significant in supporting the difference between the
languages. Namely, lexical coverage differed by ap-
proximately 10 percentage points in favor of Croat-
ian across the two domains.
Statistics for the test set are given in Table 2. Each
sample has 100 sentences or approximately 2 000
tokens. Slight variations in token, type and lemma
counts are present and reflect the domain differ-
ences. MSD tag and syntactic tag counts reflect the
respective formalisms, as not all HOBS and HOBS
+ Sub syntactic tags are utilized, while all 15 SE-
TIMES.HR tags are present in all the samples. HOBS
tag counts are once again given separately for the
basic and the full tagset, while only the basic subset
was used in the experiment.
Inter-annotator agreement for HOBS, HOBS +
Sub and SETIMES.HR is investigated in (Agic? and
Merkler, 2013). It favors SETIMES.HR over HOBS
+ Sub and HOBS + Sub over HOBS with a statis-
tically significant difference. The CoNLL shared
tasks in dependency parsing (Buchholz and Marsi,
2006; Nivre et al, 2007) used test sets of approx-
imately 5 000 tokens. This may raise an issue re-
garding the relatively small size of our domain test
samples. However, in the experiment, we combine
the test sets by domain and by language and also
26
set.test wiki.test
LAS hr sr hr sr overall
HOBS 59.9 58.7 55.5 55.4 57.6
HOBS + Sub 68.3 66.9 62.4 62.7 65.3
SETIMES.HR 76.7 75.4 71.9 72.4 74.3
UAS
HOBS 73.7 75.9 72.3 72.6 73.8
HOBS + Sub 78.1 79.0 76.5 76.5 77.6
SETIMES.HR 81.6 80.6 80.0 80.6 80.8
Table 3: Parsing accuracy (LAS, UAS) with manual pre-
processing. Results are given for each test set and overall,
i.e., with all four test sets merged into one.
merge them into a single test set, thus accounting
for the size of the individual samples.
3.5 Parser setup
Here we use MSTParser with the non-projective
maximum spanning tree parsing algorithm and sec-
ond order features (decode-type:non-proj
order:2 training-k:5 iters:10), as it
was previously established as the optimal set-
ting for parsing Croatian using MSTParser (Agic?,
2012) with a statistically significant margin over
the transition-based approach. In training and test-
ing, we separate the MTE v5 MSD tags into POS
(CPOSTAG) and full MSD (POSTAG). We do not
separate the MSD tags into atomic features, i.e., we
do not utilize the FEATS column of the CoNLL-X
format. Thus the MSD tags themselves are consid-
ered as atomic features in the experiment, both for
the full MTE v5 tagset and its reductions.
4 Results and discussion
Here we report and discuss the obtained results. We
discuss the results in batches, as in the experiment
workflow description. In addition, we give a brief
linguistic analysis of the parsing errors considering
the difference between the two languages and the
fact that Croatian models were used for parsing both
Croatian and Serbian text.
4.1 Formalism selection
In the first experiment batch, we trained the parsing
models using three treebanks, HOBS, HOBS + Sub
and SETIMES.HR, and tested them on our Croatian
and Serbian test sets from Wikipedia and newspa-
per text. We present the overall scores in Table 3,
the learning curves are plotted in the first diagram
of Figure 1 and the accuracy for selected syntactic
categories are given in Table 4.
Regarding the formalism selection process, in-
specting the overall observed LAS and UAS, it is
evident that models based on SETIMES.HR outper-
form HOBS-based models by a large margin. They
outperform HOBS + Sub by approximately 9 LAS
and 3 UAS points, while their overall advantage is
even more substantial in comparison with the scores
of basic HOBS models ? approximately 17 LAS
and 7 UAS points. Benefits of explicit annotation
of predicates by introducing tags for subordinating
syntactic conjunctions are also evident as HOBS
+ Sub parsers outperform HOBS by 8 LAS and 4
UAS points. These observations maintain the con-
clusions about the three formalisms given in previ-
ous research (Agic? and Merkler, 2013).8 Moreover,
the introduction of a held-out test set further steep-
ens these differences, as the previous tests were per-
formed by tenfold cross-validation using treebank
data only. The observed differences in overall LAS
and UAS scores are shown to be significant by the
approximate randomization test (p < 0.01).9
As stated in the presentation of treebanks in the
previous section, since the three formalisms are
closely related to one another and to the general
guidelines for describing the properties of Croatian
dependency syntax, we find this comparison to hold
true regardless of the formal differences between
the models. Moreover, since the accuracy for the
PDT-based formalisms in this and previous experi-
ments with Croatian dependency parsing (Agic? and
Merkler, 2013) is below the margins set by similar
languages such as Czech and Slovene (Buchholz and
8Importance of standard compliance should be noted regard-
ing the morphosyntactic tagset impact on the observed results.
Namely, HOBS ?slightly deviates? from MTE v4 by design,
while still claiming de facto compliance. As the test sets fully
comply with MTE v4 and v5, this has an effect on parsing.
9We test by randomly (prob = 0.5) inserting alternate syn-
tactic annotations for entire test set sentences and evaluating
with respect to annotation style, i.e., selecting to match the sen-
tence annotations against HOBS, HOBS + Sub or SETIMES.HR
layer in the gold standard annotation.
27
Figure 1: Labeled attachment learning curves for the three treebanks using gold standard and automatic lemmatization
and morphosyntactic tagging
Marsi, 2006)10, we argue that HOBS requires thor-
ough further revision if it is to be the Croatian coun-
terpart of PDT in terms of expressivity and usability
in research and practical applications. This is further
supported by the data in Table 4, where the assign-
ment of specific syntactic tags is explored. However,
extrinsic evaluation would also be beneficial.
The differences in LAS and UAS scores between
the two languages are virtually non-existent across
formalisms and domains. The parsing models fa-
vor Croatian newspaper text by less than 2 LAS
points for all three formalisms, while UAS is ap-
proximately 1 UAS point higher in Serbian newspa-
per text for HOBS and HOBS + Sub, in contrast with
SETIMES.HR, which scores 1 UAS point higher for
the Croatian sample. In the Wikipedia samples, LAS
and UAS may be approximated as identical. In total,
as a top-performer, the SETIMES.HR model scored
74.5 LAS and 80.9 UAS on Croatian samples and
74.1 LAS and 80.6 UAS on Serbian samples. We be-
lieve this indicates that the parsing models trained on
Croatian treebank data can be used reliably for both
Croatian and Serbian text. We also use these figures
to imply no need for syntactic annotation projection
between Croatian and Serbian in this test scenario.
The cross-domain differences in LAS and UAS
are, in contrast with the cross-language differences,
10This holds even with the Slovene treebank of the CoNLL
2006 shared task having more than 2 000 sentences less than
HOBS, with both using the PDT formalism
much more substantial. As all treebanks were built
on top of Croatian newspaper text, scores are ex-
pectedly higher for these test samples in comparison
with the Wikipedia samples? scores. This difference
amounts to approximately 5 LAS points and 2 UAS
points in favor of the newspaper text samples across
the two languages and three formalisms.
We plotted the LAS learning curves by merging
the test samples into a single mixed-language test
set, incrementally creating 8 parsing models per for-
malism (12.5% to 100% of full size) and testing
them on this merged test set. The left plot of Figure 1
represents the learning curves for the three treebanks
peaking at previously discussed scores from Table 3.
The curves clearly reflect the overall differences in
scores. Their rate of increase is consistently com-
parable, with the overall difference in favor of SE-
TIMES.HR due to its smaller yet still informative
syntactic tagset and its formalism better suited for
Croatian syntax. With this fact now once again
empirically supported, we select the top-performing
SETIMES.HR parsing model for further inspection.
Thus, our further discussion deals exclusively with
parsing using SETIMES.HR.
First we observe parsing accuracy regarding syn-
tactic categories, where we still do compare SE-
TIMES.HR with HOBS + Sub as a final reference
point. We merged our test sets by language to pro-
vide Croatian and Serbian cross-domain test sam-
ples and calculate the LAS per syntactic category for
28
HOBS + Sub SETIMES.HR
Syntactic tag hr sr hr sr
Adverb 50.4 46.6 50.4 47.2
Attribute 81.4 82.3 87.9 88.4
Object 56.4 51.3 68.9 70.2
Predicate 75.1 71.9 80.7 81.2
Preposition 65.5 66.4 66.4 64.0
Subject 70.3 71.3 74.8 77.6
Table 4: LAS for main syntactic tags separated for Croat-
ian and Serbian test set. Manual preprocessing was used.
Best scores are boldfaced and split by language.
set.test wiki.test
MTE v4 hr sr hr sr overall
Lemma 96.1 94.6 93.9 95.8 95.1
POS 95.2 92.3 91.5 90.8 92.5
MSD 86.2 83.4 80.2 81.8 83.1
MTE v5
Lemma 95.6 94.2 94.3 96.1 95.1
POS 96.4 93.0 92.2 91.8 93.5
MSD 86.7 84.4 80.5 82.4 83.7
Table 5: Lemmatization, POS and MSD tagging accuracy
on the test sets and overall. Scores are given separately
for the two morphosyntactic tagsets used.
the two languages. This data is presented in Table 4.
Once again, the language variety is seen to be of no
significance to the parsing models. The scores ac-
tually alternate in favoring the two languages. SE-
TIMES.HR substantially outperforms HOBS + Sub
on the most frequent and arguably the most infor-
mative categories, such as predicate and subject (at
least 5 LAS points), object (almost 20 LAS points)
and attribute (6 points LAS).
4.2 Preprocessing and features
Here we discuss the impact of automatic preprocess-
ing, i.e., lemmatization and MSD tagging on depen-
dency parsing in our test framework. As announced,
this discussion deals exclusively with SETIMES.HR.
We lemmatize and tag the test samples by using
freely available state-of-the-art models for Croatian
and Serbian (Agic? et al, 2013), parse them using
our best SETIMES.HR model and observe LAS and
UAS. Preprocessing performance is given in Table 5
set.test wiki.test
LAS hr sr hr sr overall
HOBS 57.2 55.9 49.9 51.0 53.8
HOBS + Sub 65.2 62.5 56.7 58.0 60.9
SETIMES.HR 73.4 70.4 65.3 67.4 69.4
UAS
HOBS 71.6 71.8 67.4 69.0 70.1
HOBS + Sub 76.2 74.4 71.8 72.5 73.9
SETIMES.HR 79.4 76.9 75.2 77.8 77.4
Table 6: Parsing accuracy (LAS, UAS) with automatic
preprocessing
as a reference point while, more importantly, the de-
pendency parsing scores are given in Table 6. The
second plot of Figure 1 provides the learning curves
for the automatically preprocessed test sets.
Table 6 scores are easily elaborated using the pre-
viously discussed scores with manual, i.e., gold or
perfect preprocessing. Namely, the impact of differ-
ences between manual and automatic preprocessing
on parsing quality basically amounts to a very sim-
ple formula: LAS is reduced by 3-4 points and UAS
by 2 points when introducing preprocessing noise
by automatic lemmatization and tagging. This ob-
servation is valid across the languages and domains
of our test set and thus applies generally. Keeping
in mind the more complex prospective NLP systems
for Croatian and Serbian, we consider this fact to
be very favorable as the observed 16% error rate in
full MSD tagging, 5-6% for POS and lemmatization,
amounts for a significantly smaller decrease in pars-
ing quality as quantified by LAS and UAS.
To further support this observation, we conducted
an experiment with purposely corrupting lemmatiza-
tion and tagging. In this, as previously for learning
curves, we use the single merged test sample. For
lemmatization, we randomly drop lemmas from the
manually annotated test sample, replacing them with
empty features.11 For MSD tagging, we implement
two procedures. The first is identical with the one for
lemmatization, while in the second we replace the
valid tag with a randomly selected Croatian tag from
the full MTE v5 morphosyntactic tagset. For each
11In terms of the CoNLL-X format, we simply replace the
valid entry from the LEMMA field by an underscore.
29
Croatian Serbian
Features LAS UAS LAS UAS
Adjective
Type 74.3 80.7 74.6 81.2
Degree 74.3 80.7 73.7 80.2
Gender 74.1 80.7 74.5 81.0
Number 74.5 81.0 74.3 80.8
Case 75.0 81.5 74.4 81.1
Noun
Type 74.3 80.8 72.9 80.0
Gender 74.4 80.8 74.1 80.7
Number 74.1 80.7 74.0 80.7
Case 73.3 81.0 72.3 80.0
Verb
Type 74.6 81.3 74.3 80.8
Form 74.3 80.9 74.3 81.0
Person 74.3 81.0 73.5 80.0
Number 74.4 80.8 74.1 80.6
Gender 74.4 80.8 74.4 81.0
Full feature set 74.5 80.9 74.1 80.6
Table 7: Impact of morphosyntactic feature exclusion on
parsing. Improvements boldfaced and split by language.
of these scenarios, we provide 11 test sets: step-
ping by 10% of removals or random insertions, from
0% to 100% preprocessing accuracy. The results
are plotted in Figure 2. Evidently, lemmatization
is of no influence to dependency parsing using our
model. This is an important observation to consider
in, e.g., the future tasks of parsing large web cor-
pora of Croatian and Serbian. The large impact of
morphosyntactic tagging, i.e., morphosyntactic fea-
tures on parsing is also evident from the figure. It
is also supported by previous research in parsing us-
ing SETIMES.HR (Agic? and Merkler, 2013), where
a significant bias towards MSD-based parsing mod-
els was found over the POS-only-based models. Tag
removal and tag randomization appear to induce a
very similar effect of near-linear functional depen-
dency between tagging and parsing. We note that
this is not entirely supported by our realistic prepro-
cessing test scenario. It is purely due to the fact that
our noise introduction procedure does not relate to
the modus in which the stochastic tagger errs in pro-
cessing unseen text. Namely, MSD tagging errors
Figure 2: Overall SETIMES.HR parsing accuracy in rela-
tion with lemmatization and morphosyntactic tagging
tend to occur on certain morphosyntactic features,
corrupting these much more often than entire tags.
Thus, even when it yields a feature error, the tagger
still provides the parser with other valid features to
work with. This consideration of MSD features, in
pair with the following set of results, sketches our
plans for further research.
Following the previous note on MSD tagset and
features, we also implemented a simple experiment
in feature weight assessment. In it, we used the SE-
TIMES.HR treebank with full MTE v5 tagset and
created from it several instances, each with its own
reduced MTE v5 tagset. Each reduction was de-
fined by dropping one MSD feature from one part
of speech. More precisely, we dropped all MSD
features of adjectives (5 features), nouns (4) and
verbs (5). This amounted at 14 different MTE v5
reductions. We trained 14 parsing models using SE-
TIMES.HR with the reduced tagsets and tested them
on the test samples merged by language and imple-
menting the respective tagset reductions.
The results are given in Table 7. Most notably,
we observed an increase in parsing accuracy when
dropping adjective case and verb type. The most
substantial decrease occurred with the removal of
noun case, indicating the importance of this feature
in parsing the two languages. We consider the ad-
jective case removal gain an important observation
for future work, as adjectives are the most difficultly
30
Adv Ap Atr Atv Aux Co Elp Obj Oth Pnom Pred Prep Punc Sb Sub
Adv 0 15 1 0 2 2 5 13 2 1 3 0 2 2
Ap 1 10 0 0 0 2 3 0 1 0 0 0 5 0
Atr 23 9 6 1 0 14 23 3 3 3 0 0 25 2
Atv 0 1 6 0 0 0 0 0 1 26 0 0 1 0
Aux 0 0 0 0 1 0 0 0 0 28 0 0 0 1
Co 0 0 1 0 0 0 0 5 0 0 2 11 0 0
Elp 1 2 12 0 0 0 0 4 3 2 0 0 4 0
Obj 6 3 16 3 0 0 1 0 1 1 0 0 2 0
Oth 14 4 3 0 0 12 1 1 0 0 1 0 1 24
Pnom 3 0 8 0 0 0 3 0 0 24 1 0 3 0
Pred 1 0 2 5 26 0 0 1 1 23 0 0 0 0
Prep 1 0 0 0 1 1 0 0 2 0 0 0 0 0
Punc 0 0 0 0 0 17 0 0 0 0 0 0 1 1
Sb 2 11 26 1 0 0 5 1 4 4 1 0 0 1
Sub 1 0 0 0 0 0 0 0 2 0 0 0 0 0
Table 8: Confusion matrices for LAS (Croatian: bottom left, Serbian: top right)
tagged category for Croatian and Serbian.
4.3 Error analysis
Here we provide a brief insight to the error instances.
We discuss LAS errors for both languages, i.e., in-
stances of invalid head attachments paired with tag
misassignments. These are given in Table 8 in the
form of two confusion matrices for LAS.
We isolate several clusters of errors with shared
linguistic properties. Firstly, the subject-attribute-
apposition group (Sb-Atr-Ap), in which we find
the error instances to be closely related to the or-
der of attachment and assignment in multi-word
units representing foreign personal names, titles or
functions and occupations of persons. Next, the
attribute-adverb-object group (Atr-Adv-Obj) expect-
edly appears as these are inherently ambiguous cat-
egories.12 The predicate-nominal-auxiliary group of
errors (Pred-Pnom-Aux) reflects the interaction of
MSD annotation choices and syntactic annotation
principles, as participes are MSD-tagged as adjec-
tives, thus confusing the parser in predicate annota-
tion. Moreover, SETIMES.HR has documented is-
sues with consistency in complex predicate anno-
tation that seek resolution and negatively influence
the parsing scores. Lastly, the only error group sub-
stantially reflecting the language difference is the
one involving predicates and predicate complements
(Pred-Atv), as it appears only in the Serbian confu-
sions. Namely, the infinitive predicate complement
is frequent in Croatian and non-existent in Serbian.
Infinitives in Serbian only appear for the future tense
paired with auxiliary verbs, confusing the parser to
12PDT, e.g., has an AtrAdv, AdvAtr, AtrObj and ObjAtr am-
biguity classes to address this. However, the sum of their fre-
quencies in HOBS is negligibly small (< 0.03%).
annotate these infinitives as predicate complements
as observed in the Croatian training data.
5 Conclusions and future work
We have described an experiment with dependency
parsing of two closely related and under-resourced
languages, Croatian and Serbian, by using parsing
models trained on Croatian treebanks. We investi-
gated three different parsing formalisms, the effects
of lemmatization, morphosyntactic tagging and fea-
ture selection on parsing quality for both languages.
We observed state-of-the-art parsing scores. All re-
sources used in the experiment are made publicly
available under a permissive license.13
The results of this experiment sketch the path for
our future research. Experiments with syntactic pro-
jection between Croatian and Serbian are not feasi-
ble given the negligible differences in the observed
scores. In contrast, domain adaptation for pars-
ing the two languages should be investigated given
the observed accuracy decrease when moving from
newspaper text to Wikipedia. We have already initi-
ated further enlargements of the SETIMES.HR tree-
bank and the test sets with Croatian data from other
domains. Experiments with newer and more ad-
vanced dependency parsers (Koo and Collins, 2010;
Bohnet and Nivre, 2012; Zhang and McDonald,
2012; Martins et al, 2013) should be conducted to
provide up-to-date scores.
We are currently experimenting with morphosyn-
tactic tagset design for improved dependency pars-
ing of Croatian and Serbian. We aim at finding the
optimal tagset by closely investigating morphosyn-
tactic feature influences and dependencies.
13http://nlp.ffzg.hr/
31
References
Z?. Agic?. 2012. K-Best Spanning Tree Dependency Pars-
ing With Verb Valency Lexicon Reranking. In: Pro-
ceedings of COLING 2012: Posters, pp. 1?12. COL-
ING 2012 Organizing Committee.
Z?. Agic?, D. Merkler. 2013. Three Syntactic Formalisms
for Data-Driven Dependency Parsing of Croatian. In:
Text, Speech and Dialogue. Lecture Notes in Computer
Science, 8082:560?567. Springer.
Z?. Agic?, N. Ljubes?ic?, D. Merkler. 2013. Lemmatization
and Morphosyntactic Tagging of Croatian and Serbian.
In: Proceedings of BSNLP 2013. ACL.
B. Bekavac, S. Seljan, I. Simeon. 2008. Corpus-Based
Comparison of Contemporary Croatian, Serbian and
Bosnian. In: Proceedings of FASSBL 2008, pp. 33?39.
Croatian Language Technologies Society.
D. Berovic?, Z?. Agic?, M. Tadic?. 2012. Croatian Depen-
dency Treebank: Recent Development and Initial Ex-
periments. In: Proceedings of LREC 2012, pp. 1902?
1906. ELRA.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova, B. Hladka?. 2003.
The Prague Dependency Treebank: A Three-Level
Annotation Scenario. In: Treebanks: Building and Us-
ing Parsed Corpora. Springer.
B. Bohnet, J. Nivre. 2012. A Transition-Based System
for Joint Part-of-Speech Tagging and Labeled Non-
Projective Dependency Parsing. In: Proceedings of
EMNLP-CoNLL 2012, pp. 1455?1465. ACL.
S. Buchholz, E. Marsi. 2006. CoNLL-X Shared Task on
Multilingual Dependency Parsing. In: Proceedings of
CoNLL-X, pp. 149?164. ACL.
T. Erjavec. 2012. MULTEXT-East: Morphosyntac-
tic Resources for Central and Eastern European Lan-
guages. Language Resources and Evaluation, 46 (1),
131?142. Springer.
Y. Goldberg, M. Elhadad. 2009. Hebrew Dependency
Parsing: Initial Results. In: Proceedings of IWPT
2009, pp. 129?133. ACL.
K. Hall. 2007. K-Best Spanning Tree Parsing. In: Pro-
ceedings of ACL 2007, pp. 392?399. ACL.
T. Koo, M. Collins. 2010. Efficient Third-Order De-
pendency Parsers. In: Proceedings of ACL 2010, pp.
1?11. ACL.
M. Martinovic?. 2008. Transfer Of Natural Lan-
guage Processing Technology: Experiments, Possibil-
ities and Limitations ? Case Study: English to Serbian.
Infotheca ? Journal of Informatics and Librarianship,
9 (1-2):11?20.
A. Martins, M. Almeida, N. Smith. 2013. Turning
on the Turbo: Fast Third-Order Non-Projective Turbo
Parsers. In: Proceedings of ACL 2013. ACL.
R. McDonald, K. Lerman, F. Pereira. 2006. Multilingual
Dependency Parsing With a Two-Stage Discriminative
Parser. In: Proceedings of CoNLL-X, pp. 216?220.
ACL.
D. Merkler, Z?. Agic?, A. Agic?. 2013. Babel Treebank of
Public Messages in Croatian. In: Proceedings of CILC
2013. Proceedia ? Social and Behavioral Sciences, in
press. Elsevier.
E. Mohamed. 2011. The Effect of Automatic Tok-
enization, Vocalization, Stemming, and POS Tagging
on Arabic Dependency Parsing. In: Proceedings of
CoNLL 2011, pp. 10?18. ACL.
G. Nenadic?. 2000. Local Grammars and Parsing Coor-
dination of Nouns in Serbo-Croatian. In: Text, Speech
and Dialogue. Lecture Notes in Computer Science,
1902:57?62. Springer.
G. Nenadic?, I. Spasic?, S. Ananiadou. 2003. Morphosyn-
tactic Clues for Terminological Processing in Serbian.
In: Proceedings of the EACL Workshop on Morpho-
logical Processing of Slavic Languages, pp. 79?86.
ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, D. Yuret. The CoNLL 2007 Shared
Task on Dependency Parsing. In: Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pp. 915?932. ACL.
M. Silberztein. 2004. NooJ : An Object-Oriented Ap-
proach. In: INTEX pour la Linguistique et le Traite-
ment Automatique des Langue, pp. 359?369. Presses
Universitaires de Franche-Comte?.
J. Silic?, I. Pranjkovic?. 2005. Gramatika hrvatskoga
jezika za gimnazije i visoka uc?ilis?ta. S?kolska knjiga,
Zagreb.
A. S?gaard. 2013. Semi-Supervised Learning and Do-
main Adaptation for NLP. Morgan & Claypool Pub-
lishers.
Z?. Stanojc?ic?, Lj. Popovic?. 2008. Gramatika srp-
skog jezika: za gimnazije i srednje s?kole. Zavod za
udz?benike i nastavna sredstva, Beograd.
M. Tadic?. 2007. Building the Croatian Dependency
Treebank: The Initial Stages. Suvremena lingvistika,
63 (1), 85?92. Hrvatsko filolos?ko drus?tvo.
M. Tadic?, D. Brozovic?-Ronc?evic?, A. Kapetanovic?. 2012.
The Croatian Language in the Digital Age. META-
NET White Paper Series. Springer.
M. Tadic?, T. Va?radi. 2012. Central and South-East Euro-
pean Resources in META-SHARE. In: Proceedings of
COLING 2012: Demonstration Papers, pp. 431?438.
COLING 2012 Organizing Committee.
D. Vitas, C. Krstev, I. Obradovic?, Lj. Popovic?, G.
Pavlovic?-Laz?etic?. 2003. An Overview of Resources
and Basic Tools for Processing of Serbian Written
Texts. In: Proceedings of the Workshop on Balkan
Language Resources, First Balkan Conference in In-
formatics.
32
D. Vitas, Lj. Popovic?, C. Krstev, I. Obradovic?, G.
Pavlovic?-Laz?etic?, M. Stanojevic?. 2012. The Serbian
Language in the Digital Age. META-NET White Pa-
per Series. Springer.
K. Vuc?kovic?, M. Tadic?, Z. Dovedan. 2008. Rule-Based
Chunker for Croatian. In: Proceedings of LREC 2008,
pp. 2544?2549. ELRA.
D. Yarowsky, G. Ngai, Richard Wicentowski. 2001. In-
ducing Multilingual Text Analysis Tools via Robust
Projection Across Aligned Corpora. In: Proceedings
of HLT 2001, pp. 1?8. ACL.
H. Zhang, R. McDonald. 2012. Generalized Higher-
Order Dependency Parsing With Cube Pruning. In:
Proceedings of EMNLP 2012. ACL.
33
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 13?24,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Cross-lingual Dependency Parsing of Related Languages with Rich
Morphosyntactic Tagsets
?
Zeljko Agi
?
c J
?
org Tiedemann Kaja Dobrovoljc
zagic@uni-potsdam.de jorg.tiedemann@lingfil.uu.se kaja.dobrovoljc@trojina.si
Simon Krek Danijela Merkler Sara Mo?ze
simon.krek@ijs.si dmerkler@ffzg.hr s.moze@wlv.ac.uk
Abstract
This paper addresses cross-lingual depen-
dency parsing using rich morphosyntac-
tic tagsets. In our case study, we experi-
ment with three related Slavic languages:
Croatian, Serbian and Slovene. Four dif-
ferent dependency treebanks are used for
monolingual parsing, direct cross-lingual
parsing, and a recently introduced cross-
lingual parsing approach that utilizes sta-
tistical machine translation and annota-
tion projection. We argue for the benefits
of using rich morphosyntactic tagsets in
cross-lingual parsing and empirically sup-
port the claim by showing large improve-
ments over an impoverished common fea-
ture representation in form of a reduced
part-of-speech tagset. In the process, we
improve over the previous state-of-the-art
scores in dependency parsing for all three
languages.
1 Introduction
A large majority of human languages are under-
resourced in terms of text corpora and tools avail-
able for applications in natural language process-
ing (NLP). According to recent surveys (Bender,
2011; Uszkoreit and Rehm, 2012; Bender, 2013),
this is especially apparent with syntactically anno-
tated corpora, i.e., treebanks ? both dependency-
based ones and others. In this paper, we fo-
cus on dependency parsing (K?ubler et al., 2009),
but the claims should hold in general. The lack
of dependency treebanks is due to the fact that
they are expensive and time-consuming to con-
struct (Abeill?e, 2003). Since dependency parsing
of under-resourced languages nonetheless draws
substantial interest in the NLP research commu-
nity, over time, we have seen a number of research
efforts directed towards their processing despite
the absence of training data for supervised learn-
ing of parsing models. We give a brief overview of
the major research directions in the following sub-
section. Here, we focus on supervised learning of
dependency parsers, as the performance of unsu-
pervised approaches still falls far behind the state
of the art in supervised parser induction.
1.1 Related Work
There are two basic strategies for data-driven pars-
ing of languages with no dependency treebanks:
annotation projection and model transfer. Both
fall into the general category of cross-lingual de-
pendency parsing as they attempt to utilize ex-
isting dependency treebanks or parsers from a
resource-rich language (source) for parsing the
under-resourced (target) language.
Annotation projection: In this approach, de-
pendency trees are projected from a source lan-
guage to a target language using word alignments
in parallel corpora. It is based on a presumption
that source-target parallel corpora are more read-
ily available than dependency treebanks. The ap-
proach comes in two varieties. In the first one, par-
allel corpora are exploited by applying the avail-
able state-of-the-art parsers on the source side
and subsequent projection to the target side us-
ing word alignments and heuristics for resolving
possible link ambiguities (Yarowsky et al., 2001;
Hwa et al., 2005). Since dependency parsers typ-
ically make heavy use of various morphological
and other features, the apparent benefit of this ap-
proach is the possibility of straightforward pro-
jection of these features, resulting in a feature-
rich representation for the target language. On the
downside, the annotation projection noise adds up
to dependency parsing noise and errors in word
alignment, influencing the quality of the resulting
target language parser.
The other variety is rare, since it relies on paral-
lel corpora in which the source side is a depen-
13
dency treebank, i.e., it is already manually an-
notated for syntactic dependencies (Agi?c et al.,
2012). This removes the automatic parsing noise,
while the issues with word alignment and annota-
tion heuristics still remain.
Model transfer: In its simplest form, transfer-
ring a model amounts to training a source lan-
guage parser and running it directly on the target
language. It is usually coupled with delexicaliza-
tion, i.e., removing all lexical features from the
source treebank for training the parser (Zeman and
Resnik, 2008; McDonald et al., 2013). This in turn
relies on the same underlying feature model, typi-
cally drawing from a shared part-of-speech (POS)
representation such as the Universal POS Tagset of
Petrov et al. (2012). Negative effects of using such
an impoverished shared representation are typi-
cally addressed by adapting the model to better fit
the target language. This includes selecting source
language data points appropriate for the target lan-
guage (S?gaard, 2011; T?ackstr?om et al., 2013),
transferring from multiple sources (McDonald et
al., 2011) and using cross-lingual word clusters
(T?ackstr?om et al., 2012). These approaches need
no projection and enable the usage of source-side
gold standard annotations, but they all rely on
a shared feature representation across languages,
which can be seen as a strong bottleneck. Also,
while most of the earlier research made use of
heterogenous treebanks and thus yielded linguisti-
cally implausible observations, research stemming
from an uniform dependency scheme across lan-
guages (De Marneffe and Manning, 2008; Mc-
Donald et al., 2013) made it possible to perform
more consistent experiments and to assess the ac-
curacy of dependency labels.
Other approaches: More recently, Durrett et
al. (2012) suggested a hybrid approach that in-
volves bilingual lexica in cross-lingual phrase-
based parsing. In their approach, a source-side
treebank is adapted to a target language by ?trans-
lating? the source words to target words through
a bilingual lexicon. This approach is advanced
by Tiedemann et al. (2014), who utilize full-
scale statistical machine translation (SMT) sys-
tems for generating synthetic target language tree-
banks. This approach relates to annotation pro-
jection, while bypassing the issue of dependency
parsing noise as gold standard annotations are pro-
jected. The SMT noise is in turn mitigated by
better word alignment quality for synthetic data.
The influence of various projection algorithms in
this approach is further investigated by Tiedemann
(2014). This line of cross-lingual parsing research
substantially improves over previous work.
1.2 Paper Overview
All lines of previous cross-lingual parsing research
left the topics of related languages and shared rich
feature representations largely unaddressed, with
the exception of Zeman and Resnik (2008), who
deal with phrase-based parsing test-cased on Dan-
ish and Swedish treebanks, utilizing a mapping
over relatively small POS tagsets.
In our contribution, the goal is to observe the
properties of cross-lingual parsing in an envi-
ronment of relatively free-word-order languages,
which are related and characterized by rich mor-
phology and very large morphosyntactic tagsets.
We experiment with four different small- and
medium-size dependency treebanks of Croatian
and Slovene, and cross-lingually parse into Croa-
tian, Serbian and Slovene. Along with monolin-
gual and direct transfer parsing, we make use of
the SMT framework of Tiedemann et al. (2014).
We are motivated by:
? observing the performance of various ap-
proaches to cross-lingual dependency parsing
for closely related languages, including the very
recent treebank translation approach by Tiede-
mann et al. (2014);
? doing so by using rich morphosyntactic tagsets,
in contrast to virtually all other recent cross-
lingual dependency parsing experiments, which
mainly utilize the Universal POS tagset of
Petrov et al. (2012);
? reliably testing for labeled parsing accuracy in
an environment with heterogenous dependency
annotation schemes; and
? improving the state of the art for Croatian,
Slovene and Serbian dependency parsing across
these heterogenous schemes.
In Section 2, we describe the language resources
used: treebanks, tagsets and test sets. Section 3
describes the experimental setup, which includes
a description of parsing, machine translation and
annotation projection. In Section 4, we discuss the
results of the experiments, and we conclude the
discussion by sketching the possible directions for
future research in Section 5.
14
Figure 1: Histogram of edge distances in the tree-
banks. Edge distance is measured in tokens be-
tween heads and dependents. Distance of 1 de-
notes adjacent tokens.
Figure 2: Histogram of average tree depths.
2 Resources
We make use of the publicly available language re-
sources for Croatian, Serbian and Slovene. These
include dependency treebanks, test sets annotated
for morphology and dependency syntax, and a
morphosyntactic feature representation drawing
from the Multext East project (Erjavec, 2012).
A detailed assessment of the current state of de-
velopment for morphosyntactic and syntactic pro-
cessing of these languages is given by Agi?c et al.
(2013) and Uszkoreit and Rehm (2012). Here, we
provide only a short description.
2.1 Treebanks
We use two Croatian and two Slovene dependency
treebanks.
1
One for each language is based on the
Prague Dependency Treebank (PDT) (B?ohmov?a
et al., 2003) annotation scheme, while the other
two introduced novel and more simplified syntac-
tic tagsets. All four treebanks use adaptations of
1
No treebanks of Serbian were publicly available at the
time of conducting this experiment.
Feature hr PDT hr SET sl PDT sl SSJ
Sentences 4,626 8,655 1,534 11,217
Tokens 117,369 192,924 28,750 232,241
Types 25,038 37,749 7,128 48,234
Parts of speech 13 13 12 13
MSDs 821 685 725 1,142
Syntactic tags 26 15 26 10
Table 1: Basic treebank statistics.
the Multext East version 4 tagset (Erjavec, 2012)
for the underlying morphological annotation layer,
which we shortly describe further down. Basic
statistics for the treebanks are given in Table 1.
hr PDT: This treebank is natively referred to
as the Croatian Dependency Treebank (HOBS)
(Tadi?c, 2007; Berovi?c et al., 2012). Its most recent
instance, HOBS 2.0 (Agi?c et al., 2014) slightly de-
parts from the PDT scheme. Thus, in this exper-
iment, we use the older version, HOBS 1.0, and
henceforth refer to it as hr PDT for consistency and
more clear reference to its annotation.
2
hr SET: The SETIMES.HR dependency treebank
of Croatian has a 15-tag scheme. It is targeted
towards high parsing accuracy, while maintaining
a clear distinction between all basic grammatical
categories of Croatian. Its publicly available 1.0
release consists of approximately 2,500 sentences
(Agi?c and Merkler, 2013), while release 2.0 has
just under 4,000 sentences (Agi?c and Ljube?si?c,
2014) of newspaper text. Here, we use an even
newer, recently developed version with more than
8,500 sentences from multiple domains.
3
sl PDT: The PDT-based Slovene Dependency
Treebank (D?zeroski et al., 2006) is built on top of
a rather small portion of Orwell?s novel 1984 from
the Multext East project (Erjavec, 2012). Even if
the project was discontinued, it is still heavily used
as part of the venerable CoNLL 2006 and 2007
shared task datasets (Buchholz and Marsi, 2006;
Nivre et al., 2007).
4
sl SSJ: The Slovene take on simplifying syntac-
tic annotations resulted in the 10-tag strong JOS
Corpus of Slovene (Erjavec et al., 2010). Similar
to hr SET, this new annotation scheme is loosely
2
HOBS is available through META-SHARE (Tadi?c and
V?aradi, 2012).
3
http://nlp.ffzg.hr/resources/corpora/
setimes-hr/
4
http://nl.ijs.si/sdt/
15
PDT-based, but considerably reduced to facilitate
manual annotation. The initial 100,000 token cor-
pus has recently doubled in size, as described by
Dobrovoljc et al. (2012). We use the latter version
in our experiment.
5
The statistics in Table 1 show a variety of tree-
bank sizes and annotations. Figure 1 illustrates the
structural complexity of the treebanks by provid-
ing a histogram of egdes by token distance. While
adjacent edges expectedly dominate the distribu-
tions, it is interesting to see that almost 30% of
all edges in sl SSJ attach to root, resulting in an
easily parsable flattened tree structure. Knowing
that relations denoting attributes account for more
than one third of all non-root dependents in the re-
mainder, one can expect dependency parsing per-
formance comparable to CoNLL-style chunking
(Tjong Kim Sang and Buchholz, 2000). This is
further supported by the distributions of sentences
in the four treebanks by average tree depth in Fig-
ure 2. We can see that virtually all sl SSJ trees have
average depths of 1 to 3, while the other treebanks
exhibit the more common structural properties of
dependency trees.
In these terms of complexity, the Croatian tree-
banks are richer than their Slovene counterparts.
In sl SSJ, attributes and edges to root account for
more than 60% of all dependencies. Even in the
other three treebanks, 20-30% of the edges are la-
beled as attributes, while the rest is spread more
evenly between the basic syntactic categories such
as predicates, subject and objects. More detailed
and more linguistically motivated comparisons of
the three annotation guidelines fall outside the
scope of our paper. Instead, we refer to the pre-
viously noted publications on the respective tree-
banks, and to (Agi?c and Merkler, 2013; Agi?c et
al., 2013) for comparisons between PDT and SET
in parsing Croatian and Serbian.
2.2 Morphosyntactic Tagset
All four treebanks were manually created: they
are sentence- and token-split, lemmatized, mor-
phosyntactically tagged and syntactically anno-
tated. In morphosyntactic annotation, they all
make use of the Multext East version 4 (MTE
4) guidelines (Erjavec, 2012).
6
MTE 4 is a po-
sitional tagset in which morphosyntactic descrip-
tors of word forms are captured by a morphosyn-
5
http://eng.slovenscina.eu/
tehnologije/ucni-korpus
6
http://nl.ijs.si/ME/V4/
tactic tag (MSD) created by merging atomic at-
tributes in the predefined positions. This is illus-
trated in Table 2 through an example verb tag. The
first character of the tag denotes the part of speech
(POS), while each of the following characters en-
codes a specific attribute in a specific position.
Both the positions and the attributes are language-
dependent in MTE 4, but the attributes are still
largely shared between these three languages due
to their relatedness.
The Slovene treebanks closely adhere to the
specification, while each of the Croatian treebanks
implements slight adaptations of the tagset to-
wards Croatian specifics. In hr PDT, the adaptation
is governed by and documented in the Croatian
Morphological Lexicon (Tadi?c and Fulgosi, 2003),
and the modifications in hr SET were targeted to
more closely match the ones for Slovene.
7
2.3 Test Sets
Recent research by McDonald et al. (2013) has
uncovered the downsides of experimenting with
parsing using heterogenous dependency annota-
tions, while at the same time providing possi-
bly the first reliable results in cross-lingual pars-
ing. They did so by creating the uniformly anno-
tated Universal Dependency Treebanks collection
based on Stanford Typed Dependencies (De Marn-
effe and Manning, 2008), which in turn also en-
abled measuring both labeled (LAS) and unla-
beled (UAS) parsing accuracy.
Having four treebanks with three different an-
notation schemes, we seek to enable reliable ex-
perimentation through our test sets. Along with
Croatian and Slovene, which are represented in the
training sets, we introduce Serbian as a target-only
language in the test data. Following the CoNLL
shared tasks setup (Buchholz and Marsi, 2006;
Nivre et al., 2007), our test sets have 200 sentences
(approx. 5,000 tokens) per language, split 50:50
between newswire and Wikipedia text. Each test
set is manually annotated for morphosyntax, fol-
lowing the MTE 4 guidelines for the respective
languages, and checked by native speakers for va-
lidity. On top of that, all test sets are annotated
with all three dependency schemes: PDT, SET and
SSJ. This enables observing LAS in a heteroge-
nous experimental environment, as we test each
monolingual and cross-lingual parser on an anno-
7
http://nlp.ffzg.hr/data/tagging/
msd-hr.html
16
Language MSD tag Attribute-value pairs
hr Vmn Category = Verb, Type = main, Vform = infinitive
sl Vmen Category = Verb, Type = main, Aspect = perfective, VForm = infinitive
sr Vmn----an-n---e Category = Verb, Type = main, VForm = infinitive, Voice = active,
Negative = no, Clitic = no, Aspect = perfective
Table 2: Illustration of the Multext East version 4 tagset for Croatian, Serbian and Slovene. The attributes
are language-dependent, as well as their positions in the tag, which are also dependent on the part of
speech, denoted by position zero in the tag.
tation layer matching its training set. In contrast,
the MTE 4 tagsets are not adjusted, i.e., each test
set only has a single language-specific MTE 4 an-
notation. We rely on their underlying similarities
in feature representations to suffice for improved
cross-lingual parsing performance.
3 Experiment Setup
This section describes the experiment settings. We
list the general workflow of the experiment and
then provide the details on the parser setup and
the more advanced approaches used for target lan-
guage adaptation of the models.
3.1 Workflow
The experiment consists of three work packages:
(1) monolingual parsing, (2) direct cross-lingual
parsing, and (3) cross-lingual parsing using syn-
thetic training data from SMT. In the first one, we
train dependency parsers on the four treebanks and
test them on the corresponding languages, thus
assessing the monolingual parsing performance.
The second stage observes the effects of directly
applying the parsers from the first stage across the
languages. Finaly, in the third work package, we
use four different approaches to automatic transla-
tion to create synthetic training data. We translate
the Croatian treebanks to Slovene and vice versa,
project the annotations using two different projec-
tion algorithms, and train and apply the adapted
parsers across the languages. The details are in-
cluded in the two following subsections.
Two general remarks apply to our experiment.
First, we perform cross-lingual parsing, and not
cross-annotation-scheme parsing. Thus, we do not
compare the dependency parsing scores between
the annotation schemes, but rather just between
the in-scheme parsers. Second, we use Serbian as
a test-set-only language. As there are no treebanks
of Serbian, we cannot use it as a source language,
and we leave SMT and annotation projection into
Serbian for future work.
3.2 Dependency Parsing
In all experiments, we use the graph-based de-
pendency parser by Bohnet (2010) with default
settings. We base our parser choice on its state-
of-the-art performance across various morpholog-
ically rich languages in the SPMLR 2013 shared
task (Seddah et al., 2013). While newer contribu-
tions targeted at joint morphological and syntactic
analysis (Bohnet and Kuhn, 2012; Bohnet et al.,
2013) report slightly higher scores, we chose the
former one for speed and robustness, and because
we use gold standard POS/MSD annotations. The
choice of gold standard preprocessing is motivated
by previous research in parsing Croatian and Ser-
bian (Agi?c et al., 2013), and by insight of Sed-
dah et al. (2013), who report a predictable linear
decrease in accuracy for automatic preprocessing.
This decrease amounts to approximately 3 points
LAS for Croatian and Serbian across various test
cases in (Agi?c et al., 2013).
We observe effects of (de)lexicalization and of
using full MSD tagset as opposed to only POS tags
in all experiments. Namely, in all work packages,
we compare parsers trained with {lexicalized,
delexicalized} ? {MSD, POS} features. In lexi-
calized parsers, we use word forms and features,
while we exclude lemmas from all experiments ?
both previous research using MSTParser (McDon-
ald et al., 2005) and our own test runs show no
use for lemmas as features in dependency parsing.
Delexicalized parsers are stripped of all lexical
features, i.e., word forms are omitted from training
and testing data. Full MSD parsers use both the
POS information and the sub-POS features in the
form of atomic attribute-value pairs, while POS-
only parsers are stripped of the MSD features ?
they use just the POS information. The delexi-
calized POS scenario is thus very similar to the
17
direct transfer by McDonald et al. (2013), since
MTE 4 POS is virtually identical to Universal POS
(Petrov et al., 2012).
8
3.3 Treebank Translation and Annotation
Projection
For machine translation, we closely adhere to the
setup implemented by Tiedemann et al. (2014) in
their treebank translation experiments. Namely,
our translations are based on automatic word
alignment and subsequent extraction of translation
equivalents as common in phrase-based SMT. We
perform word alignment by using GIZA++ (Och
and Ney, 2003), while utilizing IBM model 4 for
creating the Viterbi word alignments for parallel
corpora. For the extraction of translation tables,
we use the de facto standard SMT toolbox Moses
(Koehn et al., 2007) with default settings. Phrase-
based SMT models are tuned using minimum er-
ror rate training (Och, 2003). Our monolingual
language modeling using KenLM tools
9
(Heafield,
2011) produces standard 5-gram language mod-
els using modified Kneser-Ney smoothing without
pruning.
For building the translation models, we use
the OpenSubtitles parallel resources from OPUS
10
(Tiedemann, 2009) for the Croatian-Slovene pair.
Even if we expect this to be a rather noisy paral-
lel resource, we justify the choice by (1) the fact
that no other parallel corpora
11
of Croatian and
Slovene exist, other than Orwell?s 1984 from the
Multext East project, which is too small for SMT
training and falls into a very narrow domain, and
(2) evidence from (Tiedemann et al., 2014) that the
SMT-supported cross-lingual parsing approach is
very robust to translation noise.
For translating Croatian treebanks into Slovene
and vice versa, we implement and test four dif-
ferent methods of translation. They are coupled
with approaches to annotation projection from the
source side gold dependency trees to the target
translations via the word alignment information
available from SMT.
8
A mapping from Slovene MTE 4 to Universal
POS is available at https://code.google.com/p/
universal-pos-tags/ as an example.
9
https://kheafield.com/code/kenlm/
10
http://opus.lingfil.uu.se/
11
We note the Croatian-Slovene parallel corpus project de-
scribed by Po?zgaj Had?zi and Tadi?c (2000), but it appears that
the project was not completed and the corpus itself is not pub-
licly available.
LOOKUP: The first approach to translation in
our experiment is the dictionary lookup approach.
We simply select the most reliable translations of
single words in the source language into the tar-
get language by looking up the phrase translation
tables extracted from the parallel corpus. This is
very similar to what Agi?c et al. (2012) did for the
Croatian-Slovene pair. However, their approach
involved both translating and testing on the same
small corpus (Orwell?s novel), while here we ex-
tract the translations from full-blown SMT phrase
tables on a much larger scale. The trees projec-
tion from source to target is trivial since the num-
ber and the ordering of words between them does
not change. Thus, the dependencies are simply
copied.
CHAR: By this acronym, we refer to an ap-
proach known as character-based statistical ma-
chine translation. It is shown to perform very
well for closely related languages (Vilar et al.,
2007; Tiedemann, 2012; Tiedemann and Nakov,
2013). The motivation for character-level transla-
tion is the ability of such models to better gener-
alize the mapping between similar languages es-
pecially in cases of rich productive morphology
and limited amounts of training data. With this,
character-level models largely reduce the num-
ber of out-of-vocabulary words. In a nutshell,
our character-based model performs word-to-word
translation using character-level modeling. Simi-
lar to LOOKUP, this is also a word-to-word trans-
lation model, which also requires no adaptation of
the source dependency trees ? they are once again
simply copied to target sentences.
WORD: Our third take on SMT is slightly more
elaborate but still restricts the translation model
to one-to-one word mappings. In particular, we
extract all single word translation pairs from the
phrase tables and apply the standard beam-search
decoder implemented in Moses to translate the
original treebanks to all target languages. Thus,
we allow word reordering and use a language
model while still keeping the projection of anno-
tated data as simple as possible. The language
model may influence not only the word order but
also the lexical choice as we now allow multiple
translation options in our phrase table. Also note
that this approach may introduce additional non-
projectivity in the projected trees. This system
is the overall top-performer in (Tiedemann et al.,
18
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
eo
dp
rtj
ei
nfo
rm
aci
jsk
ep
isa
rne
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Atv
Atr
Obj
Sb
Pre
d
Atv
Atr
Obj
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
eo
dp
rtj
e
pis
arn
e
inf
orm
ati
vn
e
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Atv
Atr
Obj
Sb
Pre
d
Atv
Atr
Obj
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
e
,
da
bo
od
prl
a
DU
MM
Y
inf
orm
aci
jsk
ep
isa
rne
Nc
fs
n
Vm
ip
3s
--
du
mm
y
du
mm
y
du
mm
y
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Pre
d
Atv
Obj
Sb
Pre
d
Atv
dum
mydu
mmyd
umm
y
Obj
Atr
Figure 3: An illustration of the projections. Left side = CHAR, middle = WORD, right side = PHRASE. As
illustrated, WORD might introduce reorderings, while PHRASE can enter dummy nodes and edges to the
dependency trees. The sentence: The government plans to open information offices. See (Tiedemann et
al., 2014; Tiedemann, 2014) for detailed insight into projection algorithms.
2014), where reordering played an important role
in adapting the models to the target languages. We
test whether it holds for related languages as well.
PHRASE: This model implements translation
based on the entire phrase table using the standard
approach to phrase-based SMT. We basically run
the Moses decoder with default settings and the
parameters and models trained on our parallel cor-
pus. Here, we can have many-to-many word align-
ments, which require a more elaborate approach to
the projection of the source side dependency an-
notatio s. It is important for the annotation trans-
fer to keep track of the alignment between phrases
and words of the input and output sentences. The
Moses decoder provides both, phrase seg enta-
tion and word alignment. We use the annotation
projection algorithm of Hwa et al. (2005). As
illustrated in Figure 3, it resolves many-to-many
alignments by introducing dummy nodes to the
dependency trees. We use the implementation by
Tiedemann (2014), which addresses certain issues
with algorithm choices for ambiguous alignments
which were left unaccounted for in the original
work. Since this paper does not focus on the intri-
cacies of annotation projection, but rather on ap-
plying it in an environment of related languages
and rich MSD tagsets, we refer the reader to re-
lated work regarding the details.
We translate from Croatian to Slovene and vice
versa using four different treebanks and these
four different methods of translation and annota-
tion projection. As we stated in the experiment
overview, for each of these, we also experiment
with (de)lexicalization and MSD vs. POS, and we
test on all three languages. The three experimental
batches ? monolingual, direct and SMT-supported
transfer ? produce a large number of observations,
all of which we assess in the following section.
4 Results and Discussion
We split our discussion of the parsing results into
the following three subsections. We first observe
the performance of monolingual parsers. Sec-
ondly, we measure the quality of these when ap-
plied directly on the other two languages. Finally,
we look into the accuracy of parsers trained on
SMT-generated artificial treebank data when ap-
plied across the test languages.
4.1 Monolingual Parsing
Accuracies of parsers trained and applied on train-
ing and testing data belonging to the same lan-
guage ? i.e., our monolingual parsers ? are pro-
vided in the g ayed out sections of Table 3.
Parsing Croatian using hr PDT yields a high
score of 69.45 LAS, better than the former state
of the art on this test set (Agi?c et al., 2013) simply
due to applying a newer generation parser. This
score is provided by a lexicalized model with the
full MSD feature set. Replacing MSD with POS or
delexicalizing this model results in a 3-point drop
in LAS, while applying both replacements sub-
stantially decreases the score ? by more than 11
points LAS. We observe virtually the same pattern
for the other Croatian treebank, hr SET, where this
latter drop is even more significant, at 14 points.
Incidentally, 76.36 points LAS is also the new
state of the art for hr SET parsing, owing to the
recent enlargement of the treebank.
The Slovene parsers exhibit effectively the same
behavior as the Croatian ones. The lexicalized
MSD models of sl PDT and sl SSJ both record new
state-of-the-art scores, although the latter one on a
different test set than in previous research (Dobro-
voljc et al., 2012). At over 92 points LAS, sl SSJ
19
lexicalized delexicalized
hr sl sr hr sl sr
MSD POS MSD POS MSD POS MSD POS MSD POS MSD POS
hr PDT 69.45 66.95 60.09 50.19 69.42 66.96 66.03 57.79 57.98 42.66 66.79 57.41
SET 76.36 73.02 68.65 59.52 76.08 73.37 72.52 62.31 68.16 55.17 72.71 62.04
sl PDT 51.19 47.99 76.46 73.33 52.46 49.64 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 78.50 74.18 92.38 88.93 78.94 75.96 75.23 66.23 87.19 77.92 75.25 67.47
Table 3: Monolingual and direct cross-lingual parsing accuracy, expressed by the labeled accuracy metric
(LAS). Scores are split for lexicalized and delexicalized, full MSD and POS only parsers. Monolingual
scores are in grey. Row indices represent source languages and treebanks.
expectedly shows to be the easiest to parse, most
likely due to the relatively flat tree structure and its
small label set.
We note the following general pattern of fea-
ture importance. Dropping MSD features seems
to carry the most weight in all models, followed
by lexicalization. Dropping MSD is compensated
in part by lexical features paired with POS, while
dropping both MSD and word forms severely de-
grades all models. At this point, it is very impor-
tant to note that at 60-70 points LAS, these de-
creased scores closely resemble those of McDon-
ald et al. (2013) for the six languages in the Uni-
versal Treebanks. This observation is taken further
in the next subsection.
4.2 Direct Cross-lingual Parsing
The models used for monolingual parsing are here
directly applied on all languages but the treebank
source language, thus constituting a direct cross-
lingual parsing scenario. Its scores are also given
in Table 3, but now in the non-grey parts.
Croatian models are applied to Slovene and Ser-
bian test sets. For hr PDT, the highest score is
60.09 LAS on Slovene and 69.42 LAS on Serbian,
the latter noted as the state of the art for Serbian
PDT parsing. Comparing the cross-lingual score to
monolingual Slovene, the difference is substantial
as expected and comparable to the drops observed
by McDonald et al. (2013) in their experiments.
Our ranking of feature significance established in
the monolingual experiments holds here as well,
or rather, the absolute differences are even more
pronounced. Most notably, the difference between
the lexicalized MSD model and the delexicalized
POS model is 17 points LAS in favor of the for-
mer one on Slovene. hr SET appears to be more
resilient to delexicalization and tagset reduction
when applied on Slovene and Serbian, most likely
due to the treebank?s size, well-balanced depen-
dency label set and closer conformance with the
official MTE 4 guidelines. That said, the feature
patterns still hold. Also, 76.08 LAS for Serbian is
the new state of the art for SET parsing.
Slovene PDT is an outlier due to its small size,
as its training set is just over 1,500 sentences. Still,
the scores maintain the level of those in related
research, and the feature rankings hold. Perfor-
mance of parsing Croatian and Serbian using sl
SSJ is high, arguably up to the level of usability
in down-stream applications. These are the first
recorded scores in parsing the two languages us-
ing SSJ, and they reach above 78 points LAS for
both. Even if the scores are not comparable across
the annotation schemes due to their differences, it
still holds that the SSJ scores are the highest ab-
solute parsing scores recorded in the experiment.
This might hold significance in applications that
require robust parsing for shallow syntax.
Generally, the best transfer scores are quite
high in comparison with those on Universal Tree-
banks (McDonald et al., 2013; Tiedemann et al.,
2014). This is surely due to the relatedness of
the three languages. However, even for these ar-
guably closely related languages, the performance
of delexicalized models that rely only on POS fea-
tures ? averaging at around 55 points LAS ? is vir-
tually identical to that on more distant languages
test-cased in related work. We see this as a very
strong indicator of fundamental limitations of us-
ing linguistically impoverished shared feature rep-
resentations in cross-lingual parsing.
4.3 Cross-lingual Parsing with Treebank
Translation
Finally, we discuss what happens to parsing per-
formance when we replace direct cross-lingual ap-
plication of parsers with training models on trans-
lated treebanks. We take a treebank, Croatian or
Slovene, and translate it into the other language.
20
Target Approach PDT SET SSJ
hr monolingual 69.45 76.36 ?
direct 51.19 ? 78.50
translated 67.55 ? 74.68 ? 79.51 ?
sl monolingual 76.46 ? 92.38
direct 60.09 68.65 ?
translated 72.35 ? 70.52 ? 88.71 ?
sr monolingual ? ? ?
direct 69.42 76.08 78.94
translated 68.11 ? 74.31 ? 79.81 ??
Legend: ? CHAR ? LOOKUP ? PHRASE ? WORD
Table 4: Parsing score (LAS) summary for the top-
performing systems with respect to language and
approach to parser induction. All models are MSD
+ lexicalized.
We then train a parser on the translation and ap-
ply it on all three target test sets. We do this for all
the treebanks, and in all variations regarding trans-
lation and projection methods, morphological fea-
tures and lexicalization.
All scores for this evaluation stage are given in
Table 5 for completeness. The table contains 192
different LAS scores, possibly constituting a te-
dious read. Thus, in Table 4 we provide a sum-
mary of information on the top-performing parsers
from all three experimental stages, which includes
treebank translation.
We can see that the best models based on
translating the treebanks predominantly stem from
word-to-word SMT, i.e., from WORD transla-
tion models that basically enrich the lexical fea-
ture space and perform word reordering, enabling
straightforward copying of syntactic structures
from translation sources to translation targets. Fol-
lowing them are the CHAR and LOOKUP models,
expectedly leaving ? although not too far behind
? PHRASE behind given the similarities of the lan-
guage pair. Since Croatian and Slovene are related
languages, the differences between the models are
not as substantial as in (Tiedemann et al., 2014),
but WORD models still turn out to be the most ro-
bust ones, even if word reordering might not be so
frequent in this language pair as in the data from
(McDonald et al., 2013). Further, when compar-
ing the best SMT-supported models to monolin-
gual parsers, we see that the models with trans-
lation come really close to monolingual perfor-
mance. In comparison with direct transfer, models
trained on translated treebanks manage to outper-
form them in most cases, especially for the more
distant language pairs. For example, the sl ? hr
SSJ WORD model is 1 point LAS better on Croat-
ian than the directly applied Slovene model, and
the same holds for testing on Serbian with the
same dataset. On the other side, directly applied
models from Croatian SET outperform the trans-
lated ones for Serbian. For PDT, the translated
models are substantially better between Croatian
and Slovene since sl PDT is an outlier in terms
of size and dataset selection, while direct trans-
fer from Croatian seems to work better for Serbian
than the translated models.
Reflecting on the summary in Table 4 more
generally, by and large, we see high parsing ac-
curacies. Averages across the formalisms reach
well beyond 70 points LAS. We attribute this to
the relatedness of the languages selected for this
case study, as well as to the quality of the un-
derlying language resources. From another view-
point, the table clearly shows the prominence of
lexical and especially rich morphosyntactic tagset
features throughout the experiment. Across our
monolingual, direct and SMT-supported parsing
experiments, these features are represented in the
best systems, and dropping them incurs significant
decreases in accuracy.
5 Conclusions and Future Work
In this contribution, we addressed the topic of
cross-lingual dependency parsing, i.e., applying
dependency parsers from typically resource-rich
source languages to under-resourced target lan-
guages. We used three Slavic languages ? Croat-
ian, Slovene and Serbian ? as a test case for related
languages in different stages of language resource
development. As these are relatively free-word-
order languages with rich morphology, we were
able to test the cross-lingual parsers for perfor-
mance when using training features drawing from
large morphosyntactic tagsets ? typically consist-
ing of over 1,000 different tags ? in contrast to
impoverished common part-of-speech representa-
tions. We tested monolingual parsing, direct cross-
lingual parsing and a very recent promising ap-
proach with artificial creation of training data via
machine translation. In the experiments, we ob-
served state-of-the-art results in dependency pars-
ing for all three languages. We strongly argued
and supported the case for using common rich rep-
resentations of morphology in dependency parsing
21
lexicalized delexicalized
hr sl sr hr sl sr
MSD POS MSD POS MSD POS MSD POS MSD POS MSD POS
CHAR hr ? sl PDT 66.92 60.25 61.49 55.57 67.83 62.04 66.56 57.63 58.34 43.04 66.89 57.65
SET 73.65 64.64 70.52 66.11 72.95 64.44 72.98 62.98 69.03 54.81 72.74 62.73
sl ? hr PDT 51.96 48.14 72.35 63.71 53.11 49.47 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 78.69 75.45 88.21 78.88 79.25 77.09 75.23 66.23 87.19 77.92 75.25 67.47
LOOKUP hr ? sl PDT 67.55 59.96 60.81 56.54 67.78 61.41 66.56 57.63 58.34 43.04 66.89 57.65
SET 73.58 64.98 69.93 68.09 73.70 64.25 72.52 62.72 68.47 55.27 72.71 62.73
sl ? hr PDT 51.74 49.15 72.02 63.08 53.49 51.33 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 79.25 77.06 88.10 78.53 79.81 77.23 75.23 66.23 87.19 77.92 75.25 67.47
WORD hr ? sl PDT 67.33 59.24 61.80 57.14 68.11 61.13 65.84 57.12 58.17 42.99 67.12 57.70
SET 73.26 65.87 69.98 68.98 73.63 65.85 72.71 62.29 68.50 55.06 73.14 62.40
sl ? hr PDT 51.67 49.58 71.47 63.51 54.62 51.82 50.25 43.17 71.27 62.79 50.79 44.07
SSJ 79.51 76.89 88.71 79.69 79.81 78.03 75.95 67.19 86.92 77.28 75.89 68.18
PHRASE hr ? sl PDT 67.28 58.90 60.53 56.79 67.92 61.36 65.77 55.06 58.18 45.41 66.16 55.79
SET 74.68 65.29 69.42 68.55 74.31 65.17 73.36 60.77 68.16 58.42 72.15 61.55
sl ? hr PDT 49.92 46.82 68.18 58.18 52.15 49.42 47.73 41.08 68.51 55.29 48.93 42.59
SSJ 79.29 78.09 88.24 78.75 79.32 78.85 75.33 68.10 86.59 75.66 75.91 68.67
Table 5: Parsing scores (LAS) for cross-lingual parsers trained on translated treebanks. Scores are
split for lexicalized and delexicalized, full MSD and POS only parsers, and with respect to the trans-
lation/projection approaches. Row indices represent source languages and treebanks, and indicate the
direction of applying SMT (e.g., hr ? sl denotes a Croatian treebank translated to Slovene).
for morphologically rich languages. Through our
multilayered test set annotation, we also facilitated
a reliable cross-lingual evaluation in a heteroge-
nous testing environment. We list our most impor-
tant observations:
? Even for closely related languages, using only
the basic POS features ? which are virtually
identical to the widely-used Universal POS of
Petrov et al. (2012) ? substantially decreases
parsing accuracy up to the level comparable with
results of McDonald et al. (2013) across the Uni-
versal Treebanks language groups.
? Adding MSD features heavily influences all the
scores in a positive way. This has obvious im-
plications for improving over McDonald et al.
(2013) on the Universal Treebanks dataset.
? Other than that, we show that it is possible
to cross-lingually parse Croatian, Serbian and
Slovene using all three syntactic annotation
schemes, and with high accuracy. A treebank for
Serbian does not exist, but we accurately parse
Serbian by using PDT, SET and SSJ-style annota-
tions. We parse Croatian using SSJ (transferred
from Slovene) and Slovene using SSJ (trans-
ferred from Croatian). This clearly indicates the
possibilities of uniform downstream pipelining
for any of the schemes.
? We show clear benefits of using the SMT ap-
proach for transferring SSJ parsers to Croatian
and SET parsers to Slovene. We observe these
benefits regardless of the low-quality, out-of-
domain SMT training data (OpenSubs).
Given the current interest for cross-lingual depen-
dency parsing in the natural language processing
community, we will seek to further test our obser-
vations on shared morphological features by us-
ing other pairs of languages of varying relatedness,
drawing from datasets such as Google Universal
Treebanks (McDonald et al., 2013) or HamleDT
(Zeman et al., 2012; Rosa et al., 2014). The goal
of cross-lingual processing in general is to enable
improved general access to under-resourced lan-
guages. With this in mind, seeing how we intro-
duced a test case of Serbian as a language cur-
rently without a treebank, we hope to explore other
options for performing cross-lingual experiments
on actual under-resourced languages, rather than
in an exclusive group of resource-rich placehold-
ers, possibly by means of down-stream evaluation.
Acknowledgments The second author was sup-
ported by the Swedish Research Council (Veten-
skapsr?adet), project 2012-916. The fifth author is
funded by the EU FP7 STREP project XLike.
22
References
Anne Abeill?e. 2003. Treebanks: Building and Using
Parsed Corpora. Springer.
?
Zeljko Agi?c and Nikola Ljube?si?c. 2014. The SE-
Times.HR Linguistically Annotated Corpus of Croa-
tian. In Proc. LREC, pages 1724?1727.
?
Zeljko Agi?c and Danijela Merkler. 2013. Three
Syntactic Formalisms for Data-Driven Dependency
Parsing of Croatian. LNCS, 8082:560?567.
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2012. Slovene-Croatian Treebank Transfer Using
Bilingual Lexicon Improves Croatian Dependency
Parsing. In Proc. IS-LTC, pages 5?9.
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2013. Parsing Croatian and Serbian by Using Croat-
ian Dependency Treebanks. In Proc. SPMRL, pages
22?33.
?
Zeljko Agi?c, Da?sa Berovi?c, Danijela Merkler, and
Marko Tadi?c. 2014. Croatian Dependency Tree-
bank 2.0: New Annotation Guidelines for Improved
Parsing. In Proc. LREC, pages 2313?2319.
Emily Bender. 2011. On achieving and evaluating
language-independence in nlp. Linguistic Issues in
Language Technology, 6(3):1?26.
Emily Bender. 2013. Linguistic Fundamentals for
Natural Language Processing: 100 Essentials from
Morphology and Syntax. Morgan & Claypool Pub-
lishers.
Da?sa Berovi?c,
?
Zeljko Agi?c, and Marko Tadi?c. 2012.
Croatian Dependency Treebank: Recent Develop-
ment and Initial Experiments. In Proc. LREC, pages
1902?1906.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank.
In Treebanks, pages 103?127.
Bernd Bohnet and Jonas Kuhn. 2012. The Best of
Both Worlds ? A Graph-based Completion Model
for Transition-based Parsers. In Proc. EACL, pages
77?87.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Rich?ard Farkas, Filip Ginter, and Jan Hajic. 2013.
Joint Morphological and Syntactic Analysis for
Richly Inflected Languages. TACL, 1:415?428.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proc. COL-
ING, pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proc. CoNLL, pages 149?164.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The Stanford Typed Dependencies Rep-
resentation. In Proc. COLING, pages 1?8.
Kaja Dobrovoljc, Simon Krek, and Jan Rupnik. 2012.
Skladenjski raz?clenjevalnik za sloven?s?cino. In Proc.
IS-LTC, pages 42?47.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic Transfer Using a Bilingual Lexicon. In Proc.
EMNLP-CoNLL, pages 1?11.
Sa?so D?zeroski, Toma?z Erjavec, Nina Ledinek, Petr Pa-
jas, Zdenek
?
Zabokrtsky, and Andreja
?
Zele. 2006.
Towards a Slovene Dependency Treebank. In Proc.
LREC, pages 1388?1391.
Toma?z Erjavec, Darja Fi?ser, Simon Krek, and Nina
Ledinek. 2010. The JOS Linguistically Tagged Cor-
pus of Slovene. In Proc. LREC, pages 1806?1809.
Toma?z Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic Resources for Central and Eastern European
Languages. Language Resources and Evaluation,
46(1):131?142.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proc. WSMT, pages
187?197.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping Parsers via Syntactic Projection across Parallel
Texts. Natural Language Engineering, 11(3):311?
325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
ACL, pages 177?180.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan & Claypool
Publishers.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective Dependency Pars-
ing Using Spanning Tree Algorithms. In Proc. HLT-
EMNLP, pages 523?530.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proc. EMNLP, pages 62?72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013.
Universal Dependency Annotation for Multilingual
Parsing. In Proc. ACL, pages 92?97.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In Proc. CoNLL, pages 915?932.
23
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. ACL,
pages 160?167.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proc. LREC,
pages 2089?2096.
Vesna Po?zgaj Had?zi and Marko Tadi?c. 2000. Croatian-
Slovene Parallel Corpus. In Proc. IS-LTC.
Rudolf Rosa, Jan Ma?sek, David Mare?cek, Martin
Popel, Daniel Zeman, and Zden?ek
?
Zabokrtsk?y.
2014. HamleDT 2.0: Thirty Dependency Treebanks
Stanfordized. In Proc. LREC, pages 2334?2341.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D. Choi, Rich?ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi?orkowski, Ryan Roth, Wolfgang
Seeker, Yannick Versley, Veronika Vincze, Marcin
Woli?nski, Alina Wr?oblewska, and Eric Villemonte
de la Clergerie. 2013. Overview of the SPMRL
2013 Shared Task: Cross-framework Evaluation of
Parsing Morphologically Rich Languages. In Proc.
SPMRL, pages 146?182.
Anders S?gaard. 2011. Data Point Selection for Cross-
language Adaptation of Dependency Parsers. In
Proc. ACL, pages 682?686.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proc. NAACL,
pages 477?487.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013. Target Language Adaptation of Discrimina-
tive Transfer Parsers. In Proc. NAACL, pages 1061?
1071.
Marko Tadi?c and Sanja Fulgosi. 2003. Building the
Croatian Morphological Lexicon. In Proc. BSNLP,
pages 41?46.
Marko Tadi?c and Tam?as V?aradi. 2012. Central and
South-East European Resources in META-SHARE.
Proc. COLING, pages 431?438.
Marko Tadi?c. 2007. Building the Croatian Depen-
dency Treebank: The Initial Stages. Suvremena
lingvistika, 63:85?92.
J?org Tiedemann and Preslav Nakov. 2013. Analyzing
the Use of Character-Level Translation with Sparse
and Noisy Datasets. In Proc. RANLP, pages 676?
684.
J?org Tiedemann,
?
Zeljko Agi?c, and Joakim Nivre. 2014.
Treebank Translation for Cross-Lingual Parser In-
duction. In Proc. CoNLL, pages 130?140.
J?org Tiedemann. 2009. News from OPUS: A Collec-
tion of Multilingual Parallel Corpora with Tools and
Interfaces. In Proc. RANLP, volume 5, pages 237?
248.
J?org Tiedemann. 2012. Character-Based Pivot Trans-
lations for Under-Resourced Languages and Do-
mains. In Proc. EACL, pages 141?151.
J?org Tiedemann. 2014. Rediscovering Annotation
Projection for Cross-Lingual Parser Induction. In
Proc. COLING.
Erik F Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 Shared Task:
Chunking. In Proc. CoNLL, pages 127?132.
Hans Uszkoreit and Georg Rehm. 2012. Language
White Paper Series. Springer.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can We Translate Letters? In Proc. WMT,
pages 33?39.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing Multilingual Text Analy-
sis Tools via Robust Projection Across Aligned Cor-
pora. In Proc. HLT, pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
Language Parser Adaptation between Related Lan-
guages. In Proc. IJCNLP, pages 35?42.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan Step?anek, Zdenek
Zabokrtsk`y, and Jan Hajic. 2012. HamleDT: To
Parse or Not to Parse? In Proc. LREC, pages 2735?
2741.
24
