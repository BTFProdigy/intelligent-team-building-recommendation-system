Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 61?64,
New York, June 2006. c?2006 Association for Computational Linguistics
Investigating Cross-Language Speech Retrieval for a  
Spontaneous Conversational Speech Collection  
 
Diana Inkpen, Muath Alzghool Gareth J.F. Jones Douglas W. Oard 
School of Info. Technology and Eng. School of Computing College of Info. Studies/UMIACS  
University of Ottawa Dublin City University University of Maryland 
Ottawa, Ontario, Canada, K1N 6N5 Dublin 9, Ireland College Park, MD 20742, USA 
{diana,alzghool}@site.uottawa.ca Gareth.Jones@computing.dcu.ie oard@umd.edu 
 
  
Abstract 
Cross-language retrieval of spontaneous 
speech combines the challenges of working 
with noisy automated transcription and lan-
guage translation. The CLEF 2005 Cross-
Language Speech Retrieval (CL-SR) task 
provides a standard test collection to inves-
tigate these challenges. We show that we 
can improve retrieval performance: by care-
ful selection of the term weighting scheme; 
by decomposing automated transcripts into 
phonetic substrings to help ameliorate tran-
scription errors; and by combining auto-
matic transcriptions with manually-assigned 
metadata. We further show that topic trans-
lation with online machine translation re-
sources yields effective CL-SR. 
1 Introduction 
The emergence of large collections of digitized 
spoken data has encouraged research in speech re-
trieval. Previous studies, notably those at TREC 
(Garafolo et al 2000), have focused mainly on 
well-structured news documents. In this paper we 
report on work carried out for the Cross-Language 
Evaluation Forum (CLEF) 2005 Cross-Language 
Speech Retrieval (CL-SR) track (White et al 2005). 
The document collection for the CL-SR task is a 
part of the oral testimonies collected by the USC 
Shoah Foundation Institute for Visual History and 
Education (VHI) for which some Automatic Speech 
Recognition (ASR) transcriptions are available 
(Oard et al, 2004). The data is conversional spon-
taneous speech lacking clear topic boundaries; it is 
thus a more challenging speech retrieval task than 
those explored previously. The CLEF data is also 
annotated with a range of automatic and manually 
generated sets of metadata. While the complete VHI 
dataset contains interviews in many languages, the 
CLEF 2005 CL-SR task focuses on English speech. 
Cross-language searching is evaluated by making 
the topic statements (from which queries are auto-
matically formed) available in several languages. 
This task raises many interesting research ques-
tions; in this paper we explore alternative term 
weighting methods and content indexing strategies.  
The remainder of this paper is structured as fol-
lows: Section 2 briefly reviews details of the CLEF 
2005 CL-SR task; Section 3 describes the system 
we used to investigate this task; Section 4 reports 
our experimental results; and Section 5 gives con-
clusions and details for our ongoing work.   
2 Task description 
The CLEF-2005 CL-SR collection includes 8,104 
manually-determined topically-coherent segments 
from 272 interviews with Holocaust survivors, wit-
nesses and rescuers, totaling 589 hours of speech. 
Two ASR transcripts are available for this data, in 
this work we use transcripts provided by IBM Re-
search in 2004 for which a mean word error rate of 
38% was computed on held out data. Additional, 
metadata fields for each segment include: two sets 
of 20 automatically assigned thesaurus terms from 
different kNN classifiers (AK1 and AK2), an aver-
age of 5 manually-assigned thesaurus terms (MK), 
and a 3-sentence summary written by a subject mat-
ter expert. A set of 38 training topics and 25 test 
topics were generated in English from actual user 
requests. Topics were structured as Title, Descrip-
tion and Narrative fields, which correspond roughly 
to a 2-3 word Web query, what someone might first 
say to a librarian, and what that librarian might ul-
timately understand after a brief reference inter-
view. To support CL-SR experiments the topics 
were re-expressed in Czech, German, French, and 
Spanish by native speakers in a manner reflecting 
61
the way questions would be posed in those lan-
guages. Relevance judgments were manually gener-
ated using by augmenting an interactive search-
guided procedure and purposive sampling designed 
to identify additional relevant segments. See (Oard 
et al 2004) and (White et al 2005) for details.  
3 System Overview 
Our Information Retrieval (IR) system was built 
with off-the-shelf components.  Topics were trans-
lated from French, Spanish, and German into Eng-
lish using seven free online machine translation 
(MT) tools. Their output was merged in order to 
allow for variety in lexical choices. All the transla-
tions of a topic Title field were combined in a 
merged Title field of the translated topics; the same 
procedure was adopted for the Description and Nar-
rative fields. Czech language topics were translated 
using InterTrans, the only web-based MT system 
available to us for this language pair. Retrieval was 
carried out using the SMART IR system (Buckley 
et al 1993) applying its standard stop word list and 
stemming algorithm.  
In system development using the training topics we 
tested SMART with many different term weighting 
schemes combining collection frequency, document 
frequency and length normalization for the indexed 
collection and topics (Salton and Buckley, 1988). In 
this paper we employ the notation used in SMART 
to describe the combined schemes: xxx.xxx. The 
first three characters refer to the weighting scheme 
used to index the document collection and the last 
three characters refer to the weighting scheme used 
to index the topic fields. For example, lpc.atc means 
that lpc was used for documents and atc for queries. 
lpc would apply log term frequency weighting (l) 
and probabilistic collection frequency weighting (p) 
with cosine normalization to the document collec-
tion (c). atc would apply augmented normalized 
term frequency (a), inverse document frequency 
weight (t) with cosine normalization (c). 
One scheme in particular (mpc.ntn) proved to 
have much better performance than other combina-
tions. For weighting document terms we used term 
frequency normalized by the maximum value (m) 
and probabilistic collection frequency weighting (p) 
with cosine normalization (c). For topics we used 
non-normalized term frequency (n) and inverse 
document frequency weighting (t) without vector 
normalization (n). This combination worked very 
well when all the fields of the query were used; it 
also worked well with Title plus Description, but 
slightly less well with the Title field alone. 
4 Experimental Investigation 
In this section we report results from our experi-
mental investigation of the CLEF 2005 CL-SR task. 
For each set of experiments we report Mean unin-
terpolated Average Precision (MAP) computed us-
ing the trec_eval script. The topic fields used are 
indicated as: T for title only, TD for title + descrip-
tion, TDN for title + description + narrative. The 
first experiment shows results for different term 
weighting schemes; we then give cross-language 
retrieval results. For both sets of experiments, 
?documents? are represented by combining the 
ASR transcription with the AK1 and AK2 fields. 
Thus each document representation is generated 
completely automatically. Later experiments ex-
plore two alternative indexing strategies. 
4.1 Comparison of Term Weighting Schemes 
The CLEF 2005 CL-SR collection is quite small by 
IR standards, and it is well known that collection 
size matters when selecting term weighting schemes 
(Salton and Buckley, 1988).  Moreover, the docu-
ments in this case are relatively short, averaging 
about 500 words (about 4 minutes of speech), and 
that factor may affect the optimal choice of weight-
ing schemes as well.  We therefore used the training 
topics to explore the space of available SMART 
term weighting schemes.  Table 1 presents results 
for various weighting schemes with  English topics. 
There are 3,600 possible combinations of weighting 
schemes available: 60 schemes (5 x 4 x 3) for 
documents and 60 for queries. We tested a total of 
240 combinations. In Table 1 we present the results 
for 15 combinations (the best ones, plus some oth-
ers to illustate  the diversity of the results). mpc.ntn 
is still the best for the test topic set; but, as shown, a 
few other weighting schemes achieve similar per-
formance. Some of the weighting schemes perform 
better when indexing all the topic fields (TDN), 
some on TD, and some on title only (T). npn.ntn 
was best for TD and lsn.ntn and lsn.atn are best for 
T. The mpc.ntn weighting scheme is used for all 
other experiments in this section.  We are investi-
gating the reasons for the effectiveness of this 
weighting scheme in our experiments. 
62
TDN TD T  Weighting 
scheme Map Map Map 
1 Mpc.mts 0.2175 0.1651 0.1175 
2 Mpc.nts 0.2175 0.1651 0.1175 
3 Mpc.ntn  0.2176 0.1653 0.1174 
4 npc.ntn 0.2176 0.1653 0.1174 
5 Mpc.mtc 0.2176 0.1653 0.1174 
6 Mpc.ntc 0.2176 0.1653 0.1174 
7 Mpc.mtn 0.2176 0.1653 0.1174 
8 Npn.ntn 0.2116 0.1681 0.1181 
9 lsn.ntn 0.1195 0.1233 0.1227 
10 lsn.atn 0.0919 0.1115 0.1227 
11 asn.ntn 0.0912 0.0923 0.1062 
12 snn.ntn 0.0693 0.0592 0.0729 
13 sps.ntn 0.0349 0.0377 0.0383 
14 nps.ntn 0.0517 0.0416 0.0474 
15 Mtc.atc 0.1138 0.1151 0.1108 
Table 1. MAP, 25 English test topics. Bold=best scores. 
4.2 Cross-Language Experiments 
Table 2 shows our results for the merged ASR, 
AK1 and AK2 documents with multi-system topic 
translations for French, German and Spanish, and 
single-system Czech translation. We can see that 
Spanish topics perform well compared to monolin-
gual English. However, results for German and 
Czech are much poorer. This is perhaps not surpris-
ing for the Czech topics where only a single transla-
tion is available. For German, the quality of 
translation was sometimes low and some German 
words were retained untranslated. For French, only 
TD topic fields were available.  In this case we can 
see that cross-language retrieval effectiveness is 
almost identical to monolingual English. Every re-
search team participating in the CLEF 2005 CL-SR 
task submitted at least one TD English run, and 
among those our mpc.ntn system yielded the best 
MAP (Wilcoxon signed rank test for paired sam-
ples, p<0.05). However, as we show in Table 4, 
manual metadata can yield better retrieval effec-
tiveness than automatic description.  
 
Topic 
Language 
System Map Fields 
English Our system 0.1653 TD 
English Our system 0.2176 TDN 
Spanish Our system 0.1863 TDN 
French Our system 0.1685 TD 
German Our system 0.1281 TDN 
Czech Our system 0.1166 TDN 
Table 2. MAP, cross-language, 25 test topics 
Language Map Fields Description 
English 0.1276 T Phonetic 
English 0.2550 TD Phonetic 
English 0.1245 T Phonetic+Text 
English 0.2590 TD Phonetic+Text 
Spanish 0.1395 T Phonetic 
Spanish 0.2653 TD Phonetic 
Spanish 0.1443 T Phonetic+Text 
Spanish 0.2669 TD Phonetic+Text 
French 0.1251 T Phonetic 
French 0.2726 TD Phonetic 
French 0.1254 T Phonetic+Text 
French 0.2833 TD Phonetic+Text 
German 0.1163 T Phonetic 
German 0.2356 TD Phonetic 
German 0.1187 T Phonetic+Text 
German 0.2324 TD Phonetic+Text 
Czech 0.0776 T Phonetic 
Czech 0.1647 TD Phonetic 
Czech 0.0805 T Phonetic+Text 
Czech 0.1695 TD Phonetic+Text 
Table 3. MAP, phonetic 4-grams, 25 test topics. 
4.3 Results on Phonetic Transcriptions 
In Table 3 we present results for an experiment 
where the text of the collection and topics, without 
stemming, is transformed into a phonetic transcrip-
tion. Consecutive phones are then grouped into 
overlapping n-gram sequences (groups of n sounds, 
n=4 in our case) that we used for indexing. The 
phonetic n-grams were provided by Clarke (2005), 
using NIST?s text-to-phone tool1. For example, the 
phonetic form for the query fragment child survi-
vors is: ch_ay_l_d s_ax_r_v ax_r_v_ay r_v_ay_v 
v_ay_v_ax ay_v_ax_r v_ax_r_z. 
The phonetic form helps compensate for the 
speech recognition errors. With TD queries, the re-
sults improve substantially compared with the text 
form of the documents and queries (9% relative). 
Combining phonetic and text forms (by simply in-
dexing both phonetic n-grams and text) yields little 
additional improvement. 
4.4 Manual summaries and keywords 
Manually prepared transcripts are not available 
for this test collection, so we chose to use manually 
assigned metadata as a reference condition.  To ex-
plore the effect of merging automatic and manual 
fields, Table 4 presents the results combining man-
                                                          
1 http://www.nist.gov/speech/tools/ 
63
ual keywords and manual summaries with ASR 
transcripts, AK1, and AK2. Retrieval effectiveness 
increased substantially for all topic languages. The 
MAP score improved with 25% relative when add-
ing the manual metadata for English TDN.  
Table 4 also shows comparative results between 
and our results and results reported by the Univer-
sity of Maryland at CLEF 2005 using a widely used 
IR system (InQuery) that has a standard term 
weighting algorithm optimized for large collections. 
For English TD, our system is 6% (relative) better 
and for French TD 10% (relative) better.  The Uni-
versity of Maryland results with only automated 
fields are also lower than the results we report in 
Table 2 for the same fields. 
 
Table 4. MAP, indexing all fields (MK, summaries, 
ASR transcripts, AK1 and AK2), 25 test topics. 
Language System Map Fields 
English Our system 0.4647 TDN 
English Our system 0.3689 TD 
English InQuery 0.3129 TD 
English Our system 0.2861 T 
Spanish Our system 0.3811 TDN 
French Our system 0.3496 TD 
French InQuery 0.2480 TD 
French Our system 0.3496 TD 
German Our system 0.2513 TDN 
Czech Our system 0.2338 TDN 
5 Conclusions and Further Investigation 
The system described in this paper obtained the best 
results among the seven teams that participated in 
the CLEF 2005 CL-SR track. We believe that this 
results from our use of the 38 training topics to find 
a term weighting scheme that is particularly suitable 
for this collection. Relevance judgments are typi-
cally not available for training until the second year 
of an IR evaluation; using a search-guided process 
that does not require system results to be available 
before judgments can be performed made it possi-
ble to accelerate that timetable in this case.  Table 2 
shows that performance varies markedly with the 
choice of weighting scheme.  Indeed, some of the 
classic weighting schemes yielded much poorer 
results than the one  we ultimately selected. In this 
paper we presented results on the test queries, but 
we observed similar effects on the training queries. 
On combined manual and automatic data, the 
best MAP score we obtained for English topics is 
0.4647. On automatic data, the best MAP is 0.2176. 
This difference could result from ASR errors or 
from terms added by human indexers that were not 
available to the ASR system to be recognized. In 
future work we plan to investigate methods of re-
moving or correcting some of the speech recogni-
tion errors in the ASR transcripts using semantic 
coherence measures. 
In ongoing further work we are exploring the re-
lationship between properties of the collection and 
the weighting schemes in order to better understand 
the underlying reasons for the demonstrated effec-
tiveness of the mpc.ntn weighting scheme.  
The challenges of CLEF CL-SR task will con-
tinue to expand in subsequent years as new collec-
tions are introduced (e.g., Czech interviews in 
2006). Because manually assigned segment bounda-
ries are available only for English interviews, this 
will yield an unknown topic boundary condition 
that is similar to previous experiments with auto-
matically transcribed broadcast news the Text Re-
trieval Conference (Garafolo et al 2000), but with 
the additional caveat that topic boundaries are not 
known for the ground truth relevance judgments.    
References 
Chris Buckley, Gerard Salton, and James Allan. 1993. 
Automatic retrieval with locality information using 
SMART. In Proceedings of the First Text REtrieval 
Conference (TREC-1), pages 59?72. 
Charles L. A. Clarke. 2005. Waterloo Experiments for 
the CLEF05 SDR Track, in Working Notes for the 
CLEF 2005 Workshop, Vienna, Austria 
John S. Garofolo, Cedric G.P. Auzanne and Ellen M. 
Voorhees. 2000. The TREC Spoken Document Re-
trieval Track: A Success Story. In Proceedings of the 
RIAO Conference: Content-Based Multimedia Infor-
mation Access, Paris, France, pages 1-20. 
Douglas W. Oard, Dagobert Soergel, David Doermann, 
Xiaoli Huang, G. Craig Murray, Jianqiang Wang, 
Bhuvana Ramabhadran, Martin Franz and Samuel 
Gustman. 2004. Building an Information Retrieval 
Test Collection for Spontaneous Conversational 
Speech, in  Proceedings of SIGIR, pages 41-48. 
Gerard Salton and Chris Buckley. 1988. Term-weighting 
approaches in automatic retrieval. Information Proc-
essing and Management, 24(5):513-523. 
Ryen W. White, Douglas W. Oard, Gareth J. F. Jones, 
Dagobert Soergel and Xiaoli Huang. 2005. Overview 
of the CLEF-2005 Cross-Language Speech Retrieval 
Track, in Working Notes for the CLEF 2005 Work-
shop, Vienna, Austria 
64
