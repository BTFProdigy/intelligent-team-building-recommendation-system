Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 37?46,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
A Three-Way Perspective on Scientific Discourse Annotation for       Knowledge Extraction   Maria Liakata Aberystwyth University, UK / EMBL-EBI, UK liakata@ebi.ac.uk Paul Thompson University of Manchester, UK paul.thompson@manchester.ac.uk Anita de Waard Elsevier Labs, USA / UiL-OTS, Universiteit Utrecht, NL a.dewaard@elsevier.com    Raheel Nawaz University of Manchester, UK raheel.nawaz@cs.man.ac.uk Henk Pander Maat UiL-OTS, Universiteit Utrecht, NL h.l.w.pandermaat@uu.nl Sophia Ananiadou University of Manchester, UK sophia.ananiadou@manchester.ac.uk   Abstract 
This paper presents a three-way perspective on the annotation of discourse in scientific literature. We use three different schemes, each of which focusses on different aspects of discourse in scientific articles, to annotate a corpus of three full-text papers, and compare the results. One scheme seeks to identify the core components of scientific investigations at the sentence level, a second annotates meta-knowledge pertaining to bio-events and a third considers how epistemic knowledge is conveyed at the clause level. We present our analysis of the comparison, and a discussion of the contributions of each scheme.  1 Introduction The literature boom in the life sciences over the past few years has sparked increasing interest into text mining tools, which facilitate the automatic extraction of useful knowledge from text (Ananiadou et al, 2006; Ananiadou  &  McNaught, 2006; Zweigenbaum et al, 2007; Cohen  &  Hunter, 2008). Most of these tools have focussed on entity recognition and relation extraction and with few exceptions, e.g., (Hyland, 1996; Light et al, 2004; S?ndor, 2007; Vincze et al, 2008), do not take into account the discourse context of the knowledge extracted. However, failure to take this context into account results in the loss of information vital for the correct interpretation of extracted knowledge, e.g. the scope of the relations, or the level of certainty with which they are expressed. A particular piece of 
knowledge may represent, e.g., an accepted fact, hypothesis, results of an experiment, analysis based on experimental results, factual or speculative statements etc. Furthermore, this knowledge may represent the author's current work, or work reported elsewhere. The ability to recognise different discourse elements automatically provides crucial information for the correct interpretation of extracted knowledge, allowing scientific claims to be linked to experimental evidence, or newly reported experimental knowledge to be isolated. The importance of categorising such knowledge becomes more pronounced as analysis moves from abstracts to full papers, where the content is richer and linguistic constructions are more complex (Cohen et al, 2010). Analysis of full papers is extremely important, since less than 8% of scientific claims occur in abstracts (Blake, 2010). Various different schemes for annotating discourse elements in scientific texts have been proposed. The schemes vary along several axes, including perspective, motivation, complexity and the granularity of the units of text to which the scheme is applied. Faced with such variety, it is important to be able to select the best scheme(s) for the purpose at hand. Answers to questions such as the following can help in the selection process: 1. What are the relative merits of the different schemes? 2. What are the similarities and differences between schemes? 3. Can annotation according to multiple schemes provide enhanced information?  
37
Category Description Hypothesis An unconfirmed statement which is a stepping stone of the investigation Motivation The reasons behind an investigation Background Generally accepted background knowledge and previous work Goal A target state of the investigation where intended discoveries are made Object-New An entity which is a product or main theme of the investigation Object-New-Advantage Advantage of an object Object-New-Disadvantage Disadvantage of an object Method-New Means by which authors seek to achieve a goal of the investigation Method-New-Advantage Advantage of a Method Method-New-Disadvantage Disadvantage of a Method Method-Old A method mentioned pertaining to previous work Method-Old-Advantage Advantage of a Method Method-Old-Disadvantage Disadvantage of a Method Experiment An experimental method Model A statement about a theoretical model or framework Observation The data/phenomena recorded in an investigation Result Factual statements about the outputs, interpretation of observations Conclusion Statements inferred from observations & results  Table 1. The CoreSC Annotation scheme: layers 1 & 2 	 ?4. Is there any advantage in merging annotation schemes or is it better to allow complementary and different dimensions of scientific discourse annotation?	 ?As a starting point to addressing such questions, we provide a comparison of three different schemes for the annotation of discourse elements within scientific papers. Each scheme has a different perspective and motivation:, one is content-driven, seeking to identify the main components of a scientific investigation, another is driven by the need to describe events of biomedical relevance and the third focusses on how epistemic knowledge is conveyed in discourse.  These different viewpoints mean that the schemes vary in both the type and complexity of the discourse elements identified, as well as the types of units to which the annotation is applied, i.e. complete sentences, segments of sentences, or specific relations/events occurring within these sentences. To facilitate the comparison, we have annotated three full papers according to each of the schemes. The analysis resulting from this three-way annotation considers mappings between schemes, their relative merits, and how the information annotated by the different schemes can 
complement each other to provide enriched details about knowledge extracted from the texts. In the following sections, we firstly provide a description of the three schemes, and then explain how they have been used in our corpus annotation. Finally we discuss the results from the comparison, and the features of each scheme. 2 Sentence annotation: CoreSC scheme  The reasoning behind this scheme is that a paper is the human-readable representation of a scientific investigation. Therefore, the goal of the annotation is to retrieve the content model of scientific investigations as reflected within scientific discourse. The hypothesis is that there is a set of core scientific concepts (CoreSC), which constitute the key components of a scientific investigation. CoreSCs consist of 11 concepts originating from the CISP (Core Information about Scientific Papers) meta-data (Soldatova  &  Liakata, 2007), which are a subset of classes from the EXPO ontology for the description of scientific experiments (Soldatova  &  King, 2006). The CoreSCs are: Motivation, Goal, Object, Background, Hypothesis, Method, Model, Experiment, Observation, Result and Conclusion. 
38
Figure 1.  Bio-Event Representation 
The CoreSC scheme (Liakata et al, 2010; Liakata et al, 2012) implements the above-mentioned concepts as a 3-layered sentence-based annotation scheme. This means that each sentence in a document is assigned one of the 11 CoreSC concepts. The scheme also considers a layer designated to properties of the concepts (e.g. New Method vs Old Method) as well as identifiers which link instances of the same concept across sentences. A short definition of CoreSC categories and their properties can be found in Table 1.  The CoreSC scheme is accompanied by 47-page annotation guidelines, and has been used by 16 domain experts to annotate a corpus of 265 full papers from physical chemistry & biochemistry (Liakata  &  Soldatova, 2009; Liakata et al, 2010). This corpus consists of 40,000 sentences, containing over 1 million words and was developed in three phases (for details see Liakata et al (2012)). Inter-annotator agreement between experts was measured in terms of Cohen?s kappa (Cohen, 1960) on 41 papers and ranged between 0.5 and 0.7. Machine learning classifiers have been trained on the CoreSC corpus, achieving > 51% accuracy across the eleven categories. The most accurately predicted category is Experiment, the category describing experimental methods (Liakata et al, 2012). Classifiers trained on 1000 Biology abstracts annotated with CoreSC have obtained an accuracy of over 80% (Guo et al, 2010). Models trained on the CoreSC corpus papers have been used to create automatic summaries of the papers, which have been evaluated in a question answering task (Liakata et al, 2012). Lastly, the CoreSC scheme was used to annotate 50 papers from Pubmed Central pertaining to Cancer Risk Assessment. A web tool (SAPIENTA 1 ) allows users to annotate their full papers with Core Scientific concepts, and can be combined with manual annotation. A UIMA framework 2 implementation of this code for large-scale annotation of CoreSC concepts is in progress. 3 Event annotation: Meta-knowledge for bio-events The motivation for this annotation scheme is to allow the training of more sophisticated event-                                                1 http://www.sapientaproject.com/software 2 http://uima.apache.org/ 
based information extraction systems. In contrast to the sentence-based scheme described in section 2, this scheme is applied at the level of events (Ananiadou et al, 2010), of which there may be several within a single sentence. 3.1 Bio-Events Events are template-like, structured representations of pieces of knowledge contained within sentences. Normally, events are ?anchored? to a trigger (typically a verb or noun) around which the knowledge expressed is organised. Each event has one of more participants, which describe different aspects of the event. Participants can correspond to entities or other events, and are often labelled with semantic roles, e.g., CAUSE, THEME, LOCATION, etc. The work described here focusses specifically on bio-events, which are complex structured relations representing fine-grained relations between bio-entities and their modifiers. Figure 1 provides some examples of bio-events. Event extraction systems (Bj?rne et al, 2009; Miwa et al, 2010; Miwa et al, 2012; Quirk et al, 2011) are typically trained on text corpora, in which events and their participants have been manually annotated by domain experts. Research into bio-event extraction has been boosted by the two recent shared tasks at BioNLP 2009/2011 (Kim et al, 2011; Pyysalo et al, In Press). Several gold standard event annotated corpora exist; examples include the GENIA Event Corpus (Kim et al, 2008), GREC (Thompson et al, 2009) and BioInfer (Pyysalo et al, 2007), in addition to the corpora produced for the shared tasks. 
3.2 Meta-knowledge Annotation Until recently, the only attempts to recognise information relating to the correct interpretation of events were restricted to sparse details regarding negation and speculation (Kim et al, 2011). 
39
In order to address this problem, a multi-dimensional annotation scheme especially tailored to bio-events was developed (Nawaz et al, 2010; Thompson et al, 2011). The scheme identifies and categorises several different types of contextual details regarding events (termed meta-knowledge), including discourse information. Different types of meta-knowledge are encoded through five distinct dimensions (Figure 2). The advantage of using multiple dimensions is that the interplay between the assigned values in each dimension can reveal both subtle and substantial differences in the types of meta-knowledge expressed. In the majority of cases, meta-knowledge is expressed through the presence of particular ?clue? words or phrases, although other features can also come into play, such as the tense of the event trigger, or the relative position within the text. 
Figure 2: Meta-knowledge annotation 	 ?The annotation task consists of assigning an appropriate value from a fixed set for each dimension, as well as marking the textual evidence for this assignment. The five meta-knowledge dimensions and their values are as follows: Knowledge Type (KT): Captures the general information content of the event. Each event is classified as one of: Investigation (enquiries and examinations, etc.), Observation (direct experimental observations), Analysis (inferences, interpretations and conjectures, etc.), Fact (known facts), Method (methods) or Other (general events that provide incomplete information or do not fit into any other category).  Certainty Level (CL): Encodes the confidence or certainty level ascribed to the event in the given text. The epistemic scale is partitioned into three distinct levels: L3 (no expression of uncertainty), 
L2 (high confidence or slight speculation) and L1 (low confidence or considerable speculation). Polarity: Identifies negated events. Negation is defined as the absence or non-existence of an entity or a process. Manner: Captures information about the rate, level, strength or intensity of the event, using three values: High, Low, or Neutral (no indication of rate/intensity). Source:  Encodes the source of the knowledge being expressed by the event as Current (the current study) or Other (any other source).      Of these five dimensions, only KT, CL and Source were considered during the comparison with the other two schemes, since they are directly related to discourse analysis.  The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al, 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3  (Thompson et al, 2011). Inter-annotator agreement rates ranged between 0.84?0.93 (Cohen?s Kappa).  Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al, In Press).  In addition, the EventMine-MK service (Miwa et al, In Press), based on EventMine (Miwa et al, 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP?09 Shared Task corpus (Kim et al 2011). EventMine-MK is available as a component of the U-Compare interoperable text mining system4 (Kano et al, 2011). 4 Clause annotation: Segments for epistemic knowledge The third scheme we consider uses a Discourse Segment Type classification of segments at, roughly, a clause level, i.e., each segment has a main verb. This means that the level of granularity of argumentational elements in this scheme lies between the other two schemes, i.e. it is usually more granular than CoreSC, but sometimes less granular than the event-based scheme.                                                  3 http://www.nactem.ac.uk/meta-knowledge/ 4 http://www.nactem.ac.uk/ucompare/ 
40
 Table 2:  Discourse Segment Types 	 ?The segment annotation scheme identifies a taxonomy of discourse segment types that seem to be exclusive and useful (de Waard & Pander Maat, 2009). Three classes of segment types are defined:  ? Basic segment types: segments referring directly to the topic of study ? see Table 2.  ? ?Other?-segment types: segments referring to conceptual or experimental work in other research papers than the current one ? Regulatory segment types: ?regulatory? clauses that control and introduce other segments.  A list of segment types is presented in Table 2; further details, including a list of all segment types and correlations with verb tense can be found in de Waard  &  Pander Maat (2009). The focus of this work is to identify linguistic features that characterise these discourse segment types, according to three aspects: ? Verb tense, aspect, mood and voice ? Semantic verb class ? Epistemic modality markers So far, 6 full-text papers (comprising about 2300 segments) have been manually annotated with segment types and correlated with the above features. A first automated validation was promising (de Waard, Buitelaar and Eigener, 2009). The need for parsing at a clause level is especially prominent in biological text, since specific semantic roles are played by particular clause types. We give four examples of typical 
clause constructions that play a specific rhetorical role: firstly, reporting clauses are often sentence-initial ?that? matrix clauses (1a): 1. a.  This suggests that  1.b. miR-372 and miR-373 caused the observed  selective growth advantage. Secondly, descriptions confirming certain accepted characteristics of biological entities are often given as nonrestrictive relative clauses (2b):  2.a. We also generated BJ/ET cells expressing the  RASV12-ERTAM chimera gene,  2. b. which is only active when tamoxifen is added  Thirdly, a subordinate gerund clause is often used to describe a method (3a), with a main (finite) clause describing a result (3b) and fourthly, experimental goals are often given as a (mostly sentence-initial) clause with a to-infinitive (4a) often preceding a past-tense methods clause (4b). 3. a. Using fluorescence microscopy and luciferase assays, b. we observed potent and specific miRNA activity expressed from each miR-Vec (Figure S2). 4. a. To identify miRNAs that can interfere with this process  4. b. we transduced BJ/ET fibroblasts with miR-Lib  However, the lack of simple robust clause parsers has prevented the automated identification of semantic roles at the clause level. Therefore, this scheme has so far only been manually 
Segment Description Examples  Fact knowledge accepted to be true, a known fact. mature miR-373 is a homolog of miR-372,  Hypothesis  a proposed idea, not supported by evidence This could for instance be a result of high mdm2 levels  Problem unresolved, contradictory, or unclear issue However, further investigation is required to demonstrate the exact mechanism of LATS2 action Goal research goal To identify novel functions of miRNAs, Method  experimental method Using fluorescence microscopy and luciferase assays,  Result a restatement of the outcome of an experiment all constructs yielded high expression levels of mature miRNAs   Implication  an interpretation of the results, in light of data our procedure is sensitive enough to detect mild growth differences   Other-Hypothesis an idea proposed by others [It is generally believed that] transcription factors are the final common pathway driving differentiation] Regulatory-Hypothesis a matrix clause introducing a hypothesis It is generally believed that [transcription factors are the final common pathway driving differentiation] 
41
implemented. Despite being less widely implemented than the other two schemes, we believe that the segment scheme offers some useful pointers for linguistic features that can identify particular rhetorical classes in the text, and secondly, offers an interesting perspective on the fact that in biological text, several rhetorical moves are made within a single sentence.  5 Data and methods Three papers already annotated according to the GENIA event annotation scheme (Kim et al, 2008), were further annotated according to the three annotation schemes described above. We obtained all corresponding CoreSCs, events and segments per sentence. Each sentence has a single CoreSC annotation and one or more segment annotations (depending on the number of clauses). Event annotations in a sentence may range from zero to multiple, according to whether any relevant biomedical events are described in the sentence.  Events within a sentence are mapped to segments by identifying which segment contains the trigger for a particular event. The three meta-knowledge dimensions for events considered in this comparison, i.e., KT, CL and Source, result in 16 different combinations of values encountered in the three papers. The numbers for CoreSC and Segment labels encountered were 12 and 22, respectively. Confusion matrices were obtained for each paper and for each pair of annotation schemes. Note that, as bio-events are largely unconcerned with describing methodology, the Methods sections of these papers do not contain event annotation or meta-knowledge annotation. The pairwise confusion matrices from each paper were combined, resulting in three matrices (Tables 3, 4 and 5), which describe the associations between the annotation schemes in the three papers examined. We have highlighted the highest frequencies per row and where appropriate also the highest values per column. The use of two different colours aims to facilitate readability. 6 Results and Discussion We present the results from analysing the pairwise confusion matrices for the three schemes and discuss the merits of each scheme. 
6.1 Event Meta-knowledge v. CoreSC In Tables 3 (and 5), the meta-knowledge categories combine KT, CL and Source ((O)ther) values. Table 3 shows some straightforward and expected mappings, e.g.,Method (Met,L3) events are almost always found within CoreSC Experiment or Method sentences, whilst Investigation events (Inv,L3) occur most frequently within CoreSC Goal or Motivation sentences.  For other categories, information from the two schemes can complement each other in different ways. For example, KT and Source information about events can help to distinguish different types of information within CoreSC Background sentences (top left corner of Table 3). Such information mainly corresponds to facts, observations from previous studies, or analyses of information. Conversely, information from the CoreSC scheme can help to further classify the interpretation of events. For example, events with an analytical interpretation (Ana,L1,L2,L3) may occur as background information to a study (Bac), as hypotheses (Hyp),  as part of observations (Obs), when reporting the results of the current study (Res) or when making concluding remarks about the study (Con). CoreSCs can also help to further refine events relating to outcomes (Obs,L3) according to whether they pertain to (Obs)ervations, (Res)ults or (Con)clusions. CoreSC Conclusion, Result and Observation sentences contain mainly Observation events concerned with the current study. However, such sentences often also include an analytical part, with varying levels of certainty, which event information can help to isolate. The CL annotated for events is also useful in helping to determine the confidence with which information is stated in CoreSC Conclusion and Hypothesis sentences.  Due to the nature of bio-event annotation, only a small number of events correspond to methods. Thus, CoreSC provides a more detailed characterisation of method-related sentences, i.e., Experiment, Method_New, Model and Object. 6.2 Discourse Segments v. CoreSC In most cases, there seems to be natural mapping between the two schemes (See Table 4). CoreSC Observation maps to Result, CoreSC Method and Experiment map to Method, CoreSC Hypothesis maps to Hypothesis, CoreSC Goal maps to Goal, 
42
CoreSC Conclusion maps to Implication and Hypothesis, CoreSC Result maps to Implication and Result, and Problem is equivalent to CoreSC Motivation. The bulk of CoreSC Background maps to Fact and Other-Implication, but the ?Other? Segment categories provide a substantial refinement of the CoreSC Background category.   
 Table 3. Event Meta-knowledge vs CoreSC    On the other hand, CoreSC refines Method, Result and Implication segments. CoreSC Result may include both Fact and Method clauses, which can be captured by the Segment scheme, since annotation is performed at the clause level. CoreSC Conclusion maps to both Implication and Hypothesis segments, suggesting that there may be differences in the certainty levels of these conclusions. This is supported by preliminary classification experiments (paper in progress).  	 ?6.3 Discourse Segments v. Event Meta-Knowledge	 ? Some straightforward mappings exist between segment and event meta-knowledge categories (Table 5). For example, Investigation events (Inv, L3) are generally found within Goal and Problem segments; Method events (Met,L3) are normally found within Method segments, Observation events (Obs,L3) are found mainly within Result, Fact and Implication segments and (Ana,L1,L2) events correspond mainly to Hypotheses and Implications. Whilst these are similar findings to the comparison between event meta-knowledge and CoreSCs, the variance of the distribution is often smaller when mapping from Events to Segments. This is to be expected ? the information encoded by many events has the scope of roughly a clause, which corresponds closely to the scope of 
discourse segments. This could permit cleaner one-to-one mappings between categories. 
 Table 4: Segments vs CoreSC 	 ?  Hypothesis and Implication segments mainly contain (Ana)lysis events. The differing certainty levels of events can help to refine information about the statements made within these segments. Likewise, these segment types could help to refine the nature of the analysis described by the event.   Similarly to the CoreSC scheme, the results suggest that Result segments could be refined by the meta-knowledge scheme to distinguish between results emerging from direct experimental observations, and those obtained through analysis of experimental observations. Another interesting result is that Fact segments can contain Fact, (Ana)lysis or (Obs)ervation events. This may suggest that Fact segments are actually a rather general category, containing a range of different information. Few events occur within the Regulatory segments, as these mainly introduce content-bearing segments.  The majority of Method segments and a significant number of the Result segments do not correspond to events, as none of the methods sections have been annotated with event information, for reasons explained previously.  
	 ?Table 5: Segments vs Event Meta-Knowledge 
Sheet1
Page 1
Bac Con Exp Goa Hyp Met_New Met_Old Mod Mot Obj_New Obs Res0 42 24 49 7 7 25 1 13 6 7 47 54Obs,L3,O 166 0 0 0 0 0 3 0 12 0 0 2Ana,L3,O 33 1 0 0 0 0 0 1 0 0 0 0Ana,L2,O 3 0 0 0 0 0 0 0 0 0 0 0Fact,L3,O 7 0 0 0 0 0 0 0 0 0 0 0Fact,L3 24 1 0 1 0 0 0 0 5 3 0 2Oth,L3 125 30 0 8 16 5 3 2 8 3 9 42Ana,L1 2 10 0 0 6 0 0 0 1 0 0 6Ana,L2 30 15 0 1 14 0 0 2 1 0 8 33Ana,L3 11 11 0 0 2 1 2 0 3 0 14 28Met,L3 4 1 15 1 0 5 0 0 0 0 2 6Inv,L2 0 0 0 0 0 0 0 0 0 0 1 1Inv,L3 5 3 1 6 2 4 3 0 8 0 1 1Inv,L3,O 0 0 0 0 2 0 1 0 0 0 0 0Obs,L1 1 0 0 0 0 0 0 0 0 0 0 0Obs,L2 1 0 0 0 0 0 0 0 0 0 1 0Obs,L3 31 34 3 1 10 3 0 2 7 1 59 87
Sheet1
Page 1
Bac Con Exp Goa Hyp Met_New Met_Old Mod Mot Obj_New Obs ResFact 118 3 0 3 7 0 0 1 15 7 5 34OtherFact 70 4 0 0 0 0 0 3 1 0 0 0OtherGoal 2 1 0 0 0 0 0 0 0 0 0 0OtherHypothesis 14 0 0 0 0 0 0 0 0 0 0 0OtherImplication 124 1 0 0 3 0 0 1 5 0 0 1OtherMethod 5 0 0 0 0 0 3 0 0 0 0 2OtherProblem 1 0 0 0 0 0 0 0 0 0 0 0OtherResult 64 1 0 0 0 0 6 0 0 3 0 9RegFact 1 3 0 0 0 0 0 0 0 0 0 2Implication 13 58 0 0 2 0 0 3 1 0 3 80RegImplication 5 6 0 1 0 0 0 0 0 0 1 10Method 6 2 54 2 2 32 0 6 1 0 8 13Goal 2 0 5 12 6 9 2 2 4 0 0 5RegGoal 0 1 0 0 0 0 0 0 0 0 0 0Hypothesis 24 31 0 5 34 1 0 5 0 0 0 12RegHypothesis 6 4 0 0 2 0 0 1 0 0 0 2Problem 7 6 0 0 0 0 2 0 11 0 0 2RegProblem 0 3 0 0 0 0 0 0 0 0 0 0Result 13 6 1 1 2 0 0 2 8 0 112 75RegResult 1 0 0 0 0 0 0 0 0 1 1 2Intertextual 4 0 7 0 1 0 0 0 0 0 0 3Intratextual 2 0 1 0 0 0 0 2 0 0 8 4
Sheet1
Page 1
0 Ana Ana Ana Ana Ana Fact Fact Met Oth Inv Inv Inv ObsObsObsObsL1 L2 L2,OL3 L3,OL3 L3,O L3 L3 L2 L3 L3,OL1 L2 L3 L3,OHypothesis 8 18 26 1 0 0 0 0 1 39 0 4 1 0 0 14 0Implication 22 2 30 0 34 2 2 0 0 38 2 1 0 0 0 27 0OtherHypothesis 0 0 3 1 0 0 0 0 0 9 0 1 0 0 0 0 0OtherImplication 8 1 6 1 4 28 0 3 3 27 0 2 0 1 0 5 46RegImplication 11 0 2 0 0 1 0 0 0 5 0 1 0 0 0 3 0RegHypothesis 1 0 0 0 0 0 0 0 0 6 0 1 0 0 0 7 0Fact 15 0 18 0 6 0 28 0 0 55 0 1 0 0 1 44 25RegFact 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 5 0OtherGoal 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0OtherProblem 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0Method 80 0 1 0 0 0 0 0 23 9 0 2 0 0 0 8 3OtherMethod 7 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0Goal 13 0 0 0 0 0 1 0 0 18 0 11 1 0 0 3 0RegGoal 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Problem 9 4 0 0 2 0 0 0 0 5 0 8 0 0 0 0 0RegProblem 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Result 51 0 14 0 20 0 0 0 6 18 0 0 0 0 1 103 7OtherResult 11 0 1 0 0 1 0 1 0 10 0 0 0 0 0 12 47OtherFact 4 0 1 0 0 2 5 3 0 7 0 0 0 0 0 2 54RegResult 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Intertextual 13 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0Intratextual 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0280 4 35 0 28 3 34 4 29 127 0 24 2 0 2 178 136
43
7 Related Work A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight  &  Srinivasan, 2003; Ruch et al, 2007; Hirohata et al, 2008; Bj?rne et al, 2009). The work of Hirohata et al (2009) has been integrated with the MEDIE service5 (Miyao et al, 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al, 1999; Teufel  &  Moens, 2002; Teufel et al, 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al (2005) in order to facilitate information extraction, and more recently Teufel et al (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012).  Modality and negation in text have also been the focus of recent workshops (Farkas et al(2010), Morante & Sporleder (2012)). Finally, Shatkay et al(2008) define a multi-dimensional scheme, which combines several of the above-mentioned aspects.      Recent work has compared schemes to discover mappings and relative merits. Liakata et al (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al (2008), a cut-down version of the                                                 5 http://www.nactem.ac.uk/medie/ 
scheme proposed by Teufel et al (2009) and CoreSC (1st layer), from general to specific.	 ?8  Conclusion We have compared three different schemes, each taking a different perspective to the annotation of scientific discourse. The comparison shows that the three schemes are complementary, with different strengths and points of focus. CoreSC offers a fine-grained characterisation of methods, outcomes and objectives. It has been used to annotate a collection of 265 full papers, and subsequently CoreSC recognition has been fully automated, creating the online SAPIENTA tool. The discourse segment annotation scheme can help to provide a finer-grained characterisation of background work, and could also help to split multi-clause CoreSC sentences into appropriate segments. Recognition of event meta-knowledge has been fully automated in the U-Compare framework, and the KT values of the scheme can help to provide a finer-grained analysis of certain segment and sentence types. The CL dimension also allows confidence values to be ascribed to the Conclusion, Result, Implication and Hypothesis categories of the other two schemes.   Future work will focus on annotating texts with several discourse perspectives to investigate the advantages of the schemes. Ideally we would like to propose a unified approach for scientific discourse annotation, but recognize that choices such as the unit of annotation are often task-oriented, and that users should be able to mix and match discourse segments as required. This said, the analysis in this paper paves the way for potential harmonisation, revealing points of union and intersection between the schemes. Acknowledgements This work has been supported through funding for Maria  Liakata by JISC, the Leverhulme Trust and EBI-EMBL. It has also been supported by the BBSRC through grant number BB/G013160/1UK (Automated Biological Event Extraction from the Literature for Drug Discovery), the MetaNet4U project (ICT PSP Programme, Grant Agreement: No. 270893) and the JISC-funded ISHER project.  
44
References  Ananiadou, S., Kell, D.B. and Tsujii, J. (2006). Text mining and its potential applications in systems biology. Trends Biotechnol, 24(12): 571-9. Ananiadou, S. and McNaught, J., Eds. (2006). Text Mining for Biology and Biomedicine. Boston / London, Artech House. Ananiadou, S., Pyysalo, S., Tsujii, J. and Kell, D.B. (2010). Event extraction for systems biology by text mining the literature. Trends Biotechnol, 28(7): 381-90. Bj?rne, J., Heimonen, J., Ginter, F., Airola, A., Pahikkala, T. and Salakoski, T. (2009). Extracting Complex Biological Events with Rich Graph-Based Feature Sets. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pp.  10-18. Blake, C. (2010). Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles. Journal of Biomedical Informatics, 43(2): 173-189. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and psychological measurement, 20: 37-46. Cohen, K.B. and Hunter, L. (2008). Getting started in text mining. PLoS Comput Biol, 4(1): e20. Cohen, K.B., Johnson, H.L., Verspoor, K., Roeder, C. and Hunter, L.E. (2010). The structural and content aspects of abstracts versus bodies of full text journal articles are different. BMC Bioinformatics, 11: 492. de Waard, A., Buitelaar, P., Eigner, T. (2009). Identifying the epistemic value of discourse segments in biology texts. Proceedings of the Eighth International Conference on Computational Semantics, pp. 351-354 de Waard, A. and Pander Maat, H. (2009). Categorizing Epistemic Segment Types in Biology Research Articles. In Proceedings of the Workshop on Linguistic and Psycholinguistic Approaches to Text Structuring (LPTS 2009) de Waard, A. and Pander Maat, H. (2012). Knowledge Attribution in Scientific Discourse: A Taxonomy of Types and Overview of Features, In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse (DSDD), ACL 2012. Farkas, R.	 ?Vincze, V., M?ra, G., Csirik, J. and Szarvas, G. 2010. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, Uppsala, Sweden. 
Association for Computational Linguistics, pp. 1- 12. Guo, Y., Korhonen, A., Liakata, M., Silins, I., LiSun, L. and Stenius, U. (2010). Identifying the information structure of scientific abstracts: An investigation of three different schemes. In Proceedings of BioNLP 2010, pp.  99-107. Hirohata, K., Okazaki, N., Ananiadou, S. and Ishizuka, M. (2008). Identifying Sections in Scientific Abstracts using Conditional Random Fields. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pp.  381-388. Hyland, K. (1996). Writing without conviction? Hedging in science research articles. Applied Linguistics, 17(4): 433-454. Kano, Y., Miwa, M., Cohen, K.B., Hunter, L.E., Ananiadou, S. and Tsujii, J. (2011). U-Compare: A modular NLP workflow construction and evaluation system. IBM Journal of Research and Development, 55(3): 11:1-11:10. Kilicoglu, H. and Bergler, S. (2008). Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC Bioinformatics, 9(Suppl 11): S10. Kim, J.-D., Ohta, T. and Tsujii, J. (2008). Corpus annotation for mining biomedical events from literature. BMC Bioinformatics, 9(10). Kim, J.D., Ohta, T., Pyysalo, S., Kano, Y. and Tsujii, J. (2011). Extracting Bio-Molecular Events from Literature - The BioNLP'09 Shared Task. Computational Intelligence, 27(4): 513-540. Liakata, M., Saha, S., Dobnik, S., Batchelor, C. and Rebholz-Schuhmann, D. (2012). Automatic recognition of conceptualisation zones in scientific articles and two life science applications. Bioinformatics, 28 (7). Liakata, M. and Soldatova, L.N. (2009). The ART corpus. Technical Report. Aberystwth University. Liakata, M., Teufel, S., Siddharthan, A. and Batchelor, C. (2010). Corpora for the conceptualisation and zoning of scientific papers. In Proceedings of LREC, pp.  2054-2061. Light, M., Qiu, X.Y. and Srinivasan, P. (2004). The language of bioscience: Facts, speculations, and statements in between. In Proceedings of the BioLink 2004 Workshop at HLT/NAACL, pp.  17?24. McKnight, L. and Srinivasan, P. (2003). Categorization of sentence types in medical abstracts. In AMIA Annu Symp Proc, pp.  440-4. Miwa, M., Saetre, R., Kim, J.D. and Tsujii, J. (2010). Event extraction with complex event 
45
classification using rich features. J Bioinform Comput Biol, 8(1): 131-46. Miwa, M., Thompson, P. and Ananiadou, S. (2012). Boosting automatic event extraction from the literature using domain adaptation and coreference resolution. Bioinformatics.  Miwa, M., Thompson, P., McNaught, J, Kell, D.B and Ananiadou, S. (In Press). Extracting semantically enriched events from biomedical literature. BMC Bioinformatics.  Miyao, Y., Ohta, T., Masuda, K., Tsuruoka, Y., Yoshida, K., Ninomiya, T. and Tsujii, J. (2006). Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases. In Proceedings of ACL, pp.  1017-1024. Mizuta, Y., Korhonen, A., Mullen, T. and Collier, N. (2005). Zone Analysis in Biology Articles as a Basis for Information Extraction. International Journal of Medical Informatics,75(6): 468-487. Morante R., and Sporleder C, (2012). Modality and negation: An introduction to the special issue. Computational Linguistics, 38(2): 1?38. Nawaz, R., Thompson, P. and Ananiadou, S. (In Press). Identification of Manner in Bio-Events. Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC 2012). Nawaz, R., Thompson, P., McNaught, J. and Ananiadou, S. (2010). Meta-Knowledge Annotation of Bio-Events. In Proceedings of LREC 2010, pp.  2498-2507. Pyysalo, S., Ginter, F., Heimonen, J., Bjorne, J., Boberg, J., Jarvinen, J. and Salakoski, T. (2007). BioInfer: a corpus for information extraction in the biomedical domain. BMC Bioinformatics, 8: 50. Pyysalo, S., Ohta, T., Rak, R., Sullivan, D., Mao, C., Wang, C., Sobral, B., Tsujii, J. and Ananiadou, S. (In Press). Overview of the ID, EPI and REL tasks of BioNLP Shared Task 2011. BMC Bioinformatics. Quirk, C., Choudhury, P., Gamon, M. and Vanderwende, L. (2011). MSR-NLP Entry in BioNLP Shared Task 2011. In Proceedings of BioNLP Shared Task 2011 Workshop, pp.  155-163. Ruch, P., Boyer, C., Chichester, C., Tbahriti, I., Geissbuhler, A., Fabry, P., Gobeill, J., Pillet, V., Rebholz-Schuhmann, D., Lovis, C. and Veuthey, A.L. (2007). Using argumentation to extract key sentences from biomedical abstracts. Int J Med Inform, 76(2-3): 195-200. S?ndor, ?. (2007). Modeling metadiscourse conveying the author?s rhetorical strategy in biomedical 
research abstracts. Revue Fran?aise de Linguistique Appliqu?e, 200(2): 97-109. Shatkay, H., Pan, F., Rzhetsky, A. and Wilbur, W.J. (2008). Multi-dimensional classification of biomedical text: toward automated, practical provision of high-utility text to diverse users. Bioinformatics, 24(18): 2086-2093. Soldatova, L.N. and King, R.D. (2006). An ontology of scientific experiments. Journal of the Royal Society Interface, 3(11): 795-803. Soldatova, L.N. and Liakata, M. (2007). An ontology methodology and cisp-the proposed core information about scientific papers., Aberystwyth University. Technical Report JISC Project Report. Teufel, S. (2010). The Structure of Scientific Articles: Applications to Citation Indexing and Summarization. Stanford, CA, CSLI Publications. Teufel, S., Carletta, J. and Moens, M. (1999). An annotation scheme for discourse-level argumentation in research articles. In Proceedings of EACL, pp.  110-117. Teufel, S. and Moens, M. (2002). Summarizing scientific articles: experiments with relevance and rhetorical status. Computational Linguistics, 28(4): 409-445. Teufel, S., Siddharthan, A. and Batchelor, C. (2009). Towards discipline-independent argumentative zoning: Evidence from chemistry and computational linguistics. In Proceedings of EMNLP 2009, pp.  1493-1502. Thompson, P., Iqbal, S.A., McNaught, J. and Ananiadou, S. (2009). Construction of an annotated corpus to support biomedical information extraction. BMC Bioinformatics, 10: 349. Thompson, P., Nawaz, R., McNaught, J. and Ananiadou, S. (2011). Enriching a biomedical event corpus with meta-knowledge annotation. BMC Bioinformatics, 12: 393. Vincze, V., Szarvas, G., Farkas, R., Mora, G. and Csirik, J. (2008). The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC Bioinformatics, 9(Suppl 11): S9. Zweigenbaum, P., Demner-Fushman, D., Yu, H. and Cohen, K.B. (2007). Frontiers of biomedical text mining: current progress. Briefings in Bioinformatics, 8(5): 358-375.  
46
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 47?55,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Epistemic Modality and Knowledge Attribution in Scientific Discourse:  A Taxonomy of Types and Overview of Features 
Anita de Waard Henk Pander Maat Elsevier Labs  Utrecht Institute of Linguistics  Jericho, VT, USA Utrecht, The Netherlands a.dewaard@elsevier.com h.l.w.pandermaat@uu.nl   Abstract We propose a model for knowledge attribution and epistemic evaluation in scientific discourse, consisting of three dimensions with different values: source (author, other, unknown); value (unknown, possible, probable, presumed true) and basis (reasoning, data, other). Based on a literature review, we investigate four linguistic features that mark different types epistemic evaluation (modal auxiliary verbs, adverbs/adjectives, reporting verbs and references). A corpus study on two biology papers indicates the usefulness of this model, and suggest some typical trends. In particular, we find that matrix clauses with a reporting verb of the form ?These results suggest?, are the predominant feature indicating knowledge attribution in scientific text.  
1 Introduction Our main research goal is to linguistically ?specify the precise time and place in the process of fact construction when a statement became transformed into a fact?, as Latour and Woolgar (1979) put it. Specifically, we are interested in creating a linguistically motivated framework of biological sensemaking to help extract newly claimed knowledge from large text corpora.  Biological understanding consists of a conceptual model of the system at study, which is collaboratively created by the scientists working on that system. In contributing a new building block to the model, authors will need to argue, first: that their experiments are appropriate, and performed well; second, that they can draw certain conclusions from these experiments; and third, 
that, and how, these conclusions fit within the existing knowledge model for their field. Their observations and inferences might confirm or contradict other thoughts about the model, expressed in other papers. This need to indicate certainty and agreement/disagreement means that biological papers contain many explicit truth evaluations of their own and other authors? propositions (epistemic modality), and where needed, the explicit attribution of the creator of the propositions (knowledge attribution1). Therefore, to understand how biological knowledge is formulated in language, it is essential to understand the linguistic mechanisms of modality and attribution.  In this paper, we present an overview of work in linguistics, genre studies, bioinformatics and computational linguistics, related to epistemic evaluation. From this, we distill a three-tiered taxonomy and a set of linguistic cues or markers that distinguish various forms of epistemic evaluation. We try out this taxonomy and marker set in a small manual corpus exploration of two biology papers, and discuss some correlations between different types and market. We conclude with a proposal for the application of this work.  2 Epistemic Evaluation Taxonomy 
2.1 Overview of current work Strictly speaking, every factual proposition or piece of Propositional Content (Hengeveld and Mackenzie, 2008) contains an (implicit) epistemic evaluation: if a statement is given without further comment on its truth value, we read ? irony aside ? that the author agrees with the proposition it contains. ?Water is wet.? ? or ?LPS-induced IL-6                                                            1 To avoid the use of the cumbersome contraction ?epistemic modality evaluation and knowledge attribution? we will henceforth use the term ?epistemic evaluation? to cover both evaluation and attribution. 
47
gene transcription in murine monocytes is controlled by NF-B?are statements that do not contain any epistemic modifiers, and are therefore read to be unconditionally accepted by the author. In other cases, however, this truth value is modified: ?These results suggest that water is wet.? or attributed: ?Author X et al (2010) report that water is wet.?  Here, we investigate modifiers of propositional content that define either epistemic modality, i.e. the degree of authorial commitment to a proposition, e.g. ?5' untranslated exon 1 may have a regulatory function?, or knowledge attribution: the source of the propositional knowledge, such as when a reference indicates the source of the claim: ?GATA-1 transactivates the EOS47 promoter through a site in the 5'UTR [34].? There is a body of work pertaining to knowledge attribution and epistemic evaluation in scientific text, within at least four different fields: linguistics, genre studies, bioinformatics, and sentiment detection. A detailed overview of the hedging types and markers found in this literature overview is posted in Dataverse (de Waard, 2012) but we will provide a summary here.  Within linguistics, truth evaluations and source attributions are an important subject within most modern theories of language; here, only a small overview of some pertinent theories can be given. Hengeveld and Mackenzie (2008) characterize truth evaluations as ?modifiers of Propositional Content?, concerning ?the kind and degree of commitment of a rational being to Propositional Content, or a specification of the (non-verbal) source of the Propositional Content?. These two categories ? knowledge evaluation and knowledge attribution- are also indicated by the concepts ?epistemic modality? and ?evidentiality?, respectively. De Haan (1999) strongly argues that they are separate phenomena ? and we agree ? but for our purposes, establishing modes of truth evaluation and attribution in scientific text, both are relevant. Verstraete (2001) distinguishes between objective and subjective modality: in an objectively modal clause, the truth value of the state of knowledge is brought into question (?This subject is unknown?), but the certainty the author has pertaining to the clause is not; in a subjective modal clause, the author expresses uncertainty regarding the extent of his or her knowledge (?It might be (that this is the case)?).  
In genre studies, a body of work revolves around the concept of hedging: ?the expression of tentativeness and possibility in language? (Lakoff, 1972; Hyland, 1995). The focus here is on the rhetorical/sociological motivation for, and surface features of, these ?politeness markers?. Myers (1992) identifies stereotypical sentence patterns for hedging from a corpus study of fifty related articles in molecular genetics. Salager-Meyer (1994) defines hedging as presenting ?the true state of the writers? understanding, namely, the strongest claim a careful researcher can make.? She identifies three reasons for hedging: (1) that of purposive fuzziness and vagueness (threat- minimizing strategy); (2) that which reflects the authors? modesty for their achievements and avoidance of personal involvement; and (3) that related to the impossibility or unwillingness of reaching absolute accuracy and of quantifying all the phenomena under observation. Very influentially, Hyland (1995, 2005) proposes an explanatory framework for scientific hedging which combines sociological, linguistic, and discourse analytic perspectives and proposes a three-part taxonomy, distinguishing writer-oriented, accuracy-oriented and reader-oriented hedges. Countering Hyland, Crompton (1997) reviews and evaluates some of the different ways in which the term ?hedge? has been defined in the literature thus far. His new definition is that ?a hedge is an item of language, which a speaker uses to explicitly qualify his/her lack of commitment to the truth of a proposition he/she utters.? Mart?n-Mart?n (2008) analyses three different hedging strategies and multiple surface features for hedging in a corpus of full-text papers in English and Spanish, and presents a detailed taxonomy of hedging types and cues, based on literature and corpus studies.  Within bioinformatics and bio-computational linguistics, a body of work has been done on identifying ?speculative language? (Light, 2004). The main purpose here is to enable the automated identification of truth and speculation, in order to enable the construction of databases of known, and candidate, biological facts. The differences with earlier discussions are twofold: first, there is less (or no) effort to study communicative functions: for instance, there is no interest in identifying the authors? rhetorical intent, or the sociological or political motivations for using a particular type of hedge.  Second, bioinformatics focuses more on 
48
identifying different types of speculation: is the opinion presented positive or negative, strong or weak, etc. Light et al (2004) annotate a corpus of Medline sentences as highly speculative, low speculative, or definite, and then train a classifier to automatically recognize speculative sentences. (As an interesting result, they find that almost all speculations appear in the final or penultimate sentence of the abstract).  Wilbur et al (2006) are motivated by the need to identify and characterize locations in published papers where reliable scientific facts can be found, and present a set of guidelines and the results of an annotation task to annotate a full-text corpus with a five-dimensional set of quantities focus, polarity, certainty, evidence, and directionality. Of these, certainty and evidence relate to knowledge attribution and epistemic evaluation. Medlock and Briscoe (2007) develop a set of guidelines for identifying speculative sentences and an annotated corpus, to test their automated speculation classification tool. Kilicoglu and Bergler (2008) explore a linguistically motivated approach to the problem of recognizing speculative language in biomedical research articles. Building on Hyland?s work, they identify a set of syntactic patterns, which they use for detecting speculative sentences out of a corpus. Thompson et al (2008) propose a multi-dimensional classification of a preliminary set of words and phrases that express modality within biomedical texts, and present the results of an annotation experiment where sentences are annotated with level of speculation, type/source of the evidence and the level of certainty towards the statement writer or other. Vincze et al (2008) describe the BioScope corpus, a collection of Medline abstracts and four full-text papers annotated with instances of negation and speculation. In the subfield of computational linguistics pertaining to sentiment detection, the goal has been to create overviews of large set of documents summarizing collective opinions and emotion about some topic. Here a more ?mathematical? definition of modality is evolving, which considers the proposition being evaluated as being ?operated on? by the evaluator. A distinction is made between the holder of the opinion, and the strength, polarity and other attributes of the opinion. Similar to work in (bio)computational linguistics, this work has focused is on different types of opinions , 
and the clues that allow automated detection. Most work in this field has focused on other domains, such as news and product reviews, see e.g. Wilson and Wiebe (2003), Kim and Hovy (2004), and Tang et al, (2009).  2.2 Our proposal Following the formalism used in opinion/sentiment analysis (e.g., Wilson and Wiebe, 2003; Hovy, 2011) and Functional Discourse Grammar (Hengeveld and Mackenzie, 2008) we differentiate between, firstly, Propositions (similar to FDG?s Propositional Content), which can consist of either experimental (?all thymocytes stained positive for GFP?) or conceptual (?CCR3 is expressed strongly on eosinophils?) statements about the (conceived or acted upon/perceived) world, and secondly, modifiers, that modify on these Propositions and modify their truth value or the knowledge attribution. Building on the literature as summarized above, we define a taxonomy of epistemic evaluation along three facets:   1. Epistemic valuations possess a value or level of certainty. Both Hengeveld and Mackenzie (2008) and Wilbur et al (2006) propose a tripartite division:  ? ?Doxastic? (firm belief in truth, Wilbur?s category 3) ? ?Dubitative? (some doubt about the truth exists; Wilbur?s category 2)  ? ?Hypothetical? (where the truth value is only proposed; Wilbur?s category 1) ? Wilbur also adds the useful category ?Lack of knowledge? (level 0).  2. There can different bases of the evaluation: ? Reasoning: based mostly or solely on argumentation, and not directly on data (e.g., ?it is thought that?, ?we expected?)  ? Data: based explicitly on data (e.g., ?these data suggest that?, ?CCR3 has been shown to be?) ? Implicit or absent: if it is unclear what the evaluation or attribution is based on (e.g., ?GATA-1 transactivates the EOS47 promoter, through a site in the 5'UTR?)  3. The source of the knowledge is identified: ? Explicit source of knowledge: the knowledge evaluation can be explicitly 
49
owned by the author (?We therefore conclude that??) or by a named referent (?Vijh et al [28] demonstrated that??) ? Implicit source of knowledge: if there is no explicit source named, knowledge can implicitly still be attributed to the author (?these results suggest??) or an external source (?It is generally believed that...?) ? No source of knowledge: the source of knowledge can be absent entirely, e.g. in factual statements, such as ?transcription factors are the final common pathway driving differentiation?.   Table 1 summarizes our proposed classification.  3 Epistemic evaluation markers 
To use our taxonomy to find instances and classes of epistemic evaluation in text, we need to know with what lexicogrammatical cues they are typically marked. Table A1 in the Appendix shows the details, but in summary, a literature review shows widespread agreement on the following cue types: ? Modal auxiliary verbs (e.g. can, could, might)  ? Qualifying adverbs and adjectives (e.g. interestingly, possibly, likely, potential, somewhat, slightly, powerful, unknown, undefined) ? References, either external (e.g. ?[Voorhoeve et al, 2006]?) or internal (e.g. ?See fig. 2a?).  ? Reporting verbs (e.g. suggest, imply, indicate, show, seem - see. e.g. Thomas and Hawes (1994) and Hyland (2005) for examples and definitions)   We decided not to add two further categories of epistemic evaluation cues that are often mentioned: Personal pronouns. (?we?, ?our results?, or similar). Closer analysis of the papers that mention this shows that in all cases where personal pronouns are mentioned as a hedging device, epistemic verbs are present, in phrases such as: ?we show?, ?our results suggest?, etc. Therefore, simply mentioning personal pronouns does not add a useful feature; it does lead to a great deal of false positives, since (first-)personal pronouns are often used in describing methods (?next, we injected?, etc.) 
Table 1: Proposed classification for epistemic modality and knowledge attribution  In a similar vein, passives are sometimes suggested as an indication of epistemic evaluation, but since they are e.g. often used in Methods sections (?the rats were injected??) they do not indicate markers of epistemic modality or attribution. 4 Small Test of Correlation between Epistemic Types and Cues Using these four features, we want to explore whether all cases where epistemic evaluation occurs are covered by these cues; conversely, do the unmarked cases not have any cues? In other words, are the cues any good at identifying epistemic evaluation, and do certain clues identify certain types?  To investigate these issues, we conducted a small corpus study on two full-text papers in biology (Voorhoeve et. al, 2006; Zimmermann et al, 2005). First, we manually parsed them into clauses via the criteria outlined in (de Waard and 
Concept  Values 0 - Lack of knowledge 1 ? Hypothetical: low certainty  2 ? Dubitative: higher likelihood but short of complete certainty  
Value 
3 ? Doxastic: complete certainty, reflecting an accepted, known and/or proven fact. R ? Reasoning (?Therefore, one can argue??) D ? Data (?These results suggest??) 
Basis 
0 ? Unidentified (?Studies report that??) A - Author: Explicit mention of author/speaker or current paper as source (?We hypothesize that??; ?Figure 2a shows that??) N - Named external source, either explicitly or as a reference (??several reports have documented this expression [11-16,42].?) IA - Implicit attribution to the author (?Electrophoretic mobility shift analysis revealed that??) NN ? Nameless external source (?no eosinophil-specific transcription factors have been reported??) 
Source 
0 ? No source of knowledge (?transcription factors are the final common pathway driving differentiation?) 
50
Pander Maat, 2009), leading to a total of 812 clauses. For each clause, we identified the epistemic/knowledge attribution value/source/basis according to the taxonomy in Table 1. Next, we identified the incidence of the four cue types under investigation: modal auxiliary verbs, qualifying adverbs/adjectives, reporting verbs (clauses containing a reporting verb and subordinate clauses controlled by matrix clause with a reporting verb), and references. A sample of this markup, with the clause, attribution/evaluation type, and presence or absence of markers, is given in Table A1.  This sample is too small to draw any quantitative conclusions from. However, we do believe our results support the validity of our model, in two ways: first, because we easily can identify a modality type (value/source/basis) for each of the 812 clauses, and second, because all statements of value < 3 are indicated by one of the four cue types which we have identified. Next to these general findings, a few correlations between cue type and epistemic evaluation type become apparent (for details, see Table A2):  ? Modal auxiliary verbs (?might, can, could?) mark potentiality; in our sample, they only indicate clauses of ?possible? value (=1). ? Lack of cues indicates certainty.  47 out of 144 segments with value = 3 have no epistemic cues and no segments of value < 3 have no cues. ? Validating adverbs and adjectives rarely occur; when they do, they usually refer to ?Certain? segments (value = 3). These indicate focus and aim to draw attention to a finding or statement, and are: important(ly) (5x), interestingly, striking (example), presumably, and apparently.  ? References mostly occur in ?Certain? segments. This can be because references usually occur when results are cited (3/D/N) or when reference to a figure is made (3/D/IA).  ? Within our corpus, 44 discourse segments could not be classified as containing any type of knowledge attribution or evaluation. These were mostly goal statements (?To identify this process??) or methods reports (?We injected all animals??). 16 of these (36%) did have a reporting verb (the reporting verbs used here were analyze, address, assess, define, 
determine, identify, investigate, localize, and test). 12 of these cases were indeed goal clauses containing a to-infinitive verb form. These results suggest that a combination of verb tense/aspect as well as semantic verb class should be taken into account when analyzing cues for epistemic modality.  The one epistemic type that remains unidentified is ?lack of knowledge? (indicated by a knowledge value of 0); these are marked by different verb types, not just reporting verbs. These clauses are usually marked by specific negational forms of adverbs, verb forms, or nouns (?has not been established?, ?is unknown?, ?yet to be determined? etc. ? see Table 2). Therefore, our markers do not adequately cover the ?lack of knowledge? case and finding these constructions by string matching is probably the best way to automate the identification of open research questions in text.    Overall, however, the most prevalent cue we observe is that of a reporting verb, either directly within a clause or governing it, in a matrix clause construction. Half of all statements with Value = 3, 90% of the statements with Value = 2 and 33% of the statements with Value = 1 either contain or are governed by (i.e. are a subordinate clause to a matrix clause containing) a reporting verb. Since this is such a strongly prevalent marker, we wanted to explore if certain reporting verbs perhaps specifically contribute to a particular type of modality.  In Table 2, we show the reporting verbs vs. the knowledge value found in the 812 clauses that we analyzed. Specifically, particular knowledge values can be associated with certain verbs:  ? hypothetical statements are reported with ?hypothesize? (5 x) and cognitive verbs such as ?think? and ?suspect?, though they are also often indicated by a modal auxiliary, as discussed above;  ? probable statements are marked by ?indicate? (12x) and ?suggest? (18 x); ? statements presumed to be true are indicated by ?find? and especially ?demonstrate? (15 x).  
51
 Value = 0 (Lack of Knowledge) 
establish, (remain to be) elucidated, be (clear/useful), (remain to be) examined/determined, describe, make difficult to infer, report Value = 1 (Hypothetical) be important, consider, expect, hypothesize (5x), give insight, raise possibility that, suspect, think Value = 2 (Dubitative) appear, believe, implicate (2x), imply, indicate (12x), play a role, represent, suggest (18x), validate (2x) Value = 3 (Doxastic) be able/apparent/important /positive/visible, compare (2x), confirm (2x), define,  demonstrate (15x), detect (5x), discover, display (3x), eliminate, find (3x), identify (4x), know, need, note (2x), observe (2x), obtain (success/results- 3x), prove to be, refer, report(2x),  reveal (3x), see(2x) show (24x), study, view Table 2: Reporting verbs vs. knowledge value for 2 papers Since the segments containing these reporting verbs are so pivotal to knowledge attribution, they bear closer scrutiny. Generally these are sentence-initial clauses that adhere to the following word order (where Noun Phrases and Verb Phrases are always present, and the others are optional): Adverb/Connective + Determiner + Adverb/Adjective + NP + Modal  + Adjective + VP + Preposition All values found in the 42 clauses of this type in one of the papers we examined (Zimmermann et al (2005)) are provided in Table 3.  Adverb/ Connective thus, therefore, together, recently, in summary  Determiner/ Pronoun  it, this, these, we/our Adverb/ Adjective previous, future, better Noun phrase data, report, study; method or reference Modal form of ?to be?, will, remain Adjective often, recently, generally Verb show, obtain, consider, view, reveal, suggest, hypothesize, indicate, believe Preposition  that, to Table 3: Values of Parts-of-Speech for Regulatory segments in Zimmermann (2005) 
5 Conclusion and implementations In summary, we have presented a taxonomy of knowledge assessment and attribution and a set of linguistic cues based on a literature overview of from various fields. A small corpus study indicated that the system is simple to use, yet complex enough to cover the many different ways in which biologists attribute knowledge statements. We find that the majority of cases of epistemic evaluation in biological text is instantiated by regulatory segments governed by a reporting verb, prototypically of the form: ?These results suggest?.  To see if this correlation to epistemic evaluation holds at larger volumes, we plan to try out the above structure in an NLP environment. To begin this, we are examining the case where Value = 2/3 and Source = (I)A: in other words, the author posits a claim. These clauses constitute a specific subset of Propositional Content, which we are calling ?Claimed Knowledge Updates? (S?ndor, ?. and de Waard, A., 2012). We are exploring whether an automated syntactic parsing system, combined with a specific subset of reporting verbs will allow the identification of such authorial claims of new knowledge. We plan to use this knowledge to explore what linguistic changes occur when these Claimed Knowledge Updates are cited, and study how knowledge attribution and epistemic modality erode, in the evolution from a claim to a fact.  Acknowledgments We wish to thank Eduard Hovy for providing the insight that modality can be thought of like sentiment, and our anonymous reviewers for their constructive comments. Anita de Waard?s research is supported by Elsevier Labs and a grant from the Dutch funding organization NWO, under their Casimir Programme.  References  Crompton, P. (1997) Hedging in Academic Writing: Some Theoretical Problems, Eng Spec Purposes, Vol. 16, No. 4, pp. 271-287,1997. De Haan, F. (1999), Evidentiality and Epistemic Modality: Setting Boundaries. Southwest Journal of Linguistics 18.83-101. De Waard, A., Pander Maat, H. (2009). Categorizing Epistemic Segment Types in Biology Research 
52
Articles. Wkshp on Linguistic and Psycholinguistic Approaches to Text Structuring (LPTS 2009), September 21-23, 2009.  De Waard, 2012. Anita de Waard, 2012-05-23, "Overview of epistemic evaluation and cues from literature", V1, http://hdl.handle.net/1902.1/18253  Hengeveld, K. & Mackenzie, J. L. (2008), Functional Discourse Grammar: A Typologically-Based Theory of Language Structure. Oxford Univ. Press, 2008.  Hovy, E.H. (2011). Private correspondence. Hyland, K. (1995). The Author in the Text: Hedging Scientific Writing. Hong Kong Papers In Linguistics And Language Teaching, 18 (1995). Hyland, K. (2005). Stance and engagement: a model of interaction in academic discourse. Discourse Studies, Vol 7(2): 173?192. Kilicoglu H., Bergler S. (2008). Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC Bioinformatics. 2008 Nov 19;9 Suppl 11:S10. Kim, S-M. Hovy, E.H. (2004). Determining the Sentiment of Opinions. Proceedings of the COLING conference, Geneva, 2004.  Lakoff G. (1972). Hedges: a study in meaning criteria and the logic of fuzzy concepts. Chicago Linguistics Society Papers 1972, 8:183-228. Latour, B., Woolgar, S. (1979). Laboratory Life: The Social Construction of Scientific Facts.  Beverly Hills: Sage Publications. ISBN 0-80-390993-4. Light M., Qiu X.Y., Srinivasan P. (2004). The language of bioscience: facts, speculations, and statements in between. BioLINK 2004: Linking Biological Literature, Ontologies and Databases 2004:17-24. Mart?n-Mart?n, P. (2008). The Mitigation Of Scientific Claims In Research Papers: A Comparative Study. Int Jnl of English 2008 8(2): 133-152. Medlock B., Briscoe T. (2007). Weakly supervised learning for hedge classification in scientific literature. ACL 2007:992-999. 
Myers, G. (1992). ?In this paper we report?: Speech acts scientific facts, Jnl of Pragmatlcs 17 (1992) 295-313 Salager-Meyer, F. (1994), Hedges and Textual Communicative Function in Medical English Written Discourse, English for Specific Purposes, Vol. 13, No. 2, PP. 149-170, 1994. S?ndor, ?. and de Waard, A (2012). Identifying Claimed Knowledge Updates in Biomedical Research Articles, Workshop on Detecting Structure in Scholarly Discourse at ACL 2012 (this workshop).  Tang, H., Tan, S., Cheng, X. (2009), A survey on sentiment detection of reviews, Expert Systems with Applications 36 (2009) 10760?10773. Thomas, S. and Hawes, Th. P. (1994). Reporting Verbs in Medical Journal Articles, English for Specific Purposes, 1994 13(2), pp. 129-148. Thompson P., Venturi G., McNaught J, Montemagni S, Ananiadou S. (2008). Categorising modality in biomedical texts.. LREC 2008: Building and Evaluating Resources for Biomedical Text Mining 2008. Verstraete, J.-C. (2001). Jnl of Pragmatics 33 (2001). Vincze, V., Szarvas, Farkas, M?ra and Csirik, (2008). The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes, BMC Bioinformatics 2008, 9 (Suppl 11):S9.  Voorhoeve P.M., le Sage C., et. al (2006). A genetic screen implicates miRNA-372 and miRNA-373 as oncogenes in testicular germ cell tumors. Cell. 2006 Mar 24;124(6):1169-81. Wilson, T. and Wiebe, J., (2003), Annotating Opinions in the World Press, 2003, SigDAIL Wilbur W.J., Rzhetsky A, Shatkay H (2006). New directions in biomedical text annotations: definitions, guidelines and corpus construction. BMC Bioinformatics 2006, 7:356. Zimmermann,N., Colyer, JL, Koch, LE and Rothenberg, ME. (2005). Analysis of the CCR3 promoter reveals a regulatory region in exon 1 that binds GATA-1, BMC Immunology 2005, 6:7-7.  
53
Appendix:  Table A1: Example of markup with epistemic evaluation/knowledge attribution types and markers from Zimmermann (2005) ? for table headers see caption. 
Clause  Value Basis Source Modal Adv/ Adj  Refs RV?  Ruled by RV? DNase I hypersensitivity indicated that 2 D IA      1   a region consistent with exon 1 is active in CCR3 transcription. 2 D IA        1 Together with our previous data showing that 3 D A      1   untranslated exon 1 has an important role in CCR3 transcription [27], 3 D N  1 1   1 we hypothesized that 1 R A      1   nuclear proteins bind to exon 1, 2 D IA        1 and in turn regulate the transcription of CCR3. 2 D IA        1 In order to test this hypothesis,            1   a double-stranded oligonucleotide probe that corresponds to bp +10 to +60 of the CCR3 gene was prepared, 3 0 NN 
 
        referred to as E1-FL (exon 1- full length, Figure 2A). 3 D A    1 1   This is the exact sequence 3 D N          that was deleted in the CCR3(-exon1).pGL3 plasmid 3 D N          that demonstrated decreased activity 3 D N      1   compared to the full length 1.6 kb construct [27]. 3 D N    1 1 1 Nuclear extracts from AML14.3D10 cells were incubated with the probe                and resolved on a polyacrylamide gel.                Two bands were visible (Figure 2B). 3 D IA    1 1   The upper band was eliminated 3 D IA      1   when 150x molar excess of the unlabelled probe was used (CC: E1-FL in Figure Figure2B),2B),       
 
  1     indicating that 2 D IA      1   this is the specific band. 2 D IA        1 The specific band was eliminated with E1-B and E1-C cold competitors 3 D IA      1   indicating that 2 D IA      1   the factor binds in the region between +25 and +60 (Figure 2B). 2 D IA    1   1 In summary, these data indicate 2 D IA      1   the presence of proteins in the nuclei of AML14.3D10 cells that bind to CCR3 exon 1 between bp 25 and 60. 2 D IA 
 
      1  
?Modal? = containing a modal auxiliary verb; ?Refs? = containing a reference; ?Adverb/Adj? = containing a qualifying adverb or adjective; ?RV? = Reporting verb; ?Ruled by RV? = in a subclause ruled by a matrix clause containing a reporting verb. 
54
Table A2: Correlation between modality type (rows) and modality cues (columns) for two full-text papers 
  
Value Basis Source Modal Aux  Reporting Verb Ruled by RV Adverbs/ Adjectives References None Total  3 0 0           8 8 3 0 IA   5 2 2     9 3 0 N   8 5 2 8 2 25 3 0 NN 1 2 2     12 17 3 D A   20 1   16 2 39 3 D IA   33 6 1 9 17 62 3 D N   7 7 1 8 6 29 3 D NN   3         3 3 R IA   2 1 1     4 3 R NN   1         1 Total value = 3 1 (0.5%) 81 (40%) 24 (12%) 7 (4%) 41 (20%) 47 (24%) 201(100%) 2 0 N     1   1   2 2 0 NN   1 1       2 2 D 0     1       1 2 D A   1         1 2 D IA   22 17  1   40 2 D NN   1      1 2 R 0     2 1 1   4 2 R IA   2     1   3 2 R N   1 1       2 2 R NN   1         1 Total Value = 2 0 29 (51%) 23 (40%) 1 (2%) 4(7%) 0 57(100%) 1 0 0     1       1 1 0 NN 1 1 1   1   4 1 D IA 5 5 3 1     14 1 R A 2 2 5       9 1 R IA 1 1        2 1 R NN   2 1       3 Total Value = 1 9(27%) 11(33%) 11(33%) 1(3%) 1(3%) 0 33(100%) 0 0 0    6 1       7 0 0 N   1     1   2 0 D 0     1       1 0 D N     1       1 0 D NN       1     1 0 R A   1         1 0 R IA   1         1 Total Value = 0 0 9 (64%) 3 (21%) 1(7%) 1(7%) 0 14(100%) Total No Modality 0 16 3 0 3 22 44 Overall Total 10 (2%) 146(23%) 64(10%) 10(2%) 50(8%) 69(11%) 640(100%) 
55
