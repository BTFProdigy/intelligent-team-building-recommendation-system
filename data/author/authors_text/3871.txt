Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 994?1002,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Polynomial-Time Parsing Algorithm for TT-MCTAG
Laura Kallmeyer
Collaborative Research Center 441
Universita?t Tu?bingen
Tu?bingen, Germany
lk@sfs.uni-tuebingen.de
Giorgio Satta
Department of Information Engineering
University of Padua
Padova, Italy
satta@dei.unipd.it
Abstract
This paper investigates the class of Tree-
Tuple MCTAG with Shared Nodes, TT-
MCTAG for short, an extension of Tree
Adjoining Grammars that has been pro-
posed for natural language processing, in
particular for dealing with discontinuities
and word order variation in languages such
as German. It has been shown that the uni-
versal recognition problem for this formal-
ism is NP-hard, but so far it was not known
whether the class of languages generated
by TT-MCTAG is included in PTIME. We
provide a positive answer to this ques-
tion, using a new characterization of TT-
MCTAG.
1 Introduction
For a large range of linguistic phenomena, exten-
sions of Tree Adjoining Grammars (Joshi et al,
1975), or TAG for short, have been proposed based
on the idea of separating the contribution of a lex-
ical item into several components. Instead of sin-
gle trees, these grammars contain (multi-)sets of
trees. Examples are tree-local and set-local mul-
ticomponent TAG (Joshi, 1985; Weir, 1988), MC-
TAG for short, non-local MCTAG with dominance
links (Becker et al, 1991), Vector-TAG with dom-
inance links (Rambow, 1994) and, more recently,
Tree-Tuple MCTAG with Shared Nodes (Lichte,
2007)), or TT-MCTAG for short.
For some of the above formalisms the word
recognition problem is NP-hard. This has been
shown for non-local MCTAG (Rambow and Satta,
1992), even in the lexicalized case (Champollion,
2007). Some others generate only polynomial lan-
guages but their generative capacity is too limited
to deal with all natural language phenomena. This
has been argued for tree-local and even set-local
MCTAG on the basis of scrambling data from lan-
guages such as German (Becker et al, 1992; Ram-
bow, 1994).
In this paper, we focus on TT-MCTAG (Lichte,
2007). So far, it has been shown that the univer-
sal recognition problem for TT-MCTAG is NP-
hard (S?gaard et al, 2007). A restriction on TT-
MCTAG has been proposed in (Kallmeyer and
Parmentier, 2008): with such a restriction, the uni-
versal recognition problem is still NP-hard, but
the class of generated languages is included in
PTIME, i.e., all these languages can be recognized
in deterministic polynomial time. In this paper, we
address the question of whether for general TT-
MCTAG, i.e., TT-MCTAG without the constraint
from (Kallmeyer and Parmentier, 2008), the class
of generated languages is included in PTIME. We
provide a positive answer to this question.
The TT-MCTAG definition from (Lichte, 2007;
Kallmeyer and Parmentier, 2008) imposes a con-
dition on the way different tree components from a
tree tuple in the grammar combine with each other.
This condition is formulated in terms of mapping
between argument and head trees, i.e., in order to
test such a condition one has to guess some group-
ing of the tree components used in a derivation into
instances of tree tuples from the grammar. This re-
sults in a combinatorial explosion of parsing anal-
yses. In order to obtain a polynomial parsing al-
gorithm, we need to avoid this effect.
On this line, we propose an alternative charac-
terization of TT-MCTAG that only requires (i) a
counting of tree components and (ii) the check of
some local conditions on these counts. This allows
for parsing in polynomial deterministic time.
TT-MCTAG uses so-called ?parallel unordered?
rewriting. The first polynomial time parsing
results on this class were presented in (Ram-
bow and Satta, 1994; Satta, 1995) for some
string-based systems, exploiting counting tech-
niques closely related to those we use in this pa-
per. In contrast to string-based rewriting, the tree
994
rewriting formalisms we consider here are struc-
turally more complex and require specializations
of the above techniques. Polynomial parsing re-
sults for tree rewriting systems based on paral-
lel unordered rewriting have also been reported
in (Rambow, 1994; Rambow et al, 1995). How-
ever, in the approach proposed by these authors,
tree-based grammars are first translated into equiv-
alent string-based systems, and the result is again
provided on the string domain.
2 Tree Adjoining Grammars
Tree Adjoining Grammars (Joshi et al, 1975) are
a formalism based on tree rewriting. We briefly
summarize here the relevant definitions and refer
the reader to (Joshi and Schabes, 1997) for a more
complete introduction.
Definition 1 A Tree Adjoining Grammar
(TAG) is a tuple G = (VN , VT , S, I,A) where
VN and VT are disjoint alphabets of non-terminal
and terminal symbols, respectively, S ? VN is the
start symbol, and I and A are finite sets of initial
and auxiliary trees, respectively. 2
Trees in I ?A are called elementary trees. The
internal nodes in the elementary trees are labeled
with non-terminal symbols, the leaves with non-
terminal or terminal symbols. As a special prop-
erty, each auxiliary tree ? has exactly one of its
leaf nodes marked as the foot node, having the
same label as the root. Such a node is denoted by
Ft(?). Leaves with non-terminal labels that are
not foot nodes are called substitution nodes.
In a TAG, larger trees can be derived from the
elementary trees by subsequent applications of the
operations substitution and adjunction. The sub-
stitution operation replaces a substitution node ?
with an initial tree having root node with the same
label as ?. The adjunction operation replaces
an internal node ? in a previously derived tree ?
with an auxiliary tree ? having root node with the
same label as ?. The subtree of ? rooted at ? is
then placed below the foot node of ?. Only inter-
nal nodes can allow for adjunction, adjunction at
leaves is not possible. See figure 1 for an example
of a tree derivation.
Usually, a TAG comes with restrictions on the
two operations, specified at each node ? by sets
Sbst(?) and Adj (?) listing all elementary trees
that can be substituted or adjoined, respectively.
Furthermore, adjunction at ? might be obligatory.
NP
John
S
NP VP
V
laughs
VP
ADV VP?
always
derived tree:
S
NP VP
John ADV VP
always V
laughs
derivation tree:
laugh
1 2
john always
Figure 1: TAG derivation for John always laughs
TAG derivations are represented by derivation
trees that record the history of how the elemen-
tary trees are put together. A derivation tree is
an unordered tree whose nodes are labeled with
elements in I ? A and whose edges are labeled
with Gorn addresses of elementary trees.1 Each
edge in a derivation tree stands for an adjunction
or a substitution. E.g., the derivation tree in fig-
ure 1 indicates that the elementary tree for John is
substituted for the node at address 1 and always is
adjoined at node address 2.
In the following, we write a derivation tree D
as a directed graph ?V,E, r? where V is the set of
nodes, E ? V ? V is the set of arcs and r ? V is
the root. For every v ? V , Lab(v) gives the node
label and for every ?v1, v2? ? E, Lab(?v1, v2?)
gives the edge label.
A derived tree is the result of carrying out the
substitutions and the adjunctions in a derivation
tree, i.e., the derivation tree describes uniquely the
derived tree; see again figure 1.
3 TT-MCTAG
3.1 Introduction to TT-MCTAG
For a range of linguistic phenomena, multicompo-
nent TAG (Weir, 1988) have been proposed, also
called MCTAG for short. The underlying motiva-
tion is the desire to split the contribution of a single
lexical item (e.g., a verb and its arguments) into
several elementary trees. An MCTAG consists of
(multi-)sets of elementary trees, called tree sets.
If an elementary tree from some set is used in a
derivation, then all of the remaining trees in the
set must be used as well. Several variants of MC-
TAGs can be found the literature, differing on the
1In this convention, the root address is ? and the jth child
of a node with address p has address p ? j.
995
specific definition of the derivation process.
The particular MCTAG variant we are con-
cerned with is Tree-Tuple MCTAG with Shared
Nodes, TT-MCTAG (Lichte, 2007). TT-MCTAG
were introduced to deal with free word order phe-
nomena in languages such as German. An exam-
ple is (1) where the argument es of reparieren pre-
cedes the argument der Mann of versucht and is
not adjacent to the predicate it depends on.
(1) ... dass es der Mann zu reparieren versucht
... that it the man to repair tries
?... that the man tries to repair it?
A TT-MCTAG is slightly different from stan-
dard MCTAGs since each elementary tree set con-
tains one specially marked lexicalized tree called
the head, and all of the remaining trees in the set
function as arguments of the head. Furthermore, in
a TT-MCTAG derivation the argument trees must
either adjoin directly to their head tree, or they
must be linked in the derivation tree to an elemen-
tary tree that attaches to the head tree, by means
of a chain of adjunctions at root nodes. In other
words, in the corresponding TAG derivation tree,
the head tree must dominate the argument trees in
such a way that all positions on the path between
them, except the first one, must be labeled by ?.
This captures the notion of adjunction under node
sharing from (Kallmeyer, 2005).2
Definition 2 A TT-MCTAG is a tuple G = (VN ,
VT , S, I,A,T ) where GT = (VN , VT , S, I,A) is
an underlying TAG and T is a finite set of tree
tuples of the form ? = ??, {?1, . . . , ?r}? where
? ? (I ? A) has at least one node with a terminal
label, and ?1, . . . , ?n ? A. 2
For each ? = ??, {?1, . . . , ?r}? ? T , we call ?
the head tree and the ?j?s the argument trees.
We informally say that ? and the ?j?s belong to ?,
and write |?| = r + 1.
As a remark, an elementary tree ? from the un-
derlying TAG GT can be found in different tree tu-
ples in G, or there could even be multiple instances
of such a tree within the same tree tuple ?. In these
cases, we just treat these tree instances as distinct
trees that are isomorphic and have identical labels.
2The intuition is that, if a tree ?? adjoins to some ?, its
root in the resulting derived tree somehow belongs both to ?
and ?? or, in other words, is shared by them. A further tree ?
adjoining to this node can then be considered as adjoining to
?, not only to ?? as in standard TAG. Note that we assume that
foot nodes do not allow adjunctions, otherwise node sharing
would also apply to them.
For a given argument tree ? in ?, h(?) denotes the
head of ? in ?. For a given ? ? I?A, a(?) denotes
the set of argument trees of ?, if there are any, or
the empty set otherwise. Furthermore, for a given
TT-MCTAG G, H(G) is the set of head trees and
A(G) is the set of argument trees. Finally, a node
v in a derivation tree for G with Lab(v) = ? is
called a ?-node.
Definition 3 Let G = (VN , VT , S, I,A,T ) be
some TT-MCTAG. A derivation tree D =
?V,E, r? in the underlying TAG GT is licensed in
G if and only if the following conditions (MC) and
(SN-TTL) are both satisfied.
? (MC): For all ? from G and for all ?1, ?2
in ?, we have |{v | v ? V, Lab(v) = ?1}| =
|{v | v ? V, Lab(v) = ?2}|.
? (SN-TTL): For all ? ? A(G) and n ? 1,
let v1, . . . , vn ? V be pairwise different
h(?)-nodes, 1 ? i ? n. Then there are
pairwise different ?-nodes u1, . . . , un ? V ,
1 ? i ? n. Furthermore, for 1 ? i ?
n, either ?vi, ui? ? E, or else there are
ui,1, . . . , ui,k, k ? 2, with auxiliary tree la-
bels, such that ui = ui,k, ?vi, ui,1? ? E and,
for 1 ? j ? k ? 1, ?ui,j, ui,j+1? ? E with
Lab(?ui,j , ui,j+1?) = ?. 2
The separation between (MC) and (SN-TTL)
in definition 3 is motivated by the desire to
separate the multicomponent property that TT-
MCTAG shares with a range of related formalisms
(e.g., tree-local and set-local MCTAG, Vector-
TAG, etc.) from the notion of tree-locality with
shared nodes that is peculiar to TT-MCTAG.
Figure 2 shows a TT-MCTAG derivation for (1).
Here, the NPnom auxiliary tree adjoins directly to
versucht (its head) while the NPacc tree adjoins to
the root of a tree that adjoins to the root of a tree
that adjoins to reparieren.
TT-MCTAG can generate languages that, in
a strong sense, cannot be generated by Linear
Context-Free Rewriting Systems (Vijay-Shanker
et al, 1987; Weir, 1988), or LCFRS for
short. An example is the language of all strings
pi(n[1] . . . n[m])v[1] . . . v[m] with m ? 1, pi a per-
mutation, and n[i] = n is a nominal argument of
v[i] = v for 1 ? i ? m, i.e., these occurrences
come from the same tree set in the grammar. Such
a language has been proposed as an abstract de-
scription of the scrambling phenomenon as found
in German and other free word order languages,
996
*VP
VP? versucht
,
(
VP
NPnom VP?
) + *
NPnom
der Mann
, {}
+
*
VP
zu reparieren
,
(
VP
NPacc VP?
) + *
NPacc
es
, {}
+
derivation tree:
reparieren
?
versucht
?
NPnom
1 ?
Mann NPacc
1
es
Figure 2: TT-MCTAG derivation of (1)
* ? VP
v
,
( ?1 VPv=?
n VP?NA
)+
* ?2 VP
v VP?NAv=+
,
( ?3 VPv=?
n VP?NA
)+
Figure 3: TT-MCTAG
and cannot be generated by a LCFRS (Becker et
al., 1992; Rambow, 1994). Figure 3 reports a TT-
MCTAG for this language.
Concerning the other direction, at the time of
writing it is not known whether there are lan-
guages generated by LCFRS but not by TT-
MCTAG. It is well known that LCFRS is closed
under the finite-copy operator. This means that,
for any fixed k > 1, if L is generated by a LCFRS
then the language {w |w = uk, u ? L} can
also be generated by a LCFRS. We conjecture that
TT-MCTAG does not have such a closure prop-
erty. However, from a first inspection of the MC-
TAG analyses proposed for natural languages (see
Chen-Main and Joshi (2007) for an overview), it
seems that there are no important natural language
phenomena that can be described by LCFRS and
not by TT-MCTAG. Any construction involving
some kind of component stacking along the VP
projection such as subject-auxiliary inversion can
be modelled with TT-MCTAG. Unbounded extra-
position phenomena cannot be described with TT-
MCTAG but they constitute a problem for any lo-
cal formalism and so far the nature of these phe-
nomena is not sufficiently well-understood.
Note that, in contrast to non-local MCTAG, in
TT-MCTAG the trees coming from the same in-
stance of a tuple in the grammar are not required
to be added at the same time. TT-MCTAGs share
this property of ?non-simultaneity? with other vec-
tor grammars such as Unordered Vector Gram-
mars (Cremers and Mayer, 1973) and Vector-
TAG (Rambow, 1994), V-TAG for short, and it
is crucial for the polynomial parsing algorithm.
The non-simultaneity seems to be an advantage
when using synchronous grammars to model the
syntax-semantics interface (Nesson and Shieber,
2008). The closest formalism to TT-MCTAG is
V-TAG. However, there are fundamental differ-
ences between the two. Firstly, they make a dif-
ferent use of dominance links: In V-TAG domi-
nance links relate different nodes in the trees of
a tree set from the grammar. They present domi-
nance requirements that constrain the derived tree.
In TT-MCTAG, there are no dominance links be-
tween nodes in elementary trees. Instead, the node
of a head tree in the derivation tree must domi-
nate all its arguments. Furthermore, even though
TT-MCTAG arguments can adjoin with a delay
to their head, their possible adjunction site is re-
stricted with respect to their head. As a result,
one obtains a slight degree of locality that can
be exploited for natural language phenomena that
are unbounded only in a limited domain. This is
proposed in (Lichte and Kallmeyer, 2008) where
the fact that substitution nodes block argument ad-
junction to higher heads is used to model the lim-
ited domain of scrambling in German. V-TAG
does not have any such notion of locality. Instead,
it uses explicit constraints, so-called integrity con-
straints, to establish islands.
3.2 An alternative characterization of
TT-MCTAG
The definition of TT-MCTAG in subsection 3.1 is
taken from (Lichte, 2007; Kallmeyer and Parmen-
tier, 2008). The condition (SN-TTL) on the TAG
derivation tree is formulated in terms of heads and
arguments belonging together, i.e., coming from
the same tuple instance. For our parsing algo-
rithm, we want to avoid grouping the instances
of elementary trees in a derivation tree into tu-
ple instances. In other words, we want to check
whether a TAG derivation tree is a valid TT-
997
MCTAG derivation tree without deciding, for ev-
ery occurrence of some argument ?, which of the
h(?)-nodes represents its head. Therefore we pro-
pose to reformulate (SN-TTL).
For a node v in a derivation tree D, we write
Dv to represent the subtree of D rooted at v. For
? ? (I ? A), we define Dom(v, ?) as the set of
nodes of Dv that are labeled by ?. Furthermore,
for an argument tree ? ? A(G), we let pi(v, ?) =
|Dom(v, ?)| ? |Dom(v, h(?))|.
Lemma 1 Let G be a TT-MCTAG with underlying
TAG GT , and let D = ?V,E, r? be a derivation
tree in GT that satisfies (MC). D satisfies (SN-
TTL) if and only if, for every v ? V and every
? ? A(G), the following conditions both hold.
(i) pi(v, ?) ? 0.
(ii) If pi(v, ?) > 0, then one of the following con-
ditions must be satisfied:
(a) Lab(v) = ? and pi(v, ?) = 1;
(b) Lab(v) = ? and pi(v, ?) > 1, and there
is some ?v, v?? ? E with Lab(?v, v??) =
? and pi(v?, ?) + 1 = pi(v, ?);
(c) Lab(v) /? {?, h(?)} and there is some
?v, v?? ? E with Lab(?v, v??) = ? and
pi(v?, ?) = pi(v, ?);
(d) Lab(v) = h(?) and there is some
?v, v?? ? E with Lab(?v, v??) = ? and
pi(v, ?) ? pi(v?, ?) ? pi(v, ?) + 1.
Intuitively, condition (i) in lemma 1 captures the
fact that heads always dominate their arguments
in the derivation tree. Condition (ii)b states that,
if v is a ?-node and if v is not the only ?pend-
ing? ?-node in Dv, then all pending ?-nodes in
Dv, except v itself, must be below the root adjoin-
ing node. Here pending means that the node is
not matched to a head-node within Dv. Condition
(ii)c treats the case in which there are pending ?-
nodes in Dv for some node v whose label is neither
? nor h(?). Then the pending nodes must all be
below the root adjoining node. Finally, condition
(ii)d deals with the case of a h(?)-node v where,
besides the ?-node that serves as an argument of
v, there are other pending ?-nodes in Dv. These
other pending ?-nodes must all be in Dv? , where
v? is the (unique) root adjoining node, if it exists.
The argument of v might as well be below v?, and
then the number of pending ?-nodes in Dv? is the
number of pending nodes in Dv, incremented by
1, since the argument of v is not pending in Dv
but it is pending in Dv? . Otherwise, the argument
of v is a pending ?-node below some other daugh-
ter of v. Then the number of pending ?-nodes in
Dv? is the same as in Dv.
PROOF We first show that (SN-TTL) implies both
(i) and (ii).
Condition (i): Assume that there is a v ? V
and a ? ? A(G) with pi(v, ?) < 0. Then for
some n and for pairwise different v1, . . . , vn with
?v, vi? ? E?, Lab(vi) = h(?) (1 ? i ? n),
we cannot find pairwise different u1, . . . , un with
?vi, ui? ? E?, Lab(ui) = ?. This is in contradic-
tion with (SN-TTL). Consequently, condition (i)
must be satisfied.
Condition (ii): Assume ? and v as in the state-
ment of the lemma, with pi(v, ?) > 0. Let
v1, . . . , vn be all the h(?)-nodes in D. There
is a bijection f? from these nodes to n pairwise
distinct ?-nodes in D, such that every pair vi,
f?(vi) = ui satisfies the conditions in (SN-TTL).
Because of (MC), the nodes u1, . . . , un must be
all the ?-nodes in D. There must be at least one vi
(1 ? i ? n) with ?vi, v? ? E+, ?v, f?(vi)? ? E?.
Then we have one of the following cases.
(a) ui = v and vi is the only h(?)-node dominat-
ing v with a corresponding ?-node dominated by
v. In this case (ii)a holds.
(b) Lab(v) = ?, i.e., ?f?1? (v), v? ? E+ and there
are other nodes u ? Dom(v, ?), u 6= v with
?f?1? (u), v? ? E+. Then, with (SN-TTL), there
must be a v? with ?v, v?? ? E, Lab(?v, v??) = ?
and for all such nodes u, ?v?, u? ? E?. Conse-
quently, (ii)b holds.
(c) Lab(v) /? {?, h(?)}. Then, as in (b), there
must be a v? with ?v, v?? ? E, Lab(?v, v??) = ?
and for all u ? Dom(v, ?) with ?f?1? (u), v? ?
E+, ?v?, u? ? E?. Consequently, (ii)c holds.
(d) Lab(v) = h(?). If f?(v) is dominated by a v?
that is a daughter of v with Lab(?v, v??) = ?, then
for all u ? Dom(v, ?) with ?f?1? (u), v? ? E+
we have ?v?, u? ? E?. Consequently, pi(v?, ?) =
pi(v, ?) + 1. Alternatively, f?(v) is dominated by
some other daughter v? of v with Lab(?v, v??) 6=
?. In this case v? must still exist and, for all
u ? Dom(v, ?) with u 6= f?(v) and with
?f?1? (u), v? ? E+, we have ?v?, u? ? E?. Conse-
quently, pi(v?, ?) = pi(v, ?).
Now we show that (i) and (ii) imply (SN-TTL).
With (MC), the number of ?-nodes and h(?)-
nodes in V are the same, for every ? ? A(G). For
every ? ? A(G), we construct a bijection f? of the
998
same type as in the first part of the proof, and show
that (SN-TTL) is satisfied. To construct f?, for ev-
ery v ? V we define sets V?,v ? Dom(v, ?) of ?-
nodes v? that have a matching head f?(v?) domi-
nating v. The definition satisfies |V?,v| = pi(v, ?).
For every v with v1, . . . , vn being all its daughters:
a) If Lab(v) = ?, then (by (ii)) for every 1 ? j ?
n with Lab(?v, vj?) 6= ?, V?,vj = ?. If there is a
vi with Lab(?v, vi?) = ?, then V?,v = V?,vi ?{v},
else V?,v = {v}.
b) If Lab(v) /? {?, h(?)}, then (by (ii)) V?,vj = ?
for every 1 ? j ? n with Lab(?v, vj?) 6= ?. If
there is a vi with Lab(?v, vi?) = ?, then V?,v =
V?,vi , else V?,v = ?.
c) If Lab(v) = h(?), then there must be some i,
1 ? i ? n, such that V?,vi 6= ?. We need to
distinguish two cases. In the first case we have
Lab(?v, vi?) 6= ?, |V?,vi | = 1 and, for every
1 ? j ? n with j 6= i, either V?,vj = ? or
Lab(?v, vj?) = ?. In this case we define f?(v) =
v? for {v?} = V?,vi . In the second case we have
Lab(?v, vi?) = ? and, for every 1 ? j ? n with
j 6= i, V?,vj = ?. In this case we pick an arbitrary
v? ? V?,vi and let f?(v) = v?. In both cases we let
V?,v = (
?n
i=1 V?,vi) \ {f?(v)}.
With this mapping, (SN-TTL) is satisfied when
choosing for each h(?)-node vi the ?-node ui =
f?(vi) as its corresponding node. 
4 Parsing algorithm
In this section we present a recognition algorithm
for TT-MCTAG working in polynomial time in the
size of the input string. The algorithm can be eas-
ily converted into a parsing algorithm. The ba-
sic idea is to use a parsing algorithm for TAG,
and impose on-the-fly additional restrictions on
the underlying derivation trees that are being con-
structed, in order to fulfill the definition of valid
TT-MCTAG derivation. To simplify the presenta-
tion, we assume without loss of generality that all
elementary trees in our grammars are binary trees.
The input string has the form w = a1 ? ? ? an with
each ai ? VT and n ? 0 (n = 0 means w = ?).
4.1 TAG recognition
We start with the discussion of a baseline recogni-
tion algorithm for TAG, along the lines of (Vijay-
Shanker and Joshi, 1985). The algorithm is
specified by means of deduction rules, follow-
ing (Shieber et al, 1995), and can be implemented
using standard tabular techniques. Items have the
form [?, pt, i, f1, f2, j] where ? ? I ? A, p is the
address of a node in ?, subscript t ? {?,?} speci-
fies whether substitution or adjunction has already
taken place (?) or not (?) at p, and 0 ? i ? f1 ?
f2 ? j ? n are indices with i, j indicating the left
and right edges of the span recognized by p and
f1, f2 indicating the span of a gap in case a foot
node is dominated by p. We write f1 = f2 = ? if
no gap is involved. For combining indices, we use
the operator f ??f ?? = f where f = f ? if f ?? = ?,
f = f ?? if f ? = ?, and f is undefined otherwise.
The deduction rules are shown in figure 4.
The algorithm walks bottom-up on the deriva-
tion tree. Rules (1) and (2) process leaf nodes
in elementary trees and require precondition
Lab(?, p) = wi+1 and Lab(?, p) = ?, respec-
tively. Rule (3) processes the foot node of aux-
iliary tree ? ? A by guessing the portion of w
spanned by the gap. Note that we use p? in the
consequent item in order to block adjunction at
foot nodes, as usually required in TAG.
We move up along nodes in an elementary
tree by means of rules (4) and (5), depending on
whether the current node has no sibling or has a
single sibling, respectively.
Rule (6) substitutes initial tree ? at p in ?, un-
der the precondition ? ? Sbst(?, p). Similarly,
rule (7) adjoins auxiliary tree ? at p in ?, under the
precondition ? ? Adj (?, p). Both these rules use
p? in the consequent item in order to block mul-
tiple adjunction or substitution at p, as usually re-
quired in TAG. Rule (8) processes nodes at which
adjunction is not obligatory.
The algorithm recognizes w if and only if some
item [?, ??, 0,?,?, n] can be inferred with ? ? I
and Lab(?, ?) = S.
4.2 TT-MCTAG recognition
We now extend the recognition algorithm of fig-
ure 4 to TT-MCTAG. Let G be an input TT-
MCTAG. We assume that the tuples in T are num-
bered from 1 to |T |, and that the elementary trees
in each ?i are also numbered from 1 to |?i|, with
the first element being the head. We then write ?q,r
for the r-th elementary tree in the q-th tuple in T .
A t-counter is a ragged array T of integers with
primary index q ranging over {1, . . . , |T |} and
with secondary index r ranging over {1, . . . , |?i|}.
We write T (q,r) to denote the t-counter with
T [q, r] = 1 and zero everywhere else. We also use
the sum and the difference of t-counters, which are
999
[?, p?, i,?,?, i + 1] (1)
[?, p?, i,?,?, i] (2)
[?,Ft(?)?, i, i, j, j] (3)
[?, (p ? 1)?, i, f1, f2, j]
[?, p?, i, f1, f2, j] (4)
[?, (p ? 1)?, i, f1, f2, k]
[?, (p ? 2)?, k, f ?1, f ?2, j]
[?, p?, i, f1 ? f ?1, f2 ? f ?2, j]
(5)
[?, ??, i,?,?, j]
[?, p?, i,?,?, j] (6)
[?, ??, i, f1, f2, j]
[?, p?, f1, f ?1, f ?2, f2]
[?, p?, i, f ?1, f ?2, j]
(7)
[?, p?, i, f1, f2, j]
[?, p?, i, f1, f2, j] (8)
Figure 4: A baseline recognition algorithm for TAG. Rule preconditions and goal item are described in
the text.
[?q,r, p?, i,?,?, i + 1, T (q,r)] (9)
[?q,r, p?, i,?,?, i, T (q,r)] (10)
[?q,r,Ft(?q,r)?, i, i, j, j, T (q,r)] (11)
[?q,r, (p ? 1)?, i, f1, f2, j, T ]
[?q,r, p?, i, f1, f2, j, T ] (12)
[?q,r, (p ? 1)?, i, f1, f2, k, T1]
[?q,r, (p ? 2)?, k, f ?1, f ?2, j, T2]
[?q,r, p?, i, f1 ? f ?1, f2 ? f ?2, j, T1 + T2 ? T (q,r)]
(13)
[?q?,r? , ??, i,?,?, j, T ?]
[?q,r, p?, i,?,?, j, T ? + T (q,r)] (14)
[?q?,r? , ??, i, f1, f2, j, T ?]
[?q,r, p?, f1, f ?1, f ?2, f2, T ]
[?q,r, p?, i, f ?1, f ?2, j, T + T ?]
(15)
[?, p?, i, f1, f2, j, T ]
[?, p?, i, f1, f2, j, T ] (16)
Figure 5: A recognition algorithm for TT-MCTAG. Rule preconditions are the same as for figure 4,
filtering conditions on rules are described in the text.
defined elementwise in the obvious way.
Let D be a derivation tree generated by the TAG
underlying G. We associate D with the t-counter
T such that T [q, r] equals the count of all occur-
rences of elementary tree ?q,r appearing in D. In-
tuitively, we use t-counters to represent informa-
tion about TAG derivation trees that are relevant
to the licensing of such trees by the input TT-
MCTAG G.
We are now ready to present a recognizer based
on TT-MCTAG. To simplify the presentation, we
first discuss how to extend the algorithm of fig. 4
in order to compute t-counters, and will later spec-
ify how to apply TT-MCTAG filtering conditions
through such counters. The reader should however
keep in mind that the two processes are strictly
interleaved, with filtering conditions being tested
right after the construction of each new t-counter.
We use items of the form [?q,r, pt, i, f1, f2, j,
T ], where the first six components are defined as
in the case of TAG items, and the last component is
a t-counter associated with the constructed deriva-
tions. Our algorithm is specified in figure 5.
The simplest case is that of rules (12) and (16).
These rules do not alter the underlying derivation
tree, and thus the t-counter is simply copied from
the antecedent item to the consequent item.
Rules (9), (10) and (11) introduce ?q,r as the
first elementary tree in the analysis (?q,r ? A in
case of rule (11)). Therefore we set the associated
t-counter to T (q,r).
In rule (14) we substitute initial tree ?q?,r? at
node p in ?q,r. In terms of derivation structures,
we extend a derivation tree D? rooted at node v?
with Lab(v?) = ?q?,r? to a new derivation tree D
with root node v, Lab(v) = ?q,r. Node v has a sin-
gle child represented by the root of D?. Thus the
t-counter associated with D should be T ? +T (q,r).
A slightly different operation needs to be per-
formed when applying rule (15). Here we have
a derivation tree D with root node v, Lab(v) =
?q,r and a derivation tree D? with root node v?,
Lab(v?) = ?q?,r? . When adjoining ?q?,r? into ?q,r,
we need to add to the root of D a new child node,
represented by the root of D?. This means that
the t-counter associated with the consequent item
should be the sum of the t-counters associated with
D and D?.
Finally, rule (13) involves derivation trees D1
and D2, rooted at nodes v1 and v2, respectively.
Nodes v1 and v2 have the same label ?q,r. The ap-
plication of the rule corresponds to the ?merging?
of v1 and v2 into a new node v with label ?q,r as
well, Node v inherits all of the children of v1 and
v2. In this case the t-counter associated with the
consequent item is T1 + T2 ? T (q,r). Here T (q,r)
1000
needs to be subtracted because the contribution of
tree ?q,r is accounted for in both v1 and v2.
We can now discuss the filtering conditions that
need to be applied when using the above deduc-
tion rules. We start by observing that the algo-
rithm in figure 5 might not even stop if there is an
infinite set of derivation trees for the input string
w = a1 ? ? ? an in the underlying TAG GT . This
is because each derivation can have a distinct t-
counter. However, the definition of TT-MCTAG
imposes that the head tree of each tuple contains
at least one lexical element. Together with con-
dition (MC), this implies that no more than n tu-
ple instances can occur in a derivation tree for w
according to G. To test for such a condition, we
introduce a norm for t-counters
||T ||m =
|T |
?
q=1
max|?q|r=1 T [q, r] .
We then impose ||T ||m ? n for each t-counter con-
structed by our deduction rule, and block the cor-
responding derivation if this is not satisfied.
We also need to test conditions (i) and (ii) from
lemma 1. Since these conditions apply to nodes
of the derivation tree, this testing is done at each
deduction rule in which a consequent item may be
constructed for a node ??, that is, rules (14), (15)
and (16). We introduce two specialized predicates
F?(T ) ? ?(q, r) : T [q, 1] ? T [q, r] ;
F=(T ) ? ?(q, r) : T [q, 1] = T [q, r] .
We then test F?(T ), which amounts to testing
condition (i) for each argument tree in A(G).
Furthermore, if at some rule we have F?(T ) ?
?F=(T ), then we need to test for condition (ii).
To do this, we consider each argument tree ?q,r,
r 6= 1, and compare the elementary tree ?q,r in the
consequent item of the current rule with ?q,r and
h(?q,r) = ?q,1, to select the appropriate subcondi-
tion of (ii).
As an example, assume that we are applying
rule (15) as in figure 5, with p = ?. Let Tc =
T + T ? be the t-counter associated with the con-
sequent item. When we come to process some ar-
gument tree ?q,r such that Tc[q, r] ? Tc[q, 1] > 0
and ?q,r 6? {?q,r, ?q,1}, we need to test (ii)c. This
is done by requiring
T ?[q, r]? T ?[q, 1] = Tc[q, r]? Tc[q, 1].
If we are instead applying rule (16) with p = ?
and T [q, r] ? T [q, 1] > 0, then we test (ii)a, since
there is no adjunction at the root node, by requir-
ing ?q,r = ?q,r and T [q, r] ? T [q, 1] = 1.
We block the current derivation whenever the
conditions in lemma 1 are not satisfied.
The algorithm recognizes w if and only if some
item [?q,1, ??, 0,?,?, n, T ] can be inferred sat-
isfying ?q,1 ? I , Lab(?q,1, ?) = S and F=(T ).
The correctness immediately follows from the cor-
rectness of the underlying TAG parser and from
lemma 1.
Finally, we turn to the computational analysis
of the algorithm. We assume a tabular implemen-
tation of the process of item inference using our
deduction rules. Our algorithm clearly stops after
some finite amount of time, because of the filtering
condition ||T ||m ? n. We then need to derive an
upper bound on the number of applications of de-
duction rules. To do this, we use an argument that
is rather standard in the tabular parsing literature.
The number of t-counters satisfying ||T ||m ? n
is O(ncG), with cG =
?|T |
i=1 |?i|. Since all of
the other components in an item are bounded by
O(n4), there are polynomially (in n) many items
that can be constructed for an input w. It is not dif-
ficult to see that each individual item can be con-
structed by a number of rule applications bounded
by a polynomial as well. Therefore, the total num-
ber of applications of our deduction rules is also
bounded by some polynomial in n. We thus con-
clude that the languages generated by the class TT-
MCTAG are all included in PTIME.
5 Conclusion and open problems
We have shown in this paper that the class of lan-
guages generated by TT-MCTAG is included in
PTIME, by characterizing the definition of TT-
MCTAG through some conditions that can be
tested locally. PTIME is one of the required
properties in the definition of the class of Mildly
Context-Sensitive (MCS) formalisms (Joshi et al,
1991). In order to settle membership in MCS for
TT-MCTAG, what is still missing is the constant-
growth property or, more generally, the semilin-
earity property.
Acknowledgments
The work of the first author has been sup-
ported by the DFG within the Emmy-Noether
Program. The second author has been partially
supported by MIUR under project PRIN No.
2007TJNZRE 002.
1001
References
Tilman Becker, Aravind K. Joshi, and Owen Rambow.
1991. Long-distance scrambling and tree adjoining
grammars. In Proceedings of ACL-Europe.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The Derivationel Generative Power of Formal
Systems or Scrambling is Beyond LCFRS. Tech-
nical Report IRCS-92-38, Institute for Research in
Cognitive Science, University of Pennsylvania.
Lucas Champollion. 2007. Lexicalized non-local MC-
TAG with dominance links is NP-complete. In Ger-
ald Penn and Ed Stabler, editors, Proceedings of
Mathematics of Language (MOL) 10, CSLI On-Line
Publications.
Joan Chen-Main and Aravind Joshi. 2007. Some
observations on a graphical model-theoretical ap-
proach and generative models. In Model Theoretic
Syntax at 10. Workshop, ESSLLI 2007, Dublin, Ire-
land.
Armin B. Cremers and Otto Mayer. 1973. On matrix
languages. Information and Control, 23:86?96.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoning Grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, pages
69?123. Springer, Berlin.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree Adjunct Grammars. Journal of
Computer and System Science, 10:136?163.
A. Joshi, K. Vijay-Shanker, and D. Weir. 1991. The
convergence of mildly context-sensitive grammati-
cal formalisms. In P. Sells, S. Shieber, and T. Wa-
sow, editors, Foundational Issues in Natural Lan-
guage Processing. MIT Press, Cambridge MA.
Aravind K. Joshi. 1985. Tree adjoining grammars:
How much contextsensitivity is required ro provide
reasonable structural descriptions? In D. Dowty,
L. Karttunen, and A. Zwicky, editors, Natural Lan-
guage Parsing, pages 206?250. Cambridge Univer-
sity Press.
Laura Kallmeyer and Yannick Parmentier. 2008. On
the relation between Multicomponent Tree Adjoin-
ing Grammars with Tree Tuples (TT-MCTAG) and
Range Concatenation Grammars (RCG). In Carlos
Mart??n-Vide, Friedrich Otto, and Henning Fernaus,
editors, Language and Automata Theory and Ap-
plications. Second International Conference, LATA
2008, number 5196 in Lecture Notes in Computer
Science, pages 263?274. Springer-Verlag, Heidel-
berg Berlin.
Laura Kallmeyer. 2005. Tree-local multicomponent
tree adjoining grammars with shared nodes. Com-
putational Linguistics, 31(2):187?225.
Timm Lichte and Laura Kallmeyer. 2008. Factorizing
Complementation in a TT-MCTAG for German. In
Proceedings of the Ninth International Workshop on
Tree Adjoining Grammars and Related Formalisms
(TAG+9), pages 57?64, Tu?bingen, June.
Timm Lichte. 2007. An MCTAG with Tuples for Co-
herent Constructions in German. In Proceedings
of the 12th Conference on Formal Grammar 2007,
Dublin, Ireland.
Rebecca Nesson and Stuart Shieber. 2008. Syn-
chronous Vector TAG for Syntax and Semantics:
Control Verbs, Relative Clauses, and Inverse Link-
ing. In Proceedings of the Ninth International Work-
shop on Tree Adjoining Grammars and Related For-
malisms (TAG+9), Tu?bingen, June.
Owen Rambow and Giorgio Satta. 1992. Formal prop-
erties of non-locality. In Proceedings of 1st Interna-
tional Workshop on Tree Adjoining Grammars.
Owen Rambow and Giorgio Satta. 1994. A rewrit-
ing system for free word order syntax that is non-
local and mildly context sensitive. In C. Mart??n-
Vide, editor, Current Issues in Mathematical Lin-
guistics, North-Holland Linguistic series, Volume
56. Elsevier-North Holland, Amsterdam.
Owen Rambow, K. Vijay-shanker, and David Weir.
1995. Parsing d-Ttree grammars. In Proceedings of
the Fourth International Workshop on Parsing Tech-
nologies, Prague, pages 252?259.
Owen Rambow. 1994. Formal and Computational
Aspects of Natural Language Syntax. Ph.D. thesis,
University of Pennsylvania.
Giorgio Satta. 1995. The membership problem for un-
ordered vector languages. In Developments in Lan-
guage Theory, pages 267?275.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and Implementation of
Deductive Parsing. Journal of Logic Programming,
24(1&2):3?36.
Anders S?gaard, Timm Lichte, and Wolfgang Maier.
2007. The complexity of linguistically motivated
extensions of tree-adjoining grammar. In Recent
Advances in Natural Language Processing 2007,
Borovets, Bulgaria.
K. Vijay-Shanker and Aravind K. Joshi. 1985. Some
computational properties of Tree Adjoining Gram-
mars. In Proceedings of the 23rd Annual Meeting
of the Association for Computational Linguistics,
pages 82?93.
K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In 25th Meet-
ing of the Association for Computational Linguistics
(ACL?87).
David J. Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
1002
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 9?12,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
An Earley Parsing Algorithm for Range Concatenation Grammars
Laura Kallmeyer
SFB 441
Universit?at T?ubingen
72074 T?ubingen, Germany
lk@sfs.uni-tuebingen.de
Wolfgang Maier
SFB 441
Universit?at T?ubingen
72074 T?ubingen, Germany
wo.maier@uni-tuebingen.de
Yannick Parmentier
CNRS - LORIA
Nancy Universit?e
54506 Vand?uvre, France
parmenti@loria.fr
Abstract
We present a CYK and an Earley-style
algorithm for parsing Range Concatena-
tion Grammar (RCG), using the deduc-
tive parsing framework. The characteris-
tic property of the Earley parser is that we
use a technique of range boundary con-
straint propagation to compute the yields
of non-terminals as late as possible. Ex-
periments show that, compared to previ-
ous approaches, the constraint propagation
helps to considerably decrease the number
of items in the chart.
1 Introduction
RCGs (Boullier, 2000) have recently received a
growing interest in natural language processing
(S?gaard, 2008; Sagot, 2005; Kallmeyer et al,
2008; Maier and S?gaard, 2008). RCGs gener-
ate exactly the class of languages parsable in de-
terministic polynomial time (Bertsch and Neder-
hof, 2001). They are in particular more pow-
erful than linear context-free rewriting systems
(LCFRS) (Vijay-Shanker et al, 1987). LCFRS is
unable to describe certain natural language phe-
nomena that RCGs actually can deal with. One
example are long-distance scrambling phenom-
ena (Becker et al, 1991; Becker et al, 1992).
Other examples are non-semilinear constructions
such as case stacking in Old Georgian (Michaelis
and Kracht, 1996) and Chinese number names
(Radzinski, 1991). Boullier (1999) shows that
RCGs can describe the permutations occurring
with scrambling and the construction of Chinese
number names.
Parsing algorithms for RCG have been intro-
duced by Boullier (2000), who presents a di-
rectional top-down parsing algorithm using pseu-
docode, and Barth?elemy et al (2001), who add an
oracle to Boullier?s algorithm. The more restricted
class of LCFRS has received more attention con-
cerning parsing (Villemonte de la Clergerie, 2002;
Burden and Ljungl?of, 2005). This article proposes
new CYK and Earley parsers for RCG, formulat-
ing them in the framework of parsing as deduction
(Shieber et al, 1995). The second section intro-
duces necessary definitions. Section 3 presents a
CYK-style algorithm and Section 4 extends this
with an Earley-style prediction.
2 Preliminaries
The rules (clauses) of RCGs
1
rewrite predicates
ranging over parts of the input by other predicates.
E.g., a clause S(aXb)? S(X) signifies that S is
true for a part of the input if this part starts with an
a, ends with a b, and if, furthermore, S is also true
for the part between a and b.
Definition 1. A RCG G = ?N,T, V, P, S? con-
sists of a) a finite set of predicates N with an arity
function dim: N ? N \ {0} where S ? N is
the start predicate with dim(S) = 1, b) disjoint fi-
nite sets of terminals T and variables V , c) a finite
set P of clauses ?
0
? ?
1
. . . ?
m
, where m ? 0
and each of the ?
i
, 0 ? i ? m, is a predicate of
the form A
i
(?
1
, . . . , ?
dim(A
i
)
) with A
i
? N and
?
j
? (T ? V )
?
for 1 ? j ? dim(A
i
).
Central to RCGs is the notion of ranges on
strings.
Definition 2. For every w = w
1
. . . w
n
with
w
i
? T (1 ? i ? n), we define a) Pos(w) =
{0, . . . , n}. b) ?l, r? ? Pos(w) ? Pos(w) with
l ? r is a range in w. Its yield ?l, r?(w) is the
substring w
l+1
. . . w
r
. c) For two ranges ?
1
=
?l
1
, r
1
?, ?
2
= ?l
2
, r
2
?: if r
1
= l
2
, then ?
1
? ?
2
=
?l
1
, r
2
?; otherwise ?
1
? ?
2
is undefined. d) A vec-
tor ? = (?x
1
, y
1
?, . . . , ?x
k
, y
k
?) is a range vector
of dimension k in w if ?x
i
, y
i
? is a range in w for
1 ? i ? k. ?(i).l (resp. ?(i).r) denotes then the
1
In this paper, by RCG, we always mean positive RCG,
see Boullier (2000) for details.
9
first (resp. second) component of the ith element
of ?, that is x
i
(resp. y
i
).
In order to instantiate a clause of the grammar,
we need to find ranges for all variables in the
clause and for all occurrences of terminals. For
convenience, we assume the variables in a clause
and the occurrences of terminals to be equipped
with distinct subscript indices, starting with 1 and
ordered from left to right (where for variables,
only the first occurrence is relevant for this order).
We introduce a function ? : P ? N that gives the
maximal index in a clause, and we define ?(c, x)
for a given clause c and x a variable or an occur-
rence of a terminal as the index of x in c.
Definition 3. An instantiation of a c ? P with
?(c) = j w.r.t. to some string w is given by a
range vector ? of dimension j. Applying ? to
a predicate A(~?) in c maps all occurrences of
x ? (T ? V ) with ?(c, x) = i in ~? to ?(i). If
the result is defined (i.e., the images of adjacent
variables can be concatenated), it is called an in-
stantiated predicate and the result of applying ? to
all predicates in c, if defined, is called an instanti-
ated clause.
We also introduce range constraint vectors, vec-
tors of pairs of range boundary variables together
with a set of constraints on these variables.
Definition 4. Let V
r
= {r
1
, r
2
, . . . } be a set
of range boundary variables. A range constraint
vector of dimension k is a pair ?~?, C? where a)
~? ? (V
2
r
)
k
; we define V
r
(~?) as the set of range
boundary variables occurring in ~?. b) C is a set
of constraints c
r
that have one of the following
forms: r
1
= r
2
, k = r
1
, r
1
+ k = r
2
,
k ? r
1
, r
1
? k, r
1
? r
2
or r
1
+ k ? r
2
for r
1
, r
2
? V
r
(~?) and k ? N.
We say that a range vector ? satisfies a range
constraint vector ??, C? iff ? and ? are of the same
dimension k and there is a function f : V
r
? N
that maps ?(i).l to ?(i).l and ?(i).r to ?(i).r for
all 1 ? i ? k such that all constraints in C are sat-
isfied. Furthermore, we say that a range constraint
vector ??, C? is satisfiable iff there exists a range
vector ? that satisfies it.
Definition 5. For every clause c, we define its
range constraint vector ??, C? w.r.t. aw with |w| =
n as follows: a) ? has dimension ?(c) and all
range boundary variables in ? are pairwise differ-
ent. b) For all ?r
1
, r
2
? ? ?: 0 ? r
1
, r
1
? r
2
,
r
2
? n ? C. For all occurrences x of terminals
in cwith i = ?(c, x): ?(i).l+1 = ?(i).r ? C. For
all x, y that are variables or occurrences of termi-
nals in c such that xy is a substring of one of the
arguments in c: ?(?(c, x)).r = ?(?(c, y)).l ? C.
These are all constraints in C.
The range constraint vector of a clause c cap-
tures all information about boundaries forming a
range, ranges containing only a single terminal,
and adjacent variables/terminal occurrences in c.
An RCG derivation consists of rewriting in-
stantiated predicates applying instantiated clauses,
i.e. in every derivation step ?
1
?
w
?
2
, we re-
place the lefthand side of an instantiated clause
with its righthand side (w.r.t. a word w). The lan-
guage of an RCG G is the set of strings that can
be reduced to the empty word: L(G) = {w |
S(?0, |w|?)
+
?
G,w
?}.
The expressive power of RCG lies beyond mild
context-sensitivity. As an example, consider the
RCG from Fig. 3 that generates a language that is
not semilinear.
For simplicity, we assume in the following with-
out loss of generality that empty arguments (?)
occur only in clauses whose righthand sides are
empty.
2
3 Directional Bottom-Up Chart Parsing
In our directional CYK algorithm, we move a dot
through the righthand side of a clause. We there-
fore have passive items [A, ?] where A is a pred-
icate and ? a range vector of dimension dim(A)
and active items. In the latter, while traversing
the righthand side of the clause, we keep a record
of the left and right boundaries already found
for variables and terminal occurrences. This is
achieved by subsequently enriching the range con-
straint vector of the clause. Active items have the
form [A(~x)? ? ??, ??, C?] with A(~x)? ?? a
clause, ?? 6= ?, ?(A(~x ? ??)) = j and ??, C?
a range constraint vector of dimension j. We re-
quire that ??, C? be satisfiable.
3
2
Any RCG can be easily transformed into an RCG satis-
fying this condition: Introduce a new unary predicate Eps
with a clause Eps(?) ? ?. Then, for every clause c with
righthand side not ?, replace every argument ? that occurs in
c with a new variable X (each time a distinct one) and add
the predicate Eps(X) to the righthand side of c.
3
Items that are distinguished from each other only by a bi-
jection of the range variables are considered equivalent. I.e.,
if the application of a rule yields a new item such that an
equivalent one has already been generated, this new one is
not added to the set of partial results.
10
Scan:
[A, ?]
A(~x)? ? ? P with instantiation ?
such that ?(A(~x)) = A(?)
Initialize:
[A(~x)? ??, ??,C?]
A(~x)? ? ? P with
range constraint vector
??,C?,? 6= ?
Complete:
[B,?
B
],
[A(~x)? ? ?B(x
1
...y
1
, ..., x
k
...y
k
)?, ??,C?]
[A(~x)? ?B(x
1
...y
1
, ..., x
k
...y
k
) ??, ??,C
?
?]
where C
?
= C ? {?
B
(j).l = ?(?(x
j
)).l, ?
B
(j).r =
?(?(y
j
)).r | 1 ? j ? k}.
Convert:
[A(~x)? ??, ??,C?]
[A, ?]
A(~x)? ? ? P with
an instantiation ? that
satisfies ??,C?,
?(A(~x)) = A(?)
Goal: [S, (?0, n?)]
Figure 1: CYK deduction rules
The deduction rules are shown in Fig. 1. The
first rule scans the yields of terminating clauses.
Initialize introduces clauses with the dot on the
left of the righthand side. Complete moves the dot
over a predicate provided a corresponding passive
item has been found. Convert turns an active item
with the dot at the end into a passive item.
4 The Earley Algorithm
We now add top-down prediction to our algorithm.
Active items are as above. Passive items have
an additional flag p or c depending on whether
the item is predicted or completed, i.e., they ei-
ther have the form [A, ??, C?, p] where ??, C? is a
range constraint vector of dimension dim(A), or
the form [A, ?, c] where ? is a range vector of di-
mension dim(A).
Initialize:
[S, ?(?r
1
, r
2
?), {0 = r
1
, n = r
2
}?, p]
Predict-rule:
[A, ??,C?, p]
[A(x
1
. . . y
1
, . . . , x
k
. . . y
k
)? ??, ??
?
, C
?
?]
where ??
?
, C
?
? is obtained from the range constraint vector
of the clause A(x
1
. . . y
1
, . . . , x
k
. . . y
k
) ? ? by taking all
constraints from C, mapping all ?(i).l to ?
?
(?(x
i
)).l and
all ?(i).r to ?
?
(?(y
i
)).r, and then adding the resulting con-
straints to the range constraint vector of the clause.
Predict-pred:
[A(...)? ? ?B(x
1
...y
1
, ..., x
k
...y
k
)?, ??,C?]
[B, ??
?
, C
?
?, p]
where ?
?
(i).l = ?(?(x
i
)).l, ?
?
(i).r = ?(?(y
i
)).r for all
1 ? i ? k and C
?
= {c | c ? C, c contains only range
variables from ?
?
}.
Scan:
[A, ??,C?, p]
[A, ?, c]
A(~x)? ? ? P with an
instantiation ? satisfying ??,C?
such that ?(A(~x)) = A(?)
Figure 2: Earley deduction rules
The deduction rules are listed in Fig. 2. The
axiom is the prediction of an S ranging over the
entire input (initialize). We have two predict op-
erations: Predict-rule predicts active items with
the dot on the left of the righthand side, for a
given predicted passive item. Predict-pred pre-
dicts a passive item for the predicate following the
dot in an active item. Scan is applied whenever a
predicted predicate can be derived by an ?-clause.
The rules complete and convert are the ones from
the CYK algorithm except that we add flags c to
the passive items occurring in these rules. The
goal is again [S, (?0, n?), c].
To understand how this algorithm works, con-
sider the example in Fig. 3. The crucial property of
this algorithm, in contrast to previous approaches,
is the dynamic updating of a set of constraints on
range boundaries. We can leave range boundaries
unspecified and compute their values in a more in-
cremental fashion instead of guessing all ranges of
a clause at once at prediction.
4
For evaluation, we have implemented a direc-
tional top-down algorithm where range bound-
aries are guessed at prediction (this is essentially
the algorithm described in Boullier (2000)), and
the new Earley-style algorithm. The algorithms
were tested on different words of the language
L = {a
2
n
|n ? 0}. Table 1 shows the number
of generated items.
Word Earley TD
a
2
15 21
a
4
30 55
a
8
55 164
a
9
59 199
Word Earley TD
a
16
100 539
a
30
155 1666
a
32
185 1894
a
64
350 6969
Table 1: Items generated by both algorithms
Clearly, range boundary constraint propagation
increases the amount of information transported
in single items and thereby decreases considerably
the number of generated items.
5 Conclusion and future work
We have presented a new CYK and Earley pars-
ing algorithms for the full class of RCG. The cru-
cial difference between previously proposed top-
down RCG parsers and the new Earley-style algo-
rithm is that while the former compute all clause
instantiations during predict operations, the latter
4
Of course, the use of constraints makes comparisons be-
tween items more complex and more expensive which means
that for an efficient implementation, an integer-based repre-
sentation of the constraints and adequate techniques for con-
straint solving are required.
11
Grammar for {a
2
n
|n > 0}: S(XY )? S(X)eq(X,Y ), S(a
1
)? ?, eq(a
1
X, a
2
Y )? eq(X,Y ), eq(a
1
, a
2
)? ?
Parsing trace for w = aa:
Item Rule
1 [S, ?(?r
1
, r
2
?), {0 = r
1
, r
1
? r
2
, 2 = r
2
}?, p] initialize
2 [S(XY )? ?S(X)eq(X,Y ), {X.l ? X.r,X.r = Y.l, Y.l ? Y.r, 0 = X.l, 2 = Y.r}] predict-rule from 1
3 [S, ?(?r
1
, r
2
?), {0 = r
1
, r
1
? r
2
}?, p] predict-pred from 2
4 [S, (?0, 1?), c] scan from 3
5 [S(XY )? ?S(X)eq(X,Y ), {X.l ? X.r,X.r = Y.l, Y.l ? Y.r, 0 = X.l, }] predict-rule from 3
6 [S(XY )? S(X) ? eq(X,Y ), {. . . , 0 = X.l, 2 = Y.r, 1 = X.r}] complete 2 with 4
7 [S(XY )? S(X) ? eq(X,Y ), {X.l ? X.r,X.r = Y.l, Y.l ? Y.r, 0 = X.l, 1 = X.r}] complete 5 with 4
8 [eq, ?(?r
1
, r
2
?, ?r
3
, r
4
?), {r
1
? r
2
, r
2
= r
3
, r
3
? r
4
, 0 = r
1
, 2 = r
4
, 1 = r
2
}?] predict-pred from 6
9 [eq(a
1
X, a
2
Y )? ?eq(X,Y ), {a
1
.l + 1 = a
1
.r, a
1
.r = X.l,X.l ? X.r,
a
2
.l + 1 = a
2
.r, a
2
.r = Y.l, Y.l ? Y.r,X.r = a
2
.l, 0 = a
1
.l, 1 = X.r, 2 = Y.r}] predict-rule from 8
. . .
10 [eq, (?0, 1?, ?1, 2?), c] scan 8
11 [S(XY )? S(X)eq(X,Y )?, {. . . , 0 = X.l, 2 = Y.r, 1 = X.r, 1 = Y.l}] complete 6 with 10
12 [S, (?0, 2?), c] convert 11
Figure 3: Trace of a sample Earley parse
avoids this using a technique of dynamic updating
of a set of constraints on range boundaries. Exper-
iments show that this significantly decreases the
number of generated items, which confirms that
range boundary constraint propagation is a viable
method for a lazy computation of ranges.
The Earley parser could be improved by allow-
ing to process the predicates of the righthand sides
of clauses in any order, not necessarily from left
to right. This way, one could process predicates
whose range boundaries are better known first. We
plan to include this strategy in future work.
References
Franc?ois Barth?elemy, Pierre Boullier, Philippe De-
schamp, and
?
Eric de la Clergerie. 2001. Guided
parsing of Range Concatenation Languages. In Pro-
ceedings of ACL, pages 42?49.
Tilman Becker, Aravind K. Joshi, and Owen Rambow.
1991. Long-distance scrambling and tree adjoining
grammars. In Proceedings of EACL.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The Derivationel Generative Power of Formal
Systems or Scrambling is Beyond LCFRS. Tech-
nical Report IRCS-92-38, Institute for Research in
Cognitive Science, University of Pennsylvania.
E. Bertsch and M.-J. Nederhof. 2001. On the complex-
ity of some extensions of RCG parsing. In Proceed-
ings of IWPT 2001, pages 66?77, Beijing, China.
Pierre Boullier. 1999. Chinese numbers, mix, scram-
bling, and range concatenation grammars. In Pro-
ceedings of EACL, pages 53?60, Bergen, Norway.
Pierre Boullier. 2000. Range concatenation grammars.
In Proceedings of IWPT 2000, pages 53?64, Trento.
H?akan Burden and Peter Ljungl?of. 2005. Parsing lin-
ear context-free rewriting systems. In Proceedings
of IWPT 2005, pages 11?17, Vancouver.
Laura Kallmeyer, Timm Lichte, Wolfgang Maier, Yan-
nick Parmentier, and Johannes Dellert. 2008. De-
veloping an MCTAG for German with an RCG-
based parser. In Proceedings of LREC-2008, Mar-
rakech, Morocco.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of the 13th Conference on Formal Grammar 2008,
Hamburg, Germany.
Jens Michaelis and Marcus Kracht. 1996. Semilinear-
ity as a Syntactic Invariant. In Logical Aspects of
Computational Linguistics, Nancy.
Daniel Radzinski. 1991. Chinese number-names, tree
adjoining languages, and mild context-sensitivity.
Computational Linguistics, 17:277?299.
Beno??t Sagot. 2005. Linguistic facts as predicates over
ranges of the sentence. In Proceedings of LACL 05,
number 3492 in Lecture Notes in Computer Science,
pages 271?286, Bordeaux, France. Springer.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24(1& 2):3?36.
Anders S?gaard. 2008. Range concatenation gram-
mars for translation. In Proceedings of COLING,
Manchester, England.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of ACL.
Eric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata.
In Proceedings of COLING, Taipei, Taiwan.
12
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Quantifier Scope in German: An MCTAG Analysis
Laura Kallmeyer
University of T?bingen
Collaborative Research Center 441
lk@sfs.uni-tuebingen.de
Maribel Romero
University of Pennsylvania
Department of Linguistics
romero@ling.upenn.edu
Abstract
Relative quantifier scope in German de-
pends, in contrast to English, very much
on word order. The scope possibilities of a
quantifier are determined by its surface po-
sition, its base position and the type of the
quantifier. In this paper we propose a mul-
ticomponent analysis for German quanti-
fiers computing the scope of the quantifier,
in particular its minimal nuclear scope, de-
pending on the syntactic configuration it
occurs in.
1 Introduction: The data
(1) A man loves every woman.
? > ?, ? > ?
In English, in sentences with several quantifica-
tional NPs, in principle all scope orders are pos-
sible independent from word order. (1) for exam-
ple has two readings, the ? > ? reading and the
inverse scope ? > ? reading. This is different in
German where word order is crucial for scope pos-
sibilities.
(2) a. Viele M?nner haben mindestens eine
many mennom have at least one
Frau hofiert.
womanacc flattered.
?Many men have flattered at least one woman.?
viele > eine, ?eine > viele
b. Mindestens eine Frau haben viele
at least one womanacc have many
M?nner hofiert.
mennom flattered.
?Many men have flattered at least one woman.?
viele > eine, eine > viele
In German, for quantifiers in base order, the sur-
face order determines scope.1 (2a) has only the
scope order viele > eine corresponding to sur-
face order, that is, the inverse order eine > viele
is not available. In contrast to this, if the word
order differs from the base order, ambiguities are
possible. (2b) for example displays both scope or-
ders, viele > eine and eine > viele.
In the literature, the following generalizations
have been noticed for German: For two quantifiers
Q1, Q2 with Q1 preceding Q2 in the surface order
of a sentence, the scope order Q1 > Q2 is always
possible. Furthermore, the inverse reading Q2 >
Q1 is possible if
(Q1) Q1 has been moved so that Q2 c-commands
the trace of Q1 ((Frey, 1993)), and
(Q2) Q1 is a weak quantifier (e.g., irgendein
?some?, viele ?many?, cardinals) ((Lechner,
1998)).
Evidence for (Q2) ?and further evidence for
(Q1)? are the examples in (3)?(4). In (3), the (a)-
example is in base order and thus has only surface
scope, but moving the weak quantifier over the da-
tive quantifier in the (b)-version results in scope
ambiguity. This contrasts with (4). In (4), the (a)-
version with base order has only surface scope, as
before. But now we move the strong quantifier
over the dative quantifier, and this does not yield
ambiguity. That is, even though the dative quan-
tifier c-commands the trace of the moved quanti-
fier both in (3b) and in (4b), only when the moved
1Throughout the paper we assume an unmarked intona-
tion. With a different intonation, other scope orders become
available because of the change in information structure. But
this lies outside the scope of this paper.
The base order depends on the verb; in most cases it is Sub-
ject - (Indirect Object) - Direct Object.
73
element is a weak quantifier do we obtain scope
ambiguity.
(3) a. . . . dass er [fast jedem Verlag]
. . . that he almost every publisher
[mindestens ein Gedicht] anbot.
at least one poem proposed_to.
?. . . that he proposed some poem to almost every
publisher.?
jedem > ein, ?ein > jedem
b. . . . dass er [mindestens ein Gedicht]1
. . . that he some poem
[fast jedem Verlag] t1 anbot.
almost every publisher proposed_to.
jedem > ein, ein > jedem
(4) a. . . . dass er [mindestens einem Verleger]
. . . that he at least one publisher
[fast jedes Gedicht] anbot.
almost every poem proposed_to
?. . . that he proposed almost every poem to at least
one publisher.?
jedes > einem, ?einem > jedes
b. . . . dass er [fast jedes Gedicht]1
. . . that he almost every poem
[mindestens einem Verleger] t1
at least one publisher
anbot.
proposed_to.
jedes > einem, ?einem > jedes
(Kiss, 2000) claims that if two quantifiers have
been moved such that among themselves they re-
main in base order, inverse scope is not possible
between them. Because of this, he argues for a
non-movement-based theory of German quantifier
scope. However, Kiss? claim is not true as can be
seen with the example (5) from (Frey, 1993):
(5) a. weil der freundliche Museumsdirektor
because the friendly curatornom
[mindestens einer Frau]1
at least one womandat
[fast jedes Gem?lde]2 gezeigt hat
almost every paintingacc has shown
?because the friendly curator has shown almost ev-
ery painting to at least one woman?
Q1 > Q2, ?Q2 > Q1
b. weil [mindestens einer Frau]1 [fast jedes
Gem?lde]2 der freundliche Museumsdi-
rektor t1 t2 gezeigt hat
Q1 > Q2, Q2 > Q1
In both cases, (5a) and (5b), the two quanti-
fiers are in base order. According to Kiss there
should be, contrary to fact, no ambiguity in (5b).
The difference between the two is that in (5a) the
quantifiers are in base position while in (5b) both
of them have been scrambled with the result that
Q2 c-commands the trace of Q1. We assume with
(Frey, 1993) that this is why the inverse scope or-
der becomes available.
We therefore stick to the above-mentioned gen-
eralizations (Q1) and (Q2) and try to capture them
in our LTAG analysis. This means that, in order to
capture (Q1), we need a syntactic analysis of Ger-
man NPs that takes into account movement and
base positions.
2 English quantifier scope in LTAG
We use the LTAG semantics framework from
(Kallmeyer and Romero, 2004; Kallmeyer and
Romero, 2005). Semantic computation is done on
the derivation tree. Each elementary tree is linked
to a semantic representation (a set of Ty2 formu-
las and scope constraints). Ty2 formulas (Gallin,
1975) are typed ?-terms with individuals and situ-
ations as basic types. The scope constraints of the
form x ? y specify subordination relations be-
tween Ty2 expressions. In other words, x ? y
indicates that y is a component of x.
A semantic representation is equipped with a
semantic feature structure description. Semantic
computation consists of certain feature value iden-
tifications between mother and daughter nodes in
the derivation tree. The feature structure descrip-
tions do not encode the semantic expressions one
is interested in. They only encode their contribu-
tions to functional applications by restricting the
argument slots of certain predicates in the seman-
tic representations: They state which elements are
contributed as possible arguments for other se-
mantic expressions and which arguments need to
be filled. They thereby simulate lambda abstrac-
tion and functional application. A sample feature
for this simulation of functional application is the
feature I that serves to pass the individual con-
tributed by an NP to the predicate taking it as an
argument. Besides this functional application as-
pects, the feature structure descriptions also con-
tain features that determine the scope semantics,
i.e., features specifying boundaries for the scope
of different operators. Sample features for scope
are MINS and MAXS encoding the minimal and
74
maximal scope of attaching quantifiers.
Features can be global (feature GLOBAL, here
abbreviated with GL) or they can be linked to spe-
cific node positions (features S, VP, . . . ). The latter
are divided into top (T) and bottom (B) features.
The equations of top and bottom features linked
to specific node positions in the elementary trees
are parallel to the syntactic unifications in FTAG
(Vijay-Shanker and Joshi, 1988). The global fea-
tures that are not linked to specific nodes can be
passed from mothers to daughters and vice versa
in the derivation tree.
(6) Everybody laughs.
As a sample derivation let us sketch the anal-
ysis of quantificational NPs in English from
(Kallmeyer, 2005). Fig. 1 shows the LTAG anal-
ysis of (6). More precisely, it shows the deriva-
tion tree with the semantic representations and fea-
ture structure descriptions of laughs and every-
body as node labels. The feature identifications
are depicted by dotted lines. The semantic repre-
sentation of the NP everybody contains the gen-
eralized quantifier every that binds the variable x
and that has a restrictive scope 4 and a nuclear
scope 5 . Furthermore, it contains the proposi-
tion person(x) that must be part of the restrictive
scope (constraint 4 ? l3). Concerning functional
application, the NP provides the individual vari-
able x in the global feature I as a possible argu-
ment for the verb predicate laugh.
l1 : laugh( 1 ),
2 ? 3
?
?
?
?
?
?
?
?
?
?
?
?
GL
[
MINS l1
MAXS 2
]
S
[
B
[
P 3
]
]
VP
[
T
[
P 3
]
B
[
P l1
]
]
NP
[
GL
[
I 1
]
]
?
?
?
?
?
?
?
?
?
?
?
?
np
l2 : every(x, 4 , 5 ),
l3 : person(x),
4 ? l3,
6 ? 5 , 5 ? 7
?
?
?
?
GL
[
I x
]
NP
[
GL
[
MINS 7
MAXS 6
]
]
?
?
?
?
Figure 1: LTAG analysis of (6) everybody laughs
Quantificational NPs in English can in princi-
ple scope freely; an analysis of quantifier scope
must guarantee only two things: 1. the proposition
corresponding to the predicate to which a quanti-
fier attaches must be in its nuclear scope, and 2. a
quantifier cannot scope higher than the first finite
clause. (Kallmeyer and Romero, 2005) model this
by defining a scope window delimited by some
maximal scope (global feature MAXS and some
minimal scope (global feature MINS) for a quanti-
fier. In Fig. 1, the nuclear scope 5 of the quantifier
is delimited by the maximal and minimal scope
boundaries provided by the verb the quantifier at-
taches to (constraints 6 ? 5 , 5 ? 7 ). The feature
identifications in Fig. 1 lead then to the constraints
2 ? 5 , 5 ? l1.
Applying the assignments following from the
feature identifications and building the union of
the semantic representations leads to the under-
specified representation (7):
(7)
l1 : laugh(x),
l2 : every(x, 4 , 5 ), l3 : person(x)
2 ? l1,
4 ? l3, 2 ? 5 , 5 ? l1
As the only possible disambiguation, we obtain
2 ? l2, 4 ? l3, 5 ? l1 which yields the seman-
tics every(x,person(x), laugh(x)).
3 Syntax of German quantificational NPs
Recall that, according to criterion (Q1), not only
the position of an NP but also -if the NP was
moved- the position of its trace are crucial for the
scope properties. In order to capture this, our anal-
ysis needs to take into account movements (scram-
bling, topicalization, etc.) of NPs including traces
at base positions. We therefore cannot adopt the
analyses proposed by (Rambow, 1994) in V-TAG
where the slot for the NP is generated at the sur-
face position and there is only one initial tree for
NPs, whether moved or not.2
(8) a. . . . dass jeder/irgendeiner
. . . that everybody/someone
irgendein Buch/jedes Buch liest
some book/every book reads
?. . . that everybody/someone reads some
book/every book?
SUBJ > DOBJ
b. . . . dass [jedes Buch]1 irgendeiner t1 liest
. . . that every book someone reads
DOBJ > SUBJ
2To avoid misunderstandings, let us emphasize that in
LTAG, there is no movement outside the lexicon. Therefore,
either the NP or the slot of the NP must be localized together
with the corresponding trace inside one elementary structure.
This elementary structure can be a tree or, in MCTAG, a set
of trees.
75
c. . . . dass [irgendein Buch]1 jeder t1 liest
. . . that some book everybody reads
SUBJ > DOBJ, DOBJ > SUBJ
To illustrate our analysis, in this and the follow-
ing section, we restrict ourselves to the sentences
in (8). For the syntax, we adopt a multicompo-
nent analysis for NPs that have been moved con-
sisting of an auxiliary tree for the moved mate-
rial and an initial tree for the trace. Our analysis
can be adopted using V-TAG (Rambow, 1994) or
something in the style of SN-MCTAG (Kallmeyer,
2005). Note that, in order to account for scram-
bling, we need some type of MCTAG anyway, in-
dependent from quantifier scope.
VP
NP VP
NP V
liest
for each NP, e.g., irgendein Buch:
?1
NP
irgendein Buch
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? VP
NP VP?
irgendein Buch
?2 NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Elementary trees for (8)
The elementary trees for (8) are in Fig. 2. ?1
is used for NPs in base position, while the set
{?2, ?} is used for moved NPs. We assume that,
if possible, ?1 is used. I.e., starting from the verb,
trees of type ?1 are substituted to its left as long
as possible. {?2, ?} sets are used when ?1 could
not possibly yield the desired surface word order.
Fig. 3 shows a derivation of a sentence of type (8a)
(with no movement). Fig. 4 shows the derivation
of (8b). ((8c) is similar to (8b).)
NP
irgendeiner
NP
jedes Buch
VP
NP VP
NP V
liest
derivation liest
tree: np1 np2
irgendeiner jedes_Buch
Figure 3: Derivation for (8a)
VP
NP VP
NP V
liest
NP
irgendeiner
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP
NP VP?
jedes Buch
NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
derivation liest
tree: np1 np2 vp1
irgendeiner tjedes_Buch jedes_Buch
Figure 4: Derivation for (8b)
Note that, in the derivation trees, each node rep-
resents a single elementary tree, not a set of el-
ementary trees from the grammar. An MCTAG
derivation tree as defined in (Weir, 1988) with each
node representing a set is available only for tree-
local or set-local MCTAG, not for the MCTAG
variants we need (SN-MCTAG or V-TAG). There-
fore we take the undelying TAG derivation tree
as the derivation structure semantics will be com-
puted on.
4 Semantics of German quantificational
NPs
Because of the generalizations above, the fol-
lowing must be guaranteed: i) Strong quantifiers
scope over the next element in surface order (take
scope where they attach).3 ii) The minimal nu-
clear scope of a weak quantifier is the closest ?un-
moved? element following its base position. Con-
sequently, we need different lexical entries for
weak and strong quantifiers.
We characterize the scope possibilities of a
quantifier in terms of its minimal scope. Consider
first the verb tree for liest ?read? in Fig. 5. In con-
trast to English, MINS is not a global feature since,
depending on the position where the quantifier at-
taches, its minimal scope is different. In the liest-
tree, MINS appears in the feature structure of dif-
ferent nodes, with each MINS value determined in
the following way: the value of MINS at the NP2
address is the label l1 of the verb; the value of
MINS at the NP1 address depends on what is at-
tached at NP2 (see variables 4 and 0 , which in
this case will be identified with each other); and
the value of MINS at the top VP address depends
on what is attached at NP1 ( 5 ).
3But see section 5, where more complex examples show
that this generalization needs to be refined.
76
VP
NP1 VP
NP2 V
liest
l1 : read( 1 , 2 )
3 ? l1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
GL
[
MAXS 3
]
VP1
[
B
[
MINS 5
]
]
NP1
[
T
[
MINS 0
NEXT 5
]
]
VP2
[
T
[
MINS 0
]
B
[
MINS 4
]
]
NP2
[
T
[
MINS l1
NEXT 4
]
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 5: Semantics for liest
NP
l2 : quant(x, 6 , 7 )
l3 : restriction(x)
6 ? l3,
8 ? 7 , 7 ? 9
?
?
?
NP
?
?
?
GL
[
MAXS 8
]
B
[
MINS 9
NEXT l2
]
?
?
?
?
?
?
Figure 6: Quantifiers in base position
The idea is that, when an NP (part) is attached
at a given address, the label of that NP is the new
MINS to be passed up the verb tree; when a trace
(part) is attached instead, the MINS of the verb ad-
dress is passed up unmodified. This feature pass-
ing is technically achieved by articulating the VP
spine with the feature MINS (much like the use
of the P feature in English for adverbial scope in
Kallmeyer and Romero, 2005), and by adding the
feature NEXT for passing between NP substitution
nodes (since substitution nodes do not have T and
B features that allow feature percolations between
mothers and daughters).
The lexical entries for the three types of quanti-
fiers we must distinguish (non-moved quantifiers,
weak moved quantifiers and strong moved quanti-
fiers) are shown in Fig. 6?8. Quantificational NPs
that have not been moved (Fig. 6) receive their
MINS boundary (variable 9 ) simply from their at-
tachment position. Weak and strong quantifiers
that have been moved differ in how their own
MINS is determined: Strong quantifiers (see Fig. 7)
get their MINS from the VP node they attach to,
i.e., from their surface position (see variable 13 ).
In contrast to this, weak quantifiers (see Fig. 8) get
their MINS from the base order position, i.e., from
their trace position (see variable 18 ).
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP
NP VP?
NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
l4 : quant(x, 10 , 11 )
l5 : restriction(x)
10 ? l5,
12 ? 11 , 11 ? 13
?
?
?
NP
?
?
?
GL
[
MAXS 12
]
B
[
MINS 14
NEXT 14
]
?
?
?
?
?
?
?
?
?
VPr
[
B
[
MINS l4
]
]
VPf
[
B
[
MINS 13
]
]
?
?
?
Figure 7: Strong quantifiers that have been moved
As sample analyses consider Fig. 9 and Fig. 10
showing the analyses of (8b) and (8c) where the
accusative object quantifier has been moved. (The
features of the internal VP node are omitted since
they are not relevant here.) In the first case, it is a
strong quantifier, in the second case a weak quanti-
fier. For Fig. 9, we obtain the identifications 12 =
l1 = 4 = 8 , 5 = l2 = 11 (depicted with dotted
lines). Consequently, the only scope order is wide
scope of jedes Buch: l4 > 10 ? l2 > 7 ? l1.
In Fig. 10, we obtain 11 = l1 = 4 = 8 , 5 = l2
which leads to the scope constraints l2 > 7 ? l1
and l4 > 10 ? l1. Consequently, we have
an underspecified representation allowing for both
scope orders.
The analysis proposed in this section has
demonstrated that some features ?in this case
MINS? are global in some languages (e.g. English)
while being local in other languages (e.g. Ger-
man). We take this as further evidence that the
distinction between the two kinds of features, ad-
vocated in (Kallmeyer and Romero, 2005) is em-
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP
NP VP?
NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
l6 : quant(x, 15 , 16 )
l7 : restriction(x)
15 ? l7,
17 ? 16 , 16 ? 18
?
?
?
NP
?
?
?
GL
[
MAXS 17
]
B
[
MINS 18
NEXT 18
]
?
?
?
?
?
?
[
VPr
[
B
[
MINS l6
]
]
]
Figure 8: Weak quantifiers that have been moved
77
l1 : read( 1 , 2 )
?
?
?
?
?
?
?
?
?
?
VP
[
B
[
MINS 5
]
]
NP1
[
T
[
MINS 4
NEXT 5
]
]
NP2
[
T
[
MINS l1
NEXT 4
]
]
?
?
?
?
?
?
?
?
?
?
vp np1 np2
l4 : every(x, 9 , 10 )
l5 : book(x)
9 ? l5, 10 ? 11
l2 : some(x, 6 , 7 )
l3 : person(x)
6 ? l3, 7 ? 8
?
?
?
VPr
[
B
[
MINS l4
]
]
VPf
[
B
[
MINS 11
]
]
?
?
?
[
NP
[
B
[
MINS 8
NEXT l2
]
]] [
NP
[
B
[
MINS 12
NEXT 12
]
]]
Figure 9: Analysis of dass [jedes Buch]1 irgendeiner t1 liest
l1 : read( 1 , 2 )
?
?
?
?
?
?
?
?
?
?
VP
[
B
[
MINS 5
]
]
NP1
[
T
[
MINS 4
NEXT 5
]
]
NP2
[
T
[
MINS l1
NEXT 4
]
]
?
?
?
?
?
?
?
?
?
?
vp np1 np2
l4 : some(x, 9 , 10 )
l5 : book(x)
9 ? l5, 10 ? 11
l2 : every(x, 6 , 7 )
l3 : person(x)
6 ? l3, 7 ? 8
[
VPr
[
B
[
MINS l4
]
]
]
[
NP
[
B
[
MINS 8
NEXT l2
]
]] [
NP
[
B
[
MINS 11
NEXT 11
]
]]
Figure 10: Semantic analysis of dass [irgendein Buch]1 jeder t1 liest
pirically justified.
5 Long-distance scrambling and
quantifier scope
So far we have examined cases where local scram-
bling affects quantifier scope order. In this section,
we will demonstrate how our analysis carries over
to long-distance scrambling.
(9) . . . dass [irgendein Lied]1 Maria
. . . that some songacc Marianom
[fast jedem]2 [ t1 zu singen]
almost everybodydat to sing
versprochen hat
promised has
?that Maria has promised almost everybody to sing
some song?
Q1 > Q2, Q2 > Q1
In (9) both scope orders are possible.
Fig. 11 shows the syntactic analysis for (9). Ac-
cording to the treatment of weak quantifiers pro-
posed above, the minimal nuclear scope of irgen-
dein Lied is determined by the position of the
trace; it is therefore the proposition of singen. As
for fast jedem, its minimal nuclear scope is re-
quired to include the proposition of versprochen
hat. Nothing else is required, and consequently
irgendein can scope over or under fast jedem.
A problematic configuration that can occur with
scrambling concerns cases where two weak quan-
tifiers Q2 and Q3 have been moved with a third
quantifier Q1 preceding them where Q1 is either a
strong quantifier or a weak quantifier in base posi-
tion. Then Q1 has scope over Q2 and Q3 but the
scope order between Q2 and Q3 is unspecified. An
example is (10):
78
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP
NP VP?
irgendein Lied
NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP
NP VP
NP VP
VP? V
versprochen hat
NP
Maria
NP
fast jedem
VP
PRO VP
NP V
zu singen
Figure 11: Derivation for (9)
(10) . . . dass [jeder Mitarbeiter]1
. . . that [every colleague]
[vielen Besuchern]2 [mindestens ein Bild]3
[many visitors]dat [at least one picture]acc
gerne [t2 t3 zu zeigen] bereit war
with pleasure to show willing was
?. . . that every colleague is happy to show at
least one picture to many visitors.?
Q1 > Q2 > Q3, Q1 > Q3 > Q2
The syntactic derivation is shown in Fig. 12.
Such examples are problematic for our analysis:
our approach predicts that Q2 and Q3 have the
same minimal scope, namely the zeigen proposi-
tion, and that the minimal scope of Q1 is the quan-
tifier it precedes, namely Q2. But nothing in the
analysis prevents Q3 from having scope over Q1,
contrary to fact.
This example indicates that the generalization
(i) in section 4 -that the minimal scope of a strong
quantifier is the proposition of the next quantifier
in surface order- needs to be refined. More accu-
rately, the minimal scope of a strong quantifier is
the highest proposition following in surface order.
We propose to model this using the feature NEXT
also in VP nodes. Here NEXT stands for the max-
imal scope of all quantifiers following in surface
order. An attaching weak quantifier has to do two
things: 1. equate the current NEXT feature with
the new MINS that provides the minimal scope for
higher strong quantifiers, and 2. state that NEXT
is its own maximal scope. The corresponding re-
vised lexical entry for moved weak quantifiers is
shown in Fig. 13.
Fig. 14 shows the way the minimal scope for
the unmoved quantifier in (10) is computed from
combining the auxiliary trees of the moved weak
quantifiers with bereit. (The adverb is left aside.)
In the tree of a verb and also in the auxiliary trees
of moved strong quantifiers, an additional feature
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP
NP VP?
NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
l6 : quant(x, 15 , 16 )
l7 : restriction(x)
15 ? l7,
17 ? 16 , 16 ? 18
[
NP
[
B
[
MINS 18
NEXT 18
]
]]
?
?
?
?
VPr
[
B
[
MINS 17
NEXT 17
]
]
VPf
[
T
[
NEXT 17
]
]
?
?
?
?
Figure 13: Moved weak quantifiers (revised)
NEXT is added, linked to the bottom of VP nodes.
The value of this feature is required to be higher
than the value of the bottom MINS at that position.
Whenever a moved strong quantifier adjoins, noth-
ing happens with this NEXT feature. Moved weak
quantifiers take the NEXT feature as their maximal
scope and pass it as the new MINS. This is how
in Fig. 14, the final MINS at the top of the root
of the leftmost moved weak quantifier contains all
moved quantifiers and is passed to the NP node
as new MINS limit. A (weak or strong) quantifier
substituting into the NP slot takes this new MINS
as its minimal scope. Consequently, it scopes over
both moved weak quantifiers.
6 Conclusion
It has been shown that, although quantifier scope
is usually read off surface word order in German,
ambiguities can arise from movement of weak
quantifiers. We have developed an MCTAG anal-
ysis using traces. In our approach, the scope pos-
sibilities of a quantifier are characterized in terms
of its minimal scope. In contrast to English, MINS
in German is not global but depends on the po-
79
NP
jeder Mitarbeiter
VP
gerne VP?
VP
NP VP
VP? V
bereit war
VP
PRO VP
NP VP
NP V
zu zeigen
?
?
?
?
?
VP
NP VP?
mindestens ein Bild
NP
?
?
?
?
?
?
?
?
?
?
?
VP
NP VP?
vielen Besuchern
NP
?
?
?
?
?
?
Figure 12: Derivation for (10)
l1 : willing( 1 , 2 )
4 ? 3 , 7 ? 6
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VPr
[
B
[
MINS 3
NEXT 4
]
]
NP1
[
T
[
MINS 5
NEXT 3
]
]
VP
?
?
?
T
[
MINS 5
]
B
[
MINS 6
NEXT 7
]
?
?
?
VPf
[
T . . .
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
vp
l2 : q3(x, 9 , 10 )
l3 : picture(x)
9 ? l3,
12 ? 10 , 10 ? 11
?
?
?
?
VPr
[
B
[
MINS 12
NEXT 12
]
]
VPf
[
T
[
NEXT 12
]
]
?
?
?
?
vpr
l4 : q2(y, 13 , 14 )
l5 : visitor(y)
13 ? l5,
16 ? 14 , 14 ? 15
?
?
?
?
VPr
[
B
[
MINS 16
NEXT 16
]
]
VPf
[
T
[
NEXT 16
]
]
?
?
?
?
q2 = many, q3 = at_least_one
Figure 14: Attaching the moved weak quantifiers
in (10)
sition of the quantifier. The minimal scope of
weak and strong quantifiers is determined differ-
ently: The minimal scope of a moved weak quan-
tifier depends on its trace; the minimal scope of a
moved strong quantifier depends on the position of
the moved material.
Acknowledgments
For fruitful discussions of the work presented in
this paper, we want to thank Timm Lichte and
Wolfgang Maier. Furthermore, we are grateful to
three anonymous reviewers for helpful comments.
References
Tilman Becker, Aravind K. Joshi, and Owen Rambow.
1991. Long-distance scrambling and tree adjoining
grammars. In Proceedings of ACL-Europe.
Werner Frey. 1993. Syntaktische Bedingungen f?r
die semantische Interpretation: ?ber Bindung, im-
plizite Argumente und Skopus. studia grammatica.
Akademie Verlag, Berlin.
Daniel Gallin. 1975. Intensional and Higher-Order
Modal Logic with Applications to Montague Seman-
tics. North Holland mathematics studies 19. North-
Holland Publ. Co., Amsterdam.
Laura Kallmeyer and Maribel Romero. 2004. LTAG
Semantics with Semantic Unification. In Proceed-
ings of TAG+7, pages 155?162, Vancouver.
Laura Kallmeyer and Maribel Romero. 2005. Scope
and Situation Binding in LTAG using Semantic Uni-
fication. Submitted to Research on Language and
Computation. 57 pages., December.
Laura Kallmeyer. 2005. Tree-local multicomponent
tree adjoining grammars with shared nodes. Com-
putational Linguistics, 31(2):187?225.
Tibor Kiss. 2000. Configurational and Relational
Scope Determination in German. In Tibor Kiss
and Detmar Meurers, editors, Constraint-Based
Approaches to Germanic Syntax, pages 141?176.
CSLI.
Winfried Lechner. 1998. Two Kinds of Reconstruc-
tion. Studia Linguistica, 52(3):276?310.
Owen Rambow. 1994. Formal and Computational
Aspects of Natural Language Syntax. Ph.D. thesis,
University of Pennsylvania.
K. Vijay-Shanker and Aravind K. Joshi. 1988. Feature
structures based tree adjoining grammar. In Pro-
ceedings of COLING, pages 714?719, Budapest.
David J. Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
80
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 81?90,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Licensing German Negative Polarity Items in LTAG
Timm Lichte
University of Tu?bingen
Collaborative Research Center 441
timm.lichte@uni-tuebingen.de
Laura Kallmeyer
University of Tu?bingen
Collaborative Research Center 441
lk@sfs.uni-tuebingen.de
Abstract
Our paper aims at capturing the distri-
bution of negative polarity items (NPIs)
within lexicalized Tree Adjoining Gram-
mar (LTAG). The condition under which
an NPI can occur in a sentence is for it to
be in the scope of a negation with no quan-
tifiers scopally intervening. We model this
restriction within a recent framework for
LTAG semantics based on semantic uni-
fication. The proposed analysis provides
features that signal the presence of a nega-
tion in the semantics and that specify its
scope. We extend our analysis to mod-
elling the interaction of NPI licensing and
neg raising constructions.
1 Introduction
1.1 Negative Polarity Items
NPIs are distributionally restricted to linguistic en-
vironments that exhibit a trigger for negativity (see
e.g., Ladusaw, 1980; Linebarger, 1987; Zwarts,
1997). More precisely, NPIs seek to be placed
within the scope of a negative operator at the level
of semantics. We say that the NPI has to be li-
censed by an exponent of negativity, the licenser.
Examples in German can be found in (1)?(5) (the
NPI is underlined while the licenser is in bold
face).
(1) a. Hans
Hans
war
was
nicht
not
sonderlich
very
zufrieden
happy
mit
with
seiner
his
Arbeit
work
b.*Hans war sonderlich zufrieden mit seiner
Arbeit
(2) a. Er
he
hat
has
es
it
nicht
not
wahrhaben
accept to be true
wollen
want
(?He did not want to accept it to be true?)
b.*Er hat es wahrhaben wollen.
(3) a. Es
it
schert
bothers
ihn
him
nicht
not
(?He does not give a damn about it?)
b.*Es schert ihn.
(4) a. Du
you
brauchst
need
diese
these
Bu?cher
books
nicht
not
zu
to
lesen
read
(?You need not read these books?)
b.*Du brauchst diese Bu?cher zu lesen.
(5) a. Niemand
nobody
hat
has
auch nur einen Cent
even one cent
gespendet.
donated
(?Nobody has donated any cent at all.?)
b.*Auch nur einen Cent hat niemand
gespendet.
We will mainly be concerned with verbal NPIs
such as wahrhaben wollen (?accept to be true?) and
scheren (?to give a damn about?). Another group
of NPIs we will pay closer attention to are min-
imizers, here exemplified by auch nur ein Cent
(?any Cent at all?). They are quantifiers denot-
ing the bottom line of a scale and therefore show
affinity with negation due to pragmatic reasons.
Furthermore, minimizers as quantifiers are subject
to particular position restrictions with respect to
negation (see next section). A group of NPIs we
will leave aside in this paper, however, is that of
adjectival NPIs such as sonderlich (?very?).
1.2 NPI Licensers
Various items and constructions can license NPIs.
Besides the more obvious ones such as not, no-
body and never, also (among others) few, re-
81
strictors of universal quantifiers, conditional ante-
cendents and questions can license at least some
of the NPIs. There has been much controversy
about what the characterizing logical property of
licensers is. One proposal is based on the notion
of downward entailment (DE, Ladusaw, 1980),
which holds for operators whose truth value is per-
sistent over specification. While the DE property
can be found in most of the licensers, there are
some, such as questions, where it is hard to detect
(see van der Wouden, 1997 for an overview).1
In our proposal we don?t make use of DE as an
NPI licensing criterion. Instead we only require
the negation operator (?) in the semantic represen-
tation as licensing feature. We thereby restrict our-
selves to triggers of ?classic? negation; we go even
further and only implement non-contrastive nega-
tion. We use this term after Jacobs (1982) where
non-contrastive negation (NCN) and contrastive
negation (CN) are examined for German. They
differ in that sentences with CN can be extended
by a but-phrase (Sondern-Phrase) while adding a
but-phrase to sentences with NCN gives odd re-
sults. Put differently, CN focuses on parts of a
sentence while NCN does not.2 Whether CN or
NCN is available, is indicated by intonation and
position of the negative element. However, am-
biguous indications are possible. In our analysis,
we leave aside intonation and stick to unambigu-
ous NCN as far as possible.
1.3 Semantic Scope and Range of Licensing
It is not sufficient for an NPI to just co-occur
with a licenser in the same sentence; it has to be
in the licenser?s scope. Furthermore, additional
constraints have been proposed in the literature.
One of the most extensively discussed requires the
NPI to be c-commanded by the licenser on sur-
face structure (c-command constraint, Ladusaw,
1980). As Hoeksema (2000) points out, the c-
command constraint is too restrictive when ap-
plied to languages with a considerably freer word
order than English, e.g. Dutch and German (see
(4) for an example that does not respect the c-
command constraint). He also points out that
the need for the c-command constraint only arises
1Giannakidou (1997) therefore proposes the idea of non-
veridicality as being the basic logical property of NPI-
licensers - eventually facing the problem of being less restric-
itive than required.
2If CN is available NPIs can only be licensed in the part
focused by CN.
from capturing the distribution of minimizers. All
other NPIs obey a simple scope constraint in terms
of Linebarger?s immediate scope constraint (ISC,
Linebarger, 1980; Linebarger, 1987), namely that
no other propositional operators (i.e. ?logical ele-
ments? that are capable of entering into scope am-
biguities) may intervene between the licenser and
the NPI on LF.
While the ISC seems to hold for quantifiers,
quantificational adverbs and operators that con-
join propositions such as because, there are in
fact some operators that may scopally intervene.
Among them are non-quantificational adverbs,
minimizers and modals, as in (6):
(6) Peter
Peter
hat
has
keinen
no
Finger
finger
ru?hren
move
mu?ssen.
must
(?Peter didn?t need to lift a finger.?)
In (6), the negation always has wide scope with
respect to the modal mu?ssen (must), hence mu?ssen
intervenes between negation and NPI, but still the
sentence is grammatical.
Thus, our criterion for an NPI to be licensed is
1. to be in the scope of a negation that is seman-
tically interpreted in the same finite clause, and
2. not to allow regular quantifiers to scopally in-
tervene between negation and NPI. In this paper,
we will also refer to these criterions as immedi-
ate scope.3 Minimizers seem to add a third crite-
rion, namely that the licenser has to syntactically
c-command the minimizer.
Independently from the ISC, one has to keep in
mind that negative elements in German are able to
cancel each other out, that is to constitute double
negation. We will come back to this briefly in sec-
tion 3.
1.4 Neg Raising Constructions
We extend our analysis to so-called neg raising
(NR, cf. Horn, 1978) constructions because there
are interesting interactions between NPI licensing
and neg raising.
3Note that with this approach, one negation can even li-
cense several NPIs as in (i):
(i) Kein
no
Schu?ler
pupil
hat
has
jemals
ever
in
in
den
the
Ferien
holidays
sonderlich
particularly
viel
much
gelernt.
learned
(?No pupil has ever learned very much during the hol-
idays.?)
82
An example of a NR-verb is glauben (?believe?)
in (7).
(7) Hans
Hans
glaubt
believes
nicht,
not
dass
that
Peter
Peter
kommt.
comes
(?Hans does not believe that Peter is com-
ing.?)
The negation can either take scope at its surface
position, i.e., scope over glauben, or it can scope
within the embedded sentence. Hence, two inter-
pretations are generally available: (a) ?believe(p)
and (b) believe(?p). The second reading is possi-
ble only with NR-verbs.
In LTAG, lexical material is generated at its sur-
face structure position, there is no movement out-
side the lexicon. Therefore it is natural to assume
with respect to sentences as (7), that the negation
is syntactically generated in the matrix clause and
that neg raising attitude verbs such as glauben al-
low for semantic lowering of an attached negation.
This negation then receives wide scope within the
sentential complement. In this, we follow the
HPSG analysis proposed in Sailer (to appear).
The presence of an NPI in the embedded sen-
tence as in (8) forces the negation to scope un-
der the bridge verb, that is the (b)-interpretation
is chosen.
(8) Hans
Hans
glaubt
believes
nicht,
not
dass
that
Peter
Peter
sonderlich
very
glu?cklich
happy
sein
be
wird.
will
(?Hans does not believe that Peter will be
very happy.?)
2 The LTAG Semantics Framework
We use the Kallmeyer and Romero (2005) frame-
work for semantics. Each elementary tree is linked
to a semantic representation containing Ty2 terms
and scope constraints. Ty2 terms are typed ?-
terms providing individuals and situations as basic
types. The terms can be labeled, and they can con-
tain meta-variables. The scope constraints are sub-
ordination constraints of the form x ? y (?y is a
component of x?) with x and y being either propo-
sitional labels or propositional meta-variables.
The semantic representations are equipped with
feature structure descriptions. Semantic compu-
tation is done on the derivation tree and consists
of certain feature value equations between mother
and daughter nodes of edges in the derivation tree.
l1 : laugh( 1 )
?
?
?
NP
[
GLOBAL
[
I 1
]
]
VP
[
B
[
P l1
]
]
?
?
?
np vp
john(x) l2 : always( 3 ),3 ? 4
[
GLOBAL
[
I x
]
] ?
?
?
VPr
[
B
[
P l2
]
]
VPf
[
B
[
P 4
]
]
?
?
?
Figure 1: LTAG semantics of (9)
The meta-variables from the semantic representa-
tions can occur in the feature structure descrip-
tions. In this case they can receive values follow-
ing from the feature value equations performed on
the derivation tree.
As an example see Fig. 1 showing the deriva-
tion tree for (9) with semantic representations and
semantic feature structure descriptions as node la-
bels.
(9) John always laughs
The additional feature equations in this example
are depicted with dotted links. They arise from
top-bottom feature identifications parallel to the
unifications performed in FTAG (Vijay-Shanker
and Joshi, 1988) and from identifications of global
features. They yield 1 = x and 4 = l1. Apply-
ing these identities to the semantic representations
after having built their union leads to (10). The
constraint 3 ? l1 states that l1 : laugh(x) is a
component of 3 .
(10)
john(x), l2 : always( 3 ),
l1 : laugh(x),
3 ? l1
We assume a scope window for quantifiers
specifying an upper boundary MAXS (?maximal
scope?) and a lower boundary MINS (?minimal
scope?) for the nuclear scope. In this we follow
Kallmeyer and Romero (2005). In addition, how-
ever, we make use of the feature MINP (?minimal
proposition?). In their analysis, which was devel-
oped for English, MINS and MINP are the same, in
other words, there is no separate MINP feature. In
German, the minimal scope of a quantifier seems
to depend not only on the verb the quantifier at-
taches to but also on other factors (see Kallmeyer
83
and Romero, 2006 in this volume for the influ-
ence of word order on quantifier scope in Ger-
man). This justifies the assumption that German
MINS if different from English MINS. The scope
order is of course such that MAXS is higher than
MINS which is in turn higher than MINP.
In order to deal with NPI-licensing we intro-
duce three new features: a global and a local NEG-
feature and the global feature N-SCOPE. Not sur-
prisingly, the latter represents the scope of a nega-
tive operator, while the former is needed to check
the presence of a negative operator. The next sec-
tion offers detailed examples.
3 The Analysis of Licensers
In this section we give the elementary trees for
non-contrastive nicht (not) and niemand (nobody).
A strong trigger for NCN is nicht attached to
the verb. Based on the topological field theory
for German the attachment takes place at the right
satzklammer, a position that together with the left
satzklammer contains the verbal expression.4 As
an example see the derivation for (11) in Fig. 2.
(11) Peter
Peter
ruft
calls
Hans
Hans
nicht
not
an
PART
(?Peter does not call Hans?)
Similar to Gerdes (2002), the VP nodes carry fea-
tures VF (?Vorfeld?), LK (?Linke Satzklammer?),
MF (?Mittelfeld?), and RK (?Rechte Satzklammer?)
for the topological fields. In German, the vorfeld,
the position preceding the left satzklammer, must
be filled by exactly one constituent. We guaran-
tee this with the feature VF: The different VF fea-
tures at the highest VP node in the tree for ruft an
make sure that adjunction to the vorfeld is obliga-
tory. At the same time, elements adjoining to any
of the topological fields (see the tree for Peter)
have a foot node feature VF = ? and have equal
top and bottom features VF at their root. When
4Exceptions to this generalization are found with verbs
that express movement:
(i) a. Peter
Peter
geht
goes
nicht
not
ins
to the
Kino.
movies
(?Peter does not go to the movies?)
b. *...
...
dass
that
Peter
Peter
ins
to the
Kino
movies
nicht
not
geht.
goes
(?... that Peter does not go to the movies?)
Here the NC-nicht is always attached to the adverb that ex-
presses the direction or target of the movement, thus not to the
second satzklammer directly. For this paper, we leave these
cases aside.
VP
[V F+]
[V F?]
V
[LK+, RK?]
VP
[V F?,MF+]
ruft NPnom VP
[V F?,MF+]
NPacc V
[LK?, RK+]
an
NPacc
Hans
V
nicht V [RK+]?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
VP
[V F 10 ]
[V F 10 ]
NP VP[V F?]?
Peter
NPnom
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Syntactic analysis for (11)
adjoining to the vorfeld, these receive values +.
Consequently, further adjunctions of similar ele-
ments at the new root node are not possible. An
adjunction at the foot node of the auxiliary tree of
the vorfeld element can be excluded by some other
feature. This guarantees that exactly one element
gets adjoined into the vorfeld.
Note that we consider the base position of the
subject NP being in the mittelfeld and consider the
subject as being moved into the vorfeld. Alterna-
tively, any other element could be moved in to the
vorfeld instead.
The semantic combination of nicht and ruft an
is shown in Fig. 3.
The MINP feature from ruft indicates the propo-
sition contributed by the verb which is the mini-
mal proposition of the whole elementary tree. It is
included in the scope of all operators (quantifiers,
negation, modals, . . . ) attaching to this verb (An
exception is of course neg raising where the scope
of the negation does not include the MINP value of
the NR-verb.).
The unifications between the two feature struc-
tures in Fig. 3 are depicted with dotted lines. They
yield in particular 9 = 7 , therefore, with con-
straint 7 ? l1, l1 is in the scope of the negation.
The presence of a negation is indicated by a
global NEG = yes. In case there is no negation,
we have to make sure we obtain NEG = no and not
just an unspecified NEG value. Therefore, the VP
spine is articulated with non-global NEG features
that switch from no to yes once a negation occurs.
Here this is the case at node position V, conse-
quently 6 = 5 = 4 = 3 = yes. The topmost
84
l1 : call( 1 , 2 )
7 ? l1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
GLOBAL
?
?
N-SCOPE 7
MINP l1
NEG 3
?
?
VP?
[
T
[
NEG 3
]
B
[
NEG 4
]
]
VP2
[
T
[
NEG 4
]
B
[
NEG 5
]
]
VP22
[
T
[
NEG 5
]
B
[
NEG 6
]
]
V
[
T
[
NEG 6
]
B
[
NEG no
]
]
NPnom
[
GLOBAL
[
I 1
]
]
NPacc
[
GLOBAL
[
I 2
]
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
v
l2 : ? 9
?
?
?
Vr
[
B
[
NEG yes
]
]
Vf
[
GLOBAL
[
N-SCOPE 9
]
]
?
?
?
Figure 3: Semantic computation for ... ruft ...
nicht an
NEG then becomes the global NEG.
Cases of double negation, though not consid-
ered here, could be captured by assuming that each
negation on the verbal spine makes the value of
the local NEG feature switch (from no to yes or, if
there was already negation, from yes to no). This
way, double negation would lead to a global NEG
feature with value no.
The negative quantifier niemand has the distri-
bution of an NP. The elementary trees in Fig. 4
for niemand reflect the ?? reading which is pre-
ferred by an analysis assuming that the NPI must
be in the scope of a negation with no quantifiers in-
tervening. The features NEG, MINP and N-SCOPE
work in the same way as in the case of nicht. The
global I feature linked to the initial tree with the
trace passes the argument variable to the verb.
Note that this is an analysis for the case where
niemand is ?moved?. If niemand is in base posi-
tion, the lexical item comes with an initial tree that
is substituted at the corresponding NP slot. How-
ever, since the NEG-feature can only be switched
to yes by adjoining an auxiliary tree carrying
negation to a VP node, even in these cases we
need an additional VP auxiliary tree contributing
the sentential negation.5
5Another option would be to let the initial tree of niemand
directly access the semantic features of a VP node.
?
?
?
?
?
?
?
VP
[V F 20 ]
[V F 20 ]
NP VP
[V F?]
?
niemand
NPnom
?
?
?
?
?
?
?
?
Semantics:
VP
[V F 20 ]
[V F 20 ]
NP VP
[V F?]
?
niemand
l2 : forall(x, 7 , 8 ),
l3 : person(x),
l4 : ? 9 ,
7 ? l3, 8 ? l4
?
?
?
VPr
[
B
[
NEG yes
]
]
VPf
[
GLOBAL
[
N-SCOPE 9
]
]
?
?
?
NPnom
? [
GLOBAL
[
I x
]
]
Figure 4: Lexical entry for niemand
4 The Analysis of NPIs
For this paper we restrict ourselves to verbal NPIs
and minimizers.
As an example for a verbal NPI consider
scheren (?to give a damn about sth.?) in (3). Its
lexical entry is shown in Fig. 5. As in the case of
ruft, the verbal spine is articulated with the NEG
feature. Furthermore, GLOBAL contains the re-
quirement of a negation (NEG = yes). In partic-
ular, the topmost NEG feature on the verbal spine
is yes while the value of the lowest NEG feature is
no. This means that at some point on the verbal
spine a negation must be added that switches the
value from no to yes.
Concerning the scope relation between NPI and
negation, the following should hold: 1. the NPI
must be in the scope of the negation, and 2. quan-
tifiers must not intervene between negation and
NPI.
The first condition is guaranteed with constraint
9 ? l1.
In order to capture the second restriction, the
distinction between MINS and MINP allows us
to draw a border line between the domain where
quantifiers can take scope and the domain where
the negation and the NPI are positioned. Other
scope taking operators (modals, adverbs, . . . )
are not concerned by this limit. This border line
is the MINS value, and the crucial NPI-specific
constraint is 8 ? 9 stating that the negation must
85
VP
[V F+]
[V F?]
V
[LK+, RK?]
VP
[V F?,MF+]
schert NPnom VP
[V F?,MF+]
NPacc V
[LK?, RK+]
?
l1 : scheren( 1 , 2 )
7 ? 8 , 8 ? l1,
8 ? 9 , 9 ? l1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
GLOBAL
?
?
?
?
?
?
MINP l1
MINS 8
MAXS 7
N-SCOPE 9
NEG yes
?
?
?
?
?
?
VP?
[
T
[
NEG yes
]
B
[
NEG 4
]
]
VP2
[
T
[
NEG 4
]
B
[
NEG 5
]
]
VP22
[
T
[
NEG 5
]
B
[
NEG 6
]
]
V
[
T
[
NEG 6
]
B
[
NEG no
]
]
NPnom
[
GLOBAL
[
I 1
]
]
NPacc
[
GLOBAL
[
I 2
]
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 5: Lexical entry for schert
scope under the minimal scope of all quantifiers.
The scope relations then can be summarised as in
Fig. 6.
no NPI involved:
MAXS
MINS ?
MINP
NPI involved:
MAXS
MINS
?
NPI
MINP
Figure 6: Scope relations of MAXS, MINS and ?
with and without the involvement of an NPI.
As mentioned in 1.3 minimizers show a more
restrictive distribution than verbal NPIs. In addi-
tion to the two licensing conditions of verbal NPIs
stated above minimizers also obey a third licensing
condition in German: the negation must precede
the minimizer in the same clause or the negation
must have wide scope with respect to the sentence
containing the minimizer, such as in NR construc-
tions. Consider the minimizer auch nur einen Cent
(?any cent at all?) in example (5) and its proposed
lexical entry in Fig. 7.
?
?
?
?
?
?
?
VP
NP VP?
auch nur einen Cent
NPnom
?
?
?
?
?
?
?
?
l1 : exists(x, 1 , 2 )
l2 : Cent(x)
1 ? l2, 2 ? 6 , 4 ? l1,
5 ? 4
?
?
?
?
?
?
?
?
?
?
VPf
?
?
?
?
?
?
?
?
?
GLOBAL
?
?
?
?
N-SCOPE 4
MINS 5
MINP 6
NEG yes
?
?
?
?
T
[
NEG no
]
B
[
NEG no
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
[
GLOBAL
[
I x
]
]
Figure 7: Lexical entry for auch nur einen Cent
We propose a multicomponent lexical entry for
minimizers here, since they have to access the se-
mantic feature structure of the VP spine, and there-
fore have to be adjoined. This is different from
verbal NPIs (that are part of the VP spine by def-
inition), but similar to the negative quantifier nie-
mand. As for verbal NPIs the presence of a nega-
tion is ensured by the global NEG feature, that is
required to be yes. The scope condition is satis-
fied by the constraints 4 ? l1 and 5 ? 4 : the for-
mer one ensures that the semantic contribution of
auch nur einen Cent is part of N-SCOPE, while the
latter one prohibits any intervening regular quanti-
fier (by requiring N-SCOPE to be a subexpression
of MINS).6
In order to meet the third condition we have to
make sure that the negation appears somewhere to
the left of the minimizer. In other words, the nega-
tion is not attached between the right satzklammer
and the minimizer, but somewhere else (as ensured
by the global NEG feature). Remember that the
position of a negation is signaled by the local NEG
feature on the VP spine and its switch from no to
yes. One way to exploit this is to let the mini-
mizer semantically specify the VP node to which
6Note that, though being quantifiers, minimizers are not
concerned by the MAXS-MINS scope window. Instead, their
scope window is specified by N-SCOPE as upper limit and
MINP as lower limit (the latter results from constraint 2 ? 6 .
86
it can be attached. This is accomplished by the
VPf feature in the lexical entry for auch nur einen
Cent, where the local NEG is required to be no,
while the global NEG is yes. Thereby it is guaran-
teed that somewhere between the position where
the adjunction of the minimizer takes place and the
maximal projection of the VP the NEG feature has
to switch to yes with the aid of a negative item.
5 The Analysis of Neg Raising
Now let us turn to the neg raising examples from
section 1.4. Attitude verbs that optionally offer
neg raising are mapped onto two lexical entries
representing a non-NR- and a NR-reading. In
the latter, the negation takes wide scope within
the embedded clause. In other words, quantifiers
cannot scopally intervene between the embedding
verb and the negation. This is exemplified in (12).
(12) Peter
Peter
glaubt
believes
nicht,
not
dass
that
jeder
each
seiner
of his
Freunde
friends
kommen
come
wird.
will.
(?Peter does not believe that each of his
friends will come?)
The NR-reading (believes(p, ? ? ? ? ? ? ?) does not
exclude that Peter believes that some of his friends
will come. A reading where Peter believes that
none of his friends will come is not available. In
other words, the quantifier has to scope under the
negation.
The lexical entry for glaubt with the NR-
reading is shown in Fig. 8. In the syntax we as-
sume a substitution node for the sentential com-
plement. Long-distance dependencies are then
analysed with multicomponents. This choice was
motivated because in German, taking into ac-
count scrambling, more movement-based word or-
der variations are possible than in English. For
these we need multicomponents anyway (see the
elementary tree set for niemand), and then senten-
tial complements might be treated in parallel. The
S substitution node carries a syntactic feature NR
indicating that this is a neg raising construction.
The lowering of the negation is expressed as fol-
lows: the N-SCOPE of glaubt (variable 7 ), i.e., the
scope of the attaching negation, does not contain
the MINP of glaubt as in non-NR readings. In-
stead, it contains the MAXS (variable 9 ) of the em-
bedded sentence (constraint 7 ? 9 ). This MAXS
is usually contained in the propositional argument
VP
[V F+]
[V F?]
V
[LK+, RK?]
VP
[V F?,MF+]
glaubt NPnom VP
[V F?,MF+]
V
[LK?, RK+]
S
[nr+]
?
l1 : believe( 1 , 8 )
8 ? 7
7 ? 9
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
GLOBAL
?
?
MINP l1
N-SCOPE 7
NEG no
?
?
VP?
[
T
[
NEG yes
]
B
[
NEG 4
]
]
VP1
[
T
[
NEG 4
]
B
[
NEG 5
]
]
VP12
[
T
[
NEG 5
]
B
[
NEG 6
]
]
V
[
T
[
NEG 6
]
B
[
NEG no
]
]
S
[
GLOBAL
[
N-SCOPE 7
MAXS 9
]
]
NPnom
[
GLOBAL
[
I 1
]
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 8: Lexical entry for glaubt
of believe (see Kallmeyer and Romero, 2005); in
this special neg raising entry we even require the
N-SCOPE to be contained in this argument (con-
straint 8 ? 7 ). The MAXS feature 9 marks the
upper limit for the scope of all quantifiers occur-
ring inside the embedded clause. Consequently,
wide scope of the lowered negation with respect
to the embedded sentence is ensured.
The lexical entry for glaubt with NR-reading
also has to make sure that a negative element is at-
tached to its verbal spine. In this respect its seman-
tic feature structure resembles the one of a ver-
bal NPI, that is the NEG value has to be switched
to yes by adjunction. However, semantically the
negation is interpreted in the embedded sentence
and NPIs cannot be licensed in the matrix clause.
Therefore, the value of the global NEG feature is
no.
The complementizer of the embedded clause
takes care of setting the value of the embedded
global NEG to yes by identifying the NEG feature
of its S node with the topmost NEG feature on the
87
verbal spine of the embedded clause. In a non-NR-
reading, the complementizer only passes the NEG
value upwards, i.e., the global NEG of the embed-
ded clause specifies whether a negation is present
in the embedded clause.
S [nr+]
Comp VP [V F+]?
dass
[
S
[
T
[
NEG yes
]
]
]
Figure 9: Complementizer dass in neg raising con-
struction
With this analysis, if a NR-verb embeds an NPI
as in (8), the NPI requires the NR-reading; oth-
erwise the global NEG feature of the embedded
clause is no.
Next, we want to give an example derivation
of a sentence that contains an unlicensed NPI and
which amounts to contradicting scope constraints.
It concerns the following sentence:
(13) *Hans
Hans
glaubt
believes
nicht,
not,
dass
that
es
it
jeden
everybody
schert.
bothers
(?Hans doesn?t believe that everybody
gives a damn about it.?)
The NPI schert is not licensed due to the inter-
vening quantifier jeden (every). The defective
dervation of (13) is shown in Fig. 10. Syntacti-
cally, the S leaf of the Hans glaubt nicht tree
is substituted by the dass es schert tree and the
jeder tree is substituted into the dass es schert
tree. This works fine. In the semantic represen-
tation, however, we observe a clash of the scope
constraints. Remember that we analyse the ver-
bal NPI schert as requiring immediate scope, that
is MINS ? N-SCOPE. On the other side, the
NR-verb glauben demands the negation to have
wide scope with respect to the embedded sentence,
hence N-SCOPE ? MAXS (constraint l2 ? 3 ) . If
we put these two constraints together we obtain
the constraint MINS = MAXS, which means that
the area where quantifiers take scope (the MAXS-
MINS window) is empty and hence there cannot
be any quantifiers. A quantifer such as jeden is
then ruled out due to two semantic constraints it
contributes: its semantic content is a subexpres-
sion of MAXS (constraint 3 ? l3) and MINS is
a subexpression of its nuclear scope (constraint
6 ? l2). However, this can only hold if MINS
6= MAXS which is not true for (13) as has been
shown.
Hans glaubt nicht
l1 : believe(Hans, 1 )
l5 : ?l2
1 ? l2, l2 ? 3
?
?
?
?
?
?
?
GLOBAL
?
?
MINP l1
N-SCOPE 7
NEG no
?
?
Sf
[
GLOBAL
[
N-SCOPE 7
MAXS 3
]
]
?
?
?
?
?
?
?
dass es schert
l2 : es schert( 4 , es)
7 ? l2
?
?
?
?
GLOBAL
?
?
?
?
MINP l2
MINS 7
N-SCOPE 7
NEG yes
?
?
?
?
?
?
?
?
jeden
l3 : every(x, 5 , 6 )
l4 : person(x)
5 ? l4, 3 ? 6 , 6 ? 7
3 ? l3
?
?
?
?
GLOBAL
[
I x
]
NP
[
GLOBAL
[
MINS 7
MAXS 3
]
]
?
?
?
?
Figure 10: Defective derivation tree for Hans
glaubt nicht, dass es jeden schert
6 Conclusion and further research
We propose an LTAG analysis of the distribution
of German NPIs. The crucial criterion for an NPI
is the requirement to be in the scope of a nega-
tion that is semantically in the same finite clause
such that no quantifier can scopally intervene be-
tween negation and NPI. Technically we achieved
this using the features NEG and N-SCOPE, that sig-
nal the presence of a negation and make its imme-
diate scope available for the NPI. 7 The specific
constraints for quantifiers when occurring with
7Note however, that, even though we have called the fea-
ture signalling the presence of a potential NPI licenser NEG,
we might as well call it differently and give it a different
meaning (for example, encoding downward entailment in-
stead of negation). The licensing mechanism and the way this
feature is used could stay the same. In this sense our analysis
is independent from the concrete logical characterization of
NPI licensers.
88
NPI licensing negations are obtained by a distinc-
tion between the feature MINS characterizing the
lower boundary of quantifier scope and the mini-
mal proposition contributed by a verb that charac-
terizes the lower boundary for the scope of nega-
tions.
We think LTAG is particularly well suited to de-
scribe this phenomenon since the relation between
licenser and licensee can be localized within sin-
gle elementary trees.8 The only exception are neg
raising constructions where the licensing property
needs to be passed down to the embedded clause.
This is not non-local either and can be easily mod-
elled in LTAG. This shows that LTAG?s extended
domain of locality has advantages not only for
syntax (see Kroch, 1987) but also for semantics.
The analyses discussed in this paper have
demonstrated the usefulness of semantic feature
structure descriptions that specify the combination
possibilities of semantic representations and that
are separated from the semantic representations
themselves. On the one hand the semantic features
encode the contributions of the semantic represen-
tations to functional applications. I.e., they state
which elments are contributed as possible argu-
ments for other semantic expressions and which
arguments need to be filled. They thereby simu-
late lambda abstraction and functional application.
On the other hand they also serve to model the
scopal behaviour of different operators and to cap-
ture the different boundaries for scope. The com-
bination of LTAG?s extended domain of locality
with a semantics using feature structure unifica-
tion enables us to capture these constraints within
a mildly context-sensitive framework: The struc-
tures underlying the computation of syntax and se-
mantics are the context-free derivation trees.
One line of further research we want to pursue is
an extension of the proposed analysis to adjectival
and adverbial NPIs. We already started working
on this. But for reasons of space we left this out in
this paper.
Acknowledgements
For many inspiring discussions of the topics
treated in this paper, we are grateful to our col-
leagues Wolfgang Maier, Frank Richter, Manfred
8In the HPSG analysis from Soehn (2006) for example,
where we do not have an extended domain of locality, one
has to specify explicitely that the licenser of an NPI must be
found within the next complete clause containing the NPI.
Sailer and Jan-Philipp So?hn. Furthermore, the pa-
per benefitted a lot from the useful comments of
three anonymous reviewers.
References
Kim Gerdes. 2002. DTAG? In Proceedings of TAG+6
Workshop, pages 242?251. Venice.
Anastasia Giannakidou. 1997. The Landscape of Po-
larity Items. Ph.D. thesis, Rijksuniversiteit Gronin-
gen.
Jack Hoeksema. 2000. Negative Polarity Items: Trig-
gering, Scope and C-Command. In Laurence Horn
and Yasuhiko Kato, editors, Negation and Polarity,
pages 115?146. Oxford University Press, Oxford.
Laurence R. Horn. 1978. Remarks on Neg-Raising.
In Peter Cole, editor, Pragmatics, pages 129?220.
Academic Press, New York, San Francisco, London.
Joachim Jacobs. 1982. Syntax und Semantik der Nega-
tion im Deutschen. Wilhelm Fink Verlag, Mu?nchen.
Laura Kallmeyer and Maribel Romero. 2005. Scope
and Situation Binding in LTAG using Semantic Uni-
fication. Research on Language and Computation.
57 pages, submitted.
Laura Kallmeyer and Maribel Romero. 2006. Quan-
tifier Scope in German: An MCTAG Analysis.
In Proceedings of The Eighth International Work-
shop on Tree Adjoining Grammar and Related For-
malisms (TAG+8), Sydney, Australia, July.
Anthony S. Kroch. 1987. Unbounded Dependen-
cies and Subjacency in a Tree Adjoining Grammar.
In A. Manaster-Ramer, editor, Mathematics of Lan-
guage, pages 143?172. John Benjamins, Amster-
dam.
William Ladusaw. 1980. Polarity Sensitivity as Inher-
ent Scope relations. Garland Press, New York.
Marcia Linebarger. 1980. The Grammar of Negative
Polarity. Ph.D. thesis, MIT. cited after the repro-
duction by the Indiana University Linguistics Club,
Indiana, 1981.
Marcia Linebarger. 1987. Negative Polarity and
Grammatical Representation. Linguistics and Phi-
losophy, 10:325?387.
Manfred Sailer. to appear. ?Don?t Believe? in Under-
specified Semantics. an LRS Analysis of Neg Rais-
ing. Empirical Issues in Formal Syntax and Seman-
tics 6.
Jan-Philipp Soehn. 2006. ?Uber Ba?rendienste und er-
staunte Bauklo?tze - Idiome ohne freie Lesart in der
HPSG. Ph.D. thesis, Fiedrich-Schiller Universita?t
Jena.
89
Ton van der Wouden. 1997. Negative Contexts. Collo-
cation, Polarity and Multiple Negation. Routledge,
London.
K. Vijay-Shanker and Aravind K. Joshi. 1988. Fea-
ture Structures Based Tree Adjoining Grammar. In
Proceedings of COLING, pages 714?719, Budapest.
Frans Zwarts. 1997. Three Types of Polarity. In
Fritz Hamm and Erhard W. Hinrichs, editors, Plu-
rality and Quantification, pages 177?237. Kluwer
Academic Publishers, Dordrecht.
90
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 109?114,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Constraint-based Computational Semantics:
A Comparison between LTAG and LRS
Laura Kallmeyer
University of T?bingen
Collaborative Research Center 441
lk@sfs.uni-tuebingen.de
Frank Richter
University of T?bingen
Collaborative Research Center 441
fr@sfs.uni-tuebingen.de
Abstract
This paper compares two approaches to
computational semantics, namely seman-
tic unification in Lexicalized Tree Ad-
joining Grammars (LTAG) and Lexical
Resource Semantics (LRS) in HPSG.
There are striking similarities between the
frameworks that make them comparable in
many respects. We will exemplify the dif-
ferences and similarities by looking at sev-
eral phenomena. We will show, first of all,
that many intuitions about the mechanisms
of semantic computations can be imple-
mented in similar ways in both frame-
works. Secondly, we will identify some
aspects in which the frameworks intrin-
sically differ due to more general differ-
ences between the approaches to formal
grammar adopted by LTAG and HPSG.
1 Introduction
This paper contrasts two frameworks for compu-
tational semantics, the proposal for semantics in
LTAG described in (Kallmeyer and Romero, 2005)
and LRS (Richter and Sailer, 2004), a computa-
tional semantics framework formulated in Head-
Driven Phrase Structure Grammar (HPSG).
There are significant differences between LTAG
and HPSG. LTAG is a mildly context-sensitive
lexicalized formalism characterized by an ex-
tended domain of locality. HPSG is based on the
idea of a separation of the lexicon and syntactic
structure and on the strict locality of general gram-
mar principles that are formulated in an expres-
sive and very flexible logical description language.
These fundamental differences are reflected in the
respective architectures for semantics: LTAG as-
sumes a separate level of underspecified semantic
representations; LRS uses the description logic of
syntax for semantic specifications.
However, despite the different mathematical
structures, we find striking similarities between
LTAG semantics with unification and LRS. They
both show similar intuitions underlying specific
analyses, use the same higher order type-theoretic
language (Ty2, (Gallin, 1975)) as a means for
specifying the truth conditions of sentences, and
employ a feature logic in the combinatorial seman-
tics instead of the lambda calculus. Because of
these similarities, analyses using both approaches
are closely related and can benefit from each other.
The paper is structured as follows: Sections 2
and 3 will introduce the two frameworks. The
next three sections (4?6) will sketch analyses of
some phenomena in both frameworks that will re-
veal relevant relations between them. Section 7
presents a summary and conclusion.
2 LTAG semantics
In (Kallmeyer and Romero, 2005), each elemen-
tary tree is linked to a semantic representation (a
set of Ty2 formulas and scope constraints). Ty2
formulas (Gallin, 1975) are typed ?-terms with in-
dividuals and situations as basic types. The scope
constraints of the form x ? y specify subordina-
tion relations between Ty2 terms. In other words,
x ? y indicates that y is a component of x.
A semantic representation is equipped with a
semantic feature structure description. Semantic
computation is done on the derivation tree and
consists of certain feature value equations between
mother and daughter nodes in the derivation tree.
(1) John always laughs.
As an example, see Fig. 1 showing the deriva-
tion tree for (1) with semantic representations and
109
l1 : laugh( 1 )
?
?
?
NP
[
GLOBAL
[
I 1
]
]
VP
[
B
[
P l1
]
]
?
?
?
np vp
john(x) l2 : always( 3 ),
3 ? 4
[
GLOBAL
[
I x
]
] ?
?
?
VPr
[
B
[
P l2
]
]
VPf
[
B
[
P 4
]
]
?
?
?
Figure 1: LTAG semantics of (1)
semantic feature structure descriptions as node
labels. The additional feature equations in this
example are depicted using dotted lines. They
arise from top-bottom feature identifications par-
allel to the unifications performed in FTAG (Vijay-
Shanker and Joshi, 1988) and from identifications
of global features. They yield 1 = x and 4 = l1.
Applying these identities to the semantic represen-
tations after having built their union leads to (2).
The constraint 3 ? l1 states that l1 : laugh(x) is
a component of 3 .
(2) john(x), l2 : always( 3 ), l1 : laugh(x),
3 ? l1
Note that the feature structure descriptions do
not encode the semantic expressions one is inter-
ested in. They only encode their contributions to
functional applications by restricting the argument
slots of certain predicates in the semantic repre-
sentations: They state which elements are con-
tributed as possible arguments for other seman-
tic expressions and which arguments need to be
filled. They thereby simulate lambda abstraction
and functional application while assembling the
semantic representations. To achieve this, a re-
stricted first order logic is sufficient.
Semantic computation is local on the derivation
tree: The new feature equations that are added de-
pend only on single edges in the derivation tree.
Because of this, even with the extension to seman-
tics, the formalism is still mildly context-sensitive.
3 LRS
In LRS the feature logic specifies the entire gram-
mar, including well-formed Ty2 terms as seman-
tic representations, and their mode of composi-
tion. Instead of the lambda calculus of tradi-
tional Montague Grammar, LRS crucially uses a
novel distinction between three aspects of the log-
ical representations of signs (external content, in-
ternal content, and parts). LRS constraints es-
tablish sub-term relationships between pieces of
semantic representations within and across signs,
thereby specifying the combinatorial properties of
the semantics. The subterm or component-of con-
ditions (symbolized as /) are imposed by gram-
mar principles. Since these principles are descrip-
tions of object-language expressions, they permit
the application of various underspecification tech-
niques of computational semantics, although an
LRS grammar does not employ underspecified se-
mantic representations, in contrast to LTAG se-
mantics.
Fig. 2 shows an HPSG description of the syn-
tactic tree and the LRS specifications of (1). The
syntactic trees in HPSG correspond to the derived
trees of LTAG. Since HPSG does not have deriva-
tion trees, the LRS principles refer to derived trees.
NP
?
?
exc 1
inc 1
p ? 1 john?
?
?
John
A
?
?
exc 5
inc 5 always( 3 )
p ? 5 , 5a always?
?
?
always
V
?
?
exc 4
inc 2 laugh( 1 )
p ? 2 , 2a laugh?
?
?
laughs
adj head
VP
?
?
exc 4
inc 2
p ? 2 , 2a , 5 , 5a ?
?
?
& 2 / 3 & 5 / 4
comp head
S
?
?
exc 4 always(laugh(john))
inc 2
p ? 2 , 2a , 5 , 5a , 1 ?
?
?
Figure 2: LRS analysis of (1)
Each word lexically specifies its contribution to
the overall meaning of the sentence (P(ARTS)), the
part of its semantics which is outscoped by all
signs the word combines with (INC(ONT)), and
the overall semantic contribution of its maximal
projection (EXC(ONT)). Feature percolation prin-
ciples identify INC and EXC, respectively, along
head projections and collect the elements of the
PARTS lists of the daughters at each phrase. The
combination of the adjunct with a verbal pro-
jection introduces two component-of constraints:
The EXC of always must be within the EXC of
laughs, and the INC of laughs must be in the
scope of always. The semantic argument of
110
laughs (john) is identified by subcategorization
(not shown in Fig. 2). A closure condition requires
that the semantic representation of an utterance
use up all and only the PARTS contributions of all
signs, which yields 4 = always(laugh(john)).
4 Quantifier scope
4.1 Specifying a scope window
(3) Exactly one student admires every professor:
? > ?,? > ?
(4) John seems to have visited everybody:
seem > ?,? > seem
Quantificational NPs in English can in princi-
ple scope freely (see (3) and (4)). An analysis of
quantifier scope must guarantee only two things:
1. the proposition to which a quantifier attaches
must be in its nuclear scope, and 2. a quantifier
cannot scope higher than the next finite clause.
One way to model this is to define a scope win-
dow delimited by a maximal scope and a minimal
scope for a quantifier. Both LTAG and LRS, spec-
ify such scope windows for quantifiers. We will
now outline the two analyses.
(5) Everybody laughs.
(Kallmeyer and Romero, 2005) use global fea-
tures MAXS and MINS for the limits of the scope
window. Fig. 3 shows the LTAG analysis of (5).
The feature identifications (indicated by dotted
lines) lead to the constraints 2 ? 5 , 5 ? l1.
These constraints specify an upper and a lower
boundary for the nuclear scope 5 . With the as-
signments following from the feature identifica-
tions we obtain the semantic representation (6):
(6)
l1 : laugh(x),
l2 : every(x, 4 , 5 ), l3 : person(x)
2 ? l1,
4 ? l3, 2 ? 5 , 5 ? l1
There is one possible disambiguation consis-
tent with the scope constraints, namely 2 ? l2,
4 ? l3, 5 ? l1. This leads to the semantics
every(x, person(x), laugh(x)).
In LRS, the EXCONT value of the utterance is
the upper boundary while the INCONT value of the
syntactic head a quantifier depends on is the lower
boundary for scope, as illustrated in Fig. 4. The
upper boundary is obtained through the interaction
of 1) a PROJECTION PRINCIPLE stating that the
l1 : laugh( 1 ),
2 ? 3
np
l2 : every(x, 4 , 5 ),
l3 : person(x),
4 ? l3,
6 ? 5 , 5 ? 7
?
?
?
?
GLOBAL
[
MINS l1
MAXS 2
]
NP
[
GLOBAL
[
I 1
]
]
?
?
?
?
?
?
?
?
GLOBAL
[
I x
]
NP
[
GLOBAL
[
MINS 7
MAXS 6
]
]
?
?
?
?
Figure 3: LTAG analysis of (5) Everybody laughs
PARTS list of a phrase contains all elements on the
PARTS lists of its daughters, and 2) the EXCONT
PRINCIPLE which states that a) the PARTS list of
each non-head contains its own EXCONT, and b)
in an utterance, everything on the PARTS list is a
component of the EXCONT. This leads to the con-
straint 4  6 in Fig. 4, among others. The lower
boundary is obtained from the SEMANTICS PRIN-
CIPLE which states that if the non-head of a headed
phrase is a quantifier, then the INCONT of the head
is a component of its nuclear scope. This yields
1  ? in Fig. 4.
S
?
?
EXC 6 ?x
(
person
(
x
)
? laugh
(
x
))
INC 1
P ?x, 1 , 1a , 2 , 2a , 4 , 4a ?
?
?
NP VP
?
?
?
EXC 4 ?x (? ? ?)
INC 2 person
(
x
)
P ?x, 2 , 2a person,
4 , 4a ? ? ??
?
?
?
?
?
EXC 6
INC 1 laugh
(
x
)
P ? 1 , 1a laugh?
?
?
everybody laughs
Relevant subterm constraints: 2  ? (from the lexical entry
of everybody), 1  ?, 4  6
Figure 4: LRS analysis of (5) Everybody laughs
The striking similarity between the two anal-
yses shows that, despite the fundamental differ-
ences between the frameworks, central insights
can be modelled in parallel.
4.2 Nested quantifiers
The use of the upper limit of the scope windows is,
however, slightly different: EXCONT contains the
quantifier itself as a component while MAXS limits
only the nuclear scope, not the quantifier. Conse-
quently, in LTAG the quantifier can scope higher
111
than the MAXS limiting its nuclear scope but in
this case it takes immediate scope over the MAXS.
(7) Two policemen spy on someone from every
city: ? > ? > 2 (among others)
The LTAG analysis is motivated by nested quan-
tifiers. In sentences such as (7), the embedded
quantifier can take scope over the embedding one
but if so, this must be immediate scope. In other
words, other quantifiers cannot intervene. In (7),
the scope order ? > 2 > ? is therefore not pos-
sible.1 The LTAG analysis is such that the max-
imal nuclear scope of the embedded quantifier is
the propositional label of the embedding quanti-
fier.2
In LRS, the way the scope window is speci-
fied, a corresponding constraint using the EXCONT
of the embedded quantifier cannot be obtained.
The LRS principle governing the distribution of
embedded quantifiers in complex NPs states di-
rectly that in this syntactic environment, the em-
bedded quantifier may only take direct scope over
the quantifier of the matrix NP. This principle
does not refer to the notion of external content at
all. At this point it is an open question whether
LRS could learn from LTAG here and adapt the
scope window so that an analogous treatment of
nested quantifiers would be possible.
5 LTAG?s extended domain of locality
Whereas the treatment of quantification sketched
in the preceding section highlights the similarities
between LTAG semantics and LRS, this and the
following section will illustrate some fundamental
differences between the frameworks.
In spite of the parallels mentioned above, even
INCONT and MINS differ sometimes, namely in
sentences containing bridge verbs. This is related
to the fact that LTAG has an extended domain of
locality whereas HPSG does not. Let us illustrate
the difference with the example (8).
(8) Mary thinks John will come.
1(Joshi et al, 2003) propose an extra mechanism that
groups quantifiers into sets in order to derive these con-
straints. (Kallmeyer and Romero, 2005) however show that
these constraints can be derived even if the upper limit MAXS
for nuclear scope is used as sketched above.
2Note that this approach requires constraints of the form
l ? n with l being a label, n a variable. This goes
beyond the polynomially solvable normal dominance con-
straints (Althaus et al, 2003). This extension, though, is
probably still polynomially solvable (Alexander Koller, per-
sonal communication).
In LTAG, the two elementary verb trees (for
thinks and will come) have different global MINS
features. The one for thinks is the label of the think
proposition while the one for will come is the label
of the embedded proposition. As a consequence, a
quantifier which attaches to the matrix verb cannot
scope into the embedded clause. This distinction
of different MINS values for different verb trees is
natural in LTAG because of the extended domain
of locality.
In LRS, all verbal nodes in the constituent struc-
ture of (8) carry the same INCONT value, namely
the proposition of the embedded verb. Conse-
quently, the minimal scope of quantifiers attaching
either to the embedding or to the embedded verb
is always the proposition of the embedded verb.
However, due to the requirement that variables be
bound, a quantifier binding an argument of the em-
bedding verb cannot have narrow scope over the
embedded proposition.
How to implement the LTAG idea of different
INCONT values for the embedding and the embed-
ded verb in LRS is not obvious. One might intro-
duce a new principle changing the INCONT value
at a bridge verb, whereby the new INCONT would
get passed up, and the embedded INCONT would
no longer be available. This would be problem-
atic: Take a raising verb as in (9) (adjoining to the
VP node in LTAG) instead of a bridge verb:
(9) Most people seem to everybody to like the
film.
Here the minimal scope of most people should
be the like proposition while the minimal scope
of everybody is the seem proposition. In LTAG
this does not pose a problem since, due to the ex-
tended domain of locality, most people attaches to
the elementary tree of like even though the seem
tree is adjoined in between. If the INCONT treat-
ment of LRS were modified as outlined above and
seem had an INCONT value that differed from the
INCONT value of the embedded like proposition,
then the new INCONT value would be passed up
and incorrectly provide the minimal scope of most
people. LRS must identify the two INCONTs.
The difference between the two analyses illus-
trates the relevance of LTAG?s extended domain of
locality not only for syntax but also for semantics.
6 Negative Concord
The analysis of negative concord in Polish de-
scribed in this section highlights the differences
112
in the respective implementation of underspeci-
fication techniques in LTAG and LRS. Recall
that both LTAG and LRS use component-of con-
straints. But in LTAG, these constraints link ac-
tual Ty2-terms (i.e., objects) to each other, while
in LRS, these constraints are part of a description
of Ty2-terms.
(10) Janek nie pomaga ojcu.
Janek NM helps father
?Janek doesn?t help his father.?
(11) a. Janek nie pomaga nikomu.
Janek NM helps nobody
?Janek doesn?t help anybody.?
b. ?Janek pomaga nikomu.
(12) Nikt nie przyszed?.
nobody NM came
?Nobody came.?
The basic facts of sentential negation and nega-
tive concord in Polish are illustrated in (10)?(12):
The verbal prefix nie is obligatory for sentential
negation, and it can co-occur with any number
of n-words (such as nikt, ?anybody?) without ever
leading to a double negation reading. As a conse-
quence, (12) expresses only one logical sentential
negation, although the negation prefix nie on the
verb and the n-word nikt can carry logical nega-
tion alone in other contexts. LRS takes advantage
of the fact that its specifications of semantic repre-
sentations are descriptions of logical expressions
which can, in principle, mention the same parts
of the expressions several times. Fig. 5 shows
that both nikt and the verb nie przyszed? introduce
descriptions of negations ( 4 and 2 , respectively).
The constraints of negative concord in Polish will
then conspire to force the negations contributed by
the two words to be the same in the overall logical
representation 6 of the sentence.
Such an analysis is not possible in LTAG. Each
negation in the interpretation corresponds to ex-
actly one negated term introduced in the seman-
tic representations. Therefore, the negative parti-
cle nie necessarily introduces the negation while
the n-word nikt requires a negation in the proposi-
tion it attaches to. An analysis along these lines is
sketched in Fig. 6 (?GL? stands for ?GLOBAL?).
The requirement of a negation is checked with
a feature NEG indicating the presence of a nega-
tion. The scope of the negation (feature N-SCOPE)
?
?
EXC 6 ??e?x
(
person
(
x
)
? come
(
e, x
))
INC 1
P ?e, x, 0 , 1 , 1a , 1b , 2 , 3 , 3a , 4 , 5 , 5a ?
?
?
nikt nie przyszed?
?
?
?
EXC 5 ?x (? ? ?)
INC 3 person
(
x
)
P ?x, 3 , 3a person,
4??, 5 , 5a ? ? ??
?
?
?
?
?
?
?
?
EXC 6
INC 1 come
(
e, x
)
P ?e, 1 , 1a come e,
1b come, 2??,
0 ?e??
?
?
?
?
?
1  ?, 2  6 , 5  ?, 3  ?, 1  ?, 1  ?, 1  ?
Figure 5: LRS analysis of (12) Nikt nie przyszed?
marks the maximal scope of the existential quan-
tifier of the n-word nikt (constraint 7 ? 6 ).3
S
NP VP
V
NP nie V
nikt przyszed?
l1 : ? 1 ,
l2 : come( 2 , 3 )
1 ? l2, 4 ? l1
np
l3 : some(x, 5 , 6 ),
l4 : person(x)
5 ? l4,
7 ? 6 , 6 ? 8
?
?
?
?
?
?
?
GL
?
?
?
MAXS 4
N-SCOPE 1
MINS l2
NEG yes
?
?
?
NP
[
GL
[
I 2
]
]
?
?
?
?
?
?
?
?
?
?
?
?
GL
[
I x
]
NP
?
?GL
[
N-SCOPE 7
MINS 8
NEG yes
]
?
?
?
?
?
?
?
Figure 6: LTAG analysis of (12) Nikt nie przyszed?
This example illustrates that the two frame-
works differ substantially in their treatment of un-
derspecification: 1. LRS employs partial descrip-
tions of fully specified models, whereas LTAG
generates underspecified representations in the
style of (Bos, 1995) that require the definition of
a disambiguation (a ?plugging? in the terminol-
ogy of Bos). 2. LRS constraints contain not Ty2
terms but descriptions of Ty2 terms. Therefore, in
contrast to LTAG, two descriptions can denote the
same formula. Here, LTAG is more limited com-
pared to LRS. On the other hand, the way seman-
tic representations are defined in LTAG guarantees
3See (Lichte and Kallmeyer, 2006) for a discussion of
NEG and N-SCOPE in the context of NPI-licensing.
113
that they almost correspond to normal dominance
constraints, which are known to be polynomially
parsable. The difference in the use of underspecifi-
cation techniques reflects the more general differ-
ence between a generative rewriting system such
as LTAG, in which the elements of the grammar
are objects, and a purely description-based for-
malism such as HPSG, in which token identities
between different components of linguistic struc-
tures are natural and frequently employed.
7 Summary and Conclusion
LTAG and LRS have several common characteris-
tics: They both 1. use a Ty2 language for seman-
tics; 2. allow underspecification (LTAG scope con-
straints ? versus LRS component-of constraints
); 3. use logical descriptions for semantic com-
putation; 4. are designed for computational appli-
cations. Due to these similarities, some analyses
can be modelled in almost identical ways (e.g., the
quantifier scope analyses, and the identification of
arguments using attribute values rather than func-
tional application in the lambda calculus). We take
the existence of this clear correspondence as in-
dicative of deeper underlying insight into the func-
tioning of semantic composition in natural lan-
guages.
Additionally, the differences between the
frameworks that can be observed on the level of
syntax carry over to semantics: 1. LTAG?s ex-
tended domain of locality allows the localization
within elementary trees of syntactic and seman-
tic relations between elements far apart from each
other on the level of constituent structure. 2. LTAG
(both syntax and semantics) is a formalism with
restricted expressive power that guarantees good
formal properties. The restrictions, however, can
be problematic. Some phenomena can be more
easily described in a system such as HPSG and
LRS while their description is less straightfor-
ward, perhaps more difficult or even impossible
within LTAG. The concord phenomena described
in section 7 are an example of this.
A further noticable difference is that within the
(Kallmeyer and Romero, 2005) framework, the
derivation tree uniquely determines both syntac-
tic and semantic composition in a context-free
way. Therefore LTAG semantics is mildly context-
sensitive and can be said to be compositional.
As far as LRS is concerned, it is not yet known
whether it is compositional or not; compositional-
ity (if it holds at all) is at least less straightforward
to show than in LTAG.
In conclusion, we would like to say that the sim-
ilarities between these two frameworks permit a
detailed and direct comparison. Our comparative
study has shed some light on the impact of the dif-
ferent characteristic properties of our frameworks
on concrete semantic analyses.
Acknowledgments
For many long and fruitful discussions of various
aspects of LTAG semantics and LRS, we would
like to thank Timm Lichte, Wolfgang Maier, Mari-
bel Romero, Manfred Sailer and Jan-Philipp S?hn.
Furthermore, we are grateful to three anonymous
reviewers for helpful comments.
References
Ernst Althaus, Denys Duchier, Alexander Koller, Kurt
Mehlhorn, Joachim Niehren, and Sven Thiel. 2003.
An efficient graph algorithm for dominance con-
straints. Journal of Algorithms, 48(1):194?219.
Johan Bos. 1995. Predicate logic unplugged. In Paul
Dekker and Martin Stokhof, editors, Proceedings of
the 10th Amsterdam Colloquium, pages 133?142.
Daniel Gallin. 1975. Intensional and Higher-Order
Modal Logic with Applications to Montague Seman-
tics. North Holland mathematics studies 19. North-
Holland Publ. Co., Amsterdam.
Aravind K. Joshi, Laura Kallmeyer, and Maribel
Romero. 2003. Flexible Composition in LTAG:
Quantifier Scope and Inverse Linking. In Harry
Bunt, Ielka van der Sluis, and Roser Morante, ed-
itors, Proceedings of the Fifth International Work-
shop on Computational Semantics IWCS-5, pages
179?194, Tilburg.
Laura Kallmeyer and Maribel Romero. 2005. Scope
and Situation Binding in LTAG using Semantic Uni-
fication. Submitted to Research on Language and
Computation. 57 pages., December.
Timm Lichte and Laura Kallmeyer. 2006. Licensing
German Negative Polarity Items in LTAG. In Pro-
ceedings of The Eighth International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+8), Sydney, Australia, July.
Frank Richter and Manfred Sailer. 2004. Basic con-
cepts of lexical resource semantics. In Arnold Beck-
mann and Norbert Preining, editors, ESSLLI 2003 ?
Course Material I, (= Collegium Logicum, 5), pages
87?143. Kurt G?del Society, Wien.
K. Vijay-Shanker and Aravind K. Joshi. 1988. Feature
structures based tree adjoining grammar. In Pro-
ceedings of COLING, pages 714?719, Budapest.
114
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 61?64,
Paris, October 2009. c?2009 Association for Computational Linguistics
An Incremental Earley Parser for Simple Range Concatenation Grammar
Laura Kallmeyer and Wolfgang Maier
Collaborative Research Center 833
University of Tu?bingen
Tu?bingen, Germany
{lk,wmaier}@sfs.uni-tuebingen.de
Abstract
We present an Earley-style parser for
simple range concatenation grammar, a
formalism strongly equivalent to linear
context-free rewriting systems. Further-
more, we present different filters which
reduce the number of items in the pars-
ing chart. An implementation shows that
parses can be obtained in a reasonable
time.
1 Introduction
Linear context-free rewriting systems (LCFRS)
(Vijay-Shanker et al, 1987), the equivalent mul-
tiple context-free grammars (MCFG) (Seki et al,
1991) and simple range concatenation grammars
(sRCG) (Boullier, 1998) have recently attracted
an increasing interest in the context of natu-
ral language processing. For example, Maier
and S?gaard (2008) propose to extract simple
RCGs from constituency treebanks with crossing
branches while Kuhlmann and Satta (2009) pro-
pose to extract LCFRS from non-projective depen-
dency treebanks. Another application area of this
class of formalisms is biological computing (Kato
et al, 2006).
This paper addresses the symbolic parsing of
sRCG/LCFRS. Starting from the parsing algo-
rithms presented in Burden and Ljunglo?f (2005)
and Villemonte de la Clergerie (2002), we pro-
pose an incremental Earley algorithm for simple
RCG. The strategy is roughly like the one pur-
sued in Villemonte de la Clergerie (2002). How-
ever, instead of the automaton-based formalization
in Villemonte de la Clergerie?s work, we give a
general formulation of an incremental Earley al-
gorithm, using the framework of parsing as de-
duction. In order to reduce the search space, we
introduce different types of filters on our items.
We have implemented this algorithm and tested it
on simple RCGs extracted from the German tree-
banks Negra and Tiger.
In the following section, we introduce simple
RCG and in section 3, we present an algorithm for
symbolic parsing of simple RCG. Section 4 then
presents different filtering techniques to reduce the
number of items. We close discussing future work.
2 Grammar Formalism
A range concatenation grammar (RCG) is a 5-
tupleG = (N,T, V, P, S). N is a finite set of non-
terminals (predicate names) with an arity function
dim: N ? N+, T and V are disjoint finite sets of
terminals and variables. P is a finite set of clauses
of the form ?0 ? ?1 . . . ?m, where m ? 0 and
each of the ?i, 0 ? i ? m, is a predicate of the
form Ai(?i1, . . . , ?idim(A)). Each ?ij ? (T ? V )?,
1 ? j ? dim(A) and 0 ? i ? k, is an argument.
As a shorthand notation for Ai(?1, . . . , ?dim(A)),
we use Ai(~?). S ? N is the start predicate name
with dim(S) = 1.
Note that the order of right-hand side (RHS)
predicates in a clause is of no importance. Sub-
classes of RCGs are introduced for further ref-
erence: An RCG G = (N,T, V, P, S) is sim-
ple if for all c ? P , it holds that every variable
X occurring in c occurs exactly once in the left-
hand side (LHS) and exactly once in the RHS, and
each argument in the RHS of c contains exactly
one variable. A simple RCG is ordered if for all
?0 ? ?1 ? ? ??m ? P , it holds that if a variable X1
precedes a variable X2 in a ?i, 1 ? i ? m, then
X1 also precedes X2 in ?0. The ordering require-
ment does not change the expressive power, i.e.,
ordered simple RCG is equivalent to simple RCG
(Villemonte de la Clergerie, 2002). An RCG is
?-free if it either contains no ?-rules or there is ex-
actly one rule S(?) ? ? and S does not appear in
any of the righthand sides of the rules in the gram-
mar. A rule is an ?-rule if one of the arguments
61
of the lefthand side is the empty string ?. (Boul-
lier, 1998) shows that for every simple RCG, one
can construct an equivalent ?-free simple RCG. An
RCG G = (N,T, V, P, S) is a k-RCG if for all
A ? N, dim(A) ? k.
The language of RCGs is based on the notion
of range. For a string w1 ? ? ?wn a range is a pair
of indices ?i, j? with 0 ? i ? j ? n, i.e., a
string span, which denotes a substring wi+1 ? ? ?wj
in the source string or a substring vi+1 ? ? ? vj in
the target string. Only consecutive ranges can be
concatenated into new ranges. Terminals, vari-
ables and arguments in a clause are bound to
ranges by a substitution mechanism. An instan-
tiated clause is a clause in which variables and ar-
guments are consistently replaced by ranges; its
components are instantiated predicates. For ex-
ample A(?g ? ? ?h?) ? B(?g + 1 ? ? ? h?) is an in-
stantiation of the clause A(aX1) ? B(X1) if
the target string is such that wg+1 = a. A de-
rive relation ? is defined on strings of instanti-
ated predicates. If an instantiated predicate is the
LHS of some instantiated clause, it can be replaced
by the RHS of that instantiated clause. The lan-
guage of an RCG G = (N,T, V, P, S) is the set
L(G) = {w1 ? ? ?wn | S(?0, n?) ?? ?}, i.e., an in-
put string w1 ? ? ?wn is recognized if and only if the
empty string can be derived from S(?0, n?). In this
paper, we are dealing only with ordered simple
RCGs. The ordering requirement does not change
the expressive power (Villemonte de la Clergerie,
2002). Furthermore, without loss of generality, we
assume that for every clause, there is a k ? 0 such
that the variables occurring in the clause are ex-
actly X1, . . . ,Xk.
We define derivation trees for simple RCGs as
unordered trees whose internal nodes are labelled
with predicate names and whose leaves are la-
belled with ranges such that all internal nodes
are licensed by RCG clause instantiations: given
a simple RCG G and a string w, a tree D =
?V,E, r? is a derivation tree of w = a1 . . . an
iff 1. there are exactly n leaves in D labelled
?0, 1?, . . . , ?n ? 1, n? and 2. for all v0 ? V with
v1, . . . , vn ? V , n ? 1 being all vertices with
?v0, vi? ? E (1 ? i ? n) such that the leftmost
range dominated by vi precedes the leftmost range
dominated by vi+1 (1 ? i < n): there is a clause
instantiation A0(~?0) ? A1(~?1) . . . An( ~?n) such
that a) l(vi) = Ai for 0 ? i ? n and b) the yield
of the leaves dominates by vi is ~?i.
3 Parsing
Our parsing algorithm is a modification of the
?incremental algorithm? of Burden and Ljunglo?f
(2005) with a strategy very similar to the strategy
adopted by Thread Automata (Villemonte de la
Clergerie, 2002). It assumes the grammar to be
ordered and ?-free. We refrain from supporting
non-?-free grammars since the treebank grammars
used with our implementation are all ?-free. How-
ever, note that only minor modifications would be
necessary in order to support non-?-free grammars
(see below).
We process the arguments of LHS of clauses in-
crementally, starting from an S-clause. Whenever
we reach a variable, we move into the clause of
the corresponding RHS predicate (predict or re-
sume). Whenever we reach the end of an argu-
ment, we suspend this clause and move into the
parent clause that has called the current one. In
addition, we treat the case where we reach the end
of the last argument and move into the parent as a
special case. Here, we first convert the item into
a passive one and then complete the parent item
with this passive item. This allows for some addi-
tional factorization.
The item form for passive items is [A, ~?] where
A a predicate of some arity k, ~? is a range vector of
arity k. The item form for active items: [A(~?) ?
A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?] where A(~?) ?
A1( ~?1) . . . Am( ~?m) ? P ; pos ? {0, . . . , n} is the
position up to which we have processed the input;
?i, j? ? N2 marks the position of our dot in the
arguments of the predicate A: ?i, j? indicates that
we have processed the arguments up to the jth ele-
ment of the ith argument; ~? is an range vector con-
taining the bindings of the variables and terminals
occurring in the lefthand side of the clause (~?(i)
is the range the ith element is bound to). When
first predicting a clause, it is initialized with a vec-
tor containing only symbols ??? for ?unknown?.
We call such a vector (of appropriate arity) ~?init.
We introduce an additional piece of notation. We
write ~?(X) for the range bound to the variable X
in ~?. Furthermore, we write ~?(?i, j?) for the range
bound to the jth element in the ith argument of the
clause lefthand side.
Applying a range vector ~? containing variable
bindings for a given clause c to the argument vec-
tor of the lefthand side of c means mapping the ith
element in the arguments to ~?(i) and concatenat-
ing adjacent ranges. The result is defined iff every
62
argument is thereby mapped to a range.
We start by predicting the S-predicate:
[S(~?) ? ~?, 0, ?1, 0?, ~?init] S(
~?) ? ~? ? P
Scan: Whenever the next symbol after the dot
is the next terminal in the input, we can scan it:
[A(~?) ? ~?, pos, ?i, j?, ~?]
[A(~?) ? ~?, pos+ 1, ?i, j + 1?, ~??]
~?(i, j+1) = wpos+1
where ~?? is ~? updated with ~?(i, j + 1) =
?pos, pos+ 1?.
In order to support ?-free grammars, one would
need to store the pair of indices a ? is mapped to
in the range vector, along with the mappings of
terminals and variables. The indices could be ob-
tained through a Scan-? operation, parallel to the
Scan operation.
Predict: Whenever our dot is left of a variable
that is the first argument of some RHS predicate
B, we predict new B-clauses:
[A(~?) ? . . . B(X, . . . ) . . . , pos, ?i, j?, ~?A]
[B(~?) ? ~?, pos, ?1, 0?, ~?init]
with the side condition ~?(i, j + 1) = X,B(~?) ?
~? ? P .
Suspend: Whenever we arrive at the end of an
argument that is not the last argument, we suspend
the processing of this clause and we go back to the
item that was used to predict it.
[B(~?) ? ~?, pos?, ?i, j?, ~?B ],
[A(~?) ? . . . B(~?) . . . , pos, ?k, l?, ~?A]
[A(~?) ? . . . B(~?) . . . , pos?, ?k, l + 1?, ~?]
where the dot in the antecedent A-item precedes
the variable ~?(i), |~?(i)| = j (the ith argument has
length j and has therefore been completely pro-
cessed), |~?| < i (the ith argument is not the last
argument of B), ~?B(~?(i)) = ?pos, pos?? and for
all 1 ? m < i: ~?B(~?(m)) = ~?A(~?(m)). ~? is ~?A
updated with ~?A(~?(i)) = ?pos, pos??.
Convert: Whenever we arrive at the end of the
last argument, we convert the item into a passive
one:
[B(~?) ? ~?, pos, ?i, j?, ~?B ]
[B, ?]
|~?(i)| = j, |~?| = i,
~?B(~?) = ?
Complete: Whenever we have a passive B item
we can use it to move the dot over the variable of
the last argument of B in a parent A-clause that
was used to predict it.
[B, ~?B], [A(~?) ? . . . B(~?) . . . , pos, ?k, l?, ~?A]
[A(~?) ? . . . B(~?) . . . , pos?, ?k, l + 1?, ~?]
where the dot in the antecedent A-item precedes
the variable ~?(|~?B |), the last range in ~?B is
?pos, pos??, and for all 1 ? m < |~?B |: ~?B(m) =
~?A(~?(m)). ~? is ~?A updated with ~?A(~?(|~?B |)) =
?pos, pos??.
Resume: Whenever we are left of a variable
that is not the first argument of one of the RHS
predicates, we resume the clause of the RHS pred-
icate.
[A(~?) ? . . . B(~?) . . . , pos, ?i, j?, ~?A],
[B(~?) ? ~?, pos?, ?k ? 1, l?, ~?B]
[B(~?) ? ~?, pos, ?k, 0?, ~?B]
where ~?(i)(j + 1) = ~?(k), k > 1 (the next el-
ement is a variable that is the kth element in ~?,
i.e., the kth argument of B), |~?(k ? 1)| = l, and
~?A(~?(m)) = ~?B(~?)(m) for all 1 ? m ? k ? 1.
The goal item has the form [S, ?0, n?].
Note that, in contrast to a purely bottom-up
CYK algorithm, the Earley algorithm presented
here is prefix valid, provided that the grammar
does not contain useless symbols.
4 Filters
During parsing, various optimizations known from
(P)CFG parsing can be applied. More concretely,
because of the particular form of our simple
RCGs, we can use several filters to reject items
very early that cannot lead to a valid parse tree for
a given input w = w1 . . . wn.
Since our grammars are ?-free, we know that
each variable or occurrence of a terminal in the
clause must cover at least one terminal in the in-
put. Furthermore, since separations between ar-
guments are generated only in cases where be-
tween two terminals belonging to the yield of a
non-terminal, there is at least one other terminals
that is not part of the yield, we know that between
different arguments of a predicate, there must be at
least one terminal in the input. Consequently, we
obtain as a filtering condition on the validity of an
active item that the length of the remaining input
must be greater or equal to the number of variables
and terminal occurrences plus the number of argu-
ment separations to the right of the dot in the left-
hand side of the clause. More formally, an active
item [A(~?) ? A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?]
satisfies the length filter iff
(n? pos)
? (|~?(i)| ? j) + ?dim(A)k=i+1 |~?(k)| + (dim(A) ? i)
The length filter is applied to results of predict,
resume, suspend and complete.
A second filter, first proposed in Klein and
Manning (2003), checks for the presence of re-
quired preterminals. In our case, we assume the
63
preterminals to be treated as terminals, so this fil-
ter amounts to checking for the presence of all
terminals in the predicted part of a clause (the
part to the right of the dot) in the remaining in-
put. Furthermore, we check that the terminals
appear in the predicted order and that the dis-
tance between two of them is at least the num-
ber of variables/terminals and argument separa-
tions in between. In other words, an active item
[A(~?) ? A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?] satis-
fies the terminal filter iff we can find an injec-
tive mapping fT : Term = {?k, l? | ~?(k)(l) ? T
and either k > i or (k = i and l > j)} ?
{pos+ 1, . . . , n} such that
1. wfT (?k,l?) = ~?(k)(l) for all ?k, l? ? Term;
2. for all ?k1, l1?, ?k2, l2? ? Term with k1 = k2
and l1 < l2: fT (?k2, l2?) ? fT (?k1, l1?) +
(l2 ? l1);
3. for all ?k1, l1?, ?k2, l2? ? Term with k1 <
k2: fT (?k2, l2?) ? fT (?k1, l1?) + (|~?(k1)| ?
l1) + ?k2?1k=k1+1|~?(k)| + l2 + (k2 ? k1).
Checking this filter amounts to a linear traversal
of the part of the lefthand side of the clause that
is to the right of the dot. We start with index i =
pos + 1, for every variable or gap we increment
i by 1. For every terminal a, we search the next
a in the input, starting at position i. If it occurs
at position j, then we set i = j and continue our
traversal of the remaining parts of the lefthand side
of the clause.
The preterminal filter is applied to results of the
predict and resume operations.
We have implemented the incremental Earley
parser with the filtering conditions on items. In
order to test it, we have extracted simple RCGs
from the first 1000 sentences of Negra and Tiger
(with removed punctuation) using the algorithm
described in Maier and S?gaard (2008) and parsed
the sentences 1001-1100 with it. The grammars
contained 2474 clauses (Negra) and 2554 clauses
(Tiger). The following table contains the to-
tal number of sentences for different length and
resp. the number of sentences for which a parse
was found, along with the average parsing times
of those that had a parse:
Negra Tiger
parse/tot av. t. parse/tot av. t.
|w| ? 20 73/84 0.40 sec. 50/79 0.32
20 ?
|w| ? 35 14/16 2.14 sec. 10/19 2.16
5 Conclusion and Future Work
We have presented an Earley-style algorithm for
simple range concatenation grammar, formulated
as deduction system. Furthermore, we have pre-
sented a set of filters on the chart reducing the
number of items. An implementation and a test
with grammars extracted from treebanks showed
that reasonable parsing times can be achieved.
We are currently working on a probabilistic
k-best extension of our parser which resumes
comparable work for PCFG (Huang and Chiang,
2005). Unfortunately, experiments with the Ear-
ley algorithm have shown that with grammars of a
reasonable size for data-driven parsing (> 15, 000
clauses), an exhaustive parsing is no longer ef-
ficient, due to the highly ambiguous grammars.
Algorithms using only passive items seem more
promising in this context since they facilitate the
application of A? parsing techniques.
References
Pierre Boullier. 1998. Proposal for a natural lan-
guage processing syntactic backbone. Rapport de
Recherche RR-3342, INRIA.
Ha?kan Burden and Peter Ljunglo?f. 2005. Parsing lin-
ear context-free rewriting systems. In Proceedings
of IWPT 2005.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT 2005.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple context-free grammar for RNA
pseudoknot modeling. In Proceedings of TAG+8.
Dan Klein and Christopher D. Manning. 2003. A*
Parsing: Fast Exact Viterbi Parse Selection. In Pro-
ceedings of HLT-NAACL.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar 2008.
Hiroyuki Seki, Takahashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of ACL.
Eric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata.
In Proceedings of COLING.
64
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 69?72,
Paris, October 2009. c?2009 Association for Computational Linguistics
Synchronous Rewriting in Treebanks
Laura Kallmeyer
University of Tu?bingen
Tu?bingen, Germany
lk@sfs.uni-tuebingen.de
Wolfgang Maier
University of Tu?bingen
Tu?bingen, Germany
wo.maier@uni-tuebingen.de
Giorgio Satta
University of Padua
Padova, Italy
satta@dei.unipd.it
Abstract
Several formalisms have been proposed
for modeling trees with discontinuous
phrases. Some of these formalisms allow
for synchronous rewriting. However, it
is unclear whether synchronous rewriting
is a necessary feature. This is an impor-
tant question, since synchronous rewrit-
ing greatly increases parsing complexity.
We present a characterization of recursive
synchronous rewriting in constituent tree-
banks with discontinuous annotation. An
empirical investigation reveals that syn-
chronous rewriting is actually a neces-
sary feature. Furthermore, we transfer this
property to grammars extracted from tree-
banks.
1 Introduction
Discontinuous phrases are frequent in natural
language, particularly in languages with a rela-
tively free word order. Several formalisms have
been proposed in the literature for modeling trees
containing such phrases. These include non-
projective dependency grammar (Nivre, 2006),
discontinuous phrase structure grammar (DPSG)
(Bunt et al, 1987), as well as linear context-
free rewriting systems (LCFRS) (Vijay-Shanker et
al., 1987) and the equivalent formalism of sim-
ple range concatenation grammar (sRCG) (Boul-
lier, 2000). Kuhlmann (2007) uses LCFRS for
non-projective dependency trees. DPSG have
been used in Plaehn (2004) for data-driven pars-
ing of treebanks with discontinuous constituent
annotation. Maier and S?gaard (2008) extract
sRCGs from treebanks with discontinuous con-
stituent structures.
Both LCFRS and sRCG can model discontinu-
ities and allow for synchronous rewriting as well.
We speak of synchronous rewriting when two or
more context-free derivation processes are instan-
tiated in a synchronous way. DPSG, which has
also been proposed for modeling discontinuities,
does not allow for synchronous rewriting because
the different discontinuous parts of the yield of a
non-terminal are treated locally, i.e., their deriva-
tions are independent from each other. So far, syn-
chronous rewriting has not been empirically mo-
tivated by linguistic data from treebanks. In this
paper, we fill this gap by investigating the exis-
tence of structures indicating synchronous rewrit-
ing in treebanks with discontinuous annotations.
The question of whether we can find evidence for
synchronous rewriting has consequences for the
complexity of parsing. In fact, parsing with syn-
chronous formalisms can be carried out in time
polynomial in the length of the input string, with
a polynomial degree depending on the maximum
number of synchronous branches one can find in
derivations (Seki et al, 1991).
In this paper, we characterize synchronous
rewriting as a property of trees with crossing
branches and in an empirical evaluation, we con-
firm that treebanks do contain recursive syn-
chronous rewriting which can be linguistically
motivated. Furthermore, we show how this char-
acterization transfers to the simple RCGs describ-
ing these trees.
2 Synchronous Rewriting Trees in
German treebanks
By synchronous rewriting we indicate the syn-
chronous instantiation of two or more context-free
derivation processes. As an example, consider the
language L = {anbncndn | n ? 1}. Each
of the two halves of some w ? L can be ob-
tained through a stand-alone context-free deriva-
tion, but for w to be in L the two derivations must
be synchronized somehow. For certain tasks, syn-
chronous rewriting is a desired property for a for-
malism. In machine translation, e.g., synchronous
69
rewriting is extensively used to model the syn-
chronous dependence between the source and tar-
get languages (Chiang, 2007). The question we
are concerned with in this paper is whether we can
find instances of recursive synchronous rewriting
in treebanks that show discontinuous phrases.
We make the assumption that, if the annota-
tion of a treebank allows to express synchronous
rewriting, then all cases of synchronous rewriting
are present in the annotation. This means that, on
the one hand, there are no cases of synchronous
rewriting that the annotator ?forgot? to encode.
Therefore unrelated cases of parallel iterations in
different parts of a tree are taken to be truly unre-
lated. On the other hand, if synchronous rewrit-
ing is annotated explicitely, then we take it to be a
case of true synchronous rewriting, even if, based
on the string, it would be possible to find an anal-
ysis that does not require synchronous rewriting.
This assumption allows us to concentrate only on
explicit cases of synchronous rewriting .
We concentrate on German treebanks annotated
with trees with crossing branches. In such trees,
synchronous rewriting amounts to cases where dif-
ferent components of a non-terminal category de-
velop in parallel. In particular, we search for cases
where the parallelism can be iterated. An exam-
ple is the relative clause in (1), found in TIGER.
Fig. 1 gives the annotation. As can be seen in
the annotation, we have two VP nodes, each of
which has a discontinuous span consisting of two
parts. The two parts are separated by lexical ma-
terial not belonging to the VPs. The two com-
ponents of the second VP (Pop-Idol and werden)
are included in the two components of the first,
higher, VP (genausogut auch Pop-Idol and wer-
den ko?nnen). In other words, the two VP compo-
nents are rewritten in parallel containing again two
smaller VP components.
(1) . . . der
. . . who
genausogut
as well
auch
also
Pop-Idol
pop-star
ha?tte
AUX
werden
become
ko?nnen
could
?who could as well also become a pop-star?
Let us assume the following definitions: We
map the elements of a string to their positions. We
then say that the yield ? of a node n in a tree is
the set of all indices i such that n dominates the
leaf labeled with the ith terminal. A yield ? has a
gap if there are i1 < i2 < i3 such that i1, i3 ? ?
and i2 /? ?. For all i, j ? ? with i < j, the set
??i,j? = {k | i ? k ? j} is a component of ? if
??i,j? ? ? and i?1 /? ? and j+1 /? ?. We order
the components of ? such that ??i1,j1? < ??i2,j2?
if i1 < i2.
Trees showing recursive synchronous rewrit-
ing can be characterized as follows: We have a
non-terminal node n1 with label A whose yield
has a gap. n1 dominates another node n2 with la-
bel A such that for some i 6= j, the ith component
of the yield of n2 is contained in the ith component
of the yield of n1 and similar for the jth compo-
nent. We call the path from n1 to n2 a recursive
synchronous rewriting segment (RSRS).
Table 1 shows the results obtained from search-
ing for recursive synchronous rewriting in the Ger-
man TIGER and NeGra treebanks. In a prepro-
cessing step, punctuation has been removed, since
it is directly attached to the root node and therefore
not included in the annotation.
TIGER NeGra
number of trees 40,013 20,597
total num. of RSRS in all trees 1476 600
av. RSRS length in all trees 2.13 2.12
max. RSRS length in all trees 5 4
Table 1: Synchronous rewriting in treebanks
Example (1) shows that we find instances of re-
cursive synchronous rewriting where each of the
rewriting steps adds something to both of the par-
allel components. (1) was not an isolated case.
The annotation of (1) in Fig. 1 could be turned
into a context-free structure if the lowest node
dominating the material in the gap while not
dominating the synchronous rewriting nodes (here
VAFIN) is attached lower, namely below the lower
VP node. (Note however that there is good linguis-
tic motivation for attaching it high.) Besides such
cases, we even encountered cases where the dis-
continuity cannot be removed this way. An exam-
ple is (2) (resp. Fig. 2) where we have a gap con-
taining an NP such that the lowest node dominat-
ing this NP while not dominating the synchronous
rewriting nodes has a daughter to the right of the
yields of the synchronous rewriting nodes, namely
the extraposed relative clause. This structure is of
the type ancbnd, where a and b depend on each
other in a left-to-right order and can be nested,
and c and d also depend on each other and must
be generated together. This is a structure that re-
quires synchronous rewriting, even on the basis of
the string language. Note that the nesting of VPs
can be iterated, as can be seen in (3).
(2) . . . ob
. . . whether
auf
on
deren
their
Gela?nde
premises
der
the
Typ
type
von
of
70
S
VP
VP
PRELS ADV ADV NN VAFIN VAINF VMINF
der genausogut auch Pop-Idol ha?tte werden ko?nnen
Figure 1: Example for recursive synchronous rewriting
Abstellanlage
parking facility
gebaut
built
werden
be
ko?nne,
could,
der
which
. . .
. . .
?whether on their premises precisely the type of parking
facility could be built, which . . . ?
(3) . . . ob
. . . whether
auf
on
deren
their
Gela?nde
premises
der
the
Typ
type
von
of
Abstellanlage
parking facility
eigentlich
actually
ha?tte
had
schon
already
gebaut
built
werden
be
sollen,
should,
der
which
. . .
. . .
?whether on their premises precisely the type of parking
facility should actually already have been built, which
. . . ?
As a conclusion from these empirical results,
we state that to account for the data we can find in
treebanks with discontinuities, i.e., with crossing
branches, we need a formalism that can express
synchronous rewriting.
3 Synchronous Rewriting in Grammars
Extracted from Treebanks
In the following, we will use simple RCG (which
are equivalent to LCFRS) to model our treebank
annotations. We extract simple RCG rewriting
rules from NeGra and TIGER and check them for
the possibility to generate recursive synchronous
rewriting.
A simple RCG (Boullier, 2000) is a tuple G =
(N,T, V, P, S) where a) N is a finite set of pred-
icate names with an arity function dim: N ? N,
b) T and V are disjoint finite sets of terminals and
variables, c) P is a finite set of clauses of the form
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
for m ? 0 where A,A1, . . . , Am ? N , X(i)j ?
V for 1 ? i ? m, 1 ? j ? dim(Ai) and ?i ?
(T ? V )? for 1 ? i ? dim(A), and e) S ? N is
the start predicate name with dim(S) = 1. For all
c ? P , it holds that every variable X occurring in
c occurs exactly once in the left-hand side (LHS)
and exactly once in the RHS. A simple RCG G =
(N,T, V, P, S) is a simple k-RCG if for all A ?
N, dim(A) ? k.
For the definition of the language of a simple
RCG, we borrow the LCFRS definitions here: Let
G = ?N,T, V, P, S? be a simple RCG. For every
A ? N , we define the yield of A, yield(A) as
follows:
a) For every A(~?) ? ?, ~? ? yield(A);
b) For every clause
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
and all ~?i ? yield(Ai) for 1 ? i ? m,
?f(?1), . . . , f(?dim(A))? ? yield(A) where
f is defined as follows:
(i) f(t) = t for all t ? T ,
(ii) f(X(i)j ) = ~?i(j) for all 1 ? i ? m, 1 ?
j ? dim(Ai) and
(iii) f(xy) = f(x)f(y) for all x, y ? (T ?
V )+.
c) Nothing else is in yield(A).
The language is then {w | ?w? ? yield(S)}.
We are using the algorithm from Maier and
S?gaard (2008) to extract simple RCGs from Ne-
Gra and TIGER. For the tree in Fig. 1, the algo-
rithm produces for instance the following clauses:
PRELS(der) ? ?
ADV(genausogut) ? ?
. . .
S(X1X2X3X4) ? PRELS(X1)VP2(X1,X4) VAFIN(X3)
VP2(X1X2X3,X4X5) ? ADV(X1) ADV(X2)
VP2(X3,X4) VMINF(X5)
VP2(X1,X2) ? NN(X1) VAINF(X2)
We distinguish different usages of the same cat-
egory depending on their numbers of yield com-
ponents. E.g., we distinguish non-terminals VP1,
VP2, . . . depending on the arity of the VP. We de-
fine cat(A) for A ? N as the category of A, inde-
pendent from the arity, e.g., cat(VP2) =VP.
In terms of simple RCG, synchronous rewrit-
ing means that in a single clause distinct variables
occurring in two different arguments of the LHS
predicate are passed to two different arguments of
the same RHS predicate. We call this recursive
71
S
NP
VP
VP
VP
PP NP
ob auf dem Gela?nde der Typ von Abstellanlage . . . ha?tte . . . gebaut werden sollen, der. . .
Figure 2: Iterable treebank example for synchronous rewriting
if, by a sequence of synchronous rewriting steps,
we can reach the same two arguments of the same
predicate again. Derivations using such cycles of
synchronous rewriting lead exactly to the recursive
synchronous rewriting trees characterized in sec-
tion 2. In the following, we check to which extent
the extracted simple RCG allows for such cycles.
In order to detect synchronous rewriting in a
simple k-RCG G, we build a labeled directed
graph G = (VG , EG , l) from the grammar with
VG a set of nodes, EG a set of arcs and l :
VG ? N ? ? {0, . . . , k} ? {0, . . . , k} where N ? =
{cat(A) |A ? N} a labeling function. G is con-
structed as follows. For each clause A0(~?) ?
A1( ~?1) . . . Am( ~?m) ? P we consider all pairs of
variables Xs,Xt for which the following condi-
tions hold: (i) Xs and Xt occur in different argu-
ments i and j of A0, 1 ? i < j ? dim(A0); and
(ii) Xs and Xt occur in different arguments q and
r of the same occurrence of predicate Ap in the
RHS, 1 ? q < r ? dim(Ap) and 1 ? p ? m.
For each of these pairs, two nodes with labels
[cat(A0), i, j] and [cat(Ap), q, r], respectively, are
added to VG (if they do not yet exist, otherwise we
take the already existing nodes) and a directed arc
from the first node to the second node is added to
EG . The intuition is that an arc in G represents
one or more clauses from the grammar in which
a gap between two variables in the LHS predicate
is transferred to the same RHS predicate. To de-
tect recursive synchronous rewriting, we then need
to discover all elementary cycles in G, i.e., all cy-
cles in which no vertex appears twice. In order to
accomplish this task efficiently, we exploit the al-
gorithm presented in Johnson (1975). On a gram-
mar extracted from NeGra (19,100 clauses), the
algorithm yields a graph with 28 nodes containing
206,403 cycles of an average length of 12.86 and
a maximal length of 28.
4 Conclusion
The starting point of this paper was the question
whether synchronous rewriting is a necessary fea-
ture of grammer formalisms for modelling natu-
ral languages. In order to answer this question,
we have characterized synchronous rewriting in
terms of properties of treebank trees with crossing
branches. Experiments have shown that recursive
cases of synchronous rewriting occur in treebanks
for German which leads to the conclusion that,
in order to model these data, we need formalisms
that allow for synchronous rewriting. In a second
part, we have extracted a simple RCG from these
treebanks and we have characterized the grammar
properties that are necessary to obtain recursive
synchronous rewriting. We then have investigated
the extent to which a grammar extracted from Ne-
Gra allows for recursive synchronous rewriting.
References
Pierre Boullier. 2000. Range concatenation grammars.
In Proceedings of IWPT.
Harry Bunt, Jan Thesingh, and Ko van der Sloot. 1987.
Discontinuous constituents in trees, rules and pars-
ing. In Proceedings of EACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Donald B. Johnson. 1975. Finding all the elementary
circuits of a directed graph. SIAM Journal on Com-
puting.
Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Dissertation, Saarland Uni-
versity.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Oliver Plaehn. 2004. Computing the most probable
parse for a discontinuous phrase-structure grammar.
In New developments in parsing technology. Kluwer.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical
Computer Science.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of ACL.
72
123
124
125
126
127
128
129
130
Tree-Local Multicomponent Tree-Adjoining
Grammars with Shared Nodes
Laura Kallmeyer?
TALaNa/Lattice, Universite? Paris 7
This article addresses the problem that the expressive power of tree-adjoining grammars (TAGs)
is too limited to deal with certain syntactic phenomena, in particular, with scrambling in free-
word-order languages. The TAG variants proposed so far in order to account for scrambling are
not entirely satisfying. Therefore, the article introduces an alternative extension of TAG that
is based on the notion of node sharing, so-called (restricted) tree-local multicomponent TAG
with shared nodes (RSN-MCTAG). The analysis of some German scrambling data is sketched
in order to show that this TAG extension can deal with scrambling. Then it is shown that
for RSN-MCTAGs of a specific type, equivalent simple range concatenation grammars can
be constructed. As a consequence, these RSN-MCTAGs are mildly context-sensitive and in
particular polynomially parsable. These specific RSN-MCTAGs probably can deal not with all
scrambling phenomena, but with an arbitrarily large subset.
1. Introduction: LTAG and Scrambling
1.1 Lexicalized Tree-Adjoining Grammars
Tree-adjoining grammar (TAG) is a tree-rewriting formalism originally defined by Joshi,
Levy, and Takahashi (1975). A TAG (see Joshi and Schabes 1997 for an introduction)
consists of a finite set of trees (elementary trees). The nodes of these trees are labeled
with nonterminals and terminals (terminals label only leaf nodes). Starting from the
elementary trees, larger trees are derived using composition operations of substitution
(replacing a leaf with a new tree) and adjunction (replacing an internal node with a new
tree). In the case of an adjunction, the tree being adjoined has exactly one leaf node that
is marked as the foot node (marked with an asterisk). Such a tree is called an auxiliary
tree. When such a tree is adjoined to a node ?, in the resulting tree, the subtree with root
node ? from the old tree is put below the foot node of the new auxiliary tree. Elementary
trees that are not auxiliary trees are called initial trees. Each derivation starts with an
initial tree. In the final derived tree, all leaves must have terminal labels.1
? UFR de Linguistique, Case 7003, 2 Place Jussiec, 75005 Paris. E-mail: Laura.Kallmeyer@linguist.jussieu.fr.
1 Additionally, TAG allows for each internal node to specify the set of auxiliary trees that can be adjoined
using so-called adjunction constraints and, furthermore, to specify whether adjunction at that node is
obligatory. This is an important feature of TAG, since it influences the generative capacity of the
formalism: {anbncndn |n ? 0}, for example, is a language that can be generated by a TAG with adjunction
constraints but not by a TAG without adjunction constraints (Joshi 1985). For this article, however,
adjunction constraints do not play any important role.
Submission received: 1st September 2003; Revised submission received: 4th May 2004; Accepted for
publication: 17th June 2004
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 2
Figure 1
TAG derivation for John always laughs.
Figure 1 shows a sample TAG derivation. Here, the three elementary trees for laughs,
John, and always are combined: Starting from the elementary tree for laughs, the tree for
John is substituted for the noun phrase (NP) leaf and the tree for always is adjoined at
the verb phrase (VP) node.
TAG derivations are represented by derivation trees that record the history of how
the elementary trees are put together. A derivation tree is the result of carrying out
substitutions and adjunctions. Each edge in the derivation tree stands for an adjunction
or a substitution. The edges are labeled with Gorn addresses of the nodes where the
substitutions and adjunctions have taken place: The root has the address , and the jth
child of the node with address p has address pj. In Figure 1, for example, the derivation
tree indicates that the elementary tree for John is substituted for the node at address 1
and always is adjoined at node address 2.
What we have sketched so far are the mathematical aspects of the TAG formal-
ism. For natural languages, TAGs with specific properties are used. These properties
are not part of the formalism itself, but they are additional linguistic principles that
are respected when a TAG is constructed for a natural language. First, a TAG for
natural languages is lexicalized (Schabes 1990), which means that each elementary
tree has a lexical anchor (usually unique, but in some cases, there is more than one
anchor). Second, the elementary trees of a lexicalized TAG (LTAG) represent extended
projections of lexical items (the anchors) and encapsulate all syntactic arguments of
the lexical anchor; that is, they contain slots (nonterminal leaves) for all arguments.
Furthermore, elementary trees are minimal in the sense that only the arguments of the
anchor are encapsulated; all recursion is factored away. This amounts to the condition
on elementary tree minimality (CETM) from Frank (1992) (see also Frank [2002] for
further discussions of the linguistic principles underlying TAG).2 The tree for laughs in
Figure 1, for example, contains only a nonterminal leaf for the subject NP (a substitution
node), and there is no slot for a VP adjunct. The adverb always is added by adjunction
at an internal node.
Because of these principles, in linguistic applications, combining two elementary
trees by substitution or adjunction corresponds to the application of a predicate to
an argument. The derivation tree then reflects the predicate-argument structure of the
sentence. This is why most approaches to semantics in TAG use the derivation tree as an
interface between syntax and semantics (see, e.g., Candito and Kahane 1998; Joshi and
Vijay-Shanker 1999; Kallmeyer and Joshi 2003). In this article, we are not particularly
concerned with semantics, but one of the goals of the article is to obtain analyses with
derivation trees representing the correct predicate-argument dependencies.
2 This minimality is actually the reason that the substitution operation is needed; formally TAGs without
substitution and TAGs as introduced above have the same weak and strong generative capacity.
188
Kallmeyer Multicomponent TAGs with Shared Nodes
An extension of TAG that has been shown to be useful for several linguistic ap-
plications is multicomponent TAG (MCTAG) (Joshi 1987; Weir 1988). Instead of single
elementary trees, an MCTAG has sets of elementary trees. In each derivation step, one
of these sets is chosen, and all trees from the set are added simultaneously. Depending
on the nodes to which the different trees from the set attach, different kinds of MCTAGs
are distinguished: If all nodes are required to be part of the same elementary tree, the
MCTAG is called tree-local; if all nodes are required to be part of the same tree set, the
grammar is set-local; and otherwise the grammar is nonlocal.
1.2 Scrambling in TAG
Roughly, scrambling can be described as the permutation of elements (arguments and
adjuncts) of a sentence (we use the term scrambling in a purely descriptive sense without
implying any theory involving actual movement). A special case of scrambling is so-
called long-distance scrambling, in which arguments or adjuncts of an embedded
infinitive are ?moved? out of the embedded VP. This occurs, for instance, in languages
such as German, Hindi, Japanese, and Korean. As an example of long-distance scram-
bling in German, consider example (1):
(1) . . . dass [es]1 der Mechaniker [t1 zu reparieren] verspricht
. . . that it the mechanic to repair promises
?. . . that the mechanic promises to repair it?
In example (1), the accusative NP es is an argument of the embedded infinitive zu
reparieren, but it precedes der Mechaniker, the subject of the main verb verspricht, and
it is not part of the embedded VP.
It has been argued (see Rambow 1994a) that in German, there is no bound on the
number of scrambled elements and no bound on the depth of scrambling (i.e., in terms
of movement, the number of VP borders crossed by the moved element).
TAGs are not powerful enough to describe scrambling in German in an adequate
way (Becker, Joshi, and Rambow 1991). By this we mean that a TAG analysis of
scrambling respecting the CETM and therefore giving the correct predicate-argument
structure (i.e., an analysis with each argument attaching to the verb it depends on) is
not possible.
Let us consider the TAG analysis of example (1) in order to see why scram-
bling poses a problem for TAG. If we leave aside the complementizer dass, standard
TAG elementary trees for verspricht and reparieren in the style of the XTAG grammar
(XTAG Research Group 1998) might look as shown in Figure 2. In the derivation, the
Figure 2
Standard TAG combination of der Mechaniker, zu reparieren, and verspricht in example (1).
189
Computational Linguistics Volume 31, Number 2
verspricht-tree adjoins to the root node of the reparieren-tree, and the nominative NP der
Mechaniker is substituted for the subject node in the verspricht-tree. This leads to the tree
on the right in Figure 2.
When es is added, there is a problem: It should be added to reparieren, since it is
one of its arguments. But at the same time, it should precede der Mechaniker; that is,
it must be adjoined either to the root or to the NPnom node. The root node belongs to
verspricht, and the NPnom node belongs to der Mechaniker. Consequently, an adjunction
to one of them would not give the desired predicate-argument structure. If one wanted
to analyze only example (1), one could add a tree to the grammar for reparieren with a
scrambled NP that allows adjunction of verspricht between the NP and the verb. But as
soon as there are several scrambled elements that are arguments of different verbs, this
no longer works.
This example has given an idea of why scrambling is problematic for TAG. How-
ever, adopting specific elementary trees, it is possible to deal with a part of the difficult
scrambling data: It has been shown (see Joshi, Becker, and Rambow 2000) that TAG can
describe scrambling up to depth two (two crossed VP borders). But this is not sufficient.
Even though examples of scrambling of depth greater than two are rare, they can occur.
An example is example (2), taken from Kulick (2000):
(2) . . . dass [den Ku?hlschrank]1 niemand [[[t1 zu reparieren] zu versuchen]
. . . that the refrigerator nobody to repair to try
zu versprechen] bereit ist
to promise willing is
?. . . that nobody is willing to promise to try to repair the refrigerator?
Consequently, TAG is not powerful enough to account for scrambling.3
Becker, Rambow, and Niv (1992) argue that even linear context-free rewriting sys-
tems (LCFRSs) (Weir 1988) are not powerful enough to describe scrambling. (LCFRSs
are weakly equivalent to set-local MCTAGs and therefore more powerful than TAGs.)
Although we think that the language Becker, Rambow, and Niv define as a kind of test
language for scrambling is not exactly what one needs (see section 2.3), we still suspect
that they are right in claiming that LCFRSs cannot describe scrambling.
1.3 TAG Variants Proposed for Scrambling
The problem with long-distance scrambling and TAG is that the trees representing
the syntax of scrambled German subordinate clauses do not have the simple nested
structure that ordinary TAG generates. The CETM requires that (positions for) all of
the arguments of the lexical anchor of an elementary tree be included in that tree. But
in a scrambled tree, the arguments of several verbs are interleaved freely. All TAG
extensions that have been proposed to accommodate this interleaving involve factoring
the elementary structures into multiple components and inserting these components at
multiple positions in the course of the derivation.
One of the first proposals made was an analysis of German scrambling data using
nonlocal MCTAG with additional dominance constraints (Becker, Joshi, and Rambow
1991). However, the formal properties of nonlocal MCTAG are not well understood, and
3 See also Gerdes (2002) for a discussion of the limitation of TAG with respect to scrambling in German.
190
Kallmeyer Multicomponent TAGs with Shared Nodes
it is assumed that the formalism is not polynomially parsable. Therefore this approach
is no longer pursued, but it has influenced the different subsequent proposals.
An alternative formalism for scrambling is V-TAG (Rambow 1994a, 1994b; Rambow
and Lee 1994), a formalism that has nicer formal properties than nonlocal MCTAG.
V-TAG also uses multicomponent sets (vectors) for scrambled elements; in this it is
a variant of MCTAG. Additionally, there are dominance links among the trees of the
same vector. In contrast to MCTAG, the trees of a vector in V-TAG are not required to be
added simultaneously. The lexicalized V-TAGs that are of interest for natural languages
are polynomially parsable. Rambow (1994a) proposes detailed analyses of a large range
of different word order phenomena in German using V-TAG and thereby shows the
linguistic usefulness of V-TAG.
Even though V-TAG does not pose the problems of nonlocal MCTAG in terms
of parsing complexity, it is still a nonlocal formalism in the sense that, as long as
the dominance links are respected, arbitrary nodes can be chosen to attach the single
components of a vector. Therefore, in order to formulate certain locality restrictions
(e.g., for wh-movement and also for scrambling), one needs an additional means of
putting constraints on what can interleave with the different trees of a vector, or in
other words, constraints on how far a dominance link can be stretched. V-TAG allows
us to put integrity constraints on certain nodes that disallow the occurrence of these
nodes between two trees linked by a dominance link. This has the effect of making these
nodes act as barriers. With integrity constraints, constructions involving long-distance
movements can be correctly analyzed. But the explicit marking of barriers is somewhat
against the original appealing TAG idea that such constraints result from imposition
of the CETM, according to which the position of the moved element and the verb it
depends on must be in the same elementary structure, and from the further combination
possibilities of this structure. In other words, in local formalisms with an extended
domain of locality such as TAG or tree-local and set-local MCTAG, such constraints
result from the form of the elementary structures and from the locality of the derivation
operation. That is, they follow from general properties of the grammar, and they need
not be stated explicitly. This is one of the aspects that make TAG so attractive from a
linguistic point of view, and it gets lost in nonlocal TAG variants.
D-tree substitution grammars (DSGs) (Rambow, Vijay-Shanker, and Weir 2001) are
another TAG variant one could use for scrambling. DSGs are a description-based for-
malism; that is, the objects a DSG deals with are tree descriptions. A problem with DSG
is that the expressive power of the formalism is probably too limited to deal with all
natural language phenomena: According to Rambow, Vijay-Shanker, and Weir (2001)
it ?does not appear to be possible for DSG to generate the copy language? (page 101).
This means that the formalism is probably not able to describe cross-serial dependencies
in Swiss German. Furthermore, DSG is nonlocal and therefore, as in the case of V-TAG,
additional constraints (path constraints) have to be placed on material interleaving with
the different parts of an elementary structure.
Another TAG variant using tree descriptions is local tree description grammar
(TDG) (Kallmeyer 2001). Local TDG can be used for scrambling in a way similar to DSG
or V-TAG. The languages generated by local TDGs are semilinear. However, the formal-
ism allows one to generate tree descriptions with underspecified dominance relations,
and the process of resolving the remaining dominance links is nonlocal. Therefore one
may have the same problem as in the case of DSG and V-TAG. Furthermore, so far it
has not been shown that the formalism is polynomially parsable, and it is not clear
whether such parsing is possible without any additional constraint or limitation on the
underspecified tree descriptions.
191
Computational Linguistics Volume 31, Number 2
A further TAG variant proposed in order to deal with scrambling is segmented tree-
adjoining grammar (SegTAG) (Kulick 2000). SegTAG uses an operation on trees called
segmented adjunction that consists partly of a standard TAG adjunction and partly of
a kind of tree merging or tree unification. In this operation, two different things get
mixed up, the more or less resource-sensitive adjoining operation of standard TAG, in
which subtrees cannot be identified,4 and the completely different unification operation.
Perhaps using tree descriptions instead of trees, a more coherent definition of SegTAG
can be achieved. But we will not pursue this here.
The formal properties of SegTAG are not clear. Kulick (2000) suggests that SegTAGs
are probably in the class of LCFRSs, but there is no actual proof of this. However, if
SegTAG is in LCFRS, the generative power of the formalism is probably too limited
to deal with scrambling in a general way. But it seems that the limit imposed by the
grammar on the complexity of the scrambling data is fixed but arbitrarily high. (With
increasing complexity, the elementary trees, however, get larger and larger.) This means
that one can probably define a SegTAG that can analyze scrambling up to some com-
plexity level n for any n ? IN. (A definition of what a complexity level is, is not given;
it is perhaps the depth of scrambling.) In this sense, a general treatment of scrambling
might be possible. We follow a similar approach in this article by proposing a mildly
context-sensitive formalism that can deal with scrambling up to some fixed complexity
limit n that can be chosen arbitrarily high.
All these TAG variants are interesting with respect to scrambling, and they give
a great deal of insight into what kind of structures are needed for scrambling. But as
explained above, none of them is entirely satisfying. The most convincing one is V-
TAG, since this formalism can deal with scrambling, lexicalized V-TAG is polynomially
parsable, and the set of languages V-TAG generates contains the set of all tree-adjoining
languages (TALs) (in particular, the copy language). Furthermore, a large range of word
order phenomena has been treated with V-TAG, and thereby the usefulness of V-TAG
for linguistic applications has been shown. But as already mentioned, V-TAG has the
inconvenience of being a nonlocal formalism. For the reasons explained above, it is
desireable to find a local TAG extension for scrambling (as opposed to the nonlocality
of derivations in V-TAG, DSG, and nonlocal MCTAG) such that locality constraints
for movements follow only from the form of the elementary structures and from the
local character of derivations. This article proposes a local TAG variant that can deal
with scrambling (at least with an arbitrarily large set of scrambling phenomena), that
is polynomially parsable, and that properly extends TAG in the sense that the set of all
TALs is a proper subset of the languages it generates.
In section 2, tree-local MCTAG with shared nodes (SN-MCTAG) and in particular
restricted SN-MCTAG (RSN-MCTAG) are introduced, formalisms that extend TAG in
the sense mentioned above. Section 3 shows linguistic applications of RSN-MCTAG, in
particular, an analysis of scrambling. In section 4, a relation between RSN-MCTAG and
range concatenation grammar (RCG) (Boullier 1999, 2000) is established. This relation
allows us to show that certain subclasses of RSN-MCTAG are mildly context-sensitive
and therefore in particular polynomially parsable. These subclasses do not cover all
cases of long-distance scrambling but, in contrast to TAG, they cover an arbitrarily large
4 More precisely, only the root of the new elementary tree and eventually (i.e., in the case of an adjunction)
the foot node get identified with the node the new tree attaches to. But there is no unification of whole
subtrees. Consequently, every edge occurring in the derived tree comes from exactly one edge in an
elementary tree, and every edge from the elementary trees used in the derivation occurs exactly once in
the derived tree. In this sense the operation is resource-sensitive.
192
Kallmeyer Multicomponent TAGs with Shared Nodes
set, providing scrambling analyses that respect the CETM. This means that the limit they
impose on the complexity of the scrambling data one can analyze is variable. Based on
empirical studies, it can be chosen sufficiently great such that the grammar covers all
scrambling cases that one assumes to occur.
2. The Formalism
An informal introduction of (restricted) tree-local MCTAG with shared nodes can also
be found in Kallmeyer and Yoon (2004).
2.1 Motivation: The Idea of Shared Nodes
Let us consider again example (1) in order to illustrate the general idea of shared nodes.
In standard TAG, nodes to which new elementary trees are adjoined or substituted
disappear; that is, they are replaced by the new elementary tree. For example, after
having performed the derivation steps shown in Figure 2, the root node of the reparieren
tree does not exist any longer. It is replaced by the verspricht tree, and its daughters have
become daughters of the foot node of the verspricht tree. That is, the root node of the
derived tree is considered to belong only to the verspricht tree. Therefore, an adjunction
at that node is an adjunction at the verspricht tree.
However, this standard TAG view is not completely justified: In the derived tree,
the root node and the lower S node might as well be considered to belong to reparieren,
since they are results of identifying the root node of reparieren with the root and the foot
node of verspricht.5 Therefore, we propose that the two nodes in question belong to both
verspricht and reparieren. In other words, these nodes are shared by the two elementary
trees. Consequently, they can be used to add new elementary trees to verspricht and (in
contrast to standard TAG) also to reparieren.
In the following, we use an MCTAG, and we assume tree-locality; that is, the nodes
to which the trees of such a set are added must all belong to the same elementary tree.
Standard tree-local MCTAGs are weakly and even strongly equivalent to TAGs, but they
allow us to generate a richer set of derivation structures. In combination with shared
nodes, tree-local multicomponent derivation extends the weak generative power of the
grammar (see Figure 4 for a sample tree-local MCTAG with shared nodes that generates
a language that is not a tree-adjoining language).6
Let us go back to example (1). Assume the tree set in Figure 3 for the scrambled NP
es. If the idea of shared nodes is adopted, this tree set can be added to reparieren using
the root of the derived tree for adjunction of the first tree and the NPacc substitution
node for substitution of the second tree. The operation is tree-local, since both nodes are
part of the reparieren tree.
5 Actually, in a feature-structure based TAG (FTAG) (Vijay-Shanker and Joshi 1988), the top feature
structure of the root of the derived tree is the unification of the top of the root of verspricht and the top of
the root of reparieren. The bottom feature structure of the lower S node is the unification of the bottom of
the foot of verspricht and the bottom of the root of reparieren. In this sense, the root of the reparieren tree
gets split into two parts. The upper part merges with the root node of the verspricht tree, and the lower
part merges with the foot node of the verspricht tree.
6 In a way, the idea of node sharing is already present in description-based definitions of TAG-related
formalisms (see Vijay-Shanker 1992; Rogers 1994; Kallmeyer 2001). This is why these formalisms are
monotonic with respect to the node properties described in the tree descriptions. However, the possibility
of exploiting this in order to obtain multiple adjunctions combined with multicomponent tree
descriptions has not been pursued so far.
193
Computational Linguistics Volume 31, Number 2
Figure 3
Derivation of (1) dass es der Mechaniker zu reparieren verspricht (?that the mechanic promises to
repair it?) using shared nodes.
The notion of shared nodes means in particular that a node can be used for more
than one adjunction. (E.g., in Figure 3, two trees were adjoined at the root of the
reparieren tree.) A similar idea has led to the definition of extended derivation in Schabes
and Shieber (1994). For certain auxiliary trees, Schabes and Shieber allow more than one
adjunction at the same node. However, the definition of the derived tree in Schabes and
Shieber (1994) is such that if first ?1 and then ?2 are adjoined at some node ? (i.e., in
the derivation tree there are edges from some ? to ?1 and ?2, both with the position
p of the node ? in ?), then first the whole tree derived from ?1 is added to position p,
and afterwards the whole tree derived from ?2 is added to position p. In other words,
before ?2 is adjoined, all the trees to be added by adjunction or substitution to ?1 must
be added. This is different in the case of shared nodes: After ?1 and then ?2 have been
adjoined, the root node of ?2 in the derived tree is shared by ?1 and ?2 and consequently
can be used for adjunctions at ?1.7 In other words, trees to be adjoined at the roots of
?1 and ?2 can be adjoined in any order. This is important for obtaining all the possible
permutations of scrambled elements.
2.2 Formal Definition of Tree-Local MCTAG with Shared Nodes
As already mentioned, the idea of tree-local MCTAG with shared nodes is the following:
In the case of a substitution of an elementary tree ? into an elementary tree ?, in
the resulting tree, the root node of the subtree ? is considered to be part of ? and
of ?. Similarly, when an elementary tree ? is adjoined at a node that is part of the
elementary trees ?1, . . . ,?n, then in the resulting tree, the root and foot node of ? are
both considered to be part of ?1, . . . ,?n and ?. Consequently, if an elementary tree ??
is added to an elementary tree ?, and if there is then a sequence of adjunctions at root
7 In this case, one obtains crossed dotted edges in the SN-derivation structure defined later (see Figure 14
for an example).
194
Kallmeyer Multicomponent TAGs with Shared Nodes
or foot nodes starting from ??, then each of these adjunctions can be considered an
adjunction at ?, since it takes place at a node shared by ?,??, and all the subsequently
adjoined trees.
Therefore, one way to define SN-MCTAG refers to the standard TAG derivation tree
in the following way. Define the grammar as an MCTAG and then allow only derivation
trees that satisfy the following tree-locality condition: For each instance {?1, . . . ,?k} of
an elementary tree set in the derivation tree, there is a ? such that each of the ?i is either
a daughter of ? or is linked to one of the daughters of ? by a chain of adjunctions at root
or foot nodes.
As an example, consider the derivation tree for (1) in Figure 3. It shows that the trees
used in the derivation are the reparieren tree, the verspricht tree, the Mechaniker tree, and
the two trees es and -es from the tree set in Figure 3. -es is substituted into reparieren
at position 21, and verspricht is adjoined to reparieren at position . Then, Mechaniker is
substituted into verspricht at position 1, and es is adjoined to verspricht at position . The
derivation is tree-local in the node-sharing sense, since for the tree set {-es, es}, -es
is a daughter of reparieren in the derivation tree and es is linked to reparieren by a first
adjunction of verspricht to reparieren and a further adjunction of es to the root of verspricht.
In the following, we adopt this way of viewing derivations in SN-MCTAG as
specific multicomponent TAG derivations; that is, we define SN-MCTAG as a variant
of MCTAG. This avoids formalizing a notion of shared nodes, even though this was the
starting motivation for the formalism.
We assume a definition of TAG as a tuple G = ?I, A, N, T? with I being the set of
initial trees, A the set of auxiliary trees, and N and T the nonterminal and terminal
node labels, respectively (see, for example, Vijay-Shanker [1987] for a formal definition
of TAG). Now we formally introduce multicomponent tree-adjoining grammars (Joshi
1987; Weir 1988):
Definition 1
A multicomponent tree-adjoining grammar is a tuple G = ?I, A, N, T,A? such that
 GTAG := ?I, A, N, T? is a TAG;
 A ? P(I ? A) is a set of subsets of I ? A, the set of elementary tree sets.8
? ? ?? is a multicomponent derivation step in G iff there is an instance {?1, . . . ,?n}
of an elementary tree set in A and there are pairwise different node addresses p1, . . . , pn
such that ?? = ?[p1,?1] . . . [pn,?n], where ?[p1,?1] . . . [pn,?n] is the result of adding the
?i (1 ? i ? n) at node positions pi in ?.9
As in TAG, a derivation starts from an initial tree, and in the end, in the final derived
tree, there must not be any obligatory adjunction constraint, and all leaves must be
labeled by a terminal or by the empty word.
In each MCTAG derivation step, an elementary tree set is chosen, and the trees from
this set are added to the already derived tree. Since they are added to pairwise different
8 P(X) is the set of subsets of some set X.
9 As usual (see Vijay-Shanker 1987; Weir 1988), ?[p,??] is defined as follows: If ?? is (derived from) an
initial tree and the node at position p in ? is a substitution node, then ?[p,??] is the tree one obtains by
substitution of ?? into ? at node position p. If ?? is (derived from) an auxiliary tree and the node at
position p in ? is an internal node, then ?[p,??] is the tree one obtains by adjunction of ?? to ? at node
position p. Otherwise ?[p,??] is undefined.
195
Computational Linguistics Volume 31, Number 2
nodes, one can just as well add them one after the other; that is, each multicomponent
derivation in an MCTAG G = ?I, A, N, T,A? corresponds to a derivation in the TAG
GTAG := ?I, A, N, T?. Let us define the TAG derivation tree of such a multicomponent
derivation as the corresponding derivation tree in GTAG. We can then define tree-local,
set-local, and nonlocal MCTAG and also the different variants of SN-MCTAG this article
deals with by putting different constraints on this derivation tree.10 Note that for each
operation ?[p,?i], the node address p in the derived tree ? points at a node that is
at some address p? in some elementary tree ?? that was already added (?? and p? are
unique). In the TAG derivation tree, there will be in this case an edge from ?? to ?i with
position p?.
A TAG derivation tree can be considered a tuple of nodes and edges. As usual
in finite trees, the edges are directed from the mother node to the daughter. Linear
precedence is not needed in a derivation tree, since it does not influence the result of the
derivation. So a derivation tree is a tuple ?N , E?, with N being a finite set of instances of
elementary trees and with E ? N ?N ? IN?, where IN? is the set of Gorn addresses. We
define the parent relation as the relation between mothers and daughters in a derivation
tree, the dominance relation as the reflexive transitive closure of the parent relation,
and the node-sharing relation as the relation between nodes that either are mother and
daughter or are linked first by a substitution/adjunction and then a chain of adjunctions
at root or foot nodes:
Definition 2
Let D = ?N , E? be a derivation tree in a TAG.
 PD := {?n1, n2? |n1, n2 ? N , and there is a p ? IN? such that ?n1, n2, p? ? E}
is the parent relation in D.
 DD := {?n1, n2? |n1, n2 ? N , and either n1 = n2, or there is a n3 such that
?n1, n3? ? PD and ?n3, n2? ? DD} is the dominance relation in D.
 SND := {?n1, n2? | either ?n1, n2? ? PD or there are t1, . . . , tk ? N , such that
?n1, t1? ? PD, n2 = tk and for all j, 1 ? j ? k ? 1: ?ti, ti+1, p?? ? E with either
p? =  or ti being an auxiliary tree with foot node address p?} is the
node-sharing relation in D.
A node-sharing relation ??1,?2? that corresponds to an actual parent relation is
called a primary node-sharing relation, and ?2 is called a primary SN-daughter of ?1,
whereas any other node-sharing relation ??1,?2? is called secondary and in this case ?2
is called a secondary SN-daughter of ?1.
The TAG derivation trees for MCTAG derivations have certain properties resulting
from the requirement that the elements of instances of elementary tree sets must be
added simultaneously to the already derived tree: First, if an elementary tree set is used,
then all trees from this set must occur in the derivation tree. Secondly, one tree from an
elementary tree set cannot be substituted or adjoined into another tree from the same
set, and, thirdly, two tree sets cannot be interleaved. For nonlocal MCTAG, these are all
constraints the TAG derivation tree needs to satisfy.
10 This TAG derivation tree is not the MCTAG derivation tree defined in Weir (1988). The nodes of Weir?s
MCTAG derivation trees are labeled by sequences of elementary trees (i.e., by elementary tree sets), and
each edge stands for simultaneous adjunctions/substitutions of all elements of such a set.
196
Kallmeyer Multicomponent TAGs with Shared Nodes
Lemma 1
Let G = ?I, A, N, T,A? be an MCTAG, GTAG := ?I, A, N, T?. Let D = ?N , E? be a derivation
tree in GTAG with the corresponding derived tree t being in L(GTAG).
D is a possible TAG derivation tree in G with t ? L(G) iff D is such that
 (MC1) The root of D is an instance of an initial tree ? ? I and all other
nodes are instances of trees from tree sets in A such that for all instances ?
of elementary tree sets from A and for all ?1,?2 ? ?, if ?1 ? N , then
?2 ? N .
 (MC2) For all instances ? of elementary tree sets from A and for all
?1,?2 ? ?, ?1 = ?2: ??1,?2? ? DD.
 (MC3) For all pairwise different instances ?1,?2, . . . ,?n, n ? 2, of
elementary tree sets from A, there are no ?(i)1 ,?
(i)
2 ? ?i, 1 ? i ? n, such that
??(1)1 ,?
(n)
2 ? ? DD and ??
(i)
1 ,?
(i?1)
2 ? ? DD for 2 ? i ? n.
The proof of this lemma is given in the appendix. The lemma gives us a way
to characterize nonlocal MCTAG via the properties of the TAG derivation trees the
grammar licenses. With this characterization we get rid of the original simultaneity
requirement: The corresponding properties are now captured in the three constraints
(MC1)?(MC3). But since these constraints need to hold only for the TAG derivation
trees that correspond to derived trees in the tree language, subderivation trees need not
satisfy them. In other words, ?1 and ?2 from the same elementary tree set can be added
at different moments of the derivation as long as the final complete TAG derivation tree
satisfies (MC1)?(MC3).
We now define tree-local, set-local, SN-tree-local, and SN-set-local TAG derivation
trees by imposing further conditions. Basically, the difference between the first two and
their SN variants is that in the first two, the definition refers to the parent relation,
whereas in the second two, it refers to the node-sharing relation.
Definition 3
Let G = ?I, A, N, T,A? be an MCTAG. Let D = ?N , E? be a TAG derivation tree for some
t ? L(?I, A, N, T?).
 D is a multicomponent derivation tree iff it satisfies (MC1)?(MC3).
 D is tree-local iff for all instances {?1, . . . ,?n} of elementary tree sets
with ?1, . . . ,?n ? N , there is one ? such that ??,?1?, . . . , ??,?n? ? PD.
 D is set-local iff for all instances {?1, . . . ,?n} of elementary tree sets with
?1, . . . ,?n ? N , there is an instance ? of an elementary tree set such that
for all 1 ? i ? n, there is a ti ? ? with ?ti,?i? ? PD.
 D is SN-tree-local iff for all instances {?1, . . . ,?n} of elementary tree
sets with ?1, . . . ,?n ? N , there is one ? such that ??,?1?, . . . , ??,?n?
? SND.
 D is SN-set-local iff for all instances {?1, . . . ,?n} of elementary tree sets
with ?1, . . . ,?n ? N , there is an instance ? of an elementary tree set such
that for all 1 ? i ? n, there is a ti ? ? with ?ti,?i? ? SND.
197
Computational Linguistics Volume 31, Number 2
Figure 4
SN-MCTAG for {w3 |w ? T?}.
The formalism we are proposing for scrambling is MCTAG with SN-tree-local TAG
derivation trees. We call these grammars tree-local MCTAGs with shared nodes:
Definition 4
Let G be an MCTAG. G is a tree-local MCTAG with shared nodes iff the set of trees
generated by G, LT(G), is defined as the set of those trees that can be derived with an
SN-tree-local multicomponent TAG derivation tree in G.
As usual, the string language LS(G) is then defined as the set of strings yielded by the
trees in LT(G).
All tree-adjoining languages can be generated by SN-MCTAGs, since a TAG cor-
responds to an MCTAG with unary multicomponent sets. For such an MCTAG, each
TAG derivation tree is trivially SN-tree-local. In other words, in this case the tree sets
are the same, whether the grammar is considered a TAG, a tree-local MCTAG, or an
SN-MCTAG.11 In particular, all TAG analyses proposed so far can be maintained, since
each TAG is trivially also an instance of SN-MCTAG.
SN-MCTAG is a proper extension of TAG (and of tree-local MCTAG) in the sense
that there are languages that can be generated by an SN-MCTAG but not by a TAG.
As an example, consider Figure 4, which shows an SN-MCTAG for {www |w ? T?}.12
Similar to the grammar in Figure 4, for all copy languages {wn |w ? T?} for some n ? IN,
an SN-MCTAG can be found. Other languages that can be generated by SN-MCTAG
and that are not TALs are the counting languages {an1 . . . ank |n ? 1} for any k > 4 (for
k ? 4, these languages are tree-adjoining languages).
There are two crucial differences between V-TAG and SN-MCTAG: First, in V-
TAG, the adjunctions of auxiliary trees from the same set need not be simultaneous.
In this respect, V-TAG differs not only from SN-MCTAG, but from any of the different
11 However, viewing a TAG as an SN-MCTAG allows us to obtain a richer set of SN-derivation structures,
as introduced in the next section. This is exploited in Kallmeyer (2002) for semantics.
12 The subscript NA in the figure stands for null adjunction; that is, it disallows adjunctions at the node in
question.
198
Kallmeyer Multicomponent TAGs with Shared Nodes
MCTAGs mentioned above. Secondly, V-TAG is nonlocal in the sense of nonlocal MC-
TAG, whereas SN-MCTAG is local, even though the locality is not based on the parent
relation in the derivation tree, as is the case in standard local MCTAG, but on the SN-
dominance relation in the derivation tree. As a consequence of the locality, we do not
need dominance links (i.e., dominance constraints that have to be satisfied by the de-
rived tree) in SN-MCTAG, in contrast to other TAG variants for scrambling. The locality
condition put on the derivation sufficiently constrains the possibilities for attaching
the trees from elementary tree sets: Different trees from a tree set attach to different
nodes of the same elementary tree. Consequently, the dominance relations among these
different nodes determine the dominance relations among the different trees from the
tree set. Therefore extra dominance links are not necessary. This is different for nonlocal
TAG variants such as V-TAG or DSG, in which one can in principle attach the different
components of an elementary structure at arbitrary nodes in the derived tree.
2.3 SN-MCTAG and Scrambling: Formal Considerations
Figure 5 shows an SN-MCTAG generating a language that cannot even be generated by
linear context-free rewriting systems (see Becker, Rambow, and Niv [1992] for a proof),
and therefore not by set-local MCTAG. This example, however, concerns neither weak
nor strong generative capacity, but something that Becker, Rambow, and Niv (1992) call
derivational capacity: the derivation of nkvk must be such that the ?(i)th n and the ith v
come from the same elementary tree set in the grammar.
The grammar in Figure 5 works in the following way: Each derivation starts with ?.
Then a first instance of the tree set (yielding n1 and v1) is added to the N and V nodes in
?. For each further instance of the tree set (yielding ni and vi), ?v is adjoined to the root
node of the ?v tree of vi?1. Therefore all ?v adjunctions except the first are occurring
at root nodes, and consequently all ?v are (primary or secondary) SN-daughters of ?.
The ?n tree of ni can be adjoined to any of the root or foot nodes of the ?n that have
already been added, since in this way all adjunctions of ?n except the first one occur
at root or foot nodes, and therefore all these ?n are SN-daughters of ?. This allows us
to place ni at any position in the string already containing {n1, . . . , ni?1}, and thereby
any permutation of the ns can be obtained. Since all nodes in the derivation tree are SN-
daughters of ?, the derivation is SN-tree-local. Note that in the grammar in Figure 5,
there is no NA constraint on the foot node of the first auxiliary tree in the tree set. This
is crucial for allowing all permutations of the n1, . . . , nk. In this respect, the elementary
trees differ from what is usually done in TAG.
Becker, Rambow, and Niv (1992) argue that a formalism that cannot generate the
language in Figure 5 is not able to analyze scrambling in an adequate way. We think,
Figure 5
SN-MCTAG for {n?(k) . . .n?(1)vk . . . v1 | k ? 0, ni = n, vi = v, and ni and vi are in the same
elementary tree set and they were added in the ith derivation step for all i, 1 ? i ? k, and ? is a
permutation of (1, . . . , k)}.
199
Computational Linguistics Volume 31, Number 2
Figure 6
Predicate argument structure for SCR.
however, that this language is not exactly what one needs for scrambling. The assump-
tion underlying the language in Figure 5 is that ni is an argument of vi. But in this case,
instead of adding ni and vi at the same time, ni should be added to vi. If one makes
the additional assumption that argument NPs are added by substitution, then one can
require that the argument NPs have already been substituted (this is what Joshi, Becker,
and Rambow [2000] call the weak co-occurrence constraint), that is, that the tree for
vi contain ni. In this case, the language in Figure 5 is an appropriate test language for
scrambling. But we do not want to make this assumption.
Furthermore, there are more predicate-argument dependencies: vi is also an argu-
ment of vi?1 for i ? 2. This is what Joshi, Becker, and Rambow (2000) call the strong
co-occurrence constraint. In other words, the dependency tree should be as in Figure 6.
Additionally to the permutation of the n1, . . . , nk, also the vi can be moved leftward,
as long as they do not permute among themselves. Consequently, for scrambling data
(without extraposition), one rather wants to generate the following language: SCR :=
{w = ?(n1 . . . nkv1 . . . vk) | k ? 1, ni = n, vi = v, for all 1 ? i ? k, and ? is a permutation of
n1 . . . nkv1 . . . vk such that ni precedes vi in w for all 1 ? i ? k and vi precedes vi?1 in w
for all 1 < i ? k} with the derivation structure in Figure 6. An SN-MCTAG generating
this language is shown in Figure 7.
The SN-MCTAG in Figure 7 yields the following derivations: Either start with ?2, in
which case an instance of {?n,?n} must be added and nv is obtained with n depending
on v, or start with ?1 for v1, in which case, for all v except the leftmost one, the set
{?v1,?v1} is added, for the leftmost v, a set {?v2,?v2} is added, and for all the ns, sets
{?n,?n} are added. These sets can be added in any order; the auxiliary tree is always
adjoined to the root node of the already derived tree that is shared by all auxiliary trees
that have been used so far and by the first ?1. The initial tree is primarily substituted
Figure 7
SN-MCTAG for SCR.
200
Kallmeyer Multicomponent TAGs with Shared Nodes
into the argument slot it fills. So the only condition for adding such a tree set is that
the verb it depends on has already been added, since the tree of this verb provides the
substitution node for the initial tree. Therefore, since the lexical material is always left
of the foot node, one obtains that vi precedes vi?1 for all 1 < i ? k and ni precedes vi for
all 1 ? i ? k.
Note that in Figure 7, for a scrambled ni, the substitution node is filled with an
empty node, while the n is adjoined higher at a node that is not yet available in the
elementary structure of vi. So the combination of ni and vi cannot be precompiled here.
2.4 Restricted SN-MCTAG
When the formal properties of SN-MCTAG are examined, it becomes clear that the for-
malism is hard to compare to other local TAG-related formalisms, since in the derivation
tree, arbitrarily many trees can be secondary SN-daughters of a single elementary tree,
such that these secondary links are considered to be adjunctions to that tree. This means
that these secondary links are relevant for the SN-tree-locality of the derivation. An
example is the grammar in Figure 5, in which in each derivation step, the relevant node-
sharing relations are the links between ? and the two auxiliary trees of the new set.
This means that for a word of length k, there are k SN-daughters of ? that are relevant
for the SN-tree-locality of the derivation. The grammar in Figure 5 indicates that this
property of SN-MCTAG is at least partly responsible for the fact that SN-MCTAG
allows us to generate languages that are not even mildly context-sensitive (i.e., that
are not in the class of languages that can be generated by LCFRS). However, it would
be desirable to stay inside the class of mildly context-sensitive languages. Therefore, in
the following, we define a restricted version, RSN-MCTAG, that limits the number of
relevant secondary SN-daughters of an elementary tree. The restriction is obtained as
follows: We require that in each derivation step, among the SN-relations between the old
? and the new set ?, there be at least one primary SN-relation. The number of primary
SN-daughters of a specific elementary tree is limited, since the primary SN-daughters
correspond to substitutions/adjunctions at pairwise different nodes and the number of
nodes in an elementary tree is limited. Consequently, the number of relevant secondary
SN-daughters for a node is limited as well.
An example of a derivation satisfying the new constraint is that in Figure 3, in which
es is a secondary SN-daughter of reparieren, while the second element of the tree set, -es,
is a primary SN-daughter of reparieren.
Definition 5
 Let G = ?I, A, N, T,A? be an MCTAG. Let D = ?N , E? be the TAG derivation
tree of a tree t ? LT(?I, A, N, T?). D is RSN-tree-local iff for all instances
{?1, . . . ,?n} of an elementary tree set with ?1, . . . ,?n ? N , there is one ?
such that
1. ??,?1?, . . . , ??,?n? ? SND;
2. there is one i, 1 ? i ? n, with ??,?i? ? PD.
 An MCTAG G is called a restricted SN-MCTAG iff the set of trees
generated by G, LT(G), is defined as the set of those trees that can be
derived with an RSN-tree-local multicomponent TAG derivation tree in G.
201
Computational Linguistics Volume 31, Number 2
The first condition of the definition says that the grammar is SN-tree-local, and the
second condition ensures that at least one of the relevant SN-daughters of ? is a primary
SN-daughter, that is, an actual daughter of ?.
As for SN-MCTAG, all tree-adjoining languages can also be generated by RSN-
MCTAGs. The sample grammars in Figures 4 and 5 are not RSN-MCTAGs. We suspect
that there is no RSN-MCTAG that generates the language in Figure 5. But the grammar
in Figure 7 for the language SCR is an RSN-MCTAG.
It can be shown that for the TAG derivation trees of an RSN-MCTAG, the following
holds: For each instance of an elementary tree set ?, the ? to which all elements of ?
are linked by node-sharing relations with at least one primary link is unique (which is
not necessarily the case for general SN-MCTAG). This is formulated in the following
lemma:
Lemma 2
Let G = ?I, A, N, T,A? be an RSN-MCTAG. Let D = ?N , E? be a TAG derivation tree
in G.
Then for all instances {?1, . . . ,?n} of elementary tree sets with ?1, . . . ,?n ? N , there
is exactly one ? such that ??,?1?, . . . , ??,?n? ? SND, and there is one i, 1 ? i ? n, with
??,?i? ? PD.
For such an elementary tree set {?1, . . . ,?n}, with ? being the unique elementary
tree as described in the lemma, all ??,?i? ? SND \ PD, 1 ? i ? n, are called secondary
adjunction links in D. The proof of the lemma is given in the appendix.
Now we introduce the SN-derivation structure of a TAG derivation tree D in an
RSN-MCTAG. It consists of D enriched with additional links for the secondary adjunc-
tions. These links are equipped with the positions of the first substitutions/adjunctions
on the chain that corresponds to the secondary adjunctions.
Definition 6
Let G = ?I, A, N, T,A? be an RSN-MCTAG. Let D = ?N , E? be a TAG derivation
tree in G. The SN-derivation structure of D, DSN, is then DSN := ?N , E ??,
with E ? E ?.
 For all secondary adjunction links ??1,?2? in D with ?? and p such that
??1,??, p? ? E and ???,?2? ? DD: ??1,?2, p? ? E ?.
 These are all elements of E ?.
All e ? E are called primary edges in DSN, and all e ? E ? \ E are called secondary
edges in DSN.
With the notion of the SN-derivation structure, we can formulate the limitation on
the maximal number of secondary adjunctions to an elementary tree that we mentioned
at the beginning of this section:
Lemma 3
Let G = ?I, A, N, T,A? be an RSN-MCTAG. Then there is a constant c such that for all
TAG derivation trees D in G with SN-derivation structure DSN := ?N , E?, the following
holds:
202
Kallmeyer Multicomponent TAGs with Shared Nodes
There is no n ? N such that there exist m ? c + 1 pairwise different n1, . . . , nm such
that for all i, 1 ? i ? m, there is a p such that
 either ?n, ni, p? is a secondary edge in DSN;
 or ?n, ni, p? is a primary edge in DSN, and there are no n? and p? such that
?n?, ni, p?? is a secondary edge in DSN.
That this lemma holds is nearly immediate: Each secondary adjunction must be as-
sociated with a primary adjunction or substitution into the same tree instance. There
are at most k primary adjunctions or substitutions into any tree instance if k is the
maximal number of nodes per elementary tree. Consequently there are at most k ? n
secondary adjunctions per node if n + 1 is the maximal number of trees per elementary
tree set.
In linguistic applications, the SN-derivation structure is intended to reflect the
predicate-argument dependencies of a sentence in the following way: For each tree in
the SN-derivation structure, if this tree is secondarily adjoined to some other tree ?,
then it depends on ?. Otherwise it depends on its mother node in the TAG derivation
tree. In this way, the grammar for SCR in Figure 7 yields the desired dependency
structure.
3. Linguistic Applications
3.1 Scrambling with RSN-MCTAG
In this section, we present a small German grammar that allows us to analyze some
cases of scrambling. The aim is not an exhaustive treatment of the phenomenon, but
just to show that in principle, an analysis of scrambling in German is possible using
RSN-MCTAG. The data to which we restrict ourselves are word order variations of
example (3) without extraposition, that is, under the assumption that the order of the
verbs is zu reparieren zu versuchen verspricht:
(3) . . . dass er dem Kunden das Fahrrad zu reparieren
. . . that henom the customerdat the bikeacc to repair
zu versuchen verspricht
to try promises
?. . . that he promises the customer to try to repair the bike?
The elementary trees and tree sets for example (3) are shown in Figure 8. In contrast
to standard TAG practices, which are often guided by technical considerations, we
represent all arguments of a verb (including an embedded VP) by substitution nodes.
For those parts that might be scrambled, there is a single elementary tree (for the case
without scrambling) and a tree set used for scrambling. The tree set contains an auxiliary
tree that can be primarily or secondarily adjoined to some root node and a tree with the
empty word that is intended to fill the argument position. In order to avoid spurious
ambiguities, we assume that whenever a derivation using the single elementary tree is
possible, this is chosen.
A scrambled element always adjoins to a VP node, and the scrambled element is
to the left of the foot node. Therefore it precedes everything that is below or on the
203
Computational Linguistics Volume 31, Number 2
Figure 8
Elementary trees for scrambling.
right of the VP node to which it adjoins. Consequently, given the form of the verbal
elementary trees in Figure 8, in which the verb is always below or to the right of all VP
nodes allowing adjunction, the order x v for an x being a nominal or a verbal argument
of v is always respected.
For an element (a lexical item), the tree set for scrambling is used whenever one of
the following three cases holds:
 The element is scrambled.
 Scrambling of depth more than one out of the element takes place.
 The element intervenes between some element A (on its right) and some
element B (on its left) scrambled out of A, and the element itself does not
belong to A.
In other words, the fact that the set for scrambling is used for some element does
not necessarily mean that this element is scrambled. It just means that one of the
three cases above holds, that is, that some scrambling around this element takes
place.
One could actually do without the single trees and always use the tree sets. In this
case, even if no scrambling took place, all argument slots would be filled by empty
words, and all lexical material would be adjoined to the root node of the derived tree.
At first glance, this seems rather odd. But if one does not consider the substitution
nodes argument slots but rather some kind of subcategorization features marking which
arguments need to be added, an analysis using only the tree sets makes sense. However,
for this article, we keep the single trees.
For example (3), a derivation without secondary adjunctions and using only the
single trees is possible. Let us consider the following word orders as examples of how
secondary adjunction is used for scrambling:
204
Kallmeyer Multicomponent TAGs with Shared Nodes
(4) . . . dass er1 [[das Fahrrad zu reparieren] zu versuchen]2 t1 dem Kunden t2
. . . that he the bike to repair to try the customer
verspricht
promises
(5) . . . dass er [das Fahrrad zu reparieren]1 dem Kunden [t1 zu versuchen]
. . . that he the bike to repair the customer to try
verspricht
promises
(6) . . . dass [das Fahrrad]1 er2 [[t1 zu reparieren] zu versuchen]3 t2 dem Kunden t3
. . . that the bike he to repair to try the customer
verspricht
promises
In example (4), the versuchen-VP and er are scrambled.13 Consequently, for versuchen and
er, the sets with two trees are used, whereas for all the other elements, the single trees
can be used. In example (5), the reparieren-VP is scrambled out of the versuchen-VP, with
dem Kunden intervening between the two. Therefore, the tree sets are used for reparieren
and dem Kunden. For versuchen, the single tree can be used, since the scrambling out of
versuchen is of depth one. In example (6), we have the same scrambling as in example (4),
and additionally, das Fahrrad is scrambled out of the reparieren-VP and the versuchen-VP
(depth two). Consequently, in this case one needs tree sets for Fahrrad, er, versuchen, and
reparieren.
Let us consider the analysis of example (4): Starting with verspricht, the single tree
for dem Kunden and the tree set for versuchen (with adjunction of the auxiliary tree at
the root) are added. This leads to the first tree in Figure 9. The VP nodes in boldface
type in the figure are shared by versuchen and verspricht; that is, they can be used for
further adjunction at the verspricht tree. (Of course, only the root node can be used for
adjunction, since the other nodes have NA constraints.) It does not matter in which order
er and zu reparieren are added. For er, the tree set is used. The auxiliary tree is secondarily
adjoined to the root node, and the initial tree is substituted for the NPnom node in the
verspricht tree. This leads to the second tree in Figure 9. For reparieren and das Fahrrad,
the single trees are added below the VP substitution node in the versuchen tree. The
corresponding SN-derivation structure (see Figure 9) contains the desired predicate-
argument dependencies. The TAG derivation tree is RSN-tree-local.
Next, let us consider example (5). Here, the single trees for er and versuchen are
added to verspricht. This leads to the first tree in Figure 10. The VP node in boldface type
in the figure belongs to verspricht and versuchen. It is next used for secondary adjunction
of dem Kunden to the verspricht tree. The initial tree is substituted at the NPdat slot. This
leads to the second tree. Here, the bold VP node belongs to verspricht, versuchen, and
Kunde. It is next used for secondary adjunction of the auxiliary tree of reparieren to
versuchen, while the initial tree is substituted for the VP leaf in the versuchen tree. This
13 Actually, er here is not really scrambled, but since in our formalism, scrambled elements attach at the left
of a VP, any other element even more to the left is treated as if it is scrambled (even if it depends on the
matrix verb).
205
Computational Linguistics Volume 31, Number 2
Figure 9
Analysis of example (4).
Figure 10
Analysis of example (5).
leads to the third tree. After that, one needs only to add the single tree for das Fahrrad
to reparieren. Note that this is a derivation in which the foot node of the elementary tree
containing the lexical material does not dominate the tree with the empty word.
Now let us consider the derivation of example (6). Here, only for dem Kunden, the
single tree is added by substitution. In all other cases, the tree set is used with (primary
206
Kallmeyer Multicomponent TAGs with Shared Nodes
or secondary) adjunction at the root node of the already derived tree. This root node
consequently belongs to all verbs that have already occurred in the derivation and can
therefore be used to add arguments to any of them.
We leave it to the reader to verify that all word orders can be generated. This kind
of analysis also works for more than two embeddings.
Since all scrambled elements attach to a VP node in the elementary tree of the verb
they depend on, they cannot attach to the VP of a higher finite verb that embeds the
sentence in which the scrambling occurs. In this way, a barrier effect is obtained without
establishing any explicit barrier, as is done in V-TAG. Instead, this locality of scrambling
is a consequence of the form of the elementary trees and of the locality of the derivations.
Concerning adjunct scrambling, each adjunct has a single auxiliary tree as in stan-
dard TAG and additionally a set of two auxiliary trees, a lower auxiliary tree with an
empty word and a higher auxiliary tree with the adjunct. This is shown in Figure 11.
The internal VP node of the higher tree in the tree set serves as an adjunction site for the
lower parts of other adjuncts. Similarly, the elementary trees of verbs need an extra VP
node in order to adjoin adverbs.
For more analyses of scrambling, including scrambling in combination with extra-
position and topicalization, and also for an extension of the analysis presented here to
Korean data, see Kallmeyer and Yoon (2004).
3.2 Raising Verbs and Subject-Auxiliary Inversion
Other phenomena often mentioned in the TAG literature (see, e.g., Rambow, Vijay-
Shanker, and Weir 1995; Kulick 2000; Dras, Chiang, and Schuler 2004) as being
problematic for TAG and tree-local MCTAG are sentences with raising verbs and
subject-auxiliary inversion, as in examples (7) and (8):
(7) Does Gabriel seem to be likely to eat gnocchi?
(8) What does John seem to be certain to like?
The standard TAG analyses of examples (7) and (8) (see Figure 12 for the analysis of
example (8)) start with the eat and like tree, respectively, adjoin an auxiliary tree for
likely and certain, respectively, and then add the trees for does and seem, respectively. If
we assume that these trees are in the same elementary tree set, then this last derivation
step is nonlocal, since the does tree adjoins to eat and like, respectively, while the seem
tree adjoins to likely and certain, respectively. Though different from scrambling, this
problem seems to be of a similar nature, and formalisms that have been proposed for
scrambling have also been used to treat these examples (see Kulick 2000).
RSN-MCTAG allows us to analyze examples (7) and (8) in a way that puts does and
seem into a single elementary tree set: After having adjoined to be likely and to be certain,
Figure 11
Trees for adjuncts.
207
Computational Linguistics Volume 31, Number 2
Figure 12
Derivation for (8).
respectively, the root nodes of the adjoined trees are considered still to be part of the
elementary trees of eat and like, respectively. These elementary trees can then be used to
add the elementary tree set for does and seem: Both auxiliary trees are adjoined to these
trees. Figure 12 shows the corresponding SN-derivation structure.
4. RSN-MCTAG and Range Concatenation Grammar
In the following, we show that for each RSN-MCTAG of a certain type (i.e., with
an additional restriction), a weakly equivalent simple range concatenation grammar
(Boullier 1999, 2000) can be constructed. It has been shown that RCGs generate ex-
actly the class of all polynomially parsable languages (Bertsch and Nederhof 2001;
appendix A). Furthermore, as shown in Boullier (1998b), simple RCGs in particular
are even weakly equivalent to linear context-free rewriting systems (Weir 1988). As a
consequence, one obtains that the languages generated by simple RSN-MCTAGs are
mildly context-sensitive. This last property was introduced in Joshi (1985). It includes
formalisms that are polynomially parsable, are semilinear, and allow only a limited
number of crossing dependencies. (We do not give formal definitions of mild context-
sensitivity and of LCFRS, since we do not need these definitions in this article.)
Concerning RSN-MCTAGs in general, that is, without any further restriction, we are
almost sure that they are not mildly context-sensitive. Perhaps they can even generate
languages that are not in the class of languages generated by RCGs.
4.1 Range Concatenation Grammars
This section defines range concatenation grammars.14
Definition 7
A range concatenation grammar is a tuple G = ?N, T, V, S, P? such that
 N is a finite set of predicates, each with a fixed arity;
 T and V are disjoint finite sets of terminals and of variables;
 S ? N is the start predicate, a predicate of arity 1;
 P is a finite set clauses of the form A0(x01, . . . , x0a0 ) ? ,
or A0(x01, . . . , x0a0 ) ? A1(x11, . . . , x1a1 ) . . .An(xn1, . . . , xnan ),
with n ? 1 and Ai ? N, xij ? (T ? V)? and ai being the arity of Ai.
14 Since throughout the article, we use only positive RCGs; whenever we say ?RCG,? we actually mean
?positive RCG.?
208
Kallmeyer Multicomponent TAGs with Shared Nodes
When applying a clause with respect to a string w = t1 ? ? ? tn, the arguments of the
predicates in the clause are instantiated with substrings of w, more precisely, with the
corresponding ranges. A range ?i, j? with 0 ? i < j ? n corresponds to the substring
between positions i and j, that is, to the substring ti+1 ? ? ? tj. If i = j, then ?i, j? corresponds
to the empty string . If i > j, then ?i, j? is undefined.
Definition 8
For a given clause, an instantiation with respect to a string w = t1 . . . tn consists of
a function f : {t? | t? is an occurrence of some t ? T in the clause} ? V ? {?i, j? | i ? j, i,
j ? IN} such that
 for all occurrences t? of a t ? T in the clause: f (t?) := ?i, i + 1? for some
i, 0 ? i < n, such that ti = t;
 for all v ? V: f (v) = ?j, k? for some 0 ? j ? k ? n;
 if consecutive variables and occurrences of terminals in an argument in the
clause are mapped to ?i1, j1?, . . . , ?ik, jk? for some k, then jm = im+1 for
1 ? m < k. By definition, we then state that f maps the whole argument to
?i1, jk?.
The derivation relation is defined as follows. For a predicate A of arity k, a clause
A(. . .) ? . . . , and ranges ?i1, j1?, . . . , ?ik, jk? with respect to a given w: If there is an instan-
tiation of this clause with left-hand side A(?i1, j1?, . . . , ?ik, jk?), then A(?i1, j1?, . . . , ?ii, jk?)
can be replaced with the right-hand side of this instantiation.
The language of an RCG G is the set of strings that can be reduced to the empty
word, that is, {w |S(?0, |w|?) ??  with respect to w}.15 An RCG with maximal predicate
arity n is called an RCG of arity n.
For illustration, let us consider a sample RCG: The RCG with N = {S, A, B},
T = {a, b}, V = {X, Y, Z}, start predicate S, and clauses S(X Y Z) ? A(X, Z) B(Y),
A(a X, a Y) ? A(X, Y), B(b X) ? B(X), A(,) ? , B() ?  has the string language
{anbkan | k, n ? IN}. Consider the reduction of w = aabaa:
We start from S(?0, 5?). First we can apply the following clause instantiation:
S(X Y Z ) ? A(X , Z ) B(Y )
?0, 2? ?2, 3? ?3, 5? ?0, 2? ?3, 5? ?2, 3?
aa b aa aa aa b
With this instantiation, S(?0, 5?) ? A(?0, 2?, ?3, 5?)B(?2, 3?). Then
B(b X ) ? B(X )
?2, 3? ?3, 3? ?3, 3?
b  
15 |w| is the length of the word w; that is, the range ?0, |w|? with respect to w corresponds to the whole
word w.
209
Computational Linguistics Volume 31, Number 2
and B() ?  lead to A(?0, 2?, ?3, 5?)B(?2, 3?) ? A(?0, 2?, ?3, 5?)B(?3, 3?) ? A(?0, 2?,
?3, 5?). Next,
A(a X a Y ) ? A(X , Y )
?0, 1? ?1, 2? ?3, 4? ?4, 5? ?1, 2? ?4, 5?
a a a a a a
leads to A(?0, 2?, ?3, 5?) ? A(?1, 2?, ?4, 5?). Then
A(a X a Y ) ? A(X , Y )
?1, 2? ?2, 2? ?4, 5? ?5, 5? ?2, 2? ?5, 5?
a  a   
and A(,) ?  lead to A(?1, 2?, ?4, 5?) ? A(?2, 2?, ?5, 5?) ? .
An RCG is said to be noncombinatorial if each of the arguments in the right-hand
sides of the clauses are single variables. It is said to be linear if no variable appears more
than once in the left-hand sides of the clauses and no variable appears more than once
in the right-hand side of the clauses. It is said to be nonerasing if for each clause, each
variable occurring in the left-hand side occurs also in the right-hand side and vice versa.
It is said to be simple if it is noncombinatorial, linear, and nonerasing.
Simple RCGs and LCFRSs are equivalent (Boullier 1998b).
4.2 Relation between RSN-MCTAG and Simple RCG
The goal of this section is to construct an equivalent simple RCG for a given RSN-
MCTAG. In order to be able to perform this construction, in the following we further
constrain the formalism of RSN-MCTAG by defining RSN-MCTAG of a specific arity
n. For this version of RSN-MCTAG, the construction of an equivalent simple RCG is
possible.
First, let us sketch the general idea of the transformation from TAG to RCG (see
Boullier 1998a). The RCG contains predicates ???(X) and ???(L, R) for initial and aux-
iliary trees, respectively. X covers the yield of ? and all trees added to ?, and L and R
cover those parts of the yield of ? (including all trees added to ?) that are to the left
and the right of the foot node of ?. The clauses in the RCG reduce the argument(s)
of these predicates by identifying those parts that come from the elementary tree ?/?
itself and those parts that come from one of the elementary trees added by substitution
or adjunction. A sample TAG with an equivalent RCG is shown in Figure 13.
For the construction of an equivalent RCG from a given RSN-MCTAG, we follow
the same ideas while considering a secondary adjunction of ? at some ? as adjunction
at ? and not as adjunction at the elementary tree that is the mother node of ? in the
TAG derivation tree. There are two main differences between RSN-MCTAG and TAG
that influence the construction of an equivalent RCG.
First, more than one tree can be added to a node. Therefore we allow predicates
of the form ???1 . . . ?k? and ??0?1 . . . ?k?. The first means that at the node in question,
first ? was added by substitution, and then ?1 . . . ?k (in this order) were secondarily
adjoined. The second means that at the node in question (an internal node), first ?0 was
primarily adjoined, and then ?1 . . . ?k were secondarily adjoined. Since the number of
secondary adjunctions at a node is limited by some constant depending on the grammar
210
Kallmeyer Multicomponent TAGs with Shared Nodes
Figure 13
A sample TAG and an equivalent RCG.
(see Lemma 3), k is limited as well, and therefore this extension with respect to TAG adds
only a finite number of predicates.
Second, the contribution of an elementary tree ?/? including the trees added
to it can be separated into arbitrarily many parts. Since each of the arguments
of the predicates in the RCG has to cover a true substring of the input string,
one needs predicates of arbitrary arities, namely, ?? . . .?(Ln, . . . , L1, X, R1, . . . , Rn) and
?? . . .?(Ln, . . . , L1, L0, R0, R1, . . . , Rn), for the case where n auxiliary trees were added at
the root of ?/? that were actually secondarily adjoined at some higher tree such that
these n trees separate the contribution of ?/? into 2n + 1 / 2n + 2 parts, respectively.
This extension is problematic, since it leads to an RCG with predicates of arbitrary arity:
a dynamic RCG (Boullier 2001), a variant of RCG that is not polynomially parsable and
that we therefore want to avoid. For this reason, we need an additional constraint on
the RSN-MCTAGs we employ.
An example in which the contribution of an elementary tree is separated into
three different parts is example (9), analyzed with the RSN-MCTAG in section 3.1
(see Figure 14). In the derived tree, the VP das Fahrrad zu reparieren zu versuchen (the
broken triangles), which is the contribution of versuchen, is separated into three parts,
Figure 14
Analysis of example (9).
211
Computational Linguistics Volume 31, Number 2
since reparieren secondarily adjoins at versuchen and das Fahrrad secondarily adjoins at
reparieren.
(9) . . . dass [das Fahrrad]1 er [t1 zu reparieren]2 dem Kunden [t2 zu versuchen]
. . . that the bike he to repair the customer to try
verspricht
promises
The crucial point in example (9) is that in the SN-derivation structure (see Figure 14),
there are two crossings of secondary edges inside one group of secondary links. This
means that the contribution of versuchen is interrupted twice by arguments of verspricht
(by Kunde and er). In order to avoid predicates of arbitrary arity, we therefore limit
the number of crossings of secondary links. We define the arity of an RSN-MCTAG
depending on the maximal number of crossings that are allowed.
First, we define special subgraphs of the SN-derivation structure, secondary
groups. These are subgraphs consisting of a chain of one primary substitu-
tion/adjunction and subsequent adjunctions at root or foot nodes such that there are
secondary adjunctions along the whole chain. For example, the nodes verspricht, zu
versuchen, Kunde, zu reparieren, er, and Fahrrad in the SN-derivation structure in Figure 14
form such a group. For an SN-derivation structure of a certain arity, the number of
crossings of secondary edges inside a single secondary group is then limited: For an
SN-derivation structure of arity n, the number of crossings of secondary edges per
secondary group is limited to n2 ? 1. In other words, if i is the maximal number of
crossings, then 2(i + 1) is the arity of the grammar. Of course, the arity is chosen such
that an equivalent RCG of the same arity can be constructed. TAG, for example, is a
grammar with 0 crossings, that is, an arity 2(0 + 1) = 2 if the grammar is viewed as an
SN-MCTAG, and the corresponding RCG is actually of arity 2.
Definition 9
Let DSN = ?N , E? be a SN-derivation structure.
1. ?N ?, E ?? is a secondary group in DSN iff
 N ? = {n0, n1, . . . , nk} ? N for some k > 1 such that there are
primary edges ?ni, ni+1, pi? for 0 ? i < k with pi ? IN;
 E ? ? E such that for all n, n? ? N , p ? IN with ?n, n?, p? ? E : if
n, n? ? N ?, then ?n, n?, p? ? E ?;
 for all i, 0 < i < k, there are i1, i2 with i1 ? i ? i2, i1 = i2, such that
?ni1 , ni2 , p? ? E ? is a secondary edge in D for some p ? IN.
2. DSN is of arity n iff for each secondary group ?N ?, E ?? in DSN with primary
edges ?n0, n1, p0?, ?n1, n2, p1?, . . . , ?nk?1, nk, pk?1? as above, there are at most
i ? n2 ? 1 pairwise different sets of the form {j0, j1, j2, j3} such that
j0 < j1 < j2 < j3 and there are secondary edges ?nj0 , nj2 , p1? and ?nj1 , nj3 , p2?
for some p1, p2 ? IN.
Definition 10
Let G be an MCTAG, n ? 1. G is a restricted tree-local MCTAG with shared nodes of
arity n iff the set of trees generated by G, LT(G), is defined as the set of those trees that
can be derived in G with an RSN-tree-local multicomponent TAG derivation tree such
that the corresponding SN-derivation structure is of arity n.
212
Kallmeyer Multicomponent TAGs with Shared Nodes
Figure 15
Sample RSN-MCTAG of arity four.
Consider a simple example of a construction of an equivalent RCG for a given
RSN-MCTAG. We choose an RSN-MCTAG of arity four, and we see that the arity
of the corresponding RCG is four as well. The RSN-MCTAG is shown in Figure 15.
Whether this grammar is considered to be a general RSN-MCTAG or an RSN-MCTAG
of arity four does not matter in this case, since even in the general case, all possible
SN-derivation structures are of arity four. However, in the case of other RSN-MCTAGs,
the restriction to a certain arity might exclude certain TAG derivation trees and thereby
decrease the language generated by the grammar.
The language generated by the RSN-MCTAG in Figure 15 is {er zu kommen (zu
versuchen)? verspricht, zu kommen (zu versuchen)+ er (zu versuchen)? verspricht}. The SN-
derivation structures corresponding to the different strings are shown in Figure 15. The
last one contains one crossing of secondary links; that is, the RSN-MCTAG is of arity
four.
Now let us look at the corresponding RCG. Since the arity of the RSN-MCTAG is
four, the predicates of the corresponding RCG are of arity three (for initial trees) and
four (for auxiliary trees).
The contribution of ?1 is never separated into parts, therefore the first and the third
arguments of the predicate ??1? are always . Looking at the SN-derivation structures
in Figure 15, we have three different possibilities for ??1?:
213
Computational Linguistics Volume 31, Number 2
??1?(,LN V verspricht R,) ? ??2?(,L,R,)???2?(,N,)??2?(,V,) |
??1?2?(,L,R,)???2?(,N,)???1?(,V,) |
??2?1?(,L,R,)???2?(,N,)???1?(,V,)
The interesting part of the grammar is the clauses for ??1?2?, where two trees were
added to the same node and further adjunctions at the root of ?1 are possible. The point
is that the part covered by ?1 and the trees added to it can be separated into different
substrings. This leads to
??1?2?(,L1 L2 L3,R3 R2 R1,) ? ??1?(L1,L3,R3,R1)??2?(,L2,R2,)
??1?2?(L1,L2 L3,R3 R2,R1) ? ??1?(L1,L3,R3,R1)??2?(,L2,R2,)
??1?2?(L1 L2,L3,R3,R2 R1) ? ??1?(L1,L3,R3,R1)??2?(,L2,R2,)
Concerning ??2?1?, the contribution of ?2 cannot be separated into different parts,
since nothing can be adjoined to ?2. Consequently
??2?1?(,L1 L2,R2 R1,) ? ??2?(,L2,R2,)??1?(,L1,R1,)
??2?1?(L1,L2,R2,R1) ? ??2?(,L2,R2,)??1?(,L1,R1,)
Concerning ??1?(L1, L3, R3, R1), either (L1, R1) cover something adjoined to ?1 (this
can only be ?1), or (L1, R1) cover something adjoined to something . . . adjoined to ?1.
In this case, (L3, R3) cover ?1 and ?1 adjoined to its root, respectively. This leads to
??1?(L1,L2 V zu versuchen,R2,R1) ? ??1?(L1,L2,R2,R1)???1?(,V,)
If the inner parts are empty, the outer parts are moved:
??1?(L,,,R) ? ??1?(,L,R,)
Furthermore, there is a clause for ??1? without adjunction at the root:
??1?(,V zu versuchen,,) ? ??2?(,V,)
For elementary trees where nothing needs to be added, simple -clauses are introduced:
??2?(, zu kommen,) ?  ???1?(,,) ? 
??2?(,er,,) ?  ???2?(,,) ? 
In order to see how the RCG simulates the RSN-MCTAG, let us consider the deriva-
tion for zu kommen zu versuchen er zu versuchen verspricht:
??1?(, zu kommen zu versuchen er zu versuchen verspricht,)
? ??1?2?(, zu kommen zu versuchen er zu versuchen,,)???2?(,,)???1?(,,)
?? ??1?(zu kommen zu versuchen, zu versuchen,,)??2?(,er,,)
?? ??1?(zu kommen zu versuchen,,,)???1?(,,)
?? ??1?(, zu kommen zu versuchen,,)
? ??2?(, zu kommen,) ? 
214
Kallmeyer Multicomponent TAGs with Shared Nodes
This example should give an idea of how an equivalent RCG for a given RSN-MCTAG
of arity n can be constructed.
As already mentioned, in an RSN-MCTAG, the number of substitutions and
(primary or secondary) adjunctions that can occur at each node is limited (see Lemma 3).
Therefore, the number of predicates needed in the corresponding RCG is limited as well.
Furthermore, in an RSN-MCTAG of arity n, the contribution of an elementary tree is
separated into at most n parts. This still needs to be shown:
Lemma 4
Let G be an RSN-MCTAG of arity n. Then for all w in the string language of G and for
all elementary trees ? used to derive w in G, the contribution of ?, that is, the yield of ?
and everything added to ?, is separated into at most n parts.
The proof is given in the appendix.
Theorem 1
For each RSN-MCTAG G of arity n, a simple RCG G? of arity n can be constructed such
that L(G) = L(G?).
The construction algorithm and a sketch of the proof are presented in the appendix. As
a consequence of this theorem, the following corollary holds:
Corollary
For a given n, the string languages generated by RSN-MCTAGs of arity n are mildly
context-sensitive, and they are in particular polynomially parsable.
Since we have shown that for RSN-MCTAG with a fixed arity, one obtains gram-
mars that are LCFRSs, we know that we can even construct a weakly equivalent set-
local MCTAG. This set-local MCTAG, however, does not present an alternative to the
RSN-MCTAG with fixed arity: It is very large, containing a large number of elementary
trees per tree set (the number depends on the arity of the grammar) and, furthermore, a
large number of trees without lexical material and a large number of internal nodes that
are needed only to provide adjunction sites.
An example is the set-local MCTAG in Figure 16. It is weakly equivalent to the
RSN-MCTAG of arity four in Figure 15, and it even gives the correct dependency
structure. The verspricht tree contains several VP nodes that are needed in order to
provide adjunction sites for the different parts of er and versuchen. The versuchen tree
set needs an extra auxiliary tree that provides an additional VP node for adjunction
and has to be separated from the tree containing versuchen, since the contribution
of versuchen might be separated into different parts. Of course this little grammar
is still simple, since there are almost no possibilities of adjoining different trees at
the same node or of separating the contribution of one lexical item into different
parts.
As we have seen in Lemma 4, the linguistic signification of restricting the arity
of the grammar to some n is that the lexical material containing a verb, all its argu-
ments (including arguments and adjuncts of these arguments, etc.), and all its adjuncts
cannot be separated into more than n discontinuous substrings in the whole sentence.
For example, an RSN-MCTAG of arity two with elementary tree sets similar to those
proposed above for scrambling would not be able to analyze example (9). However,
RSN-MCTAGs of arity n for some sufficiently large fixed n can perhaps even describe
215
Computational Linguistics Volume 31, Number 2
Figure 16
Equivalent set-local MCTAG for the RSN-MCTAG from Figure 15.
all cases of scrambling: See again the analysis of example (9) in Figure 14. Here, the
contribution of versuchen and its arguments is split only by other elements secondarily
adjoined to verspricht. If only a limited number of such secondary adjunctions were
possible (this is the case), and if none of these other secondarily adjoined elements
allowed for further secondary adjunctions at its root or foot node (this still needs to
be investigated), then the number of crossings might be limited. We leave this issue for
further research.
Even if RSN-MCTAG with a fixed arity could not analyze all scrambling data, based
on empirical studies, n could be chosen sufficiently great such that the grammar would
cover all scrambling cases that one assumes to occur.16 The important point is that the
complexity limit given by the fixed n is variable; that is, an arbitrary n can be chosen.
This is different from TAG, for example, in which the limit is fixed (assuming, of course,
that we desire only analyses respecting the CETM). In this sense one can say that RSN-
MCTAG can analyze scrambling in general.
5. Conclusion and Future Work
This article addresses the problem of scrambling in tree-adjoining grammar, a formalism
known not to be powerful enough to treat scrambling phenomena. In order to keep
the advantages of TAG while being able to analyze scrambling, a local TAG variant is
proposed that is based on the notion of node sharing, so-called (restricted) tree-local
multicomponent TAG with node sharing. RSN-MCTAG is a true extension of TAG in
the sense that the formalism can generate all tree-adjoining languages. The analysis of
some German scrambling data is sketched in order to show that this TAG extension can
treat scrambling.
Then, RSN-MCTAGs of specific arities are defined, and it is shown that for each
RSN-MCTAG of a fixed arity n, an equivalent simple RCG of arity n can be constructed.
Simple RCGs are mildly context-sensitive and in particular polynomially parsable and
therefore, this also holds for RSN-MCTAGs of a fixed arity. RSN-MCTAGs of arity n
perhaps cannot analyze all scrambling phenomena but, if the n is appropriately chosen,
it can analyze an arbitrarily large set.
16 Joshi, Becker, and Rambow (2000) even argue that there might be a competence limit regarding the
complexity of scrambling data. However, we do not discuss this issue here.
216
Kallmeyer Multicomponent TAGs with Shared Nodes
The scrambling data analyzed in section 3 present just a small part of the possible
scrambling configurations. As already noted, this article does not present an exhaustive
treatment of the phenomenon. Even though the examples we looked at indicate that
RSN-MCTAGs are able to deal with scrambling, an exhaustive analysis of a larger
amount of data still needs to be done, in particular, of scrambling in combination
with other ?movements? that cause word order variations, such as topicalization or
extraposition. A first proposal in this direction can be found in Kallmeyer and Yoon
(2004), but this proposal does not cover all phenomena one needs to take into account.
So this is still an important issue for further research.
A formal issue one would like to see investigated more in detail is the relations
between the different types of MCTAG. We have shown that the languages of RSN-
MCTAG with fixed arity are in the class of set-local MCTALs. Furthermore, general
SN-MCTAG can generate languages that cannot be generated by set-local MCTAG.
However, this leaves open many interesting questions concerning the relations be-
tween set-local MCTAG and non-local MCTAG and the different formalisms defined
in this article, namely, RSN-MCTAG of fixed arity and RSN-MCTAG and MCTAG with
SN-tree-local and SN-set-local derivations. We plan to address these questions in the
future.
Appendix: Proofs
Proof of Lemma 1
Let G = ?I, A, N, T,A? be an MCTAG, GTAG := ?I, A, N, T?. Let D = ?N , E? be a derivation
tree in GTAG with corresponding derived tree t ? L(GTAG).
1. First show ? of the iff: Let D be a TAG derivation tree with t ? L(G). It is
immediate that the root of D is an instance of an initial tree and that all other nodes
are elements of instances of elementary tree sets.
Assume that
 either there is an instance ? of elementary tree sets from A such that there
are ?1,?2 ? ?, with ?1 ? N and ?2 ? N , and ?1 is not the root of D. ? it is
not possible that ? has been used in one of the multicomponent derivation
steps in the course of the derivation of t. Contradiction.
 or there is an instance ? of an elementary tree set such that there are
?1,?2 ? ?, ?1 = ?2, with ??1,?2? ? DD ? ?2 has been added to a tree
derived from ?1. Contradiction to condition that all elements of ? must
have been added simultaneously.
 or there are pairwise different instances ?1,?2, . . . ,?n of elementary tree
sets from A such that there are ?(i)1 ,?
(i)
2 ? ?i, 1 ? i ? n, with
??(1)1 ,?
(n)
2 ? ? DD and ??
(i)
1 ,?
(i?1)
2 ? ? DD for 2 ? i ? n. ? ?
(i)
1 was added
before ?(i?1)2 for 2 ? i ? n, and since all elements from ?i must be added
simutaneously for 1 ? i ? n, ?n was added before ?1. ? ??(1)1 ,?
(n)
2 ? ? DD.
Contradiction.
Consequently, D satisfies (MC1)?(MC3).
2. Then show ? of the iff: Let D be a derivation tree in GTAG satisfying (MC1)?
(MC3).
217
Computational Linguistics Volume 31, Number 2
There are different orderings of the derivation steps in D possible: Let the node
positions on the derived tree be pairs ??, p?, with ? being an instance of an elementary
tree and p being a position in ?. Every top-down order read off D (no matter whether
[partly] depth first or not and whether left to right or right to left) is a possible deriva-
tion order in GTAG for the derivation tree D, since in order to perform the derivation
step . . . [??1, p?,?2] corresponding to an edge ??1,?2, p? in D, one needs only to ensure
that ?1 (i.e., the mother node of ?2) has already been added.
Because of (MC1), the root of D is an initial tree, and the set of all other nodes in D
can be partitioned into pairwise different instances of elementary tree sets.
To show: There is a top-down traversal of D such that the traversal starts with an
initial tree and then there is always one instance ? of an elementary set whose members
are visited next in any order (i.e., simultaneously).
The top-down traversal has to start with the root node (i.e., an initial tree ?). Assume
that at some point of the traversal, the choice of a new instance of an elementary set to
be visited next is not possible. ? for each set ? that has not been visited yet, there is
at least one ? ? ? whose mother node has not been visited yet (otherwise ? could be
visited next).
Pick an unvisited ?1 with at least one ?
(1)
1 ? ?1 whose mother node has been visited.
Assume ?(1)2 ? ?1 with mother not yet visited. Suppose ?
(2)
1 to be the highest unvisited
node dominating ?(1)2 . Since ?
(2)
1 = ?
(1)
2 and ??
(2)
1 ,?
(1)
2 ? ? DD, (with (MC2)) ?
(2)
1 ? ?2 =
?1.
Then there is a ?(2)2 ? ?2 with unvisited mother such that
(a) either ??(1)1 ,?
(2)
2 ? ? DD. Contradiction to (MC3) with n = 2.
?(1)1 ?
(2)
1
?(2)2 ?
(1)
2
(b) or ??(1)1 ,?
(2)
2 ? /? DD. Because of (MC2), ??
(2)
1 ,?
(2)
2 ? /? DD. Let ?
(3)
1 ? ?3 be the
highest unvisited node dominating ?(2)2 . Because of (MC2), ?3 = ?2, and
because of (MC3), ?3 = ?1.
?(1)1 ?
(2)
1 ?
(3)
1
?(1)2 ?
(2)
2
In the (b) case, there is a ?(3)2 ? ?3 with unvisited mother node. Because of (MC2)
and (MC3), ??(1)1 ,?
(3)
2 ? /? DD, ??
(2)
1 ,?
(3)
2 ? /? DD, and ??
(3)
1 ,?
(3)
2 ? /? DD. Then there is a high-
est unvisited node ?(4)1 ? ?4 dominating ?
(3)
2 with ?4 = ?3,?4 = ?2, and ?4 = ?1. And
there is a ?(4)2 ? ?4 with unvisited mother node.
In general, for each of the ?n, 1 ? n, with ?(n)1 ,?
(n)
2 as above, the situation is
as follows: ?n = ?i for 1 ? i < n (otherwise contradiction to (MC2) or (MC3)) and
??(i)1 ,?
(n)
2 ? /? DD for i ? n (otherwise contradiction to (MC2) for i = n or to (MC3)
218
Kallmeyer Multicomponent TAGs with Shared Nodes
for i = n). Consequently, there is always a new ?n+1, with a new ?(n+1)1 being the
highest unvisited node dominating ?(n)2 and ?
(n+1)
2 being a node with unvisited
mother.
?(1)1 ?
(2)
1 ?
(n)
1 ?
(n+1)
1
?(1)2 . . . ?
(n?1)
2 ?
(n)
2
Contradiction to the finiteness of the number of nodes in D. ? there is a top-down
traversal of D that corresponds to a multicomponent derivation in G in the sense that it
allows us to visit the instances of elementary tree sets one after the other. 
Proof of Lemma 2
Only the uniqueness needs to be shown.
Let G = ?I, A, N, T,A? be an RSN-MCTAG. Let D = ?N , E? be a TAG derivation tree
in G.
Assume that there is an instance {?1, . . . ,?n} of an elementary tree set such that
there are ?,?? with ? = ?? and ??,?1?, . . . , ??,?n?, ???,?1?, . . . , ???,?n? ? SND and there
are i, j, 1 ? i, j ? n with ??,?i?, ???,?j? ? PD.
? since ???,?j? ? PD and ??,?j? ? DD with ? = ?j, there is a ??? with ??,???? ? PD
and ????,??? ? DD. Furthermore, ??i,???? /? DD (otherwise ??i,?j? ? DD which would
contradict (MC2)) and ????,?i? /? DD (otherwise, since ?i = ???, ??,?i? /? PD). Conse-
quently ???,?i? /? DD. Contradiction to assumption. 
Proof of Lemma 4
Let G be an RSN-MCTAG of arity n, w ? L(G), such that the elementary tree ? was used
to derive w. Assume that the contribution of ? is separated into m > n parts.
Then the SN-derivation structure for this derivation is as shown in Figure 17.
Consequently, there are at least m2 ? 1 crossings, and the arity of G is (m2 ? 1+
1) ? 2. ? if m is even, G is of arity m, and if m is odd, G is of arity m + 1. This is a
contradiction to the assumption that the arity of G is n < m. 
Proof of Theorem 1
For reasons of space, we do not give the whole proof of the theorem but re-
strict ourselves to the construction algorithm and a rough outline of the rest of the
proof.
Construction algorithm. Let G be an RSN-MCTAG of arity n.
Construction of a weakly equivalent RCG G?:
The terminals and nonterminals will be implicitly defined by the clauses of the
grammar.
Predicates: Let k1 be the maximal number of nodes in an elementary tree in G and
k2 be the maximal number of trees in an elementary tree set. k := k1(k2 ? 1).
219
Computational Linguistics Volume 31, Number 2
Figure 17
SN-derivation structure for proof of Lemma 4.
 There is a unary predicate S.
 Each ??? and each ???1 ? ? ??l?, with l < k, ? an initial tree, and ?1, . . . ,?l
auxiliary trees is an n ? 1-ary predicate.
 Each ??? and each ???1 ? ? ??l? with l < k and ?,?1, . . . ,?l auxiliary trees is
an n-ary predicate.
Define the decoration string ?? of an elementary tree ? as in Boullier (1999), except
that a root node ? has n variables, L?1 . . . L? n2 on the left and R? n2 . . .R?1 on the right.
Every other internal node ? has two variables L? and R?, and each substitution node
has one variable X. In a top-down, left-to-right traversal, the left variables are collected
during the top-down traversal, the terminals and variables of substitution nodes are
collected while visiting the leaves, and the right variables are collected during bottom-
up traversal.
Construction of the clauses:
In the following, P(, . . . ,, x,, . . . ,) signifies that x is the n2 th argument,
and P(, . . . ,, x1, x2,, . . . ,) signifies that x1 is the n2 th and x2 the (
n
2 + 1)th
argument.
220
Kallmeyer Multicomponent TAGs with Shared Nodes
(1) Predicate S
For each initial ?, there is a clause
S(X ) ? ???(, . . . ,,X,, . . . ,)
(2) Predicates ???
For each elementary ?: lhs := ??, rhs := .
For each combination of substitutions and adjunctions at ?, with substitutions at all
substitution nodes and adjunctions at all internal nodes, with obligatory adjunction that
respects the conditions for restricted tree-local multicomponent derivation:
For all nodes ? in ?:
 If ? is an internal node that is not the root, and no adjunction takes place at
?, then delete L? and R? in lhs.
 If ? is the root node and no adjunction takes place at ?, then delete
L?1 ? ? ? L? n2 and R? n2 ? ? ?R?1 in lhs.
 If only the initial tree ? is substituted at ?,
rhs := ???(, . . . ,, S?,, . . . ,)rhs.
 If ? is substituted at ? and then ?1, . . . ,?m (in that order) are secondarily
adjoined at ?, then rhs := ???1 ? ? ??m?(, . . . ,, S?,, . . . ,)rhs.
 If ? is an internal node that is not the root and ?1, . . . ,?m are adjoined (in
that order) at ?, then rhs := ??1 ? ? ??m?(, . . . ,, L?, R?,, . . . ,)rhs.
If there is no adjunction at the root of ?, there is a clause
???(, . . . ,, lhs,, . . . ,) ? rhs
If ?1, . . . ,?m are adjoined in this order at the root of ?, and if L1, . . . , L n2 , R n2 , . . . , R1
are the parts of the root in lhs such that lhs = L1 ? ? ? L n2 lhs
?R n
2
? ? ?R1, then there is a
clause
???(L1, . . . ,Ln2?1,Ln2 lhs
?Rn
2
,Rn
2?1, . . . ,R1) ?
??1 ? ? ??m?(L1, . . . ,Ln2 ,Rn2 , . . . ,R1)rhs
Further, for each initial ? and for all i, 1 ? i ? n2 ? 2 , there are clauses
???(L1, . . . ,Li ,,Li+1, . . . ,Ln2?2,X,Rn2?2, . . . ,Ri+1,,Ri , . . . ,R1) ?
???(,L1, . . . ,Ln2?2,X,Rn2?2, . . . ,R1,)
And for each auxiliary ? and for all i, 1 ? i ? n2 ? 1 , there are clauses
???(L1, . . . ,Li ,,Li+1, . . . ,Ln2?1,Rn2?1, . . . ,Ri+1,,Ri , . . . ,R1) ?
???(,L1, . . . ,Ln2?1,Rn2?1, . . . ,R1,)
221
Computational Linguistics Volume 31, Number 2
(3) Predicates ??1?2 ? ? ??m?
For each ??1?2 ? ? ??m? with m ? 2 occurring in the clauses constructed so far:
Define sets of variables
L := {L1(?1), . . . ,Ln2 (?1),L1(?2), . . . ,Ln2 (?m )}, and
R := {R1(?1), . . . ,Rn2 (?1),R1(?2), . . . ,Rn2 (?m )}, and
three other pairwise different variables X1,X2,X /? L?R.
Define for all x ? L? : R(x) := y ? R? such that if Lj(?i) is the kth letter of x, then Rj(?i)
is the (|x| ? k + 1)th letter of y.
For all w ? L? such that
(a) each L ? L occurs exactly once in w,
(b) for all 1 ? i1 < i2 ? m, L n2 (?i1 ) is to the right of L n2 (?i2 ) in w, and
(c) for all 1 ? i ? m and 1 ? j1 < j2 ? n2 , Lj1 (?i) is to the left of Lj2 (?i) in w,
and for all x1, . . . , x n2 ? L
? with x1 ? ? ? x n2 = w,
there is the following clause:
If ?1 is an initial tree, a clause with L n2 (?1) eliminated from the x1, . . . , x n2 :
??1 ? ? ??m?(x1, . . . , x n2 X R(x n2 ), . . . ,R(x1)) ?
??1?(L1(?1), . . . ,Ln2?1(?1),X,Rn2?1(?1), . . . ,R1(?1))
??2?(L1(?2), . . . ,Ln2 (?2),Rn2 (?2), . . . ,R1(?2))
...
??m?(L1(?m ), . . . ,Ln2 (?m ),Rn2 (?m ), . . . ,R1(?m ))
If ?1 is an auxiliary tree, a clause
??1 ? ? ??m?(x1, . . . , x n2 ,R(x n2 ), . . . ,R(x1)) ?
??1?(L1(?1), . . . ,Ln2 (?1),Rn2 (?1), . . . ,R1(?1))
...
??m?(L1(?m ), . . . ,Ln2 (?m ),Rn2 (?m ), . . . ,R1(?m ))
(4) These are all clauses.
Sketch of Proof. We do not give the whole proof of the correctness of the construction,
but we sketch the principal steps:
Mainly, two lemmas, concerning, respectively, the clauses constructed under para-
graph 2 above and under paragraph 3 above, are shown:
Lemma 5
For each elementary tree ? in G with decoration string ?? as defined above:
There is a ?? derived from ? with yield w (if ? is an initial tree) or ?wl, wr? (if ? is an
auxiliary tree with wl on the left and wr on the right of the foot node) such that
 all leaves in ?? have terminal labels;
 there are no OA constraints in ??;
222
Kallmeyer Multicomponent TAGs with Shared Nodes
 for all node positions p in ? at which substitutions or adjunctions took
place, the elementary trees ?(p)1 ,?
(p)
1 , . . . ,?
(p)
m (in that order) were
substituted/adjoined at the node at position p in ? (these are all trees
attached to this node).
iff
There is a ???-clause in G? corresponding to the attachments to ? in this derivation in
the way described in the construction, with ?? being the decoration string of ? without
the symbols for the nodes to which nothing was attached such that
There is an instantiation f : {t? | t? is an occurrence of some t ? T in ??} ? V ?
{?i, j? | 0 ? i ? j ? |w|} as defined in Definition 8, and the following hold for f :
 For each substitution node ? in ? with position p, the yield of the trees
?(p)1 ,?
(p)
1 , . . . ,?
(p)
m and everything added to them is the connected substring
f (S?), even if the derivation of ?? from ? is part of a larger derivation.
 For each internal node ? in ? with position p at which adjunctions took
place, the yield of the trees ?(p)1 ,?
(p)
1 , . . . ,?
(p)
m consists of the two connected
substrings ? f (L?), f (R?)?, even if the derivation of ?? from ? is part of a
larger derivation.
 If adjunctions at the root took place, then the yield of the trees
?(p)1 ,?
(p)
1 , . . . ,?
(p)
m consists of the substrings ? f (L1) f (L2) ? ? ? f (L n2 ),
f (R n
2
) ? ? ? f (R1)?, and this yield can be disconnected if the derivation of ??
from ? is part of a larger derivation; it can be separated into disconnected
substrings f (L1), f (L2), . . . , f (L n2 ), f (R n2 ), . . . , f (R1).
Proof by induction on structure of ?.
Lemma 6
For all ?1,?2, . . . ,?m:
There is a derivation in G of a tree with yield w in which ?1,?2, . . . ,?m (in that
order) attach to some node ? in an elementary tree ? such that the yield of the trees
?1,?2, . . . ,?m is separated into at most n disconnected substrings (ranges) of w. If ?1
is an auxiliary tree, it is separated into the substrings l1, . . . , l n2 , r n2 , . . . , r1; otherwise
(?1 initial), the substrings are l1, . . . , l n2 ?1, x, r n2 ?1, . . . , r1.
iff
There is a ??1?2 ? ? ??m?-clause as described in paragraph 3 of the construction above
such that there is an instantiation f of the clause such that
 If ?1 is initial, then f (x1) = l1, . . . , f (x n2 ?1) = l n2 ?1, f (x n2 XR(x n2 )) =
x, f (R(x n
2 ?1)) = r n2 ?1, . . . , f (R(x1)) = r1, and if ?1 is auxiliary, then
f (x1) = l1, . . . , f (x n2 ) = l n2 1, f (R(x n2 )) = r n2 , . . . , f (R(x1)) = r1.
 If ?1 is initial, then its yield in w consists of the n ? 1 substrings (ranges)
f (L1(?1)), . . . , f (L n2 ?1(?1)), f (X), f (R n2 ?1(?1)), . . . , f (R1(?1)).
 For all auxiliary ?i, 1 ? i ? m, the yield of ?i in w consists of the n
substrings (ranges) f (L1(?i)), . . . , f (L n2 (?i)), f (R n2 (?i)), . . . , f (R1(?i)).
Proof by induction on m.
The whole theorem can then be proven using these two lemmas.
223
Computational Linguistics Volume 31, Number 2
Acknowledgments
For valuable suggestions, helpful comments
and fruitful discussions of the subject of this
article, we would like to thank Anne Abeille?,
Pierre Boullier, David Chiang, Eric de la
Clergerie, Chung-Hye Han, Aravind Joshi,
Tony Kroch, Seth Kulick, Maribel Romero
and SinWon Yoon. Furthermore, we are really
grateful to three anonymous reviewers who
gave many very helpful comments and whose
suggestions for improvements influenced
considerably the final form of the article.
References
Becker, Tilman, Aravind K. Joshi, and Owen
Rambow. 1991. Long-distance scrambling
and tree adjoining grammars. In
Proceedings of ACL-Europe, Berlin.
Becker, Tilman, Owen Rambow, and Michael
Niv. 1992. The derivational generative
power of formal systems, or Scrambling is
beyond LCFRS. Technical Report
IRCS-92-38, Institute for Research in
Cognitive Science, University of
Pennsylvania.
Bertsch, Eberhard and Mark-Jan Nederhof.
2001. On the complexity of some
extensions of RCG parsing. In Proceedings
of the Seventh International Workshop on
Parsing Technologies, pages 66?77, Beijing,
October.
Boullier, Pierre. 1998a. A generalization of
mildly context-sensitive formalisms. In
Proceedings of the Fourth International
Workshop on Tree Adjoining Grammars and
Related Formalisms (TAG+4), pages 17?20,
University of Pennsylvania, Philadelphia.
Boullier, Pierre. 1998b. A proposal for a
natural language processing syntactic
backbone. Technical Report 3342, Institut
National de Recherche en Informatique et
en Automatique (INRIA) Rocquencourt.
Boullier, Pierre. 1999. On TAG parsing. In
Proceedings of TALN 99: Sixie`me Confe?rence
Annuelle sur le Traitement Automatique des
Langues Naturelles, pages 75?84, Carge`se,
Corse, July.
Boullier, Pierre. 2000. Range concatenation
grammars. In Proceedings of the Sixth
International Workshop on Parsing
Technologies (IWPT2000), pages 53?64,
Trento, Italy, February.
Boullier, Pierre. 2001. From contextual
grammars to range concatenation
grammars. In Proceedings of the Sixth
Conference on Formal Grammar and Seventh
Conference on Mathematics of Language
(FG/MOL?01), Helsinki, August.
Candito, Marie-He?le`ne and Sylvain Kahane.
1998. Can the TAG derivation tree
represent a semantic graph? An answer in
the light of meaning-text theory. In
Proceedings of the Fourth International
Workshop on Tree Adjoining Grammars and
Related Formalisms, IRCS Report 98?12,
pages 25?28, University of Pennsylvania,
Philadelphia.
Dras, Mark, David Chiang, and William
Schuler. 2004. On relations of constituency
and dependency grammars. Research on
Language and Computation, 2(2):281?305.
Frank, Robert. 1992. Syntactic Locality and Tree
Adjoining Grammar: Grammatical,
Acquisition and Processing Perspectives.
Ph.D. thesis, University of Pennsylvania.
Frank, Robert. 2002. Phrase Structure
Composition and Syntactic Dependencies.
MIT Press, Cambridge, MA.
Gerdes, Kim. 2002. Topologie et grammaires
formelles de l?allemand. Ph.D. thesis,
Universite? Paris 7.
Joshi, Aravind K. 1985. Tree adjoining
grammars: How much context sensitivity
is required ro provide reasonable
structural descriptions? In D. Dowty,
L. Karttunen, and A. Zwicky, editors,
Narural Language Parsing. Cambridge
University Press, Cambridge, pages
206?250.
Joshi, Aravind K. 1987. An introduction to
tree adjoining grammars. In
A. Manaster-Ramer, editor, Mathematics of
Language. John Benjamins, Amsterdam,
pages 87?114.
Joshi, Aravind K., Tilman Becker, and Owen
Rambow. 2000. Complexity of scrambling:
A new twist to the competence/
performance distinction. In Anne Abeille?
and Owen Rambow, editors, Tree Adjoining
Grammars: Formalisms, Linguistic Analyses
and Processing. Center for the Study of
Language and Information (CSLI)
Publications, Stanford, CA, pages 167?181.
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Science,
10:136?163.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-adjoining grammars. In G. Rozenberg
and A. Salomaa, editors, Handbook of
Formal Languages. Springer, Berlin, pages
69?123.
Joshi, Aravind K. and K. Vijay-Shanker. 1999.
Compositional semantics with lexicalized
tree-adjoining grammar (LTAG): How
much underspecification is necessary? In
H. C. Blunt and E. G. C. Thijsse, editors,
224
Kallmeyer Multicomponent TAGs with Shared Nodes
Proceedings of the Third International
Workshop on Computational Semantics
(IWCS-3), pages 131?145, Tilburg, The
Netherlands.
Kallmeyer, Laura. 2001. Local tree
description grammars: A local extension of
TAG allowing underspecified dominance
relations. Grammars, 4:85?137.
Kallmeyer, Laura. 2002. Using an enriched
TAG derivation structure as basis for
semantics. In Proceedings of TAG+6
Workshop, pages 127?136, Venice,
May.
Kallmeyer, Laura and Aravind K. Joshi. 2003.
Factoring predicate argument and scope
semantics: Underspecified semantics with
LTAG. Research on Language and
Computation, 1(1?2):3?58.
Kallmeyer, Laura and Sinwon Yoon. 2004.
Tree-local MCTAG with shared nodes:
Word order variation in German and
Korean. In Proceedings of TAG+7,
Vancouver.
Kulick, Seth Norman. 2000. Constraining
Non-local Dependencies in Tree Adjoining
Grammar: Computational and Linguistic
Perspectives. Ph.D. thesis, University of
Pennsylvania.
Rambow, Owen. 1994a. Formal and
Computational Aspects of Natural Language
Syntax. Ph.D. thesis, University of
Pennsylvania.
Rambow, Owen. 1994b. Multiset-valued
linear index grammars: Imposing
dominance constraints on derivations.
In Proceedings of ACL, Las Cruces,
NM.
Rambow, Owen and Young-Suk Lee. 1994.
Word order variation and tree-adjoining
grammars. Computational Intelligence,
10(4):386?400.
Rambow, Owen, K. Vijay-Shanker, and
David Weir. 1995. D-tree grammars. In
Proceedings of ACL, Cambridge, MA.
Rambow, Owen, K. Vijay-Shanker, and
David Weir. 2001. D-tree substitution
grammars. Computational Linguistics,
27(1):87?121.
Rogers, James. 1994. Studies in the Logic of
Trees with Applications to Grammar
Formalisms. Ph.D. thesis, University of
Delaware.
Schabes, Yves. 1990. Mathematical and
Computational Aspects of Lexicalized
Grammars. Ph.D. thesis, University of
Pennsylvania.
Schabes, Yves and Stuart M. Shieber. 1994.
An alternative conception of tree-adjoining
derivation. Computational Linguistics,
20(1):91?124.
Vijay-Shanker, K. 1987. A Study of Tree
Adjoining Grammars. Ph.D. thesis,
University of Pennsylvania.
Vijay-Shanker, K. 1992. Using descriptions of
trees in a tree adjoining grammar.
Computational Linguistics, 18(4):481?517.
Vijay-Shanker, K. and Aravind K. Joshi. 1988.
Feature structures based tree adjoining
grammar. In Proceedings of COLING, pages
714?719, Budapest.
Weir, David J. 1988. Characterizing mildly
context-sensitive grammar formalisms. Ph.D.
thesis, University of Pennsylvania.
XTAG Research Group. 1998. A lexicalized
tree adjoining grammar for English.
Technical Report IRCS 98-18, Institute for
Research in Cognitive Science, University
of Pennsylvania.
225

A query tool for syntactically annotated corpora* 
Laura  Ka l lmeyer  
UFRL,  Universit4 Paris 7 
2, place Jussieu 
75251 Paris cedex 05 
lmara ,  kallmeyer@linguist, jussieu, f r  
Abst rac t ;  
This paper presents a query tool for syntacti- 
caUy ~.nnotated corpora. The query tool is de- 
veloped to search the Verbmobil treebanks an- 
notated at the University of Tfibingen. How- 
ever, in principle it also can be adapted to 
other corpora such as the Negra Corpus, the 
Penn Treebank or the French treebank devel- 
oped in Paris. The tool uses a query language 
that allows to search for tokens, syntactic at- 
egories, grammatical functions and binary re- 
lations of (immediate) dominance and linear 
precedence between odes. The overall idea 
is to extract in an initializing phase the rele- 
vant information from the corpus and store it 
in a relational database. An incoming query 
is then translated into a corresponding SQL 
query that is evaluated on the database. 
1 In t roduct ion  
1.1 Syntact ic  annotat ion  and  
l inguist ic research 
With the increasing availability of large 
amounts of electronic texts, linguists have ac- 
cess to more and more material for empiri- 
cally based linguistic research. Furthermore, 
electronic orpora are more and more richly 
annotated and thereby more and more de- 
tailed and structured information contained 
in the corpora becomes accessable. Currently 
many corpora are tagged with morphosyntac- 
tic categories (part-of-speech) and there are 
already several syntactically annotated cor- 
pora. Examples are the Penn Treebank (Mar- 
cus et al, 1994; Bies et al, 1995) annotated 
at the University of Pennsylvania, the Ne- 
gra corpus (Brauts et al, 1999) developed in 
Saarbriicken, the Verbmobil treebank~ (Hin- 
richs et al, 2000) annotated in Tiibingen 
*The work presented here was done as part of a 
project in SFB 441 "Linguistic Data Structures" at 
the University of Tiibingen. 
and the French treebank annotated in Paris 
(Abeill4 and C14ment, 1999). 
However, in order to have access to these 
rich linguistic annotations, adequate query 
tools are needed. 
In the following, an example of a linguisti- 
cally relevant construction is considered that 
illustrates how useful access to structural in- 
formation in a corpus might be. 
fiber Chomsky habe ich ein Buch 
about Chomsky have I a book 
(1) gelesen 
read 
'I have read a book about Chomsky' 
Linguists are often concerned with con- 
structions that seem not very natural and 
where intuitions about grammaticality fail. 
An example is (1) where we have an accusative 
object (ein Buch) which is positioned between 
the two verbal elements and whose modifier 
(the prepositional phrase iiber Chomsky) is 
topicailzed. 
Some people claim (1) to be ungrammatical 
whereas other people are inclined to accept it. 
In these cases it is very useful to search in an 
adequate corpus for more natural data show- 
ing the same construction (see also (Meurers, 
1999) for other ex~.mples of the use of corpora 
for linguistic research). 
In order to find structures like (1) in a Ger- 
man corpus, one needs to search for 
(a) a prepositional phrase modifying the ac- 
cusative object and preceding the finite 
verb (i.e. in the so-called vorfeld), and 
(b) an accusative object between finite verb 
and infinite verb forms (i.e. in the so- 
called rnittelfeld) 
Obviously, two things need to be available 
in order to enable such a search. On the one 
hand, one needs a corpus with an annotation 
that is rich enough to encode the properties 
190 
(a) and (b). On the other hand, one needs a 
query tool with a query language that allows 
to express the properties (a) and (b). 
Corpora encoding features uch as (a) and 
(b) are for example the Verbmobil treebanks. 
1.2 Cur rent  query  tools 
Query tools such as xkwic (Christ, 1994) that 
allow to search on the tokens and their part- 
of-speech categories using regular expressions 
do not allow a direct search on the syntac- 
tic annotation structures of the corpus, i.e. a 
search for specific relations between odes in 
the annotation such as dominance or linear 
precedence. Therefore many queries a linguist 
would like to ask using a syntactically anno- 
tated corpus either cannnot be expressed in a 
regular expression based language or at least 
cannot be expressed in an intuitive way. 
Even more recent query languages as 
SgmlQL (Le Maitre et al, 1998) and XML-QL 
(Deutsch et al, 1999) that refer to the SGML 
or XML annotation of a corpus are in gen- 
eral not adequate for syntactically annotated 
corpora: if the annotations are trees and 
the nesting of SGML/XML elements encodes 
the tree structure, such query languages work 
nicely. With regular path expressions as sup- 
ported by XML-QL, it is possible to search 
not only for parent but also for dominance 
relations. However, in order to deal with 
discontinuous constituents, most syntactically 
annotated corpora do not contain trees but 
slightly different data structures. The Penn 
Treebank for example consists of trees with 
an additional coindexation relation, Negra al- 
lows crossing branches and in Verbmobil, an 
element (a tree-like structure) in the corpus 
might contain completely disconnected nodes. 
In order to express these annotations in XML, 
one has to encode for example ach node and 
each edge as a single element as in (Mengel 
and Lezius, 2000). But then a query for a 
dominance relation can no longer be formu- 
lated with a regular path expression. 
In this paper, I propose a query tool that 
allows to search for parent, dominance and 
linear precedence r lations even in corpora n- 
notated with structures slightly different from 
trees. 
2 The  Verbmobi l  t reebanks  
The German Verbmobil corpus (Stegmann et 
al., 1998; Hinrichs et al, 2000) is a tree- 
bank annotated at the University of Tiibingen 
SIMPX 
I 
VF 
! 
I 
PX 
- - I  
I 
NX 
I 
D 
I I 
APPR NE 
fiber C. 
I 
I I 
LK VC 
I I I 
VXFIN NX VXINF 
I I I 
I I \[ 
VAFIN PPER VVPP 
habe ich gel~en 
? 
I 
MF 
I 
I 
NX 
I I 
I I 
ART NN 
ein Buch 
Figure 1: Annotation of (1) in Verbmobil for- 
mat 
that contains approx. 38.000 trees (or rather 
tree-like annotation structures ince, as al- 
ready mentioned, the structures are not al- 
ways trees). The corpus consists of spoken 
texts restricted to the domain of arrangement 
of business appointments. 
The Verbmobil corpus is part-of-speech 
tagged using the Stuttgart Tiibingen tagset 
(STTS) described in (Schiller et al, 1995). 
One of the design decisions in Verbmobil 
was that for the purpose of reusability of the 
treebank, the annotation scheme should not 
reflect a commitment to a particular syntac- 
tic theory. Therefore a surface-oriented am 
notation scheme was adopted that is inspired 
by the notion of topological fields in the sense 
of (HShle, 1985). The discontinuous position- 
ing of the verbal elements in verb-first and 
verb-second sentences (as in (1) for example) 
is the traditional reason to structure the Ger- 
man sentence by means of topological fields: 
The verbal elements have the categories LK 
( linke Klammer) and VC (verbal complex), and 
roughly everything preceding the LK forms the 
'voffeld' VF, everything between LK and vc 
forms the 'mittelfeld' MF and the 'nachfeld' NF 
follows the verbal complex. 
The Verbmobil corpus is annotated with 
syntactic ategories as node labels, grammat- 
ical functions as edge labels and dependency 
relations. The syntactic ategories are based 
on traditional phrase structure and on the the- 
ory of topological fields. In contrast o Negra 
or Penn Treebank, there are neither crossing 
branches nor empty categories. Instead, de- 
191 
pendency relations are expressed within the 
? grammatical functions (e.g. OA-MOD for a con- 
stituent modifying the accusative object). 
A sample annotation conformant o the 
Verbmobil annotation scheme is the annota- 
tion of (1) shown in Fig. 1. (The elements set 
in boxes are edge labels.) 
In order to search for structures as in Fig. 1, 
one needs to search for trees containing a node 
nl with label PX and grammatical function 
0A-MOD, a node n2 with label VF that domi- 
nates nl, a node n3 with label MF and a node 
n4 with label NX and gra.mmatical function 0A 
that is immediately dominated by n3. 
Evaluating a query for structures as in 
Fig. 1 on the Verbmobil corpus gives results 
such as (2) that sound much more natural 
than the constructed example (1). 
? t ja ,  fiber Flugverbindungen habe ich 
about flight connections have I 
(2) leider keine Information. 
unfortunately no information 
'unfortunately I have no information 
about flight connections.' 
This example illustrates the usefulness of 
syntactic annotations for linguistic research 
? and it shows the need of query languages and 
query tools that allow access to these annota- 
tions. 
3 The  query  language 
3.1 Syntax 
As query language for the German Verbmo- 
bil corpus, a first order logic without quan- 
tification is chosen where variables are inter- 
preted as existentially quantified. Negation 
is only allowed for atomic formula. It seems 
that even this very simple logic already gives 
a high degree of expressive power with respect 
to the queries linguists are interested in (see 
for example (Kallmeyer, 2000) for theoretical 
investigations of query languages). However, 
it might be that at a later stage the query 
language will be extended. 
Let C (the node labels, i.e. syntactic ate- 
gories and part-of-speech categories), E (the 
edge labels, i.e. grammatical functions) and T 
(the terminals, i.e. tokens) be pairwise disjoint 
finite sets. >, >>, . .  are constants for the 
binary relations immediate dominance (par- 
ent relation), dominance (reflexive transitive 
closure of immediate dominance) and linear 
precedence. The set 1N of natural numbers is 
used as variables. Further, ~, I , ! are logi- 
cal connectives (conjunction, disjunction, and 
negation). 
Def in i t ion 1 ((C, E, T) -quer ies)  
( C, E, T)-queries are inductively defined: 
(a) for all iE  IN, tE  T: 
token( i )=t  and token( i )  !=t are 
queries, 
(b) for all iE  IN, cE C: 
cat ( i )=c and cat  ( i )  !=c are queries, 
(c) for all iE  IN, eE E:  
f c t ( i )=e  and f c t ( i ) !=e  are queries, 
(d) for all i ,  j E IN: 
i > j and i  !> j are queries, 
i >> j and i  !>> j are queries, 
i . .  j and i  ! . .  j are queries, 
(e) for all queries ql, q2: 
ql ~ q2 and (ql I q2) are queries. 
Of course, when adapting this language to 
another corpus, depending on the specific an- 
notation scheme, other unary or binary pred- 
icates might be added to the query language. 
This does not change the complexity of the 
query language in general. 
However, it is also possible that at a later 
point negation eeds to be allowed in a general 
way or that quantification needs to be added 
to the query language for linguistic reasons. 
Such modifications would affect the complex- 
ity of the language and the performance of 
the tool. Therefore the decision was taken to 
keep the language as simple as possible in the 
beginning. 
3.2 In tended mode ls  
In the case of the German Verbmobil corpus, 
the data structures are not trees, since struc- 
tures as in Fig. 2, which shows the annotation 
of the long-distance wh-movement in (3), can 
occur. The  structure in Fig. 2 does not have 
a unique root node, and the two nodes with 
label SINPX have neither a dominance nor a 
linear precedence r lation. 
(3) wen glaubst du liebt Maria 
whom believe you loves Maria 
'whom do you believe Maria loves' 
Therefore, the models of our queries are de- 
fined as more general structures than finite 
trees. 
A model is a tuple (/g, T ~, T), ?, p, ~/, a) 
where/g is the set of nodes, 7 ~, T~ and ? are the 
192 
SIMPX 
I I 
D D 
VF MF 
I I 
1 I 
NX NX 
I I 
I I 
PWS NE 
wen Maria 
I 
E3 
SIMPX \] 
I I 
D \[3 
I I 
LK MF LK 
I I I 
I I I 
VXFIN NX VXFIN 
I I I 
I I I 
WFIN PPER VVFIN 
glaubst du liebt 
Figure 2: Annotation of (3) in Verbmobil for- 
mat 
binary relations immediate dominance (par- 
ent), dominance and linear precedence, # is 
a function assigning syntactic categories or 
part-of-speech tags to nodes, r/ is a function 
mapping edges to grammatical functions, and 
a assigns tokens to the leaves (i.e. the nodes 
that do not dominate any other node). 
, . ?  , 
Def in i t ion 2 (Query  model )  
Let C, E and T be disjoint alphabets. 
(ILl, 79, 73, ?, #, rl, a) is a query model with cat- 
egories C, edge labels E and terminals T i f f  
1. l~ is a finite set with Lt n (C U E U T) = O, 
the set of nodes. 
2. P, ?, 73 ? ILl ? U, such that: 
(a) 79 is irreflexive, and for all x ? ILl 
there is at most one v ? ILl with 
(v, x) ? 79. 
(b) 73 is the reflexive transitive closure of 
79, and 73 is antisymmetric. 
(c) ? is transitive. 
(d) .for all x, y ? lg: if  (x, y) ? ?, then 
<~, y) ? 73 and (u, x) ? 73. 
(e) for all z,y ? U: (x,y) ? ? i# for 
all z, w ? ld with (x, z), (y, w) ? 73, 
(z, w) ? ? holds. 
(f) for all x, y, z ? L(: i f  (x, y), (x, z) ? 73, 
then either (x, z) ? 73 or (z, x) ? 73 or 
(x, z) ? ? or (z, x) ? ?. 
3. # : Lt ~ C is a total .function. 
4. rl : 7 ~ ~ E is a total .function. 
5. a : {u ? Ltl there is no u' with (u,u') ? 
79} ~ T is a total .function. 
With (b), (c) and (d), ? is also irreflexive 
and antisymmetric. 
In contrast o finite trees, our query mod- 
els do not necessarily have a unique root 
node, i.e. a node that dominates all other 
nodes. Consequently, the so-called exhaus- 
tiveness property does not hold since two 
nodes in a query model might be completely 
disconnected. In other words, it does not hold 
in general that (x,y) ? 73 or (y,x) ? 73 or 
(x, y) ? ? or (y, x) ? ? for all x, y ? /4 .  This 
holds only for nodes x, y ? /4  where a node z 
exists that dominates x and y. 
3.3 Semant ics  
Satisfiability of a query q by a query model 
M is defined in the classical model-theoretic 
way with respect o an injective assignment g 
mapping node variables to nodes in the query 
model. 
Def in i t ion 3 (Satisf iabi l i ty) 
Let M = (Lt, P, 73, ?, #, rl, a) be a query model 
and let g : iN ~ Lt be an injective .function. 
For all i6  iN, t6  T, c6 C, e6 E: 
? M ~g token( i )=t i f fa(g( i ) )  =t. 
? M ~g token( i ) !=t  i f fa(g( i ) )  #t. 
? M ~g cat ( i )=c  iff#(g(i)) =c. 
? M ~g cat ( i )  !=c iff ~(g(i)) ~c. 
? M ~g fc t ( i )=e  iff there is a u 6 lg with 
(u,g(i)) 6 P and r/((u,g(i))) =e. 
? M ~9 fc t (?)  !=e iffthere is no u 6 Lt with 
(~,g(i)) e p and ~((~,g(i)>) =e 
For all i ,  j 6 iN: 
? i ~g i > j iff (g(i),g(j)) 6 7 9 (i.e. g(i) 
immediately dominates g(j)) 
? M ~9 i !> j iff (g(i),g(j)) ~ 79 
? i ~g i >> j i f f (g( i ) ,g( j ) )  6 73 (i.e. g(i) 
dominates g(j))  
? MEg i !>> j i f f (g( i ) ,g( j ) )  ~73 
? i ~g i . .  j iff (g(i) ,g(j))  6 ? 
(i.e. gCi) is le# oSg(j)) 
? M~g i !.. j i f f (g( i ) ,g( j ) )  ~?  
For all queries ql, q2: 
? M~gqx a q2 i f fM~gql  andM~gq2 
? M ~a (q l  I q2) i f fM  ~a ql o rM ~g q2. 
Note that the condition that g needs to 
be injective means that different variables are 
considered to refer to different-nodes. In this 
respect, Def. 3 differs from traditional model- 
theoretic semantics. 
193 
node_pair_i\[tree_id\[no.dellnode2 \[fi._id \[ 
pair_class 
\[ clad \[ n._idl \ [ ~ ~  pl\[ p21 dl I d21 11\[12 I 
 ode_c\]ass \[ n-id \] cat I fct I 
tokens../\[ tree_id \[ n_A,d_.~ I word J 
Figure 3: The relational database schema 
As an example, consider the query for struc- 
tures as in (1) that is shown in (4). The struc- 
ture in Fig. 1 is a query model satisfying (4). 
(4) cat(1)=PX & fct(1)=0A-MOD 
-g~ cat(2)=VF ~ 2>>1 
cat (3)=?IF ~ cat (4) =NX 
& fct(4)=0A ~ 3>>4 
4 Stor ing  the  corpus  in a database  
As already mentioned, the general idea of the 
query tool is to store the information one 
wants to search for in a relational database 
and then to translate an expression in the 
query language presented in the previous ec- 
tion into an SQL expression that is evaluated 
on the database. The first part is performed 
by an initializing component and needs to be 
done only once per corpus, usually by the cor- 
pus administrator. The second part, i.e. the 
querying of the corpus, is performed by a 
query component. 
The tool is implemented in Java with Java 
Database Connectivity (JDBC) as interface 
and mysql as database management system. 
4.1 The  relat ional  database  schema 
The German Verbmobil corpus consist of sev- 
eral subcorpora. In the relational database 
there are two global tables, node_class and 
pair_class. Besides these, for each of the sub- 
corpora identified by i there are tables to- 
kens_/ and node_pair_/. The database schema 
is shown in Fig. 3. The arrows represent 
foreign keys. The colnmn cl_id in the ta- 
ble node_pair_/, for example, is a foreign key 
referring to the colnmn clad in the table 
pair_class. This means that each entry for 
clad in node_pair_/uniquely refers to One en- 
try for clad in pair_class. 
The content of the tables is as follows: 
? node_class contains node classes charac- 
terized by category (node label) and gram- 
matical function (edge label between the 
node and its mother). Each node class 
has a unique identifier, namely the column 
n_id. 
? pair_class contains classes of node pairs 
characterized by the two node classes and 
the parent, dominance and linear prece- 
dence relation between the two nodes. The 
columns pl, p2, dl, d2, 11 and 12 stand for 
binary relations and have values 1 or 0 de- 
pending on whether the relation holds or 
not. pl signifies immediate dominance of 
the first node over the second, p2 immedi- 
ate dominance of the second over the first, 
d l  dominance of the first over the second, 
etc. Each node pair class has a unique iden- 
tifier, namely its clad. 
? tokens_/ contains all leaves from subcor- 
pus i with their tokens (word). 
? node_pair_/contains all node pairs from 
subcorpus i with their pair class. Of 
course, only pairs of nodes belonging to one 
single annotation structure are stored. 
4.2 Init ia l iz ing the  database  
The storage of the corpus in the database is 
done by an initializing component. This com- 
ponent extracts information from the struc- 
tures in export format (the format used for the 
German Verbmobil corpus) and stores them 
in the database. The export format explicitly 
encodes tokens, categories and edge labels, 
linear precedence between leaves and the par- 
ent (immediate dominance) relation. Domi- 
nance and linear precedence in general how- 
ever need to be precompiled. 
First the dominance relation is computed 
simply as reflexive transitive closure of the 
parent relation. 
Linear precedence on the leaves can be im- 
mediately extracted from the export format. 
When computing linear precedence for inter- 
nal nodes, the specific properties of the data 
structures in Verbmobil (see Section 3) must 
be taken into account. Unlike in finite trees, 
for two nodes Ul,U2, the fact that ul domi- 
nates some x and u2 dominates some y and x 
is left of y is not s,fl~cient to decide that Ul 
is left of u2. Instead (see axiom (e) in Def. 2) 
the following holds for two nodes ul, u2: ul is 
left of u2 iff for all x ,y  dominated by ul ,u2 
respectively: x is left of y. 
In general, the database schema itself does 
194 
sche inbar  ADV - -  HD 500 
n icht  PTKNEG - -  HD 501 
be ides  P IS  - -  HD 502 
zusammen ADV - -  HI) 503 
$. - -  - -  0 
#500 ADVX - -  - 505  
#501 ADVX - -  - 505  
#502 NX - -  HD 504 
#503 ADVX - -  - 504  
#504 NX - -  HI) 505  
#505 NX . . . .  0 
#E0S 24  
Corresponding structure: 
NX 
I 
I 
NX 
I 
. 
B B @ 
I \] I I 
ADVX ADVX NX ADVX 
I l I I 
I I I \] 
ADV PTKNEG PIS ADV 
schembar nicht beides zus. 
195 
Figure 4: Export format of sentence 24 in cd20 
and corresponding structure 
not reflect he concrete properties of the query 
model, in particular the properties of the bi- 
nary relations are not part of the database 
schema, e.g. considering only the database, 
the dominance and linear precedence r lations 
are not necessarily trA.n~itive. Therefore, the 
query tool can be easily adapted to other data 
structures, for example to feature structures 
with reentrancy as annotations. In this case, 
a modification of the part of the initializing 
component that computes the binary relations 
would be sufficient. 
As an example, consider how sentence 24 
in the subcorpus cd20 (identifier 20) is stored 
in the database. This sentence was chosen 
for the simple reason that it is not too long 
but contains enough nodes to provide a useful 
example. Besides this, its construction and its 
tokens are not of any interest here. 
Fig. 4 shows the sentence in its export for- 
mat, i.e. the way it originally occurs in the 
corpus, together with a picture of the corre- 
sponding structure. Parts of the tables in the 
tokens_20 
tree_id 
24 
24 
24 
24 
24 
node word 
0 scheinbar 
1 nicht 
2 beides 
3 zusammen 
4 
node_pair.20 
treedd nodel 
24 I 0 24 0 
24 0 
. ,  
24 I 9 
pair_class 
node2 clad 
1 1459 
2 2608 
3 120 
10 11327 
did I nJdl nJd2 I pl I P21 dl I d21 11112 
120 I 13 13 " l 'o lo lo lo l l l o  
13271 24 25 I '01110111010 
? . .  
node_class 
n_id cat fct 
. . .  
13 ADV HD 
? ? .  
24 NX HD 
25 NX - 
Figure 5: Sentence 24 in the database 
database concerning sentence 24 are shown in 
Fig. 5. Each line in the export format cor- 
responds to one node. The nodes are as- 
signed numbers 0, 1, . . .  in the order of the 
lines in the export format. The nodes with 
tokens (i.e. that are leaves) are inserted into 
the table tokens_20. Furthermore, each pair 
of nodes occurring in sentence 24 is inserted 
into the table node_pair_20 together with its 
pair class. Both orders of a pair are stored. 1
The pair classes and node classes belonging to 
a pair can be found in the two global tables. 
Consider for example the nodes 9 and 10 in 
sentence 24 (the node labelled NX that domi- 
nates beides zusammen and the topmost node 
with label NX). The c lad of this pair is 1327. 
*In a previous version just one order was stored 
but it turned out that for some queries this causes an 
exponential time complexity depending on the number 
of variables occurring in the query. This problem is 
avoided storing both orders of a node pair. 
#BOS 24 25 898511955 1 
The corresponding entry in pair_class tells us 
that the second node is the :mother of the first, 
that the second dominates the first, and that 
there is no linear precedence r lation between 
the two nodes. Furthermore, the node classes 
identified by n_idl and had2 are such that the 
first node has label NX and grammatical func- 
tion HD whereas the second\[ has label NX and 
no grammatical function. 
4.3 The size of  the  database  
So far, in order to test the tool, approximately 
one quarter of the German Verbmobil corpus 
is stored in the database, namely the following 
subcorpora: 
id sub- trees tokens pairs 
corpus 
15 cdl5 1567 15474 1326416 
20 cd20 2151 21069 1941056 
21 cd21 2416 22360 1761082 
22 cd22 1723 16587 1229324 
24 cd24 2255 22763 2129548 
The table pa~_class has 23024 entries and 
node_class has 213 entries. The following ta- 
ble shows the current size of the files: 
Table data file index file 
(KS) (KB) 
node_class 
pair_class 
tokens_15 
tokens_20 
tokens_21 
tokens_22 
tokens_24 
node_pair_15 
node_pair_20 
node_pair_21 
node_pair_22 
node_pair_24 
1 10 
585 1500 
303 293 
413 403 
439 423 
325 311 
446 430 
9067 72556 
13269 106377 
12039 96383 
8404 67153 
14557 116694 
5 Search ing  the  corpus  
In order to search the corpus, one needs of 
course to know the specific properties of the 
annotation scheme. These are described in the 
STTS guidelines (Schiller et al, 1995) and the 
Verbmobil stylebook (Stegmann et al, 1998) 
that must be both available to any user of the 
query tool. 
Currently, the query component does not 
yet process all possible expressions in the 
query language. In particular, it does not 
allow disjunctions and it does not allow to 
query for tokens. Other atomic queries com- 
bined with with negations and conjunctions 
are possible. In particular, complex syntac- 
tic structures involving category and edge la- 
bels and binary relations can be searched. The 
query component will be completed very soon 
to process all queries defined in Section 3. 
The query component takes an expression 
in the query language as input and trans- 
lates this into a corresponding SQL expres- 
sion, which is then passed to the database. 
As an example, consider again the query (4) 
repeated here as (5): 
(5) cat(1)=PX & fct(1)=0A-MOD 8~ 
cat(2)=VF & 2>>1 & cat(3)=MF & 
cat(4)=NX ~ fct(4)=0A & 3>>4 
For query (5) as input performed on the 
subcorpus cd20, the query component pro- 
duces the following SQL query: 
SELECT DISTINCT npl.tree_id 
FROM 
node_class AS ncl, node_class AS nc2, 
node_class AS nc3, node_class AS nc4, 
node_pair_20 AS npl, pair_class AS pc1, 
node_pair_20 AS np2, pair_class AS pc2 
WHERE 
ncl.cat='PX' AND ncl.fct='0A-MOD' 
AND nc2.cat='VF ' AND nc3.cat='MF' 
AND nc4.cat='NX'  AND nc4 . fc t= '0A '  
AND pc1 .n_ id l=nc2 .n_ id  
AND pcl.n_id2=ncl.n_id AND pcl.dl=l 
AND pc2 .n_id1=nc3.n_id 
AND pc2.n_id2=nc4.n_id AND pc2.dl=l 
AND npl. cl_id=pcl, c1_id 
AND npl.tree_id=np2, tree_id 
AND np2. cl_id=pc2, cl_id; 
As a second example consider the search for 
long distance wh-movements a in (3). The 
annotation of (3) using the Verbmobil annota- 
tion scheme was shown in Fig. 2. Such struc- 
tures might be characterized by the following 
properties: there is an interrogative pronoun 
(part-of-speech tag PWS for substituting inter- 
rogative pronoun) that is part of a simplex 
clause and there is another simplex clause con- 
tainlng a finite verb such that the two sim- 
plex clauses are not connected and the pro- 
noun precedes the finite verb. This leads to 
the query (6): 
(6) cat(1)=PWS & cat(2)=SIMPX & 2>>1 
& cat(3)=SIMPX & cat(4)=VVFIN 
& 2!>>3 & 3!>>2 & 2! . .3  ~ 3! . .2  
8~ 1 . .4  ~ 3>>4 
196 
Performed on cd20, (6) as input leads to the 
following SQL query: 
SELECT DISTINCT npl.tree_id 
FROM 
node_class AS ncl, node_class AS nc2, 
node_class AS nc3, node_class AS nc4, 
node_pair_20 AS npl, 
pair_class AS pcl, 
node_pair_20 AS np2, 
pair_class AS pc2, 
node_pair_20 AS np3, 
pair_class AS pc3, 
node_pair_20 AS np4, 
pair_class AS pc4 
WHERE 
ncl.cat='PWS' AND nc2.cat='SIMPX' 
AND nc3.cat='SIMPX' 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
.,AND 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
AND 
nc4. cat='WFIN' 
pc I. n_idl=nc2, n_id 
pcl.n_id2=ncl.n_id AND pcl.dl=l 
pc2.n_idl=nc2 .n_id 
pc2. n_id2=nc3, n_id 
pc2.dl=O AND pc2.d2=O 
pc2.11=0 AND pc2.12=0 
pc3. n_idl=nc 1.n_id 
pc3.n_id2=nc4.n_id AND pc3.11=1 
pc4. n_idl=nc3.n_id 
pc4.n_id2=nc4.n_id AND pc4. d1=1 
npl. cl_id=pc 1. cl_id 
npl. nodel=np2, node 1 
np 1. node2=np3, node 1 
npl. tree_id=np2, tree_id 
np2. cl_id=pc2, cl_id 
np2. node2=np4, node 1 
np2. tree_id=np3, tree_id 
np3. cl_id=pc3, cl_id 
np3. node 2=np4. node 2 
np3. tree_id=np4, tree_id 
np4. cl_id=pc4, cl_id; 
Currently the database and the tool are 
running on a Pentium II PC 400MHz 128MB 
under Linux. On this machine, example (5) 
takes 1.46 sec to be answered by mysql, and 
example (6) takes 6.43 sec to be answered. 
This shows that although the queries, in par- 
ticular the last one, are quite complex and in- 
volve many intermediate r sults, the perfor- 
mauce of the system is quite efficient. 
The performance of course depends cru- 
cially on the size of intermediate results. 
In cases where more than one node pair is 
searched for (as in the two examples above) 
the order of the pairs is important since the 
result set of the first pair restricts the second 
pair. In (5) for example, first a node pair with 
a PX with function OA-MOD dominated by a VF 
is searched for. Afterwards, the search for the 
NX with function 0A in the lffF is restricted to 
those trees that were found when searching 
for the first pair. Obviously, the first pair is 
much more restrictive than the second. If the 
order is reversed, the query takes much more 
time to process. Currently the ordering of the 
pairs needs to be done by the user, i.e. depends 
on the incoming query. However, we plan to 
implement at least partly an ordering of the 
binary conjuncts in the query depending on 
the frequency of the syntactic ategories and 
grammatical functions involved in the pairs. 
The obvious advantage ofusing a relational 
database to store the corpus is that some parts 
of the work are taken over by the database 
management system such as the search of the 
corpus. Furthermore, and this is crucial, the 
indexing functionalities of the database man- 
agement system can be used to increase the 
performance of the tool, e.g. indexes are put 
on clad in node_pair_/and on nAdl and had2 
in pair_class. 
6 Conc lus ion  and  fu ture  work  
In this paper, I have presented a query tool for 
syntactically annotated corpora that is devel- 
oped for the German Verbmobil treebank an- 
notated at the University of Tiibingen. The 
key idea is to extract in an initializing phase 
the information one wants to search for from 
the corpus and to store it in a relational 
database. The search itself is done by trans- 
lating an input query that is an expression i  
a simple quantifier f ee first order logic into an 
SQL query that is then passed to the database 
system. 
An obvious advantage ofthis architecture is 
that a considerable amount of work is taken 
over by the database management system and 
therefore needs not to be implemented. Fur- 
thermore, the mysql indexing functionalities 
can be used to directly affect he performance 
of the search. 
The query tool is work in progress, and I 
briefly want to point out some of the things 
that still need to be done. First, the set of 
queries the tool can process needs to be ex- 
tended to all queries allowed in the query lan- 
guage. This will be done very soon. An- 
other task for the near future is, as men- 
tioned in the previous ection, to add an or- 
197 
dering mechanism on binary conjuncts in or- 
der to ensure that the more restrictive node 
pairs are searched for first. Further, the de- 
sign of a graphical user-interface to enter the 
queries is planned, allowing to specify queries 
by drawing partial trees instead of typing in 
the expressions in the query language. Finally, 
we also want to implement a web-based user- 
interface for the query tool. 
Besides these tasks that all concern the cur- 
rent query tool for the German Verbmobil cor- 
pus, a more general issue to persue in the fu- 
ture is to adapt the tool to other corpora. In 
some cases, this implies a modification of the 
way binary relations are precompiled, and in 
some other cases this would even lead to a 
modification of the query language and the 
database schema, namely in those cases where 
other binary relations are needed, e.g. the 
coindexation relation in the case of the Penn 
Treebank. 
Acknowledgments  
For fruitful discussions I would like to thank 
Oliver Plaehn and Ilona Steiner. Further- 
more, I am grateful to three anonymous re- 
viewers for their valuable comments. 
Re ferences  
Anne Abeill~ and Lionel ClEment. 1999. A tagged 
reference Corpus for French. In Proceedings off 
EA CL-LINC, Bergen. 
Ann Bies, Mark Ferguson, Karen Katz, and 
Robert MacIntyre. 1995. Bracketing Guidelines 
for Treebank II Style Penn Treebank Project. 
University of Pennsylvania. 
Thorsten Brants, Wojciech Skut, and Hans Uszko- 
reit. 1999. Syntactic Annotation of a German 
Newspaper Corpus. In Journdes ATALA, 18- 
19 juin 1999, Corpus annotds pour la syntaxe, 
pages 69-76, Paris. 
Oliver Christ. 1994. A modular and flexible archi- 
tecture for an integrated corpus query system. 
In Proceedings o\[ COMPLEX'9~. 
Alin Deutsch, Mary Fernandez, Daniela Florescu, 
Alon Levy, and Dan Suciu. 1999. A Query Lan- 
guage for XML. In Proceedings of the Interna- 
tional World Wide Web Conference (WWW), 
volume 31, pages 1155-1169. 
Erhard W. Hinrichs, Julia Bartels, Yasuhiro 
Kawata, Valia Kordoni, and Heike Telljohann. 
2000. The VERBMOBIL Treebanks. In Pro- 
ceedings of KONVENS 2000, October. To ap- 
pear. 
Tilman HShie. 1985. Der Begriff 'Mittelfeld'. An- 
merkungen fiber die Theorie der topologischen 
Felder. In A. SchSne, editor, Kontroversen alte 
und neue. Akten des 7. Internationalen Ger- 
manistenkongresses G6ttingen, pages 329--340. 
Laura Kallmeyer. 2000. On the Complexity of 
Queries for Structurally Annotated Linguistic 
Data. In Proceedings of ACIDCA'2000, pages 
105-110, March. 
Jacques Le Maitre, Elisabeth Murisasco, and 
Monique Rolbert. 1998. From Annotated Cor- 
pora to Databases: the SgmlQL Language. In 
John Nerbonne, editor, Linguistic databases. 
CSLI. 
Mitchell Marcus, Grace Kim, Mary Arm 
Marcinkiewicz, Robert MacIntyre, Ann Bies, 
Mark Ferguson, Karen Katz, and Britta Schas- 
berger. 1994. The Penn Treebank: Annotating 
Predicate Argument Structure. In ARPA '94. 
Andreas Mengel and Wolfgang Lezius. 2000. An 
XML-based encoding format for syntactically 
annotated corpora. In Proceedings of LREC 
2000. 
Detmar Meurers. 1999. Von pa~ie\]len Kon- 
stituenten, erstaunlichen Passiven und ver- 
wirrten Franken. zur Verwendung yon Korpora 
fiir die theoretische Linguistik. Handout at the 
DGfS Jahrestagung, February. 
A. Schiller, S. Teufel, and C. Thielen. 1995. 
Guidelines fiir das Tagging deutscher Textcor- 
pora mit STTS. Manuskript Universit~t 
Stuttgart und Universit~t Tfibingen. 
Rosemary Stegrnann, Heike Schulz, and Er- 
hard W. Hinrichs. 1998. Stylebook for the 
German Treebank in VERBMOBIL. Eberhard- 
Karls Universit~it Tiibingen. 
198 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 537?545,
Beijing, August 2010
Data-Driven Parsing with Probabilistic Linear Context-Free Rewriting
Systems
Laura Kallmeyer and Wolfgang Maier
SFB 833, University of Tu?bingen
{lk,wmaier}@sfs.uni-tuebingen.de
Abstract
This paper presents a first efficient imple-
mentation of a weighted deductive CYK
parser for Probabilistic Linear Context-
Free Rewriting Systems (PLCFRS), to-
gether with context-summary estimates
for parse items used to speed up pars-
ing. LCFRS, an extension of CFG, can de-
scribe discontinuities both in constituency
and dependency structures in a straight-
forward way and is therefore a natural
candidate to be used for data-driven pars-
ing. We evaluate our parser with a gram-
mar extracted from the German NeGra
treebank. Our experiments show that data-
driven LCFRS parsing is feasible with
a reasonable speed and yields output of
competitive quality.
1 Introduction
Data-driven parsing has largely been dominated
by Probabilistic Context-Free Grammar (PCFG).
The use of PCFG is tied to the annotation princi-
ples of popular treebanks, such as the Penn Tree-
bank (PTB) (Marcus et al, 1994), which are used
as a data source for grammar extraction. Their an-
notation generally relies on the use of trees with-
out crossing branches, augmented with a mech-
anism that accounts for non-local dependencies.
In the PTB, e.g., labeling conventions and trace
nodes are used which establish additional implicit
edges in the tree beyond the overt phrase struc-
ture. In contrast, some other treebanks, such as the
German NeGra and TIGER treebanks allow anno-
tation with crossing branches (Skut et al, 1997).
Non-local dependencies can then be expressed di-
rectly by grouping all dependent elements under a
single node.
However, given the expressivity restrictions of
PCFG, work on data-driven parsing has mostly
excluded non-local dependencies. When us-
ing treebanks with PTB-like annotation, label-
ing conventions and trace nodes are often dis-
carded, while in NeGra, resp. TIGER, tree trans-
formations are applied which resolve the crossing
branches (Ku?bler, 2005; Boyd, 2007, e.g.). Espe-
cially for these treebanks, such a transformation is
questionable, since it is non-reversible and implies
information loss.
Some research has gone into incorporating non-
local information into data-driven parsing. Levy
and Manning (2004) distinguish three approaches:
1. Non-local information can be incorporated di-
rectly into the PCFG model (Collins, 1999), or
can be reconstructed in a post-processing step af-
ter PCFG parsing (Johnson, 2002; Levy and Man-
ning, 2004). 2. Non-local information can be
incorporated into complex labels (Hockenmaier,
2003). 3. A formalism can be used which accom-
modates the direct encoding of non-local informa-
tion (Plaehn, 2004). This paper pursues the third
approach.
Our work is motivated by the following re-
cent developments: Linear Context-Free Rewrit-
ing Systems (LCFRS) (Vijay-Shanker et al, 1987)
have been established as a candidate for mod-
eling both discontinuous constituents and non-
projective dependency trees as they occur in tree-
banks (Kuhlmann and Satta, 2009; Maier and
Lichte, 2009). LCFRS extend CFG such that
non-terminals can span tuples of possibly non-
537
CFG:
A
?
LCFRS: ?
A
? ?
?1 ?2 ?3
Figure 1: Different domains of locality
adjacent strings (see Fig. 1). PCFG techniques,
such as Best-First Parsing (Charniak and Cara-
ballo, 1998), Weighted Deductive Parsing (Neder-
hof, 2003) and A? parsing (Klein and Manning,
2003a), can be transferred to LCFRS. Finally,
German has attracted the interest of the parsing
community due to the challenges arising from its
frequent discontinuous constituents (Ku?bler and
Penn, 2008).
We bring together these developments by pre-
senting a parser for probabilistic LCFRS. While
parsers for subclasses of PLCFRS have been pre-
sented before (Kato et al, 2006), to our knowl-
edge, our parser is the first for the entire class of
PLCFRS. We have already presented an applica-
tion of the parser on constituency and dependency
treebanks together with an extensive evaluation
(Maier, 2010; Maier and Kallmeyer, 2010). This
article is mainly dedicated to the presentation of
several methods for context summary estimation
of parse items, and to an experimental evaluation
of their usefulness. The estimates either act as
figures-of-merit in a best-first parsing context or
as estimates for A? parsing. Our evaluation shows
that while our parser achieves a reasonable speed
already without estimates, the estimates lead to a
great reduction of the number of produced items,
all while preserving the output quality.
Sect. 2 and 3 of the paper introduce probabilis-
tic LCFRS and the parsing algorithm. Sect. 4
presents different context summary estimates. In
Sect. 5, the implementation and evaluation of the
work is discussed.
2 Probabilistic LCFRS
LCFRS are an extension of CFG where the non-
terminals can span not only single strings but, in-
stead, tuples of strings. We will notate LCFRS
with the syntax of simple Range Concatenation
Grammars (SRCG) (Boullier, 1998), a formalism
that is equivalent to LCFRS.
A LCFRS (Vijay-Shanker et al, 1987) is a tu-
ple ?N,T, V, P, S? where a) N is a finite set of
non-terminals with a function dim: N ? N that
determines the fan-out of each A ? N ; b) T and V
are disjoint finite sets of terminals and variables;
c) S ? N is the start symbol with dim(S) = 1; d)
P is a finite set of rules
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
for m ? 0 where A,A1, . . . , Am ? N , X(i)j ?
V for 1 ? i ? m, 1 ? j ? dim(Ai) and
?i ? (T ? V )? for 1 ? i ? dim(A). For all
r ? P , it holds that every variable X occurring in
r occurs exactly once in the left-hand side (LHS)
and exactly once in the right-hand side (RHS).
A rewriting rule describes how the yield of
the LHS non-terminal can be computed from
the yields of the RHS non-terminals. The rules
A(ab, cd) ? ? and A(aXb, cY d) ? A(X,Y )
for instance specify that 1. ?ab, cd? is in the yield
of A and 2. one can compute a new tuple in the
yield of A from an already existing one by wrap-
ping a and b around the first component and c and
d around the second.
For every A ? N in a LCFRS G, we define the
yield of A, yield(A) as follows:
a) For every A(~?) ? ?, ~? ? yield(A);
b) For every rule
A(?1, . . . , ?dim(A)) ? A1(X(1)1 , . . . ,X(1)dim(A1))
? ? ?Am(X(m)1 , . . . ,X(m)dim(Am))
and all ~?i ? yield(Ai) for 1 ? i ? m,
?f(?1), . . . , f(?dim(A))? ? yield(A) where f
is defined as follows: (i) f(t) = t for all t ? T ,
(ii) f(X(i)j ) = ~?i(j) for all 1 ? i ? m, 1 ?
j ? dim(Ai) and (iii) f(xy) = f(x)f(y) for
all x, y ? (T ?V )+. f is the composition func-
tion of the rule.
c) Nothing else is in yield(A).
The language is then {w | ?w? ? yield(S)}.
The fan-out of an LCFRS G is the maximal fan-
out of all non-terminals in G. Furthermore, the
RHS length of a rewriting rules r ? P is called the
rank of r and the maximal rank of all rules in P
is called the rank of G. We call a LCFRS ordered
if for every r ? P and every RHS non-terminal A
in r and each pair X1, X2 of arguments of A in
538
the RHS of r, X1 precedes X2 in the RHS iff X1
precedes X2 in the LHS.
A probabilistic LCFRS (PLCFRS) (Kato et
al., 2006) is a tuple ?N,T, V, P, S, p? such that
?N,T, V, P, S? is a LCFRS and p : P ?
[0..1] a function such that for all A ? N :
?A(~x)?~??Pp(A(~x) ? ~?) = 1.
3 The CYK Parser
We use a probabilistic version of the CYK parser
from (Seki et al, 1991), applying techniques of
weighted deductive parsing (Nederhof, 2003).
LCFRS can be binarized (Go?mez-Rodr??guez et
al., 2009) and ?-components in the LHS of rules
can be removed (Boullier, 1998). We can there-
fore assume that all rules are of rank 2 and do not
contain ? components in their LHS. Furthermore,
we assume POS tagging to be done before pars-
ing. POS tags are non-terminals of fan-out 1. The
rules are then either of the form A(a) ? ? with A
a POS tag and a ? T or of the form A(~?) ? B(~x)
or A(~?) ? B(~x)C(~y) where ~? ? (V +)dim(A),
i.e., only the rules for POS tags contain terminals
in their LHSs.
For every w ? T ?, where w = w1 . . . wn with
wi ? T for 1 ? i ? n, we define: Pos(w) :=
{0, . . . , n}. A pair ?l, r? ? Pos(w) ? Pos(w)
with l ? r is a range in w. Its yield ?l, r?(w) is
the string wl+1 . . . wr. The yield ~?(w) of a vec-
tor of ranges ~? is the vector of the yields of the
single ranges. For two ranges ?1 = ?l1, r1?, ?2 =
?l2, r2?: if r1 = l2, then ?1 ? ?2 = ?l1, r2?; other-
wise ?1 ? ?2 is undefined.
For a given rule p : A(?1, . . . , ?dim(A)) ?
B(X1, . . . ,Xdim(B))C(Y1, . . . ,Xdim(C)) we
now extend the composition function f to ranges,
given an input w: for all range vectors ~?B and
~?C of dimensions dim(B) and dim(C) respec-
tively, fr( ~?B , ~?C) = ?g(?1), . . . , g(?dim(A))?
is defined as follows: g(Xi) = ~?B(i) for all
1 ? i ? dim(B), g(Yi) = ~?C(i) for all
1 ? i ? dim(C) and g(xy) = g(x) ? g(y) for all
x, y ? V +. p : A(fr( ~?B , ~?C)) ? B( ~?B)C( ~?C)
is then called an instantiated rule.
For a given input w, our items have the
form [A, ~?] where A ? N , ~? ? (Pos(w) ?
Pos(w))dim(A). The vector ~? characterizes the
span of A. We specify the set of weighted parse
Scan: 0 : [A, ??i, i + 1??] A POS tag of wi+1
Unary: in : [B, ~?]in + |log(p)| : [A, ~?] p : A(~?) ? B(~?) ? P
Binary: inB : [B, ~?B], inC : [C, ~?C ]inB + inC + log(p) : [A, ~?A]
where p : A( ~?A) ? B( ~?B)C( ~?C) is an instantiated rule.
Goal: [S, ??0, n??]
Figure 2: Weighted CYK deduction system
add SCAN results to A
while A 6= ?
remove best item x : I from A
add x : I to C
if I goal item
then stop and output true
else
for all y : I ? deduced from x : I and items in C:
if there is no z with z : I ? ? C ? A
then add y : I ? to A
else if z : I ? ? A for some z
then update weight of I ? in A to max (y, z)
Figure 3: Weighted deductive parsing
items via the deduction rules in Fig. 2. Our parser
performs a weighted deductive parsing (Nederhof,
2003), based on this deduction system. We use a
chart C and an agenda A, both initially empty, and
we proceed as in Fig. 3.
4 Outside Estimates
In order to speed up parsing, we add an estimate of
the log of the outside probabilities of the items to
their weights in the agenda. All our outside esti-
mates are admissible (Klein and Manning, 2003a)
which means that they never underestimate the ac-
tual outside probability of an item. However, most
of them are not monotonic. In other words, it can
happen that we deduce an item I2 from an item I1
where the weight of I2 is greater than the weight
of I1. The parser can therefore end up in a local
maximum that is not the global maximum we are
searching for. In other words, our outside weights
are only figures of merit (FOM). Only for the full
SX estimate, the monotonicity is guaranteed and
we can do true A? parsing as described in (Klein
and Manning, 2003a) that always finds the best
parse.
All outside estimates are computed for a certain
maximal sentence length lenmax.
539
POS tags: 0 : [A, ?1?] A a POS tag
Unary: in : [B,
~l]
in + log(p) : [A,~l] p : A(~?) ? B(~?) ? P
Binary: inB : [B,
~lB], inC : [C,~lC ]
inB + inC + log(p) : [A,~lA]
where p : A( ~?A) ? B( ~?B)C( ~?C) ? P and the follow-
ing holds: we define B(i) as {1 ? j ? dim(B) | ~?B(j)
occurs in ~?A(i)} and C(i) as {1 ? j ? dim(C) | ~?C(j)
occurs in ~?A(i)}. Then for all i, 1 ? i ? dim(A):
~lA(i) = ?j?B(i)~lB(j) + ?j?C(i)~lC(j).
Figure 4: Inside estimate
4.1 Full SX estimate
The full SX estimate, for a given sentence length
n, is supposed to give the minimal costs (maxi-
mal probability) of completing a category X with
a span ? into an S with span ??0, n??.
For the computation, we need an estimate of
the inside probability of a category C with a span
?, regardless of the actual terminals in our in-
put. This inside estimate is computed as shown
in Fig. 4. Here, we do not need to consider the
number of terminals outside the span of C (to
the left or right or in the gaps), they are not rel-
evant for the inside probability. Therefore the
items have the form [A, ?l1, . . . , ldim(A)?], where
A is a non-terminal and li gives the length of its
ith component. It holds that ?1?i?dim(A)li ?
lenmax ? dim(A) + 1.
A straight-forward extension of the CFG algo-
rithm from (Klein and Manning, 2003a) for com-
puting the SX estimate is given in Fig. 5. For a
given range vector ? = ??l1, r1?, . . . , ?lk, rk?? and
a sentence length n, we define its inside length
vector lin(?) as ?r1 ? l1, . . . , rk ? lk? and its
outside length vector lout(?) as ?l1, r1 ? l1, l2 ?
r1, . . . , lk ? rk?1, rk ? lk, n? rk?.
This algorithm has two major problems: Since
it proceeds top-down, in the Binary rules, we must
compute all splits of the antecedent X span into
the spans of A and B which is very expensive.
Furthermore, for a category A with a certain num-
ber of terminals in the components and the gaps,
we compute the lower part of the outside estimate
several times, namely for every combination of
number of terminals to the left and to the right
(first and last element in the outside length vec-
Axiom : 0 : [S, ?0, len, 0?] 1 ? len ? lenmax
Unary: w : [A,
~l]
w + log(p) : [B,~l] p : A(~?) ? B(~?) ? P
Binary-right:
w : [X,~lX ]
w + in(A,~l?A) + log(p) : [B,~lB]
Binary-left:
w : [X,~lX ]
w + in(B,~l?B) + log(p) : [A,~lA]
where, for both rules, there is an instantiated rule p :
X(~?) ? A( ~?A)B( ~?B) such that ~lX = lout(?), ~lA =
lout(?A),~l?A = lin(?A), ~lB = lout(?B,~lB = lin(?B .
Figure 5: Full SX estimate top-down
tor). In order to avoid these problems, we now
abstract away from the lengths of the part to the
left and the right, modifying our items such as to
allow a bottom-up strategy.
The idea is to compute the weights of items rep-
resenting the derivations from a certain lower C
up to some A (C is a kind of ?gap? in the yield of
A) while summing up the inside costs of off-spine
nodes and the log of the probabilities of the corre-
sponding rules. We use items [A,C, ?A, ?C , shift ]
where A,C ? N and ?A, ?C are range vectors,
both with a first component starting at position 0.
The integer shift ? lenmax tells us how many po-
sitions to the right the C span is shifted, compared
to the starting position of the A. ?A and ?C repre-
sent the spans of C and A while disregarding the
number of terminals to the left the right. I.e., only
the lengths of the components and of the gaps are
encoded. This means in particular that the length
n of the sentence does not play a role here. The
right boundary of the last range in the vectors is
limited to lenmax. For any i, 0 ? i ? lenmax,
and any range vector ?, we define shift(?, i) as the
range vector one obtains from adding i to all range
boundaries in ? and shift(?,?i) as the range vec-
tor one obtains from subtracting i from all bound-
aries in ?.
The weight of [A,C, ?A, ?C , i] estimates the
costs for completing a C tree with yield ?C into
an A tree with yield ?A such that, if the span of A
starts at position j, the span of C starts at position
i + j. Fig. 6 gives the computation. The value of
in(A,~l) is the inside estimate of [A,~l].
The SX-estimate for some predicate C with
540
POS tags: 0 : [C,C, ?0, 1?, ?0, 1?, 0] C a POS tag
Unary: 0 : [B,B, ?B, ?B, 0]log(p) : [A,B, ?B, ?B, 0] p : A(~?) ? B(~?) ? P
Binary-right:
0 : [A,A, ?A, ?A, 0], 0 : [B,B, ?B, ?B , 0]
in(A, l(?A)) + log(p) : [X,B, ?X , ?B, i]
Binary-left:
0 : [A,A, ?A, ?A, 0], 0 : [B,B, ?B, ?B, 0]
in(B, l(?B)) + log(p) : [X,A, ?X , ?A, 0]
where i is such that for shift(?B, i) = ??B p : X(?X) ?
A(?A)B(??B) is an instantiated rule.
Starting sub-trees with larger gaps:
w : [B,C, ?B, ?C , i]
0 : [B,B, ?B, ?B, 0]
Transitive closure of sub-tree combination:
w1 : [A,B, ?A, ?B, i], w2 : [B,C, ?B, ?C , j]
w1 + w2 : [A,C, ?A, ?C , i + j]
Figure 6: Full SX estimate bottom-up
span ? where i is the left boundary of the
first component of ? and with sentence length
n is then given by the maximal weight of
[S,C, ?0, n?, shift (?i, ?), i]. Among our esti-
mates, the full SX estimate is the only one that
is monotonic and that allows for true A? parsing.
4.2 SX with Left, Gaps, Right, Length
A problem of the previous estimate is that with
a large number of non-terminals the computation
of the estimate requires too much space. Our ex-
periments have shown that for treebank parsing
where we have, after binarization and markoviza-
tion, appr. 12,000 non-terminals, its computation
is not feasible. We therefore turn to simpler es-
timates with only a single non-terminal per item.
We now estimate the outside probability of a non-
terminal A with a span of a length length (the
sum of the lengths of all the components of the
span), with left terminals to the left of the first
component, right terminals to the right of the
last component and gaps terminals in between the
components of the A span, i.e., filling the gaps.
Our items have the form [X, len , left , right , gaps ]
with X ? N , len+ left +right +gaps ? lenmax,
len ? dim(X), gaps ? dim(X) ? 1.
Let us assume that, in the rule X(~?) ?
A( ~?A)B( ~?B), when looking at the vector ~?, we
have leftA variables for A-components preceding
the first variable of a B component, rightA vari-
ables for A-components following the last vari-
Axiom : 0 : [S, len, 0, 0, 0] 1 ? len ? lenmax
Unary: w : [X, len, l, r, g]w + log(p) : [A, len, l, r, g]
where p : X(~?) ? A(~?) ? P .
Binary-right:
w : [X, len, l, r, g]
w + in(A, len ? lenB) + log(p) : [B, lenB , lB, rB, gB]
Binary-left:
w : [X, len, l, r, g]
w + in(B, len ? lenA) + log(p) : [A, lenA, lA, rA, gA]
where, for both rules, p : X(~?) ? A( ~?A)B( ~?B) ? P .
Figure 7: SX with length, left, right, gaps
POS tags: 0 : [A, 1] A a POS tag
Unary: in : [B, l]in + log(p) : [A, l] p : A(~?) ? B(~?) ? P
Binary: inB : [B, lB], inC : [C, lC ]inB + inC + log(p) : [A, lB + lC ]
where either p : A( ~?A) ? B( ~?B)C( ~?C) ? P or p :
A( ~?A) ? C( ~?C)B( ~?B) ? P .
Figure 8: Inside estimate with total span length
able of a B component and rightB variables for
B-components following the last variable of a A
component. (In our grammars, the first LHS argu-
ment always starts with the first variable from A.)
Furthermore, gapsA = dim(A)?leftA?rightA,
gapsB = dim(B) ? rightB .
Fig. 7 gives the computation of the estimate.
The following side conditions must hold: For
Binary-right to apply, the following constraints
must be satisfied: a) len + l + r + g = lenB +
lB+rB+gB , b) lB ? l+ leftA, c) if rightA > 0,
then rB ? r+rightA, else (rightA = 0), rB = r,
d) gB ? gapsA. Similarly, for Binary-left to ap-
ply, the following constraints must be satisfied: a)
len + l+ r+ g = lenA + lA + rA + gA, b) lA = l,
c) if rightB > 0, then rA ? r + rightB , else
(rightB = 0), rA = r d) gA ? gapsB.
The value in(X, l) for a non-terminal X and a
length l, 0 ? l ? lenmax is an estimate of the
probability of an X category with a span of length
l. Its computation is specified in Fig. 8.
The SX-estimate for a sentence length n and
for some predicate C with a range characterized
by ~? = ??l1, r1?, . . . , ?ldim(C), rdim(C)?? where
len = ?dim(C)i=1 (ri ? li) and r = n ? rdim(C)
is then given by the maximal weight of the item
[C, len , l1, r, n ? len? l1 ? r].
541
Axiom : 0 : [S, len, 0, 0] 1 ? len ? lenmax
Unary: w : [X, len, lr , g]w + log(p) : [A, len, lr , g]
where p : X(~?) ? A(~?) ? P .
Binary-right:
w : [X, len, lr , g]
w + in(A, len ? lenB) + log(p) : [B, lenB, lrB, gB ]
Binary-left:
w : [X, len, lr , g]
w + in(B, len ? lenA) + log(p) : [A, lenA, lrA, gA]
where, for both rules, p : X(~?) ? A( ~?A)B( ~?B) ? P .
Figure 9: SX estimate with length, LR, gaps
4.3 SX with LR, Gaps, Length
In order to further decrease the space complex-
ity, we can simplify the previous estimate by sub-
suming the two lengths left and right in a sin-
gle length lr . I.e., the items now have the form
[X, len , lr , gaps ] with X ? N , len + lr +gaps ?
lenmax, len ? dim(X), gaps ? dim(X) ? 1.
The computation is given in Fig. 9. Again, we
define leftA, gapsA, rightA and gapsB , rightB
for a rule X(~?) ? A( ~?A)B( ~?B) as above. The
side conditions are as follows: For Binary-right to
apply, the following constraints must be satisfied:
a) len + lr + g = lenB + lrB + gB , b) lr < lrB,
and c) gB ? gapsA. For Binary-left to apply, the
following must hold: a) len + lr + g = lenA +
lrA + gA, b) if rightB = 0 then lr = lrA, else
lr < lrA and c) gA ? gapsB.
The SX-estimate for a sentence length n
and for some predicate C with a span ~? =
??l1, r1?, . . . , ?ldim(C), rdim(C)?? where len =
?dim(C)i=1 (ri ? li) and r = n ? rdim(C) is then the
maximal weight of [C, len , l1+r, n?len?l1?r].
5 Evaluation
The goal of our evaluation of our parser is to
show that, firstly, reasonable parser speed can be
achieved and, secondly, the parser output is of
promising quality.
5.1 Data
Our data source is the German NeGra treebank
(Skut et al, 1997). In a preprocessing step,
following common practice (Ku?bler and Penn,
2008), we attach punctuation (not included in the
NeGra annotation) as follows: In a first pass, us-
ing heuristics, we attach punctuation as high as
possible while avoiding to introduce new crossing
branches. In a second pass, parentheses and quo-
tation marks preferably attach to the same node.
Grammatical function labels on the edges are dis-
carded.
We create data sets of different sizes in order
to see how the size of the training set relates to
the gain using context summary estimates and to
the output quality of the parser. The first set uses
the first 4000 sentences and the second one all
sentences of NeGra. Due to memory limitations,
in both sets, we limit ourselves to sentences of a
maximal length of 25 words. We use the first 90%
of both sets as training set and the remaining 10%
as test set. Tab. 1 shows the resulting sizes.
NeGra-small NeGra
training test training test
size 2839 316 14858 1651
Table 1: Test and training sets
5.2 Treebank Grammar Extraction
S
VP
VP
PROAV VMFIN VVPP VAINF
daru?ber mu? nachgedacht werden
about it must thought be
?It must be thought about it?
Figure 10: A sample tree from NeGra
As already mentioned, in NeGra, discontinu-
ous phrases are annotated with crossing branches
(see Fig. 10 for an example with two discontin-
uous VPs). Such discontinuities can be straight-
forwardly modelled with LCFRS. We use the al-
gorithm from Maier and S?gaard (2008) to extract
LCFRS rules from NeGra and TIGER. It first cre-
ates rules of the form P (a) ? ? for each pre-
terminal P dominating some terminal a. Then
for all other nonterminals A0 with the children
A1 ? ? ?Am, a clause A0 ? A1 ? ? ?Am is cre-
ated. The arguments of the A1 ? ? ?Am are sin-
gle variables where the number of arguments is
the number of discontinuous parts in the yield of
a predicate. The arguments of A0 are concate-
nations of these variables that describe how the
542
discontinuous parts of the yield of A0 are ob-
tained from the yields of its daughters. Differ-
ent occurrences of the same non-terminal, only
with different fan-outs, are distinguished by corre-
sponding subscripts. Note that this extraction al-
gorithm yields only monotone LCFRS (equivalent
to ordered simple RCG). See Maier and S?gaard
(2008) for further details. For Fig. 10, we obtain
for instance the rules in Fig. 11.
PROAV(Daru?ber) ? ? VMFIN(mu?) ? ?
VVPP(nachgedacht) ? ? VAINF(werden) ? ?
S1(X1X2X3) ? VP2(X1, X3) VMFIN(X2)
VP2(X1, X2X3) ? VP2(X1, X2) VAINF(X3)
VP2(X1, X2) ? PROAV(X1) VVPP(X2)
Figure 11: LCFRS rules for the tree in Fig. 10
5.3 Binarization and Markovization
Before parsing, we binarize the extracted LCFRS.
For this we first apply Collins-style head rules,
based on the rules the Stanford parser (Klein and
Manning, 2003b) uses for NeGra, to mark the
resp. head daughters of all non-terminal nodes.
Then, we reorder the RHSs such that the sequence
? of elements to the right of the head daughter is
reversed and moved to the beginning of the RHS.
We then perform a binarization that proceeds from
left to right. The binarization works like the trans-
formation into Chomsky Normal Form for CFGs
in the sense that for RHSs longer than 2, we in-
troduce a new non-terminal that covers the RHS
without the first element. The rightmost new rule,
which covers the head daughter, is binarized to
unary. We do not use a unique new non-terminal
for every new rule. Instead, to the new symbols
introduced during the binarization (VPbin in the
example), a variable number of symbols from the
vertical and horizontal context of the original rule
is added in order to achieve markovization. Fol-
lowing the literature, we call the respective quan-
tities v and h. For reasons of space we restrict
ourselves here to the example in Fig. 12. Refer to
Maier and Kallmeyer (2010) for a detailed presen-
tation of the binarization and markovization.
The probabilities are then computed based on
the rule frequencies in the transformed treebank,
using a Maximum Likelihood estimator.
S
VP
PDS VMFIN PIS AD V VVINF
das mu? man jetzt machen
that must one now do
?One has to do that now?
Tree after binarization:
S
Sbin
VP
VPbin
Sbin VPbin
PDS VMFIN PIS ADV VVINF
Figure 12: Sample binarization
5.4 Evaluation of Parsing Results
In order to assess the quality of the output of
our parser, we choose an EVALB-style metric,
i.e., we compare phrase boundaries. In the con-
text of LCFRS, we compare sets of items [A, ~?]
that characterize the span of a non-terminal A in
a derivation tree. One set is obtained from the
parser output, and one from the corresponding
treebank trees. Using these item sets, we compute
labeled and unlabeled recall (LR/UR), precision
(LP/UP), and the F1 measure (LF1/UF1). Note
that if k = 1, our metric is identical to its PCFG
equivalent.We are aware of the recent discussion
about the shortcomings of EVALB. A discussion
of this issue is presented in Maier (2010).
5.5 Experiments
In all experiments, we provide the parser with
gold part-of-speech tags. For the experi-
ments with NeGra-small, the parser is given the
markovization settings v = 1 and h = 1. We com-
pare the parser performance without estimates
(OFF) with its performance with the estimates de-
scribed in 4.2 (SIMPLE) and 4.3 (LR). Tab. 2
shows the results. Fig. 13 shows the number of
items produced by the parser, indicating that the
estimates have the desired effect of preventing un-
necessary items from being produced. Note that it
is even the case that the parser produces less items
for the big set with LR than for the small set with-
out estimate.
We can see that the estimates lead to a slightly
543
OFF SIMPLE LR
UP/UR 72.29/72.40 70.49/71.81 72.10/72.60
UF1 72.35 71.14 72.35
LP/LR 68.31/68.41 64.93/66.14 67.35/66.14
LF1 68.36 65.53 65.53
Parsed 313 (99.05%) 313 (99.05%) 313 (99.05%)
Table 2: Experiments with NeGra-small
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
 16  18  20  22  24
N
o.
 o
f i
te
m
s 
(in
 10
00
)
Sentence length
OFF (NeGra)
LR (NeGra)
OFF (NeGra-small)
SIMPLE (NeGra-small)
LR (NeGra-small)
Figure 13: Items produced by the parser
lower F-score. However, while the losses in terms
of F1 are small, the gains in parsing time are sub-
stantial, as Fig. 13 shows.
Tab. 3 shows the results of experiments with
NeGra, with the markovization settings v = 2
and h = 1 which have proven to be successful
for PCFG parsing of NeGra (Rafferty and Man-
ning, 2008). Unfortunately, due to memory re-
strictions, we were not able to compute SIMPLE
for the large data set.1 Resp. LR, the findings
are comparable to the ones for NeGra-short. The
speedup is paid with a lower F1.
OFF LR
UP/UR 76.89/77.35 75.22/75.99
UF1 77.12 75.60
LP/LR 73.03/73.46 70.98/71.70
LF1 73.25 71.33
Parsed 1642 (99.45%) 1642 (99.45%)
Table 3: Experiments with NeGra
Our results are not directly comparable with
PCFG parsing results, since LCFRS parsing is a
1SIMPLE also proved to be infeasible to compute for the
small set for the markovization settings v = 2 and h = 1
due to the greatly increased label set with this settings.
harder task. However, since the EVALB met-
ric coincides for constituents without crossing
branches, in order to place our results in the con-
text of previous work on parsing NeGra, we cite
some of the results from the literature which were
obtained using PCFG parsers2: Ku?bler (2005)
(Tab. 1, plain PCFG) obtains 69.4, Dubey and
Keller (2003) (Tab. 5, sister-head PCFG model)
71.12, Rafferty and Manning (2008) (Tab. 2, Stan-
ford parser with markovization v = 2 and h = 1)
77.2, and Petrov and Klein (2007) (Tab. 1, Berke-
ley parser) 80.1. Plaehn (2004) obtains 73.16 La-
beled F1 using Probabilistic Discontinuous Phrase
Structure Grammar (DPSG), albeit only on sen-
tences with a length of up to 15 words. On those
sentences, we obtain 81.27.
The comparison shows that our system deliv-
ers competitive results. Additionally, when com-
paring this to PCFG parsing results, one has
to keep in mind that LCFRS parse trees con-
tain non-context-free information about disconti-
nuities. Therefore, a correct parse with our gram-
mar is actually better than a correct CFG parse,
evaluated with respect to a transformation of Ne-
Gra into a context-free treebank where precisely
this information gets lost.
6 Conclusion
We have presented the first parser for unrestricted
Probabilistic Linear Context-Free Rewriting Sys-
tems (PLCFRS), implemented as a CYK parser
with weighted deductive parsing. To speed up
parsing, we use context summary estimates for
parse items. An evaluation on the NeGra treebank,
both in terms of output quality and speed, shows
that data-driven parsing using PLCFRS is feasi-
ble. Already in this first attempt with a straight-
forward binarization, we obtain results that are
comparable to state-of-the-art PCFG results in
terms of F1, while yielding parse trees that are
richer than context-free trees since they describe
discontinuities. Therefore, our approach demon-
strates convincingly that PLCFRS is a natural and
tractable alternative for data-driven parsing which
takes non-local dependencies into consideration.
2Note that these results were obtained on sentences with
a length of ? 40 words and that those parser possibly would
deliver better results if tested on our test set.
544
References
Boullier, Pierre. 1998. A Proposal for a Natural Lan-
guage Processing Syntactic Backbone. Technical
Report 3342, INRIA.
Boyd, Adriane. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations.
In The Linguistic Annotation Workshop at ACL
2007.
Charniak, Eugene and Sharon A. Caraballo. 1998.
New figures of merit for best-first probabilistic chart
parsing. Computational Linguistics, 24.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Dubey, Amit and Frank Keller. 2003. Probabilistic
parsing for German using sisterhead dependencies.
In Proceedings of ACL.
Go?mez-Rodr??guez, Carlos, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proceedings of NAACL-HLT.
Hockenmaier, Julia. 2003. Data and models for Statis-
tical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Johnson, Mark. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of ACL.
Kato, Yuki, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple context-free grammar for rna
pseudoknot modeling. In Proceedings of TAG+8.
Klein, Dan and Christopher D. Manning. 2003a. A*
Parsing: Fast Exact Viterbi Parse Selection. In Pro-
ceedings of NAACL-HLT.
Klein, Dan and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems 15 (NIPS).
Ku?bler, Sandra and Gerald Penn, editors. 2008. Pro-
ceedings of the Workshop on Parsing German at
ACL 2008.
Ku?bler, Sandra. 2005. How do treebank annotation
schemes influence parsing results? Or how not to
compare apples and oranges. In Proceedings of
RANLP 2005.
Kuhlmann, Marco and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL.
Levy, Roger and Christopher D. Manning. 2004. Deep
dependencies from context-free statistical parsers:
correcting the surface dependency approximation.
In Proceedings of ACL.
Maier, Wolfgang and Laura Kallmeyer. 2010. Discon-
tinuity and non-projectivity: Using mildly context-
sensitive formalisms for data-driven parsing. In
Proceedings of TAG+10.
Maier, Wolfgang and Timm Lichte. 2009. Charac-
terizing Discontinuity in Constituent Treebanks. In
Proceedings of Formal Grammar 2009.
Maier, Wolfgang and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar 2008.
Maier, Wolfgang. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
SPMRL workshop at NAACL HLT 2010.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
HLT.
Nederhof, Mark-Jan. 2003. Weighted Deductive Pars-
ing and Knuth?s Algorithm. Computational Lin-
guistics, 29(1).
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007.
Plaehn, Oliver. 2004. Computing the most proba-
ble parse for a discontinuous phrase-structure gram-
mar. In New developments in parsing technology.
Kluwer.
Rafferty, Anna and Christopher D. Manning, 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Ku?bler and Penn
(2008).
Seki, Hiroyuki, Takahashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88(2).
Skut, Wojciech, Brigitte Krenn, Thorten Brants, and
Hans Uszkoreit. 1997. An Annotation Scheme
for Free Word Order Languages. In Proceedings of
ANLP.
Vijay-Shanker, K., David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descriptions
produced by various grammatical formalisms. In
Proceedings of ACL.
545
Data-Driven Parsing using Probabilistic
Linear Context-Free Rewriting Systems
Laura Kallmeyer?
Heinrich-Heine-Universita?t Du?sseldorf
Wolfgang Maier??
Heinrich-Heine-Universita?t Du?sseldorf
This paper presents the first efficient implementation of a weighted deductive CYK parser for
Probabilistic Linear Context-Free Rewriting Systems (PLCFRSs). LCFRS, an extension of CFG,
can describe discontinuities in a straightforward way and is therefore a natural candidate to be
used for data-driven parsing. To speed up parsing, we use different context-summary estimates
of parse items, some of them allowing for A? parsing. We evaluate our parser with grammars
extracted from the German NeGra treebank. Our experiments show that data-driven LCFRS
parsing is feasible and yields output of competitive quality.
1. Introduction
Recently, the challenges that a rich morphology poses for data-driven parsing have
received growing interest. A direct effect of morphological richness is, for instance, data
sparseness on a lexical level (Candito and Seddah 2010). A rather indirect effect is that
morphological richness often relaxes word order constraints. The principal intuition is
that a rich morphology encodes information that otherwise has to be conveyed by a
particular word order. If, for instance, the case of a nominal complement is not provided
by morphology, it has to be provided by the position of the complement relative to other
complements in the sentence. Example (1) provides an example of case marking and free
word order in German. In turn, in free word order languages, word order can encode
information structure (Hoffman 1995).
(1) a. der
the
kleine
little
Jungenom
boy
schickt
sends
seiner
his
Schwesterdat
sister
den
the
Briefacc
letter
b. Other possible word orders:
(i) der kleine Jungenom schickt den Briefacc seiner Schwesterdat
(ii) seiner Schwesterdat schickt der kleine Jungenom den Briefacc
(iii) den Briefacc schickt der kleine Jungenom seiner Schwesterdat
? Institut fu?r Sprache und Information, Universita?tsstr. 1, D-40225 Du?sseldorf, Germany.
E-mail: kallmeyer@phil.uni-duesseldorf.de.
?? Institut fu?r Sprache und Information, Universita?tsstr. 1, D-40225 Du?sseldorf, Germany.
E-mail: maierw@hhu.de.
Submission received: September 29, 2011; revised submission received: May 20, 2012; accepted for publication:
August 3, 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
It is assumed that this relation between a rich morphology and free word order does
not hold in both directions. Although it is generally the case that languages with a rich
morphology exhibit a high degree of freedom in word order, languages with a free word
order do not necessarily have a rich morphology. Two examples for languages with a
very free word order are Turkish and Bulgarian. The former has a very rich and the
latter a sparse morphology. See Mu?ller (2002) for a survey of the linguistics literature on
this discussion.
With a rather free word order, constituents and single parts of them can be displaced
freely within the sentence. German, for instance, has a rich inflectional system and
allows for a free word order, as we have already seen in Example (1): Arguments can
be scrambled, and topicalizations and extrapositions underlie few restrictions. Conse-
quently, discontinuous constituents occur frequently. This is challenging for syntactic
description in general (Uszkoreit 1986; Becker, Joshi, and Rambow 1991; Bunt 1996;
Mu?ller 2004), and for treebank annotation in particular (Skut et al 1997).
In this paper, we address the problem of data-driven parsing of discontinuous constit-
uents on the basis of German. In this section, we inspect the type of data we have to deal
with, and we describe the way such data are annotated in treebanks. We briefly discuss
different parsing strategies for the data in question and motivate our own approach.
1.1 Discontinuous Constituents
Consider the sentences in Example (2) as examples for discontinuous constituents
(taken from the German NeGra [Skut et al 1997] and TIGER [Brants et al 2002] tree-
banks). Example (2a) shows several instances of discontinuous VPs and Example (2b)
shows a discontinuous NP. The relevant constituent is printed in italics.
(2) a. Fronting:
(i) Daru?ber
Thereof
muss
must
nachgedacht
thought
werden.
be
(NeGra)
?One must think of that?
(ii) Ohne internationalen Schaden
Without international damage
ko?nne
could
sich
itself
Bonn
Bonn
von dem Denkmal
from the monument
nicht
not
distanzieren,
distance
... (TIGER)
?Bonn could not distance itself from the monument without international
damage.?
(iii) Auch
Also
wu?rden
would
durch die Regelung
through the regulation
nur
only
?sta?ndig
?constantly
neue
new
Altfa?lle
old cases
entstehen?. (TIGER)
emerge?
?Apart from that, the regulation would only constantly produce new old cases.?
b. Extraposed relative clauses:
(i) . . . ob
. . . whether
auf
on
deren
their
Gela?nde
terrain
der
the
Typ von Abstellanlage
type of parking facility
gebaut
built
werden
get
ko?nne,
could,
der ...
which . . .
(NeGra)
?. . . whether one could build on their premises the type of parking facility,
which . . . ?
88
Kallmeyer and Maier PLCFRS Parsing
Examples of other such languages are Bulgarian and Korean. Both show discontin-
uous constituents as well. Example (3a) is a Bulgarian example of a PP extracted out of
an NP, taken from the BulTreebank (Osenova and Simov 2004), and Example (3b) is an
example of fronting in Korean, taken from the Penn Korean Treebank (Han, Han, and
Ko 2001).
(3) a. Na kyshtata
Of house-DET
toi
he
popravi
repaired
pokriva.
roof.
?It is the roof of the house he repairs.?
b. Gwon.han-u?l
Authority-OBJ
nu.ga
who
ka.ji.go
has
iss.ji?
not?
?Who has no authority??
Discontinuous constituents are by no means limited to languages with freedom
in word order. They also occur in languages with a rather fixed word order such
as English, resulting from, for instance, long-distance movements. Examples (4a) and
(4b) are examples from the Penn Treebank for long extractions resulting in discontin-
uous S categories and for discontinuous NPs arising from extraposed relative clauses,
respectively (Marcus et al 1994).
(4) a. Long Extraction in English:
(i) Those chains include Bloomingdale?s, which Campeau recently said it
will sell.
(ii) What should I do.
b. Extraposed nominal modifiers (relative clauses and PPs) in English:
(i) They sow a row of male-fertile plants nearby, which then pollinate the male-
sterile plants.
(ii) Prices fell marginally for fuel and electricity.
1.2 Treebank Annotation and Data-Driven Parsing
Most constituency treebanks rely on an annotation backbone based on Context-Free
Grammar (CFG). Discontinuities cannot be modeled with CFG, because they require a
larger domain of locality than the one offered by CFG. Therefore, the annotation back-
bone based on CFG is generally augmented with a separate mechanism that accounts
for the non-local dependencies. In the Penn Treebank (PTB), for example, trace nodes
and co-indexation markers are used in order to establish additional implicit edges in the
tree beyond the overt phrase structure. In Tu?Ba-D/Z (Telljohann et al 2012), a German
Treebank, non-local dependencies are expressed via an annotation of topological fields
(Ho?hle 1986) and special edge labels. In contrast, some other treebanks, among them
NeGra and TIGER, give up the annotation backbone based on CFG and allow annota-
tion with crossing branches (Skut et al 1997). In such an annotation, non-local depen-
dencies can be expressed directly by grouping all dependent elements under a single
node. Note that both crossing branches and traces annotate long-distance dependencies
in a linguistically meaningful way. A difference is, however, that crossing branches
are less theory-dependent because they do not make any assumptions about the base
positions of ?moved? elements.
Examples for the different approaches of annotating discontinuities are given in
Figures 1 and 2. Figure 1 shows the NeGra annotation of Example (2a-i) (left), and an
89
Computational Linguistics Volume 39, Number 1
Figure 1
A discontinuous constituent. Original NeGra annotation (left) and a Tu?Ba-D/Z-style annotation
(right).
What
WP
should
MD
I
PRP
do
VB
*T*
-NONE-
?
.
WHNP NP NP
VP
SBJ
SQ
SBARQ
*T*
What
WP
should
MD
I
PRP
do
VB
?
.
WHNP NP
VP
SBJ
SQ
SBARQ
Figure 2
A discontinuous wh-movement. Original PTB annotation (left) and NeGra-style annotation
(right).
annotation of the same sentence in the style of the Tu?Ba-D/Z treebank (right). Figure 2
shows the PTB annotation of Example (4a-ii) (on the left, note that the directed edge
from the trace to the WHNP element visualizes the co-indexation) together with a
NeGra-style annotation of the same sentence (right).
In the past, data-driven parsing has largely been dominated by Probabilistic
Context-Free Grammar (PCFG). In order to extract a PCFG from a treebank, the trees
need to be interpretable as CFG derivations. Consequently, most work has excluded
non-local dependencies; either (in PTB-like treebanks) by discarding labeling conven-
tions such as the co-indexation of the trace nodes in the PTB, or (in NeGra/TIGER-like
treebanks) by applying tree transformations, which resolve the crossing branches (e.g.,
Ku?bler 2005; Boyd 2007). Especially for the latter treebanks, such a transformation is
problematic, because it generally is non-reversible and implies information loss.
Discontinuities are no minor phenomenon: Approximately 25% of all sentences
in NeGra and TIGER have crossing branches (Maier and Lichte 2011). In the Penn
Treebank, this holds for approximately 20% of all sentences (Evang and Kallmeyer
2011). This shows that it is important to properly treat such structures.
1.3 Extending the Domain of Locality
In the literature, different methods have been explored that allow for the use of non-
local information in data-driven parsing. We distinguish two classes of approaches.
The first class consists of approaches that aim at using formalisms which produce
trees without crossing branches but provide a larger domain of locality than CFG?
for instance, through complex labels (Hockenmaier 2003) or through the derivation
90
Kallmeyer and Maier PLCFRS Parsing
CFG:
A
?
LCFRS: ?
A
? ?
?1 ?2 ?3
Figure 3
Different domains of locality.
mechanism (Chiang 2003). The second class, to which we contribute in this paper,
consists of approaches that aim at producing trees which contain non-local information.
Some methods realize the reconstruction of non-local information in a post- or pre-
processing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004;
Cai, Chiang, and Goldberg 2011). Other work uses formalisms that accommodate the
direct encoding of non-local information (Plaehn 2004; Levy 2005). We pursue the latter
approach.
Our work is motivated by the following recent developments. Linear Context-Free
Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been estab-
lished as a candidate for modeling both discontinuous constituents and non-projective
dependency trees as they occur in treebanks (Maier and S?gaard 2008; Kuhlmann and
Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where
the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Be-
cause LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs,
PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted
deductive parsing (Nederhof 2003), and A? parsing (Klein and Manning 2003a) can
be transferred to LCFRS. Finally, as mentioned before, languages such as German
have recently attracted the interest of the parsing community (Ku?bler and Penn 2008;
Seddah, Ku?bler, and Tsarfaty 2010).
We bring together these developments by presenting a parser for Probabilistic
LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser pro-
duces trees with crossing branches and thereby accounts for syntactic long-distance
dependencies while not making any additional assumptions concerning the position
of hypothetical traces. We have implemented a CYK parser and we present several
methods for context summary estimation of parse items. The estimates either act as
figures-of-merit in a best-first parsing context or as estimates for A? parsing. A test on
a real-world-sized data set shows that our parser achieves competitive results. To our
knowledge, our parser is the first for the entire class of PLCFRS that has successfully
been used for data-driven parsing.1
The paper is structured as follows. Section 2 introduces probabilistic LCFRS. Sec-
tions 3 and 4 present the binarization algorithm, the parser, and the outside estimates
which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from
a treebank and we present grammar refinement methods for these specific treebank
grammars. Finally, Section 6 presents evaluation results and Section 7 compares our
work to other approaches.
1 Parts of the results presented in this paper have been presented earlier. More precisely, in Kallmeyer and
Maier (2010), we presented the general architecture of the parser and all outside estimates except the LN
estimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012). In Maier and
Kallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2. Finally,
Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics).
91
Computational Linguistics Volume 39, Number 1
2. Probabilistic Linear Context-Free Rewriting Systems
2.1 Definition of PLCFRS
LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG in which a non-
terminal can span not only a single string but a tuple of strings of size k ? 1. k is thereby
called its fan-out. We will notate LCFRS with the syntax of Simple Range Concate-
nation Grammars (SRCG) (Boullier 1998b), a formalism that is equivalent to LCFRS.
A third formalism that is equivalent to LCFRS is Multiple Context-Free Grammar
(MCFG) (Seki et al 1991).
Definition 1 (LCFRS)
A Linear Context-Free Rewriting System (LCFRS) is a tuple ?N, T, V, P, S? where
a) N is a finite set of non-terminals with a function dim: N ? N that
determines the fan-out of each A ? N;
b) T and V are disjoint finite sets of terminals and variables;
c) S ? N is the start symbol with dim(S) = 1;
d) P is a finite set of rules
A(?1, . . . ,?dim(A) ) ? A1(X(1)1 , . . . , X
(1)
dim(A1)
) ? ? ?Am(X(m)1 , . . . , X
(m)
dim(Am )
)
for m ? 0 where A, A1, . . . , Am ? N, X(i)j ? V for 1 ? i ? m, 1 ? j ? dim(Ai)
and ?i ? (T ? V)? for 1 ? i ? dim(A). For all r ? P, it holds that every
variable X occurring in r occurs exactly once in the left-hand side and
exactly once in the right-hand side of r.
A rewriting rule describes how the yield of the left-hand side non-terminal can be
computed from the yields of the right-hand side non-terminals. The rules A(ab, cd) ? ?
and A(aXb, cYd) ? A(X, Y) from Figure 4 for instance specify that (1) ?ab, cd? is in the
yield of A and (2) one can compute a new tuple in the yield of A from an already existing
one by wrapping a and b around the first component and c and d around the second.
A CFG rule A ? BC would be written A(XY) ? B(X)C(Y) as an LCFRS rule.
Definition 2 (Yield, language)
Let G = ?N, T, V, P, S? be an LCFRS.
1. For every A ? N, we define the yield of A, yield(A) as follows:
a) For every rule A(?) ? ?, ? ? yield(A);
A(ab, cd) ? ?
A(aXb, cYd) ? A(X, Y)
S(XY) ? A(X, Y)
Figure 4
Sample LCFRS for {anbncndn | n ? 1}.
92
Kallmeyer and Maier PLCFRS Parsing
b) For every rule A(?1, . . . ,?dim(A) ) ? A1(X(1)1 , . . . , X
(1)
dim(A1)
) ? ? ?
Am(X
(m)
1 , . . . , X
(m)
dim(Am )
) and for all ?i ? yield(Ai) (1 ? i ? m):
? f (?1), . . . , f (?dim(A) )? ? yield(A) where f is defined as follows:
(i) f (t) = t for all t ? T,
(ii) f (X(i)j ) = ?i(j) for all 1 ? i ? m, 1 ? j ? dim(Ai) and
(iii) f (xy) = f (x)f (y) for all x, y ? (T ? V)+.
We call f the composition function of the rule.
c) Nothing else is in yield(A).
2. The language of G is then L(G) = {w | ?w? ? yield(S)}.
As an example, consider again the LCFRS in Figure 4. The last rule tells us that,
given a pair in the yield of A, we can obtain an element in the yield of S by concate-
nating the two components. Consequently, the language generated by this grammar is
{anbncndn |n ? 1}.
The terms of grammar fan-out and rank and the properties of monotonicity and
?-freeness will be referred to later and are therefore introduced in the following defini-
tion. They are taken from the LCFRS/MCFG terminology; the SRCG term for fan-out is
arity and the property of being monotone is called ordered in the context of SRCG.
Definition 3
Let G = ?N, T, V, P, S? be an LCFRS.
1. The fan-out of G is the maximal fan-out of all non-terminals in G.
2. Furthermore, the right-hand side length of a rewriting rule r ? P is called
the rank of r and the maximal rank of all rules in P is called the rank of G.
3. G is monotone if for every r ? P and every right-hand side non-terminal A
in r and each pair X1, X2 of arguments of A in the right-hand side of r, X1
precedes X2 in the right-hand side iff X1 precedes X2 in the left-hand side.
4. A rule r ? P is called an ?-rule if one of the left-hand side components of r
is ?.
G is ?-free if it either contains no ?-rules or there is exactly one ?-rule
S(?) ? ? and S does not appear in any of the right-hand sides of the rules
in the grammar.
For every LCFRS there exists an equivalent LCFRS that is ?-free (Seki et al 1991;
Boullier 1998a) and monotone (Michaelis 2001; Kracht 2003; Kallmeyer 2010).
The definition of a probabilistic LCFRS is a straightforward extension of the defini-
tion of PCFG and thus it follows (Levy 2005; Kato, Seki, and Kasami 2006) that:
Definition 4 (PLCFRS)
A probabilistic LCFRS (PLCFRS) is a tuple ?N, T, V, P, S, p? such that ?N, T, V, P, S? is an
LCFRS and p : P ? [0..1] a function such that for all A ? N:
?A(x)???Pp(A(x) ? ?) = 1
93
Computational Linguistics Volume 39, Number 1
PLCFRS with non-terminals {S, A, B}, terminals {a} and start symbol S:
0.2 : S(X) ? A(X) 0.8 : S(XY) ? B(X, Y)
0.7 : A(aX) ? A(X) 0.3 : A(a) ? ?
0.8 : B(aX, aY) ? B(X, Y) 0.2 : B(a, a) ? ?
Figure 5
Sample PLCFRS.
As an example, consider the PLCFRS in Figure 5. This grammar simply generates
a+. Words with an even number of as and nested dependencies are more probable
than words with a right-linear dependency structure. For instance, the word aa receives
the two analyses in Figure 6. The analysis (a) displaying nested dependencies has
probability 0.16 and (b) (right-linear dependencies) has probability 0.042.
3. Parsing PLCFRS
3.1 Binarization
Similarly to the transformation of a CFG into Chomsky normal form, an LCFRS can be
binarized, resulting in an LCFRS of rank 2. As in the CFG case, in the transformation,
we introduce a non-terminal for each right-hand side longer than 2 and split the rule
into two rules, using this new intermediate non-terminal. This is repeated until all
right-hand sides are of length 2. The transformation algorithm is inspired by Go?mez-
Rodr??guez et al (2009) and it is also specified in Kallmeyer (2010).
3.1.1 General Binarization. In order to give the algorithm for this transformation, we
need the notion of a reduction of a vector ? ? [(T ? V)?]i by a vector x ? Vj where all
variables in x occur in ?. A reduction is, roughly, obtained by keeping all variables in ?
that are not in x. This is defined as follows:
Definition 5 (Reduction)
Let ?N, T, V, P, S? be an LCFRS, ? ? [(T ? V)?]i and x ? Vj for some i, j ? IN.
Let w = ?1$ . . . $?i be the string obtained from concatenating the components of ?,
separated by a new symbol $ /? (V ? T).
Let w? be the image of w under a homomorphism h defined as follows: h(a) = $ for
all a ? T, h(X) = $ for all X ? {x1, . . .xj} and h(y) = y in all other cases.
Let y1, . . . ym ? V+ such that w? ? $?y1$+y2$+ . . . $+ym$?. Then the vector
?y1, . . . ym? is the reduction of ? by x.
For instance, ?aX1, X2, bX3? reduced with ?X2? yields ?X1, X3? and ?aX1X2bX3? re-
duced with ?X2? yields ?X1, X3? as well.
S
B
a a
(a)
S
A
a A
a
(b)
Figure 6
The two derivations of aa.
94
Kallmeyer and Maier PLCFRS Parsing
for all rules r = A(?) ? A0( ?0) . . .Am( ?m) in P with m > 1 do
remove r from P
R := ?
pick new non-terminals C1, . . . , Cm?1
add the rule A(?) ? A0( ?0)C1( ?1) to R where ?1 is obtained by reducing ? with ?0
for all i, 1 ? i ? m ? 2 do
add the rule Ci(?i) ? Ai(?i)Ci+1( ?i+1) to R where ?i+1 is obtained by reducing ?i with ?i
end for
add the rule Cm?1( ?m?2) ? Am?1( ?m?1)Am( ?m) to R
for every rule r? ? R do
replace right-hand side arguments of length > 1 with new variables (in both sides) and
add the result to P
end for
end for
Figure 7
Algorithm for binarizing an LCFRS.
The binarization algorithm is given in Figure 7. As already mentioned, it proceeds
like the CFG binarization algorithm in the sense that for right-hand sides longer than
2, we introduce a new non-terminal that covers the right-hand side without the first
element. Figure 8 shows an example. In this example, there is only one rule with a right-
hand side longer than 2. In a first step, we introduce the new non-terminals and rules
that binarize the right-hand side. This leads to the set R. In a second step, before adding
the rules from R to the grammar, whenever a right-hand side argument contains several
variables, these are collapsed into a single new variable.
The equivalence of the original LCFRS and the binarized grammar is rather straight-
forward. Note, however, that the fan-out of the LCFRS can increase.
The binarization depicted in Figure 7 is deterministic in the sense that for every rule
that needs to be binarized, we choose unique new non-terminals. Later, in Section 5.3.1,
we will introduce additional factorization into the grammar rules that reduces the set
of new non-terminals.
3.1.2 Minimizing Fan-Out and Number of Variables. In LCFRS, in contrast to CFG, the order
of the right-hand side elements of a rule does not matter for the result of a derivation.
Original LCFRS:
S(XYZUVW) ? A(X, U)B(Y, V)C(Z, W)
A(aX, aY) ? A(X, Y) A(a, a) ? ?
B(bX, bY) ? B(X, Y) B(b, b) ? ?
C(cX, cY) ? C(X, Y) C(c, c) ? ?
Rule with right-hand side of length > 2: S(XYZUVW) ? A(X, U)B(Y, V)C(Z, W)
For this rule, we obtain
R = {S(XYZUVW) ? A(X, U)C1(YZ, VW), C1(YZ, VW) ? B(Y, V)C(Z, W)}
Equivalent binarized LCFRS:
S(XPUQ) ? A(X, U)C1(P, Q)
C1(YZ, VW) ? B(Y, V)C(Z, W)
A(aX, aY) ? A(X, Y) A(a, a) ? ?
B(bX, bY) ? B(X, Y) B(b, b) ? ?
C(cX, cY) ? C(X, Y) C(c, c) ? ?
Figure 8
Sample binarization of an LCFRS.
95
Computational Linguistics Volume 39, Number 1
Therefore, we can reorder the right-hand side of a rule before binarizing it. In the
following, we present a binarization order that yields a minimal fan-out and a minimal
variable number per production and binarization step. The algorithm is inspired by
Go?mez-Rodr??guez et al (2009) and has first been published in this version in Kallmeyer
(2010). We assume that we are only considering partitions of right-hand sides where one
of the sets contains only a single non-terminal.
For a given rule c = A0(x0) ? A1(x1) . . .Ak(xk), we define the characteristic string
s(c, Ai) of the Ai-reduction of c as follows: Concatenate the elements of x0, separated with
new additional symbols $ while replacing every component from xi with a $. We then
define the arity of the characteristic string, dim(s(c, Ai)), as the number of maximal sub-
strings x ? V+ in s(Ai). Take, for example, a rule c = VP(X, YZU) ? VP(X, Z)V(Y)N(U).
Then s(c, VP) =$$Y$U, s(c, V) = X$$ZU.
Figure 9 shows how in a first step, for a given rule r with right-hand side length > 2,
we determine the optimal candidate for binarization based on the characteristic string
s(r, B) of some right-hand side non-terminal B and on the fan-out of B: On all right-
hand side predicates B we check for the maximal fan-out (given by dim(s(r, B))) and the
number of variables (dim(s(r, B)) + dim(B)) we would obtain when binarizing with this
predicate. This check provides the optimal candidate. In a second step we then perform
the same binarization as before, except that we use the optimal candidate now instead
of the first element of the right-hand side.
3.2 The Parser
We can assume without loss of generality that our grammars are ?-free and monotone
(the treebank grammars with which we are concerned all have these properties) and that
they contain only binary and unary rules. Furthermore, we assume POS tagging to be
done before parsing. POS tags are non-terminals of fan-out 1. Finally, according to our
grammar extraction algorithm (see Section 5.1), a separation between two components
always means that there is actually a non-empty gap in between them. Consequently,
two different components in a right-hand side can never be adjacent in the same
component of the left-hand side. The rules are then either of the form A(a) ? ? with A a
POS tag and a ? T or of the form A(x) ? B(x) or A(?) ? B(x)C(y) where ? ? (V+)dim(A),
x ? Vdim(B), y ? Vdim(C), that is, only the rules for POS tags contain terminals in their left-
hand sides.
cand = 0
fan-out = number of variables in r
vars = number of variables in r
for all i = 0 to m do
cand-fan-out = dim(s(r, Ai));
if cand-fan-out < fan-out and dim(Ai) < fan-out then
fan-out = max({cand-fan-out, dim(Ai)});
vars = cand-fan-out + dim(Ai);
cand = i;
else if cand-fan-out ? fan-out, dim(Ai) ? fan-out and cand-fan-out + dim(Ai) < vars then
fan-out = max({cand-fan-out, dim(Ai)});
vars = cand-fan-out + dim(Ai);
cand = i
end if
end for
Figure 9
Optimized version of the binarization algorithm, determining binarization order.
96
Kallmeyer and Maier PLCFRS Parsing
During parsing we have to link the terminals and variables in our LCFRS rules
to portions of the input string. For this purpose we need the notions of ranges, range
vectors, and rule instantiations. A range is a pair of indices that characterizes the span
of a component within the input. A range vector characterizes a tuple in the yield of a
non-terminal. A rule instantiation specifies the computation of an element from the left-
hand side yield from elements in the yields of the right-hand side non-terminals based
on the corresponding range vectors.
Definition 6 (Range)
Let w ? T? with w = w1 . . .wn where wi ? T for 1 ? i ? n.
1. Pos(w) := {0, . . . , n}.
2. We call a pair ?l, r? ? Pos(w) ? Pos(w) with l ? r a range in w. Its yield
?l, r?(w) is the substring wl+1 . . .wr.
3. For two ranges ?1 = ?l1, r1?,?2 = ?l2, r2?, if r1 = l2, then the concatenation
of ?1 and ?2 is ?1 ? ?2 = ?l1, r2?; otherwise ?1 ? ?2 is undefined.
4. A ? ? (Pos(w) ? Pos(w))k is a k-dimensional range vector for w iff
? = ??l1, r1?, . . . , ?lk, rk?? where ?li, ri? is a range in w for 1 ? i ? k.
We now define instantiations of rules with respect to a given input string. This
definition follows the definition of clause instantiations from Boullier (2000). An in-
stantiated rule is a rule in which variables are consistently replaced by ranges. Because
we need this definition only for parsing our specific grammars, we restrict ourselves to
?-free rules containing only variables.
Definition 7 (Rule instantiation)
Let G = (N, T, V, P, S) be an ?-free monotone LCFRS. For a given rule r = A(?) ?
A1(x1) ? ? ?Am( xm) ? P (0 < m) that does not contain any terminals,
1. an instantiation with respect to a string w = t1 . . . tn consists of a function
f : V ? {?i, j? | 1 ? i ? j ? |w|} such that for all x, y adjacent in one of the
elements of ?, f (x) ? f (y) must be defined; we then define f (xy) = f (x) ? f (y),
2. if f is an instantiation of r, then A( f (?)) ? A1( f (x1)) ? ? ?Am( f ( xm)) is an
instantiated rule where f (?x1, . . . , xk?) = ? f (x1), . . . , f (xk)?.
We use a probabilistic version of the CYK parser from Seki et al (1991). The algo-
rithm is formulated using the framework of parsing as deduction (Pereira and Warren
1983; Shieber, Schabes, and Pereira 1995; Sikkel 1997), extended with weights (Nederhof
2003). In this framework, a set of weighted items representing partial parsing results is
characterized via a set of deduction rules, and certain items (the goal items) represent
successful parses.
During parsing, we have to match components in the rules we use with portions of
the input string. For a given input w, our items have the form [A, ?] where A ? N and ?
is a range vector that characterizes the span of A. Each item has a weight in that encodes
the Viterbi inside score of its best parse tree. More precisely, we use the log probability
log(p) where p is the probability.
The first rule (scan) tells us that the POS tags that we receive as inputs are given.
Consequently, they are axioms; their probability is 1 and their weight therefore 0. The
97
Computational Linguistics Volume 39, Number 1
Scan: 0 : [A, ??i, i + 1??] A is the POS tag of wi+1
Unary:
in : [B, ?]
in + log(p) : [A, ?] p : A(?) ? B(?) ? P
Binary:
inB : [B, ?B], inC : [C, ?C]
inB + inC + log(p) : [A, ?A]
p : A( ?A ) ? B( ?B )C( ?C )
is an instantiated rule
Goal: [S, ??0, n??]
Figure 10
Weighted CYK deduction system.
second rule, unary, is applied whenever we have found the right-hand side of an
instantiation of a unary rule. In our grammar, terminals only occur in rules with POS
tags and the grammar is ordered and ?-free. Therefore, the components of the yield of
the right-hand side non-terminal and of the left-hand side terminals are the same. The
rule binary applies an instantiated rule of rank 2. If we already have the two elements
of the right-hand side, we can infer the left-hand side element. In both cases, unary
and binary, the probability p of the new rule is multiplied with the probabilities of the
antecedent items (which amounts to summing up the antecedent weights and log(p)).
We perform weighted deductive parsing, based on the deduction system from
Figure 10. We use a chart C and an agenda A, both initially empty, and we proceed
as in Figure 11. Because for all our deduction rules, the weight functions f that compute
the weight of a consequent item from the weights of the antecedent items are monotone
non-increasing in each variable, the algorithm will always find the best parse without
the need of exhaustive parsing. All new items that we deduce involve at least one of
the agenda items as an antecedent item. Therefore, whenever an item is the best in the
agenda, we can be sure that we will never find an item with a better (i.e., higher) weight.
Consequently, we can safely store this item in the chart and, if it is a goal item, we have
found the best parse.
As an example consider the development of the agenda and the chart in Figure 12
when parsing aa with the PLCFRS from Figure 5, transformed into a PLCFRS with
pre-terminals and binarization (i.e., with a POS tag Ta and a new binarization non-
terminal B?). The new PLCFRS is given in Figure 13.
In this example, we find a first analysis for the input (a goal item) when combining
an A with span ??0, 2?? into an S. This S has however a rather low probability and is
therefore not on top of the agenda. Later, when finding the better analysis, the weight
add SCAN results to A
while A = ?
remove best item x : I from A
add x : I to C
if I goal item
then stop and output true
else
for all y : I? deduced from x : I and items in C:
if there is no z with z : I? ? C ? A
then add y : I? to A
else if z : I? ? A for some z
then update weight of I? in A to max(y, z)
Figure 11
Weighted deductive parsing.
98
Kallmeyer and Maier PLCFRS Parsing
chart agenda
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?]
0 : [Ta, ?0, 1?] 0 : [Ta, ?1, 2?],?0.5 : [A, ?0, 1?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?] ?0.5 : [A, ?0, 1?],?0.5 : [A, ?1, 2?],
?0.7 : [B, ?0, 1?, ?1, 2?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?], ?0.5 : [A, ?1, 2?],?0.7 : [B, ?0, 1?, ?1, 2?],
?0.5 : [A, ?0, 1?] ?1.2 : [S, ?0, 1?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?], ?0.65 : [A, ?0, 2?],?0.7 : [B, ?0, 1?, ?1, 2?],
0.5 : [A, ?0, 1?],?0.5 : [A, ?1, 2?] ?1.2 : [S, ?0, 1?],?1.2 : [S, ?1, 2?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?], ?0.7 : [B, ?0, 1?, ?1, 2?],?1.2 : [S, ?0, 1?],
?0.5 : [A, ?0, 1?],?0.5 : [A, ?1, 2?], ?1.2 : [S, ?1, 2?],?1.35 : [S, ?0, 2?]
?0.65 : [A, ?0, 2?]
0 : [Ta, ?0, 1?], 0 : [Ta, ?1, 2?], ?0.8 : [S, ?0, 2?],?1.2 : [S, ?0, 1?],
?0.5 : [A, ?0, 1?],?0.5 : [A, ?1, 2?], ?1.2 : [S, ?1, 2?]
?0.65 : [A, ?0, 2?],?0.7 : [B, ?0, 1?, ?1, 2?]
Figure 12
Parsing of aa with the grammar from Figure 5.
PLCFRS with non-terminals {S, A, B, B?, Ta}, terminals {a} and start symbol S:
0.2 : S(X) ? A(X) 0.8 : S(XY) ? B(X, Y)
0.7 : A(XY) ? Ta(X)A(Y) 0.3 : A(X) ? Ta(X)
0.8 : B(ZX, Y) ? Ta(Z)B?(X, Y) 1 : B?(X, UY) ? B(X, Y)Ta(U)
0.2 : B(X, Y) ? Ta(X)Ta(Y) 1 : Ta(a) ? ?
Figure 13
Sample binarized PLCFRS (with pre-terminal Ta).
of the S item in the agenda is updated and then the goal item is the top agenda item and
therefore parsing has been successful.
Note that, so far, we have only presented the recognizer. In order to extend it to a
parser, we do the following: Whenever we generate a new item, we store it not only with
its weight but also with backpointers to its antecedent items. Furthermore, whenever
we update the weight of an item in the agenda, we also update the backpointers. In
order to read off the best parse tree, we have to start from the goal item and follow the
backpointers.
4. Outside Estimates
So far, the weights we use give us only the Viterbi inside score of an item. In order
to speed up parsing, we add the estimate of the costs for completing the item into a
goal item to its weight?that is, to the weight of each item in the agenda, we add an
estimate of its Viterbi outside score2 (i.e., the logarithm of the estimate). We use context
summary estimates. A context summary is an equivalence class of items for which we
can compute the actual outside scores. Those scores are then used as estimates. The
challenge is to choose the estimate general enough to be efficiently computable and
specific enough to be helpful for discriminating items in the agenda.
2 Note that just as Klein and Manning (2003a), we use the terms inside score and outside score to
denote the Viterbi inside and outside scores. They are not to be confused with the actual inside or
outside probability.
99
Computational Linguistics Volume 39, Number 1
Admissibility and monotonicity are two important conditions on estimates. All
our outside estimates are admissible (Klein and Manning 2003a), which means that
they never underestimate the actual outside score of an item. In other words, they
are too optimistic about the costs of completing the item into an S item spanning the
entire input. For the full SX estimate described in Section 4.1 and the SX estimate with
span and sentence length in Section 4.4, the monotonicity is guaranteed and we can do
true A? parsing as described by Klein and Manning. Monotonicity means that for each
antecedent item of a rule it holds that its weight is greater than or equal to the weight
of the consequent item. The estimates from Sections 4.2 and 4.3 are not monotonic. This
means that it can happen that we deduce an item I2 from an item I1 where the weight of
I2 is greater than the weight of I1. The parser can therefore end up in a local maximum
that is not the global maximum we are searching for. In other words, those estimates are
only figures of merit (FOM).
All outside estimates are computed off-line for a certain maximal sentence length
lenmax.
4.1 Full SX Estimate
The full SX estimate is a PLCFRS adaption of the SX estimate of Klein and Manning
(2003a) (hence the name). For a given sentence length n, the estimate gives the maximal
probability of completing a category X with a span ? into an S with span ??0, n??.
For its computation, we need an estimate of the inside score of a category C with a
span ?, regardless of the actual terminals in our input. This inside estimate is computed
as shown in Figure 14. Here, we do not need to consider the number of terminals outside
the span of C (to the left or right or in the gaps), because they are not relevant for the
inside score. Therefore the items have the form [A, ?l1, . . . , ldim(A)?], where A is a non-
terminal and li gives the length of its ith component. It holds that
?1?i?dim(A)li ? lenmax ? dim(A) + 1
because our grammar extraction algorithm ensures that the different components in
the yield of a non-terminal are never adjacent. There is always at least one terminal in
between two different components that does not belong to the yield of the non-terminal.
The first rule in Figure 14 tells us that POS tags always have a single component
of length 1; therefore this case has probability 1 (weight 0). The rules unary and binary
are roughly like the ones in the CYK parser, except that they combine items with length
information. The rule unary for instance tells us that if the log of the probability of
building [B,l] is greater or equal to in and if there is a rule that allows to deduce an
POS tags: 0 : [A, ?1?] A a POS tag Unary:
in : [B,l]
in + log(p) : [A,l]
p : A(?) ? B(?) ? P
Binary:
inB : [B,lB], inC : [C,lC]
inB + inC + log(p) : [A,lA]
where p : A( ?A) ? B( ?B)C( ?C) ? P and the following holds: we define B(i) as
{1 ? j ? dim(B) | ?B( j) occurs in ?A(i)} and C(i) as {1 ? j ? dim(C) | ?C( j) occurs in ?A(i)}.
Then for all i, 1 ? i ? dim(A):lA(i) = ?j?B(i)lB( j) +?j?C(i)lC( j).
Figure 14
Estimate of the Viterbi inside score.
100
Kallmeyer and Maier PLCFRS Parsing
Axiom : 0 : [S, ?0, len, 0?] 1 ? len ? lenmax Unary:
out : [A,l]
out + log(p) : [B,l]
p : A(?) ? B(?) ? P
Binary-right:
out : [X,lX]
out + in(A,l?A) + log(p) : [B,lB]
Binary-left:
out : [X,lX]
out + in(B,l?B) + log(p) : [A,lA]
where, for both binary rules, there is an instantiated rule p : X(?) ? A( ?A)B( ?B) such that
lX = lout(?),lA = lout(?A),l?A = lin(?A),lB = lout(?B),l
?
B = lin(?B).
Figure 15
Full SX estimate first version (top?down).
A item from [B,l] with probability p, then the log of the probability of [A,l] is greater
or equal to in + log(p). For each item, we record its maximal weight (i.e., its maximal
probability). The rule binary is slightly more complicated because we have to compute
the length vector of the left-hand side of the rule from the right-hand side length vectors.
A straightforward extension of the CFG algorithm from Klein and Manning (2003a)
for computing the SX estimate is given in Figure 15. Here, the items have the form [A,l]
where the vectorl tells us about the lengths of the string to the left of the first component,
the first component, the string in between the first and second component, and so on.
The algorithm proceeds top?down. The outside estimate of completing an S with
component length len and no terminals to the left or to the right of the S component
(item [S, ?0, len, 0?]) is 0. If we expand with a unary rule (unary), then the outside
estimate of the right-hand side item is greater or equal to the outside estimate of the
left-hand side item plus the log of the probability of the rule. In the case of binary rules,
we have to further add the inside estimate of the other daughter. For this, we need a
different length vector (without the lengths of the parts in between the components).
Therefore, for a given range vector ? = ??l1, r1?, . . . , ?lk, rk?? and a sentence length n,
we distinguish between the inside length vector lin(?) = ?r1 ? l1, . . . , rk ? lk? and the
outside length vector lout(?) = ?l1, r1 ? l1, l2 ? r1, . . . , lk ? rk?1, rk ? lk, n ? rk?.
This algorithm has two major problems: Because it proceeds top?down, in the
binary rules we must compute all splits of the antecedent X span into the spans of
A and B, which is very expensive. Furthermore, for a category A with a certain number
of terminals in the components and the gaps, we compute the lower part of the outside
estimate several times, namely, for every combination of number of terminals to the left
and to the right (first and last element in the outside length vector). In order to avoid
these problems, we now abstract away from the lengths of the part to the left and the
right, modifying our items such as to allow a bottom?up strategy.
The idea is to compute the weights of items representing the derivations from a
certain lower C up to some A (C is a kind of ?gap? in the yield of A) while summing up
the inside costs of off-spine nodes and the log of the probabilities of the corresponding
rules. We use items [A, C,?A,?C, shift] where A, C ? N and ?A,?C are range vectors, both
with a first component starting at position 0. The integer shift ? lenmax tells us how many
positions to the right the C span is shifted, compared to the starting position of the A.
?A and ?C represent the spans of C and A while disregarding the number of terminals
to the left and the right (i.e., only the lengths of the components and of the gaps are
encoded). This means in particular that the length n of the sentence does not play a
role here. The right boundary of the last range in the vectors is limited to lenmax. For
101
Computational Linguistics Volume 39, Number 1
any i, 0 ? i ? lenmax, and any range vector ?, we define shift(?, i) as the range vector one
obtains from adding i to all range boundaries in ? and shift(?,?i) as the range vector
one obtains from subtracting i from all boundaries in ?.
The weight of [A, C,?A,?C, i] estimates the log of the probability of completing a
C tree with yield ?C into an A tree with yield ?A such that, if the span of A starts at
position j, the span of C starts at position i + j. Figure 16 gives the computation. The
value of in(A,l) is the inside estimate of [A,l].
The SX-estimate for some predicate C with span ? where i is the left boundary of the
first component of ? and with sentence length n is then given by the maximal weight of
[S, C, ?0, n?, shift(?,?i), i].
4.2 SX with Left, Gaps, Right, Length
A problem of the previous estimate is that with a large number of non-terminals (for
treebank parsing, approximately 12,000 after binarization and markovization), the com-
putation of the estimate requires too much space. We therefore turn to simpler estimates
with only a single non-terminal per item. We now estimate the outside score of a non-
terminal A with a span of a length length (the sum of the lengths of all the components
of the span), with left terminals to the left of the first component, right terminals to the
right of the last component, and gaps terminals in between the components of the A
span (i.e., filling the gaps). Our items have the form [X, len, left, right, gaps] with X ? N,
len + left + right + gaps ? lenmax, len ? dim(X), gaps ? dim(X) ? 1.
Let us assume that, in the rule X(?) ? A( ?A)B( ?B), when looking at the vector ?,
we have leftA variables for A-components preceding the first variable of a B component,
rightA variables for A-components following the last variable of a B component, and
rightB variables for B-components following the last variable of an A component. (In our
grammars, the first left-hand side argument always starts with the first variable from A.)
Furthermore, we set gapsA = dim(A) ? leftA ? rightA and gapsB = dim(B) ? rightB.
Figure 17 gives the computation of the estimate. It proceeds top?down, as the
computation of the full SX estimate in Figure 15, except that now the items are simpler.
POS tags: 0 : [C, C, ?0, 1?, ?0, 1?, 0] C a POS tag
Unary:
0 : [B, B,?B,?B, 0]
log(p) : [A, B,?B,?B, 0]
p : A(?) ? B(?) ? P
Binary-right:
0 : [A, A,?A,?A, 0], 0 : [B, B,?B,?B, 0]
in(A, lin(?A)) + log(p) : [X, B,?X,?B, i]
Binary-left:
0 : [A, A,?A,?A, 0], 0 : [B, B,?B,?B, 0]
in(B, lin(?B)) + log(p) : [X, A,?X,?A, i]
where i is such that for shift(?B, i) = ??B p : X(?X ) ? A(?A)B(??B) is an instantiated rule.
Starting sub-trees with larger gaps:
out : [B, C,?B,?C, i]
0 : [B, B,?B,?B, 0]
Transitive closure of sub-tree combination:
out1 : [A, B,?A,?B, i], out2 : [B, C,?B,?C, j]
out1 + out2 : [A, C,?A,?C, i + j]
Figure 16
Full SX estimate second version (bottom?up).
102
Kallmeyer and Maier PLCFRS Parsing
Axiom : 0 : [S, len, 0, 0, 0] 1 ? len ? lenmax
Unary:
out : [X, len, l, r, g]
out + log(p) : [A, len, l, r, g] p : X(?) ? A(?) ? P
Binary-right:
out : [X, len, l, r, g]
out + in(A, len ? lenB) + log(p) : [B, lenB, lB, rB, gB]
Binary-left:
out : [X, len, l, r, g]
out + in(B, len ? lenA) + log(p) : [A, lenA, lA, rA, gA]
where, for both binary rules, p : X(?) ? A( ?A)B( ?B) ? P.
Further side conditions for Binary-right:
a) len + l + r + g = lenB + lB + rB + gB, b) lB ? l + leftA,
c) if rightA > 0, then rB ? r + rightA, else (rightA = 0), rB = r, d) gB ? gapsA.
Further side conditions for Binary-left:
a) len + l + r + g = lenA + lA + rA + gA, b) lA = l,
c) if rightB > 0, then rA ? r + rightB, else (rightB = 0), rA = r d) gA ? gapsB.
Figure 17
SX estimate depending on length, left, right, gaps.
The value in(X, l) for a non-terminal X and a length l, 0 ? l ? lenmax is an estimate
of the probability of an X category with a span of length l. Its computation is specified
in Figure 18.
The SX-estimate for a sentence length n and for some predicate C with a range
characterized by ? = ??l1, r1?, . . . , ?ldim(C), rdim(C)?? where len = ?dim(C)i=1 (ri ? li) and r =
n ? rdim(C) is then given by the maximal weight of the item [C, len, l1, r, n ? len ? l1 ? r].
4.3 SX with LR, Gaps, Length
In order to further decrease the space complexity of the computation of the outside
estimate, we can simplify the previous estimate by subsuming the two lengths left and
right in a single length lr. The items now have the form [X, len, lr, gaps] with X ? N,
len + lr + gaps ? lenmax, len ? dim(X), gaps ? dim(X) ? 1.
The computation is given in Figure 19. Again, we define leftA, gapsA, rightA and
gapsB, rightB for a rule X(?) ? A( ?A)B( ?B) as before. Furthermore, in both Binary-left
and Binary-right, we have limited lr in the consequent item to the lr of the antecedent
plus the length of the sister (lenB, resp. lenA). This results in a further reduction of the
number of items while having only little effect on the parsing results.
The SX-estimate for a sentence length n and for some predicate C with a span
? = ??l1, r1?, . . . , ?ldim(C), rdim(C)?? where len = ?dim(C)i=1 (ri ? li) and r = n ? rdim(C) is then the
maximal weight of [C, len, l1 + r, n ? len ? l1 ? r].
POS tags: 0 : [A, 1] A a POS tag Unary:
in : [B, l]
in + log(p) : [A, l] p : A(?) ? B(?) ? P
Binary:
inB : [B, lB], inC : [C, lC]
inB + inC + log(p) : [A, lB + lC]
where either p : A( ?A) ? B( ?B)C( ?C) ? P or p : A( ?A) ? C( ?C)B( ?B) ? P.
Figure 18
Estimate of the inside score with total span length.
103
Computational Linguistics Volume 39, Number 1
Axiom : 0 : [S, len, 0, 0] 1 ? len ? lenmax
Unary:
out : [X, len, lr, g]
out + log(p) : [A, len, lr, g] p : X(?) ? A(?) ? P
Binary-right:
out : [X, len, lr, g]
out + in(A, len ? lenB) + log(p) : [B, lenB, lrB, gB] p : X(?) ? A( ?A )B( ?B ) ? P
Binary-left:
out : [X, len, lr, g]
out + in(B, len ? lenA) + log(p) : [A, lenA, lrA, gA] p : X(?) ? A( ?A )B( ?B ) ? P
Further side conditions for Binary-right:
a) len + lr + g = lenB + lrB + gB b) lr < lrB c) gB ? gapsA
Further side conditions for Binary-left:
a) len + lr + g = lenA + lrA + gA b) if rightB = 0 then lr = lrA, else lr < lrA c) gA ? gapsB
Figure 19
SX estimate depending on length, LR, gaps.
4.4 SX with Span and Sentence Length
We will now present a further simplification of the last estimate that records only the
span length and the length of the entire sentence. The items have the form [X, len, slen]
with X ? N, dim(X) ? len ? slen. The computation is given in Figure 20. This last esti-
mate is actually monotonic and allows for true A? parsing.
The SX-estimate for a sentence length n and for some predicate C with a span
? = ??l1, r1?, . . . , ?ldim(C), rdim(C)?? where len = ?dim(C)i=1 (ri ? li) is then the maximal weight
of [C, len, n].
In order to prove that this estimate allows for monotonic weighted deductive pars-
ing and therefore guarantees that the best parse will be found, let us have a look at the
CYK deduction rules when being augmented with the estimate. Only Unary and Binary
are relevant because Scan does not have antecedent items. The two rules, augmented
with the outside estimate, are shown in Figure 21.
We have to show that for every rule, if this rule has an antecedent item with weight
w and a consequent item with weight w?, then w ? w?.
Let us start with Unary. To show: inB + outB ? inB + log(p) + outA. Because of the
Unary rule for computing the outside estimate and because of the unary production,
Axiom : 0 : [S, len, len] 1 ? len ? lenmax
Unary:
out : [X, lX, slen]
out + log(p) : [A, lX, slen]
p : X(?) ? A(?) ? P
Binary-right:
out : [X, lX, slen]
out + in(A, lX ? lB) + log(p) : [B, lB, slen] p : X(?) ? A( ?A )B( ?B ) ? P
Binary-left:
out : [X, lX, slen]
out + in(B, lX ? lA) + log(p) : [A, lA, slen] p : X(?) ? A( ?A )B( ?B ) ? P
Figure 20
SX estimate depending on span and sentence length.
104
Kallmeyer and Maier PLCFRS Parsing
Unary:
inB + outB : [B, ?]
inB + log(p) + outA : [A, ?]
p : A(?) ? B(?) ? P
Binary:
inB + outB : [B, ?B], inC + outC : [C, ?C]
inB + inC + log(p) + outA : [A, ?A]
p : A( ?A ) ? B( ?B )C( ?C )
is an instantiated rule
(Here, outA, outB, and outC are the respective outside estimates of [A, ?A], [B, ?B] and [C, ?C].)
Figure 21
Parsing rules including outside estimate.
we obtain that, given the outside estimate outA of [A, ?] the outside estimate outB of the
item [B, ?] is at least outA + log(p), namely, outB ? log(p) + outA.
Now let us consider the rule Binary. We treat only the relation between the weight
of the C antecedent item and the consequent. The treatment of the antecedent B is
symmetric. To show: inC + outC ? inB + inC + log(p) + outA. Assume that lB is the length
of the components of the B item and n is the sentence length. Then, because of the
Binary-right rule in the computation of the outside estimate and because of our in-
stantiated rule p : A( ?A) ? B( ?B)C( ?C), we have that the outside estimate outC of the
C-item is at least outA + in(B, lB) + log(p). Furthermore, in(B, lB) ? inB. Consequently
outC ? inB + log(p) + outA.
4.5 Integration into the Parser
Before parsing, the outside estimates of all items up to a certain maximal sentence length
lenmax are precomputed. Then, when performing the weighted deductive parsing as
explained in Section 3.2, whenever a new item is stored in the agenda, we add its outside
estimate to its weight.
Because the outside estimate is always greater than or equal to the actual outside
score, given the input, the weight of an item in the agenda is always greater than or
equal to the log of the actual product of the inside and outside score of the item. In this
sense, the outside estimates given earlier are admissible.
Additionally, as already mentioned, note that the full SX estimate and the SX esti-
mate with span and sentence length are monotonic and allow for A? parsing. The other
two estimates, which are both not monotonic, act as FOMs in a best-first parsing context.
Consequently, they contribute to speeding up parsing but they decrease the quality of
the parsing output. For further evaluation details see Section 6.
5. Grammars for Discontinuous Constituents
5.1 Grammar Extraction
The algorithm we use for extracting an LCFRS from a constituency treebank with cross-
ing branches has originally been presented in Maier and S?gaard (2008). It interprets
the treebank trees as LCFRS derivation trees. Consider for instance the tree in Figure 22.
The S node has two daughters, a VMFIN node and a VP node. This yields a rule
S ? VP VMFIN. The VP is discontinuous with two components that wrap around the
yield of the VMFIN. Consequently, the LCFRS rule is S(XYZ) ? VP(X, Z) VMFIN(Y).
The extraction of an LCFRS from treebanks with crossing branches is almost im-
mediate, except for the fan-out of the non-terminal categories: In the treebank, we can
have the same non-terminal with different fan-outs, for instance a VP without a gap
(fan-out 1), a VP with a single gap (fan-out 2), and so on. In the corresponding LCFRS,
105
Computational Linguistics Volume 39, Number 1
S
VP
VP
PROAV VMFIN VVPP VAINF
daru?ber mu? nachgedacht werden
about it must thought be
?It must be thought about it?
Figure 22
A sample tree from NeGra.
we have to distinguish these different non-terminals by mapping them to different
predicates.
The algorithm first creates a so-called lexical clause P(a) ? ? for each pre-terminal
P dominating some terminal a. Then for all other non-terminals A0 with the children
A1 ? ? ?Am, a clause A0 ? A1 ? ? ?Am is created. The number of components of the A1 ? ? ?Am
is the number of discontinuous parts in their yields. The components of A0 are concate-
nations of variables that describe how the discontinuous parts of the yield of A0 are
obtained from the yields of its daughters.
More precisely, the non-terminals in our LCFRS are all Ak where A is a non-terminal
label in the treebank and k is a possible fan-out for A. For a given treebank tree ?V, E, r, l?
where V is the set of nodes, E ? V ? V the set of immediate dominance edges, r ? V
the root node, and l : V ? N ? T the labeling function, the algorithm constructs the
following rules. Let us assume that w1, . . . , wn are the terminal labels of the leaves
in ?V, E, r? with a linear precedence relation wi ? wj for 1 ? i < j ? n. We introduce a
variable Xi for every wi, 1 ? i ? n.
 For every pair of nodes v1, v2 ? V with ?v2, v2? ? E, l(v2) ? T, we add
l(v1)(l(v2)) ? ? to the rules of the grammar. (We omit the fan-out subscript
here because pre-terminals are always of fan-out 1.)
 For every node v ? V with l(v) = A0 /? T such that there are exactly m
nodes v1, . . . , vm ? V (m ? 1) with ?v, vi? ? E and l(vi) = Ai /? T for all
1 ? i ? m, we now create a rule
A0(x
(0)
1 , . . . , x
(0)
dim(A0 )
)
? A1(x(1)1 , . . . , x
(1)
dim(A1 )
) . . .Am(x
(m)
1 , . . . , x
(m)
dim(Am )
)
where for the predicate Ai, 0 ? i ? m, the following must hold:
1. The concatenation of all arguments of Ai, x
(i)
1 . . . x
(i)
dim(Ai )
is the
concatenation of all X ? {Xi | ?vi, v?i? ? E? with l(v?i ) = wi} such that
Xi precedes Xj if i < j, and
2. a variable Xj with 1 ? j < n is the right boundary of an argument of
Ai if and only if Xj+1 /? {Xi | ?vi, v?i? ? E? with l(v?i ) = wi}, that is, an
argument boundary is introduced at each discontinuity.
As a further step, in this new rule, all right-hand side arguments of length
> 1 are replaced in both sides of the rule with a single new variable.
Finally, all non-terminals A in the rule are equipped with an additional
subscript dim(A), which gives us the final non-terminal in our LCFRS.
106
Kallmeyer and Maier PLCFRS Parsing
PROAV(Daru?ber) ? ?
VMFIN(mu?) ? ?
VVPP(nachgedacht) ? ?
VAINF(werden) ? ?
S1(X1X2X3) ? VP2(X1, X3)VMFIN(X2)
VP2(X1, X2X3) ? VP2(X1, X2)VAINF(X3)
VP2(X1, X2) ? PROAV(X1)VVPP(X2)
Figure 23
LCFRS rules extracted from the tree in Figure 22.
For the tree in Figure 22, the algorithm produces for instance the rules in Figure 23.
As standard for PCFG, the probabilities are computed using Maximum Likelihood
Estimation.
5.2 Head-Outward Binarization
As previously mentioned, in contrast to CFG the order of the right-hand side elements
of a rule does not matter for the result of an LCFRS derivation. Therefore, we can reorder
the right-hand side of a rule before binarizing it.
The following, treebank-specific reordering results in a head-outward binarization
where the head is the lowest subtree and it is extended by adding first all sisters to its
left and then all sisters to its right. It consists of reordering the right-hand side of the
rules extracted from the treebank such that first, all elements to the right of the head are
listed in reverse order, then all elements to the left of the head in their original order, and
then the head itself. Figure 24 shows the effect this reordering and binarization has on
the form of the syntactic trees. In addition to this, we also use a variant of this reordering
Tree in NeGra format:
S
VP
NN VMFIN NN AV VAINF
das mu? man jetzt machen
that must one now do
?One has to do that now?
Rule extracted for the S node: S(XYZU) ? VP(X, U)VMFIN(Y)NN(Z)
Reordering for head-outward binarization: S(XYZU) ? NN(Z)VP(X, U)VMFIN(Y)
New rules resulting from binarizing this rule:
S(XYZ) ? Sbin1(X, Z)NN(Y) Sbin1(XY, Z) ? VP(X, Z)VMFIN(Y)
Rule extracted for the VP node: VP(X, YZ) ? NN(X)AV(Y)VAINF(Z)
New rules resulting from binarizing this rule:
VP(X, Y) ? NN(X)VPbin1(Y) VPbin1(XY) ? AV(X)VAINF(Y)
Tree after binarization:
S
Sbin1
VP
VPbin1
NN VMFIN NN AV VAINF
Figure 24
Sample head-outward binarization.
107
Computational Linguistics Volume 39, Number 1
where we add first the sisters to the right and then the ones to the left. This is what Klein
and Manning (2003b) do. To mark the heads of phrases, we use the head rules that the
Stanford parser (Klein and Manning 2003c) uses for NeGra.
In all binarizations, there exists the possibility of adding additional unary rules
when deriving the head. This allows for a further factorization. In the experiments,
however, we do not insert unary rules, neither at the highest nor at the lowest new
binarization non-terminal, because this was neither beneficial for parsing times nor for
the parsing results.
5.3 Incorporating Additional Context
5.3.1 Markovization. As already mentioned in Section 3.1, a binarization that introduces
unique new non-terminals for every single rule that needs to be binarized produces
a large amount of non-terminals and fails to capture certain generalizations. For this
reason, we introduce markovization (Collins 1999; Klein and Manning 2003b).
Markovization is achieved by introducing only a single new non-terminal for the
new rules introduced during binarization and adding vertical and horizontal context
from the original trees to each occurrence of this new non-terminal. As vertical context,
we add the first v labels on the path from the root node of the tree that we want to
binarize to the root of the entire treebank tree. The vertical context is collected during
grammar extraction and then taken into account during binarization of the rules. As
horizontal context, during binarization of a rule A(?) ? A0( ?0) . . .Am( ?m), for the new
non-terminal that comprises the right-hand side elements Ai . . .Am (for some 1 ? i ?
m), we add the first h elements of Ai, Ai?1, . . . , A0.
Figure 25 shows an example of a markovization of the tree from Figure 24 with v = 1
and h = 2. Here, the superscript is the vertical context and the subscript the horizontal
context of the new non-terminal X. Note that in this example we have disregarded the
fan-out of the context categories. The VP, for instance, is actually a VP2 because it has
fan-out 2. For the context symbols, one can either use the categories from the original
treebank (without fan-out) or the ones from the LCFRS rules (with fan-out). We chose
the latter approach because it delivered better parsing results.
5.3.2 Further Category Splitting. Grammar annotation (i.e., manual enhancement of an-
notation information through category splitting) has previously been successfully used
in parsing German (Versley 2005). In order to see if such modifications can have a
beneficial effect in PLCFRS parsing as well, we perform different category splits on the
(unbinarized) NeGra constituency data.
We split the category S (?sentence?) into SRC (?relative clause?) and S (all other
categories S). Relative clauses mostly occur in a very specific context, namely, as the
S
XSVP,NN
VP
XVPADV,NN
NN VMFIN NN ADV VAINF
Figure 25
Sample markovization with v = 1, h = 2.
108
Kallmeyer and Maier PLCFRS Parsing
Table 1
NeGra: Properties of the data with crossing branches.
training test
number of sentences 16,502 1,833
average sentence length 14.56 14.62
average tree height 4.62 4.72
average children per node 2.96 2.94
sentences without gaps 12,481 (75.63%) 1,361 (74.25%)
sentences with one gap 3,320 (20.12%) 387 (21.11%)
sentences with ? 2 gaps 701 (4.25%) 85 (4.64%)
maximum gap degree 6 5
right part of an NP or a PP. This splitting should therefore speed up parsing and increase
precision. Furthermore, we distinguish NPs by their case. More precisely, to all nodes
with categories N, we append the grammatical function label to the category label. We
finally experiment with the combination of both splits.
6. Experiments
6.1 Data
Our data source is the NeGra treebank (Skut et al 1997). We create two different data
sets for constituency parsing. For the first one, we start out with the unmodified NeGra
treebank and remove all sentences with a length of more than 30 words. We pre-process
the treebank following common practice (Ku?bler and Penn 2008), attaching all nodes
which are attached to the virtual root node to nodes within the tree such that, ideally,
no new crossing edges are created. In a second pass, we attach punctuation which
comes in pairs (parentheses, quotation marks) to the same nodes. For the second data
set we create a copy of the pre-processed first data set, in which we apply the usual
tree transformations for NeGra PCFG parsing (i.e., moving nodes to higher positions
until all crossing branches are resolved). The first 90% of both data sets are used as the
training set and the remaining 10% as test set. The first data set is called NeGraLCFRS
and the second is called NeGraCFG.
Table 1 lists some properties of the training and test (respectively, gold) parts of
NeGraLCFRS, namely, the total number of sentences, the average sentence length, the
average tree height (the height of a tree being the length of the longest of all paths
from the terminals to the root node), and the average number of children per node
(excluding terminals). Furthermore, gap degrees (i.e., the number of gaps in the spans
of non-terminal nodes) are listed (Maier and Lichte 2011).
Our findings correspond to those of Maier and Lichte except for small differences
due to the fact that, unlike us, they removed the punctuation from the trees.
6.2 Parser Implementation
We have implemented the CYK parser described in the previous section in a system
called rparse. The implementation is realized in Java.3
3 rparse is available under the GNU General Public License 2.0 at http://www.phil.hhu.de/rparse.
109
Computational Linguistics Volume 39, Number 1
Table 2
NeGraLCFRS: PLCFRS parsing results for different binarizations.
Head-driven KM L-to-R Optimal Deterministic
LP 74.00 74.00 75.08 74.92 72.40
LR 74.24 74.13 74.69 74.88 71.80
LF1 74.12 74.07 74.88 74.90 72.10
UP 77.09 77.20 77.95 77.77 75.67
UR 77.34 77.33 77.54 77.73 75.04
UF1 77.22 77.26 77.75 77.75 75.35
6.3 Evaluation
For the evaluation of the constituency parses, we use an EVALB-style metric. For a tree
over a string w, a single constituency is represented by a tuple ?A, ?? with A being a
node label and ? ? (Pos(w) ? Pos(w))dim(A). We compute precision, recall, and F1 based
on these tuples from gold and de-binarized parsed test data from which all category
splits have been removed. This metric is equivalent to the corresponding PCFG metric
for dim(A) = 1. Despite the shortcomings of such a measure (Rehbein and van Genabith
2007), it still allows to some extent a comparison to previous work in PCFG parsing (see
also Section 7). Note that we provide the parser with gold POS tags in all experiments.
6.4 Markovization and Binarization
We use the markovization settings v = 1 and h = 2 for all further experiments. The
setting which has been reported to yield the best results for PCFG parsing of NeGra,
v = 2 and h = 1 (Rafferty and Manning 2008), required a parsing time which was too
high.4
Table 2 contains the parsing results for NeGraLCFRS using five different binariza-
tions: Head-driven and KM are the two head-outward binarizations that use a head
chosen on linguistic grounds (described in Section 5.2); L-to-R is another variant in
which we always choose the rightmost daughter of a node as its head.5 Optimal reorders
the left-hand side such that the fan-out of the binarized rules is optimized (described in
Section 3.1.2). Finally, we also try a deterministic binarization (Deterministic) in which
we binarize strictly from left to right (i.e., we do not reorder the right-hand sides of
productions, and choose unique binarization labels).
The results of the head-driven binarizations and the optimal binarization lie close
together; the results for the deterministic binarization are worse. This indicates that the
presence or absence of markovization has more impact on parsing results than the actual
binarization order. Furthermore, the non-optimal binarizations did not yield a binarized
grammar of a higher fan-out than the optimal binarization: For all five binarizations,
the fan-out was 7 (caused by a VP interrupted by punctuation).
4 Older versions of rparse contained a bug that kept the priority queue from being updated correctly
(i.e., during an update, the corresponding node in the priority queue was not moved to its top, and
therefore the best parse was not guaranteed to be found); however, higher parsing speeds were achieved.
The current version of rparse implements the update operation correctly, using a Fibonacci queue to
ensure efficiency (Cormen et al 2003). Thanks to Andreas van Cranenburgh for pointing this out.
5 The term head is not used in its proper linguistic sense here.
110
Kallmeyer and Maier PLCFRS Parsing
 0
 100
 200
 300
 400
 500
 600
 700
 15  17  19  21  23  25  27  29
ite
m
s 
(in
 10
00
)
sentence length
Head-driven
KM
L-to-R
Optimal
Deterministic
 50
 100
 150
 200
 250
 300
 350
 400
 15  17  19  21  23  25  27  29
ite
m
s 
(in
 10
00
)
sentence length
Baseline
NP
S
NP+S
 50
 100
 150
 200
 250
 300
 350
 400
 15  17  19  21  23  25  27  29
ite
m
s 
(in
 10
00
)
sentence length
OFF
LR
LN
Figure 26
NeGraLCFRS: Items for PLCFRS parsing (left-to-right): binarizations, baseline and category splits,
and estimates.
The different binarizations result in different numbers of items, and therefore allow
for different parsing speeds. The respective leftmost graph in Figures 26 and 27 show
a visual representation of the number of items produced by all binarizations, and the
corresponding parsing times. Note that when choosing the head with head rules the
number of items is almost not affected by the choice of adding first the children to
the left of the head and then to the right of the head or vice versa. The optimal bina-
rization produces the best results. Therefore we will use it in all further experiments,
in spite of its higher parsing time.
6.5 Baseline Evaluation and Category Splits
Table 3 presents the constituency parsing results for NeGraLCFRS and NeGraCFG, both
with and without the different category splits. Recall that NeGraLCFRS has crossing
branches and consequently leads to a PLCFRS of fan-out > 1 whereas NeGraCFG does
not contain crossing branches and consequently leads to a 1-PLCFRS?in other words,
 0.1
 1
 10
 100
 15  17  19  21  23  25  27  29
tim
e 
(se
c.)
sentence length
Head-driven
KM
L-to-R
Optimal
Deterministic
 0.1
 1
 10
 100
 15  17  19  21  23  25  27  29
tim
e 
(se
c.)
sentence length
Baseline
NP
S
NP+S
 0.1
 1
 10
 100
 15  17  19  21  23  25  27  29
tim
e 
(se
c.)
sentence length
OFF
LR
LN
Figure 27
NeGraLCFRS: Parsing times for PLCFRS parsing (left-to-right): binarizations, baseline and
category splits, and estimates (log scale).
111
Computational Linguistics Volume 39, Number 1
Table 3
NeGraLCFRS and NeGraCFG: baseline and category splits.
w/ category splits w/ category splits
NeGraLCFRS NP S NP ? S NeGraCFG NP S NP ? S
LP 74.92 75.21 75.81 75.93 76.32 76.79 77.39 77.58
LR 74.88 74.95 75.65 75.57 76.36 77.23 77.35 77.99
LF1 74.90 75.08 75.73 75.75 76.34 77.01 77.37 77.79
UP 77.77 78.16 78.31 78.60 79.12 79.62 79.84 80.09
UR 78.73 77.88 78.15 78.22 79.17 80.08 79.80 80.52
UF1 77.75 78.02 78.23 78.41 79.14 79.85 79.82 80.30
a PCFG. We evaluate the parser output against the unmodified gold data; that is,
before we evaluate the experiments with category splits, we replace all split labels in
the parser output with the corresponding original labels.
We take a closer look at the properties of the trees in the parser output for
NeGraLCFRS. Twenty-nine sentences had no parse, therefore, the parser output has 1,804
sentences. The average tree height is 4.72, and the average number of children per node
(excluding terminals) is 2.91. These values are almost identical to the values for the gold
data. As for the gap degree, we get 1,401 sentences with no gaps (1,361 in the gold set),
334 with gap degree 1 (387 in the gold set), and 69 with 2 or 3 gaps (85 in the gold set).
Even though the difference is only small, one can see that fewer gaps are preferred. This
is not surprising, since constituents with many gaps are rare events and therefore end
up with a probability which is too low.
We see that the quality of the PLCFRS parser output on NeGraLCFRS (which contains
more information than the output of a PCFG parser) does not lag far behind the quality
of the PCFG parsing results on NeGraCFG. With respect to the category splits, the results
show furthermore that category splitting is indeed beneficial for the quality of the
PLCFRS parser output. The gains in speed are particularly visible for sentences with
a length greater than 20 words (cf. the number of produced items and parsing times in
Figures 26 and 27 [middle]).
6.6 Evaluating Outside Estimates
We compare the parser performance without estimates (OFF) with its performance
with the estimates described in Sections 4.3 (LR) and 4.4 (LN).
Unfortunately, the full estimates seem to be only of theoretical interest because they
were too expensive to compute both in terms of time and space, given the restrictions
imposed by our hardware. We could, however, compute the LN and the LR estimate.
Unlike the LN estimate, which allows for true A? parsing, the LR estimate lets the
quality of the parsing results deteriorate: Compared with the baseline, labeled F1 drops
from 74.90 to 73.76 and unlabeled F1 drops from 77.91 to 76.89. The respective rightmost
graphs in Figures 26 and 27 show the average number of items produced by the
parser and the parsing times for different sentence lengths. The results indicate that the
estimates have the desired effect of preventing unnecessary items from being produced.
This is reflected in a significantly lower parsing time.
The different behavior of the LR and the LN estimate raises the question of the
trade-off between maintaining optimality and obtaining a higher parsing speed. In
112
Kallmeyer and Maier PLCFRS Parsing
other words, it raises the question of whether techniques such as pruning or coarse-
to-fine parsing (Charniak et al 2006) would probably be superior to A? parsing. A first
implementation of a coarse-to-fine approach has been presented by van Cranenburgh
(2012). He generates a CFG from the treebank PLCFRS, based on the idea of Barthe?lemy
et al (2001). This grammar, which can be seen as a coarser version of the actual PLCFRS,
is then used for pruning of the search space. The problem that van Cranenburgh tackles
is specific to PLCFRS: His PCFG stage generalizes over the distinction of labels by their
fan-out. The merit of his work is an enormous increase in efficiency: Sentences with a
length of up to 40 words can now be parsed in a reasonable time. For a comparison of
the results of van Cranenburgh (2012) with our work, the same version of evaluation
parameters would have to be used. The applicability and effectiveness of other coarse-
to-fine approaches (Charniak et al 2006; Petrov and Klein 2007) on PLCFRS remain to
be seen.
7. Comparison to Other Approaches
Comparing our results with results from the literature is a difficult endeavor, because
PLCFRS parsing of NeGra is an entirely new task that has no direct equivalent in
previous work. In particular, it is a harder task than PCFG parsing. What we can
provide in this section is a comparison of the performance of our parser on NeGraCFG
to the performance of previously presented PCFG parsers on the same data set and
an overview on previous work on parsing which aims at reconstructing crossing
branches.
For the comparison of the performance of our parser on NeGraCFG, we have per-
formed experiments with Helmut Schmid?s LoPar (Schmid 2000) and with the Stanford
Parser (Klein and Manning 2003c) on NeGraCFG.6 For the experiments both parsers
were provided with gold POS tags. Recall that our parser produced labeled precision,
recall, and F1 of 76.32, 76.46, and 76.34, respectively. The plain PCFG provided by LoPar
delivers lower results (LP 72.86, LR 74.43, and LF1 73.63). The Stanford Parser results
(markovization setting v = 2, h = 1 [Rafferty and Manning 2008], otherwise default
parameters) lie in the vicinity of the results of our parser (LP 74.27, LR 76.19, LF1 75.45).
Although the results for LoPar are no surprise, given the similarity of the models
implemented by our parser and the Stanford parser, it remains to be investigated why
the lexicalization component of the Stanford parser does not lead to better results. In
any case the comparison shows that on a data set without crossing branches, our parser
obtains the results one would expect. A further data set to which we can provide a
comparison is the PaGe workshop experimental data (Ku?bler and Penn 2008).7 Table 4
lists the results of some of the papers in Ku?bler and Penn (2008) on TIGER, namely,
for Petrov and Klein (2008) (P&K), who use the Berkeley Parser (Petrov and Klein
2007); Rafferty and Manning (2008) (R&M), who use the Stanford parser (see above);
and Hall and Nivre (2008) (H&N), who use a dependency-based approach (see next
paragraph). The comparison again shows that our system produces good results. Again
the performance gap between the Stanford parser and our parser warrants further
investigation.
6 We have obtained the former parser from http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/
LoPar.html and the latter (Version 2.0.1) from http://nlp.stanford.edu/software/lex-parser.shtml.
7 Thanks to Sandra Ku?bler for providing us with the experimental data.
113
Computational Linguistics Volume 39, Number 1
Table 4
PaGe workshop data.
here P&K R&M H&N
LP 66.93 69.23 58.52 67.06
LR 60.79 70.41 57.63 58.07
LF1 63.71 69.81 58.07 65.18
As for the work that aims to create crossing branches, Plaehn (2004) obtains 73.16
Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit
only on sentences with a length of up to 15 words. On those sentences, we obtain 83.97.
The crucial difference between DPSG rules and LCFRS rules is that the former explicitly
specify the material that can occur in gaps whereas LCFRS does not. Levy (2005), like us,
proposes to use LCFRS but does not provide any evaluation results of his work. Very
recently, Evang and Kallmeyer (2011) followed up on our work. They transform the
Penn Treebank such that the trace nodes and co-indexations are converted into crossing
branches and parse them with the parser presented in this article, obtaining promising
results. Furthermore, van Cranenburgh, Scha, and Sangati (2011) and van Cranenburgh
(2012) have also followed up on our work, introducing an integration of our approach
with Data-Oriented Parsing (DOP). The former article introduces an LCFRS adaption
of Goodman?s PCFG-DOP (Goodman 2003). For their evaluation, the authors use the
same data as we do in Maier (2010), and obtain an improvement of roughly 1.5 points
F-measure. They are also confronted with the same efficiency issues, however, and
encounter a bottleneck in terms of parsing time. In van Cranenburgh (2012), a coarse-
to-fine approach is presented (see Section 6.6). With this approach much faster parsing
is made possible and sentences with a length of up to 40 words can be parsed. The cost
of the speed, however, is that the results lie well below the baseline results for standard
PLCFRS parsing.
A comparison with non-projective dependency parsers (McDonald et al 2005;
Nivre et al 2007) might be interesting as well, given that non-projectivity is the
dependency-counterpart to discontinuity in constituency parsing. A meaningful com-
parison is difficult to do for the following reasons, however. Firstly, dependency parsing
deals with relations between words, whereas in our case words are not considered in
the parsing task. Our grammars take POS tags for a given and construct syntactic trees.
Also, dependency conversion algorithms generally depend on the correct identification
of linguistic head words (Lin 1995). We cannot rely on grammatical function labels, such
as, for example, Boyd and Meurers (2008). Therefore we would have to use heuristics for
the dependency conversion of the parser output. This would introduce additional noise.
Secondly, the resources one obtains from our PLCFRS parser and from dependency
parsers (the probabilistic LCFRS and the trained dependency parser) are quite different
because the former contains non-lexicalized internal phrase structure identifying mean-
ingful syntactic categories such as VP or NP while the latter is only concerned with rela-
tions between lexical items. A comparison would concentrate only on relations between
lexical items and the rich phrase structure provided by a constituency parser would
not be taken into account. To achieve some comparison, one could of course transform
the discontinuous constituency trees into dependency trees with dependencies between
heads and with edge labels that encode enough of the syntactic structure to retrieve
the original constituency tree (Hall and Nivre 2008). The result could then be used for
114
Kallmeyer and Maier PLCFRS Parsing
a dependency evaluation. It is not clear what is to gain by this evaluation because
the head-to-head dependencies one would obtain are not necessarily the predicate-
argument dependencies one would aim at when doing direct dependency parsing
(Rambow 2010).8
8. Conclusion
We have presented the first efficient implementation of a weighted deductive CYK
parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), showing
that LCFRS indeed allows for data-driven parsing while modeling discontinuities in
a straightforward way. To speed up parsing, we have introduced different context-
summary estimates of parse items, some acting as figures-of-merit, others allowing for
A? parsing. We have implemented the parser and we have evaluated it with grammars
extracted from the German NeGra treebank. Our experiments show that data-driven
LCFRS parsing is feasible and yields output of competitive quality.
There are three main directions for future work on this subject.
 On the symbolic side, LCFRS seems to offer more power than necessary.
By removing symbolic expressivity, a lower parsing complexity can be
achieved. One possibility is to disallow the use of so-called ill-nested
LCFRS rules. These are rules where, roughly, the spans of two right-hand
side non-terminals interleave in a cross-serial way. See the parsing
algorithm in Go?mez-Rodr??guez, Kuhlmann, and Satta (2010).
Nevertheless, this seems to be too restrictive for linguistic modeling
(Chen-Main and Joshi 2010; Maier and Lichte 2011). Our goal for future
work is therefore to define reduced forms of ill-nested rules with which we
get a lower parsing complexity.
Another possibility is to reduce the fan-out of the extracted grammar. We
have pursued the question whether the fan-out of the trees in the treebank
can be reduced in a linguistically meaningful way in Maier, Kaeshammer,
and Kallmeyer (2012).
 On the side of the probabilistic model, there are certain independence
assumptions made in our model that are too strong. The main problem in
respect is that, due to the definition of LCFRS, we have to distinguish
between occurrences of the same category with different fan-outs. For
instance, VP1 (no gaps), VP2 (one gap), and so on, are different
non-terminals. Consequently, the way they expand are considered
independent from each other. This is of course not true, however.
Furthermore, some of these non-terminals are rather rare; we therefore
have a sparse data problem here. This leads to the idea to separate the
development of a category (independent from its fan-out) and the fan-out
and position of gaps. We plan to integrate this into our probabilistic model
in future work.
8 A way to overcome this difference in the content of the dependency annotation would be to use
an evaluation along the lines of Tsarfaty, Nivre, and Andersson (2011); this is not available yet for
annotations with crossing branches, however.
115
Computational Linguistics Volume 39, Number 1
 Last, it is clear that a more informative evaluation of the parser output is
still necessary, particularly with respect to its performance at the task of
finding long distance dependencies and with respect to its behavior when
not provided with gold POS tags.
Acknowledgments
We are particularly grateful to Giorgio Satta
for extensive discussions of the details of the
probabilistic treebank model presented in
this paper. Furthermore, we owe a debt to
Kilian Evang who participated in the
implementation of the parser. Thanks to
Andreas van Cranenburgh for helpful
feedback on the parser implementation.
Finally, we are grateful to our three
anonymous reviewers for many valuable
and helpful comments and suggestions.
A part of the work on this paper was funded
by the German Research Foundation DFG
(Deutsche Forschungsgemeinschaft) in the
form of an Emmy Noether Grant and a
subsequent DFG research project.
References
Barthe?lemy, Franc?ois, Pierre Boullier,
Philippe Deschamp, and E?ric Villemonte
de la Clergerie. 2001. Guided parsing
of range concatenation languages.
In Proceedings of the 39th Annual Meeting
of the Association for Computational
Linguistics, pages 42?49, Toulouse.
Becker, Tilman, Aravind K. Joshi, and
Owen Rambow. 1991. Long-distance
scrambling and tree-adjoining grammars.
In Proceedings of the Fifth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 21?26,
Berlin.
Boullier, Pierre. 1998a. A generalization of
mildly context-sensitive formalisms.
In Proceedings of the Fourth International
Workshop on Tree Adjoining Grammars and
Related Formalisms (TAG+4), pages 17?20,
Philadelphia, PA.
Boullier, Pierre. 1998b. A proposal for a
natural language processing syntactic
backbone. Technical Report 3342, INRIA,
Roquencourt.
Boullier, Pierre. 2000. Range concatenation
grammars. In Proceedings of the Sixth
International Workshop on Parsing
Technologies (IWPT2000), pages 53?64,
Trento.
Boyd, Adriane. 2007. Discontinuity revisited:
An improved conversion to context-free
representations. In the Linguistic
Annotation Workshop at ACL 2007,
pages 41?44, Prague.
Boyd, Adriane and Detmar Meurers.
2008. Revisiting the impact of different
annotation schemes on PCFG parsing:
A grammatical dependency evaluation.
In Proceedings of the Workshop on Parsing
German at ACL 2008, pages 24?32,
Columbus, OH.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank.
In Proceedings of the 1st Workshop on
Treebanks and Linguistic Theories,
pages 24?42, Sozopol.
Bunt, Harry. 1996. Formal tools for
describing and processing discontinuous
constituency structure. In Harry Bunt and
Arthur van Horck, editors, Discontinuous
Constituency, volume 6 of Natural Language
Processing. Mouton de Gruyter, Berlin,
pages 63?83.
Cai, Shu, David Chiang, and Yoav Goldberg.
2011. Language-independent parsing
with empty elements. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 212?216, Portland, OR.
Candito, Marie and Djame? Seddah. 2010.
Parsing word clusters. In Proceedings of the
First Workshop on Statistical Parsing of
Morphologically-Rich Languages at NAACL
HLT 2010, pages 76?84, Los Angeles, CA.
Caraballo, Sharon A. and Eugene Charniak.
1998. New figures of merit for best-first
probabilistic chart parsing. Computational
Linguistics, 24(2):275?298.
Charniak, Eugene, Mark Johnson, Micha
Elsner, Joseph Austerweil, David Ellis,
Isaac Haxton, Catherine Hill, R. Shrivaths,
Jeremy Moore, Michael Pozar, and Theresa
Vu. 2006. Multilevel coarse-to-fine PCFG
parsing. In Proceedings of the Human
Language Technology Conference of the
NAACL, Main Conference, pages 168?175,
New York, NY.
Chen-Main, Joan and Aravind Joshi. 2010.
Unavoidable ill-nestedness in natural
language and the adequacy of tree
local-MCTAG induced dependency
structures. In Proceedings of the Tenth
116
Kallmeyer and Maier PLCFRS Parsing
International Workshop on Tree Adjoining
Grammar and Related Formalisms (TAG+10),
pages 119?126, New Haven, CT.
Chiang, David. 2003. Statistical parsing with
an automatically extracted tree adjoining
grammar. In Rens Bod, Remko Scha, and
Khalil Sima?an, editors, Data-Oriented
Parsing, CSLI Studies in Computational
Linguistics. CSLI Publications, Stanford,
CA, pages 299?316.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Cormen, Thomas H., Charles E. Leiserson,
Ronald L. Rivest, and Clifford Stein. 2003.
Introduction to Algorithms. MIT Press,
Cambridge, 2nd edition.
Dienes, Pe?ter. 2003. Statistical Parsing with
Non-local Dependencies. Ph.D. thesis,
Saarland University.
Evang, Kilian and Laura Kallmeyer. 2011.
PLCFRS parsing of English discontinuous
constituents. In Proceedings of the 12th
International Conference on Parsing
Technologies, pages 104?116, Dublin.
Go?mez-Rodr??guez, Carlos, Marco
Kuhlmann, and Giorgio Satta. 2010.
Efficient parsing of well-nested linear
context-free rewriting systems. In Human
Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 276?284, Los Angeles, CA.
Go?mez-Rodr??guez, Carlos, Marco
Kuhlmann, Giorgio Satta, and David Weir.
2009. Optimal reduction of rule length in
linear context-free rewriting systems. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 539?547,
Boulder, CO.
Goodman, Joshua. 2003. Efficient parsing of
DOP with PCFG-reductions. In Rens Bod,
Remko Scha, and Khalil Sima?an, editors,
Data-Oriented Parsing, CSLI Studies in
Computational Linguistics. CSLI
Publications, Stanford, CA, pages 125?146.
Hall, Johan and Joakim Nivre. 2008.
A dependency-driven parser for
German dependency and constituency
representations. In Proceedings of the
Workshop on Parsing German at ACL 2008,
pages 47?54, Columbus, OH.
Han, Chung-hye, Na-Rae Han, and
Eon-Suk Ko. 2001. Bracketing guidelines
for Penn Korean TreeBank. Technical
Report 01-10, IRCS, University of
Pennsylvania, Philadelphia, PA.
Hockenmaier, Julia. 2003. Data and models
for Statistical Parsing with Combinatory
Categorial Grammar. Ph.D. thesis,
University of Edinburgh.
Hoffman, Beryl. 1995. Integrating ?free?
word order syntax and information
structure. In Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 245?251,
Dublin.
Ho?hle, Tilman. 1986. Der Begriff
?Mittelfeld??Anmerkungen u?ber die
Theorie der topologischen Felder.
In Akten des Siebten Internationalen
Germanistenkongresses 1985, Go?ttingen,
Germany.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for recovering
empty nodes and their antecedents. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
pages 136?143, Philadelphia, PA.
Kallmeyer, Laura. 2010. Parsing Beyond
Context-Free Grammars. Springer, Berlin.
Kallmeyer, Laura and Wolfgang Maier. 2010.
Data-driven parsing with probabilistic
linear context-free rewriting systems.
In Proceedings of the 23rd International
Conference on Computational Linguistics
(COLING 2010), pages 537?545, Beijing.
Kato, Yuki, Hiroyuki Seki, and Tadao
Kasami. 2006. Stochastic multiple
context-free grammar for RNA
pseudoknot modeling. In Proceedings
of the Eighth International Workshop
on Tree Adjoining Grammar and Related
Formalisms (TAG+8), pages 57?64,
Sydney.
Klein, Dan and Christopher D. Manning.
2003a. A* Parsing: Fast exact viterbi parse
selection. In Proceedings of the 2003 Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 40?47,
Edmonton.
Klein, Dan and Christopher D. Manning.
2003b. Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 423?430, Sapporo.
Klein, Dan and Christopher D. Manning.
2003c. Fast exact inference with a factored
model for natural language parsing. In
Advances in Neural Information Processing
Systems 15 (NIPS), pages 3?10, Vancouver.
Kracht, Marcus. 2003. The Mathematics
of Language. Number 63 in Studies in
Generative Grammar. Mouton de Gruyter,
Berlin.
117
Computational Linguistics Volume 39, Number 1
Ku?bler, Sandra. 2005. How do treebank
annotation schemes influence parsing
results? Or how not to compare apples
and oranges. In Recent Advances in Natural
Language Processing 2005 (RANLP 2005),
pages 293?300, Borovets.
Ku?bler, Sandra and Gerald Penn, editors.
2008. Proceedings of the Workshop on
Parsing German at ACL 2008. Association
for Computational Linguistics,
Columbus, OH.
Kuhlmann, Marco and Giorgio Satta.
2009. Treebank grammar techniques for
non-projective dependency parsing.
In Proceedings of the 12th Conference
of the European Chapter of the Association
for Computational Linguistics,
pages 478?486, Athens.
Levy, Roger. 2005. Probabilistic Models of
Word Order and Syntactic Discontinuity.
Ph.D. thesis, Stanford University.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In Proceedings
of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main
Volume, pages 328?335, Barcelona.
Lin, Dekang. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of the 14th
International Joint Conference on Artificial
Intelligence (IJCAI 95), pages 1420?1427,
Montreal.
Maier, Wolfgang. 2010. Direct parsing of
discontinuous constituents in German.
In Proceedings of the First Workshop on
Statistical Parsing of Morphologically-Rich
Languages at NAACL HLT 2010,
pages 58?66, Los Angeles, CA.
Maier, Wolfgang, Miriam Kaeshammer,
and Laura Kallmeyer. 2012. Data-driven
PLCFRS parsing revisited: Restricting
the fan-out to two. In Proceedings of the
Eleventh International Conference on Tree
Adjoining Grammars and Related Formalisms
(TAG+11), pages 126?134, Paris.
Maier, Wolfgang and Laura Kallmeyer. 2010.
Discontinuity and non-projectivity: Using
mildly context-sensitive formalisms for
data-driven parsing. In Proceedings of the
Tenth International Workshop on Tree
Adjoining Grammars and Related
Formalisms (TAG+10), New Haven, CT.
Maier, Wolfgang and Timm Lichte. 2011.
Characterizing discontinuity in constituent
treebanks. In Formal Grammar. 14th
International Conference, FG 2009. Bordeaux,
France, July 25-26, 2009. Revised Selected
Papers, volume 5591 of Lecture Notes in
Artificial Intelligence, pages 167?182,
Springer-Verlag, Berlin/Heidelberg/
New York.
Maier, Wolfgang and Anders S?gaard. 2008.
Treebanks and mild context-sensitivity.
In Proceedings of the 13th Conference on
Formal Grammar (FG-2008), pages 61?76,
Hamburg.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. In Proceedings of the Human
Language Technology Conference,
pages 114?119.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic?. 2005.
Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 523?530, Vancouver.
Michaelis, Jens. 2001. On Formal Properties
of Minimalist Grammars. Ph.D. thesis,
Universita?t Potsdam.
Mu?ller, Gereon. 2002. Free word order,
morphological case, and sympathy theory.
In Gisbert Fanselow and Caroline Fery,
editors, Resolving Conflicts in Grammars:
Optimality Theory in Syntax, Morphology,
and Phonology. Buske Verlag, Hamburg,
pages 265?397.
Mu?ller, Stefan. 2004. Continuous or
discontinuous constituents? Research on
Language & Computation, 2(2):209?257.
Nederhof, Mark-Jan. 2003. Weighted
deductive parsing and knuth?s algorithm.
Computational Linguistics, 29(1):135?143.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?lsen Eryigit,
Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser:
A language-independent system for
data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Osenova, Petya and Kiril Simov. 2004.
BTB-TR05: BulTreebank Stylebook.
Technical Report 05, BulTreeBank
Project, Sofia, Bulgaria.
Pereira, Fernando C. N. and David Warren.
1983. Parsing as deduction. In Proceedings
of the 21st Annual Meeting of the Association
for Computational Linguistics, pages 137?144,
Cambridge, MA.
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
118
Kallmeyer and Maier PLCFRS Parsing
In Human Language Technologies 2007: The
Conference of the North American Chapter of
the Association for Computational Linguistics;
Proceedings of the Main Conference,
pages 404?411, Rochester, NY.
Petrov, Slav and Dan Klein. 2008. Parsing
German with latent variable grammars.
In Proceedings of the Workshop on Parsing
German at ACL 2008, pages 24?32,
Columbus, OH.
Plaehn, Oliver. 2004. Computing the most
probable parse for a discontinuous
phrase-structure grammar. In Harry Bunt,
John Carroll, and Giorgio Satta, editors,
New Developments in Parsing Technology,
volume 23 of Text, Speech And Language
Technology. Kluwer, Dordrecht,
pages 91?106.
Rafferty, Anna and Christopher D. Manning.
2008. Parsing three German treebanks:
Lexicalized and unlexicalized baselines.
In Proceedings of the Workshop on Parsing
German at ACL 2008, pages 40?46,
Columbus, OH.
Rambow, Owen. 2010. The simple
truth about dependency and phrase
structure representations: An opinion
piece. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 337?340,
Los Angeles, CA.
Rehbein, Ines and Josef van Genabith.
2007. Evaluating evaluation measures.
In Proceedings of the 16th Nordic
Conference of Computational Linguistics,
pages 372?379, Tartu.
Schmid, Helmut. 2000. LoPar: Design and
implementation. Arbeitspapiere des
Sonderforschungsbereiches 340 149,
IMS, University of Stuttgart, Stuttgart,
Germany.
Seddah, Djame, Sandra Ku?bler, and Reut
Tsarfaty, editors. 2010. Proceedings of the
First Workshop on Statistical Parsing of
Morphologically-Rich Languages at
NAACL HLT 2010. Association for
Computational Linguistics,
Los Angeles, CA.
Seki, Hiroyuki, Takahashi Matsumura,
Mamoru Fujii, and Tadao Kasami. 1991.
On multiple context-free grammars.
Theoretical Computer Science, 88(2):191?229.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24(1?2):3?36.
Sikkel, Klaas. 1997. Parsing Schemata. Texts in
Theoretical Computer Science. Springer,
Berlin, Heidelberg, New York.
Skut, Wojciech, Brigitte Krenn, Thorten
Brants, and Hans Uszkoreit. 1997. An
annotation scheme for free word order
languages. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing (ANLP), pages 88?95,
Washington, DC.
Telljohann, Heike, Erhard W. Hinrichs,
Sandra Ku?bler, Heike Zinsmeister, and
Kathrin Beck. 2012. Stylebook for the
Tu?bingen Treebank of Written German
(Tu?Ba-D/Z). Technical report, Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen,
Tu?bingen, Germany. http://www.sfs.
uni.tuebingen.de/resources/tuebadz-
stylebook-1201.pdf.
Tsarfaty, Reut, Joakim Nivre, and Evelina
Andersson. 2011. Evaluating dependency
parsing: Robust and heuristics-free
cross-annotation evaluation. In Proceedings
of the 2011 Conference on Empirical Methods
in Natural Language Processing,
pages 385?396, Edinburgh.
Uszkoreit, Hans. 1986. Linear precedence
in discontinuous constituents: Complex
fronting in German. CSLI report
CSLI-86-47, Center for the Study of
Language and Information, Stanford
University, Stanford, CA.
van Cranenburgh, Andreas. 2012. Efficient
parsing with linear context-free rewriting
systems. In Proceedings of the 13th
Conference of the European Chapter of the
Association for Computational Linguistics,
pages 460?470, Avignon.
van Cranenburgh, Andreas, Remko Scha,
and Federico Sangati. 2011. Discontinuous
data-oriented parsing: A mildly
context-sensitive all-fragments grammar.
In Proceedings of the Second Workshop on
Statistical Parsing of Morphologically Rich
Languages (SPMRL 2011), pages 34?44,
Dublin.
Versley, Yannick. 2005. Parser evaluation
across text types. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic
Theories, pages 209?220, Barcelona, Spain.
Vijay-Shanker, K., David J. Weir, and
Aravind K. Joshi. 1987. Characterizing
structural descriptions produced by
various grammatical formalisms. In
Proceedings of the 25th Annual Meeting of the
Association for Computational Linguistics,
pages 104?111, Stanford, CA.
119

Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 1?8
Manchester, August 2008
TuLiPA: Towards a Multi-Formalism Parsing Environment for
Grammar Engineering
Laura Kallmeyer
SFB 441
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
lk@sfs.uni-tuebingen.de
Yannick Parmentier
CNRS - LORIA
Nancy Universite?
F-54506, Vand?uvre, France
parmenti@loria.fr
Timm Lichte
SFB 441
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
timm.lichte@uni-tuebingen.de
Johannes Dellert
SFB 441 - SfS
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
{jdellert,kevang}@sfs.uni-tuebingen.de
Wolfgang Maier
SFB 441
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
wo.maier@uni-tuebingen.de
Kilian Evang
SFB 441 - SfS
Universita?t Tu?bingen
D-72074, Tu?bingen, Germany
Abstract
In this paper, we present an open-source
parsing environment (Tu?bingen Linguistic
Parsing Architecture, TuLiPA) which uses
Range Concatenation Grammar (RCG)
as a pivot formalism, thus opening the
way to the parsing of several mildly
context-sensitive formalisms. This en-
vironment currently supports tree-based
grammars (namely Tree-Adjoining Gram-
mars (TAG) and Multi-Component Tree-
Adjoining Grammars with Tree Tuples
(TT-MCTAG)) and allows computation not
only of syntactic structures, but also of the
corresponding semantic representations. It
is used for the development of a tree-based
grammar for German.
1 Introduction
Grammars and lexicons represent important lin-
guistic resources for many NLP applications,
among which one may cite dialog systems, auto-
matic summarization or machine translation. De-
veloping such resources is known to be a complex
task that needs useful tools such as parsers and
generators (Erbach, 1992).
Furthermore, there is a lack of a common frame-
work allowing for multi-formalism grammar engi-
neering. Thus, many formalisms have been pro-
posed to model natural language, each coming
with specific implementations. Having a com-
mon framework would facilitate the comparison
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
between formalisms (e.g., in terms of parsing com-
plexity in practice), and would allow for a better
sharing of resources (e.g., having a common lex-
icon, from which different features would be ex-
tracted depending on the target formalism).
In this context, we present a parsing environ-
ment relying on a general architecture that can
be used for parsing with mildly context-sensitive
(MCS) formalisms1 (Joshi, 1987). Its underly-
ing idea is to use Range Concatenation Grammar
(RCG) as a pivot formalism, for RCG has been
shown to strictly include MCS languages while be-
ing parsable in polynomial time (Boullier, 2000).
Currently, this architecture supports tree-based
grammars (Tree-Adjoining Grammars and Multi-
Component Tree-Adjoining Grammars with Tree
Tuples (Lichte, 2007)). More precisely, tree-
based grammars are first converted into equivalent
RCGs, which are then used for parsing. The result
of RCG parsing is finally interpreted to extract a
derivation structure for the input grammar, as well
as to perform additional processings (e.g., seman-
tic calculus, extraction of dependency views).
The paper is structured as follows. In section 2,
we present the architecture of the TuLiPA parsing
environment and show how the use of RCG as a
pivot formalism makes it easier to design a modu-
lar system that can be extended to support several
dimensions (syntax, semantics) and/or formalisms.
In section 3, we give some desiderata for gram-
mar engineering and present TuLiPA?s current state
1A formalism is said to be mildly context sensitive (MCS)
iff (i) it generates limited cross-serial dependencies, (ii) it is
polynomially parsable, and (iii) the string languages gener-
ated by the formalism have the constant growth property (e.g.,
{a
2
n
|n ? 0} does not have this property). Examples of MCS
formalisms include Tree-Adjoining Grammars, Combinatory
Categorial Grammars and Linear Indexed Grammars.
1
with respect to these. In section 4, we compare
this system with existing approaches for parsing
and more generally for grammar engineering. Fi-
nally, in section 5, we conclude by presenting fu-
ture work.
2 Range Concatenation Grammar as a
pivot formalism
The main idea underlying TuLiPA is to use RCG
as a pivot formalism for RCG has appealing for-
mal properties (e.g., a generative capacity lying be-
yond Linear Context Free Rewriting Systems and
a polynomial parsing complexity) and there ex-
ist efficient algorithms, for RCG parsing (Boullier,
2000) and for grammar transformation into RCG
(Boullier, 1998; Boullier, 1999).
Parsing with TuLiPA is thus a 3-step process:
1. The input tree-based grammar is converted
into an RCG (using the algorithm of
Kallmeyer and Parmentier (2008) when deal-
ing with TT-MCTAG).
2. The resulting RCG is used for parsing the in-
put string using an extension of the parsing
algorithm of Boullier (2000).
3. The RCG derivation structure is interpreted to
extract the derivation and derived trees with
respect to the input grammar.
The use of RCG as a pivot formalism, and thus
of an RCG parser as a core component of the sys-
tem, leads to a modular architecture. In turns, this
makes TuLiPA more easily extensible, either in
terms of functionalities, or in terms of formalisms.
2.1 Adding functionalities to the parsing
environment
As an illustration of TuLiPA?s extensibility, one
may consider two extensions applied to the system
recently.
First, a semantic calculus using the syn-
tax/semantics interface for TAG proposed by Gar-
dent and Kallmeyer (2003) has been added. This
interface associates each tree with flat semantic
formulas. The arguments of these formulas are
unification variables, which are co-indexed with
features labelling the nodes of the syntactic tree.
During classical TAG derivation, trees are com-
bined, triggering unifications of the feature struc-
tures labelling nodes. As a result of these unifica-
tions, the arguments of the semantic formulas are
unified (see Fig. 1).
S
NP?x VP
NP
j
V NP?y NP
m
John loves Mary
name(j,john) love(x,y) name(m,mary)
; love(j,m),name(j,john),name(m,mary)
Figure 1: Semantic calculus in Feature-Based
TAG.
In our system, the semantic support has been in-
tegrated by (i) extending the internal tree objects to
include semantic formulas (the RCG-conversion is
kept unchanged), and (ii) extending the construc-
tion of the derived tree (step 3) so that during the
interpretation of the RCG derivation in terms of
tree combinations, the semantic formulas are car-
ried and updated with respect to the feature unifi-
cations performed.
Secondly, let us consider lexical disambigua-
tion. Because of the high redundancy lying within
lexicalized formalisms such as lexicalized TAG,
it is common to consider tree schemata having a
frontier node marked for anchoring (i.e., lexical-
ization). At parsing time, the tree schemata are
anchored according to the input string. This an-
choring selects a subgrammar supposed to cover
the input string. Unfortunately, this subgrammar
may contain many trees that either do not lead to
a parse or for which we know a priori that they
cannot be combined within the same derivation
(so we should not predict a derivation from one
of these trees to another during parsing). As a re-
sult, the parser could have poor performance be-
cause of the many derivation paths that have to
be explored. Bonfante et al (2004) proposed to
polarize the structures of the grammar, and to ap-
ply an automaton-based filtering of the compatible
structures. The idea is the following. One compute
polarities representing the needs/resources brought
by a given tree (or tree tuple for TT-MCTAG).
A substitution or foot node with category NP re-
flects a need for an NP (written NP-). In the same
way, an NP root node reflects a resource of type
NP (written NP+). Then you build an automaton
whose edges correspond to trees, and states to po-
larities brought by trees along the path. The au-
tomaton is then traversed to extract all paths lead-
ing to a final state with a neutral polarity for each
category and +1 for the axiom (see Fig. 2, the state
2
7 is the only valid state and {proper., trans., det.,
noun.} the only compatible set of trees).
0
John
1 1
eats
2 2
a
3 3
cake
4
0 1
NP+
2
S+
3
S+ NP-
4
S+
5
S+ NP-
6
S+ NP+
7
S+
proper.
intrans.
trans.
det.
det.
noun.
noun.
Figure 2: Polarity-based lexical disambiguation.
In our context, this polarity filtering has been
added before step 1, leaving untouched the core
RCG conversion and parsing steps. The idea is
to compute the sets of compatible trees (or tree
tuples for TT-MCTAG) and to convert these sets
separately. Indeed the RCG has to encode only
valid adjunctions/substitutions. Thanks to this
automaton-based ?clustering? of the compatible
tree (or tree tuples), we avoid predicting incompat-
ible derivations. Note that the time saved by using
a polarity-based filter is not negligible, especially
when parsing long sentences.2
2.2 Adding formalisms to the parsing
environment
Of course, the two extensions introduced in the
previous section may have been added to other
modular architectures as well. The main gain
brought by RCG is the possibility to parse not
only tree-based grammars, but other formalisms
provided they can be encoded into RCG. In our
system, only TAG and TT-MCTAG have been
considered so far. Nonetheless, Boullier (1998)
and S?gaard (2007) have defined transformations
into RCG for other mildly context-sensitive for-
malisms.3
To sum up, the idea would be to keep the core
RCG parser, and to extend TuLiPA with a specific
conversion module for each targeted formalism.
On top of these conversion modules, one should
also provide interpretation modules allowing to de-
code the RCG derivation forest in terms of the in-
put formalism (see Fig. 3).
2An evaluation of the gain brought by this technique when
using Interaction Grammar is given by Bonfante et al (2004).
3These include Multi-Component Tree-Adjoining Gram-
mar, Linear Indexed Grammar, Head Grammar, Coupled
Context Free Grammar, Right Linear Unification Grammar
and Synchronous Unification Grammar.
Figure 3: Towards a multi-formalism parsing envi-
ronment.
An important point remains to be discussed. It
concerns the role of lexicalization with respect to
the formalism used. Indeed, the tree-based gram-
mar formalisms currently supported (TAG and TT-
MCTAG) both share the same lexicalization pro-
cess (i.e., tree anchoring). Thus the lexicon format
is common to these formalisms. As we will see
below, it corresponds to a 2-layer lexicon made of
inflected forms and lemma respectively, the latter
selecting specific grammatical structures. When
parsing other formalisms, it is still unclear whether
one can use the same lexicon format, and if not
what kind of general lexicon management module
should be added to the parser (in particular to deal
with morphology).
3 Towards a complete grammar
engineering environment
So far, we have seen how to use a generic parsing
architecture relying on RCG to parse different for-
malisms. In this section, we adopt a broader view
and enumerate some requirements for a linguistic
resource development environment. We also see
to what extent these requirements are fulfilled (or
partially fulfilled) within the TuLiPA system.
3.1 Grammar engineering with TuLiPA
As advocated by Erbach (1992), grammar en-
gineering needs ?tools for testing the grammar
with respect to consistency, coverage, overgener-
ation and accuracy?. These characteristics may
be taken into account by different interacting soft-
ware. Thus, consistency can be checked by a semi-
automatic grammar production device, such as the
XMG system of Duchier et al (2004). Overgen-
eration is mainly checked by a generator (or by
a parser with adequate test suites), and coverage
and accuracy by a parser. In our case, the TuLiPA
system provides an entry point for using a gram-
mar production system (and a lexicon conversion
3
tool introduced below), while including a parser.
Note that TuLiPA does not include any generator,
nonetheless it uses the same lexicon format as the
GenI surface realizer for TAG4.
TuLiPA?s input grammar is designed using
XMG, which is a metagrammar compiler for tree-
based formalisms. In other terms, the linguist de-
fines a factorized description of the grammar (the
so-called metagrammar) in the XMG language.
Briefly, an XMG metagrammar consists of (i) ele-
mentary tree fragments represented as tree descrip-
tion logic formulas, and (ii) conjunctive and dis-
junctive combinations of these tree fragments to
describe actual TAG tree schemata.5 This meta-
grammar is then compiled by the XMG system to
produce a tree grammar in an XML format. Note
that the resulting grammar contains tree schemata
(i.e., unlexicalized trees). To lexicalize these, the
linguist defines a lexicon mapping words with cor-
responding sets of trees. Following XTAG (2001),
this lexicon is a 2-layer lexicon made of morpho-
logical and lemma specifications. The motivation
of this 2-layer format is (i) to express linguistic
generalizations at the lexicon level, and (ii) to al-
low the parser to only select a subgrammar accord-
ing to a given sentence, thus reducing parsing com-
plexity. TuLiPA comes with a lexicon conversion
tool (namely lexConverter) allowing to write a lex-
icon in a user-friendly text format and to convert it
into XML. An example of an entry of such a lexi-
con is given in Fig. 4.
The morphological specification consists of a
word, the corresponding lemma and morphologi-
cal features. The main pieces of information con-
tained in the lemma specification are the ?ENTRY
field, which refers to the lemma, the ?CAT field
referring to the syntactic category of the anchor
node, the ?SEM field containing some semantic in-
formation allowing for semantic instantiation, the
?FAM field, which contains the name of the tree
family to be anchored, the ?FILTERS field which
consists of a feature structure constraining by uni-
fication the trees of a given family that can be
anchored by the given lemma (used for instance
for non-passivable verbs), the ?EQUATIONS field
allowing for the definition of equations targeting
named nodes of the trees, and the ?COANCHORS
field, which allows for the specification of co-
anchors (such as by in the verb to come by).
4http://trac.loria.fr/?geni
5See (Crabbe?, 2005) for a presentation on how to use the
XMG formalism for describing a core TAG for French.
Morphological specification:
vergisst vergessen [pos=v,num=sg,per=3]
Lemma specification:
?ENTRY: vergessen
?CAT: v
?SEM: BinaryRel[pred=vergessen]
?ACC: 1
?FAM: Vnp2
?FILTERS: []
?EX:
?EQUATIONS:
NParg1 ? cas = nom
NParg2 ? cas = acc
?COANCHORS:
Figure 4: Morphological and lemma specification
of vergisst.
From these XML resources, TuLiPA parses a
string, corresponding either to a sentence or a con-
stituent (noun phrase, prepositional phrase, etc.),
and computes several output pieces of informa-
tion, namely (for TAG and TT-MCTAG): deriva-
tion/derived trees, semantic representations (com-
puted from underspecified representations using
the utool software6, or dependency views of the
derivation trees (using the DTool software7).
3.2 Grammar debugging
The engineering process introduced in the preced-
ing section belongs to a development cycle, where
one first designs a grammar and corresponding
lexicons using XMG, then checks these with the
parser, fixes them, parses again, and so on.
To facilitate grammar debugging, TuLiPA in-
cludes both a verbose and a robust mode allow-
ing respectively to (i) produce a log of the RCG-
conversion, RCG-parsing and RCG-derivation in-
terpretation, and (ii) display mismatching features
leading to incomplete derivations. More precisely,
in robust mode, the parser displays derivations step
by step, highlighting feature unification failures.
TuLiPA?s options can be activated via an intu-
itive Graphical User Interface (see Fig. 5).
6See http://www.coli.uni-saarland.de/
projects/chorus/utool/, with courtesy of Alexander
Koller.
7With courtesy of Marco Kuhlmann.
4
Figure 5: TuLiPA?s Graphical User Interface.
3.3 Towards a functional common interface
Unfortunately, as mentioned above, the linguist
has to move back-and-forth from the gram-
mar/lexicon descriptions to the parser, i.e., each
time the parser reports grammar errors, the linguist
fixes these and then recomputes the XML files and
then parses again. To avoid this tedious task of re-
sources re-compilation, we started developing an
Eclipse8 plug-in for the TuLiPA system. Thus, the
linguist will be able to manage all these resources,
and to call the parser, the metagrammar compiler,
and the lexConverter from a common interface (see
Fig. 6).
Figure 6: TuLiPA?s eclipse plug-in.
The motivation for this plug-in comes from
the observation that designing electronic gram-
mars is a task comparable to designing source
8See http://www.eclipse.org
code. A powerful grammar engineering environ-
ment should thus come with development facili-
ties such as precise debugging information, syntax
highlighting, etc. Using the Eclipse open-source
development platform allows for reusing several
components inherited from the software develop-
ment community, such as plug-ins for version con-
trol, editors coupled with explorers, etc.
Eventually, one point worth considering in the
context of grammar development concerns data en-
coding. To our knowledge, only few environments
provide support for UTF-8 encoding, thus guaran-
tying the coverage of a wide set of charsets and
languages. In TuLiPA, we added an UTF-8 sup-
port (in the lexConverter), thus allowing to design
a TAG for Korean (work in progress).
3.4 Usability of the TuLiPA system
As mentioned above, the TuLiPA system is made
of several interacting components, that one cur-
rently has to install separately. Nonetheless, much
attention has been paid to make this installation
process as easy as possible and compatible with
all major platforms.9
XMG and lexConverter can be installed by com-
piling their sources (using a make command).
TuLiPA is developed in Java and released as an ex-
ecutable jar. No compilation is needed for it, the
only requirement is the Gecode/GecodeJ library10
(available as a binary package for many platforms).
Finally, the TuLiPA eclipse plug-in can be installed
easily from eclipse itself. All these tools are re-
leased under Free software licenses (either GNU
GPL or Eclipse Public License).
This environment is being used (i) at the Univer-
sity of Tu?bingen, in the context of the development
of a TT-MCTAG for German describing both syn-
tax and semantics, and (ii) at LORIA Nancy, in the
development of an XTAG-based metagrammar for
English. The German grammar, called GerTT (for
German Tree Tuples), is released under a LGPL li-
cense for Linguistic Resources11 and is presented
in (Kallmeyer et al, 2008). The test-suite cur-
rently used to check the grammar is hand-crafted.
A more systematic evaluation of the grammar is in
preparation, using the Test Suite for Natural Lan-
guage Processing (Lehmann et al, 1996).
9See http://sourcesup.cru.fr/tulipa.
10See http://www.gecode.org/gecodej.
11See http://infolingu.univ-mlv.
fr/DonneesLinguistiques/
Lexiques-Grammaires/lgpllr.html
5
4 Comparison with existing approaches
4.1 Engineering environments for tree-based
grammar formalisms
To our knowledge, there is currently no available
parsing environment for multi-component TAG.
Existing grammar engineering environments for
TAG include the DyALog system12 described in
Villemonte de la Clergerie (2005). DyALog is a
compiler for a logic programming language using
tabulation and dynamic programming techniques.
This compiler has been used to implement efficient
parsing algorithms for several formalisms, includ-
ing TAG and RCG. Unfortunately, it does not in-
clude any built-in GUI and requires a good know-
ledge of the GNU build tools to compile parsers.
This makes it relatively difficult to use. DyALog?s
main quality lies in its efficiency in terms of pars-
ing time and its capacity to handle very large re-
sources. Unlike TuLiPA, it does not compute se-
mantic representations.
The closest approach to TuLiPA corresponds to
the SemTAG system13, which extends TAG parsers
compiled with DyALog with a semantic calculus
module (Gardent and Parmentier, 2007). Unlike
TuLiPA, this system only supports TAG, and does
not provide any graphical output allowing to easily
check the result of parsing.
Note that, for grammar designers mainly inter-
ested in TAG, SemTAG and TuLiPA can be seen
as complementary tools. Indeed, one may use
TuLiPA to develop the grammar and check spe-
cific syntactic structures thanks to its intuitive pars-
ing environment. Once the grammar is stable, one
may use SemTAG in batch processing to parse
corpuses and build semantic representations using
large grammars. This combination of these 2 sys-
tems is made easier by the fact that both use the
same input formats (a metagrammar in the XMG
language and a text-based lexicon). This approach
is the one being adopted for the development of a
French TAG equipped with semantics.
For Interaction Grammar (Perrier, 2000), there
exists an engineering environment gathering the
XMG metagrammar compiler and an eLEtrOstatic
PARser (LEOPAR).14 This environment is be-
ing used to develop an Interaction Grammar for
French. TuLiPA?s lexical disambiguation module
12See http://dyalog.gforge.inria.fr
13See http://trac.loria.fr/?semconst
14See http://www.loria.fr/equipes/
calligramme/leopar/
reuses techniques introduced by LEOPAR. Unlike
TuLiPA, LEOPAR does not currently support se-
mantic information.
4.2 Engineering environments for other
grammar formalisms
For other formalisms, there exist state-of-the-art
grammar engineering environments that have been
used for many years to design large deep grammars
for several languages.
For Lexical Functional Grammar, one may cite
the Xerox Linguistic Environment (XLE).15 For
Head-driven Phrase Structure Grammar, the main
available systems are the Linguistic Knowledge
Base (LKB)16 and the TRALE system.17 For
Combinatory Categorial Grammar, one may cite
the OpenCCG library18 and the C&C parser.19
These environments have been used to develop
broad-coverage resources equipped with semantics
and include both a generator and a parser. Un-
like TuLiPA, they represent advanced projects, that
have been used for dialog and machine translation
applications. They are mainly tailored for a spe-
cific formalism.20
5 Future work
In this section, we give some prospective views
concerning engineering environments in general,
and TuLiPA in particular. We first distinguish be-
tween 2 main usages of grammar engineering en-
vironments, namely a pedagogical usage and an
application-oriented usage, and finally give some
comments about multi-formalism.
5.1 Pedagogical usage
Developing grammars in a pedagogical context
needs facilities allowing for inspection of the struc-
tures of the grammar, step-by-step parsing (or gen-
eration), along with an intuitive interface. The idea
is to abstract away from technical aspects related to
implementation (intermediate data structures, opti-
mizations, etc.).
15See http://www2.parc.com/isl/groups/
nltt/xle/
16See http://wiki.delph-in.net/moin
17See http://milca.sfs.uni-tuebingen.de/
A4/Course/trale/
18See http://openccg.sourceforge.net/
19See http://svn.ask.it.usyd.edu.au/trac/
candc/wiki
20Nonetheless, Beavers (2002) encoded a CCG in the
LKB?s Type Description Language.
6
The question whether to provide graphical or
text-based editors can be discussed. As advo-
cated by Baldridge et al (2007), a low-level text-
based specification can offer more flexibility and
bring less frustration to the grammar designer, es-
pecially when such a specification can be graph-
ically interpreted. This is the approach chosen
by XMG, where the grammar is defined via an
(advanced or not) editor such as gedit or emacs.
Within TuLiPA, we chose to go further by using
the Eclipse platform. Currently, it allows for dis-
playing a summary of the content of a metagram-
mar or lexicon on a side panel, while editing these
on a middle panel. These two panels are linked
via a jump functionality. The next steps concern
(i) the plugging of a graphical viewer to display
the (meta)grammar structures independently from
a given parse, and (ii) the extension of the eclipse
plug-in so that one can easily consistently modify
entries of the metagrammar or lexicon (especially
when these are split over several files).
5.2 Application-oriented usage
When dealing with applications, one may demand
more from the grammar engineering environment,
especially in terms of efficiency and robustness
(support for larger resources, partial parsing, etc.).
Efficiency needs optimizations in the parsing
engine making it possible to support grammars
containing several thousands of structures. One
interesting question concerns the compilation of a
grammar either off-line or on-line. In DyALog?s
approach, the grammar is compiled off-line into
a logical automaton encoding all possible deriva-
tions. This off-line compilation can take some
minutes with a TAG having 6000 trees, but the re-
sulting parser can parse sentences within a second.
In TuLiPA?s approach, the grammar is compiled
into an RCG on-line. While giving satisfactory re-
sults on reduced resources21 , it may lead to trou-
bles when scaling up. This is especially true for
TAG (the TT-MCTAG formalism is by definition a
factorized formalism compared with TAG). In the
future, it would be useful to look for a way to pre-
compile a TAG into an RCG off-line, thus saving
the conversion time.
Another important feature of grammar engineer-
ing environments consists of its debugging func-
21For a TT-MCTAG counting about 300 sets of trees and an
and-crafted lexicon made of about 300 of words, a 10-word
sentence is parsed (and a semantic representation computed)
within seconds.
tionalities. Among these, one may cite unit and
integration testing. It would be useful to extend
the TuLiPA system to provide a module for gen-
erating test-suites for a given grammar. The idea
would be to record the coverage and analyses of
a grammar at a given time. Once the grammar is
further developed, these snapshots would allow for
regression testing.
5.3 About multi-formalism
We already mentioned that TuLiPA was opening
a way towards multi-formalism by relying on an
RCG core. It is worth noticing that the XMG
system was also designed to be further extensi-
ble. Indeed, a metagrammar in XMG corresponds
to the combination of elementary structures. One
may think of designing a library of such structures,
these would be dependent on the target gram-
mar formalism. The combinations may represent
general linguistic concepts and would be shared
by different grammar implementations, following
ideas presented by Bender et al (2005).
6 Conclusion
In this paper, we have presented a multi-formalism
parsing architecture using RCG as a pivot formal-
ism to parse mildly context-sensitive formalisms
(currently TAG and TT-MCTAG). This system has
been designed to facilitate grammar development
by providing user-friendly interfaces, along with
several functionalities (e.g., dependency extrac-
tion, derivation/derived tree display and semantic
calculus). It is currently used for developing a core
grammar for German.
At the moment, we are working on the extension
of this architecture to include a fully functional
Eclipse plug-in. Other current tasks concern op-
timizations to support large scale parsing and the
extension of the syntactic and semantic coverage
of the German grammar under development.
In a near future, we plan to evaluate the parser
and the German grammar (parsing time, correction
of syntactic and semantic outputs) with respect to
a standard test-suite such as the TSNLP (Lehmann
et al, 1996).
Acknowledgments
This work has been supported by the Deutsche
Forschungsgemeinschaft (DFG) and the Deutscher
Akademischer Austausch Dienst (DAAD, grant
7
A/06/71039). We are grateful to three anonymous
reviewers for valuable comments on this work.
References
Baldridge, Jason, Sudipta Chatterjee, Alexis Palmer,
and Ben Wing. 2007. DotCCG and VisCCG: Wiki
and programming paradigms for improved grammar
engineering with OpenCCG. In King, Tracy Hol-
loway and Emily M. Bender, editors, Proceedings of
the GEAF07 workshop, pages 5?25, Stanford, CA.
CSLI.
Beavers, John. 2002. Documentation: A CCG Imple-
mentation for the LKB. LinGO Working Paper No.
2002-08, CSLI, Stanford University, Stanford, CA.
Bender, Emily, Dan Flickinger, Frederik Fouvry, and
Melanie Siegel. 2005. Shared representation in mul-
tilingual grammar engineering. Research on Lan-
guage & Computation, 3(2):131?138.
Bonfante, Guillaume, Bruno Guillaume, and Guy Per-
rier. 2004. Polarization and abstraction of grammat-
ical formalisms as methods for lexical disambigua-
tion. In Proceedings of the International Conference
on Computational Linguistics (CoLing 2004), pages
303?309, Geneva, Switzerland.
Boullier, Pierre. 1998. Proposal for a natural lan-
guage processing syntactic backbone. Rapport de
Recherche 3342, INRIA.
Boullier, Pierre. 1999. On TAG and Multicomponent
TAG Parsing. Rapport de Recherche 3668, INRIA.
Boullier, Pierre. 2000. Range concatenation gram-
mars. In Proceedings of the International Workshop
on Parsing Technologies (IWPT 2000), pages 53?64,
Trento, Italy.
Crabbe?, Benoit. 2005. Grammatical development with
XMG. In Proceedings of the conference on Logical
Aspects of Computational Linguistics 2005 (LACL
05), pages 84?100, Bordeaux, France.
Duchier, Denys, Joseph Le Roux, and Yannick Parmen-
tier. 2004. The Metagrammar Compiler: An NLP
Application with a Multi-paradigm Architecture. In
Proceedings of the 2nd International Mozart/Oz
Conference (MOZ?2004), pages 175?187, Charleroi,
Belgium.
Erbach, Gregor. 1992. Tools for grammar engineer-
ing. In 3rd Conference on Applied Natural Lan-
guage Processing, pages 243?244, Trento, Italy.
Gardent, Claire and Laura Kallmeyer. 2003. Semantic
Construction in FTAG. In Proceedings of the Con-
ference of the European chapter of the Association
for Computational Linguistics (EACL 2003), pages
123?130, Budapest, Hungary.
Gardent, Claire and Yannick Parmentier. 2007. Sem-
tag: a platform for specifying tree adjoining gram-
mars and performing tag-based semantic construc-
tion. In Proceedings of the International Confer-
ence of the Association for Computational Linguis-
tics (ACL 2007), Companion Volume Proceedings of
the Demo and Poster Sessions, pages 13?16, Prague,
Czech Republic.
Joshi, Aravind K. 1987. An introduction to Tree Ad-
joining Grammars. In Manaster-Ramer, A., editor,
Mathematics of Language, pages 87?114. John Ben-
jamins, Amsterdam.
Kallmeyer, Laura and Yannick Parmentier. 2008. On
the relation between Multicomponent Tree Adjoin-
ing Grammars with Tree Tuples (TT-MCTAG) and
Range Concatenation Grammars (RCG). In Pro-
ceedings of the 2nd International Conference on
Language and Automata Theories and Applications
(LATA 2008), pages 277?288, Tarragona, Spain.
Kallmeyer, Laura, Timm Lichte, Wolfgang Maier, Yan-
nick Parmentier, and Johannes Dellert. 2008. De-
velopping an MCTAG for German with an RCG-
based Parser. In Proceedings of the Language, Re-
source and Evaluation Conference (LREC 2008),
Marrakech, Morocco.
Lehmann, Sabine, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve? Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP ?
Test Suites for Natural Language Processing. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (Coling 1996), volume 2, pages
711?716, Copenhagen, Denmark.
Lichte, Timm. 2007. An MCTAG with tuples for co-
herent constructions in German. In Proceedings of
the 12th Conference on Formal Grammar, Dublin,
Ireland.
Perrier, Guy. 2000. Interaction grammars. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (CoLing 2000), pages 600?606,
Saarbruecken, Germany.
S?gaard, Anders. 2007. Complexity, expressivity and
logic of linguistic theories. Ph.D. thesis, University
of Copenhagen, Copenhagen, Denmark.
Villemonte de la Clergerie, ?Eric. 2005. DyALog: a
tabular logic programming based environment for
NLP. In Proceedings of the workshop on Constraint
Satisfaction for Language Processing (CSLP 2005),
pages 18?33, Barcelona, Spain.
XTAG-Research-Group. 2001. A lexicalized tree
adjoining grammar for english. Technical Re-
port IRCS-01-03, IRCS, University of Pennsylva-
nia. Available at http://www.cis.upenn.
edu/?xtag/gramrelease.html.
8
