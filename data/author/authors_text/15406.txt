Proceedings of NAACL-HLT 2013, pages 709?714,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distributional semantic models for the evaluation of disordered language
Masoud Rouhizadeh?, Emily Prud?hommeaux?, Brian Roark?, Jan van Santen?
?Center for Spoken Language Understanding, Oregon Health & Science University
?Center for Language Sciences, University of Rochester
{rouhizad,vansantj}@ohsu.edu, {emilypx,roarkbr}@gmail.com
Abstract
Atypical semantic and pragmatic expression is
frequently reported in the language of children
with autism. Although this atypicality often
manifests itself in the use of unusual or un-
expected words and phrases, the rate of use
of such unexpected words is rarely directly
measured or quantified. In this paper, we
use distributional semantic models to automat-
ically identify unexpected words in narrative
retellings by children with autism. The classi-
fication of unexpected words is sufficiently ac-
curate to distinguish the retellings of children
with autism from those with typical develop-
ment. These techniques demonstrate the po-
tential of applying automated language anal-
ysis techniques to clinically elicited language
data for diagnostic purposes.
1 Introduction
Autism spectrum disorder (ASD) is a neurodevelop-
mental disorder characterized by impaired commu-
nication and social behavior. Although the symp-
toms of ASD are numerous and varied, atypical
and idiosyncratic language has been one of the
core symptoms observed in verbal individuals with
autism since Kanner first assigned a name to the
disorder (Kanner, 1943). Atypical language cur-
rently serves as a diagnostic criterion in many of the
most widely used diagnostic instruments for ASD
(Lord et al, 2002; Rutter et al, 2003), and the phe-
nomenon is especially marked in the areas of seman-
tics and pragmatics (Tager-Flusberg, 2001; Volden
and Lord, 1991).
Because structured language assessment tools are
not always sensitive to the particular atypical seman-
tic and pragmatic expression associated with ASD,
measures of atypical language are often drawn from
spontaneous language samples. Expert manual an-
notation and analysis of spontaneous language in
young people with ASD has revealed that children
and young adults with autism include significantly
more bizarre and irrelevant content (Loveland et al,
1990; Losh and Capps, 2003) in their narratives and
more abrupt topic changes (Lam et al, 2012) in
their conversations than their language-matched typ-
ically developing peers. Most normed clinical in-
struments for analyzing children?s spontaneous lan-
guage, however, focus on syntactic measures and
developmental milestones related to the acquisition
of vocabulary and syntactic structures. Measures of
semantic and pragmatic atypicality in spontaneous
language are rarely directly measured. Instead, the
degree of language atypicality is often determined
via subjective parental reports (e.g., asking a par-
ent whether their child has ever used odd phrases
(Rutter et al, 2003)) or general impressions dur-
ing clinical examination (e.g., rating the child?s de-
gree of ?stereotyped or idiosyncratic use of words or
phrases? on a four-point scale (Lord et al, 2002)).
This has led to a lack of reliable and objective infor-
mation about the frequency of atypical language use
and its precise nature in ASD.
In this study, we attempt to automatically detect
instances of contextually atypical language in spon-
taneous speech at the lexical level in order to quan-
tify its prevalence in the ASD population. We first
determine manually the off-topic, surprising, or in-
709
appropriate words in a set of narrative retellings
elicited in a clinical setting from children with ASD
and typical development. We then apply word rank-
ing methods and distributional semantic modeling to
these narrative retellings in order to automatically
identify these unexpected words. The results indi-
cate not only that children with ASD do in fact pro-
duce more semantically unexpected and inappropri-
ate words in their narratives than typically develop-
ing children but also that our automated methods
for identifying these words are accurate enough to
serve as an adequate substitute for manual annota-
tion. Although unexpected off-topic word use is just
one example of the atypical language observed in
ASD, the work presented here highlights the poten-
tial of computational language evaluation and analy-
sis methods for improving our understanding of the
linguistic deficits associated with ASD.
2 Data
Participants in this study included 37 children with
typical development (TD) and 21 children with
autism spectrum disorder (ASD). ASD was diag-
nosed via clinical consensus according to the DSM-
IV-TR criteria (American Psychiatric Association,
2000) and the established threshold scores on two
diagnostic instruments: the Autism Diagnostic Ob-
servation Schedule (ADOS) (Lord et al, 2002), a
semi-structured series of activities designed to allow
an examiner to observe behaviors associated with
autism; and the Social Communication Question-
naire (SCQ) (Rutter et al, 2003), a parental ques-
tionnaire. None of the children in this study met
the criteria for a language impairment, and there
were no significant between-group differences in
age (mean=6.4) or full-scale IQ (mean=114).
The narrative retelling task analyzed here is the
Narrative Memory subtest of the NEPSY (Korkman
et al, 1998), a large and comprehensive battery of
tasks that test neurocognitive functioning in chil-
dren. The NEPSY Narrative Memory (NNM) sub-
test is a narrative retelling test in which the subject
listens to a brief narrative, excerpts of which are
shown in Figure 1, and then must retell the narra-
tive to the examiner. The NNM was administered
to each participant in the study, and each partici-
pant?s retelling was recorded and transcribed. Us-
ing Amazon?s Mechanical Turk, we also collected
a large corpus of retellings from neurotypical adults,
who listened to a recording of the story and provided
written retellings. We describe how this corpus was
used in Section 3, below.
Two annotators, blind to the diagnosis of the ex-
perimental subjects, identified every word in each
retelling transcript that was unexpected or inappro-
priate given the larger context of the story. For in-
stance, in the sentence T-rex could smell things, both
T-rex and smell were marked as unexpected, since
there is no mention of either concept in the story. In
a seemingly more appropriate sentence, the boy sat
up off the bridge, the word bridge is considered un-
expected since the boy is trapped up in a tree rather
than on a bridge.
3 Methods
We start with the expectation that different retellings
of the same source narrative will share a common
vocabulary and semantic space. The presence of
words outside of this vocabulary or semantic space
in a retelling may indicate that the speaker has
strayed from the topic of the story. Our approach for
automatically identifying these unexpected words
relies on the ranking of words according to the
strength of their association with the target topic of
the corpus. The word association scores used in the
Figure 1: Excerpts from the NNM narrative.
Jim was a boy whose best friend was Pepper. Pepper was a big black dog. [...] Near Jim?s house was a
very tall oak tree with branches so high that he couldn?t reach them. Jim always wanted to climb that tree,
so one day he took a ladder from home and carried it to the oak tree. He climbed up [...] When he started
to get down, his foot slipped, his shoe fell off, and the ladder fell to the ground. [...] Pepper sat below the
tree and barked. Suddenly Pepper took Jim?s shoe in his mouth and ran away. [...] Pepper took the shoe to
Anna, Jim?s sister. He barked and barked. Finally, Anna understood that Jim was in trouble. She followed
Pepper to the tree where Jim was stuck. Anna put the ladder up and rescued Jim.
710
ranking are informed by the frequency of a word
in the child?s retelling relative to the frequency of
that word in other retellings in the larger corpus of
retellings. These association measures are similar
to those developed for the information retrieval task
of topic modeling, in which the goal is to identify
topic-specific words ? i.e., words that appear fre-
quently in only a subset of documents ? in order
to cluster together documents about a similar topic.
Details about how these scores are calculated and in-
terpreted are provided in the following sections.
The pipeline for determining the set of unusual
words in each retelling begins by calculating word
association scores, described below, for each word
in each retelling and ranking the words according to
these scores. A threshold over these scores is de-
termined for each child using leave-one-out cross
validation in order to select a set of potentially un-
expected words. This set of potential unexpected
words is then filtered using two external resources
that allow us to eliminate words that were not used
in other retellings but are likely to be semantically
related to topic of the narrative. This final set of
words is evaluated against the set of manually iden-
tified words in order determine the accuracy of our
unexpected word classification.
3.1 Word association measures
Before calculating the word association measures,
we tokenize, downcase, and stem (Porter, 1980) the
transcripts and remove all punctuation. We then use
two association measures to score each word in each
child?s retelling: tf-idf, the term frequency-inverse
document frequency measure (Salton and Buckley,
1988), and the log odds ratio (van Rijsbergen et al,
1981). We use the following formulation to calcu-
late tf-idf for each child?s retelling i and each word
in that retelling j, where cij is the count of word j
in retelling i; fj is the number of retellings from the
full corpus of child and adult retellings containing
that word j; and D is the total number of retellings
in the full corpus (Manning et al, 2008):
tf-idfij =
{
(1 + log cij) log Dfj if cij ? 1
0 otherwise
The log odds ratio, another association measure
used in information retrieval and extraction tasks, is
the ratio between the odds of a particular word, j,
appearing in a child?s retelling, i, as estimated us-
ing its relative frequency in that retelling, and the
odds of that word appearing in all other retellings,
again estimated using its relative frequency in all
other retellings. Letting the probability of a word
appearing in a retelling be p1 and the probability of
that word appearing in all other retellings be p2, we
can express the odds ratio as follows:
odds ratio =
odds(p1)
odds(p2)
=
p1/(1? p1)
p2/(1? p2)
A large tf-idf or log odds score indicates that the
word j is very specific to the retelling i, which in
turn suggests that the word might be unexpected or
inappropriate in the larger context of the NNM nar-
rative. Thus we expect that the words with higher as-
sociation measure scores are likely to be the words
that were manually identified as unexpected in the
context of the NNM narrative.
3.2 Application of word association measures
As previously mentioned, both of these word associ-
ation measures are used in information retrieval (IR)
to cluster together documents about a similar target
topic. In IR, words that appear only in a subset of
documents from a large and varied corpus of docu-
ments will have high word association scores, and
the documents containing those words will likely be
focused on the same topic. In our task, however,
we have a single cluster of documents focused on
a single topic: the NNM narrative. Topic-specific
words ought to occur much more frequently than
other words. As a result, words with high tf-idf and
log odds scores are likely to be those unrelated to
the topic of the NNM story. If a child veers away
from the topic of the NNM story and uses words that
do not occur frequently in the retellings produced
by neurotypical speakers, his retellings will contain
more words with high word association scores. We
predict that this set of high-scoring words is likely to
overlap significantly with the set of words identified
by the manual annotators as unexpected or off-topic.
Applying these word association scoring ap-
proaches to each word in each child?s retelling yields
a list of words from each retelling ranked in order of
decreasing tf-idf or log odds score. We use cross-
validation to determine, for each measure, the op-
711
erating point that maximizes the unexpected word
identification accuracy in terms of F-measure. For
each child, the threshold is found using the data from
all of the other children. This threshold is then ap-
plied to the ranked word list of the held-out child.
All words above this threshold are potential unex-
pected words, while all words below this threshold
are considered to be expected and appropriate in the
context of the NNM narrative. Table 1 shows the
recall, precision, and F-measure using the two word
association measures discussed here. We see that
these two techniques result in high recall at the ex-
pense of precision. The next stage in the pipeline is
therefore to use external resources to eliminate any
semantically appropriate words from the set of po-
tentially unexpected or inappropriate words gener-
ated via thresholding on the tf-idf or log odds score.
3.3 Filtering with external resources
Recall that the corpus of retellings used to gener-
ate the word association measures described above,
is very small. It is therefore quite possible that a
child may have used an entirely appropriate word
that by chance was never used by another child or
one of the neurotypical adults. One way of increas-
ing the lexical coverage of the corpus of retellings
is through semantic expansion using an external re-
source. For each word in the set of potential un-
expected words, we located the WordNet synset for
that word (Fellbaum, 1998). If any of the WordNet
synonyms of the potentially unexpected word was
present in the source narrative or in one of the adult
retellings, that word was removed from the set of
unexpected words.
In the final step, we used the CHILDES corpus
of transcripts of children?s conversational speech
(MacWhinney, 2000) to generate topic estimates for
each remaining potentially unexpected word. For
each of these words, we located every utterance in
the CHILDES corpus containing that potentially un-
expected word. We then measured the association
of that word with every other open-class word that
appeared in an utterance with that word using the
log likelihood ratio (Dunning, 1993). The 20 words
from the CHILDES corpus with the highest log like-
lihood ratio (i.e., the words most strongly associ-
ated with the potentially unexpected word), were as-
sumed to collectively represent a particular topic. If
more than two of the words in the vector of words
representing this topic were also present in the NNM
source narrative or the adult retellings, the word that
generated that topic was eliminated from the set of
unexpected words.
We note that the optimized threshold described
in Section 3.2, above, is determined after filtering.
There is therefore potentially a different threshold
for each condition tested, and hence we do not nec-
essarily expect precision to increase and recall to
decrease after filtering. Rather, since the threshold
is selected in order to optimize F-measure, we ex-
pect that if the filtering is effective, F-measure will
increase with each additional filtering condition ap-
plied.
4 Results
We evaluated the performance of our two word rank-
ing techniques, both individually and combined by
taking either the maximum of the two measures or
the sum, against the set of manually annotations de-
scribed in Section 2. In addition, we report the re-
sults of applying these word ranking techniques in
combination with the two filtering techniques. We
compare these results with a simple baseline method
in which every word used in a retelling that is never
used in another retelling is considered to be unex-
pected. Table 1 shows the precision, accuracy, and
F-measure of these approaches. We see that all of
the more sophisticated unexpected word identifica-
tion approaches outperform the baseline by a wide
margin, and that tf-idf and log odds perform compa-
rably under the condition without filtering and both
filtering conditions. Filtering improves F-measure
under both word ranking schemes, and combining
the two measures results in further improvements
under both filtering conditions. Although apply-
ing topic-estimate filtering yields the highest preci-
sion, the simple WordNet-based approach results in
the highest F-measure and a reasonable balance be-
tween precision and recall.
Recall that the purpose of identifying these un-
expected words was to determine whether children
with ASD produce unexpected and inappropriate
words at a higher rate than children with typical de-
velopment. This appears to be true in our manu-
ally annotated data. On average, 7.5% of the words
712
Unexpected word identification method P R F1
Baseline 46.3 74.0 57.0
TF-IDF 72.1 79.5 75.6
Log-odds 70.5 79.5 74.7
Sum(TF-IDF, Log-odds) 72.2 83.3 77.4
Max(TF-IDF, Log-odds) 69.9 83.3 76.0
TF-IDF+WordNet 83.8 80.5 82.1
Log-odds+WordNet 82.1 83.1 82.6
Sum(TF-IDF, Log-odds)+WordNet 84.2 83.1 83.7
Max(TF-IDF, Log-odds)+WordNet 83.3 84.4 83.9
TF-IDF+WordNet+topic 85.7 77.9 81.7
Log-odds+WordNet+topic 83.8 80.5 82.1
Sum(TF-IDF, Log-odds)+WordNet+topic 86.1 80.5 83.2
Max(TF-IDF, Log-odds)+WordNet+topic 85.1 81.8 83.4
Table 1: Accuracy of unexpected word identification.
types produced by children with ASD were marked
as unexpected, while only 2.5% of words produced
by children with TD were marked as unexpected, a
significant difference (p < 0.01, using a one-tailed
t-test). This significant between-group difference
in rate of unexpected word use holds even when
using the automated methods of unexpected word
identification, with the best performing unexpected
word identification method estimating a mean of
6.6% in the ASD group and 2.5% in the TD group
(p < 0.01).
5 Conclusions and future work
The automated methods presented here for rank-
ing and filtering words according to their distribu-
tions in different corpora, which are adapted from
techniques originally developed for topic modeling
in the context of information retrieval and extrac-
tion tasks, demonstrate the utility of automated ap-
proaches for the analysis of semantics and pragmat-
ics. We were able to use these methods to iden-
tify unexpected or inappropriate words with high
enough accuracy to replicate the patterns of unex-
pected word use manually observed in our two di-
agnostic groups. This work underscores the poten-
tial of automated techniques for improving our un-
derstanding of the prevalence and diagnostic utility
of linguistic features associated with ASD and other
communication and language disorders.
In future work, we plan to use a development set
to determine the optimal number of topical words
to select during the topic estimate filtering stage of
the pipeline in order to maintain improvements in
precision without a loss in recall. We would also
like to investigate using part-of-speech, word sense,
and parse information to improve our approaches
for both semantic expansion and topic estimation.
Although the rate of unexpected word use alone is
unlikely to provide sufficient power to classify the
two diagnostic groups investigated here, we expect
that it can serve as one feature in an array of fea-
tures that capture the broad range of semantic and
pragmatic atypicalities observed in the spoken lan-
guage of children with autism. Finally, we plan to
apply these same methods to identify the confabula-
tions and topic shifts often observed in the narrative
retellings of the elderly with neurodegenerative con-
ditions.
Acknowledgments
This work was supported in part by NSF
Grant #BCS-0826654, and NIH NIDCD grant
#1R01DC012033-01. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NSF or the NIH.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
713
Christian Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Leo Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation, San Antonio.
Yan Grace Lam, Siu Sze, and Susanna Yeung. 2012.
Towards a convergent account of pragmatic language
deficits in children with high-functioning autism: De-
picting the phenotype using the pragmatic rating scale.
Research in Autism Spectrum Disorders, 6:792797.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Molly Losh and Lisa Capps. 2003. Narrative ability
in high-functioning children with autism or asperger?s
syndrome. Journal of Autism and Developmental Dis-
orders, 33(3):239?251.
Katherine Loveland, Robin McEvoy, and Belgin Tunali.
1990. Narrative story telling in autism and down?s
syndrome. British Journal of Developmental Psychol-
ogy, 8(1):9?23.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates,
Mahwah, NJ.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval. Cambridge University Press.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Helen Tager-Flusberg. 2001. Understanding the lan-
guage and communicative impairments in autism. In-
ternational Review of Research in Mental Retardation,
23:185?205.
C.J. van Rijsbergen, D.J. Harper, and M.F. Porter. 1981.
The selection of good search terms. Information Pro-
cessing and Management, 17(2):77?91.
Joanne Volden and Catherine Lord. 1991. Neologisms
and idiosyncratic language in autistic speakers. Jour-
nal of Autism and Developmental Disorders, 21:109?
130.
714
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 249?252,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Autism and Interactional Aspects of Dialogue
Peter A. Heeman, Rebecca Lunsford, Ethan Selfridge, Lois Black, and Jan van Santen
Center for Spoken Language Understanding
Oregon Health & Science University
heemanp@ohsu.edu
Abstract
Little research has been done to explore
differences in the interactional aspects of
dialogue between children with Autis-
tic Spectrum Disorder (ASD) and those
with typical development (TD). Quantify-
ing the differences could aid in diagnosing
ASD, understanding its nature, and better
understanding the mechanisms of dialogue
processing. In this paper, we report on a
study of dialogues with children with ASD
and TD. We find that the two groups differ
substantially in how long they pause be-
fore speaking, and their use of fillers, ac-
knowledgments, and discourse markers.
1 Introduction
Autism Spectrum Disorders (ASD) form a group
of severe neuropsychiatric conditions whose fea-
tures can include impairments in reciprocal social
interaction and in communication (APA, 2000).
These impairments may take different forms,
ranging from individuals with little or no com-
munication to fully verbal individuals with fluent,
grammatically correct speech. In this latter verbal
group, shortcomings in communication have been
noted, including using and processing social cues
during conversations. This is no surprise, since
negotiating a conversation requires many abilities,
several of which are generally impaired in ASD,
such as generating appropriate prosody (Kanner,
1943) and ?theory of mind? (Baron-Cohen, 2000).
We make a distinction between transactional
and interactional aspects of dialogue (Brown and
Yule, 1983). The transactional aspect refers to
message content and interactional focuses on ex-
pressing social relations and personal attitudes.
In this paper, we focus on surface behaviors
that speakers use to help manage the interaction,
namely turn-taking, and the use of fillers, dis-
course markers, and acknowledgments. One ad-
vantage of these behaviors is that they do not re-
quire complete understanding of the dialogue, and
thus lend themselves to automatic analysis. In
addition, these behaviors are under the speaker?s
control and should be robust to what the other
speaker is doing. We hypothesize that just as in-
teractional aspects in general are affected in ASD,
so are these surface behaviors. However, to our
knowledge, little or no work has been done on this.
Investigating how the interactional aspects of
dialogue are affected in ASD serves several pur-
poses. First, it can help in the diagnostic process.
Currently, diagnosing ASD is subjective. Objec-
tive measures based on dialogue interaction could
improve the reliability of the diagnostic process.
Second, it can help us refine the behavioral phe-
notypes of ASD, which is critical for progress on
the basic science front. Third, it can help us re-
fine therapy for people with ASD to address di-
alogue interaction deficits. Fourth, understand-
ing what dialogue aspects are affected in high-
functioning verbal children with ASD can help de-
termine which aspects of dialogue are primarily
social in nature. For example, do speakers use
fillers to signal that there is a communication prob-
lem, or are fillers a symptom of it (cf. Clark and
Fox Tree, 2002)?
In this paper, we report on a study of interac-
tional aspects of dialogues between clinicians and
children with ASD. The dialogues were recorded
during administration of the Autism Diagnostic
Observation Schedule (Lord et al, 2000), which
is an instrument used to assist in diagnosing ASD.
We compare the performance of these children
with a group of children with typical development
(TD).
2 Data
The data used in this paper was collected dur-
ing administration of the ADOS on 22 TD chil-
dren and 26 with ASD, ranging in age from 4 to
8 years old. The children with ASD were high-
functioning and verbal. The speech of the clini-
cian and child was transcribed into utterance-like
units, with a start and an end time. Activities were
annotated in a separate tier. The transcriptions in-
cluded the punctuation marks ?.?, ?!?, and ??? to
mark syntactically and semantically complete sen-
249
tences, and ?>? to mark incomplete ones. As a sin-
gle audio channel was used, the timing of overlap-
ping speech was marked as best as possible. Each
child on average said 2221 words, 574 utterances,
and 316 turns.
3 Results
Pauses between Turns: We first examine how
long children wait before starting their turn. We
hypothesized that children with ASD would wait
longer on average to respond, either because they
are less aware of (a) the turn-taking cues, (b) the
social obligation to minimize inter-turn pauses, or
(c) they have a slower processing and response
times. For this analysis, we look at all turns in
which there is no overlap between the beginning of
the child?s turn and the clinician?s speech. Data is
available on 4412 pauses for the TD children and
5676 for the children with ASD. The grand means
of the children?s pauses are shown in Table 1 along
with the standard deviations. The TD children?s
average pause length is 0.876s. For the children
with ASD, it is 1.115s, 27.3% longer. This dif-
ference is significant, a-priori independent t-test
t=2.34 (df=39), p<.02 one-tailed.
TD ASD
all 0.876 (0.24) 1.115 (0.45)
after question 0.748 (0.25) 1.005 (0.40)
after non-question 1.076 (0.37) 1.329 (0.74)
Table 1: Pauses before new turns.
We also examine the pauses following ques-
tions by the clinician versus non-questions. Ques-
tions are interesting as they impose a social obli-
gation for the child to respond, and they have
strong prosodic cues at their ending. We identified
questions as utterances transcribed with a ques-
tion mark, which might include rhetorical ques-
tions. After a non-question (e.g., a statement), the
average pause is 1.076s for the TD children and
1.329s for children with ASD. This difference is
not statistically significant by independent t-test,
t<1.6, NS. After a question, the average pause
is 0.748s for the TD children and 1.005s for the
children with ASD, a significant difference by a-
priori independent t-test t=2.72 (df=42), p<.005
one-tailed. The ASD children on average take
34.4% longer to respond. Thus, after a question,
the difference between children with TD and ASD
is more pronounced.
Pauses by Activity: The ADOS includes hav-
ing the child engage in different activities. For
this research, we collapse the activities into three
types: converse is when there is no non-speech
task; describe is when the child is doing a men-
tal task, such as describing a picture; and play is
when the child is interacting with the clinician in
a play session. To better understand the difference
between questions and non-questions, we examine
the pauses in each activity (Table 2).
TD ASD
question non-ques. question non-ques.
converse 0.730 0.30 0.656 0.27 0.890 0.34 0.932 0.88
describe 0.853 0.44 0.879 0.37 1.056 0.51 1.282 1.21
play 0.720 0.34 1.825 0.78 1.289 1.51 1.887 1.37
Table 2: Pauses for each type of activity.
After a question, the TD children tend to re-
spond with similar pauses in each activity (the dif-
ferences in column 2 between activities are not
significant by pairwise paired t-test, all t?s<1.6,
NS). After a question, the child has a social obli-
gation to respond, and this does not seem to be
overridden by whether there is a separate task
they are involved in. Even after a non-question,
conversants have a social obligation to keep the
speaking floor occupied and so to minimize inter-
utterance pauses (Sacks et al, 1974). However, as
seen in the third column, the pauses are affected
by the type of activity, and the differences are
statistically significant by pairwise paired t-test,
(df=21), two-tailed: converse-describe t=2.24,
p<.04; describe-play t=5.68, p<.0001; converse-
play t=6.87, p<.0001. The biggest difference is
with play. Here, it seems that the conversants
physical interaction lessens the social obligation
of maintaining the speaking floor. These findings
are interesting for social-linguistics as it suggests
that the social obligations of turn-taking are al-
tered by the presence of a non-speech task.
We next compare the children with ASD to the
TD children. For the converse activity, we see that
the children with ASD take longer to respond, af-
ter questions and non-questions. The difference
after questions is significant by independent t-test,
t=1.74 (df=46) p<.05, one-tailed, whereas the dif-
ference after non-questions is marginal, t=1.47
(df=28) p<.08. This result could be explained by
the slower processing and response times associ-
ated with ASD.
Just as with the TD children, we see that after
a non-question, the children with ASD take longer
to respond when there is another task. The differ-
ences in pause lengths between converse and play
are significant, by paired t-test, t=2.89 (df=23)
250
p<.009, two-tailed. The difference between de-
scribe and play is marginal, t=2.03 (df=25) p<.06,
and there was no significant difference between
converse and describe, t<1, NS.
After a question, the children with ASD take
longer to respond when there is another task, espe-
cially for play, although the pairwise differences in
pause length between activities are not significant.
This suggests that the children with ASD become
distracted when there is another task, and so be-
come less sensitive to either the question prosody
or the social obligation of questions.
Fillers: We next examine the rate of fillers, at
the beginning of turns, beginning of utterances,
and in the middle of utterances. We look at these
contexts individually as fillers can serve different
roles, such as turn-taking, stalling for time or as
part of a disfluency, and their role is correlated
to their position in a turn. The rates are reported
in Table 3, along with the total number of fillers
within each category. Interestingly, the rate of ?uh?
between children with TD and ASD is similar for
all positions (independent t-test, all g?s<1, NS).
uh um
TD ASD TD ASD
turn init. 1.70% 112 1.84% 159 3.86% 243 1.65% 146
utt. init. 1.31% 43 1.20% 33 2.29% 73 0.52% 10
utt. medial 0.25% 103 0.31% 137 1.03% 492 0.21% 123
Table 3: Rate of fillers.
The more interesting finding, though, is in the
usage of ?um?. Children with ASD use it signifi-
cantly less than the TD children in every position,
from 1/2 the rate in turn-initial position to 1/5 in
utterance-medial position, independent two-tailed
t-test: turn initial t=2.74 (df=38), p<.01; utterance
initial t=2.53 (df=31), p<.02; and utterance me-
dial t=3.94 (df=24), p<.001.
TD ASD
converse 1.76% 569 0.56% 190
describe 1.15% 115 0.33% 31
play 0.96% 124 0.45% 58
Table 4: Use of ?um? by activity.
We also examined the overall usage of ?um? in
each activity (Table 4). The TD children use ?um?
more often in each activity than the children with
ASD, and the differences are statistically signif-
icant by independent two-tailed t-test: converse
t=3.62 (df=29), p<.002; describe t=2.83 (df=27),
p<.01; play t=2.42 (df=33), p<.03. This result
supports the robustness of the findings about ?um?.
Many researchers have speculated on the role
of ?um? and ?uh?. In recent work, Clark and Fox
Tree (2002) argued that they signal a delay, and
that ?um? signals more delay than ?uh?. They view
both as linguistic devices that are planned for, just
as any other word is. Our work suggests that ?um?
and ?uh? arise from different cognitive processes,
and that the process that accounts for ?uh? is not
affected by ASD, while the process for ?um? is.1
Acknowledgments: We next look at the rate of
acknowledgments: single word utterances that are
used to show agreement or understanding. Thus,
the use of acknowledgments requires awareness of
the other person?s desire to ensure mutual under-
standing. As the corpus did not have these words
explicitly marked, we identify a word as an ac-
knowledgment if it meets the following criteria:
(a) it is one of the words listed in Table 5 (based
on Heeman and Allen, 1999); (b) it is first in the
speaker?s turn; and (c) it does not follow a question
by the clinician. The TD children used acknowl-
edgments in 17.42% of their turns that did not fol-
low a question, while the children with ASD did
this only 13.39% of the time (Table 5), a statis-
tically significant difference by a-priori indepen-
dent t-test t=1.78 (df=46), p<.05 one-tailed.
TD ASD
total 17.42% 568 13.39% 459
yeah 7.49% 248 5.87% 215
no 2.78% 78 2.06% 63
mm-hmm 2.06% 75 1.07% 35
mm 0.99% 29 1.35% 42
ok 1.87% 65 0.83% 27
yes 0.92% 32 0.88% 32
right 0.14% 5 0.23% 8
hm 0.73% 21 0.69% 20
uh-huh 0.44% 15 0.42% 17
Table 5: Use of acknowledgments.
Discourse Markers: We next examine dis-
course markers, which are words such as ?well?
and ?oh? that express how the current utterance
relates to the discourse context (Schiffrin, 1987).
We classified a word as a discourse marker if it
was the first word in an utterance and is one of
the words in Table 6 (Heeman and Allen, 1999).
As shown in Table 6, the children with ASD use
discourse markers significantly less than the TD
children in both conditions by a-priori indepen-
dent, one-tailed t-test: turn-initial t=3.24 (df=43)
p<.002; utterance-initial t=4.01 (df=44) p<.0001.
1In Lunsford et al (2010) we investigate the rate and
length of pauses after ?uh? and ?um?. In addition, we veri-
fied the t-tests using Wilcoxon rank sum tests.
251
As can be seen, most of the difference is in the use
of ?and?. The data for the other discourse markers
was sparse, so we compared ?and? against all of the
others combined. The decreased usage of ?and?
in the ASD children is statistically significant
for both conditions by a-priori independent, one-
tailed t-test: turn-initial t=4.47 (df=30), p<.0001;
utterance-initial t=3.79 (df=43), p<.0002. There
is little difference in the use of all of the other dis-
course markers combined, and the difference is not
statistically significant.
Turn Initial Utterance Initial
TD ASD TD ASD
all 19.2% 1290 12.8% 1196 28.7% 2053 19.4% 1330
and 10.7% 731 5.0% 471 19.5% 1419 12.0% 844
then 0.6% 38 1.0% 89 1.5% 97 1.4% 79
but 2.1% 144 1.3% 113 3.6% 238 2.7% 194
well 2.2% 143 2.7% 271 1.1% 74 1.2% 79
oh 2.0% 135 1.8% 160 1.0% 67 1.3% 68
so 1.2% 75 0.7% 60 1.6% 129 0.7% 49
wait 0.2% 9 0.2% 21 0.2% 17 0.2% 15
actually 0.2% 15 0.1% 11 0.2% 12 0.0% 2
not and 8.5% 559 7.8% 725 9.2% 634 7.4% 486
Table 6: Use of discourse markers.
The use of ?and? is also lower in each activity
for the ASD children (Table 7), a significant dif-
ference by a-priori independent one-tailed t-test:
converse t=3.00 (df=41), p<.003; describe t=4.79
(df=38), p<.0001, play t=4.07 (df=30), p<.0002.
TD ASD
converse 13.36% 1139 7.95% 755
describe 21.77% 587 10.76% 339
play 12.97% 424 5.18% 221
Table 7: Use of ?and? in each activity.
One explanation for the decreased usage of
?and? and not the other discourse markers might
be that, of all the discourse markers, ?and? seems
to have the least meaning. It simply signifies
that there is some continuation between the new
speech and the previous context. This might make
it difficult for children with ASD to learn its use. A
second explanation is that the children with ASD
are using ?and? correctly, but simply do not pro-
duce as many utterances that are related to the pre-
vious context (cf. Bishop et al, 2000).
4 Conclusion
In this paper, we examined a number of interac-
tional aspects of dialogue in the speech of children
with ASD and TD. We found that children with
ASD have a lower rate of the filler ?um?, acknowl-
edgments, and the discourse marker ?and.? We also
found that in certain situations, they take longer to
respond. These deficits might prove useful for im-
proved diagnosis of ASD. We also found that chil-
dren with ASD have a lower rate of ?um? but not
of ?uh?, and that only the discourse marker ?and?
seems to be affected. This might prove useful for
both better understanding the nature of ASD as
well as better understanding the role of these phe-
nomena in dialogue. Although the results reported
in this work are preliminary, they do show the po-
tential of our approach. More work is needed to
ensure that our automatic identification of turn-
taking events, discourse markers, and acknowl-
edgments is correct and to explore alternate expla-
nations for the results that we observed.
Acknowledgments
Funding gratefully received from the National In-
stitute of Heath under grants IR21DC010239 and
5R01DC007129, and the National Science Foun-
dation under IIS-0713698. The views herein are
those of the authors and reflect the views neither
of the funding agencies.
References
American Psychiatric Association, Washington DC,
2000. Diagnostic and Statistical Manual of Mental
Disorders, 4th Edition, Text Revision (DSM-IV-TR).
S. Baron-Cohen. 2000. Theory of mind and autism:
A review. In L. M. Glidden, editor, International
Review of Research in Mental Retardation, volume
23: Autism, pages 170?184. Academic Press.
D. Bishop et al 2000. Conversational responsive-
ness in specific language impairment: Evidence of
disproportionate pragmatic difficulties in a subset
of children. Development and Psychopathology,
12(2):177?199.
G. Brown and G. Yule. 1983. Discourse Analysis.
Cambridge University Press.
H. Clark and J. Fox Tree. 2002. Using uh and um in
spontaneous speaking. Cognition, 8:73?111.
P. Heeman and J. Allen. 1999. Speech repairs, in-
tonational phrases and discourse markers: Model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?572.
L. Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
C. Lord et al 2000. The autism diagnostic observa-
tion schedule-generic: a standard measure of social
and communication deficits associted with the spec-
trum of autium. Journal of Austism Developmental
Disorders, 30(3):205?223, June.
R. Lunsford et al 2010. Autism and the use of fillers:
differences between ?um? and ?uh?. In 5th Workshop
on Disfluency in Spontaneous Speech, Tokyo.
H. Sacks, E. Schegloff, and G. Jefferson. 1974. A sim-
plest systematics for the organization of turn-taking
for conversation. Language, 50(4):696?735.
D. Schiffrin. 1987. Discourse Markers. Cambridge
University Press, New York.
252
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 88?96,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Classification of atypical language in autism
Emily T. Prud?hommeaux, Brian Roark, Lois M. Black, and Jan van Santen
Center for Spoken Language Understanding
Oregon Health & Science University
20000 NW Walker Rd., Beaverton, Oregon 97006
{emily,roark,lmblack,vansanten}@cslu.ogi.edu
Abstract
Atypical or idiosyncratic language is a char-
acteristic of autism spectrum disorder (ASD).
In this paper, we discuss previous work iden-
tifying language errors associated with atyp-
ical language in ASD and describe a proce-
dure for reproducing those results. We de-
scribe our data set, which consists of tran-
scribed data from a widely used clinical di-
agnostic instrument (the ADOS) for children
with autism, children with developmental lan-
guage disorder, and typically developing chil-
dren. We then present methods for automati-
cally extracting lexical and syntactic features
from transcripts of children?s speech to 1)
identify certain syntactic and semantic errors
that have previously been found to distinguish
ASD language from that of children with typ-
ical development; and 2) perform diagnostic
classification. Our classifiers achieve results
well above chance, demonstrating the poten-
tial for using NLP techniques to enhance neu-
rodevelopmental diagnosis and atypical lan-
guage analysis. We expect further improve-
ment with additional data, features, and clas-
sification techniques.
1 Introduction
Atypical language and communication have been as-
sociated with autism spectrum disorder (ASD) since
Kanner (1943) first gave the name autism to the dis-
order. The Autism Diagnostic Observation Sched-
ule (ADOS) (Lord et al, 2002) and other widely
used diagnostic instruments include unusual word
use as a diagnostic criterion. The broad and con-
flicting definitions used in diagnostic instruments for
ASD, however, can lead to difficulty distinguishing
the language peculiarities associated with autism.
The most recent and the most systematic study of
unusual word use in ASD (Volden and Lord, 1991)
found that certain types of atypical word use were
significantly more prevalent in ASD speech than
in the speech of children with typical development
(TD). Although the results provided interesting in-
formation about unusual language in ASD, the pro-
cess of coding these types of errors was laborious
and required substantial linguistic and clinical ex-
pertise.
In this paper, we first use our own data to repro-
duce a subset of the results reported in Volden and
Lord (1991). We then present a method of automat-
ically identifying the types of errors associated with
ASD using spoken language features and machine
learning techniques. These same features are then
used to differentiate subjects with ASD or a devel-
opmental language disorder (DLD) from those with
TD. Although these linguistic features yield strong
classification results, they also reveal a number of
obstacles to distinguishing language characteristics
associated with autism from those associated with
language impairment.
2 Previous Work
Since it was first recognized as a neurodevelop-
mental disorder, autism has been associated with
language described variously as: ?seemingly non-
sensical and irrelevant?, ?peculiar and out of place
in ordinary conversation? (Kanner, 1946); ?stereo-
typed?, ?metaphorical?, ?inappropriate? (Bartak et
al., 1975); and characterized by ?a lack of ease in
88
the use of words? (Rutter, 1965) and ?the use of
standard, familiar words or phrases in idiosyncratic
but meaningful way? (Volden and Lord, 1991). The
three most common instruments used in ASD diag-
nosis ? the Autism Diagnostic Observation Sched-
ule (ADOS) (Lord et al, 2002), the Autism Di-
agnostic Interview-Revised (ADI-R) (Lord et al,
1994), and the Social Communication Questionnaire
(SCQ) (Rutter et al, 2003) ? make reference to
these language particularities in their scoring algo-
rithms. Unfortunately, the guidelines for identify-
ing this unusual language are often vague (SCQ:
?odd?, ADI-R: ?idiosyncratic?, ADOS: ?unusual?)
and sometimes contradictory (ADOS: ?appropriate?
vs. ADI-R: ?inappropriate?; ADOS: ?phrases...they
could not have heard? vs. SCQ: ?phrases that he/she
has heard other people use?).
In what is one of the only studies focused specif-
ically on unusual word use in ASD, Volden and
Lord (1991) transcribed two 10-minute speech sam-
ples from the ADOS for 20 school-aged, high-
functioning children with autism and 20 with typi-
cal development. Utterances containing non-English
words or the unusual use of a word or phrase were
flagged by student workers and then categorized by
the authors into one of three classes according to the
type of error:
? Developmental syntax error: a violation of a
syntactic rule normally acquired in early child-
hood, such as the use of object pronoun in sub-
ject position or an overextension of a regular
morphological rule, e.g., What does cows do?
? Non-developmental syntax error: a syntactic
error not commonly observed in the speech of
children acquiring language, e.g., But in the car
it?s some.
? Semantic error: a syntactically intact sentence
with an odd or unexpected word given the con-
text and intended meaning, e.g., They?re siding
the table.
The authors found that high-functioning chil-
dren with ASD produced significantly more non-
developmental and semantic errors than children
with typical development. The number of develop-
mental syntax errors was not significantly different
between these two groups.
Although there has been virtually no previous
work on automated analysis of unannotated tran-
scripts of the speech of children with ASD, auto-
matically extracted language features have shown
promise in the identification of other neurological
disorders such as language impairment and cogni-
tive impairment. Gabani et al (2009) used part-of-
speech language models to derive perplexity scores
for transcripts of the speech of children with and
without language impairment. These scores offered
significant diagnostic power, achieving an F1 mea-
sure of roughly 70% when used within an support
vector machine (SVM) for classification. Roark et
al. (in press) extracted a much larger set of lan-
guage complexity features derived from syntactic
parse trees from transcripts of narratives produced
by elderly subjects for the diagnosis of mild cogni-
tive impairment. Selecting a subset of these features
for classification with an SVM yielded accuracy, as
measured by the area under the receiver operating
characteristic curve, of 0.73.
Language models have also been applied to the
task of error identification, but primarily in writ-
ing samples of ESL learners. Gamon et al (2008)
used word-based language models to detect and
correct common ESL errors, while Leacock and
Chodorow (2003) used part-of-speech bigram lan-
guage models to identify potentially ungrammatical
two-word sequences in ESL essays. Although these
tasks differ in a number of ways from our tasks, they
demonstrate the utility of using both word and part-
of-speech language models for error detection.
3 Data Collection
3.1 Subjects
Our first objective was to gather data in order repro-
duce the results reported in Volden and Lord (1991).
As shown in Table 1, the participants in our study
were 50 children ages 4 to 8 with a performance
IQ greater than 80 and a diagnosis of either typical
Diagnosis Count Age (s.d.) IQ (s.d.)
TD 17 6.24 (1.38) 125.7 (11.63)
ASD 20 6.38 (1.25) 108.9 (16.41)
DLD 13 7.01 (1.10) 100.6 (10.95)
Table 1: Count, mean age and IQ by subject group.
89
development (TD, n=17), autism spectrum disorder
(ASD, n=20), or developmental language disorder
(DLD, n=13).
Developmental language disorder (DLD), also
sometimes known as specific language impairment
(SLI), is generally defined as the delayed or im-
paired acquisition of language without accompany-
ing comparable delays or deficits in hearing, cogni-
tion, and socio-emotional development (McCauley,
2001). The language impairments that characterize
DLD are not related to articulation or ?speech im-
pediments? but rather are associated with more pro-
found problems producing and often comprehend-
ing language in terms of its pragmatics, syntax, se-
mantics, and phonology. The DSM-IV-TR (Ameri-
can Psychiatric Association, 2000) includes neither
DLD nor SLI as a disorder, but for the purposes
of this work, DLD corresponds to the DSM?s des-
ignations Expressive Language Disorder and Mixed
Expressive-Receptive Language Disorder.
For this study, a subject received a diagnosis of
DLD if he or she met one of two commonly used
criteria: 1) The Tomblin Epi-SLI criteria (Tomblin,
et al, 1996), in which diagnosis of language im-
pairment is indicated when scores in two out of five
domains (vocabulary, grammar, narrative, receptive,
and expressive) are greater than 1.25 standard devia-
tions below the mean; and 2) The CELF-Preschool-
2/CELF-4 criteria, in which diagnosis of language
impairment is indicated when one out of three index
scores and one out of three spontaneous language
scores are more than one standard deviation below
the mean.
A diagnosis of ASD required a previous medi-
cal, educational, or clinical diagnosis of ASD, which
was then confirmed by our team of clinicians ac-
cording to the criteria of the DSM-IV-TR (Ameri-
can Psychiatric Association, 2000), the revised al-
gorithm of the ADOS (Lord et al, 2002), and the
SCQ parental interview (Rutter et al, 2003). Fifteen
of the 20 ASD subjects participating in this study
also met at least one of the above described criteria
for DLD.
3.2 Data Preparation
The ADOS (Lord et al, 2002), a semi-structured se-
ries of activities designed to reveal behaviors asso-
ciated with autism, was administered to all 50 sub-
jects. Five of the ADOS activities that require sig-
nificant amounts spontaneous speech (Make-Believe
Play, Joint Interactive Play, Description of a Pic-
ture, Telling a Story From a Book, and Conversa-
tion and Reporting) were then transcribed at the ut-
terance level for all 50 speakers. All utterances from
the transcripts longer than four words (11,244) were
presented to individuals blind to the purposes of the
study, who were asked to flag any sentence with
atypical or unusual word use. Those sentences were
then classified by the authors as having no errors or
one of the three error types described in Volden and
Lord. Examples from our data are given in Table 2.
3.3 Reproducing Previous Results
In order to compare our results to those reported in
Volden and Lord, we calculated the rates of the three
types of errors for each subject, as shown in Ta-
ble 2. With a two-sample (TD v. ASD) t-test, the
rates of nondevelopmental and semantic errors were
significantly higher in the ASD group than in the
TD group, while there was no significant difference
in developmental errors between the two groups.
These results reflect the same trends observed in
Volden and Lord, in which the raw counts of both
developmental and semantic errors were higher in
the ASD group.
Using ANOVA for significance testing over all
three diagnostic groups, we found that the rate of
developmental errors was significantly higher in the
DLD group than in the other groups. The difference
in semantic error rate between TD and ASD using
the t-test was preserved, but the difference in nonde-
velopmental error rate was lost when comparing all
three diagnostic groups with ANOVA, as shown in
Figure 1.
Error Example
Dev.
I have a games.
The baby drinked it.
The frogs was watching TV.
Nondev.
He locked him all of out.
Would you like to be fall down?
He got so the ball went each way.
Sem.
Something makes my eyes poke.
It smells like it?s falling on your head.
All the fish are leaving in the air.
Table 2: Examples of error types.
90
00.02
0.04
0.06
0.08
Dev. Nondev. Sem.
TD
ASD
DLD
*
*
*
Figure 1: Error rates by diagnostic group (*p <0.05).
The process of manually identifying sentences
with atypical or unusual language was relatively
painless, but determining the specific error types is
subjective and time-consuming, and requires a great
deal of expertise. In addition, although we do ob-
serve significant differences between groups, it is
not clear whether the differences are sufficient for
diagnostic classification or discrimination.
We now propose automatically extracting from
the transcripts various measures of linguistic likeli-
hood, complexity, and surprisal that have the poten-
tial to objectively capture qualities that differentiate
1) the three types of errors described above, and 2)
the three diagnostic groups discussed above. In the
next three sections, we will discuss the various lin-
guistic features we extract; methods for using these
features to classify each sentence according to its er-
ror type for the purpose of automatic error-detection;
and methods for using these features, calculated for
each subject, for diagnostic classification.
4 Features
N-gram cross entropy. Following previous work
in both error detection (Gamon et al, 2008; Leacock
and Chodorow, 2003) and neurodevelopmental di-
agnostic classification (Gabani et al, 2009), we be-
gin with simple bigram language model features. A
bigram language model provides information about
the likelihood of a given item (e.g., a word or part
of speech) in a sentence given the previous item in
that sentence. We suspect that some of the types
of unusual language investigated here, in particular
those seen in the syntactic errors shown in Table 2,
are characterized by unlikely words (drinked) and
word or part-of-speech sequences (a games, all of
out) and hence might be distinguished by language
model-based scores.
We build a word-level bigram language model and
a part-of-speech level bigram language model from
the Switchboard (Godfrey et al, 1992) corpus. We
then automatically generate part-of-speech tags for
each sentence (where the tags were derived from
the best scoring output of the full syntactic parser
mentioned below), and then apply the two models
to each sentence. For each sentence, we calculate
its cross entropy and perplexity. For a word string
w1 . . . wn of length n, the cross entropy H is
H(w1 . . . wn) = ?
1
n
log P(w1 . . . wn) (1)
where P(w1 . . . wn) is calculated as the product of
the n-gram probabilities of each word in the string.
The corresponding measure can be calculated for the
POS-tag sequence, based on an n-gram model of
tags. Perplexity is simply 2H .
While we would prefer to use a corpus that is
closer to the child language that we are attempting
to model, we found the conversational style of the
Switchboard corpus to be the most effective large
corpus that we had at our disposal for this study.
As the size of our small corpus grows, we intend to
make use of the text to assist with model building,
but for this study, we used all out-of-domain data
for n-gram language models and parsing models.
Using Switchboard also allowed us to use the same
corpus to train both n-gram and parsing models.
Surprisal-based features. Surprisal, or the unex-
pectedness of a word or syntactic category in a given
context, is often used as a psycholinguistic mea-
sure of sentence-processing difficulty (Hale, 2001;
Boston et al, 2008). Although surprisal is usually
discussed in the context of cognitive load for lan-
guage processing, we hoped that it might also cap-
ture some of the language characteristics of the se-
mantic errors like those in Table 2, which often con-
tain common words used in surprising ways, and
the nondevelopmental syntax errors, which often in-
clude strings of function words presented in an order
that would be difficult to anticipate.
To derive surprisal-based features, each sentence
is parsed using the Roark (2001) incremental
top-down parser relying on a model built again on
91
the Switchboard corpus. The incremental output of
the parser shows the surprisal for each word, as well
as other scores, as presented in Roark et al (2009).
For each sentence, we collected the mean surprisal
(equivalent to the cross entropy given the model);
the mean syntactic surprisal; and the mean lexical
surprisal. The lexical and syntactic surprisal are a
decomposition of the total surprisal into that portion
due to probability mass associated with building
non-terminal structure (syntactic surprisal) and that
portion due to probability mass associated with
building terminal lexical items in the tree (lexical
surprisal). We refer the reader to that paper for
further details.
Other linguistic complexity measures The non-
developmental syntax errors in Table 2 are charac-
terized by their ill-formed syntactic structure. Fol-
lowing Roark et al (in press), in which the authors
explored the relationship between linguistic struc-
tural complexity and cognitive decline, and Sagae
(2005), in which the authors used automatic syntac-
tic annotation to assess syntactic development, we
also investigated the following measures of linguis-
tic complexity: words per clause, tree nodes per
word, dependency length per word, and Ygnve and
Frazier scores per word. Each of these scores can
be calculated from a provided syntactic parse tree,
and to generate these we made use of the Charniak
parser (Charniak, 2000), also trained on the Switch-
board treebank.
Briefly, words per clause is the total number of
words divided by the total number of clauses; and
tree nodes per word is the total number of nodes
in the parse tree divided by the number of words.
The dependency length for a word is the distance (in
word tokens) between that word and its governor,
as determined through standard head-percolation
methods from the output of the Charniak parser. We
calculate the mean of this length over all words in
the utterance. The Yngve score of a word is the
size of the stack of a shift-reduce parser after that
word; and the Frazier score essentially counts how
many intermediate nodes exist in the tree between
the word and its lowest ancestor that is either the
root or has a left sibling in the tree. We calculate
the mean of both of these scores over the utterance.
We refer the reader to the above cited paper for more
details on these measures.
As noted in Roark et al (in press), some of these
measures are influenced by particular characteristics
of the Penn Treebank style trees ? e.g., flat noun
phrases, etc. ? and measures vary in the degree to
which they capture divergence from typical struc-
tures. Some (including Yngve) are sensitive to the
breadth of trees (e.g., flat productions with many
children); others (including Frazier) are sensitive to
depth of trees. This variability is a key reason for
including multiple, complementary features, such as
both Frazier and Yngve scores, to capture more sub-
tle syntactic characteristics than would be available
from any of these measures alone.
Although we were not able to measure parsing ac-
curacy on our data set and how it might affect the re-
liability of these features, Roark et al (in press) did
investigate this very issue. They found that all of the
above described syntactic measures, when they were
derived from automatically generated parse trees,
correlated very highly (greater than 0.9) with those
measures when they were derived from manually
generated parse trees. For the moment, we assume
that the same principle holds true for our data set,
though we do intend both to verify this assump-
tion and to supplement our parsing models with data
from child speech. Based on manual inspection of
parser output, the current parsing model does seem
to be recovering largely valid structures.
5 Error Classification
The values for 8 of the 12 features were significantly
different over the three error classes, as measured
by one-way ANOVA: words per clause, Yngve, de-
pendency, word cross-entropy all significant at p <
0.001; Frazier, nodes per word at p < 0.01; overall
surprisal and lexical surprisal at p < 0.05. We built
classification and regression trees (CART) using the
Weka data mining software (Hall et al, 2009) us-
ing all of the 12 features described above to predict
which error each sentence contained, and we report
the accuracy, weighted F measure, and area under
the receiver operating characteristic curve (AUC).
Including all 12 features in the CART using 10-
fold cross validation resulted in an AUC of 0.68,
while using only those features with significant
between-group differences yielded an AUC of 0.65.
92
Classifier Acc. F1 AUC
Baseline 1 41% 0.24 0.5
Baseline 2 33% 0.32 0.5
All features 53% 0.53 0.68
Feature subset 49% 0.49 0.65
Table 3: Error-type classification results.
These are both substantial improvements over a
baseline with an unbalanced corpus in which the
most frequent class is chosen for all input items
(Baseline 1) or a baseline with a balanced corpus in
which class is chosen at random (Baseline 2), which
both have an AUC of 0.5. The results for each of
these classifiers, provided in Table 3, show potential
for automating the identification of error type.
6 Diagnostic Classification
In Section 3, we found a number of significant dif-
ferences in error type production rates across our
three diagnostic groups. Individual rates of error
production, however, provide almost no classifica-
tion power within a CART (AUC = 0.51). Perhaps
the phenomena being observed in ASD and DLD
language are related to subtle language features that
are less easily identified than simply the membership
of a sentence in one of these three error categories.
Given the ability of our language features to dis-
criminate error types moderately well, as shown in
Section 5, we decided to extract these same 12 fea-
tures from every sentence longer than 4 words from
the entire transcript for each of the subjects. We
then took the mean of each feature over all of the
sentences for each speaker. These per-speaker fea-
ture vectors were used for diagnostic classification
within a CART.
We first performed classification over the three di-
agnostic groups using the full set of 12 features de-
scribed in Section 4. This results in only modest
gains in performance over the baseline that uses er-
ror rates as the only features. We then used ANOVA
to determine which of the 12 features differed sig-
nificantly across the three groups. Only four fea-
tures were found to be significantly different across
the three groups (words per clause, Yngve, depen-
dency, word cross entropy), and none of them dif-
ferent significantly between the ASD group and the
DLD group. As expected, classification did not im-
Features Acc. F1 AUC
Error rates 33% 0.32 0.51
All features 42% 0.38 0.59
Feature subset 40% 0.37 0.6
Table 4: All subjects: Diagnostic classification results.
prove with this feature subset, as reported in Table 4.
Recall that 15 of the 20 ASD subjects also met at
least one criterion for a developmental language dis-
order. Perhaps the language peculiarities we observe
in our subjects with ASD are related in part to lan-
guage characteristics of DLD rather than ASD. We
now attempt to tease apart these two sources of un-
usual language by investigating three separate clas-
sification tasks: TD vs. ASD, TD vs. DLD, and
ASD vs. DLD.
6.1 TD vs. ASD
We perform classification of the TD and ASD sub-
jects with three feature sets: 1) per-subject error
rates; 2) all 12 features described in Section 4; and
3) the subset of significantly different features. We
found that 7 of the 12 features explored in Section 4
differed significantly between the TD group and the
ASD group: words per clause, Yngve, dependency,
word cross-entropy, overall surprisal, syntactic sur-
prisal, and lexical surprisal. Classification results are
shown in Table 5. We see that using the automati-
cally derived linguistic features improves classifica-
tion substantially over the baseline using per-subject
error rates, particularly when we use the feature sub-
set. Note that the best classification accuracy results
are comparable to those reported in related work on
language impairment and mild cognitive impairment
described in Section 2.
6.2 TD vs. DLD
We perform classification of TD and DLD subjects
with the same three feature sets used for the TD
vs. ASD classification. We found that 6 of the 12
Features Acc. F1 AUC
Error rates 62% 0.62 0.56
All features 62% 0.62 0.65
Feature subset 68% 0.67 0.72
Table 5: TD vs. ASD: Diagnostic classification results.
93
Features Acc. F1 AUC
Error rates 67% 0.67 0.72
All features 80% 0.79 0.75
Feature subset 77% 0.75 0.66
Table 6: TD vs. DLD: Diagnostic classification results.
features explored in Section 4 different significantly
between the TD group and the ASD group: words
per clause, Yngve, dependency, word cross-entropy,
overall surprisal, and lexical surprisal. Note that this
is a subset of the features that differed between the
TD group and ASD group. Classification results are
shown in Table 6. Interestingly, using per-subject er-
ror rates for classification of TD and DLD subjects
was quite robust. Using all of the features improved
classification somewhat, while using only a subset
resulted in degraded performance. We see that the
discriminative power of these features is superior to
that reported in earlier work using LM-based fea-
tures for classification of specific language impair-
ment (Gabani et al, 2009).
6.3 ASD vs. DLD
Finally, we perform classification of the ASD and
DLD subjects using only the first two features
sets, since there were no features found to be even
marginally significantly different between these two
groups. Classification results, which are dismal for
both feature sets, are shown in Table 7.
6.4 Discussion
It seems quite clear that the error rates, feature val-
ues, and classification performance are all being in-
fluenced by the fact that a majority of the ASD sub-
jects also meet at least one criterion for a develop-
mental language disorder. Neither error rates nor
feature values could discriminate between the ASD
and DLD group. Nevertheless we see that our ASD
group and DLD group do not follow the same pat-
terns in their error production or language feature
scores. Clearly there are differences in the language
Features Acc. F1 AUC
Error rates 55% 0.52 0.48
All features 58% 0.44 0.40
Table 7: ASD vs. DLD: Diagnostic classification results.
patterns of the two groups that are not being cap-
tured with any of the methods discussed here.
We also observe that the error rates them-
selves, while sometimes significantly different
across groups as originally observed in Volden and
Lord, do not perform well as diagnostic features
for ASD in our framework. Volden and Lord did
not attempt classification in their study, so it is not
known whether the authors would have encountered
the same problem. There are, however, a number
of possible explanations for a discrepancy between
our results and theirs. First, our data was gath-
ered from pre-school and young school-aged chil-
dren, while the Volden and Lord subjects were gen-
erally teenagers and young adults. The way in which
their spoken language samples were elicited allowed
Volden and Lord to use raw error counts rather than
error rates. There may also have been important dif-
ferences in the way we carried out the manual er-
ror identification process, despite our best efforts to
replicate their procedure. Further development of
our classification methods and additional data col-
lection are needed to determine the utility of error
type identification for diagnostic purposes.
7 Future Work
Although our classifiers using automatically ex-
tracted features were generally robust, we expect
that including additional classification techniques,
subjects (especially ASD subjects without DLD),
and features will further improve our results. In
particular, we would like to explore semantic and
lexical features that are less dependent on linear or-
der and syntactic structure, such as Resnik similarity
and features derived using latent semantic analysis.
We also plan to expand the training input for
the language model and parser to include children?s
speech. The Switchboard corpus is conversational
speech, but it may fail to adequately model many lin-
guistic features characteristic of small children. The
CHILDES database of children?s speech, although
it is not large enough to be used on its own for our
analysis and would require significant manual syn-
tactic annotation, might provide enough data for us
to adapt our models to the child language domain.
Finally, we would like to investigate how infor-
mative the error types are and whether they can be
94
reliably coded by multiple judges. When we exam-
ined the output of our error-type classifier, we no-
ticed that many of the misclassified examples could
be construed, upon closer inspection, as belonging
to multiple error classes. The sentence He?s flying
in a lily-pond, for instance, could contain a devel-
opmental error (i.e., the child has not yet acquired
the correct meaning of in) or a semantic error (i.e.,
the child is using the word flying instead of swim-
ming). Without knowing the context in which the
sentence was uttered, it is not possible to determine
the type of error through any manual or automatic
means. The seemingly large number of misclassifi-
cations of sentences like this indicates the need for
further investigation of the existing coding proce-
dure and in-depth classification error analysis.
8 Conclusions
Our method of automatically identifying error type
shows promise as a supplement to, or substitute for,
the time-consuming and subjective manual coding
process described in Volden and Lord (Volden and
Lord, 1991). However, the superior performance of
our automatically extracted language features sug-
gests that perhaps it may not be the errors them-
selves that characterize the speech of children with
ASD and DLD but rather a preference for certain
structures and word sequences that sometimes mani-
fest themselves as clear language errors. Such varia-
tions in complexity and likelihood might be too sub-
tle for humans to reliably observe.
In summary, the methods explored in this paper
show potential for improving diagnostic discrimina-
tion between typically developing children and those
with these neurodevelopmental disorders. Further
research is required, however, in finding the most re-
liable markers that can be derived from such spoken
language samples.
Acknowledgments
This work was supported in part by NSF Grant
#BCS-0826654; an Innovative Technology for
Autism grant from Autism Speaks; and NIH NIDCD
grant #1R01DC007129-01. Any opinions, findings,
conclusions or recommendations expressed in this
publication are those of the authors and do not nec-
essarily reflect the views of the NSF, Autism Speaks,
or the NIH. Thanks to Meg Mitchell and Cheryl
Greene for their assistance with this project.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC, 4th edition.
Laurence Bartak, Michael Rutter, and Anthony Cox.
1975. A comparative study of infantile autism
and specific developmental receptive language disor-
der. I. The children. British Journal of Psychiatry,
126:27145.
Mariss Ferrara Boston, John Hale, Reinhold Kliegl, and
Shravan Vasishth. 2008. Surprising parser actions
and reading difficulty. In Proceedings of ACL-08:HLT,
Short Papers.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Conference of the
North American Chapter of the ACL, pages 132?139.
Keyur Gabani, Melissa Sherman, Thamar Solorio, and
Yang Liu. 2009. A corpus-based approach for the
prediction of language impairment in monolingual en-
glish and spanish-english bilingual children. In Pro-
ceedings of NAACL-HLT, pages 46?55.
Michael Gamon, Jianfeng Gao, Chris Brockett, and
Re Klementiev. 2008. Using contextual speller tech-
niques and language modeling for ESL error correc-
tion. In Proceedings of IJCNLP.
John J. Godfrey, Edward Holliman, and Jane McDaniel.
1992. SWITCHBOARD: telephone speech corpus for
research and development. In Proceedings of ICASSP,
volume 1, pages 517?520.
John T. Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the 2nd
meeting of NAACL.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Leo Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
Leo Kanner. 1946. Irrelevant and metaphorical lan-
guage. American Journal of Psychiatry, 103:242?246.
Claudia Leacock and Martin Chodorow. 2003. Auto-
mated grammatical error detection. In M.D. Shermis
and J. Burstein, editors, Automated essay scoring: A
cross-disciplinary perspective. Lawrence Erlbaum As-
sociates, Inc., Hillsdale, NJ.
Catherine Lord, Michael Rutter, and Anne LeCouteur.
1994. Autism diagnostic interview-revised: A revised
95
version of a diagnostic interview for caregivers of in-
dividuals with possible pervasive developmental disor-
ders. Journal of Autism and Developmental Disorders,
24:659?685.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Rebecca McCauley. 2001. Assessment of language dis-
orders in children. Lawrence Erlbaum Associates,
Mahwah, NJ.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In Pro-
ceedings of EMNLP, pages 324?333.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristina Hollingshead, and Jeffrey Kaye. in press.
Spoken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Audio,
Speech and Language Processing.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
Michael Rutter. 1965. Speech disorders in a series of
autistic children. In A. Franklin, editor, Children with
communication problems, pages 39?47. Pitman.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual
Meeting of the ACL.
Joanne Volden and Catherine Lord. 1991. Neologisms
and idiosyncratic language in autistic speakers. Jour-
nal of Autism and Developmental Disorders, 21:109?
130.
96
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?10,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
The Utility of Manual and Automatic Linguistic Error Codes
for Identifying Neurodevelopmental Disorders?
Eric Morley, Brian Roark and Jan van Santen
Center for Spoken Language Understanding, Oregon Health & Science University
morleye@gmail.com, roarkbr@gmail.com, vansantj@ohsu.edu
Abstract
We investigate the utility of linguistic features
for automatically differentiating between chil-
dren with varying combinations of two po-
tentially comorbid neurodevelopmental disor-
ders: autism spectrum disorder and specific
language impairment. We find that certain
manual codes for linguistic errors are useful
for distinguishing between diagnostic groups.
We investigate the relationship between cod-
ing detail and diagnostic classification perfor-
mance, and find that a simple coding scheme
is of high diagnostic utility. We propose a sim-
ple method to automate the pared down coding
scheme, and find that these automatic codes
are of diagnostic utility.
1 Introduction
In Autism Spectrum Disorders (ASD), language im-
pairments are common, but not universal (American
Psychiatric Association, 2000). Whether these lan-
guage impairments are distinct from those in Spe-
cific Language Impairment (SLI) is an unresolved
issue (Williams et al, 2008; Kjelgaard and Tager-
Flusberg, 2001). Accurate and detailed characteri-
zation of these impairments is important not only for
resolving this issue, but also for diagnostic practice
and remediation.
Language ability is typically assessed with struc-
tured instruments (?tests?) that elicit brief, easy to
?This research was supported in part by NIH NIDCD award
R01DC012033 and NSF award #0826654. Any opinions, find-
ings, conclusions or recommendations expressed in this publi-
cation are those of the authors and do not reflect the views of
the NIH or NSF. Thanks to Emily Prud?hommeaux for useful
discussion on this topic and help with the data.
score, responses to a sequence of items. For exam-
ple, the CELF-4 includes nineteen multi-item sub-
tests with tasks such as object naming, word defini-
tion, reciting the days of the week, or repeating sen-
tences (Semel et al, 2003). Researchers are begin-
ning to discuss the limits of structured instruments in
terms of which language impairments they tap into
and how well they do so, and are advocating the po-
tential benefits of language sample analysis ? an-
alyzing natural language samples ? to complement
structured assessment, specifically for language as-
sessment in ASD where pragmatic and social com-
munication issues are paramount yet are hard to
assess in a conventional test format (e.g. Tager-
Flusberg et al 2009). However, language sample
analysis faces two labor-intensive steps: transcrip-
tion and detailed coding of the transcripts.
To illustrate the latter, consider the Systematic
Analysis of Language Transcripts (SALT) (Miller
and Chapman, 1985; Miller et al, 2011), which is
the de-facto standard choice by clinicians looking
to code elicited language samples. SALT comprises
a scheme for coding transcripts of recorded speech,
together with software that tallies these codes, com-
putes scores describing utterance length and error
counts, and compares these scores with normative
samples. SALT codes indicate bound morphemes,
edits (which are referred to in the clinical literature
as ?mazes?), and several types of errors in transcripts
of natural language, e.g., omitted or inappropriate
words.
Although this has not been formally documented,
our experience with SALT coding has shown that the
codes vary in terms of: 1) difficulty of manual cod-
ing ? e.g., relatively subtle pragmatic errors versus
overgeneralization or marking bound morphemes;
1
2) utility for identifying particular disorders; and 3)
difficulty of automating the code. This raises an im-
portant question: Is there a combination of codes
that jointly discriminate well between relevant diag-
nostic groups, and at the same time are either easy
to code manually or can in principle be automated?
This paper explores, first, how well the various man-
ual SALT codes classify certain diagnostic groups;
and, second, whether we can automate manual codes
that are of diagnostic utility. Our goal is limited: it
is not the automation of all SALT codes, but the au-
tomation of those that in combination are of high di-
agnostic utility. Automating all SALT codes is sub-
stantially more challenging; yet, we note that even
when some of these codes do not aid in classify-
ing groups, they nevertheless may be of importance
for developing remediation strategies for individual
children. We are particularly interested in the im-
pact of Autism in addition to language impairments
for the utility of particular SALT codes.
The diagnostic groups are carefully chosen to
be pairwise matched either on language abilities or
on autism symptomatology, thus enabling a pre-
cise, ?surgical? determination of the degrees to
which SALT codes reflect language-specific vs.
autism-specific factors. Specifically, the groups in-
clude children with ASD with language impairment
(ALI); ASD with no language impairment (ALN);
SLI alone; and typically developing (TD), which is
strictly defined to exclude any neurodevelopmental
disorder. The TD and ALN groups, as well as the
ALI and SLI groups, are matched on language and
overall cognitive abilities, while the ALN and ALI
groups are matched on autism symptomatology but
not on language and overall cognitive abilities; all
groups are matched on chronological age.
Regarding our algorithmic approach, we note that
automatic detection of relatively subtle errors may
be exceedingly difficult, but perhaps such subtle er-
rors are less critical for diagnosis than more obvi-
ous ones. Most prior work in grammaticality de-
tection in spoken language has focused on special-
ized detectors (e.g., Caines and Buttery 2010; Has-
sanali and Liu 2011), such as mis-use of particular
verb constructions rather than coarser detectors for
the presence of diverse classes of errors. We demon-
strate that these specialized error detectors can break
down when confronted with real world dialogue, and
that in general, the features in these detectors re-
stricts their utility in detecting other sorts of errors.
We implement a detector to automatically extract
coarse SALT codes from an uncoded transcript. This
detector only depends upon part of speech tags, as
opposed to the parse features that are often used in
grammaticality detectors. In most cases, these au-
tomatically extracted codes enable us to distinguish
between diagnostic groups more effectively than do
features that can be extracted trivially from an un-
coded transcript.
As far as we know, researchers have not pre-
viously considered the utility of grammatical er-
ror codes to identify ASD or SLI. Prudhommeaux
and Rouhizadeh (2012), however, found that au-
tomatically extracted pragmatic features are useful
for identifying children with ASD, among children
both with and without SLI. Gabani et al (2009)
found that features derived from language models
are useful for distinguishing between children with
and without a language impairment, both in mono-
lingual English speakers, and in children who are
bilingual in English and Spanish.
Improving the characterization of a child?s lan-
guage impairments is a prerequisite to developing a
sound plan for language training and education for
that child. This paper presents a step in the direction
of effective automated analysis of linguistic samples
that can provide useful information even in the face
of comorbid disorders such as ASD and SLI.
2 Systematic Analysis of Language
Transcripts
Here we give an overview of what SALT requires of
transcriptions, and of SALT coding. The approach
has been in wide use for nearly 30 years (Miller and
Chapman, 1985), and now also exists as a software
package1 providing transcription and coding support
along with tools for aggregating statistics for man-
ual codes over the annotated corpora and comparing
with age norms. The SALT software is not the focus
of this investigation, so we do not discuss it further.
2.1 Basic Transcription
We apply the automated methods to what will be
called basic transcripts. Key for this concept is that,
first, these transcripts do not require linguistic ex-
pertise and thus can be performed by standard tran-
scription services; and, second, that ? as we shall
1http://www.saltsoftware.com/
2
see ? useful features can be automatically computed
from them.
Following the SALT guidelines, a basic transcript
should indicate: the speaker of each utterance, par-
tial words (or stuttering), overlapping speech, unin-
telligible words, and non-speech sounds. It should
be verbatim, regardless of whether a child?s utter-
ance contains neologisms (novel words) or gram-
matical errors (for example ?I goed? should be writ-
ten as such).
A somewhat subtle issue is that SALT prescribes
that the basic transcript be broken into communi-
cation units (which in this paper will be synony-
mous with utterance). Communication units are
defined as ?a main clause with all its dependent
clauses? (Miller et al, 2011). One reason for defin-
ing utterance boundaries with communication units,
rather than turns or sentences, is that in addition to
this being standard practice in language sample anal-
ysis, doing so does not reward children for making
long, but rather simple statements, nor does it penal-
ize children for being interrupted. To illustrate the
first point, the utterance ?I like apples, and bananas,
and pears, and oranges, and grapes.? is one sen-
tence long, but has five communication units (one at
each comma). If the sentence were used as the ba-
sic unit, the utterance would indicate the same level
complexity as the obviously more intricate ?for the
past three years we have lived in an apartment?. In
the basic transcript, each communication unit should
be terminated by one of the following punctuation
marks: ??? if it is a question, ??? if the speaker was
interrupted, ?>? if the speaker abandoned the utter-
ance, and ?.? in all other cases. Thus, the above
example would be transcribed as ?C: I like apples.
. . . C: and grapes.?
2.2 Markup
There are three broad categories of SALT codes: in-
dicators of 1) certain bound morphemes, 2) edits
(discussed below), and 3) errors.
Morphology The following inflectional suffixes
must be coded according to the SALT guidelines:
plural -s (/S), possessive -?s (/Z), possessive plural
-s? (/S/Z), past tense -ed (/ED), 3rd person singular
-s (/3S), progressive -ing (/ING). The following cl-
itics must also be delimited with a ?/?, provided the
resulting root is unmodified in the surface form: n?t,
?t, ?d, ?re, ?s, ?ve. Since these morphemes are only in-
dicated if the root is unmodified in the surface form,
?won?t? will remain unsegmented because ?wo? is
not the root; ?can?t? will be segmented ?can/?T? and
?don?t? will be segmented ?do/N?T?, so as to pre-
serve their respective roots. Nominal or verbal forms
with any of the preceding suffixes or clitics are writ-
ten as the base form with the code appended, for ex-
ample hitting? hit/ING, bases? base/S.
Edits Edits consist of filler words such as ?like?,
?um? and ?uh?, false starts, and revisions. There may
be multiple edits in a single utterance, as well as
multiple adjacent edits. Edits are indicated by paren-
theses, for example: ?(And they like) and she (like)
faint/3S.? Note that in the SALT manual, and the lan-
guage sample analysis literature, edits are referred to
as mazes. We use the term edit here because this is
the more widely used term for this phenomenon in
natural language processing.
Error codes The exact set of error codes used de-
pends upon the clinician?s needs and the errors of
interest. Here we consider several key errors out-
lined in the SALT manual. These error codes and
examples are shown in Table 1. Some of these codes
describe precise classes of errors, for example [EO]
or [OW], but others do not. For example, [EW]
can describe using the wrong verb, tense, preposi-
tion or pronoun (in terms of case, person or gender),
as well as other errors. Note that [EU] (and [EC]) er-
ror codes can occur in grammatical utterances. The
[EU] code marks utterances that are ungrammatical
for reasons not captured by the other error codes, for
example severe problems with word order, or utter-
Table 1: SALT error codes and examples
Code Meaning Example Count in Corpus
[EC] Inappropriate response Did you help yourself stop? Mom[EC]. 9
[EO] Overgeneralization Yeah, cuz I almost saw/ED[EO] one. 229
[EW] Error word I play/ED of[EW] the cat. 1,456
[EU] Utterance-level error You can see it very hard because it/?S under my hair[EU]. 532
[EX] Extraneous word Would you like to be[EX] fall down? 322
[OM] Omitted morpheme The cat eat[OM] fish. 881
[OW] Omitted word He [OW] going now. 770
3
ances which are simply nonsensical, as in Table 1.
3 Evaluation of Manual Codes
In this section we use features extracted from SALT-
coded transcripts for classification. We consider two
different types of features: baseline features, which
are easily derived from a basic transcript; and fea-
tures derived from SALT codes. We investigate
these features to determine which SALT codes are
most worth automating for classification.
3.1 Data
Our data is a collection of 144 transcripts of the
Autism Diagnostic Observation Schedule (ADOS),
which is a semi-structured task that includes an
examiner and a child (Lord et al, 2002). Semi-
structured means that the examiner carries out a
sequence of rigorously specified activities, but her
prompts and questions are not scripted verbatim for
all of them. Detailed guidelines exist for scoring
the ADOS, but these are not considered in the cur-
rent paper. All transcripts have been manually coded
with SALT codes, described in Table 1.
Subjects ranged in age between 4 and 8 years and
were required to be intelligible, to have a full-scale
IQ of greater than 70, and to have a mean length of
utterance (MLU) of at least 3. Diagnoses of ASD
and of SLI followed standard procedures, and were
based on clinical consensus in accordance to diag-
nostic criteria outlined in the DSM-IV (American
Psychiatric Association, 2000). Furthermore, ASD
diagnosis required ADOS and Social Communica-
tion Questionnaire scores (SCQ) (Berument et al,
1999) to meet conventional thresholds. Diagnosis
of SLI required a CELF Core Language Score of at
least 1 standard deviation below the mean, in addi-
tion to exclusion of ASD.
Children were partitioned into pairs of groups
matched on certain key measures. Table 2 shows
these pairs and what they were matched on. The
individuals were selected from the initial pool of
all participants using the algorithm proposed by van
Santen et al (2010), in which, for a given pair of
groups, children are iteratively removed from each
group until there is no significant difference (at p <
0.02) on any measure on which we want the pair to
be matched. We combined some groups into com-
posite groups: ASD (ALI and ALN), nASD (SLI
and TD), LN (?language normal?: ALN and TD),
and LI (?language impaired?: ALI and SLI).
Group 1 Group 2
Group N Group N Matched on
ALI 25 ALN 21 Age, ADOS, SCQ
ALI 24 SLI 19 Age, NVIQ, VIQ
ALN 25 TD 27 Age, NVIQ, VIQ
ASD 48 nASD 61 Age
LN 61 LI 39 Age
SLI 15 TD 38 Age
Table 2: Matched measures for paired groups (ADOS =
ADOS score, NVIQ = non-verbal IQ, VIQ = verbal IQ)
3.2 Features
The term ?feature? will be used to refer to instances
of various classes of SALT codes as well as to in-
stances of other events that can be trivially extracted
from the basic transcripts but do not involve SALT
codes (e.g, the ratio of ?uh? to ?um?). We distinguish
between five levels of features, enumerated in Table
3, that vary in the number and complexity of codes
required. This ranges from the baseline features that
require no manual codes to SALT-5 features that re-
quire full SALT coding. We consider two normal-
ized variants of each feature: one normalized by the
number of utterances spoken by the child, and the
other normalized by the number of words spoken
by the child (except for TKCT). The ratios OCRAT
and UMUHRAT are never normalized. Each feature
level includes all features on lower levels. Finally,
to make our investigation into feature combinations
more tractable, we do not consider combining two
different normalizations of the same feature.
3.3 Classification
We perform six classification tasks in our investi-
gation, according to the paired groups in Table 2:
ALI/ALN; ALI/SLI; ALN/TD; ASD/nASD; LN/LI;
and SLI/TD. We extract various features from the
ADOS transcripts, and then classify the children in
a leave-pair-out (LPO) schema (Cortes et al, 2007)
using the scikit logistic regression classifier with de-
fault parameters (Pedregosa et al, 2011). For LPO
analysis, we iterate over all possible pairs that con-
tain one positive and one negative instance (i.e. chil-
dren with different diagnoses), training on all other
instances, and testing on that pair. We count a trial
as a success if the classifier assigns a higher proba-
bility of being positive to the positive instance than
to the negative instance. We then divide the num-
ber of successes by the number of pairs to get an
unbiased estimate of the area under the receiver op-
erating curve (AUC) (Airola et al, 2011). AUC is
4
Group Feature Description
Baseline CEOLP # of times examiner speaks while child is talking
ECOLP # of times child speaks while examiner is talking
INCCT Incomplete word count
OCRAT Ratio of open- to closed-class words
TKCT Token count
TPCT Type count
UMUHRAT Ratio of ?uh? to ?um?
UINTCT Unintelligible word count
SALT-1 All baseline features +
MPCT Morpheme count
EDITCT Edit count
SALT-2 All SALT-1 features +
NERRUTT Number of utterances with any SALT error codes
SALT-3 All SALT-2 features +
ERRCT Count of SALT error codes
SALT-4 All SALT-3 features +
UTLERRCT Count of utterance level errors (EC / EU)
WDLERRCT Count of word level errors (all other error codes)
SALT-5 All SALT-4 features +
XCT Count of individual error codes (X=EC, EO, . . . ; see Table 1)
Table 3: Features by Level
the probability that the classifier will assign a higher
score to a randomly chosen positive example than to
a randomly chosen negative example.
3.4 Determining Relevant Features
We use a t-test based criterion as a simple way to de-
termine which features to investigate for each clas-
sification task. For a given classification task, we
perform a t-test for independent samples on each
feature under both normalization schemes (if ap-
propriate). We retain a feature for investigation if
that feature is significantly different between the two
groups at the ? = 0.10 level. If a particular feature
varies significantly between groups under both nor-
malization schemes, we retain the version that has
the larger T-statistic. For the sake of brevity, we
do not report all of the features that varied between
groups here, but this data is available upon request
from the authors.
3.5 Initial Feature Ablation
We perform feature ablation to see which features
are most useful for performing each classification
task. Figure 1 shows the maximum performance (in
terms of AUC) over all subsets of features at each
feature level (on the x-axis) on each of the six di-
agnostic classification tasks. Missing values for a
particular level of features for any comparison indi-
cate that no features in that level that passed the t-test
based criterion for the two groups being compared.
Figure 1 illustrates two important points. First,
classification difficulty depends heavily on the pair
that is being compared. For example, the AUC
for ALI/SLI is at most 0.723 (SALT-5), while the
AUC for SLI/TD reaches 0.982 (SALT-5). This is
not surprising, as some pairs, most notably SLI/TD,
differ widely in coarse measures of language abil-
ity (such as non-verbal IQ), while other pairs, in-
cluding ALI/SLI, do not. Second, in many of the
tasks, SALT-derived features are of high utility, but
the biggest gain in classification performance comes
with SALT-2, which is a count of the number of
sentences containing any SALT error code. In fact,
for all but one classification task (ASD/nASD), the
AUC achieved with SALT-2 is at least 96% of the
maximum AUC. Furthermore, the best feature set
using SALT-2 features for most of these tasks is ei-
ther the NERRUTT feature alone, or in the case of
ALI/SLI, NERRUTT and TPCT. These results lead
us to conclude that the most important SALT-derived
feature to code is NERRUTT.
Perhaps surprisingly, Figure 1 also shows that for
ALN/TD and SLI/TD, performance at SALT-1 is
lower than the baseline. There are two reasons for
this, which we explain in turn: 1) the SALT-1 fea-
ture set must include a feature that is less useful than
those in the optimal baseline feature set, and 2) the
classifier will not ignore this feature. MPCT must be
included in SALT-1 for both pairs, because the only
5
Figure 1: Maximum classification performance (AUC) at different feature levels (Bln=Baseline, S-N=SALT-N)
other SALT-1 feature, EDITCT, does not vary signif-
icantly between either ALN/TD or SLI/TD. Further-
more, MPCT is highly correlated with TKCT, yet
TKCT is not in the best baseline feature set for ei-
ther of these pairs. Therefore, the SALT-1 feature
set is required to include a feature that is less useful
than the most useful ones in the baseline set, which
results in lower performance. Once MPCT is in-
cluded in the SALT-1 feature set, the logistic regres-
sion classifier will not ignore it by assigning it a zero
coefficient. This is because MPCT distinguishes be-
tween groups, and because the classifier is trained
at each round of LPO classification to maximize the
likelihood of the training data, rather than the AUC
estimate provided by LPO classification.
3.6 Counting Specific Error Codes
The single feature in SALT-2, NERRUTT, counts
how many utterances spoken by the child contain at
least one SALT error code. Some of these heteroge-
nous errors, for example overgeneralization errors
([EO]), should be straightforward to identify auto-
matically. Automatically identifying others, for ex-
ample utterances that are inappropriate in context
([EC]), would be more difficult. Therefore, before
automating the extraction of NERRUTT, we should
see which errors most need to be identified, and
which can safely be ignored. To do this, we repeat
our LPO classification procedure on various tasks
using SALT-2 features.
We perform the following procedure to identify
the most diagnostically informative errors: for each
subset s of SALT error codes, 1) compute the fea-
ture NERRUTTSUBSET by counting the number of
utterances that contain any of the errors in s; then 2)
perform the LPO diagnostic classification task using
NERRUTTSUBSET as the only feature. The results
of this experiment are in Table 4. The ?% Max? col-
umn shows classification performance when a par-
ticular subset of error codes were counted, relative
to the maximum performance yielded by any subset
of error codes for that particular task. We exclude
the ALN/TD and ASD/nASD tasks from this exper-
iment because NERRUTT does not improve perfor-
mance on these tasks. This is perhaps unsurprising,
because SALT codes were designed to be diagnostic
of SLI, not ASD.
We find that in all tasks, ignoring certain error
codes raises performance. These results also show
that it is not necessary, and indeed not ideal, to iden-
tify utterances containing any SALT code. Identi-
fying utterances that contain any of the following
three codes is sufficient to achieve at least 97% of
the maximum AUC enabled by counting any sub-
set of SALT codes: [EW], [OM], [OW]. For clarity,
NERRUTTMOD is the count of utterances that con-
tain any of those three SALT codes.
Table 4: AUC from Counting Subsets of Errors
Classification Errors Counted AUC % Max
ALI/ALN EW, OM 0.762 100
EW, OM, OW 0.739 97
all 0.724 93
ALI/SLI EW, OM 0.715 100
EW, OM, OW 0.704 98
all 0.676 95
LN/LI EW, OM, OW 0.901 100
all 0.881 98
SLI/TD OM, OW 0.984 100
EW, OM, OW 0.970 99
all 0.951 97
6
3.7 Robustness of NERRUTTMOD feature to
noise: a simulation experiment
We will consider two general ways of automatically
extractingNERRUTTMOD. The first way is to build
a detector to identify utterances that contain at least
one relevant error. The second way is to make de-
tectors for the each relevant error, then combine the
output of these detectors. It is unlikely that any error
detector will perform perfectly. Prior to investiga-
tion of automation strategies, we would like to get an
idea of how much such errors will affect diagnostic
classification performance. To this end, we investi-
gate how well we can perform the diagnostic classi-
fication tasks when noise is deliberately introduced
into the NERRUTTMOD values via simulation.
We consider two scenarios. In the first, we as-
sume a single error detector will be used to extract
NERRUTTMOD. We take each manually coded ut-
terance, then randomly change whether or not that
sentence is counted as having an error to simulate
different precision and recall levels of the automated
NERRUTTMOD extractor. We repeat this procedure
100 times for each classification task, and then ex-
amine the mean AUC over all trials. In the sec-
ond scenario, we assume a detector for each error
code that counts a sentence as having an error any
time one of the detectors fires. We randomly cor-
rupt the detection of each error code considered in
NERRUTTMOD in turn to simulate different preci-
sion and recall levels of each individual error detec-
tor. We assume perfect detection of all errors not
being randomly corrupted. Again, we repeat this
procedure 100 times for each classification task, and
consider the mean AUC over all trials.
In both experiments, and in all classification tasks,
we find that the NERRUTTMOD feature is ex-
tremely robust to noise. For example, finding the
NERRUTTMOD feature with a single detector with
a precision/recall of 0.1/0.3 enables SLI/TD clas-
sification with an average AUC of 0.975, as com-
pared to the maximum AUC of 0.984, enabled by
a perfect detector. When we use a cascaded de-
tector to corrupt each of the two errors counted in
NERRUTTMOD for classifying SLI/TD, so long as
one error is detected perfectly, the other error only
needs to be detected with precision and recall of 0.1
to enable a classification AUC within 0.02 of the
maximum.
The extreme robustness of this feature may appear
surprising, but it is easily explained by the data. The
mean value of NERRUTTMOD for the SLI group
is 7.8 times the mean value of this feature for the
TD group. So long as there is a correlation between
the true value of NERRUTTMOD and the estimated
value, as we have assumed in this experiment, then
the estimated value is bound to be of utility in clas-
sification. This bodes well for the utility of automa-
tion, even for a difficult task of discovering some of
the relatively subtle errors coded in SALT.
4 Automatic Feature Extraction
4.1 Evaluating Hassanali and Liu?s System
Hassanali and Liu developed two grammaticality de-
tectors that they used to identify ungrammatical ut-
terances in transcriptions of speech from children
both with and without language impairments (Has-
sanali and Liu, 2011). They tested their grammati-
cality detectors on the Paradise corpus, which con-
sists of conversations with children elicited during
an investigation of otitis media, a hearing disor-
der. They present both a rule-based and a statis-
tical grammaticality detector. Both detectors con-
sist of sub-detectors for the errors shown in Table
5. The rule-based and statistical detectors perform
well, with the statistical detector outperforming the
rule-based one (F1=0.967 vs. 0.929). The statistical
detector, however, requires each error identified by
any of the sub-detectors to be manually identified in
the training data.
We reimplement both the rule based and statis-
tical detectors proposed by Hassanali and Liu, and
apply it to our data, with three modifications. The
first two are minor: 1) we substitute the Charniak-
Johnson reranking parser (2005) for Charniak?s
original parser (Charniak, 2000), and 2) we use the
scikit multinomial naive bayes classifier (Pedregosa
et al, 2011) instead of the one in WEKA (Hall et al,
2009). The third difference is that we use these de-
tectors to identify SALT error codes rather than the
errors these classifiers were originally built to detect.
The mapping of the original errors to SALT error
codes is given in Table 5. To clarify, if we are train-
ing the ?Missing Verb? detector, then any utterance
with an [OW] code is taken to be a positive exam-
ple. This issue does not present itself with the rule-
based detector because it is not trained. Note that the
two verb agreement features may correspond to ei-
ther [EW] or [OM] SALT codes. For example, ?you
does? would be [EW] because of the otiose 3rd per-
7
Error SALT code
Misuse of -ing participle [EW]
Missing copulae [OW]
Missing verb [OW]
Subject-auxilliary agreement [EW]
Subject-verb agreement [EW]/[OM]
Missing infinitive ?to? [OW]
Table 5: Error detectors proposed by Hassanali and Liu
son singular suffix, while ?he do? would be an [OM]
because it is missing that same suffix.
Hassanali and Liu?s error detectors perform
poorly on our data. Table 6 reports the performance
of their detectors detecting utterances with various
error codes. Five of the six statistical error detec-
tors that Hassanali and Liu proposed are unable to
identify any of the errors in our data. The?misuse
of -ing participle? detector, however, is an excep-
tion, and its performance detecting the analogous
error code [EW], using 10-fold cross validation is,
shown in Table 6. To detect the two pairs of er-
ror codes, [EW][OM] and [OM][OW], and all three
relevant error codes ([EW][OM][OW]), we use the
appropriate rule based detectors. For example, to
detect utterances with either [EW] or [OM] errors,
we pool the detectors for the analogous error codes:
?misuse of -ing participle?, ?subject-auxilliary agree-
ment?, and ?subject-verb agreement?.
There are three factors that may explain the poor
performance observed with most of Hassanali and
Liu?s error detectors when used with our data. The
first is that the three SALT codes we try to detect
([EW], [OM], and [OW]) capture a wider variety of
errors than the six in Hassanali and Liu?s system.
This could account for the low recall. Second, there
are many utterances in our data that Hassanali and
Liu?s system would label an error, but which are not
marked with any SALT error codes. For example, if
the examiner asks the child what she is doing, ?eat-
ing spaghetti? is a faultless response, even though it
is missing both the subject and auxiliary verb. Such
utterances may account for the low precision. Fi-
nally, most of Hassanali and Liu?s sub-detectors de-
pend upon features describing the presence or ab-
sence of specific structures in the parses of the input.
The exception to this is the statistical ?misuse of -ing
participle? detector, which uses part of speech (POS)
tag bigrams and skip bigrams as features. It should
come as no surprise then that the ?misuse of -ing par-
ticiple? is the most robust of these detectors. Indeed,
Codes
System Detected P R F1
Hassanali [EW]? 0.074 0.218 0.110
& Liu [EW][OM]* 0.049 0.277 0.083
[OM][OW]* 0.028 0.191 0.049
All three* 0.066 0.354 0.111
POS-tag [EW] 0.074 0.218 0.110
feature- [OM] 0.070 0.191 0.103
based [OW] 0.064 0.210 0.099
classifier [EW][OM] 0.102 0.269 0.148
[OM][OW] 0.102 0.269 0.148
All three 0.127 0.308 0.180
Table 6: Performance on automatic detection of utter-
ances with certain error codes using Hassanali and Liu?s
detectors, and general POS-tag-feature-based classifier.
? = ?misuse of -ing participle?, statistical; * = rule-based
in what follows, we make use of general POS-tag
features (tag n-gram and skip n-grams) as they do in
this detector, for a general purpose detector not tar-
geted specifically at this particular construction, but
rather to detect the presence of arbitrary given sets
of error tags.
4.2 Automatic SALT error code detection
We compare three types of automatic error code de-
tectors: 1) individual error code detectors; 2) pair
detectors, each of which detects a pair of error codes
included in NERRUTTMOD, following Table 4; and
3) a generic detector that identifies any utterance
containing any of the following SALT codes: [EW],
[OM], or [OW]. We investigate four different fea-
tures, all of which are easily derived from the basic
transcript: bigrams and skip bigrams of words, and
POS tags. We use POS tags extracted from the out-
put of the Charniak-Johnson reranking parser (2005)
(also used in our reimplementation of Hassanali and
Liu?s detectors) for simplicity. We use the Bernoulli
Naive Bayes classifier in scikit with the default set-
tings (Pedregosa et al, 2011).
We find that the word features do not aid clas-
sification in any condition, and that using both bi-
grams and skip bigrams of POS tags improves on
using either alone. We report the performance of
the three types of error detectors in Table 6. These
results are from 10-fold cross-validation using POS
tag bigrams and skip bigrams as features. Note that
the general POS-tag-feature-based classifier uses the
same features as Hassanali and Liu?s statistical ?mis-
use of -ing participle? detector, which is why the
performance for detecting [EW] error codes alone
8
Manual features Automatic extraction
Baseline SALT-2 SALT-2 features
Baseline ? Optimized ?
Diagnoses AUC AUC ? AUC ? AUC
ALI/ALN 0.619? 0.723 0.5 0.611 0.94 0.676
ALI/SLI 0.562 0.686 0.5 0.632 0.99 0.671
LN/LI 0.755 0.881 0.5 0.801 0.50 0.801
SLI/TD 0.840 0.951 0.5 0.805 0.99 0.840
? SALT-1; no significantly different baseline features
Table 7: Diagnostic classification AUC using automatically extracted NERRUTTMOD
is identical between the two systems.
The generic error detector yields higher perfor-
mance than either the individual or pair error detec-
tors. Coding training data for the generic detector is
simpler than doing so for the others because it only
involves a single round of binary coding.
4.3 Diagnostic Classification
We repeat the LPO diagnostic classification tasks
using the automatically extracted NERRUTTMOD
feature. We recompute NERRUTTMOD for each
speaker at each iteration, training on all data except
for the two speakers in the test pair, and the speaker
whose NERRUTTMOD feature we are predicting.
The results from this task are shown in Table 7.
As can be seen in Table 7, diagnostic classifica-
tion performance using the automatically extracted
the NERRUTTMOD feature is markedly lower than
when we extracted this feature from manual codes.
However, raising the probability threshold ? at
which utterances are counted as containing an er-
ror from its default value of 0.5, improves diagnos-
tic classification performance for all but one pair
(LN/LI). This is because increasing the probability
threshold at which we count an utterance as hav-
ing an error improves in NERRUTTMOD detection.
For example, in the ALI/SLI group, using the de-
fault ? = 0.5, and a leave-one-out scenario, we can
automatically extract NERRUTTMOD with a preci-
sion/recall score of 0.19/0.47. When we increase ?
to 0.99, the precision and recall become 0.23/0.24.
Even though there is a massive drop in recall, the
improvement in precision is able to boost diagnostic
classification performance.
In all but one pair (SLI/TD), the automati-
cally extracted NERRUTTMOD feature improves
classification over the baseline, even though the
NERRUTTMOD extractor performs poorly in terms
of intrinsic evaluation, with an F1 score of 0.180.
These results are in line with the experiments per-
forming diagnostic classification with an artificially
noisy NERRUTTMOD feature (see Section 3.7).
These results also demonstrate that the automati-
cally extracted values of NERRUTTMOD are suffi-
ciently correlated with the true values of this feature
to be of some diagnostic utility.
5 Conclusions
We have found that the SALT codes provide use-
ful information for distinguishing between certain
diagnostic groups, but not all of them. Specifi-
cally, and not surprisingly given SALT?s focus on
language disorders and not generally on atypical
language use characteristic of ASD, adding SALT-
derived features to baseline features added little
to ASD/nASD, ALI/SLI, or ALN/TD classifica-
tion accuracy, but added substantially to SLI/TD,
ALI/ALN, and LN/LI classification accuracy. Fur-
thermore, we found that a simplified coding schema
is almost as useful as the complete one for differ-
entiating between these groups. Finally, we have
proposed a simple method to automatically extract
a variant of the most useful SALT-derived feature,
NERRUTTMOD, which is a count of sentences that
contain any of three types of errors (omitted mor-
phemes or words, and generic word-level errors).
Although this feature?s utility degrades when ex-
tracted automatically, it still has considerable dis-
criminative value.
In future work, we will investigate the util-
ity of more sophisticated features for extracting
NERRUTTMOD and other SALT-derived features.
We will also investigate the utility of other linguistic
features, for example parse structure, for the diag-
nostic classification task. Finally, we will also con-
sider whether we can perform the diagnostic classi-
fication task more effectively using cascaded binary
classifiers (for example language impaired vs. lan-
guage normal), as opposed to having a classifier for
every diagnostic pair.
9
References
Antti Airola, Tapio Pahikkala, Willem Waegeman,
Bernard De Baets, and Tapio Salakoski. 2011. An ex-
perimental comparison of cross-validation techniques
for estimating the area under the roc curve. Computa-
tional Statistics & Data Analysis, 55(4):1828?1844.
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washington,
DC, 4th edition.
Sibel Kazak Berument, Michael Rutter, Catherine Lord,
Andrew Pickles, and Anthony Bailey. 1999. Autism
screening questionnaire: diagnostic validity. The
British Journal of Psychiatry, 175(5):444?451.
Andrew Caines and Paula Buttery. 2010. You talking to
me?: A predictive model for zero auxiliary construc-
tions. In Proceedings of the 2010 Workshop on NLP
and Linguistics: Finding the Common Ground, pages
43?51. Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180. Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139. Morgan Kaufmann
Publishers Inc.
Corinna Cortes, Mehryar Mohri, and Ashish Rastogi.
2007. An alternative ranking problem for search en-
gines. In Proceedings of WEA-2007, LNCS 4525,
pages 1?21. Springer-Verlag.
Keyur Gabani, Melissa Sherman, Thamar Solorio, Yang
Liu, Lisa M Bedore, and Elizabeth D Pena. 2009.
A corpus-based approach for the prediction of lan-
guage impairment in monolingual english and spanish-
english bilingual children. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 46?55. Association
for Computational Linguistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
K. Hassanali and Y. Liu. 2011. Measuring language de-
velopment in early childhood education: a case study
of grammar checking in child language transcripts. In
Proceedings of the 6th Workshop on Innovative Use
of NLP for Building Educational Applications, pages
87?95. Association for Computational Linguistics.
Margaret M Kjelgaard and Helen Tager-Flusberg. 2001.
An investigation of language impairment in autism:
Implications for genetic subgroups. Language and
cognitive processes, 16(2-3):287?308.
Catherine Lord, Michael Rutter, PC DiLavore, and Susan
Risi. 2002. Autism diagnostic observation schedule:
ADOS. Western Psychological Services.
J. Miller and R. Chapman. 1985. Systematic analysis of
language transcripts. Madison, WI: Language Analy-
sis Laboratory.
Jon F. Miller, Karen Andriacchi, and Ann Nockerts.
2011. Assessing language production using SALT soft-
ware: A Clinician?s Guide to Language Sample Anal-
ysis. SALT Software, LLC.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
Emily Prudhommeaux and Masoud Rouhizadeh. 2012.
Automatic detection of pragmatic deficits in children
with autism. In Proceedings of the 3rd Workshop on
Child, Computer and Interaction (WOCCI 2012).
Eleanor Messing Semel, Elisabeth Hemmersam Wiig,
and Wayne Secord. 2003. Clinical evaluation of lan-
guage fundamentals. The Psychological Corporation,
A Harcourt Assessment Company, Toronto, Canada,
fourth edition.
Helen Tager-Flusberg, Sally Rogers, Judith Cooper, Re-
becca Landa, Catherine Lord, Rhea Paul, Mabel Rice,
Carol Stoel-Gammon, Amy Wetherby, and Paul Yoder.
2009. Defining spoken language benchmarks and se-
lecting measures of expressive language development
for young children with autism spectrum disorders.
Journal of Speech, Language and Hearing Research,
52(3):643.
Jan PH van Santen, Emily T Prud?hommeaux, Lois M
Black, and Margaret Mitchell. 2010. Computational
prosodic markers for autism. Autism, 14(3):215?236.
David Williams, Nicola Botting, and Jill Boucher. 2008.
Language in autism and specific language impair-
ment: Where are the links? Psychological Bulletin,
134(6):944.
10
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 46?50,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Detecting linguistic idiosyncratic interests in autism
using distributional semantic models
Masoud Rouhizadeh
?
, Emily Prud?hommeaux
?
, Jan van Santen
?
, Richard Sproat
?
?
Center for Spoken Language Understanding, Oregon Health & Science University
?
Center for Language Sciences, University of Rochester
?
Google, Inc.
{rouhizad,vansantj}@ohsu.edu, emilypx@gmail.com, rws@xoba.com
Abstract
Children with autism spectrum disorder
often exhibit idiosyncratic patterns of be-
haviors and interests. In this paper, we fo-
cus on measuring the presence of idiosyn-
cratic interests at the linguistic level in
children with autism using distributional
semantic models. We model the semantic
space of children?s narratives by calculat-
ing pairwise word overlap, and we com-
pare the overlap found within and across
diagnostic groups. We find that the words
used by children with typical development
tend to be used by other children with typ-
ical development, while the words used
by children with autism overlap less with
those used by children with typical devel-
opment and even less with those used by
other children with autism. These findings
suggest that children with autism are veer-
ing not only away from the topic of the
target narrative but also in idiosyncratic
semantic directions potentially defined by
their individual topics of interest.
1 Introduction
Autism spectrum disorder (ASD) is a neurode-
velopmental disorder characterized by impaired
communication and social behavior. One of the
core deficits associated with ASD is an intense
preoccupation with a restricted set of interests
(American Psychiatric Association, 2000; Amer-
ican Psychiatric Association, 2013), which can of-
ten be observed in an individual?s tendency to per-
severate on specific, idiosyncratic topics of con-
versation. Because this symptom is explicitly
mentioned among the diagnostic criteria for ASD
used in the DSM-IV and DSM-5, many diagnos-
tic instruments (Lord et al., 2002; Rutter et al.,
2003) require a qualitative assessment of this phe-
nomenon. Instances of perseveration on a partic-
ular topic in the spontaneous spoken language of
children with ASD, however, are not typically ex-
plicitly counted in a clinical setting, making com-
parisons with typically developing children diffi-
cult to quantify.
Expert manual analysis of conversations and
narratives of individuals with ASD has shown that
children and teenagers with autism include signif-
icantly more bizarre and irrelevant content in their
narratives (Loveland et al., 1990; Losh and Capps,
2003) and introduce more abrupt topic changes in
their conversations (Lam et al., 2012) than their
typically developing peers. Automatic detection
of poor topic maintenance has also been explored
using techniques originally developed for infor-
mation extraction (Rouhizadeh et al., 2013). There
has been little work, however, in annotating the
precise direction of the departure from a target
topic. Thus, it is not clear whether children with
ASD are instigating similar topic changes or pur-
suing idiosyncratic directions in their narratives
and conversations consistent with their restricted
interests.
In this paper, we attempt to automatically iden-
tify topic changes and idiosyncratic interests ex-
pressed in the language of children with ASD
by measuring the semantic similarity of narrative
retellings produced by children with and without
ASD. We first use word overlap measures to cal-
culate the semantic similarity between every pos-
sible pair of narratives. We then build three pair-
wise comparison matrices: one comparing pairs of
typically developing (TD) children; one compar-
ing pairs of children with ASD; and a third com-
46
paring pairs consisting of one child with ASD and
one child with TD. We calculate the significance
of the differences between the pairs in the three
matrices using the Monte Carlo method to shuffle
the diagnosis label of each child.
We find that TD children share the greatest
word overlap with one another, while children
with ASD have significantly less word overlap
with TD children and even less word overlap with
other ASD children. These results indicate that
TD children tend to adhere to the target topic in
the narrative retellings, while children with ASD
often stray from the target topic. Furthermore,
the fact that the word choices of an individual
child with ASD seem not to resemble the word
choices of other children with ASD suggests that
when a child with ASD chooses to abandon the
target topic, he or she does so in an idiosyncratic
way. Although these results are only indirect in-
dications of the presence of restricted interests,
the work presented here highlights the potential of
computational language analysis methods for im-
proving our understanding of the social and lin-
guistic deficits associated with the disorder.
2 Data
Participants in this study included 39 children with
typical development (TD) and 21 children with
autism spectrum disorder (ASD). ASD was di-
agnosed via clinical consensus according to the
DSM-IV-TR criteria (American Psychiatric Asso-
ciation, 2000) and the established threshold scores
on two diagnostic instruments: the Autism Di-
agnostic Observation Schedule (ADOS) (Lord et
al., 2002), a semi-structured series of activities de-
signed to allow an examiner to observe behaviors
associated with autism; and the Social Communi-
cation Questionnaire (SCQ) (Rutter et al., 2003),
a parental questionnaire. None of the children
in this study met the criteria for a language im-
pairment, and there were no significant between-
group differences in age (mean=6.3) or full-scale
IQ (mean=115.5).
The narrative retelling task analyzed here is the
Narrative Memory subtest of the NEPSY (Kork-
man et al., 1998), a large and comprehensive bat-
tery of tasks that test neurocognitive functioning in
children. The NEPSY Narrative Memory (NNM)
subtest is a narrative retelling test in which the sub-
ject listens to a brief narrative about a boy and his
dog and then must retell the narrative to the ex-
aminer. Under standard administration, the NNM
free recall score is calculated by counting how
many from a set of 17 story elements were used
in a retelling. Following the free recall portion of
the test is the cued recall task, in which the ex-
aminer then asks the subject to provide answers to
questions about all of the story elements that were
omitted in the retelling.
The NNM was administered to each participant
in the study, and each participant?s retelling was
recorded and transcribed. The responses for the
cued recall portion of the subtest were not in-
cluded in this work presented here. There was no
significant difference between the two diagnostic
groups in the standard NNM free recall score.
3 Methods
We expect that two different retellings of the same
source will lie in the same lexico-semantic space.
As a result, they should include high percentage
of overlapping words. When a pair of retellings
has a low word overlap measure, it could be that
one or both retellings include intrusions from un-
related topics. An alternative explanation is that
the subjects recalled a non-overlapping set of story
elements or simply a small set of story elements.
However, since we did not find any significant dif-
ference between the TD and ASD groups in the
standard narrative recall score, we infer that a low
percentage of word overlap indicates a difference
in topic between the two retellings.
3.1 Word overlap measures
In order to calculate the similarity between a pair
of narratives i and j, we use type and token over-
lap measures based on the Jaccard similarity coef-
ficient. Token similarity is defined as the size of
intersection of the words (i.e., the actual number
of tokens in common) in narratives i and j relative
to the size of the union of the words in the two
narratives (i.e., summing over all tokens in both
narratives, the maximum number of instances of
that token in either narrative). Type similarity is
defined as the size of intersection of the types (i.e.,
unique words) in narratives i and j relative to the
size of the union of the types in the two narratives.
For instance, for the following set of words i and
j:
i = {a, b, c, d, c}
j = {a, c, e, c, a, a},
the token intersection is equal to {a, c, c} and
47
Group Means
TD.TD TD.ASD ASD.ASD
Type Overlap .23 .17 .13
Token Overlap .19 .14 .11
Table 3: Word overlap pairwise group means
the token union is {a, a, a, c, c, b, e, d}. The token
overlap similarity between the two sets i and j is
therefore 3/8. The type intersection of i and j is
equal to {a, c} and the type union is {a, c, b, e, d},
yielding a type overlap similarity of 2/5.
3.2 Pairwise similarity matrix
We next build a similarity matrix for the type and
token overlap measures, comparing every possi-
ble pair of children. Every child in the TD and
ASD groups is compared to the children in his own
group (TD.TD and ASD.ASD), as well as the chil-
dren in the other group (TD.ASD). The pairwise
similarity matrix is diagonally symmetrical, and
we thus consider only the top right section of the
matrix above the diagonal in our analysis.
3.3 Monte Carlo permutation
Since we may not have enough information to
make an assumption that the pairwise similarity
measures of all children are from a particular dis-
tribution, we utilize a non-parametric procedure,
the Monte Carlo permutation approach, which is
widely used in non-standard significance testing
situations.
Given the three sub-matrices in the similarity
matrix described above (TD.TD, TD.ASD, and
ASD.ASD), we first calculate for each pair of sub-
matrices (e.g., TD.TD vs ASD.ASD) three statis-
tics that compare all cells in one submatrix with
the cells in other submatrices: the difference be-
tween the means, t-statistics (using the Welch
Two Sample t-test), and w-statistics (using the
Wilcoxon rank sum test). We label these observed
values observed-mean, observed-t, and observed-
w. We next take a large random sample with re-
placement from all possible permutations of the
data by shuffling the diagnosis labels of the chil-
dren 1000 times, and then calculate each of the
three above statistics for each shuffle. Finally, we
determine the number of times the observed values
exceed the values generated by the 1000 shuffles.
4 Results
The comparison of the group means of each of
the three sub-matrices described in Section 3.2
show that TD children have the greatest overlap
with each other; children with ASD have less
word overlap with TD children than TD children
have with one another and even less word over-
lap with other ASD children. The group means
of both type and token overlap are summarized
in Table 3. In addition, examples of overlapping
and non-overlapping terms between the groups are
provided in Tables 1 and 2 respectively.
The level plot of the pairwise token overlap
is shown in figure 1. We see that the TD.TD
sub-matrix has the lightest color, indicating higher
overlap, followed by TD.ASD. The ASD.ASD
submatrix has the darkest color, indicating low
word overlap.
In the next step, we determine the significance
of the group mean differences. As described in
Section 3.3, using the Monte Carlo permutation to
test the significance of the following comparisons:
TD.TD vs ASD.ASD, TD.TD vs TD.ASD, and
TD.ASD vs ASD.ASD. The results of these signif-
Group Top 10 overlapping words
TD.TD shoe, tree, climb, ladder, fall, Pepper, Jim, dog, sister, branch
TD.ASD shoe, tree, Jim, climb, dog, ladder, Pepper, fall, branch, sister
ASD.ASD shoe, tree, Jim, dog, climb, Pepper, ladder, branch, boy, run
Table 1: Top 10 overlapping words between the groups
Group Examples of non-overlapping words
TD.TD coconut, couch, jew, lie, picture, spike, stuff, t-rex, tight, watch
TD.ASD arm, bottom, cousin, doctor, eat, fruit, giant, meat, push, sense
ASD.ASD bite, bridge, crunch, donut, gadget, lizard, microphone, sell, table, vision
Table 2: Examples of non-overlapping words between the groups
48
??
??
??
Figure 1: Level plot of the pairwise token overlap
(lighter colors indicate higher overlap)
icance tests are summarized in table 4, and in all
cases the differences are significant at p < 0.05.
5 Conclusions and future work
The methods presented for comparing the lexical
choices made by children with and without ASD
while generating a narrative retelling demonstrate
the utility of language analysis for revealing diag-
nostically interesting information. The low rates
of word overlap between retellings produced by
children with ASD and those produced by typi-
cally developing children suggest that the children
with ASD are having difficulty maintaining the
target topic. Furthermore, the low overlap between
pairs of children with ASD suggests that children
with ASD are not straying from the topic in sim-
ilar ways but are instead exploring topics that are
of idiosyncratic interest.
These findings can be potentially used for
diagnostic purposes in combinations of other
applications of speech and language process-
ing for automated narrative retelling assessment
(Lehr et al., 2013), detection of off-topic words
(Rouhizadeh et al., 2013), and pragmatic deficits
(Prud?hommeaux and Rouhizadeh, 2012). From a
clinical standpoint, diagnostic measures utilizing
these methods for automated evaluation of disor-
dered language could be very useful in diagnosis
and planning interventions.
One major focus of our future work will be to
manually annotate the narrative retellings used in
this study to determine the frequency of topic de-
partures and the nature of these departures. Given
the vocabulary differences seen here, we expect
to find not only that children with ASD are aban-
doning the topic of the source narrative more fre-
quently than children with typical development
but also that the topics they choose to pursue are
related to their own individual specific interests.
A second area we hope to explore is the use
of external resources, such as WordNet, to ex-
pand the set of terms used to calculate word over-
lap. It is perfectly reasonable to expect that people
will use synonyms and paraphrases in their narra-
tive retellings. It is therefore possible that chil-
dren with autism are discussing the appropriate
topic but choosing unusual words within that topic
space in their retellings, which could be consis-
tent with the type of atypical language often ob-
served in children with ASD. By considering se-
mantic overlap rather than simple word overlap,
we may be able to distinguish instances of atypical
language from true examples of poor topic main-
tenance.
Third, we are also interested in applying the
analysis described above to a set of retellings from
seniors with and without mild cognitive impair-
ment, a frequent precursor to dementia. Like chil-
dren with ASD, seniors with dementia are also
more likely to include irrelevant information in
overlap statistic
p-values
TD.TD vs ASD.ASD TD.TD vs TD.ASD TD.ASD vs ASD.ASD
Type Overlap
Means .004 .042 .008
t.test .009 .012 .008
Wilcoxon test .004 .002 .002
Token Overlap
Means .012 .034 .028
t.test .014 .022 .022
Wilcoxon test .012 .002 .002
Table 4: Monte Carlo significance test results
49
their narrative retellings. These intrusions, how-
ever, are often informed by real-world knowledge,
and thus may not result in a decrease in measures
of word overlap with narratives produced by unim-
paired individuals.
Finally, we plan to apply our methods to the out-
put of an automatic speech recognition (ASR) sys-
tem rather than manual transcripts. Although the
ASR output is likely to contain many errors, the
fact that our methods focus on content words may
make them robust to the sorts of function word
recognition errors typically produced by ASR sys-
tems.
Acknowledgments
This work was supported in part by NSF grant
#BCS-0826654, and NIH NIDCD grants #R01-
DC007129 and #1R01DC012033-01. Any opin-
ions, findings, conclusions or recommendations
expressed in this publication are those of the au-
thors and do not necessarily reflect the views of
the NSF or the NIH.
References
American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washing-
ton, DC.
American Psychiatric Association. 2013. Diagnostic
and statistical manual of mental disorders (5th ed.).
American Psychiatric Publishing, Washington, DC.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological as-
sessment. The Psychological Corporation, San An-
tonio.
Yan Grace Lam, Siu Sze, and Susanna Yeung. 2012.
Towards a convergent account of pragmatic lan-
guage deficits in children with high-functioning
autism: Depicting the phenotype using the prag-
matic rating scale. Research in Autism Spectrum
Disorders, 6(2):792?797.
Maider Lehr, Izhak Shafran, Emily Prud?hommeaux,
and Brian Roark. 2013. Discriminative joint model-
ing of lexical variation and acoustic confusion for
automated narrative retelling assessment. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Molly Losh and Lisa Capps. 2003. Narrative ability in
high-functioning children with autism or asperger?s
syndrome. Journal of Autism and Developmental
Disorders, 33(3):239?251.
Katherine Loveland, Robin McEvoy, and Belgin Tu-
nali. 1990. Narrative story telling in autism and
down?s syndrome. British Journal of Developmen-
tal Psychology, 8(1):9?23.
Emily Prud?hommeaux and Masoud Rouhizadeh.
2012. Automatic detection of pragmatic deficits
in children with autism. In Proceedings of the
3rd Workshop on Child, Computer and Interaction
(WOCCI).
Masoud Rouhizadeh, Emily Prud?hommeaux, Brian
Roark, and Jan van Santen. 2013. Distributional
semantic models for the evaluation of disordered
language. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
50
