Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?36,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
CHISPA on the GO
A mobile Chinese-Spanish translation service for travelers in trouble
Jordi Centelles
1,2
, Marta R. Costa-juss
`
a
1,2
and Rafael E. Banchs
2
1
Universitat Polit`ecnica de Catalunya, Barcelona
2
Institute for Infocomm Research, Singapore
{visjcs,vismrc,rembanchs}@i2r.a-star.edu.sg
Abstract
This demo showcases a translation service that
allows travelers to have an easy and convenient
access to Chinese-Spanish translations via a mo-
bile app. The system integrates a phrase-based
translation system with other open source compo-
nents such as Optical Character Recognition and
Automatic Speech Recognition to provide a very
friendly user experience.
1 Introduction
During the last twenty years, Machine Transla-
tion technologies have matured enough to get out
from the academic world and jump into the com-
mercial area. Current commercially available ma-
chine translation services, although still not good
enough to replace human translations, are able to
provide useful and reliable support in certain ap-
plications such as cross-language information re-
trieval, cross-language web browsing and docu-
ment exploration.
On the other hand, the increasing use of smart-
phones, their portability and the availability of in-
ternet almost everywhere, have allowed for lots of
traditional on-line applications and services to be
deployed on these mobile platforms.
In this demo paper we describe ?CHISPA on the
GO? a Chinese-Spanish translation service that in-
tends to provide a portable and easy to use lan-
guage assistance tool for travelers between Chi-
nese and Spanish speaking countries.
The main three characteristics of the presented
demo system are as follows:
? First, the system uses a direct translation be-
tween Chinese and Spanish, rather than using
a pivot language as intermediate step as most
of the current commercial systems do when
dealing with distant languages.
? Second, in addition to support on-line trans-
lations, as other commercial systems, our
system also supports access from mobile
platforms, Android and iOS, by means of na-
tive mobile apps.
? Third, the mobile apps combine the base
translation technology with other supporting
technologies such as Automatic Speech
Recognition (ASR), Optical Character
Recognition (OCR), Image retrieval and
Language detection in order to provide a
friendly user experience.
2 SMT system description
The translation technology used in our system
is based on the well-known phrase-based trans-
lation statistical approach (Koehn et al., 2003).
This approach performs the translation splitting
the source sentence in segments and assigning to
each segment a bilingual phrase from a phrase-
table. Bilingual phrases are translation units that
contain source words and target words, and have
different scores associated to them. These bilin-
gual phrases are then selected in order to max-
imize a linear combination of feature functions.
Such strategy is known as the log-linear model
(Och and Ney, 2002). The two main feature func-
tions are the translation model and the target lan-
guage model. Additional models include lexical
weights, phrase and word penalty and reordering.
2.1 Experimental details
Generally, Chinese-Spanish translation follows
pivot approaches to be translated (Costa-juss`a et
al., 2012) because of the lack of parallel data to
train the direct approach. The main advantage
of our system is that we are using the direct ap-
proach and at the same time we rely on a pretty
large corpus. For Chinese-Spanish, we use (1) the
Holy Bible corpus (Banchs and Li, 2008), (2) the
33
United Nations corpus, which was released for re-
search purposes (Rafalovitch and Dale, 2009), (3)
a small subset of the European Parliament Plenary
Speeches where the Chinese part was syntheti-
cally produced by translating from English, (4) a
large TAUS corpus (TausData, 2013) which comes
from technical translation memories, and (5) an in-
house developed small corpus in the transportation
and hospitality domains. In total we have 70 mil-
lion words.
A careful preprocessing was developed for all
languages. Chinese was segmented with Stanford
segmenter (Tseng et al., 2005) and Spanish was
preprocessed with Freeling (Padr?o et al., 2010).
When Spanish is used as a source language, it is
preprocessed by lower-casing and unaccented the
input. Finally, we use the MOSES decoder (Koehn
et al., 2007) with standard configuration: align-
grow-final-and alignment symmetrization, 5-gram
language model with interpolation and kneser-ney
discount and phrase-smoothing and lexicalized re-
ordering. We use our in-house developed corpus
to optimize because our application is targeted to
the travelers-in-need domain.
3 Web Translator and Mobile
Application
This section describes the main system architec-
ture and the main features of web translator and
the mobile applications.
3.1 System architecture
Figure 1 shows a block diagram of the system ar-
chitecture. Below, we explain the main compo-
nents of the architecture, starting with the back-
end and ending with the front-end.
3.1.1 Back-end
As previously mentioned, our translation system
uses MOSES. More specifically, we use the open
source MOSES server application developed by
Saint-Amand (2013). Because translation tables
need to be kept permanently in memory, we use bi-
nary tables to reduce the memory space consump-
tion. The MOSES server communicates with a PHP
script that is responsible for receiving the query to
be translated and sending the translation back.
For the Chinese-Spanish language pair, we
count with four types of PHP scripts. Two of them
communicate with the web-site and the other two
with the mobile applications. In both cases, one
Figure 1: Block diagram of the system architec-
ture
of the two PHP scripts supports Chinese to Span-
ish translations and the other one the Spanish to
Chinese translations.
The functions of the PHP scripts responsible
for supporting translations are: (1) receive the
Chinese/Spanish queries from the front-end; (2)
preprocess the Chinese/Spanish queries; (3) send
these preprocessed queries to the Chinese/Spanish
to Spanish/Chinese MOSES servers; (4) receive the
translated queries; and (5) send them back to the
front-end.
3.1.2 Front-end
HTML and Javascript constitute the main code
components of the translation website.Another
web development technique used was Ajax, which
allows for asynchronous communication between
the MOSES server and the website. This means that
the website does not need to be refreshed after ev-
ery translation.
The HTTP protocol is used for the communica-
tions between the web and the server. Specifically,
34
we use the POST method, in which the server re-
ceives data through the request message?s body.
The Javascript is used mainly to implement the
input methods of the website, which are a Spanish
keyboard and a Pinyin input method, both open
source and embedded into our code. Also, using
Javascript, a small delay was programmed in order
to automatically send the query to the translator
each time the user stops typing.
Another feature that is worth mentioning is the
support of user feedback to suggest better transla-
tions. Using MYSQL, we created a database in
the server where all user suggestions are stored.
Later, these suggestions can be processed off-line
and used in order to improve the system.
Additionally, all translations processed by the
system are stored in a file. This information is to
be exploited in the near future, when a large num-
ber of translations has been collected, to mine for
the most commonly requested translations. The
most common translation set will be used to im-
plement an index and search engine so that any
query entered by a user, will be first checked
against the index to avoid overloading the trans-
lation engine.
3.2 Android and iphone applications
The android app was programmed with the An-
droid development tools (ADT). It is a plug-in for
the Eclipse IDE that provides the necessary envi-
ronment for building an app.
The Android-based ?CHISPA on the GO? app
is depicted in Figure 2.
For the communication between the Android
app and the server we use the HTTPClient inter-
face. Among other things, it allows a client to
send data to the server via, for instance, the POST
method, as used on the website case.
For the Iphone app we use the xcode software
provided by apple and the programming language
used is Objective C.
In addition to the base translation system, the
app also incorporates Automatic Speech Recogni-
tion (ASR), Optical Character Recognition tech-
nologies as input methods (OCR), Image retrieval
and Language detection.
3.2.1 ASR and OCR
In the case of ASR, we relay on the native ASR
engines of the used mobile platforms: Jelly-bean
in the case of Android
1
and Siri in the case of
1
http://www.android.com/about/jelly-bean/
Figure 2: Android application
iOS
2
. Regarding the OCR implemented technol-
ogy, this is an electronic conversion of scanned
images into machine-encoded text. We adapted
the open-source OCR Tesseract (released under the
Apache license) (Tesseract, 2013).
3.2.2 Image retrieval
For image retrieving, we use the popular website
flickr (Ludicorp, 2004). The image retrieving is
activated with an specific button ?search Image?
button in the app (see Figure 2). Then, an URL
(using the HTTPClient method) is sent to a flickr
server. In the URL we specify the tag (i.e. the
topic of the images we want), the number of im-
ages, the secret key (needed to interact with flickr)
and also the type of object we expect (in our case,
a JSON object). When the server response is re-
ceived, we parse the JSON object. Afterwards,
with the HTTPConnection method and the infor-
mation parsed, we send the URL back to the server
and we retrieve the images requested. Also, the
JAVA class that implements all these methods ex-
tends an AsyncTask in order to not block the
user interface meanwhile is exchanging informa-
tion with the flickr servers.
3.2.3 Language detection
We have also implemented a very simple but ef-
fective language detection system, which is very
suitable for distinguishing between Chinese and
Spanish. Given the type of encoding we are using
2
http://www.apple.com/ios/siri/
35
(UTF-8), codes for most characters used in Span-
ish are in the range from 40 to 255, and codes for
most characters used in Chinese are in the range
from 11,000 and 30,000. Accordingly, we have
designed a simple procedure which computes the
average code for the sequence of characters to be
translated. This average value is compared with a
threshold to determine whether the given sequence
of characters represents a Chinese or a Spanish in-
put.
4 Conclusions
In this demo paper, we described ?CHISPA on
the GO? a translation service that allows travelers-
in-need to have an easy and convenient access to
Chinese-Spanish translations via a mobile app.
The main characteristics of the presented sys-
tem are: the use direct translation between Chi-
nese and Spanish, the support of both website as
well as mobile platforms, and the integration of
supporting input technologies such as Automatic
Speech Recognition, Optical Character Recogni-
tion, Image retrieval and Language detection.
As future work we intend to exploit collected
data to implement an index and search engine for
providing fast access to most commonly requested
translations. The objective of this enhancement is
twofold: supporting off-line mode and alleviating
the translation server load.
Acknowledgments
The authors would like to thank the Universitat
Polit`ecnica de Catalunya and the Institute for In-
focomm Research for their support and permission
to publish this research. This work has been par-
tially funded by the Seventh Framework Program
of the European Commission through the Inter-
national Outgoing Fellowship Marie Curie Action
(IMTraP-2011-29951) and the HLT Department of
the Institute for Infocomm Reseach.
References
R. E. Banchs and H. Li. 2008. Exploring Span-
ish Morphology effects on Chinese-Spanish SMT.
In MATMT 2008: Mixing Approaches to Machine
Translation, pages 49?53, Donostia-San Sebastian,
Spain, February.
M. R. Costa-juss`a, C. A. Henr??quez Q, and R. E.
Banchs. 2012. Evaluating indirect strategies for
chinese-spanish statistical machine translation. J.
Artif. Int. Res., 45(1):761?780, September.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL?03).
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?07), pages
177?180, Prague, Czech Republic, June.
Ludicorp. 2004. Flickr. accessed online May 2013
http://www.flickr.com/.
F.J. Och and H. Ney. 2002. Dicriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL?02), pages 295?302, Philadelphia,
PA, July.
L. Padr?o, M. Collado, S. Reese, M. Lloberes, and
I. Castell?on. 2010. FreeLing 2.1: Five Years of
Open-Source Language Processing Tools. In Pro-
ceedings of 7th Language Resources and Evaluation
Conference (LREC 2010), La Valleta, Malta, May.
A. Rafalovitch and R. Dale. 2009. United Nations
General Assembly Resolutions: A Six-Language
Parallel Corpus. In Proceedings of the MT Summit
XII, pages 292?299, Ottawa.
H. Saint-Amand. 2013. Moses server. accessed
online May 2013 http://www.statmt.org/
moses/?n=Moses.WebTranslation.
TausData. 2013. Taus data. accessed online May 2013
http://www.tausdata.org.
Tesseract. 2013. Ocr. accessed online
May 2013 https://code.google.com/p/
tesseract-ocr/.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and
C. Manning. 2005. A conditional random field
word segmenter. In Fourth SIGHAN Workshop on
Chinese Language Processing.
Appendix: Demo Script Outline
The presenter will showcase the ?CHISPA on the
GO? app by using the three different supported in-
put methods: typing, speech and image. Trans-
lated results will be displayed along with related
pictures of the translated items and/or locations
when available. A poster will be displayed close
to the demo site, which will illustrate the main ar-
chitecture of the platform and will briefly explain
the technology components of it.
36
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 82?86,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
Chinese-to-Spanish rule-based machine translation system
Jordi Centelles
1
and Marta R. Costa-juss
`
a
2
1
Centre de Tecnologies i Aplicacions del llenguatge i la Parla (TALP),
Universitat Polit`ecnica de Catalunya (UPC), Barcelona
2
Centro de Investigaci?on en Computaci?on (CIC), Instituto Polit?ecnico Nacional (IPN), Mexico
1
jordi.centelles.sabater@alu-etsetb.upc.edu,
2
marta@nlp.cic.ipn.mx
Abstract
This paper describes the first freely avail-
able Chinese-to-Spanish rule-based ma-
chine translation system. The system has
been built using the Apertium technology
and combining manual and statistical tech-
niques. Evaluation in different test sets
shows a high coverage between 82-88%.
1 Introduction
Chinese and Spanish are two of the most spoken
languages in the world and they are gaining inter-
est in the actual information society. In this sense,
machine translation (MT) between these two lan-
guages would be clearly of interest for companies,
tourists, students and politicians. Eventhough the
necessity is a fact, there are not many Chinese-
to-Spanish MT systems available in the Internet.
In addition, the translation quality is quite be-
hind the standards. Most of the approaches are
corpus-based and they seem to produce translation
through English pivoting to compensate the lack of
Chinese-Spanish parallel corpora.
When it comes to academic research, there have
been few works in these pair of languages which
mainly are reviewed in Costa-juss`a et al (2012b)
and they also rely on the pivoting procedure.
The linguistic differences (mainly at the level of
morphology) between the two languages makes
the training of data-driven systems rather diffi-
cult. Actually, Chinese and Spanish are languages
with many linguistic differences, especially at the
level of morphology and semantics. Chinese is
an isolating language, which means that there is
a one-to-one correspondence between words and
morphemes. Whereas, Spanish is a fusional lan-
guage, which means that words and morphemes
are mixed together without clear limits. Regarding
semantics, Chinese is a language that has a mas-
sive number of homophonyms at the lexical level
(Zhang et al., 2006). Therefore, lexical semantic
disambiguation towards Spanish will be harder.
Given these challenges, we decided to build
a Chinese-to-Spanish rule-based machine transla-
tion (RBMT) system. These types of systems pro-
vide a translation based on linguistic knowledge
in contrast to the existing and popular corpus-
based approaches. The translation process is di-
vided in: analysis, transfer and generation. Anal-
ysis and generation cover mainly the morpholog-
ical and semantic variations of the languages, the
transfer phase is in charge of the grammatical as-
pects (Hutchins and Sommers, 1992). The main
advantages of RBMT are that they use linguistic
knowledge and the produced errors can be traced.
Among the different linguistic motivations to
build a Chinese-to-Spanish RBMT, we can list the
following: (1) the proposed system will coherently
manage the difference in morphology from Chi-
nese to Spanish; (2) and the RBMT approach is
able to exploit the use of linguistic tools which are
available separately for Chinese and Spanish.
The main drawback of a RBMT system is that
it requires a lot of human dedication and years of
development (Costa-Juss`a et al., 2012a) and that
they exhibit weakness in lexical selection transfer,
which is quite relevant in this pair of languages.
However, in our case, we are using the Apertium
platform (Forcada et al., 2011) that eases the pro-
cess. In addition, when building the proposed
RBMT approach, we use automatic techniques to
feed the system from parallel corpus.
The rest of the paper is organized as follows.
Section 2 reports a detailed description of the
Chinese-to-Spanish RBMT architecture including
the procedure of compiling monolingual and bilin-
gual dictionaries as well as the computation of
grammatical transfer rules. Section 3 reports an
evaluation of the system in terms of coverage. Fi-
nally, Section 4 discusses the results and it draws
the final conclusions.
82
Figure 1: Block diagram of a typical rule-based
machine translation system.
2 Rule-based machine translation
architecture
The general architecture of a RBMT translation
architecture has been defined in the literature in
works such as (Hutchins and Sommers, 1992) or
(Forcada et al., 2011), which is the open-source
toolbox that we are using in this paper. In this sec-
tion, we describe in detail how the system has been
developed following similar procedures as (Cort?es
et al., 2012). Novelties in our work are that we
are aiming a really challenging language pair with
few bilingual speakers capable of developing the
resources required to compile the targeted system.
Human annotation counted with two bilin-
gual English-Spanish annotators and one trilingual
annotator Chinese-English-Spanish, who was in
charge of checking every step out.
2.1 System architecture
The system is based on the Apertium platform
(Forcada et al., 2011) which is a free/open-source
toolbox for shallow transfer machine translation.
As well as the platform, the linguistic data for the
MT systems are also available under the terms of
the GNU GPL.
The platform was originally designed for the
Romance languages of Spain, but it is moving
away from those initial objectives (see the list of
available languages in wiki.apertium.org. In prac-
tice, we use the architecture of the system, but,
differently, we are using statistical techniques to
complete our system.
Figure 1 shows the representative block dia-
gram modules of the RBMT system. In this first
description of the system, the only step that is not
addressed is the lexical transfer.
Development to date has consisted of: feeding
monolingual and bilingual dictionaries, to extend
coverage, with statistical methods and with hu-
man annotation; filtering and cleaning monolin-
gual and bilingual dictionaries to make them con-
sistent; and computing grammatical transfer rules.
Although the monolingual and bilingual dictionar-
ies require the same entries, the function of each
one is different. The monolingual dictionary con-
tains morphological information and the bilingual
dictionary contains the translation entry itself.
This first track of development has taken place
in over the course of five months, which con-
trasts with the long time required to develop clas-
sical RBMT systems. The key point here is that
our system has been developed using a hybrid ap-
proach and that, although the system is capable of
achieving state-of-the-art translations, it is still un-
der construction. The last version of the system is
available for download at the Apertium site
1
.
2.2 Bilingual dictionary
The bilingual dictionary was computed following
two methodologies or procedures.
The first one is manual by using the Yellow
Bridge resource
2
. This web is, as mentioned by
the authors, the premier guide to Chinese language
and culture for English speakers. They provide
comprehensive tools for learning the Chinese lan-
guage. Although there are many Chinese-related
websites, this one is well-organized and complete.
For Chinese, they provide a list of words classified
following grammatical categories, including: ad-
jectives, adverbs, conjunctions, interjections, mea-
sure words, nouns, numerals, onomatopoeia, par-
ticles, prefixes, prepositions, pronouns, question
words, suffixes, time words and different types of
verbs. For each category, each word has its corre-
sponding translation into English. Then, this dic-
tionary was used to feed the dictionary. But to
double-check the translations provided, each word
was translated using another on-line dictionary
3
and Google Translate. This procedure allowed to
add several hundreds of numerals, conjunctions,
adverbs, pronouns, determinants, adjectives, 3,000
nouns and 2,000 verbs.
The second procedure is statistical-based. The
parallel corpus of the United Nations (UN)
(Rafalovitch and Dale, 2009) was aligned at the
1
http://sourceforge.net/projects/
2
http://www.yellowbridge.com/chinese/chinese-parts-of-
speech.php
3
http://www.chinese-tools.com/
83
level of word by using the standard GIZA++ (Och
and Ney, 2003) software. Alignment was per-
formed from source to target and target to source.
Symmetrization was done using intersection be-
cause it provides the most reliable links. Then,
we extracted phrases of length one, which means
that we extracted translations from word-to-word.
This dictionary was manually filtered to eliminate
incorrect entries. This procedure allowed to add
around 3,500 words in the dictionaries. Our dic-
tionary has around 9,000 words.
2.3 Chinese monolingual dictionary
The Chinese monolingual dictionary was ex-
tracted from the source part of the bilingual dic-
tionary. Additionally, it was filtered with regular
expressions to avoid repeated entries.
Regarding the morphological analysis, Chinese
is an isolating language, which in brief means that
words (or symbols) cannot be segmented in sub-
morphemes. In this sense, no morphological anal-
ysis is required. However, the main challenge of
Chinese is that most of the time symbols appear
concatenated and sentences are not segmented into
words as it is most common in other languages.
Therefore, Chinese requires to be segmented. We
used the ZhSeg (Dyer, 2013) programmed in C++.
We evaluated the performance of this segmenter
in comparison to the Left to Right Longest Match
(LRLM), which is the parsing strategy used by
Apertium in analysis mode. This procedure read
tokens from left to right, matching the longest
sequence that is in the dictionary (like ?greedy?
matching of regular expressions). Both ZhSeg
and LRLM were compared using an in-house seg-
mented test set of 456 words as a reference. The
Word Error Rate (WER) measure for the ZhSeg
was 16.56% and 16.89% for LRLM. Given that
results were comparable, we decided to use the
Apertium LRLM strategy.
It is mandatory that the monolingual and the
bilingual dictionary are coherent, which means
that they have to have the same entries. Both
dictionaries were cleaned up with different regu-
lar expressions. Therefore, we have to ensure that
there are not situations like there is a word in the
monolingual dictionary, which is not in the bilin-
gual dictionary and the other way round. In order
to check out this, we used testvoc. As mentioned
in the Apertium documentation
4
, a testvoc is liter-
4
http://wiki.apertium.org/wiki/Testvoc
ally a test of vocabulary. At the most basic level, it
just expands the monolingual dictionary, and runs
each possibly analyzed lexical form through all the
translation stages to see that for each possible in-
put, a sensible translation in the target language is
generated. This tool was used to clean up dictio-
naries.
2.4 Spanish generation
This part of the translator was taken from the
repository of Apertium given that is has been de-
veloped during years. Some previous publications
that explain Spanish generation can by found in
(Armentano-Oller et al., 2006; Corb??-Bellot et al.,
2005). Basically, it consists of the three modules
of Apertium: morphological generator that deliv-
ers a surface (inflected) form for each transferred
word. This is done using the generation dictio-
nary, which for each lemma and part-of-speech
tag is able to generate the final form. Then, the
post-generator that performs orthographic opera-
tions such as contractions (e.g. de el and del).
2.5 Transfer-rules
Grammatical transfer-rules were extracted follow-
ing a manual procedure, which consisted in per-
forming a translation of a source text and contrast-
ing the output translation, the source and the refer-
ence. From this observation, manual patterns were
extracted in order to design a rule that could cover
the necessary modifications to be done. Following
this procedure, there were 28 rules extracted in-
trasyntagms, which modify inside a syntagm, and
34 intersyntagms, which modify among different
sytnagms.
As follows we show an example of rule ex-
tracted intrasyntagm.
< rule comment = RULE : adj nom >
< pattern >
< pattern? itemn = adj/ >
< pattern? itemn = nom/ >
< /pattern >
< action >
< call?macron = f ? concord2 >
< with? parampos = 2/ >
< with? parampos = 1/ >
< /call?macro >
< out >
< chunkname = j ncase = caseFirstWord >
< tags >
< tag >< lit? tagv = SN/ >< /tag >
< tag >< clip pos = 2side = tlpart = gen/ >< /tag >
< tag >< clip pos = 2side = tlpart = nbr/ >< /tag >
< tag >< lit? tagv = p3/ >< /tag >
< /tags >
< lu >
< clip pos = 2side = tlpart = whole/ >
< /lu >
< b pos = 1/ >
< lu >
< clip pos = 1side = tlpart = lem/ >
84
< clip pos = 1side = tlpart = a adj/ >
< clip pos = 1side = tlpart = gen/ >
< clip pos = 1side = tlpart = nbr/ >
< /lu >
< /chunk >
< /out >
< /action >
< /rule >
This rule reorders adjective + noun into noun
+ adjective. Moreover, this rule ensures that the
number and gender of the noun and the adjective
agree.
3 Evaluation framework
This section reports the evaluation framework we
have used to analyze the quality of the Chinese-to-
Spanish RBMT described.
Dataset Domain Words Coverage
Dev News 1,651 88.7
Test UN 35,914 83.8
In-house 10,361 82.8
Table 1: Coverage results.
We can evaluate the rule-based MT systems in
terms of coverage. We are using texts from dif-
ferent domains to perform the evaluation. Do-
mains include news (extracted from the web
56
)
for checking the evolution of the rule-based sys-
tem; a subcorpus of UN (Rafalovitch and Dale,
2009); and an in-house developed small corpus in
the transportation and hospitality domains. To do
the evaluation of coverage we do not need a ref-
erence of translation. Table 1 shows the coverage
results of our system.
This rule-based MT approach can be the base-
line system towards a hybrid architecture. Inspired
in previous promising works (Espa?na-Bonet et al.,
2011), we have identified some ways of building a
hybrid architecture given a rule-based MT system
and available parallel and monolingual corpus:
? Starting with the core of a rule-based system,
there is the necessity of extracting transfer-
rules from parallel corpus and offering a
translation probability to each one. This
would allow to building rule-based MT sys-
tems by a monolingual human linguist. At
the moment, rule-based MT systems have
to be developed by bilingual native linguists
5
http://politics.people.com.cn/n/2013/0709/c1001-
22134594.html
6
http://finance.people.com.cn/n/2013/0722/c1004-
22275982.html
or at least people who are proficient in the
source and target language.
? In order to help rule-based MT systems be
more fluent and natural, it would be nice to
integrate a language model in the generation
step. The language model could be n-gram-
based, syntax-based or trained on neural-
based. In each case, a different decoding
would be required to be integrated in the sys-
tem.
? Additional feature functions as the popular
lexical ones or others that introduce source
context information can be used together
with the above language model.
4 Conclusions and further work
This paper has described the construction of the
first Chinese-to-Spanish open-source RBMT sys-
tem;. Particularly, the human knowledge has been
used for providing exhaustive monolingual and
bilingual dictionaries as well as for defining gram-
matical transfer rules. The statistical knowledge
has complemented the creation of dictionaries.
Therefore, we have shown effective techniques of
building dictionaries using hybrid techniques. The
new RBMT system has shown a high coverage in
different domains.
As future work, the RBMT has to be improved
mainly with new dictionary entries and more com-
plex transfer rules. Both enhancements can be
done combining human and statistical knowledge.
5 Acknowledgements
This work has been partially supported by the
Google Summer of Code and the Seventh Frame-
work Program of the European Commission
through the International Outgoing Fellowship
Marie Curie Action (IMTraP-2011-29951). Au-
thors want to thank Apertium experts that believed
in this project and helped a lot during the devel-
opment, specially Francis Tyers, V??ctor S?anchez-
Cartagena, Filip Petkovsky, Gema Ram??rez and
Mikel Forcada.
85
References
C. Armentano-Oller, R. C. Carrasco, A. M. Corb-
Bellot, M. L. Forcada, M. Ginest??-Rosell, S. Ortiz-
Rojas, J. A. P?erez-Ortiz, G. Ram??rez-S?anchez,
F. S?anchez-Mart??nez, and M. A. Scalco. 2006.
Open-source Portuguese-Spanish machine transla-
tion. In R. Vieira, P. Quaresma, M.d.G.V. Nunes,
N.J. Mamede, C. Oliveira, and M.C. Dias, edi-
tors, Computational Processing of the Portuguese
Language, Proc. of the 7th International Workshop
on Computational Processing of Written and Spo-
ken Portuguese, PROPOR, volume 3960 of Lecture
Notes in Computer Science, pages 50?59. Springer-
Verlag, May.
A. M. Corb??-Bellot, M. L. Forcada, S. Ortiz-Rojas,
J. A. P?erez-Ortiz, G. Ram??rez-S?anchez, F. S?anchez-
Mart??nez, I. Alegria, A. Mayor, and K. Sarasola.
2005. An open-source shallow-transfer machine
translation engine for the romance languages of
spain. In Proceedings of the Tenth Conference of
the European Association for Machine Translation,
pages 79?86, May.
J. P. Mart??nez Cort?es, J. O?Regan, and F. M. Tyers.
2012. Free/open source shallow-transfer based ma-
chine translation for spanish and aragonese. In
LREC, pages 2153?2157.
M. R. Costa-Juss`a, M. Farr?us, J. B. Mari?no, and
J. A. R. Fonollosa. 2012a. Study and compari-
son of rule-based and statistical catalan-spanish ma-
chine translation systems. Computing and Informat-
ics, 31(2):245?270.
M. R. Costa-juss`a, C. A. Henr??quez Q, and R. E.
Banchs. 2012b. Evaluating indirect strategies for
Chinese-Spanish statistical machine translation. J.
Artif. Int. Res., 45(1):761?780.
C. Dyer. 2013. http://code.google.com/p/zhseg/.
C. Espa?na-Bonet, G. Labaka, A
?
. D??az de Ilarraza,
L. M`arquez, and K. Sarasola. 2011. Hybrid ma-
chine translation guided by a rule-based system. In
Proc of the 13th Machine Translation Summit, pages
554?561, Xiamen, China, sep.
M. L. Forcada, M. Ginest??-Rosell, J. Nordfalk,
J. O?Regan, S. Ortiz-Rojas, J. A. P?erez-Ortiz,
F. S?anchez-Mart??nez, G. Ram??rez-S?anchez, and
F. M. Tyers. 2011. Apertium: a free/open-source
platform for rule-based machine translation. Ma-
chine Translation, 25(2):127?144.
W. J. Hutchins and L. Sommers. 1992. An introduc-
tion to machine translation. Academic Press, 362.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
A. Rafalovitch and R. Dale. 2009. United Nations
General Assembly Resolutions: A Six-Language
Parallel Corpus. In Proc. of the MT Summit XII,
pages 292?299, Ottawa.
Y. Zhang, N. Wu, and M. Yip. 2006. Lexical ambigu-
ity resolution in chinese sentence processing. Hand-
book of East Asian Psycholinguistics, 1:268?278.
86
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 79?83,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
English-to-Hindi system description for WMT 2014:
Deep Source-Context Features for Moses
Marta R. Costa-juss
`
a
1
, Parth Gupta
2
, Rafael E. Banchs
3
and Paolo Rosso
2
1
Centro de Investigaci?on en Computaci?on, Instituto Polit?ecnico Nacional, Mexico
2
NLE Lab, PRHLT Research Center, Universitat Polit`ecnica de Val`encia
3
Human Language Technology, Institute for Infocomm Research, Singapore
1
marta@nlp.cic.ipn.mx,
2
{pgupta,prosso}@dsic.upv.es,
3
rembanchs@i2r.a-star.edu.sg
Abstract
This paper describes the IPN-UPV partici-
pation on the English-to-Hindi translation
task from WMT 2014 International Evalu-
ation Campaign. The system presented is
based on Moses and enhanced with deep
learning by means of a source-context fea-
ture function. This feature depends on the
input sentence to translate, which makes
it more challenging to adapt it into the
Moses framework. This work reports the
experimental details of the system putting
special emphasis on: how the feature func-
tion is integrated in Moses and how the
deep learning representations are trained
and used.
1 Introduction
This paper describes the joint participation of the
Instituto Polit?ecnico Nacional (IPN) and the Uni-
versitat Polit`ecnica de Valencia (UPV) in cooper-
ation with Institute for Infocomm Research (I2R)
on the 9th Workshop on Statistical Machine Trans-
lation (WMT 2014). In particular, our participa-
tion was in the English-to-Hindi translation task.
Our baseline system is an standard phrase-
based SMT system built with Moses (Koehn et al.,
2007). Starting from this system we propose to in-
troduce a source-context feature function inspired
by previous works (R. Costa-juss`a and Banchs,
2011; Banchs and Costa-juss`a, 2011). The main
novelty of this work is that the source-context fea-
ture is computed in a new deep representation.
The rest of the paper is organized as follows.
Section 2 presents the motivation of this seman-
tic feature and the description of how the source
context feature function is added to Moses. Sec-
tion 3 explains how both the latent semantic in-
dexing and deep representation of sentences are
used to better compute similarities among source
contexts. Section 4 details the WMT experimental
framework and results, which proves the relevance
of the technique proposed. Finally, section 5 re-
ports the main conclusions of this system descrip-
tion paper.
2 Integration of a deep source-context
feature function in Moses
This section reports the motivation and descrip-
tion of the source-context feature function, to-
gether with the explanation of how it is integrated
in Moses.
2.1 Motivation and description
Source context information in the phrase-based
system is limited to the length of the translation
units (phrases). Also, all training sentences con-
tribute equally to the final translation.
We propose a source-context feature func-
tion which measures the similarity between
the input sentence and all training sen-
tences. In this way, the translation unit
should be extended from source|||target to
source|||target|||trainingsentence, with the
trainingsentence the sentence from which
the source and target phrases were extracted.
The measured similarity is used to favour those
translation units that have been extracted from
training sentences that are similar to the current
sentence to be translated and to penalize those
translation units that have been extracted from
unrelated or dissimilar training sentences as
shown in Figure 2.1.
In the proposed feature, sentence similarity is
measured by means of the cosine distance in a
reduced dimension vector-space model, which is
constructed either by means of standard latent se-
mantic analysis or using deep representation as de-
cribed in section 3.
79
S1: we could not book the room in time
T1: hm smy m\ EVkV aArE?fta nhF\ kr sk\
S2: I want to write the book in time
T2: m{\ smy m\ EktaAb ElKnA cAhtaA h 
Input: i am reading a nice book
book : EktaAb
book : aArE?fta krnA
S2
S1
Input
Figure 1: Illustration of the method
2.2 Integration in Moses
As defined in the section above and, previously,
in (R. Costa-juss`a and Banchs, 2011; Banchs
and Costa-juss`a, 2011), the value of the proposed
source context similarity feature depends on each
individual input sentence to be translated by the
system. We are computing the similarity between
the source input sentence and all the source train-
ing sentences.
This definition implies the feature function de-
pends on the input sentence to be translated. To
implement this requirement, we followed our pre-
vious implementation of an off-line version of the
proposed methodology, which, although very in-
efficient in the practice, allows us to evaluate the
impact of the source-context feature on a state-of-
the-art phrase-based translation system. This prac-
tical implementation follows the next procedure:
1. Two sentence similarity matrices are com-
puted: one between sentences in the develop-
ment and training sets, and the other between
sentences in the test and training datasets.
2. Each matrix entry m
ij
should contain the
similarity score between the i
th
sentence in
the training set and the j
th
sentence in the de-
velopment (or test) set.
3. For each sentence s in the test and develop-
ment sets, a phrase pair list L
S
of all poten-
tial phrases that can be used during decoding
is extracted from the aligned training set.
4. The corresponding source-context similarity
values are assigned to each phrase in lists L
S
according to values in the corresponding sim-
ilarity matrices.
5. Each phrase list L
S
is collapsed into a phrase
table T
S
by removing repetitions (when re-
moving repeated entries in the list, the largest
value of the source-context similarity feature
is retained).
6. Each phrase table is completed by adding
standard feature values (which are computed
in the standard manner).
7. Moses is used on a sentence-per-sentence ba-
sis, using a different translation table for each
development (or test) sentence.
3 Representation of Sentences
We represent the sentences of the source language
in the latent space by means of linear and non-
linear dimensionality reduction techniques. Such
models can be seen as topic models where the low-
dimensional embedding of the sentences represent
conditional latent topics.
3.1 Deep Autoencoders
The non-linear dimensionality reduction tech-
nique we employ is based on the concept of deep
learning, specifically deep autoencoders. Autoen-
coders are three-layer networks (input layer, hid-
den layer and output layer) which try to learn an
identity function. In the neural network represen-
tation of autoencoder (Rumelhart et al., 1986), the
visible layer corresponds to the input layer and
hidden layer corresponds to the latent features.
The autoencoder tries to learn an abstract repre-
sentation of the data in the hidden layer in such
a way that minimizes reconstruction error. When
the dimension of the hidden layer is sufficiently
small, autoencoder is able to generalise and derive
powerful low-dimensional representation of data.
We consider bag-of-words representation of text
sentences where the visible layer is binary feature
vector (v) where v
i
corresponds to the presence
or absence of i
th
word. We use binary restricted
Boltzmann machines to construct an autoencoder
as shown in (Hinton et al., 2006). Latent repre-
sentation of the input sentence can be obtained as
shown below:
p(h|v) = ?(W ? v + b) (1)
where W is the symmetric weight matrix between
visible and hidden layer and b is hidden layer
bias vector and ?(x) is sigmoid logistic function
1/(1 + exp(?x)).
80
Autoencoders with single hidden layer do not
have any advantage over linear methods like
PCA (Bourlard and Kamp, 1988), hence we
consider deep autoencoder by stacking multiple
RBMs on top of each other (Hinton and Salakhut-
dinov, 2006). The autoencoders have always been
difficult to train through back-propagation until
greedy layerwise pre-training was found (Hinton
and Salakhutdinov, 2006; Hinton et al., 2006; Ben-
gio et al., 2006). The pre-training initialises the
network parameters in such a way that fine-tuning
them through back-propagation becomes very ef-
fective and efficient (Erhan et al., 2010).
3.2 Latent Semantic Indexing
Linear dimensionality reduction technique, latent
semantic indexing (LSI) is used to represent sen-
tences in abstract space (Deerwester et al., 1990).
The term-sentence matrix (X) is created where x
ij
denotes the occurrence of i
th
term in j
th
sentence.
Matrix X is factorized using singular value decom-
position (SVD) method to obtain top m principle
components and the sentences are represented in
this m dimensional latent space.
4 Experiments
This section describes the experiments carried out
in the context of WMT 2014. For English-Hindi
the parallel training data was collected by Charles
University and consisted of 3.6M English words
and 3.97M Hindi words. There was a monolingual
corpus for Hindi comming from different sources
which consisted of 790.8M Hindi words. In ad-
dition, there was a development corpus of news
data translated specifically for the task which con-
sisted of 10.3m English words and 10.1m Hindi
words. For internal experimentation we built a
test set extracted from the training set. We se-
lected randomly 429 sentences from the training
corpus which appeared only once, removed them
from training and used them as internal test set.
Monolingual Hindi corpus was used to build a
larger language model. The language model was
computed doing an interpolation of the language
model trained on the Hindi part of the bilingual
corpus (3.97M words) and the language model
trained on the monolingual Hindi corpus (790.8M
words). Interpolation was optimised in the de-
velopment set provided by the organizers. Both
language models interpolated were 5-grams using
Kneser-Ney smoothing.
The preprocessing of the corpus was done with
the standard tools from Moses. English was low-
ercased and tokenized. Hindi was tokenized with
the simple tokenizer provided by the organizers.
We cleaned the corpus using standard parameters
(i.e. we keep sentences between 1 and 80 words
of length).
For training, we used the default Moses op-
tions, which include: the grow-diag-final and
word alignment symmetrization, the lexicalized
reordering, relative frequencies (conditional and
posterior probabilities) with phrase discounting,
lexical weights and phrase bonus for the trans-
lation model (with phrases up to length 10), a
language model (see details below) and a word
bonus model. Optimisation was done using the
MERT algorithm available in Moses. Optimisa-
tion is slow because of the way integration of the
feature function is done that it requires one phrase
table for each input sentence.
During translation, we dropped unknown words
and used the option of minimum bayes risk de-
coding. Postprocessing consisted in de-tokenizing
Hindi using the standard detokenizer of Moses
(the English version).
4.1 Autoencoder training
The architecture of autoencoder we consider was
n-500-128-500-n where n is the vocabulary size.
The training sentences were stemmed, stopwords
were removed and also the terms with sentences
frequency
1
less than 20 were also removed. This
resulted in vocabulary size n=7299.
The RBMs were pretrained using Contrastive
Divergence (CD) with step size 1 (Hinton, 2002).
After pretraining, the RBMs were stacked on top
of each other and unrolled to create deep autoen-
coder (Hinton and Salakhutdinov, 2006). During
the fine-tuning stage, we backpropagated the re-
construction error to update network parameters.
The size of mini-batches during pretraining and
fine-tuning were 25 and 100 respectively. Weight
decay was used to prevent overfitting. Addition-
ally, in order to encourage sparsity in the hid-
den units, Kullback-Leibler sparsity regularization
was used. We used GPU
2
based implementation of
autoencoder to train the models which took around
4.5 hours for full training.
1
total number of training sentences in which the term ap-
pears
2
NVIDIA GeForce GTX Titan with Memory 5 GiB and
2688 CUDA cores
81
4.2 Results
Table 1 shows the improvements in terms of
BLEU of adding deep context over the baseline
system for English-to-Hindi (En2Hi) over devel-
opment and test sets. Adding source-context infor-
mation using deep learning outperforms the latent
semantic analysis methodology.
En2Hi
Dev Test
baseline 9.42 14.99
+lsi 9.83 15.12
+deep context 10.40
?
15.43
?
Table 1: BLEU scores for En2Hi translation task..
?
depicts statistical significance (p-value<0.05).
Our source-context feature function may be
more discriminative in a task like English-to-Hindi
where the target language has a larger vocabulary
than the source one.
Table 2 shows an example of how the translation
is improving in terms of lexical semantics which is
the goal of the methodology presented in the pa-
per. The most frequent sense of word cry is ronA,
which literally means ?to cry? while the example
in Table 2 refers to the sense of cry as cFK, which
means to scream. Our method could identify the
context and hence the source context feature (scf )
of the unit cry|||cFK is higher than for the unit
scf (cry|||ronA) as shown in Table 3 and for this
particular input sentence.
5 Conclusion
This paper reports the IPN-UPV participation in
the WMT 2014 Evaluation Campaign. The system
is Moses-based with an additional feature function
based on deep learning. This feature function in-
troduces source-context information in the stan-
dard Moses system by adding the information of
how similar is the input sentence to the different
training sentences. Significant improvements over
System Translation
Source soft cry from the depth
Baseline ghrAiyo\ s m lAym ron lgta
+deep context ghrAiyo\ s m lAym cFK
Reference ghrAiyo\ s koml cFK
Table 2: Manual analysis of a translation output.
cp pp scf
cry|||ronA 0.23 0.06 0.85
cry|||cFK 0.15 0.04 0.90
Table 3: Probability values (conditional, cp, and
posterior, pp, as standard features in a phrase-
based system) for the word cry and two Hindi
translations.
the baseline system are reported in the task from
English to Hindi.
As further work, we will implement our feature
function in Moses using suffix arrays in order to
make it more efficient.
Acknowledgements
This work has been supported in part by
Spanish Ministerio de Econom??a y Compet-
itividad, contract TEC2012-38939-C03-02 as
well as from the European Regional Develop-
ment Fund (ERDF/FEDER). The work of the
second and fourth authors is also supported
by WIQ-EI (IRSES grant n. 269180) and
DIANA-APPLICATIONS (TIN2012-38603-C02-
01) project and VLC/CAMPUS Microcluster on
Multimodal Interaction in Intelligent Systems.
References
Rafael E. Banchs and Marta R. Costa-juss`a. 2011. A
semantic feature for statistical machine translation.
In Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
SSST-5, pages 126?134.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2006. Greedy layer-wise training
of deep networks. In NIPS, pages 153?160.
Herv?e Bourlard and Yves Kamp. 1988. Auto-
association by multilayer perceptrons and singu-
lar value decomposition. Biological Cybernetics,
59(4):291?294, September.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Dumitru Erhan, Yoshua Bengio, Aaron C. Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research, 11:625?660.
82
Geoffrey Hinton and Ruslan Salakhutdinov. 2006. Re-
ducing the dimensionality of data with neural net-
works. Science, 313(5786):504 ? 507.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Computation, 18(7):1527?1554.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Computation, 14(8):1771?1800.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180.
Marta R. Costa-juss`a and Rafael E. Banchs. 2011. The
bm-i2r haitian-cr?eole-to-english translation system
description for the wmt 2011 evaluation campaign.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 452?456, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533?536.
83
