Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 123?131,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatically Extracting Polarity-Bearing Topics for Cross-Domain
Sentiment Classification
Yulan He Chenghua Lin? Harith Alani
Knowledge Media Institute, The Open University
Milton Keynes MK7 6AA, UK
{y.he,h.alani}@open.ac.uk
? School of Engineering, Computing and Mathematics
University of Exeter, Exeter EX4 4QF, UK
cl322@exeter.ac.uk
Abstract
Joint sentiment-topic (JST) model was previ-
ously proposed to detect sentiment and topic
simultaneously from text. The only super-
vision required by JST model learning is
domain-independent polarity word priors. In
this paper, we modify the JST model by in-
corporating word polarity priors through mod-
ifying the topic-word Dirichlet priors. We
study the polarity-bearing topics extracted by
JST and show that by augmenting the original
feature space with polarity-bearing topics, the
in-domain supervised classifiers learned from
augmented feature representation achieve the
state-of-the-art performance of 95% on the
movie review data and an average of 90% on
the multi-domain sentiment dataset. Further-
more, using feature augmentation and selec-
tion according to the information gain criteria
for cross-domain sentiment classification, our
proposed approach performs either better or
comparably compared to previous approaches.
Nevertheless, our approach is much simpler
and does not require difficult parameter tun-
ing.
1 Introduction
Given a piece of text, sentiment classification aims
to determine whether the semantic orientation of the
text is positive, negative or neutral. Machine learn-
ing approaches to this problem (?; ?; ?; ?; ?; ?) typ-
ically assume that classification models are trained
and tested using data drawn from some fixed distri-
bution. However, in many practical cases, we may
have plentiful labeled examples in the source do-
main, but very few or no labeled examples in the
target domain with a different distribution. For ex-
ample, we may have many labeled books reviews,
but we are interested in detecting the polarity of
electronics reviews. Reviews for different produces
might have widely different vocabularies, thus clas-
sifiers trained on one domain often fail to produce
satisfactory results when shifting to another do-
main. This has motivated much research on sen-
timent transfer learning which transfers knowledge
from a source task or domain to a different but re-
lated task or domain (?; ?; ?; ?).
Joint sentiment-topic (JST) model (?; ?) was ex-
tended from the latent Dirichlet alocation (LDA)
model (?) to detect sentiment and topic simultane-
ously from text. The only supervision required by
JST learning is domain-independent polarity word
prior information. With prior polarity words ex-
tracted from both the MPQA subjectivity lexicon1
and the appraisal lexicon2, the JST model achieves
a sentiment classification accuracy of 74% on the
movie review data3 and 71% on the multi-domain
sentiment dataset4. Moreover, it is also able to ex-
tract coherent and informative topics grouped under
different sentiment. The fact that the JST model
does not required any labeled documents for training
makes it desirable for domain adaptation in senti-
ment classification. Many existing approaches solve
the sentiment transfer problem by associating words
1http://www.cs.pitt.edu/mpqa/
2http://lingcog.iit.edu/arc/appraisal_
lexicon_2007b.tar.gz
3http://www.cs.cornell.edu/people/pabo/
movie-review-data
4http://www.cs.jhu.edu/?mdredze/
datasets/sentiment/index2.html
123
from different domains which indicate the same sen-
timent (?; ?). Such an association mapping problem
can be naturally solved by the posterior inference in
the JST model. Indeed, the polarity-bearing topics
extracted by JST essentially capture sentiment asso-
ciations among words from different domains which
effectively overcome the data distribution difference
between source and target domains.
The previously proposed JST model uses the sen-
timent prior information in the Gibbs sampling in-
ference step that a sentiment label will only be sam-
pled if the current word token has no prior sentiment
as defined in a sentiment lexicon. This in fact im-
plies a different generative process where many of
the word prior sentiment labels are observed. The
model is no longer ?latent?. We propose an alter-
native approach by incorporating word prior polar-
ity information through modifying the topic-word
Dirichlet priors. This essentially creates an informed
prior distribution for the sentiment labels and would
allow the model to actually be latent and would be
consistent with the generative story.
We study the polarity-bearing topics extracted by
the JST model and show that by augmenting the
original feature space with polarity-bearing topics,
the performance of in-domain supervised classifiers
learned from augmented feature representation im-
proves substantially, reaching the state-of-the-art re-
sults of 95% on the movie review data and an aver-
age of 90% on the multi-domain sentiment dataset.
Furthermore, using simple feature augmentation,
our proposed approach outperforms the structural
correspondence learning (SCL) (?) algorithm and
achieves comparable results to the recently proposed
spectral feature alignment (SFA) method (?). Never-
theless, our approach is much simpler and does not
require difficult parameter tuning.
We proceed with a review of related work on
sentiment domain adaptation. We then briefly de-
scribe the JST model and present another approach
to incorporate word prior polarity information into
JST learning. We subsequently show that words
from different domains can indeed be grouped un-
der the same polarity-bearing topic through an illus-
tration of example topic words extracted by JST be-
fore proposing a domain adaptation approach based
on JST. We verify our proposed approach by con-
ducting experiments on both the movie review data
and the multi-domain sentiment dataset. Finally, we
conclude our work and outline future directions.
2 Related Work
There has been significant amount of work on algo-
rithms for domain adaptation in NLP. Earlier work
treats the source domain data as ?prior knowledge?
and uses maximum a posterior (MAP) estimation to
learn a model for the target domain data under this
prior distribution (?). Chelba and Acero (?) also
uses the source domain data to estimate prior dis-
tribution but in the context of a maximum entropy
(ME) model. The ME model has later been studied
in (?) for domain adaptation where a mixture model
is defined to learn differences between domains.
Other approaches rely on unlabeled data in the
target domain to overcome feature distribution dif-
ferences between domains. Motivated by the alter-
nating structural optimization (ASO) algorithm (?)
for multi-task learning, Blitzer et al (?) proposed
structural correspondence learning (SCL) for do-
main adaptation in sentiment classification. Given
labeled data from a source domain and unlabeled
data from target domain, SCL selects a set of pivot
features to link the source and target domains where
pivots are selected based on their common frequency
in both domains and also their mutual information
with the source labels.
There has also been research in exploring care-
ful structuring of features for domain adaptation.
Daume? (?) proposed a kernel-mapping function
which maps both source and target domains data to
a high-dimensional feature space so that data points
from the same domain are twice as similar as those
from different domains. Dai et al(?) proposed trans-
lated learning which uses a language model to link
the class labels to the features in the source spaces,
which in turn is translated to the features in the
target spaces. Dai et al (?) further proposed us-
ing spectral learning theory to learn an eigen fea-
ture representation from a task graph representing
features, instances and class labels. In a similar
vein, Pan et al (?) proposed the spectral feature
alignment (SFA) algorithm where some domain-
independent words are used as a bridge to con-
struct a bipartite graph to model the co-occurrence
relationship between domain-specific words and
domain-independent words. Feature clusters are
124
generated by co-align domain-specific and domain-
independent words.
Graph-based approach has also been studied in
(?) where a graph is built with nodes denoting
documents and edges denoting content similarity
between documents. The sentiment score of each
unlabeled documents is recursively calculated until
convergence from its neighbors the actual labels of
source domain documents and pseudo-labels of tar-
get document documents. This approach was later
extended by simultaneously considering relations
between documents and words from both source and
target domains (?).
More recently, Seah et al (?) addressed the issue
when the predictive distribution of class label given
input data of the domains differs and proposed Pre-
dictive Distribution Matching SVM learn a robust
classifier in the target domain by leveraging the la-
beled data from only the relevant regions of multiple
sources.
3 Joint Sentiment-Topic (JST) Model
Assume that we have a corpus with a collection ofD
documents denoted by C = {d1, d2, ..., dD}; each
document in the corpus is a sequence of Nd words
denoted by d = (w1, w2, ..., wNd), and each word
in the document is an item from a vocabulary index
with V distinct terms denoted by {1, 2, ..., V }. Also,
let S be the number of distinct sentiment labels, and
T be the total number of topics. The generative
process in JST which corresponds to the graphical
model shown in Figure ??(a) is as follows:
? For each document d, choose a distribution
pid ? Dir(?).
? For each sentiment label l under document d,
choose a distribution ?d,l ? Dir(?).
? For each word wi in document d
? choose a sentiment label li ? Mult(pid),
? choose a topic zi ? Mult(?d,li),
? choose a word wi from ?lizi , a Multino-
mial distribution over words conditioned
on topic zi and sentiment label li.
Gibbs sampling was used to estimate the posterior
distribution by sequentially sampling each variable
of interest, zt and lt here, from the distribution over
w
 
!
 
z"
NdS*T
# $
D
l
S
(a) JST model.
w
 
!
 
z"
NdS*T
# $
D
l
SS
!
S
(b) Modified JST model.
Figure 1: JST model and its modified version.
that variable given the current values of all other
variables and data. Letting the superscript ?t de-
note a quantity that excludes data from tth position,
the conditional posterior for zt and lt by marginaliz-
ing out the random variables ?, ?, and pi is
P (zt = j, lt = k|w, z?t, l?t, ?, ?, ?) ?
N?twt,j,k + ?
N?tj,k + V ?
?
N?tj,k,d + ?j,k
N?tk,d +
?
j ?j,k
?
N?tk,d + ?
N?td + S?
. (1)
where Nwt,j,k is the number of times word wt ap-
peared in topic j and with sentiment label k, Nj,k
is the number of times words assigned to topic j
and sentiment label k, Nj,k,d is the number of times
a word from document d has been associated with
topic j and sentiment label k, Nk,d is the number of
times sentiment label k has been assigned to some
word tokens in document d, andNd is the total num-
ber of words in the document collection.
In the modified JST model as shown in Fig-
ure ??(b), we add an additional dependency link of
? on the matrix ? of size S?V which we use to en-
code word prior sentiment information into the JST
model. For each word w ? {1, ..., V }, if w is found
in the sentiment lexicon, for each l ? {1, ..., S}, the
element ?lw is updated as follows
?lw =
{
1 if S(w) = l
0 otherwise
, (2)
where the function S(w) returns the prior sentiment
label of w in a sentiment lexicon, i.e. neutral, posi-
125
Book DVD Book Elec. Book Kitch. DVD Elec. DVD Kitch. Elec. Kitch.
P
os
.
recommend funni interest pictur interest qualiti concert sound movi recommend sound pleas
highli cool topic clear success easili rock listen stori highli excel look
easi entertain knowledg paper polit servic favorit bass classic perfect satisfi worth
depth awesom follow color clearli stainless sing amaz fun great perform materi
strong worth easi accur popular safe talent acoust charact qulati comfort profession
N
eg
.
mysteri cop abus problem bore return bore poorli horror cabinet tomtom elimin
fbi shock question poor tediou heavi plot low alien break region regardless
investig prison mislead design cheat stick stupid replac scari install error cheapli
death escap point case crazi defect stori avoid evil drop code plain
report dirti disagre flaw hell mess terribl crap dead gap dumb incorrect
Table 1: Extracted polarity words by JST on the combined data sets.
tive or negative.
The matrix ? can be considered as a transforma-
tion matrix which modifies the Dirichlet priors ? of
size S ? T ? V , so that the word prior polarity can
be captured. For example, the word ?excellent? with
index i in the vocabulary has a positive polarity. The
corresponding row vector in ? is [0, 1, 0] with its el-
ements representing neutral, positive, and negative.
For each topic j, multiplying ?li with ?lji, only the
value of ?lposji is retained, and ?lneuji and ?lnegji
are set to 0. Thus, the word ?excellent? can only
be drawn from the positive topic word distributions
generated from a Dirichlet distribution with param-
eter ?lpos .
4 Polarity Words Extracted by JST
The JST model allows clustering different terms
which share similar sentiment. In this section, we
study the polarity-bearing topics extracted by JST.
We combined reviews from the source and target
domains and discarded document labels in both do-
mains. There are a total of six different combi-
nations. We then run JST on the combined data
sets and listed some of the topic words extracted as
shown in Table ??. Words in each cell are grouped
under one topic and the upper half of the table shows
topic words under the positive sentiment label while
the lower half shows topic words under the negative
sentiment label.
We can see that JST appears to better capture sen-
timent association distribution in the source and tar-
get domains. For example, in the DVD+Elec. set,
words from the DVD domain describe a rock con-
cert DVD while words from the Electronics domain
are likely relevant to stereo amplifiers and receivers,
and yet they are grouped under the same topic by the
JST model. Checking the word coverage in each do-
main reveals that for example ?bass? seldom appears
in the DVD domain, but appears more often in the
Electronics domain. Likewise, in the Book+Kitch.
set, ?stainless? rarely appears in the Book domain
and ?interest? does not occur often in the Kitchen
domain and they are grouped under the same topic.
These observations motivate us to explore polarity-
bearing topics extracted by JST for cross-domain
sentiment classification since grouping words from
different domains but bearing similar sentiment has
the effect of overcoming the data distribution differ-
ence of two domains.
5 Domain Adaptation using JST
Given input data x and a class label y, labeled pat-
terns of one domain can be drawn from the joint
distribution P (x, y) = P (y|x)P (x). Domain adap-
tation usually assume that data distribution are dif-
ferent in source and target domains, i.e., Ps(x) 6=
Pt(x). The task of domain adaptation is to predict
the label yti corresponding to x
t
i in the target domain.
We assume that we are given two sets of training
data, Ds and Dt, the source domain and target do-
main data sets, respectively. In the multiclass clas-
sification problem, the source domain data consist
of labeled instances, Ds = {(xsn; y
s
n) ? X ? Y :
1 ? n ? N s}, where X is the input space and Y
is a finite set of class labels. No class label is given
in the target domain, Dt = {xtn ? X : 1 ? n ?
N t, N t  N s}. Algorithm ?? shows how to per-
form domain adaptation using the JST model. The
source and target domain data are first merged with
document labels discarded. A JST model is then
126
learned from the merged corpus to generate polarity-
bearing topics for each document. The original doc-
uments in the source domain are augmented with
those polarity-bearing topics as shown in Step 4 of
Algorithm ??, where li zi denotes a combination of
sentiment label li and topic zi for word wi. Finally,
feature selection is performed according to the infor-
mation gain criteria and a classifier is then trained
from the source domain using the new document
representations. The target domain documents are
also encoded in a similar way with polarity-bearing
topics added into their feature representations.
Algorithm 1 Domain adaptation using JST.
Input: The source domain data Ds = {(xsn; y
s
n) ? X ?
Y : 1 ? n ? Ns}, the target domain data, Dt =
{xtn ? X : 1 ? n ? N
t, N t  Ns}
Output: A sentiment classifier for the target domain Dt
1: Merge Ds and Dt with document labels discarded,
D = {(xsn, 1 ? n ? N
s;xtn, 1 ? n ? N
t}
2: Train a JST model on D
3: for each document xsn = (w1, w2, ..., wm) ? D
s do
4: Augment document with polarity-bearing topics
generated from JST,
xs
?
n = (w1, w2, ..., wm, l1 z1, l2 z2, ..., lm zm)
5: Add {xs
?
n ; y
s
n} into a document pool B
6: end for
7: Perform feature selection using IG on B
8: Return a classifier, trained on B
As discussed in Section ?? that the JST model di-
rectly models P (l|d), the probability of sentiment
label given document, and hence document polar-
ity can be classified accordingly. Since JST model
learning does not require the availability of docu-
ment labels, it is possible to augment the source do-
main data by adding most confident pseudo-labeled
documents from the target domain by the JST model
as shown in Algorithm ??.
6 Experiments
We evaluate our proposed approach on the two
datasets, the movie review (MR) data and the multi-
domain sentiment (MDS) dataset. The movie re-
view data consist of 1000 positive and 1000 neg-
ative movie reviews drawn from the IMDB movie
archive while the multi-domain sentiment dataset
contains four different types of product reviews ex-
tracted from Amazon.com including Book, DVD,
Electronics and Kitchen appliances. Each category
Algorithm 2 Adding pseudo-labeled documents.
Input: The target domain data, Dt = {xtn ? X :
1 ? n ? N t, N t  N s}, document sentiment
classification threshold ?
Output: A labeled document pool B
1: Train a JST model parameterized by ? on Dt
2: for each document xtn ? D
t do
3: Infer its sentiment class label from JST as
ln = arg maxs P (l|xtn; ?)
4: if P (ln|xtn; ?) > ? then
5: Add labeled sample (xtn, ln) into a docu-
ment pool B
6: end if
7: end for
of product reviews comprises of 1000 positive and
1000 negative reviews and is considered as a do-
main. Preprocessing was performed on both of the
datasets by removing punctuation, numbers, non-
alphabet characters and stopwords. The MPQA sub-
jectivity lexicon is used as a sentiment lexicon in our
experiments.
6.1 Experimental Setup
While the original JST model can produce reason-
able results with a simple symmetric Dirichlet prior,
here we use asymmetric prior ? over the topic pro-
portions which is learned directly from data using a
fixed-point iteration method (?).
In our experiment, ? was updated every 25 itera-
tions during the Gibbs sampling procedure. In terms
of other priors, we set symmetric prior ? = 0.01 and
? = (0.05?L)/S, where L is the average document
length, and the value of 0.05 on average allocates 5%
of probability mass for mixing.
6.2 Supervised Sentiment Classification
We performed 5-fold cross validation for the per-
formance evaluation of supervised sentiment clas-
sification. Results reported in this section are av-
eraged over 10 such runs. We have tested several
classifiers including Na??ve Bayes (NB) and support
vector machines (SVMs) from WEKA5, and maxi-
mum entropy (ME) from MALLET6. All parameters
are set to their default values except the Gaussian
5http://www.cs.waikato.ac.nz/ml/weka/
6http://mallet.cs.umass.edu/
127
prior variance is set to 0.1 for the ME model train-
ing. The results show that ME consistently outper-
forms NB and SVM on average. Thus, we only re-
port results from ME trained on document vectors
with each term weighted according to its frequency.
85
90
95
100
ccu
rac
y (%
)
Movie Review Book DVD Electronics Kitchen
75
80
1 5 10 15 30 50 100 150 200
Acc
ura
cy (
%)
No. of Topics
Figure 2: Classification accuracy vs. no. of topics.
The only parameter we need to set is the number
of topics T . It has to be noted that the actual num-
ber of feature clusters is 3 ? T . For example, when
T is set to 5, there are 5 topic groups under each
of the positive, negative, or neutral sentiment labels
and hence there are altogether 15 feature clusters.
The generated topics for each document from the
JST model were simply added into its bag-of-words
(BOW) feature representation prior to model train-
ing. Figure ?? shows the classification results on the
five different domains by varying the number of top-
ics from 1 to 200. It can be observed that the best
classification accuracy is obtained when the number
of topics is set to 1 (or 3 feature clusters). Increas-
ing the number of topics results in the decrease of
accuracy though it stabilizes after 15 topics. Never-
theless, when the number of topics is set to 15, us-
ing JST feature augmentation still outperforms ME
without feature augmentation (the baseline model)
in all of the domains. It is worth pointing out that
the JST model with single topic becomes the stan-
dard LDA model with only three sentiment topics.
Nevertheless, we have proposed an effective way to
incorporate domain-independent word polarity prior
information into model learning. As will be shown
later in Table ?? that the JST model with word po-
larity priors incorporated performs significantly bet-
ter than the LDA model without incorporating such
prior information.
For comparison purpose, we also run the LDA
model and augmented the BOW features with the
Method MR
MDS
Book DVD Elec. Kitch.
Baseline 82.53 79.96 81.32 83.61 85.82
LDA 83.76 84.32 85.62 85.4 87.68
JST 94.98 89.95 91.7 88.25 89.85
[YE10] 91.78 82.75 82.85 84.55 87.9
[LI10] - 79.49 81.65 83.64 85.65
Table 2: Supervised sentiment classification accuracy.
generated topics in a similar way. The best accu-
racy was obtained when the number of topics is set
to 15 in the LDA model. Table ?? shows the clas-
sification accuracy results with or without feature
augmentation. We have performed significance test
and found that LDA performs statistically signifi-
cant better than Baseline according to a paired t-test
with p < 0.005 for the Kitchen domain and with
p < 0.001 for all the other domains. JST performs
statistically significant better than both Baseline and
LDA with p < 0.001.
We also compare our method with other recently
proposed approaches. Yessenalina et al (?) ex-
plored different methods to automatically generate
annotator rationales to improve sentiment classifica-
tion accuracy. Our method using JST feature aug-
mentation consistently performs better than their ap-
proach (denoted as [YE10] in Table ??). They fur-
ther proposed a two-level structured model (?) for
document-level sentiment classification. The best
accuracy obtained on the MR data is 93.22% with
the model being initialized with sentence-level hu-
man annotations, which is still worse than ours. Li
et al (?) adopted a two-stage process by first clas-
sifying sentences as personal views and impersonal
views and then using an ensemble method to per-
form sentiment classification. Their method (de-
noted as [LI10] in Table ??) performs worse than ei-
ther LDA or JST feature augmentation. To the best
of our knowledge, the results achieved using JST
feature augmentation are the state-of-the-art for both
the MR and the MDS datasets.
6.3 Domain Adaptation
We conducted domain adaptation experiments on
the MDS dataset comprising of four different do-
mains, Book (B), DVD (D), Electronics (E), and
Kitchen appliances (K). We randomly split each do-
128
main data into a training set of 1,600 instances and a
test set of 400 instances. A classifier trained on the
training set of one domain is tested on the test set of
a different domain. We preformed 5 random splits
and report the results averaged over 5 such runs.
Comparison with Baseline Models
We compare our proposed approaches with two
baseline models. The first one (denoted as ?Base? in
Table ??) is an ME classifier trained without adapta-
tion. LDA results were generated from an ME clas-
sifier trained on document vectors augmented with
topics generated from the LDA model. The number
of topics was set to 15. JST results were obtained
in a similar way except that we used the polarity-
bearing topics generated from the JST model. We
also tested with adding pseudo-labeled examples
from the JST model into the source domain for ME
classifier training (following Algorithm ??), denoted
as ?JST-PL? in Table ??. The document sentiment
classification probability threshold ? was set to 0.8.
Finally, we performed feature selection by selecting
the top 2000 features according to the information
gain criteria (?JST-IG?)7.
There are altogether 12 cross-domain sentiment
classification tasks. We showed the adaptation loss
results in Table ?? where the result for each domain
and for each method is averaged over all three pos-
sible adaptation tasks by varying the source domain.
The adaptation loss is calculated with respect to the
in-domain gold standard classification result. For
example, the in-domain goal standard for the Book
domain is 79.96%. For adapting from DVD to Book,
baseline achieves 72.25% and JST gives 76.45%.
The adaptation loss is 7.71 for baseline and 3.51 for
JST.
It can be observed from Table ?? that LDA only
improves slightly compared to the baseline with an
error reduction of 11%. JST further reduces the er-
ror due to transfer by 27%. Adding pseudo-labeled
examples gives a slightly better performance com-
pared to JST with an error reduction of 36%. With
feature selection, JST-IG outperforms all the other
approaches with a relative error reduction of 53%.
7Both values of 0.8 and 2000 were set arbitrarily after an ini-
tial run on some held-out data; they were not tuned to optimize
test performance.
Domain Base LDA JST JST-PL JST-IG
Book 10.8 9.4 7.2 6.3 5.2
DVD 8.3 6.1 4.8 4.4 2.9
Electr. 7.9 7.7 6.3 5.4 3.9
Kitch. 7.6 7.6 6.9 6.1 4.4
Average 8.6 7.7 6.3 5.5 4.1
Table 3: Adaptation loss with respect to the in-domain
gold standard. The last row shows the average loss over
all the four domains.
Parameter Sensitivity
There is only one parameters to be set in the JST-
IG approach, the number of topics. We plot the clas-
sification accuracy versus different topic numbers in
Figure ?? with the number of topics varying between
1 and 200, corresponding to feature clusters varying
between 3 and 600. It can be observed that for the
relatively larger Book and DVD data sets, the accu-
racies peaked at topic number 10, whereas for the
relatively smaller Electronics and Kitchen data sets,
the best performance was obtained at topic number
50. Increasing topic numbers results in the decrease
of classification accuracy. Manually examining the
extracted polarity topics from JST reveals that when
the topic number is small, each topic cluster contains
well-mixed words from different domains. How-
ever, when the topic number is large, words under
each topic cluster tend to be dominated by a single
domain.
Comparison with Existing Approaches
We compare in Figure ?? our proposed approach
with two other domain adaptation algorithms for
sentiment classification, SCL and SFA. Each set of
bars represent a cross-domain sentiment classifica-
tion task. The thick horizontal lines are in-domain
sentiment classification accuracies. It is worth not-
ing that our in-domain results are slightly different
from those reported in (?; ?) due to different ran-
dom splits. Our proposed JST-IG approach outper-
forms SCL in average and achieves comparable re-
sults to SFA. While SCL requires the construction of
a reasonable number of auxiliary tasks that are use-
ful to model ?pivots? and ?non-pivots?, SFA relies
on a good selection of domain-independent features
for the construction of bipartite feature graph before
running spectral clustering to derive feature clusters.
129
70
75
80
85
ura
cy (%
)
D >B E >B K >B B >D E >D K >D
60
65
1 5 10 15 30 50 100 150 200
Acc
ura
cy (%
)
No. of topics
(a) Adapted to Book and DVD data sets.
80
85
ura
cy (%
)
B >E D >E K >E B >K D >K E >K
70
75
1 5 10 15 30 50 100 150 200
Acc
ura
cy (%
)
No. of topics
(b) Adapted to Electronics and Kitchen data sets.
Figure 3: Classification accuracy vs. no. of topics.
On the contrary, our proposed approach based on
the JST model is much simpler and yet still achieves
comparable results.
7 Conclusions
In this paper, we have studied polarity-bearing top-
ics generated from the JST model and shown that by
augmenting the original feature space with polarity-
bearing topics, the in-domain supervised classi-
fiers learned from augmented feature representation
achieve the state-of-the-art performance on both the
movie review data and the multi-domain sentiment
dataset. Furthermore, using feature augmentation
and selection according to the information gain cri-
teria for cross-domain sentiment classification, our
proposed approach outperforms SCL and gives sim-
ilar results as SFA. Nevertheless, our approach is
much simpler and does not require difficult parame-
ter tuning.
There are several directions we would like to ex-
plore in the future. First, polarity-bearing topics
generated by the JST model were simply added into
the original feature space of documents, it is worth
investigating attaching different weight to each topic
79.96 81.32
75
80
85
ura
cy (%
)
baseline SCL MI SFA JST IG
65
70
D >B E >B K >B B >D E >D K >D
Acc
ura
cy (%
)
(a) Adapted to Book and DVD data sets.
83.61 85.82
80
85
90
ura
cy (%
)
baseline SCL MI SFA JST IG
65
70
75
B >E D >E K >E B >K D >K E >K
Acc
ura
cy (%
)
(b) Adapted to Electronics and Kitchen data sets.
Figure 4: Comparison with existing approaches.
maybe in proportional to the posterior probability of
sentiment label and topic given a word estimated by
the JST model. Second, it might be interesting to
study the effect of introducing a tradeoff parameter
to balance the effect of original and new features.
Finally, our experimental results show that adding
pseudo-labeled examples by the JST model does not
appear to be effective. We could possibly explore in-
stance weight strategies (?) on both pseudo-labeled
examples and source domain training examples in
order to improve the adaptation performance.
Acknowledgements
This work was supported in part by the EC-FP7
projects ROBUST (grant number 257859).
References
R.K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. The Journal of Machine Learning Re-
search, 6:1817?1853.
A. Aue and M. Gamon. 2005. Customizing sentiment
classifiers to new domains: a case study. In Proceed-
ings of Recent Advances in Natural Language Process-
ing (RANLP).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
130
2003. Latent Dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL, page 440?
447.
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy classifier: Little data can help a lot. In
EMNLP.
W. Dai, Y. Chen, G.R. Xue, Q. Yang, and Y. Yu. 2008.
Translated learning: Transfer learning across different
feature spaces. In NIPS, pages 353?360.
W. Dai, O. Jin, G.R. Xue, Q. Yang, and Y. Yu. 2009.
Eigentransfer: a unified framework for transfer learn-
ing. In ICML, pages 193?200.
H. Daume? III and D. Marcu. 2006. Domain adaptation
for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26(1):101?126.
H. Daume?. 2007. Frustratingly easy domain adaptation.
In ACL, pages 256?263.
J. Jiang and C.X. Zhai. 2007. Instance weighting for
domain adaptation in NLP. In ACL, pages 264?271.
A. Kennedy and D. Inkpen. 2006. Sentiment clas-
sification of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?125.
S. Li, C.R. Huang, G. Zhou, and S.Y.M. Lee. 2010.
Employing personal/impersonal views in supervised
and semi-supervised sentiment classification. In ACL,
pages 414?423.
C. Lin and Y. He. 2009. Joint sentiment/topic model for
sentiment analysis. In Proceedings of the 18th ACM
international conference on Information and knowl-
edge management (CIKM), pages 375?384.
C. Lin, Y. He, and R. Everson. 2010. A Compara-
tive Study of Bayesian Models for Unsupervised Sen-
timent Detection. In Proceedings of the 14th Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 144?152.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In ACL, pages 432?
439.
T. Minka. 2003. Estimating a Dirichlet distribution.
Technical report.
S.J. Pan, X. Ni, J.T. Sun, Q. Yang, and Z. Chen. 2010.
Cross-domain sentiment classification via spectral fea-
ture alignment. In Proceedings of the 19th interna-
tional conference on World Wide Web (WWW), pages
751?760.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL, page 271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In EMNLP, pages 79?86.
B. Roark and M. Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In
NAACL-HLT, pages 126?133.
C.W. Seah, I. Tsang, Y.S. Ong, and K.K. Lee. 2010. Pre-
dictive Distribution Matching SVM for Multi-domain
Learning. In ECML-PKDD, pages 231?247.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proceedings of the ACM international conference
on Information and Knowledge Management (CIKM),
pages 625?631.
Q. Wu, S. Tan, and X. Cheng. 2009. Graph ranking for
sentiment transfer. In ACL-IJCNLP, pages 317?320.
Q. Wu, S. Tan, X. Cheng, and M. Duan. 2010. MIEA:
a Mutual Iterative Enhancement Approach for Cross-
Domain Sentiment Classification. In COLING, page
1327-1335.
A. Yessenalina, Y. Choi, and C. Cardie. 2010a. Auto-
matically generating annotator rationales to improve
sentiment classification. In ACL, pages 336?341.
A. Yessenalina, Y. Yue, and C. Cardie. 2010b. Multi-
Level Structured Models for Document-Level Senti-
ment Classification. In EMNLP, pages 1046?1056.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding re-
dundant features for CRFs-based sentence sentiment
classification. In EMNLP, pages 117?126.
131
Exploring English Lexicon Knowledge for Chinese Sentiment Analysis
Yulan He Harith Alani
Knowledge Media Institute
The Open University
Milton Keynes MK6 6AA, UK
{y.he, h.alani}@open.ac.uk
Deyu Zhou
School of Computer Science and Engineering
Southeast University
Nanjing, China
d.zhou@seu.edu.cn
Abstract
This paper presents a weakly-supervised
method for Chinese sentiment analysis
by incorporating lexical prior knowledge
obtained from English sentiment lexi-
cons through machine translation. A
mechanism is introduced to incorpo-
rate the prior information about polarity-
bearing words obtained from existing
sentiment lexicons into latent Dirichlet
allocation (LDA) where sentiment labels
are considered as topics. Experiments
on Chinese product reviews on mobile
phones, digital cameras, MP3 players,
and monitors demonstrate the feasibil-
ity and effectiveness of the proposed ap-
proach and show that the weakly su-
pervised LDA model performs as well
as supervised classifiers such as Naive
Bayes and Support vector Machines with
an average of 83% accuracy achieved
over a total of 5484 review documents.
Moreover, the LDA model is able to
extract highly domain-salient polarity
words from text.
1 Introduction
Sentiment analysis aims to understand subjec-
tive information such as opinions, attitudes, and
feelings expressed in text. It has become a hot
topic in recent years because of the explosion in
availability of people?s attitudes and opinions ex-
pressed in social media including blogs, discus-
sion forums, tweets, etc. Research in sentiment
analysis has mainly focused on the English lan-
guage. There have been few studies in sentiment
analysis in other languages due to the lack of re-
sources, such as subjectivity lexicons consisting
of a list of words marked with their respective
polarity (positive, negative or neutral) and manu-
ally labeled subjectivity corpora with documents
labeled with their polarity.
Pilot studies on cross-lingual sentiment anal-
ysis utilize machine translation to perform senti-
ment analysis on the English translation of for-
eign language text (Banea et al, 2008; Bautin
et al, 2008; Wan, 2009). The major problem
is that they cannot be generalized well when
there is a domain mismatch between the source
and target languages. There have also been in-
creasing interests in exploiting bootstrapping-
style approaches for weakly-supervised senti-
ment classification in languages other than En-
glish (Zagibalov and Carroll, 2008b; Zagibalov
and Carroll, 2008a; Qiu et al, 2009). Other
approaches use ensemble techniques by either
combining lexicon-based and corpus-based algo-
rithms (Tan et al, 2008) or combining sentiment
classification outputs from different experimen-
tal settings (Wan, 2008). Nevertheless, all these
approaches are either complex or require careful
tuning of domain and data specific parameters.
This paper proposes a weakly-supervised ap-
proach for Chinese sentiment classification by
incorporating language-specific lexical knowl-
edge obtained from available English senti-
ment lexicons through machine translation. Un-
like other cross-lingual sentiment classification
methods which often require labeled corpora for
training and therefore hinder their applicability
for cross-domain sentiment analysis, the pro-
posed approach does not require labeled docu-
ments. Moreover, as opposed to existing weakly-
supervised sentiment classification approaches
which are rather complex, slow, and require care-
ful parameter tuning, the proposed approach is
simple and computationally efficient; rendering
more suitable for online and real-time sentiment
classification from the Web.
Our experimental results on the Chinese re-
views of four different product types show that
the LDA model performs as well as the super-
vised classifiers such as Naive Bayes and Sup-
port Vector Machines trained from labeled cor-
pora. Although this paper primarily studies sen-
timent analysis in Chinese, the proposed ap-
proach is applicable to any other language so
long as a machine translation engine is available
between the selected language and English.
The remainder of the paper is organized as
follows. Related work on cross-lingual senti-
ment classification and weakly-supervised sen-
timent classification in languages other than En-
glish are discussed in Section 2. The proposed
mechanism of incorporating prior word polarity
knowledge into the LDA model is introduced in
Section 3. The experimental setup and results of
sentiment classification on the Chinese reviews
of four different products are presented in Sec-
tion 4 and 5 respectively. Finally, Section 6 con-
cludes the paper.
2 Related Work
Pilot studies on cross-lingual sentiment analysis
rely on English corpora for subjectivity classifi-
cation in other languages. For example, Mihal-
cea et al (2007) make use of a bilingual lexicon
and a manually translated parallel text to gener-
ate the resources to build subjectivity classifiers
based on Support Vector Machines (SVMs) and
Naive Bayes (NB) in a new language; Banea et
al. (2008) use machine translation to produce a
corpus in a new language and train SVMs and
NB for subjectivity classification in the new lan-
guage. Bautin et al (2008) also utilize machine
translation to perform sentiment analysis on the
English translation of a foreign language text.
More recently, Wan (2009) proposed a co-
training approach to tackle the problem of cross-
lingual sentiment classification by leveraging an
available English corpus for Chinese sentiment
classification. Similar to the approach proposed
in (Banea et al, 2008), Wan?s method also uses
machine translation to produced a labeled Chi-
nese review corpus from the available labeled
English review data. However, in order to allevi-
ate the language gap problem that the underlying
distributions between the source and target lan-
guage are different, Wan builds two SVM classi-
fiers, one based on English features and the other
based on Chinese features, and uses a bootstrap-
ping method based on co-training to iteratively
improve classifiers until convergence.
The major problem of the aforementioned
cross-lingual sentiment analysis algorithms is
that they all utilize supervised learning to train
sentiment classifiers from annotated English cor-
pora (or the translated target language corpora
generated by machine translation). As such, they
cannot be generalized well when there is a do-
main mismatch between the source and target
language. For example, For example, the word
?compact? might express positive polarity when
used to describe a digital camera, but it could
have negative orientation if it is used to describe
a hotel room. Thus, classifiers trained on one
domain often fail to produce satisfactory results
when shifting to another domain.
Recent efforts have also been made for
weakly-supervised sentiment classification in
Chinese. Zagibalov and Carroll (2008b) starts
with a one-word sentiment seed vocabulary and
use iterative retraining to gradually enlarge the
seed vocabulary by adding more sentiment-
bearing lexical items based on their relative fre-
quency in both the positive and negative parts
of the current training data. Sentiment direction
of a document is then determined by the sum
of sentiment scores of all the sentiment-bearing
lexical items found in the document. The prob-
lem with this approach is that there is no princi-
pal way to set the optimal number of iterations.
They then suggested an iteration control method
in (Zagibalov and Carroll, 2008a) where itera-
tive training stops when there is no change to the
classification of any document over the previous
two iterations. However, this does not necessar-
ily correlate to the best classification accuracy.
Similar to (Zagibalov and Carroll, 2008b),
Qiu et al (2009) also uses a lexicon-based iter-
ative process as the first phase to iteratively en-
large an initial sentiment dictionary. But instead
of using a one-word seed dictionary as in (Za-
gibalov and Carroll, 2008b), they started with a
much larger HowNet Chinese sentiment dictio-
nary1 as the initial lexicon. Documents classified
by the first phase are taken as the training set to
train the SVMs which are subsequently used to
revise the results produced by the first phase.
Other researchers investigated ensemble tech-
niques for weakly-supervised sentiment classifi-
cation. Tan et al (2008) proposed a combination
of lexicon-based and corpus-based approaches
that first labels some examples from a give do-
main using a sentiment lexicon and then trains
a supervised classifier based on the labeled ones
from the first stage. Wan (2008) combined sen-
timent scores calculated from Chinese product
reviews using the Chinese HowNet sentiment
dictionary and from the English translation of
Chinese reviews using the English MPQA sub-
jectivity lexicon2. Various weighting strategies
were explored to combine sentiment classifica-
tion outputs from different experimental settings
in order to improve classification accuracy.
Nevertheless, all these weakly-supervised
sentiment classification approaches are rather
complex and require either iterative training or
careful tuning of domain and data specific pa-
rameters, and hence unsuitable for online and
real-time sentiment analysis in practical applica-
tions.
3 Incorporating Prior Word Polarity
Knowledge into LDA
Unlike existing approaches, we view sentiment
classification as a generative problem that when
an author writes a review document, he/she first
decides on the overall sentiment or polarity (pos-
itive, negative, or neutral) of a document, then
for each sentiment, decides on the words to be
used. We use LDA to model a mixture of only
three topics or sentiment labels, i.e. positive,
negative and neutral.
Assuming that we have a total number of S
sentiment labels; a corpus with a collection of D
1http://www.keenage.com/download/
sentiment.rar
2http://www.cs.pitt.edu/mpqa/
documents is denoted by C = {d1, d2, ..., dD};
each document in the corpus is a sequence of Nd
words denoted by d = (w1, w2, ..., wNd), and
each word in the document is an item from a vo-
cabulary index with V distinct terms denoted by
{1, 2, ..., V }. The generative process is as fol-
lows:
? Choose distributions ? ? Dir(?).
? For each document d ? [1, D], choose dis-
tributions pid ? Dir(?).
? For each of the Nd word posi-
tion wt, choose a sentiment label
lt ? Multinomial(pid), and then choose a
word wt ?Multinomial(?lt).
The joint probability of words and sentiment
label assignment in LDA can be factored into
two terms:
P (w, l) = P (w|l)P (l|d). (1)
Letting the superscript ?t denote a quantity that
excludes data from the tth position, the condi-
tional posterior for lt by marginalizing out the
random variables ? and pi is
P (lt = k|w, l?t, ?,?) ?
N?twt,k + ?
N?tk + V ?
?
N?tk,d + ?k
N?td +
?
k ?k
, (2)
where Nwt,k is the number of times word wt has
associated with sentiment label k; Nk is the the
number of times words in the corpus assigned to
sentiment label k; Nk,d is the number of times
sentiment label k has been assigned to some
word tokens in document d; Nd is the total num-
ber of words in the document collection.
Each words in documents can either bear pos-
itive polarity (lt = 1), or negative polarity (lt =
2), or is neutral (lt = 0). We now show how
to incorporate polarized words in sentiment lex-
icons as prior information in the Gibbs sampling
process. Let
Qt,k =
N?twt,k + ?
N?tk + V ?
?
N?tk,d + ?k
N?td +
?
k ?k
(3)
We can then modify the Gibbs sampling equa-
tion as follows:
P (lt = k|w, l?t, ?,?) ?
{
1I(k = S(wt))?Qt,k if S(wt) is defined
Qt,k otherwise
(4)
where the function S(wt) returns the prior senti-
ment label of wt in a sentiment lexicon and it is
defined if word wt is found in the sentiment lex-
icon. 1I(k = S(wt)) is an indicator function that
takes on value 1 if k = S(wt) and 0 otherwise.
Equation 4 in fact applies a hard constraint
that when a word is found in a sentiment lexi-
con, its sampled sentiment label is restricted to
be the same as its prior sentiment label defined
in the lexicon. This constraint can be relaxed by
introducing a parameter to control the strength of
the constraint such that when wordwt is found in
the sentiment lexicon, Equation 4 becomes
P (lt = k|w, l?t, ?,?) ?
(1? ?)?Qt,k + ?? 1I(k = S(wt))?Qt,k
(5)
where 0 ? ? ? 1. When ? = 1, the hard con-
straint will be applied; when ? = 0, Equation 5
is reduced to the original unconstrained Gibbs
sampling as defined in Equation 2.
While sentiment prior information is incor-
porated by modifying conditional probabilities
used in Gibbs sampling here, it is also possible to
explore other mechanisms to define expectation
or posterior constraints, for example, using the
generalized expectation criteria (McCallum et
al., 2007) to express preferences on expectations
of sentiment labels of those lexicon words. We
leave the exploitation of other mechanisms of in-
corporating prior knowledge into model training
as future work.
The document sentiment is classified based on
P (l|d), the probability of sentiment label given
document, which can be directly obtained from
the document-sentiment distribution. We de-
fine that a document d is classified as positive
if P (lpos|d) > P (lneg|d), and vice versa.
Table 2: Data statistics of the four Chinese prod-
uct reviews corpora.
No. of Reviews Vocab
Corpus positive Negative Size
Mobile 1159 1158 8945
DigiCam 853 852 5716
MP3 390 389 4324
Monitor 341 342 4712
4 Experimental Setup
We conducted experiments on the four corpora3
which were derived from product reviews har-
vested from the website IT1684 with each cor-
responding to different types of product reviews
including mobile phones, digital cameras, MP3
players, and monitors. All the reviews were
tagged by their authors as either positive or neg-
ative overall. The statistics of the four corpora
are shown in Table 2.
We explored three widely used English sen-
timent lexicons in our experiments, namely the
MPQA subjectivity lexicon, the appraisal lexi-
con5, and the SentiWordNet6 (Esuli and Sebas-
tiani, 2006). For all these lexicons, we only ex-
tracted words bearing positive or negative polar-
ities and discarded words bearing neutral polar-
ity. For SentiWordNet, as it consists of words
marked with positive and negative orientation
scores ranging from 0 to 1, we extracted a subset
of 8,780 opinionated words, by selecting those
whose orientation strength is above a threshold
of 0.6.
We used Google translator toolkit7 to translate
these three English lexicons into Chinese. After
translation, duplicate entries, words that failed to
translate, and words with contradictory polarities
were removed. For comparison, we also tested a
Chinese sentiment lexicon, NTU Sentiment Dic-
tionary (NTUSD)8 (Ku and Chen, 2007) which
3http://www.informatics.sussex.ac.uk/
users/tz21/dataZH.tar.gz
4http://product.it168.com
5http://lingcog.iit.edu/arc/
appraisal_lexicon_2007b.tar.gz
6http://sentiwordnet.isti.cnr.it/
7http://translate.google.com
8http://nlg18.csie.ntu.edu.tw:
Table 1: Matched polarity words statistics (positive/negative).
Lexicon
Chinese English
Mobile DigiCam MP3 Monitors Mobile DigiCam MP3 Monitors
(a)MPQA 261/253 183/174 162/135 169/147 293/331 220/241 201/153 210/174
(b)Appraisal 279/165 206/127 180/104 198/105 392/271 330/206 304/153 324/157
(c)SentiWN 304/365 222/276 202/213 222/236 394/497 306/397 276/310 313/331
(d)NTUSD 338/319 263/242 239/167 277/241 ?
(a)+(c) 425/465 307/337 274/268 296/289 516/607 400/468 356/345 396/381
(a)+(b)+(c) 495/481 364/353 312/280 344/302 624/634 496/482 447/356 494/389
(a)+(c)+(d) 586/608 429/452 382/336 421/410 ?
was automatically generated by enlarging an ini-
tial manually created seed vocabulary by con-
sulting two thesauri, tong2yi4ci2ci2lin2 and the
Academia Sinica Bilingual Ontological Word-
Net 3.
Chinese word segmentation was performed on
the four corpora using the conditional random
fields based Chinese Word Segmenter9. The to-
tal numbers of matched polarity words in each
corpus using different lexicon are shown in Ta-
ble 1 with the left half showing the statistics
against the Chinese lexicons (the original En-
glish lexicons have been translated into Chinese)
and the right half listing the statistics against the
English lexicons. We did not translate the Chi-
nese lexicon NTUSD into English since we fo-
cused on Chinese sentiment classification here.
It can be easily seen from the table that in gen-
eral the matched positive words outnumbered the
matched negative words using any single lexi-
con except SentiWordNet. But the combination
of the lexicons results in more matched polarity
words and thus gives more balanced number of
positive and negative words. We also observed
the increasing number of the matched polarity
words on the translated English corpora com-
pared to their original Chinese corpora. How-
ever, as will be discussed in Section 5.2 that the
increasing number of the matched polarity words
does not necessarily lead to the improvement of
the sentiment classification accuracy.
We modified GibbsLDA++ package10 for the
model implementation and only used hard con-
8080/opinion/pub1.html
9http://nlp.stanford.edu/software/
stanford-chinese-segmenter-2008-05-21.
tar.gz
10http://gibbslda.sourceforge.net/
straints as defined in Equation 4 in our experi-
ments. The word prior polarity information was
also utilized during the initialization stage that
if a word can be found in a sentiment lexicon,
the word token is assigned with its correspond-
ing sentiment label. Otherwise, a sentiment label
is randomly sampled for the word. Symmetric
Dirichlet prior ? was used for sentiment-word
distribution and was set to 0.01, while asym-
metric Dirichlet prior ? was used for document-
sentiment distribution and was set to 0.01 for
positive and neutral sentiment labels, and 0.05
for negative sentiment label.
5 Experimental Results
This section presents the experimental results
obtained under two different settings: LDA
model with translated English lexicons tested on
the original Chinese product review corpora; and
LDA model with original English lexicons tested
on the translated product review corpora.
5.1 Results with Different Sentiment
Lexicons
Table 3 gives the classification accuracy results
using the LDA model with prior sentiment la-
bel information provided by different sentiment
lexicons. Since we did not use any labeled in-
formation, the accuracies were averaged over 5
runs and on the whole corpora. For comparison
purposes, we have also implemented a baseline
model which simply assigns a score +1 and -1
to any matched positive and negative word re-
spectively based on a sentiment lexicon. A re-
view document is then classified as either posi-
tive or negative according to the aggregated sen-
timent scores. The baseline results were shown
in brackets in Table 3 .
Table 3: Sentiment classification accuracy (%) by LDA, numbers in brackets are baseline results.
Lexicon Mobile DigiCam MP3 Monitors Average
(a)MPQA 82.00 (63.53) 80.93 (67.59) 78.31 (68.42) 81.41 (64.86) 80.66 (66.10)
(b)Appraisal 71.95 (56.28) 80.46 (60.54) 77.28 (61.36) 80.67 (57.98) 77.59 (59.04)
(c)SentiWN 81.10 (62.45) 78.52 (57.13) 79.08 (64.57) 75.55 (55.34) 78.56 (59.87)
(d)NTUSD 82.61 (71.21) 78.70 (68.23) 78.69 (75.87) 84.63 (74.96) 81.16 (72.57)
(a)+(c) 81.18 (65.95) 78.70 (65.18) 83.83 (67.52) 80.53 (62.08) 81.06 (65.18)
(a)+(b)+(c) 81.48 (62.84) 80.22 (65.88) 80.23 (65.60) 78.62 (61.35) 80.14 (63.92)
(a)+(c)+(d) 82.48 (69.96) 84.33 (69.58) 83.70 (71.12) 82.72 (65.59) 83.31 (69.06)
Naive Bayes 86.52 82.27 82.64 86.21 84.41
SVMs 84.49 82.04 79.43 83.87 82.46
It can be observed from Table 3 that the
LDA model performs significantly better than
the baseline model. The improvement ranges be-
tween 9% and 19% and this roughly corresponds
to how much the model learned from the data.
We can thus speculate that LDA is indeed able to
learn the sentiment-word distributions from data.
Translated English sentiment lexicons per-
form comparably with the Chinese sentiment
lexicon NTUSD. As for the individual lexicon,
using MPQA subjectivity lexicon gives the best
result among all the English lexicons on all the
corpora except the MP3 corpus where MPQA
performs slightly worse than SentiWordNet. The
combination of MPQA and SentiWordNet per-
forms significantly better than other lexicons on
the MP3 corpus, with almost 5% improvement
compared to the second best result. We also
notice that the combination of all the three En-
glish lexicons does not lead to the improvement
of classification accuracy which implies that the
quality of a sentiment lexicon is indeed impor-
tant to sentiment classification. The above re-
sults suggest that in the absence of any Chinese
sentiment lexicon, MPQA subjectivity lexicon
appears to be the best candidate to be used to
provide sentiment prior information to the LDA
model for Chinese sentiment classification.
We also conducted experiments by includ-
ing the Chinese sentiment lexicon NTUSD and
found that the combination of MPQA, Senti-
WordNet, and NTUSD gives the best overall
classification accuracy with 83.31% achieved.
For comparison purposes, we list the 10-fold
cross validation results obtained using the super-
vised classifiers, Naive Bayes and SVMs, trained
on the labeled corpora as previously reported in
(Zagibalov and Carroll, 2008a). It can be ob-
served that using only English lexicons (the com-
bination of MPQA and SentiWordNet), we ob-
tain better results than both NB and SVMs on
the MP3 corpus. With an additional inclusion
of NTUSD, LDA outperforms NB and SVMs
on both DigiCam and MP3. Furthermore, LDA
gives a better overall accuracy when compared
to SVMs. Thus, we may conclude that the un-
supervised LDA model performs as well as the
supervised classifiers such as NB and SVMs on
the Chinese product review corpora.
5.2 Results with Translated Corpora
We ran a second set of experiments on the trans-
lated Chinese product review corpora using the
original English sentiment lexicons. Both the
translated corpora and the sentiment lexicons
have gone through stopword removal and stem-
ming in order to reduce the vocabulary size and
thereby alleviate data sparseness problem. It can
be observed from Figure 1 that in general senti-
ment classification on the original Chinese cor-
pora using the translated English sentiment lex-
icons gives better results than classifying on the
translated review corpora using the original En-
glish lexicons on both the Mobile and Digicam
corpora. However, reversed results are observed
on the Monitor corpus that classifying on the
translated review corpus using the English sen-
timent lexicons outperforms classifying on the
85
Mobi
le
8085
y?(%)
Mobi
le
70758085
Accurac
Mobi
le
6570758085
()M
PQA
(b)A
il
()S
iWN
()(
)
()(b
)()
Mobi
le
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
seCo
rpora
Englis
hCor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
se?Co
rpora
Englis
h?Cor
pora
85
DigiC
am
8085
y?(%)
DigiC
am
70758085
Accurac
DigiC
am
6570758085
(a)M
PQA
(b)Ap
praisa
l(c)
SentiW
N
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
85
MP3
8085
y?(%)
MP3
70758085
Accurac
MP3
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
85
Moni
tor
8085
y?(%)
Moni
tor
70758085
Accurac
Moni
tor
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
Figure 1: Comparison of the performance on the Chinese corpora and their translated corpora in
English.
original Chinese review corpus using the trans-
lated sentiment lexicons. In particular, the com-
bination of the MPQA subjectivity lexicon and
SentiWordNet gives the best result of 84% on
the Monitor corpus. As for the MP3 corpus,
classifying on the original Chinese reviews or on
the translated reviews does not differ much ex-
cept that a better result is obtained on the Chi-
nese corpus when using the combination of the
MPQA subjectivity lexicon and SentiWordNet.
The above results can be partially explained by
the ambiguities and changes of meanings intro-
duced in the translation. The Mobile and Digi-
Cam corpora are relatively larger than the MP3
and Monitors corpora and we therefore expect
more ambiguities being introduced which might
result in the change of document polarities.
5.3 Extracted Polarity-Bearing Words
LDA is able to extract polarity-bearing words.
Table 4 lists some of the polarity words identi-
fied by the LDA model which are not found in
the original sentiment lexicons. We can see that
LDA is indeed able to recognize domain-specific
positive or negative words, for example, ?Y
(bluetooth) for mobile phones, ? (compact)
for digital cameras,?^ (metallic) for MP3,?
s (flat screen) and?b (deformation) for mon-
itors.
The iterative approach proposed in (Zagibalov
and Carroll, 2008a) can also automatically ac-
quire polarity words from data. However, it ap-
pears that only positive words were identified
by their approach. Our proposed LDA model
can extract both positive and negative words and
most of them are highly domain-salient as can be
seen from Table 4.
6 Conclusions
This paper has proposed a mechanism to incor-
porate prior information about polarity words
from English sentiment lexicons into LDA
model learning for weakly-supervised Chinese
sentiment classification. Experimental results of
sentiment classification on Chinese product re-
views show that in the absence of a language-
specific sentiment lexicon, the translated En-
glish lexicons can still produce satisfactory re-
sults with the sentiment classification accuracy
of 81% achieved averaging over four different
types of product reviews. With the incorpora-
tion of the Chinese sentiment lexicon NTUSD,
the classification accuracy is further improved to
83%. Compared to the existing approaches to
cross-lingual sentiment classification which ei-
ther rely on labeled corpora for classifier learn-
ing or iterative training for performance gains,
the proposed approach is simple and readily to
Table 4: Extracted example polarity words by LDA.
Corpus Positive Negative
Mobile ? (advantage), ' (large), }( (easy to
use),? (fast), (comfortable),?Y (blue-
tooth),? (new),? (easy)
O (bad), ? (poor), b (slow), ? (no;not), ?
(difficult;hard), (less),?/ (but),? (repair)
DigiCam  ? (advantage),  ? (compact), :
(strong;strength), & (telephoto), ? (dy-
namic), h (comprehensive), 