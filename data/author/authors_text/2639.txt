Portability Issues for Speech Recognition Technologies 
Lori Lamel, Fabrice Lefevre, Jean-Luc Gauvain and Gilles Adda
Spoken Language Processing Group,
CNRS-LIMSI, 91403 Orsay, France
flamel,lefevre,gauvain,gaddag@limsi.fr
ABSTRACT
Although there has been regular improvement in speech recog-
nition technology over the past decade, speech recognition is far
from being a solved problem. Most recognition systems are tuned
to a particular task and porting the system to a new task (or lan-
guage) still requires substantial investment of time and money, as
well as expertise. Todays state-of-the-art systems rely on the avail-
ability of large amounts of manually transcribed data for acous-
tic model training and large normalized text corpora for language
model training. Obtaining such data is both time-consuming and
expensive, requiring trained human annotators with substantial a-
mounts of supervision.
In this paper we address issues in speech recognizer portabil-
ity and activities aimed at developing generic core speech recogni-
tion technology, in order to reduce the manual effort required for
system development. Three main axes are pursued: assessing the
genericity of wide domain models by evaluating performance under
several tasks; investigating techniques for lightly supervised acous-
tic model training; and exploring transparent methods for adapting
generic models to a specific task so as to achieve a higher degree of
genericity.
1. INTRODUCTION
The last decade has seen impressive advances in the capability
and performance of speech recognizers. Todays state-of-the-art
systems are able to transcribe unrestricted continuous speech from
broadcast data with acceptable performance. The advances arise
from the increased accuracy and complexity of the models, which
are closely related to the availability of large spoken and text cor-
pora for training, and the wide availability of faster and cheaper
computational means which have enabled the development and im-
plementation of better training and decoding algorithms. Despite
the extent of progress over the recent years, recognition accuracy is
still extremely sensitive to the environmental conditions and speak-
ing style: channel quality, speaker characteristics, and background
This work was partially financed by the European Commission
under the IST-1999 Human Language Technologies project 11876
Coretex.
.
noise have an important impact on the acoustic component of the
speech recognizer, whereas the speaking style and the discourse
domain have a large impact on the linguistic component.
In the context of the EC IST-1999 11876 project CORETEX we
are investigating methods for fast system development, as well as
development of systems with high genericity and adaptability. By
fast system development we refer to: language support, i.e., the
capability of porting technology to different languages at a reason-
able cost; and task portability, i.e. the capability to easily adapt a
technology to a new task by exploiting limited amounts of domain-
specific knowledge. Genericity and adaptability refer to the capac-
ity of the technology to work properly on a wide range of tasks and
to dynamically keep models up to date using contemporary data.
The more robust the initial generic system is, the less there is a
need for adaptation. Concerning the acoustic modeling component,
genericity implies that it is robust to the type and bandwidth of the
channel, the acoustic environment, the speaker type and the speak-
ing style. Unsupervised normalization and adaptation techniques
evidently should be used to enhance performance further when the
system is exposed to data of a particular type.
With today?s technology, the adaptation of a recognition system
to a new task or new language requires the availability of suffi-
cient amount of transcribed training data. When changing to new
domains, usually no exact transcriptions of acoustic data are avail-
able, and the generation of such transcribed data is an expensive
process in terms of manpower and time. On the other hand, there
often exist incomplete information such as approximate transcrip-
tions, summaries or at least key words, which can be used to pro-
vide supervision in what can be referred to as ?informed speech
recognition?. Depending on the level of completeness, this infor-
mation can be used to develop confidence measures with adapted or
trigger language models or by approximate alignments to automatic
transcriptions. Another approach is to use existing recognizer com-
ponents (developed for other tasks or languages) to automatically
transcribe task-specific training data. Although in the beginning the
error rate on new data is likely to be rather high, this speech data
can be used to re-train a recognition system. If carried out in an
iterative manner, the speech data base for the new domain can be
cumulatively extended over time without direct manual transcrip-
tion.
The overall objective of the work presented here is to reduce
the speech recognition development cost. One aspect is to develop
?generic? core speech recognition technology, where by ?generic?
we mean a transcription engine that will work reasonably well on a
wide range of speech transcription tasks, ranging from digit recog-
nition to large vocabulary conversational telephony speech, with-
out the need for costly task-specific training data. To start with we
assess the genericity of wide domain models under cross-task con-
Table 1: Brief descriptions and best reported error rates for the corpora used in this work.
Corpus Test Year Task Train (#spkr) Test (#spkr) Textual Resources Best WER
BN 98 TV & Radio News 200h 3h Closed-captions, commercial transcripts,
manual transcripts of audio data
13.5
TI-digits 93 Small Vocabulary 3.5h (112) 4h (113) - 0.2
ATIS 93 H-M Dialog 40h (137) 5h (24) Transcriptions 2.5
WSJ 95 News Dictation 100h (355) 45mn (20) Newspaper, newswire 6.6
S9 WSJ 93 Spontaneous Dictation 43mn (10) Newspaper, newswire 19.1
ditions, i.e., by recognizing task-specific data with a recognizer de-
veloped for a different task. We chose to evaluate the performance
of broadcast news acoustic and language models, on three com-
monly used tasks: small vocabulary recognition (TI-digits), read
and spontaneous text dictation (WSJ), and goal-oriented spoken di-
alog (ATIS). The broadcast news task is quite general, covering a
wide variety of linguistic and acoustic events in the language, en-
suring reasonable coverage of the target task. In addition, there are
sufficient acoustic and linguistic training data available for this task
that accurate models covering a wide range of speaker and language
characteristics can be estimated.
Another research area is the investigation of lightly supervised
techniques for acoustic model training. The strategy taken is to
use a speech recognizer to transcribe unannotated data, which are
then used to estimate more accurate acoustic models. The light
supervision is applied to the broadcast news task, where unlim-
ited amounts of acoustic training data are potentially available. Fi-
nally we apply the lightly supervised training idea as a transpar-
ent method for adapting the generic models to a specific task, thus
achieving a higher degree of genericity. In this work we focus on
reducing training costs and task portability, and do not address lan-
guage transfer.
We selected the LIMSI broadcast news (BN) transcription sys-
tem as the generic reference system. The BN task covers a large
number of different acoustic and linguistic situations: planned to
spontaneous speech; native and non-native speakers with different
accents; close-talking microphones and telephone channels; quiet
studio, on-site reports in noisy places to musical background; and
a variety of topics. In addition, a lot of training resources are avail-
able including a large corpus of annotated audio data and a huge
amount of raw audio data for the acoustic modeling; and large
collections of closed-captions, commercial transcripts, newspapers
and newswires texts for linguistic modeling. The next section pro-
vides an overview of the LIMSI broadcast news transcription sys-
tem used as our generic system.
2. SYSTEM DESCRIPTION
The LIMSI broadcast news transcription system has two main
components, the audio partitioner and the word recognizer. Data
partitioning [6] serves to divide the continuous audio stream into
homogeneous segments, associating appropriate labels for cluster,
gender and bandwidth with the segments. The speech recognizer
uses continuous density HMMs with Gaussian mixture for acous-
tic modeling and n-gram statistics estimated on large text corpora
for language modeling. Each context-dependent phone model is a
tied-state left-to-right CD-HMM with Gaussian mixture observa-
tion densities where the tied states are obtained by means of a de-
cision tree. Word recognition is performed in three steps: 1) initial
hypothesis generation, 2) word graph generation, 3) final hypoth-
esis generation. The initial hypotheses are used for cluster-based
acoustic model adaptation using the MLLR technique [13] prior to
word graph generation. A 3-gram LM is used in the first two de-
coding steps. The final hypotheses are generated with a 4-gram LM
and acoustic models adapted with the hypotheses of step 2.
In the baseline system used in DARPA evaluation tests, the acous-
tic models were trained on about 150 hours of audio data from the
DARPA Hub4 Broadcast News corpus (the LDC 1996 and 1997
Broadcast News Speech collections) [9]. Gender-dependent acous-
tic models were built using MAP adaptation of SI seed models for
wide-band and telephone band speech [7]. The models contain
28000 position-dependent, cross-word triphone models with 11700
tied states and approximately 360k Gaussians [8].
The baseline language models are obtained by interpolation of
models trained on 3 different data sets (excluding the test epochs):
about 790M words of newspaper and newswire texts; 240M word
of commercial broadcast news transcripts; and the transcriptions of
the Hub4 acoustic data. The recognition vocabulary contains 65120
words and has a lexical coverage of over 99% on all evaluation test
sets from the years 1996-1999. A pronunciation graph is associated
with each word so as to allow for alternate pronunciations. The
pronunciations make use of a set of 48 phones set, where 3 phone
units represent silence, filler words, and breath noises. The lexicon
contains compound words for about 300 frequent word sequences,
as well as word entries for common acronyms, providing an easy
way to allow for reduced pronunciations [6].
The LIMSI 10x system obtained a word error of 17.1% on the
1999 DARPA/NIST evaluation set and can transcribe unrestricted
broadcast data with a word error of about 20% [8].
3. TASK INDEPENDENCE
Our first step in developing a ?generic? speech transcription en-
gine is to assess the most generic system we have under cross-
task conditions, i.e., by recognizing task-specific data with a rec-
ognizer developed for a different task. Three representative tasks
have been retained as target tasks: small vocabulary recognition
(TI-digits), goal-oriented human-machine spoken dialog (ATIS),
and dictation of texts (WSJ). The broadcast news transcription task
(Hub4E) serves as the baseline. The main criteria for the task se-
lection were that they are realistic enough and task-specific data
should be available. The characteristics of these four tasks and the
available corpora are summarized in Table 1.
For the small vocabulary recognition task, experiments are car-
ried out on the adult speaker portion of the TI-digits corpus [14],
containing over 17k utterances from a total of 225 speakers. The
vocabulary contains 11 words, the digits ?1? to ?9?, plus ?zero? and
?oh?. Each speaker uttered two versions of each digit in isolation
and 55 digit strings. The database is divided into training and test
sets (roughly 3.5 hours each, corresponding to 9k strings). The
speech is of high quality, having been collected in a quiet environ-
ment. The best reported WERs on this task are around 0.2-0.3%.
The digit phonemic coverage being very low, only 108 context-
dependent models are used in our recognition system. The task-
Table 2: Word error rates (%) for BN98, TI-digits, ATIS94,
WSJ95 and S9 WSJ93 test sets after recognition with three dif-
ferent configurations: (left) BN acoustic and language models;
(center) BN acoustic models combined with task-specific lex-
ica and LMs and (right) task-dependent acoustic and language
models.
Test Set BN models Task LMs Task models
BN98 13.6 13.6 13.6
TI-digits 17.5 1.7 0.4
ATIS94 22.7 4.7 4.4
WSJ95 11.6 9.0 7.6
S9 WSJ93 12.1 13.6 15.3
specific LM for the TI-digits is a simple grammar allowing any se-
quence of up to 7 digits. Our task-dependent system performance
is 0.4% WER.
The DARPA Air Travel Information System (ATIS) task is cho-
sen as being representative of a goal-oriented human-machine di-
alog task, and the ARPA 1994 Spontaneous Speech Recognition
(SPREC) ATIS-3 data (ATIS94) [4] is used for testing purposes.
The test data amounts for nearly 5 hours of speech from 24 speakers
recorded with a close-talking microphone. Around 40h of speech
data are available for training. The word error rates for this task in
the 1994 evaluation were mainly in the range of 2.5% to 5%, which
we take as state-of-the-art for this task. The acoustic models used
in our task-specific system include 1641 context-dependent phones
with 4k independent HMM states. A back-off trigram language
model has been estimated on the transcriptions of the training ut-
terances. The lexicon contains 1300 words, with compounds words
for multi-word entities in the air-travel database (city and airport
names, services etc.). The WER obtained with our task-dependent
system is 4.4%.
For the dictation task, the Wall Street Journal continuous speech
recognition corpus [17] is used, abiding by the ARPA 1995 Hub3
test (WSJ95) conditions. The acoustic training data consist of 100
hours of speech from a total of 355 speakers taken from the WSJ0
and WSJ1 corpora. The Hub3 baseline test data consist of stu-
dio quality read speech from 20 speakers with a total duration of
45 minutes. The best result reported at the time of the evaluation
was 6.6%. A contrastive experiment is carried out with the WSJ93
Spoke 9 data comprised of 200 spontaneous sentences spoken by
journalists [11]. The best performance reported in the 1993 evalua-
tion on the spontaneous data was 19.1% [18], however lower word
error rates have since been reported on comparable test sets (14.1%
on the WSJ94 Spoke 9 test data). 21000 context and position-
dependent models have been trained for the WSJ system, with 9k
independent HMM states. A 65k-word vocabulary was selected
and a back-off trigram model obtained by interpolating models trained
on different data sets (training utterance transcriptions and newspa-
pers data). The task-dependent WSJ system has a WER of 7.6% on
the read speech test data and 15.3% on the spontaneous data.
For the BN transcription task, we follow the conditions of the
1998 ARPA Hub4E evaluation (BN98) [15]. The acoustic training
data is comprised of 150 hours of North-American TV and radio
shows. The best overall result on the 1998 baseline test was 13.5%.
Three sets of experiments are reported. The first are cross-task
recognition experiments carried out using the BN acoustic and lan-
guage models to decode the test data for the other tasks. The second
set of experiments made use of mixed models, that is the BN acous-
tic models and task-specific LMs. Due to the different evaluation
paradigms, some minor modifications were made in the transcrip-
tion procedure. First of all, in contrast with the BN data, the data
for the 3 tasks is already segmented into individual utterances so
the partitioning step was eliminated. With this exception, the de-
coding process for the WSJ task is exactly the same as described in
the previous section. For the TI-digits and ATIS tasks, word decod-
ing is carried out in a single trigram pass, and no speaker adaptation
was performed.
The WERs obtained for the three recognition experiments are
reported in Table 2. A comparison with Table 1 shows that the
performances of the task-dependent models are close to the best re-
ported results even though we did not devote too much effort in op-
timizing these models. We can also observe by comparing the task-
dependent (Table 2, right) and mixed (Table 2, middle) conditions,
that the BN acoustic models are relatively generic. These mod-
els seem to be a good start towards truly task-independent acoustic
models. By using task-specific language models For the TI-digits
and ATIS we can see that the gap in performance is mainly due
a linguistic mismatch. For WSJ the language models are more
closely matched to BN and only a small 1.6% WER reduction is
obtained. On the spontaneous journalist dictation (WSJ S9 spoke)
test data there is even an increase in WER using the WSJ LMs,
which can be attributed to a better modelization of spontaneous
speech effects (such as breath and filler words) in the BN models.
Prior to introducing our approach for lightly supervised acoustic
model training, we describe our standard training procedure in the
next section.
4. ACOUSTIC MODEL TRAINING
HMM training requires an alignment between the audio signal
and the phone models, which usually relies on a perfect ortho-
graphic transcription of the speech data and a good phonetic lex-
icon. In general it is easier to deal with relatively short speech seg-
ments so that transcription errors will not propagate and jeopardize
the alignment. The orthographic transcription is usually considered
as ground truth and training is done in a closely supervised man-
ner. For each speech segment the training algorithm is provided
with the exact orthographic transcription of what was spoken, i.e.,
the word sequence that the speech recognizer should hypothesize
when confronted with the same speech segment.
Training acoustic models for a new corpus (which could also re-
flect a change of task and/or language), usually entails the follow-
ing sequence of operations once the audio data and transcription
files have been loaded:
1. Normalize the transcriptions to a common format (some ad-
justment is always needed as different text sources make use
of different conventions).
2. Produce a word list from the transcriptions and correct blatant
errors (these include typographical errors and inconsistencies).
3. Produce a phonemic transcription for all words not in our mas-
ter lexicon (these are manually verified).
4. Align the orthographic transcriptions with the signal using ex-
isting models and the pronunciation lexicon (or bootstrap mod-
els from another task or language). This procedure often re-
jects a substantial portion of the data, particularly for long seg-
ments.
5. Eventually correct transcription errors and realign (or just ig-
nore these if enough audio data is available)
6. Run the standard EM training procedure.
This sequence of operations is usually iterated several times to
refine the acoustic models. In general each iteration recovers a por-
tion of the rejected data.
5. LIGHTLY SUPERVISED ACOUSTIC
MODEL TRAINING
One can imagine training acoustic models in a less supervised
manner, by using an iterative procedure where instead of using
manual transcriptions for alignment, at each iteration the most likely
word transcription given the current models and all the information
available about the audio sample is used. This approach still fits
within the EM training framework, which is well-suited for miss-
ing data training problems. A completely unsupervised training
procedure is to use the current best models to produce an ortho-
graphic transcription of the training data, keeping only words that
have a high confidence measure. Such an approach, while very en-
ticing, is limited since the only supervision is provided by the con-
fidence measure estimator. This estimator must in turn be trained
on development data, which needs to be small to keep the approach
interesting.
Between using carefully annotated data such as the detailed tran-
scriptions provided by the LDC and no transcription at all, there is
a wide spectrum of possibilities. What is really important is the
cost of producing the associated annotations. Detailed annotation
requires on the order of 20-40 times real-time of manual effort, and
even after manual verification the final transcriptions are not ex-
empt from errors [2]. Orthographic transcriptions such as closed-
captions can be done in a few times real-time, and therefore are
quite a bit less costly. These transcriptions have the advantage that
they are already available for some television channels, and there-
fore do not have to be produced specifically for training speech
recognizers. However, closed-captions are a close, but not exact
transcription of what is being spoken, and are only coarsely time-
aligned with the audio signal. Hesitations and repetitions are not
marked and there may be word insertions, deletions and changes
in the word order. They also are missing some of the additional
information provided in the detailed speech transcriptions such as
the indication of acoustic conditions, speaker turns, speaker identi-
ties and gender and the annotation of non-speech segments such as
music. NIST found the disagreement between the closed-captions
and manual transcripts on a 10 hour subset of the TDT-2 data used
for the SDR evaluation to be on the order of 12% [5].
Another approach is to make use of other possible sources of
contemporaneous texts from newspapers, newswires, summaries
and the Internet. However, since these sources have only an indirect
correspondence with the audio data, they provide less supervision.
The basic idea is of light supervision is to use a speech recog-
nizer to automatically transcribe unannotated data, thus generat-
ing ?approximate? labeled training data. By iteratively increasing
the amount of training data, more accurate acoustic models are ob-
tained, which can then be used to transcribe another set of unanno-
tated data. The modified training procedure used in this work is:
1. Train a language model on all texts and closed captions after
normalization
2. Partition each show into homogeneous segments and label the
acoustic attributes (speaker, gender, bandwidth) [6]
3. Train acoustic models on a very small amount of manually
annotated data (1h)
4. Automatically transcribe a large amount of training data
5. (Optional) Align the closed-captions and the automatic tran-
scriptions (using a standard dynamic programming algorithm)
6. Run the standard acoustic model training procedure on the
speech segments (in the case of alignment with the closed
captions only keep segments where the two transcripts are in
agreement)
7. Reiterate from step 4.
It is easy to see that the manual work is considerably reduced, not
only in generating the annotated corpus but also during the training
procedure, since we no longer need to extend the pronunciation lex-
icon to cover all words and word fragments occurring in the training
data and we do not need to correct transcription errors. This ba-
sic idea was used to train acoustic models using the automatically
generated word transcriptions of the 500 hours of audio broadcasts
used in the spoken document retrieval task (part of the DARPA
TDT-2 corpus used in the SDR?99 and SDR?00 evaluations) [3].
This corpus is comprised of 902 shows from 6 sources broadcast
between January and June 1998: CNN Headline News (550 30-
minute shows), ABC World News Tonight (139 30-minute shows),
Public Radio International The World (122 1-hour shows), Voice of
America VOA Today and World Report (111 1-hour shows). These
shows contain about 22k stories with time-codes identifying the
beginning and end of each story.
First, the recognition performance as a function of the available
acoustic and language model training data was assessed. Then we
investigated the accuracy of the acoustic models obtained after rec-
ognizing the audio data using different levels of supervision via
the language model. With the exception of the baseline Hub4 lan-
guage models, none of the language models include a component
estimated on the transcriptions of the Hub4 acoustic training data.
The language model training texts come from contemporaneous
sources such as newspapers and newswires, and commercial sum-
maries and transcripts, and closed-captions. The former sources
have only an indirect correspondence with the audio data and pro-
vide less supervision than the closed captions. For each set of LM
training texts, a new word list was selected based on the word fre-
quencies in the training data. All language models are formed by
interpolating individual LMs built on each text source. The interpo-
lation coefficients were chosen in order to minimize the perplexity
on a development set composed of the second set of the Nov98
evaluation data (3h) and a 2h portion of the TDT2 data from Jun98
(not included in the LM training data). The following combinations
were investigated:
 LMa (baseline Hub4 LM): newspaper+newswire (NEWS), com-
mercial transcripts (COM) predating Jun98, acoustic transcripts
 LMn t c: NEWS, COM, closed-captions through May98
 LMn t: NEWS, COM through May98
 LMn c: NEWS, closed-captions through May98
 LMn: NEWS through May98
 LMn to: NEWS through May98, COM through Dec97
 LMno: NEWS through Dec97
Table 3: Word error rate for various conditions using acous-
tic models trained on the HUB4 training data with detailed
manual transcriptions. All runs were done in less than 10xRT,
except the last row. ?1S? designates one set of gender-
independent acoustic models, whereas ?4S? designates four sets
of gender and bandwidth dependent acoustic models.
Training Conditions bn99 1 bn99 2 Average
1h 1S, LMn t c 35.2 31.9 33.3
69h 1S, LMn t c 20.2 18.0 18.9
123h 1S, LMn t c 19.3 17.1 18.0
123h 4S, LMn t c 18.5 16.1 17.1
123h 4S, LMa 18.3 16.3 17.1
123h 4S, LMa, 50x 17.1 14.5 15.6
Table 4: Word error rate for different language models and increasing quantities of automatically labeled training data on the 1999
evaluation test sets using gender and bandwidth independent acoustic models. LMn t c: NEWS, COM, closed-captions through
May98 LMn t: NEWS, COM through May98 LMn c: NEWS, closed-captions through May98 LMn: NEWS through May98
LMn to: NEWS through May98, COM through Dec97 LMno: NEWS through Dec97.
Amount of training data %WER
raw unfiltered LMn t c LMn t LMn c LMn LMn to LMno
150h 123h 18.0 18.6 19.1 20.6 18.7 20.9
1h 1h 33.3 33.7 34.4 35.9 33.9 36.1
14h 8h 26.4 27.6 27.4 29.0 27.6 30.6
28h 17h 25.2 25.7 25.6 28.1 25.7 28.9
58h 28h 24.3 25.2 25.7 27.4 25.1 27.9
It should be noted that all of the conditions include newspaper
and newswire texts from the same epoch as the audio data. These
provide an important source of knowledge particularly with re-
spect to the vocabulary items. Conditions which include the closed
captions in the LM training data provide additional supervision in
the decoding process when transcribing audio data from the same
epoch.
For testing purposes we use the 1999 Hub4 evaluation data, which
is comprised of two 90 minute data sets selected by NIST. The first
set was extracted from 10 hours of data broadcast in June 1998,
and the second set from a set of broadcasts recorded in August-
September 1998 [16]. All recognition runs were carried out in un-
der 10xRT unless stated otherwise. The LIMSI 10x system ob-
tained a word error of 17.1% on the evaluation set (the combined
scores in the penultimate row in Table 3 4S, LMa) [8]. The word
error can be reduced to 15.6% for a system running at 50xRT (last
entry in Table 3).
As can be seen in Table 3, the word error rates with our orig-
inal Hub4 language model (LMa) and the one without the tran-
scriptions of the acoustic data (LMn t c) give comparable results
using the 1999 acoustic models trained on 123 hours of manually
annotated data (123h, 4S). The quality of the different language
models listed above are compared in the first row of Table 3 us-
ing speaker-independent (1S) acoustic models trained on the same
Hub4 data (123h). As can be observed, removing any text source
leads to a degradation in recognition performance. It appears it is
more important to include commercial transcripts (LMn t), even
if they are old (LMn to) than the closed captions (LMn c). This
suggests that the commercial transcripts more accurately represent
spoken language than closed-captioning. Even if only newspaper
and newswire texts are available (LMn), the word error increases
by only 14% over the best configuration (LMn t c), and even using
older newspaper and newswire texts (LMno) does not substantially
increase the word error rate. The second row of Table 3 gives the
word error rates with acoustic models trained on only 1 hour of
manually transcribed data. These are the models used to initialize
the process of automatically transcribing large quantities of data.
These word error rates range from 33% to 36% across the language
models.
We compared a straightforward approach of training on all the
automatically annotated data with one in which the closed-captions
are used to filter the hypothesized transcriptions, removing words
that are ?incorrect?. In the filtered case, the hypothesized transcrip-
tions are aligned with the closed captions story by story, and only
regions where the automatic transcripts agreed with the closed cap-
tions were kept for training purposes. To our surprise, somewhat
comparable recognition results were obtained both with and with-
out filtering, suggesting that inclusion of the closed-captions in the
language model training material provided sufficient supervision
(see Table 5).1 It should be noted that in both cases the closed-
caption story boundaries are used to delimit the audio segments
after automatic transcription.
To investigate this further we are assessing the effects of reduc-
ing the amount of supervision provided by the language model
training texts on the acoustic model accuracy (see Table 4). With
14 hours (raw) of approximately labeled training data, the word er-
ror is reduced by about 20% for all LMs compared with training on
1h of data which has carefully manual transcriptions. Using larger
amounts of data transcribed with the same initial acoustic models
gives smaller improvements, as seen by the entries for 28h and 58h.
The commercial transcripts (LMn+t and LMn+to), even if predat-
ing the data epoch, are seen to be more important than the closed-
captions (LMn+c), supporting the earlier observation that they are
closer to spoken language. Even if only news texts from the same
period (LMn) are available, these provide adequate supervision for
lightly supervised acoustic model training.
Table 5: Word error rates for increasing quantities of auto-
matically label training data on the 1999 evaluation test sets
using gender and bandwidth independent acoustic models with
the language model LMn t c (trained on NEWS, COM, closed-
captions through May98).
Amount of training data %WER
raw unfiltered filtered unfiltered filtered
14h 8h 6h 26.4 25.7
28h 17h 13h 25.2 23.7
58h 28h 21h 24.3 22.5
140h 76h 57h 22.4 21.1
287h 140h 108h 21.0 19.9
503h 238h 188h 20.2 19.4
6. TASK ADAPTATION
The experiments reported in the section 3 show that while direct
recognition with the reference BN acoustic models gives relatively
1The difference in the amounts of data transcribed and actually
used for training is due to three factors. The first is that the total du-
ration includes non-speech segments which are eliminated prior to
recognition during partitioning. Secondly, the story boundaries in
the closed captions are used to eliminate irrelevant portions, such
as commercials. Thirdly, since there are many remaining silence
frames, only a portion of these are retained for training.
Table 6: Word error rates (%) for TI-digits, ATIS94, WSJ95 and S9 WSJ93 test sets after recognition with three different configura-
tions, all including task-specific lexica and LMs: (left) BN acoustic models, (middle left) unsupervised adaptation of the BN acoustic
models, (middle right) supervised adaptation of the BN acoustic models and (right) task-dependent acoustic models.
Test Set BN models Unsupervised Adaptation Supervised Adaptation Task-dep. models
BN models BN models
TI-digits 1.7 0.8 0.5 0.4
ATIS94 4.7 4.7 3.2 4.4
WSJ95 9.0 6.9 6.7 7.6
S9 WSJ93 13.6 12.6 11.4 15.3
competitive results, the WER on the targeted tasks can still be im-
proved. Since we want to minimize the cost and effort involved in
tuning to a target task, we are investigating methods to transpar-
ently adapt the reference acoustic models. By transparent we mean
that the procedure is automatic and can be carried out without any
human expertise. We therefore apply the approach presented in the
previous section, that is the reference BN system is used to tran-
scribe the training data of the destination task. This supposes of
course that audio data have been collected. However, this can be
carried out with an operational system and the cost of collecting
task-specific training data is greatly reduced since no manual tran-
scriptions are needed. The performance of the BN models under
cross task conditions is well within the range for which the approx-
imate transcriptions can be used for acoustic model adaptation.
The reference acoustic models are then adapted by means of a
conventional adaptation technique such as MLLR and MAP. Thus
there is no need to design a new set of models based on the training
data characteristics. Adaptation is also preferred to the training
of new models as it is likely that the new training data will have
a lower phonemic contextual coverage than the original reference
models.
The cross-task unsupervisedadaptation is evaluated for the tasks:
TI-digits, ATIS and WSJ. The 100 hours of the WSJ data were tran-
scribed using the BN acoustic and language models. For ATIS, only
26 of the 40 hours of training data from 276 speakers were tran-
scribed, due to time constraints. For TI-digits, the training data was
transcribed using a mixed configuration, combining the BN acous-
tic models with the simple digit loop grammar.2 For completeness
we also used the task-specific audio data and the associated tran-
scriptions to carry out supervised adaptation of the BN models.
Gender-dependent acoustic models were estimated using the cor-
responding gender-dependent BN models as seeds and the gender-
specific training utterances as adaptation data. For WSJ and ATIS,
the speaker ids were directly used for gender identification since
in previous experiments with this test set there were no gender
classification errors. Only the acoustic models used in the sec-
ond and third word decoding passes have been adapted. For the
TI-digits, the gender of each training utterance was automatically
classified by decoding each utterance twice, once with each set of
gender-dependent models. Then, the utterance gender was deter-
mined based on the best global score between the male and female
models (99.0% correct classification).
Both the MLLR and MAP adaptation techniques were applied.
The recognition tests were carried out under mixed conditions (i.e.,
with the adapted acoustic models and the task-dependent LM). The
2In order to assess the quality of the automatic transcription, we
compared the system hypotheses to the manually provided training
transcriptions. For resulting word error rates on the training data
are 11.8% for WSJ, 29.1% for ATIS and 1.2% for TI-digits.
BN models are first adapted using MLLR with a global transforma-
tion, followed by MAP adaptation.
The word error rates obtained with the task-adapted BN mod-
els are given in Table 6 for the four test sets. Using unsupervised
adaptation the performance is improved for TIdigits (53% relative),
WSJ (19% relative) and S9 (7% relative).
The manual transcriptions for the targeted tasks were used to
carry out supervised model adaptation. The results (see the 4th col-
umn of Table 6) show a clear improvement over unsupervisedadap-
tation for both the TI-digits (60% relative) and ATIS (47% relative)
tasks. A smaller gain of about 10% relative is obtained for the spon-
taneous dictation task, and only 3% relative for read WSJ data. The
gain appears to be correlated with the WER of the transcribed data:
the difference between BN and task specific models is smaller for
WSJ than ATIS and TI-digits. The TI-digit task is the only task for
which the best performance is obtained using task-dependent mod-
els rather than BN models adapted with supervised. For the other
tasks, the lowest WER is obtained when the supervised adapted BN
acoustic models are used: 3.2% for ATIS, 6.7% for WSJ and 11.4%
for S9. This result confirms our hypothesis that better performance
can be achieved by adapting generic models with task-specific data
than by directly training task-specific models.
7. CONCLUSIONS
This paper has explored methods to reduce the cost of developing
models for speech recognizers. Two main axes have been explored:
developing generic acoustic models and the use of low cost data for
acoustic model training.
We have explored the genericity of state-of-the-art speech recog-
nition systems, by testing a relatively wide-domain system on data
from three tasks ranging in complexity. The generic models were
taken from the broadcast news task which covers a wide range of
acoustic and linguistic conditions. These acoustic models are rel-
atively task-independent as there is only a small increase in word
error relative to the word error obtained with task-dependent acous-
tic models, when a task-dependent language model is used. There
remains a large difference in performance on the digit recogni-
tion task which can be attributed to the limited phonetic coverage
of this task. On a spontaneous WSJ dictation task, the broadcast
news acoustic and language are more robust to deviations in speak-
ing style than the read-speech WSJ models. We also have shown
that unsupervised acoustic model adaptation can reduce the perfor-
mance gap between task-independent and task-dependent acoustic
models, and that supervised adaptation of generic models can lead
to better performance than that achieved with task-specific models.
Both supervised and unsupervised adaptation are less effective for
the digits task indicating that these may be a special case.
We have investigated the use of low cost data to train acoustic
models for broadcast news transcription, with supervision provided
the language models. Recognition results obtained with acoustic
models trained on large quantities of automatically annotated data
are comparable (under a 10% relative increase in word error) to
results obtained with acoustic models trained on large quantities
of manually annotated data. Given the significantly higher cost of
detailed manual transcription (substantially more time consuming
than producing commercial transcripts, and more expensive since
closed captions and commercial transcripts are produced for other
purposes), such approaches are very promising as they require sub-
stantial computation time, but little manual effort. Another advan-
tage offered by this approach is that there is no need to extend the
pronunciation lexicon to cover all words and word fragments oc-
curring in the training data. By eliminating the need for manual
transcription, automated training can be applied to essentially un-
limited quantities of task-specific training data. While the focus of
our work has been on reducing training costs and task portability,
we have been exploring these in a multi-lingual context.
REFERENCES
[1] G. Adda, M. Jardino, J.L. Gauvain, ?Language Modeling for Broad-
cast News Transcription,? ESCA Eurospeech?99, Budapest, 4, pp.
1759-1760, Sept. 1999.
[2] C. Barras, E. Geoffrois et al,?Transcriber: development and use of a
tool for assisting speech corpora production,? SpeechCommunication,
33(1-2), pp. 5-22, Jan. 2001.
[3] C. Cieri, D. Graff, M. Liberman, ?The TDT-2 Text and Speech
Corpus,? DARPA Broadcast News Workshop, Herndon. (see also
http://morph.ldc.upenn.edu/TDT).
[4] D. Dahl, M. Bates et al, ?Expanding the Scope of the ATIS Task : The
ATIS-3 Corpus,? Proc. ARPA Spoken Language Systems Technology
Workshop, Plainsboro, NJ, pp. 3-8, 1994.
[5] J. Garofolo, C. Auzanne, E. Voorhees, W. Fisher, ?1999 TREC-8 Spo-
ken Document Retrieval Track Overview and Results,? 8th Text Re-
trieval Conference TREC-8, Nov. 1999.
[6] J.L. Gauvain, G. Adda, et al, ?Transcribing Broadcast News: The
LIMSI Nov96 Hub4 System,? Proc. ARPA Speech Recognition Work-
shop, pp. 56-63, Chantilly, Feb. 1997.
[7] J.L. Gauvain, C.H. Lee, ?Maximum a Posteriori Estimation for Mul-
tivariate Gaussian Mixture Observation of Markov Chains,? IEEE
Trans. on SAP, 2(2), pp. 291-298, April 1994.
[8] J.L. Gauvain, L. Lamel, ?Fast Decoding for Indexation of Broadcast
Data,? ICSLP?2000, 3, pp. 794-798, Beijing, Oct. 2000.
[9] D. Graff, ?The 1996 Broadcast News Speech and Language-Model
Corpus,? Proc. DARPA Speech Recognition Workshop, Chantilly, VA,
pp. 11-14, Feb. 1999.
[10] T. Kemp, A. Waibel, ?UnsupervisedTraining of a Speech Recognizer:
Recent Experiments,? Eurospeech?99, 6, Budapest, pp. 2725-2728,
Sept. 1999.
[11] F. Kubala, J. Cohen et al, ?The Hub and Spoke Paradigm for CSR
Evaluation,? Proc. ARPA SpokenLanguageSystems TechnologyWork-
shop, Plainsboro, NJ, pp. 9-14, 1994.
[12] L. Lamel, J.L. Gauvain, G. Adda, ?Lightly Supervised Acoustic
Model Training,? Proc. ISCA ITRW ASR2000, pp. 150-154, Paris,
Sept. 2000.
[13] C.J. Leggetter, P.C. Woodland, ?Maximum likelihood linear regres-
sion for speaker adaptation of continuous density hidden Markov
models,? Computer Speech & Language, 9(2), pp. 171-185, 1995.
[14] R.G. Leonard, ?A Database for speaker-independent digit recogni-
tion,? Proc. ICASSP, 1984.
[15] D.S. Pallett, J.G. Fiscus, et al ?1998 Broadcast News Benchmark Test
Results,? Proc. DARPA Broadcast News Workshop, pp. 5-12, Hern-
don, VA, Feb. 1999.
[16] D. Pallett, J. Fiscus, M. Przybocki, ?Broadcast News 1999 Test Re-
sults,? NIST/NSA Speech Transcription Workshop, College Park, May
2000.
[17] D.B. Paul, J.M. Baker, ?The Design for the Wall Street Journal-based
CSR Corpus,? Proc. ICSLP, Kobe, Nov. 1992.
[18] G. Zavaliagkos, T. Anastsakos et al, ?ImprovedSearch, Acoustic, and
Language Modeling in the BBN BYBLOS Large Vocabulary CSR
Systems,? Proc. ARPA Spoken Language Systems Technology Work-
shop, Plainsboro, NJ, pp. 81-88, 1994.
[19] G. Zavaliagkos, T. Colthurst, ?Utilizing Untranscribed Training Data
to Improve Performance,? DARPA Broadcast News Transcription and
Understanding Workshop, Landsdowne, pp. 301-305, Feb. 1998.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 201?208, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Training Neural Network Language Models
On Very Large Corpora ?
Holger Schwenk and Jean-Luc Gauvain
LIMSI-CNRS
BP 133, 91436 Orsay cedex, FRANCE
schwenk,gauvain@limsi.fr
Abstract
During the last years there has been grow-
ing interest in using neural networks for
language modeling. In contrast to the well
known back-off n-gram language models,
the neural network approach attempts to
overcome the data sparseness problem by
performing the estimation in a continuous
space. This type of language model was
mostly used for tasks for which only a
very limited amount of in-domain training
data is available.
In this paper we present new algorithms to
train a neural network language model on
very large text corpora. This makes pos-
sible the use of the approach in domains
where several hundreds of millions words
of texts are available. The neural network
language model is evaluated in a state-of-
the-art real-time continuous speech recog-
nizer for French Broadcast News. Word
error reductions of 0.5% absolute are re-
ported using only a very limited amount
of additional processing time.
1 Introduction
Language models play an important role in many
applications like character and speech recognition,
machine translation and information retrieval. Sev-
eral approaches have been developed during the last
?This work was partially financed by the European Commis-
sion under the FP6 Integrated Project TC-STAR.
decades like n-gram back-off word models (Katz,
1987), class models (Brown et al, 1992), structured
language models (Chelba and Jelinek, 2000) or max-
imum entropy language models (Rosenfeld, 1996).
To the best of our knowledge word and class n-gram
back-off language models are still the dominant ap-
proach, at least in applications like large vocabulary
continuous speech recognition or statistical machine
translation. In many publications it has been re-
ported that modified Kneser-Ney smoothing (Chen
and Goodman, 1999) achieves the best results. All
the reference back-off language models (LM) de-
scribed in this paper are build with this technique,
using the SRI LM toolkit (Stolcke, 2002).
The field of natural language processing has re-
cently seen some changes by the introduction of new
statistical techniques that are motivated by success-
ful approaches from the machine learning commu-
nity, in particular continuous space LMs using neu-
ral networks (Bengio and Ducharme, 2001; Bengio
et al, 2003; Schwenk and Gauvain, 2002; Schwenk
and Gauvain, 2004; Emami and Jelinek, 2004), Ran-
dom Forest LMs (Xu and Jelinek, 2004) and Ran-
dom cluster LMs (Emami and Jelinek, 2005). Usu-
ally new approaches are first verified on small tasks
using a limited amount of LM training data. For
instance, experiments have been performed using
the Brown corpus (1.1M words), parts of the Wall-
street journal corpus (19M words) or transcriptions
of acoustic training data (up to 22M words). It is
much more challenging to compare the new statis-
tical techniques to carefully optimized back-off LM
trained on large amounts of data (several hundred
millions words). Training may be difficult and very
201
time consuming and the algorithms used with sev-
eral tens of millions examples may be impracticable
for larger amounts. Training back-off LMs on large
amounts of data is not a problem, as long as power-
ful machines with enough memory are available in
order to calculate the word statistics. Practice has
also shown that back-off LMs seem to perform very
well when large amounts of training data are avail-
able and it is not clear that the above mentioned new
approaches are still of benefit in this situation.
In this paper we compare the neural network
language model to n-gram model with modified
Kneser-Ney smoothing using LM training corpora
of up to 600M words. New algorithms are pre-
sented to effectively train the neural network on such
amounts of data and the necessary capacity is ana-
lyzed. The LMs are evaluated in a real-time state-
of-the-art speech recognizer for French Broadcast
News. Word error reductions of up to 0.5% abso-
lute are reported.
2 Architecture of the neural network LM
The basic idea of the neural network LM is to project
the word indices onto a continuous space and to use
a probability estimator operating on this space (Ben-
gio and Ducharme, 2001; Bengio et al, 2003). Since
the resulting probability functions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. A neural
network can be used to simultaneously learn the pro-
jection of the words onto the continuous space and
to estimate the n-gram probabilities. This is still a
n-gram approach, but the LM posterior probabilities
are ?interpolated? for any possible context of length
n-1 instead of backing-off to shorter contexts.
The architecture of the neural network n-gram
LM is shown in Figure 1. A standard fully-
connected multi-layer perceptron is used. The
inputs to the neural network are the indices of
the n?1 previous words in the vocabulary hj =
wj?n+1, ..., wj?2, wj?1 and the outputs are the pos-
terior probabilities of all words of the vocabulary:
P (wj = i|hj) ?i ? [1, N ] (1)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the i-th word
of the vocabulary is coded by setting the i-th ele-
ment of the vector to 1 and all the other elements to
projection
layer hidden
layer
output
layerinput
projections
shared
continuousrepresentation: representation:
indices in wordlist
LM probabilitiesdiscrete for all words
probability estimation
Neural Network
N
wj?1 P
H
N
P (wj=1|hj)
wj?n+1
wj?n+2
P (wj=i|hj)
P (wj=N|hj)
P dimensional vectors
ck
oiM
Vdj
p1 =
pN =
pi =
Figure 1: Architecture of the neural network
language model. hj denotes the context
wj?n+1, ..., wj?1. P is the size of one projec-
tion and H and N is the size of the hidden and
output layer respectively. When shortlists are used
the size of the output layer is much smaller then the
size of the vocabulary.
0. The i-th line of the N ?P dimensional projection
matrix corresponds to the continuous representation
of the i-th word. Let us denote ck these projections,
dj the hidden layer activities, oi the outputs, pi their
softmax normalization, and mjl, bj , vij and ki the
hidden and output layer weights and the correspond-
ing biases. Using these notations the neural network
performs the following operations:
dj = tanh
(
?
l
mjl cl + bj
)
(2)
oi =
?
j
vij dj + ki (3)
pi = e
oi /
N?
k=1
eok (4)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj). Training is
performed with the standard back-propagation algo-
rithm minimizing the following error function:
E =
N?
i=1
ti log pi + ?(
?
jl
m2jl +
?
ij
v2ij) (5)
where ti denotes the desired output, i.e., the proba-
bility should be 1.0 for the next word in the training
202
sentence and 0.0 for all the other ones. The first part
of this equation is the cross-entropy between the out-
put and the target probability distributions, and the
second part is a regularization term that aims to pre-
vent the neural network from overfitting the training
data (weight decay). The parameter ? has to be de-
termined experimentally.
It can be shown that the outputs of a neural net-
work trained in this manner converge to the posterior
probabilities. Therefore, the neural network directly
minimizes the perplexity on the training data. Note
also that the gradient is back-propagated through the
projection-layer, which means that the neural net-
work learns the projection of the words onto the con-
tinuous space that is best for the probability estima-
tion task. The complexity to calculate one probabil-
ity with this basic version of the neural network LM
is quite high:
O = (n? 1)? P ?H + H + H ?N + N (6)
where P is the size of one projection and H and N is
the size of the hidden and output layer respectively.
Usual values are n=4, P=50 to 200, H=400 to 1000
and N=40k to 200k. The complexity is dominated
by the large size of the output layer. In this paper the
improvements described in (Schwenk, 2004) have
been used:
1. Lattice rescoring: speech recognition is done
with a standard back-off LM and a word lattice
is generated. The neural network LM is then
used to rescore the lattice.
2. Shortlists: the neural network is only used to
predict the LM probabilities of a subset of the
whole vocabulary.
3. Regrouping: all LM probabilities needed for
one lattice are collected and sorted. By these
means all LM probability requests with the
same context ht lead to only one forward pass
through the neural network.
4. Block mode: several examples are propagated
at once through the neural network, allowing
the use of faster matrix/matrix operations.
5. CPU optimization: machine specific BLAS
libraries are used for fast matrix and vector op-
erations.
The idea behind shortlists is to use the neural
network only to predict the s most frequent words,
s ? |V |, reducing by these means drastically the
complexity. All words of the word list are still con-
sidered at the input of the neural network. The LM
probabilities of words in the shortlist (P?N ) are cal-
culated by the neural network and the LM probabil-
ities of the remaining words (P?B) are obtained from
a standard 4-gram back-off LM:
P? (wt|ht) =
{
P?N (wt|ht)PS(ht) if wt ? shortlist
P?B(wt|ht) else
(7)
PS(ht) =
?
w?shortlist(ht)
P?B(w|ht) (8)
It can be considered that the neural network redis-
tributes the probability mass of all the words in the
shortlist. This probability mass is precalculated and
stored in the data structures of the back-off LM. A
back-off technique is used if the probability mass for
a requested input context is not directly available.
Normally, the output of a speech recognition sys-
tem is the most likely word sequence given the
acoustic signal, but it is often advantageous to pre-
serve more information for subsequent processing
steps. This is usually done by generating a lattice,
a graph of possible solutions where each arc cor-
responds to a hypothesized word with its acoustic
and language model scores. In the context of this
work LIMSI?s standard large vocabulary continuous
speech recognition decoder is used to generate lat-
tices using a n-gram back-off LM. These lattices are
then processed by a separate tool and all the LM
probabilities on the arcs are replaced by those calcu-
lated by the neural network LM. During this lattice
rescoring LM probabilities with the same context ht
are often requested several times on potentially dif-
ferent nodes in the lattice. Collecting and regrouping
all these calls prevents multiple forward passes since
all LM predictions for the same context are immedi-
ately available at the output.
Further improvements can be obtained by prop-
agating several examples at once though the net-
work, also known as bunch mode (Bilmes et al,
1997; Schwenk, 2004). In comparison to equation 2
and 3, this results in using matrix/matrix instead of
matrix/vector operations which can be aggressively
optimized on current CPU architectures. The Intel
203
Math Kernel Library was used.1 Bunch mode is also
used for training the neural network. Training of a
typical network with a hidden layer with 500 nodes
and a shortlist of length 2000 (about 1M parameters)
take less than one hour for one epoch through four
million examples on a standard PC.
3 Application to Speech Recognition
In this paper the neural network LM is evaluated
in a real-time speech recognizer for French Broad-
cast News. This is a very challenging task since
the incorporation of the neural network LM into
the speech recognizer must be very effective due
to the time constraints. The speech recognizer it-
self runs in 0.95xRT2 and the neural network in less
than 0.05xRT. The compute platform is an Intel Pen-
tium 4 extreme (3.2GHz, 4GB RAM) running Fe-
dora Core 2 with hyper-threading.
The acoustic model uses tied-state position-
dependent triphones trained on about 190 hours of
Broadcast News data. The speech features consist
of 39 cepstral parameters derived from a Mel fre-
quency spectrum estimated on the 0-8kHz band (or
0-3.8kHz for telephone data) every 10ms. These
cepstral coefficients are normalized on a segment
cluster basis using cepstral mean removal and vari-
ance normalization. The feature vectors are linearly
transformed (MLLT) to better fit the diagonal co-
variance Gaussians used for acoustic modeling.
Decoding is performed in two passes. The first
fast pass generates an initial hypothesis, followed
by acoustic model adaptation (CMLLR and MLLR)
and a second decode pass using the adapted mod-
els. Each pass generates a word lattice which is ex-
panded with a 4-gram LM. The best solution is then
extracted using pronunciation probabilities and con-
sensus decoding. Both passes use very tight prun-
ing thresholds, especially for the first pass, and fast
Gaussian computation based on Gaussian short lists.
For the final decoding pass, the acoustic models
include 23k position-dependent triphones with 12k
tied states, obtained using a divisive decision tree
based clustering algorithm with a 35 base phone set.
1http://www.intel.com/software/products/mkl/
2In speech recognition, processing time is measured in mul-
tiples of the length of the speech signal, the real time factor
xRT. For a speech signal of 2h, a processing time of 0.5xRT
corresponds to 1h of calculation.
The system is described in more detail in (Gauvain
et al, 2005).
The neural network LM is used in the last pass
to rescore the lattices. A short-list of length 8192
was used in order to fulfill the constraints on the pro-
cessing time (the complexity of the neural network
to calculate a LM probability is almost linear with
the length of the short-list). This gives a coverage of
about 85% when rescoring the lattices, i.e. the per-
centage of LM requests that are actually performed
by the neural network.
3.1 Language model training data
The following resources have been used for lan-
guage modeling:
? Transcriptions of the acoustic training data
(4.0M words)
? Commercial transcriptions (88.5M words)
? Newspaper texts (508M words)
? WEB data (13.6M words)
First a language model was built for each cor-
pus using modified Kneser-Ney smoothing as imple-
mented in the SRI LM toolkit (Stolcke, 2002). The
individual LMs were then interpolated and merged
together. An EM procedure was used to determine
the coefficients that minimize the perplexity on the
development data. Table 1 summarizes the charac-
teristics of the individual text corpora.
corpus #words Perpl. Coeffs.
Acoustic transcr. 4M 107.4 0.43
Commercial transcr. 88.5M 137.8 0.14
Newspaper texts 508M 103.0 0.35
WEB texts 13.6M 136.7 0.08
All interpolated 614M 70.2 -
Table 1: Characteristics of the text corpora (number
of words, perplexity on the development corpus and
interpolation coefficients)
Although the detailed transcriptions of the audio
data represent only a small fraction of the available
data, they get an interpolation coefficient of 0.43.
This shows clearly that they are the most appropriate
text source for the task. The commercial transcripts,
204
the newspaper and WEB texts reflect less well the
speaking style of broadcast news, but this is to some
extent counterbalanced by the large amount of data.
One could say that these texts are helpful to learn
the general grammar of the language. The word list
includes 65301 words and the OOV rate is 0.95% on
a development set of 158k words.
3.2 Training on in-domain data only
Following the above discussion, it seems natural to
first train a neural network LM on the transcrip-
tions of the acoustic data only. The architecture
of the neural network is as follows: a continuous
word representation of dimension 50, one hidden
layer with 500 neurons and an output layer limited
to the 8192 most frequent words. This results in
3.2M parameters for the continuous representation
of the words and about 4.2M parameters for the sec-
ond part of the neural network that estimates the
probabilities. The network is trained using standard
stochastic back-propagation.3 The learning rate was
set to 0.005 with an exponential decay and the regu-
larization term is weighted with 0.00003. Note that
fast training of neural networks with more than 4M
parameters on 4M examples is already a challenge.
The same fast algorithms as described in (Schwenk,
2004) were used. Apparent convergence is obtained
after about 40 epochs though the training data, each
one taking 2h40 on standard PC equipped with two
Intel Xeon 2.8GHz CPUs.
The neural network LM alone achieves a perplex-
ity of 103.0 which is only a 4% relative reduction
with respect to the back-off LM (107.4, see Table 1).
If this neural network LM is interpolated with the
back-off LM trained on the whole training set the
perplexity decreases from 70.2 to 67.6. Despite this
small improvements in perplexity a notable word er-
ror reduction was obtained from 14.24% to 14.02%,
with the lattice rescoring taking less than 0.05xRT.
In the following sections, it is shown that larger im-
provements can be obtained by training the neural
network on more data.
3.3 Adding selected data
Training the neural network LM with stochastic
back-propagation on all the available text corpora
3The weights are updated after each example.
would take quite a long time. The estimated time
for one training epoch with the 88M words of com-
mercial transcriptions is 58h, and more than 12 days
if all the 508M words of newspaper texts were used.
This is of course not very practicable. One solution
to this problem is to select a subset of the data that
seems to be most useful for the task. This was done
by selecting six month of the commercial transcrip-
tions that minimize the perplexity on the develop-
ment set. This gives a total of 22M words and the
training time is about 14h per epoch.
One can ask if the capacity of the neural network
should be augmented in order to deal with the in-
creased number of examples. Experiments with hid-
den layer sizes from 400 to 1000 neurons have been
performed (see Table 2).
size 400 500 600 1000?
Tr. time 11h20 13h50 16h15 11+16h
Px alone 100.5 100.1 99.5 94.5
interpol. 68.3 68.3 68.2 68.0
Werr 13.99% 13.97% 13.96% 13.92%
? Interpolation of networks with 400 and 600
hidden units.
Table 2: Performance for a neural network LM and
training time per epoch as a function of the size of
the hidden layer (fixed 6 months subset of commer-
cial transcripts).
Although there is a small decrease in perplexity
and word error when increasing the dimension of the
hidden layer, this is at the expense of a higher pro-
cessing time. The training and recognition time are
in fact almost linear to the size of the hidden layer.
An alternative approach to augment the capacity of
the neural network is to modify the dimension of the
continuous representation of the words (in the range
50 to 150). The idea behind this is that the proba-
bility estimation may be easier in a higher dimen-
sional space (instead of augmenting the capacity of
the non-linear probability estimator itself). This is
similar in spirit to the theory behind support vector
machines (Vapnik, 1998).
Increasing the dimension of the projection layer
has several advantages as can be seen from the Fig-
ure 2. First, the perplexity and word error rates
are lower than those obtained when the size of the
205
 90
 95
 100
 105
 110
 115
 120
 0  10  20  30  40  50
Pe
rp
le
xi
ty
Epochs
dim 50
dim 60
dim 70
dim 100
dim 120
dim 150
Figure 2: Perplexity in function of the size of the
continuous word representation (500 hidden units,
fixed 6 months subset of commercial transcripts).
hidden layer is increased. Second, convergence is
faster: the best result is obtained after about 15
epochs while up to 40 are needed with large hidden
layers. Finally, increasing the size of the continu-
ous word representation has only a small effect on
the training and recognition complexity of the neu-
ral network4 since most of the calculation is done
to propagate and learn the connections between the
hidden and the output layer (see equation 6). The
best result was obtained with a 120 dimensional
continuous word representation. The perplexity is
67.9 after interpolation with the back-off LM and
the word error rate is 13.88%.
3.4 Training on all available data
In this section an algorithm is proposed for training
the neural network on arbitrary large training cor-
pora. The basic idea is quite simple: instead of
performing several epochs over the whole training
data, a different small random subset is used at each
epoch. This procedure has several advantages:
? There is no limit on the amount of training data,
? After some epochs, it is likely that all the train-
ing examples have been seen at least once,
? Changing the examples after each epoch adds
noise to the training procedure. This potentially
increases the generalization performance.
This algorithm is summarized in figure 4. The
parameters of this algorithm are the size of the ran-
dom subsets that are used at each epoch. We chose
414h20 for P=120 and H=500.
 80
 85
 90
 95
 100
 105
 110
 115
 120
 0  5  10  15  20  25  30  35  40  45  50
Pe
rp
le
xi
ty
Epochs
6 month fix
1% resampled
5% resampled
10% resampled
20% resampled
Figure 3: Perplexity when resampling different ran-
dom subsets of the commercial transcriptions. (word
representation of dimension 120, 500 hidden units)
to always use the full corpus of transcriptions of the
acoustic data since this is the most appropriate data
for the task. Experiments with different random sub-
sets of the commercial transcriptions and the news-
paper texts have been performed (see Figure 3 and
5). In all cases the same neural network architecture
was used, i.e a 120 dimensional continuous word
representation and 500 hidden units. Some experi-
ments with larger hidden units showed basically the
same convergence behavior. The learning rate was
again set to 0.005, but with a slower exponential de-
cay.
First of all it can be seen from Figure 3 that the
results are better when using random subsets instead
of a fixed selection of 6 months, although each ran-
dom subset is actually smaller (for instance a total of
12.5M examples for a subset of 10%). Best results
were obtained when taking 10% of the commercial
+ Train network for one epoch
Repeat
Select training data:
? Use all acoustic transcriptions (4M words)
? Extract random subset of examples  from the large corpora
? Shuffle data
   (performing weight updates after each example)
+ Test performance on development data
Until convergence
Figure 4: Training algorithm for large corpora
206
Back-off LM Neural Network LM
Training data [#words] 600M 4M 22M 92.5M? 600M?
Training time [h/epoch] - 2h40 14h 9h40 12h 3 ? 12h
Perplexity (NN LM alone) - 103.0 97.5 84.0 80.0 76.5
Perplexity (interpolated LMs) 70.2 67.6 67.9 66.7 66.5 65.9
Word error rate (interpolated LMs) 14.24% 14.02% 13.88% 13.81% 13.75% 13.61%
? By resampling different random parts at the beginning of each epoch.
Table 3: Comparison of the back-off and the neural network LM using different amounts of training data.
The perplexities are given for the neural network LM alone and interpolated with the back-off LM trained
on all the data. The last column corresponds to three interpolated neural network LMs.
transcriptions. The perplexity is 66.7 after interpo-
lation with the back-off LM and the word error rate
is 13.81% (see summary in Table 3). Larger sub-
sets of the commercial transcriptions lead to slower
training, but don?t give better results.
Encouraged by these results, we also included the
508M words of newspaper texts in the training data.
The size of the random subsets were chosen in order
to use between 4 and 9M words of each corpus. Fig-
ure 5 summarizes the results. There seems to be no
obvious benefit from resampling large subsets of the
individual corpora. We choose to resample 10% of
the commercial transcriptions and 1% of the news-
paper texts.
 80
 85
 90
 95
 100
 105
 110
 0  5  10  15  20  25  30  35  40  45  50
Pe
rp
le
xi
ty
Epochs
6 month transcription fix
10% transcriptions
5% transcr + 1% journal
5% transcr + 2% journal
10% transcr + 1% journal
10% transcr + 2% journal
Figure 5: Perplexity when resampling different ran-
dom subsets of the commercial transcriptions and
the newspaper texts.
Table 3 summarizes the results of the different
neural network LMs. It can be clearly seen that the
perplexity of the neural network LM alone decreases
significantly with the amount of training data used.
The perplexity after interpolation with the back-off
LM changes only by a small amount, but there is a
notable improvement in word error rate. This is an-
other experimental evidence that the perplexity of a
LM is not directly related to the word error rate.
The best neural network LM achieves a word er-
ror reduction of 0.5% absolute with respect to the
carefully tuned back-off LM (14.24% ? 13.75%).
The additional processing time needed to rescore the
lattices is less than 0.05xRT. This is a significant im-
provement, in particular for a fast real-time continu-
ous speech recognition system. When more process-
ing time is available a word error rate of 13.61% can
be achieved by interpolating three neural networks
together (in 0.14xRT).
3.5 Using a better speech recognizer
The experimental results have also been validated
using a second speech recognizer running in about
7xRT. This systems differs from the real-time recog-
nizer by a larger 200k word-list, additional acoustic
model adaptation passes and less pruning. Details
are described in (Gauvain et al, 2005). The word er-
ror rate of the reference system using a back-off LM
is 10.74%. This can be reduced to 10.51% using a
neural network LM trained on the fine transcriptions
only and to 10.20% when the neural network LM
is trained on all data using the described resampling
approach. Lattice rescoring takes about 0.2xRT.
4 Conclusions and future work
Neural network language models are becoming a
serious alternative to the widely used back-off lan-
guage models. Consistent improvements in perplex-
ity and word error rate have been reported (Bengio
et al, 2003; Schwenk and Gauvain, 2004; Schwenk
and Gauvain, 2005; Emami and Jelinek, 2004). In
these works, the amount of training data was how-
207
ever limited to a maximum of 20M words due to the
high complexity of the training algorithm.
In this paper new techniques have been described
to train neural network language models on large
amounts of text corpora (up to 600M words). The
evaluation with a state-of-the-art speech recognition
system for French Broadcast News showed a signif-
icant word error reduction of 0.5% absolute. The
neural network LMs is incorporated into the speech
recognizer by rescoring lattices. This is done in less
than 0.05xRT.
Several extensions of the learning algorithm it-
self are promising. We are in particular interested
in smarter ways to select different subsets from the
large corpus at each epoch (instead of a random
choice). One possibility would be to use active
learning, i.e. focusing on examples that are most
useful to decrease the perplexity. One could also
imagine to associate a probability to each training
example and to use these probabilities to weight the
random sampling. These probabilities would be up-
dated after each epoch. This is similar to boosting
techniques (Freund, 1995) which build sequentially
classifiers that focus on examples wrongly classified
by the preceding one.
5 Acknowledgment
The authors would like to thank Yoshua Bengio for
fruitful discussions and helpful comments. The au-
thors would like to recognize the contributions of
G. Adda, M. Adda and L. Lamel for their involve-
ment in the development of the speech recognition
systems on top of which this work is based.
References
Yoshua Bengio and Rejean Ducharme. 2001. A neural
probabilistic language model. In NIPS, volume 13.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3(2):1137?1155.
Jeff Bilmes, Krste Asanovic, Chee whye Chin, and Jim
Demmel. 1997. Using phipac to speed error back-
propagation learning. In ICASSP, pages V:4153?
4156.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?470.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech & Language,
13(4):283?332.
Stanley F. Chen and Joshua T. Goodman. 1999. An
empirical study of smoothing techniques for language
modeling. Computer Speech & Language, 13(4):359?
394.
Ahmad Emami and Frederick Jelinek. 2004. Exact train-
ing of a neural syntactic language model. In ICASSP,
pages I:245?248.
Ahmad Emami and Frederick Jelinek. 2005. Random
clusterings for language modeling. In ICASSP, pages
I:581?584.
Yoav Freund. 1995. Boosting a weak learning al-
gorithm by majority. Information and Computation,
121(2):256?285.
Jean-Luc Gauvain, Gilles Adda, Martine Adda-Decker,
Alexandre Allauzen, Veronique Gendner, Lori Lamel,
and Holger Schwenk. 2005. Where are we in tran-
scribing BN french? In Eurospeech.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of
a speech recognizer. IEEE Transactions on ASSP,
35(3):400?401.
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech & Language, 10(3):187?228.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In ICASSP, pages I: 765?
768.
Holger Schwenk and Jean-Luc Gauvain. 2004. Neu-
ral network language models for conversational speech
recognition. In ICSLP, pages 1215?1218.
Holger Schwenk and Jean-Luc Gauvain. 2005. Build-
ing continuous space language models for transcribing
european languages. In Eurospeech.
Holger Schwenk. 2004. Efficient training of large neu-
ral networks for language modeling. In IJCNN, pages
3059?3062.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP, pages II: 901?904.
Vladimir Vapnik. 1998. Statistical Learning Theory.
Wiley, New York.
Peng Xu and Frederick Jelinek. 2004. Random forest in
language modeling. In EMNLP, pages 325?332.
208
Processing Broadcast Audio for Information Access
Jean-Luc Gauvain, Lori Lamel, Gilles Adda, Martine Adda-Decker,
Claude Barras, Langzhou Chen, and Yannick de Kercadio
Spoken Language Processing Group
LIMSI-CNRS, B.P. 133, 91403 Orsay cedex, France
(gauvain@limsi.fr http://www.limsi.fr/tlp)
Abstract
This paper addresses recent progress in
speaker-independent, large vocabulary,
continuous speech recognition, which
has opened up a wide range of near and
mid-term applications. One rapidly ex-
panding application area is the process-
ing of broadcast audio for information
access. At LIMSI, broadcast news tran-
scription systems have been developed
for English, French, German, Mandarin
and Portuguese, and systems for other
languages are under development. Au-
dio indexation must take into account
the specificities of audio data, such as
needing to deal with the continuous
data stream and an imperfect word tran-
scription. Some near-term applications
areas are audio data mining, selective
dissemination of information and me-
dia monitoring.
1 Introduction
A major advance in speech processing technology
is the ability of todays systems to deal with non-
homogeneous data as is exemplified by broadcast
data. With the rapid expansion of different me-
dia sources, there is a pressing need for automatic
processing of such audio streams. Broadcast au-
dio is challenging as it contains segments of vari-
ous acoustic and linguistic natures, which require
appropriate modeling. A special section in the
Communications of the ACM devoted to ?News
on Demand? (Maybury, 2000) includes contribu-
tions from many of the sites carrying out active
research in this area.
Via speech recognition, spoken document re-
trieval (SDR) can support random access to rel-
evant portions of audio documents, reducing the
time needed to identify recordings in large multi-
media databases. The TREC (Text REtrieval Con-
ference) SDR evaluation showed that only small
differences in information retrieval performance
are observed for automatic and manual transcrip-
tions (Garofolo et al, 2000).
Large vocabulary continuous speech recogni-
tion (LVCSR) is a key technology that can be used
to enable content-based information access in au-
dio and video documents. Since most of the lin-
guistic information is encoded in the audio chan-
nel of video data, which once transcribed can be
accessed using text-based tools. This research has
been carried out in a multilingual environment in
the context of several recent and ongoing Euro-
pean projects. We highlight recent progress in
LVCSR and describe some of our work in de-
veloping a system for processing broadcast au-
dio for information access. The system has two
main components, the speech transcription com-
ponent and the information retrieval component.
Versions of the LIMSI broadcast news transcrip-
tion system have been developed in American En-
glish, French, German, Mandarin and Portuguese.
2 Progress in LVCSR
Substantial advances in speech recognition tech-
nology have been achieved during the last decade.
Only a few years ago speech recognition was pri-
marily associated with small vocabulary isolated
word recognition and with speaker-dependent (of-
ten also domain-specific) dictation systems. The
same core technology serves as the basis for a
range of applications such as voice-interactive
database access or limited-domain dictation, as
well as more demanding tasks such as the tran-
scription of broadcast data. With the exception of
the inherent variability of telephone channels, for
most applications it is reasonable to assume that
the speech is produced in relatively stable envi-
ronmental and in some cases is spoken with the
purpose of being recognized by the machine.
The ability of systems to deal with non-
homogeneous data as is found in broadcast au-
dio (changing speakers, languages, backgrounds,
topics) has been enabled by advances in a vari-
ety of areas including techniques for robust signal
processing and normalization; improved training
techniques which can take advantage of very large
audio and textual corpora; algorithms for audio
segmentation; unsupervised acoustic model adap-
tation; efficient decoding with long span language
models; ability to use much larger vocabularies
than in the past - 64 k words or more is common
to reduce errors due to out-of-vocabulary words.
With the rapid expansion of different media
sources for information dissemination including
via the internet, there is a pressing need for au-
tomatic processing of the audio data stream. The
vast majority of audio and video documents that
are produced and broadcast do not have associ-
ated annotations for indexation and retrieval pur-
poses, and since most of today?s annotation meth-
ods require substantial manual intervention, and
the cost is too large to treat the ever increasing
volume of documents. Broadcast audio is chal-
lenging to process as it contains segments of vari-
ous acoustic and linguistic natures, which require
appropriate modeling. Transcribing such data re-
quires significantly higher processing power than
what is needed to transcribe read speech data
in a controlled environment, such as for speaker
adapted dictation. Although it is usually as-
sumed that processing time is not a major issue
since computer power has been increasing con-
tinuously, it is also known that the amount of data
appearing on information channels is increasing
at a close rate. Therefore processing time is an
important factor in making a speech transcription
system viable for audio data mining and other re-
lated applications. Transcription word error rates
of about 20% have been reported for unrestricted
broadcast news data in several languages.
As shown in Figure 1 the LIMSI broadcast
news transcription system for automatic indexa-
tion consists of an audio partitioner and a speech
recognizer.
3 Audio partitioning
The goal of audio partitioning is to divide the
acoustic signal into homogeneous segments, la-
beling and structuring the acoustic content of the
data, and identifying and removing non-speech
segments. The LIMSI BN audio partitioner re-
lies on an audio stream mixture model (Gauvain
et al, 1998). While it is possible to transcribe the
continuous stream of audio data without any prior
segmentation, partitioning offers several advan-
tages over this straight-forward solution. First,
in addition to the transcription of what was said,
other interesting information can be extracted
such as the division into speaker turns and the
speaker identities, and background acoustic con-
ditions. This information can be used both di-
rectly and indirectly for indexation and retrieval
purposes. Second, by clustering segments from
the same speaker, acoustic model adaptation can
be carried out on a per cluster basis, as opposed
to on a single segment basis, thus providing more
adaptation data. Third, prior segmentation can
avoid problems caused by linguistic discontinu-
ity at speaker changes. Fourth, by using acoustic
models trained on particular acoustic conditions
(such as wide-band or telephone band), overall
performance can be significantly improved. Fi-
nally, eliminating non-speech segments substan-
tially reduces the computation time. The result
of the partitioning process is a set of speech seg-
ments usually corresponding to speaker turns with
speaker, gender and telephone/wide-band labels
(see Figure 2).
4 Transcription of Broadcast News
For each speech segment, the word recognizer de-
termines the sequence of words in the segment,
associating start and end times and an optional
confidence measure with each word. The LIMSI
system, in common with most of today?s state-of-
the-art systems, makes use of statistical models
of speech generation. From this point of view,
message generation is represented by a language
model which provides an estimate of the probabil-
ity of any given word string, and the encoding of
the message in the acoustic signal is represented
by a probability density function. The speaker-
independent 65k word, continuous speech rec-
ognizer makes use of 4-gram statistics for lan-
guage modeling and of continuous density hidden
Markov models (HMMs) with Gaussian mixtures
for acoustic modeling. Each word is represented
by one or more sequences of context-dependent
phone models as determined by its pronunciation.
The acoustic and language models are trained on
large, representative corpora for each task and
language.
Processing time is an important factor in mak-
ing a speech transcription system viable for au-
tomatic indexation of radio and television broad-
casts. For many applications there are limita-
tions on the response time and the available com-
putational resources, which in turn can signifi-
cantly affect the design of the acoustic and lan-
guage models. Word recognition is carried out in
one or more decoding passes with more accurate
acoustic and language models used in successive
passes. A 4-gram single pass dynamic network
decoder has been developed (Gauvain and Lamel,
2000) which can achieve faster than real-time de-
coding with a word error under 30%, running in
less than 100 Mb of memory on widely available
platforms such Pentium III or Alpha machines.
5 Multilinguality
A characteristic of the broadcast news domain is
that, at least for what concerns major news events,
similar topics are simultaneously covered in dif-
ferent emissions and in different countries and
languages. Automatic processing carried out on
contemporaneous data sources in different lan-
guages can serve for multi-lingual indexation and
retrieval. Multilinguality is thus of particular in-
terest for media watch applications, where news
may first break in another country or language.
At LIMSI broadcast news transcription systems
have been developed for the American English,
French, German, Mandarin and Portuguese lan-
guages. The Mandarin language was chosen be-
cause it is quite different from the other lan-
guages (tone and syllable-based), and Mandarin
resources are available via the LDC as well as ref-
erence performance results.
Our system and other state-of-the-art sys-
tems can transcribe unrestricted American En-
glish broadcast news data with word error rates
under 20%. Our transcription systems for French
and German have comparable error rates for news
broadcasts (Adda-Decker et al, 2000). The
character error rate for Mandarin is also about
20% (Chen et al, 2000). Based on our expe-
rience, it appears that with appropriately trained
models, recognizer performance is more depen-
dent upon the type and source of data, than on the
language. For example, documentaries are partic-
ularly challenging to transcribe, as the audio qual-
ity is often not very high, and there is a large pro-
portion of voice over.
6 Spoken Document Retrieval
The automatically generated partition and word
transcription can be used for indexation and in-
formation retrieval purposes. Techniques com-
monly applied to automatic text indexation can
be applied to the automatic transcriptions of the
broadcast news radio and TV documents. These
techniques are based on document term frequen-
cies, where the terms are obtained after standard
text processing, such as text normalization, tok-
enization, stopping and stemming. Most of these
preprocessing steps are the same as those used to
prepare the texts for training the speech recog-
nizer language models. While this offers advan-
tages for speech recognition, it can lead to IR er-
rors. For better IR results, some words sequences
corresponding to acronymns, multiword named-
entities (e.g. Los Angeles), and words preceded
by some particular prefixes (anti, co, bi, counter)
are rewritten as a single word. Stemming is used
to reduce the number of lexical items for a given
word sense. The stemming lexicon contains about
32000 entries and was constructed using Porter?s
algorithm (Porter80, 1980) on the most frequent
words in the collection, and then manually cor-
rected.
The information retrieval system relies on a un-
Lexicon
Acoustic models
Recognition
Word
Audio signal
Language model
Analysis
Acoustic
partitioned
speech acoustic models
Music, noise and
non speech
Filter out
segments
telephone/non-tel models
word transcription
(SGML file)data
Male/female models
Iterative 
segmentation 
and labelling
Figure 1: Overview of an audio transcription system. The audio partitioner divides the data stream into
homogeneous acoustic segments, removing non-speech portions. The word recognizer identifies the
words in each speech segment, associating time-markers with each word.
 
audiofile filename=19980411 1600 1630 CNN HDL language=english 
 
segment type=wideband gender=female spkr=1 stime=50.25 etime=86.83 
 
wtime stime=50.38 etime=50.77  c.n.n.
 
wtime stime=50.77 etime=51.10  headline
 
wtime stime=51.10 etime=51.44  news
 
wtime stime=51.44 etime=51.63  i?m
 
wtime stime=51.63 etime=51.92  robert
 
wtime stime=51.92 etime=52.46  johnson
it is a day of final farewells in alabama the first funerals for victims of this week?s tornadoes are being held today along
with causing massive property damage the twisters killed thirty three people in alabama five in georgia and one each
in mississippi and north carolina the national weather service says the tornado that hit jefferson county in alabama had
winds of more than two hundred sixty miles per hour authorities speculated was the most powerful tornado ever to hit the
southeast twisters destroyed two churches to fire stations and a school parishioners were in one church when the tornado
struck
  /segment 
 
segment type=wideband gender=female spkr=2 stime=88.37 etime=104.86 
at one point when the table came onto my back i thought yes this is it i?m ready ready protects protect the children because
the children screaming the children were screaming they were screaming in prayer that were screaming god help us
  /segment 
 
segment type=wideband gender=female spkr=1 stime=104.86 etime=132.37 
vice president al gore toured the area yesterday he called it the worst tornado devastation he?s ever seen we will have a
complete look at the weather across the u. s. in our extended weather forecast in six minutes
  /segment 
. . .
 
segment type=wideband gender=male spkr=19 stime=1635.60 etime=1645.71 
so if their computing systems don?t tackle this problem well we have a potential business disruption and either erroneous
deliveries or misdeliveries or whatever savvy businesses are preparing now so the january first two thousand would just be
another day on the town not a day when fast food and everything else slows down rick lockridge c.n.n.
  /segment 
  /audiofile 
Figure 2: Example system output obtained by automatic processing of the audio stream of a CNN show
broadcasted on April 11, 1998 at 4pm. The output includes the partitioning and transcription results. To
improve readability, word time stamps are given only for the first 6 words. Non speech segments have
been removed and the following information is provided for each speech segment: signal bandwidth
(telephone or wideband), speaker gender, and speaker identity (within the show).
Transcriptions Werr Base BRF
Closed-captions - 46.9% 54.3%
10xRT 20.5% 45.3% 53.9%
1.4xRT 32.6% 40.9% 49.4%
Table 1: Impact of the word error rate on the
mean average precision using using a 1-gram doc-
ument model. The document collection contains
557 hours of broadcast news from the period of
February through June 1998. (21750 stories, 50
queries with the associated relevance judgments.)
igram model per story. The score of a story is ob-
tained by summing the query term weights which
are simply the log probabilities of the terms given
the story model once interpolated with a general
English model. This term weighting has been
shown to perform as well as the popular TF  IDF
weighting scheme (Hiemstra and Wessel, 1998;
Miller et al, 1998; Ng, 1999; Spa?rk Jones et al,
1998).
The text of the query may or may not include
the index terms associated with relevant docu-
ments. One way to cope with this problem is to
use query expansion (Blind Relevance Feedback,
BRF (Walker and de Vere, 1990)) based on terms
present in retrieved contemporary texts.
The system was evaluated in the TREC SDR
track, with known story boundaries. The SDR
data collection contains 557 hours of broadcast
news from the period of February through June
1998. This data includes 21750 stories and a set
of 50 queries with the associated relevance judg-
ments (Garofolo et al, 2000).
In order to assess the effect of the recogni-
tion time on the information retrieval results we
transcribed the 557 hours of broadcast news data
using two decoder configurations: a single pass
1.4xRT system and a three pass 10xRT system.
The word error rates are measured on a 10h test
subset (Garofolo et al, 2000). The information
retrieval results are given in terms of mean av-
erage precision (MAP), as is done for the TREC
benchmarks in Table 1 with and without query ex-
pansion. For comparison, results are also given
for manually produced closed captions. With
query expansion comparable IR results are ob-
tained using the closed captions and the 10xRT
0
5
10
15
20
25
30
35
40
45
50
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Pe
rc
en
ta
ge
 o
f s
ec
tio
ns
Number of speaker turns
Figure 3: Histogram of the number of speaker
turns per section in 100 hours of audio data from
radio and TV sources (NPR, ABC, CNN, CSPAN)
from May-June 1996.
transcriptions, and a moderate degradation (4%
absolute) is observed using the 1.4xRT transcrip-
tions.
7 Locating Story Boundaries
The broadcast news transcription system also pro-
vides non-lexical information along with the word
transcription. This information is available in
the partition of the audio track, which identifies
speaker turns. It is interesting to see whether or
not such information can be used to help locate
story boundaries, since in the general case these
are not known. Statistics were made on 100 hours
of radio and television broadcast news with man-
ual transcriptions including the speaker identities.
Of the 2096 sections manually marked as reports
(considered stories), 40% start without a manu-
ally annotated speaker change. This means that
using only speaker change information for detect-
ing document boundaries would miss 40% of the
boundaries. With automatically detected speaker
changes, the number of missed boundaries would
certainly increase. At the same time, 11,160 of
the 12,439 speaker turns occur in the middle of a
document, resulting in a false alarm rate of almost
90%. A more detailed analysis shows that about
50% of the sections involve a single speaker, but
that the distribution of the number of speaker
turns per section falls off very gradually (see Fig-
ure 3). False alarms are not as harmful as missed
detections, since it may be possible to merge ad-
jacent turns into a single document in subsequent
processing. These results show that even perfect
00.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0 30 60 90 120 150 180 210 240 270 300
D
en
si
ty

Duration (seconds)
1997 Hub-4
0
0.005
0.01
0.015
0.02
0.025
0 30 60 90 120 150 180 210 240 270 300
D
en
si
ty

Duration (seconds)
TREC-9 SDR Corpus
Figure 4: Distribution of document durations for
100 hours of data from May-June 1996 (top) and
for 557 hours from February-June 1998 (bottom).
speaker turn boundaries cannot be used as the pri-
mary cue for locating document boundaries. They
can, however, be used to refine the placement
of a document boundary located near a speaker
change.
We also investigated using simple statistics on
the durations of the documents. A histogram of
the 2096 sections is shown in Figure 4. One
third of the sections are shorter than 30 seconds.
The histogram has a bimodal distribution with a
sharp peak around 20 seconds, and a smaller, flat
peak around 2 minutes. Very short documents
are typical of headlines which are uttered by sin-
gle speaker, whereas longer documents are more
likely to contain data from multiple talkers. This
distribution led us to consider using a multi-scale
segmentation of the audio stream into documents.
Similar statistics were measured on the larger cor-
pus (Figure 4 bottom).
As proposed in (Abberley et al, 1999; John-
son et al, 1999), we segment the audio stream
into overlapping documents of a fixed duration.
As a result of optimization, we chose a 30 sec-
ond window duration with a 15 second overlap.
Since there are many stories significantly shorter
than 30s in broadcast shows (see Figure 4) we
conjunctured that it may be of interest to use a
double windowing system in order to better tar-
get short stories (Gauvain et al, 2000). The win-
dow size of the smaller window was selected to
be 10 seconds. So for each query, we indepen-
dently retrieved two sets of documents, one set
for each window size. Then for each document
set, document recombination is done by merging
overlapping documents until no further merges
are possible. The score of a combined document
is set to maximum score of any one of the com-
ponents. For each document derived from the
30s windows, we produce a time stamp located
at the center point of the document. However,
if any smaller documents are embedded in this
document, we take the center of the best scor-
ing document. This way we try to take advantage
of both window sizes. The MAP using a single
30s window and the double windowing strategy
are shown in Table 2. For comparison, the IR re-
sults using the manual story segmentation and the
speaker turns located by the audio partitioner are
also given. All conditions use the same word hy-
potheses obtained with a speech recognizer which
had no knowledge about the story boundaries.
manual segmentation (NIST) 59.6%
audio partitioner 33.3%
single window (30s) 50.0%
double window 52.3%
Table 2: Mean average precision with manual and
automatically determined story boundaries. The
document collection contains 557 hours of broad-
cast news from the period of February through
June 1998. (21750 stories, 50 queries with the
associated relevance judgments.)
From these results we can clearly see the inter-
est of using a search engine specifically designed
to retrieve stories in the audio stream. Using an
a priori acoustic segmentation, the mean aver-
age precision is significantly reduced compared
to a ?perfect? manual segmentation, whereas the
window-based search engine results are much
closer. Note that in the manual segmentation all
non-story segments such as advertising have been
removed. This reduces the risk of having out-of-
topic hits and explains part of the difference be-
tween this condition and the other conditions.
The problem of locating story boundaries is be-
ing further pursued in the context of the ALERT
project, where one of the goals is to identify ?doc-
uments? given topic profiles. This project is in-
vestigating the combined use of audio and video
segmentation to more accurately locate document
boundaries in the continuous data stream.
8 Recent Research Projects
The work presented in this paper has benefited
from a variety of research projects both at the Eu-
ropean and National levels. These collaborative
efforts have enabled access to real-world data al-
lowing us to develop algorithms and models well-
suited for near-term applications.
The European project LE-4 OLIVE: A
Multilingual Indexing Tool for Broadcast
Material Based on Speech Recognition
(http://twentyone.tpd.tno.nl/ olive/) addressed
methods to automate the disclosure of the infor-
mation content of broadcast data thus allowing
content-based indexation. Speech recognition
was used to produce a time-linked transcript of
the audio channel of a broadcast, which was then
used to produce a concept index for retrieval.
Broadcast news transcription systems for French
and German were developed. The French data
come from a variety of television news shows and
radio stations. The German data consist of TV
news and documentaries from ARTE. OLIVE also
developed tools for users to query the database,
as well as cross-lingual access based on off-line
machine translation of the archived documents,
and online query translation.
The European project IST ALERT: Alert sys-
tem for selective dissemination (http://www.fb9-
ti.uni-duisburg.de/alert) aims to associate state-
of-the-art speech recognition with audio and
video segmentation and automatic topic index-
ing to develop an automatic media monitoring
demonstrator and evaluate it in the context of real
world applications. The targeted languages are
French, German and Portuguese. Major media-
monitoring companies in Europe are participating
in this project.
Two other related FP5 IST projects are: CORE-
TEX: Improving Core Speech Recognition Tech-
nology and ECHO: European CHronicles On-
line. CORETEX (http://coretex.itc.it/), aims at
improving core speech recognition technologies,
which are central to most applications involv-
ing voice technology. In particular the project
addresses the development of generic speech
recognition technology and methods to rapidly
port technology to new domains and languages
with limited supervision, and to produce en-
riched symbolic speech transcriptions. The ECHO
project (http://pc-erato2.iei.pi.cnr.it/echo) aims to
develop an infrastructure for access to histori-
cal films belonging to large national audiovisual
archives. The project will integrate state-of-the-
art language technologies for indexing, searching
and retrieval, cross-language retrieval capabilities
and automatic film summary creation.
9 Conclusions
This paper has described some of the ongoing re-
search activites at LIMSI in automatic transcrip-
tion and indexation of broadcast data. Much of
this research, which is at the forefront of todays
technology, is carried out with partners with real
needs for advanced audio processing technolo-
gies.
Automatic speech recognition is a key tech-
nology for audio and video indexing. Most of
the linguistic information is encoded in the au-
dio channel of video data, which once transcribed
can be accessed using text-based tools. This is in
contrast to the image data for which no common
description language is widely adpoted. A va-
riety of near-term applications are possible such
as audio data mining, selective dissemination of
information (News-on-Demand), media monitor-
ing, content-based audio and video retrieval.
It appears that with word error rates on the
order of 20%, comparable IR results to those
obtained on text data can be achieved. Even
with higher word error rates obtained by run-
ning a faster transcription system or by transcrib-
ing compressed audio data (Barras et al, 2000;
J.M. Van Thong et al, 2000) (such as that can be
loaded over the Internet), the IR performance re-
mains quite good.
Acknowledgments
This work has been partially financed by the Eu-
ropean Commission and the French Ministry of
Defense. The authors thank Jean-Jacques Gan-
golf, Sylvia Hermier and Patrick Paroubek for
their participation in the development of differ-
ent aspects of the automatic indexation system de-
scribed here.
References
Dave Abberley, Steve Renals, Dan Ellis and Tony
Robinson, ?The THISL SDR System at TREC-8?,
Proc. of the 8th Text Retrieval Conference TREC-8,
Nov 1999.
Martine Adda-Decker, Gilles Adda, Lori Lamel, ?In-
vestigating text normalization and pronunciation
variants for German broadcast transcription,? Proc.
ICSLP?2000, Beijing, China, October 2000.
Claude Barras, Lori Lamel, Jean-Luc Gauvain, ?Auto-
matic Transcription of Compressed Broadcast Au-
dio Proc. ICASSP?2001, Salt Lake City, May 2001.
Langzhou Chen, Lori Lamel, Gilles Adda and Jean-
Luc Gauvain, ?Broadcast News Transcription in
Mandarin,? Proc. ICSLP?2000, Beijing, China, Oc-
tober 2000.
John S. Garofolo, Cedric G.P. Auzanne, and Ellen
M. Voorhees, ?The TREC Spoken Document Re-
trieval Track: A Success Story,? Proc. of the 6th
RIAO Conference, Paris, April 2000. Also John
S. Garofolo et al, ?1999 Trec-8 Spoken Docu-
ment Retrieval Track Overview and Results,? Proc.
8th Text Retrieval Conference TREC-8, Nov 1999.
(http://trec.nist.gov).
Jean-Luc Gauvain, Lori Lamel, ?Fast Decoding for
Indexation of Broadcast Data,? Proc. ICSLP?2000,
3:794-798, Oct 2000.
Jean-Luc Gauvain, Lori Lamel, Gilles Adda, ?Parti-
tioning and Transcription of Broadcast News Data,?
ICSLP?98, 5, pp. 1335-1338, Dec. 1998.
Jean-Luc Gauvain, Lori Lamel, Claude Barras, Gilles
Adda, Yannick de Kercadio ?The LIMSI SDR sys-
tem for TREC-9,? Proc. of the 9th Text Retrieval
Conference TREC-9, Nov 2000.
Alexander G. Hauptmann and Michael J. Witbrock,
?Informedia: News-on-Demand Multimedia Infor-
mation Acquisition and Retrieval,? Proc Intelli-
gent Multimedia Information Retrieval, M. May-
bury, ed., AAAI Press, pp. 213-239, 1997.
Djoerd Hiemstra, Wessel Kraaij, ?Twenty-One at
TREC-7: Ad-hoc and Cross-language track,? Proc.
of the 8th Text Retrieval Conference TREC-7, Nov
1998.
Sue E. Johnson, Pierre Jourlin, Karen Spa?rck Jones,
Phil C. Woodland, ?Spoken Document Retrieval for
TREC-8 at Cambridge University?, Proc. of the 8th
Text Retrieval Conference TREC-8, Nov 1999.
Mark Maybury, ed., Special Section on ?News on De-
mand?, Communications of the ACM, 43(2), Feb
2000.
David Miller, Tim Leek, Richard Schwartz, ?Using
Hidden Markov Models for Information Retrieval?,
Proc. of the 8th Text Retrieval Conference TREC-7,
Nov 1998.
Kenney Ng, ?A Maximum Likelihood Ratio Informa-
tion Retrieval Model,? Proc. of the 8th Text Re-
trieval Conference TREC-8, 413-435, Nov 1999.
M. F. Porter, ?An algorithm for suffix stripping?, Pro-
gram, 14, pp. 130?137, 1980.
Karen Spa?rk Jones, S. Walker, Stephen E. Robert-
son, ?A probabilistic model of information retrieval:
development and status,? Technical Report of the
Computer Laboratory, University of Cambridge,
U.K., 1998.
J.M. Van Thong, David Goddeau, Anna Litvi-
nova, Beth Logan, Pedro Moreno, Michael Swain,
?SpeechBot: a Speech Recognition based Audio In-
dexing System for the Web?, Proc. of the 6th RIAO
Conference, Paris, April 2000.
S. Walker, R. de Vere, ?Improving subject retrieval in
online catalogues: 2. Relevance feedback and query
expansion?, British Library Research Paper 72,
British Library, London, U.K., 1990.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723?730,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Continuous Space Language Models for Statistical Machine Translation
Holger Schwenk and Daniel Dchelotte and Jean-Luc Gauvain
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
{schwenk,dechelot,gauvain}@limsi.fr
Abstract
Statistical machine translation systems are
based on one or more translation mod-
els and a language model of the target
language. While many different trans-
lation models and phrase extraction al-
gorithms have been proposed, a standard
word n-gram back-off language model is
used in most systems.
In this work, we propose to use a new sta-
tistical language model that is based on a
continuous representation of the words in
the vocabulary. A neural network is used
to perform the projection and the proba-
bility estimation. We consider the trans-
lation of European Parliament Speeches.
This task is part of an international evalua-
tion organized by the TC-STAR project in
2006. The proposed method achieves con-
sistent improvements in the BLEU score
on the development and test data.
We also present algorithms to improve the
estimation of the language model proba-
bilities when splitting long sentences into
shorter chunks.
1 Introduction
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source sen-
tence f . Among all possible target sentences the
one with maximal probability is chosen. The clas-
sical Bayes relation is used to introduce a target
language model (Brown et al, 1993):
e? = argmaxe Pr(e|f) = argmaxe Pr(f |e) Pr(e)
where Pr(f |e) is the translation model and Pr(e)
is the target language model. This approach is
usually referred to as the noisy source-channel ap-
proach in statistical machine translation.
Since the introduction of this basic model, many
improvements have been made, but it seems that
research is mainly focused on better translation
and alignment models or phrase extraction algo-
rithms as demonstrated by numerous publications
on these topics. On the other hand, we are aware
of only a small amount of papers investigating
new approaches to language modeling for statis-
tical machine translation. Traditionally, statistical
machine translation systems use a simple 3-gram
back-off language model (LM) during decoding to
generate n-best lists. These n-best lists are then
rescored using a log-linear combination of feature
functions (Och and Ney, 2002):
e? ? argmaxe Pr(e)
?1 Pr(f |e)?2 (1)
where the coefficients ?i are optimized on a devel-
opment set, usually maximizing the BLEU score.
In addition to the standard feature functions, many
others have been proposed, in particular several
ones that aim at improving the modeling of the tar-
get language. In most SMT systems the use of a
4-gram back-off language model usually achieves
improvements in the BLEU score in comparison
to the 3-gram LM used during decoding. It seems
however difficult to improve upon the 4-gram LM.
Many different feature functions were explored in
(Och et al, 2004). In that work, the incorporation
of part-of-speech (POS) information gave only a
small improvement compared to a 3-gram back-
off LM. In another study, a factored LM using
POS information achieved the same results as the
4-gram LM (Kirchhoff and Yang, 2005). Syntax-
based LMs were investigated in (Charniak et al,
723
2003), and reranking of translation hypothesis us-
ing structural properties in (Hasan et al, 2006).
An interesting experiment was reported at the
NIST 2005 MT evaluation workshop (Och, 2005):
starting with a 5-gram LM trained on 75 million
words of Broadcast News data, a gain of about
0.5 point BLEU was observed each time when the
amount of LM training data was doubled, using at
the end 237 billion words of texts. Most of this
additional data was collected by Google on the In-
ternet. We believe that this kind of approach is dif-
ficult to apply to other tasks than Broadcast News
and other target languages than English. There are
many areas where automatic machine translation
could be deployed and for which considerably less
appropriate in-domain training data is available.
We could for instance mention automatic trans-
lation of medical records, translation systems for
tourism related tasks or even any task for which
Broadcast news and Web texts is of limited help.
In this work, we consider the translation of Eu-
ropean Parliament Speeches from Spanish to En-
glish, in the framework of an international evalua-
tion organized by the European TC-STAR project
in February 2006. The training data consists of
about 35M words of aligned texts that are also
used to train the target LM. In our experiments,
adding more than 580M words of Broadcast News
data had no impact on the BLEU score, despite
a notable decrease of the perplexity of the target
LM. Therefore, we suggest to use more complex
statistical LMs that are expected to take better ad-
vantage of the limited amount of appropriate train-
ing data. Promising candidates are random forest
LMs (Xu and Jelinek, 2004), random cluster LMs
(Emami and Jelinek, 2005) and the neural network
LM (Bengio et al, 2003). In this paper, we inves-
tigate whether the latter approach can be used in a
statistical machine translation system.
The basic idea of the neural network LM, also
called continuous space LM, is to project the word
indices onto a continuous space and to use a prob-
ability estimator operating on this space. Since the
resulting probability functions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. A neu-
ral network can be used to simultaneously learn
the projection of the words onto the continuous
space and to estimate the n-gram probabilities.
This is still a n-gram approach, but the LM pos-
terior probabilities are ?interpolated? for any pos-
sible context of length n-1 instead of backing-off
to shorter contexts. This approach was success-
fully used in large vocabulary speech recognition
(Schwenk and Gauvain, 2005), and we are inter-
ested here if similar ideas can be applied to statis-
tical machine translation.
This paper is organized as follows. In the next
section we first describe the baseline statistical
machine translation system. Section 3 presents
the architecture of the continuous space LM and
section 4 summarizes the experimental evaluation.
The paper concludes with a discussion of future
research directions.
2 Statistical Translation Engine
A word-based translation engine is used based on
the so-called IBM-4 model (Brown et al, 1993).
A brief description of this model is given below
along with the decoding algorithm.
The search algorithm aims at finding what tar-
get sentence e is most likely to have produced the
observed source sentence f . The translation model
Pr(f |e) is decomposed into four components:
1. a fertility model;
2. a lexical model of the form t(f |e), which
gives the probability that the target word e
translates into the source word f ;
3. a distortion model, that characterizes how
words are reordered when translated;
4. and probabilities to model the insertion of
source words that are not aligned to any tar-
get words.
An A* search was implemented to find the best
translation as predicted by the model, when given
enough time and memory, i.e., provided pruning
did not eliminate it. The decoder manages par-
tial hypotheses, each of which translates a subset
of source words into a sequence of target words.
Expanding a partial hypothesis consists of cover-
ing one extra source position (in random order)
and, by doing so, appending one, several or possi-
bly zero target words to its target word sequence.
For details about the implemented algorithm, the
reader is referred to (De?chelotte et al, 2006).
Decoding uses a 3-gram back-off target lan-
guage model. Equivalent hypotheses are merged,
and only the best scoring one is further expanded.
The decoder generates a lattice representing the
724
we
I
we
should
should
must
remember
remind
remember
that
,
that
that
,
that
you
,
,
,
becausebecause
because
it
I
they
that
can
can
can be
say
be
, because
can
itthey
we
that
can
can
can
be
be
have
be
have
be
have
it
it
has
forgotten
has forgotten
has
has
forgotten
forgotten
been
forgotten
been
forgotten
forgotten
.
.
forgotten
.
.
.
.
.
.
Figure 1: Example of a translation lattice. Source
sentence: ?conviene recordarlo , porque puede
que se haya olvidado .?, Reference 1: ?it is ap-
propriate to remember this , because it may have
been forgotten .? Reference 2: ?it is good to re-
member this , because maybe we forgot it .?
explored search space. Figure 1 shows an example
of such a search space, here heavily pruned for the
sake of clarity.
2.1 Sentence Splitting
The execution complexity of our SMT decoder in-
creases non-linearly with the length of the sen-
tence to be translated. Therefore, the source text
is split into smaller chunks, each one being trans-
lated separately. The chunks are then concatenated
together. Several algorithms have been proposed
in the literature that try to find the best splits, see
for instance (Berger et al, 1996). In this work, we
first split long sentences at punctuation marks, the
remaining segments that still exceed the allowed
length being split linearly. In a second pass, ad-
joining very short chunks are merged together.
During decoding, target LM probabilities of the
type Pr(w1|<s>) and Pr(</s>|wn?1wn) will be
requested at the beginning and at the end of the
hypothesized target sentence respectively.1 This is
correct when a whole sentence is translated, but
leads to wrong LM probabilities when processing
smaller chunks. Therefore, we define a sentence
break symbol, <b>, that is used at the beginning
and at the end of a chunk. During decoding a 3-
gram back-off LM is used that was trained on text
where sentence break symbols have been added.
Each chunk is translated and a lattice is gen-
1The symbols <s> and </s> denote the begin and end of
sentence marker respectively.
erated. The individual lattices are then joined,
omitting the sentence break symbols. Finally, the
resulting lattice is rescored with a LM that was
trained on text without sentence breaks. In that
way we find the best junction of the chunks. Sec-
tion 4.1 provides comparative results of the differ-
ent algorithms to split and join sentences.
2.2 Parameter Tuning
It is nowadays common practice to optimize the
coefficients of the log-linear combination of fea-
ture functions by maximizing the BLEU score on
the development data (Och and Ney, 2002). This
is usually done by first creating n-best lists that
are then reranked using an iterative optimization
algorithm.
In this work, a slightly different procedure was
used that operates directly on the translation lat-
tices. We believe that this is more efficient than
reranking n-best lists since it guarantees that al-
ways all possible hypotheses are considered. The
decoder first generates large lattices using the cur-
rent set of parameters. These lattices are then
processed by a separate tool that extracts the best
path, given the coefficients of six feature functions
(translations, distortion, fertility, spontaneous in-
sertion, target language model probability, and a
sentence length penalty). Then, the BLEU score
of the extracted solution is calculated. This tool is
called in a loop by the public numerical optimiza-
tion tool Condor (Berghen and Bersini, 2005). The
solution vector was usually found after about 100
iterations. In our experiments, only two cycles
of lattice generation and parameter optimization
were necessary (with a very small difference in the
BLEU score).
In all our experiments, the 4-gram back-off and
the neural network LM are used to calculate lan-
guage model probabilities that replace those of the
default 3-gram LM. An alternative would be to de-
fine each LM as a feature function and to combine
them under the log-linear model framework, us-
ing maximum BLEU training. We believe that this
would not make a notable difference in our experi-
ments since we do interpolate the individual LMs,
the coefficients being optimized to minimize per-
plexity on the development data. However, this
raises the interesting question whether the two cri-
teria lead to equivalent performance. The result
section provides some experimental evidence on
this topic.
725
3 Continuous Space Language Models
The architecture of the neural network LM is
shown in Figure 2. A standard fully-connected
multi-layer perceptron is used. The inputs to
the neural network are the indices of the n?1
previous words in the vocabulary hj=wj?n+1,
. . . , wj?2, wj?1 and the outputs are the posterior
probabilities of all words of the vocabulary:
P (wj = i|hj) ?i ? [1, N ] (2)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith word
of the vocabulary is coded by setting the ith ele-
ment of the vector to 1 and all the other elements
to 0. The ith line of the N ? P dimensional pro-
jection matrix corresponds to the continuous rep-
resentation of the ith word. Let us denote cl these
projections, dj the hidden layer activities, oi the
outputs, pi their softmax normalization, and mjl,
bj , vij and ki the hidden and output layer weights
and the corresponding biases. Using these nota-
tions, the neural network performs the following
operations:
dj = tanh
(?
l
mjl cl + bj
)
(3)
oi =
?
j
vij dj + ki (4)
pi = eoi /
N?
r=1
eor (5)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj). Training is
performed with the standard back-propagation al-
gorithm minimizing the following error function:
E =
N?
i=1
ti log pi + ?
?
??
jl
m2jl +
?
ij
v2ij
?
? (6)
where ti denotes the desired output, i.e., the prob-
ability should be 1.0 for the next word in the train-
ing sentence and 0.0 for all the other ones. The
first part of this equation is the cross-entropy be-
tween the output and the target probability dis-
tributions, and the second part is a regulariza-
tion term that aims to prevent the neural network
from overfitting the training data (weight decay).
The parameter ? has to be determined experimen-
tally. Training is done using a resampling algo-
rithm (Schwenk and Gauvain, 2005).
projection
layer hidden
layer
output
layerinput
projections
shared
LM probabilities
for all words
probability estimation
Neural Network
discrete
representation:
indices in wordlist
continuous
representation:
P dimensional vectors
N
wj?1 P
H
N
P (wj=1|hj)
wj?n+1
wj?n+2
P (wj=i|hj)
P (wj=N|hj)
cl
oiM
Vdj
p1 =
pN =
pi =
Figure 2: Architecture of the continuous space
LM. hj denotes the context wj?n+1, . . . , wj?1. P
is the size of one projection and H ,N is the size
of the hidden and output layer respectively. When
short-lists are used the size of the output layer is
much smaller then the size of the vocabulary.
It can be shown that the outputs of a neural net-
work trained in this manner converge to the poste-
rior probabilities. Therefore, the neural network
directly minimizes the perplexity on the train-
ing data. Note also that the gradient is back-
propagated through the projection-layer, which
means that the neural network learns the projec-
tion of the words onto the continuous space that is
best for the probability estimation task.
The complexity to calculate one probability
with this basic version of the neural network LM is
quite high due to the large output layer. To speed
up the processing several improvements were used
(Schwenk, 2004):
1. Lattice rescoring: the statistical machine
translation decoder generates a lattice using
a 3-gram back-off LM. The neural network
LM is then used to rescore the lattice.
2. Shortlists: the neural network is only used to
predict the LM probabilities of a subset of the
whole vocabulary.
3. Efficient implementation: collection of all
LM probability requests with the same con-
text ht in one lattice, propagation of several
examples at once through the neural network
and utilization of libraries with CPU opti-
mized matrix-operations.
The idea behind short-lists is to use the neural
726
network only to predict the s most frequent words,
s being much smaller than the size of the vocab-
ulary. All words in the vocabulary are still con-
sidered at the input of the neural network. The
LM probabilities of words in the short-list (P?N )
are calculated by the neural network and the LM
probabilities of the remaining words (P?B) are ob-
tained from a standard 4-gram back-off LM:
P? (wt|ht) =
{
P?N (wt|ht)PS(ht) if wt ? short-list
P?B(wt|ht) else
(7)
PS(ht) =
?
w?short?list(ht)
P?B(w|ht) (8)
It can be considered that the neural network redis-
tributes the probability mass of all the words in the
short-list. This probability mass is precalculated
and stored in the data structures of the back-off
LM. A back-off technique is used if the probability
mass for a input context is not directly available.
It was not envisaged to use the neural network
LM directly during decoding. First, this would
probably lead to slow translation times due to the
higher complexity of the proposed LM. Second, it
is quite difficult to incorporate n-gram language
models into decoding, for n>3. Finally, we be-
lieve that the lattice framework can give the same
performances than direct decoding, under the con-
dition that the alternative hypotheses in the lattices
are rich enough. Estimates of the lattice oracle
BLEU score are given in the result section.
4 Experimental Evaluation
The experimental results provided here were ob-
tained in the framework of an international evalua-
tion organized by the European TC-STAR project2
in February 2006. This project is envisaged as a
long-term effort to advance research in all core
technologies for speech-to-speech translation.
The main goal of this evaluation is to trans-
late public European Parliament Plenary Sessions
(EPPS). The training material consists of the min-
utes edited by the European Parliament in sev-
eral languages, also known as the Final Text Edi-
tions (Gollan et al, 2005). These texts were
aligned at the sentence level and they are used
to train the statistical translation models (see Ta-
ble 1 for some statistics). In addition, about 100h
of Parliament plenary sessions were recorded and
transcribed. This data is mainly used to train
2http://www.tc-star.org/
Spanish English
Sentence Pairs 1.2M
Total # Words 37.7M 33.8M
Vocabulary size 129k 74k
Table 1: Statistics of the parallel texts used to train
the statistical machine translation system.
the speech recognizers, but the transcriptions were
also used for the target LM of the translation sys-
tem (about 740k words).
Three different conditions are considered in
the TC-STAR evaluation: translation of the Fi-
nal Text Edition (text), translation of the tran-
scriptions of the acoustic development data (ver-
batim) and translation of speech recognizer output
(ASR). Here we only consider the verbatim condi-
tion, translating from Spanish to English. For this
task, the development data consists of 792 sen-
tences (25k words) and the evaluation data of 1597
sentences (61k words). Parts of the test data ori-
gins from the Spanish parliament which results in
a (small) mismatch between the development and
test data. Two reference translations are provided.
The scoring is case sensitive and includes punctu-
ation symbols.
The translation model was trained on 1.2M sen-
tences of parallel text using the Giza++ tool. All
back-off LMs were built using modified Kneser-
Ney smoothing and the SRI LM-toolkit (Stolcke,
2002). Separate LMs were first trained on the
English EPPS texts (33.8M words) and the tran-
scriptions of the acoustic training material (740k
words) respectively. These two LMs were then in-
terpolated together. Interpolation usually results in
lower perplexities than training directly one LM
on the pooled data, in particular if the corpora
come from different sources. An EM procedure
was used to find the interpolation coefficients that
minimize the perplexity on the development data.
The optimal coefficients are 0.78 for the Final Text
edition and 0.22 for the transcriptions.
4.1 Performance of the sentence splitting
algorithm
In this section, we first analyze the performance of
the sentence split algorithm. Table 2 compares the
results for different ways to translate the individ-
ual chunks (using a standard 3-gram LM versus
an LM trained on texts with sentence breaks in-
serted), and to extracted the global solution (con-
727
LM used Concatenate Lattice
during decoding 1-best join
Without
sentence breaks 40.20 41.63
With
sentence breaks 41.45 42.35
Table 2: BLEU scores for different ways to trans-
late sentence chunks and to extract the global so-
lution (see text for details).
catenating the 1-best solutions versus joining the
lattices followed by LM rescoring). It can be
clearly seen that joining the lattices and recalculat-
ing the LM probabilities gives better results than
just concatenating the 1-best solutions of the in-
dividual chunks (first line: BLEU score of 41.63
compared to 40.20). Using a LM trained on texts
with sentence breaks during decoding gives an ad-
ditional improvement of about 0.7 points BLEU
(42.35 compared to 41.63).
In our current implementation, the selection of
the sentence splits is based on punctuation marks
in the source text, but our procedure is compatible
with other methods. We just need to apply the sen-
tence splitting algorithm on the training data used
to build the LM during decoding.
4.2 Using the continuous space language
model
The continuous space language model was trained
on exactly the same data than the back-off refer-
ence language model, using the resampling algo-
rithm described in (Schwenk and Gauvain, 2005).
In this work, we use only 4-gram LMs, but the
complexity of the neural network LM increases
only slightly with the order of the LM. For each
experiment, the parameters of the log-linear com-
bination were optimized on the development data.
Perplexity on the development data set is a pop-
ular and easy to calculate measure to evaluate the
quality of a language model. However, it is not
clear if perplexity is a good criterion to predict
the improvements when the language model will
be used in a SMT system. For information, and
comparison with the back-off LM, Figure 3 shows
the perplexities for different configurations of the
continuous space LM. The perplexity clearly de-
creases with increasing size of the short-list and a
value of 8192 was used. In this case, 99% of the
requested LM probabilities are calculated by the
neural network when rescoring a lattice.
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 0  5  10  15  20  25  30  35
Pe
rp
le
xit
y
Number of training epochs
4-gram back-off LM
short-list of 2k
short-list of 4k
short-list of 8k
Figure 3: Perplexity of different configurations of
the continuous space LM.
Although the neural network LM could be used
alone, better results are obtained when interpolat-
ing it with the 4-gram back-off LM. It has even
turned out that it was advantageous to train several
neural network LMs with different context sizes3
and to interpolate them altogether. In that way,
a perplexity decrease from 79.6 to 65.0 was ob-
tained. For the sake of simplicity we will still call
this interpolation the neural network LM.
Back-off LM Neural LM
3-gram 4-gram 4-gram
Perplexity 85.5 79.6 65.0
Dev data:
BLEU 42.35 43.36 44.42
WER 45.9% 45.1% 44.4%
PER 31.8% 31.3% 30.8%
Eval data:
BLEU 39.77 40.62 41.45
WER 48.2% 47.4% 46.7%
PER 33.6% 33.1% 32.8%
Table 3: Result comparison for the different LMs.
BLEU uses 2 reference translations. WER=word
error rate, PER=position independent WER.
Table 3 summarizes the results on the devel-
opment and evaluation data. The coefficients of
the feature functions are always those optimized
on the development data. The joined translation
lattices were rescored with a 4-gram back-off and
the neural network LM. Using a 4-gram back-
off LM gives an improvement of 1 point BLEU
3The values are in the range 150. . .400. The other param-
eters are: H=500, ?=0.00003 and the initial learning rate was
0.005 with an exponential decay. The networks were trained
for 20 epochs through the training data.
728
Spanish: es el nico premio Sajarov que no ha podido recibir su premio despus de ms de tres
mil quinientos das de cautiverio .
Backoff LM: it is only the Sakharov Prize has not been able to receive the prize after three thousand
, five days of detention .
CSLM : it is the only Sakharov Prize has not been able to receive the prize after three thousand
five days of detention .
Reference 1: she is the only Sakharov laureate who has not been able to receive her prize after
more than three thousand five hundred days in captivity .
Reference 2: she is the only Sacharov prizewinner who couldn?t yet pick up her prize after more
than three thousand five hundred days of imprisonment .
Figure 4: Example translation using the back-off and the continuous space language model (CSLM).
on the Dev data (+0.8 on Test set) compared to
the 3-gram back-off LM. The neural network LM
achieves an additional improvement of 1 point
BLEU (+0.8 on Test data), on top of the 4-gram
back-off LM. Small improvements of the word er-
ror rate (WER) and the position independent word
error rate (PER) were also observed.
As usually observed in SMT, the improvements
on the test data are smaller than those on the de-
velopment data which was used to tune the param-
eters. As a rule of thumb, the gain on the test data
is often half as large as on the Dev-data. The 4-
gram back-off and neural network LM show both
a good generalization behavior.
 42.8
 43
 43.2
 43.4
 43.6
 43.8
 44
 44.2
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
 64
 66
 68
 70
 72
 74
 76
 78
BL
EU
 s
co
re
Pe
rp
le
xit
y
Interpolation coefficient
4-gram back-off LM
BLEU score
Perplexity
Figure 5: BLEU score and perplexity in function
of the interpolation coefficient of the back-off 4-
gram LM.
Figure 5 shows the perplexity and the BLEU
score for different interpolation coefficients of the
4-gram back-off LM. For a value of 1.0 the back-
off LM is used alone, while only the neural net-
work LMs are used for a value of 0.0. Using an
EM procedure to minimize perplexity of the inter-
polated model gives a value of 0.189. This value
also seems to correspond to the best BLEU score.
This is a surprising result, and has the advan-
tage that we do not need to tune the interpola-
tion coefficient in the framework of the log-linear
feature function combination. The weights of the
other feature functions were optimized separately
for each experiment. We noticed a tendency to
a slightly higher weight for the continuous space
LM and a lower sentence length penalty.
In a contrastive experiment, the LM training
data was substantially increased by adding 352M
words of commercial Broadcast News data and
232M words of CNN news collected on the Inter-
net. Although the perplexity of the 4-gram back-
off LM decreased by 5 points to 74.1, we observed
no change in the BLEU score. In order to estimate
the oracle BLEU score of the lattices we build a 4-
gram back-off LM on the development data. Lat-
tice rescoring achieved a BLEU score of 59.10.
There are many discussions about the BLEU
score being or not a meaningful measure to as-
sess the quality of an automatic translation sys-
tem. It would be interesting to verify if the contin-
uous space LM has an impact when human judg-
ments of the translation quality are used, in partic-
ular with respect to fluency. Unfortunately, this is
not planed in the TC-STAR evaluation campaign,
and we give instead an example translation (see
Figure 4). In this case, two errors were corrected
(insertion of the word ?the? and deletion of the
comma).
5 Conclusion and Future Work
Some SMT decoders have an execution complex-
ity that increases rapidly with the length of the
sentences to be translated, which are usually split
729
into smaller chunks and translated separately. This
can lead to translation errors and bad modeling
of the LM probabilities of the words at both ends
of the chunks. We have presented a lattice join-
ing and rescoring approach that obtained signifi-
cant improvements in the BLEU score compared
to simply concatenating the 1-best solutions of
the individual chunks. The task considered is the
translation of European Parliament Speeches in
the framework of the TC-STAR project.
We have also presented a neural network LM
that performs probability estimation in a contin-
uous space. Since the resulting probability func-
tions are smooth functions of the word represen-
tation, better generalization to unknown n-grams
can be expected. This is particularly interesting
for tasks where only limited amounts of appropri-
ate LM training material are available, but the pro-
posed LM can be also trained on several hundred
millions words. The continuous space LM is used
to rescore the translation lattices. We obtained
an improvement of 0.8 points BLEU on the test
data compared to a 4-gram back-off LM, which it-
self had already achieved the same improvement
in comparison to a 3-gram LM.
The results reported in this paper have been ob-
tained with a word based SMT system, but the
continuous space LM can also be used with a
phrase-based system. One could expect that the
target language model plays a different role in
a phrase-based system since the phrases induce
some local coherency on the target sentence. This
will be studied in the future. Another promis-
ing direction that we have not yet explored, is to
build long-span LM, i.e. with n much greater than
4. The complexity of our approach increases only
slightly with n. Long-span LM could possibly im-
prove the word-ordering of the generated sentence
if the translation lattices include the correct paths.
References
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(2):1137?1155.
A. Berger, S. Della Pietra, and Vincent J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22:39?71.
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
powell?s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
P. Brown, S. Della Pietra, Vincent J. Della Pietra, and
R. Mercer. 1993. The mathematics of statisti-
cal machine translation. Computational Linguistics,
19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for machine transla-
tion. In Machine Translation Summit.
Daniel De?chelotte, Holger Schwenk, and Jean-Luc
Gauvain. 2006. The 2006 LIMSI statistical ma-
chine translation system for TC-STAR. In TC-STAR
Speech to Speech Translation Workshop, Barcelona.
Ahmad Emami and Frederick Jelinek. 2005. Ran-
dom clusterings for language modeling. In ICASSP,
pages I:581?584.
C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and
H. Ney. 2005. Cross domain automatic transcrip-
tion on the TC-STAR EPPS corpus. In ICASSP.
Sasa Hasan, Olivier Bender, and Hermann Ney. 2006.
Reranking translation hypothesis using structural
properties. In LREC.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation.
In ACL?05 workshop on Building and Using Paral-
lel Text, pages 125?128.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302,
University of Pennsylvania.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In NAACL, pages 161?168.
Franz-Joseph Och. 2005. The Google statistical ma-
chine translation system for the 2005 Nist MT eval-
uation, Oral presentation at the 2005 Nist MT Eval-
uation workshop, June 20.
Holger Schwenk and Jean-Luc Gauvain. 2005. Train-
ing neural network language models on very large
corpora. In EMNLP, pages 201?208.
Holger Schwenk. 2004. Efficient training of large
neural networks for language modeling. In IJCNN,
pages 3059?3062.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP, pages II: 901?
904.
Peng Xu and Frederick Jelinek. 2004. Random forest
in language modeling. In EMNLP, pages 325?332.
730
Proceedings of the Third Workshop on Statistical Machine Translation, pages 107?110,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?08
Daniel D?chelotte, Gilles Adda, Alexandre Allauzen, H?l?ne Bonneau-Maynard,
Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais? and Fran?ois Yvon
LIMSI/CNRS
firstname.lastname@limsi.fr
Abstract
This paper describes our statistical machine
translation systems based on the Moses toolkit
for the WMT08 shared task. We address the
Europarl and News conditions for the follow-
ing language pairs: English with French, Ger-
man and Spanish. For Europarl, n-best rescor-
ing is performed using an enhanced n-gram
or a neuronal language model; for the News
condition, language models incorporate extra
training data. We also report unconvincing re-
sults of experiments with factored models.
1 Introduction
This paper describes our statistical machine trans-
lation systems based on the Moses toolkit for the
WMT 08 shared task. We address the Europarl and
News conditions for the following language pairs:
English with French, German and Spanish. For Eu-
roparl, n-best rescoring is performed using an en-
hanced n-gram or a neuronal language model, and
for the News condition, language models are trained
with extra training data. We also report unconvinc-
ing results of experiments with factored models.
2 Base System architecture
LIMSI took part in the evaluations on Europarl data
and on News data, translating French, German and
Spanish from and to English, amounting a total
of twelve evaluation conditions. Figure 1 presents
the generic overall architecture of LIMSI?s transla-
tion systems. They are fairly standard phrase-based
?Univ. Montr?al, felipe@iro.umontreal.ca
Other
Targettext
Targettext
MosestextSource or
Translation model 4g language model 4g language model
and extractionRescoring
$n$?besttranslations
LM InterpolationPhrase pairextraction Neural network
or
or
+ News Co.EuroparlEuroparl EuroparlNews Co.sources
Figure 1: Generic architecture of LIMSI?s SMT systems.
Depending on the condition, the decoder generates ei-
ther the final output or n-best lists. In the latter case,
the rescoring incorporates the same translation features,
except for a better target language model (see text).
translation systems (Och and Ney, 2004; Koehn et
al., 2003) and use Moses (Koehn et al, 2007) to
search for the best target sentence. The search uses
the following models: a phrase table, providing 4
scores and a phrase penalty, a lexicalized reordering
model (7 scores), a language model score and a word
penalty. These fourteen scores are weighted and lin-
early combined (Och and Ney, 2002; Och, 2003);
their respective weights are learned on development
data so as to maximize the BLEU score. In the fol-
lowing, we detail several aspects of our systems.
2.1 Translation models
The translation models deployed in our systems for
the europarl condition were trained on the provided
Europarl parallel data only. For the news condition,
they were trained on the Europarl data merged with
107
the news-commentary parallel data, as depicted on
Figure 1. This setup was found to be more favor-
able than training on Europarl data only (for obvious
mismatching domain reasons) and than training on
news-commentary data only, most probably because
of a lack of coverage. Another, alternative way of
benefitting from the coverage of the Europarl corpus
and the relevance of the news-commentary corpus
is to use two phrase-tables in parallel, an interest-
ing feature of Moses. (Koehn and Schroeder, 2007)
found that this was the best way to ?adapt? a transla-
tion system to the news-commentary task. These re-
sults are corroborated in (D?chelotte, 2007)1 , which
adapts a ?European Parliament? system using a ?Eu-
ropean and Spanish Parliaments? development set.
However, we were not able to reproduce those find-
ings for this evaluation. This might be caused by the
increase of the number of feature functions, from 14
to 26, due to the duplication of the phrase table and
the lexicalized reordering model.
2.2 Language Models
2.2.1 Europarl language models
The training of Europarl language models (LMs)
was rather conventional: for all languages used in
our systems, we used a 4-gram LM based on the
entire Europarl vocabulary and trained only on the
available Europarl training data. For French, for
instance, this yielded a model with a 0.2 out-of-
vocabulary (OOV) rate on our LM development set,
and a perplexity of 44.9 on the development data.
For French also, a more accurate n-gram LM was
used to rescore the first pass translation; this larger
model includes both Europarl and giga word corpus
of newswire text, lowering the perplexity to 41.9 on
the development data.
2.2.2 News language models
For this condition, we took advantage of the a
priori information that the test text would be of
newspaper/newswire genre and from the November-
december 2007 period. We consequently built much
larger LMs for translating both to French and to En-
glish, and optimized their combination on appropri-
1(D?chelotte, 2007) further found that giving an increased
weight to the small in-domain data could out-perform the setup
with two phrase-tables in parallel. We haven?t evaluated this
idea for this evaluation.
ate source of data. For French, we interpolated five
different LMs trained on corpus containing respec-
tively newspapers, newswire, news commentary and
Europarl data, and tuned their combination with text
downloaded from the Internet. Our best LM had an
OOV rate of about 2.1% and a perplexity of 111.26
on the testset. English LMs were built in a similar
manner, our largest model combining 4 LMs from
various sources, which, altogether, represent about
850M words. Its perplexity on the 2008 test set was
approximately 160, with an OOV rate of 2.7%.
2.2.3 Neural network language models
Neural-Network (NN) based continuous space
LMs similar to the ones in (Schwenk, 2007) were
also trained on Europarl data. These networks com-
pute the probabilities of all the words in a 8192 word
output vocabulary given a context in a larger, 65000-
word vocabulary. Each word in the context is first
associated with a numerical vector of dimension 500
by the input layer. The activity of the 500 neurons in
the hidden layer is computed as the hyperbolic tan-
gent of the weighted sum of these vectors, projecting
the context into a [?1, 1] hypercube of dimension
500. Final projection on a set of 8192 output neurons
yields the final probabilities through a softmax-ed,
weighted sum of the coordinates in the hypercube.
The final NN-based model is interpolated with the
main LM model in a 0.4-0.6 ratio, and yields a per-
plexity reduction of 9% relative with respect to the
n-gram LM on development data.
2.3 Tuning procedure
We use MERT, distributed with the Moses decoder,
to tune the first pass of the system. The weights
were adjusted to maximize BLEU on the develop-
ment data. For the baseline system, a dozen Moses
runs are necessary for each MERT optimization, and
several optimization runs were started and compared
during the system?s development. Tuning was per-
formed using dev2006 for the Europarl task and on
News commentary dev2007 for the news task.
2.4 Rescoring and post processing
For the Europarl condition, distinct 100 best trans-
lations from Moses were rescored with improved
LMs: when translating to French, we used the
French model described in section 2.2.1; when
108
Es-En En-Es Fr-En En-Fr
baseline 32.21 31.62 32.41 29.31
Limsi 32.49 31.23 32.62 30.27
Table 1: Comparison of two tokenization policies
All results on Europarl test2007
CI system CS system
En?Fr 27.23 27.55
Fr?En 30.96 30.98
Table 2: Effect of training on true case texts, for English
to French (case INsensitive BLEU scores, untuned sys-
tems, results on test2006 dataset)
translating to English, we used the neuronal LM de-
scribed in section 2.2.3.
For all the ?lowcase? systems (see below), recase-
ing was finally performed using our own recaseing
tool. Case is restored by creating a word graph al-
lowing all possible forms of caseing for each word
and each component of a compound word. This
word graph is then decoded using a cased 4-gram
LM to obtain the most likely form. In a final step,
OOV words (with respect to the source language
word list) are recased to match their original form.
3 Experiments with the base system
3.1 Word tokenization and case
We developed our own tokenizer for English, French
and Spanish, and used the baseline tokenizer for
German. Experiments on the 2007 test dataset for
Europarl task show the impact of the tokenization
on the BLEU scores, with 3-gram LMs. Results are
always improved with our own tokenizer, except for
English to Spanish (Table 1).
Our systems were initially trained on lowercase
texts, similarly to the proposed baseline system.
However, training on true case texts proved bene-
ficial when translating from English to French, even
when scoring in a case insensitive manner. Table 2
shows an approximate gain of 0.3 BLEU for that di-
rection, and no impact on French to English perfor-
mance. Our English-French systems are therefore
case sensitive.
3.2 Language Models
For Europarl, we experimented with LMs of increas-
ing orders: we found that using a 5-gram LM only
yields an insignificant improvement over a 4-gram
LM. As a result, we used 4-gram LMs for all our
first pass decodings. For the second pass, the use
of the Neural Network LMs, if used with an appro-
priate (tuned) weight, yields a small, yet consistent
improvement of BLEU for all pairs.
Performance on the news task are harder to ana-
lyze, due to the lack of development data. Throwing
in large set of in-domain data was obviously helpful,
even though we are currently unable to adequately
measure this effect.
4 Experiments with factored models
Even though these models were not used in our sub-
missions, we feel it useful to comment here our (neg-
ative) experiments with factored models.
4.1 Overview
In this work, factored models (Koehn and Hoang,
2007) are experimented with three factors : the sur-
face form, the lemma and the part of speech (POS).
The translation process is composed of different
mapping steps, which either translate input factors
into output factors, or generate additional output fac-
tors from existing output factors. In this work, four
mapping steps are used with two decoding paths.
The first path corresponds to the standard and di-
rect mapping of surface forms. The second decod-
ing path consists in two translation steps for respec-
tively POS tag and the lemmas, followed by a gener-
ation step which produces the surface form given the
POS-lemma couple. The system also includes three
reordering models.
4.2 Training
Factored models have been built to translate from
English to French for the news task. To estimate the
phrase and generation tables, the training texts are
first processed in order to compute the lemmas and
POS information. The English texts are tagged and
lemmatized using the English version of the Tree-
tagger2. For French, POS-tagging is carried out
with a French version of the Brill?s tagger trained
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
109
on the MULTITAG corpus (Allauzen and Bonneau-
Maynard, 2008). Lemmatization is performed with
a French version of the Treetagger.
Three phrase tables are estimated with the Moses
utilities, one per factor. For the surface forms, the
parallel corpus is the concatenation of the official
training data for the tasks Europarl and News com-
mentary, whereas only the parallel data of news
commentary are used for lemmas and POS. For the
generation step, the table built on the parallel texts of
news commentary is augmented with a French dic-
tionary of 280 000 forms. The LM is the largest LM
available for French (see section 2.2.2).
4.3 Results and lessons learned
On the news test set of 2008, this system obtains a
BLEU score of 20.2, which is worse than our ?stan-
dard? system (20.9). A similar experiment on the
Europarl task proved equally unsuccessful.
Using only models which ignore the surface form
of input words yields a poor system. Therefore, in-
cluding a model based on surface forms, as sug-
gested (Koehn and Hoang, 2007), is also neces-
sary. This indeed improved (+1.6 BLEU for Eu-
roparl) over using one single decoding path, but not
enough to match our baseline system performance.
These results may be explained by the use of auto-
matic tools (POS tagger and lemmatizer) that are not
entirely error free, and also, to a lesser extend, by the
noise in the test data. We also think that more effort
has to be put into the generation step.
Tuning is also a major issue for factored trans-
lation models. Dealing with 38 weights is an op-
timization challenge, which took MERT 129 itera-
tions to converge. The necessary tradeoff between
the huge memory requirements of these techniques
and computation time is also detrimental to their use.
Although quantitative results were unsatisfactory,
it is finally worth mentioning that a manual exami-
nation of the output revealed that the explicit usage
of gender and number in our models (via POS tags)
may actually be helpful when translating to French.
5 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT 08 shared task. As ex-
pected, regarding the Europarl condition, our BLEU
improvements over the best 2007 results are limited:
paying attention to tokenization and caseing issues
brought us a small pay-off; rescoring with better
language models gave also some reward. The news
condition was new, and more challenging: our satis-
factory results can be attributed to the use of large,
well tuned, language models. In comparison, our ex-
periments with factored models proved disappoint-
ing, for reasons that remain to be clarified. On a
more general note, we feel that the performance of
MT systems for these tasks are somewhat shadowed
by normalization issues (tokenization errors, incon-
sistent use of caseing, typos, etc), making it difficult
to clearly analyze our systems? performance.
References
A. Allauzen and H. Bonneau-Maynard. 2008. Training
and evaluation of POS taggers on the French multitag
corpus. In Proc. LREC?08, To appear.
D. D?chelotte. 2007. Traduction automatique de la pa-
role par m?thodes statistiques. Ph.D. thesis, Univ.
Paris XI, December.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proc. EMNLP-CoNLL, pages 868?876.
P. Koehn and J. Schroeder. 2007. Experiments in domain
adaptation for statistical machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 224?227, Prague, Czech Republic.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127?133, Edmonton, Canada, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL, demonstration
session, Prague, Czech Republic.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, Sapporo, Japan.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
110
