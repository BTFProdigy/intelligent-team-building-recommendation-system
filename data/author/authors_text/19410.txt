Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 57?60,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Graphical Interface for Automatic Error Mining in Corpora
Gregor Thiele Wolfgang Seeker Markus G
?
artner Anders Bj
?
orkelund Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
Abstract
We present an error mining tool that is de-
signed to help human annotators to find
errors and inconsistencies in their anno-
tation. The output of the underlying al-
gorithm is accessible via a graphical user
interface, which provides two aggregate
views: a list of potential errors in con-
text and a distribution over labels. The
user can always directly access the ac-
tual sentence containing the potential er-
ror, thus enabling annotators to quickly
judge whether the found candidate is in-
deed incorrectly labeled.
1 Introduction
Manually annotated corpora and treebanks are the
primary tools that we have for developing and
evaluating models and theories for natural lan-
guage processing. Given their importance for test-
ing our hypotheses, it is imperative that they are
of the best quality possible. However, manual an-
notation is tedious and error-prone, especially if
many annotators are involved. It is therefore desir-
able to have automatic means for detecting errors
and inconsistencies in the annotation.
Automatic methods for error detection in tree-
banks have been developed in the DECCA
project
1
for several different annotation types, for
example part-of-speech (Dickinson and Meurers,
2003a), constituency syntax (Dickinson and Meur-
ers, 2003b), and dependency syntax (Boyd et al.,
2008). These algorithms work on the assumption
that two data points that appear in identical con-
texts should be labeled in the same way. While
the data points in question, or nuclei, can be single
tokens, spans of tokens, or edges between two to-
kens, context is usually modeled as n-grams over
the surrounding tokens. A nucleus that occurs
1
http://www.decca.osu.edu
multiple times in identical contexts but is labeled
differently shows variation and is considered a po-
tential error.
Natural language is ambiguous and variation
found by an algorithm may be a genuine ambigu-
ity rather than an annotation error. Although we
can support an annotator in finding inconsisten-
cies in a treebank, these inconsistencies still need
to be judged by humans. In this paper, we present
a tool that allows a user to run automatic error de-
tection on a corpus annotated with part-of-speech
or dependency syntax.
2
The tool provides the user
with a graphical interface to browse the variation
nuclei found by the algorithm and inspect their la-
bel distribution. The user can always switch be-
tween high-level aggregate views and the actual
sentences containing the potential error in order to
decide if that particular annotation is incorrect or
not. The interface thus brings together the output
of the error detection algorithm with a direct ac-
cess to the corpus data. This speeds up the pro-
cess of tracking down inconsistencies and errors
in the annotation considerably compared to work-
ing with the raw output of the original DECCA
tools. Several options allow the user to fine-tune
the behavior of the algorithm. The tool is part of
ICARUS (G?artner et al., 2013), a general search
and exploration tool.
3
2 The Error Detection Algorithm
The algorithm, described in Dickinson and Meur-
ers (2003a) for POS tags, works by starting from
individual tokens (the nuclei) by recording their
assigned part-of-speech over an entire treebank.
From there, it iteratively increases the context for
each instance by extending the string to both sides
to include adjacent tokens. It thus successively
builds larger n-grams by adding tokens to the left
2
Generalizing the tool to support any kind of positional
annotation is planned.
3
http://www.ims.uni-stuttgart.de/data/icarus.html
57
Figure 1: The variation n-gram view.
or to the right. Instances are grouped together if
their context is identical, i. e. if their token n-
grams match. Groups where all instances have
the same label do not show variation and are dis-
carded. The algorithm stops when either no vari-
ation nuclei are left or when none of them can be
further extended. All remaining groups that show
variation are considered potential errors. Erro-
neous annotations that do not show variation in the
data cannot be found by the algorithm. This limits
the usefulness of the method for very small data
sets. Also, given the inherent ambiguity of nat-
ural language, the algorithm is not guaranteed to
exclusively output errors, but it achieves very high
precision in experiments on several languages.
The algorithm has been extended to find errors
in constituency and dependency structures (Dick-
inson and Meurers, 2003b; Boyd et al., 2008),
where the definition of a nucleus is changed to
capture phrases and dependency edges. Context
is always modeled using n-grams over surround-
ing tokens, but see, e. g., Boyd et al. (2007) for
extensions.
3 Graphical Error Mining
To start the error mining, a treebank and an error
mining algorithm (part-of-speech or dependency)
must be selected. The algorithm is then executed
on the data to create the variation n-grams. The
user can choose between two views for browsing
the potential errors in the treebank: (1) a view
showing the list of variation n-grams found by the
error detection algorithm and (2) a view showing
label distributions over word forms.
3.1 The Variation N-Gram View
Figure 1 shows a screenshot of the view where the
user is presented with the list of variation n-grams
output by the error detection algorithm. The main
window shows the list of n-grams. When the user
selects one of the n-grams, information about the
nucleus is displayed below the main window. The
user can inspect the distribution over labels (here
part-of-speech tags) with their absolute frequen-
cies. Above the main window, the user can adjust
the length of the presented n-grams, sort them, or
search for specific strings.
For example, Figure 1 shows a part of the vari-
ation n-grams found in the German TiGer corpus
(Brants et al., 2002). The minimum and maximum
length was restricted to four, thus the list contains
only 4-grams. The 4-gram so hoch wie in was se-
lected, which contains wie as its nucleus. In the
lower part, the user can see that wie occurs with
four different part-of-speech tags in the treebank,
namely KOKOM, PWAV, KON, and KOUS. Note
that the combination with KOUS occurs only once
in the entire treebank.
Double clicking on the selected 4-gram in the
list will open up a new tab that displays all sen-
tences that contain this n-gram, with the nucleus
being highlighted. The user can then go through
each of the sentences and decide whether the an-
notated part-of-speech tag is correct. Each time
the user clicks on an n-gram, a new tab will be
created, so that the user can jump back to previous
results without having to recreate them.
A double click on one of the lines in the lower
part of the window will bring up all sentences that
contain that particular combination of word form
58
Figure 2: The label distribution view.
and part-of-speech tag. The fourth line will, for
example, show the one sentence where wie has
been tagged as KOUS, making it easy to quickly
judge whether the tag is correct. In this case, the
annotation is incorrect (it should have been PWAV)
and should thus be marked for correction.
3.2 The Label Distribution View
In addition to the output of the algorithm by Dick-
inson and Meurers (2003a), the tool also provides
a second view, which displays tag distributions of
word forms to the user (see Figure 2). To the left,
a list of unique label combinations is shown. Se-
lecting one of them displays a list of word forms
that occur with exactly these tags in the corpus.
This list is shown below the list of label combina-
tions. To the right, the frequencies of the differ-
ent labels are shown in a bar chart. The leftmost
bar for each label always shows the total frequency
summed over all word forms in the set. Selecting
one or more in the list of word forms adds addi-
tional bars to the chart that show the frequencies
for each selected word form.
As an example, Figure 2 shows the tag combi-
nation [VVINF][VVIZU], which are used to tag in-
finitives with and without incorporated zu in Ger-
man. There are three word forms in the cor-
pus that occur with these two part-of-speech tags:
hinzukommen, aufzul?osen, and anzun?ahern. The
chart on the right shows the frequencies for each
word form and part-of-speech tag, revealing that
hinzukommen is mostly tagged as VVINF but once
as VVIZU, whereas for the other two word forms it
is the other way around. This example is interest-
ing if one is looking for annotation errors in the
TiGer treebank, because the two part-of-speech
tags should have a complementary distribution (a
German verb either incorporates zu or it does not).
Double clicking on the word forms in the list in
the lower left corner will again open up a tab that
shows all sentences containing this word form, re-
gardless of their part-of-speech tag. The user may
then inspect the sentences and decide whether the
annotations are erroneous or not. If the user wants
to see a specific combination, which is more use-
ful if the total number of sentences is large, she
can also click on one of the bars in the chart to get
all sentences matching that combination. In the
example, the one instance of hinzukommen being
tagged as VVIZU is incorrect,
4
and the instances of
the two other verbs tagged as VVINF are as well.
3.3 Dependency Annotation Errors
As mentioned before, the tool also allows the user
to search for errors in dependency structures. The
error mining algorithm for dependency structures
(Boyd et al., 2008) is very similar to the one for
part-of-speech tags, and so is the interface to the
n-gram list or the distribution view. Dependency
edges are therein displayed as triples: the head,
the dependent, and the edge label with the edge?s
direction. As with the part-of-speech tags, the user
can always jump directly to the sentences that con-
tain a particular n-gram or dependency relation.
4
Actually, the word form hinzukommen can belong to two
different verbs, hinzu-kommen and hin-kommen. However,
the latter, which incorporates zu, does not occur in TiGer.
59
4 Error Detection on TiGer
We ran the error mining algorithm for part-of-
speech on the German TiGer Treebank (the de-
pendency version by Seeker and Kuhn (2012)) and
manually evaluated a small sample of n-grams in
order to get an idea of how useful the output is.
We manually checked 115 out of the 207 vari-
ation 6-grams found by the tool, which amounts
to 119 different nuclei. For 99.16% of these nu-
clei, we found erroneous annotations in the asso-
ciated sentences. 95.6% of these are errors where
we are able to decide what the right tag should
be, the remaining ones are more difficult to disam-
biguate because the annotation guidelines do not
cover them.
These results are in line with findings by Dick-
inson and Meurers (2003a) for the Penn Treebank.
They show that even manually annotated corpora
contain errors and an automatic error mining tool
can be a big help in finding them. Furthermore,
it can help annotators to improve their annotation
guidelines by pointing out phenomena that are not
covered by the guidelines, because these phenom-
ena will be more likely to show variation.
5 Related Work
We are aware of only one other graphical tool that
was developed to help with error detection in tree-
banks: Ambati et al. (2010) and Agarwal et al.
(2012) describe a graphical tool that was used in
the annotation of the Hindi Dependency Treebank.
To find errors, it uses a statistical and a rule-based
component. The statistical component is recall-
oriented and learns a MaxEnt model, which is used
to flag dependency edges as errors if their proba-
bility falls below a predefined threshold. In or-
der to increase the precision, the output is post-
processed by the rule-based component, which is
tailored to the treebank?s annotation guidelines.
Errors are presented to the annotators in tables,
also with the option to go to the sentences di-
rectly from there. Unlike the algorithm we im-
plemented, this approach needs annotated training
data for training the classifier and tuning the re-
spective thresholds.
6 Conclusion
High-quality annotations for linguistic corpora are
important for testing hypotheses in NLP and lin-
guistic research. Automatically marking potential
annotation errors and inconsistencies are one way
of supporting annotators in their work. We pre-
sented a tool that provides a graphical interface for
annotators to find and evaluate annotation errors
in treebanks. It implements the error detection al-
gorithms by Dickinson and Meurers (2003a) and
Boyd et al. (2008). The user can view errors from
two perspectives that aggregate error information
found by the algorithm, and it is always easy to
go directly to the actual sentences for manual in-
spection. The tool is currently extended such that
annotators can make changes to the data directly
in the interface when they find an error.
Acknowledgements
We thank Markus Dickinson for his comments.
Funded by BMBF via project No. 01UG1120F,
CLARIN-D, and by DFG via SFB 732, project D8.
References
Rahul Agarwal, Bharat Ram Ambati, and Anil Kumar
Singh. 2012. A GUI to Detect and Correct Errors in
Hindi Dependency Treebank. In LREC 2012, pages
1907?1911.
Bharat Ram Ambati, Mridul Gupta, Samar Husain, and
Dipti Misra Sharma. 2010. A High Recall Error
Identification Tool for Hindi Treebank Validation.
In LREC 2010.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2007. Increasing the Recall of Corpus Annota-
tion Error Detection. In TLT 2007, pages 19?30.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2008. On Detecting Errors in Dependency
Treebanks. Research on Language and Computa-
tion, 6(2):113?137.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In TLT 2002, pages 24?41.
Markus Dickinson and W. Detmar Meurers. 2003a.
Detecting Errors in Part-of-Speech Annotation. In
EACL 2003, pages 107?114.
Markus Dickinson and W. Detmar Meurers. 2003b.
Detecting Inconsistencies in Treebanks. In TLT
2003, pages 45?56.
Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj?orkelund, and Jonas Kuhn. 2013. ICARUS
? An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55?60.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In LREC 2012, pages 3132?3139.
60
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 55?60,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ICARUS ? An Extensible Graphical Search Tool
for Dependency Treebanks
Markus Ga?rtner Gregor Thiele Wolfgang Seeker Anders Bjo?rkelund Jonas Kuhn
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
firstname.lastname@ims.uni-stuttgart.de
Abstract
We present ICARUS, a versatile graphi-
cal search tool to query dependency tree-
banks. Search results can be inspected
both quantitatively and qualitatively by
means of frequency lists, tables, or depen-
dency graphs. ICARUS also ships with
plugins that enable it to interface with tool
chains running either locally or remotely.
1 Introduction
In this paper we present ICARUS1 a search and
visualization tool that primarily targets depen-
dency syntax. The tool has been designed such
that it requires minimal effort to get started with
searching a treebank or system output of an auto-
matic dependency parser, while still allowing for
flexible queries. It enables the user to search de-
pendency treebanks given a variety of constraints,
including searching for particular subtrees. Em-
phasis has been placed on a functionality that
makes it possible for the user to switch back and
forth between a high-level, aggregated view of the
search results and browsing of particular corpus
instances, with an intuitive visualization of the
way in which it matches the query. We believe this
to be an important prerequisite for accessing anno-
tated corpora, especially for non-expert users.
Search queries in ICARUS can be constructed
either in a graphical or a text-based manner. Build-
ing queries graphically removes the overhead of
learning a specialized query language and thus
makes the tool more accessible for a wider audi-
ence. ICARUS provides a very intuitive way of
breaking down the search results in terms of fre-
quency statistics (such as the distribution of part-
of-speech on one child of a particular verb against
the lemma of another child). The dimensions for
1Interactive platform for Corpus Analysis and Research
tools, University of Stuttgart
the frequency break-down are simply specified by
using grouping operators in the query. The fre-
quency tables are filled and updated in real time
as the search proceeds through the corpus ? allow-
ing for a quick detection of misassumptions in the
query.
ICARUS uses a plugin-based architecture that
permits the user to write his own plugins and in-
tegrate them into the system. For example, it
comes with a plugin that interfaces with an exter-
nal parser that can be used to parse a sentence from
within the user interface. The constraints for the
query can then be copy-pasted from the resulting
parse visualization. This facilitates example-based
querying, which is particularly helpful for inexpe-
rienced users ? they do not have to recall details
of the annotation conventions outside of their fo-
cus of interests but can go by what the parser pro-
vides.2
ICARUS is written entirely in Java and runs out
of the box without requiring any installation of
the tool itself or additional libraries. This makes
it platform independent and the only requirement
is that a Java Runtime Environment (JRE) is in-
stalled on the host system. It is open-source and
freely available for download.3
As parsers and other Natural Language Pro-
cessing (NLP) tools are starting to find their way
into other sciences such as (digital) humanities or
social sciences, it gets increasingly important to
provide intuitive visualization tools that integrate
seamlessly with existing NLP tools and are easy
to use also for non-linguists. ICARUS interfaces
readily with NLP tools provided as web services
by CLARIN-D,4 the German incarnation of the
European Infrastructure initiative CLARIN.
2This is of course only practical with rather reliable auto-
matic parsers, but in our experience, the state-of-the-art qual-
ity is sufficient.
3www.ims.uni-stuttgart.de/forschung/
ressourcen/werkzeuge/icarus.en.html
4http://de.clarin.eu
55
The remainder of this paper is structured as fol-
lows: In Section 2 we elaborate on the motivation
for the tool and discuss related work. Section 3
presents a running example of how to build queries
and how results are visualized. In Section 4 we
outline the details of the architecture. Section 5
discusses ongoing work, and Section 6 concludes.
2 Background
Linguistically annotated corpora are among the
most important sources of knowledge for empir-
ical linguistics as well as computational modeling
of natural language. Moreover, for most users the
only way to develop a systematic understanding
of the phenomena in the annotations is through a
process of continuous exploration, which requires
suitable and intuitive tools.
As automatic analysis tools such as syntactic
parsers have reached a high quality standard, ex-
ploration of large collections of auto-parsed cor-
pus material becomes more and more common. Of
course, the querying problem is the same no matter
whether some target annotation was added manu-
ally, as in a treebank, or automatically. Yet, the
strategy changes, as the user will try to make sure
he catches systematic parsing errors and develops
an understanding of how the results he is deal-
ing with come about. While there is no guaran-
teed method for avoiding erroneous matches, we
believe that an easy-to-use transparent querying
mechanism that allows the user to look at the same
or similar results from various angles is the best
possible basis for an informed usage: frequency
tables breaking down the corpus distributions in
different dimensions are a good high-level hint,
and the actual corpus instances should be only one
or two mouse clicks away, presented with a con-
cise visualization of the respective instantiation of
the query constraints.
Syntactic annotations are quite difficult to query
if one is interested in specific constructions that
are not directly encoded in the annotation labels
(which is the case for most interesting phenom-
ena). Several tools have been developed to enable
researchers to do this. However, many of these
tools are designed for constituent trees only.
Dependency syntax has become popular as a
framework for treebanking because it lends itself
naturally to the representation of free word order
phenomena and was thus adopted in the creation of
treebanks for many languages that have less strict
word order, such as the Prague Dependency Tree-
bank for Czech (Hajic? et al, 2000) or SynTagRus
for Russian (Boguslavsky et al, 2000).
A simple tool for visualization of dependency
trees is What?s wrong with my NLP? (Riedel,
2008). Its querying functionality is however lim-
ited to simple string-searching on surface forms. A
somewhat more advanced tool is MaltEval (Nils-
son and Nivre, 2008), which offers a number of
predefined search patterns ranging from part-of-
speech tag to branching degree.
On the other hand, powerful tools such as PML-
TQ (Pajas and S?te?pa?nek, 2009) or INESS (Meurer,
2012) offer expressive query languages and can
facilitate cross-layer queries (e.g., involving both
syntactic and semantic structures). They also
accommodate both constituent and dependency
structures.
In terms of complexity in usage and expressiv-
ity, we believe ICARUS constitutes a middle way
between highly expressive and very simple visu-
alization tools. It is easy to use, requires no in-
stallation, while still having rich query and visual-
ization capabilities. ICARUS is similar to PML-
TQ in that it also allows the user to create queries
graphically. It is also similar to the search tool
GrETEL (Augustinus et al, 2012) as it interfaces
with a parser, allowing the user to create queries
starting from an automatic parse. Thus, queries
can be created without any prior knowledge of the
treebank annotation scheme.
As for searching constituent treebanks, there
is a plethora of existing search tools, such
as TGrep2 (Rohde, 2001), TigerSearch (Lezius,
2002), MonaSearch (Maryns, 2009), and Fangorn
(Ghodke and Bird, 2012), among others. They im-
plement different query languages with varying ef-
ficiency and expressiveness.
3 Introductory Example
Before going into the technical details, we show
an example of what you can do with ICARUS.
Assume that a user is interested in passive con-
structions in English, but does not know exactly
how this is annotated in a treebank. As a first step,
he can use a provided plugin that interfaces with
a tool chain5 to parse a sentence that contains a
passive construction (thus adopting the example-
based querying approach laid out in the introduc-
5using mate-tools by Bohnet (2010); available at
http://code.google.com/p/mate-tools
56
tion). Figure 1 shows the parser interface. In the
lower field, the user entered the sentence. The
other two fields show the output of the parser, once
as a graph and once as a feature value description.
Figure 1: Parsing the sentence ?Mary was kissed
by a boy.? with a predefined tool chain.
In the second step, the user can then mark parts
of the output graph by selecting some nodes and
edges, and have ICARUS construct a query struc-
ture from it, following the drag-and-drop scheme
users are familiar with from typical office soft-
ware. The automatically built query can be man-
ually adjusted by the user (relaxing constraints)
and then be used to search for similar structures
in a treebank. The parsing step can of course be
skipped altogether, and a query can be constructed
by hand right away. Figure 2 shows the query
builder, where the user can define or edit search
graphs graphically in the main window, or enter
them as a query string in the lower window.
Figure 2: Query builder for constructing queries.
For the example, Figure 3 shows the query as it
is automatically constructed by ICARUS from the
partial parse tree (3a), and what it might look like
after the user has changed it (3b). The modified
query matches passive constructions in English, as
annotated in the CoNLL 2008 Shared Task data set
(Surdeanu et al, 2008), which we use here.
(a) automatically extracted (b) manually edited
Figure 3: Search graphs for finding passive con-
structions. (a) was constructed automatically from
the parsed sentence, (b) is a more general version.
The search returns 6,386 matches. Note that
the query (Figure 3b) contains a <*>-expression.
This grouping operator groups the results accord-
ing to the specified dimension, in this case by the
lemma of the passivized verb. Figure 4 shows
the result view. On the left, a list of lemmas is
presented, sorted by frequency. Clicking on the
lemma displays the list of matches containing that
particular lemma on the right side. The match-
ing sentences can then be browsed, with the active
sentence also being shown as a tree. Note that the
instantiation of the query constraints is highlighted
in the tree display.
Figure 4: Passive constructions in the treebank
grouped by lemma and sorted by frequency.
The query could be further refined to restrict it
to passives with an overt logical subject, using a
more complex search graph for the by-phrase and
a second instance of the grouping operator. The
results will then also be grouped by the lemma of
the logical subject, and are therefore presented as
a two-dimensional table. Figure 5 shows the new
query and the resulting view. The user is presented
with a frequency table, where each cell contains
the number of hits for this particular combination
of verb lemma and logical subject. Clicking on
the cell opens up a view similar to the right part of
Figure 4 where the user can then again browse the
actual trees.
57
Figure 5: Search graph and result view for passive
constructions with overt logical subjects, grouped
by lemma of the verb and the lemma of the logical
subject.
Finally, we can add a third grouping operator.
Figure 6 shows a further refined query for passives
with an overt logical subject and an object. In the
results, the user is presented with a list of values
for the first grouping operator to the left. Clicking
on one item in that list opens up a table on the right
presenting the other two dimensions of the query.
Figure 6: Search graph and result view for passive
constructions with an overt logical subject and an
object, grouped by lemma of the verb, the logical
subject, and the object.
This example demonstrates a typical use case
for a user that is interested in certain linguistic
constructions in his corpus. Creating the search
graph and interpreting the results does not re-
quire any specialized knowledge other than fa-
miliarity with the annotation of the corpus being
searched. It especially does not require any pro-
gramming skills, and the possibility to graphically
build a query obviates the need to learn a special-
ized query language.
4 Architecture
This section goes into more details about the in-
ner workings of ICARUS. A main component
is the search engine, which enables the user to
quickly search treebanks for whatever he is inter-
ested in. A second important feature of ICARUS
is the plugin-based architecture, which allows for
the definition of custom extensions. Currently,
ICARUS can read the commonly used CoNLL de-
pendency formats, and it is easy to write exten-
sions in order to add additional formats.
4.1 Search Engine and Query Builder
ICARUS has a tree-based search engine for tree-
banks, and includes a graphical query builder.
Structure and appearance of search graphs are sim-
ilar to the design used for displaying dependency
trees (cf. Figure 1), which is realized with the
open-source library JGraph.6 Queries and/or their
results can be saved to disk and later reloaded for
further processing.
Defining a query graphically basically amounts
to drawing a partial graph structure that defines
the type of structure that the user is interested in.
In practice, this is done by creating nodes in the
query builder and connecting them by edges. The
nodes correspond to words in the dependency trees
of the treebank. Several features like word iden-
tity, lemma, part of speech, etc. can be specified
for each node in the search graph in order to re-
strict the query. Dominance and precedence con-
straints over a set of nodes can be defined by sim-
ply linking nodes with the appropriate edge type.
Edges can be further specified for relation type,
distance, direction, projectivity, and transitivity. A
simple example is shown in Figures 2 and 3. The
search engine supports regular expressions for all
string-properties (form, lemma, part of speech, re-
lation). It also supports negation of (existence of)
nodes and edges, and their properties.
As an alternative to the search graph, the user
can also specify the query in a text-based format
by constructing a comma separated collection of
constraints in the form of key=value pairs for a
single node contained within square brackets. Hi-
erarchical structures are expressed by nesting their
textual representation. Figure 7 shows the text-
based form of the three queries used in the exam-
ples in Section 3.
6http://www.jgraph.com/
58
Query 1: [lemma=be[pos=VBN,lemma=<*>,rel=VC]]
Query 2: [lemma=be[pos=VBN,lemma=<*>,rel=VC[form=by,rel=LGS[lemma=<*>,rel=PMOD]]]]
Query 3: [lemma=be[pos=VBN,lemma=<*>,rel=VC[form=by,rel=LGS[lemma=<*>,rel=PMOD]]
[lemma=<*>,rel=OBJ]]]
Figure 7: Text representation of the three queries used in the example in Section 3.
A central feature of the query language is the
grouping operator (<*>), which will match any
value and cause the search engine to group result
entries by the actual instance of the property de-
clared to be grouped. The results of the search
will then be visualized as a list of instances to-
gether with their respective frequencies. Results
can be sorted alphabetically or by frequency (ab-
solute or relative counts) . Depending on the num-
ber of grouping operators used (up to a maximum
of three) the result is structured as a list of fre-
quencies (cf. Figure 4), a table of frequencies for
pairs of instances (cf. Figure 5), or a list where
each item then opens up a table of frequency re-
sults (cf. Figure 6). In the search graph and the
result view, different colors are used to distinguish
between different grouping operators.
The ICARUS search engine offers three differ-
ent search modes:
Sentence-based. Sentence based search stops at
the first successful hit in a sentence and returns
every sentence on a list of results at most once.
Exhaustive sentence-based. The exhaustive
sentence-based search mode extends the sentence
based search by the possibility of processing mul-
tiple hits within a single sentence. Every sentence
with at least one hit is returned exactly once. In the
result view, the user can then browse the different
hits found in one sentence.
Hit-based. Every successful hit is returned sepa-
rately on the corresponding list of results.
When a query is issued, the search results are
displayed on the fly as the search engine is pro-
cessing the treebank. The sentences can be ren-
dered in one of two ways: either as a tree, where
nodes are arranged vertically by depth in the tree,
or horizontally with all the nodes arranged side-
by-side. If a tree does not fit on the screen, part of
it is automatically collapsed but can be expanded
again by the user.
4.2 Extensibility
ICARUS relies on the Java Plugin Framework,7
which provides a powerful XML-based frame-
7http://jpf.sourceforge.net/
work for defining plugins similarly to the engine
used by the popular Eclipse IDE project. The
plugin-based architecture makes it possible for
anybody to write extensions to ICARUS that are
specialized for a particular task. The parser inte-
gration of mate-tools demonstrated in Section 3 is
an example for such an extension.
The plugin system facilitates custom extensions
that make it possible to intercept certain stages
of an ongoing search process and interact with it.
This makes it possible for external tools to pre-
process search data and apply additional annota-
tions and/or filtering, or even make use of exist-
ing indices by using search constraints to limit the
amount of data passed to the search engine. With
this general setup, it is for example possible to eas-
ily extend ICARUS to work with constituent trees.
ICARUS comes with a dedicated plugin that
enables access to web services provided by
CLARIN-D. The project aims to provide tools and
services for language-centered research in the hu-
manities and social sciences. In contrast to the in-
tegration of, e.g., mate-tools, where the tool chain
is executed locally, the user can define a tool chain
by chaining several web services (e.g., lemmatiz-
ers, part-of-speech taggers etc.) together and ap-
ply them to his own data. To do this, ICARUS
is able to read and write the TCF exchange for-
mat (Heid et al, 2010) that is used by CLARIN-D
web services. The output can then be inspected
and searched using ICARUS. As new NLP tools
are added as CLARIN-D web services they can be
immediately employed by ICARUS.
5 Upcoming Extensions
An upcoming release includes the following ex-
tensions:
? Currently, treebanks are assumed to fit into
the executing computer?s main memory.
The new implementation will support asyn-
chronous loading of data, with notifications
passed to the query engine or a plugin when
required data is available. Treebanks with
millions of entries can then be loaded in less
59
memory consuming chunks, thus keeping the
system responsive when access is requested.
? The search engine is being extended with an
operator that allows disjunctions of queries.
This will enable the user to aggregate fre-
quency output over multiple queries.
6 Conclusion
We have presented ICARUS, a versatile and user-
friendly search and visualization tool for depen-
dency trees. It is aimed not only at (computa-
tional) linguists, but also at people from other dis-
ciplines, e.g., the humanities or social sciences,
who work with language data. It lets the user
create queries graphically and returns results (1)
quantitatively by means of frequency lists and ta-
bles as well as (2) qualitatively by connecting the
statistics to the matching sentences and allowing
the user to browse them graphically. Its plugin-
based architecture enables it to interface for exam-
ple with external processing pipelines, which lets
the user apply processing tools directly from the
user interface.
In the future, specialized plugins are planned
to work with different linguistic annotations, e.g.
cross-sentence annotations as used to annotate
coreference chains. Additionally, a plugin is in-
tended that interfaces the search engine with a
database.
Acknowledgments
This work was funded by the Deutsche
Forschungsgemeinschaft (DFG) via the SFB
732 ?Incremental Specification in Context?,
project D8, and by the Bundesministerium fu?r
Bildung und Forschung (BMBF) via project No.
01UG1120F, CLARIN-D center Stuttgart. The
authors are also indebted to Andre? Blessing and
Heike Zinsmeister for reading an earlier draft of
this paper.
References
Liesbeth Augustinus, Vincent Vandeghinste, and
Frank Van Eynde. 2012. Example-based Treebank
Querying. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey. ELRA.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency Treebank for Russian: Concept, Tools,
Types of Information. In COLING 2000, pages
987?991, Saarbru?cken, Germany.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In COLING
2010, pages 89?97, Beijing, China.
Sumukh Ghodke and Steven Bird. 2012. Fangorn: A
System for Querying very large Treebanks. In COL-
ING 2012: Demonstration Papers, pages 175?182,
Mumbai, India.
Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora
Vidova?-Hladka?. 2000. The Prague Dependency
Treebank: A Three-Level Annotation Scenario. In
Treebanks: Building and Using Parsed Corpora,
pages 103?127. Amsterdam:Kluwer.
Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Er-
hard Hinrichs. 2010. A Corpus Representation For-
mat for Linguistic Web Services: The D-SPIN Text
Corpus Format and its Relationship with ISO Stan-
dards. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. ELRA.
Wolfgang Lezius. 2002. Ein Suchwerkzeug fu?r syn-
taktisch annotierte Textkorpora. Ph.D. thesis, IMS,
University of Stuttgart.
Hendrik Maryns. 2009. MonaSearch ? A Tool for
Querying Linguistic Treebanks. In Proceedings of
TLT 2009, Groningen.
Paul Meurer. 2012. INESS-Search: A Search System
for LFG (and Other) Treebanks. In Miriam Butt and
Tracy Holloway King, editors, Proceedings of the
LFG2012 Conference. CSLI Publications.
Jens Nilsson and Joakim Nivre. 2008. MaltEval: an
Evaluation and Visualization Tool for Dependency
Parsing. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. ELRA.
Petr Pajas and Jan S?te?pa?nek. 2009. System for Query-
ing Syntactically Annotated Corpora. In Proceed-
ings of the ACL-IJCNLP 2009 Software Demonstra-
tions, pages 33?36, Suntec, Singapore. Association
for Computational Linguistics.
Sebastian Riedel. 2008. What?s Wrong With My
NLP?
http://code.google.com/p/
whatswrong/.
Douglas L.T. Rohde. 2001. TGrep2 the next-
generation search engine for parse trees.
http://tedlab.mit.edu/?dr/Tgrep2/.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL 2008,
pages 159?177, Manchester, England.
60
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 7?12,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
Visualization, Search, and Error Analysis for Coreference Annotations
Markus G
?
artner Anders Bj
?
orkelund Gregor Thiele Wolfgang Seeker Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
Abstract
We present the ICARUS Coreference Ex-
plorer, an interactive tool to browse and
search coreference-annotated data. It can
display coreference annotations as a tree,
as an entity grid, or in a standard text-
based display mode, and lets the user
switch freely between the different modes.
The tool can compare two different an-
notations on the same document, allow-
ing system developers to evaluate errors in
automatic system predictions. It features
a flexible search engine, which enables
the user to graphically construct search
queries over sets of documents annotated
with coreference.
1 Introduction
Coreference resolution is the task of automatically
grouping references to the same real-world entity
in a document into a set. It is an active topic in cur-
rent NLP research and has received considerable
attention in recent years, including the 2011 and
2012 CoNLL shared tasks (Pradhan et al., 2011;
Pradhan et al., 2012).
Coreference relations are commonly repre-
sented by sets of mentions, where all mentions
in one set (or coreference cluster) are considered
coreferent. This type of representation does not
support any internal structure within the clusters.
However, many automatic coreference resolvers
establish links between pairs of mentions which
are subsequently transformed to a cluster by tak-
ing the transitive closure over all links, i.e., placing
all mentions that are directly or transitively classi-
fied as coreferent in one cluster. This is particu-
larly the case for several state-of-the-art resolvers
(Fernandes et al., 2012; Durrett and Klein, 2013;
Bj?orkelund and Kuhn, 2014). These pairwise de-
cisions, which give rise to a clustering, can be ex-
ploited for detailed error analysis and more fine-
grained search queries on data automatically an-
notated for coreference.
We present the ICARUS Coreference Explorer
(ICE), an interactive tool to browse and search
coreference-annotated data. In addition to stan-
dard text-based display modes, ICE features two
other display modes: an entity-grid (Barzilay and
Lapata, 2008) and a tree view, which makes use
of the internal pairwise links within the clusters.
ICE builds on ICARUS (G?artner et al., 2013), a
platform for search and exploration of dependency
treebanks.
1
ICE is geared towards two (typically) distinct
users: The NLP developer who designs corefer-
ence resolution systems can inspect the predic-
tions of his system using the three different dis-
play modes. Moreover, ICE can compare the pre-
dictions of a system to a gold standard annotation,
enabling the developer to inspect system errors in-
teractively. The second potential user is the cor-
pus linguist, who might be interested in brows-
ing or searching a document, or a (large) set of
documents for certain coreference relations. The
built-in search engine of ICARUS now also allows
search queries over sets of documents in order to
meet the needs of this type of user.
2 Data Representation
ICE reads the formats used in the 2011 and 2012
CoNLL shared tasks as well as the SemEval 2010
format (Recasens et al., 2010).
2
Since these for-
mats cannot accommodate pairwise links, an aux-
iliary file with standoff annotation can be pro-
vided, which we call allocation. An allocation is a
list of pairwise links between mentions. Multiple
1
ICE is written in Java and is therefore platform indepen-
dent. It is open source (under GNU GPL) and we provide
both sources and binaries for download on http://www.
ims.uni-stuttgart.de/data/icarus.html
2
These two formats are very similar tabular formats, but
differ slightly in the column representations.
7
allocations can be associated with a single docu-
ment and the user can select one of these for dis-
play or search queries. An allocation can also in-
clude properties on mentions and links. The set
of possible properties is not constrained, and the
user can freely specify properties as a list of key-
value pairs. Properties on mentions may include,
e.g., grammatical gender or number, or informa-
tion status labels. Additionally, a special property
that indicates the head word of a mention can be
provided in an allocation. The head property en-
ables the user to access head words of mentions
for display or search queries.
The motivation for keeping the allocation file
separate from the CoNLL or SemEval files is two-
fold: First, it allows ICE to work without hav-
ing to provide an allocation file, thereby making it
easy to use with the established formats for coref-
erence. The user is still able to introduce addi-
tional structure by the use of the allocation file.
Second, multiple allocation files allow the user to
switch between different allocations while explor-
ing a set of documents. Moreover, as we will see
in Section 3.3, ICE can also compare two different
allocations in order to highlight the differences.
In addition to user-specified allocations, ICE
will always by default provide an internal structure
for the clusters, in which the correct antecedent
of every mention is the closest coreferent mention
with respect to the linear order of the document
(this is equivalent to the training instance creation
heuristic proposed by Soon et al. (2001)). There-
fore, the user is not required to define an allocation
on their own.
3 Display Modes
In this section we describe the entity grid and tree
display modes by means of screenshots. ICE addi-
tionally includes a standard text-based view, sim-
ilar to other coreference visualization tools. The
example document is taken from the CoNLL 2012
development set (Pradhan et al., 2012) and we
use two allocations: (1) the predictions output by
Bj?orkelund and Kuhn (2014) system (predicted)
and (2) a gold allocation that was obtained by
running the same system in a restricted setting,
where only links between coreferent mentions are
allowed (gold). The complete document can be
seen in the lower half of Figure 1.
3.1 Entity grid
Barzilay and Lapata (2008) introduce the entity
grid, a tabular view of entities in a document.
Specifically, rows of the grid correspond to sen-
tences, and columns to entities. The cells of the ta-
ble are used to indicate that an entity is mentioned
in the corresponding sentence. Entity grids pro-
vide a compact view on the distribution of men-
tions in a document and allow the user to see how
the description of an entity changes from mention
to mention.
Figure 1 shows ICE?s entity-grid view for the
example document using the predicted allocation.
When clicking on a cell in the entity grid the im-
mediate textual context of the cell is shown in the
lower pane. In Figure 1, the cell with the blue
background has been clicked, which corresponds
to the two mentions firms from Taiwan and they.
These mentions are thus highlighted in the lower
pane. The user can also right-click on a cell and
jump straight to the tree view, centered around the
same mentions.
3.2 Label Patterns
The information that is displayed in the cells of
the entity grid (and also on the nodes in the tree
view, see Section 3.3) can be fully customized by
the user. The customization is achieved by defin-
ing label patterns. A label pattern is a string that
specifies the format according to which a mention
will be displayed. The pattern can extract infor-
mation on a mention according to three axes: (1)
at the token- level for the full mention, extracting,
e.g., the sequence of surface forms or the part-of-
speech tags of a mention; (2) at the mention- level,
extracting an arbitrary property of a mention as de-
fined in an allocation; (3) token-level information
from the head word of a mention.
Label patterns can be defined interactively
while displaying a document and the three axes are
referenced by dedicated operators. For instance,
the label pattern $form$ extracts the full surface
form of a mention, whereas #form# only extracts
the surface form of the head word of a mention.
All properties defined by the user in the allocation
(see Section 2) are accessible via label patterns.
For example, the allocations we use for Fig-
ure 1 include a number of properties on the
mentions, most of which are internally com-
puted by the coreference system: The TYPE of
a mention, which can take any of the values
8
Figure 1: Entity grid over the predicted clustering in the example document.
{Name, Common, Pronoun} and is inferred from
the part- of-speech tags in the CoNLL file; The
grammatical NUMBER of a mention, which is as-
signed based on the number and gender data com-
piled by Bergsma and Lin (2006) and can take
the values {Sin, Plu, Unknown}. The label pat-
tern for displaying the number property associated
with a mention would be %Number%.
The label pattern used in Figure 1 is defined
as ("$form$" - %Type% - %Number%). This pat-
tern accesses the full surface form of the mentions
($form$), as well as the TYPE (%Type%) and gram-
matical NUMBER (%Number%) properties defined
in the allocation file.
Custom properties and label patterns can be
used for example to display the entity grid in the
form proposed by Barzilay and Lapata (2008): In
the allocation, we assign a coarse-grained gram-
matical function property (denoted GF) to every
mention, where each mention is tagged as either
subject, object, or other (denoted S, O, X, respec-
tively).
3
The label pattern %GF% then displays the
grammatical function of each mention in the entity
grid, as shown in Figure 2.
3.3 Tree view
Pairwise links output by an automatic coreference
system can be treated as arcs in a directed graph.
Linking the first mention of each cluster to an ar-
tificial root node creates a tree structure that en-
codes the entire clustering in a document. This
representation has been used in coreference re-
3
The grammatical function was assigned by converting
the phrase-structure trees in the CoNLL file (which lack
grammatical function information) to Stanford dependencies
(de Marneffe and Manning, 2008), and then extracting the
grammatical function from the head word in each mention.
Figure 2: Example entity grid, using the labels by
Barzilay and Lapata (2008).
solvers (Fernandes et al., 2012; Bj?orkelund and
Kuhn, 2014), but ICE uses it to display links be-
tween mentions introduced by an automatic (pair-
wise) resolver.
Figure 3 shows three examples of the tree view
of the same document as before: The gold allo-
cation (3a), the predicted allocation (3b), as well
as the differential view, where the two allocations
are compared (3c). Each mention corresponds to
a node in the trees and all mentions are directly or
transitively dominated by the artificial root node.
Every subtree under the root constitutes its own
cluster and a solid arc between two mentions de-
notes that the two mentions are coreferent accord-
ing to a coreference allocation. The information
displayed in the nodes of the tree can be cus-
tomized using label patterns.
In the differential view (Figure 3c), solid arcs
correspond to the predicted allocation. Dashed
nodes and arcs are present in the gold allocation,
but not in the prediction. Discrepancies between
the predicted and the gold allocations are marked
9
(a) Tree representing the gold allocation. (b) Tree representing the predicted allocation.
(c) Differential view displaying the difference between the gold and predicted allocations.
Figure 3: Tree view over the example document (gold, predicted, differential).
with different colors denoting different types of er-
rors. The example in Figure 3c contains two errors
made by the system:
1. A false negative mention, denoted by the
dashed red node Shangtou. In the gold
standard (Figure 3a) this mention is clus-
tered with other mentions such as Shantou ?s,
Shantou City, etc. The dashed arc between
Shantou ?s and Shangtou is taken from the
gold allocation, and indicates what the sys-
tem prediction should have been like.
4
2. A foreign antecedent, denoted by the solid
orange arc between Shantou ?s new high level
technology development zone and Shantou.
In this case, the coreference system erro-
neously clustered these two mentions. The
correct antecedent is indicated by the dashed
arc that originates from the document root.
4
This error likely stems from the fact that Shantou is
spelled two different ways within the same document which
causes the resolver?s string-matching feature to fail.
This error is particularly interesting since the
system effectively merges the two clusters
corresponding to Shantou and Shantou? s new
high level technology development zone. The
tree view, however, shows that the error stems
from a single link between these two men-
tions, and that the developer needs to address
this.
Since the tree-based view makes pairwise de-
cisions explicit, the differential view shown in
Figure 3c is more informative to NLP develop-
ers when inspecting errors by automatic system
than comparing a gold standard clustering to a pre-
dicted one. The problem with analyzing the error
on clusterings instead of trees is that the clusters
would be merged, i.e., it is not clear where the ac-
tual mistake was made.
Additional error types not illustrated by Fig-
ure 3c include false positive mentions, where
the system invents a mention that is not part
of the gold allocation. When a false positive
mention is assigned as an antecedent of another
10
mention, the corresponding link is marked as an
invented antecedent. Links that erroneously start
a new cluster when it is coreferent with other men-
tions to the left is marked as false new.
4 Searching
The search engine in ICE makes the annotations
in the documents searchable for, e. g., a corpus lin-
guist who is interested in specific coreference phe-
nomena. It allows the user to express queries over
mentions related through the tree. Queries can ac-
cess the different layers of annotation, both from
the allocation file and the underlying document,
using various constructs such as, e.g., transitivity,
regular expressions, and/or disjunctions. The user
can construct queries either textually (through a
query language) or graphically (by creating nodes
and configuring constraints in dialogues). For a
further discussion of the search engine we refer to
the original ICARUS paper (G?artner et al., 2013).
Figure 4 shows a query that matches cataphoric
pronouns, i.e., pronouns that precede their an-
tecedents. The figure shows the query expressed
as a subgraph (on the left) and the corresponding
results (right) obtained on the development set of
the English CoNLL 2012 data using the manual
annotation represented in the gold allocation.
The query matches two mentions that are di-
rectly or transitively connected through the graph.
The first mention (red node) matches mentions of
the type Pronoun that have to be attached to the
document root node. In the tree formalism we
adopt, this implies that it must be the first men-
tion of its cluster. The second mention (green
node) matches any mention that is not of the type
Pronoun.
(a)
(b)
Figure 4: Example search query and correspond-
ing results.
The search results are grouped along two axes:
the surface form of the head word of the first (red)
node, and the type property of the second mention
(green node), indicated by the special grouping
operator <
*
> inside the boxes. The correspond-
ing results are shown in the right half of Figure 4,
where the first group (surface form) runs verti-
cally, and the second group (mention type) runs
horizontally. The number of hits for each configu-
ration is shown in the corresponding cell. For ex-
ample, the case that the first mention of a chain is
the pronoun I and the closest following coreferent
mention that is not a pronoun is of type Common,
occurs 6 times. By clicking on a cell, the user can
jump straight to a list of the matches, and browse
them using any of the three display modes.
5 Related Work
Two popular annotation and visualization tools
for coreference are PAlinkA (Or?asan, 2003) and
MMAX2 (M?uller and Strube, 2006), which fo-
cus on a (customizable) textual visualization with
highlighting of clusters. The TrED (Pajas and
?
St?ep?anek, 2009) project is a very flexible multi-
level annotation tool centered around tree-based
annotations that can be used to annotate and vi-
sualize coreference. It also features a powerful
search engine. Recent annotation tools include the
web-based BRAT (Stenetorp et al., 2012) and its
extension WebAnno (Yimam et al., 2013). A ded-
icated query and exploration tool for multi-level
annotations is ANNIS (Zeldes et al., 2009).
The aforementioned tools are primarily meant
as annotation tools. They have a tendency of lock-
ing the user into one type of visualization (tree- or
text-based), while often lacking advanced search
functionality. In contrast to them, ICE is not meant
to be yet another annotation tool, but was designed
as a dedicated coreference exploration tool, which
enables the user to swiftly switch between differ-
ent views. Moreover, none of the existing tools
provide an entity-grid view.
ICE is also the only tool that can graphically
compare predictions of a system to a gold standard
with a fine-grained distinction on the types of dif-
ferences. Kummerfeld and Klein (2013) present
an algorithm that transforms a predicted corefer-
ence clustering into a gold clustering and records
the necessary transformations, thereby quantify-
ing different types of errors. However, their algo-
rithm only works on clusterings (sets of mentions),
not pairwise links, and is therefore not able to pin-
point some of the mistakes that ICE can (such as
the foreign antecedent described in Section 3).
11
6 Conclusion
We presented ICE, a flexible coreference visual-
ization and search tool. The tool complements
standard text-based display modes with entity-grid
and tree visualizations. It is also able to dis-
play discrepancies between two different corefer-
ence annotations on the same document, allow-
ing NLP developers to debug coreference sys-
tems in a graphical way. The built-in search en-
gine allows corpus linguists to construct complex
search queries and provide aggregate result views
over large sets of documents. Being based on the
ICARUS platform?s plugin-engine, ICE is extensi-
ble and can easily be extended to cover additional
data formats.
Acknowledgments
This work was funded by the German Federal
Ministry of Education and Research (BMBF) via
CLARIN-D, No. 01UG1120F and the German
Research Foundation (DFG) via the SFB 732,
project D8.
References
Regina Barzilay and Mirella Lapata. 2008. Model-
ing Local Coherence: An Entity-Based Approach.
Computational Linguistics, 34(1):1?34.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In COLING-ACL,
pages 33?40, Sydney, Australia, July.
Anders Bj?orkelund and Jonas Kuhn. 2014. Learning
Structured Perceptrons for Coreference Resolution
with Latent Antecedents and Non-local Features. In
ACL, Baltimore, MD, USA, June.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation.
Greg Durrett and Dan Klein. 2013. Easy Victo-
ries and Uphill Battles in Coreference Resolution.
In EMNLP, pages 1971?1982, Seattle, Washington,
USA, October.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent Structure Perceptron with Feature In-
duction for Unrestricted Coreference Resolution. In
EMNLP-CoNLL: Shared Task, pages 41?48, Jeju Is-
land, Korea, July.
Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj?orkelund, and Jonas Kuhn. 2013. ICARUS
? An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55?60, Sofia, Bulgaria, August.
Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
Driven Analysis of Challenges in Coreference Res-
olution. In EMNLP, pages 265?277, Seattle, Wash-
ington, USA, October.
Christoph M?uller and Michael Strube. 2006. Multi-
level annotation of linguistic data with MMAX2. In
Corpus Technology and Language Pedagogy: New
Resources, New Tools, New Methods, pages 197?
214. Peter Lang.
Constantin Or?asan. 2003. PALinkA: A highly cus-
tomisable tool for discourse annotation. In Akira
Kurematsu, Alexander Rudnicky, and Syun Tutiya,
editors, Proceedings of the Fourth SIGdial Work-
shop on Discourse and Dialogue, pages 39?43.
Petr Pajas and Jan
?
St?ep?anek. 2009. System for
Querying Syntactically Annotated Corpora. In ACL-
IJCNLP: Software Demonstrations, pages 33?36,
Suntec, Singapore.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In CoNLL:
Shared Task, pages 1?27, Portland, Oregon, USA,
June.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In EMNLP-
CoNLL: Shared Task, pages 1?40, Jeju Island, Ko-
rea, July.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
1?8, Uppsala, Sweden, July.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a Web-based Tool for NLP-Assisted
Text Annotation. In EACL: Demonstrations, pages
102?107, April.
Seid Muhie Yimam, Iryna Gurevych, Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno: A Flexible, Web-based and Visually
Supported System for Distributed Annotations. In
ACL: System Demonstrations, pages 1?6, August.
Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian
Chiarcos. 2009. ANNIS: a search tool for multi-
layer annotated corpora. In Proceedings of Corpus
Linguistics.
12
