Learning to Predict Problematic Situations in a Spoken Dialogue 
System: Experiments with How May I Help You? 
Mar i lyn  Walker ,  I rene  Langk i lde ,  Je r ry  Wr ight ,  A l ien  Gor in ,  D iane  L i tman 
AT&T Labs- -Research 
180 Park Avenue 
F lorham Park, NJ 07932-0971 USA 
walker, jwright, algor, diane@research, art. corn, ilangkil@isi, edu 
Abst rac t  
Current spoken dialogue systems are deficient in 
their strategies for preventing, identifying and re- 
pairing problems that arise in the conversation. This 
paper reports results on learning to automatically 
identify and predict problematic human-computer 
dialogues in a corpus of 4774 dialogues collected with 
the How May I Help You spoken dialogue system. 
Our expectation is that the ability to predict prob- 
lematic dialogues will allow the system's dialogue 
manager to modify its behavior to repair problems, 
and even perhaps, to prevent them. We train a 
problematic dialogue classifier using automatically- 
obtainable features that can identify problematic 
dialogues ignificantly better (23%) than the base- 
line. A classifier trained with only automatic fea- 
tures from the first exchange in the dialogue can 
predict problematic dialogues 7% more accurately 
than the baseline, and one trained with automatic 
features from the first two exchanges can perform 
14% better than the baseline. 
1 In t roduct ion  
Spoken dialogue systems promise fficient and nat- 
ural access to a large variety of information sources 
and services from any phone. Systems that sup- 
port short utterances to select a particular function 
(through a statement such as "Say credit card, col- 
lect or person-to-person") are saving companies mil- 
lions of dollars. Research prototypes exist for appli- 
cations uch as personal email and calendars, travel 
and restaurant information, and personal banking 
(Baggia et al, 1998; Walker et al, 1998; Seneff et 
al., 1995; Sanderman et al, 1998; Chu-Carroll and 
Carpenter, 1999) inter alia. Yet there are still many 
research challenges: current systems are limited in 
the interaction they support and brittle in many re- 
spects. We show how spoken dialogue systems can 
learn to support more natural interaction on the ba- 
sis of their previous experience. 
One way that current spoken dialogue systems are 
quite limited is in their strategies for detecting and 
repairing problems that arise in conversation. If 
a problem can be detected, the system can either 
transfer the call to a human operator or modify its 
dialogue strategy in an attempt to repair the prob- 
lem. We can train systems to improve their ability to 
detect problems by exploiting dialogues collected in 
interactions with human users. The initial segments 
of these dialogues can be used to predict hat a prob- 
lem is likely to occur. We expect hat the ability to 
predict that a dialogue is likely to be problematic 
will allow the system's dialogue manager to apply 
more sophisticated strategies to repairing problems, 
and even perhaps, to prevent hem. 
This paper reports experiments on predicting 
problems in spoken dialogue interaction by train- 
ing a problematic dialogue predictor on a corpus of 
4774 dialogues collected in an experimental trial of 
AT~;T's How May I Help You (HMIHY) spoken dia- 
logue system (Gorin et al, 1997; Riccardi and Gorin, 
to appear; E. Ammicht and Alonso, 1999). In this 
trial, the HMIHY system was installed at an AT&T 
customer care center. HMIHY answered calls from 
live customer traffic and successfully automated a 
large number of customer requests. An example of 
a dialogue that HMIHY completed successfully is
shown in Figure 13 
St: AT&T How may I help you? 
UI: I need to \[ uh \] put a call on my calling card please 
$2: May I have your card number, please? 
U2:765432 10987654 
$3: What number would you like to call? 
U3:8 1 4 7 7 7 6 6 6 6 (misrecognized) 
$4: May I have that number again? 
U4:8147776666 
$5: Thank you. 
Figure 1: Sample TASKSUCCESS Dialogue 
We shall refer to the dialogues with a TASKSUC- 
CESS outcome, in which HMIHY successfully auto- 
mates the customer's call, as the TASKSUCCESS dia- 
logues. In addition to the TASKSUCCESS dialogues, 
there are several other call outcomes, to be described 
in detail below, which we consider problematic. 
tThe phone numbers, card numbers, and pin numbers in 
the sample dialogues are artificial. 
210 
This paper reports results from a set of experi- 
ments that test whether we can learn to automat- 
ically predict that a dialogue will be problematic 
on the basis of information the system has: (1) 
early on in the dialogue; and (2) in real time. We 
train an automatic lassifer for predicting problem- 
atic dialogues from features that can be automat- 
ically extracted from the HMIHY corpus. The re- 
sults show that we can learn to predict problematic 
dialogues using fully automatic features with an ac- 
curacy ranging from 72% to 87%, depending on how 
much of the dialogue the system has seen so far. 
Section 2 describes HMIHY and the dialogue corpus 
which the experiments are based on. Section 3 de- 
tails the encoding of the dialogues and the methods 
used for utilizing the machine learning program RIP- 
PER to train an automatic problematic dialogue pre- 
dictor (Cohen, 1996). Section 4 presents the results 
and section 5 summarizes the paper and describes 
future work. 
2 Exper imenta l  Sys tem and Data  
HMIHY is a spoken dialogue system based on the no- 
tion of call routing (Gorin et al, 1997; Chu-Carroll 
and Carpenter, 1999). In the HMIHY call rout- 
ing system, services that the user can access are 
classified into 14 categories, plus a category called 
other for calls that cannot be automated and must 
be transferred to a human operator (Gorin et al, 
1997). ~ Each category describes a different task, 
such as person-to-person dialing, or receiving credit 
for a misdialed number. The system determines 
which task the caller is requesting on the basis of 
its understanding of the cMler's response to the 
open-ended system greeting A T~ T, How May I Help 
You?. Once the task has been determined, the infor- 
mation needed for completing the caller's request is 
obtained using dialogue submodules that are specific 
for each task (Abella and Gorin, 1999). 
In addition to the TASKSUCCESS dialogues in 
which HMIHY successfully automates the cus- 
tomer's call, illustrated in Figure 1, and the calls 
that are transferred to a human operator, there are 
three other possible outcomes for a call, all of which 
are problematic. The first category, which we call 
HANGUP, results from a customer's decision to hang 
up on the system. A sample HANGUP dialogue is 
in Figure 2. A caller may hang up because s/he is 
frustrated with the system; our goal is to learn from 
the corpus which system behaviors led to the caller's 
frustration. 
The second problematic ategory (WIZARD), re- 
suits from a human customer care agent's decision to 
take over the call from the system. Because HMIHY 
is experimental, each call was monitored by a human 
2 One potential use of our results is to provide better ules 
for making the transfer decision. 
SI: AT&T How may I help you? 
UI: I need credit please. 
$2: Okay. What is the reason for the credit? 
U2: Miss, uh, Miss, different area code than I needed. 
$3: Sorry. I need to know whether you need credit for 
a wrong number, bad connection or a call that was cut  
off. 
U3: It is a wrong number. (impatient humming) 
$4: What was the number that you dialed? 
USER HANGS UP 
Figure 2: Sample HANGUP Dialogue 
agent serving as a wizard who could override the 
system. There were a number of agents who partici- 
pated as wizards during the trial of HMIHY and each 
wizard was simply told to take over the call if s/he 
perceived problems with the system's performance. 
The wizard's decision was logged by the experimen- 
tal setup, resulting in labelling the call as one that 
the wizard took over. s A dialogue where the wizard 
decided that the dialogue was problematic and took 
over the call is shown in Figure 3. 
Sh AT&T How may I help you? 
Uh (silence) 
$2: Sorry. Please briefly tell me how I may  help you? 
U2: I'm trying to call 1 8 hundred call A T T. 
$3: Do you want to place a call? 
U3: Through my calling card. 
$4: May I have your card number, please? 
U4: It's 8 7 6 5 4 3 2 1 0 and then my pin number is 8 
7 6 5. (misrecognized) 
$5: Sorry. Please enter or say your card number again.  
WIZARD STEPS IN 
Figure 3: Sample WIZARD Dialogue 
The third problematic category, the TASKFAILURE 
dialogues, are cases where the system completed the 
call, but carried out a task that was not the task 
that the customer was actually requesting. An ex- 
ample TASKFAILURE dialogue is given in Figure 4: 
HMIHY interpreted utterance U2 as a request to 
make a third-party call e.g. to bill it to my home 
phone. HMIHY then asked the caller for the infor- 
mation it needed to carry out this task, the caller 
complied, and the system completed the call. 
The corpus of 4774 dialogues used in our exper- 
iments was collected in severM experimental trials 
of HMIHY on live customer traffic (Pdccardi and 
Gorin, to appear; E. Ammicht and Alonso, 1999), 
and is referred to as HM2 in (Riccardi and Gorin, to 
appear)). During the trial, all of the system behav- 
iors were automatically recorded in a log file, and 
later the dialogues were transcribed by humans and 
labelled with a semantic ategory representing the 
sOf  course we can only infer what might have mot ivated 
the wizard to take over the call, but we assume that the wizard 
had good reason for doing so. 
211 
SI: AT&T How may I help you? 
UI: (silence) 
$2: Sorry. Please briefly tell me how I may help you? 
U2: I wanta find out how much my bill's worth. 
$3: What phone number would you like the call billed 
to? 
U3: Urn the access number. 
$4: Sorry. Could I have that phone number again? 
U4:18hundred2255288.  
$5: What number would you like to call? 
U5:800 225 5288 (touchtone input) 
$6: Thank you. 
Figure 4: Sample TASKFAILURE Dialogue 
task that the caller was asking HMIHY to perform, 
on a per utterance basis. The logfiles also included 
labels indicating whether the wizard had taken over 
the call or the user had hung up. 
3 T ra in ing  an  Automat ic  
P rob lemat ic  D ia logue  Pred ic tor  
Our experiments apply the machine learning pro- 
gram RIPPER (Cohen, 1996) to automatically induce 
a "problematic dialogue" classification model. RIP- 
PER takes as input the names of a set of classes to 
be learned, the names and ranges of values of a fixed 
set of features, and training data specifying the class 
and feature values for each example in a training set. 
Its output is a classification model for predicting the 
class of future examples. In RIPPER, the classifica- 
tion model is learned using greedy search guided by 
an information gain metric, and is expressed as an 
ordered set of if-then rules. 
To apply RIPPER, the dialogues in the corpus must 
be encoded in terms of a set of classes (the output 
classification) and a set of input features that are 
used as predictors for the classes. We start with the 
dialogue categories described above, but since our 
goal is to develop algorithms that predict/identify 
problematic dialogues, we treat HANGUP, WIZARD 
and TASKFAILURE as equivalently problematic. Thus 
we train the classifier to distinguish between two 
classes: TASKSUCCESS and PROBLEMATIC. Note that 
our categorization is inherently noisy because we do 
not know the real reasons why a caller hangs up or a 
wizard takes over the call. The caller may hang up 
because she is frustrated with the system, or she may 
simply dislike automation, or her child may have 
started crying. Similarly, one wizard may have low 
confidence in the system's ability to recover from er- 
rors and use a conservative approach that results in 
taking over many calls, while another wizard may be 
more willing to let the system try to recover. Nev- 
ertheless we take these human actions as a human 
labelling of these calls as problematic. Given this 
classification, approximately 36% of the calls in the 
corpus of 4774 dialogues are PROBLEMATIC and 64% 
are TASKSUCCESS. 
Next, we encoded each dialogue in terms of a set 
of 196 features that were either automatically ogged 
by one of the system modules, hand-labelled by hu- 
mans, or derived from raw features. We use the 
hand-labelled features to produce a TOPLINE, an es- 
timation of how well a classifier could do that had 
access to perfect information. The entire feature set 
is summarized in Figure 5. 
? Acoust ic /ASR Features 
- recog, recog-numwords, ASR-duration, dtmf- 
flag, rg-modality, rg-grammar 
? NLU Features 
- a confidence measure for all of the possible 
tasks that the user could be trying to do 
- salience-coverage, inconsistency, context-shift, 
top-task, nexttop-task, top-confidence, dill- 
confidence 
? D ia logue  Manager  Features  
- sys-label, utt-id, prompt, reprompt, confirma- 
tion, subdial 
- running tallies: num-reprompts, num- 
confirms, num-subdials, reprompt%, confir- 
mation%, subdialogue% 
? Hand-Labe l led  Features  
- tscript, human-label, age, gender, user- 
modality, clean-tscript, cltscript-numwords, 
rsuccess 
? Whole-Dialogue Features 
num-utts, num-reprompts, percent-reprompts, 
num-confirms, percent-confirms, num- 
subdials, percent-subdials, dial-duration. 
Figure 5: Features for spoken dialogues. 
There are 8 features that describe the whole dia- 
logue, and 47 features for each of the first four ex- 
changes. We encode features for the first four ex- 
changes because we want to predict failures before 
they happen. Since 97% of the dialogues in our cor- 
pus are five exchanges or less, in most cases, any 
potential problematic outcome will have occurred 
by the time the system has participated in five ex- 
changes. Because the system needs to be able to 
predict whether the dialogue will be problematic us- 
ing information it has available in the initial part of 
the dialogue, we train classifiers that only have ac- 
cess to input features from exchange 1, or only the 
features from exchange 1 and exchange 2. To see 
whether our results generalize, we also experiment 
with a subset of features that are task-independent. 
We compare results for predicting problematic din- 
212 
logues, with results for identifying problematic di- 
alogues, when the classifier has access to features 
representing the whole dialogue. 
We utilized features logged by the system because 
they are produced automatically, and thus could be 
used during runtime to alter the course of the dia- 
logue. The system modules that we collected infor- 
mation from were the acoustic processer/automatic 
speech recognizer (ASR) (Riccardi and Gorin, to ap- 
pear), the natural anguage understanding (NLU) 
module (Gorin et al, 1997), and the dialogue man- 
ager (DM) (Abella and Gorin, 1999). Below we de- 
scribe each module and the features obtained from 
it. 
ASR takes as input the acoustic signal and 
outputs a potentially errorful transcription of what 
it believes the caller said. The ASR features for 
each of the first four exchanges were the output 
of the speech recognizer (recog), the number of 
words in the recognizer output (recog-numwords), 
the duration in seconds of the input to the 
recognizer (asr-duration), a flag for touchtone 
input (dtmf-flag), the input modality expected 
by the recognizer (rg-modality) (one of: none, 
speech, touchtone, speech+touchtone, touchtone- 
card, speech+touchtone-card, touchtone-date, 
speech+touchtone-date, or none-final-prompt), and 
the grammar used by the recognizer (rg-grammar). 
The motivation for the ASR features is that any 
one of them may have impacted performance. For 
example, it is well known that longer utterances 
are less likely to be recognized correctly, thus asr- 
duration could be a clue to incorrect recognition re- 
suits. In addition, the larger the grammar is, the 
more likely an ASR error is, so the name of the 
grammar vg-grammar could be a predictor of incor- 
rect recognition. 
The natural language understanding (NLU) mod- 
ule takes as input a transcription ofthe user's utter- 
ance from ASR and produces 15 confidence scores 
representing the likelihood that the caller's task is 
one of the 15 task types. It also extracts other 
relevant information, such as phone or credit card 
numbers. Thus 15 of the NLU features for each ex- 
change represent the 15 confidence scores. There 
are also features that the NLU module calculates 
based on processing the utterance. These include 
an intra-utterance measure of the inconsistency be- 
tween tasks that the user appears to be requesting 
(inconsistency), a measure of the coverage of the 
utterance by salient grammar fragments (salience- 
coverage), a measure of the shift in context between 
utterances (context-shift), he task with the highest 
confidence score (top-task), the task with the second 
highest confidence score (nexttop-task), the value of 
the highest confidence score (top-confidence), and 
the difference in values between the top and next- 
to-top confidence scores (diff-confidence). 
The motivation for these NLU features i  to make 
use of information that the NLU module has based 
on processing the output of ASR and the current dis- 
course context. For example, for utterances that fol- 
low the first utterance, the NLU module knows what 
task it believes the caller is trying to complete. If it 
appears that the caller has changed her mind, then 
the NLU module may have misunderstood a previ- 
ous utterance. The context-shift feature indicates 
the NLU module's belief that it may have made an 
error (or be making one now). 
The dialogue manager (DM) takes the output of 
NLU and the dialogue history and decides what it 
should say to the caller next. It decides whether it 
believes there is a single unambiguous task that the 
user is trying to accomplish, and how to resolve any 
ambiguity. The DM features for each of the first four 
exchanges are the task-type label which includes a 
label that indicates task ambiguity (sys-label), utter- 
ance id within the dialogue (implicit in the encod- 
ing), the name of the prompt played before the user 
utterance (prompt), and whether that prompt was a 
reprompt (reprompt), a confirmation (confirm), or a 
subdialogue prompt (subdia O, a superset of the re- 
prompts and confirmation prompts. 
The DM features are primarily motivated by pre- 
vious work. The task-type label feature is to cap- 
ture the fact that some tasks may be harder than 
others. The utterance id feature is motivated by the 
idea that the length of the dialogue may be impor- 
tant, possibly in combination with other features like 
task-type. The different prompt features for initial 
prompts, reprompts, confirmation prompts and sub- 
dialogue prompts are motivated by results indicating 
that reprompts and confirmation prompts are frus- 
trating for callers and that callers are likely to hy- 
perarticulate when they have to repeat hemselves, 
which results in ASR errors (Shriberg et al, 1992; 
Levow, 1998). 
The DM features also include running tallies for 
the number of reprompts (num-reprompts), number 
of confirmation prompts (num.confirms), and num- 
ber of subdialogue prompts (num-subdials), that had 
been played up to each point in the diMogue, as well 
as running percentages (percent-reprompts, ercent- 
confirms, percent-subdials). The use of running tal- 
lies and percentages is based on the assumption that 
these features are likely to produce generalized pre- 
dictors (Litman et al, 1999). 
The features obtained via hand-labelling were hu- 
man transcripts of each user utterance (tscript), a 
set of semantic labels that are closely related to the 
system task-type labels (human-label), age (age) and 
gender (gender) of the user, the actual modality of 
the user utterance (user-modality) (one of: nothing, 
speech, touchtone, speech+touchtone, on-speech), 
213 
and a cleaned transcript with non-word noise infor- 
mation removed (clean-tscript). From these features 
we calculated two derived features. The first was the 
number of words in the cleaned transcript (cltscript 
numwords), again on the assumption that utterance 
length is strongly correlated with ASR and NLU er- 
rors. The second derived feature was based on cal- 
culating whether the human-label matches the sys- 
label from the dialogue manager (rsuccess). There 
were four values for rsuccess: rcorrect, rmismatch, 
rpartial-match and rvacuous-match, indicating re- 
spectively correct understanding, incorrect under- 
standing, partial understanding, and the fact that 
there had been no input for ASR and NLU to oper- 
ate on, either because the user didn't say anything 
or because she used touch-tone. 
The whole-dialogue f atures derived from the per- 
utterance features were: num-utts, num-reprompts, 
percent-reprampts, hum.confirms, percent-confirms, 
num-subdials, and per-cent-subdials for the whole di- 
alogue, and the duration of the entire dialogue in 
seconds (dial-duration). 
In the experiments, the features in Figure 5 except 
the Hand-Labelled features are referred to as the AU- 
TOMATIC feature set. We examine how well we can 
identify or predict problematic dialogues using these 
features, compared to the full feature set including 
the Hand-Labelled features. As mentioned earlier, 
we wish to generalize our problematic dialogue pre- 
dictor to other systems. Thus we also discuss how 
well we can predict problematic dialogues using only 
features that are both automatically acquirable dur- 
ing runtime and independent of the HMIHY task. 
The subset of features from Figure 5 that fit this 
qualification are in Figure 6. We refer to them as 
the AUTO, TASK-INDEP feature set. 
The output of each RIPPER. experiment is a clas- 
sification model learned from the training data. To 
evaluate these results, the error rates of the learned 
classification models are estimated using the resam- 
pling method of cross-validation. In 5-fold cross- 
validation, the total set of examples is randomly di- 
vided into 5 disjoint test sets, and 5 runs of the learn- 
ing program are performed. Thus, each run uses the 
examples not in the test set for training and the re- 
maining examples for testing. An estimated error 
rate is obtained by averaging the error rate on the 
testing portion of the data from each of the 5 runs. 
Since we intend to integrate the rules learned 
by RIPPER into the HMIHY system, we examine 
the precision and recall performance of specific hy- 
potheses. Because hypotheses from different cross- 
validation experiments cannot readily be combined 
together, we apply the hypothesis learned on one 
randomly selected training set (80% of the data) to 
that set's respective test data. Thus the precision 
and recall results reported below are somewhat less 
? Acoust ic /ASR Features 
- recog, recog-numwords, ASR-duration, dtmf- 
flag, rg-modality 
? NLU Features 
- salience-coverage, inconsistency, context-shift, 
top-confidence, dig-confidence 
? D ia logue  Manager  Features  
- utt-id, reprompt, confirmation, subdial 
- running tallies: num-reprompts, num- 
confirms, num-subdials, reprompt%, confir- 
mation%, subdialogue% 
Figure 6: Automatic task independent (AUTO, 
TASK-INDEP) features available at runtime. 
reliable than the error rates from cross-validation. 
4 Results 
We present results for both predicting and identi- 
fying problematic dialogues. Because we are inter- 
ested in predicting that a dialogue will be problem- 
atic at a point in the dialogue where the system can 
do something about it, we compare prediction ac- 
curacy after having only seen the first exchange of 
the diMogue with prediction accuracy after having 
seen the first two exchanges, with identification ac- 
curacy after having seen the whole dialogue. For 
each of these situations we also compare results for 
the AUTOMATIC and AUTO, TASK-INDEP feature sets 
(as described earlier), with results for the whole fea- 
ture set including hand-labelled features. Table 1 
summarizes the results. 
The baseline on the first line of Table 1 repre- 
sents the prediction accuracy from always guess- 
ing the majority class. Since 64% of the dialogues 
are TASKSUCCESS dialogues, we can achieve 64% ac- 
curacy from simply guessing TASKSUCCESS without 
having seen any of the dialogue yet. 
The first EXCHANGE 1 row shows the results of 
using the AUTOMATIC features from only the first 
exchange to predict whether the dialogue outcome 
will be TASKSUCCESS or PROBLEMATIC. The results 
show that the machine-learned classifier can predict 
problematic dialogues 8% better than the baseline 
after having seen only the first user utterance. Using 
only task-independent automatic features (Figure 6) 
the EXCHANGE 1 classifier can still do nearly as well. 
The ALL row for EXCHANGE 1 indicates that even 
if we had access to human perceptual ability (the 
hand-labelled features) we would still only be able 
to distinguish between TASKSUCCESS and PROBLEM- 
ATIC dialogues with 77% accuracy after having seen 
the first exchange. 
214 
Features Used 
BASELINE (majority class) 
EXCHANGE 1 AUTOMATIC 
AUTO, TASK-INDEP 
ALL 
EXCHANGES l&2 AUTOMATIC 
AUTO, TASK-INDEP 
ALL 
FULL DIALOGUE AUTOMATIC 
AUTO, TASK-INDEP 
TOPLINE ALL 
\]Accuracy (SE) 
64.0 % 
72.3 % 1.04 % 
71.6 % 1.05 % 
77.0 % 0.56 % 
79.9 % 0.58 % 
78.6 % 0.37 % 
86.7 % 0.33 % 
87.0 % 0.72 % 
86.7 % 0.82 % 
92.3 % 0.72 % 
Table 1: Results for predicting and identifying problematic dialogues (SE --- Standard Error) 
The EXCHANGE l&2 rows of Table 1 show the re- 
suits using features from the first two exchanges in 
the dialogue to predict he outcome of the dialogue. 4 
The additional exchange gives roughly an additional 
7% boost in predictive accuracy using either of the 
AUTOMATIC feature sets. This is only 8% less than 
the accuracy we can achieve using these features af- 
ter having seen the whole dialogue (see below). The 
ALL row for EXCHANGE l&2 shows that we could 
achieve over 86% accuracy if we had the ability to 
utilize the hand-labelled features. 
The FULL DIALOGUE row in Table 1 for AUTO- 
MATIC and AUTO, TASK-INDEP features hows the 
ability of the classifier to identify problematic dia- 
logues, rather than predict them, using features for 
the whole dialogue. The ALL row for the FULL DI- 
ALOGUE shows that we could correctly identify over 
92% of the outcomes accurately if we had the ability 
to utilize the hand-labelled features. 
Note that the task-independent automatic fea- 
tures always perform within 2% error of the auto- 
matic features, and the hand-labelled features con- 
sistently perform with accuracies ranging from 6-8% 
greater. 
The rules that RIPPER learned on the basis of the 
Exchange 1 automatic features are below. 
Exchange 1, Automat ic  Features:  
i f  (el-top-confidence _< .924) A (el-dtmf-f lag = '1') 
then  problematic, 
if (el-cliff-confidence _<.916) A (el-asr-duration > 6.92) 
then problematic, 
default is tasksuccess. 
According to these rules, a dialogue will be prob- 
lematic if the confidence score for the top-ranked 
4Since 23% of the dialogues consisted of only two ex- 
changes, we exclude the second exchange features for those 
dialogues where the second exchange consists only of the sys- 
tem playing a closing prompt. We also excluded any features 
that indicated to the classifier that the second exchange was 
the last exchange in the dialogue. 
task (given by the NLU module) is moderate or low 
and there was touchtone input in the user utterance. 
The second rule says that if the difference between 
the top confidence score and the second-ranked con- 
fidence score is moderate or low, and the duration 
of the user utterance is more than 7 seconds, predict 
PROBLEMATIC. 
The performance of these rules is summarized in
Table 2. These results show that given the first ex- 
change, this ruleset predicts that 22% of the dia- 
logues will be problematic, while 36% of them ac- 
tually will be. Of the dialogues that actually will 
be problematic, it can predict 41% of them. Once 
it predicts that a dialogue will be problematic, it is 
correct 69% of the time. As mentioned earlier, this 
reflects an overMl improvement in accuracy of 8% 
over the baseline. 
The rules learned by training on the automatic 
task-independent features for exchanges 1 and 2 are 
given below. As in the first rule set, the features that 
the classifier appears to be exploiting are primarily 
those from the ASR and NLU modules. 
Exchanges l&2, Automatic Task- 
Independent Features: 
i f  (e2-recog-numwords < 0) A (el-cliff-confidence < .95) 
then  problematic. 
if (el-salience-coverage < .889) A (e2-recog contains 
"I') A (e2-asr-duration > 7.48) then problematic. 
if (el-top-confidence < .924) A (e2-asr-duration >_ 5.36) 
A (el-asr-duration > 8.6) then problematic. 
if (e2-recog is blank) A (e2-asr-duration > 2.8) then 
problematic. 
if (el-salience-coverage < .737) A (el-recog contains 
"help") A (el-asr-duration < 7.04) then problematic. 
if (el-cliff-confidence < .924) A (el-dtmf-flag = '1') A 
(el-asr-duration < 6.68) then problematic. 
default is tasksuccess. 
The performance of this ruleset is summarized in
Table 3. These results show that, given the first 
two exchanges, this ruleset predicts that 26% of the 
215 
Class 
Success 
Problematic 
Occur red  Pred ic ted  Recal l  P rec i s ion  
64.1% 78.3 % 89.44 % 73.14 % 
35.9 % 21.7 % 41.47 % 68.78 % 
Table 2: Precision and Recall with Exchange 1 Automatic Features 
Class 
Success 
Problematic 
Occur red  Pred ic ted  Recal l  Prec is ion  
64.1% 75.3 % 91.42 % 77.81% 
35.9 % 24.7 %' 53.53 % 77.78 % 
Table 3: Precision and Recall with Exchange l&2 Automatic, Task-Independent Features 
dialogues will be problematic, while 36% of them 
actually will be. Of the problematic dialogues, it 
can predict 57% of them. Once it predicts that a 
dialogue will be problematic, it is correct 79% of 
the time. Compared with the classifier for the first 
utterance alone, this classifier has an improvement 
of 16% in recall and 10% in precision, for an overall 
improvement in accuracy of 7% over using the first 
exchange alone. 
One observation from these hypotheses i  the clas- 
sifier's preference for the asr-duration feature over 
the feature for the number of words recognized 
(recog-numwords). One would expect longer utter- 
ances to be more difficult, but the learned rulesets 
indicate that duration is a better measure of utter- 
ance length than the number of words. Another ob- 
servation is the usefulness of the NLU confidence 
scores and the NLU salience-coverage in predicting 
problematic dialogues. These features eem to pro- 
vide good general indicators of the system's uccess 
in recognition and understanding. The fact that the 
main focus of the rules is detecting ASR and NLU 
errors and that none of the DM behaviors are used 
as predictors also indicates that, in all likelihood, the 
DM is performing as well as it can, given the noisy 
input that it is getting from ASR and NLU. 
To identify potential improvements in the prob- 
lematic dialogue predictor, we analyzed which hand- 
labelled features made large performance improve- 
ments, under the assumption that future work can 
focus on developing automatic features that ap- 
proximate the information provided by these hand- 
labelled features. The analysis indicated that the 
vsuceess feature alone improves the performance of 
the TOPLINE from 88.5%, as reported in (Langkilde 
et al, 1999), to 92.3%. Using rsuccess as the only 
feature results in 73.75% accuracy for exchange 1, 
81.9% accuracy for exchanges 18z2 and 85.3% accu- 
racy for the full dialogue. In addition, for Exchanges 
l&2, the accuracy of the AUTOMATIC, TASK-INDEP 
feature set plus the rsuccess feature is 86.5%, which 
is only 0.2% less than the accuracy of ALL the lea- 
tures for Exchanges l&2 as shown in Table 1. The 
rules that RIPPER learns for Exchanges 1&52 when 
the AUTOMATIC, TASK-INDEP feature set is aug- 
mented with the single hand-labelled rsuccess fea- 
ture is shown below. 
Exchanges  1~2,  Rsuccess -b Automat ic  
Task - Independent  Features: 
ife2-salience-coverage ~ 0.651 A e2-asr-duration >_0.04 
A e2-rsuccess=Rvacuous-match then problematic, 
if e2-rsuccess=Rmismatch A el-top-confidence < 0.909 
then problematic, 
if e2-rsuccess=Rmismatch A e2-context-shift < 0.014 A 
e2-salience-coverage ~ 0.2 A e2-recog-numwords < 12 ( 
then problematic, 
if e2-rsuccess=Rmismatch ^ el-rsuccess=Rmismatch 
then problematic, 
if e2-rsuccess=Rmismatch A e2-top-confidence < 0.803 
^ e2-asr-duration >__2.68 ^  e2-asr-duration < 6.32 then 
problematic, 
if el-rsuccess=Rmismatch A el-diff-confidence > 0.83 
then problematic, 
if e2-rsuccess=Rmismatch A e2-context-shift >_ 0.54 
then problematic, 
ife2-asr-duration > 5.24 A e2-salience-coverage < 0.833 
A e2-top-confidence < 0.801 A e2-recog-numwords < 7 
A e2-asr-duration < 16.08 then problematic, 
if el-diff-confidence < 0.794 A el-asr-duration > 7.2 
A el-inconsistency > 0.024 A el-inconsistency > 0.755 
then problematic, 
default is tasksuccess 
Note that the rsuccess feature is frequently used in 
the rules and that RIPPER learns rules that combine 
the rsuccess feature with other features, such as the 
confidence, asr-duration, and salience-coverage fea- 
tures. 
5 D iscuss ion  and Future  Work  
In summary, our results show that: (1) All feature 
sets significantly improve over the baseline; (2) Us- 
ing automatic features from the whole dialogue, we 
can identify problematic dialogues 23% better than 
the baseline; (3) Just the first exchange provides ig- 
216 
nificantly better prediction (8%) than the baseline; 
(4) The second exchange provides an additional sig- 
nificant (7%) improvement, (5) A classifier based on 
task-independent automatic features performs with 
less than 1% degradation in error rate relative to 
the automatic features. Even with current accuracy 
rates, the improved ability to predict problematic 
dialogues means that it may be possible to field the 
system without human agent oversight, and we ex- 
pect to be able to improve these results. 
The research reported here is the first that we 
know of to automatically analyze a corpus of logs 
from a spoken dialogue system for the purpose of 
learning to predict problematic situations. Our work 
builds on earlier research on learning to identify di- 
alogues in which the user experienced poor speech 
recognizer performance (Litman et al, 1999). How- 
ever, that work was based on a much smaller set of 
experimental dialogues where the notion of a good or 
bad dialogue was automatically approximated rather 
than being labelled by humans. In addition, because 
that work was based on features ynthesized over the 
entire dialogues, the hypotheses that were learned 
could not be used for prediction during runtime. 
We are exploring several ways to improve the per- 
formance of and test the problematic dialogue pre- 
dictor. First, we noted above the extent to which 
the hand-labelled feature rsuccess improves classifier 
performance. In other work we report results from 
training an rsuccess classifier on a per-utterance level 
(Walker et al, 2000), where we show that we can 
achieve 85% accuracy using only fully automatic fea- 
tures. In future work we intend to use the (noisy) 
output from this classifier as input to our problem- 
atic dialogue classifier with the hope of improving 
the performance of the fully automatic feature sets. 
In addition, since it is more important o minimize 
errors in predicting PROBLEMATIC dialogues than er- 
rors in predicting TASKSUCCESS dialogues, we intend 
to experiment with RIPPER'S loss ratio parameter, 
which instructs RIPPER to achieve high accuracy for 
the PROBLEMATIC class, while potentially reducing 
overall accuracy. Finally, we plan to integrate the 
learned rulesets into the HMIHY dialogue system to 
improve the system's overall performance. 
Re ferences  
A. Abella and A.L. Gorin. 1999. Construct algebra: 
An analytical method for dialog management. In
Proc. of the Association for Computational Lin- 
guistics. 
P. Baggia, G. Castagneri, and M. Danieli. 1998. 
Field trials of the Italian Arise Train Timetable 
System. In Interactive Voice Technology for 
Telecommunications Applications, IVTTA, pages 
97-102. 
J. Chu-Carroll and R. Carpenter. 1999. Vector- 
based natural language call routing. Computa- 
tional Linguistics, 25-3:361-387. 
W. Cohen. 1996. Learning trees and rules with set- 
valued features. In l~th Conference of the Amer- 
ican Association of Artificial Intelligence, AAAI. 
A.L. Gorin E. Ammicht and T. Alonso. 1999. 
Knowledge collection for natural anguage spoken 
dialog systems. In Proc. of EUROSPEECH 99. 
A.L. Gorin, G. Riccardi, and J.H. Wright. 1997. 
How may I Help You? Speech Communication, 
23:113-127. 
I. Langkilde, M. Walker, J. Wright, A. Gorin, and 
D. Litman. 1999. Automatic prediction of prob- 
lematic human-computer dialogues in How May 
I Help You? In Proc. IEEE Workshop on Auto- 
matic Speech Recognition and Understanding. 
G. A. Levow. 1998. Characterizing and recogniz- 
ing spoken corrections in human-computer dia- 
logue. In Proc. of the 36th Annual Meeting of the 
Association of Computational Linguistics, COL- 
ING/ACL 98, pages 736-742. 
D. J. Litman, M. A. Walker, and M. J. Kearns. 1999. 
Automatic detection of poor speech recognition at 
the dialogue level. In Proc. of the 37th Annual 
Meeting of the Association of Computational Lin- 
guistics, ACL99, pages 309-316. 
G. Riccardi and A.L. Gorin. to appear. Spoken lan- 
guage adaptation over time and state in a natu- 
ral spoken dialog system. IEEE Transactions on 
Speech and Audio. 
A. Sanderman, J. Sturm, E. den Os, L. Bores, and 
A. Cremers. 1998. Evaluation of the Dutch Train 
Timetable Information System developed in the 
ARISE project. In Interactive Voice Technology 
for Telecommunications Applications, pages 91- 
96. 
S. Seneff, V. Zue, J. Polifroni, C. Pao, L. Hethering- 
ton, D. Goddeau, and J. Glass. 1995. The pre- 
liminary development of a displayless PEGASUS 
system. In ARPA SLT Workshop. 
E. Shriberg, E. Wade, and P. Price. 1992. Human- 
machine problem solving using spoken language 
systems (SLS): Factors affecting performance and 
user satisfaction. In Proc. of the DARPA Speech 
and NL Workshop, pages 49-54. 
M. A. Walker, J. C. Fromer, and S. Narayanan. 
1998. Learning optimal dialogue strategies: A 
ease study of a spoken dialogue agent for email. 
In Proc. of the 36th Annual Meeting of the 
Association of Computational Linguistics, COL- 
ING/ACL 98, pages 1345-1352. 
M. Walker, I. Langkilde, and J. Wright. 2000. Us- 
ing NLP and Discourse features to identify under- 
standing errors in the How May I Help You spoken 
dialogue system. In Submission. 
217 
Predicting Automatic Speech Recognition Performance Using 
Prosodic Cues 
Diane  J .  L i tman and Ju l ia  B .  H i rschberg  Marc  Swer ts  
AT&T Labs - -  Research IPO,  Center for User-System Interact ion 
F lo rham Park,  NJ 07932-0971 USA Eindhoven, The Nether lands 
{ diane,julia} @research. att.com swerts@ipo.tue.nl 
Abst rac t  
In spoken dialogue systems, it is important for a 
system to know how likely a speech recognition hy- 
pothesis is to be correct, so it can reprompt for 
fresh input, or, in cases where many errors have 
occurred, change its interaction strategy or switch 
the caller to a human attendant. We have discov- 
ered prosodic features which more accurately predict 
when a recognition hypothesis contains a word error 
than the acoustic onfidence score thresholds tradi- 
tionally used in automatic speech recognition. We 
present analytic results indicating that there are sig- 
nificant prosodic differences between correctly and 
incorrectly recognized turns in the TOOT train in- 
formation corpus. We then present machine learn- 
ing results howing how the use of prosodic features 
to automatically predict correct versus incorrectly 
recognized turns improves over the use of acoustic 
confidence scores alone. 
1 I n t roduct ion  
One of the central tasks of the dialogue manager 
in most current spoken dialogue systems (SDSs) is 
error handling. The automatic speech recognition 
(ASR) component of such systems i prone to error, 
especially when the system has to operate in noisy 
conditions or when the domain of the system is large. 
Given that it is impossible to fully prevent ASR er- 
rors, it is important for a system to know how likely 
a speech recognition hypothesis to be correct, so 
it can take appropriate action, since users have con- 
siderable difficulty correcting incorrect information 
that is presented by the system as true (Krahmer 
et al, 1999). Such action may include verifying the 
user's input, reprompting for fresh input, or, in cases 
where many errors have occurred, changing the in- 
teraction strategy or switching the caller to a human 
attendant (Smith, 1998; Litman et al, 1999; Langk- 
ilde et al, 1999). Traditionally, the decision to re- 
ject a recognition hypothesis i based on acoustic 
confidence score thresholds, which provide a relia- 
bility measure on the hypothesis and are set in the 
application (Zeljkovic, 1996). However, this process 
often fails, as there is no simple one-to-one mapping 
between low confidence scores and incorrect recog- 
nitions, and the setting of a rejection threshold is 
a matter of trial and error (Bouwman et al, 1999). 
Also, some incorrect recognitions do not necessarily 
lead to misunderstandings at aconceptual level (e.g. 
"a.m." recognized as "in the morning"). 
The current paper looks at prosody as one possible 
predictor of ASR performance. ASR performance 
is known to vary based upon speaking style (Wein- 
traub et al, 1996), speaker gender and age, na- 
tive versus non-native speaker status, and, in gen- 
eral, the deviation of new speech from the training 
data. Some of this variation is linked to prosody, as 
prosodic differences have been found to character- 
ize differences in speaking style (Blaauw, 1992) and 
idiosyncratic differences (Kraayeveld, 1997). Sev- 
eral other studies (Wade et al, 1992; Oviatt et al, 
1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell 
and Gustafson, 1999) report that hyperarticulated 
speech, characterized by careful enunciation, slowed 
speaking rate, and increase in pitch and loudness, 
often occurs when users in human-machine interac- 
tions try to correct system errors. Still others have 
shown that such speech also decreases recognition 
performance (Soltau and Waibel, 1998). Prosodic 
features have also been shown to be effective in 
ranking recognition hypotheses, asa post-processing 
filter to score ASR hypotheses (Hirschberg, 1991; 
Veilleux, 1994; Hirose, 1997). 
In this paper we present results of empirical stud- 
ies testing the hypothesis that prosodic features pro- 
vide an important clue to ASR performance. We 
first present results comparing prosodic analyses of 
correctly and incorrectly recognized speaker turns 
in TOOT, an experimental SDS for obtaining train 
information over the phone. We then describe ma- 
chine learning experiments based on these results 
that explore the predictive power of prosodic fea- 
tures alone and in combination with other automat- 
ically available information, including ASR confi- 
dence scores and recognized string. Our results in- 
dicate that there are significant prosodic differences 
between correctly and incorrectly recognized utter- 
ances. These differences can in fact be used to pre- 
218 
dict whether an utterance has been misrecognized, 
with a high degree of accuracy. 
2 The  TOOT Corpus  
Our corpus consists of a set of dialogues between 
humans and TOOT, an SDS for accessing train 
schedules from the web via telephone, which was 
collected to study both variations in SDS strat- 
egy and user-adapted interaction (Litman and Pan, 
1999). TOOT is implemented on a platform com- 
bining ASR, text-to-speech, a phone interface, a 
finite-state dialogue manager, and application func- 
tions (Kamm et al, 1997). The speech recognizer is 
a speaker-independent hidden Markov model system 
with context-dependent phone models for telephone 
speech and constrained grammars for each dialogue 
state. Confidence scores for recognition were avail- 
able only at the turn, not the word, level (Zeljkovic, 
1996). An example TOOT dialogue is shown in Fig- 
ure 1. 
Subjects performed four tasks with one of sev- 
eral versions of TOOT, that differed in terms of locus 
of initiative (system, user, or mixed), confirmation 
strategy (explicit, implicit, or none), and whether 
these conditions could be changed by the user during 
the task. Subjects were 39 students, 20 native speak- 
ers of standard American English and 19 non-native 
speakers; 16 subjects were female and 23 male. Dia- 
logues were recorded and system and user behavior 
logged automatically. The concept accuracy (CA) of 
each turn was manually labeled by one of the exper- 
imenters. If the ASR output correctly captured all 
the task-related information in the turn (e.g. time, 
departure and arrival cities), the turn was given a 
CA score of 1 (a semantically correct recognition). 
Otherwise, the CA score reflected the percentage of 
correctly recognized task information in the turn. 
The dialogues were also transcribed by hand and 
these transcriptions automatically compared to the 
ASR recognized string to produce a word error rate 
(WEPt) for each turn. Note that a concept can be 
correctly recognized even though all words are not, 
so the CA metric does not penalize for errors that 
are unimportant to overall utterance interpretation. 
For the study described below, we examined 1994 
user turns from 152 dialogues in this corpus. The 
speech recognizer was able to generate a recognized 
string and an associated acoustic confidence score 
per turn for 1975 of these turns. 1 1410 of these 1975 
turns had a CA score of 1 (for an overall conceptual 
accuracy score of 71%) and 961 had a WER of 0 (for 
an overall transcription accuracy score of 49%, with 
a mean WER per turn of 47%). 
1For the remaining turns, ASR output "no speech" (and 
TOOT played a timeout message) or "garbage" (TOOT played 
a rejection message). 
3 D is t ingu ish ing  Cor rect  f rom 
Incor rect  Recogn i t ions  
We first looked for distinguishing prosodic charac- 
teristics of misrecognitions, defining misrecognitions 
in two ways: a) as turns with WER>0; and b) as 
turns with CA<I.  As noted in Section 1, previous 
studies have speculated that hyperarticulated speech 
(slower and louder speech which contains wider pitch 
excursions) may be associated with recognition fail- 
ure. So, we examined the following features for each 
user turn: 2 
? maximum and mean fundamental frequency 
values (F0 Max, F0 Mean) 
? maximum and mean energy values (RMS Max, 
RMS Mean) 
? total duration 
? length of pause preceding the turn (Prior Pause) 
* speaking rate (Tempo) 
? amount of silence within the turn (% Silence) 
F0 and I:LMS values, representing measures of pitch 
excursion and loudness, were calculated from the 
output of Entropic Research Laboratory's pitch 
tracker, get_fO, with no post-correction. Timing vari- 
ation was represented by four features. Duration 
within and length of pause between turns was com- 
puted from the temporal labels associated with each 
turn's beginning and end. Speaking rate was ap- 
proximated in terms of syllables in the recognized 
string per second, while % Silence was defined as the 
percentage of zero frames in the turn, i.e., roughly 
the percentage of time within the turn that the 
speaker was silent. These features were chosen based 
upon previous findings (see Section 1) and observa- 
tions from our data. 
To ensure that our results were speaker indepen- 
dent, we calculated mean values for each speaker's 
recognized turns and their misrecognized turns for 
every feature. Then, for each feature, we created 
vectors of speaker means for recognized and misrec- 
ognized turns and performed paired t-tests on the 
vectors. For example, for the feature "F0 max", 
we calculated mean maxima for misrecognized turns 
and for correctly recognized turns for each of our 
thirty-nine speakers. We then performed a paired 
t-test on these thirty-nine pairs of means to de- 
rive speaker-independent results for differences in F0 
maxima between correct and incorrect recognitions. 
Tables 1 and 2 show results of these compar- 
isons when we calculate misrecognition i terms of 
2While the features were automatically computed, turn 
beginnings and endings were hand segmented in dialogue-level 
speech files, as the turn-level files created by TOOT were not 
available. 
219 
Toot: 
User: 
Toot: 
User: 
Toot: 
Hi, this is AT&T Amtrak schedule system. This is TOOT. How may I help you? 
I want the trains from New York City to Washington DC on Monday at 9:30 in the evening. 
Do you want me to find the trains from New York City to Washington DC on Monday 
approximately at 9:30 in the evening now? 
Yes. 
I am going to get the train schedule for you ...  
Figure 1: Example Dialogue Excerpt with TOOT. 
Table 1: Comparison of Misrecognized (WER>0) 
vs. Recognized Turns by Prosodic Feature Across 
Speakers. Fe tur0 I st tlMeanMisrecdRocd PII 
*F0 Max 7.83 30.31 Hz 0 
*F0 Mean 3.66 ~I.12 Hz 0 
*RMS Max 5.70 235.93 0 
RMS Mean -.57 -8.50 .57 
*Duration 10.30 2.20 sec 0 
*Prior Pause 5.55 .35 sec 0 
Tempo -.05 .15 sps .13 
*% Silence -5.15 -.06% 0 
*significant at a 95% confidence level 
Table 2: Comparison of Misrecognized (CA<I)  
vs. Recognized Turns by Prosodic Feature Across 
Speakers. Fe turo I st t  ? nMisrecdl rlq ecd 
*F0 Max 5.60 29.64 Hz 0 
F0 Mean 1.70 2.10 Hz .10 
*RMS Max 2.86 173.87 .007 
RMS Mean -1.85 -27.75 .07 
*Duration 9.80 2.15 sec 0 
*Prior Pause 4.05 .38 sec 0 
*Tempo -4.21 -.58 sps 0 
% Silence -1.42 -.02% .16 
*significant at a 95% confidence level (p< .05) 
WER>0 and CA<l ,  respectively. These results in- 
dicate that misrecognized turns do differ from cor- 
rectly recognized ones in terms of prosodic features, 
although the features on which they differ vary 
slightly, depending upon the way "misrecognition" 
is defined. Whether defined by WER or CA, mis- 
recognized turns exhibit significantly higher F0 and 
RMS maxima, longer durations, and longer preced- 
ing pauses than correctly recognized speaker turns. 
For a traditional WER definition of misrecognition, 
misrecognitions are slightly higher in mean F0 and 
contain a lower percentage of internal silence. For a 
CA definition, on the other hand, tempo is a signif- 
icant factor, with misrecognitions spoken at a faster 
rate than correct recognitions - - contrary to our hy- 
pothesis about the role of hyperarticulation in recog- 
nition error. 
While the comparisons in Tables 1 and 2 were 
made on the means of raw values for all prosodic fea- 
tures, little difference is found when values are nor- 
malized by value of first or preceding turn, or by con- 
verting to z scores. 3 From this similarity between the 
performance of raw and normalized values, it would 
seem to be relative differences in speakers' prosodic 
values, not deviation from some 'acceptable' range, 
that distinguishes recognition failures from success- 
ful recognitions. A given speaker's turns that are 
The only differences occur for CA defined misrecognition, 
where normalizing by first utterance results in significant dif- 
ferences in mean RMS, and normalizing by preceding turn 
results in no significant differences in tempo. 
higher in pitch or loudness, or that are longer, or 
that follow longer pauses, are less likely to be recog- 
nized correctly than that same speaker's turns that 
are lower in pitch or loudness, shorter, and follow 
shorter pauses - -  however correct recognition is de- 
fined. 
It is interesting to note that the features we found 
to be significant indicators of failed recognitions (F0 
excursion, loudness, long prior pause, and longer du- 
ration) are all features previously associated with 
hyperarticulated speech. Since prior research has 
suggested that speakers may respond to failed recog- 
nition attempts by hyperarticulating, which itself 
may lead to more recognition failures, had we in fact 
simply identified a means of characterizing and iden- 
tifying hyperarticulated speech prosodically? 
Since we had independently labeled all speaker 
turns for evidence of hyperarticulation (two of the 
authors labeled each turn as "not hyperarticulated", 
"some hyperarticulation in the turn", and "hyperar- 
ticulated", following Wade et al (1992)), we were 
able to test this possibility. We excluded any turn 
either labeler had labeled as partially or fully hy- 
perarticulated, and again performed paired t-tests 
on mean values of misrecognized versus recognized 
turns for each speaker. Results show that for both 
WER-defined and CA-defined misrecognitions, not 
only are the same features ignificant differentiators 
when hyperarticulated turns are excluded from the 
analysis, but in addition, tempo also is significant 
for WER-defined misrecognition. So, our findings 
220 
for the prosodic characteristics of recognized and of 
misrecognized turns hold even when perceptibly hy- 
perarticulated turns are excluded from the corpus. 
4 P red ic t ing  M is recogn i t ions  Us ing  
Mach ine  Learn ing  
Given the prosodic differences between misrecog- 
nized and correctly recognized utterances in our 
corpus, is it possible to predict accurately when a 
particular utterance will be misrecognized or not? 
This section describes experiments using the ma- 
chine learning program RIPPER (Cohen, 1996) to au- 
tomatically induce prediction models, using prosodic 
as well as additional features. Like many learning 
programs, RIPPER takes as input the classes to be 
learned, a set of feature names and possible values, 
and training data specifying the class and feature 
values for each training example. RIPPER outputs 
a classification model for predicting the class of fu- 
ture examples. The model is learned using greedy 
search guided by an information gain metric, and is 
expressed as an ordered set of if-then rules. 
Our predicted classes correspond to correct recog- 
nition (T) or not (F). As in Section 3, we examine 
both WER-defined and CA-defined notions of cor- 
rect recognition, and represent each user turn as a 
set of features. The features used in our learning 
experiments include the raw prosodic features in Ta- 
bles 1 and 2 (which we will refer to as the feature set 
"Prosody"), the hyperarticulation score discussed in 
Section 3, and the following additional potential pre- 
dictors of misrecognition (described in Section 2): 
? ASR grammar 
? ASR confidence 
? ASR string 
? system adaptability 
? dialogue strategy 
? task number 
? subject 
? gender 
? native speaker 
The first three features are derived from the ASR 
process (the context-dependent grammar used to 
recognize the turn, the turn-level acoustic onfidence 
score output by the recognizer, and the recognized 
string). We included these features as a baseline 
against which to test new methods of predicting 
misrecognitions, although, currently, we know of no 
ASR system that includes recognized string in its 
rejection calculations. 4 TOOT itself used only the 
4Note that, while the entire recognized string is provided 
to the learning algorithm, RIPPER rules test for the presence 
of particular words in the string. 
first two features to calculate rejections and ask the 
user to repeat the utterance, whenever the confi- 
dence score fell below a pre-defined grammar-specific 
threshold. The other features represent he exper- 
imental conditions under which the data was col- 
lected (whether users could adapt TOOT's dialogue 
strategies, TOOT's initial initiative and confirmation 
strategies, experimental task, speaker's name and 
characteristics). We included these features to de- 
termine the extent o which particulars of task, sub- 
ject, or interaction influenced ASR success rates or 
our ability to predict them; previous work showed 
that these factors impact TOOT's performance (Lit- 
man and Pan, 1999; Hirschberg et al, 1999). Except 
for the task, subject, gender, native language, and 
hyperarticulation scores, all of our features are au- 
tomatically available. 
Table 3 shows the relative performance of a num- 
ber of the feature sets we examined; results here 
are for misrecognition defined in terms of WER. 5 A 
baseline classifier for misrecognition, predicting that 
ASR is always wrong (the majority class of F), has 
an error of 48.66%. The best performing feature 
set includes only the raw prosodic and ASR features 
and reduces this error to an impressive 6.53% +/ -  
.63%. Note that this performance is not improved 
by adding manually labeled features or experimen- 
tal conditions: the feature set corresponding to ALL 
features yielded the statistically equivalent 6.68% 
+/ -  0.63%. 
With respect o the performance of prosodic fea- 
tures, Table 3 shows that using them in conjunction 
with ASR features (error of 6.53%) significantly out- 
performs prosodic features alone (error of 12.76%), 
which, in turn, significantly outperforms any single 
prosodic feature; duration, with an error of 17.42%, 
is the best such feature. Although not shown in 
the table, the unnormalized prosodic features ig- 
nificantly outperform the normalized versions by 7- 
13%. Recall that prosodic features normalized by 
first task utterance, by previous utterance, or by 
z scores showed little performance difference in the 
analyses performed in Section 3. This difference may 
indicate that there are indeed limits on the ranges 
in features uch as F0 and RMS maxima, duration 
and preceding pause within which recognition per- 
formance is optimal. It seems reasonable that ex- 
treme deviation from characteristics of the acoustic 
training material should in fact impact ASR perfor- 
mance, and our experiments may have uncovered, if
not the critical variants, at least important acoustic 
correlates of them. However, it is difficult to com- 
SThe errors and standard errors (SE) result from 25-fold 
cross-validation the 1975 turns where ASR yielded a string 
and confidence. When two errors plus or minus twice the stan- 
dard error do not overlap, they are statistically significantly 
different. 
221 
Table 3: Estimated Error for Predicting Misrecognized Turns (WER>0). 
Features Used Error \] SE 
Prosody, ASR Confidence, ASR String, ASR Grammar 6.53% .63 
ALL 6.68% .63 
Prosody, ASR String 7.34% .75 
ASR Confidence, ASR String, ASR Grammar 9.01% .70 
Prosody, ASR Confidence, ASR Grammar 10.63% .88 
Prosody, ASR Confidence 10.99% .87 
Prosody 12.76% .79 
ASR String 15.24% 1.11 
Duration 17.42% .88 
ASR Confidence, ASR Grammar 17.77% .72 
ASR Confidence 22.23% 1.16 
ASR Grammar 26.28% .84 
Tempo 32.76% 1.03 
Hyperarticulation 35.24% 1.46 
% Silence 36.46% .79 
Prior Pause 36.61% .97 
F0 Max 38.73% .82 
RMS Max 42.23% .96 
F0 Mean 46.33% 1.10 
RMS Mean 48.35% 1.15 
II Majority Baseline J. 48.66%_%_\[___~ 
pare our machine learning results with the statisti- 
cal analyses, since a) the statistical analyses looked 
at only a single prosodic variable at a time, and b) 
data points for that analysis were means calculated 
per speaker, while the learning algorithm operated 
on all utterances, allowing for unequal contributions 
by speaker. 
We now address the issue of what prosodic fea- 
tures are contributing to misrecognition identifica- 
tion, relative to the more traditional ASR tech- 
niques. Do our prosodic features imply correlate 
with information already in use by ASR systems 
(e.g., confidence score, grammar), or at least avail- 
able to them (e.g., recognized string)? First, the 
error using ASR confidence score alone (22.23%) 
is significantly worse than the error when prosodic 
features are combined with ASR confidence scores 
(10.99%) - -  and is also significantly worse than 
the use of prosodic features alone (12.76%). Simi- 
larly, the error using ASR confidence scores and the 
ASR grammar (17.77%) is significantly worse than 
prosodic features alone (12.76%). Thus, prosodic 
features, either alone or in conjunction with tradi- 
tional ASR features, significantly outperform these 
traditional features alone for predicting WER-based 
misrecognitions. 
Another interesting finding from our experiments 
is the predictive power of information available to 
current ASR systems but not made use of in calcu- 
lating rejection likelihoods, the identity of the recog- 
nized string. This feature is in fact the best perform- 
ing single feature in predicting our data (15.24%). 
And, at a 95% confidence level, the error using 
ASR confidence scores, the recognized string, and 
grammar (9.01%) matches the performance of our 
best performing feature set (6.53%). It seems that, 
at least in our task and for our ASR system, the 
appearance of particular words in the recognized 
strings is an extremely useful cue to recognition ac- 
curacy. So, even by making use of information cur- 
rently available from the traditional ASR process, 
ASR systems could improve their performance on 
identifying rejections by a considerable margin. A 
caveat here is that this feature, like grammar state, 
is unlikely to generalize from task to task or recog- 
nizer to recognizer, but these findings suggest hat 
both should be considered as a means of improving 
rejection performance in stable systems. 
The classification model earned from the best per- 
forming feature set in Table 3 is shown in Figure 2. 6 
The first rule RIPPER finds with this feature set is 
that if the user turn is less than .9 seconds and the 
recognized string contains the word "yes" (and possi- 
bly other words as well), with an acoustic onfidence 
score > -2.6, then predict hat the turn will be cor- 
rectly recognized.7 Note that all of the prosodic fea- 
6Rules are presented in order of importance in classifying 
data. When multiple rules are applicable, RIPPER uses the 
first rule. 
7The confidence scores observed in our data ranged from 
a high of -0.087662 to a low of-9.884418. 
222 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
else F 
< 0.897073) A (confidence > -2.62744 ) A (string contains 'yes') then T 
< 1.03872 ) A (confidence > -2.69775) A (string contains 'no') then T 
< 0.982051) A (confidence > -1.99705) A (tempo > 3.1147) then T 
< 0.813633) A (duration > 0.642652) A (confidence > -3.33945) A (F0 Mean > 176.794) then T 
< 1.30312) A (confidence > -3.37301) A (% silences ~_ 0.647059) then T 
0.610734) A (confidence > -3.37301) A (% silences > 0.521739) then T 
< 1.09537) A (string contains 'Baltimore') then T 
< 0.982051) A (string contains 'no') then T 
< 1.1803) A (confidence > -2.93085) A (grammar ---- date) then T 
< 1.09537) A (confidence > -2.30717) A (% silences > 0.356436) A (F0 Max > 249.225) then T 
< 0.868743) A (confidence > -4.14926 ) A (% silences > 0.51923) A (F0 Max > 205.296) then T 
< 1.18036) A (string contains 'Philadelphia') then T 
Figure 2: Ruleset for Predicting Correctly Recognized Turns (WER = 0) from Prosodic and ASR Features. 
tures except for RMS mean, max, and prior pause 
appear in at least one rule, and that the features 
shown to be significant in our statistical analyses 
(Section 3) are not the same features as in the rules. 
But, as noted above, our data points in these two 
experiments differ. It is useful to note though, that 
while this ruleset contains all three ASR features, 
none of the experimental parameters was found to 
be a useful predictor, suggesting that our results are 
not specific to the particular conditions of and par- 
ticipants in the corpus collection, although they are 
specific to the lexicon and grammars. 
Results of our learning experiments with mis- 
recognition defined in terms of CA rather than WER 
show the overall role of the features which predict 
WER-defined misrecognition to be less successful 
in predicting CA-defined error. Table 4 shows the 
relative performance of the same feature sets dis- 
cussed above, with misrecognition ow defined in 
terms of CA<I.  As with the WER experiments, the 
best performing feature set makes use of prosodic 
and ASR-derived features. However, the predictive 
power of prosodic over ASR features decreases when 
misrecognition is defined in terms of CA - -  which is 
particularly interesting since ASR confidence scores 
are intended to predict WER rather than CA; the er- 
ror rate using ASR confidence scores alone (13.52%) 
is now significantly lower than the error obtained 
using prosody (18.18%). However, prosodic features 
still improve the predictive power of ASR confidence 
scores, to 11.34%, although this difference is not sig- 
nificant at a 95% confidence level. And the error 
rate of the three ASR features combined (11.70%) is 
reduced to the lowest error rate in our table when 
prosodic features are added (10.43%); this error rate 
is (just) significantly different from the use of ASR 
confidence scores alone. Thus, for CA-defined mis- 
recognitions, our experiments have uncovered only 
minor improvements over traditional ASR rejection 
calculation procedures. 
5 D iscuss ion  
A statistical comparison of recognized versus mis- 
recognized utterances indicates that F0 excursion, 
loudness, longer prior pause, and longer duration 
are significant prosodic haracteristics of both WER 
and CA-defined failed recognition attempts. Results 
from a set of machine learning experiments show 
that prosodic differences can in fact be used to im- 
prove the prediction of misrecognitions with a high 
degree of accuracy (12.76% error) for WER-based 
misrecognit ions-  and an even higher degree (6.53% 
error) when combined with information currently 
available from ASR systems. The use of ASR confi- 
dence scores alone had a predicted WER of 22.23%, 
so the improvement over traditional methods is quite 
considerable. For CA-defined misrecognitions, the 
improvement provided by prosodic features is con- 
siderably less. One of our future research directions 
will be to understand this difference. 
Another future direction will be to address the is- 
sue of just why  prosodic features provide such use- 
ful indicators of recognition failure. Do the features 
themselves make recognition difficult, or are they 
instead indirect correlates of other phenomena not 
captured in our study? While the negative influence 
of speaking rate variation on ASR has been reported 
before (e.g. (Ostendorf et al, 1996), it is tradition- 
ally assumed that ASR is impervious to differences 
in F0 and RMS; yet, it is known that F0 and RMS 
variations co-vary to some extent with spectral char- 
acteristics (e.g. (Swerts and Veldhuis, 1997; Fant et 
al., 1995)), so that it is not unlikely that utterances 
with extreme values for these may differ critically 
from the training data. Other prosodic features may 
be more indirect indicators of errors. Longer ut- 
terances may simply provide more chance for error 
than shorter ones, while speakers who pause longer 
before utterances and take more time making them 
may also produce more disfluencies than others. 
We are currently replicating our experiment on a 
new domain with a new speech recognizer. We are 
examining the W99 corpus, which was collected in a 
223 
Table 4: Estimated Error for Predicting Misrecognized Turns (CA<l). 
Features Used \[ Error 
Prosody, ASR Confidence~ ASR String, ASR Grammar 10.43% .63 
ALL 10.68% .71 
Prosody, ASR Confidence, ASR Grammar 11.24% .68 
Prosody, ASR Confidence 11.34% .64 
ASR Confidence, ASR String, ASR Grammar 11.70% .68 
ASR Confidence 13.52% .82 
ASR Confidence, ASR Grammar 13.52% .84 
ASR String 13.62% .83 
Prosody, ASR String 15.04% .84 
Prosody 18.18% .85 
Duration 18.38% .90 
ASR Grammar 22.73% .96 
Tempo 24.61% 1.28 
Hyperarticulation 25.27% 1.05 
F0 Mean 28.61% 1.19 
F0 Max 28.76% .90 
RMS Mean 28.86% 1.17 
% Silence 28.91% 1.23 
RMS Max 29.01% 1.16 
Prior Pause 29.22% 1.26 
Majority Baseline \[ 28.61% 
spoken dialogue system that supported registration, 
checking paper status, and information access for the 
IEEE Automatic Speech Recognition and Under- 
standing Workshop (ASRU99) (Rahim et al, 1999). 
This system employed the AT&T WATSON speech 
recognition technology (Sharp et al, 1997). Prelim- 
inary results indicate that our TOOT results do in 
fact hold up across recognizers. We also are extend- 
ing our TOOT corpus analysis to include prosodic 
analyses of turns in which users become aware of 
misrecognitions and correct them. In addition, we 
are exploring whether prosodic differences can help 
explain the "goat" phenomenon - -  the fact that 
some voices are recognized much more poorly than 
others (Doddington et al, 1998; Hirschberg et al, 
1999). Our ultimate goal is to provide prosodically- 
based mechanisms for identifying and reacting to 
ASR failures in SDS systems. 
Acknowledgements  
We would like to thank Jennifer Chu-Carroll, Candy 
Kamm, participants in the AT&T "SLUG" seminar 
series, and participants in the 1999 JHU Summer 
Language Engineering Workshop, for providing us 
with useful comments on this research and on earlier 
versions of this paper. 
References 
Linda Bell and Joakim Gustafson. 1999. Repe- 
tition and its phonetic realizations: Investigat- 
ing a Swedish database of spontaneous computer- 
directed speech. In Proceedings of ICPhS-99, San 
Francisco. International Congress of Phonetic Sci- 
ences. 
E. Blaauw. 1992. Phonetic differences between read 
and spontaneous speech. In Proceedings of IC- 
SLP92, volume 1, pages 751-758, Banff. 
A. G. Bouwman, J. Sturm, and L. Boves. 1999. 
Incorporating confidence measures in the dutch 
train timetable information system developed in 
the ARISE project. In Proc. International Con- 
ference on Acoustics, Speech and Signal Process- 
ing, volume 1, pages 493-496, Phoenix. 
William Cohen. 1996. Learning trees and rules with 
set-valued features. In l$th Conference of the 
American Association of Artificial Intelligence, 
AAAI. 
George Doddington, Walter Liggett, Alvin Martin, 
Mark Przybocki, and Douglas Reynolds. 1998. 
Sheep, goats, lambs and wolves: A statistical anal- 
ysis of speaker performance in the NIST 1998 
speaker ecognition evaluation. In Proceedings of 
ICSLP-98. 
G. Fant, J. Liljencrants, I. Karlsson, and 
M. B?veg?rd. 1995. Time and frequency do- 
main aspects of voice source modelling. BR 
Speechmaps 6975, ESPRIT. Deliverable 27 WP 
1.3. 
Keikichi Hirose. 1997. Disambiguating recogni- 
tion results by prosodic features. In Computing 
224 
Prosody: Computational Models for Processing 
Spontaneous Speech, pages 327-342. Springer. 
Julia Hirschberg, Diane Litman, and Marc Swerts. 
1999. Prosodic cues to recognition errors. In Pro- 
ceedings of the Automatic Speech Recognition and 
Understandin9 Workshop (ASRU'99). 
Julia Hirschberg. 1991. Using text analysis to pre- 
dict intonational boundaries. In Proceedings of the 
Second European Conference on Speech Commu- 
nication and Technology, Genova. ESCA. 
C. Kamm, S. Narayanan, D. Dutton, and R. Rite- 
nour. 1997. Evaluating spoken dialog systems 
for telecommunication services. In 5th European 
Conference on Speech Technology and Communi- 
cation, EUROSPEECH 97. 
Hans Kraayeveld. 1997. Idiosyncrasy in prosody. 
Speaker and speaker group identification i  Dutch 
using melodic and temporal information. Ph.D. 
thesis, Nijmegen University. 
E. Krahmer, M. Swerts, M. Theune, and 
M. Weegels. 1999. Error spotting in human- 
machine interactions. In Proceedings of 
E UR OSPEECH- 99. 
Irene Langkilde, Marilyn Walker, Jerry Wright, 
A1 Gorin, and Diane Litman. 1999. Automatic 
prediction of problematic human-computer dia- 
logues in 'how may i help you?'. In Proceedings 
of the Automatic Speech Recognition and Under- 
standin 9 Workshop (ASRU'99). 
Gina-Anne Levow. 1998. Characterizing and recog- 
nizing spoken corrections in human-computer dia- 
logue. In Proceedings of the 36th Annual Meeting 
of the Association of Computational Linguistics, 
COLING/ACL 98, pages 736-742. 
Diane J. Litman and Shimei Pan. 1999. Empirically 
evaluating an adaptable spoken dialogue system. 
In Proceedings of the 7th International Conference 
on User Modeling (UM). 
Diane J. Litman, Marilyn A. Walker, and Michael J. 
Kearns. 1999. Automatic detection of poor 
speech recognition at the dialogue level. In Pro- 
ceedings of the 37th Annual Meeting of the As- 
sociation of Computational Linguistics , ACL99, 
pages 309-316. 
M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke, 
A. Gunawardana, K. Ross, S. Roweis, E. Shriberg, 
D. Talkin, A. Waibel, B. Wheatley, and T. Zep- 
penfeld. 1996. Modeling systematic variations 
in pronunciation via a language-dependent hid- 
den speaking mode. Report on 1996 CLSP/JHU 
Workshop on Innovative Techniques for Large Vo- 
cabulary Continuous Speech Recognition. 
S. L. Oviatt, G. Levow, M. MacEarchern, and 
K. Kuhn. 1996. Modeling hyperarticulate speech 
during human-computer error resolution. In Pro- 
ceedings of ICSLP-96, pages 801-804, Philadel- 
phia. 
M. Rahim, R. Pieracini, W. Eckert, E. Levin, G. Di 
Fabbrizio, G. Riccardi, C. Lin, and C. Kamm. 
1999. W99 - a spoken dialogue system for the 
asru'99 workshop. In Proc. ASRU'99. 
R.D. Sharp, E. Bocchieri, C. Castillo, 
S. Parthasarathy, C. Rath, M. Riley, and 
J Rowland. 1997. The watson speech recognition 
engine. In Proc. ICASSP97, pages 4065-4068. 
Ronnie W. Smith. 1998. An evaluation of strate- 
gies for selectively verifying utterance meanings 
in spoken natural language dialog. International 
Journal of Human- Computer Studies, 48:627-647. 
Hagen Soltau and Alex Waibel. 1998. On the in- 
fluence of hyperarticulated speech on recognition 
performance. In Proceedings of ICSLP-98, Syd- 
ney. International Conference on Spoken Lan- 
guage Processing. 
M. Swerts and M. Ostendorf. 1997. Prosodic 
and lexical indications of discourse structure in 
human-machine interactions. Speech Communica- 
tion, 22:25-41. 
Marc Swerts and Raymond Veldhuis. 1997. Interac- 
tions between intonation and glottal-pulse char- 
acteristics. In A. Botinis, G. Kouroupetroglou, 
and G. Carayiannis, editors, Intonation: Theory, 
Models and Applications, pages 297-300, Athens. 
ESCA. 
Nanette Veilleux. 1994. Computational Models of 
the Prosody/Syntax Mapping for Spoken Language 
Systems. Ph.D. thesis, Boston University. 
E. Wade, E. E. Shriberg, and P. J. Price. 1992. User 
behaviors affecting speech recognition. In Pro- 
ceedings of ICSLP-92, volume 2, pages 995-998, 
Banff. 
M. Weintraub, K. Taussig, K. Hunicke-Smith, and 
A. Snodgrass. 1996. Effect of speaking style on 
LVCSR performance. In Proceedings of ICSLP- 
96, Philadelphia. International Conference on 
Spoken Language Processing. 
Ilija Zeljkovic. 1996. Decoding optimal state se- 
quences with smooth state likelihoods. In Interna- 
tional Conference on Acoustics, Speech, and Sig- 
nal Processing, ICASSP 96, pages 129-132. 
225 
Automatic Optimization of Dialogue Management 
Diane J. Litman, Michael S. Kearns, Satinder Singh, Mari lyn A. Walker  
AT&T Labs -  Research  
180 Park  Avenue 
F lo rham Park ,  N J  07932 USA 
{d iane ,mkearns ,bave j  a,walker} @research.ar t .corn  
Abstract 
Designing the dialogue strategy of a spoken dialogue 
system involves many nontrivial choices. This pa- 
per I)resents a reinforcement learning approach for 
automatically optimizing a dialogue strategy that 
addresses the technical challenges in applying re- 
inforcement learning to a working dialogue system 
with hulnan users. \?e then show that our approach 
measurably improves performance in an experimen- 
tal system. 
1 I n t roduct ion  
Recent advances in spoken language understanding 
have made it 1)ossible to develop dialogue systems 
tbr many applications. The role of the dialogue man- 
ager in such systems is to interact in a naturM w~y 
to hel 1 ) the user complete the tasks that the system 
is designed to support. Tyl)ically, an expert designs 
a dialogue manager by hand, and has to make m~ny 
nontrivial design choices that can seriously impact 
system performance. This paper applies reintbrce- 
ment learning (RL) to automatically learn design 
choices that optimize system pertbrnmnee for a cho- 
seLL pertbrmance measure (Levin et al, 2000; Walker 
et al, 1998). 
Consider the spoken dialogue system named N J- 
Fun, wlfich we built to help users find fun places 
to go in New Jersey. A sample dialogue with N J- 
Fun is shown in Figure 1, with system utterances 
labeled Si and user utterances labeled Ui. When 
NJFun greets the user, it can provide an open greet- 
ing such as utterance S1, expecting that the user 
will take the initiative. Alternatively, it can pro- 
vide a directive greeting in which the system takes 
the initiative, as in "Welcome to NJFun. Please say 
an activity name or say 'list activities' for a list of 
activities I know about." After tile user's response 
U1, NJFun must decide whether it should explicitly 
confirm its understanding, as in utterances $2 and 
$3. NJFun can also simply continue on with the di- 
alogue, as when it does not explicitly confirm that 
the user wants to find out M)out wineries. In NJFun, 
as shown in more detail below, decisions about ini- 
tiative and confirmation strategies alone result in a 
SI: Welcome to N.llSm. How may I help you? 
UI: I'd like to find mn winetasting in Lambertville 
in the morning. (ASR output: I'd like to find 
out wirterics the in the Lambertvillc in the 
mornin.q. ) 
$2: Did you say you are interested in Lambertville? 
U2: Yes. 
$3: Did you say you want to go in the morning? 
U3: Yes. 
$4: I found a winery near l~ambertville that is 
open in the morning, it is \[...\] Please give 
lne feedback by saying 'good', ~so-so', or 'bad'. 
U4: Good. 
Figure 1: An example dialogue with NJFun. 
search space of 242 potential globnl dialogue strate- 
gies. Furthermore, the performance of a dialogue 
strategy depends on many other factors, such as 
the user population, the robustness of the automatic 
speech recognizer (ASR), and task difficulty (Kamm 
et al, 1998; DanMi and Gerbino, 1995). 
In the main, previous research has treated the 
specification of the dialogue management s rategy 
as an iterative design problem: several versions of a 
system are created, dialogue corpora are collected 
with human users interacting with different ver- 
sions of tile system, a number of evaluation met- 
rics are collected ibr each dialogue, and the differ- 
ent versions are statistically compared (Danieli and 
Gerbino, 1995; Sanderman et al, 1998). Due to the 
costs of experimentation, only a few global strategies 
are typically explored in any one experiment. 
However, recent work has suggested that dialogue 
strategy can be designed using tile formalism of 
Markov decision processes (MDPs) and the algo- 
rithms of RL (Biermann and Long, 1996; Levin et 
al., 2000; Walker et nl., 1998; Singh et al, 1999). 
More specifically, the MDP formalism suggests a 
method for optimizing dialogue strategies from sam- 
ple dialogue data. The main advantage of this ap- 
proach is the 1)otential tbr computing an optilnal di- 
alogue strategy within a much larger search space, 
using a relatively small nmnber of training dialogues. 
This paper presents an application of RL to the 
502 
problem of oi)timizing dialogue strategy selection in 
the NJFnn system, and exl)erimentally demonstrates 
the utility of the ~l)proach. Section 2 exl)lahls how 
we apply RL to dialogue systems, then Se('tion 3 
describes t.he NJFun system in detail. Section 4 dee 
scribes how NJFun optimizes its dialogue strategy 
from experimentally obtained dialogue data. Sec- 
tion 5 reports results from testing the learned strat- 
egy demonstrating that our al)l)roach improves task 
coml)letion rates (our chosen measure for 1)erfor- 
mance optimization). A conll)alliOll paper provides 
only an al)brevi~tted system and dialogue manager 
description, but includes additional results not pre- 
sented here (Singh et al, 2000), such as analysis es- 
tablishing the veracity of the MDP we learn, and 
comparisons of our learned strategy to strategies 
hand-picked by dialogue xperts. 
2 Reinforcement Learning for 
Dialogue 
Due to space limitations, we 1)resent only a 1)rief 
overview of how di~dogue strategy optimization can 
be viewed as an llL 1)roblem; for more details, 
see Singh ctal .  (\]999), Walker el; a.1. (\]998), Levin 
et al (2000). A dialogue strategy is a mapl)ing h'om 
a set ot! states (which summarize the entire dialogue 
so far) to a set of actions (such as the system's utter- 
mines and database queries). There are nmltil)l(~ rea- 
sonable action choices in each state; tyl)ically these 
choices are made by the system designer. Our RL- 
I)ased at)l)roach is to build a system that explores 
these choices in a systematic way through experi- 
ments with rel)resentative human us(!rs. A scalar 
i)erf()rmanee l l leasllre, called a rewal'd, is t h(m (;al- 
eulated for each Cxl)erimental diMogue. (We dis- 
cuss various choices for this reward measure later, 
but in our experiments only terminal dialogue states 
ha,re nonzero  rewi-l,rds, s l id  the reward lneasul'(}s arc  
quantities directly obtMnable from the experimental 
set-up, such as user satisfaction or task coml)letion. ) 
This experimental data is used to construct an MDP 
which models the users' intera(:tion with the system. 
The l)roblem of learning the best dialogue strategy 
from data is thus reduced to computing the optimal 
policy tbr choosing actions in an MDP - that is, the 
system's goal is to take actions so as to maximize 
expected reward. The comput~ttion of the ol)timal 
policy given the MDP can be done etficiently using 
stan&trd RL algorithms. 
How do we build the desired MDP from sample 
dialogues? Following Singh et al (1999), we can 
view a dialogue as a trajectory in the chosen state 
space determined by the system actions and user 
resl) onses: 
S1 -~a l , r l  '5'2 --}a~,rs 83 "-~aa,ra " '"  
Here si -%,,,.~ si+l indicates that at the ith ex- 
change, the system was in state si, executed action 
ai, received reward ri, and then the state changed 
to si+~. Dialogue sequences obtained froln training 
data can be used to empirically estimate the transi- 
tion probabilities P(.s"la', a) (denoting the probabil- 
ity of a mmsition to state s', given that the system 
was in state .s and took ;ration a), and the reward 
function R(.s, (t). The estilnated transition 1)tel)abil- 
ities and rewi~rd flmction constitute an MDP model 
of the nser population's interaction with the system. 
Given this MDP, the exl)ected cumnlative reward 
(or Q-value) Q(s, a) of taking action a from state s 
can be calculated in terms of the Q-wdues of succes- 
sor states via the following recursive quation: 
Q(.% = ,) + r(,,'l,% ,)n,a:,: Q(,,", , ' ) .  
t ;  ! 
These Q-values can be estimated to within a desired 
threshold using the standard RL value iteration al- 
gorithm (Sutton, 1.991.), which iteratively updates 
the estimate of Q(s, a) based on the current Q-vahms 
of neighboring states. Once value iteration is con> 
pleted, the optima\] diah)gue strategy (according to 
our estimated model) is obtained by selecting the 
action with the maximum Q-value at each dia.logue 
state. 
While this apl)roach is theoretically appealing, the 
cost of obtaining sample human dialogues makes it 
crucial to limit the size of the state space, to mini- 
mize data sparsity problems, while retaining enough 
information in the state to learn an accurate model. 
Our approad~ is to work directly in a minimal but 
carefully designed stat;e space (Singh et al, 1999). 
The contribution of this paper is to eml)irically 
vMi(tate a practical methodology for using IlL to 
build a dialogue system that ol)timizes its behav- 
ior from dialogue data. Our methodology involves 
1) representing a dialogue strategy as a mapl)il~g 
fronl each state in the chosen state space S to a 
set of dialogue actions, 2) deploying an initial trah> 
ing system that generates exploratory training data 
with respect o S, 3) eonstrncting an MDP model 
from the obtained training data, 4) using value iter- 
ation to learn the optimal dialogue strategy in the 
learned MDP, and 4) redeploying the system using 
the learned state/~mtion real)ping. The next section 
details the use of this methodology to design the 
NJFun system. 
3 The N JFun  System 
NJFnn is a real-time spoken dialogue system that 
provides users with intbrmation about things to do 
in New Jersey. NJFun is built using a general pur- 
pose 1)latt'ornl tbr spoken dialogue systems (Levin 
et al, 1.999), with support tbr modules tbr attto- 
rustic speech recognition (ASI/.), spoken language 
503 
Action Prompt 
Greets Welcome to NJIqm. Please say an activity name or say 'list activities' for a list of activities I know 
about. 
GreetU \Velcome to NdPun. How may I help you? 
ReAsklS Iknow about amusement parks, aquariums, cruises, historic sites, museums, parks, theaters, 
wineries, and zoos. Please say an activity name from this list. 
ReAsklM Please tell me the activity type.You can also tell me the location and time. 
Ask2S Please say the name of the town or city that you are interested in. 
Ask2U Please give me more information. 
ReAsk2S Please teli me the name of the town or city that you are interested in. 
ReAsk2M Please tell me the location that you are interested in. You call also tell me the time. 
Figure 2: Sample initiative strategy choices. 
understanding, text-to-speech (TTS), database ac- 
cess, and dialogue management. NJFnn uses a 
speech recognizer with stochastic language and un- 
derstanding models trained from example user ut- 
terances, and a TTS system based on concatena- 
tive diphone synthesis. Its database is populated 
from the nj .  on l ine  webpage to contain information 
about activities. NJFun indexes this database using 
three attributes: activity type, location, and time of 
day (which can assume values morning, afternoon, 
or evening). 
hffornmlly, the NJFun dialogue manager sequen- 
tially queries the user regarding the activity, loca- 
tion and time attributes, respectively. NJFun first 
asks the user for the current attribute (and 1)ossibly 
the other attributes, depending on the initiative). 
If the current attribute's value is not obtained, N.J- 
Fun asks for the attrilmte (and possibly the later 
attributes) again. If NJFun still does not obtain 
a value, NJFun moves on to the next attribute(s). 
Whenever NJFun successihlly obtains a value, it 
can confirm the vMue, or move on to the next at- 
tribute(s). When NJFun has finished acquiring at- 
tributes, it queries the database (using a wildcard 
for each unobtained attribute value). The length of 
NJFun dialogues ranges from 1 to 12 user utterances 
before the database query. Although the NJFun di- 
alogues are fairly short (since NJFun only asks for 
an attribute twice), the information access part of 
the dialogue is similar to more complex tasks. 
As discussed above, our methodology for using RL 
to optimize dialogue strategy requires that all poten- 
tim actions tbr each state be specified. Note that at 
some states it is easy for a human to make the cor- 
rect action choice. We made obvious dialogue strat- 
egy choices in advance, and used learIfing only to 
optimize the difficult choices (Walker et al, 1998). 
Ill NJFun, we restricted the action choices to 1) the 
type of initiative to use when asking or reasking for 
an attribute, and 2) whether to confirm an attribute 
value once obtained. The optimal actions may vary 
with dialogue state, and are subject o active debate 
in the literature. 
Tile examples in Figure 2 show that NJFun can 
ask the user about the first 2 attributes I using three 
types of initiative, based on the combination of tile 
wording of the system prompt (open versus direc- 
tive), and the type of grammar NJFun uses during 
ASR (restrictivc versus non-restrictive). If NJFun 
uses an open question with m~ unrestricted gram- 
mar, it is using v.scr initiative (e.g., Greet\[l). If N J- 
Fun instead uses a directive prompt with a restricted 
grammar, the system is using systcm initiative (e.g., 
GreetS). If NJFun uses a directive question with a 
non-restrictive granmlar, it is using mizcd initiative, 
because it allows the user to take the initiative by 
supplying extra intbrnlation (e.g., ReAsklM). 
NJFun can also vary the strategy used to confirm 
each attribute. If NJFun asks the user to explicitly 
verify an attribute, it is using czplicit confirmation 
(e.g., ExpConf2 for the location, exemplified by $2 
in Figure 1). If NJFun does not generate any COlt- 
firnmtion prompt, it is using no confirmation (the 
NoConf action). 
Solely tbr the purposes of controlling its operation 
(as opposed to the le~trning, which we discuss in a 
moment), NJNm internally maintains an opcratio'ns 
vector of 14 variables. 2 variables track whether the 
system has greeted the user, and which attribute 
the system is currently attempting to obtain. For 
each of the 3 attributes, 4 variables track whether 
the system has obtained the attribute's value, the 
systent's confidence in the value (if obtained), the 
number of times the system has asked the user about 
the attribute, and the type of ASR grammar most 
recently used to ask for the attribute. 
The formal state space S maintained by NJFun 
for tile purposes of learning is nmch silnt)ler than 
the operations vector, due to the data sparsity con- 
cerns already discussed. The dialogue state space 
$ contains only 7 variables, as summarized in Fig- 
sire 3. S is computed from the operations vector us- 
ing a hand-designed algorithm. The "greet" variable 
1 "Greet" is equivalent toasking for the first attribute. N J- 
Fun always uses system initiative for the third attribute, be- 
cause at that point the user can only provide the time of (lay. 
504 
greet attr conf val times gram hist \] 
0,1 1,2,3,4 0,1,2,3,4 0,1 0,1,2 0,1 0,1 \] 
Figure 3: State features and vahles. 
tracks whether tile system has greeted tile user or  
not (no=0, yes=l). "Attr ~: specifies which attrihute 
NJFun is ('urrently ~tttelnpting to obtain or ver- 
ify (activity=l, location=2, time=a, done with at- 
tributes=4). "Conf" tel)resents the confidence that 
NaFun has after obtaining a wdue for an attribute. 
The values 0, 1, and 2 represent the lowest, middle 
and highest ASR confidence vahms? The wdues 3 
and 4 are set when ASR hears "yes" or "no" after a 
confirmation question. "Val" tracks whether NaFun 
has obtained a value, for tile attribute (no=0, yes=l). 
"Times" tracks the number of times that N,lFun has 
aske(1 the user ~d)out he attribute. "(4ram" tracks 
the type of grammar most ree(mtly used to obtain 
the attribute (0=non-restrictive, 1=restrictive). Fi- 
nally, "hist" (history) represents whether Nalflm had 
troullle understanding the user ill the earlier p~trt of 
the conversation (bad=0, good=l).  We omit the full 
detinition, but as a,n ex~unl>le , when N.lFun is work- 
ing on the secon(1 attribute (location), the history 
variable is set to 0 if NJFun does not have an ac- 
tivity, has an activity but has no confidence in the 
value, or needed two queries to obtain the activity. 
As mentioned above, the goal is to design a small 
state space that makes enough critical distin('tions to 
suPi)ort learning. The use of 6" redu(:es the mmfl)er 
of states to only 62, and SUl)l)orts the constru('tion of
mt MI)P model that is not sparse with respect o ,g, 
even using limite(1 trMning (btta. :~ Tit(.' state sp~t(;e 
that we utilize here, although minimal, allows us 
to make initiative decisions based on the success of 
earlier ex(:ha.nges, and confirmation de(:isions based 
on ASR. confidence scores and gralnmars. 
'.Phe state/~t('tiol: real)ping r(-`l)resenting NaFun's 
initial dialogue strategy EIC (Explor:ttory for Ini- 
tiative and Confirmation) is in Figure 4. Only the 
exploratory portion of the strategy is shown, namely 
those states for which NaFun has an action choice. 
~klr each such state, we list tile two choices of actions 
available. (The action choices in boldfime are the 
ones eventually identified as el)ritual 1)y the learning 
process, an(1 are discussed in detail later.) The EIC 
strategy chooses random, ly between these two ac- 
21"or each uttermme, the ASH. outfmt includes 11o|, only the 
recognized string, but also aIl asso(:ia.ted acoustic (:onJld(mce 
score, iBased on data obtaintM dm'ing system deveJolmmnt , 
we defined a mapl)ing from raw confidence, values into 3 ap- 
proximately equally potmlated p~rtitions. 
362 refers to those states that can actually occur in a di- 
alogue. \[<)r example, greet=0 is only possible in the initial 
dialogue state "0 1 0 0 0 0 0". Thus, all other states beginning 
with 0 (e.g. "0 I 0 0 I 0 0") will never occur. 
g 
1 0 0 (} 0 0 
1 1 0 (} 1 0 0 
1 1 0 1 0 0 0 
1 1 0 1 0 1 0 
1 1 1 1 0 0 0 
1 1 1 1 0 \] 0 
1 1 2 1 0 0 0 
1 1 2 1 0 1 0 
1 1 4 0 0 0 0 
\] 1 4 (} 1 0 0 
St~tte Action Choices 
a c v t g it 
1 2 0 0 0 0 0 
1 2 0 0 0 0 1 
1 2 (} 0 1 (} 0 
1 2 0 (} 1 0 1 
1 2 0 1 0 (} 0 
1 2 0 \] 0 0 1 
1 2 0 1 0 1 0 
1 2 0 1 0 1 1 
1 2 1 1 0 (1 0 
1 2 1 \] 0 0 1 
1 2 1 1 0 1 0 
\] 2 1 1 0 \] 1 
1 2 2 1 0 0 0 
1 2 2 \] 0 0 1 
1 2 2 \] 0 1 0 
\] 2 2 1 0 1 1 
1 2 4 0 0 (1 0 
1 2 4 0 0 0 1 
1 2 4 0 1 0 0 
1 2 4 0 1 0 1 
1 3 () 1 () () 0 
\] 3 0 1 0 0 1 
1 3 (1 1 0 1 0 
1 3 \[) 1 0 \] \] 
1. 3 1 1 0 0 0 
1 3 1 1 0 0 1 
1 3 1 1. 0 1 0 
1 3 1 1 0 \] 1 
1 3 2 1 0 0 0 
1 3 2 1 0 0 1 
1 3 2 1 0 \] 0 
\] 3 2 1 0 1 1 
GreetS,GreetU 
ReAsklS,ReAsklM 
NoConf, ExpConf l  
NoConf, ExpColffl 
NoCont, Exp Confl  
NoConf, ExpConf l  
NoConf~ExpConfl 
NoConf, ExpConfl 
ReAsklS,ReAsklM 
ReAsklS,RcAsklM 
Ask2S,Ask2U 
Ask2S,Ask2U 
I{eAsk2S,ReAsk2M 
ReAsk2S,ReAsk2M 
NoConf, ExpCont'2 
NoConf, ExpConP2 
NoConf~ExpCont)2 
NoConf,ExpConf2 
NoCoiff, Exp C oaF2 
NoConf, ExpConf2 
NoConf, ExpCont2 
NoConf,ExpCong2 
NoConf~ExpConf2 
NoConf, ExpConf2 
NoConf, ExpConf2 
NoConf, ExpCon\[2 
ReAsk2S,RcAsk2M 
I/.eAsk2S,ReAsk2M 
RcAsk2S,ReAsk2M 
ReAsk2S,ReAsk2M 
NoConf, ExpCon\[3 
NoConf, Exl) Conf3 
NoConf, ExpConf3 
NoConf,Ext)ConF3 
NoConf, ExpCont~ 
NoConf, ExpConf3 
NoConf, ExpConf3 
NoConf,ExpConF3 
NoConf, ExpConi~J 
NoConf, ExpConf3 
NoColff, ExpConf3 
NoConf, ExpConf3 
Figure 4: Exploratory portion of EIC strategy. 
tions in the indicated state, to maximize xploration 
and minimize data sparseness when constructing our 
model. Since there are 42 states with 2 choices each, 
there is n search space of 242 potential global di- 
alogue strategies; the goal of RL is to identify an 
apparently optimal strategy fl'om this large search 
space. Note that due to the randomization of the 
EIC strategy, the prompts are designed to ensure 
the coherence of all possible action sequences. 
Figure 5 illustrates how the dialogue strategy in 
Figure 4 generates the diMogue in Figure 1. Each 
row indicates the state that NJFun is in, the ac- 
505 
State Action %Irn Reward 
gacvtgh  
0100000 GreetU $1 0 
1121000 NoConf 0 
1221001 ExpConf2 $2 0 
1 3 2 1 0 0 1 ExpConf3 $3 0 
1400000 Tell $4 1 
Figure 5: Generating the dialogue in Figure 1. 
tion executed in this state, the corresponding turn 
in Figure 1, and the reward received. The initial 
state represents that NaFun will first attempt o ob- 
tain attribute 1. NJFun executes GreetU (although 
as shown in Figure 4, GreetS is also possible), gen- 
erating the first utterance in Figure 1. Alter the 
user's response, the next state represents that N J- 
Fun has now greeted the user and obtained the ac- 
tivity value with high confidence, by using a non- 
restrictive grmnmar. NJFnn then chooses the No- 
Conf strategy, so it does not attempt to confirm 
the activity, which causes the state to change but 
no prompt to be generated. The third state repre- 
sents that NJFun is now working on the second at- 
tribute (location), that it already has this vahle with 
high confidence (location was obtained with activity 
after the user's first utterance), and that the dia- 
logue history is good. 4 This time NaFun chooses the 
ExpConf2 strategy, and confirms the attribute with 
the second NJFun utterance, and the state changes 
again. The processing of time is similar to that of lo- 
cation, which leads NJFun to the final state, where it 
performs the action "Tell" (corresponding to query- 
ing the database, presenting the results to the user, 
and asking the user to provide a reward). Note that 
in NJFun, the reward is always 0 except at the ter- 
minal state, as shown in the last column of Figure 5. 
4 Experimental ly Optimizing a 
Strategy 
We collected experimental dialogues for both train- 
ing and testing our system. To obtain training di- 
alogues, we implemented NJFun using the EIC dia- 
logue strategy described in Section 3. We used these 
dialogues to build an empirical MDP, and then com- 
puted the optimal dialogue strategy in this MDP (as 
described in Section 2). In this section we describe 
our experimental design and the learned dialogue 
strategy. In the next section we present results from 
testing our learned strategy and show that it im- 
proves task completion rates, the performance mea- 
sure we chose to optimize. 
Experimental subjects were employees not associ- 
a, ted with the NJFun project. There were 54 sub- 
4Recall that only the current attribute's features are ill the 
state, lIowever, the operations vector contains information 
regarding previous attributes. 
jects for training and 21 for testing. Subjects were 
distributed so tile training and testing pools were 
balanced for gender, English as a first language, and 
expertise with spoken dialogue systems. 
During both training and testing, subjects carried 
out free-form conversations with NJFun to complete 
six application tasks. For examl)le , the task exe- 
cuted by the user in Figure 1 was: "You feel thirsty 
and want to do some winetasting in the morning. 
Are there any wineries (;lose by your house in Lam- 
bertville?" Subjects read task descriptions on a web 
page, then called NJFun from their office phone. 
At the end of the task, NJFun asked for feedback 
on their experience (e.g., utterance $4 in Figure 1). 
Users then hung up the phone and filled out a user 
survey (Singh et al, 2000) on the web. 
The training phase of the experiment resulted in 
311 complete dialogues (not all subjects completed 
all tasks), for which NJFun logged the sequence 
of states and the corresponding executed actions. 
The number of samples per st~tte for the initi~fl ask 
choices are: 
0 1 0 0 0 0 0 GreetS=IS5 GreetU=156 
1 2 0 0 0 0 0 Ask2S=93 Ask2U=72 
1 2 0 0 0 0 1 Ask2S=36 Ask2U=48 
Such data illustrates that the random action choice 
strategy led to a fairly balanced action distribution 
per state. Similarly, the small state space, and the 
fact that we only allowed 2 action choices per state, 
prevented a data sparseness problem. The first state 
in Figure 4, the initial state for every dialogue, was 
the most frequently visited state (with 311 visits). 
Only 8 states that occur near the end of a dialogue 
were visited less tlmn 10 times. 
The logged data was then used to construct he 
empirical MDP. As we have mentioned, the measure 
we chose to optinfize is a binary reward flmction 
based on the strongest possible measure of task com- 
pletion, called S t rongComp,  that takes on value 
1 if NJFun queries the database using exactly the 
attributes pecified in the task description, and 0 
otherwise. Then we eoml)uted the optimal dialogue 
strategy in this MDP using RL (cf. Section 2). The 
action choices constituting the learned strategy are 
in boldface in Figure 4. Note that no choice was 
fixed for several states, inealfing that the Q-values 
were identical after value iteration. Thus, even when 
using the learned strategy, NJFun still sometimes 
chooses randomly between certain action pairs. 
Intuitively, the learned strategy says that the op- 
timal use of initiative is to begin with user initia- 
tive, then back off to either mixed or system ini- 
tiative when reasking for an attribute. Note, how- 
ever, that the specific baekoff method differs with 
attribute (e.g., system initiative for attribute 1, but 
gcnerMly mixed initiative for attribute 2). With 
respect to confirmation, the optimal strategy is to 
506 
mainly contirm at lower contidenee -values. Again, 
however, the point where contirlnation becomes un- 
necessary difl'ers across attributes (e.g., confidence 
level 2 for attribute 1, but sometimes lower levels 
for attributes 2 and 3), and  also dt!txmds on other 
features of the state besides confidence (e.g., gram- 
mar and history). This use (if ASP, (:ontidence. by the 
dialogue strategy is more Sol)hisli('ated than previ- 
ous al)proaches, e.g. (Niimi and Kot)ayashi, 1996; 
Lit\]nan and Pan, 2000). N.lI,'un ('an learn such line- 
grained distinctions l}ecause the el)ritual strategy is 
based on a eonll)arisoi) of 24~ l}ossible exl}h)ratory 
strategies. Both the initiative and confirmation re- 
suits sugge.sl that the begimfing of the dialogue was 
the most problenmtie for N.lli'un. Figure I ix an ex- 
ample dialogue using the Ol)tilnal strategy. 
5 Experimentally Evaluating the 
Strategy 
For the testing i)\]tase, NJFun was reilnplemented to
use the learned strategy. 2:t test sul)je(;Is then per- 
formed the same 6 tasks used during training, re- 
sulling in 124 complete test dialogues. ()he of our 
main resull;s is that task completion its measured by 
StrongCom 11 increased front 52cX} in training 1o 64% 
in testing (p < .06)) 
There is also a signilicant in~twaction (!II'(~c.t 
between strategy nnd task (p<.01) for Strong- 
Colnl).  \]'revious work has suggested l;hat novic(~ 
users l)erform (:Oml)arably to eXl)erts after only 2 
tasks (Kamm et ill., \] 9!18). Sill('e Ollr \]oarllt}d sl.rat- 
egy was based on 6 tasks with each user, one (?xpla- 
nation of the interaction eft'cot is that the learnc.d 
strategy is slightly optimized for expert users. ~lb 
explore this hyi)othesis, we divided our corpus into 
diah)gues with "novice" (tasks \] and 2) and "ex- 
pert" (tasks 3-6) users. We fOltltd that the learned 
strategy did in fact lc'a(l to a large an(1 significant 
improvement in StrongComp tbr (;Xl)erts (EIC=.d6, 
learned==.69, 11<.001), and a non-signilieant degra- 
dation for novices (1,31C=.66, learned=.55, 11<.3). 
An apparent limitation of these results is that EIC 
may not 1)e the best baseline strategy tbr coral)arisen 
to our learned strategy. A more standard alternative 
would be comparison to the very best hand-designed 
fixed strategy. However, there is no itgreement in the 
literature, nor amongst he authors, its to what the 
1)est hand-designed strategy might have been. There 
is agreement, however, that the best strategy ix sen- 
sitive to lnally unknown and unmodeled factors: the 
aThe ('.xlmrimental design (lescribed above Colmists of 2 
factors: the within-in groul) fa(:tor sl~ntefly aim the l)etween- 
groui)s facl;or task. \,Ve 11812, ~1, l,WO-~,g~l,y D.llO.ly,qiS of variance 
(ANOVA) to comtmte wlmtlmr main and int(!raction (!flk!cts 
of strategy are statistically signitica nt (t)<.05) or indicative 
of a statistical trend (p < .101. Main effe.cts of strategy are 
task-in(lel)endent , while interaction eIt'(!cts involving strat(%y 
are task-dependent. 
~4(~aSIlIX~ 
StrongComp 
\VcakComp 
ASR 
Fecdlmck 
UserSat 
EIC 
(n=:3111 
0.52 
1.75 
2.50 
0.18 
1.3.38 
v _ _  
l~eatned p 
(n=124) 
0.64 
2.19 .02 
2.67 .04 
0.11 .d2 
13.29 .86 
Table 1: Main ett'ects of dialogue strategy. 
user 1)olmlation, the specitics of the, task, the 1)ar- 
ticular ASR used, etc. Furthernlore, \]P, IC was (:are- 
fully designed so that the random choices it makes 
never results in tm unnatural dialogue. Finally, a 
companion paper (Singh et al, 2000) shows that the 
1)erforntanee of the learned strategy is better tha l l  
several "stmtdard" fixed strategies (such as always 
use system-initiative and no-confirmation). 
Although many types of measures have been used 
to evaluate dialogue systems (e.g., task success, 
dialogue quality, ettit:ieney, usability (l)anieli and 
Gerbino, 1995; Kamm et al, 11998)), we optimized 
only tbr one task success measure, StrongConll). 
Ilowever, we also examined the 1)erl 'ornmnee of the 
learned strategy using other ewduation measures 
(which t)ossibly could have llo011 used its our  reward 
function). WeakComp is a relaxed version of task 
comt)letion that gives partial credit: if all attribute 
values are either correct or wihh:ards, the value is the 
sum of the correct munl)er of attrilmtes. ()tlmrwise, 
at least one attribute is wrong (e.g., the user says 
"Lanfl)ertvilhf' but the system hears "Morristown"), 
and the wdue is -1. ASR is a dialogue quality lllea- 
sure that itl)l)roxinmtes Sl)eech recognition act:uracy 
for tl,e datM)ase query, a.nd is computed 1:) 3, adding 
1 for each correct attribute value altd .5 for every 
wihtca.rd. Thus, if the task ix to go winetasting 
near Lambertville in the morning, and the systenl 
queries the database for an activity in New Jersey 
in the morning, StrongComp=0, \VeakComp=l, and 
ASR=2. In addition to the objective measures dis- 
cussed a,bove, we also COmlmted two subjective us- 
ability measures. Feedback  is obtained front the 
dialogue (e.g. $4 in Figure 5), by mapping good, 
so-so, bad to 1, 0, m~d -1, respectively. User satis- 
faction (UserSat, ranging front 0-20) is obtained by 
summing the answers of the web-based user survey. 
Table I summarizes the diflhrence in performance 
of NJFun tbr our original reward flmction and the 
above alternative valuation measures, from trail> 
ing (EIC) to test (learned strategy for StrongComp). 
For WeakComp, the average reward increased from 
1.75 to 2.19 (p < 0.02), while tbr ASll the average 
reward increased from 2.5 to 2.67 (p < 0.04). Again, 
these iml)rovements occur even though the learned 
strategy was not optilnized for these measures. 
The last two rows of the table show that for the 
507 
subjective measures, i)erformmme does not signifi- 
cantly differ for the EIC and learned strategies. In- 
terestingly, the distributions of the subjective mea- 
sures move to the middle from training to testing, 
i.e., test users reply to the survey using less extreme 
answers than training users. Explaining the subjec- 
tire results is an area for future work. 
6 Discussion 
This paper presents a practical methodology for ap- 
plying RL to optimizing dialogue strategies in spo- 
ken dialogue systems, and shows empirically that the 
method improves performance over the EIC strategy 
in NJFun. A companion paper (Singh et al, 2000) 
shows that the learned strategy is not only better 
than EIC, but also better than other fixed choices 
proposed in the literature. Our results demonstrate 
that the application of RL allows one to empirically 
optimize a system's dialogue strategy by searching 
through a much larger search space than can be ex- 
plored with more traditional lnethods (i.e. empiri- 
cally testing several versions of a systent). 
RL has been appled to dialogue systems in pre- 
vious work, but our approach ditlhrs from previous 
work in several respects. Biermann and Long (1996) 
did not test RL in an implemented system, and the 
experiments of Levin et 31. (2000) utilized a simu- 
lated user model. Walker et al (1998)'s methodol- 
ogy is similar to that used here, in testing RL with 
an imt)lelnented system with human users. However 
that work only explored strategy choices at 13 states 
in the dialogue, which conceivably could have been 
explored with more traditional methods (~ts com- 
pared to the 42 choice states explored here). 
We also note that our learned strategy made di- 
alogue decisions based on ASR confidence in con- 
junction with other features, mid alto varied initia- 
tive and confirmation decisions at a finer grain than 
previous work; as such, our learned strategy is not; 
a standard strategy investigated in the dialogue sys- 
teln literature. For example, we would not have pre- 
dicted the complex and interesting back-off strategy 
with respect o initiative when reasking for an at- 
tribute. 
To see how our method scales, we are al)plying RL 
to dialogue systems for eustolner care and tbr travel 
planning, which are more complex task-oriented do- 
mains. As fllture work, we wish to understand 
the aforementioned results on the subjective reward 
measures, explore the potential difference between 
optimizing tbr expert users and novices, automate 
the choice of state space for dialogue systems, ilwes- 
tigate the use of a learned reward function (Walker 
et al, 1998), and explore the use of more informative 
non-terminal rewards. 
Acknowledgements 
The authors thank Fan Jiang for his substantial effort 
in implenmnting NJFun, Wieland Eckert, Esther Levin, 
Roberto Pieraccini, and Mazin R.ahinl for their technical 
help, Julia Hirsehberg for her comments on a draft of this 
paper, and David McAllester, I~ichard Sutton, Esther 
Levin and Roberto Pieraccini for hell)tiff conversations. 
References 
A. W. Biermann and P. M. Long. 1996. The composition 
of messages in sl)eeeh-graphies interactive systems. In 
Proe. of the International Symposium on Spoken Dia- 
logue, pages 97 100. 
M. Danieli and E. Gerbino. 1995. Metrics for evaluating 
dialogue strategies in a spoken language system. In 
P~vc. of the AAAI  Spring Symposium on Empirical 
Methods in Discourse Interpretation and Generation, 
pages 34 39. 
C. Kamm, D. Litman, and M. A. Walker. 1998. From 
novice to expert: The effect of tutorials on user exl)er- 
tise with spoken dialogue systems. In P~vc. of the In- 
ternational Conference on Spolccn Language P~vccss- 
in.q, ICSLP98. 
E. Levin, R. Pieraccini, W. Eekere, G. Di Fabbrizio, and 
S. Narayanan. 1999. Spoken language dialogue: lh'om 
theory to practice. In Pwc. IEEE Workshop on Au- 
tomatic Speech R.ecognition and Understanding, AS- 
R U U99. 
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas- 
tic model of human machine interaction for learning 
dialog strategies. IEEE TTnnsactions on Speech and 
Audio Processing, 8(1):11-23. 
D. J. Litman and S. Pan. 2000. Predicting and adapting 
to poor Sl)eech recognition in a spoken dialogue sys- 
tern. In Proc. of the Scv('ntccnth National Confl:rcncc 
on Artificial Intclligcncc, AAAI-2000. 
Y. Niimi and Y. Kobayashi. 1996. A dialog control strat- 
egy based on the reliability of speech recognition. In 
Proc. of the International Symposium on Spoken Dia- 
loguc, pages 157--160. 
A. Sanderman, J. Sturm, E. den Os, L. Boves, and 
A. Cremers. 1998. Evaluation of the dutchtrain 
timetable inibrmation system developed in the arise 
project. In Interactive Voice Technology for Tclccom- 
munications Applications, IVT2'A, pages 91-96. 
S. Singh, M. S. Kearns, D. J. Litman, and M. A. \Valker. 
1999. Reinforcement learning for spoken dialogue sys- 
tems. In Proc. NIPS99. 
S. B. Singh, M. S. Kearns, D. J. Litman, and 
M. A. Walker. 2000. Empirical evaluation of a rein- 
forccment learning spoken dialogue system. In Proc. 
of thc Scvcntccnth National Conference on Artificial 
Intelligence, AAAI-2000. 
R. S. Sutton. 1991. Plamfing by incremental dynamic 
programming. In Proc. Ninth Confcwztcc on Machine 
Learning, pages 353-357. 
M. A. Walker, J. C. Promer, and S. Narayanan. 1998. 
Learning optimal dialogue strategies: A ease study of 
a Sl)oken dialogue agent br email. In P~vc. of the 36th 
Annual Meeting of the Association of Computational 
Linguistics, COLING//ACL 98, pages 1345 1352. 
508 
 
		Predicting User Reactions to System Error
Diane Litman and Julia Hirschberg
AT&T Labs?Research
Florham Park, NJ, 07932 USA
 
diane/julia  @research.att.com
Marc Swerts
IPO, Eindhoven, The Netherlands,
and CNTS, Antwerp, Belgium
m.g.j.swerts@tue.nl
Abstract
This paper focuses on the analysis and
prediction of so-called aware sites,
defined as turns where a user of a
spoken dialogue system first becomes
aware that the system has made a
speech recognition error. We describe
statistical comparisons of features of
these aware sites in a train timetable
spoken dialogue corpus, which re-
veal significant prosodic differences
between such turns, compared with
turns that ?correct? speech recogni-
tion errors as well as with ?normal?
turns that are neither aware sites nor
corrections. We then present machine
learning results in which we show how
prosodic features in combination with
other automatically available features
can predict whether or not a user turn
was a normal turn, a correction, and/or
an aware site.
1 Introduction
This paper describes new results in our continu-
ing investigation of prosodic information as a po-
tential resource for error recovery in interactions
between a user and a spoken dialogue system. In
human-human interaction, dialogue partners ap-
ply sophisticated strategies to detect and correct
communication failures so that errors of recog-
nition and understanding rarely lead to a com-
plete breakdown of the interaction (Clark and
Wilkes-Gibbs, 1986). In particular, various stud-
ies have shown that prosody is an important cue
in avoiding such breakdown, e.g. (Shimojima et
al., 1999). Human-machine interactions between
a user and a spoken dialogue system (SDS) ex-
hibit more frequent communication breakdowns,
due mainly to errors in the Automatic Speech Re-
cognition (ASR) component of these systems. In
such interactions, however, there is also evidence
showing prosodic information may be used as a
resource for error recovery. In previous work,
we identified new procedures to detect recogni-
tion errors. In particular, we found that pros-
odic features, in combination with other inform-
ation already available to the recognizer, can dis-
tinguish user turns that are misrecognized by the
system far better than traditional methods used in
ASR rejection (Litman et al, 2000; Hirschberg et
al., 2000). We also found that user corrections
of system misrecognitions exhibit certain typical
prosodic features, which can be used to identify
such turns (Swerts et al, 2000; Hirschberg et al,
2001). These findings are consistent with previ-
ous research showing that corrections tend to be
hyperarticulated ? higher, louder, longer . . . than
other turns (Wade et al, 1992; Oviatt et al, 1996;
Levow, 1998; Bell and Gustafson, 1999).
In the current study, we focus on another turn
category that is potentially useful in error hand-
ling. In particular, we examine what we term
aware sites ? turns where a user, while interact-
ing with a machine, first becomes aware that the
system has misrecognized a previous user turn.
Note that such aware sites may or may not also be
corrections (another type of post-misrecognition
turn), since a user may not immediately provide
correcting information. We will refer to turns
that are both aware sites and corrections as corr-
awares, to turns that are only corrections as corrs,
to turns that are only aware sites as awares, and to
turns that are neither aware sites nor corrections as
norm.
We believe that it would be useful for the
dialogue manager in an SDS to be able to de-
tect aware sites for several reasons. First, if
aware sites are detectable, they can function as
backward-looking error-signaling devices, mak-
ing it clear to the system that something has gone
wrong in the preceding context, so that, for ex-
ample, the system can reprompt for information.
In this way, they are similar to what others have
termed ?go-back? signals (Krahmer et al, 1999).
Second, aware sites can be used as forward-
looking signals, indicating upcoming corrections
or more drastic changes in user behavior, such
as complete restarts of the task. Given that, in
current systems, both corrections and restarts of-
ten lead to recognition error (Swerts et al, 2000),
aware sites may be useful in preparing systems to
deal with such problems.
In this paper, we investigate whether aware
sites share acoustic properties that set them apart
from normal turns, from corrections, and from
turns which are both aware sites and corrections.
We also want to test whether these different turn
categories can be distinguished automatically, via
their prosodic features or from other features
known to or automatically detectible by a spoken
dialogue system. Our domain is the TOOT spoken
dialogue corpus, which we describe in Section 2.
In Section 3, we present some descriptive findings
on different turn categories in TOOT. Section 4
presents results of our machine learning experi-
ments on distinguishing the different turn classes.
In Section 5 we summarize our conclusions.
2 Data
The TOOT corpus was collected using an experi-
mental SDS developed for the purpose of compar-
ing differences in dialogue strategy. It provides
access to train information over the phone and
is implemented using an internal platform com-
bining ASR, text-to-speech, a phone interface,
and modules for specifying a finite-state dialogue
manager, and application functions. Subjects per-
formed four tasks with versions of TOOT, which
varied confirmation type and locus of initiative
(system initiative with explicit system confirma-
tion, user initiative with no system confirmation
until the end of the task, mixed initiative with im-
plicit system confirmation), as well as whether
the user could change versions at will using voice
commands. Subjects were 39 students, 20 nat-
ive speakers of standard American English and
19 non-native speakers; 16 subjects were female
and 23 male. The exchanges were recorded and
the system and user behavior logged automatic-
ally. Dialogues were manually transcribed and
user turns automatically compared to the corres-
ponding ASR (one-best) recognized string to pro-
duce a word accuracy score (WA) for each turn.
Each turn?s concept accuracy (CA) was labeled
by the experimenters from the dialogue recordings
and the system log; if the recognizer correctly cap-
tured all the task-related information given in the
user?s original input (e.g. date, time, departure or
arrival cities), the turn was given a CA score of
1, indicating a semantically correct recognition.
Otherwise, the CA score reflected the percentage
of correctly recognized task concepts in the turn.
For the study described below, we examined 2328
user turns from 152 dialogues generated during
these experiments. 194 of the 2320 turns were re-
jected by the system.
To identify the different turn categories in the
corpus, two authors independently labeled each
turn as to whether or not it constituted a correction
of a prior system failure (a CA error or a rejection)
and what turn was being corrected, and whether
or not it represented an aware site for a prior fail-
ure, and, if so, the turn which the system had failed
on. Labeler disagreement was subsequently re-
solved by consensus. The fragment in Figure 1,
produced with a version of TOOT in which the
user has the initiative with no confirmation until
the end of the task, illustrates these labels. This
example illustrates cases of corraware, in which
both the user?s awareness and correction of a mis-
recognition occur in the same turn (e.g. turns
1159 and 1160, after system prompts for informa-
tion already given in turn 1158). It also illustrates
cases in which aware sites and corrections occur
in different turns. For example, after the immedi-
ate explicit system confirmation of turn 1162, the
user first becomes aware of the system errors (turn
1163), then separately corrects them (turn 1164);
turn 1163 is thus an aware turn and turn 1164 a
corr. When no immediate confirmation of an ut-
terance occurs (as with turn 1158), it may take sev-
eral turns before the user becomes aware of any
Turn Turn ID Aware of Corr of Type
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1158 NA NA NORM
S: Which city do you want to go to?
U: New York City 1159 1158 1158 CORRAWARE
S: Which city do you want to leave from?
U: Baltimore 1160 1158 1158 CORRAWARE
S: Do you want me to find the trains from
Baltimore to New York City today at anytime now?
U: No 1161 1158 NA AWARE
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1162 NA 1158 CORR
S: Do you want me to find the trains from
Baltimore to New York City today at anytime now?
U: No 1163 1162 NA AWARE
S: How may I help you?
U: Get me the train on Sunday at eight thirty pm 1164 NA 1162 CORR
Figure 1: Dialogue Fragment with Aware and Correction Labels.
misrecognition errors. For example, it is not un-
til turn 1161 that the user first becomes aware of
the error in date and time from 1158; the user then
corrects the error in 1162. So, 1161 is classified as
an aware and 1162 as a corr. Note that corr turns
represent 13% of the turns in our corpus, awares
represent 14%, corrawares account for 16%, and
norm turns represent 57% of the turns in the cor-
pus.
3 Descriptive Analysis and Results
We examined prosodic features for each user turn
which had previously been shown to be useful for
predicting misrecognized turns and corrections:
maximum and mean fundamental frequency val-
ues (F0 Max, F0 Mean), maximum and mean en-
ergy values (RMS Max, RMS Mean), total dur-
ation (Dur), length of pause preceding the turn
(Ppau), speaking rate (Tempo) and amount of si-
lence within the turn (%Sil). F0 and RMS val-
ues, representing measures of pitch excursion and
loudness, were calculated from the output of En-
tropic Research Laboratory?s pitch tracker, get f0,
with no post-correction. Timing variation was
represented by four features. Duration within and
length of pause between turns was computed from
the temporal labels associated with each turn?s be-

While the features were automatically computed, begin-
nings and endings were hand segmented from recordings of
the entire dialogue, as the turn-level speech files used as in-
put in the original recognition process created by TOOT were
unavailable.
ginning and end. Speaking rate was approximated
in terms of syllables in the recognized string per
second, while %Sil was defined as the percentage
of zero frames in the turn, i.e., roughly the per-
centage of time within the turn that the speaker
was silent.
To see whether the different turn categories
were prosodically distinct from one another, we
applied the following procedure. We first calcu-
lated mean values for each prosodic feature for
each of the four turn categories produced by each
individual speaker. So, for speaker A, we divided
all turns produced into four classes. For each
class, we then calculated mean F0 Max, mean F0
Mean, and so on. After this step had been repeated
for each speaker and for each feature, we then cre-
ated four vectors of speaker means for each indi-
vidual prosodic feature. Then, for each prosodic
feature, we ran a one-factor within subjects anova
on the means to learn whether there was an overall
effect of turn category.
Table 1 shows that, overall, the turn categor-
ies do indeed differ significantly with respect to
different prosodic features; there is a signific-
ant, overall effect of category on F0 Max, RMS
Max, RMS Mean, Duration, Tempo and %Sil. To
identify which pairs of turns were significantly
different where there was an overall significant ef-
fect, we performed posthoc paired t-tests using the
Bonferroni method to adjust the p-level to 0.008
(on the basis of the number of possible pairs that
Turn categories
Feature Normal Correction Aware Corraware  -stat
***F0 Max (Hz) 220.05 263.40 216.87 229.00 
	  =10.477
F0 Mean (Hz) 161.78 173.43 162.61 158.24 
	  =1.575
***RMS Max (dB) 1484.14 1833.62 1538.91 1925.38 
	  =7.548
*RMS Mean (dB) 372.47 379.65 425.96 464.16  
	  =3.190
***Dur (sec) 1.43 4.39 1.12 2.33 
	  =34.418
Ppau (sec) 0.60 0.93 0.87 0.80 
	  =1.325
**Tempo (syls/sec) 2.59 2.38 2.16 2.43  
	  =4.206
*%Sil (sec) 0.46 0.41 0.44 0.42  
	  =3.182
Significance level: *(p  .05), **(p  .01), ***(p  .001)
Table 1: Mean Values of Prosodic Features for Turn Categories.
Prosodic features
Classes F0 max F0 mean RMS max RMS mean Dur Ppau Tempo %Sil
norm/corr ? ? ? +
norm/aware +
norm/corraware ? ?
aware/corr ? ? ? ?
aware/corraware ? ? ?
corraware/corr ? ?
Table 2: Pairwise Comparisons of Different Turn Categories by Prosodic Feature.
can be drawn from an array of 4 means). Res-
ults are summarized in Table 2, where ? + ? or
? ? ? indicates that the feature value of the first cat-
egory is either significantly higher or lower than
the second. Note that, for each of the pairs, there
is at least one prosodic feature that distinguishes
the categories significantly, though it is clear that
some pairs, like aware vs. corr and norm vs. corr
appear to have more distinguishing features than
others, like norm vs. aware. It is also interesting to
see that the three types of post-error turns are in-
deed prosodically different: awares are less prom-
inent in terms of F0 and RMS maximum than cor-
rawares, which, in turn, are less prominent than
corrections, for example. In fact, awares, except
for duration, are prosodically similar to normal
turns.
4 Predictive Results
We next wanted to determine whether the pros-
odic features described above could, alone or
in combination with other automatically avail-
able features, be used to predict our turn categor-
ies automatically. This section describes experi-
ments using the machine learning program RIP-
PER (Cohen, 1996) to automatically induce pre-
diction models from our data. Like many learn-
ing programs, RIPPER takes as input the classes
to be learned, a set of feature names and possible
values, and training data specifying the class and
feature values for each training example. RIPPER
outputs a classification model for predicting the
class of future examples, expressed as an ordered
set of if-then rules. The main advantages of RIP-
PER for our experiments are that RIPPER supports
?set-valued? features (which allows us to repres-
ent the speech recognizer?s best hypothesis as a set
of words), and that rule output is an intuitive way
to gain insight into our data.
In the current experiments, we used 10-fold
cross-validation to estimate the accuracy of the
rulesets learned. Our predicted classes corres-
pond to the turn categories described in Section
2 and variations described below. We repres-
ent each user turn using the feature set shown in
Figure 2, which we previously found useful for
predicting corrections (Hirschberg et al, 2001).
A subset of the features includes the automatic-
ally computable raw prosodic features shown in
Table 1 (Raw), and normalized versions of these
features, where normalization was done by first
turn (Norm1) or by previous turn (Norm2) in a
dialogue. The set labeled ?ASR? contains stand-
ard input and output of the speech recognition pro-
cess, which grammar was used for the dialogue
state the system believed the user to be in (gram),
Raw: f0 max, f0 mean, rms max, rms mean, dur, ppau,
tempo, %sil;
Norm1: f0 max1, f0 mean1, rms max1, rms mean1, dur1,
ppau1, tempo1, %sil1;
Norm2: f0 max2, f0 mean2, rms max2, rms mean2, dur2,
ppau2, tempo2, %sil2;
ASR: gram, str, conf, ynstr, nofeat, canc, help, wordsstr,
syls, rejbool;
System Experimental: inittype, conftype, adapt, realstrat;
Dialogue Position: diadist;
PreTurn: features for preceding turn (e.g., pref0max);
PrepreTurn: features for preceding preceding turn (e.g.,
ppref0max);
Prior: for each boolean-valued feature (ynstr, nofeat,
canc, help, rejbool), the number/percentage of
prior turns exhibiting the feature (e.g., prioryn-
strnum/priorynstrpct);
PMean: for each continuous-valued feature, the mean of the
feature?s value over all prior turns (e.g., pmnf0max);
Figure 2: Feature Set.
the system?s best hypothesis for the user input
(str), and the acoustic confidence score produced
by the recognizer for the turn (conf). As subcases
of the str feature, we also included whether or not
the recognized string included the strings yes or no
(ynstr), some variant of no such as nope (nofeat),
cancel (canc), or help (help), as these lexical items
were often used to signal problems in our sys-
tem. We also derived features to approximate the
length of the user turn in words (wordsstr) and in
syllables (syls) from the str features. And we ad-
ded a boolean feature identifying whether or not
the turn had been rejected by the system (rejbool).
Next, we include a set of features representing
the system?s dialogue strategy when each turn was
produced. These include the system?s current ini-
tiative and confirmation strategies (inittype, conf-
type), whether users could adapt the system?s dia-
logue strategies (adapt), and the combined initiat-
ive/confirmation strategy in effect at the time of
the turn (realstrat). Finally, given that our previ-
ous studies showed that preceding dialogue con-
text may affect correction behavior (Swerts et al,
2000; Hirschberg et al, 2001), we included a fea-
ture (diadist) reflecting the distance of the current
turn from the beginning of the dialogue, and a set
of features summarizing aspects of the prior dia-
logue: for the latter features, we included both the
number of times prior turns exhibited certain char-
acteristics (e.g. priorcancnum) and the percent-
age of the prior dialogue containing one of these
features (e.g. priorcancpct). We also examined
means for all raw and normalized prosodic fea-
tures and some word-based features over the en-
tire dialogue preceding the turn to be predicted
(pmn ). Finally, we examined more local con-
texts, including all features of the preceding turn
(pre ) and for the turn preceding that (ppre ).
We provided all of the above features to the
learning algorithm first to predict the four-way
classification of turns into normal, aware, corr and
corraware. A baseline for this classification (al-
ways predicting norm, the majority class) has a
success rate of 57%. Compared to this, our fea-
tures improve classification accuracy to 74.23%
(+/? 0.96%). Figure 3 presents the rules learned
for this classification. Of the features that appear
in the ruleset, about half are features of current
turn and half features of the prior context. Only
once does a system feature appear, suggesting that
the rules generalize beyond the experimental con-
ditions of the data collection. Of the features spe-
cific to the current turn, prosodic features domin-
ate, and, overall, timing features (dur and tempo
especially) appear most frequently in the rules.
About half of the contextual features are prosodic
ones and half are ASR features, with ASR confid-
ence score appearing to be most useful. ASR fea-
tures of the current turn which appear most often
are string-based features and the grammar state
the system used for recognizing the turn. There
appear to be no differences in which type of fea-
tures are chosen to predict the different classes.
If we express the prediction results in terms of
precision and recall, we see how our classification
accuracy varies for the different turn categories
(Table 3). From Table 3, we see that the majority
class (normal) is most accurately classified. Pre-
dictions for the other three categories, which oc-
cur about equally often in our corpus, vary consid-
erably, with modest results for corr and corraware,
and rather poor results for aware. Table 4 shows a
confusion matrix for the four classes, produced by
if (gram=universal)  (dur2  7.31) then CORR
if (dur2  2.19)  (priornofeatpct  0.09)  (tempo  1.50)  (pmntempo  2.39) then CORR
if (dur2  1.53)  (pmnwordsstr  2.06)  (tempo1  1.07)  (predur  0.80)  (prenofeat=F)  (presyls  4) then CORR
if (predur1  0.26)  (dur  0.79)  (rmsmean2  1.51)  (f0mean  173.49) then CORR
if (dur2  1.41)  (prenofeat=T)  (str contains word ?eight?) then CORR
if (predur1  0.18)  (dur2  4.21)  (dur1  0.50)  (f0mean  276.43) then CORR
if (predur1  0.19)  (ppregram=cityname)  (rmsmax1  1.10)  (pmntempo2  1.64) then CORR
if (realstrat=SystemImplicit)  (gram=cityname)  (pmnf0mean1  0.96) then CORR
if (preconf  -2.66)  (dur2  0.31)  (pprenofeat=T)  (tempo2  0.61) then AWARE
if (preconf  -2.85)  (syls  2)  (predur  1.05)  (pref0max  4.82)  (tempo2  0.58)  (pmn%sil  0.53) then AWARE
if (preconf  -3.34)  (syls  2)  (ppau  0.57)  (conf  -3.07)  (preppau  0.72) then AWARE
if (dur  0.74)  (pmndur  2.57)  (preconf  -4.36)  (f0mean2  0.90) then CORRAWARE
if (preconf  -2.80)  (pretempo  2.16)  (preconf  -3.95)  (tempo1  4.67) then CORRAWARE
if (preconf  -2.80)  (dur  0.66)  (rmsmean  488.56) then CORRAWARE
if (preconf  -3.56)  (dur2  0.64)  (prerejbool=T) then CORRAWARE
if (pretempo  0.71)  (tempo  3.31) then CORRAWARE
if (preconf  -3.01)  (tempo2  0.78)  (pmndur  2.83)  (pmnf0mean  199.84) then CORRAWARE
if (pmnconf  -3.10)  (prestr contains the word ?help?)  (pmndur2  2.01)  (ppau  0.98) then CORRAWARE
if (pmnconf  -3.10)  (gram=universal)  (pregram=universal)  ( %sil  0.39) then CORRAWARE
else NORM
Figure 3: Rules for Predicting 4 Turn Categories.
Precision (%) Recall (%)
norm 80.09 89.39
corr 72.86 61.66
aware 61.01 39.79
corraware 61.76 61.72
Accuracy: 74.23% (  0.96%); baseline: 57%
Table 3: 4-way Classification Performance.
applying our best ruleset to the whole corpus. This
Classified as
norm corr aware corraware
norm 1263 14 11 38
corr 68 219 0 7
aware 149 1 130 47
corraware 53 5 8 315
Table 4: Confusion Matrix, 4-way Classification.
matrix clearly shows a tendency for the minority
classes, aware, corr and corraware, to be falsely
classified as normal. It also shows that aware and
corraware are more often confused than the other
categories.
These confusability results motivated us to col-
lapse the aware and corraware into one class,
which we will label isaware; this class thus rep-
resents all turns in which users become aware of
a problem. From a system perspective, such a
3-way classification would be useful in identify-
ing the existence of a prior system failure and in
further identifying those turns which simply rep-
resent corrections; such information might be as
useful, potentially, as the 4-way distinction, if we
could achieve it with greater accuracy.
Indeed, when we predict the three classes
(isaware, corr, and norm) instead of four, we
do improve in predictive power ? from 74.23%
to 81.14% (+/? 0.83%) classification success.
Again, this compares to the baseline (predicting
norm, which is still the majority class) of 57%. We
also get a corresponding improvement in terms of
precision and recall, as shown in Table 5, with
the isaware category considerably better distin-
guished than either aware or corraware in Table 3.
The ruleset for the 3-class predictions is given in
Precision (%) Recall(%)
norm 84.49 87.48
corr 72.07 67.38
isaware 80.52 77.07
Accuracy: 81.14% (  0.83%); baseline: 57%
Table 5: 3-way Classification Performance.
Figure 4. The distribution of features in this rule-
set is quite similar to that in Figure 3. However,
there appear to be clear differences in which fea-
tures best predict which classes. First, the features
used to predict corrections are balanced between
those from the current turn and features from the
preceding context, whereas isaware rules primar-
ily make use of features of the preceding context.
Second, the features appearing most often in the
rules predicting corrections are durational features
(dur2, predur1, dur), while duration is used only
if (gram=universal)  (dur2  7.31) then CORR
if (dur2  2.25)  (priornofeatpct  0.11)  (%sil  0.55)
 (wordsstr  4) then CORR
if (dur2  2.75)  (gram=universal)  (pre%sil1  1.17)
then CORR
if (predur1  0.24)  (dur  0.85)  (priornofeatnum  2)
 (pmnconf  -3.11)  (pmn%sil  0.45) then CORR
if (predur1  0.19)  (dur  1.21)  (pmnf0mean2  0.99)
 (predur2  0.90)  (%sil  0.70)  (tempo  3.25) then
CORR
if (predur1  0.20)  (ynstr=F)  (pregram=cityname) 
(ppref0mean  171.58) then CORR
if (dur2  0.75)  (gram=cityname)  (pmnsyls  3.67) 
(pmnconf  -3.23)  (%sil  0.41) then CORR
if (prenofeat=T)  (preconf  -0.72) then CORR
if (preconf  -4.07) then ISAWARE
if (preconf  -2.76)  (pmntempo  2.39)  (tempo2 
1.56)  (preynstr=F) then ISAWARE
if (preconf  -2.75)  (ppau  0.46)  (tempo  1.20) then
ISAWARE
if (pretempo  0.23) then ISAWARE
if (pmnconf  -3.10)  (ppregram=universal)  (ppre%sil 
0.34)  (tempo1  2.94) then ISAWARE
if (predur  1.27)  (pretempo  2.36)  (prermsmean 
229.33)  (tempo2  0.83) then ISAWARE
if (preconf  -2.80)  (nofeat=T)  (f0mean  205.56) then
ISAWARE
else NORM
Figure 4: Rules for Predicting 3 Turn Categories.
once in isaware rules. Instead, these rules make
considerable use of the ASR confidence score of
the preceding turn; in cases where aware turns im-
mediately follow a rejection or recognition error,
one would expect this to be true. Isaware rules
also appear distinct from correction rules in that
they make frequent use of the tempo feature. It
is also interesting to note that rules for predicting
isaware turns make only limited use of the nofeat
feature, i.e. whether or not a variant of the word
no appears in the turn. We might expect this lex-
ical item to be a more useful predictor, since in
the explicit confirmation condition, users should
become aware of errors while responding to a re-
quest for confirmation.
Note that corrections, now the minority class,
are more poorly distinguished than other classes in
our 3-way classification task (Table 5). In a third
set of experiments, we merged corrections with
normal turns to form a 2-way distinction over all
between aware turns and all others. Thus, we only
distinguish turns in which a user first becomes
aware of an ASR failure (our original isaware and
corraware categories) from those that are not (our
original corr and norm categories). Such a dis-
tinction could be useful in flagging a prior sys-
tem problem, even though it fails to target the ma-
terial intended to correct that problem. For this
new 2-way distinction, we obtain a higher de-
gree of classification accuracy than for the 3-way
classification ? 87.80% (+/? 0.61%) compared to
81.14%. Note, however, that the baseline (predict
majority class of !isaware) for this new classifica-
tion is 70%, considerably higher than the previous
baseline. Table 6 shows the improvement in terms
of accuracy, precision, and recall.
Precision (%) Recall (%)
!isaware 91.7 91.6
isaware 80.7 81.1
Accuracy: 87.80% (  0.61%); baseline: 70%
Table 6: 2-way Classification Performance.
The ruleset for the 2-way distinction is shown in
Figure 5. The features appearing most frequently
if (preconf  -4.06)  (pretempo  2.65)  (ppau  0.25)
then T
if (preconf  -3.59)  (prerejbool=T) then T
if (preconf  -2.85)  (predur  1.039)  (tempo2  1.04)
 (preppau  0.57)  (pretempo  2.18) then T
if (preconf  -3.78)  (pmnsyls  4.04) then T
if (preconf  -2.75)  (prestr contains the word ?help?) then
T
if (pregram=universal)  (pprewordsstr  2) then T
if (preconf  -2.60)  (predur  1.04)  (%sil1  1.06) 
(prermsmean  370.65) then T
if (pretempo  0.13) then T
if (predur  1.27)  (pretempo  2.36)  (prermsmean 
245.36) then T
if (pretempo  0.80)  (pmntempo  1.75)  (ppretempo2
 1.39) then T
then F
Figure 5: Rules for Predicting 2 Turn Categories:
ISAWARE (T) versus the rest (F).
in these rules are similar to those in the previous
two rulesets in some ways, but quite different in
others. Like the rules in Figures 3 and 4, they ap-
pear independent of system characteristics. Also,
of the contextual features appearing in the rules,
about half are prosodic features and half ASR-
related; and, of the current turn features, pros-
odic features dominate. And timing features again
(especially tempo) dominate the prosodic features
that appear in the rules. However, in contrast to
previous classification rulesets, very few features
of the current turn appear in the rules at all. So,
it would seem that, for the broader classification
task, contextual features are far more important
than for the more fine-grained distinctions.
5 Conclusion
Continuing our earlier research into the use of
prosodic information to identify system misrecog-
nitions and user corrections in a SDS, we have
studied aware sites, turns in which a user first no-
tices a system error. We find first that these sites
have prosodic properties which distinguish them
from other turns, such as corrections and normal
turns. Subsequent machine learning experiments
distinguishing aware sites from corrections and
from normal turns show that aware sites can be
classified as such automatically, with a consid-
erable degree of accuracy. In particular, in a 2-
way classification of aware sites vs. all other turns
we achieve an estimated success rate of 87.8%.
Such classification, we believe, will be especially
useful in error-handling for SDS. We have pre-
viously shown that misrecognitions can be clas-
sified with considerable accuracy, using prosodic
and other automatically available features. With
our new success in identifying aware sites, we
acquire another potentially powerful indicator of
prior error. Using these two indicators together,
we hope to target system errors considerably more
accurately than current SDS can do and to hypo-
thesize likely locations of user attempts to correct
these errors. Our future research will focus upon
combining these sources of information identify-
ing system errors and user corrections, and invest-
igating strategies to make use of this information,
including changes in dialogue strategy (e.g. from
user or mixed initiative to system initiative after
errors) and the use of specially trained acoustic
models to better recognize corrections.
References
L. Bell and J. Gustafson. 1999. Repetition and its
phonetic realizations: Investigating a Swedish data-
base of spontaneous computer-directed speech. In
Proceedings of ICPhS-99, San Francisco. Interna-
tional Congress of Phonetic Sciences.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as
a collaborative process. Cognition, 22:1?39.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In 14th Conference of the American
Association of Artificial Intelligence, AAAI.
J. Hirschberg, D. Litman, and M. Swerts. 2000.
Generalizing prosodic prediction of speech recog-
nition errors. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing,
Beijing.
J. Hirschberg, D. Litman, and M. Swerts. 2001.
Identifying user corrections automatically in spoken
dialogue systems. In Proceedings of NAACL-2001,
Pittsburgh.
E. Krahmer, M. Swerts, M. Theune, and M. Weegels.
1999. Error spotting in human-machine interac-
tions. In Proceedings of EUROSPEECH-99.
G. Levow. 1998. Characterizing and recognizing
spoken corrections in human-computer dialogue.
In Proceedings of the 36th Annual Meeting of the
Association of Computational Linguistics, COL-
ING/ACL 98, pages 736?742.
D. Litman, J. Hirschberg, and M. Swerts. 2000. Pre-
dicting automatic speech recognition performance
using prosodic cues. In Proceedings of NAACL-00,
Seattle, May.
S. L. Oviatt, G. Levow, M. MacEarchern, and K. Kuhn.
1996. Modeling hyperarticulate speech during
human-computer error resolution. In Proceedings
of ICSLP-96, pages 801?804, Philadelphia.
A. Shimojima, K. Katagiri, H. Koiso, and M. Swerts.
1999. An experimental study on the informational
and grounding functions of prosodic features of Ja-
panese echoic responses. In Proceedings of the
ESCA Workshop on Dialogue and Prosody, pages
187?192, Veldhoven.
M. Swerts, D. Litman, and J. Hirschberg. 2000.
Corrections in spoken dialogue systems. In Pro-
ceedings of the Sixth International Conference on
Spoken Language Processing, Beijing.
E. Wade, E. E. Shriberg, and P. J. Price. 1992. User
behaviors affecting speech recognition. In Proceed-
ings of ICSLP-92, volume 2, pages 995?998, Banff.
NJFun: A Reinforcement Learning Spoken Dialogue System 
Diane  L i tman,  Sat inder  S ingh ,  M ichae l  Kearns  and  Mar i l yn  Walker  
AT&T Labs - -  Research 
180 Park  Avenue 
F lo rham Park,  NJ  07932 USA 
{diane,bavej  a,mkearns,walker} @research.att .com 
Abst rac t  
This paper describes NJFun, a real-time spoken dia- 
logue systemthat-provides users with information 
about things to d~ in New Jersey. NJFun auto- 
matically optimizes its dialogue strategy over time, 
by using a methodology for applying reinforcement 
learning to a working dialogue system with human 
users .  
1 In t roduct ion  
Using the formalism of Markov decision processes 
(MDPs) and the algorithms of reinforcement learn- 
ing (RL) has become a standard approach to many 
AI problems that involve an agent learning to 
optimize reward by interaction with its environ- 
ment (Sutton and Barto, 1998). We have adapted 
the methods of RL to the problem of automatically 
learning a good dialogue strategy in a fielded spoken 
dialogue system. Here is a summary of our proposed 
methodology for developing and evaluating spoken 
dialogue systems using R.L: 
? Choose an appropriate reward measure for di- 
alogues, and an appropriate representation for 
dialogue states. 
? Build an initial state-based training system that 
creates an exploratory data set. Despite being 
exploratory, this system should provide the de- 
sired basic functionality. 
? Use these training dialogues to build an empir- 
ical MDP model on the state space. 
? Compute the optimal dialogue policy according 
to this MDF, using RL. 
? Reimplement the system using the learned dia- 
logue policy. 
In this demonstration session paper, we briefly de- 
scribe our system, present some sample dialogues, 
and summarize our main contributions and limita- 
tions. Full details of our work (e.g. our reinforce- 
ment learning methodology, analysis establishing the 
veracity of the MDP we learn, a description of an 
experimental evaluation of NJFun, analysis of our 
learned ialogue strategy) can be found in two forth- 
coming technical papers (Singh et al, 2000; Litman 
et al, 2000). 
2 The  N JFun  System 
NJFun is a reM-time spoken dialogue system that 
provides users with information about things to do 
in New Jersey. 1 An example dialogue with NJFun is 
shown in Figure 1. NJFun is built using an internal 
platform for spoken dialogue systems. NJFun uses 
a speech recognizer with stochastic language models 
trained from example user utterances, and a TTS 
system based on concatenative diphone synthesis. 
Its database is populated from the nj.  onl ine web- 
page to contain information about activities. NJFun 
indexes this database using three attributes: activity 
type, location, and time of day. 
Informally, the NJFun dialogue manager sequen- 
tially queries the user regarding the activity, location 
and time attributes, respectively. NJFun first asks 
the user for the current attribute (and possibly the 
other attributes, depending on the initiative). If the 
current attribute's value is not obtained, NJFun asks 
for the attribute (and possibly the later attributes) 
again. If NJFun still does not obtain a value, N J- 
Fun moves on to the next attribute(s). Whenever 
NJFun successfully obtains a value, it can confirm 
the value, or move on and attempt o obtain the 
next attribute(s)? When NJFun has finished asking 
about the attributes, it queries the database (using 
a wildcard for each unobtained attribute value). 
We use reinforcement learning (RL) to optimize 
dialogue strategy, lq.L requires that all potential ac- 
tions for each state be specified. Note that at some 
states it is easy for a human to make the correct 
action choice. We made obvious dialogue strategy 
choices in advance, and used learning only to opti- 
mize the difficult choices. In NJFun, we restricted 
the action choices to 1) the type of initiative to use 
1This system description section is taken from (Litman et 
al., 2000) .
2Note that it is possible for users to specify multiple at- 
tributes, in any order, in a single utterance. However, NJFun 
will always process multiple attributes using its predefined 
sequential ordering. 
17 
SI: Welcome to N JFun .  How may i help you? ._ 
UI :  I 'd like to find um winetas t ing  in Lambertv i l le  in the morn ing .  
(ASR output: I 'd like to f ind out wineries the in the Lambertville in the morning.) 
$2: Did you say you are interested in Lambertv i l le?  
U2: Yes. 
$3: Did you say you want  to go in the morn ing?  
U3: Yes. 
$4: I found a winery near  Lambertv i l le  that  is open in the morn ing .  It is \ [ . . .  \] 
P lease give me feedback by say ing 'good ' ,  'so-so',  or  'bad ' .  
U4: Good.  
Figure 1: An example dialogue with NJFun (after optimization via RL). 
Act ion  
GreetS 
GreetU  
ReAsk 1 S 
ReAsk  1 M 
Ask2S 
Ask2U 
ReAsk2S 
ReAsk2~ 
m 
Welcome to N JFun.  P lease say an act iv i ty  name or say 'l ist act iv i t ies '  for a list of act iv i t ies I know about .  
Welcome to N JFun.  How may I help you?  
I know about  amusement  parks ,  aquar iums,  cruises, histor ic sites, museums,  parks ,  theaters ,  wineries,  
and  zoos. P lease say an act iv i ty  name f rom this  list. 
P lease tell me the act iv i ty  type.You can  also tell me the locat ion  and time. 
P lease say the name of the town or city that  you are interested in. 
P lease give me more in format ion.  
P lease tell me the name of the town or c i ty  that  you are interested in. 
"P lease  tell me the locat ion that  you are interested in. You can  also tell me the t ime. 
Figure 2: Sample initiative strategy choices. 
when asking or reasking for an attribute, and 2) 
whether to confirm an attribute value once obtained. 
The optimal actions may vary with dialogue state, 
and are subject o active debate in the literature. 
The examples in Figure 2 shows that NJFun can 
ask the user about the first 2 attributes 3 using three 
types of initiative, based on the combination of the 
wording of the system prompt (open versus direc- 
tive), and the type of grammar NJFun uses during 
ASR (restrictive versus non-restrictive). If NJFun 
uses an open question with an unrestricted gram- 
mar, it is using user initiative (e.g., GreetU). If N J- 
Fun instead uses a directive prompt with a restricted 
grammar, the system is using system initiative (e.g., 
GreetS). If NJFun uses a directive question with a 
non-restrictive grammar, it is using mixed initiative, 
because it is giving the user an opportunity to take 
the initiative by supplying extra information (e.g., 
ReAsklM). 
NJFun can also vary the strategy used to confirm 
each attribute. If NJFun asks the user to explicitly 
verify an attribute, it is using explicit confirmation 
(e.g., ExpConf2 for the location, exemplified by $2 
in Figure 1). If NJFun does not generate any con- 
firmation prompt, it is using no confirmation (an 
action we call NoConf). 
Solely for the purposes of controlling its operation 
(as opposed to the learning, which we discuss in a 
moment), NJFun internally maintains an operations 
vector of 14 variables. 2variables track whether the 
system has greeted the user, and which attribute 
the system is currently attempting to obtain. For 
each of the 3 attributes, 4 variables track whether 
'~ "Greet"  is equ iva lent  to  ask ing  for the f i rs t  a t t r ibute .  N J -  
Fun  a lways  uses  sys tem in i t ia t ive  fo r  the  th i rd  a t t r ibute ,  be -  
cause  a t  that  po in t  the  user  can  on ly  prov ide  the time of day .  
the system has obtained the attribute's value, the 
system's confidence in the value (if obtained), the 
number of times the system has asked the user about 
the attribute, and the type of ASR grammar most 
recently used to ask for the attribute. 
The formal state space S maintained by NJFun 
for the purposes of learning is much simpler than 
the operations vector, due to data sparsity concerns. 
The dialogue state space $ contains only 7 variables, 
which are summarized in Figure 3, and is easily com- 
puted from the operations vector. The "greet" vari- 
able tracks whether the system has greeted the user 
or not (no=0, yes=l). "Attr" specifies which at- 
tribute NJFun is currently attempting to obtain or 
verify (activity=l, location=2, time=3, done with 
attributes=4). "Conf" represents the confidence 
that NJFun has after obtaining a value for an at- 
tribute. The values 0, 1, and 2 represent low, 
medium and high ASR confidence. The values 3 
and 4 are set when ASR hears "yes" or "no" after a 
confirmation question. "Val" tracks whether NJFun 
has obtained avalue for the attribute (no=0, yes=l). 
"Times" tracks the number of times that NJFun has 
asked the user about the attribute. "Gram" tracks 
the type of grammar most recently used to obtain 
the attribute (0=non-restrictive, l=restrictive). Fi- 
nally, "history" represents whether NJFun had trou- 
ble understanding the user in the earlier part of the 
conversation (bad=0, good=l). We omit the full 
definition, but as an example, when NJFun is work- 
ing on the second attribute (location), the history 
variable is set to 0 if NJFun does not have an ac- 
tivity, has an activity but has no confidence in the 
value, or needed two queries to obtain the activity. 
In order to apply RL with a limited amount of 
training data, we need to design a small state space 
18 
I greet  a t t r  conf  val t imes gram history \[ 
0,1 1,2,3,4 0,1,2,3,4 0,1 0,1,2 0,1 0,1 I 
Figure 3: State features and values. 
that makes enough critical distinctions to support 
learning. The use of S yields a state space of size 
62. The state space that we utilize here, although 
minimal, allows us to make initiative decisions based 
on the success of earlier exchanges, and confirmation 
decisions based on ASR confidence scores and gram- 
mars. 
In order to learn a good dialogue strategy via RL 
we have to explore the state action space. The 
state/action mapping representing NJFun's initial 
exploratory dialog@ strategy EIC (Exploratory for 
Initiative and Confirmation) is given in Figure 4. 
Only the exploratory portion of the strategy is 
shown, namely all those states for which NJFun has 
an action choice. For each such state, we list the 
two choices of actions available. (The action choices 
in boldface are the ones eventually identified as op- 
timal by the learning process.) The EIC strategy 
chooses randomly between these two actions when in 
the indicated state, in order to maximize xploration 
and minimize data sparseness when constructing our 
model. Since there are 42 states with 2 choices 
each, there is a search space of 242 potential dia- 
logue strategies; the goal of the RL is to identify an 
apparently optimal strategy from this large search 
space. Note that due to the randomization of the 
EIC strategy, the prompts are designed to ensure 
the coherence of all possible action sequences. 
Figure 5 illustrates how the dialogue strategy in 
Figure 4 generates the dialogue in Figure 1. Each 
row indicates the state that NJFun is in, the ac- 
tion executed in this state, the corresponding turn 
in Figure 1, and the reward received. The initial 
state represents hat NJFun will first attempt to ob- 
tain attribute 1. NJFun executes GreetU (although 
as shown in Figure 4, Greets is also possible), gen- 
erating the first utterance in Figure 1. After the 
user's response, the next state represents that N J- 
Fun has now greeted the user and obtained the ac- 
tivity value with high confidence, by using a non- 
restrictive grammar. NJFun chooses not to confirm 
the activity, which causes the state to change but no 
prompt o be generated. The third state represents 
that NJFun is now working on the second attribute 
(location), that it already has this value with high 
confidence (location was obtained with activity af- 
ter the user's first utterance), and that the dialogue 
history is good. This time NJFun chooses to confirm 
the attribute with the second NJFun utterance, and 
the state changes again. The processing of time is 
similar to that of location, which leads NJFun to the 
final state, where it performs the action "Tell" (cor- 
State  
C V t g 
0 1 0 0 0 0 0 
1 1 0 0 1 0 0 
1 1 0 1 0 0 0 
1 1 0 1 0 1 0 
1 1 1 1 0 0 0 
1 1 1 1 0 1 0 
1 1 2 I 0 0 0 
1 1 2 1 0 1 0 
1 1 4 0 0 0 0 
1 1 4 0 1 0 
1 2 0 0 0 0 
1 2 0 0 0 0 
1 2 0 0 1 0 
1 2 0 0 1 0 
1 2 0 1 0 0 
1 2 0 1 0 0 
1 2 0 1 0 1 
I 2 0 I 0 I 
1 2 i 1 0 0 
1 2 1 1 0 0 
1 2 1 1 0 1 
1 2 1 1 0 1 
1 2 2 1 0 0 
1 2 2 1 0 0 
I 2 2 1 0-  1 
1 2 2 1 0 1 
I 2 4 0 0 0 
1 2 4 0 0 0 
1 2 4 0 I 0 
1 2 4 0 1 0 
1 3 0 1 0 0 
1 3 0 1 0 0 
1 3 0 1 0 1 
1 3 0 1 0 1 
1 3 1 1 0 0 
1 3 1 1 0 0 
1 3 1 1 0 1 
i 3 1 I 0 i 
1 3 2 1 0 0 
1 3 2 1 0 0 
1 3 2 1 0 1 
i 3 2 1 0 I 
Act ion  Choices  
GreetS ,GreetU  
ReAsk  1 S ,ReAsk  1 M 
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
NoConf ,ExpConf l  
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
ReAsk lS ,ReAsk lM 
0 ReAsk lS ,ReAsk lM 
0 - Ask2S,Ask2U 
1 Ask2S,Ask2U 
0 ReAsk2S,ReAsk2M 
1 ReAsk2S,ReAsk2  M 
0 NoConf ,  ExpConf2  
1 NoConf, ExpConf2  
0 NoConf, ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf, ExpConf2 
0 ReAsk2S,ReAsk2M 
1 ReAsk2S,ReAsk2M 
0 ReAsk2S,R .eAsk2M 
1 ReAsk2S,ReAskSM 
0 NoConf ,  ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf, ExpConf3 
I NoConf, ExpConf3 
0 NoConf, ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf ,  ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf ,ExpCon: f3  
1 NoConf ,ExpConf3  
0 NoConf ,  ExpConf3  
I NoConf, ExpConf3  
Figure 4: Exploratory portion of EIC strategy. 
State  Act ion Turn Reward  
gaevtgh  
0100000 GreetU S1 0 
I 121000 NoConf  0 
1 2 2 1 0 0 1 ExpConf2  $2 0 
1 3 2 1 0 0 1 ExpConf3  $3 0 
1 4 0 0 0 0 0 Tell S4 1 
Figure 5: Generating the dialogue in Figure 1. 
responding to querying the database, presenting the 
results to the user, and asking the user to provide a 
reward). Note that in NJFun, the reward is always 
0 except at the terminal state, as shown in the last 
column of Figure 5. 
3 Sample Dialogues 
Figures 1 and 6 present two dialogues that were gen- 
erated using different dialogue strategies in the EIC 
search space. For example, although NJFun begins 
both dialogues in the first state in Figure 4, NJFun 
executes GreetU in one dialogue and GreetS in the 
other. Similarly, although the next state of NJFun 
19 
SI: Welcome to N JFun.  P lease say an act iv i ty  name or say 'list act iv i t ies '  for  a list of act iv i t ies I know about .  
U I :  I would like to visit a h istor ic  site in S tanhope this  morn ing.  
(ASR output: I would like to the zoo historic sites in stanhope historic) 
$2: Did you say you are interested in going to a zoo?  
U2: No. 
$3: I know about  amusement  parks ,  aquar iums,  cruises, histor ic sites, museums,  parks,  theaters ,  wineries, and  zoos. 
P lease say an act iv i ty  name f rom this list. 
U3: I would like to visit a h istor ic  site. (ASR output: I would like to visit historic sites) 
$4: Did you say you are interested in going to a h istor ic  site? 
U4: Yes. 
$5: P lease give me more in format ion .  
U5: (user says nothing) 
S6: P lease tell me the locat ion that  you are interested in. You can also tell me the time. 
U6: Visit a histor ic .  (ASR output: What  historic) 
$7: What  t ime of  the day  do you want  to go? 
U7: S tanhope.  (no ASR output) 
$8: Do you want  to go in the morn ing ,  in the a f ternoon,  or  in the evening? 
US: Morn ing.  
S9: Did you say you want  to go in the morn ing?  
U9: Yes. 
S l0 :  I found 11 histor ic  sites in New Jersey that  are open in the morn ing .  The  first 3 \ [ . . .  \] Would you like to  hear  more?  
UiO: No. 
$11: Thank~ou for using the system.  P lease give me feedback by say ing 'good ' ,  'so-so',  or  'bad ' .  
U I I :  Bad'. _. 
Figure 6: An example training dialogue with NJFun. 
is the same in both dialogues ("1 1 2 1 0 0 0"), the 
activity is not confirmed in the first dialogue. 
4 Contributions 
The main contribution of this work is that we 
have developed and empirically validated a practi- 
cal methodology for using RL to build a real dia- 
logue system that optimizes its behavior from dia- 
logue data. Unlike traditional approaches to learn- 
ing dialogue strategy from data, which are limited 
to searching a handful of policies, our RL approach 
is able to search many tens of thousands of dialogue 
strategies. In particular, the traditional approach 
is to pick a handful of strategies that experts in- 
tuitively feel are good, implement each policy as a 
separate system, collect data from representative hu- 
man users for each system, and then use standard 
statistical tests on that data to pick the best sys- 
tem, e.g. (Danieli and Gerbino, 1995). In contrast, 
our use of RL allowed us to explore 242 strategies 
that were left in our search space after we excluded 
strategies that were clearly suboptimal. 
An empirical validation of our approach is de- 
tailed in two forthcoming technical papers (Singh 
et al, 2000; Litman et al, 2000). We obtained 311 
dialogues with the exploratory (i.e., training) ver- 
sion of NJFun, constructed an MDP from this train- 
ing data, used RL to compute the optimal dialogue 
strategy in this MDP, reimplemented NJFun such 
that it used this learned dialogue strategy, and ob- 
tained 124 more dialogues. Our main result was 
that task completion improved from 52% to 64% 
from training to test data. Furthermore, analysis 
of our MDP showed that the learned strategy was 
not only better than EIC, but also better than other 
fixed choices proposed in the literature (Singh et al, 
2000). 
5 Limitations 
The main limitation of this effort to automate the 
design of a good dialogue strategy is that our current 
framework has nothing to say about how to choose 
the reward measure, or how to best represent dia- 
logue state. In NJFun we carefully but manually de- 
signed the state space of the dialogue. In the future, 
we hope to develop a learning methodology to auto- 
mate the choice of state space for dialogue systems. 
With respect o the reward function, our empirical 
evaluation investigated the impact of using a number 
of reward measures (e.g., user feedback such as U4 in 
Figure 1, task completion rate, ASR accuracy), and 
found that some rewards worked better than others. 
We would like to better understand these differences 
among the reward measures, investigate the use of 
a learned reward function, and explore the use of 
non-terminal rewards. 
Re ferences  
M. Danieli and E. Gerbino. 1995. Metrics for eval- 
uating dialogue strategies in a spoken language 
system. In Proceedings of the 1995 AAA1 Spring 
Symposium on Empirical Methods in Discourse 
Interpretation and Generation, pages 34-39. 
D. Litman, M. Kearns, S. Singh, and M. Walker. 
2000. Automatic optimization of dialogue man- 
agement. Manuscript submitted for publication. 
S. Singh, M. Kearns, D. Litman, and M. Walker. 
2000. Empirical evaluation of a reinforcement 
learning spoken dialogue system. In Proceedings 
of AAAI 2000. 
R. S. Sutton and A. G. Barto. 1998. Reinforcement 
Learning: An Introduction. MIT Press. 
20 
Labeling Corrections and Aware Sites
in Spoken Dialogue Systems
Julia Hirschbergy and Marc Swertsz and Diane Litmany
y AT&T Labs{Research z IPO, Eindhoven, The Netherlands,
Florham Park, NJ, 07932 USA and CNTS, Antwerp, Belgium
fjulia/dianeg@research.att.com m.g.j.swerts@tue.nl
Abstract
This paper deals with user correc-
tions and aware sites of system er-
rors in the TOOT spoken dialogue
system. We rst describe our cor-
pus, and give details on our proced-
ure to label corrections and aware
sites. Then, we show that correc-
tions and aware sites exhibit some
prosodic and other properties which
set them apart from `normal' utter-
ances. It appears that some correc-
tion types, such as simple repeats,
are more likely to be correctly recog-
nized than other types, such as para-
phrases. We also present evidence
that system dialogue strategy aects
users' choice of correction type, sug-
gesting that strategy-specic meth-
ods of detecting or coaching users on
corrections may be useful. Aware
sites tend to be shorter than other
utterances, and are also more dif-
cult to recognize correctly for the
ASR system.
1 Introduction
Compared to many other systems, spoken
dialogue systems (SDS) tend to have more
diculties in correctly interpreting user in-
put. Whereas a car will normally go left if
the driver turns the steering wheel in that
direction or a vacuum cleaner will start work-
ing if one pushes the on-button, interactions
between a user and a spoken dialogue system
are often hampered by mismatches between
the action intended by the user and the action
executed by the system. Such mismatches
are mainly due to errors in the Automatic
Speech Recognition (ASR) and/or the Nat-
ural Language Understanding (NLU) com-
ponent of these systems. To solve these mis-
matches, users often have to put considerable
eort in trying to make it clear to the system
that there was a problem, and trying to cor-
rect it by re-entering misrecognized or misin-
terpreted information. Previous research has
already brought to light that it is not always
easy for users to determine whether their in-
tended actions were carried out correctly or
not, in particular when the dialogue system
does not give appropriate feedback about its
internal representation at the right moment.
In addition, users' corrections may miss their
goal, because corrections themselves are more
dicult for the system to recognize and in-
terpret correctly, which may lead to so-called
cyclic (or spiral) errors. That corrections
are dicult for ASR systems is generally ex-
plained by the fact that they tend to be hyper-
articulated | higher, louder, longer . . . than
other turns (Wade et al, 1992; Oviatt et al,
1996; Levow, 1998; Bell and Gustafson, 1999;
Shimojima et al, 1999), where ASR models
are not well adapted to handle this special
speaking style.
The current paper focuses on user correc-
tions, and looks at places where people rst
become aware of a system problem (\aware
sites"). In other papers (Swerts et al, 2000;
Hirschberg et al, 2001; Litman et al, 2001),
we have already given some descriptive stat-
istics on corrections and aware sites and we
have been looking at methods to automatic-
ally predict these two utterance categories.
One of our major ndings is that prosody,
which had already been shown to be a good
predictor of misrecognitions (Litman et al,
2000; Hirschberg et al, 2000), is also useful to
correctly classify corrections and aware sites.
In this paper, we will elaborate more on the
exact labeling scheme we used, and add fur-
ther descriptive statistics. More in particular,
we address the question whether there is much
variance in the way people react to system er-
rors, and if so, to what extent this variance
can be explained on the basis of particular
properties of the dialogue system. In the fol-
lowing section we rst provide details on the
TOOT corpus that we used for our analyses.
Then we give information on the labels for
corrections and aware sites, and on the actual
labeling procedure. The next section gives
the results of some descriptive statistics on
properties of corrections and aware sites and
on their distributions. We will end the paper
with a general discussion of our ndings.
2 The data
2.1 The TOOT corpus
Our corpus consists of dialogues between hu-
man subjects and TOOT, a spoken dialogue
system that allows access to train information
from the web via telephone. TOOT was col-
lected to study variations in dialogue strategy
and in user-adapted interaction (Litman and
Pan, 1999). It is implemented using an
IVR (interactive voice response) platform de-
veloped at AT&T, combining ASR and text-
to-speech with a phone interface (Kamm et
al., 1997). The system's speech recognizer is
a speaker-independent hidden Markov model
system with context-dependent phone models
for telephone speech and constrained gram-
mars dening vocabulary at any dialogue
state. The platform supports barge-in. Sub-
jects performed four tasks with one of several
versions of the system that diered in terms
of locus of initiative (system, user, or mixed),
conrmation strategy (explicit, implicit, or
none), and whether these conditions could
be changed by the user during the task (ad-
aptive vs. non-adaptive). TOOT's initiative
System Initiative, Explicit Conrmation
T: Which city do you want to go to?
U: Chicago.
S: Do you want to go to Chicago?
U: Yes.
User Initiative, No Conrmation
S: How may I help you?
U: I want to go to Chicago from Baltimore.
S: On which day of the week do you want
to leave?
U: I want a train at 8:00.
Mixed Initiative, Implicit Conrmation
S: How may I help you?
U: I want to go to Chicago.
S: I heard you say go to Chicago.
Which city do you want to leave from?
U: Baltimore.
Figure 1: Illustrations of various dialogue
strategies in TOOT
strategy species who has control of the dia-
logue, while TOOT's conrmation strategy
species how and whether TOOT lets the user
know what it just understood. The fragments
in Figure 1 provide some illustrations of how
dialogues vary with strategy. Subjects were
39 students; 20 native speakers and 19 non-
native, 16 female and 23 male. Dialogues
were recorded and system and user behavior
logged automatically. The concept accuracy
(CA) of each turn was manually labeled. If
the ASR correctly captured all task-related
information in the turn (e.g. time, departure
and arrival cities), the turn's CA score was
1 (semantically correct). Otherwise, the CA
score reected the percentage of correctly re-
cognized task information in the turn. The
dialogues were also transcribed and automat-
ically scored in comparison to the ASR re-
cognized string to produce a word error rate
(WER) for each turn. For the study described
below, we examined 2328 user turns (all user
input between two system inputs) from 152
dialogues.
2.2 Dening Corrections and Aware
Sites
To identify corrections
1
in the corpus two au-
thors independently labeled each turn as to
whether or not it constituted a correction of
a prior system failure (a rejection or CA er-
ror, which were the only system failure sub-
jects were aware of) and subsequently de-
cided upon a consensus label. Note that much
of the discrepancies between labels were due
to tiredness or incidental sloppiness of indi-
vidual annotators, rather than true disagree-
ment. Each turn labeled `correction' was fur-
ther classied as belonging to one of the fol-
lowing categories: REP (repetition, includ-
ing repetitions with dierences in pronunci-
ation or uency), PAR (paraphrase), ADD
(task-relevant content added, OMIT (content
omitted), and ADD/OMIT (content both ad-
ded and omitted). Repetitions were further
divided into repetitions with pronunciation
variation (PRON) (e.g. yes correcting yeah),
and repetitions where the correction was pro-
nounced using the same pronunciation as the
original turn, but this distinction was di-
cult to make and turned out not to be useful.
User turns which included both corrections
and other speech acts were so distinguished by
labeling them \2+". For user turns contain-
ing a correction plus one or more additional
dialogue acts, only the correction is used for
purposes of analysis below. We also labeled as
restarts user corrections which followed non-
initial system-initial prompts (e.g. \How may
I help you?" or \What city do you want to
go to?"); in such cases system and user es-
sentially started the dialogue over from the
beginning. Figure 2 shows examples of each
correction type and additional label for cor-
rections of system failures on I want to go
to Boston on Sunday. Note that the utter-
ance on the last line of this gure is labeled
2+PAR, given that this turn consist of two
speech acts: the goal of the no-part of this
1
The labels discussed in this section for corrections
and aware sites may well be related to more general
dialogue acts, like the ones proposed by (Allen and
Core, 1997), but this needs to be explored in more
detail in the future.
turn is to signal a problem, whereas the re-
mainder of this turn serves to correct a prior
error.
Corr Type Correction
REP I want to go to Boston on Sunday
PAR To Boston on Sunday
OMIT I want to go to Boston
ADD To Boston on Sunday at 8pm
ADD/ I want to arrive Sunday at 8pm
OMIT
2+PAR No, to Boston on Sunday
Figure 2: Examples of Correction Types
Each correction was also indexed with an
identier representing the closest prior turn
it was correcting, so that we could investigate
\chains" of corrections of a single failed turn,
by tracing back through subsequent correc-
tions of that turn. Figure 3 shows a fragment
of a TOOT dialogue with corrections labeled
as discussed above.
We also identied aware sites in our cor-
pus | turns where a user, while interact-
ing with a machine, rst becomes aware that
the system has misrecognized a previous user
turn. For our corpus, we tried to determ-
ine whether there was some evidence in the
user turn indicating that the user had be-
come aware of a mistake in the system's un-
derstanding of a previous user turn, and, if
so, which previous turn had occasioned that
error. Note that such aware sites may or
may not also be corrections (another type of
post-misrecognition turn), since a user may
not immediately provide correcting informa-
tion. Also, it may take a while before the
user is able to notice a system error. Fig-
ure 4 shows an example that illustrates cases
in which both the user's awareness and cor-
rection of a misrecognition occur in the same
turn (e.g. turns 1159 and 1160, after sys-
tem prompts for information already given in
turn 1158). It also illustrates cases in which
aware sites and corrections occur in dierent
turns. For example, after the immediate ex-
plicit system conrmation of turn 1162, the
user rst becomes aware of the system errors
Turn Turn ID Corr of Corr Type
S: On which day of the week do you want to leave?
S: Sorry, I didn't hear anything from you.
Please tell me your departure date.
U: Um tomorrow 776 NA NA
S: You can say the departure date. For example,
you can say today or tomorrow. Or you can say
Monday, Tuesday, Wednesday...
U: Tomorrow 777 776 REP
S: You can say the departure date. For example,
you can say today or tomorrow. Or you can say
Monday, Tuesday, Wednesday, Thursday, Friday...
U: Saturday 778 777 PAR
S: Do you want to leave on Saturday?
U: Yes 779 NA NA
S: What time do you want to leave?
U: Eight o'clock A M 780 NA NA
S: Do you want to leave around 10:00pm?
U: Eight o'clock 781 780 OMIT
S: Do you want to leave around 10:00pm?
U: No 782 NA NA
S: What time do you want to leave?
U: 8:00am 783 781 ADD
Figure 3: Toot Dialogue Fragment with Correction Labels
(turn 1163), then separately corrects them
(turn 1164). When no immediate conrm-
ation of an utterance occurs (as with turn
1158), it may take several turns before the
user becomes aware of any misrecognition er-
rors. For example, it is not until turn 1161
that the user rst becomes aware of the er-
ror in date and time from 1158; the user then
corrects the error in 1162. Of all turns in
our corpus, 13% represent cases of turns that
are only corrections, 14% are only aware sites,
and 16% are turns where aware sites and cor-
rections co-occur. Also, note that turns 1162
and 1164 in this dialogue fragment represent
cases of restarts after a system-initial, non-
initial prompt (\How may I help you?").
3 Characteristics of corrections
and aware sites in TOOT
3.1 Corrections
We identied 29% of the turns in the corpus
as corrections. These were a serious prob-
lem for the system, being misrecognized in
terms of transcription error about twice as
often (61%) as non-corrections (33%) and in
terms of concept error in 53% of cases for cor-
rections compared with 21% concept error for
non-corrections.
Corrections diered from non-corrections
in other ways as well, which are suggest-
ive of how they might be identied by an
SDS. We have previously discussed the pros-
odic and acoustic dierences between correc-
tions and non-correction (Swerts et al, 2000;
Hirschberg et al, 2001). In particular, in
line with previous studies, we found that cor-
rections more often tend to be hyperarticu-
lated (longer, louder, with great pitch excur-
sions,...) than other turns. More recently we
have also discovered dierences in other di-
mensions.
First, corrections appear to be longer than
non-corrections in numbers of words, when
we examine actual human transcriptions of
Turn Turn ID Aware of Corr of
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1158 NA NA
S: Which city do you want to go to?
U: New York City 1159 1158 1158
S: Which city do you want to leave from?
U: Baltimore 1160 1158 1158
S: Do you want me to nd the trains from
Baltimore to New York City today at anytime now?
U: No 1161 1158 NA
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1162 NA 1158
S: Do you want me to nd the trains from
Baltimore to New York City today at anytime now?
U: No 1163 1162 NA
S: How may I help you?
U: Get me the train on Sunday at eight thirty pm 1164 NA 1162
Figure 4: Dialogue Fragment with Aware and Correction Labels.
them, both in absolute terms (T=17.68;
df=2326; p=0) and also controlling for
speaker (T=5.32; df=38; p=0). Even the
ASR hypotheses show this dierence, with
hypotheses of corrections being longer in ab-
solute terms (T=13.72; df=2326; p=0) and
across speakers (T=5.18; df=38; p=0).
Of the correction types we labeled, the
largest number were REPs and OMITs, as
shown in Table 1, which shows over-all dis-
tribution of correction types, and distribu-
tions for each type of system failure corrected.
Table 1 shows that 39% of TOOT corrections
were simple repetitions of the previously mis-
recognized turn. While this strategy is often
suboptimal in correcting ASR errors (Levow,
1998), REPs (45% error) and OMITs (52% er-
ror) were better recognized than ADDs (90%
error) and PARs (72% error). Thus, over-
all, users tend to have a preference for correc-
tion types that are more likely to be succes-
ful. That REPs and OMITs are more often
correctly recognized can be linked to the ob-
servation that they tend to be realized with
prosody which is less marked than the pros-
ody on ADDs and PARs. Table 2 shows that
REPs and OMITs are closer to normal utter-
ances in terms of their prosodic features than
ADDs, which are considerably higher, longer
and slower. This is in line with our previous
observations that marked settings for these
prosodic features more often lead to recogni-
tion errors.
What the user was correcting also inu-
enced the type of correction chosen. Table
1 shows that corrections of misrecognitions
(Post-Mrec) were more likely to omit inform-
ation present in the original turn (OMITs),
while corrections of rejections (Post-Rej) were
more likely to be simple repetitions. The
latter nding is not surprising, since the re-
jection message for tasks was always a close
paraphrase of \Sorry, I can't understand
you. Can you please repeat your utterance?"
However, it does suggest the surprising power
of system directions, and how important it is
to craft prompts to favor the type of correc-
tion most easily recognized by the system.
Corrections following system restarts
diered in type somewhat from other correc-
tions, with more turns adding new material
to the correction and fewer of them repeating
ADD ADD/OMIT OMIT PAR REP
All 8% 2% 32% 19% 39%
% Mrec(WER) 90% 93% 52% 72% 45%
% Mrec(CA) 88% 71% 47% 65% 45%
Post-Mrec 7% 3% 40% 18% 32%
Post-Rej 6% 0% 7% 28% 59%
Table 1: Distribution of Correction Types
Feature Normal ADD ADD/OMIT OMIT PAR REP
F0max (Hz) 219.4 286.3 252.9 236.7 252.1 239.9
rmsmax 1495.0 1868.1 2646.3 1698.0 1852.4 2024.6
dur (s) 1.4 6.8 4.1 2.3 4.7 2.5
tempo (sylls/s) 2.5 1.7 1.6 2.9 2.1 2.3
Table 2: Averages for dierent prosodic features of dierent Correction Types
the original turn.
Dialogue strategy clearly aected the type
of correction users made. For example, users
more frequently repeat their misrecognized
utterance in the SystemExplicit condition,
than in the MixedImplicit or UserNoConrm;
the latter conditions have larger proportions
of OMITs and ADDs. This is an important
observation given that this suggests that some
dialogue strategies lead to correction types,
such as ADDs, which are more likely to be
misrecognized than correction types elicited
by other strategies.
As noted above, corrections in the TOOT
corpus often take the form of chains of correc-
tions of a single original error. Looking back
at Figure 3, for example, we see two chains
of corrections: In the rst, which begins with
the misrecognition of turn 776 (\Um, tomor-
row"), the user repeats the original phrase
and then provides a paraphrase (\Saturday"),
which is correctly recognized. In the second,
beginning with turn 780, the time of depar-
ture is misrecognized. The user omits some
information (\am") in turn 781, but without
success; an ADD correction follows, with the
previously omitted information restored, in
turn 783. Elsewhere (Swerts et al 2000),
we have shown that chain position has an in-
uence on correction behaviour in the sense
that more distant corrections tend to be mis-
recognized more often than corrections closer
to the original error.
3.2 Aware Sites
708 (30%) of the turns in our corpus were
labeled aware sites. The majority of these
turns (89%) immediately follow the system
failures they react to, unlike the more com-
plex cases in Figure 4 above. If a system
would be able to detect aware sites with a
reasonable accuracy, this would be useful,
given that the system would then be able to
correctly guess in the majority of the cases
that the problem occurred in the preceding
turn. Aware turns, like corrections, tend to
be misrecognized at a higher rate than other
turns; in terms of transcription accuracy, 50%
of awares are misrecognized vs. 35% of other
turns, and in terms of concept accuracy, 39%
of awares are misrecognized compared to 27%
of other turns. In other words, both types
of post-error utterances, i.e., corrections and
aware sites, share the fact that they tend to
lead to additional errors. But whereas we
have shown above that for corrections this is
probably caused by the fact that these utter-
ances are uttered in a hyperarticulated speak-
ing style, we do not nd dierences in hyper-
articulation between aware sites and `normal
utterances' (T= 0.9085; df=38; p=0.3693).
This could mean that these sites are real-
ized in a speaking style which is not per-
ceptibly dierent from normal speaking style
ADD ADD/OMIT OMIT PAR REP
MixedExplicit 1 0 4 1 4
MixedImplicit 16 8 58 44 64
MixedNoConrm 0 0 2 0 1
SystemExplicit 2 2 8 31 67
SystemImplicit 0 1 18 0 20
SystemNoConrm 0 0 5 0 4
UserExplicit 0 0 0 1 1
UserImplicit 1 0 4 3 6
UserNoConrm 31 3 116 47 98
Table 3: Number of Correction Types for dierent dialogue strategies
Single no Other Turns
Aware site 162 546
Not Aware site 122 1498
Table 4: Distribution of single no utterances
and other turns for aware sites versus other
utterances
when judged by human labelers, but which
is still suciently dierent to cause problems
for an ASR system.
In terms of distinguishing features which
might explain or help to identify these turns,
we have previously examined the acoustic
and prosodic features of aware sites (Lit-
man et al, 2001). Here we present some
additional features. Aware sites appear to
be signicantly shorter, in general, than
other turns, both in absolute terms and con-
trolling for speaker variation, and whether
we examine the ASR transcription (absolute:
T=4.86; df=2326; p=0; speaker-controlled:
T=5.37; df=38; p=0) or the human one (ab-
solute: T=3.45; df=2326; p<.0001; speaker-
controlled: T=4.69; df=38; p=0). A sizable
but not overwhelming number of aware sites
in fact consist of a simple negation (i.e., a vari-
ant of the word `no') (see Table 4). This at
the same time shows that a simple no-detector
will not be sucient as an indicator of aware
sites (see also (Krahmer et al, 1999; Krahmer
et al, to appear)), given that most aware sites
are more complex than that, such as turns
1159 and 1160 in the example of Figure 4.
More concretely, Table 4 shows that a single
no would correctly predict that the turn is an
aware site with a precision of only 57% and a
recall of only 23%.
4 Discussion
This paper has dealt with user corrections and
aware sites of system errors in the TOOT
spoken dialogue system. We have described
our corpus, and have given details on our pro-
cedure to label corrections and aware sites.
Then, we have shown that corrections and
aware sites exhibit some prosodic and other
properties which set them apart from `normal'
utterances. It appears that some correction
types, such as simple repeats, are more likely
to be correctly recognized than other types,
such as paraphrases. We have also presen-
ted evidence that system dialogue strategy
aects users' choice of correction type, sug-
gesting that strategy-specic methods of de-
tecting or coaching users on corrections may
be useful. Aware sites tend to be shorter than
other utterances, and are also more dicult
to recognize correctly for the ASR system.
In addition to the descriptive study presen-
ted in this paper, we have also tried to auto-
matically predict corrections and aware sites
using the machine learning program RIP-
PER (Cohen, 1996). These experiments show
that corrections and aware sites can be clas-
sied as such automatically, with a consider-
able degree of accuracy (Litman et al, 2001;
Hirschberg et al, 2001). Such classication,
we believe, will be especially useful in error-
handling for SDS. If aware sites are detect-
able, they can function as backward-looking
error-signaling devices, making it clear to the
system that something has gone wrong in
the preceding context, so that, for example,
the system can reprompt for information. In
this way, they are similar to what others
have termed `go-back' signals (Krahmer et
al., 1999). Aware sites can also be used as
forward-looking signals, indicating upcoming
corrections or more drastic changes in user be-
havior, such as complete restarts of the task.
Given that, in current systems, both correc-
tions and restarts often lead to recognition er-
ror (Swerts et al, 2000), aware sites may be
useful in preparing systems to deal with such
problems. An accurate detection of turns that
are corrections may trigger the use of specially
trained ASR models to better recognize cor-
rections, or can be used to change dialogue
strategy (e.g. from user or mixed initiative to
system initiative after errors).
References
J. Allen and M. Core. 1997. Dialogue markup in
several layers. Draft contribution for the Dis-
course Resource Initiative.
L. Bell and J. Gustafson. 1999. Repetition
and its phonetic realizations: Investigating a
Swedish database of spontaneous computer-
directed speech. In Proceedings of ICPhS-99,
San Francisco. International Congress of Phon-
etic Sciences.
W. Cohen. 1996. Learning trees and rules with
set-valued features. In 14th Conference of the
American Association of Articial Intelligence,
AAAI.
J. Hirschberg, D. Litman, and M. Swerts. 2000.
Generalizing prosodic prediction of speech re-
cognition errors. In Proceedings of the Sixth
International Conference on Spoken Language
Processing, Beijing.
J. Hirschberg, D. Litman, and M. Swerts. 2001.
Identifying user corrections automatically in
spoken dialogue systems. In Proceedings of
NAACL-2001, Pittsburgh.
C. Kamm, S. Narayanan, D. Dutton, and R.
Ritenour. 1997. Evaluating spoken dialogue
systems for telecommunication services. In
Proc. EUROSPEECH-97, Rhodes.
E. Krahmer, M. Swerts, M. Theune, and M. Wee-
gels. 1999. Error spotting in human-
machine interactions. In Proceedings of
EUROSPEECH-99.
E. Krahmer, M. Swerts, M. Theune, and M. Wee-
gels. to appear. The dual of denial: Two uses
of disconrmations in dialogue and their pros-
odic correlates. Accepted for Speech Commu-
nication.
G. Levow. 1998. Characterizing and recogniz-
ing spoken corrections in human-computer dia-
logue. In Proceedings of the 36th Annual Meet-
ing of the Association of Computational Lin-
guistics, COLING/ACL 98, pages 736{742.
D. Litman, J. Hirschberg, and M. Swerts. 2000.
Predicting automatic speech recognition per-
formance using prosodic cues. In Proceedings
of NAACL-00, Seattle, May.
D. Litman, J. Hirschberg, and M. Swerts. 2001.
Predicting User Reactions to System Error. In
Proceedings of ACL-01, Toulouse, July.
D. Litman and S. Pan. 1999. Empirically eval-
uating an adaptable spoken dialogue system.
In Proceedings tth International conference on
User Modeling.
S. L. Oviatt, G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate
speech during human-computer error resolu-
tion. In Proceedings of ICSLP-96, pages 801{
804, Philadelphia.
A. Shimojima, K. Katagiri, H. Koiso, and
M. Swerts. 1999. An experimental study on
the informational and grounding functions of
prosodic features of Japanese echoic responses.
In Proceedings of the ESCA Workshop on Dia-
logue and Prosody, pages 187{192, Veldhoven.
M. Swerts, D. Litman, and J. Hirschberg. 2000.
Corrections in spoken dialogue systems. In Pro-
ceedings of the Sixth International Conference
on Spoken Language Processing, Beijing.
E. Wade, E. E. Shriberg, and P. J. Price. 1992.
User behaviors aecting speech recognition. In
Proceedings of ICSLP-92, volume 2, pages 995{
998, Ban.
Using Reinforcement Learning to Build a Better Model of Dialogue State
Joel R. Tetreault
University of Pittsburgh
Learning Research and Development Center
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Diane J. Litman
University of Pittsburgh
Department of Computer Science &
Learning Research and Development Center
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
Given the growing complexity of tasks
that spoken dialogue systems are trying to
handle, Reinforcement Learning (RL) has
been increasingly used as a way of au-
tomatically learning the best policy for a
system to make. While most work has
focused on generating better policies for
a dialogue manager, very little work has
been done in using RL to construct a better
dialogue state. This paper presents a RL
approach for determining what dialogue
features are important to a spoken dia-
logue tutoring system. Our experiments
show that incorporating dialogue factors
such as dialogue acts, emotion, repeated
concepts and performance play a signifi-
cant role in tutoring and should be taken
into account when designing dialogue sys-
tems.
1 Introduction
This paper presents initial research toward the
long-term goal of designing a tutoring system that
can effectively adapt to the student. While most
work in Markov Decision Processes (MDPs) and
spoken dialogue have focused on building better
policies (Walker, 2000; Henderson et al, 2005), to
date very little empirical work has tested the utility
of adding specialized features to construct a better
dialogue state. We wish to show that adding more
complex factors to a representation of student state
is a worthwhile pursuit, since it alters what action
the tutor should make. The five dialogue factors
we explore are dialogue acts, certainty level, frus-
tration level, concept repetition, and student per-
formance. All five are factors that are not just
unique to the tutoring domain but are important
to dialogue systems in general. Our results show
that using these features, combined with the com-
mon baseline of student correctness, leads to a sig-
nificant change in the policies produced, and thus
should be taken into account when designing a
system.
2 Background
We follow past lines of research (such as (Singh
et al, 1999)) for describing a dialogue   as a tra-
jectory within a Markov Decision Process (Sutton
and Barto, 1998). A MDP has four main com-
ponents: states, actions, a policy, which specifies
what is the best action to take in a state, and a re-
ward function which specifies the utility of each
state and the process as a whole. Dialogue man-
agement is easily described using a MDP because
one can consider the actions as actions made by
the system, the state as the dialogue context, and
a reward which for many dialogue systems tends
to be task completion success or dialogue length.
Typically the state is viewed as a vector of features
such as dialogue history, speech recognition con-
fidence, etc.
The goal of using MDPs is to determine the best
policy  for a certain state and action space. That
is, we wish to find the best combination of states
and actions to maximize the reward at the end of
the dialogue. In most dialogues, the exact reward
for each state is not known immediately, in fact,
usually only the final reward is known at the end
of the dialogue. As long as we have a reward func-
tion, Reinforcement Learning allows one to auto-
matically compute the best policy. The following
recursive equation gives us a way of calculating
the expected cumulative value (V-value) of a state
 (-value):
289
 

	

	
		
 

	
		
Characterizing and Predicting Corrections
in Spoken Dialogue Systems
Diane Litman?
University of Pittsburgh
Julia Hirschberg?
Columbia University
Marc Swerts?
Tilburg University
This article focuses on the analysis and prediction of corrections, defined as turns where a
user tries to correct a prior error made by a spoken dialogue system. We describe our labeling
procedure of various corrections types and statistical analyses of their features in a corpus
collected from a train information spoken dialogue system. We then present results of machine-
learning experiments designed to identify user corrections of speech recognition errors. We
investigate the predictive power of features automatically computable from the prosody of the
turn, the speech recognition process, experimental conditions, and the dialogue history. Our
best-performing features reduce classification error from baselines of 25.70?28.99% to 15.72%.
1. Introduction
Compared to many other systems, spoken dialogue systems (SDS) tend to have more
difficulties in correctly interpreting user input. Whereas a car normally goes left if the
driver turns the steering wheel in that direction or a vacuum cleaner starts working if
one pushes the on button, interactions between a user and a spoken dialogue system
are often hampered by mismatches between the action intended by the user and the
action executed by the system. Such mismatches are mainly due to errors in the Auto-
matic Speech Recognition (ASR) and/or the Natural Language Understanding (NLU)
component of these systems; they can also be due to wrong default assumptions of the
system or the fact that a user asks out-of-topic questions for which the system was not
designed. To solve these mismatches, users often have to put considerable effort into
trying to make it clear to the system that there was a problem, and trying to correct it by
reentering misrecognized or misinterpreted information. Previous research has already
brought to light that it is not always easy for users to determine whether their intended
actions were carried out correctly or not, in particular when the dialogue system does
not give appropriate feedback about its internal representation at the right moment.
In addition, users? corrections may miss their goal because corrections themselves are
more difficult for the system to recognize and interpret correctly, which may lead to
so-called cyclic (or spiral) errors.
? E-mail: litman@cs.pitt.edu.
? E-mail: julia@cs.columbia.edu.
? E-mail: m.g.j.swerts@uvt.nl.
Submission received: 12 January 2005; revised submission received: 3 April 2006; accepted for publication:
4 May 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 3
Given that current spoken dialogue systems are not sufficiently robust, there is
need for sophisticated error-handling strategies to gracefully solve communication
problems between the system and the user. Ideally, apart from strategies to prevent
errors, error handling would consist of steps to immediately detect an error when
it occurs and to interact with the user to correct the error in subsequent exchanges.
To date, attempts to improve system performance have largely focused on improv-
ing ASR accuracy or simplifying the task, either by further constraining the domain
and functionality of the system or by further restricting the vocabulary the system
must recognize. Such studies include work on improved acoustic and semantic con-
fidence scores (Ammicht, Potamianos, and Fosler-Lussier 2001; Andorno, Laface, and
Gemello 2002; Bouwman, Sturm, and Boves 1999; Falavigna, Gretter, and Riccardi
2002; Guillevic, Gandrabur, and Normandin 2002; Moreno, Logan, and Raj 2001; Wang
and Lin 2002; Zhang and Rudnicky 2001), on new system architectures for error han-
dling (McTear et al 2005; Prodanov and Drygajlo 2005; Torres et al 2005), on new
interfaces that are more user-friendly for error recovery (Bulyko et al 2005; Karsenty
and Botherel 2005; Sturm and Boves 2005), and on the use of error-recovery strategies
that are based on analyses of human?human dialogues (Skantze 2005), including the
use of facial expressions (Barkhuysen, Krahmer, and Swerts 2005).
However, as ASR accuracy improves, dialogue systems will be called upon to
handle ever more complex tasks and ever less restricted vocabularies. So, it seems likely
that spoken dialogue systems will, for the foreseeable future, always require effective
error detection and repair strategies. In previous research (Hirschberg, Litman, and
Swerts 1999, 2004), we identified new procedures to detect recognition errors, which
perform well when tested on two different corpora, the TOOT and W99 corpora (train
information and conference registration dialogues) collected using two different ASR
systems (Sharp et al 1997; Kamm et al 1997). We found that prosodic features, in
combination with information already available to the recognizer, such as acoustic
confidence scores, grammar, and recognized string, can distinguish speaker turns that
are misrecognized far better than traditional methods for ASR rejection (the system
decision that its hypothesis is so weak that it should reprompt for fresh input), which
use acoustic confidence scores alone. Related work has been done by Lendvai (2004)
and Batliner et al (2003). In the current study, we turn to the question of how people try
to correct ASR errors in their interactions with machines and the role that prosody may
play in identifying user corrections and in helping to analyze them.
Understanding how users attempt to correct system failures and why their attempts
succeed or fail is important to improve the design of future spoken dialogue systems.
For example, knowing whether they are more likely to repeat or rephrase their utter-
ances, add new information or shorten their input, and how system behavior influences
these choices can suggest appropriate on-line modifications to the system?s interaction
strategy or to the recognition procedure it employs. Determining which speaker behav-
iors are more successful in correcting system errors can also lead to improvements in the
help information such systems provide. There is growing evidence that there is much
variance in the way people react to system errors and that the variance can be explained
on the basis of particular properties of the dialogue system or the dialogue context. In
particular, dialogue confirmation strategies may hinder users? ability to correct system
error. For instance, if a system wrongly presents information as being correct, as when it
verifies information implicitly, users become confused about how to respond (Krahmer
et al 2001). Other studies have shown that speakers tend to switch to a prosodically
?marked? speaking style after communication errors, comparing repetition corrections
with the speech being repeated (Wade, Shriberg, and Price 1992; Oviatt et al 1996;
418
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Levow 1998; Bell and Gustafson 1999). Although this speaking style may be effective in
problematic human?human communicative settings, there is evidence that suggests it
leads to further errors in human?machine interactions (Levow 1998; Soltau and Waibel
2000). That corrections are difficult for ASR systems is generally explained by the fact
that they tend to be hyperarticulated?higher, louder, longer?than other turns (Wade,
Shriberg, and Price 1992; Oviatt et al 1996; Levow 1998; Bell and Gustafson 1999;
Shimojima, et al 2001; Soltau and Waibel 1998, 2000; Soltau, Metze, and Waibel 2002),
where ASR models are not well adapted to handle this special speaking style, although
recent studies suggest that ASR systems are becoming less vulnerable to hyperarticula-
tion (Bulyko et al 2005).
So, repair strategies in human?machine interactions can be more or less effective.
Therefore, increased knowledge about the efficiency of different correction strategies
can lead to a number of possible courses of action. System strategy might be chosen to
favor the type(s) of correction the system can most easily process. Or, having chosen a
particular interaction strategy, the system repair strategy might be tuned to handle the
correction types that that strategy is likely to produce. Alternatively, the system?s dia-
logue manager might use the detection of corrections as a signal that it should modify its
interaction strategy, either locally, by beginning a subdialogue for faster error recovery,
or globally, by changing its initiative or confirmation strategies, or even directing the
user to a human operator. Or, since corrections are often hyperarticulated, detection of
a correction could serve as a signal to the ASR engine to run a recognizer trained on
hyperarticulated speech in parallel with its normal processor, to better transcribe the
speech. All of these possibilities, however, assume that user corrections can be detected
by the system reliably during the dialogue.
In this article, we describe an analysis of user corrections of system error collected in
the TOOT spoken dialogue system. In the next section, we describe the corpus itself and
how it was collected and labeled. The corpus is suitable to gain insight into the different
correction strategies that speakers exploit in different dialogue contexts and interaction
styles. Then, we characterize the nature of corrections in this corpus in terms of when
they occur, how well they are handled by the system, what distinguishes their prosody
from other utterances, their relationship to the utterances they correct, and how they
differ according to dialogue strategy. Then we present results of some machine-learning
experiments designed to automatically distinguish corrections from other user input,
using features that we derived as potentially useful from our descriptive analyses.
2. The Data
2.1 The TOOT Corpus
Our corpus consists of dialogues between human subjects and TOOT, a spoken dialogue
system that allows access to train information from the Web via telephone. TOOT
was collected to study variations in dialogue strategy and in user-adapted interac-
tion (Litman and Pan 1999). It is implemented using an interactive voice response
(IVR) platform developed at AT&T, combining ASR and text-to-speech with a phone
interface (Kamm et al 1997). The system?s speech recognizer is a speaker-independent,
hidden Markov model system with context-dependent phone models for telephone
speech and constrained (rule-based) grammars defining vocabulary at any dialogue
state. Whereas a ?universal? grammar specifying all legal utterances was used at some
points in the dialogue, seven smaller grammars were also used at many points in the
dialogue (e.g., to recognize city names, days of the week, answers to yes/no questions,
419
Computational Linguistics Volume 32, Number 3
etc.). Grammars were only written for originally expected answers; in other words,
no specific grammar for corrections was built in.1 Confidence scores for recognition
were available only at the turn level and were based on acoustic likelihoods; thresh-
olds for rejecting an utterance based on confidence scores were specified manually by
the system designers and were set differently for different grammars. The platform
supports barge-in. Subjects performed four tasks with one of several versions of the
system that differed in terms of locus of initiative (system, user, or mixed), confirmation
strategy (explicit, implicit, or none), and whether these conditions could be changed
by the user during the task (adaptive vs. non-adaptive). In the adaptive version of
the system, users were allowed to say change strategy at any point(s) in the dialogue.
TOOT would then ask the user to specify new initiative and confirmation strategies,
for example, You are using the no confirmation strategy. Which confirmation strategy do
you want to change to? No confirmation, implicit confirmation, or explicit confirmation?
TOOT?s initiative strategy specifies who has control of the dialogue, whereas TOOT?s
confirmation strategy specifies how and whether TOOT lets the user know what it just
understood. The fragments in Figure 1 provide some illustrations of how dialogues
vary with strategy. For example, in user initiative mode, the system allows the user
to specify any number of attributes in a single utterance. Thus, the system will let the
user ignore specific questions. In the example in Figure 1, although the system asks for
the day of the week, the user answers with the time, which can be recognized due to the
use of the ?universal grammar.? In contrast, in both mixed and system initiative mode,
when a specific question is asked, one of the restricted grammars is used to recognize
the response. Finally, in universal and mixed but not system initiative mode, the sys-
tem can ask both specific questions and open-ended questions (e.g., How may I help
you?). Subjects were 39 students: 20 native speakers and 19 non-native, 16 women
and 23 men. Dialogues were recorded and system and user behavior were logged
automatically. The concept accuracy (CA) of each turn was manually labeled. If the
ASR correctly captured all task-related information in the turn (e.g., time, departure,
and arrival cities), the turn?s CA score was 1 (semantically correct). Otherwise, the CA
score reflected the percentage of correctly recognized task information in the turn. The
dialogues were also transcribed and automatically scored in comparison to the ASR
recognized string (the best hypothesis output by the ASR engine) to produce a word
error rate (WER) for each turn. For the study described below, we examined 2,328 user
turns (all user input between two system inputs) from 152 dialogues.
2.2 Labeling
To identify corrections in the corpus two authors independently labeled each turn as
to whether or not it constituted a correction of a prior system failure (a rejection or CA
error, which were the only system failure subjects were aware of) and subsequently
decided upon a consensus label. Note that many of the discrepancies between labels
were due to tiredness or incidental sloppiness of individual annotators, rather than true
disagreement. Each turn labeled ?correction? was further classified as belonging to one
of the following categories: REP (repetition, including repetitions with differences in
pronunciation or fluency), PAR (paraphrase), ADD (task-relevant content added), OMIT
1 Thus, if the system prompted for a single city, but the user also included a correction of a prior utterance
(e.g., No, at 10:30 p.m. I want to go to New York City), the turn would be out of grammar. Coding our
corpus for out of vocabulary turns and examining whether corrections are more likely to be out of
grammar is an area for future work.
420
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Figure 1
Illustrations of various dialogue strategies in TOOT.
(content omitted), and ADD/OMIT (content both added and omitted). Repetitions
were further divided into repetitions with pronunciation variation (PRON) (e.g., yes
correcting yeah) and repetitions where the correction was pronounced using the same
pronunciation as the original turn, but this distinction was difficult to make and turned
out not to be useful. User turns that included both corrections and other speech acts
were so distinguished by labeling them ?2+.? For example, the turn I would like to go
to Chicago from Baltimore change strategies system contains not only an ADD correction,
but also a request to adapt the system?s dialogue strategies, followed by an inform
of the desired initiative value. As another example, the turn yes help contains a REP
correction, followed by a request for help. For user turns containing a correction plus
one or more additional dialogue acts, only the correction is used for purposes of analysis
below. We also labeled as ?restarts? user corrections that followed non-initial system-
initial prompts (e.g., How may I help you? or What city do you want to go to?); in such
cases, system and user essentially started the dialogue over from the beginning.2 Table 1
shows examples of each correction type and additional label for corrections of system
failures on I want to go to Boston on Sunday. Note that the utterance on the last line of this
figure is labeled 2+PAR, given that this turn consists of two speech acts: The goal of the
no-part of this turn is to signal a problem, whereas the remainder of this turn serves to
correct a prior error.
Each correction was also indexed with an identifier representing the closest prior
turn it was correcting, so that we could investigate ?chains? of corrections of a single
failed turn by tracing back through subsequent corrections of that turn. Figure 2 shows
a fragment of a TOOT dialogue with corrections labeled as discussed above.
3. Descriptive Analyses
This section presents the results of some descriptive analyses of the corrections we
labeled in the TOOT corpus. We provide data on the distribution of different correction
2 Restarts occurred when either the user said the phrase I?m done here at any point in the dialogue, or
answered no to the system?s request to perform a database query (e.g., Do you want me to find the trains
from Baltimore to Chicago on Tuesday around 8:45 now?).
421
Computational Linguistics Volume 32, Number 3
Table 1
Example corrections of I want to go to Boston on Sunday.
Corr Type Correction
REP I want to go to Boston on Sunday
PAR To Boston on Sunday
OMIT I want to go to Boston
ADD To Boston on Sunday at 8 p.m.
ADD/OMIT I want to arrive Sunday at 8 p.m.
2+PAR No, to Boston on Sunday
types, prosodic features of corrections, characteristics of correction chains, and variation
in features of corrections as a function of dialogue strategy.
3.1 Correction Types
Of the correction types we labeled, the largest numbers were REPs and OMITs, as
shown in Table 2, which shows overall distribution of correction types, and distri-
butions for each type of system failure corrected, following either a misrecognized
turn (with respect to concept accuracy) (Post-Misrec) or a rejected turn (Post-Rej) or
correcting an earlier system failure (Non-Immed). (The last group includes corrections
of earlier utterances that do not immediately follow a rejection or misrecognition.)
Table 2 shows that 39% of TOOT corrections were simple repetitions of a previously
rejected or misrecognized turn. Although this strategy is often suboptimal in correcting
ASR errors (Levow 1998), REPs (45% WER error) and OMITs (52% error) were better
recognized than ADDs (90% WER error) and PARs (72% WER error).
Figure 2
Toot dialogue fragment with correction labels.
422
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Table 2
Distribution of correction types.
Type ADD ADD/OMIT OMIT PAR REP N
Total 51 8% 14 2% 215 32% 127 19% 265 39% 672
Post-Misrec 39 7% 13 3% 203 40% 90 18% 173 32% 518
Post-Rej 8 6% 0 0% 9 7% 36 28% 75 59% 128
Non-Immed 4 15% 1 4% 3 12% 1 4% 17 65% 26
There was no significant difference either in the number of corrections produced
(? = 2.44, p = .12) or in correction type (?2 = 5.07, p = .28) between our native speaker
subjects and non-native speakers. However, what the user was correcting did influence
the type of correction chosen. Table 2 shows that corrections of misrecognitions (Post-
Misrec) were more likely to omit information present in the original turn (OMITs),
whereas corrections of rejections (Post-Rej) were more likely to be simple repetitions.
The latter finding is not surprising because the rejection message for tasks was always
a close paraphrase of Sorry, I can?t understand you. Can you please repeat your utterance?
However, it does suggest the surprising power of system directions and how impor-
tant it is to craft prompts to favor the type of correction most easily recognized by
the system.
3.2 Prosodic Features of Corrections
In part to test the hypothesis that corrections tend to be hyperarticulated (slower and
louder speech that contains wider pitch excursions and more internal silence), we
examined the following features for each user turn: maximum and mean fundamental
frequency values (f0 Max, f0 Mean); maximum and mean energy values (RMS Max,
RMS Mean); total duration; length of pause preceding the turn (Prior Pause); speaking
rate (Tempo), calculated in syllables per second (sps); and amount of silence within
the turn (% Silence).3 f0 and RMS values, representing measures of pitch excursion
and loudness, were calculated from the output of Entropic Research Laboratory?s pitch
tracker, get f0 (Talkin 1995), with no postcorrection. Timing variation was represented
by four features: Duration of turn and length of pause between turns was hand labeled.
Speaking rate was approximated in terms of syllables in the recognized string per
second. % Silence was defined as the percentage of zero frames in the turn, calculated
from the pitch track; this feature approximates the percentage of time within the turn
that the speaker was silent.
To ensure that our results were speaker independent, we calculated mean val-
ues for each speaker?s corrections and non-corrections for every feature. Then, for
each feature, we created vectors of speaker means for correction and non-correction
turns and performed paired t tests on the paired vectors. For example, for the feature
?f0 Max,? we calculated mean maxima for correction turns and for non-corrections
for each of our thirty-nine speakers. We then performed a paired t test on these
3 Although the features were automatically computed, turn beginnings and endings were hand segmented
in dialogue-level speech files, as the turn-level files created by TOOT were not available. Because of some
system/user overlap in the recordings, we were able to calculate prosodic features for only 1,975 user
turns.
423
Computational Linguistics Volume 32, Number 3
thirty-nine pairs of means to derive speaker-independent results for differences in
f0 maxima between corrections and non-corrections. Note, however, that there were
overall differences in the corrections produced by native and non-native speakers,
normalized by value of first turn in task: mean f0 was higher for native speakers than
for non-native speakers (t stat = ?2.72, df = 602, p = .0067), tempo was faster (t stat =
?3.18, df = 670, p = .0015), and duration was shorter (t stat = 2.20, df = 670, p = .028).
These differences do not occur in non-correction utterances.
Our results provide some explanation for why corrections are more poorly re-
cognized than non-corrections because they indicate that corrections are indeed
characterized by prosodic features associated with hyperarticulation. Table 3 shows
that corrections differ from other turns in that they are longer, louder, higher in
pitch excursion, follow longer pauses, and contain less internal silence than non-
corrections. All but the latter difference supports the hypothesis that corrections tend
to be hyperarticulated.
To confirm this hypothesis, two of the authors labeled each turn in the corpus for
evidence of perceptual hyperarticulation, following (Wade, Shriberg, and Price 1992).
Fifty-two percent of corrections in the corpus have some perceptual hyperarticulation,
compared with only 12% of other turns. Too, hyperarticulated corrections are more
likely to be misrecognized than other corrections (70% misrecognitions vs. 52%). How-
ever, it is important to note that only 59% of misrecognized corrections in the corpus are
also hyperarticulated, so recognition failure for a considerable portion of corrections
must be explained in some other way. There is still a large number of misrecognized
corrections that show no perceptual evidence of hyperarticulation.
In our earlier analysis of prosodic differences between correct and incorrectly
recognized turns (Hirschberg, Litman, and Swerts 2004), we also found that misrecog-
nized turns differed from correctly recognized turns in f0, loudness, duration, and
timing?all features associated with hyperarticulation. In addition, more misrecogni-
tions are hyperarticulated than are correctly recognized turns. But when we excluded
perceptually hyperarticulated turns from our prosodic analysis, we found that mis-
recognized turns were still prosodically different from correctly recognized turns, in
the same ways. We hypothesized there that misrecognitions might exhibit tendencies
toward hyperarticulation that are imperceptible to human listeners, but not to ASR
engines. The same may also be true of non-hyperarticulated, but still prosodically
distinct corrections. When we exclude hyperarticulated utterances from our corpus
Table 3
Corrections versus non-corrections by prosodic feature.
Feature t stat Mean corr p
- non-corr
f0 Max* 3.79 17.76 Hz < .001
f0 Mean 0.23 ?4.12 Hz .823
RMS Max* 4.88 347.75 < .001
RMS Mean* 2.57 63.44 .014
Duration* 6.68 1.16 sec < .001
Prior pause* 2.17 0.186 sec .036
Tempo 1.78 ?0.15 sps .246
% Silence* 4.75 ?0.05% < .001
*Significant at a 95% confidence level (p ? .05)
424
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
and reanalyze prosodic features of corrections versus non-corrections, we find signif-
icant differences in duration, rms maximum, rms mean, tempo, and amount of turn-
internal silence as we did with the corpus as a whole. So, again, even when corrections
are not perceptibly hyperarticulated, they share some acoustic tendencies with turns
that are.
3.3 Correction Chains
As noted above, corrections in the TOOT corpus often take the form of chains of
corrections of a single original error. Looking back at Figure 2, for example, we see
two chains of corrections: In the first, which begins with the misrecognition of turn 776
(Um, tomorrow), the user repeats the original phrase and then provides a paraphrase
(Saturday), which is correctly recognized. In the second, beginning with turn 780, the
time of departure is misrecognized. The user omits some information (a.m.) in turn
781, but without success; an ADD correction follows, with the previously omitted
information restored, in turn 783.
Distance of a correction from the original misrecognized turn?whether calculated
as position in chain (e.g., Saturday in Figure 2 is the second in the chain correcting
turn 776) or further in number of turns from that original error (e.g., Saturday here is
also two turns from the original error)?correlates significantly with prosodic variation.
An analysis of the relationship between both distance measures and our prosodic
features (using Pearson?s product?moment correlation) shows significant correlations
of distance in chain or from original error with f0 maximum (r = .20, p < .001; r = .21,
p < .001) and mean (r = .27, p < .001; r = .29, p < .001), rms maximum (r =?.09, p < .02;
r = ?.12, p < .005) and mean (r = ?.12, p < .0025; r = ?.16, p < .001), absolute duration
(r = .14, p < 0; r = .16, p < .001) and duration in number of words (r = .11, p < .01; r =
.12, p < .005), length of preceding pause (r = .11, p < .005; r = .10, p < .01), and speaking
rate (r = ?.05, p < .01; r = ?.10, p < .02). The more distant a correction is, in short, the
higher it is in pitch, the softer it is, the longer it is, the greater is its preceding pause, and
the more slowly it is spoken. In addition, more distant corrections are also more likely
to be misrecognized; for distance in turns there is a (negative) significant correlation for
concept accuracy (r = ?.13, p < .001), whereas both word and concept accuracy decline
significantly by position in chain (r = ?.08, p < .05; r = ?.15, p < .001). Table 4 shows
the mean concept accuracy of corrections for chain position through 8 (higher numbers
are very small) in the corpus. So, as speakers must try again and again to correct an
error, their attempts appear to become ever less likely to succeed, perhaps because their
prosodic behavior changes in ways that do not help the recognition process. Curiously,
however, our perceptual measure of hyperarticulation is not significantly correlated
with either of these distance measures. So, although speakers modify their speech in
ways generally consistent with hyperarticulation, their corrections do not necessarily
become more hyperarticulated as their attempts to correct continue. Another curious
Table 4
Mean concept accuracy by correction position in Chain.
Position 1 2 3 4 5 6 7 8
N 311 143 84 49 25 15 10 4
Error .43 .57 .63 .51 .60 .87 .70 1.00
425
Computational Linguistics Volume 32, Number 3
finding is that corrections that are more distant from the turn they immediately correct
(e.g., in Figure 2, turn 783 is more distant from the turn it corrects (781) than turn 781 is
from the turn it corrects, which is 780) tend to be more accurately recognized than turns
that are closer. Yet, prosodically, these turns are very like distant turns in a chain or from
the original error, being higher in f0 maximum and mean, lower in rms maximum and
mean, and longer in seconds and number of words. So, in the one case these prosodic
changes might be thought to lead to recognition error, where in the other they occur
with better recognized corrections.
3.4 Variation by Dialogue Strategy
Dialogue strategy clearly affects the type of correction users make and whether it
is successful or not. For example, users more frequently repeat their misrecognized
utterance in the SystemExplicit (75% of corrections are repetitions) condition than in
the MixedImplicit or UserNoConfirm (both 37% REP); the latter conditions have larger
proportions of OMITs and paraphrases. Perhaps this disparity is partly explained by the
larger proportion of corrections that follow rejections in the SystemExplicit condition
(39% vs. 22% and 19%). Overall, SystemExplicit turns are rejected 6% of the time,
whereas the other conditions have about 10% rejections. Table 5 shows differences in
mean length of tasks, number of corrections, number of misrecognitions, and number
of misrecognized corrections by dialogue strategy. Again, misrecognitions were defined
in terms of concept accuracy (turns with CA < 1); misrecognized corrections refer to
the intersection of user terms that were coded as both corrections and misrecognitions.
The fewer misrecognitions, corrections, and misrecognized corrections per task in the
SystemExplicit condition may well explain user ratings of the various systems (non-
adapt) in the original experiments (Litman and Pan 1999): When asked to say whether
they would be likely to use such a system in the future, on a 1?5 scale, subjects scored
SystemExplicit 3.5, MixedImplicit 2.6, and UserNoConfirm 1.7. User satisfaction scores
were similar: Where 40 is the highest score, users gave SystemExplicit 31.25, Mixed-
Implicit 24, and UserNoConfirm 22.1. So, SystemExplicit is preferred by users, even
though MixedImplicit on average takes fewer turns to accomplish a task, suggesting
that the large number of misrecognitions and consequent need for correction has a large
impact on user preferences. This is consistent with performance functions derived from
evaluations of TOOT (Litman and Pan 1999).
Perhaps because correction chains often end unsuccessfully, users frequently
?restart? a task within a session. Most restarts occurred in the MixedImplicit and
UserNoConfirm conditions and were rarely successful. In non-adaptive tasks, 42% of
corrections in the MixedImplicit condition were restarts and 31% in the UserNoConfirm,
Table 5
Corrections by system strategy.
Means SystemExplicit MixedImplicit UserNoConfirm
per task
# Turns 13.4 11.7 16.2
# Corrs 1.3 4.6 7.1
# Misrecs 2.8 6.4 9.4
# Misrec?d Corrs 0.3 3.2 4.8
426
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
whereas none occurred in the SystemExplicit condition. Restarts were misrecognized
77% of the time, compared to 65% of first turns in task. They thus seem to have been
a worse strategy than initiating a new task and might prove a useful diagnostic for
changing system strategy?or summoning a human operator.
4. Predicting Corrections
The previous section showed that corrections differ significantly from non-corrections
prosodically, being higher in pitch, louder, longer, with longer pauses preceding them
and less internal silence. In addition, they are misrecognized more frequently than
non-corrections?although they are no more likely to be rejected by the system. And
corrections more distant from the error they correct tend to exhibit greater prosodic
differences and are recognized more poorly, suggesting that users are not learning to
modify their own behavior to improve system performance. So, dealing with corrections
is a particularly difficult task for both users and systems. We also found that system dia-
logue strategy?the amount of initiative users are allowed to exercise in controlling the
flow of the dialogue and the type of confirmation strategy the system adopts?affects
users? choice of correction type (e.g., directly repeating vs. paraphrasing misrecognized
information). In the following, we turn to the question of identifying user corrections
automatically, from prosodic features as well as other features that are readily available
to a spoken dialogue system. In Section 4.1, we describe the features we use for our
machine-learning experiments. Section 4.2 presents the results of those experiments.
Section 4.3 presents further experiments using additional classifications and features,
motivated by our descriptive results. In the final section, we summarize our conclusions
and describe future research directions.
4.1 Features
In this section we describe the features used in the machine-learning experiments
and the motivation behind their selection. The entire feature set is presented in
Figure 3 and includes only features that could be automatically available to a dialogue
system.
4.1.1 Prosodic Features. Above we showed that corrections were significantly longer,
louder, higher in pitch excursion, and followed longer pauses than other turns. Thus,
we expected that these features would be useful in identifying corrections automatically.
We examined maximum and mean fundamental frequency values (f0max, f0mn) as
indicators of pitch range; maximum and mean energy values (rmsmax, rmsmn) as in-
dicators of loudness; total duration of the speaker turn (dur); length of pause preceding
the turn (ppau); speaking rate (tempo); and amount of silence within the turn (zeros).
The features were measured as indicated above. Table 6 shows the overall means and
standard deviations for these features over the corpus.
4.1.2 ASR Features. Since corrections in our corpus were misrecognized more frequently
than non-corrections (Swerts, Litman, and Hirschberg 2000), we included a set of ASR
features that were derived from TOOT?s speech recognition component and its outputs:
the grammar used as the ASR language model at each dialogue state (gram), the string
recognized by the ASR system as its best hypothesis (str), and the turn-level acoustic
427
Computational Linguistics Volume 32, Number 3
Figure 3
Feature set for predicting corrections.
Table 6
Means and standard deviations for prosodic features over all turns.
f0max f0mn rmsmax rmsmn dur ppau tempo zeros
(Hz) (Hz) (sec) (sec) (sps) (%)
Mean 227 163 1612 396 1.92 .71 2.48 44
S.D. 77 44 1020 261 2.44 .79 1.37 17
confidence score it produced (conf).4 As subcases of the str feature, we included Boolean
features representing whether or not the recognized string included the strings yes or no
(ynstr), some variant of no, such as nope (nofeat), cancel (canc), or help (help), as these
lexical items often occurred during problem resolution. To estimate durational features,
we approximated the length of the user turn in words (wordsstr) and in syllables (syls)
from the str feature, and we added a Boolean feature identifying whether or not the turn
had been rejected by the system (rejbool).
4.1.3 System Experimental Features. Our descriptive study showed that differences in
dialogue strategy affect the type and success of user corrections. For example, TOOT
users more frequently repeat their misrecognized turns and produce the fewest cor-
rections per task when TOOT has the initiative and explicitly confirms all user input.
So, we hypothesized that system conditions might prove important in our learning
experiments. We thus include features representing the system?s current initiative
and confirmation strategies (inittype, conftype), whether users could adapt the sys-
tem?s dialogue strategies (adapt), and the combined initiative and confirmation setting
(realstrat).
4 Confidence scores ranged from ?0.087662 to ?9.884418.
428
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
4.1.4 Dialogue Position and History Features. We also showed that the further a cor-
rection is from the original error, the less likely it is to be recognized correctly, and the
stronger the correlation with prosodic deviation from the mean values over a speaker?s
turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first
approximation of this distance feature, we included the feature diadist?distance of the
current turn from the beginning of the dialogue.
In addition, previous research (Litman, Walker, and Kearns 1999; Walker et al
2000) has shown that features of the dialogue as a whole and features of more local
context can be helpful in predicting ?problematic? dialogues. So we looked at a set
of features summarizing aspects of the prior dialogue for both the absolute number
of times prior turns exhibited certain characteristics (e.g., contained a key word like
cancel?priorcancnum) and the percentage of the prior dialogue containing one of these
features (e.g., priorcancpct). We also examined means for all our continuous-valued
features over the entire dialogue preceding the turn to be predicted (pmn ), such as
pmnsyls, the mean length of prior turns calculated in number of syllables per turn.
Finally, we examined more local contexts, including all features of the preceding turn
(pre ) and for the turn preceding that (ppre ).
It seemed particularly likely that lexical features of the local context?such as
whether a user had asked for help recently, or tried to cancel out of an exchange, or
replied no to a system query?might prove useful in identifying corrections.5 Also,
whether a prior turn had been rejected was clearly a useful cue to the identification
of the current turn as a correction, since users generally supplied a correction when
explicitly asked.
4.2 Machine-learning Experiments
In this section we investigate whether the features described in Section 4.1 (or inter-
esting subsets of them) can in fact be used to accurately predict whether a turn will
be a correction or not. We describe experiments using the machine-learning program
RIPPER (Cohen 1996) to automatically induce such prediction models. RIPPER takes as
input the classes to be learned, the names and possible values of a set of features, and
training data specifying the class and feature values for each training example. For our
experiments, the features presented in Figure 3 comprise the independent variables for
our learning experiments. The dependent variable to be learned, correction (T) ver-
sus non-correction (F), corresponds to the hand-labeled observations described above.
Given a vector of values for the independent and dependent variables for each speaker
turn, RIPPER outputs a classification model for classifying future examples. The model
is learned using greedy search guided by an information gain metric and is expressed as
an ordered set of if-then rules. When multiple rules are applicable, RIPPER uses the first
rule it finds. When no rules are applicable, RIPPER classifies the turn as a non-correction
(F) by default.
Table 7 shows the performance of the learned classification models for some
of the feature sets we examined; all performance figures are estimated using 25-fold
cross-validation on the 2,328 turns in our corpus. The Features column identifies the
set of features (as defined in Figure 3) used to learn the model. The second column,
DIA, indicates which type of dialogue history features (PreTurn, PrepreTurn, Prior,
5 Recall that these are lexical features from the recognized string, not from the actual user transcript.
429
Computational Linguistics Volume 32, Number 3
Table 7
Estimated error, recall, precision, and F? = 1 for predicting corrections.
class = T class = F
Features DIA Error ? SE Rec. Prec. F? = 1 Rec. Prec. F? = 1
Raw+ASR+SYS+POS PreTurn 15.72 ? 0.80 70.61 74.96 .72 89.95 88.28 .89
Raw+ASR+SYS+POS all 16.16 ? 0.58 69.80 74.65 .72 90.12 87.82 .89
PROS+ASR+SYS+POS all 16.38 ? 0.61 69.01 74.05 .71 89.60 87.61 .88
ASR all 16.41 ? 0.93 69.93 72.39 .70 88.76 87.7 .88
ASR+SYS+POS all 17.01 ? 0.78 73.73 73.38 .73 88.68 89.00 .89
ASR+SYS+POS none 18.60 ? 0.81 56.48 72.79 .63 91.33 83.76 .87
Raw+ASR+SYS+POS none 18.68 ? 0.67 58.45 71.64 .64 90.37 84.17 .87
ASR+PROS none 19.29 ? 0.78 54.54 69.97 .61 90.25 82.90 .86
POS+PROS none 19.59 ? 0.73 52.96 69.70 .60 90.38 82.47 .86
Raw all 19.68 ? 0.78 55.62 70.89 .62 90.64 83.33 .87
PROS all 20.33 ? 0.90 56.45 69.23 .61 89.43 83.42 .86
ASR+POS none 20.40 ? 0.79 52.20 71.99 .60 91.43 82.41 .87
PROS none 20.53 ? 0.81 54.86 71.72 .62 90.78 83.07 .87
conf+rejbool all 21.23 ? 0.93 59.70 65.97 .62 87.05 84.05 .85
ASR+SYS none 23.46 ? 0.72 51.55 63.40 .56 87.53 81.65 .84
ASR none 24.19 ? 0.84 45.93 60.99 .52 87.80 79.90 .84
Raw none 25.35 ? 0.93 42.26 59.46 .48 88.29 78.97 .83
POS none 29.00 ? 1.02 0.00 ? ? 99.94 70.99 .83
SYS none 29.00 ? 1.02 0.00 ? ? 100.00 71.00 .83
Prerejbool baseline error = 25.70; majority baseline error = 28.99
and/or PMean) were also included in the feature set; these features represent the
same types of information (e.g., f0max) that the Features column denotes, but for one
or more previous turns in the dialogue. The third column shows the mean error and
standard error (SE) predicted for the model specified by the first two columns. When
error estimates in different rows differ by plus or minus twice the standard error,
they are significantly different (Cohen 1995). The remaining columns show the mean
recall, precision, and F? = 1 for corrections (focus class = T) and non-corrections (focus
class = F), respectively.6 For comparison purposes, we compare our predictions to two
potential baselines. The Majority baseline predicts that all turns are non-corrections
(the majority class of F), and has a classification error of 28.99%. The Prerejbool base-
line predicts that all turns following rejected turns (prerejbool = T) are corrections?
since after rejections, TOOT asks users to repeat their turn7?and all others are non-
corrections; this baseline gives a classification error of 25.70%.
The first question addressed in our experiments is whether or not corrections can be
predicted significantly better than our baselines. Table 7 shows that in fact they can. Our
best-performing feature set (Raw+ASR+SYS+POS, DIA = PreTurn) cuts the majority
baseline error almost in half, from 28.99% to 15.72%, and predicts significantly better
than the rejection-based baseline as well. This feature set includes raw versions of all our
prosodic features and all of the non-prosodic features, for both the turn being classified
6 Recall is the percentage of actual members of a class that are identified, whereas precision is the percentage
of predicted class members that are in fact members. The definition of F? is
(?2+1)PrecisionRecall
?2Precicison+Recall
; ? = 1
equally weights precision and recall. These values are computed using our own cross-validation
program, while error is computed using RIPPER?s cross-validation option.
7 Although users are asked to repeat their turn, 29% of the turns after rejections are not in fact corrections
(e.g., the user instead asks for help or asks the system to repeat the prompt).
430
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
and the immediately prior turn. Note that even if all of the available features are used
for learning (i.e., the normalized versions of prosodic features and all of the various
history features (PROS+ASR+SYS+POS, DIA = all, error = 16.38%)), performance is
statistically comparable to this model.8 In addition, the recall, precision and F? = 1
values in Table 7 show that corrections are generally predicted with better precision
than recall whereas the reverse holds for non-corrections, and that non-corrections (the
majority class) are easier to accurately predict than corrections.
We next turn to an examination of the contribution of the different types of features
we used for prediction. First, we consider the utility of our non-prosodic features.
Table 7 shows that, using only non-prosodic features (ASR, SYS, POS), corrections can
still be predicted with an accuracy statistically equivalent to our best results. That is,
using all feature types (PROS+ASR+SYS+POS, DIA = all, error = 16.38%) is equivalent
to using only non-prosodic features (ASR+SYS+POS, DIA = all, error = 17.01%). Simi-
larly, restricting our feature set to the ASR-derived subset of our non-prosodic features
(ASR, DIA = all, error = 16.41%) or removing all dialogue history (ASR+SYS+POS,
DIA = none, error = 18.60%) yields results equivalent to our best-performing classifier.
However, when only those ASR features derived from the acoustic confidence score
(i.e., conf, preconf, ppreconf, pmnconf, rejbool, prerejbool, pprerejbool, priorrejbool-
num, priorrejboolpct) are used for prediction, then performance does significantly
degrade (conf+rejbool, DIA = all, error = 21.23%). So, it appears that there are numerous
ways to classify corrections successfully, using various combinations of feature types.
This finding is an important one because it suggests that systems that have access to
restricted kinds of information can still hope to identify user corrections with some
confidence. In particular, simply using information available to current ASR systems,
such as acoustic confidence score, recognized string, grammar, and features derived
from these, produces classification results equivalent to our best-performing classifier.
A caveat here is that some of the features in this ASR feature set (e.g., grammar and
recognized string) are less likely to generalize from task to task.
Turning now to the role of prosodic features in classifying corrections, Table 7 shows
that use of only non-prosodic features (ASR+SYS+POS, DIA = all, error = 17.01%)9
slightly (but not quite significantly) outperforms use of only raw prosodic features (Raw,
DIA = all, error = 19.68%). However, using raw prosodic features alone (error = 19.68%)
is comparable to using only ASR features alone (ASR, DIA = all, error = 16.41%). And
both significantly outperform the majority class and rejection-based baselines. Note
also that prediction from raw prosodic features alone (19.68%) is not improved by the
inclusion of their normalized versions (PROS, DIA = all, error = 20.33%). Thus, ASR-
derived features and prosodic features seem to provide equally successful classifications
of user corrections. Since ASR-derived features, in particular, acoustic confidence score,
are currently used by spoken dialogue systems to determine when to reject a turn,
our results suggest that such features can also be useful for identifying corrections.
Although prosodic features are rarely made use of in spoken dialogue systems, they
would, in fact, seem more likely to generalize across tasks and recognizers than the
ASR features.
Now we turn to the issue of how useful features of the dialogue history are in
classifying corrections. Recall that our best-performing ruleset used only a limited dia-
8 Note that removing features sometimes changes performance, which might indicate a weakness in
RIPPER?s feature selection process.
9 Recall that DIA = all includes only the same type of features as for the current utterance, in this case only
non-prosodic history features.
431
Computational Linguistics Volume 32, Number 3
Figure 4
Best performing ruleset (Raw+ASR+SYS+POS, DIA = PreTurn).
logue history?features from the preceding turn (Raw+ASR+SYS+POS, DIA = PreTurn,
error = 15.72%). While adding features of the turn two turns back (PrepreTurn ) and
of the dialogue as a whole (Prior and PMean ) does not significantly change the
error (Raw+ASR+SYS+POS, DIA = all, error = 16.16%), removing the features of the
immediately previous turn from the dialogue history does in fact cause a significant
increase in error rate (Raw+ASR+SYS+POS, DIA = none, error = 18.68%). However,
as discussed above, when only non-prosodic features are considered (ASR+SYS+POS),
there is no significant difference between DIA = all and DIA = none. So, it seems that
features of the immediate local context can improve our ability to classify corrections
accurately when prosodic features are included, but adding a larger local context win-
dow and a global context does not improve over these results. Contextual features seem
particularly important to performance when only raw prosodic features are considered
(Raw, DIA = all, error = 19.68%). When the raw prosodic features of the dialogue
history are removed, the error rate dramatically increases (Raw, DIA = none, error =
25.35%). However, if the normalized prosodic features (which themselves encode much
of the historical information) are also included, then removing the DIA versions of these
features does not significantly degrade performance (PROS, DIA = all, error = 20.33%
vs. PROS, DIA = none, error = 20.53%). We might explain the larger role that prosodic
context plays in classification by returning to the differences we found between prosodic
features of corrections and non-corrections, described in Section 3. In our descriptive
analyses we found that prosodic features such as pitch, duration, and loudness reliably
distinguish corrections based on relative differences between the two types of turns,
not absolute differences. In prediction also, it seems that some form of normalization by
context improves the performance of prosodic features.
When we examine which class of features performs best in the absence of contextual
information, we see that the prosodic features (PROS, DIA = none, error = 20.53%)
significantly outperform the ASR-derived features (ASR, DIA = none, error = 24.19%),
which in turn significantly outperform either of the remaining feature types (POS and
SYS). Table 7 also shows the cases in which the addition of new sources of knowl-
edge improves prediction performance. For DIA = none, the statistically significant
improvements involve adding the feature diadist (distance of the current turn from
the beginning of the dialogue): For example, ASR+POS (error = 20.4%) outperforms
both ASR (error = 24.19%) and POS (error = 29%), and ASR+SYS+POS (error = 18.6%)
outperforms ASR+SYS (error = 23.46%). Again, these are features that are easily made
available to current spoken dialogue systems.
The classification model learned from the best-performing feature set in Table 7 is
shown in Figure 4. Rules are presented in order of importance in classifying data. The
432
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
first rule RIPPER finds with this feature set specifies that if the duration of the current
turn is ? 3.89046 seconds, and if the acoustic confidence score of the prior turn is
? ?0.645234, and if the percentage of silence in the current turn is ? 53.9474%, then
predict that the turn is a correction; this rule correctly predicts 153 corrections and
incorrectly predicts that 10 non-corrections are corrections. So, this rule applies when
the previous turn has a low confidence score and the current turn exhibits some marked
prosodic features. The fourth rule predicts a correction after a previous rejection, but
only when the rejected turn was relatively short with a low confidence score. The
fifth rule predicts a correction when TOOT uses a particular confirmation strategy for
turns that are relatively long and far from the beginning of the dialogue. The sixth
rule predicts a correction when the previous turn is spoken soon after the prompt,
and contains the problem indicator help. Note that this use of the domain-independent
help is the only reference to a lexical item in this ruleset. This ruleset includes features
from all of the feature subsets in our inventory (PROS, ASR, SYS, POS, DIA). For the
current turn, the feature types that appear in the rules are PROS (dur, zeros), ASR (conf,
gram, syls), SYS (conftype), and POS (diadist). Of the previous turn?s features, only
two feature sets emerge as important: PROS (pref0mn, predur, preppau, pretempo)
and ASR (preconf, prestr, prewordstr, prerejbool). Furthermore, within a feature set
such as PROS, the useful features of the current and previous turns differ somewhat
(e.g., zeros is useful for the current turn, whereas tempo is useful for the prior turn),
suggesting important differences in the prosodic characteristics of corrections versus
the turns they follow.
When we look at a ruleset produced using only features commonly available to
current dialogue systems, such as ASR+SYS+POS (DIA = all), we see that creative use
of these features could in fact support correction classification (Figure 5). For example,
the fourth rule predicts that the current turn is a correction when it is not too short, and
when the pre turn indicates awareness (evidenced by the presence of no) of a problem
in the ppre turn (which was recognized with low confidence). This ruleset uses both
ASR (gram, nofeat, syls) and SYS (conftype) features of the current turn; although only
one rule in fact makes use of SYS features. For the contextual DIA features, only the
ASR features occur in the rule-set: PreTurn (preconf, prestr, prenofeat, prerejbool),
PrepreTurn (ppreconf, ppreynstr), and Prior and PMean (pmnconf, priorynstrpct,
pmnwordsstr, priorrejnum). Comparing this ruleset to the previous one (Figure 4), we
see that where timing features (dur, predur, zeros, pretempo, preppau) appear often
when prosodic features are available, related features such as syls and wordsstr (from
which, e.g., tempo is estimated) may be compensating in this ruleset. And of course the
rejection feature (prerejbool) itself is a function of the confidence score of the prior turn.
Note also that lexical features of the recognized string (nofeat, prenofeat, ppreynstr,
Figure 5
Ruleset for non-prosodic features (ASR+SYS+POS, DIA = all).
433
Computational Linguistics Volume 32, Number 3
prestr, priorynstrpct) emerge as quite useful in this ruleset?especially as contextual
features. So, what the system has recognized in prior turns is a good predictor of
whether the current turn is a correction. Also note that the overall verbosity of the
previous dialogue (pmnwordsstr) appears in two of the rules.
An example of a ruleset learned from only prosodic features (Raw, DIA = all, from
Table 7) is shown in Figure 6. This ruleset is notably terser than those shown in Figures 4
and 5 and includes primarily timing-based features (current turn features dur, zeros,
and tempo; local contextual feature pretempo; and dialogue-level features pmndur and
pmnppau). However, all prosodic feature types but f0 appear at least once in the ruleset,
and features specific to the current turn differ from those relevant to different types of
dialogue history. As with our previous descriptive findings discussed in Section 3, this
ruleset shows that corrections are longer, louder, follow longer pauses, and contain less
internal silence than non-corrections, and that these features can be used successfully to
identify them.
4.3 Other Experiments
The machine-learning experiments described in Section 4.2 were motivated by our
long-term goal to incorporate a correction predictor into future versions of our spoken
dialogue system. As such, the experiments were limited to a binary prediction task
(predicting whether a turn was a correction or a non-correction) and only considered
features readily available to our dialogue system. In this section we present additional
experiments removing some of these restrictions, with the goal of further investigating
some of the descriptive findings discussed in Section 3.
Recall from Section 3.2 that there were some differences in the prosodic features of
corrections produced by native versus non-native speakers when such features were
normalized by the first turn in the dialogue. We thus investigated whether adding a
native speaker feature (currently manually labeled) to the prosodic feature set Norm1
(DIA = all) would improve prediction accuracy. Although the error was reduced from
24.32% to 22.68%, this difference was not statistically significant. Furthermore, when
we added the native speaker feature to both the best-performing ruleset in Table 7
(Raw+ASR+SYS+POS, DIA = PreTurn) and the best-performing prosodic feature set
(Raw, DIA = all), the error rates actually increased; again, however, the differences were
not statistically significant.
Also, in Sections 3.1 and 3.4, we identified differences between different types of cor-
rections, which suggests that our features might be more effectively used to predict each
correction type differently. In other words, what would happen if instead of predicting
whether a turn was a correction (T) or not (F) (the binary classification task investigated
above), we predicted whether a turn was ADD, ADD/OMIT, OMIT, PAR, REP, or F (i.e.,
not a correction). Because, as Table 2 shows, we only have limited amounts of data for
Figure 6
Ruleset for raw prosodic features (Raw, DIA = all).
434
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
several of our classes (e.g., only 2% of our corrections are ADD/OMIT); we performed a
simpler version of this experiment, combining our three lowest frequency classes (ADD,
ADD/OMIT, and PAR) into the single class MISC.
Using the best feature set from Table 7 (Raw+ASR+SYS+POS, DIA = PreTurn),
Table 8 shows our results using 25-fold cross-validation. First, note that our overall
estimated error is now 24.13% ? 0.89%. Although this is a huge increase compared to
the 15.72% error rate of our original binary classifier, it should be noted that considering
correction types separately makes our class distribution quite skewed, with the data
for our three correction classes much smaller than the majority class. Nevertheless,
our classifier yields a slight but significant decrease compared to the majority baseline
error, and a nonsignificant decrease compared to the Prerejbool baseline error (both
baselines remain the same as in Table 7). With respect to precision and recall, although
the absolute numbers for corrections are much lower than in Table 7, we again see
that predicting corrections yields higher precision than recall, whereas predicting non-
corrections yields higher recall than precision. Finally, an examination of the learned
ruleset (which contains four rules for predicting MISC, two rules for predicting OMIT,
and seven rules for predicting REP) does show that features are used differently across
correction types. For example, the feature prestr is only used to predict repetition correc-
tions (in particular, after a turn containing help). Our rules also show some overlap with
our earlier descriptive findings. For example, we noted that corrections of rejections
were more likely to be repetitions, and find the feature prerejbool in two of the rules
for predicting repetitions. These findings suggest that if more data were available,
predicting corrections by type might prove a useful strategy.
5. Conclusions
In this article we have presented results of an analysis of corrections in the TOOT
spoken dialogue corpus. We first introduced the TOOT spoken dialogue corpus and
our labeling scheme to identify different types of corrections. The TOOT corpus is
representative of many current research and commercial dialog systems in focusing on
the travel domain. Also, since data were collected using a variety of dialog strategies
with different types of initiative and confirmation, results obtained with this corpus
are more likely to have general usefulness for builders of other spoken dialogue
systems.
We next presented a statistical description of the corrections we labeled. In general,
it appears that corrections are a serious problem for ASR, being recognized much
more poorly than non-corrections but not being rejected any more frequently. Some
corrections types are more difficult to handle for systems than others, with repetitions
Table 8
Predicting correction types (error ? SE = 24.13 ? 0.89)
Class Recall Precision F? = 1
FALSE 93.30 82.37 .87
REP 33.86 56.00 .41
MISC 36.17 48.36 .38
OMIT 25.47 50.13 .32
435
Computational Linguistics Volume 32, Number 3
and corrections that omit information from the original turn being much better recog-
nized than corrections that add or paraphrase such information. Confirming previous
studies of repetition corrections, we found that corrections in general differ from non-
corrections prosodically: They are higher in f0, softer, longer, follow longer pauses, and
contain less internal silence than non-corrections. Also, corrections more distant from
the error they are correcting are louder, higher in pitch, longer, slower, and follow
longer pauses than closer corrections. Both findings suggest a correlation between
corrections and hyperarticulation; however, most prosodic differences persist even
when perceptually hyperarticulated turns are removed from the sample, and perceptual
hyperarticulation is not significantly correlated with distance from original error. We
hypothesize that recognizers may be more sensitive to hyperarticulatory tendencies
than humans.
The second part of this article discusses results of machine-learning experiments
designed to evaluate how well we can distinguish user corrections from non-corrections
using features automatically available to dialogue systems. Clearly, new techniques
must be developed to interpret such corrections, but such techniques can only be
effective if corrections can be reliably identified as such for special handling. Using a
large set of prosodic, ASR-derived, and system-specific features, both for the current
turn and for contextual windows, and using summary features of the prior dialogue, we
have demonstrated that it is possible to classify user corrections significantly better than
either of two baseline classifiers (15.72% error vs. 25.70?28.99%). More usefully perhaps
for current spoken dialogue systems, we have found that we can derive classifiers
that perform equivalently well using only features currently available to most speech
recognizers, such as acoustic confidence score, recognized string, grammar, and features
easily derived from these data. For example, using only such features, we can classify
user corrections with an estimated success rate of 16.41%. So, it does, in fact, seem quite
feasible for current systems to identify user corrections using data they typically do not
make use of.
Given that our findings show that corrections can be classified well using quite dis-
tinct feature sets, a possible future line of research would be to try classification combi-
nation schemes. For instance, one could envision a form of metalearning or boosting that
combines classifications using different feature sets (e.g., ASR vs. prosodic vs. context),
or that combines the output of different learning algorithms (e.g., Ripper combined
with memory-based learning; see, e.g., Lendvai 2004). Kirchhoff (2001) presents some
results of classifier combination schemes, showing some improvements in detection of
corrections when using cascading, but especially when using boosting.
The next steps, developing techniques to interpret these turns more accurately
and to use correction prediction to drive modifications in dialogue strategy, are both
subjects of our future research. Also, whereas our analyses so far have given us overall
information about the relative contribution of various feature sets for the automatic
classification of corrections, one interesting problem for the future is to get more specific
information about the cues that characterize corrections, especially for the development
of on-line error-correction detection. In this respect, an interesting observation has been
made by Kirchhoff (2001), who reports that correction classification using only features
of the first half of a turn performs equally well as a classification using features of the
turn as a whole; this could be explained by the fact that speakers tend to put character-
istic cue phrases, such as ?no? or ?help,? in the beginning of a turn. Additional research
is needed to find strategies that use the detection of corrections to look back in the
dialogue history to identify the utterance being corrected or even the actual problematic
words in these turns. Finally, it would be worthwhile to investigate speaker-specific
436
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
correction strategies in more detail, the possible effect on such strategies of the user?s
experience with a system, and his or her linguistic background.
Acknowledgments
Marc Swerts is also affiliated with the
University of Antwerp. His research is
sponsored by the Netherlands Organisation
for Scientific Research (NWO). This work
was performed when the authors were at
AT&T Labs?Research.
References
Ammicht, E., A. Potamianos, and
E. Fosler-Lussier. 2001. Ambiguity
representation and resolution in spoken
dialogue systems. In Proceedings of
EUROSPEECH-01, pages 2217?2220,
Aalborg.
Andorno, M., P. Laface, and R. Gemello.
2002. Experiments in confidence scoring
for word and sentence verification. In
Proceedings of International Conference on
Spoken Language Processing-02,
pages 1377?1381, Denver.
Barkhuysen, P., E. Krahmer, and M. Swerts.
2005. Problem detection in
human?machine interactions based on
facial expressions of users. Speech
Communication, 45:343?359.
Batliner, A., K. Fischer, R. Huber, J. Spilker,
and E. No?th. 2003. How to find trouble in
communication. Speech Communication,
40:117?143.
Bell, L. and J. Gustafson. 1999. Repetition
and its phonetic realizations: Investigating
a Swedish database of spontaneous
computer-directed speech. In Proceedings
of International Congress of Phonetic
Sciences-99, pages 1221?1224, San
Francisco.
Bouwman, A. G., J. Sturm, and L. Boves.
1999. Incorporating confidence
measures in the Dutch train timetable
information system developed
in the ARISE project. In Proceedings
International Conference on Acoustics,
Speech and Signal Processing, volume 1,
pages 493?496, Phoenix.
Bulyko, I., K. Kirchhoff, M. Ostendorf, and
J. Goldberg. 2005. Error-correction
detection and response generation in a
spoken dialogue system. Speech
Communication, 45:271?288.
Cohen, P. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Boston.
Cohen, W. 1996. Learning trees and rules
with set-valued features. In 14th Conference
of the American Association of Artificial
Intelligence, AAAI, pages 709?716,
Portland.
Falavigna, D., R. Gretter, and G. Riccardi.
2002. Acoustic and word lattice based
algorithms for confidence scores. In
Proceedings of International Conference
on Spoken Language Processing-02,
pages 1621?1624, Denver.
Guillevic, D., S. Gandrabur, and
Y. Normandin. 2002. Robust semantic
confidence scoring. In Proceedings of
International Conference on Spoken Language
Processing-02, pages 853?856, Denver.
Hirschberg, J., D. Litman, and M. Swerts.
1999. Prosodic cues to recognition errors.
In Proceedings of the Automatic Speech
Recognition and Understanding Workshop
(ASRU?99), pages 349?352, Keystone.
Hirschberg, J., D. Litman, and M. Swerts.
2004. Prosodic and other cues to speech
recognition failures. Speech Communication,
43:155?175.
Kamm, C., S. Narayanan, D. Dutton, and
R. Ritenour. 1997. Evaluating spoken
dialog systems for telecommunication
services. In Proceedings of
EUROSPEECH-97, pages 2203?2206,
Rhodes.
Karsenty, L. and V. Botherel. 2005.
Transparency strategies to help users
handle system errors. Speech
Communication, 45:305?324.
Kirchhoff, Katrin. 2001. A comparison
of classification techniques for the
automatic detection of error corrections
in human?computer dialogues. In
Proceedings of the NAACL Workshop
on Adaptation in Dialogue Systems,
pages 33?40, Pittsburgh, PA.
Krahmer, E., M. Swerts, M. Theune, and
M. Weegels. 2001. Error detection in
spoken human?machine interaction.
International Journal of Speech Technology,
4(1):19?30.
Levow, Gina-Anne. 1998. Characterizing
and recognizing spoken corrections in
human?computer dialogue. In Proceedings
of the 36th Annual Meeting of the Association
of Computational Linguistics, COLING/ACL
98, pages 736?742, Montreal.
Lendvai, Piroska. 2004. Extracting
information from spoken user input. A
machine-learning approach. Unpublished
Ph.D. dissertation, Tilburg University.
Litman, D. and S. Pan. 1999. Empirically
evaluating an adaptable spoken
437
Computational Linguistics Volume 32, Number 3
dialogue system. In Proceedings of the
7th International Conference on User
Modeling (UM), pages 55?64, Banff.
Litman, D., M. Walker, and M. Kearns. 1999.
Automatic detection of poor speech
recognition at the dialogue level. In
Proceedings of the 37th Annual Meeting of the
Association of Computational Linguistics,
ACL99, pages 309?316, College Park.
McTear, M., I. A. O?Neill, P. Hanna, and
X. Liu. 2005. Handling errors and
determining confirmation strategies?an
object-based approach. Speech
Communication, 45:249?269.
Moreno, P. J., B. Logan, and B. Raj. 2001. A
boosting approach for confidence scoring.
In Proceedings of EUROSPEECH-01,
pages 2109?2112, Aalborg.
Oviatt, S. L., G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate
speech during human-computer error
resolution. In Proceedings of International
Conference on Spoken Language
Processing-96, pages 801?804, Philadelphia.
Prodanov, P. and A. Drygajlo. 2005. Bayesian
networks based multimodality fusion for
error handling in human?robot dialogues
under noisy conditions. Speech
Communication, 45:231?248.
Sharp, R. D., E. Bocchieri, C. Castillo,
S. Parthasarathy, C. Rath, M. Riley,
and J. Rowland. 1997. The Watson
speech recognition engine. In
Proceedings International Conference on
Acoustics, Speech and Signal Processing97,
pages 4065?4068, Munich.
Shimojima, A., Y. Katagiri, H. Koiso, and
M. Swerts. 2001. An experimental study on
the informational and grounding functions
of prosodic features of Japanese echoic
responses. Speech Communication,
43:155?175.
Skantze, G. 2005. Exploring human error
recovery strategies: Implications for
spoken dialogue systems. Speech
Communication, 45:325?341.
Soltau, Hagen and Alex Waibel. 1998. On the
influence of hyperarticulated speech on
recognition performance. In Proceedings of
International Conference on Spoken Language
Processing-98, pages 225?228, Sydney.
Soltau, Hagen and Alex Waibel. 2000.
Specialized acoustic models for
hyperarticulated speech. In Proceedings
of International Conference on Acoustics,
Speech and Signal Processing 2000,
pages 1779?1782, Istanbul.
Soltau, H., H. Metze, and A. Waibel. 2002.
Compensating for hyperarticulation by
modeling articulatory properties. In
Proceedings of International Conference
on Spoken Language Processing-02,
pages 83?86, Denver.
Sturm, J. and L. Boves. 2005. Effective error
recovery strategies for multimodal
form-filling applications. Speech
Communication, 45:289?303.
Swerts, M., D. Litman, and J. Hirschberg.
2000. Corrections in spoken dialogue
systems. In Proceedings of International
Conference on Spoken Language
Processing-00, pages 615?618, Beijing.
Talkin, D. 1995. A Robust algorithm for
pitch tracking (RAPT). In W. B. Klein
and K. K. Paliwal, editors, Speech
Coding and Synthesis. Elsevier Science,
Athens, pages 495?518.
Torres, F., L. Hurtado, F. Garc??a, E. Sanchis,
and E. Segarra. 2005. Error handling in a
stochastic dialog system through
confidence measures. Speech
Communication, 45:211?229.
Wade, E., E. E. Shriberg, and P. J. Price.
1992. User behaviors affecting
speech recognition. In Proceedings of
International Conference on Spoken
Language Processing-92, volume 2,
pages 995?998, Banff.
Walker, M., I. Langkilde, J. Wright,
A. Gorin, and D. Litman. 2000.
Learning to predict problematic
situations in a spoken dialogue
system: Experiments with How may
I help you? In Proceedings of NAACL-00,
pages 210?217, Seattle.
Wang, H.-M. and Y.-C. Lin. 2002. Error-
tolerant spoken language understanding
with confidence measuring. In Proceedings
of International Conference on Spoken
Language Processing-02, pages 1625?1628,
Denver.
Zhang, R. and A. Rudnicky. 2001. Word
level confidence annotation using
combinations of features. In Proceedings
of EUROSPEECH-01, pages 2105?2108,
Aalborg.
438
Towards Emotion Prediction in Spoken Tutoring Dialogues
Diane Litman
Dept. of Computer Science
LRDC, Univ. of Pittsburgh
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Kate Forbes
LRDC, Univ. of Pittsburgh
Pittsburgh PA, 15260, USA
forbesk@pitt.edu
Scott Silliman
LRDC, Univ. of Pittsburgh
Pittsburgh PA, 15260, USA
scotts@pitt.edu
Abstract
Human tutors detect and respond to student
emotional states, but current machine tutors do
not. Our preliminary machine learning experi-
ments involving transcription, emotion annota-
tion and automatic feature extraction from our
human-human spoken tutoring corpus indicate
that the spoken tutoring system we are devel-
oping can be enhanced to automatically predict
and adapt to student emotional states.
1 Introduction
Connections between learning and emotion are well-
documented (Coles, 1999), and studies have shown con-
siderable benefits of spoken tutoring (Hausmann and Chi,
2002). Human tutors can respond to both the content of
student speech and the manner with which it is spoken
(e.g. ?confidently? or ?uncertainly?), but most intelligent
tutoring dialogue systems are text-based and thus limited
in their ability to recognize such learning states (Rose and
Freedman, 2000; Rose and Aleven, 2002). Building spo-
ken dialogue tutoring systems has great potential benefit,
for speech is the most natural and easy to use form of
natural language interaction, and it supplies a rich source
of prosodic and acoustic information about the speaker?s
current mental state, which can be used to monitor the
pedagogical effectiveness of student-computer interac-
tions. The success of computer-based tutoring systems
could increase if they predicted and adapted to student
emotional states, e.g. reinforcing positive states, while
rectifying negative states (Evens, 2002).
Although (Ang et al, 2002; Litman et al, 2001; Bat-
liner et al, 2000) have hand-labeled naturally-occurring
utterances in a variety of corpora for various emotions,
then extracted acoustic, prosodic and lexical features and
used machine-learning techniques to develop predictive
models, little work to date has addressed emotion detec-
tion in computer-based educational settings. In this paper
we describe preliminary annotation of positive, negative,
and neutral emotions in a human-human tutoring corpus
and discuss the results of pilot machine learning experi-
ments whose goal is to develop computational models of
specific emotional states (Section 3) for use in a spoken
dialogue system (Section 2).
2 The ITSPOKE System and Corpus
We are developing a spoken dialogue system, called IT-
SPOKE (Intelligent Tutoring SPOKEn dialogue system),
which uses as its ?back-end? the text-based Why2-Atlas
dialogue tutoring system (VanLehn et al, 2002). In
Why2-Atlas, a student types an essay answering a quali-
tative physics problem and a computer tutor then engages
him/her in dialogue to provide feedback, correct mis-
conceptions, and elicit more complete explanations, after
which the student revises his/her essay, thereby ending
the tutoring or causing another round of tutoring/essay
revision. To date we have interfaced the Sphinx2 speech
recognizer with stochastic language models trained from
example user utterances, and the Festival speech synthe-
sizer for text-to-speech, to the Why2-Atlas back-end, and
are adapting the knowledge sources needed by the spoken
language components; e.g. we have developed a set of
dialogue-dependent language models using 4551 student
utterances from the Why2-Atlas 2002 human-computer
typed corpus and will enhance them using student utter-
ances from our human-human spoken corpus.
Our human-human spoken corpus contains spoken dia-
logues collected via a web interface supplemented with a
high quality audio link, where a human tutor performs the
same task as ITSPOKE and Why2-Atlas. Our subjects
are U. Pittsburgh students who have taken no college level
physics and are native speakers of (Amer.) English. Our
experimental procedure, taking roughly 7 hours/student
over 1-2 sessions, is as follows: students 1) take a pretest
measuring their physics knowledge, 2) read a small doc-
ument of background material, 3) use the web and voice
interface to work through up to 10 training problems with
the human tutor, and 4) take a post-test similar to the
pretest. We have to date collected 63 dialogues (1290
minutes of speech from 4 females and 4 males) and tran-
scribed 20 of them. A corpus example is shown in Fig-
ure 1, containing the problem, the student?s essay, and an
annotated excerpt from the subsequent dialogue.
PROBLEM: If a car is able to accelerate at 2 m/s2, what accel-
eration can it attain if it is towing another car of equal mass?
ORIGINAL ESSAY: If the car is towing another car of equal
mass, the maximum acceleration would be the same because
the car would be towed behind and the friction caused would
only be by the front of the first car.
. . . dialogue excerpt at 6.5 minutes into session . . .
TUTOR: Now this law that force is equal to mass times accel-
eration, what?s this law called? This is uh since this it is a very
important basic uh fact uh it is it is a law of physics. Um you
have you have read it in the background material. Can you re-
call it?
STUDENT: Um no it was one of Newton?s laws but I don?t
remember which one. (laugh) (EMOTION=NEGATIVE)
TUTOR: Right, right, that is Newton?s second law of motion.
STUDENT: Ok, because I remember one, two, and three,
but I didn?t know if there was a different name (EMO-
TION=POSITIVE)
TUTOR: Yeah that?s right. You know Newton was a genius and
uh he looked at a large number of experiments and experimen-
tal data that was available and from that he could come to this
general law...
STUDENT: mm-hm (EMOTION=NEUTRAL)
Figure 1: Human-Human Spoken Corpus Example
3 Predicting Emotional Speech
For this pilot study, we annotated 14 transcribed dia-
logues from 7 students, 2 dialogues per student. First,
turn boundaries were manually annotated (based on con-
sensus labelings from two coders). Each turn was
then manually annotated for speaker affect (by a sin-
gle coder) using three general categorizations: negative
(e.g.?uncertain?, ?frustration?), positive (e.g. ?confident?,
?certain?), or neutral/indeterminate, as shown in Figure 1.
Table 1 shows the distribution of our labeled turns.
neutral positive negative total
248 167 141 553
Table 1: Labeled Turn Counts: ITSPOKE Pilot Corpus
We next conducted experiments using the RIPPER (Co-
hen, 1996) rule induction machine learning program,
which takes as input the classes to be learned (e.g. our
emotion annotations), the names and possible values in
a feature set (discussed below), and training examples,
each specifying its class and feature values (e.g. the la-
beled student turns in our pilot corpus), then outputs a
classification model for classifying future examples, ex-
pressed as an ordered set of if-then rules. RIPPER?s ?set-
valued? features allow us to represent the speech recog-
nizer?s best hypothesis and/or the turn transcription as a
set of words, and its rule output is an intuitive way to gain
insight into our data.
For our first pilot machine learning experiment, our
feature set consisted of SUBJECT ID and PROBLEM ID,
both representing system state, TURN START-TIME (rel-
ative to start of dialogue) and TURN DURATION, both
representing timing information, TEXT IN TURN (tran-
scription), and NUMBER OF WORDS IN TURN. Figure 2
presents the ruleset that was learned for this classifica-
tion task. For example, the first learned rule states that
if the duration of the turn is greater than 0.65 seconds
and the transcribed text of the turn contains the lexical
item ?I?, then the turn is predicted to be labeled EMO-
TION=NEGATIVE. The estimated mean error and stan-
dard error of this ruleset is 33.03% +/- 2.45%, based on
25-fold cross-validation.
if (duration   0.65)  (text has ?I?) then neg
else if (duration   2.98) then neg
else if (duration   0.93)  (startTime   297.62) then pos
else if (text has ?right?) then pos
else neutral
Figure 2: All-Features Ruleset for Emotion Prediction
For comparison, our feature set in our second pilot
machine learning experiment consisted of just TEXT IN
TURN. The ruleset learned for this classification task con-
tained 21 rules; Figure 3 presents an (ordered) excerpt1.
Estimated mean error and standard error of this ruleset is
39.03% +/- 2.40%, based on 25-fold cross-validation.
if (text has ?I?)  (text has ?don?t?) then neg
else if (text has ?um?)  (text has ?  hn  ?) then neg
else if (text has ?the?)  (text has ?  fs  ?) then neg
else if (text has ?right?) then pos
else if (text has ?so?) then pos
else if (text has ?(laugh)?)  (text has ?that?s?) then pos
else neutral
Figure 3: Text-Feature Ruleset for Emotion Prediction
Although both these error rates are still fairly high,
they are a significant improvement over a majority class
1  hn  = human noise (e.g. sighs and coughs), and  fs  =
false start (e.g. ?I th- think?)
baseline that always predicts the majority class in our cor-
pus (neutral/indeterminate) - which has an error rate of
55.69%. Moreover, many of the learned rules contain
features that are intuitively associated with the predicted
emotion; for example, disfluencies such as false starts are
often associated with negative emotions such as ?uncer-
tainty?, as are lexical items such as ?um? used in combi-
nation with human noises such as sighs.
4 Future Directions
Even using a small corpus classified by one coder and
predicted using only a handful of features, our results
suggest that there are indeed features that can automati-
cally distinguish emotions in tutoring dialogues. We will
next explore the utility of a wider variety of features rep-
resenting many knowledge sources (including acoustic,
prosodic, lexical, syntactic, semantic, discourse, and lo-
cal and global contextual dialogue features), using abla-
tion studies. We will perform our learning using and com-
paring large corpora of both human-human and human-
computer data for training and testing, and will evaluate
our results using a variety of metrics (e.g. recall, pre-
cision, and F-measure). We will also investigate a va-
riety of emotion annotations with the goal of producing
a reliable annotation scheme for the emotions associated
with our tutoring domain. Previous studies have shown
low inter-annotator reliability (around 70%, Kappa val-
ues around 0.47 (Narayanan, 2002)), which originates
partly in vague descriptions of the emotions to be labeled.
Finally, we hope to use this work to demonstrate that
enhancing a spoken dialogue tutoring system to automat-
ically predict and then dynamically respond to student
emotional states will measurably improve system perfor-
mance. Our enhancements will be motivated by tutor-
ing literature (Evens, 2002; Aist et al, 2002) that ad-
dresses how a tutor might make use of such information
if it could be inferred, as well as by looking at how the
human tutor actually responded to emotionally labeled
turns. Our methodology will build on previous adaptive
(non-tutoring) dialogue systems (see (Litman and Pan,
2002)); however, our system will predict and adapt to
both problematic and positive dialogue situations in tu-
toring.
Acknowledgments
This research is supported by the NSF under Grant No.
9720359 to the Center for Interdisciplinary Research on
Constructive Learning Environments (CIRCLE) at the
University of Pittsburgh and Carnegie-Mellon University.
References
G. Aist, B. Kort, Rob R.lly, J. Mostow, and R. Pi-
card. 2002. Experimentally augmenting an intelli-
gent tutoring system with human-supplied capabilities:
Adding human-provided emotional scaffolding to an
automated reading tutor that listens. In Proc. of ITS.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP.
A. Batliner, R. Huber, H. R. Niemann, E. No?th, J. Spilker,
and K. Fischer. 2000. The recognition of emotion. In
Proc. of the ISCA Workshop on Speech and Emotion.
William Cohen. 1996. Learning trees and rules with set-
valued features. In Proc. of AAAI.
G. Coles. 1999. Literacy, emotions, and the brain. Read-
ing Online, March 1999.
M. Evens. 2002. New questions for Circsim-Tutor. Pre-
sentation at the 2002 Symposium on Natural Language
Tutoring, University of Pittsburgh.
Robert Hausmann and Michelene Chi. 2002. Can a com-
puter interface support self-explaining? The Interna-
tional Journal of Cognitive Technology, 7(1).
Diane J. Litman and Shimei Pan. 2002. Designing and
evaluating an adaptive spoken dialogue system. User
Modeling and User-Adapted Interaction, 12.
D. Litman, J. Hirschberg, and M. Swerts. 2001. Predict-
ing user reactions to system error. In Proc.of ACL.
S. Narayanan. 2002. Towards modeling user behavior in
human-machine interaction: Effect of errors and emo-
tions. In Proc. of ISLE.
C. P. Rose and V. Aleven. 2002. Proc. of the ITS 2002
workshop on empirical methods for tutorial dialogue
systems. Technical report, San Sebastian, Spain, June.
C. P. Rose and R. Freedman. 2000. Building dialogue
systems for tutorial applications. Technical Report FS-
00-01 (Working Notes of the Fall Symposium), AAAI.
K. VanLehn, P. Jordan, C. Rose?, D. Bhembe, M. Bo?ttner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, R. Srivastava, and R. Wil-
son. 2002. The architecture of Why2-Atlas: A coach
for qualitative physics essay writing. In Proc. of ITS.
Predicting Emotion in Spoken Dialogue from Multiple Knowledge Sources
Kate Forbes-Riley
University of Pittsburgh
Learning Research and Development Center
Pittsburgh PA, 15260, USA
forbesk@pitt.edu
Diane J. Litman
University of Pittsburgh
Department of Computer Science
Learning Research and Development Center
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
We examine the utility of multiple types of
turn-level and contextual linguistic features for
automatically predicting student emotions in
human-human spoken tutoring dialogues. We
first annotate student turns in our corpus for
negative, neutral and positive emotions. We
then automatically extract features represent-
ing acoustic-prosodic and other linguistic in-
formation from the speech signal and associ-
ated transcriptions. We compare the results of
machine learning experiments using different
feature sets to predict the annotated emotions.
Our best performing feature set contains both
acoustic-prosodic and other types of linguistic
features, extracted from both the current turn
and a context of previous student turns, and
yields a prediction accuracy of 84.75%, which
is a 44% relative improvement in error reduc-
tion over a baseline. Our results suggest that
the intelligent tutoring spoken dialogue system
we are developing can be enhanced to automat-
ically predict and adapt to student emotions.
1 Introduction
This paper investigates the automatic classification of
student emotional states using acoustic-prosodic, non-
acoustic-prosodic, and contextual information, in a cor-
pus of human-human spoken tutoring dialogues. Motiva-
tion for this work comes from the discrepancy between
the performance of human tutors and current machine
tutors. In recent years, the development of computa-
tional tutorial dialogue systems has become more preva-
lent (Aleven and Rose, 2003), as one method of attempt-
ing to close the performance gap between human and
computer tutors. It has been hypothesized that the suc-
cess of such computer dialogue tutors could be further
increased by modeling and adapting to student emotion;
for example (Aist et al, 2002) have shown that adding
human-provided emotional scaffolding to an automated
reading tutor increases student persistence. We are build-
ing an intelligent tutoring spoken dialogue system with
the goal of using spoken and natural language processing
capabilities to automatically predict and adapt to student
emotions. Here we present results of an empirical study
demonstrating the feasibility of modeling student emo-
tion in a corresponding corpus of human-human spoken
tutoring dialogues.
Research in emotional speech has already shown that
acoustic and prosodic features can be extracted from the
speech signal and used to develop predictive models of
emotion. Much of this research has used databases of
speech read by actors or native speakers as training data
(often with semantically neutral content) (Oudeyer, 2002;
Polzin and Waibel, 1998; Liscombe et al, 2003). How-
ever, such prototypical emotional speech does not neces-
sarily reflect natural speech (Batliner et al, 2003), such
as found in tutoring dialogues. When actors are asked
to read the same sentence with different emotions, they
are restricted to conveying emotion using only acoustic
and prosodic features. In natural interactions, however,
speakers can convey emotions using other types of fea-
tures, and can also combine acoustic-prosodic and other
feature types. As a result of this mismatch, recent work
motivated by spoken dialogue applications has started to
use naturally-occurring speech to train emotion predic-
tors (Litman et al, 2001; Lee et al, 2001; Ang et al,
2002; Lee et al, 2002; Batliner et al, 2003; Devillers et
al., 2003; Shafran et al, 2003), but often predicts emo-
tions using only acoustic-prosodic features that would be
automatically available to a dialogue system in real-time.
With noisier data and fewer features, it is not surpris-
ing that acoustic-prosodic features alone have been found
to be of less predictive utility in these studies, leading
spoken dialogue researchers to supplement such features
with features based on other sources of information (e.g.,
lexical, syntactic, discourse).
Our methodology builds on and generalizes the results
of this prior work in spoken dialogue emotion predic-
tion, by introducing new linguistic and contextual fea-
tures, and exploring emotion prediction in the domain
of naturally occurring tutoring dialogues. We first an-
notate student turns in our human-human tutoring cor-
pus for emotion. We then automatically extract acoustic-
prosodic and other types of linguistic features from the
student utterances in our corpus, and from their local and
global dialogue contexts. We perform a variety of ma-
chine learning experiments using different feature com-
binations to predict our emotion categorizations. Our ex-
periments show that 1) by using either acoustic-prosodic
or other types of features alone, prediction accuracy is
significantly improved compared to a baseline classifier
for emotion prediction, 2) the addition of features identi-
fying specific subjects and tutoring sessions only some-
times improves performance, and 3) prediction accuracy
can typically be improved by combining features across
multiple knowledge sources, and/or by adding contextual
information. Our best learned model achieves a predic-
tion accuracy of 84.75%, which is a relative improvement
of 44% over the baseline error. Our results provide an
empirical basis for enhancing the corresponding spoken
dialogue tutoring system we are developing to automat-
ically predict and ultimately to adapt to a student model
that includes emotional states.
2 The Dialogue System and Corpus
We are currently building a spoken dialogue tutorial sys-
tem called ITSPOKE (Intelligent Tutoring SPOKEn dia-
logue system) (Litman and Silliman, 2004), with the goal
of automatically predicting and adapting to student emo-
tions. ITSPOKE uses as its ?back-end? the text-based
Why2-Atlas dialogue tutoring system (VanLehn et al,
2002). In ITSPOKE, a student types an essay answering
a qualitative physics problem. ITSPOKE then engages
the student in spoken dialogue to correct misconceptions
and elicit more complete explanations, after which the
student revises the essay, thereby ending the tutoring or
causing another round of tutoring/essay revision. Student
speech is digitized from microphone input and sent to the
Sphinx2 recognizer. The most probable ?transcription?
output by Sphinx2 is then sent to the Why2-Atlas natural
language back-end for syntactic, semantic and dialogue
analysis. Finally, the text response produced by the back-
end is sent to the Cepstral text-to-speech system, then
played to the student through a headphone. ITSPOKE
has been pilot-tested and a formal evaluation with stu-
dents is in progress.
Our human-human corpus contains spoken dialogues
collected via a web interface supplemented with an au-
dio link, where a human tutor performs the same task
as ITSPOKE. Our subjects are university students who
have taken no college physics and are native speakers
of American English. Our experimental procedure, tak-
ing roughly 7 hours/student over 1-2 sessions, is as fol-
lows: students 1) take a pretest measuring their physics
knowledge, 2) read a small document of background ma-
terial, 3) use the web and voice interface to work through
up to 10 problems with the human tutor (via essay revi-
sion as described above), and 4) take a post-test similar
to the pretest.1 Our corpus contains 149 dialogues from
17 students. An average dialogue contains 45.3 student
turns (242.2 words) and 44.1 tutor turns (1096.2 words).
A corpus example is shown in Figure 1, containing the
problem, the student?s original essay, and an annotated
(Section 3) excerpt from the subsequent spoken dialogue
(some punctuation is added for clarity).
PROBLEM (TYPED): If a car is able to accelerate at 2
m/s   , what acceleration can it attain if it is towing another
car of equal mass?
ESSAY (TYPED): The maximum acceleration a car can
reach when towing a car behind it of equal mass will
be halved. Therefore, the maximum acceleration will be
1m/s   .
DIALOGUE (SPOKEN): . . . 9.1 min. into session . . .
TUTOR  : Uh let us talk of one car first.
STUDENT  : ok. (EMOTION = NEUTRAL)
TUTOR   : If there is a car, what is it that exerts force on
the car such that it accelerates forward?
STUDENT   : The engine (EMOTION = POSITIVE)
TUTOR  : Uh well engine is part of the car, so how can
it exert force on itself?
STUDENT  : um... (EMOTION = NEGATIVE)
Figure 1: Excerpt from Human-Human Spoken Corpus
3 Annotating Student Emotion
In our spoken dialogue tutoring corpus, student emotional
states can only be identified indirectly ? via what is said
and/or how it is said. We have developed an annotation
scheme for hand labeling the student turns in our corpus
with respect to three types of perceived emotions (Litman
and Forbes-Riley, 2004):
Negative: a strong expression of emotion such as con-
fused, bored, frustrated, uncertain. Because a syntactic
question by definition expresses uncertainty, a turn con-
taining only a question is by default labeled negative. An
example negative turn is student  in Figure 1. Evidence
of a negative emotion comes from the lexical item ?um?,
1The human-human corpus corresponds to the human-
computer corpus that will result from ITSPOKE?s evaluation,
in that both corpora are collected using the same experimental
method, student pool, pre- and post-test, and physics problems.
as well as acoustic and prosodic features, e.g., prior and
post-utterance pausing and low pitch, energy and tempo.
Positive: a strong expression of emotion such as con-
dent, interested, encouraged. An example is student   in
Figure 1, with its lexical expression of certainty, ?The
engine?, and acoustic and prosodic features of louder
speech and faster tempo.
Neutral: no strong expression of emotion, including
weak (negative or positive) or contrasting (negative and
positive) expressions, as well as no expression. Because
groundings serve mainly to encourage another speaker
to continue speaking, a student turn containing only a
grounding is by default labeled neutral. An example is
student  in Figure 1. In this case, acoustic and prosodic
features such as moderate loudness and tempo give evi-
dence for the neutral label (rather than overriding it).
The features mentioned in the examples above were
elicited during post-annotation discussion, for expository
use in this paper. To avoid influencing the annotator?s in-
tuitive understanding of emotion expression, and because
such features are not used consistently or unambiguously
across speakers, our manual contains examples of labeled
dialogue excerpts (as in Figure 1) with links to corre-
sponding audio files, rather than a description of particu-
lar features associated with particular labels.
Our work differs from prior emotion annotations of
spontaneous spoken dialogues in several ways. Al-
though much past work predicts only two classes (e.g.,
negative/non-negative) (Batliner et al, 2003; Ang et al,
2002; Lee et al, 2001), our experiments produced the
best predictions using our three-way distinction. In con-
trast to (Lee et al, 2001), our classifications are context-
relative (relative to other turns in the dialogue), and task-
relative (relative to tutoring), because like (Ang et al,
2002), we are interested in detecting emotional changes
across our dialogues. Although (Batliner et al, 2003)
also employ a relative classification, they explicitly as-
sociate specific features with emotional utterances.
To analyze the reliability of our annotation scheme,
we randomly selected 10 transcribed dialogues from our
human-human tutoring corpus, yielding a dataset of 453
student turns. (Turn boundaries were manually anno-
tated prior to emotion annotation by a paid transcriber.)
The 453 turns were separately annotated by two differ-
ent annotators as negative, neutral or positive, follow-
ing the emotion annotation instructions described above.
The two annotators agreed on the annotations of 385/453
turns, achieving 84.99% agreement, with Kappa = 0.68.2
This inter-annotator agreement exceeds that of prior stud-
ies of emotion annotation in naturally occurring speech
2 	
ITSPOKE: An Intelligent Tutoring Spoken Dialogue System
Diane J. Litman
University of Pittsburgh
Department of Computer Science &
Learning Research and Development Center
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Scott Silliman
University of Pittsburgh
Learning Research and Development Center
Pittsburgh PA, 15260, USA
scotts@pitt.edu
Abstract
ITSPOKE is a spoken dialogue system that
uses the Why2-Atlas text-based tutoring sys-
tem as its ?back-end?. A student first types a
natural language answer to a qualitative physics
problem. ITSPOKE then engages the student
in a spoken dialogue to provide feedback and
correct misconceptions, and to elicit more com-
plete explanations. We are using ITSPOKE
to generate an empirically-based understanding
of the ramifications of adding spoken language
capabilities to text-based dialogue tutors.
1 Introduction
The development of computational tutorial dialogue sys-
tems has become more and more prevalent (Aleven and
Rose, 2003), as one method of attempting to close the
performance gap between human and computer tutors.
While many such systems have yielded successful evalu-
ations with students, most are currently text-based (Evens
et al, 2001; Aleven et al, 2001; Zinn et al, 2002; Van-
Lehn et al, 2002). There is reason to believe that speech-
based tutorial dialogue systems could be even more ef-
fective. Spontaneous self-explanation by students im-
proves learning gains during human-human tutoring (Chi
et al, 1994), and spontaneous self-explanation occurs
more frequently in spoken tutoring than in text-based tu-
toring (Hausmann and Chi, 2002). In human-computer
tutoring, the use of an interactive pedagogical agent that
communicates using speech rather than text output im-
proves student learning, while the visual presence or ab-
sence of the agent does not impact performance (Moreno
et al, 2001). In addition, it has been hypothesized that
the success of computer tutors could be increased by rec-
ognizing and responding to student emotion. (Aist et al,
2002) have shown that adding emotional processing to
a dialogue-based reading tutor increases student persis-
tence. Information in the speech signal such as prosody
has been shown to be a rich source of information for
predicting emotional states in other types of dialogue in-
teractions (Ang et al, 2002; Lee et al, 2002; Batliner et
al., 2003; Devillers et al, 2003; Shafran et al, 2003).
With advances in speech technology, several projects
have begun to incorporate basic spoken language capabil-
ities into their systems (Mostow and Aist, 2001; Fry et al,
2001; Graesser et al, 2001; Rickel and Johnson, 2000).
However, to date there has been little examination of the
ramifications of using a spoken modality for dialogue tu-
toring. To assess the impact and evaluate the utility of
adding spoken language capabilities to dialogue tutoring
systems, we have built ITSPOKE (Intelligent Tutoring
SPOKEn dialogue system), a spoken dialogue system
that uses the Why2-Atlas conceptual physics tutoringsys-
tem (VanLehn et al, 2002) as its ?back-end.? We are
using ITSPOKE as a platform for examining whether
acoustic-prosodic information can be used to improve the
recognition of pedagogically useful information such as
student emotion (Forbes-Riley and Litman, 2004; Lit-
man and Forbes-Riley, 2004), and whether speech can
improve the performance evaluations of dialogue tutoring
systems (e.g., as measured by learning gains, efficiency,
usability, etc.) (Rose? et al, 2003).
2 Application Description
ITSPOKE is a speech-enabled version of the Why2-
Atlas (VanLehn et al, 2002) text-based dialogue tutoring
system. As in Why2-Atlas, a student first types a nat-
ural language answer to a qualitative physics problem.
In ITSPOKE, however, the system engages the student
in a spoken dialogue to correct misconceptions and elicit
more complete explanations.
Consider the screenshot shown in Figure 1. ITSPOKE
first poses conceptual physics problem 58 to the student,
as shown in the upper right of the figure. Next, the
Figure 1: Screenshot during ITSPOKE Human-Computer Spoken Dialogue
user types in a natural language essay answer (as shown
in the essay box in the middle right of Figure 1), and
clicks ?SUBMIT.? ITSPOKE then analyzes the essay, af-
ter which the spoken dialogue with the student begins.
During the dialogue, the system and student discuss
a solution to the problem relative to the student?s es-
say explanation, using spoken English. At the time the
screenshot was generated, the student had just said ?free
fall.? After each system or student utterance, the system
prompt, or the system?s understanding of the student?s re-
sponse, respectively, are added to the dialogue history (as
shown in the dialogue box in the middle left of Figure 1).1
At some point later in the dialogue, the system will
eventually ask the student to edit the typed essay explana-
tion. The system will then either express satisfaction and
end the tutoring for the current problem, or continue with
another round of spoken dialogue interaction and typed
essay revision. Upon completing the problem, the stu-
dent is presented with an example of an ideal essay.
Another excerpt with ITSPOKE is shown in Figure 2.
1The ?Tell Tutor? box is used for typed student login only.
3 System Architecture
The architecture of ITSPOKE is shown in Figure 3. Stu-
dent speech is digitized from microphone input and sent
to the Sphinx2 automatic speech recognizer. Sphinx2?s
best hypothesis is then sent to Why2-Atlas for syntactic
and semantic analysis (Rose?, 2000), discourse and do-
main processing (Jordan et al, 2003), and finite-state di-
alogue management (Rose? et al, 2001). The tutor?s text
output is sent to the Cepstral text-to-speech system, and
played through a speaker or headphone.
ITSPOKE is implemented in Python and acts as a
proxy server between the Why2-Atlas server and client.
ITSPOKE monitors the XML being sent between the two
and decides what text should be spoken and when to lis-
ten. After speech recognition, ITSPOKE submits what it
thinks the student said to Why2-Atlas (as if the student
had typed it). Sphinx2?s C API is tied into Python by us-
ing SWIG to generate a Python wrapper API. Cepstral?s
speech synthesis engine is currently run out of process
using a fork/exec, but we plan to use SWIG to tie its C
API into Python in the future. For speed, we have in-
ITSPOKE: I didn?t understand. What force acts on all objects
within close proximity to earth?
STUDENT: uh the force of gravity
ITSPOKE: Fine. Besides the Earth?s gravitational force, are
there any other forces acting on the person, keys, and elevator
after the release?
STUDENT: no
ITSPOKE: Yep. Thus, keep in mind that after the release, the
only force on the person, keys, and elevator is the force of grav-
ity. If gravity is the only force acting on an object, will it be
moving or staying still?
STUDENT: it will be moving
ITSPOKE: Fine. Not only are the person, keys, and elevator
moving, they have only gravitational forces on them. When an
object is falling and has only gravitational force on it, it is said
to be in what?
STUDENT: freefall
ITSPOKE: Good. Thus, after the release the only force on
the keys and man is the downward force of earth?s gravity, so
they are in freefall. Before the release, what is the relationship
between the man?s velocity and the keys? velocity?
STUDENT: they?re equal
Figure 2: ITSPOKE Excerpt (3.5 minutes into session)
Figure 3: The Architecture of ITSPOKE
stalled Sphinx2 and Cepstral on the ITSPOKE machine.
The dashed line in the figure reflects that Why2-Atlas can
be installed on a different machine, although we are cur-
rently running it on the same machine.
4 Performance Analysis
A formal evaluation comparing ITSPOKE and other tu-
toring methods began in November 2003, and is still on-
going. Subjects are University of Pittsburgh students who
have taken no college physics and are native speakers of
American English. Our experimental procedure, taking
roughly 4 hours/student, is as follows: students 1) read a
small document of background material, 2) take a pretest
measuring their physics knowledge, 3) use ITSPOKE to
work through 5 physics problems, and 4) take a post-test
similar to the pretest.
As of March 2004, we have collected 80 dialogues
from 16 students (21 total hours of speech, mean dialogue
time of 17 minutes). An average dialogue contains 21.3
student turns and 26.3 tutor turns. The mean student turn
length is 2.8 words (max=28, min=1).2
ITSPOKE uses 56 dialogue-state dependent language
models for speech recognition; 43 of these 56 models
have been used to process the data collected to date.3
These stochastic language models were initially trained
using 4551 typed student utterances from a 2002 eval-
uation of Why2-Atlas, then later enhanced with spo-
ken utterances obtained during ITSPOKE?s pilot testing.
For the 1600 student turns that we have collected, IT-
SPOKE?s current Word Error Rate is 31.2%. While this is
the traditional method of evaluating speech recognition,
semantic rather than transcription accuracy is more useful
for dialogue evaluation as it does not penalize for word
errors that are unimportant to overall utterance interpre-
tation. Semantic analysis based on speech recognition
is the same as based on perfect transcription 92% of the
time. An average dialogue contains 1.4 rejection prompts
(when ITSPOKE is not confident of the speech recogni-
tion output, it asks the user to repeat the utterance), and
.8 timeout prompts (when the student doesn?t say any-
thing within a specified time frame, ITSPOKE repeats its
previous question).
5 Summary
The goal of ITSPOKE is to generate an empirically-based
understanding of the implications of using speech instead
of text-based dialogue tutoring, and to use these results
to build an improved version of ITSPOKE. We are cur-
rently analyzing our corpus of dialogues with ITSPOKE
to determine whether spoken dialogues yield increased
performance compared to text with respect to a variety
of evaluation metrics, and whether acoustic-prosodic fea-
tures only found in speech can be used to better predict
pedagogically useful information such as student emo-
tions. Our next step will be to modify the dialogue man-
ager inherited from Why2-Atlas to use new tutorial strate-
gies optimized for speech, and to enhance ITSPOKE to
predict and adapt to student emotion. In previous work
on adaptive (non-tutoring) dialogue systems (Litman and
Pan, 2002), adaptation to problematic dialogue situations
measurably improved system performance.
Acknowledgments
This research has been supported by NSF Grant Nos.
9720359, 0328431 and 0325054, and by the Office of
2Word count is estimated from speech recognition output.
3The remaining language models correspond to physics
problems that are not being tested in the current evaluation.
Naval Research. Thanks to Kurt VanLehn and the Why2-
Atlas team for the use and modification of their system.
References
G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Picard.
2002. Experimentally augmenting an intelligent tutor-
ing system with human-supplied capabilities: Adding
Human-Provided Emotional Scaffolding to an Auto-
mated Reading Tutor that Listens. In Proc. of Intel-
ligent Tutoring Systems.
V. Aleven and C. P. Rose. 2003. Proc. of the AIED 2003
Workshop on Tutorial Dialogue Systems: With a View
toward the Classroom.
V. Aleven, O. Popescu, and K. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In J. D. Moore, C. L. Redfield, and W. L. John-
son, editors, Proc. of Articial Intelligence in Educa-
tion, pages 246?255.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. of ICSLP.
A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. Noth.
2003. How to find trouble in communication. Speech
Communication, 40:117?143.
M. Chi, N. De Leeuw, M. Chiu, and C. Lavancher.
1994. Eliciting self-explanations improves under-
standing. Cognitive Science, 18:439?477.
L. Devillers, L. Lamel, and I. Vasilescu. 2003. Emotion
detection in task-oriented spoken dialogs. In Proc. of
ICME.
M. Evens, S. Brandle, R. Chang, R. Freedman, M. Glass,
Y. Lee, L. Shim, C. Woo, Y. Zhang, Y. Zhou,
J. Michaeland, and Allen A. Rovick. 2001. Circsim-
tutor: An Intelligent Tutoring System Using Natural
Language Dialogue. In Proc. Midwest AI and Cogni-
tive Science Conference.
K. Forbes-Riley and D. Litman. 2004. Predicting
emotion in spoken dialogue from multiple knowledge
sources. In Proc. Human Language Technology Con-
ference and North American Chapter of the Associa-
tion for Computational Linguistics.
J. Fry, M. Ginzton, S. Peters, B. Clark, and H. Pon-Barry.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proc. SIGdial Workshop
on Discourse and Dialogue.
A. Graesser, N. Person, and D. Harter et al 2001. Teach-
ing tactics and dialog in Autotutor. International Jour-
nal of Articial Intelligence in Education.
R. Hausmann and M. Chi. 2002. Can a computer inter-
face support self-explaining? The International Jour-
nal of Cognitive Technology, 7(1).
P. Jordan, M. Makatchev, and K. VanLehn. 2003. Ab-
ductive theorem proving for analyzing student expla-
nations. In Proc. Articial Intelligence in Education.
C.M. Lee, S. Narayanan, and R. Pieraccini. 2002. Com-
bining acoustic and language information for emotion
recognition. In Proc. of ICSLP.
D. J. Litman and K. Forbes-Riley. 2004. Annotating stu-
dent emotional states in spoken tutoring dialogues. In
Proc. SIGdial Workshop on Discourse and Dialogue.
D. J. Litman and S. Pan. 2002. Designing and evaluating
an adaptive spoken dialogue system. User Modeling
and User-Adapted Interaction, 12.
R. Moreno, R.E. Mayer, H. A. Spires, and J. C. Lester.
2001. The case for social agency in computer-based
teaching: Do students learn more deeply when they
interact with animated pedagogical agents. Cognition
and Instruction, 19(2):177?213.
J. Mostow and G. Aist. 2001. Evaluating tutors that lis-
ten: An overview of Project LISTEN. In K. Forbus and
P. Feltovich, editors, Smart Machines in Education.
J. Rickel and W. L. Johnson. 2000. Task-oriented col-
laboration with embodied agents in virtual worlds. In
J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, ed-
itors, Embodied Conversational Agents.
C. P. Rose?, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. Interactive conceptual
tutoring in Atlas-Andes. In Proc. Articial Intelligence
in Education.
C. P. Rose?, D. Litman, D. Bhembe, K. Forbes, S. Silli-
man, R. Srivastava, and K. VanLehn. 2003. A com-
parison of tutor and student behavior in speech versus
text based tutoring. In Proc. HLT/NAACL Workshop:
Building Educational Applications Using NLP.
C. P. Rose?. 2000. A framework for robust sentence level
interpretation. In Proc. North American Chapter of the
Association for Computational Lingusitics.
I. Shafran, M. Riley, and M. Mohri. 2003. Voice signa-
tures. In Proc. of Automatic Speech Recognition and
Understanding.
K. VanLehn, P. W. Jordan, C. Rose?, D. Bhembe,
M. Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intelligent Tutoring Systems.
C. Zinn, J. D. Moore, and M. G. Core. 2002. A 3-tier
planning architecture for managing tutorial dialogue.
In Proc. Intelligent Tutoring Systems.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 264?271,
New York, June 2006. c?2006 Association for Computational Linguistics
Modelling User Satisfaction and Student Learning in a Spoken Dialogue
Tutoring System with Generic, Tutoring, and User Affect Parameters
Kate Forbes-Riley
Learning Research & Development Ctr
University of Pittsburgh
Pittsburgh, PA 15260
forbesk@cs.pitt.edu
Diane J. Litman
Learning Research & Development Ctr
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Abstract
We investigate using the PARADISE
framework to develop predictive models
of system performance in our spoken di-
alogue tutoring system. We represent per-
formance with two metrics: user satis-
faction and student learning. We train
and test predictive models of these met-
rics in our tutoring system corpora. We
predict user satisfaction with 2 parameter
types: 1) system-generic, and 2) tutoring-
specific. To predict student learning, we
also use a third type: 3) user affect. Al-
hough generic parameters are useful pre-
dictors of user satisfaction in other PAR-
ADISE applications, overall our parame-
ters produce less useful user satisfaction
models in our system. However, generic
and tutoring-specific parameters do pro-
duce useful models of student learning in
our system. User affect parameters can in-
crease the usefulness of these models.
1 Introduction
In recent years the development of spoken dialogue
tutoring systems has become more prevalent, in an
attempt to close the performance gap between hu-
man and computer tutors (Mostow and Aist, 2001;
Pon-Barry et al, 2004; Litman et al, 2006). Student
learning is a primary metric for evaluating the per-
formance of these systems; it can be measured, e.g.,
by comparing student pretests taken prior to system
use with posttests taken after system use.
In other types of spoken dialogue systems, the
user?s subjective judgments about using the system
are often considered a primary system performance
metric; e.g., user satisfaction has been measured
via surveys which ask users to rate systems during
use along dimensions such as task ease, speech in-
put/output quality, user expectations and expertise,
and user future use (Mo?ller, 2005b; Walker et al,
2002; Bonneau-Maynard et al, 2000; Walker et al,
2000; Shriberg et al, 1992). However, it is expen-
sive to run experiments over large numbers of users
to obtain reliable system performance measures.
The PARADISE model (Walker et al, 1997) pro-
poses instead to predict system performance, using
parameters representing interaction costs and bene-
fits between system and user, including task success,
dialogue efficiency, and dialogue quality. More for-
mally, a set of interaction parameters are measured
in a spoken dialogue system corpus, then used in
a multivariate linear regression to predict the target
performance variable. The resulting model is de-
scribed by the formula below, where there are n in-
teraction parameters, pi, each weighted by the anal-ysis with a coefficient, wi, which will be negativeor positive, depending on whether the model treats
pi as a cost or benefit, respectively. The model canthen be used to estimate performance during system
design, with the design goals of minimizing costs
and maximizing benefits.
System Performance = ?ni=1 wi * pi
We investigate using PARADISE to develop pre-
dictive models of performance in our spoken dia-
logue tutoring system. Although to our knowledge,
264
Corpus Date Voice #Dialogues #Students #with Survey #with Tests #with Affect
SYN03 2003 synthesized 100 20 0 20 20
PR05 2005 pre-recorded 140 28 28 28 17
SYN05 2005 synthesized 145 29 29 29 0
Table 1: Summary of our 3 ITSPOKE Corpora
prior PARADISE applications have only used user
satisfaction to represent performance, we hypothe-
size that other metrics may be more relevant when
PARADISE is applied to tasks that are not optimized
for user satisfaction, such as our spoken dialogue tu-
toring system. We thus use 2 metrics to represent
performance: 1) a generic metric of user satisfaction
computed via user survey, 2) a tutoring-specific met-
ric of student learning computed via student pretest
and posttest scores. We train and test predictive
models of these metrics on multiple system corpora.
To predict user satisfaction, we use 2 types of in-
teraction parameters: 1) system-generic parameters
such as used in other PARADISE applications, e.g.
speech recognition performance, and 2) tutoring-
specific parameters, e.g. student correctness. To
predict student learning, we also use a third type of
parameter: 3) manually annotated user affect. Al-
though prior PARADISE applications have tended to
use system-generic parameters, we hypothesize that
task-specific and user affect parameters may also
prove useful. We emphasize that user affect parame-
ters are still system-generic; user affect has been an-
notated and/or automatically predicted in other types
of spoken dialogue systems, e.g. as in (Lee et al,
2002; Ang et al, 2002; Batliner et al, 2003).
Our results show that, although generic param-
eters were useful predictors of user satisfaction in
other PARADISE applications, overall our parame-
ters produce less useful user satisfaction models in
our tutoring system. However, generic and tutoring-
specific parameters do produce useful models of stu-
dent learning in our system. Generic user affect pa-
rameters increase the usefulness of these models.
2 Spoken Dialogue Tutoring Corpora
ITSPOKE (Intelligent Tutoring SPOKEn dialogue
system) (Litman et al, 2006) is a speech-enabled tu-
tor built on top of the text-based Why2-Atlas con-
ceptual physics tutor (VanLehn et al, 2002). In
ITSPOKE, a student first types an essay into a
web-based interface answering a qualitative physics
problem. ITSPOKE then analyzes the essay and en-
gages the student in spoken dialogue to correct mis-
conceptions and elicit more complete explanations.
Student speech is digitized from the microphone in-
put and sent to the Sphinx2 recognizer. Sphinx2?s
most probable ?transcription? is then sent to Why2-
Atlas for syntactic, semantic and dialogue analy-
sis. Finally, the text response produced by Why2-
Atlas is converted to speech as described below, then
played in the student?s headphones and displayed on
the interface. After the dialogue, the student revises
the essay, thereby ending the tutoring or causing an-
other round of tutoring/essay revision.
For this study, we used 3 ITSPOKE corpora,
shown in Table 1.1 The SYN03 corpus was col-
lected in 2003 for an evaluation comparing learn-
ing in typed and spoken human and computer tu-
toring (Litman et al, 2006). ITSPOKE?s voice was
synthesized with the Cepstral text-to-speech system,
and its speech recognizer was trained from pilot IT-
SPOKE studies and Why2-Atlas evaluations. The
PR05 and SYN05 corpora were collected in 2005,
to evaluate the impact of tutor voice quality (Forbes-
Riley et al, 2006). For these 2 corpora, ITSPOKE
used an updated speech recognizer further trained
on the SYN03 corpus. For the SYN05 corpus, IT-
SPOKE used the synthesized tutor voice from the
SYN03 corpus; for the PR05 corpus, ITSPOKE
used a pre-recorded tutor voice from a paid voice
talent. Figure 1 gives an annotated (Section 3) PR05
excerpt (ASR shows what ITSPOKE heard).
The same experimental procedure was used to
collect all 3 ITSPOKE corpora: college students
who had taken no college physics: 1) read a
small document of background material, 2) took
a pretest measuring initial physics knowledge, 3)
work through a set of 5 problems (dialogues) with
ITSPOKE, 4) took a posttest similar to the pretest.
1The user populations of the 2005 corpora and 2003 corpus
are different, due to variation in year and recruitment method.
265
Subjects in the PR05 and SYN05 corpora also
completed a survey probing user satisfaction after
taking the posttest (SYN03 corpus subjects did not).
Our survey, shown in Figure 2, is essentially the
same as the one used in the DARPA Communicator
multi-site evaluation (Walker et al, 2002). Although
tailored lexically for a tutoring system, these state-
ments are generally applicable to spoken dialogue
systems. Students rated their degree of agreement
with each statement on a scale of 1 to 5.
PHYSICS PROBLEM: Suppose a lightweight car and a mas-
sive truck hit a patch of frictionless ice and have a head-on col-
lision. Upon which vehicle is the impact force greater? Which
vehicle undergoes the greater change in its motion? Defend
your answers.
ESSAY UNDER DISCUSSION: The truck exerts an equal and
opposite reaction force on the car. The car has a greater accel-
eration. The car will have a greater change in its motion.
. . . excerpt at 28.8 minutes into dialogue. . .
TUTOR60: So, when you add forces together, besides the mag-nitudes of the forces, what must you also consider?
STUDENT57: direct- direction (ASR: the truck in direction)
[correct, uncertain, non-frustrated/angry]
TUTOR61: Yes. [...] The vector sum of the forces acting onthe body is also equal to the mass of the body multiplied by its
acceleration. What is another term for the vector sum of forces
acting on a body?
STUDENT58: net force (ASR: net force) [correct, neutral,
non-frustrated/angry]
TUTOR62: That?s super. Another term for the resultant forceis the net force. In this relationship, what is the direction of
the net force applied to a body and the direction of the body?s
acceleration?
STUDENT59: same same same (ASR: seems same typeseems) [correct, certain, frustrated/angry]
Figure 1: Annotated PR05 Corpus Excerpt
Task Ease: It was easy to learn from the tutor.
TTS Performance: It was easy to understand the tutor.
User Expertise: I knew what I could say or do at each point in
the conversations with the tutor.
Expected Behavior: The tutor worked the way I expected it to.
Future Use: Based on my experience using the tutor to learn
physics, I would like to use such a tutor regularly.
ALMOST ALWAYS (5), OFTEN (4), SOMETIMES (3),
RARELY (2), ALMOST NEVER (1)
Figure 2: ITSPOKE Survey Questionnaire
3 Interaction Parameters
3.1 Dialogue System-Generic Parameters
Prior PARADISE applications predicted user satis-
faction using a wide range of system-generic param-
eters, which include measures of speech recognition
quality (e.g. word error rate), measures of dialogue
communication and efficiency (e.g. total turns and
elapsed time), and measures of task completion (e.g.
a binary representation of whether the task was com-
pleted) (Mo?ller, 2005a; Mo?ller, 2005b; Walker et al,
2002; Bonneau-Maynard et al, 2000; Walker et al,
2000; Walker et al, 1997). In this prior work, each
dialogue between user and system represents a sin-
gle ?task? (e.g., booking airline travel), thus these
measures are calculated on a per-dialogue basis.
In our work, the entire tutoring session represents
a single ?task?, and every student in our corpora
completed this task. Thus we extract 13 system-
generic parameters on a per-student basis, i.e. over
the 5 dialogues for each user, yielding a single pa-
rameter value for each student in our 3 corpora.
First, we extracted 9 parameters representing dia-
logue communication and efficiency. Of these pa-
rameters, 7 were used in prior PARADISE appli-
cations: Time on Task, Total ITSPOKE Turns and
Words, Total User Turns and Words, Average IT-
SPOKE Words/Turn, and Average User Words/Turn.
Our 2 additional ?communication-related? (Mo?ller,
2005a) parameters measure system-user interactiv-
ity, but were not used in prior work (to our knowl-
edge): Ratio of User Words to ITSPOKE Words, Ra-
tio of User Turns to ITSPOKE Turns.
Second, we extracted 4 parameters representing
speech recognition quality, which have also been
used in prior work: Word Error Rate, Concept Ac-
curacy, Total Timeouts, Total Rejections2 .
3.2 Tutoring-Specific Parameters
Although prior PARADISE applications tend to
use system-generic parameters, we hypothesize that
task-specific parameters may also prove useful for
predicting performance. We extract 12 tutoring-
specific parameters over the 5 dialogues for each stu-
dent, yielding a single parameter value per student,
for each student in our 3 corpora. Although these pa-
rameters are specific to our tutoring system, similar
parameters are available in other tutoring systems.
First, we hypothesize that the correctness of the
students? turns with respect to the tutoring topic
2A Timeout occurs when ITSPOKE does not hear speech
by a pre-specified time interval. A Rejection occurs when IT-
SPOKE?s confidence score for its ASR output is too low.
266
(physics, in our case) may play a role in predicting
system performance. Each of our student turns is
automatically labeled with 1 of 3 ?Correctness? la-
bels by the ITSPOKE semantic understanding com-
ponent: Correct, Incorrect, Partially Correct. La-
beled examples are shown in Figure 1. From these
3 Correctness labels, we derive 9 parameters: a To-
tal and a Percent for each label, and a Ratio of each
label to every other label (e.g. Correct/Incorrect).
Second, students write and then may modify their
physics essay at least once during each dialogue with
ITSPOKE. We thus hypothesize that like ?Correct-
ness?, the total number of essays per student may
play a role in predicting system performance.
Finally, although student test scores before/after
using ITSPOKE will be used as our student learning
metric, we hypothesize that these scores may also
play a role in predicting user satisfaction.
3.3 User Affect Parameters
We hypothesize that user affect plays a role in pre-
dicting user satisfaction and student learning. Al-
though affect parameters have not been used in other
PARADISE studies (to our knowledge), they are
generic; for example, in various spoken dialogue
systems, user affect has been annotated and automat-
ically predicted from e.g., acoustic-prosodic and lex-
ical features (Litman and Forbes-Riley, 2004b; Lee
et al, 2002; Ang et al, 2002; Batliner et al, 2003).
As part of a larger investigation into emotion
adaptation, we are manually annotating the stu-
dent turns in our corpora for affective state. Cur-
rently, we are labeling 1 of 4 states of ?Certain-
ness?: certain, uncertain, neutral, mixed (certain
and uncertain), and we are separately labeling 1
of 2 states of ?Frustration/Anger?: frustrated/angry,
non-frustrated/angry. These affective states3 were
found in pilot studies to be most prevalent in our tu-
toring dialogues4 , and are also of interest in other
dialogue research, e.g. tutoring (Bhatt et al, 2004;
Moore et al, 2004; Pon-Barry et al, 2004) and spo-
ken dialogue (Ang et al, 2002). Labeled examples
are shown in Figure 1.5 To date, one paid annotator
3We use ?affect? and ?affective state? loosely to cover stu-
dent emotions and attitudes believed to be relevant for tutoring.
4For a full list of affective states identified in these pilot stud-
ies, see (Litman and Forbes-Riley, 2004a).
5Annotations were performed from both audio and tran-
has labeled all student turns in our SYN03 corpus,
and all the turns of 17 students in our PR05 corpus.6
From these labels, we derived 25 User Affect pa-
rameters per student, over the 5 dialogues for that
student. First, for each Certainness label, we com-
puted a Total, a Percent, and a Ratio to each other la-
bel. We also computed a Total for each sequence of
identical Certainness labels (e.g. Certain:Certain),
hypothesizing that states maintained over multiple
turns may have more impact on performance than
single occurrences. Second, we computed the same
parameters for each Frustration/Anger label.
4 Prediction Models
In this section, we first investigate the usefulness of
our system-generic and tutoring-specific parameters
for training models of user satisfaction and student
learning in our tutoring corpora with the PARADISE
framework. We use the SPSS statistical package
with a stepwise multivariate linear regression pro-
cedure7 to automatically determine parameter inclu-
sion in the model. We then investigate how well
these models generalize across different user-system
configurations, by testing the models in different
corpora and corpus subsets. Finally, we investigate
whether generic user affect parameters increase the
usefulness of our student learning models.
4.1 Prediction Models of User Satisfaction
Only subjects in the PR05 and SYN05 corpora com-
pleted a user survey (Table 1). Each student?s re-
sponses were summed to yield a single user satis-
faction total per student, ranging from 9 to 24 across
corpora (the possible range is 5 to 25), with no dif-
ference between corpora (p = .46). This total was
used as our user satisfaction metric, as in (Mo?ller,
2005b; Walker et al, 2002; Walker et al, 2000).8
scription within a speech processing tool.
6In a preliminary agreement study, a second annotator la-
beled the entire SYN03 corpus for uncertain versus other, yield-
ing 90% inter-annotator agreement (0.68 Kappa).
7At each step, the parameter with the highest partial correla-
tion with the target predicted variable, controlled for all previ-
ously entered parameters, is entered in the equation, until the re-
maining parameters do not increase R2 by a significant amount
or do not yield a significant model.
8Researchers have also used average score (Mo?ller, 2005b;
Walker et al, 1997); single survey statements can also be
used (Walker et al, 1997). We tried these variations, and our
R2 results were similar, indicating robustness across variations.
267
Training Data R2 Predictors Testing Data R2
PR05 .274 INCORRECTS, ESSAYS SYN05 .001
SYN05 .068 TUT WDS/TRN PR05 .018
PR05:half1 .335 PARTCORS/INCORS PR05:half2 .137
PR05:half2 .443 STU TRNS PR05:half1 .079
SYN05:half1 .455 STU TRNS/TUT TRNS SYN05:half2 .051
SYN05:half2 .685 TUT WDS/TRN, STU WDS/TRN, CORRECTS SYN05:half1 .227
Table 2: Testing the Predictive Power of User Satisfaction Models
We trained a user satisfaction model on each cor-
pus, then tested it on the other corpus. In addition,
we split each corpus in half randomly, then trained
a user satisfaction model on each half, and tested
it on the other half. We hypothesized that despite
the decrease in the dataset size, models trained and
tested in the same corpus would have higher gen-
eralizability than models trained on one corpus and
tested on the other, due to the increased data homo-
geneity within each corpus, since each corpus used a
different ITSPOKE version. As predictors, we used
only the 13 system-generic and 12 tutoring-specific
parameters that were available for all subjects.
Results are shown in Table 2. The first and fourth
columns show the training and test data, respec-
tively. The second and fifth columns show the user
satisfaction variance accounted for by the trained
model in the training and test data, respectively. The
third column shows the parameters that were se-
lected as predictors of user satisfaction in the trained
model, ordered by degree of contribution9 .
For example, as shown in the first row, the model
trained on the PR05 corpus uses Total Incorrect stu-
dent turns as the strongest predictor of user satis-
faction, followed by Total Essays; these parameters
are not highly correlated10 . This model accounts for
27.4% of the user satisfaction variance in the PR05
corpus. When tested on the SYN05 corpus, it ac-
counts for 0.1% of the user satisfaction variance.
The low R2 values for both training and testing
in the first two rows show that neither corpus yields
9The ordering reflects the standardized coefficients (beta
weights), which are computed in SPSS based on scaling of
the input parameters, to enable an assessment of the predictive
power of each parameter relative to the others in a model.
10Hereafter, predictors in a model are not highly correlated
(R ? .70) unless noted. Linear regression does not assume that
predictors are independent, only that they are not highly corre-
lated. Because correlations above R =.70 can affect the coeffi-
cients, deletion of redundant predictors may be advisable.
a very powerful model of user satisfaction even in
the training corpus, and this model does not gener-
alize very well to the test corpus. As hypothesized,
training and testing in a single corpus yields higher
R2 values for testing, as shown in the last four rows,
although these models still account for less than a
quarter of the variance in the test data. The increased
R2 values for training here may indicate over-fitting.
Across all 6 experiments, there is almost no overlap
of parameters used to predict user satisfaction.
Overall, these results show that this method of
developing an ITSPOKE user satisfaction model
is very sensitive to changes in training data;
this was also found in other PARADISE applica-
tions (Mo?ller, 2005b; Walker et al, 2000). Some
applications have also reported similarly low R2 val-
ues for testing both within a corpus (Mo?ller, 2005b)
and also when a model trained on one system cor-
pus is tested on another system corpus (Walker et
al., 2000). However, most PARADISE applications
have yielded higher R2 values than ours for train-
ing (Mo?ller, 2005b; Walker et al, 2002; Bonneau-
Maynard et al, 2000; Walker et al, 2000).
We hypothesize two reasons for why our exper-
iments did not yield more useful user satisfaction
models. First, in prior PARADISE applications,
users completed a survey after every dialogue with
the system. In our case, subjects completed only one
survey, at the end of the experiment (5 dialogues). It
may be that this ?per-student? unit for user satisfac-
tion is too large to yield a very powerful model; i.e.,
this measure is not fine-grained enough. In addi-
tion, tutoring systems are not designed to maximize
user satisfaction, but rather, their design goal is to
maximize student learning. Moreover, prior tutor-
ing studies have shown that certain features corre-
lated with student learning do not have the same re-
lationship to user satisfaction (e.g. are not predictive
268
Training Data R2 Predictors Testing Data R2
PR05 .556 PRE, %CORRECT SYN05 .636
SYN05 .736 PRE, INCORS/CORS, STU WDS/TRN PR05 .472
PR05:half1 .840 PRE, PARTCORRECTS PR05:half2 .128
PR05:half2 .575 PARTCORS/INCORS, PRE PR05:half1 .485
SYN05:half1 .580 PRE, STU WDS/TRN SYN05:half2 .556
SYN05:half2 .855 PRE, TIMEOUTS SYN05:half1 .384
PR05+SYN03 .413 PRE, TIME SYN05 .586
PR05+SYN05 .621 PRE, INCORS/CORS SYN03 .237
SYN05+SYN03 .590 INCORS/CORS, PR05 .244
%INCORRECT, PRE, TIME
Table 3: Testing the Predictive Power of Student Learning Models with the Same Datasets
or have an opposite relationship) (Pon-Barry et al,
2004). In fact, it may be that user satisfaction is not
a metric of primary relevance in our application.
4.2 Prediction Models of Student Learning
As in other tutoring research, e.g. (Chi et al, 2001;
Litman et al, 2006), we use posttest score (POST)
controlled for pretest score (PRE) as our target stu-
dent learning prediction metric, such that POST is
our target variable and PRE is always a parameter
in the final model, although it is not necessarily the
strongest predictor.11 In this way, we measure stu-
dent learning gains, not just final test score.
As shown in Table 1, all subjects in our 3 corpora
took the pretest and posttest. However, in order to
compare our student learning models with our user
satisfaction models, our first experiments predicting
student learning used the same training and testing
datasets that were used to predict user satisfaction in
Section 4.1 (i.e. we ran the same experiments except
we predicted POST controlled for PRE instead of
user satisfaction). Results are shown in the first 6
rows of Table 3.
As shown, these 6 models all account for more
than 50% of the POST variance in the training data.
Furthermore, most of them account for close to, or
more than, 50% of the POST variance in the test
data. Although again we hypothesized that training
and testing in one corpus would yield higher R2 val-
ues for testing, this is not consistently the case; two
of these models had the highest R2 values for train-
11In SPSS, we regress two independent variable blocks. The
first block contains PRE, which is regressed with POST using
the ?enter? method, forcing inclusion of PRE in the final model.
The second block contains all remaining independent variables,
which are regressed using the stepwise method.
ing and the lowest R2 values for testing (PR05:half1
and SYN05:half2), suggesting over-fitting.
Overall, these results show that this is an effec-
tive method of developing a prediction model of stu-
dent learning for ITSPOKE, and is less sensitive to
changes in training data than it was for user satis-
faction. Moreover, there is more overlap in these
6 models of parameters that are useful for predict-
ing student learning (besides PRE); ?Correctness?
parameters and dialogue communication and effi-
ciency parameters appear to be most useful overall.
Our next 3 experiments investigated how our stu-
dent learning models are impacted by including our
third SYN03 corpus. Using the same 25 parame-
ters, we trained a learning model on each set of two
combined corpora, then tested it on the other corpus.
Results are shown in the last 3 rows of Table 3.
As shown, these models still account for close
to, or more than, 50% of the student learning vari-
ance in the training data.12 The model trained
on PR05+SYN03 accounts for the most student
learning variance in the test data, showing that the
training data that is most similar to the test data
will yield the highest generalizability. That is,
the combined PR05+SYN03 corpora contains sub-
jects drawn from the same subject pool (2005) as
the SYN05 test data, and also contains subjects
who interacted with the same tutor voice (synthe-
sized) as this test data. In contrast, the combined
PR05+SYN05 corpora did not overlap in user pop-
ulation with the SYN03 test data, and the combined
SYN05+SYN03 corpora did not share a tutor voice
with the PR05 test data. ?Correctness? parameters
12However, INCORS/CORS and %INCORRECT are highly
correlated in the SYN05+SYN03 model, showing redundancy.
269
Training Data R2 Predictors Testing Data R2
SYN03 (affect) .644 TIME, PRE, NEUTRAL PR05:17 .411
PR05:17 (affect) .835 PRE, NFA:NFA, STU WDS/TRN SYN03 .127
SYN03 .478 PRE, TIME PR05:17 .340
PR05:17 .609 PRE, STU TRNS/TUT TRNS SYN03 .164
Table 4: Testing the Predictive Power of Student Learning Models with User Affect Parameters
and dialogue communication and efficiency param-
eters are consistently used as predictors in all 9 of
these student learning models.
4.3 Adding User Affect Parameters
Our final experiments investigated whether our 25
user affect parameters impacted the usefulness of
the student learning models. As shown in Table 1,
all 20 subjects in our SYN03 corpus were annotated
for user affect, and 17 subjects in our PR05 corpus
were annotated for user affect. We trained a model
of student learning on each of these datasets, then
tested it on the other dataset.13 As predictors, we
included our 25 user affect parameters along with
the 13 system-generic and 12 tutoring-specific inter-
action parameters. These results are shown in the
first two rows of Table 4. We also reran these ex-
periments without user affect parameters, to gauge
the impact of the user affect parameters. These re-
sults are shown in the last two rows of Table 4. We
hypothesized that user affect parameters would pro-
duce more useful models, because prior tutoring re-
search has shown correlations between user affect
and student learning (e.g. (Craig et al, 2004)).
As shown in the first two rows, user affect predic-
tors appear in both models where these parameters
were included. The models trained on SYN03 use
pretest score and Total Time on Task as predictors;
when affect parameters are included, ?Neutral Cer-
tainness? is added as a predictor, which increases the
R2 values for both training and testing. However,
the two models trained on PR05:17 show no predic-
tor overlap (besides PRE). Moreover, the PR05:17
model that includes an affect predictor (Total Se-
quence of 2 Non-Frustrated/Angry turns) has the
highest training R2, but the lowest testing R2 value.
13As only 17 subjects have both user affect annotation and
user surveys, there is not enough data currently to train and test
a user satisfaction model including user affect parameters.
5 Conclusions and Current Directions
Prior work in the tutoring community has focused on
correlations of single features with learning; our re-
sults suggest that PARADISE is an effective method
of extending these analyses. For the dialogue com-
munity, our results suggest that as spoken dialogue
systems move into new applications not optimized
for user satisfaction, such as tutoring systems, other
measures of performance may be more relevant, and
generic user affect parameters may be useful.
Our experiments used many of the same system-
generic parameters as prior studies, and some of
these parameters predicted user satisfaction both in
our models and in prior studies? models (e.g., sys-
tem words/turn (Walker et al, 2002)). Nonetheless,
overall our user satisfaction models were not very
powerful even for training, were sensitive to training
data changes, showed little predictor overlap, and
did not generalize well to test data. Our user sat-
isfaction metric may not be fine-grained enough; in
other PARADISE studies, users took a survey after
every dialogue with the system. In addition, tutoring
systems are not designed to maximize user satisfac-
tion; their goal is to maximize student learning.
Our student learning models were much more
powerful and less sensitive to changes in training
data. Our best models explained over 50% of the stu-
dent learning variance for training and testing, and
both student ?Correctness? parameters and dialogue
communication and efficiency parameters were of-
ten useful predictors. User affect parameters further
improved the predictive power of one student learn-
ing model for both training and testing.
Once our user affect annotations are complete,
we can further investigate their use to predict stu-
dent learning and user satisfaction. Unlike our
other parameters, these annotations are not currently
available, although they can be predicted automati-
cally (Litman and Forbes-Riley, 2004b), in our sys-
270
tem. However, as in (Batliner et al, 2003), our prior
work suggests that linguistic features reflective of af-
fective states can replace affect annotation (Forbes-
Riley and Litman, 2005). In future work we will use
such features in our prediction models. Finally, we
are also annotating tutor and student dialogue acts
and automating the tutor act annotations; when com-
plete we can investigate their usefulness in our pre-
diction models; dialogue acts have also been used in
prior PARADISE applications (Mo?ller, 2005a).
Acknowledgements
NSF (0325034 & 0328431) supports this research.
We thank Pam Jordan and the NLP Group.
References
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. Int. Conf. Spoken Language Processing (ICSLP).
A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. Noth.2003. How to find trouble in communication. Speech
Communication, 40:117?143.
K. Bhatt, M. Evens, and S. Argamon. 2004. Hedged re-sponses and expressions of affect in human/human and
human/computer tutorial interactions. In Proc. 26th
Annual Meeting of the Cognitive Science Society.
H. Bonneau-Maynard, L. Devillers, and S. Rosset. 2000.Predictive performance of dialog systems. In Proc.
Language Resources and Evaluation Conf. (LREC)).
M. T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi, andR. G. Hausmann. 2001. Learning from human tutor-
ing. Cognitive Science, 25:471?533.
S. Craig, A. Graesser, J. Sullins, and B. Gholson. 2004.Affect and learning: An exploratory look into the roleof affect in learning. Journal of Educational Media,29:241?250.
K. Forbes-Riley and D. Litman. 2005. Correlating stu-
dent acoustic-prosodic profiles with student learning inspoken tutoring dialogues. In Proc. INTERSPEECH.
K. Forbes-Riley, D. Litman, S. Silliman, and J. Tetreault.2006. Comparing synthesized versus pre-recorded tu-tor speech in an intelligent tutoring spoken dialogue
system. In Proc. FLAIRS.
C.M. Lee, S. Narayanan, and R. Pieraccini. 2002. Com-
bining acoustic and language information for emotionrecognition. In Proc. ICSLP.
D. Litman and K. Forbes-Riley. 2004a. Annotating stu-
dent emotional states in spoken tutoring dialogues. In
Proc. SIGdial, pages 144?153.
D. Litman and K. Forbes-Riley. 2004b. Predicting stu-dent emotions in computer-human tutoring dialogues.
In Proc. ACL, pages 352?359.
D. Litman, C. Rose?, K. Forbes-Riley, K. VanLehn,D. Bhembe, and S. Silliman. 2006. Spoken versus
typed human and computer dialogue tutoring. Intnl
Jnl of Artificial Intelligence in Education, To Appear.
S. Mo?ller. 2005a. Parameters for quantifying the inter-
actioin with spoken dialogue telephone services. In
Proc. SIGdial.
S. Mo?ller. 2005b. Towards generic quality predictionmodels for spoken dialogue systems - a case study. In
Proc. INTERSPEECH.
J. D. Moore, K. Porayska-Pomsta, S. Varges, and C. Zinn.2004. Generating tutorial feedback with affect. In
Proc. FLAIRS.
J. Mostow and G. Aist. 2001. Evaluating tutors that lis-ten: An overview of Project LISTEN. In K. Forbusand P. Feltovich, editors, Smart Machines in Educa-
tion.
H. Pon-Barry, B. Clark, E. Owen Bratt, K. Schultz, andS. Peters. 2004. Evaluating the effectiveness of SCoT:
a Spoken Conversational Tutor. In Proc. of ITS 2004
Workshop on Dialogue-based Intelligent Tutoring Sys-
tems: State of the Art and New Research Directions.
E. Shriberg, E. Wade, and P. Price. 1992. Human-
machine problem solving using spoken language sys-tems (SLS): Factors affecting performance and usersatisfaction. In Proc. DARPA Speech and NL Work-
shop, pages 49?54.
K. VanLehn, P. W. Jordan, C. P. Rose?, D. Bhembe,M. Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-vastava, and R. Wilson. 2002. The architecture ofWhy2-Atlas: A coach for qualitative physics essaywriting. In Proc. Intelligent Tutoring Systems.
M. Walker, , D. Litman, C. Kamm, and A. Abella. 1997.PARADISE: A framework for evaluating spoken dia-logue agents. In Proc. ACL/EACL, pages 271?280.
M. Walker, C. Kamm, and D. Litman. 2000. Towards de-veloping general models of usability with PARADISE.
Natural Language Engineering, 6:363?377.
M. Walker, A. Rudnicky, R. Prasad, J. Aberdeen, E. Bratt,
J. Garofolo, H. Hastie, A. Le, B. Pellom, A. Potami-anos, R. Passonneau, S. Roukos, G. Sanders, S. Seneff,and D. Stallard. 2002. DARPA communicator: Cross-
system results for the 2001 evaluation. In Proc. Inter-
nat. Conf. on Spoken Language Processing (ICSLP).
271
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 272?279,
New York, June 2006. c?2006 Association for Computational Linguistics
Comparing the Utility of State Features in Spoken Dialogue Using
Reinforcement Learning
Joel R. Tetreault
University of Pittsburgh
Learning Research and Development Center
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Diane J. Litman
University of Pittsburgh
Department of Computer Science
Learning Research and Development Center
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
Recent work in designing spoken dialogue
systems has focused on using Reinforce-
ment Learning to automatically learn the
best action for a system to take at any
point in the dialogue to maximize dia-
logue success. While policy development
is very important, choosing the best fea-
tures to model the user state is equally im-
portant since it impacts the actions a sys-
tem should make. In this paper, we com-
pare the relative utility of adding three fea-
tures to a model of user state in the do-
main of a spoken dialogue tutoring sys-
tem. In addition, we also look at the ef-
fects of these features on what type of a
question a tutoring system should ask at
any state and compare it with our previ-
ous work on using feedback as the system
action.
1 Introduction
A host of issues confront spoken dialogue system
designers, such as choosing the best system action to
perform given any user state, and also selecting the
right features to best represent the user state. While
recent work has focused on using Reinforcement
Learning (RL) to address the first issue (such as
(Walker, 2000), (Henderson et al, 2005), (Williams
et al, 2005a)), there has been very little empirical
work on the issue of feature selection in prior RL ap-
proaches to dialogue systems. In this paper, we use
a corpus of dialogues of humans interacting with a
spoken dialogue tutoring system to show the com-
parative utility of adding the three features of con-
cept repetition, frustration level, and student perfor-
mance. These features are not just unique to the tu-
toring domain but are important to dialogue systems
in general. Our empirical results show that these fea-
tures all lead to changes in what action the system
should take, with concept repetition and frustration
having the largest effects.
This paper extends our previous work (Tetreault
and Litman, 2006) which first presented a method-
ology for exploring whether adding more complex
features to a representation of student state will ben-
eficially alter tutor actions with respect to feedback.
Here we present an empirical method of comparing
the effects of each feature while also generalizing
our findings to a different action choice of what type
of follow-up question should a tutor ask the student
(as opposed to what type of feedback should the tu-
tor give). In complex domains such as tutoring, test-
ing different policies with real or simulated students
can be time consuming and costly so it is important
to properly choose the best features before testing,
which this work allows us to do. This in turn aids
our long-term goal of improving a spoken dialogue
system that can effectively adapt to a student to max-
imize their learning.
2 Background
We follow past lines of research (such as (Levin and
Pieraccini, 1997) and (Singh et al, 1999)) for de-
scribing a dialogue
 
as a trajectory within a Markov
Decision Process (MDP) (Sutton and Barto, 1998).
272
A MDP has four main components: 1: states   , 2:
actions

, 3: a policy  , which specifies what is the
best action to take in a state, and 4: a reward func-
tion  which specifies the worth of the entire pro-
cess. Dialogue management is easily described us-
ing a MDP because one can consider the actions as
actions made by the system, the state as the dialogue
context (which can be viewed as a vector of features,
such as ASR confidence or dialogue act), and a re-
ward which for many dialogue systems tends to be
task completion success or dialogue length.
Another advantage of using MDP?s to model a di-
alogue space, besides the fact that the primary MDP
parameters easily map to dialogue parameters, is the
notion of delayed reward. In a MDP, since rewards
are often not given until the final states, dynamic
programming is used to propagate the rewards back
to the internal states to weight the value of each state
(called the V-value), as well as to develop an optimal
policy  for each state of the MDP. This propaga-
tion of reward is done using the policy iteration al-
gorithm (Sutton and Barto, 1998) which iteratively
updates the V-value and best action for each state
based on the values of its neighboring states.
The V-value of each state is important for our pur-
poses not only because it describes the relative worth
of a state within the MDP, but as more data is added
when building the MDP, the V-values should stabi-
lize, and thus the policies stabilize as well. Since,
in this paper, we are comparing policies in a fixed
data set it is important to show that the policies are
indeed reliable, and not fluctuating.
For this study, we used the MDP infrastructure de-
signed in our previous work which allows the user
to easily set state, action, and reward parameters. It
then performs policy iteration to generate a policy
and V-values for each state. In the following sec-
tions, we discuss our corpus, methodology, and re-
sults.
3 Corpus
For our study, we used an annotated corpus of
20 human-computer spoken dialogue tutoring ses-
sions (for our work we use the ITSPOKE system
(Litman and Silliman, 2004) which uses the text-
based Why2-ATLAS dialogue tutoring system as its
?back-end? (VanLehn et al, 2002)). The content
State Feature Values
Certainty Certain (cer)
Uncertain (unc)
Neutral (neu)
Frustration Frustrated (F)
Neutral (N),
Correctness Correct (C)
Partially Correct (PC)
Incorrect (I)
Percent Correct 50-100% (H)igh
0-49% (L)ow
Concept Repetition Concept is new (0)
Concept is repeated (R)
Table 1: Potential Student State Features in MDP
of the system, and all possible dialogue paths, were
authored by physics experts. Each session consists
of an interaction with one student over 5 different
college-level physics problems, for a total of 100 di-
alogues. Before each session, the student is asked to
read physics material for 30 minutes and then take a
pretest based on that material. Each problem begins
with the student writing out a short essay response
to the question posed by the computer tutor. The
fully-automated system assesses the essay for poten-
tial flaws in the reasoning and then starts a dialogue
with the student, asking questions to help the stu-
dent understand the confused concepts. The tutor?s
response and next question is based only on the cor-
rectness of the student?s last answer. Informally, the
dialogue follows a question-answer format. Once
the student has successfully completed the dialogue
section, he is asked to correct the initial essay. Each
of the dialogues takes on average 20 minutes and 60
turns. Finally, the student is given a posttest simi-
lar to the pretest, from which we can calculate their
normalized learning gain: 
	Proceedings of NAACL HLT 2007, pages 276?283,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Estimating the Reliability of MDP Policies: A Confidence Interval Approach
Joel R. Tetreault
University of Pittsburgh
LRDC
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Dan Bohus
Carnegie Mellon University
Dept. of Computer Science
Pittsburgh, PA, 15213, USA
dbohus@cs.cmu.edu
Diane J. Litman
University of Pittsburgh
Dept. of Computer Science
LRDC
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
Past approaches for using reinforcement
learning to derive dialog control policies
have assumed that there was enough col-
lected data to derive a reliable policy. In
this paper we present a methodology for
numerically constructing confidence inter-
vals for the expected cumulative reward
for a learned policy. These intervals are
used to (1) better assess the reliability
of the expected cumulative reward, and
(2) perform a refined comparison between
policies derived from different Markov
Decision Processes (MDP) models. We
applied this methodology to a prior ex-
periment where the goal was to select the
best features to include in the MDP state-
space. Our results show that while some
of the policies developed in the prior work
exhibited very large confidence intervals,
the policy developed from the best feature
set had a much smaller confidence interval
and thus showed very high reliability.
1 Introduction
NLP researchers frequently have to deal with issues
of data sparsity. Whether the task is machine transla-
tion or named-entity recognition, the amount of data
one has to train or test with can greatly impact the re-
liability and robustness of one?s models, results and
conclusions.
One research area that is particularly sensitive to
the data sparsity issue is machine learning, specifi-
cally in using Reinforcement Learning (RL) to learn
the optimal action for a dialogue system to make
given any user state. Typically this involves learn-
ing from previously collected data or interacting in
real-time with real users or user simulators. One of
the biggest advantages to this machine learning ap-
proach is that it can be used to generate optimal poli-
cies for every possible state. However, this method
requires a thorough exploration of the state-space to
make reliable conclusions on what the best actions
are. States that are infrequently visited in the train-
ing set could be assigned sub-optimal actions, and
therefore the resulting dialogue manager may not
provide the best interaction for the user.
In this work, we present an approach for esti-
mating the reliability of a policy derived from col-
lected training data. The key idea is to take into ac-
count the uncertainty in the model parameters (MDP
transition probabilities), and use that information to
numerically construct a confidence interval for the
expected cumulative reward for the learned policy.
This confidence interval approach allows us to: (1)
better assess the reliability of the expected cumula-
tive reward for a given policy, and (2) perform a re-
fined comparison between policies derived from dif-
ferent MDP models.
We apply the proposed approach to our previous
work (Tetreault and Litman, 2006) in using RL to
improve a spoken dialogue tutoring system. In that
work, a dataset of 100 dialogues was used to de-
velop a methodology for selecting which user state
features should be included in the MDP state-space.
But are 100 dialogues enough to generate reliable
policies? In this paper we apply our confidence in-
276
terval approach to the same dataset in an effort to in-
vestigate how reliable our previous conclusions are,
given the amount of available training data.
In the following section, we discuss the prior
work and its data sparsity issue. In section 3, we
describe in detail our confidence interval methodol-
ogy. In section 4, we show how this methodology
works by applying it to the prior work. In sections 5
and 6, we present our conclusions and future work.
2 Previous Work
Past research into using RL to improve spoken di-
alogue systems has commonly used Markov Deci-
sion Processes (MDP?s) (Sutton and Barto, 1998)
to model a dialogue (such as (Levin and Pieraccini,
1997) and (Singh et al, 1999)).
A MDP is defined by a set of states {si}i=1..n,a set of actions {ak}k=1..p, and a set of transitionprobabilities which reflect the dynamics of the en-
vironment {p(si|sj, ak)}k=1..pi,j=1..n: if the model is attime t in state sj and takes action ak, then it willtransition to state si with probability p(si|sj , ak).Additionally, an expected reward r(si, sj , ak) is de-fined for each transition. Once these model parame-
ters are known, a simple dynamic programming ap-
proach can be used to learn the optimal control pol-
icy pi?, i.e. the set of actions the model should take
at each state, to maximize its expected cumulative
reward.
The dialog control problem can be naturally cast
in this formalism: the states {si}i=1..n in the MDPcorrespond to the dialog states (or an abstraction
thereof), the actions {ak}k=1..p correspond to theparticular actions the dialog manager might take,
and the rewards r(si, sj , ak) are defined to reflecta particular dialog performance metric. Once the
MDP structure has been defined, the model param-
eters {p(si|sj, ak)}k=1..pi,j=1..n are estimated from a cor-pus of dialogs (either real or simulated), and, based
on them, the policy which maximizes the expected
cumulative reward is computed.
While most work in this area has focused on de-
veloping the best policy (such as (Walker, 2000),
(Henderson et al, 2005)), there has been relatively
little work done with respect to selecting the best
features to include in the MDP state-space. For in-
stance, Singh et al (1999) showed that dialogue
length was a useful state feature and Frampton and
Lemon (2005) showed that the user?s last dialogue
act was also useful. In our previous work, we com-
pare the worth of several features. In addition, Paek
and Chickering?s (2005) work showed how a state-
space can be reduced by only selecting features that
are relevant to maximizing the reward function.
The motivation for this line of research is that if
one can properly select the most informative fea-
tures, one develops better policies, and thus a bet-
ter dialogue system. In the following sections we
summarize our past data, approach, results, and is-
sue with policy reliability.
2.1 MDP Structure
For this study, we used an annotated corpus of
human-computer spoken dialogue tutoring sessions.
The fixed-policy corpus contains data collected from
20 students interacting with the system for five prob-
lems (for a total of 100 dialogues of roughly 50 turns
each). The corpus was annotated with 5 state fea-
tures (Table 1). It should be noted that two of the
features, Certainty and Frustration, were manually
annotated while the other three were done automat-
ically. All features are binary except for Certainty
which has three values.
State Values
Correctness Student is correct or incorrect
in the current turn
Certainty Student is certain, neutral
or uncertain in the current turn
Concept Repetition A particular concept is either new
or repeated
Frustration Student is frustrated or not
in the current turn
Percent Correct Student answers over 66% of
questions correctly in dialogue
so far, or less
Table 1: State Features in Tutoring Corpus
For the action set {ak}k=1..p, we looked at whattype of question the system could ask the student
given the previous state. There are a total of four
possible actions: ask a short answer question (one
that requires a simple one word response), a com-
plex answer question (one that requires a longer,
deeper response), ask both a simple and complex
question in the same turn, or do not ask a question
at all (give a hint). The reward function r was the
277
learning gain of each student based on a pair of tests
before and after the entire session of 5 dialogues.
The 20 students were split into two groups (high
and low learners) based on their learning gain, so
10 students and their respective five dialogues were
given a positive reward of +100, while the remain-
der were assigned a negative reward of -100. The
rewards were assigned in the final dialogue state, a
common approach when applying RL in spoken di-
alogue systems.
2.2 Approach and Results
To investigate the usefulness of different features,
we took the following approach. We started with
two baseline MDPs. The first model (Baseline 1)
used only the Correctness feature in the state-space.
The second model (Baseline 2) included both the
Correctness and Certainty features. Next we con-
structed 3 new models by adding each of the remain-
ing three features (Frustration, Percent Correct and
Concept Repetition) to the Baseline 2 model.
We defined three metrics to compare the policies
derived from these MDPs: (1) Diff?s: the number of
states whose policy differs from the Baseline 2 pol-
icy, (2) Percent Policy change (P.C.): the weighted
amount of change between the two policies (100%
indicates total change), and (3) Expected Cumula-
tive Reward (or ECR) which is the average reward
one would expect in that MDP when in the state-
space.
The intuition is that if a new feature were rele-
vant, the corresponding model would lead to a dif-
ferent policy and a better expected cumulative re-
ward (when compared to the baseline models). Con-
versely, if the features were not useful, one would
expect that the new policies would look similar
(specifically, the Diff?s count and % Policy Change
would be low) or produce similar expected cumula-
tive rewards to the original baseline policy.
The results of this analysis are shown in Table 2 1
The Diff?s and Policy Change metrics are undefined
for the two baselines since we only use these two
metrics to compare the other three features to Base-
1Please note that to due to refinements in code, there is a
slight difference between the ECR?s reported in this work and
the ECR?s reported in the previous work, for the three features
added to Baseline 2. These changes did not alter the rankings
of these models, or the conclusions of the previous work.
line 2. All three metrics show that the best feature
to add to the Baseline 2 model is Concept Repetition
since it results in the most change over the Baseline
2 policy, and also the expected reward is the highest
as well. For the remainder of this paper, when we
refer to Concept Repetition, Frustration, or Percent
Correctness, we are referring to the model that in-
cludes that feature as well as the Baseline 2 features
Correctness and Certainty.
State Feature # Diff?s % P.C. ECR
Baseline 1 N/A N/A 6.15
Baseline 2 N/A N/A 31.92
B2 + Concept Repetition 10 80.2% 42.56
B2 + Frustration 8 66.4% 32.99
B2 + Percent Correctness 4 44.3% 28.50
Table 2: Feature Comparison Results
2.3 Problem with Reliability
However, the approach discussed above assumes
that given the size of the data set, the ECR and poli-
cies are reliable. If the MDP model were very frag-
ile, that is the policy and expected cumulative reward
were very sensitive to the quality of the transition
probability estimates, then the metrics could reveal
quite different rankings. Previously, we used a qual-
itative approach of tracking how the worth of each
state (V-value) changed over time. The V-values
indicate how much reward one would expect from
starting in that state to get to a final state. We hy-
pothesized that if the V-values stabilized as data in-
creased, then the learned policy would be more reli-
able.
So is this V-value methodology adequate for as-
sessing if there is enough data to determine a sta-
ble policy, and also for assessing if one model is
better than another? Since our approach for state-
space selection is based on comparing a new pol-
icy with a baseline policy, having a stable policy is
extremely important since instability could lead to
different conclusions. For example, in one compar-
ison, a new policy could differ with the baseline in
8 out of 10 states. But if the MDP were unstable,
adding just a little more data could result in a differ-
ence of only 4 out of 10 states. Is there an approach
that can categorize whether given a certain data size,
278
that the expected cumulative reward (and thus the
policy) is reliable? In the next section we present a
new methodology for numerically constructing con-
fidence intervals for these value function estimates.
Then, in the following section, we reevaluate our
prior work with this methodology and discuss the
results.
3 Confidence Interval Methodology
3.1 Policy Evaluation with Confidence
Intervals
The starting point for the proposed methodology
is the observation that for each state sj and ac-tion ak in the MDP, the set of transition probabili-ties {p(si|sj, ak)}i=1..n are modeled as multinomialdistributions that are estimated from the transition
counts in the training data:
p?(si|sj, ak) =
c(si, sj, ak)
?n
i=1 c(si, sj , ak)
(1)
where n is the number of states in the model, and
c(si, sj , ak) is the number of times the system wasin state sj , took action ak, and transitioned to state
si in the training data.It is important to note that these parameters are
just estimates. The reliability of these estimates
clearly depends on the amount of training data, more
specifically on the transition counts c(si, sj, ak). Forinstance, consider a model with 3 states and 2 ac-
tions. Say the model was in state s1 and took action
a1 ten times. Out of these, three times the modeltransitioned back to state s1, two times it transi-tioned to state s2, and five times to state s3. Thenwe have:
p?(si|s1, a1) = ?0.3; 0.2; 0.5? = ? 310 ;
2
10 ;
5
10 ? (2)
Additionally, let?s say the same model was in state
s2 and took action a2 1000 times. Following that ac-tion, it transitioned 300 times to state s1, 200 timesto state s2, and 500 times to state s3.
p?(si|s2, a2) = ?0.3; 0.2; 0.5? = ? 3001000 ;
200
1000 ;
500
1000 ? (3)
While both sets of transition parameters have the
same value, the second set of estimates is more reli-
able. The central idea of the proposed approach is to
model this uncertainty in the system parameters, and
use it to numerically construct confidence intervals
for the value of the optimal policy.
Formally, each set of transition probabilities
{p(si|sj , ak)}i=1..n is modeled as a multinomial dis-tribution, estimated from data2. The uncertainty of
multinomial estimates are commonly modeled by
means of a Dirichlet distribution. The Dirichlet dis-
tribution is characterized by a set of parameters ?1,
?2, ..., ?n, which in this case correspond to thecounts {c(si, sj , ak)}i=1..n. For any given j, thelikelihood of the set of multinomial transition pa-
rameters {p(si|sj, ak)}i=1..n is then given by:
P ({p(si|sj , ak)}i=1..n|D) =
= 1Z(D)
?n
i=1 p(si|sj , ak)?i?1 (4)
where Z(D) =
?n
i=1 ?(?i)
?(
?n
i=1 ?i)
and ?i = c(si, sj , ak).
Note that the maximum likelihood estimates for the
formula above correspond to the frequency count
formula we have already described:
p?ML(si|sj, ak) =
?i
?n
i=1 ?i
= c(si, sj, ak)?n
i=1 c(si, sj , ak)(5)
To capture the uncertainty in the model parame-
ters, we therefore simply need to store the counts
of the observed transitions c(si, sj , ak). Based onthis model of uncertainty, we can numerically con-
struct a confidence interval for the value of the opti-
mal policy pi?. Instead of computing the value of the
policy based on the maximum likelihood transition
estimates T?ML = {p?ML(si|sj , ak)}k=1..pi,j=1..n, we gen-
erate a large number of transition matrices T?1, T?1,... T?m by sampling from the Dirichlet distributionscorresponding to the counts observed in the train-
ing data (in the experiments reported in this paper,
we used m = 1000). We then compute the value
of the optimal policy pi? in each of these models
{Vpi?(T?i)}i=1..m. Finally, we numerically constructthe 95% confidence interval for the value function
based on the resulting value estimates: the bounds
for the confidence interval are set at the lowest and
highest 2.5 percentile of the resulting distribution of
the values for the optimal policy {Vpi?(T?i)}i=1..m.The algorithm is outlined below:
2By p we will denote the true model parameters; by p? we
will denote data-driven estimates for these parameters
279
1. compute transition counts from the training set:
C = {c(si, sj, ak)}k=1..pi,j=1..n (6)
2. compute maximum likelihood estimates for
transition probability matrix:
T?ML = {p?ML(si|sj , ak)}k=1..pi,j=1..n (7)
3. use dynamic programming to compute the op-
timal policy pi? for model T?ML
4. sample m transition matrices {T?k}k=1..m, us-ing the Dirichlet distribution for each row:
{p?i(si|sj, ak)}i=1..n =
= Dir({c(si, sj , ak)}i=1..n) (8)
5. evaluate the optimal policy pi? in each of these
m models, and obtain Vpi?(T?i)
6. numerically build the 95% confidence interval
for Vpi? from these estimates.
To summarize, the central idea is to take into ac-
count the reliability of the transition probability esti-
mates and construct a confidence interval for the ex-
pected cumulative reward for the learned policy. In
the standard approach, we would compute an esti-
mate for the expected cumulative reward, by simply
using the transition probabilities derived from the
training set. Note that these transition probabilities
are simply estimates which are more or less accu-
rate, depending on how much data is available. The
proposed methodology does not fully trust these es-
timates, and asks the question: given that the real
world (i.e. real transition probabilities) might actu-
ally be a bit different than we think it is, how well
can we expect the learned policy to perform? Note
that the confidence interval we construct, and there-
fore the conclusions we draw, are with respect to the
policy learned from the current estimates, i.e. from
the current training set. If more data becomes avail-
able, a different optimal policy might emerge, about
which we cannot say much.
3.2 Related Work
Given the stochastic nature of the models, confi-
dence intervals are often used to estimate the reli-
ability of results in machine learning experiments,
e.g. (Rivals and Personnaz, 2002), (Schapire, 2002)
and (Dumais et al, 1998). In this work we use a
confidence interval methodology in the context of
MDPs. The idea of modeling the uncertainty of
the transition probability estimates using Dirichlet
models also appears in (Jaulmes et al, 2005). In
that work, the authors used the uncertainty in model
parameters to develop active learning strategies for
partially observable MDPs, a topic not previously
addressed in the literature. In our work we rely on
the same model of uncertainty for the transition ma-
trix, but use it to derive confidence intervals for the
expected cumulative reward for the learned optimal
policy, in an effort to assess the reliability of this
policy.
4 Results
Our previous results indicated that Concept Repe-
tition was the best feature to add to the Baseline 2
state-space model, but also that Percent Correctness
and Frustration (when added to Baseline 2) offered
an improvement over the Baseline MDP?s. How-
ever, these conclusions were based on a very quali-
tative approach for determining if a policy is reliable
or not. In the following subsection, we apply our ap-
proach of confidence intervals to empirically deter-
mine if given this data set of 100 dialogues, whether
the estimates of the ECR are reliable, and whether
the original rankings and conclusions hold up under
this refined analysis. In subsection 4.2, we provide
a methodology for pinpointing when one model is
better than another.
4.1 Quantitative Analysis of ECR Reliability
For our first investigation, we look at the confidence
intervals of each MDP?s ECR over the entire data set
of 20 students (later in this section we show plots for
the confidence intervals as data increases). Table 3
shows the upper and lower bounds for the ECR orig-
inally reported in Table 2. The first column shows
the original, estimated ECR of the MDP and the last
column is the width of the bound (the difference be-
tween the upper and lower bound).
So what conclusions can we make about the reli-
ability of the ECR, and hence of the learned policies
for the different MDP?s, given this amount of train-
ing data? The confidence interval for the ECR for
280
State Feature ECR Lower Bound Upper Bound Width
Baseline 1 6.15 0.21 23.73 23.52
Baseline 2 (B2) 31.92 -5.31 60.48 65.79
B2 + Concept Repetition 42.56 28.37 59.29 30.92
B2 + Frustration 32.99 -4.12 61.30 65.42
B2 + Percent Correctness 28.50 -5.89 57.82 63.71
Table 3: Confidence Intervals with complete dataset
the Baseline 1 model ranges from 0.21 to 23.73. Re-
call that the final states are capped at +100 and -100,
and are thus the maximum and minimum bounds
that one can see in this experiment. These bounds
tell us that, if we take into account the uncertainty
in the model estimates (given the small training set
size), with probability 0.95 the actual true ECR for
this policy will be greater than 0.21 and smaller than
23.73. The width of this confidence interval is 23.52.
For the Baseline 2 model, the bounds are much
wider: from -5.31 to 60.48, for a total width of
65.79. While the ECR estimate is 31.92 (which
is seemingly larger than 6.15 for the Baseline 1
model), the wide confidence interval tells us that this
estimate is not very reliable. It is possible that the
policy derived from this model with this amount of
data could perform poorly, and even get a negative
reward. From the dialogue system designer?s stand-
point, a model like this is best avoided.
Of the remaining three models ? Concept Repeti-
tion, Frustration, and Percent Correctness, the first
one exhibits a tighter confidence interval, indicat-
ing that the estimated expected cumulative reward
(42.56) is fairly reliable: with 95% probability of
being between 28.37 and 59.29. The ECR for the
other two models (Frustration and Percent Correct-
ness) again shows a wide confidence interval once
we take into account the uncertainty in the model
parameters.
These results shed more light on the shortcom-
ings of the ECR metric used to evaluate the models
in prior work. This estimate does not take into ac-
count the uncertainty of the model parameters. For
example, a model can have an optimal policy with
a very high ECR value, but have very wide confi-
dence bounds reaching even into negative rewards.
On the other hand, another model can have a rela-
tively lower ECR but if its bounds are tighter (and
the lower bound is not negative), one can know that
that policy is less affected by poor parameter esti-
mates stemming from data sparsity issues. Using the
confidence intervals associated with the ECR gives a
much more refined, quantitative estimate of the reli-
ability of the reward, and hence of the policy derived
from that data.
An extension of this result is that confidence in-
tervals can also allow us to make refined judgments
about the comparative utility of different features,
the original motivation of our prior study. Basi-
cally, a model (M1) is better than another (M2) if
M1?s lower bound is greater than the upper bound of
M2. That is, one knows that 95% of the time, the
worst case situation of M1 (the lower bound) will
always yield a higher reward than the best case of
M2. In our data, this happens only once, with Con-
cept Repetition being empirically better than Base-
line 1, since the lower bound of Concept Repetition
is 28.37 and the upper bound of Baseline 1 is 23.73.
Given this situation, Concept Repetition is a useful
feature which, when included in the model, leads to
a better policy than simply using Correctness. We
cannot draw any conclusions about the other fea-
tures, since their bounds are generally quite wide.
Given this amount of training data, we cannot say
whether Percent Correctness and Frustration are bet-
ter features than the Baseline MDP?s. Although their
ECR?s are higher, there is too much uncertainty to
definitely conclude they are better.
4.2 Pinpointing Model Cross-over
The previous analysis focused on a quantitative
method of (1) determining the reliability of the MDP
ECR estimate and policy, as well as (2) assessing
whether one model is better than another. In this
section, we present an extension to the second con-
tribution by answering the question: given that one
model is more reliable than another, is it possible
to determine at which point one model?s estimates
become more reliable than another model?s? In our
281
0 2 4 6 8 10 12 14 16 18 20
?100
?80
?60
?40
?20
0
20
40
60
80
100
Baseline 1
# of students
EC
R
 
 
Confidence Bounds
Calculated ECR
0 2 4 6 8 10 12 14 16 18 20
?100
?80
?60
?40
?20
0
20
40
60
80
100
Baseline 2 +Concept Repetition
# of students
EC
R
 
 
Confidence Bounds
Calculated ECR
Figure 1: Confidence Interval Plots
case, we want to know at what point Concept Rep-
etition becomes more reliable than Baseline 1. To
do this, we investigate how the confidence interval
changes as the amount of training data increases in-
stead of looking at the reliability estimate at only one
particular data size.
We incrementally increase the amount of train-
ing data (adding the data from one new student at a
time), and calculate the corresponding optimal pol-
icy and confidence interval for the expected cumula-
tive reward for that policy. Figure 1 shows the con-
fidence interval plots as data is added to the MDP
for the Baseline 1 and Concept Repetition MDP?s.
For reference, Baseline 2, Percent Correctness and
Frustration plots did not exhibit the same converg-
ing behavior as these two, which is not surprising
given how wide the final bounds are. For each plot,
the bold lines represent the upper and lower bounds,
and the dotted line represents the calculated ECR.
Analyzing the two MDP?s, we find that the confi-
dence intervals for Baseline 1 and Concept Repeti-
tion converge as more data is added, which is an ex-
pected trend. One useful result from observing the
change in confidence intervals is that one can de-
termine the point in one which one model becomes
empirically better than another. Superimposing the
upper and lower bounds (Figure 2) reveals that after
we include the data from the first 13 students, the
lower bound of Concept Repetition crosses over the
upper bound of Baseline 1.
Observing this behavior is especially useful for
performing model switching. In automatic model
switching, a dialogue manager runs in real time and
as it collects data, it can switch from using a sim-
ple dialogue model to a complex model. Confidence
intervals can be used to determine when to switch
from one model to the next by checking if a complex
model?s bounds cross over the bounds of the current
model. Basically, the dialogue manager switches
when it can be sure that the more complex model?s
ECR is not only higher, but statistically significantly
so.
0 2 4 6 8 10 12 14 16 18 20
?50
0
50
100
# of students
EC
R
Baseline 1 and Concept Repetition Superimposed
 
 
Baseline 1
B2 + Concept Repetition
Figure 2: Baseline 1 and Concept Repetition Bounds
5 Conclusions
Past work in using MDP?s to improve spoken dia-
logue systems have usually glossed over the issue of
whether or not there was enough training data to de-
velop reliable policies. In this work, we present a
numerical method for building confidence intervals
for the expected cumulative reward for a learned pol-
icy. The proposed approach allows one to (1) better
282
assess the reliability of the expected cumulative re-
ward for a given policy, and (2) perform a refined
comparison between policies derived from different
MDP models.
We applied this methodology to a prior experi-
ment where the objective was to select the best fea-
tures to include in the MDP state-space. Our results
show that policies constructed from the Baseline 1
and Concept Repetition models are more reliable,
given the amount of data available for training. The
Concept Repetition model (which is composed of
the Concept Repetition, Certainty and Correctness
features) was especially useful, as it led to a policy
that outperformed the Baseline 1 model, even when
we take into account the uncertainty in the model
estimates caused by data sparsity. In contrast, for
the Baseline 2, Percent Correctness, and Frustration
models, the estimates for the expected cumulative
reward are much less reliable, and no conclusion can
be reliably drawn about the usefulness of these fea-
tures. In addition, we showed that our confidence
interval approach has applications in another MDP
problem: model switching.
6 Future Work
As an extension of this work, we are currently inves-
tigating in more detail what makes some MDP?s reli-
able or unreliable for a certain data size (such as the
case where Baseline 2 does not converge but a more
complicated model does, such as Concept Repeti-
tion). Our initial findings indicate that, as more data
becomes available the bounds tighten for most pa-
rameters in the transition matrix. However, for some
of the parameters the bounds can remain wide, and
that is enough to keep the confidence interval for the
expected cumulative reward from converging.
Acknowledgments
We would like to thank Jeff Schneider, Drew Bag-
nell, Pam Jordan, as well as the ITSPOKE and Pitt
NLP groups, and the Dialog on Dialogs group for
their help and comments. Finally, we would like to
thank the four anonymous reviewers for their com-
ments on the initial version of this paper. Support for
this research was provided by NSF grants #0325054
and #0328431.
References
S. Dumais, J. Platt, D. Heckerman, and M. Sahami. 1998.Inductive learning algorithms and representations fortext categorization. In Conference on Information and
Knowledge Management.
M. Frampton and O. Lemon. 2005. Reinforcement learn-ing of dialogue strategies using the user?s last dialogueact. In IJCAI Wkshp. on K&R in Practical Dialogue
Systems.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybridreinforcement/supervised learning for dialogue poli-cies from communicator data. In IJCAI Wkshp. on
K&R in Practical Dialogue Systems.
R. Jaulmes, J. Pineau, and D. Precup. 2005. Active learn-ing in partially observable markov decision processes.In European Conference on Machine Learning.
E. Levin and R. Pieraccini. 1997. A stochastic model of
computer-human interaction for learning dialogues. In
Proc. of EUROSPEECH ?97.
T. Paek and D. Chickering. 2005. The markov assump-tion in spoken dialogue management. In 6th SIGDial
Workshop on Discourse and Dialogue.
I. Rivals and L. Personnaz. 2002. Construction of con-fidence intervals for neural networks based on leastsquares estimation. In Neural Networks.
R. Schapire. 2002. The boosting approach to machinelearning: An overview. In MSRI Workshop on Nonlin-
ear Estimation and Classification.
S. Singh, M. Kearns, D. Litman, and M. Walker. 1999.
Reinforcement learning for spoken dialogue systems.In Proc. NIPS ?99.
R. Sutton and A. Barto. 1998. Reinforcement Learning.The MIT Press.
J. Tetreault and D. Litman. 2006. Comparing the utility
of state features in spoken dialogue using reinforce-ment learning. In NAACL.
M. Walker. 2000. An application of reinforcement learn-ing to dialogue strategy selection in a spoken dialogue
system for email. JAIR, 12.
283
Proceedings of NAACL HLT 2007, Companion Volume, pages 1?4,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Comparing User Simulation Models For Dialog Strategy Learning
Hua Ai
University of Pittsburgh
Intelligent Systems Program
Pittsburgh PA, 15260, USA
hua@cs.pitt.edu
Joel R. Tetreault
University of Pittsburgh
LRDC
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Diane J. Litman
University of Pittsburgh
Dept. of Computer Science
LRDC
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
This paper explores what kind of user sim-
ulation model is suitable for developing
a training corpus for using Markov Deci-
sion Processes (MDPs) to automatically
learn dialog strategies. Our results sug-
gest that with sparse training data, a model
that aims to randomly explore more dialog
state spaces with certain constraints actu-
ally performs at the same or better than a
more complex model that simulates real-
istic user behaviors in a statistical way.
1 Introduction
Recently, user simulation has been used in the de-
velopment of spoken dialog systems. In contrast to
experiments with human subjects, which are usually
expensive and time consuming, user simulation gen-
erates a large corpus of user behaviors in a low-cost
and time-efficient manner. For example, user sim-
ulation has been used in evaluation of spoken dia-
log systems (Lo?pez-Co?zar et al, 2003) and to learn
dialog strategies (Scheffler, 2002). However, these
studies do not systematically evaluate how helpful a
user simulation is. (Schatzmann et al, 2005) pro-
pose a set of evaluation measures to assess the re-
alness of the simulated corpora (i.e. how similar
are the simulated behaviors and human behaviors).
Nevertheless, how realistic a simulated corpus needs
to be for different tasks is still an open question.
We hypothesize that for tasks like system eval-
uation, a more realistic simulated corpus is prefer-
able. Since the system strategies are evaluated and
adapted based on the analysis of these simulated dia-
log behaviors, we would expect that these behaviors
are what we are going to see in the test phase when
the systems interact with human users. However,
for automatically learning dialog strategies, it is not
clear how realistic versus how exploratory (Singh et
al., 2002) the training corpus should be. A train-
ing corpus needs to be exploratory with respect to
the chosen dialog system actions because if a cer-
tain action is never tried at certain states, we will
not know the value of taking that action in that state.
In (Singh et al, 2002), their system is designed to
randomly choose one from the allowed actions with
uniform probability in the training phase in order to
explore possible dialog state spaces. In contrast,we
use user simulation to generate exploratory training
data because in the tutoring system we work with,
reasonable tutor actions are largely restricted by stu-
dent performance. If certain student actions do not
appear, this system would not be able to explore a
state space randomly .
This paper investigates what kind of user simula-
tion is good for using Markov Decision Processes
(MDPs) to learn dialog strategies. In this study,
we compare three simulation models which differ in
their efforts on modeling the dialog behaviors in a
training corpus versus exploring a potentially larger
dialog space. In addition, we look into the impact of
different state space representations and different re-
ward functions on the choice of simulation models.
2 System and Corpus
Our system is a speech-enabled Intelligent Tutor-
ing System that helps students understand qualita-
1
tive physics questions. The dialog policy was deter-
ministic and hand-crafted in a finite state paradigm
(Ai et al, 2006). We collected 130 dialogs (1019
student utterances) with 26 human subjects. Cor-
rectness (correct(c), incorrect(ic)) is automatically
judged by the system1 and kept in the system?s logs.
Percent incorrectness (ic%) is also automatically
calculated and logged. Each student utterance was
manually annotated for certainty (certain, uncer-
tain, neutral, mixed) in a previous study2 based on
both lexical and prosodic information. In this study,
we use a two-way classification (certain(cert), not-
certain(ncert)), where we collapse uncertain, neu-
tral, and mixed to be ncert to balance our data. An
example of coded dialog between the tutor (T) and a
student (S) is given in Table 1.
3 Experimental Setup
3.1 Learning Task
Our current system can only respond to the cor-
rectness of a student?s utterances; the system thus
ignores other underlying information, for exam-
ple, certainty which is believed to provide use-
ful information for the tutor. In our corpus, the
strength of the tutor?s minimal feedback (defined be-
low) is in fact strongly correlated with the percent-
age of student certainty (chi-square test, p<0.01).
Strong Feedback (SF) is when the tutor clearly states
whether the student?s answer is correct or incor-
rect (i.e., ?This is great!?); Weak Feedback (WF)
is when the tutor does not comment on the correct-
ness of a student?s answer or gives slightly negative
feedback such as ?well?. Our goal is to learn how
to manipulate the strength of the tutor minimal feed-
back in order to maximize student?s overall certainty
in the entire dialog. We keep the other parts of the
tutor feedback (e.g. explanations, questions) so the
system?s original design of maximizing the percent-
age of student correct answers is utilized.
3.2 Simulation Models
All three models we describe below are trained from
the real corpus we collected. We simulate on the
word level because generating student?s dialog acts
alone does not provide sufficient information for
1Kappa of 0.79 is gained comparing to human judgements.
2Kappa of 0.68 is gained in a preliminary agreement study.
T1: Which law of motion would you use?
S1: Newton?s second law? [ic, ic%=1, ncert]
T2: Well... The best law to use is Newton?s
third law. Do you recall what it says?
S2: For every action there is an equal and
opposite reaction? [c, ic%=50%, ncert]
Table 1: Sample coded dialog excerpt.
our tutoring system to decide the next system?s ac-
tion. Thus, the output of the three models is a stu-
dent utterance along with the student certainty (cert,
ncert). Since it is hard to generate a natural lan-
guage utterance for each tutor?s question, we use the
student answers in the real corpus as the candidate
answers for the simulated students (Ai et al, 2006).
In addition, we simulate student certainty in a very
simple way: the simulation models output the cer-
tainty originally associated with that utterance.
Probabilistic Model (PM) is meant to capture re-
alistic student behavior in a probabilistic way. Given
a certain tutor question along with a tutor feedback,
it will first compute the probabilities of the four
types of student answers from the training corpus: c
and cert, c and ncert, ic and cert, and ic and ncert.
Then, following this distribution, the model selects
the type of student answers to output, and then it
picks an utterance that satisfies the correctness and
certainty constraints of the chosen answer type from
the candidate answer set and outputs that utterance.
We implement a back-off mechanism to count pos-
sible answers that do not appear in the real corpus.
Total Random Model (TRM) ignores what the
current question is or what feedback is given. It ran-
domly picks one utterance from all the utterances in
the entire candidate answer set. This model tries to
explore all the possible dialog states.
Restricted Random Model (RRM) differs from
the PM in that given a certain tutor question and a
tutor feedback, it chooses to give a c and cert, c and
ncert, ic and cert, or ic and ncert answer with equal
probability. This model is a compromise between
the exploration of the dialog state space and the re-
alness of generated user behaviors.
3.3 MDP Configuration
A MDP has four main components: states, actions,
a policy, and a reward function. In this study, the ac-
tions allowed in each dialog state are SF and WF;
2
the policy we are trying to learn is in every state
whether the tutor should give SF and WF in order
to maximize the percent certainty in the dialog.
Since different state space representations and re-
ward functions have a strong impact on the MDP
policy learning, we investigate different configura-
tions to avoid possible bias introduced by certain
configurations. We use two state space representa-
tions: SSR1 uses the correctness of current student
turn and percent incorrectness so far; and SSR2 adds
in the certainty of the current student turn on top of
SSR1. Two reward functions are investigated: in
RF1, we assign +100 to every dialog that has a per-
cent certainty higher than the median from the train-
ing corpus, and -100 to every dialog that has a per-
cent certainty below the median; in RF2, we assign
different rewards to every different dialog by multi-
plying the percent certainty in that dialog with 100.
Other MDP parameter settings are the same as de-
scribed in (Tetreault et al, 2006).
3.4 Methodology
We first let the three simulation models interact with
the original system to generate different training cor-
pora. Then, we learn three MDP policies in a fixed
configuration from the three training corpora sep-
arately. For each configuration, we run the sim-
ulation models until we get enough training data
such that the learned policies on that corpus do not
change anymore (40,000 dialogs are generated by
each model). After that, the learned new policies are
implemented into the original system respectively 3.
Finally, we use our most realistic model, the PM,
to interact with each new system 500 times to eval-
uate the new systems? performances. We use two
evaluation measures. EM1 is the number of dialogs
that would be assigned +100 using the old median
split. EM2 is the average of percent certainty in ev-
ery single dialog from the newly generated corpus.
A policy is considered better if it can improve the
percentage of certainty more than other policies, or
has more dialogs that will be assigned +100. The
baseline for EM1 is 250, since half of the 500 di-
alogs would be assigned +100 using the old median
3For example, the policy learned from the training corpus
generated by the RRM with SSR1 and RF1 is: give SF when
the current student answer is ic and ic%>50%, otherwise give
WF.
split. The baseline for EM2 is 35.21%, which is
obtained by calculating the percent certainty in the
corpus generated by the 40,000 interactions between
the PM and the original system.
4 Results and Discussion
Table 2 summarizes our results. There are two
columns under each ?state representation+reward
function? configuration, presenting the results using
the two evaluation approaches. EM1 measures ex-
actly what RF1 tries to optimize; while EM2 mea-
sures exactly what RF2 tries to optimize. However,
we show the results evaluated by both EM1 and
EM2 for all configurations since the two evaluation
measures have their own practical values and can
be deployed under different design requirements.
All results that significantly4 outperform the corre-
sponding baselines are marked with ?.
When evaluating using EM1, the RRM signifi-
cantly4 outperforms the other two models in all con-
figurations (in bold in Table 2). Also, the PM per-
forms better (but not statistically significantly) than
the TRM. When evaluating on EM2, the RRM sig-
nificantly4 outperforms the other two when using
SSR1 and RF1 (in bold in Table 2). In all other
configurations, the three models do not differ signif-
icantly. It is not surprising that the RRM outper-
forms the PM in most of the cases even when we
test on the PM. (Schatzmann et al, 2005) also ob-
serve that a good model can still perform well when
tested on a poor model.
We suspect that the performance of the PM is
harmed by the data sparsity issue in the real cor-
pus that we trained the model on. Consider the case
of SSR1: 25.8% of the potentially possible dialog
states do not exist in the real corpus. Although we
implement a back-off mechanism, the PM will still
have much less chance to transition to the states that
are not observed in the real corpus. Thus, when we
learn the MDP policy from the corpus generated by
this model, the actions to take in these less-likely
states are not fully learned. In contrast, the RRM
transitions from one state to each of the next possible
states with equal probability, which compensates for
the data sparsity problem. We further examine the
results obtained using SSR1 and RF1 and evaluated
4Using 2-sided t-test with Bonferroni correction, p<0.05.
3
Model Name SSR1+RF1 SSR2+RF1 SSR1+RF2 SSR2+RF2
EM1 EM2 EM1 EM2 EM1 EM2 EM1 EM2
Probabilistic Model 222 36.30% 217 37.63% 197 40.78%? 197 40.01%?
Total Random Model 192 36.30% 211 38.57% 188 40.21%? 179 40.21%?
Restricted Random Model 390? 46.11%? 368? 37.27% 309 40.21%? 301 40.21%?
Table 2: Evaluation of the new policies trained with the three simulation models
by EM1 to confirm our hypothesis. When looking
into the frequent states5, 70.1% of them are seen fre-
quently in the training corpus generated by the PM,
while 76.3% are seen frequently in the training cor-
pus generated by the RRM. A higher percentage in-
dicates the policy might be better trained with more
training instances. This explains why the RRM out-
performs the PM in this case.
While the TRM also tries to explore dialog state
space, only 65.2% of the frequent states in testing
phase are observed frequently in the training phase.
This is because the Total Random Model answers
90% of the questions incorrectly and often goes
deeply down the error-correction paths. It does ex-
plore some states that are at the end of the paths,
but since these are the infrequent states in the test
phase, exploring these states does not actually im-
prove the model?s performance much. On the other
hand, while the student correctness rate in the real
corpus is 60%, the RRM prevents itself from being
trapped in the less-likely states on incorrect answer
paths by keeping its correctness rate to be 50%.
Our results are preliminary but suggest interest-
ing points in building simulation models: 1. When
trained from a sparse data set, it may be better to
use a RRM than a more realistic PM or a more ex-
ploratory TRM; 2. State space representation may
not impact evaluation results as much as reward
functions and evaluation measures, since when us-
ing RF2 and evaluating with EM2, the differences
we see using RF1 or EM1 become less significant.
In our future work, we are going to further investi-
gate whether the trends shown in this paper general-
ize to on-line MDP policy learning. We also want to
explore other user simulations that are designed for
sparse training data (Henderson et al, 2005). More
5We define frequent states to be those that comprise at least
1% of the entire corpus. These frequent states add up to more
than 80% of the training/testing corpus. However, deciding the
threshold of the frequent states in training/testing is an open
question.
importantly, we are going to test the new policies
with the other simulations and human subjects to
validate the learning process.
Acknowledgements
NSF (0325054, 0328431) supports this research.
The authors wish to thank Tomas Singliar for his
valuable suggestions, Scott Silliman for his support
on building the simulation system, and the anony-
mous reviewers for their insightful comments.
References
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken Di-
alogue Corpora. In Proc. AAAI Workshop on Statis-
tical and Empirical Approaches for SDS.
J. Henderson, O.Lemon, and K.Georgila. 2005. Hybrid
reinforcement/supervised learning for dialogue poli-
cies from COMMUNICATOR data. In Proc. IJCAI
workshop on Knowledge and Reasoning in Practical
Dialogue Systems.
R. Lo?pez-Co?zar, A. De la Torre, J. Segura, and A. Ru-
bio. 2003. Assessment of dialog systems by means of
a new simulation technique. Speech Communication
(40): 387-407.
K. Scheffler. 2002. Automatic Design of Spoken Dialog
Systems. Ph.D. diss., Cambridge University.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quan-
titative Evaluation of User Simulation Techniques for
Spoken Dialog Systems. In Proc. of 6th SIGdial.
J. Schatzmann, M. N. Stuttle, K. Weilhammer and
S. Young. 2005. Effects of the User Model on
Simulation-based Learning of Dialogue Strategies. In
Proc. of ASRU05.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing Dialog Managment with Reinforcement
Learning: Experiments with the NJFun System. Jour-
nal of Artificial Intelligence Research, (16):105-133.
J. Tetreault and D. Litman. 2006. Comparing the Utility
of State Features in Spoken Dialogue Using Reinforce-
ment Learning.. In Proc. NAACL06.
4
Proceedings of NAACL HLT 2007, Companion Volume, pages 41?44,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploring Affect-Context Dependencies for Adaptive System Development
Kate Forbes-Riley
Learning R&D Ctr.
Univ. Pittsburgh
Pittsburgh, PA 15260
forbesk@pitt.edu
Mihai Rotaru
Computer Science Dpt.
Univ. Pittsburgh
Pittsburgh, PA 15260
mrotaru@cs.pitt.edu
Diane J. Litman
Learning R&D Ctr.
Computer Science Dpt.
Univ. Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Joel Tetreault
Learning R&D Ctr.
Univ. Pittsburgh
Pittsburgh, PA 15260
tetreaul@pitt.edu
Abstract
We use ?2 to investigate the context de-
pendency of student affect in our com-
puter tutoring dialogues, targeting uncer-
tainty in student answers in 3 automati-
cally monitorable contexts. Our results
show significant dependencies between
uncertain answers and specific contexts.
Identification and analysis of these depen-
dencies is our first step in developing an
adaptive version of our dialogue system.
1 Introduction
Detecting and adapting to user affect is being ex-
plored by many researchers to improve dialogue sys-
tem quality. Detection has received much atten-
tion (e.g., (Litman and Forbes-Riley, 2004; Lee and
Narayanan, 2005)), but less work has been done on
adaptation, due to the difficulty of developing re-
sponses and applying them at the right time. Most
work on adaptation takes a context-independent ap-
proach: use the same type of response after all in-
stances of an affective state. For example, Liu and
Picard (2005)?s health assessment system responds
with empathy to all instances of user stress.
Research suggests, however, that it may be more
effective to take a context-dependent approach: de-
velop multiple responses for each affective state,
whose use depends on the state?s context. E.g., in the
tutoring domain, Pon-Barry et al (2006) show that
human tutors use multiple responses to uncertain
student answers, depending on the answer?s correct-
ness and prior context. In the information-seeking
domain, it is commonly believed that while an apol-
ogy is a good default response to user frustration (as
in (Klein et al, 2002)), one context requires a differ-
ent response: after several frustrated user turns, the
call should be forwarded to a human operator.
A context-dependent approach to affect adapta-
tion must address 2 issues: in what contexts to adapt,
and what responses to use there. This paper ad-
dresses the first issue and targets student uncertainty
in our computer tutoring dialogues. Although our di-
alogues have a Question-Answer format, our system
contains 275 tutor questions. Treating each question
as a context is too labor-intensive for adaptation de-
velopment and creates a data sparsity issue. Instead
we treat automatically monitorable question proper-
ties as contexts. Here we examine 3 contexts: the di-
alogue act interpretation, and the discourse structure
depth and transition, of the prior tutor question. We
use ?2 to investigate the context dependency of un-
certain student answers (correct and incorrect). Our
results show that some contexts are significantly as-
sociated with uncertain answers. Our next step will
be to use these significant dependencies to develop
system responses to uncertain answers in these con-
texts. These responses will be based both on our
hypotheses about why uncertainty is associated with
these contexts, and on analyses of human tutor re-
sponses to uncertain answers in these contexts.
2 Student Uncertainty and Prior Context
ITSPOKE is a speech-enabled version of a text-
based tutoring system (VanLehn et al, 2002). The
student first types an essay answering one of 5 qual-
itative physics problems. ITSPOKE parses the es-
say, extracts dialogue topics concerning misconcep-
tions, then engages the student in dialogue. In this
study we used 2 ITSPOKE corpora containing 4590
student turns over 185 dialogues from 37 students.
Figure 1 shows an annotated dialogue excerpt.
41
ITSPOKE4: Good. Does the packet have an acceleration? If yes please specify its direction. [SAQ, Advance, D1]
STUDENT4: horizontally forward yes [non-uncertain, incorrect]
ITSPOKE5: Well... To find the packet?s acceleration we will use Newton?s 2nd law. What is the definition of this law? [LAQ,
Push, D2]
STUDENT5: an object in motion tends to stay in motion until it?s act by an outside force [uncertain, incorrect]
ITSPOKE6: Well... Newton?s 2nd law says that the net force acting on an object equals the mass times the acceleration of theobject. We can define Newton?s 2nd law as F=m*a. Hence what can we determine about the acceleration of the packet given
that we know the net force acting on it is not zero and points straight down? [DAQ, Advance, D2]
STUDENT6: I don?t know [uncertain, incorrect]
ITSPOKE7: Well... < further discussion of Newton?s 2nd law...> So once again on the original question. Does the packet have anacceleration? If yes please specify its direction.[SAQ, PopUp, D1]
Figure 1: ITSPOKE Annotated Dialogue Excerpt
2.1 Uncertainty and Correctness Annotations
ITSPOKE, like most computer tutors, responds only
to student correctness. ITSPOKE labels each an-
swer as correct or incorrect1 . If correct, ITSPOKE
moves on to the next question. If incorrect, then for
questions on simple topics, ITSPOKE gives the cor-
rect answer and moves on, while for questions on
complex topics (ITSPOKE4, Figure 1), ITSPOKEinitiates a sub-dialogue with remediation questions
(ITSPOKE5 - ITSPOKE6), before moving on.
Recent computer tutoring research has shown in-
terest in responding to student affect2 over cor-
rectness. Uncertainty is of particular interest: re-
searchers hypothesize that uncertainty and incorrect-
ness each create an opportunity to learn (VanLehn
et al, 2003). They cannot be equated, however.
First, an uncertain answer may be correct or incor-
rect (Pon-Barry et al, 2006). Second, uncertainty in-
dicates that the student perceives a possible miscon-
ception in their knowledge. Thus, system responses
to uncertain answers can address both the correct-
ness and the perceived misconception.
In our ITSPOKE corpora, each student answer
has been manually annotated as uncertain or non-
uncertain3 : uncertain is used to label answers ex-
pressing uncertainty or confusion about the material;
non-uncertain is used to label all other answers.
1We have also manually labeled correctness in our data;
agreement between ITSPOKE and human is 0.79 Kappa (90%).
2We use ?affect? to cover emotions and attitudes that affect
how students communicate. Although some argue ?emotion?
and ?attitude? should be distinguished, some speech researchers
find the narrow sense of ?emotion? too restrictive because it ex-
cludes states where emotion is present but not full-blown, in-
cluding arousal and attitude (Cowie and Cornelius, 2003).
3A second annotator relabeled our dataset, yielding inter-
annotator agreement of 0.73 Kappa (92%).
2.2 Context Annotations
Here we examine 3 automatically monitorable tutor
question properties as our contexts for uncertainty:
Tutor Question Acts: In prior work one annotator
labeled 4 Tutor Question Acts in one ITSPOKE cor-
pus (Litman and Forbes-Riley, 2006)4: Short (SAQ),
Long (LAQ), and Deep Answer Question (DAQ) dis-
tinguish the question in terms of content and the type
of answer it requires. Repeat (RPT) labels variants
of ?Can you repeat that?? after rejections. From
these annotations we built a hash table associating
each ITSPOKE question with a Question Act label;
with this table we automatically labeled ITSPOKE
questions in our second ITSPOKE corpus.
Discourse Structure Depth/Transition: In prior
work we showed that the discourse structure Depth
and Transition for each ITSPOKE turn can be au-
tomatically annotated (Rotaru and Litman, 2006).
E.g., as shown in Figure 1, ITSPOKE4,7 have depth1 and ITSPOKE5,6 have depth 2. We combine lev-els 3 and above (3+) due to data sparsity. 6 Transi-
tion labels represent the turn?s position relative to the
prior ITSPOKE turn: NewTopLevel labels the first
question after an essay. Advance labels questions at
the same depth as the prior question (ITSPOKE4,6).
Push labels the first question in a sub-dialogue
(after an incorrect answer) (ITSPOKE5). After asub-dialogue, ITSPOKE asks the original question
again, labeled PopUp (ITSPOKE7), or moves on tothe next question, labeled PopUpAdv. SameGoal la-
bels both ITSPOKE RPTS (after rejections) and re-
peated questions after timeouts.
4Our Acts are based on related work (Graesser et al, 1995).
Two annotators labeled the Acts in 8 dialogues in a parallel hu-
man tutoring corpus, with agreement of 0.75 Kappa (90%).
42
3 Uncertainty Context Dependencies
We use the ?2 test to investigate the context depen-
dency of uncertain (unc) or non-uncertain (nonunc)
student answers that are correct (C) or incorrect (I).
First, we compute an overall ?2 value between each
context variable and the student answer variable. For
example, the Question Act variable (QACT) has 4
values: SAQ, LAQ, DAQ, RPT. The answer vari-
able (SANSWER) also has 4 values: uncC, uncI,
nonuncC, nonuncI. Table 1 (last column) shows the
?2 value between these variables is 203.38, which
greatly exceeds the critical value of 16.92 (p? 0.05,
df=9), indicating a highly significant dependency.
Significance increases as the ?2 value increases.
Dependency Obs. Exp. ?2
QACT ? SANSWER 203.38
LAQ ? uncC + 72 22 133.98
LAQ ? uncI + 43 27 11.17
LAQ ? nonuncC - 96 151 50.13
LAQ ? nonuncI = 48 60 3.10
DAQ ? uncC = 22 22 0.01
DAQ ? uncI + 37 27 4.57
DAQ ? nonuncC = 135 149 3.53
DAQ ? nonuncI = 63 59 0.35
SAQ ? uncC - 285 328 41.95
SAQ ? uncI - 377 408 17.10
SAQ ? nonuncC + 2368 2271 66.77
SAQ ? nonuncI - 875 898 5.31
RPT ? uncC - 7 14 4.15
RPT ? uncI = 22 18 1.25
RPT ? nonuncC - 70 98 20.18
RPT ? nonuncI + 70 39 33.59
Table 1: Tutor Question Act Dependencies (p?.05:
critical ?2=16.92 (df=9); critical ?2=3.84 (df=1))
However, this does not tell us which variable val-
ues are significantly dependent. To do this, we create
a binary variable from each value of the context and
answer variables. E.g., the binary variable for LAQ
has 2 values: ?LAQ? and ?Anything Else?, and the
binary variable for uncC has 2 values: ?uncC? and
?Anything Else?. We then compute the ?2 value be-
tween the binary variables. Table 1 shows this value
is 133.98, which greatly exceeds the critical value of
3.84 (p? 0.05, df=1). The table also shows the ob-
served (72) and expected (22) counts. Comparison
determines the sign of the dependency: uncC occurs
significantly more than expected (+) after LAQ. The
?=? sign indicates a non-significant dependency.
Table 1 shows uncertain answers (uncC and uncI)
occur significantly more than expected after LAQs.
In contrast, non-uncertain answers occur signifi-
cantly less (-), or aren?t significantly dependent (=).
Also, uncI occurs significantly more than expected
after DAQs. We hypothesize that LAQs and DAQs
are associated with more uncertainty because they
are harder questions requiring definitions or deep
reasoning. Not surprisingly, uncertain (and incor-
rect) answers occur significantly less than expected
after SAQs (easier fill-in-the-blank questions). Un-
certainty shows very weak dependencies on RPTs.
Table 2 shows that Depth1 is associated with more
correctness and less uncertainty overall. Both types
of correct answer occur significantly more than ex-
pected, but this dependency is stronger for nonuncC.
Both incorrect answers occur significantly less than
expected, but this dependency is stronger for uncI.
Dependency Obs. Exp. ?2
Depth# ? SANSWER 53.85
Depth1 ? uncC + 250 228 5.46
Depth1 ? uncI - 230 283 27.55
Depth1 ? nonuncC + 1661 1579 24.73
Depth1 ? nonuncI - 575 625 12.66
Depth2 ? uncC - 78 101 7.80
Depth2 ? uncI + 156 125 11.26
Depth2 ? nonuncC - 664 699 5.65
Depth2 ? nonuncI + 304 277 4.80
Depth3+ ? uncC = 58 57 0.05
Depth3+ ? uncI + 93 70 9.76
Depth3+ ? nonuncC - 344 391 15.66
Depth3+ ? nonuncI + 177 155 4.94
Table 2: Depth Dependencies (p?.05: critical
?2=12.59 (df=6); critical ?2=3.84 (df=1))
At Depths 2 and 3+, correct answers occur sig-
nificantly less than expected or show no signifi-
cance. Incorrect answers occur significantly more
than expected, and the dependencies are stronger for
uncI. We hypothesize that deeper depths are asso-
ciated with increased uncertainty and incorrectness
because they correspond to deeper knowledge gaps;
uncertainty here may also relate to a perceived lack
of cohesion between sub-topic and larger solution.
Table 3 shows Pushes have the same dependen-
cies as deeper depths (increased uncertainty and in-
correctness); however, here the uncI dependency is
only slightly stronger than nonuncI, which suggests
that increased uncertainty at deeper depths is more
reliably associated with remediation questions after
the Push. Although uncertainty shows only weak
43
dependencies on PopUps, after PopUpAdvs the uncI
dependency is strong, with uncI occurring more than
expected. We hypothesize that this dependency re-
lates to students losing track of the original ques-
tion/larger topic. Uncertainty shows only weak de-
pendencies on Advances. After NewTopLevels, in-
correct answers occur less than expected, but the de-
pendency is stronger for nonuncI. After SameGoals,
incorrect answers occur more than expected, but the
dependency is stronger for nonuncI. Compared with
the RPT results, the SameGoal results suggest stu-
dents feel increased uncertainty after timeouts.
Dependency Obs. Exp. ?2
TRANS ? SANSWER 190.97
Push ? uncC = 68 57 2.89
Push ? uncI + 100 70 16.37
Push ? nonuncC - 313 392 44.51
Push ? nonuncI + 193 155 14.13
PopUp ? uncC - 23 36 5.89
PopUp ? uncI - 32 45 4.68
PopUp ? nonuncC = 260 251 0.81
PopUp ? nonuncI + 117 99 4.47
PopUpAdv ? uncC = 8 13 2.50
PopUpAdv ? uncI + 32 17 16.22
PopUpAdv ? nonuncC - 76 93 7.72
PopUpAdv ? nonuncI = 44 37 1.89
Advance ? uncC = 217 205 1.70
Advance ? uncI - 223 254 9.06
Advance ? nonuncC + 1465 1416 8.66
Advance ? nonuncI - 530 560 4.51
NewTopLevel ? uncC = 53 54 0.04
NewTopLevel ? uncI - 49 67 6.47
NewTopLevel ? nonuncC + 463 375 57.33
NewTopLevel ? nonuncI - 80 148 47.63
SameGoal ? uncC = 17 21 0.70
SameGoal ? uncI + 43 25 14.24
SameGoal ? nonuncC - 92 152 44.25
SameGoal ? nonuncI + 92 56 31.43
Table 3: Transition Dependencies (p?.05: critical
?2=25.00 (df=15); critical ?2=3.84 (df=1))
4 Current Directions
We analyzed dependencies between uncertain stu-
dent answers and 3 automatically monitorable con-
texts. We plan to examine more contexts, such as
a Topic Repetition variable that tracks similar ques-
tions about a topic (e.g. gravity) across dialogues.
Our next step will be to use the significant de-
pendencies to develop system responses to uncer-
tain answers in these contexts. These responses will
be based both on our hypotheses about why uncer-
tainty is significantly associated with these contexts,
as well as on analyses of human tutor responses
in these contexts, using our human tutoring corpus,
which was collected with our first ITSPOKE corpus
using the same experimental procedure.
We also plan to investigate context dependencies
for other affective states, such as student frustration.
Acknowledgments
NSF (#0631930, #0354420 and #0328431) and
ONR (N00014-04-1-0108) support this research.
References
R. Cowie and R. R. Cornelius. 2003. Describing the
emotional states that are expressed in speech. Speech
Communication, 40:5?32.
A. Graesser, N. Person, and J. Magliano. 1995. Collabo-
rative dialog patterns in naturalistic one-on-one tutor-ing. Applied Cognitive Psychology, 9:495?522.
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-sults. Interacting with Computers, 14:119?140.
C. M. Lee and S. Narayanan. 2005. Towards detect-
ing emotions in spoken dialogs. IEEE Transactions
on Speech and Audio Processing, 13(2), March.
D. Litman and K. Forbes-Riley. 2004. Predicting student
emotions in computer-human tutoring dialogues. In
Proc. ACL, pages 352?359.
D. J. Litman and K. Forbes-Riley. 2006. Correlations
between dialogue acts and learning in spoken tutoringdialogues. Natural Language Engineering, 12(2).
K. Liu and R. W. Picard. 2005. Embedded empathy
in continuous, interactive health assessment. In CHI
Workshop on HCI Challenges in Health Assessment.
H. Pon-Barry, K. Schultz, E. Bratt, B. Clark, and S. Pe-
ters. 2006. Responding to student uncertainty in spo-ken tutorial dialogue systems. International Journal
of Artificial Intelligence in Education, 16:171?194.
M. Rotaru and D. Litman. 2006. Exploiting discoursestructure for spoken dialogue performance analysis. In
Proceedings of EMNLP, Sydney, Australia.
K. VanLehn, P. W. Jordan, and C. P. Rose? et al 2002. Thearchitecture of Why2-Atlas: A coach for qualitativephysics essay writing. In Proceedings of ITS.
K. VanLehn, S. Siler, and C. Murray. 2003. Why doonly some events cause learning during human tutor-ing? Cognition and Instruction, 21(3):209?249.
44
Predicting Student Emotions in Computer-Human Tutoring Dialogues
Diane J. Litman
University of Pittsburgh
Department of Computer Science
Learning Research and Development Center
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Kate Forbes-Riley
University of Pittsburgh
Learning Research and Development Center
Pittsburgh PA, 15260, USA
forbesk@pitt.edu
Abstract
We examine the utility of speech and lexical fea-
tures for predicting student emotions in computer-
human spoken tutoring dialogues. We first anno-
tate student turns for negative, neutral, positive and
mixed emotions. We then extract acoustic-prosodic
features from the speech signal, and lexical items
from the transcribed or recognized speech. We com-
pare the results of machine learning experiments us-
ing these features alone or in combination to pre-
dict various categorizations of the annotated student
emotions. Our best results yield a 19-36% relative
improvement in error reduction over a baseline. Fi-
nally, we compare our results with emotion predic-
tion in human-human tutoring dialogues.
1 Introduction
This paper explores the feasibility of automatically
predicting student emotional states in a corpus of
computer-human spoken tutoring dialogues. Intel-
ligent tutoring dialogue systems have become more
prevalent in recent years (Aleven and Rose, 2003),
as one method of improving the performance gap
between computer and human tutors; recent exper-
iments with such systems (e.g., (Graesser et al,
2002)) are starting to yield promising empirical
results. Another method for closing this perfor-
mance gap has been to incorporate affective reason-
ing into computer tutoring systems, independently
of whether or not the tutor is dialogue-based (Conati
et al, 2003; Kort et al, 2001; Bhatt et al, 2004). For
example, (Aist et al, 2002) have shown that adding
human-provided emotional scaffolding to an auto-
mated reading tutor increases student persistence.
Our long-term goal is to merge these lines of dia-
logue and affective tutoring research, by enhancing
our intelligent tutoring spoken dialogue system to
automatically predict and adapt to student emotions,
and to investigate whether this improves learning
and other measures of performance.
Previous spoken dialogue research has shown
that predictive models of emotion distinctions (e.g.,
emotional vs. non-emotional, negative vs. non-
negative) can be developed using features typically
available to a spoken dialogue system in real-time
(e.g, acoustic-prosodic, lexical, dialogue, and/or
contextual) (Batliner et al, 2000; Lee et al, 2001;
Lee et al, 2002; Ang et al, 2002; Batliner et al,
2003; Shafran et al, 2003). In prior work we
built on and generalized such research, by defin-
ing a three-way distinction between negative, neu-
tral, and positive student emotional states that could
be reliably annotated and accurately predicted in
human-human spoken tutoring dialogues (Forbes-
Riley and Litman, 2004; Litman and Forbes-Riley,
2004). Like the non-tutoring studies, our results
showed that combining feature types yielded the
highest predictive accuracy.
In this paper we investigate the application of
our approach to a comparable corpus of computer-
human tutoring dialogues, which displays many dif-
ferent characteristics, such as shorter utterances, lit-
tle student initiative, and non-overlapping speech.
We investigate whether we can annotate and predict
student emotions as accurately and whether the rel-
ative utility of speech and lexical features as pre-
dictors is the same, especially when the output of
the speech recognizer is used (rather than a human
transcription of the student speech). Our best mod-
els for predicting three different types of emotion
classifications achieve accuracies of 66-73%, repre-
senting relative improvements of 19-36% over ma-
jority class baseline errors. Our computer-human
results also show interesting differences compared
with comparable analyses of human-human data.
Our results provide an empirical basis for enhanc-
ing our spoken dialogue tutoring system to automat-
ically predict and adapt to a student model that in-
cludes emotional states.
2 Computer-Human Dialogue Data
Our data consists of student dialogues with IT-
SPOKE (Intelligent Tutoring SPOKEn dialogue
system) (Litman and Silliman, 2004), a spoken dia-
logue tutor built on top of the Why2-Atlas concep-
tual physics text-based tutoring system (VanLehn et
al., 2002). In ITSPOKE, a student first types an
essay answering a qualitative physics problem. IT-
SPOKE then analyzes the essay and engages the stu-
dent in spoken dialogue to correct misconceptions
and to elicit complete explanations.
First, the Why2-Atlas back-end parses the student
essay into propositional representations, in order to
find useful dialogue topics. It uses 3 different ap-
proaches (symbolic, statistical and hybrid) compet-
itively to create a representation for each sentence,
then resolves temporal and nominal anaphora and
constructs proofs using abductive reasoning (Jor-
dan et al, 2004). During the dialogue, student
speech is digitized from microphone input and sent
to the Sphinx2 recognizer, whose stochastic lan-
guage models have a vocabulary of 1240 words and
are trained with 7720 student utterances from eval-
uations of Why2-Atlas and from pilot studies of IT-
SPOKE. Sphinx2?s best ?transcription? (recognition
output) is then sent to the Why2-Atlas back-end for
syntactic, semantic and dialogue analysis. Finally,
the text response produced by Why2-Atlas is sent
to the Cepstral text-to-speech system and played to
the student. After the dialogue, the student revises
the essay, thereby ending the tutoring or causing an-
other round of tutoring/essay revision.
Our corpus of dialogues with ITSPOKE was col-
lected from November 2003 - April 2004, as part
of an evaluation comparing ITSPOKE, Why2-Atlas,
and human tutoring (Litman et al, 2004). Sub-
jects are University of Pittsburgh students who have
never taken college physics, and who are native En-
glish speakers. Subjects first read a small document
of background physics material, then work through
5 problems (dialogues) with ITSPOKE. The corpus
contains 100 dialogues (physics problems) from 20
subjects, with a total of 2445 student turns and 398
unique words. 15 dialogues have been annotated for
emotion as described in Section 3. On average, our
dialogues last 19.4 minutes and contain 25 student
turns. While ITSPOKE?s word error rate on this
corpus is 31.2%, semantic accuracy is more useful
for dialogue evaluation as it does not penalize for
unimportant word errors. Semantic analysis based
on speech recognition is the same as based on per-
fect transcription 92.4% of the time. An emotion-
annotated corpus example is shown in Figure 1.
3 Annotating Student Turns
In our data, student ?emotions?1 can only be iden-
tified indirectly: via what is said and/or how it is
1We use the term ?emotion? loosely to cover both affects
and attitudes that can impact student learning.
. . . dialogue excerpt at 18.3 min. into session. . .
ITSPOKE   : What is the magnitude of the accel-
eration of the packet in the horizontal direction?
STUDENT  : i don?t know (NEGATIVE)
ITSPOKE    : Ok, let?s consider how forces in the
vertical direction affect the motion of a body in a
horizontal direction in a different scenario. Say an
apple falls from a tree. What force(s) are acting on
the apple as it falls?
STUDENT  : gravity (NEGATIVE)
ITSPOKE   : Yeah. Are there any other forces act-
ing on the apple as it falls?
STUDENT  	 : no (NEGATIVE)
ITSPOKE  
 : Fine. The only force acting on the
apple as it falls to the earth is the earth?s gravita-
tional force. In which direction does gravitational
force act?
STUDENT   : down (POSITIVE)
ITSPOKE   : Yep. Therefore, what is the magni-
tude of this gravitational force in the horizontal di-
rection?
STUDENT    : in the direction of the airplane
(NEUTRAL)
Figure 1: Annotated Spoken Dialogue Excerpt
said. In (Litman and Forbes-Riley, 2004), we dis-
cuss a scheme for manually annotating student turns
in a human-human tutoring dialogue corpus for in-
tuitively perceived emotions.2 These emotions are
viewed along a linear scale, shown and defined as
follows: negative  neutral  positive.
Negative: a student turn that expresses emotions
such as confused, bored, irritated. Evidence of a
negative emotion can come from many knowledge
sources such as lexical items (e.g., ?I don?t know?
in student  in Figure 1), and/or acoustic-prosodic
features (e.g., prior-turn pausing in student  	 ).
Positive: a student turn expressing emotions such
as condent, enthusiastic. An example is student   ,
which displays louder speech and faster tempo.
Neutral: a student turn not expressing a nega-
tive or positive emotion. An example is student    ,
where evidence comes from moderate loudness,
pitch and tempo.
We also distinguish Mixed: a student turn ex-
pressing both positive and negative emotions.
To avoid influencing the annotator?s intuitive un-
derstanding of emotion expression, and because
particular emotional cues are not used consistently
2Weak and strong expressions of emotions are annotated.
or unambiguously across speakers, our annotation
manual does not associate particular cues with par-
ticular emotion labels. Instead, it contains examples
of labeled dialogue excerpts (as in Figure 1, except
on human-human data) with links to corresponding
audio files. The cues mentioned in the discussion of
Figure 1 above were elicited during post-annotation
discussion of the emotions, and are presented here
for expository use only. (Litman and Forbes-Riley,
2004) further details our annotation scheme and dis-
cusses how it builds on related work.
To analyze the reliability of the scheme on our
new computer-human data, we selected 15 tran-
scribed dialogues from the corpus described in Sec-
tion 2, yielding a dataset of 333 student turns, where
approximately 30 turns came from each of 10 sub-
jects. The 333 turns were separately annotated by
two annotators following the emotion annotation
scheme described above.
We focus here on three analyses of this data, item-
ized below. While the first analysis provides the
most fine-grained distinctions for triggering system
adaptation, the second and third (simplified) analy-
ses correspond to those used in (Lee et al, 2001)
and (Batliner et al, 2000), respectively. These
represent alternative potentially useful triggering
mechanisms, and are worth exploring as they might
be easier to annotate and/or predict.
 Negative, Neutral, Positive (NPN): mixeds
are conflated with neutrals.
 Negative, Non-Negative (NnN): positives,
mixeds, neutrals are conflated as non-
negatives.
 Emotional, Non-Emotional (EnE): nega-
tives, positives, mixeds are conflated as Emo-
tional; neutrals are Non-Emotional.
Tables 1-3 provide a confusion matrix for each
analysis summarizing inter-annotator agreement.
The rows correspond to the labels assigned by an-
notator 1, and the columns correspond to the labels
assigned by annotator 2. For example, the annota-
tors agreed on 89 negatives in Table 1.
In the NnN analysis, the two annotators agreed on
the annotations of 259/333 turns achieving 77.8%
agreement, with Kappa = 0.5. In the EnE analy-
sis, the two annotators agreed on the annotations
of 220/333 turns achieving 66.1% agreement, with
Kappa = 0.3. In the NPN analysis, the two anno-
tators agreed on the annotations of 202/333 turns
achieving 60.7% agreement, with Kappa = 0.4. This
inter-annotator agreement is on par with that of
prior studies of emotion annotation in naturally oc-
curring computer-human dialogues (e.g., agreement
of 71% and Kappa of 0.47 in (Ang et al, 2002),
Kappa of 0.45 and 0.48 in (Narayanan, 2002), and
Kappa ranging between 0.32 and 0.42 in (Shafran
et al, 2003)). A number of researchers have ac-
commodated for this low agreement by exploring
ways of achieving consensus between disagreed an-
notations, to yield 100% agreement (e.g (Ang et al,
2002; Devillers et al, 2003)). As in (Ang et al,
2002), we will experiment below with predicting
emotions using both our agreed data and consensus-
labeled data.
negative non-negative
negative 89 36
non-negative 38 170
Table 1: NnN Analysis Confusion Matrix
emotional non-emotional
emotional 129 43
non-emotional 70 91
Table 2: EnE Analysis Confusion Matrix
negative neutral positive
negative 89 30 6
neutral 32 94 38
positive 6 19 19
Table 3: NPN Analysis Confusion Matrix
4 Extracting Features from Turns
For each of the 333 student turns described above,
we next extracted the set of features itemized in Fig-
ure 2, for use in the machine learning experiments
described in Section 5.
Motivated by previous studies of emotion predic-
tion in spontaneous dialogues (Ang et al, 2002; Lee
et al, 2001; Batliner et al, 2003), our acoustic-
prosodic features represent knowledge of pitch, en-
ergy, duration, tempo and pausing. We further re-
strict our features to those that can be computed
automatically and in real-time, since our goal is to
use such features to trigger online adaptation in IT-
SPOKE based on predicted student emotions. F0
and RMS values, representing measures of pitch and
loudness, respectively, are computed using Entropic
Research Laboratory?s pitch tracker, get f0, with no
post-correction. Amount of Silence is approximated
as the proportion of zero f0 frames for the turn. Turn
Duration and Prior Pause Duration are computed
Acoustic-Prosodic Features
 4 fundamental frequency (f0): max, min,
mean, standard deviation
 4 energy (RMS): max, min, mean, standard de-
viation
 4 temporal: amount of silence in turn, turn du-
ration, duration of pause prior to turn, speaking
rate
Lexical Features
 human-transcribed lexical items in the turn
 ITSPOKE-recognized lexical items in the turn
Identifier Features: subject, gender, problem
Figure 2: Features Per Student Turn
automatically via the start and end turn boundaries
in ITSPOKE logs. Speaking Rate is automatically
calculated as #syllables per second in the turn.
While acoustic-prosodic features address how
something is said, lexical features representing what
is said have also been shown to be useful for predict-
ing emotion in spontaneous dialogues (Lee et al,
2002; Ang et al, 2002; Batliner et al, 2003; Dev-
illers et al, 2003; Shafran et al, 2003). Our first set
of lexical features represents the human transcrip-
tion of each student turn as a word occurrence vec-
tor (indicating the lexical items that are present in
the turn). This feature represents the ?ideal? perfor-
mance of ITSPOKE with respect to speech recogni-
tion. The second set represents ITSPOKE?s actual
best speech recognition hypothesis of what is said in
each student turn, again as a word occurrence vec-
tor.
Finally, we recorded for each turn the 3 ?iden-
tifier? features shown last in Figure 2. Prior stud-
ies (Oudeyer, 2002; Lee et al, 2002) have shown
that ?subject? and ?gender? can play an important
role in emotion recognition. ?Subject? and ?prob-
lem? are particularly important in our tutoring do-
main because students will use our system repeat-
edly, and problems are repeated across students.
5 Predicting Student Emotions
5.1 Feature Sets and Method
We next created the 10 feature sets in Figure 3,
to study the effects that various feature combina-
tions had on predicting emotion. We compare
an acoustic-prosodic feature set (?sp?), a human-
transcribed lexical items feature set (?lex?) and
an ITSPOKE-recognized lexical items feature set
(?asr?). We further compare feature sets combin-
ing acoustic-prosodic and either transcribed or rec-
ognized lexical items (?sp+lex?, ?sp+asr?). Finally,
we compare each of these 5 feature sets with an
identical set supplemented with our 3 identifier fea-
tures (?+id?).
sp: 12 acoustic-prosodic features
lex: human-transcribed lexical items
asr: ITSPOKE recognized lexical items
sp+lex: combined sp and lex features
sp+asr: combined sp and asr features
+id: each above set + 3 identifier features
Figure 3: Feature Sets for Machine Learning
We use the Weka machine learning soft-
ware (Witten and Frank, 1999) to automatically
learn our emotion prediction models. In our human-
human dialogue studies (Litman and Forbes, 2003),
the use of boosted decision trees yielded the most
robust performance across feature sets so we will
continue their use here.
5.2 Predicting Agreed Turns
As in (Shafran et al, 2003; Lee et al, 2001), our
first study looks at the clearer cases of emotional
turns, i.e. only those student turns where the two
annotators agreed on an emotion label.
Tables 4-6 show, for each emotion classification,
the mean accuracy (%correct) and standard error
(SE) for our 10 feature sets (Figure 3), computed
across 10 runs of 10-fold cross-validation.3 For
comparison, the accuracy of a standard baseline al-
gorithm (MAJ), which always predicts the major-
ity class, is shown in each caption. For example,
Table 4?s caption shows that for NnN, always pre-
dicting the majority class of non-negative yields an
accuracy of 65.65%. In each table, the accuracies
are labeled for how they compare statistically to the
relevant baseline accuracy (  = worse,  = same, 
= better), as automatically computed in Weka using
a two-tailed t-test (p  .05).
First note that almost every feature set signif-
icantly outperforms the majority class baseline,
across all emotion classifications; the only excep-
tions are the speech-only feature sets without iden-
tifier features (?sp-id?) in the NnN and EnE tables,
which perform the same as the baseline. These re-
sults suggest that without any subject or task spe-
cific information, acoustic-prosodic features alone
3For each cross-validation, the training and test data are
drawn from utterances produced by the same set of speakers.
A separate experiment showed that testing on one speaker and
training on the others, averaged across all speakers, does not
significantly change the results.
are not useful predictors for our two binary classi-
fication tasks, at least in our computer-human dia-
logue corpus. As will be discussed in Section 6,
however, ?sp-id? feature sets are useful predictors
in human-human tutoring dialogues.
Feat. Set -id SE +id SE
sp 64.10  0.80 70.66  0.76
lex 68.20  0.41 72.74  0.58
asr 72.30  0.58 70.51  0.59
sp+lex 71.78  0.77 72.43  0.87
sp+asr 69.90  0.57 71.44b 0.68
Table 4: %Correct, NnN Agreed, MAJ (non-
negative) = 65.65%
Feat. Set -id SE +id SE
sp 59.18  0.75 70.68  0.89
lex 63.18  0.82 75.64  0.37
asr 66.36  0.54 72.91  0.35
sp+lex 63.86  0.97 69.59  0.48
sp+asr 65.14  0.82 69.64  0.57
Table 5: %Correct, EnE Agreed, MAJ (emotional)
= 58.64%
Feat. Set -id SE +id SE
sp 55.49  1.01 62.03  0.91
lex 52.66  0.62 67.84  0.66
asr 57.95  0.67 65.70  0.50
sp+lex 62.08  0.56 63.52  0.48
sp+asr 61.22  1.20 62.23  0.86
Table 6: %Correct, NPN Agreed, MAJ (neutral) =
46.52%
Further note that adding identifier features to the
?-id? feature sets almost always improves perfor-
mance, although this difference is not always sig-
nificant4; across tables the ?+id? feature sets out-
perform their ?-id? counterparts across all feature
sets and emotion classifications except one (NnN
?asr?). Surprisingly, while (Lee et al, 2002) found
it useful to develop separate gender-based emotion
prediction models, in our experiment, gender is the
only identifier that does not appear in any learned
model. Also note that with the addition of identifier
features, the speech-only feature sets (sp+id) now
do outperform the majority class baselines for all
three emotion classifications.
4For any feature set, the mean +/- 2*SE = the 95% con-
fidence interval. If the confidence intervals for two feature
sets are non-overlapping, then their mean accuracies are sig-
nificantly different with 95% confidence.
With respect to the relative utility of lexical ver-
sus acoustic-prosodic features, without identifier
features, using only lexical features (?lex? or ?asr?)
almost always produces statistically better perfor-
mance than using only speech features (?sp?); the
only exception is NPN ?lex?, which performs sta-
tistically the same as NPN ?sp?. This is consistent
with others? findings, e.g., (Lee et al, 2002; Shafran
et al, 2003). When identifier features are added
to both, the lexical sets don?t always significantly
outperform the speech set; only in NPN and EnE
?lex+id? is this the case. For NnN, just as using
?sp+id? rather than ?sp-id? improved performance
when compared to the majority baseline, the addi-
tion of the identifier features also improves the util-
ity of the speech features when compared to the lex-
ical features.
Interestingly, although we hypothesized that the
?lex? feature sets would present an upper bound on
the performance of the ?asr? sets, because the hu-
man transcription is more accurate than the speech
recognizer, we see that this is not consistently the
case. In fact, in the ?-id? sets, ?asr? always signifi-
cantly outperforms ?lex?. A comparison of the de-
cision trees produced in either case, however, does
not reveal why this is the case; words chosen as pre-
dictors are not very intuitive in either case (e.g., for
NnN, an example path through the learned ?lex? de-
cision tree says predict negative if the utterance con-
tains the word will but does not contain the word
decrease). Understanding this result is an area for
future research. Within the ?+id? sets, we see that
?lex? and ?asr? perform the same in the NnN and
NPN classifications; in EnE ?lex+id? significantly
outperforms ?asr+id?. The utility of the ?lex? fea-
tures compared to ?asr? also increases when com-
bined with the ?sp? features (with and without iden-
tifiers), for both NnN and NPN.
Moreover, based on results in (Lee et al, 2002;
Ang et al, 2002; Forbes-Riley and Litman, 2004),
we hypothesized that combining speech and lexical
features would result in better performance than ei-
ther feature set alne. We instead found that the rel-
ative performance of these sets depends both on the
emotion classification being predicted and the pres-
ence or absence of ?id? features. Although consis-
tently with prior research we find that the combined
feature sets usually outperform the speech-only fea-
ture sets, the combined feature sets frequently per-
form worse than the lexical-only feature sets. How-
ever, we will see in Section 6 that combining knowl-
edge sources does improve prediction performance
in human-human dialogues.
Finally, the bolded accuracies in each table sum-
marize the best-performing feature sets with and
without identifiers, with respect to both the %Corr
figures shown in the tables, as well as to relative
improvement in error reduction over the baseline
(MAJ) error5, after excluding all the feature sets
containing ?lex? features. In this way we give a
better estimate of the best performance our system
could accomplish, given the features it can currently
access from among those discussed. These best-
performing feature sets yield relative improvements
over their majority baseline errors ranging from 19-
36%. Moreover, although the NPN classification
yields the lowest raw accuracies, it yields the high-
est relative improvement over its baseline.
5.3 Predicting Consensus Turns
Following (Ang et al, 2002; Devillers et al, 2003),
we also explored consensus labeling, both with the
goal of increasing our usable data set for predic-
tion, and to include the more difficult annotation
cases. For our consensus labeling, the original an-
notators revisited each originally disagreed case,
and through discussion, sought a consensus label.
Due to consensus labeling, agreement rose across
all three emotion classifications to 100%. Tables 7-
9 show, for each emotion classification, the mean
accuracy (%correct) and standard error (SE) for our
10 feature sets.
Feat. Set -id SE +id SE
sp 59.10  0.57 64.20  0.52
lex 63.70  0.47 68.64  0.41
asr 66.26  0.71 68.13  0.56
sp+lex 64.69  0.61 65.40  0.63
sp+asr 65.99  0.51 67.55  0.48
Table 7: %Corr., NnN Consensus, MAJ=62.47%
Feat. Set -id SE +id SE
sp 56.13  0.94 59.30  0.48
lex 52.07  0.34 65.37  0.47
asr 53.78  0.66 64.13  0.51
sp+lex 60.96  0.76 63.01  0.62
sp+asr 57.84  0.73 60.89  0.38
Table 8: %Corr., EnE Consensus, MAJ=55.86%
A comparison with Tables 4-6 shows that overall,
using consensus-labeled data decreased the perfor-
mance across all feature sets and emotion classifi-
cations. This was also found in (Ang et al, 2002).
Moreover, it is no longer the case that every feature
5Relative improvement over the baseline (MAJ) error for
feature set x = ffCo-training for Predicting Emotions with Spoken Dialogue Data 
Beatriz Maeireizo and Diane Litman and Rebecca Hwa 
Department of Computer Science 
University of Pittsburgh 
Pittsburgh, PA 15260, U.S.A. 
beamt@cs.pitt.edu, litman@cs.pitt.edu, hwa@cs.pitt.edu 
 
Abstract 
Natural Language Processing applications 
often require large amounts of annotated 
training data, which are expensive to obtain.  
In this paper we investigate the applicability of 
Co-training to train classifiers that predict 
emotions in spoken dialogues.  In order to do 
so, we have first applied the wrapper approach 
with Forward Selection and Na?ve Bayes, to 
reduce the dimensionality of our feature set. 
Our results show that Co-training can be 
highly effective when a good set of features 
are chosen.  
1 Introduction 
In this paper we investigate the automatic 
labeling of spoken dialogue data, in order to train a 
classifier that predicts students? emotional states in 
a human-human speech-based tutoring corpus.  
Supervised training of classifiers requires 
annotated data, which demands costly efforts from 
human annotators.  One approach to minimize this 
effort is to use Co-training (Blum and Mitchell, 
1998), a semi-supervised algorithm in which two 
learners are iteratively combining their outputs to 
increase the training set used to re-train each other 
and generate more labeled data automatically.  The 
main focus of this paper is to explore how Co-
training can be applied to annotate spoken 
dialogues.  A major challenge to address is in 
reducing the dimensionality of the many features 
available to the learners. 
The motivation for our research arises from the 
need to annotate a human-human speech corpus for 
the ITSPOKE (Intelligent Tutoring SPOKEn 
dialogue System) project (Litman and Silliman, 
2004). Ongoing research in ITSPOKE aims to 
recognize emotional states of students in order to 
build a spoken dialogue tutoring system that 
automatically predicts and adapts to the student?s 
emotions.  ITSPOKE uses supervised learning to 
predict emotions with spoken dialogue data.  Al-
though a large set of dialogues have been 
collected, only 8% of them have been annotated 
(10 dialogues with a total of 350 utterances), due to 
the laborious annotation process.  We believe that 
increasing the size of the training set with more 
annotated examples will increase the accuracy of 
the system?s predictions.  Therefore, we are 
looking for a less labour-intensive approach to data 
annotation.  
2 Data 
Our data consists of the student turns in a set of 
10 spoken dialogues randomly selected from a 
corpus of 128 qualitative physics tutoring 
dialogues between a human tutor and University of 
Pittsburgh undergraduates.  Prior to our study, the 
453 student turns in these 10 dialogues were 
manually labeled by two annotators as either 
"Emotional" or "Non-Emotional" (Litman and 
Forbes-Riley, 2004).  Perceived student emotions 
(e.g. confidence, confusion, boredom, irritation, 
etc.) were coded based on both what the student 
said and how he or she said it. For this study, we 
use only the 350 turns where both annotators 
agreed on the emotion label. 51.71% of these turns 
were labeled as Non-Emotional and the rest as 
Emotional. 
Also prior to our study, each annotated turn was 
represented as a vector of 449 features 
hypothesized to be relevant for emotion prediction 
(Forbes-Riley and Litman, 2004).  The features 
represent acoustic-prosodic (pitch, amplitude, 
temporal), lexical, and other linguistic 
characteristics of both the turn and its local and 
global dialogue context.   
3 Machine Learning Techniques 
In this section, we will briefly describe the ma-
chine learning techniques used by our system. 
3.1 Co-training 
To address the challenge of training classifiers 
when only a small set of labeled examples is 
available, Blum and Mitchell (1998) proposed Co-
training as a way to bootstrap classifiers from a 
large set of unlabeled data.  Under this framework, 
two (or more) learners are trained iteratively in 
tandem.  In each iteration, the learners classify 
more unlabeled data to increase the training data 
for each other.  In theory, the learners must have 
distinct views of the data (i.e., their features are 
conditionally independent given the label 
example), but some studies suggest that Co-
training can still be helpful even when the 
independence assumption does not hold (Goldman, 
2000). 
To apply Co-training to our task, we develop 
two high-precision learners: Emotional and Non-
Emotional.  The learners use different features 
because each is maximizing the precision of its 
label (possibly with low recall).  While we have 
not proved these two learners are conditionally 
independent, this division of expertise ensures that 
the learners are different.  The algorithm for our 
Co-training system is shown in Figure 1. Each 
learner selects the examples whose predicted 
labeled corresponds to its expertise class with the 
highest confidence.  The maximum number of 
iterations and the number of examples added per 
iteration are parameters of the system. 
While iteration < MAXITERATION 
   Emo_Learner.Train(train) 
   NE_Learner.Train(train) 
 
   emo_Predictions = Emo_Learner.Predict(predict) 
   ne_Predictions = NE_Learner.Predict(predict) 
 
   emo_sorted_Predictions = Sort_by_confidence( 
                             emo_Predictions) 
   ne_sorted_Predictions = Sort_by_confidence( 
                             ne_Predictions) 
 
   best_emo = Emo_Learner.select_best( 
                             emo_sorted_Predictions, 
                             NUM_SAMPLES_TO_ADD) 
   best_ne = NE_Learner.select_best( 
                             ne_sorted_Predictions,  
                             NUM_SAMPLES_TO_ADD) 
    
   train = train ? best_emo ? best_ne 
   predict = predict ? best_emo ? best_ne 
end 
 
Figure 1. Algorithm for Co-training System 
3.2 Wrapper Approach with Forward 
Selection 
As described in Section 2, 449 features have 
been currently extracted from each utterance of the 
ITSPOKE corpus (where an utterance is a 
student?s turn in a dialogue).  Unfortunately, high 
dimensionality, i.e. large amount of input features, 
may lead to a large variance of estimates, noise, 
overfitting, and in general, higher complexity and 
inefficiencies in the learners.  Different approaches 
have been proposed to address this problem.  In 
this work, we have used the Wrapper Approach 
with Forward Selection. 
The Wrapper Approach, introduced by John et 
al. (1994) and refined later by Kohavi and John 
(1997), is a method that searches for a good subset 
of relevant features using an induction algorithm as 
part of the evaluation function.  We can apply 
different search algorithms to find this set of 
features. 
Forward Selection is a greedy search algorithm 
that begins with an empty set of features, and 
greedily adds features to the set.  Figure 2 shows 
our algorithm implemented for the forward 
wrapper approach. 
bestFeatures = [] 
while dim(bestFeatures) < MINFEATURES 
  for iterations = 1: MAXITERATIONS 
   split train into training/development 
   parameters = computeParameters(training) 
   for feature = 1:MAXFEATURES 
 
  evaluate(parameters,development, 
                      [bestFeatures + feature]) 
 
  keep validation performance 
   end 
 
 end 
 average_performance and keep average_performance 
   end 
   B = best average_performance  
   bestFeatures  B ? bestFeatures 
end 
 
Figure 2. Implemented algorithm for forward 
wrapper approach.  The variables underlined are 
the ones whose parameters we have changed in 
order to test and improve the performance. 
We can use different criteria to select the feature 
to add, depending on the object of optimization. 
Earlier, we have explained the basis of the Co-
training system.  When developing an expert 
learner in one class, we want it to be correct most 
of the time when it guesses that class.  That is, we 
want the classifier to have high precision (possibly 
at the cost of lower overall accuracy).  Therefore, 
we are interested in finding the best set of features 
for precision in each class.  In this case, we are 
focusing on Emotional and Non-Emotional 
classifiers. 
Figure 3 shows the formulas used for the 
optimization criterion on each class.  For the 
Emotional Class, our optimization criterion was to 
maximize the PPV (Positive Predictive Value), and 
for the Non-Emotional Class our optimization 
criterion was to maximize the NPV (Negative 
Predictive Value). 
 
Figure 3. Confusion Matrix, Positive Predictive 
Value (Precision for Emotional) and Negative 
Predictive Value (Precision for Non-Emotional)  
4 Experiments 
For the following experiments, we fixed the size 
of our training set to 175 examples (50%), and the 
size of our test set to 140 examples (40%).  The 
remaining 10% has been saved for later 
experiments. 
4.1 Selecting the features 
The first task was to reduce the dimensionality 
and find the best set of features for maximizing the 
PPV for Emotional class and NPV for Non-
Emotional class.  We applied the Wrapper 
Approach with Forward Selection as described in 
section 3.2, using Na?ve Bayes to evaluate each 
subset of features. 
We have used 175 examples for the training set 
(used to select the best features) and 140 for the 
test set (used to measure the performance).  The 
training set is randomly divided into two sets in 
each iteration of the algorithm: One for training 
and the other for development (65% and 35% 
respectively).  We train the learners with the 
training set and we evaluate the performance to 
pick the best feature with the development set.   
Number of 
Features 
Na?ve 
Bayes 
AdaBoost-j48 
Decision Trees 
All Features 74.5 % 83.1 % 
3 best for PPV 92.9 % 92.9 % 
Table 1. Precision of Emotional with all features 
and 3 best features for PPV using Na?ve Bayes  
(used for Feature Selection) and AdaBoost-j48 
Decision Trees (used for Co-training) 
The selected features that gave the best PPV for 
Emotional Class are 2 lexical features and one 
acoustic-prosodic feature.  By using them we 
increased the precision of Na?ve Bayes from 74.5% 
(using all 449 features) to 92.9%, and of 
AdaBoost-j48 Decision Trees from 83.1% to 
92.9% (see Table 1). 
Number of 
Features 
Na?ve 
Bayes 
AdaBoost-j48 
Decision Trees 
All Features 74.2  % 90.7 % 
1 best for NPV 100.0  % 100.0 % 
Table 2. Precision of Non-Emotional with all 
features and best feature for NPV using Na?ve 
Bayes  (used for Feature Selection) and AdaBoost-
j48 Decision Trees (used for Co-training) 
For the Non-Emotional Class, we increased the 
NPV of Na?ve Bayes from 74.2% (with all 
features) to 100% just by using one lexical feature, 
and the NPV of AdaBoost-j48 Decision Trees from 
90.7% to 100%.  This precision remained the same 
with the set of 3 best features, one lexical and two 
non-acoustic prosodic features (see Table 2). 
These two set of features for each learner are 
disjoint. 
4.2 Co-training experiments 
The two learners are initialized with only 6 
labeled examples in the training set.  The Co-
training system added examples from the 140 
?pseudo-labeled? examples1 in the Prediction Set. 
The size of the training set increased in each 
iteration by adding the 2 best examples (those with 
the highest confidence scores) labeled by the two 
learners. The Emotional learner and the Non-
Emotional learner were set to work with the set of 
features selected by the wrapper approach to 
optimize the precision (PPV and NPV) as 
described in section 4.1. 
We have applied Weka?s (Witten and Frank, 
2000) AdaBoost?s version of j48 decision trees (as 
used in Forbes-Riley and Litman, 2004) to the 140 
unseen examples of the test set for generating the 
learning curve shown in figure 4.   
Figure 4 illustrates the learning curve of the 
accuracy on the test set, taking the union of the set 
of features selected to label the examples.  We 
used the 3 best features for PPV for the Emotional 
Learner and the best feature for NPV for the Non-
Emotional Learner (see Section 4.1).  The x-axis 
shows the number of training examples added; the 
y-axis shows the accuracy of the classifier on test 
instances.  We compare the learning curve from 
Co-training with a baseline of majority class and 
an upper-bound, in which the classifiers are trained 
on human-annotated data.  Post-hoc analyses 
reveal that four incorrectly labeled examples were 
added to the training set: example numbers 21, 22, 
45, and 51 (see the x-axis).  Shortly after the 
inclusion of example 21, the Co-training learning 
curve diverges from the upper-bound.  All of them 
correspond to Non-Emotional examples that were 
labeled as Emotional by the Emotional learner with 
the highest confidence. 
The Co-training system stopped after adding 58 
examples to the initial 6 in the training set because 
the remaining data cannot be labeled by the 
learners with high precision.  However, as we can 
see, the training set generated by the Co-training 
technique can perform almost as well as the upper-
bound, even if incorrectly labeled examples are 
included in the training set. 
                                                     
1
 This means that although the example has been 
labeled, the label remains unseen to the learners. 
Learning Curve - Accuracy (features for Emotional/Non-Emotional Precision)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97 103 109 115 121 127 133 139 145 151 157 163 169 175
Majority Class Cotrain Upper-bound
 
Figure 4. Learning Curve of Accuracy using best features for Precision of Emotional/Non-Emotional 
5 Conclusion 
We have shown Co-training to be a promising 
approach for predicting emotions with spoken 
dialogue data. We have given an algorithm that 
increased the size of the training set producing 
even better accuracy than the manually labeled 
training set, until it fell behind due to its inability 
to add more than 58 examples. 
We have shown the positive effect of selecting 
a good set of features optimizing precision for 
each learner and we have shown that the features 
can be identified with the Wrapper Approach.     
In the future, we will verify the generalization 
of our results to other partitions of our data.  We 
will also try to address the limitation of noise in 
our Co-training System, and generalize our 
solution to a corresponding corpus of human-
computer data (Litman and Forbes-Riley, 2004).  
We will also conduct experiments comparing Co-
training with other semi-supervised approaches 
such as self-training and Active learning.  
6 Acknowledgements 
Thanks to R. Pelikan, T. Singliar and M. 
Hauskrecht for their contribution with Feature 
Selection, and to the NLP group at University of 
Pittsburgh for their helpful comments. This 
research is partially supported by NSF Grant No. 
0328431. 
References  
A. Blum and T. Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-training.  
Proceedings of the 11th Annual Conference on 
Computational Learning Theory: 92-100. 
K. Forbes-Riley and D. Litman. 2004.  Predicting 
Emotion in Spoken Dialogue from Multiple 
Knowledge Sources. Proceedings of Human 
Language Technology Conference of the North 
American Chapter of the Association for 
Computational Linguistics (HLT/NAACL).  
S. Goldman and Y. Zhou. 2000.  Enhancing 
Supervised Learning with Unlabeled Data. 
International Joint Conference on Machine 
Learning, 2000. 
G. H. John, R. Kohavi and K. Pleger. 1994.  
Irrelevant Features and the Subset Selection 
Problem. Machine Learning: Proceedings of 
11th International Conference:121-129, Morgan 
Kaufmann Publishers, San Francisco, CA. 
R. Kohavi and G. H. John. 1997. Wrappers for 
Feature Subset Selection. Artificial 
Intelligence, Volume 97, Issue 1-2. 
D. J. Litman and K. Forbes-Riley, 2004. 
Annotating Student Emotional States in Spoken 
Tutoring Dialogues.  Proc. 5th Special Interest 
Group on Discourse and Dialogue Workshop 
on Discourse and Dialogue (SIGdial). 
D. J. Litman and S. Silliman, 2004. ITSPOKE: An 
Intelligent Tutoring Spoken Dialogue System. 
Companion Proceedings of Human Language 
Technology conf. of the North American 
Chapter of the Association for Computational 
Linguistics (HLT/NAACL). 
I. H. Witten and E. Frank. 2000.  Data Mining: 
Practical Machine Learning Tools and 
Techniques with Java implementations. Morgan 
Kaufmann, San Francisco. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 193?200,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Dependencies between Student State and Speech Recognition  
Problems in Spoken Tutoring Dialogues 
 
 
Mihai Rotaru 
University of Pittsburgh 
Pittsburgh, USA 
mrotaru@cs.pitt.edu 
Diane J. Litman 
University of Pittsburgh 
Pittsburgh, USA 
litman@cs.pitt.edu 
 
  
 
Abstract 
Speech recognition problems are a reality 
in current spoken dialogue systems. In 
order to better understand these phenom-
ena, we study dependencies between 
speech recognition problems and several 
higher level dialogue factors that define 
our notion of student state: frustra-
tion/anger, certainty and correctness. We 
apply Chi Square (?2) analysis to a cor-
pus of speech-based computer tutoring 
dialogues to discover these dependencies 
both within and across turns. Significant 
dependencies are combined to produce 
interesting insights regarding speech rec-
ognition problems and to propose new 
strategies for handling these problems. 
We also find that tutoring, as a new do-
main for speech applications, exhibits in-
teresting tradeoffs and new factors to 
consider for spoken dialogue design. 
1 Introduction 
Designing a spoken dialogue system involves 
many non-trivial decisions. One factor that the 
designer has to take into account is the presence 
of speech recognition problems (SRP). Previous 
work (Walker et al, 2000) has shown that the 
number of SRP is negatively correlated with 
overall user satisfaction. Given the negative im-
pact of SRP, there has been a lot of work in try-
ing to understand this phenomenon and its impli-
cations for building dialogue systems. Most of 
the previous work has focused on lower level 
details of SRP: identifying components responsi-
ble for SRP (acoustic model, language model, 
search algorithm (Chase, 1997)) or prosodic 
characterization of SRP (Hirschberg et al, 2004). 
We extend previous work by analyzing the re-
lationship between SRP and higher level dia-
logue factors. Recent work has shown that dia-
logue design can benefit from several higher 
level dialogue factors: dialogue acts (Frampton 
and Lemon, 2005; Walker et al, 2001), prag-
matic plausibility (Gabsdil and Lemon, 2004). 
Also, it is widely believed that user emotions, as 
another example of higher level factor, interact 
with SRP but, currently, there is little hard evi-
dence to support this intuition. We perform our 
analysis on three high level dialogue factors: 
frustration/anger, certainty and correctness. Frus-
tration and anger have been observed as the most 
frequent emotional class in many dialogue sys-
tems (Ang et al, 2002) and are associated with a 
higher word error rate (Bulyko et al, 2005). For 
this reason, we use the presence of emotions like 
frustration and anger as our first dialogue factor. 
Our other two factors are inspired by another 
contribution of our study: looking at speech-
based computer tutoring dialogues instead of 
more commonly used information retrieval dia-
logues. Implementing spoken dialogue systems 
in a new domain has shown that many practices 
do not port well to the new domain (e.g. confir-
mation of long prompts (Kearns et al, 2002)). 
Tutoring, as a new domain for speech applica-
tions (Litman and Forbes-Riley, 2004; Pon-Barry 
et al, 2004), brings forward new factors that can 
be important for spoken dialogue design. Here 
we focus on certainty and correctness. Both fac-
tors have been shown to play an important role in 
the tutoring process (Forbes-Riley and Litman, 
2005; Liscombe et al, 2005). 
A common practice in previous work on emo-
tion prediction (Ang et al, 2002; Litman and 
Forbes-Riley, 2004) is to transform an initial 
finer level emotion annotation (five or more la-
bels) into a coarser level annotation (2-3 labels). 
We wanted to understand if this practice can im-
193
pact the dependencies we observe from the data. 
To test this, we combine our two emotion1 fac-
tors (frustration/anger and certainty) into a binary 
emotional/non-emotional annotation. 
To understand the relationship between SRP 
and our three factors, we take a three-step ap-
proach. In the first step, dependencies between 
SRP and our three factors are discovered using 
the Chi Square (?2) test. Similar analyses on hu-
man-human dialogues have yielded interesting 
insights about human-human conversations 
(Forbes-Riley and Litman, 2005; Skantze, 2005). 
In the second step, significant dependencies are 
combined to produce interesting insights regard-
ing SRP and to propose strategies for handling 
SRP. Validating these strategies is the purpose of 
the third step. In this paper, we focus on the first 
two steps; the third step is left as future work.  
Our analysis produces several interesting in-
sights and strategies which confirm the utility of 
the proposed approach. With respect to insights, 
we show that user emotions interact with SRP. 
We also find that incorrect/uncertain student 
turns have more SRP than expected. In addition, 
we find that the emotion annotation level affects 
the interactions we observe from the data, with 
finer-level emotions yielding more interactions 
and insights. 
In terms of strategies, our data suggests that 
favoring misrecognitions over rejections (by 
lowering the rejection threshold) might be more 
beneficial for our tutoring task ? at least in terms 
of reducing the number of emotional student 
turns. Also, as a general design practice in the 
spoken tutoring applications, we find an interest-
ing tradeoff between the pedagogical value of 
asking difficult questions and the system?s ability 
to recognize the student answer. 
2 Corpus 
The corpus analyzed in this paper consists of 95 
experimentally obtained spoken tutoring dia-
logues between 20 students and our system 
ITSPOKE (Litman and Forbes-Riley, 2004), a 
speech-enabled version of the text-based WHY2 
conceptual physics tutoring system (VanLehn et 
al., 2002). When interacting with ITSPOKE, stu-
dents first type an essay answering a qualitative 
physics problem using a graphical user interface. 
ITSPOKE then engages the student in spoken dia-
logue (using speech-based input and output) to 
correct misconceptions and elicit more complete 
                                                 
1 We use the term ?emotion? loosely to cover both affects 
and attitudes that can impact student learning. 
explanations, after which the student revises the 
essay, thereby ending the tutoring or causing an-
other round of tutoring/essay revision. For rec-
ognition, we use the Sphinx2 speech recognizer 
with stochastic language models. Because speech 
recognition is imperfect, after the data was col-
lected, each student utterance in our corpus was 
manually transcribed by a project staff member. 
An annotated excerpt from our corpus is shown 
in Figure 1 (punctuation added for clarity). The 
excerpts show both what the student said (the 
STD labels) and what ITSPOKE recognized (the 
ASR labels). The excerpt is also annotated with 
concepts that will be described next. 
2.1 Speech Recognition Problems (SRP) 
One form of SRP is the Rejection. Rejections 
occur when ITSPOKE is not confident enough in 
the recognition hypothesis and asks the student 
to repeat (Figure 1, STD3,4). For our ?2 analysis, 
we define the REJ variable with two values: Rej 
(a rejection occurred in the turn) and noRej (no 
rejection occurred in the turn). Not surprisingly, 
ITSPOKE also misrecognized some student turns. 
When ITSPOKE heard something different than 
what the student actually said but was confident 
in its hypothesis, we call this an ASR Misrecog-
nition (a binary version of the commonly used 
Word Error Rate) (Figure 1, STD1,2). Similarly, 
we define the ASR MIS variable with two val-
ues: AsrMis and noAsrMis. 
Semantic accuracy is more relevant for dia-
logue evaluation, as it does not penalize for word 
errors that are unimportant to overall utterance 
interpretation. In the case of form-based informa-
tion access spoken dialogue systems, computing 
semantic accuracy is straightforward (i.e. con-
cept accuracy = percentage of correctly recog-
nized concepts). In contrast, in the tutoring do-
main there are no clear forms with slots to be 
filled. We base our semantic accuracy on the 
?correctness? measure of the student turn. For 
each student turn, ITSPOKE interprets it and la-
bels its correctness with regard to whether the 
student correctly answered the tutor question (see 
the labels between square brackets in Figure 1). 
We define Semantic Misrecognition as cases 
where ITSPOKE was confident in its recognition 
hypothesis and the correctness interpretation of 
the recognition hypothesis is different from the 
correctness interpretation of the manual tran-
script (Figure 1, STD1). Similarly, we define the 
SEM MIS variable with two values: SemMis 
and noSemMis. The top part of Table 1 lists the 
distribution for our three SRP variables. 
194
3 Student State Annotation 
Our definition of student state includes annota-
tion on three dimensions (our three dialogue fac-
tors): Frustration/Anger/Hyperarticulation, Cer-
tainty and Correctness (see Table 1). The first 
two dimensions capture the student emotions and 
the last one the student correctness. 
ITSPOKE: Good. Apply the same force to an object of 
mass M3, which is three times the mass of M1. 
What is the magnitude M3's acceleration in terms of 
A1 [acceleration of the object M1]? 
STD1: third of a one (CERT=Uncertain, CRCT=C) 
[correct] 
ASR1: they're it but a one (AsrMis, SemMis) [incorrect] 
ITSPOKE: The acceleration of M3 [...] would be one-
third the acceleration of M1. [...] In general for the 
same force what remains constant? 
STD2: weight (CRCT=I) [incorrect] 
ASR2: great (AsrMis) [incorrect] 
ITSPOKE: Well... The product of the mass of a body 
and its acceleration remains constant for a given 
force. What is the relationship between a force act-
ing on the box, the mass of the box, and its accel-
eration? 
STD3: newton's second law (CERT=Uncertain, 
CRCT=PC) 
ASR3: the cancel the fire (Rej) 
ITSPOKE: Could you please repeat that? 
STD4: newton's second law (FAH=FrAng, CRCT=PC) 
ASR4: newton second long (Rej) 
Figure 1: Human-Computer Dialogue Excerpt 
The Frustration/Anger/Hyperarticulation 
dimension captures the perceived negative stu-
dent emotional response to the interaction with 
the system. Three labels were used to annotate 
this dimension: frustration-anger, hyperarticula-
tion and neutral. Similar to (Ang et al, 2002), 
because frustration and anger can be difficult to 
distinguish reliably, they were collapsed into a 
single label: frustration-anger (Figure 1, STD4). 
Often, frustration and anger is prosodically 
marked and in many cases the prosody used is 
consistent with hyperarticulation (Ang et al, 
2002). For this reason we included in this dimen-
sion the hyperarticulation label (even though hy-
perarticulation is not an emotion but a state). We 
used the hyperarticulation label for turns where 
no frustration or anger was perceived but never-
theless were hyperarticulated. For our interaction 
experiments we define the FAH variable with 
three values: FrAng (frustration-anger), Hyp 
(hyperarticulation) and Neutral. 
The Certainty dimension captures the per-
ceived student reaction to the questions asked by 
our computer tutor and her overall reaction to the 
tutoring domain (Liscombe et al, 2005). 
(Forbes-Riley and Litman, 2005) show that stu-
dent certainty interacts with a human tutor?s dia-
logue decision process (i.e. the choice of feed-
back). Four labels were used for this dimension: 
certain, uncertain (e.g. Figure 1, STD1), mixed 
and neutral. In a small number of turns, both cer-
tainty and uncertainty were expressed and these 
turns were labeled as mixed (e.g. the student was 
certain about a concept, but uncertain about an-
other concept needed to answer the tutor?s ques-
tion). For our interaction experiments we define 
the CERT variable with four values: Certain, 
Uncertain, Mixed and Neutral. 
 Vari-able Values 
Student turns 
(2334) 
Speech recognition problems 
 ASR MIS 
AsrMis 
noAsrMis 
25.4% 
74.6% 
 SEM MIS 
SemMis 
noSemMis 
5.7% 
94.3% 
 REJ Rej noRej 
7.0% 
93.0% 
Student state 
 FAH 
FrAng 
Hyp 
Neutral 
9.9% 
2.1% 
88.0% 
 CERT 
Certain 
Uncertain 
Mixed 
Neutral 
41.3% 
19.1% 
2.4% 
37.3% 
 CRCT 
C 
I 
PC 
UA 
63.3% 
23.3% 
6.2% 
7.1% 
 EnE Emotional Neutral 
64.8% 
35.2% 
Table 1: Variable distributions in our corpus. 
To test the impact of the emotion annotation 
level, we define the Emotional/Non-Emotional 
annotation based on our two emotional dimen-
sions: neutral turns on both the FAH and the 
CERT dimension are labeled as neutral2; all other 
turns were labeled as emotional. Consequently, 
we define the EnE variable with two values: 
Emotional and Neutral. 
Correctness is also an important factor of the 
student state. In addition to the correctness labels 
assigned by ITSPOKE (recall the definition of 
SEM MIS), each student turn was manually an-
notated by a project staff member in terms of 
their physics-related correctness. Our annotator 
used the human transcripts and his physics 
knowledge to label each student turn for various 
                                                 
2 To be consistent with our previous work, we label hyperar-
ticulated turns as emotional even though hyperarticulation is 
not an emotion. 
195
degrees of correctness: correct, partially correct, 
incorrect and unable to answer. Our system can 
ask the student to provide multiple pieces of in-
formation in her answer (e.g. the question ?Try 
to name the forces acting on the packet. Please, 
specify their directions.? asks for both the names 
of the forces and their direction). If the student 
answer is correct and contains all pieces of in-
formation, it was labeled as correct (e.g. ?grav-
ity, down?). The partially correct label was used 
for turns where part of the answer was correct 
but the rest was either incorrect (e.g. ?gravity, 
up?) or omitted some information from the ideal 
correct answer (e.g. ?gravity?). Turns that were 
completely incorrect (e.g. ?no forces?) were la-
beled as incorrect. Turns where the students did 
not answer the computer tutor?s question were 
labeled as ?unable to answer?. In these turns the 
student used either variants of ?I don?t know? or 
simply did not say anything. For our interaction 
experiments we defined the CRCT variable with 
four values: C (correct), I (incorrect), PC (par-
tially correct) and UA (unable to answer). 
Please note that our definition of student state 
is from the tutor?s perspective. As we mentioned 
before, our emotion annotation is for perceived 
emotions. Similarly, the notion of correctness is 
from the tutor?s perspective. For example, the 
student might think she is correct but, in reality, 
her answer is incorrect. This correctness should 
be contrasted with the correctness used to define 
SEM MIS. The SEM MIS correctness uses 
ITSPOKE?s language understanding module ap-
plied to recognition hypothesis or the manual 
transcript, while the student state?s correctness 
uses our annotator?s language understanding. 
All our student state annotations are at the turn 
level and were performed manually by the same 
annotator. While an inter-annotator agreement 
study is the best way to test the reliability of our 
two emotional annotations (FAH and CERT), 
our experience with annotating student emotions 
(Litman and Forbes-Riley, 2004) has shown that 
this type of annotation can be performed reliably. 
Given the general importance of the student?s 
uncertainty for tutoring, a second annotator has 
been commissioned to annotate our corpus for 
the presence or absence of uncertainty. This an-
notation can be directly compared with a binary 
version of CERT: Uncertain+Mixed versus Cer-
tain+Neutral. The comparison yields an agree-
ment of 90% with a Kappa of 0.68. Moreover, if 
we rerun our study on the second annotation, we 
find similar dependencies. We are currently 
planning to perform a second annotation of the 
FAH dimension to validate its reliability. 
We believe that our correctness annotation 
(CRCT) is reliable due to the simplicity of the 
task: the annotator uses his language understand-
ing to match the human transcript to a list of cor-
rect/incorrect answers. When we compared this 
annotation with the correctness assigned by 
ITSPOKE on the human transcript, we found an 
agreement of 90% with a Kappa of 0.79. 
4 Identifying dependencies using ?2 
To discover the dependencies between our vari-
ables, we apply the ?2 test. We illustrate our 
analysis method on the interaction between cer-
tainty (CERT) and rejection (REJ). The ?2 value 
assesses whether the differences between ob-
served and expected counts are large enough to 
conclude a statistically significant dependency 
between the two variables (Table 2, last column). 
For Table 2, which has 3 degrees of freedom ((4-
1)*(2-1)), the critical ?2 value at a p<0.05 is 7.81. 
We thus conclude that there is a statistically sig-
nificant dependency between the student cer-
tainty in a turn and the rejection of that turn. 
Combination  Obs. Exp. ?2
CERT ? REJ    11.45 
Certain ? Rej - 49 67 9.13 
Uncertain ? Rej + 43 31 6.15 
Table 2: CERT ? REJ interaction. 
If any of the two variables involved in a sig-
nificant dependency has more than 2 possible 
values, we can look more deeply into this overall 
interaction by investigating how particular values 
interact with each other. To do that, we compute 
a binary variable for each variable?s value in part 
and study dependencies between these variables. 
For example, for the value ?Certain? of variable 
CERT we create a binary variable with two val-
ues: ?Certain? and ?Anything Else? (in this case 
Uncertain, Mixed and Neutral). By studying the 
dependency between binary variables we can 
understand how the interaction works. 
Table 2 reports in rows 3 and 4 all significant 
interactions between the values of variables 
CERT and REJ. Each row shows: 1) the value 
for each original variable, 2) the sign of the de-
pendency, 3) the observed counts, 4) the ex-
pected counts and 5) the ?2 value. For example, 
in our data there are 49 rejected turns in which 
the student was certain. This value is smaller 
than the expected counts (67); the dependency 
between Certain and Rej is significant with a ?2 
value of 9.13. A comparison of the observed 
counts and expected counts reveals the direction 
196
(sign) of the dependency. In our case we see that 
certain turns are rejected less than expected (row 
3), while uncertain turns are rejected more than 
expected (row 4). On the other hand, there is no 
interaction between neutral turns and rejections 
or between mixed turns and rejections. Thus, the 
CERT ? REJ interaction is explained only by the 
interaction between Certain and Rej and the in-
teraction between Uncertain and Rej. 
5 Results - dependencies 
In this section we present all significant depend-
encies between SRP and student state both 
within and across turns. Within turn interactions 
analyze the contribution of the student state to 
the recognition of the turn. They were motivated 
by the widely believed intuition that emotion 
interacts with SRP. Across turn interactions look 
at the contribution of previous SRP to the current 
student state. Our previous work (Rotaru and 
Litman, 2005) had shown that certain SRP will 
correlate with emotional responses from the user. 
We also study the impact of the emotion annota-
tion level (EnE versus FAH/CERT) on the inter-
actions we observe. The implications of these 
dependencies will be discussed in Section 6. 
5.1 Within turn interactions 
For the FAH dimension, we find only one sig-
nificant interaction: the interaction between the 
FAH student state and the rejection of the current 
turn (Table 3). By studying values? interactions, 
we find that turns where the student is frustrated 
or angry are rejected more than expected (34 in-
stead of 16; Figure 1, STD4 is one of them). 
Similarly, turns where the student response is 
hyperarticulated are also rejected more than ex-
pected (similar to observations in (Soltau and 
Waibel, 2000)). In contrast, neutral turns in the 
FAH dimension are rejected less than expected. 
Surprisingly, FrAng does not interact with 
AsrMis as observed in (Bulyko et al, 2005) but 
they use the full word error rate measure instead 
of the binary version used in this paper. 
Combination  Obs. Exp. ?2
FAH ? REJ    77.92 
FrAng ? Rej + 34 16 23.61 
Hyp ? Rej + 16 3 50.76 
Neutral ? Rej - 113 143 57.90 
Table 3: FAH ? REJ interaction. 
Next we investigate how our second emotion 
annotation, CERT, interacts with SRP. All sig-
nificant dependencies are reported in Tables 2 
and 4. In contrast with the FAH dimension, here 
we see that the interaction direction depends on 
the valence. We find that ?Certain? turns have 
less SRP than expected (in terms of AsrMis and 
Rej). In contrast, ?Uncertain? turns have more 
SRP both in terms of AsrMis and Rej. ?Mixed? 
turns interact only with AsrMis, allowing us to 
conclude that the presence of uncertainty in the 
student turn (partial or overall) will result in ASR 
problems more than expected. Interestingly, on 
this dimension, neutral turns do not interact with 
any of our three SRP. 
Combination  Obs. Exp. ?2
CERT ? ASRMIS    38.41 
Certain ? AsrMis - 204 244 15.32 
Uncertain ? AsrMis + 138 112 9.46 
Mixed ? AsrMis + 29 13 22.27 
Table 4: CERT ? ASRMIS interaction. 
Finally, we look at interactions between stu-
dent correctness and SRP. Here we find signifi-
cant dependencies with all types of SRP (see Ta-
ble 5). In general, correct student turns have 
fewer SRP while incorrect, partially correct or 
UA turns have more SRP than expected. Partially 
correct turns have more AsrMis and SemMis 
problems than expected, but are rejected less 
than expected. Interestingly, UA turns interact 
only with rejections: these turns are rejected 
more than expected. An analysis of our corpus 
reveals that in most rejected UA turns the student 
does not say anything; in these cases, the sys-
tem?s recognition module thought the student 
said something but the system correctly rejects 
the recognition hypothesis. 
Combination  Obs. Exp. ?2
CRCT ? ASRMIS    65.17 
C ? AsrMis - 295 374 62.03 
I ? AsrMis + 198 137 45.95 
PC ? AsrMis + 50 37 5.9 
CRCT ? SEMMIS    20.44 
C ? SemMis + 100 84 7.83 
I ? SemMis - 14 31 13.09 
PC ? SemMis + 15 8 5.62 
CRCT ? REJ    99.48 
C ? Rej - 53 102 70.14 
I ? Rej + 84 37 79.61 
PC ? Rej - 4 10 4.39 
UA ? Rej + 21 11 9.19 
Table 5: Interactions between Correctness and SRP. 
The only exception to the rule is SEM MIS. 
We believe that SEM MIS behavior is explained 
by the ?catch-all? implementation in our system. 
In ITSPOKE, for each tutor question there is a list 
of anticipated answers. All other answers are 
197
treated as incorrect. Thus, it is less likely that a 
recognition problem in an incorrect turn will af-
fect the correctness interpretation (e.g. Figure 1, 
STD2: very unlikely to misrecognize the incor-
rect ?weight? with the anticipated ?the product of 
mass and acceleration?). In contrast, in correct 
turns recognition problems are more likely to 
screw up the correctness interpretation (e.g. mis-
recognizing ?gravity down? as ?gravity sound?). 
5.2 Across turn interactions 
Next we look at the contribution of previous SRP 
? variable name or value followed by (-1) ? to the 
current student state. Please note that there are 
two factors involved here: the presence of the 
SRP and the SRP handling strategy. In 
ITSPOKE, whenever a student turn is rejected, 
unless this is the third rejection in a row, the stu-
dent is asked to repeat using variations of ?Could 
you please repeat that??. In all other cases, 
ITSPOKE makes use of the available informa-
tion ignoring any potential ASR errors. 
Combination  Obs. Exp. ?2
ASRMIS(-1) ? FAH    7.64 
AsrMis(-1) ? FrAng -t 46 58 3.73 
AsrMis(-1) ? Hyp -t 7 12 3.52 
AsrMis(-1) ? Neutral + 527 509 6.82 
REJ(-1) ? FAH    409.31
Rej(-1) ? FrAng + 36 16 28.95 
Rej(-1) ? Hyp + 38 3 369.03
Rej(-1) ? Neutral - 88 142 182.9 
REJ(-1) ? CRCT    57.68 
Rej(-1) ? C - 68 101 31.94 
Rej(-1) ? I + 74 37 49.71 
Rej(-1) ? PC - 3 10 6.25 
Table 6: Interactions across turns (t ? trend, p<0.1). 
Here we find only 3 interactions (Table 6). We 
find that after a non-harmful SRP (AsrMis) the 
student is less frustrated and hyperarticulated 
than expected. This result is not surprising since 
an AsrMis does not have any effect on the nor-
mal dialogue flow. 
In contrast, after rejections we observe several 
negative events. We find a highly significant in-
teraction between a previous rejection and the 
student FAH state, with student being more frus-
trated and more hyperarticulated than expected 
(e.g. Figure 1, STD4). Not only does the system 
elicit an emotional reaction from the student after 
a rejection, but her subsequent response to the 
repetition request suffers in terms of the correct-
ness. We find that after rejections student an-
swers are correct or partially correct less than 
expected and incorrect more than expected. The 
REJ(-1) ? CRCT interaction might be explained 
by the CRCT ? REJ interaction (Table 5) if, in 
general, after a rejection the student repeats her 
previous turn. An annotation of responses to re-
jections as in (Swerts et al, 2000) (repeat, re-
phrase etc.) should  provide additional insights.  
We were surprised to see that a previous 
SemMis (more harmful than an AsrMis but less 
disruptive than a Rej) does not interact with the 
student state; also the student certainty does not 
interact with previous SRP. 
5.3 Emotion annotation level 
We also study the impact of the emotion annota-
tion level on the interactions we can observe 
from our corpus. In this section, we look at inter-
actions between SRP and our coarse-level emo-
tion annotation (EnE) both within and across 
turns. Our results are similar with the results of 
our previous work (Rotaru and Litman, 2005) on 
a smaller corpus and a similar annotation 
scheme. We find again only one significant in-
teraction: rejections are followed by more emo-
tional turns than expected (Table 7). The strength 
of the interaction is smaller than in previous 
work, though the results can not be compared 
directly. No other dependencies are present. 
Combination  Obs. Exp. ?2
REJ(-1) ? EnE    6.19 
Rej(-1) ? Emotional + 119 104 6.19 
Table 7: REJ(-1) ? EnE interaction. 
We believe that the REJ(-1) ? EnE interaction is 
explained mainly by the FAH dimension. Not 
only is there no interaction between REJ(-1) and 
CERT, but the inclusion of the CERT dimension 
in the EnE annotation decreases the strength of 
the interaction between REJ and FAH (the ?2 
value decreases from 409.31 for FAH to a mere 
6.19 for EnE). Collapsing emotional classes also 
prevents us from seeing any within turn interac-
tions. These observations suggest that what is 
being counted as an emotion for a binary emo-
tion annotation is critical its success. In our case, 
if we look at affect (FAH) or attitude (CERT) in 
isolation we find many interactions; in contrast, 
combining them offers little insight.  
6 Results ? insights & strategies 
Our results put a spotlight on several interesting 
observations which we discuss below. 
Emotions interact with SRP 
The dependencies between FAH/CERT and 
various SRP (Tables 2-4) provide evidence that 
user?s emotions interact with the system?s ability 
198
to recognize the current turn. This is a widely 
believed intuition with little empirical support so 
far. Thus, our notion of student state can be a 
useful higher level information source for SRP 
predictors. Similar to (Hirschberg et al, 2004), 
we believe that peculiarities in the acous-
tic/prosodic profile of specific student states are 
responsible for their SRP. Indeed, previous work 
has shown that the acoustic/prosodic information 
plays an important role in characterizing and 
predicting both FAH (Ang et al, 2002; Soltau 
and Waibel, 2000) and CERT (Liscombe et al, 
2005; Swerts and Krahmer, 2005). 
The impact of the emotion annotation level 
A comparison of the interactions yielded by 
various levels of emotion annotation shows the 
importance of the annotation level. When using a 
coarser level annotation (EnE) we find only one 
interaction. By using a finer level annotation, not 
only we can understand this interaction better but 
we also discover new interactions (five interac-
tions with FAH and CERT). Moreover, various 
state annotations interact differently with SRP. 
For example, non-neutral turns in the FAH di-
mension (FrAng and Hyp) will be always re-
jected more than expected (Table 3); in contrast, 
interactions between non-neutral turns in the 
CERT dimension and rejections depend on the 
valence (?certain? turns will be rejected less than 
expected while ?uncertain? will be rejected more 
than expected; recall Table 2). We also see that 
the neutral turns interact with SRP depending on 
the dimension that defines them: FAH neutral 
turns interact with SRP (Table 3) while CERT 
neutral turns do not (Tables 2 and 4). 
This insight suggests an interesting tradeoff 
between the practicality of collapsing emotional 
classes (Ang et al, 2002; Litman and Forbes-
Riley, 2004) and the ability to observe meaning-
ful interactions via finer level annotations. 
Rejections: impact and a handling strategy 
Our results indicate that rejections and 
ITSPOKE?s current rejection-handling strategy 
are problematic. We find that rejections are fol-
lowed by more emotional turns (Table 7). A 
similar effect was observed in our previous work 
(Rotaru and Litman, 2005). The fact that it gen-
eralizes across annotation scheme and corpus, 
emphasizes its importance. When a finer level 
annotation is used, we find that rejections are 
followed more than expected by a frustrated, an-
gry and hyperarticulated user (Table 6). More-
over, these subsequent turns can result in addi-
tional rejections (Table 3). Asking to repeat after 
a rejection does not also help in terms of correct-
ness: the subsequent student answer is actually 
incorrect more than expected (Table 6). 
These interactions suggest an interesting strat-
egy for our tutoring task: favoring misrecogni-
tions over rejections (by lowering the rejection 
threshold). First, since rejected turns are more 
than expected incorrect (Table 5), the actual rec-
ognized hypothesis for such turns turn is very 
likely to be interpreted as incorrect. Thus, ac-
cepting a rejected turn instead of rejecting it will 
have the same outcome in terms of correctness: 
an incorrect answer. In this way, instead of at-
tempting to acquire the actual student answer by 
asking to repeat, the system can skip these extra 
turn(s) and use the current hypothesis. Second, 
the other two SRP are less taxing in terms of 
eliciting FAH emotions (recall Table 6; note that 
a SemMis might activate an unwarranted and 
lengthy knowledge remediation subdialogue). 
This suggests that continuing the conversation 
will be more beneficial even if the system mis-
understood the student. A similar behavior was 
observed in human-human conversations through 
a noisy speech channel (Skantze, 2005). 
Correctness/certainty?SRP interactions 
We also find an interesting interaction between 
correctness/certainty and system?s ability to rec-
ognize that turn. In general correct/certain turns 
have less SRP while incorrect/uncertain turns 
have more SRP than expected. This observation 
suggests that the computer tutor should ask the 
right question (in terms of its difficulty) at the 
right time. Intuitively, asking a more complicated 
question when the student is not prepared to an-
swer it will increase the likelihood of an incor-
rect or uncertain answer. But our observations 
show that the computer tutor has more trouble 
recognizing correctly these types of answers. 
This suggests an interesting tradeoff between the 
tutor?s question difficulty and the system?s abil-
ity to recognize the student answer. This tradeoff 
is similar in spirit to the initiative-SRP tradeoff 
that is well known when designing information-
seeking systems (e.g. system initiative is often 
used instead of a more natural mixed initiative 
strategy, in order to minimize SRP). 
7 Conclusions 
In this paper we analyze the interactions between 
SRP and three higher level dialogue factors that 
define our notion of student state: frustra-
tion/anger/hyperarticulation, certainty and cor-
rectness. Our analysis produces several interest-
ing insights and strategies which confirm the 
199
utility of the proposed approach. We show that 
user emotions interact with SRP and that the 
emotion annotation level affects the interactions 
we observe from the data, with finer-level emo-
tions yielding more interactions and insights. 
We also find that tutoring, as a new domain 
for speech applications, brings forward new im-
portant factors for spoken dialogue design: cer-
tainty and correctness. Both factors interact with 
SRP and these interactions highlight an interest-
ing design practice in the spoken tutoring appli-
cations: the tradeoff between the pedagogical 
value of asking difficult questions and the sys-
tem?s ability to recognize the student answer (at 
least in our system). The particularities of the 
tutoring domain also suggest favoring misrecog-
nitions over rejections to reduce the negative im-
pact of asking to repeat after rejections. 
In our future work, we plan to move to the 
third step of our approach: testing the strategies 
suggested by our results. For example, we will 
implement a new version of ITSPOKE that never 
rejects the student turn. Next, the current version 
and the new version will be compared with re-
spect to users? emotional response. Similarly, to 
test the tradeoff hypothesis, we will implement a 
version of ITSPOKE that asks difficult questions 
first and then falls back to simpler questions. A 
comparison of the two versions in terms of the 
number of SRP can be used for validation. 
While our results might be dependent on the 
tutoring system used in this experiment, we be-
lieve that our findings can be of interest to practi-
tioners building similar voice-based applications. 
Moreover, our approach can be applied easily to 
studying other systems. 
Acknowledgements 
This work is supported by NSF Grant No. 
0328431. We thank Dan Bohus, Kate Forbes-
Riley, Joel Tetreault and our anonymous review-
ers for their helpful comments. 
References 
J. Ang, R. Dhillon, A. Krupski, A. Shriberg and A. 
Stolcke. 2002. Prosody-based automatic detection 
of annoyance and frustration in human-computer 
dialog. In Proc. of ICSLP. 
I. Bulyko, K. Kirchhoff, M. Ostendorf and J. Gold-
berg. 2005. Error-correction detection and response 
generation in a spoken dialogue system. Speech 
Communication, 45(3). 
L. Chase. 1997. Blame Assignment for Errors Made 
by Large Vocabulary Speech Recognizers. In Proc. 
of Eurospeech. 
K. Forbes-Riley and D. J. Litman. 2005. Using Bi-
grams to Identify Relationships Between Student 
Certainness States and Tutor Responses in a Spo-
ken Dialogue Corpus. In Proc. of SIGdial. 
M. Frampton and O. Lemon. 2005. Reinforcement 
Learning of Dialogue Strategies using the User's 
Last Dialogue Act. In Proc. of IJCAI Workshop on 
Know.&Reasoning in Practical Dialogue Systems. 
M. Gabsdil and O. Lemon. 2004. Combining Acoustic 
and Pragmatic Features to Predict Recognition 
Performance in Spoken Dialogue Systems. In Proc. 
of ACL. 
J. Hirschberg, D. Litman and M. Swerts. 2004. Pro-
sodic and Other Cues to Speech Recognition Fail-
ures. Speech Communication, 43(1-2). 
M. Kearns, C. Isbell, S. Singh, D. Litman and J. 
Howe. 2002. CobotDS: A Spoken Dialogue System 
for Chat. In Proc. of National Conference on Arti-
ficial Intelligence (AAAI). 
J. Liscombe, J. Hirschberg and J. J. Venditti. 2005. 
Detecting Certainness in Spoken Tutorial Dia-
logues. In Proc. of Interspeech. 
D. Litman and K. Forbes-Riley. 2004. Annotating 
Student Emotional States in Spoken Tutoring Dia-
logues. In Proc. of SIGdial Workshop on Discourse 
and Dialogue (SIGdial). 
H. Pon-Barry, B. Clark, E. O. Bratt, K. Schultz and S. 
Peters. 2004. Evaluating the effectiveness of Scot:a 
spoken conversational tutor. In Proc. of ITS Work-
shop on Dialogue-based Intellig. Tutoring Systems. 
M. Rotaru and D. Litman. 2005. Interactions between 
Speech Recognition Problems and User Emotions. 
In Proc. of Eurospeech. 
G. Skantze. 2005. Exploring human error recovery 
strategies: Implications for spoken dialogue sys-
tems. Speech Communication, 45(3). 
H. Soltau and A. Waibel. 2000. Specialized acoustic 
models for hyperarticulated speech. In Proc. of 
ICASSP. 
M. Swerts and E. Krahmer. 2005. Audiovisual Pros-
ody and Feeling of Knowing. Journal of Memory 
and Language, 53. 
M. Swerts, D. Litman and J. Hirschberg. 2000. Cor-
rections in Spoken Dialogue Systems. In Proc. of 
ICSLP. 
K. VanLehn, P. W. Jordan, C. P. Ros?, et al 2002. 
The Architecture of Why2-Atlas: A Coach for 
Qualitative Physics Essay Writing. In Proc. of In-
telligent Tutoring Systems (ITS). 
M. Walker, D. Litman, C. Kamm and A. Abella. 
2000. Towards Developing General Models of Us-
ability with PARADISE. Natural Language Engi-
neering. 
M. Walker, R. Passonneau and J. Boland. 2001. 
Quantitative and Qualitative Evaluation of Darpa 
Communicator Spoken Dialogue Systems. In Proc. 
of ACL. 
200
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 360?367,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Utility of a Graphical Representation of Discourse Structure in 
Spoken Dialogue Systems 
Mihai Rotaru 
University of Pittsburgh 
Pittsburgh, USA 
mrotaru@cs.pitt.edu 
Diane J. Litman 
University of Pittsburgh 
Pittsburgh, USA 
litman@cs.pitt.edu 
 
 
Abstract 
In this paper we explore the utility of the 
Navigation Map (NM), a graphical repre-
sentation of the discourse structure. We run 
a user study to investigate if users perceive 
the NM as helpful in a tutoring spoken dia-
logue system. From the users? perspective, 
our results show that the NM presence al-
lows them to better identify and follow the 
tutoring plan and to better integrate the in-
struction. It was also easier for users to 
concentrate and to learn from the system if 
the NM was present. Our preliminary 
analysis on objective metrics further 
strengthens these findings. 
1 Introduction 
With recent advances in spoken dialogue system 
technologies, researchers have turned their atten-
tion to more complex domains (e.g. tutoring 
(Litman and Silliman, 2004; Pon-Barry et al, 
2006), technical support (Acomb et al, 2007), 
medication assistance (Allen et al, 2006)). These 
domains bring forward new challenges and issues 
that can affect the usability of such systems: in-
creased task complexity, user?s lack of or limited 
task knowledge, and longer system turns. 
In typical information access dialogue systems, 
the task is relatively simple: get the information 
from the user and return the query results with 
minimal complexity added by confirmation dia-
logues. Moreover, in most cases, users have 
knowledge about the task. However, in complex 
domains things are different. Take for example 
tutoring. A tutoring dialogue system has to discuss 
concepts, laws and relationships and to engage in 
complex subdialogues to correct user misconcep-
tions. In addition, it is very likely that users of such 
systems are not familiar or are only partially famil-
iar with the tutoring topic. The length of system 
turns can also be affected as these systems need to 
make explicit the connections between parts of the 
underlying task. 
Thus, interacting with such systems can be char-
acterized by an increased user cognitive load asso-
ciated with listening to often lengthy system turns 
and the need to integrate the current information to 
the discussion overall (Oviatt et al, 2004). 
We hypothesize that one way to reduce the 
user?s cognitive load is to make explicit two pieces 
of information: the purpose of the current system 
turn, and how the system turn relates to the overall 
discussion. This information is implicitly encoded 
in the intentional structure of a discourse as pro-
posed in the Grosz & Sidner theory of discourse 
(Grosz and Sidner, 1986). 
Consequently, in this paper we propose using a 
graphical representation of the discourse structure 
as a way of improving the performance of com-
plex-domain dialogue systems (note that graphical 
output is required). We call it the Navigation Map 
(NM). The NM is a dynamic representation of the 
discourse segment hierarchy and the discourse seg-
ment purpose information enriched with several 
features (Section 3). To make a parallel with geog-
raphy, as the system ?navigates? with the user 
through the domain, the NM offers a cartographic 
view of the discussion. While a somewhat similar 
graphical representation of the discourse structure 
has been explored in one previous study (Rich and 
Sidner, 1998), to our knowledge we are the first to 
test its benefits (see Section 6). 
360
As a first step towards understanding the NM ef-
fects, here we focus on investigating whether users 
prefer a system with the NM over a system without 
the NM and, if yes, what are the NM usage pat-
terns. We test this in a speech based computer tutor 
(Section 2). We run a within-subjects user study in 
which users interacted with the system both with 
and without the NM (Section 4). 
Our analysis of the users? subjective evaluation 
of the system indicates that users prefer the version 
of the system with the NM over the version with-
out the NM on several dimensions. The NM pres-
ence allows the users to better identify and follow 
the tutoring plan and to better integrate the instruc-
tion. It was also easier for users to concentrate and 
to learn from the system if the NM was present. 
Our preliminary analysis on objective metrics fur-
ther strengthens these findings. 
2 ITSPOKE 
ITSPOKE (Litman and Silliman, 2004) is a state-
of-the-art tutoring spoken dialogue system for con-
ceptual physics. When interacting with ITSPOKE, 
users first type an essay answering a qualitative 
physics problem using a graphical user interface. 
ITSPOKE then engages the user in spoken dialogue 
(using head-mounted microphone input and speech 
output) to correct misconceptions and elicit more 
complete explanations, after which the user revises 
the essay, thereby ending the tutoring or causing 
another round of tutoring/essay revision. 
All dialogues with ITSPOKE follow a question-
answer format (i.e. system initiative): ITSPOKE 
asks a question, users answer and then the process 
is repeated. Deciding what question to ask, in what 
order and when to stop is hand-authored before-
hand in a hierarchical structure. Internally, system 
questions are grouped in question segments. 
In Figure 1, we show the transcript of a sample 
interaction with ITSPOKE. The system is discussing 
the problem listed in the upper right corner of the 
figure and it is currently asking the question Tu-
tor5. The left side of the figure shows the interac-
tion transcript (not available to the user at run-
time). The right side of the figure shows the NM 
which will be discussed in the next section. 
Our system behaves as follows. First, based on 
the analysis of the user essay, it selects a question 
segment to correct misconceptions or to elicit more 
complete explanations. Next the system asks every 
question from this question segment. If the user 
answer is correct, the system simply moves on to 
the next question (e.g. Tutor2?Tutor3). For incor-
rect answers there are two alternatives. For simple 
questions, the system will give out the correct an-
swer accompanied by a short explanation and 
move on to the next question (e.g. Tutor1?Tutor2). 
For complex questions (e.g. applying physics 
laws), ITSPOKE will engage into a remediation 
subdialogue that attempts to remediate user?s lack 
of knowledge or skills (e.g. Tutor4?Tutor5). The 
remediation subdialogue for each complex ques-
tion is specified in another question segment. 
Our system exhibits some of the issues we 
linked in Section 1 with complex-domain systems. 
Dialogues with our system can be long and com-
plex (e.g. the question segment hierarchical struc-
ture can reach level 6) and sometimes the system?s 
turn can be quite long (e.g. Tutor2). User?s reduced 
knowledge of the task is also inherent in tutoring.  
3 The Navigation Map (NM) 
We use the Grosz & Sidner theory of discourse 
(Grosz and Sidner, 1986) to inform our NM de-
sign. According to this theory, each discourse has a 
discourse purpose/intention. Satisfying the main 
discourse purpose is achieved by satisfying several 
smaller purposes/intentions organized in a hierar-
chical structure. As a result, the discourse is seg-
mented into discourse segments each with an asso-
ciated discourse segment purpose/intention. This 
theory has inspired several generic dialogue man-
agers for spoken dialogue systems (e.g. (Rich and 
Sidner, 1998)). 
The NM requires that we have the discourse 
structure information at runtime. To do that, we 
manually annotate the system?s internal representa-
tion of the tutoring task with discourse segment 
purpose and hierarchy information. Based on this 
annotation, we can easily construct the discourse 
structure at runtime. In this section we describe our 
annotation and the NM design choices we made. 
Figure 1 shows the state of the NM after turn Tu-
tor5 as the user sees it on the interface (NM line 
numbering is for exposition only). Note that Figure 
1 is not a screenshot of the actual system interface. 
The NM is the only part from the actual system 
interface. Figure 2 shows the NM after turn Tutor1. 
We manually annotated each system ques-
tion/explanation for its intention(s)/purpose(s). 
Note that some system turns have multiple inten-
361
tions/purposes thus multiple discourse segments 
were created for them. For example, in Tutor1 the 
system first identifies the time frames on which the 
analysis will be performed (Figure 1&2, NM2). 
Next, the system indicates that it will discuss about 
the first time frame (Figure 1&2, NM3) and then it 
asks the actual question (Figure 2, NM4).  
Thus, in addition to our manual annotation of 
the discourse segment purpose information, we 
manually organized all discourse segments from a 
question segment in a hierarchical structure that 
reflects the discourse structure. 
At runtime, while discussing a question seg-
ment, the system has only to follow the annotated 
hierarchy, displaying and highlighting the dis-
course segment purposes associated with the ut-
tered content. For example, while uttering Tutor1, 
the NM will synchronously highlight NM2, NM3 
and NM4. Remediation question segments (e.g. 
NM12) or explanations (e.g. NM5) activated by in-
correct answers are attached to the structure under 
the corresponding discourse segment. 
3.1 NM Design Choices 
In our graphical representation of the discourse 
structure, we used a left to right indented layout. In 
addition, we made several design choices to enrich 
the NM information content and usability. 
Figure 2. NM state after turn Tutor1 
Correct answers. In Figure 2 we show the state 
of the NM after uttering Tutor1. The current dis-
course segment purpose (NM4) indicates that the 
system is asking about the relationship between the 
two velocities. While we could have kept the same 
information after the system was done with this 
discourse segment, we thought that users will 
benefit from having the correct answer on the 
screen (recall NM4 in Figure 1). Thus, the NM was 
enhanced to display the correct answer after the 
system is done with each question. We extracted 
the correct answer from the system specifications 
for each question and manually created a new ver-
sion of the discourse segment purpose that includes 
this information. 
1
2
3
4
5
6
TUTOR1: To analyze this problem we will first de-
scribe the motion of the person and his keys while he
is holding them. Then we will look at the motion of 
the person and his keys after he lets go of them. 
  Let's begin by looking at the motion of the man and
his keys while he is holding them. 
  How does his velocity compare to that of his keys?
User answer1: man?s velocity is bigger [incorrect] 
TUTOR2: Well? The problem statement says that the
man is holding his keys motionless in front of his
face. That means they are both falling together and
are not moving with respect to each other. Hence their
velocities are the same. 
  Now let's see what happens when he releases them.
  To do so we will first find the individual forces
exerted on the man and his keys, then both net forces,
both accelerations, and finally we will describe and
compare the velocities. 
  So what are the forces exerted on the man after he
releases his keys? Please, specify their directions. 
User answer2: gravity vertically down [correct] 
TUTOR3: Good. Because there is just one force, grav-
ity's, it is trivial to determine the NET force. So what
is the direction of the NET force on the person? 
User answer3: down [correct] 
TUTOR4: Ok. Considering the net force, does the per-
son have an acceleration? If yes, specify its direction.
User answer4: the man has no acceleration [incorrect] 
TUTOR5: Well? To analyze the man's accelera-
tion we will use Newton's second law. What is
the definition of this law? 
Problem: Suppose a man is in a free-falling elevator and is 
holding his keys motionless right in front of his face.  He then 
lets go. What will be the position of the keys relative to the 
man's face as time passes?  Explain. 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Figure 1. Transcript of a sample ITSPOKE speech interaction (left). The NM as the user sees it after turn Tutor5
362
Limited horizon. Since in our case the system 
drives the conversation (i.e. system initiative), we 
always know what questions would be discussed 
next. We hypothesized that by having access to 
this information, users will have a better idea of 
where instruction is heading, thus facilitating their 
understanding of the relevance of the current topic 
to the overall discussion. To prevent information 
overload, we only display the next discourse seg-
ment purpose at each level in the hierarchy (see 
Figure 1, NM14, NM16, NM17 and NM19; Figure 2, 
NM5); additional discourse segments at the same 
level are signaled through a dotted line. To avoid 
helping the students answer the current question in 
cases when the next discourse segment hints/de-
scribes the answer, each discourse segment has an 
additional purpose annotation that is displayed 
when the segment is part of the visible horizon.  
Auto-collapse. To reduce the amount of infor-
mation on the screen, discourse segments dis-
cussed in the past are automatically collapsed by 
the system. For example, in Figure 1, NM Line 3 is 
collapsed in the actual system and Lines 4 and 5 
are hidden (shown in Figure1 to illustrate our dis-
course structure annotation.). The user can expand 
nodes as desired using the mouse. 
Information highlight. Bold and italics font 
were used to highlight important information (what 
and when to highlight was manually annotated). 
For example, in Figure 1, NM2 highlights the two 
time frames as they are key steps in approaching 
this problem. Correct answers are also highlighted. 
We would like to reiterate that the goal of this 
study is to investigate if making certain types of 
discourse information explicitly available to the 
user provides any benefits. Thus, whether we have 
made the optimal design choices is of secondary 
importance. While, we believe that our annotation 
is relatively robust as the system questions follow a 
carefully designed tutoring plan, in the future we 
would like to investigate these issues. 
4 User Study 
We designed a user study focused primarily on 
user?s perception of the NM presence/absence. We 
used a within-subject design where each user re-
ceived instruction both with and without the NM. 
Each user went through the same experimental 
procedure: 1) read a short document of background 
material, 2) took a pretest to measure initial phys-
ics knowledge, 3) worked through 2 problems with 
ITSPOKE 4) took a posttest similar to the pretest, 5) 
took a NM survey, and 6) went through a brief 
open-question interview with the experimenter. 
In the 3rd step, the NM was enabled in only one 
problem. Note that in both problems, users did not 
have access to the system turn transcript. After 
each problem users filled in a system question-
naire in which they rated the system on various 
dimensions; these ratings were designed to cover 
dimensions the NM might affect (see Section 5.1). 
While the system questionnaire implicitly probed 
the NM utility, the NM survey from the 5th step 
explicitly asked the users whether the NM was use-
ful and on what dimensions (see Section 5.1) 
To account for the effect of the tutored problem 
on the user?s questionnaire ratings, users were ran-
domly assigned to one of two conditions. The users 
in the first condition (F) had the NM enabled in the 
first problem and disabled in the second problem, 
while users in the second condition (S) had the op-
posite. Thus, if the NM has any effect on the user?s 
perception of the system, we should see a decrease 
in the questionnaire ratings from problem 1 to 
problem 2 for F users and an increase for S users. 
Other factors can also influence our measure-
ments. To reduce the effect of the text-to-speech 
component, we used a version of the system with 
human prerecorded prompts. We also had to ac-
count for the amount of instruction as in our sys-
tem the top level question segment is tailored to 
what users write in the essay. Thus the essay 
analysis component was disabled; for all users, the 
system started with the same top level question 
segment which assumed no information in the es-
say. Note that the actual dialogue depends on the 
correctness of the user answers. After the dialogue, 
users were asked to revise their essay and then the 
system moved on to the next problem. 
The collected corpus comes from 28 users (13 in 
F and 15 in S). The conditions were balanced for 
gender (F: 6 male, 7 female; S: 8 male, 7 female). 
There was no significant differences between the 
two conditions in terms of pretest (p<0.63); in both 
conditions users learned (significant difference 
between pretest and posttest, p<0.01). 
5 Results 
5.1 Subjective metrics 
Our main resource for investigating the effect of 
the NM was the system questionnaires given after 
363
each problem. These questionnaires are identical 
and include 16 questions that probed user?s percep-
tion of ITSPOKE on various dimensions. Users 
were asked to answer the questions on a scale from 
1-5 (1 ? Strongly Disagree, 2 ? Disagree, 3 ? 
Somewhat Agree, 4 ? Agree, 5 ? Strongly Agree). 
If indeed the NM has any effect we should observe 
differences between the ratings of the NM problem 
and the noNM problem (i.e. the NM is disabled). 
Table 1 lists the 16 questions in the question-
naire order. The table shows for every question the 
average rating for all condition-problem combina-
tions (e.g. column 5: condition F problem 1 with 
the NM enabled). For all questions except Q7 and 
Q11 a higher rating is better. For Q7 and Q11 
(italicized in Table 1) a lower rating is better as 
they gauge negative factors (high level of concen-
tration and task disorientation). They also served as 
a deterrent for negligence while rating. 
To test if the NM presence has a significant ef-
fect, a repeated-measure ANOVA with between-
subjects factors was applied. The within-subjects 
factor was the NM presence (NMPres) and the 
between-subjects factor was the condition (Cond)1. 
The significance of the effect of each factor and 
their combination (NMPres*Cond) is listed in the 
table with significant and trend effects highlighted 
in bold (see columns 2-4). Post-hoc t-tests between 
the NM and noNM ratings were run for each con-
dition (?s?/?t?marks significant/trend differences). 
Results for Q1-6 
Questions Q1-6 were inspired by previous work 
on spoken dialogue system evaluation (e.g. 
(Walker et al, 2000)) and measure user?s overall 
perception of the system. We find that the NM 
presence significantly improves user?s perception 
of the system in terms of their ability to concen-
trate on the instruction (Q3), in terms of their incli-
nation to reuse the system (Q6) and in terms of the 
system?s matching of their expectations (Q4). 
There is a trend that it was easier for them to learn 
from the NM enabled version of the system (Q2). 
Results for Q7-13 
Q7-13 relate directly to our hypothesis that users 
                                                 
1 Since in this version of ANOVA the NM/noNM rat-
ings come from two different problems based on the 
condition, we also run an ANOVA in which the within-
subjects factor was the problem (Prob). In this case, the 
NM effect corresponds to an effect from Prob*Cond 
which is identical in significance with that of NMPres. 
benefit from access to the discourse structure in-
formation. These questions probe the user?s per-
ception of ITSPOKE during the dialogue. We find 
that for 6 out 7 questions the NM presence has a 
significant/trend effect (Table 1, column 2).  
Structure. Users perceive the system as having 
a structured tutoring plan significantly2 more in the 
NM problems (Q8). Moreover, it is significantly 
easier for them to follow this tutoring plan if the 
NM is present (Q11). These effects are very clear 
for F users where their ratings differ significantly 
between the first (NM) and the second problem 
(noNM). A difference in ratings is present for S 
users but it is not significant. As with most of the S 
users? ratings, we believe that the NM presentation 
order is responsible for the mostly non-significant 
differences. More specifically, assuming that the 
NM has a positive effect, the S users are asked to 
rate first the poorer version of the system (noNM) 
and then the better version (NM). In contrast, F 
users? task is easier as they already have a high 
reference point (NM) and it is easier for them to 
criticize the second problem (noNM). Other factors 
that can blur the effect of the NM are domain 
learning and user?s adaptation to the system. 
Integration. Q9 and Q10 look at how well users 
think they integrate the system questions in both a 
forward-looking fashion (Q9) and a backward 
looking fashion (Q10). Users think that it is sig-
nificantly easier for them to integrate the current 
system question to what will be discussed in the 
future if the NM is present (Q9). Also, if the NM is 
present, it is easier for users to integrate the current 
question to the discussion so far (Q10, trend). For 
Q10, there is no difference for F users but a sig-
nificant one for S users. We hypothesize that do-
main learning is involved here: F users learn better 
from the first problem (NM) and thus have less 
issues solving the second problem (noNM). In con-
trast, S users have more difficulties in the first 
problem (noNM), but the presence of the NM 
eases their task in the second problem. 
Correctness. The correct answer NM feature is 
useful for users too. There is a trend that it is easier 
for users to know the correct answer if the NM is 
present (Q13). We hypothesize that speech recog-
nition and language understanding errors are re-
                                                 
2 We refer to the significance of the NMPres factor (Ta-
ble 1, column 2). When discussing individual experi-
mental conditions, we refer to the post-hoc t-tests. 
364
sponsible for the non-significant NM effect on the 
dimension captured by Q12. 
Concentration. Users also think that the NM 
enabled version of the system requires less effort in 
terms of concentration (Q7). We believe that hav-
ing the discourse segment purpose as visual input 
allows the users to concentrate more easily on what 
the system is uttering. In many of the open ques-
tion interviews users stated that it was easier for 
them to listen to the system when they had the dis-
course segment purpose displayed on the screen. 
Results for Q14-16 
Questions Q14-16 were included to probe user?s 
post tutoring perceptions. We find a trend that in 
the NM problems it was easier for users to under-
stand the system?s main point (Q14). However, in 
terms of identifying (Q15) and correcting (Q16) 
problems in their essay the results are inconclusive. 
We believe that this is due to the fact that the essay 
interpretation component was disabled in this ex-
periment. As a result, the instruction did not match 
the initial essay quality. Nonetheless, in the open-
question interviews, many users indicated using 
the NM as a reference while updating their essay. 
In addition to the 16 questions, in the system 
questionnaire after the second problem users were 
asked to choose which version of the system they 
preferred the most (i.e. the first or the second prob-
lem version). 24 out 28 users (86%) preferred the 
NM enabled version. In the open-question inter-
view, the 4 users that preferred the noNM version 
(2 in each condition) indicated that it was harder 
for them to concurrently concentrate on the audio 
and the visual input (divided attention problem) 
and/or that the NM was changing too fast. 
To further strengthen our conclusions from the 
system questionnaire analysis, we would like to 
note that users were not asked to directly compare 
the two versions but they were asked to individu-
ally rate two versions which is a noisier process 
(e.g. users need to recall their previous ratings). 
The NM survey 
While the system questionnaires probed users? 
NM usage indirectly, in the second to last step in 
the experiments, users had to fill a NM survey 
Table 1. System questionnaire results 
Question
Overall NMPres Cond
NMPres*
Cond
1. The tutor increased my understanding of the subject 0.518 0.898 0.862 4.0 > 3.9 4.0 > 3.9
2. It was easy to learn from the tutor 0.100 0.813 0.947 3.9 > 3.6 3.9 > 3.5
3. The tutor helped me to concentrate 0.016 0.156 0.854 3.5 > 3.0 3.9 >t 3.4
4. The tutor worked the way I expected it to 0.034 0.886 0.157 3.5 > 3.4 3.9 >s 3.1
5. I enjoyed working with the tutor 0.154 0.513 0.917 3.5 > 3.2 3.7 > 3.4
6. Based on my experience using the tutor to learn physics, I 
would like to use such a tutor regularly
0.004 0.693 0.988 3.7 >s 3.2 3.5 >s 3.0
During the conversation with the tutor:
7. ... a high level of concentration is required to follow the tutor 0.004 0.534 0.545 3.5 <s 4.2 3.9 <t 4.3
8. ... the tutor had a clear and structured agenda behind its 
explanations
0.008 0.340 0.104 4.4 >s 3.6 4.3 > 4.1
9. ... it was easy to figure out where the tutor's instruction was 
leading me
0.017 0.472 0.593 4.0 >s 3.4 4.1 > 3.7
10. ... when the tutor asked me a question I knew why it was 
asking me that question 
0.054 0.191 0.054 3.5 ~ 3.5 4.3 >s 3.5
11. ... it was easy to loose track of where I was in the interaction 
with the tutor 
0.012 0.766 0.048 2.5 <s 3.5 2.9 < 3.0
12. ... I knew whether my answer to the tutor's question was 
correct or incorrect
0.358 0.635 0.804 3.5 > 3.3 3.7 > 3.4
13. ... whenever I answered incorrectly, it was easy to know the 
correct answer after the tutor corrected me
0.085 0.044 0.817 3.8 > 3.5 4.3 > 3.9
At the end of the conversation with the tutor:
14. ... it was easy to understand the tutor's main point 0.071 0.056 0.894 4.0 > 3.6 4.4 > 4.1
15. ... I knew what was wrong or missing from my essay 0.340 0.965 0.340 3.9 ~ 3.9 3.7 < 4.0
16. ... I knew how to modify my essay 0.791 0.478 0.327 4.1 > 3.9 3.7 < 3.8
P1       P2
NM     noNM
P2       P1
NM     noNM
Average rating
ANOVA F condition S condition
365
which explicitly asked how the NM helped them, if 
at all. The answers were on the same 1 to 5 scale. 
We find that the majority of users (75%-86%) 
agreed or strongly agreed that the NM helped them 
follow the dialogue, learn more easily, concentrate 
and update the essay. These findings are on par 
with those from the system questionnaire analysis. 
5.2 Objective metrics 
Our analysis of the subjective user evaluations 
shows that users think that the NM is helpful. We 
would like to see if this perceived usefulness is 
reflected in any objective metrics of performance. 
Due to how our experiment was designed, the ef-
fect of the NM can be reliably measured only in 
the first problem as in the second problem the NM 
is toggled3; for the same reason, we can not use the 
pretest/posttest information. 
Our preliminary investigation 4  found several 
dimensions on which the two conditions differed in 
the first problem (F users had NM, S users did 
not). We find that if the NM was present the inter-
action was shorter on average and users gave more 
correct answers; however these differences are not 
statistically significant (Table 2). In terms of 
speech recognition performance, we looked at two 
metrics: AsrMis and SemMis (ASR/Semantic Mis-
recognition). A user turn is labeled as AsrMis if the 
output of the speech recognition is different from 
the human transcript (i.e. a binary version of Word 
Error Rate). SemMis are AsrMis that change the 
correctness interpretation. We find that if the NM 
was present users had fewer AsrMis and fewer 
SemMis (trend for SemMis, p<0.09). 
In addition, a ?2 dependency analysis showed 
that the NM presence interacts significantly with 
both AsrMis (p<0.02) and SemMis (p<0.001), with 
fewer than expected AsrMis and SemMis in the 
                                                 
3 Due to random assignment to conditions, before the 
first problem the F and S populations are similar (e.g. no 
difference in pretest); thus any differences in metrics 
can be attributed to the NM presence/absence. However, 
in the second problem, the two populations are not simi-
lar anymore as they have received different forms of 
instruction; thus any difference has to be attributed to 
the NM presence/absence in this problem as well as to 
the NM absence/presence in the previous problem.  
4 Due to logging issues, 2 S users are excluded from this 
analysis (13 F and 13 S users remaining). We run the 
subjective metric analysis from Section 5.1 on this sub-
set and the results are similar. 
NM condition. The fact that in the second problem 
the differences are much smaller (e.g. 2% for 
AsrMis) and that the NM-AsrMis and NM-
SemMis interactions are not significant anymore, 
suggests that our observations can not be attributed 
to a difference in population with respect to sys-
tem?s ability to recognize their speech. We hy-
pothesize that these differences are due to the NM 
text influencing users? lexical choice. 
Metric F (NM) S (noNM) p 
# user turns 21.8 (5.3) 22.8 (6.5) 0.65 
% correct turns 72% (18%) 67% (22%) 0.59 
AsrMis 37% (27%) 46% (28%) 0.46 
SemMis 5% (6%) 12% (14%) 0.09 
Table 2. Average (standard deviation) for  
objective metrics in the first problem 
6 Related work 
Discourse structure has been successfully used in 
non-interactive settings (e.g. understanding spe-
cific lexical and prosodic phenomena (Hirschberg 
and Nakatani, 1996) , natural language generation 
(Hovy, 1993), essay scoring (Higgins et al, 2004) 
as well as in interactive settings (e.g. predic-
tive/generative models of postural shifts (Cassell et 
al., 2001), generation/interpretation of anaphoric 
expressions (Allen et al, 2001), performance mod-
eling (Rotaru and Litman, 2006)). 
In this paper, we study the utility of the dis-
course structure on the user side of a dialogue sys-
tem. One related study is that of (Rich and Sidner, 
1998). Similar to the NM, they use the discourse 
structure information to display a segmented inter-
action history (SIH): an indented view of the inter-
action augmented with purpose information. This 
paper extends over their work in several areas. The 
most salient difference is that here we investigate 
the benefits of displaying the discourse structure 
information for the users. In contrast, (Rich and 
Sidner, 1998) never test the utility of the SIH. 
Their system uses a GUI-based interaction (no 
speech/text input, no speech output) while we look 
at a speech-based system. Also, their underlying 
task (air travel domain) is much simpler than our 
tutoring task. In addition, the SIH is not always 
available and users have to activate it manually. 
Other visual improvements for dialogue-based 
computer tutors have been explored in the past 
(e.g. talking heads (Graesser et al, 2003)). How-
ever, implementing the NM in a new domain re-
quires little expertise as previous work has shown 
366
that na?ve users can reliably annotate the informa-
tion needed for the NM (Passonneau and Litman, 
1993). Our NM design choices should also have an 
equivalent in a new domain (e.g. displaying the 
recognized user answer can be the equivalent of 
the correct answers). Other NM usages can also be 
imagined: e.g. reducing the length of the system 
turns by removing text information that is implic-
itly represented in the NM. 
7 Conclusions & Future work 
In this paper we explore the utility of the Naviga-
tion Map, a graphical representation of the dis-
course structure. As our first step towards under-
standing the benefits of the NM, we ran a user 
study to investigate if users perceive the NM as 
useful. From the users? perspective, the NM pres-
ence allows them to better identify and follow the 
tutoring plan and to better integrate the instruction. 
It was also easier for users to concentrate and to 
learn from the system if the NM was present. Our 
preliminary analysis on objective metrics shows 
that users? preference for the NM version is re-
flected in more correct user answers and less 
speech recognition problems in the NM version. 
These findings motivate future work in under-
standing the effects of the NM. We would like to 
continue our objective metrics analysis (e.g. see if 
users are better in the NM condition at updating 
their essay and at answering questions that require 
combining facts previously discussed). We also 
plan to run an additional user study with a be-
tween-subjects experimental design geared towards 
objective metrics. The experiment will have two 
conditions: NM present/absent for all problems. 
The conditions will then be compared in terms of 
various objective metrics. We would also like to 
know which information sources represented in the 
NM (e.g. discourse segment purpose, limited hori-
zon, correct answers) has the biggest impact. 
Acknowledgements 
This work is supported by NSF Grants 0328431 
and 0428472. We would like to thank Shimei Pan, 
Pamela Jordan and the ITSPOKE group. 
References 
K. Acomb, J. Bloom, K. Dayanidhi, P. Hunter, P. 
Krogh, E. Levin and R. Pieraccini. 2007. Technical 
Support Dialog Systems: Issues, Problems, and Solu-
tions. In Proc. of Workshop on Bridging the Gap: 
Academic and Industrial Research in Dialog Technologies. 
J. Allen, G. Ferguson, B. N., D. Byron, N. Chambers, 
M. Dzikovska, L. Galescu and M. Swift. 2006. Ches-
ter: Towards a Personal Medication Advisor. Journal 
of Biomedical Informatics, 39(5). 
J. Allen, G. Ferguson and A. Stent. 2001. An architec-
ture for more realistic conversational systems. In 
Proc. of Intelligent User Interfaces. 
J. Cassell, Y. I. Nakano, T. W. Bickmore, C. L. Sidner 
and C. Rich. 2001. Non-Verbal Cues for Discourse 
Structure. In Proc. of ACL. 
A. Graesser, K. Moreno, J. Marineau, A. Adcock, A. 
Olney and N. Person. 2003. AutoTutor improves deep 
learning of computer literacy: Is it the dialog or the 
talking head? In Proc. of Artificial Intelligence in 
Education (AIED). 
B. Grosz and C. L. Sidner. 1986. Attentions, intentions 
and the structure of discourse. Computational Lin-
guistics, 12(3). 
D. Higgins, J. Burstein, D. Marcu and C. Gentile. 2004. 
Evaluating Multiple Aspects of Coherence in Student 
Essays. In Proc. of HLT-NAACL. 
J. Hirschberg and C. Nakatani. 1996. A prosodic analy-
sis of discourse segments in direction-giving mono-
logues. In Proc. of ACL. 
E. Hovy. 1993. Automated discourse generation using 
discourse structure relations. Articial Intelligence, 
63(Special Issue on NLP). 
D. Litman and S. Silliman. 2004. ITSPOKE: An intelli-
gent tutoring spoken dialogue system. In Proc. of 
HLT/NAACL. 
S. Oviatt, R. Coulston and R. Lunsford. 2004. When Do 
We Interact Multimodally? Cognitive Load and Mul-
timodal Communication Patterns. In Proc. of Interna-
tional Conference on Multimodal Interfaces. 
R. Passonneau and D. Litman. 1993. Intention-based 
segmentation: Human reliability and correlation with 
linguistic cues. In Proc. of ACL. 
H. Pon-Barry, K. Schultz, E. O. Bratt, B. Clark and S. 
Peters. 2006. Responding to Student Uncertainty in 
Spoken Tutorial Dialogue Systems. International 
Journal of Artificial Intelligence in Education, 16. 
C. Rich and C. L. Sidner. 1998. COLLAGEN: A Col-
laboration Manager for Software Interface Agents. 
User Modeling and User-Adapted Interaction, 8(3-4). 
M. Rotaru and D. Litman. 2006. Exploiting Discourse 
Structure for Spoken Dialogue Performance Analy-
sis. In Proc. of EMNLP. 
M. Walker, D. Litman, C. Kamm and A. Abella. 2000. 
Towards Developing General Models of Usability 
with PARADISE. Natural Language Engineering. 
367
Proceedings of ACL-08: HLT, pages 622?629,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Assessing Dialog System User Simulation Evaluation Measures Using
Human Judges
Hua Ai
University of Pittsburgh
Pittsburgh PA, 15260, USA
hua@cs.pitt.edu
Diane J. Litman
University of Pittsburgh
Pittsburgh, PA 15260, USA
litman@cs.pitt.edu
Abstract
Previous studies evaluate simulated dialog
corpora using evaluation measures which can
be automatically extracted from the dialog
systems? logs. However, the validity of these
automatic measures has not been fully proven.
In this study, we first recruit human judges
to assess the quality of three simulated dia-
log corpora and then use human judgments
as the gold standard to validate the conclu-
sions drawn from the automatic measures. We
observe that it is hard for the human judges
to reach good agreement when asked to rate
the quality of the dialogs from given perspec-
tives. However, the human ratings give con-
sistent ranking of the quality of simulated cor-
pora generated by different simulation mod-
els. When building prediction models of hu-
man judgments using previously proposed au-
tomatic measures, we find that we cannot reli-
ably predict human ratings using a regression
model, but we can predict human rankings by
a ranking model.
1 Introduction
User simulation has been widely used in different
phases in spoken dialog system development. In
the system development phase, user simulation is
used in training different system components. For
example, (Levin et al, 2000) and (Scheffler, 2002)
exploit user simulations to generate large corpora
for using Reinforcement Learning to develop dia-
log strategies, while (Chung, 2004) implement user
simulation to train the speech recognizer and under-
standing components.
While user simulation is considered to be more
low-cost and time-efficient than experiments with
human subjects, one major concern is how well the
state-of-the-art user simulations can mimic human
user behaviors and how well they can substitute for
human users in a variety of tasks. (Schatzmann
et al, 2005) propose a set of evaluation measures
to assess the quality of simulated corpora. They
find that these evaluation measures are sufficient
to discern simulated from real dialogs. Since this
multiple-measure approach does not offer a easily
reportable statistic indicating the quality of a user
simulation, (Williams, 2007) proposes a single mea-
sure for evaluating and rank-ordering user simula-
tions based on the divergence between the simulated
and real users? performance. This new approach also
offers a lookup table that helps to judge whether an
observed ordering of two user simulations is statisti-
cally significant.
In this study, we also strive to develop a prediction
model of the rankings of the simulated users? per-
formance. However, our approach use human judg-
ments as the gold standard. Although to date there
are few studies that use human judges to directly as-
sess the quality of user simulation, we believe that
this is a reliable approach to assess the simulated
corpora as well as an important step towards devel-
oping a comprehensive set of user simulation evalu-
ation measures. First, we can estimate the difficulty
of the task of distinguishing real and simulated cor-
pora by knowing how hard it is for human judges to
reach an agreement. Second, human judgments can
be used as the gold standard of the automatic evalua-
tion measures. Third, we can validate the automatic
622
measures by correlating the conclusions drawn from
the automatic measures with the human judgments.
In this study, we recruit human judges to assess
the quality of three user simulation models. Judges
are asked to read the transcripts of the dialogs be-
tween a computer tutoring system and the simula-
tion models and to rate the dialogs on a 5-point scale
from different perspectives. Judges are also given
the transcripts between human users and the com-
puter tutor. We first assess human judges? abilities
in distinguishing real from simulated users. We find
that it is hard for human judges to reach good agree-
ment on the ratings. However, these ratings give
consistent ranking on the quality of the real and the
simulated user models. Similarly, when we use pre-
viously proposed automatic measures to predict hu-
man judgments, we cannot reliably predict human
ratings using a regression model, but we can consis-
tently mimic human judges? rankings using a rank-
ing model. We suggest that this ranking model can
be used to quickly assess the quality of a new simu-
lation model without manual efforts by ranking the
new model against the old models.
2 Related Work
A lot of research has been done in evaluating differ-
ent components of Spoken Dialog Systems as well
as overall system performance. Different evaluation
approaches are proposed for different tasks. Some
studies (e.g., (Walker et al, 1997)) build regression
models to predict user satisfaction scores from the
system log as well as the user survey. There are also
studies that evaluate different systems/system com-
ponents by ranking the quality of their outputs. For
example, (Walker et al, 2001) train a ranking model
that ranks the outputs of different language genera-
tion strategies based on human judges? rankings. In
this study, we build both a regression model and a
ranking model to evaluate user simulation.
(Schatzmann et al, 2005) summarize some
broadly used automatic evaluation measures for user
simulation and integrate several new automatic mea-
sures to form a comprehensive set of statistical eval-
uation measures. The first group of measures inves-
tigates how much information is transmitted in the
dialog and how active the dialog participants are.
The second group of measures analyzes the style of
the dialog and the last group of measures examines
the efficiency of the dialogs. While these automatic
measures are handy to use, these measures have not
been validated by humans.
There are well-known practices which validate
automatic measures using human judgments. For
example, in machine translation, BLEU score (Pa-
pineni et al, 2002) is developed to assess the quality
of machine translated sentences. Statistical analysis
is used to validate this score by showing that BLEU
score is highly correlated with the human judgment.
In this study, we validate a subset of the automatic
measures proposed by (Schatzmann et al, 2005) by
correlating the measures with human judgments. We
follow the design of (Linguistic Data Consortium,
2005) in obtaining human judgments. We call our
study an assessment study.
3 System and User Simulation Models
In this section, we describe our dialog system (IT-
SPOKE) and the user simulation models which
we use in the assessment study. ITSPOKE is
a speech-enabled Intelligent Tutoring System that
helps students understand qualitative physics ques-
tions. In the system, the computer tutor first presents
a physics question and the student types an essay
as the answer. Then, the tutor analyzes the essay
and initiates a tutoring dialog to correct misconcep-
tions and to elicit further explanations. A corpus
of 100 tutoring dialogs was collected between 20
college students (solving 5 physics problems each)
and the computer tutor, yielding 1388 student turns.
The correctness of student answers is automatically
judged by the system and kept in the system?s logs.
Our previous study manually clustered tutor ques-
tions into 20 clusters based on the knowledge (e.g.,
acceleration, Newton?s 3rd Law) that is required to
answer each question (Ai and Litman, 2007).
We train three simulation models from the real
corpus: the random model, the correctness model,
and the cluster model. All simulation models gener-
ate student utterances on the word level by picking
out the recognized student answers (with potential
speech recognition errors) from the human subject
experiments with different policies. The random
model (ran) is a simple unigram model which ran-
domly picks a student?s utterance from the real cor-
623
pus as the answer to a tutor?s question, neglecting
which question it is. The correctness model (cor)
is designed to give a correct/incorrect answer with
the same probability as the average of real students.
For each tutor?s question, we automatically compute
the average correctness rate of real student answers
from the system logs. Then, a correct/incorrect an-
swer is randomly chosen from the correct/incorrect
answer sets for this question. The cluster model
(clu) tries to model student learning by assuming
that a student will have a higher chance to give a
correct answer to the question of a cluster in which
he/she mostly answers correctly before. It computes
the conditional probability of whether a student an-
swer is correct/incorrect given the content of the tu-
tor?s question and the correctness of the student?s an-
swer to the last previous question that belongs to the
same question cluster. We also refer to the real stu-
dent as the real student model (real) in the paper.
We hypothesize that the ranking of the four student
models (from the most realistic to the least) is: real,
clu, cor, and ran.
4 Assessment Study Design
4.1 Data
We decided to conduct a middle-scale assessment
study that involved 30 human judges. We conducted
a small pilot study to estimate how long it took a
judge to answer all survey questions (described in
Section 4.2) in one dialog because we wanted to con-
trol the length of the study so that judges would not
have too much cognitive load and would be consis-
tent and accurate on their answers. Based on the pi-
lot study, we decided to assign each judge 12 dialogs
which took about an hour to complete. Each dialog
was assigned to two judges. We used three out of the
five physics problems from the original real corpus
to ensure the variety of dialog contents while keep-
ing the corpus size small. Therefore, the evaluation
corpus consisted of 180 dialogs, in which 15 dialogs
were generated by each of the 4 student models on
each of the 3 problems.
4.2 Survey Design
4.2.1 Survey questions
We designed a web survey to collect human judg-
ments on a 5-point scale on both utterance and di-
Figure 1: Utterance level questions.
alog levels. Each dialog is separated into pairs of
a tutor question and the corresponding student an-
swer. Figure 1 shows the three questions which
are asked for each tutor-student utterance pair. The
three questions assess the quality of the student an-
swers from three aspects of Grice?s Maxim (Grice,
1975): Maxim of Quantity (u QNT), Maxim of Rel-
evance (u RLV), and Maxim of Manner (u MNR).
We do not include the Maxim of Quality because in
our task domain the correctness of the student an-
swers depends largely on students? physics knowl-
edge, which is not a factor we would like to consider
when evaluating the realness of the students? dialog
behaviors.
In Figure 2, we show the three dialog level ques-
tions which are asked at the end of each dialog.
The first question (d TUR) is a Turing test type of
question which aims to obtain an impression of the
student?s overall performance. The second ques-
tion (d QLT) assesses the dialog quality from a
tutoring perspective. The third question (d PAT)
sets a higher standard on the student?s performance.
Unlike the first two questions which ask whether
the student ?looks? good, this question further asks
whether the judges would like to partner with the
particular student.
4.2.2 Survey Website
We display one tutor-student utterance pair and
the three utterance level questions on each web page.
After the judges answer the three questions, he/she
will be led to the next page which displays the next
pair of tutor-student utterances in the dialog with
the same three utterance level questions. The judge
624
Figure 2: Dialog level questions.
reads through the dialog in this manner and answers
all utterance level questions. At the end of the di-
alog, three dialog level questions are displayed on
one webpage. We provide a textbox under each di-
alog level question for the judge to type in a brief
explanation on his/her answer. After the judge com-
pletes the three dialog level questions, he/she will be
led to a new dialog. This procedure repeats until the
judge completes all of the 12 assigned dialogs.
4.3 Assessment Study
30 college students are recruited as human judges
via flyers. Judges are required to be native speak-
ers of American English to make correct judgments
on the language use and fluency of the dialog. They
are also required to have taken at least one course
on Newtonian physics to ensure that they can under-
stand the physics tutoring dialogs and make judg-
ments about the content of the dialogs. We follow
the same task assigning procedure that is used in
(Linguistic Data Consortium, 2005) to ensure a uni-
form distribution of judges across student models
and dialogs while maintaining a random choice of
judges, models, and dialogs. Judges are instructed to
work as quickly as comfortably possible. They are
encouraged to provide their intuitive reactions and
not to ponder their decisions.
5 Assessment Study Results
In the initial analysis, we observe that it is a difficult
task for human judges to rate on the 5-point scale
and the agreements among the judges are fairly low.
Table 1 shows for each question, the percentages of
d TUR d QLT d PAT u QNT u RLV u MNR
22.8% 27.8% 35.6% 39.2% 38.4% 38.7%
Table 1: Percent agreements on 5-point scale
pairs of judges who gave the same ratings on the 5-
point scale. For the rest of the paper, we collapse
the ?definitely? types of answers with its adjacent
?probably? types of answers (more specifically, an-
swer 1 with 2, and 4 with 5). We substitute scores 1
and 2 with a score of 1.5, and scores 4 and 5 with a
score of 4.5. A score of 3 remains the same.
5.1 Inter-annotator agreement
Table 2 shows the inter-annotator agreements on the
collapsed 3-point scale. The first column presents
the question types. In the first row, ?diff? stands
for the differences between human judges? ratings.
The column ?diff=0? shows the percent agreements
on the 3-point scale. We can see the improvements
from the original 5-point scale when comparing with
Table 1. The column ?diff=1? shows the percentages
of pairs of judges who agree with each other on a
weaker basis in that one of the judges chooses ?can-
not tell?. The column ?diff=2? shows the percent-
ages of pairs of judges who disagree with each other.
The column ?Kappa? shows the un-weighted kappa
agreements and the column ?Kappa*? shows the lin-
ear weighted kappa. We construct the confusion ma-
trix for each question to compute kappa agreements.
Table 3 shows the confusion matrix for d TUR. The
first three rows of the first three columns show the
counts of judges? ratings on the 3-point scale. For
example, the first cell shows that there are 20 cases
where both judges give 1.5 to the same dialog. When
calculating the linear weighted kappa, we define the
distances between the adjacent categories to be one1.
Note that we randomly picked two judges to rate
each dialog so that different dialogs are rated by dif-
ferent pairs of judges and one pair of judges only
worked on one dialog together. Thus, the kappa
agreements here do not reflect the agreement of one
pair of judges. Instead, the kappa agreements show
the overall observed agreement among every pair of
1We also calculated the quadratic weighted kappa in which
the distances are squared and the kappa results are similar to the
linear weighted ones. For calculating the two weighted kappas,
see http://faculty.vassar.edu/lowry/kappa.html for details.
625
Q diff=0 diff=1 diff=2 Kappa Kappa*
d TUR 35.0% 45.6% 19.4% 0.022 0.079
d QLT 46.1% 28.9% 25.0% 0.115 0.162
d PAT 47.2% 30.6% 22.2% 0.155 0.207
u QNT 66.8% 13.9% 19.3% 0.377 0.430
u RLV 66.6% 17.2% 16.2% 0.369 0.433
u MNR 67.5% 15.4% 17.1% 0.405 0.470
Table 2: Agreements on 3-point scale
score=1.5 score=3 score=4.5 sum
score=1.5 20 26 20 66
score=3 17 11 19 47
score=4.5 15 20 32 67
sum 52 57 71 180
Table 3: Confusion Matrix on d TUR
judges controlling for the chance agreement.
We observe that human judges have low agree-
ment on all types of questions, although the agree-
ments on the utterance level questions are better
than the dialog level questions. This observation
indicates that assessing the overall quality of sim-
ulated/real dialogs on the dialog level is a difficult
task. The lowest agreement appears on d TUR.
We investigate the low agreements by looking into
judges? explanations on the dialog level questions.
21% of the judges find it hard to rate a particular
dialog because that dialog is too short or the stu-
dent utterances mostly consist of one or two words.
There are also some common false beliefs among
the judges. For example, 16% of the judges think
that humans will say longer utterances while 9% of
the judges think that only humans will admit the ig-
norance of an answer.
5.2 Rankings of the models
In Table 4, the first column shows the name of the
questions; the second column shows the name of
the models; the third to the fifth column present the
percentages of judges who choose answer 1 and 2,
can?t tell, and answer 4 and 5. For example, when
looking at the column ?1 and 2? for d TUR, we
see that 22.2% of the judges think a dialog by a
real student is generated probably or definitely by
a computer; more judges (25.6%) think a dialog by
the cluster model is generated by a computer; even
more judges (32.2%) think a dialog by the correct-
ness model is generated by a computer; and even
Question model 1 and 2 can?t tell 4 and 5
d TUR
real 22.2% 28.9% 48.9%
clu 25.6% 31.1% 43.3%
cor 32.2% 26.7% 41.1%
ran 51.1% 28.9% 20.0%
d QLT
real 20.0% 10.0% 70.0%
clu 21.1% 20.0% 58.9%
cor 24.4% 15.6% 60.0%
ran 60.0% 18.9% 21.1%
d PAT
real 28.9% 21.1% 50.0%
clu 41.1% 17.8% 41.1%
cor 43.3% 18.9% 37.8%
ran 82.2% 14.4% 3.4%
Table 4: Rankings on Dialog Level Questions
more judges (51.1%) think a dialog by the random
model is generated by a computer. When looking at
the column ?4 and 5? for d TUR, we find that most
of the judges think a dialog by the real student is
generated by a human while the fewest number of
judges think a dialog by the random model is gen-
erated by a human. Given that more human-like is
better, both rankings support our hypothesis that the
quality of the models from the best to the worst is:
real, clu, cor, and ran. In other words, although it is
hard to obtain well-agreed ratings among judges, we
can combine the judges? ratings to produce the rank-
ing of the models. We see consistent ranking orders
on d QLT and d PAT as well, except for a disorder
of cluster and correctness model on d QLT indicated
by the underlines.
When comparing two models, we can tell which
model is better from the above rankings. Neverthe-
less, we also want to know how significant the dif-
ference is. We use t-tests to examine the significance
of differences between every two models. We aver-
age the two human judges? ratings to get an aver-
aged score for each dialog. For each pair of models,
we compare the two groups of the averaged scores
for the dialogs generated by the two models using
2-tail t-tests at the significance level of p < 0.05.
In Table 5, the first row presents the names of the
models in each pair of comparison. Sig means that
the t-test is significant after Bonferroni correction;
question mark (?) means that the t-test is signifi-
cant before the correction, but not significant after-
wards, we treat this situation as a trend; not means
that the t-test is not significant at all. The table shows
626
real- real- real- ran- ran- cor-
ran cor clu cor clu clu
d TUR sig not not sig sig not
d QLT sig not not sig sig not
d PAT sig ? ? sig sig not
u QNT sig not not sig sig not
u RLV sig not not sig sig not
u MNR sig not not sig sig not
Table 5: T-Tests Results
that only the random model is significantly different
from all other models. The correctness model and
the cluster model are not significantly different from
the real student given the human judges? ratings, nei-
ther are the two models significantly different from
each other.
5.3 Human judgment accuracy on d TUR
We look further into d TUR in Table 4 because it is
the only question that we know the ground truth. We
compute the accuracy of human judgment as (num-
ber of ratings 4&5 on real dialogs + number of rat-
ings of 1&2 on simulated dialogs)/(2*total number
of dialogs). The accuracy is 39.44%, which serves
as further evidence that it is difficult to discern hu-
man from simulated users directly. A weaker accu-
racy is calculated to be 68.35% when we treat ?can-
not tell? as a correct answer as well.
6 Validating Automatic Measures
Since it is expensive to use human judges to rate
simulated dialogs, we are interested in building pre-
diction models of human judgments using auto-
matic measures. If the prediction model can re-
liably mimic human judgments, it can be used to
rate new simulation models without collecting hu-
man ratings. In this section, we use a subset of the
automatic measures proposed in (Schatzmann et al,
2005) that are applicable to our data to predict hu-
man judgments. Here, the human judgment on each
dialog is calculated as the average of the two judges?
ratings. We focus on predicting human judgments
on the dialog level because these ratings represent
the overall performance of the student models. We
use six high-level dialog feature measures including
the number of student turns (Sturn), the number of
tutor turns (Tturn), the number of words per stu-
dent turn (Swordrate), the number of words per tu-
tor turn (Twordrate), the ratio of system/user words
per dialog (WordRatio), and the percentage of cor-
rect answers (cRate).
6.1 The Regression Model
We use stepwise multiple linear regression to model
the human judgments using the set of automatic fea-
tures we listed above. The stepwise procedure au-
tomatically selects measures to be included in the
model. For example, d TUR is predicted as 3.65 ?
0.08 ? WordRatio ? 3.21 ? Swordrate, with an
R-square of 0.12. The prediction models for d QLT
and d PAT have similar low R-square values of 0.08
and 0.17, respectively. This result is not surprising
because we only include the surface level automatic
measures here. Also, these measures are designed
for comparison between models instead of predic-
tion. Thus, in Section 6.2, we build a ranking model
to utilize the measures in their comparative manner.
6.2 The Ranking Model
We train three ranking models to mimic human
judges? rankings of the real and the simulated stu-
dent models on the three dialog level questions using
RankBoost, a boosting algorithm for ranking ((Fre-
und et al, 2003), (Mairesse et al, 2007)). We briefly
explain the algorithm using the same terminologies
and equations as in (Mairesse et al, 2007), by build-
ing the ranking model for d TUR as an example.
In the training phase, the algorithm takes as input
a group of dialogs that are represented by values of
the automatic measures and the human judges? rat-
ings on d TUR. The RankBoost algorithm treats the
group of dialogs as ordered pairs:
T = {(x, y)| x, y are two dialog samples,
x has a higher human rated score than y }
Each dialog x is represented by a set of m indica-
tor functions hs(x) (1 ? s ? m). For example:
hs(x) =
{ 1 if WordRatio(x) ? 0.47
0 otherwise
Here, the threshold of 0.47 is calculated by Rank-
Boost. ? is a parameter associated with each indi-
cator function. For each dialog, a ranking score is
627
calculated as:
F (x) =
?
s
?shs(x) (1)
In the training phase, the human ratings are used
to set ? by minimizing the loss function:
LOSS = 1|T |
?
(x,y)?T
eval(F (x) ? F (y)) (2)
The eval function returns 0 if (x, y) pair is ranked
correctly, and 1 otherwise. In other words, LOSS
score is the percentage of misordered pairs where
the order of the predicted scores disagree with the
order indicated by human judges. In the testing
phase, the ranking score for every dialog is cal-
culated by Equation 1. A baseline model which
ranks dialog pairs randomly produces a LOSS of 0.5
(lower is better).
While LOSS indicates how many pairs of dialogs
are ranked correctly, our main focus here is to rank
the performance of the four student models instead
of individual dialogs. Therefore, we propose another
Averaged Model Ranking (AMR) score. AMR is
computed as the sum of the ratings of all the dialogs
generated by one model averaged by the number of
the dialogs. The four student models are then ranked
based on their AMR scores. The chance to get the
right ranking order of the four student models by
random guess is 1/(4!).
Table 6 shows a made-up example to illustrate the
two measures. real 1 and real 2 are two dialogs gen-
erated by the real student model; ran 1 and ran 2
are two dialogs by the random model. The second
and third column shows the human-rated score as the
gold standard and the machine-predicted score in the
testing phase respectively. The LOSS in this exam-
ple is 1/6, because only the pair of real 2 and ran 1
is misordered out of all the 6 possible pair combina-
tions. We then compute the AMR of the two models.
According to human-rated scores, the real model is
scored 0.75 (=(0.9+0.6)/2) while the random model
is scored 0.3. When looking at the predicted scores,
the real model is scored 0.65, which is also higher
than the random model with a score of 0.4. We thus
conclude that the ranking model ranks the two stu-
dent models correctly according to the overall rating
measure. We use both LOSS and AMR to evaluate
the ranking models.
Dialog Human-rated Score Predicted Score
real 1 0.9 0.9
real 2 0.6 0.4
ran 1 0.4 0.6
ran 2 0.2 0.2
Table 6: A Made-up Example of the Ranking Model
Cross Validation d TUR d QLT d PAT
Regular 0.176 0.155 0.151
Minus-one-model 0.224 0.180 0.178
Table 7: LOSS scores for Regular and Minus-one-model
(during training) Cross Validations
First, we use regular 4-fold cross validation where
we randomly hold out 25% of the data for testing
and train on the remaining 75% of the data for 4
rounds. Both the training and the testing data consist
of dialogs equally distributed among the four student
models. However, since the practical usage of the
ranking model is to rank a new model against sev-
eral old models without collecting additional human
ratings, we further test the algorithm by repeating
the 4 rounds of testing while taking turns to hold out
the dialogs from one model in the training data, as-
suming that model is the new model that we do not
have human ratings to train on. The testing corpus
still consists of dialogs from all four models. We call
this approach the minus-one-model cross validation.
Table 7 shows the LOSS scores for both cross val-
idations. Using 2-tailed t-tests, we observe that the
ranking models significantly outperforms the ran-
dom baseline in all cases after Bonferroni correction
(p < 0.05). When comparing the two cross vali-
dation results for the same question, we see more
LOSS in the more difficult minus-one-model case.
However, the LOSS scores do not offer a direct
conclusion on whether the ranking model ranks the
four student models correctly or not. To address
this question, we use AMR scores to re-evaluate all
cross validation results. Table 8 shows the human-
rated and predicted AMR scores averaged over four
rounds of testing on the regular cross validation re-
sults. We see that the ranking model gives the
same rankings of the student models as the human
judges on all questions. When applying AMR on
the minus-one-model cross validation results, we see
similar results that the ranking model reproduces hu-
628
real clu cor ran
human predicted human predicted human predicted human predicted
d TUR 0.68 0.62 0.65 0.59 0.63 0.52 0.51 0.49
d QLT 0.75 0.71 0.71 0.63 0.69 0.61 0.48 0.50
d PAR 0.66 0.65 0.60 0.60 0.58 0.57 0.31 0.32
Table 8: AMR Scores for Regular Cross Validation
man judges? rankings. Therefore, we suggest that
the ranking model can be used to evaluate a new
simulation model by ranking it against several old
models. Since our testing corpus is relatively small,
we would like to confirm this result on a large corpus
and on other dialog systems in the future.
7 Conclusion and Future Work
Automatic evaluation measures are used in evaluat-
ing simulated dialog corpora. In this study, we inves-
tigate a set of previously proposed automatic mea-
sures by comparing the conclusions drawn by these
measures with human judgments. These measures
are considered as valid if the conclusions drawn by
these measures agree with human judgments. We
use a tutoring dialog corpus with real students, and
three simulated dialog corpora generated by three
different simulation models trained from the real
corpus. Human judges are recruited to read the di-
alog transcripts and rate the dialogs by answering
different utterance and dialog level questions. We
observe low agreements among human judges? rat-
ings. However, the overall human ratings give con-
sistent rankings on the quality of the real and sim-
ulated user models. Therefore, we build a ranking
model which successfully mimics human judgments
using previously proposed automatic measures. We
suggest that the ranking model can be used to rank
new simulation models against the old models in or-
der to assess the quality of the new model.
In the future, we would like to test the ranking
model on larger dialog corpora generated by more
simulation models. We would also want to include
more automatic measures that may be available in
the richer corpora to improve the ranking and the
regression models.
Acknowledgments
This work is supported by NSF 0325054. We thank
J. Tereault, M. Rotaru, K. Forbes-Riley and the
anonymous reviewers for their insightful sugges-
tions, F. Mairesse for helping with RankBoost, and
S. Silliman for his help in the survey experiment.
References
H. Ai and D. Litman. 2007. Knowledge Consistent User
Simulations for Dialog Systems. In Proc. of Inter-
speech 2007.
G. Chung. 2004. Developing a Flexible Spoken Dialog
System Using Simulation. In Proc. of ACL 04.
Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. 2003.
An Efficient Boosting Algorithm for Combining Pref-
erences. Journal of Machine Learning Research.
H. P. Grice 1975. Logic and Conversation. Syntax and
Semantics III: Speech Acts, 41-58.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochas-
tic Model of Human-Machine Interaction For learning
Dialog Strategies. IEEE Trans. On Speech and Audio
Processing, 8(1):11-23.
Linguistic Data Consortium. 2005. Linguistic Data An-
notation Specification: Assessment of Fluency and Ad-
equacy in Translations.
F. Mairesse, M. Walker, M. Mehl and R. Moore. 2007.
Using Linguistic Cues for the Automatic Recognition
of Personality in Conversation and Text. Journal of
Artificial Intelligence Research, Vol 30, pp 457-501.
K.A. Papineni, S. Roukos, R.T. Ward, and W-J. Zhu.
2002. Bleu: A Method for Automatic Evaluation of
Machine Translation. In Proc. of 40th ACL.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quan-
titative Evaluation of User Simulation Techniques for
Spoken Dialog Systems. In Proc. of 6th SIGdial.
K. Scheffler. 2002. Automatic Design of Spoken Dialog
Systems. Ph.D. diss., Cambridge University.
J. D. Williams. 2007. A Method for Evaluating and Com-
paring User Simulations: The Cramer-von Mises Di-
vergence. Proc IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU).
M. Walker, D. Litman, C. Kamm, and A. Abella. 1997.
PARADISE: A Framework for Evaluating Spoken Dia-
log Agents. In Proc. of ACL 97.
M. Walker, O. Rambow, and M. Rogati. 2001. SPoT: A
Trainable Sentence Planner. In Proc. of NAACL 01.
629
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 888?896,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Setting Up User Action Probabilities in User Simulations for Dialog
System Development
Hua Ai
University of Pittsburgh
Pittsburgh PA, 15260, USA
hua@cs.pitt.edu
Diane Litman
University of Pittsburgh
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
User simulations are shown to be useful in
spoken dialog system development. Since
most current user simulations deploy prob-
ability models to mimic human user be-
haviors, how to set up user action proba-
bilities in these models is a key problem
to solve. One generally used approach is
to estimate these probabilities from human
user data. However, when building a new
dialog system, usually no data or only a
small amount of data is available. In this
study, we compare estimating user proba-
bilities from a small user data set versus
handcrafting the probabilities. We discuss
the pros and cons of both solutions for dif-
ferent dialog system development tasks.
1 Introduction
User simulations are widely used in spoken di-
alog system development. Recent studies use
user simulations to generate training corpora to
learn dialog strategies automatically ((Williams
and Young, 2007), (Lemon and Liu, 2007)), or to
evaluate dialog system performance (Lo?pez-Co?zar
et al, 2003). Most studies show that using user
simulations significantly improves dialog system
performance as well as speeds up system devel-
opment. Since user simulation is such a useful
tool, dialog system researchers have studied how
to build user simulations from a variety of perspec-
tives. Some studies look into the impact of training
data on user simulations. For example, (Georgila
et al, 2008) observe differences between simu-
lated users trained from human users of different
age groups. Other studies explore different simu-
lation models, i.e. the mechanism of deciding the
next user actions given the current dialog context.
(Schatzmann et al, 2006) give a thorough review
of different types of simulation models. Since
most of these current user simulation techniques
use probabilistic models to generate user actions,
how to set up the probabilities in the simulations
is another important problem to solve.
One general approach to set up user action prob-
abilities is to learn the probabilities from a col-
lected human user dialog corpus ((Schatzmann et
al., 2007b), (Georgila et al, 2008)). While this
approach takes advantage of observed user behav-
iors in predicting future user behaviors, it suffers
from the problem of learning probabilities from
one group of users while potentially using them
with another group of users. The accuracy of the
learned probabilities becomes more questionable
when the collected human corpus is small. How-
ever, this is a common problem in building new
dialog systems, when often no data1 or only a
small amount of data is available. An alterna-
tive approach is to handcraft user action proba-
bilities ((Schatzmann et al, 2007a), (Janarthanam
and Lemon, 2008)). This approach is less data-
intensive, but requires nontrivial work by domain
experts. What is more, as the number of proba-
bilities increases, it is hard even for the experts to
set the probabilities. Since both handcrafting and
training user action probabilities have their own
pros and cons, it is an interesting research ques-
tion to investigate which approach is better for a
certain task given the amount of data that is avail-
able.
In this study, we investigate a manual and a
trained approach in setting up user action proba-
bilities, applied to building the same probabilis-
tic simulation model. For the manual user simula-
tions, we look into two sets of handcrafted proba-
bilities which use the same expert knowledge but
differ in individual probability values. This aims
to take into account small variations that can possi-
1When no human user data is collected with the dialog
system, Wizard-of-Oz experiments can be conducted to col-
lect training data for building user simulations.
888
bly be introduced by different domain experts. For
the trained user simulations, we examine two sets
of probabilities trained from user corpora of dif-
ferent sizes, since the amount of training data will
impact the quality of the trained probability mod-
els. We compare the trained and the handcrafted
simulations on three tasks. We observe that in our
task settings, the two manual simulations do not
differ significantly on any tasks. In addition, there
is no significant difference among the trained and
the manual simulations in generating corpus level
dialog behaviors as well as in generating training
corpora for learning dialog strategies. When com-
paring on a dialog system evaluation task, the sim-
ulation trained from more data significantly out-
performs the two manual simulations, which again
outperforms the simulation trained from less data.
Based on our observations, we answer the orig-
inal question of how to design user action proba-
bilities for simulations that are similar to ours in
terms of the complexity of the simulations2. We
suggest that handcrafted user simulations can per-
form reasonably well in building a new dialog sys-
tem, especially when we are not sure that there is
enough data for training simulation models. How-
ever, once we have a dialog system, it is use-
ful to collect human user data in order to train a
new user simulation model since the trained sim-
ulations perform better than the handcrafted user
simulations on more tasks. Since how to decide
whether enough data is available for simulation
training is another research question to answer, we
will further discuss the impact of our results later
in Section 6.
2 Related Work
Most current simulation models are probabilistic
models in which the models simulate user actions
based on dialog context features (Schatzmann et
al., 2006). We represent these models as:
P (user action|feature1, . . .,featuren) (1)
The number of probabilities involved in this
model is:
(# of possible actions-1) ?
n?
k=1
(# of feature values). (2)
Some studies handcraft these probabilities. For
example, (Schatzmann et al, 2007a) condition the
2The number of user action probabilities and the simu-
lated user behaviors will impact the design choice.
user actions on user?s goals and the agenda to
reach those goals. They manually author the prob-
abilities in the user?s agenda update model and the
goal update model, and then calculate the user ac-
tion probabilities based on the two models. (Ja-
narthanam and Lemon, 2008) handcraft 15 proba-
bilities in simulated users? initial profiles and then
author rules to update these probabilities during
the dialogs.
Other studies use a human user corpus as the
training corpus to learn user action probabilities
in user simulations. Since the human user cor-
pus often does not include all possible actions that
users may take during interactions with the dialog
system, different strategies are used to account for
user actions that do not appear in the training cor-
pus but may be present when testing the user sim-
ulations. For example, (Schatzmann et al, 2007b)
introduce a summary space approach to map the
actual dialog context space into a more tractable
summary space. Then, they use forward and back-
ward learning algorithms to learn the probabili-
ties from a corpus generated by 40 human users
(160 dialogs). (Rieser and Lemon, 2006) use a
two step approach in computing the probabilities
from a corpus consisting of dialogs from 24 hu-
man users (70 dialogs). They first cluster dialog
contexts based on selected features and then build
conditional probability models for each cluster.
In our study, we build a conditional probability
model which will be described in detail in Sec-
tion 3.2.1. There are 40 probabilities to set up in
this model3. We will explain different approaches
to assign these probabilities later in Section 3.2.2.
3 System and User Simulations
In this section, we describe the dialog system, the
human user corpus we collected with the system,
and the user simulation we used.
3.1 System and Corpus
The ITSPOKE system (Litman and Silliman,
2004) is an Intelligent Tutoring System which
teaches Newtonian physics. It is a speech-
enhanced version of the Why2-Atlas tutoring sys-
tem (Vanlehn et al, 2002). During the interac-
tion with students, the system initiates a spoken
tutoring dialog to correct misconceptions and to
3There are 2 possible actions in our model, 20 possible
values for the first feature qCluster and 2 possible values for
the second feature prevCorrectness as described later in Sec-
tion 3.2.1. Using Equation 2, 40=(2-1)*20*2.
889
SYSTEM1: Do you recall what Newton?s
third law says? [3rdLaw]
Student1: Force equals mass times
acceleration. [ic, c%=0, ncert]
SYSTEM2: Newton?s third law says ...
If you hit the wall harder, is the
force of your fist acting on the
wall greater or less? [3rdLaw]
Student2: Greater. [c, c%=50%,cert]
Dialog goes on
Table 1: Sample coded dialog excerpt.
elicit further explanation. A pretest is given before
the interaction and a posttest is given afterwards.
We calculate a Normalized Learning Gain for each
student to evaluate the performance of the system
in terms of the student?s knowledge gain:
NLG = posttest score - pretest score1-pretest score (3)
The current tutoring dialog strategy was hand-
crafted in a finite state paradigm by domain ex-
perts, and the tutor?s response is based only on the
correctness of the student?s answer4. However, tu-
toring research (Craig et al, 2004) suggests that
other underlying information in student utterances
(e.g., student certainty) is also useful in improving
learning. Therefore, we are working on learning
a dialog strategy to also take into account student
certainty.
In our prior work, a corpus of 100 dialogs (1388
student turns) was collected between 20 human
subjects (5 dialogs per subject) and the ITSPOKE
system. Correctness (correct(c), incorrect(ic)) is
automatically judged by the system and is kept in
the system?s logs. We also computed the student?s
correctness rate (c%) and labeled it after every
student turn. Each student utterance was manu-
ally annotated for certainty (certain(cert), notcer-
tain(ncert)) in a previous study based on both lex-
ical and prosodic information5. In addition, we
manually clustered tutor questions into 20 clusters
based on the knowledge that is required to answer
that question, e.g. questions on Newton?s Third
Law are put into a cluster labeled as (3rdLaw).
There are other clusters such as gravity, acceler-
ation, etc. An example of a coded dialog between
the system and a student is given in Table 1.
4Despite the limitation of the current system, students
learn significantly after interacting with the system.
5Kappa of 0.68 is gained in the agreement study.
3.2 User Simulation Model and Model
Probabilities Set-up
3.2.1 User Simulation Model
We build a Knowledge Consistency Model6 (KC
Model) to simulate consistent student behaviors
while interacting with a tutoring system. Ac-
cording to learning literature (Cen et al, 2006),
once a student acquires certain knowledge, his/her
performance on similar problems that require
the same knowledge (i.e. questions from the
same cluster we introduced in Section 3.1) will
become stable. Therefore, in the KC Model,
we condition the student action stuAction based
on the cluster of tutor question (qCluster) and
the student?s correctness when last encountering
a question from that cluster (prevCorrectness):
P (stuAction|qCluster, prevCorrectness). For
example, in Table 1, when deciding the student?s
answer after the second tutor question, the simu-
lation looks back into the dialog and finds out that
the last time (in Student1) the student answered
a question from the same cluster 3rdLaw incor-
rectly. Therefore, this time the simulation gives
a correct student answer based on the probability
P (c|3rdLaw, ic).
Since different groups of students often have
different learning abilities, we examine such dif-
ferences among our users by grouping the users
based on Normalized Learning Gains (NLG),
which is an important feature to describe user be-
haviors in tutoring systems. By dividing our hu-
man users into high/low learners based on the me-
dian of NLG, we find a significant difference in the
NLG of the two groups based on 2-tailed t-tests
(p < 0.05). Therefore, we construct a simula-
tion to represent low learners and another simula-
tion to represent high learners to better character-
ize the differences in high/low learners? behaviors.
Similar approaches are adopted in other studies in
building user simulations for dialog systems (e.g.,
(Georgila et al, 2008) simulate old versus young
users separately).
Our simulation models work on the word level 7
because generating student dialog acts alone does
not provide sufficient information for our tutoring
system to decide the next system action. Since it
is hard to generate a natural language utterance for
each tutor?s question, we use the student answers
6This is the best model we built in our previous studies
(Ai and Litman, 2007).
7See (Ai and Litman, 2006) for more details.
890
in the human user corpus as the candidate answers
for the simulated students.
3.2.2 Model Probabilities Set-up
Now we discuss how to set up user action prob-
abilities in the KC Model. We compare learning
probabilities from human user data to handcrafting
probabilities based on expert knowledge. Since we
represent high/low learners using different mod-
els, we build simulation models with separate user
action probabilities to represent the two groups of
learners.
When learning the probabilities in the Trained
KC Models, we calculate user action probabilities
for high/low learners in our human corpus sepa-
rately. We use add-one smoothing to account for
user actions that do not appear in the human user
corpus. For the first time the student answers a
question in a certain cluster, we back-off the user
action probability to P(stuAction | average cor-
rectness rate of this question in human user cor-
pus). We first train a KC model using the data
from all 20 human users to build the TrainedMore
(Tmore) Model. Then, in order to investigate the
impact of the amount of training data on the qual-
ity of trained simulations, we randomly pick 5 out
of the 10 high learners and 5 out of the 10 low
learners to get an even smaller human user corpus.
We train the TrainedLess (Tless) Model from this
small corpus .
When handcrafting the probabilities in the Man-
ual KC Models8, the clusters of questions are
first grouped into three difficulty groups (Easy,
Medium, Hard). Based on expert knowledge,
we assume on average 70% of students can cor-
rectly answer the tutor questions from the Easy
group, while for the Medium group only 60%
and for the hard group 50%. Then, we assign
a correctness rate higher than the average for
the high learners and a corresponding correctness
rate lower than the average for the low learners.
For the first Manual KC model (M1), within the
same difficulty group, the same two probabilities
P1(stuAction|qClusteri, prevCorrectness = c) and
P2(stuAction|qClusteri, prevCorrectness = ic) are
assigned to each clusteri as the averages for the
corresponding high/low learners. Since a different
human expert will possibly provide a slightly dif-
ferent set of probabilities even based on the same
mechanism, we also design another set of prob-
8The first author of the paper acts as the domain expert.
abilities to account for such variations. For the
second Manual KC model (M2), we allow dif-
ferences among the clusters within the same dif-
ficulty group. For the clusters in each difficulty
group, we randomly assign a probability that dif-
fers no more than 5% from the average. For exam-
ple, for the easy clusters, we assign average proba-
bilities of high/low learners between [65%, 75%].
Although human experts may differ to some ex-
tent in assigning individual probability values, we
hypothesize that in general a certain amount of ex-
pertise is required in assigning these probabilities.
To investigate this, we build a baseline simula-
tion with no expert knowledge, which is a Ran-
dom Model (Ran) that randomly assigns values
for these user action probabilities.
4 Evaluation Measures
In this section, we introduce the evaluation mea-
sures for comparing the simulated corpora gen-
erated by different simulation models to the hu-
man user corpus. In Section 4.1, we use a set of
widely used domain independent features to com-
pare the simulated and the human user corpora
on corpus-level dialog behaviors. These compar-
isons give us a direct impression of how similar
the simulated dialogs are to human user dialogs.
Then, we compare the simulations in task-oriented
contexts. Since simulated user corpora are often
used as training corpora for using MDPs to learn
new dialog strategies, in Section 4.2 we estimate
how different the learned dialog strategies would
be when trained from different simulated corpora.
Another way to use user simulation is to test dialog
systems. Therefore, in Section 4.3, we compare
the user actions predicted by the various simula-
tion models with actual human user actions.
4.1 Measures on Corpus Level Dialog
Behaviors
We compare the dialog corpora generated by user
simulations to our human user corpus using a com-
prehensive set of corpus level measures proposed
by (Schatzmann et al, 2005). Here, we use a sub-
set of the measures which describe high-level dia-
log features that are applicable to our data. The
measures we use include the number of student
turns (Sturn), the number of tutor turns (Tturn), the
number of words per student turn (Swordrate), the
number of words per tutor turn (Twordrate), the ra-
tio of system/user words per dialog (WordRatio),
891
and the percentage of correct answers (cRate).
4.2 Measures on Dialog Strategy Learning
In this section, we introduce two measures to com-
pare the simulations based on their performance
on a dialog strategy learning task. In recent stud-
ies (e.g., (Janarthanam and Lemon, 2008)), user
simulations are built to generate a large corpus
to build MDPs in using Reinforcement Learning
(RL) to learn new dialog strategies. When building
an MDP from a training corpus9, we compute the
transition probabilities P (st+1|st, a) (the proba-
bility of getting from state st to the next state st+1
after taking action a), and the reward of this transi-
tion R(st, a, st+1). Then, the expected cumulative
value (V-value) of a state s can be calculated using
this recursive function:
V (s) =
?
st+1
P (st+1|st, a)[R(st, a, st+1) + ?V (st+1)]
(4)
? is a discount factor which ranges between 0 and
1.
For our evaluation, we first compare the tran-
sition probabilities calculated from all simulated
corpora. The transition probabilities are only de-
termined by the states and user actions presented
by the training corpus, regardless of the rest of the
MDP configuration. Since the MDP configuration
has a big impact on the learned strategies, we want
to first factor this impact out and estimate the dif-
ferences in learned strategies that are brought in
by the training corpora alone. As a second evalua-
tion measure, we apply reinforcement learning to
the MDP representing each simulated corpus sep-
arately to learn dialog strategies. We compare the
Expected Cumulative Rewards (ECRs)(Williams
and Young, 2007) of these dialog strategies, which
show the expectation of the rewards we can obtain
by applying the learned strategies.
The MDP learning task in our study is to max-
imize student certainty during tutoring dialogs.
The dialog states are characterized using the cor-
rectness of the current student answer and the stu-
dent correctness rate so far. We represent the cor-
rectness rate as a binary feature: lc if it is below
the training corpus average and hc if it is above the
average. The end of dialog reward is assigned to
be +100 if the dialog has a percent certainty higher
9In this paper, we use off-line model-based RL (Paek,
2006) rather than learning an optimal strategy online during
system-user interactions.
than the median from the training corpus and -100
otherwise. The action choice of the tutoring sys-
tem is to give a strong (s) or weak (w) feedback.
A strong feedback clearly indicates the correctness
of the current student answer while the weak feed-
back does not. For example, the second system
turn in Table 1 contains a weak feedback. If the
system says ?Your answer is incorrect? at the be-
ginning of this turn, that would be a strong feed-
back. In order to simulate student certainty, we
simply output the student certainty originally asso-
ciated in each student utterance. Thus, the output
of the KC Models here is a student utterance along
with the student certainty (cert, ncert). In a pre-
vious study (Ai et al, 2007), we investigated the
impact of different MDP configurations by com-
paring the ECRs of the learned dialog strategies.
Here, we use one of the best-performing MDP
configurations, but vary the simulated corpora that
we train the dialog strategies on. Our goal is to see
which user simulation performs better in generat-
ing a training corpus for dialog strategy learning.
4.3 Measures on Dialog System Evaluation
In this section, we introduce two ways to com-
pare human user actions with the actions predicted
by the simulations. The aim of this comparison
is to assess how accurately the simulations can
replicate human user behaviors when encounter-
ing the same dialog situation. A simulated user
that can accurately predict human user behaviors
is needed to replace human users when evaluating
dialog systems.
We randomly divide the human user dialog cor-
pus into four parts: each part contains a balanced
amount of high/low learner data. Then we perform
four fold cross validation by always using 3 parts
of the data as our training corpus for user simula-
tions, and the remaining one part of the data as
testing data to compare with simulated user ac-
tions. We always compare high human learners
only with simulation models that represent high
learners and low human learners only with simu-
lation models that represent low learners. Compar-
isons are done on a turn by turn basis. Every time
the human user takes an action in the dialogs in the
testing data, the user simulations are used to pre-
dict an action based on related dialog information
from the human user dialog. For a KC Model, the
related dialog information includes qCluster and
prevCorrectness . We first compare the simulation
892
predicted user actions directly with human user ac-
tions. We define simulation accuracy as:
Accuracy = Correctly predicted human user actionsTotal number of human user actions (5)
However, since our simulation model is a prob-
abilistic model, the model will take an action
stochastically after the same tutor turn. In other
words, we need to take into account the probabil-
ity for the simulation to predict the right human
user action. If the simulation outputs the right ac-
tion with a small probability, it is less likely that
this simulation can correctly predict human user
behaviors when generating a large dialog corpus.
We consider a simulated action associated with a
higher probability to be ranked higher than an ac-
tion with a lower probability. Then, we use the re-
ciprocal ranking from information retrieval tasks
(Radev et al, 2002) to assess the simulation per-
formance10. Mean Reciprocal Ranking is defined
as:
MRR = 1A
A?
k=1
1
ranki (6)
In Equation 6, A stands for the total number of
human user actions, ranki stands for the ranking
of the simulated action which matches the i-th hu-
man user action.
Table 2 shows an example of comparing simu-
lated user actions with human user actions in the
sample dialog in Table 1. In the first turn Stu-
dent1, a simulation model has a 60% chance to
output an incorrect answer and a 40% chance to
output a correct answer while it actually outputs
an incorrect answer. In this case, we consider the
simulation ranks the actions in the order of: ic, c.
Since the human user gives an incorrect answer at
this time, the simulated action matches with this
human user action and the reciprocal ranking is
1. However, in the turn Student2, the simulation?s
output does not match the human user action. This
time, the correct simulated user action is ranked
second. Therefore, the reciprocal ranking of this
simulation action is 1/2.
We hypothesize that the measures introduced
in this section have larger power in differentiat-
ing different simulated user behaviors since every
10(Georgila et al, 2008) use Precision and Recall to cap-
ture similar information as our accuracy, and Expected Pre-
cision and Expected Recall to capture similar information as
our reciprocal ranking.
simulated user action contributes to the compar-
ison between different simulations. In contrast,
the measures introduced in Section 4.1 and Sec-
tion 4.2 have less differentiating power since they
compare at the corpus level.
5 Results
We let al user simulations interact with our dia-
log system, where each simulates 250 low learners
and 250 high learners. In this section, we report
the results of applying the evaluation measures we
discuss in Section 4 on comparing simulated and
human user corpora. When we talk about signifi-
cant results in the statistics tests below, we always
mean that the p-value of the test is ? 0.05.
5.1 Comparing on Corpus Level Dialog
Behavior
Figure 1 shows the results of comparisons using
domain independent high-level dialog features of
our corpora. The x-axis shows the evaluation mea-
sures; the y-axis shows the mean for each corpus
normalized to the mean of the human user cor-
pus. Error bars show the standard deviations of
the mean values. As we can see from the figure,
the Random Model performs differently from the
human and all the other simulated models. There
is no difference in dialog behaviors among the hu-
man corpus, the trained and the manual simulated
corpora.
In sum, both the Trained KC Models and
the Manual KC Models can generate human-like
high-level dialog behaviors while the Random
Model cannot.
5.2 Comparing on Dialog Strategy Learning
Task
Next, we compare the difference in dialog strategy
learning when training on the simulated corpora
using similar approaches in (Tetreault and Litman,
2008). Table 3 shows the transition probabilities
starting from the state (c, lc). For example, the
first cell shows in the Tmore corpus, the probabil-
ity of starting from state (c, lc), getting a strong
feedback, and transitioning into the same state is
24.82%. We calculate the same table for the other
three states (c, hc), (ic, lc), and (ic, hc). Using
paired-sample t-tests with bonferroni corrections,
the only significant differences are observed be-
tween the random simulated corpus and each of
the other simulated corpora.
893
i-th Turn human Simulation Model Simulation Output CorrectlyPredictedActions ReciprocalRanking
Student1 ic 60% ic, 40% c ic 1 1
Student2 c 70% ic, 30% c ic 0 1/2
Average / / / (1+0)/2 (1+1/2)/2
Table 2: An Example of Comparing Simulated Actions with Human User Actions.
Figure 1: Comparison of human and simulated dialogs by high-level dialog features.
Tmore Tless M1 M2 Ran
s?c lc 24.82 31.42 25.64 22.70 13.25
w?c lc 17.64 12.35 16.62 18.85 9.74
s?ic lc 2.11 7.07 1.70 1.63 19.31
w?ic lc 1.80 2.17 2.05 3.25 21.06
s?c hc 29.95 26.46 22.23 31.04 10.54
w?c hc 13.93 9.50 22.73 15.10 11.29
s?ic hc 5.52 2.51 4.29 0.54 7.13
w?ic hc 4.24 9.08 4.74 6.89 7.68
Table 3: Comparisons of MDP transition proba-
bilities at state (c, lc) (Numbers in this table are
percentages).
Tmore Tless M1 M2 Ran
ECR 15.10 11.72 15.24 15.51 7.03
CI ?2.21 ?1.95 ?2.07 ?3.46 ?2.11
Table 4: Comparisons of ECR of learned dialog
strategies.
We also use a MDP toolkit to learn dialog strate-
gies from all the simulated corpora and then com-
pute the Expected Cumulative Reward (ECR) for
the learned strategies. In Table 4, the upper part
of each cell shows the ECR of the learned dialog
strategy; the lower part of the cell shows the 95%
Confidence Interval (CI) of the ECR. We can see
from the overlap of the confidence intervals that
the only significant difference is observed between
the dialog strategy trained from the random simu-
lated corpus and the strategies trained from each
of the other simulated corpora. Also, it is inter-
esting to see that the CI of the two manual simu-
lations overlap more with the CI of Tmore model
than with the CI of the Tless model.
In sum, the manual user simulations work as
well as the trained user simulation when being
used to generate a training corpus to apply MDPs
to learn new dialog strategies.
Tmore Tless M1 M2 Ran
Accu- 0.78 0.60 0.70 0.72 0.41
racy (?0.01) (?0.02) (?0.02) (?0.02) (?0.02)
MRR 0.72 0.52 0.63 0.64 0.32(?0.02) (?0.02) (?0.02) (?0.01) (?0.02)
Table 5: Comparisons of correctly predicted hu-
man user actions.
5.3 Comparisons in Dialog System
Evaluation
Finally, we compare how accurately the user sim-
ulations can predict human user actions given the
same dialog context. Table 5 shows the averages
and CIs (in parenthesis) from the four fold cross
validations. The second row shows the results
based on direct comparisons with human user ac-
tions, and the third row shows the mean recipro-
cal ranking of simulated actions. We observe that
in terms of both the accuracy and the reciprocal
ranking, the performance ranking from the high-
est to the lowest (with significant difference be-
tween adjacent ranks) is: the Tmore Model, both
of the manual models (no significant differences
between these two models), the Tless Model, and
the Ran Model. Therefore, we suggest that the
handcrafted user simulation is not sufficient to be
used in evaluating dialog systems because it does
not generate user actions that are as similar to hu-
man user actions. However, the handcrafted user
simulation is still better than a user simulation
trained with not enough training data. This re-
sult also indicates that this evaluation measure has
more differentiating power than the previous mea-
sures since it captures significant differences that
are not shown by the previous measures.
In sum, the Tmore simulation performs the best
in predicting human user actions.
894
6 Conclusion and Future Work
Setting up user action probabilities in user sim-
ulation is a non-trivial task, especially when no
training data or only a small amount of data is
available. In this study, we compare several ap-
proaches in setting up user action probabilities
for the same simulation model: training from all
available human user data, training from half of
the available data, two handcrafting approaches
which use the same expert knowledge but differ
slightly in individual probability assignments, and
a baseline approach which randomly assigns all
user action probabilities. We compare the built
simulations from different aspects. We find that
the two trained simulations and the two hand-
crafted simulations outperform the random simu-
lation in all tasks. No significant difference is ob-
served among the trained and the handcrafted sim-
ulations when comparing their generated corpora
on corpus-level dialog features as well as when
serving as the training corpora for learning dialog
strategies. However, the simulation trained from
all available human user data can predict human
user actions more accurately than the handcrafted
simulations, which again perform better than the
model trained from half of the human user corpus.
Nevertheless, no significant difference is observed
between the two handcrafted simulations.
Our study takes a first step in comparing the
choices of handcrafting versus training user simu-
lations when only limited or even no training data
is available, e.g., when constructing a new dialog
system. As shown for our task setting, both types
of user simulations can be used in generating train-
ing data for learning new dialog strategies. How-
ever, we observe (as in a prior study by (Schatz-
mann et al, 2007b)) that the simulation trained
from more user data has a better chance to outper-
form the simulation trained from less training data.
We also observe that a handcrafted user simulation
with expert knowledge can reach the performance
of the better trained simulation. However, a cer-
tain level of expert knowledge is needed in hand-
crafting user simulations since a random simula-
tion does not perform well in any tasks. Therefore,
our results suggest that if an expert is available for
designing a user simulation when not enough user
data is collected, it may be better to handcraft the
user simulation than training the simulation from
the small amount of human user data. However,
it is another open research question to answer how
much data is enough for training a user simulation,
which depends on many factors such as the com-
plexity of the user simulation model. When using
simulations to test a dialog system, our results sug-
gest that once we have enough human user data, it
is better to use the data to train a new simulation
to replace the handcrafted simulation.
In the future, we will conduct follow up stud-
ies to confirm our current findings since there are
several factors that can impact our results. First
of all, our current system mainly distinguishes the
student answers as correct and incorrect. We are
currently looking into dividing the incorrect stu-
dent answers into more categories (such as par-
tially correct answers, vague answers, or over-
specific answers) which will increase the number
of simulated user actions. Also, although the size
of the human corpus which we build the trained
user simulations from is comparable to other stud-
ies (e.g., (Rieser and Lemon, 2006), (Schatzmann
et al, 2007b)), using a larger human corpus may
improve the performance of the trained simula-
tions. We are in the process of collecting another
corpus which will consist of 60 human users (300
dialogs). We plan to re-train a simulation when
this new corpus is available. Also, we would be
able to train more complex models (e.g., a simula-
tion model which takes into account a longer dia-
log history) with the extra data. Finally, although
we add some noise into the current manual simula-
tion designed by our domain expert to account for
variations of expert knowledge, we would like to
recruit another human expert to construct a new
manual simulation to compare with the existing
simulations. It would also be interesting to repli-
cate our experiments on other dialog systems to
see whether our observations will generalize. Our
long term goal is to provide guidance of how to ef-
fectively build user simulations for different dialog
system development tasks given limited resources.
Acknowledgments
The first author is supported by Mellon Fellow-
ship from the University of Pittsburgh. This work
is supported partially by NSF 0325054. We thank
K. Forbes-Riley, P. Jordan and the anonymous re-
viewers for their insightful suggestions.
References
H. Ai and D. Litman. 2006. Comparing Real-Real,
Simulated-Simulated, and Simulated-Real Spoken
895
Dialogue Corpora. In Proc. of the AAAI Workshop
on Statistical and Empirical Approaches for Spoken
Dialogue Systems.
H. Ai and D. Litman. 2007. Knowledge Consistent
User Simulations for Dialog Systems. In Proc. of
Interspeech 2007.
H. Ai, J. Tetreault, and D. Litman. 2007. Comparing
User Simulation Models for Dialog Strategy Learn-
ing. In Proc. of NAACL-HLT 2007.
H. Cen, K. Koedinger and B. Junker. 2006. Learn-
ing Factors Analysis-A General Method for Cogni-
tive Model Evaluation and Improvement. In Proc. of
8th International Conference on ITS.
S. Craig, A. Graesser, J. Sullins, and B. Gholson. 2004.
Affect and learning: an exploratory look into the
role of affect in learning with AutoTutor. Journal
of Educational Media 29(3), 241250.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc. of Interspeech
2005.
K. Georgila, M. Wolters, and J. Moore. 2008. Simu-
lating the Behaviour of Older versus Younger Users
when Interacting with Spoken Dialogue Systems. In
Proc. of 46th ACL.
S. Janarthanam and O. Lemon. 2008. User simulations
for online adaptation and knowledge-alignment in
Troubleshooting dialogue systems. In Proc. of the
12th SEMdial Workshop on on the Semantics and
Pragmatics of Dialogues.
O. Lemon and X. Liu. 2007. Dialogue Policy Learn-
ing for combinations of Noise and User Simulation:
transfer results. In Proc. of 8th SIGdial.
D. Litman and S. Silliman. 2004. ITSPOKE: An Intel-
ligent Tutoring Spoken Dialogue System. In Com-
panion Proc. of the Human Language Technology:
NAACL.
R. Lo?pez-Co?zar, A. De la Torre, J. C. Segura and A.
J. Rubio. 2003. Assessment of dialogue systems by
means of a new simulation technique. Speech Com-
munication (40): 387-407.
T. Paek. 2006. Reinforcement learning for spo-
ken dialogue systems: Comparing strengths and
weaknesses for practical deployment. In Proc.
of Interspeech-06 Workshop on ?Dialogue on Dia-
logues - Multidisciplinary Evaluation of Advanced
Speech-based Interacive Systems?.
D. Radev, H. Qi, H. Wu, and W. Fan. 2002. Evaluating
web-based question answering systems. In Proc. of
LREC 2002.
V. Rieser and O. Lemon. 2006. Cluster-based User
Simulations for Learning Dialogue Strategies. In
Proc. of Interspeech 2006.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. of 6th
SIGDial.
J. Schatzmann, K. Weilhammer, M. Stuttle, and S.
Young. 2006. A Survey of Statistical User Simula-
tion Techniques for Reinforcement-Learning of Di-
alogue Management Strategies. Knowledge Engi-
neering Review 21(2): 97-126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007a. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc. of HLT/NAACL 2007.
J. Schatzmann, B. Thomson and S. Young. 2007b. Sta-
tistical User Simulation with a Hidden Agenda. In
Proc. of 8th SIGdial.
J. Tetreault and D. Litman. 2008. A Reinforcement
Learning Approach to Evaluating State Representa-
tions in Spoken Dialogue Systems. Speech Commu-
nication (Special Issue on Evaluating new methods
and models for advanced speech-based interactive
systems), 50(8-9): 683-696.
K. VanLehn, P. Jordan, C. Rose?, D. Bhembe, M.
Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R.
Srivastava, and R. Wilson. 2002. The architecture
of Why2-Atlas: A coach for qualitative physics es-
say writing. In Proc. Intelligent Tutoring Systems
Conference..
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language 21(2): 231-
422.
896
A Comparison of Tutor and Student Behavior in Speech Versus Text Based
Tutoring
Carolyn P. Rose?, Diane Litman, Dumisizwe Bhembe, Kate Forbes, Scott Silliman,
Ramesh Srivastava, Kurt VanLehn
Learning Research and Development Center, University of Pittsburgh,
3939 O?Hara St., Pittsburgh, PA 15260
rosecp,bhembe,forbesk,scotts,rcsriva@pitt.edu
litman,vanlehn@cs.pitt.edu
Abstract
This paper describes preliminary work in ex-
ploring the relative effectiveness of speech ver-
sus text based tutoring. Most current tuto-
rial dialogue systems are text based (Evens et
al., 2001; Rose and Aleven, 2002; Zinn et
al., 2002; Aleven et al, 2001; VanLehn et al,
2002). However, prior studies have shown con-
siderable benefits of tutoring through spoken
interactions (Lemke, 1990; Chi et al, 1994;
Hausmann and Chi, 2002). Thus, we are cur-
rently developing a speech based dialogue sys-
tem that uses a text based system for tutoring
conceptual physics (VanLehn et al, 2002) as
its ?back-end?. In order to explore the relative
effectiveness between these two input modal-
ities in our task domain, we have started by
collecting parallel human-human tutoring cor-
pora both for text based and speech based tu-
toring. In both cases, students interact with the
tutor through a web interface. We present here
a comparison between the two on a number
of features of dialogue that have been demon-
strated to correlate reliably with learning gains
with students interacting with the tutor using
the text based interface (Rose? et al, submitted).
1 Introduction
This paper describes preliminary work in exploring the
relative effectiveness of speech versus text based tuto-
rial dialogue systems. Tutorial dialogue is a natural way
to provide students with a learning environment that ex-
hibits characteristics that have been shown to correlate
with student learning gains, such as student activity. For
example, it has been demonstrated that generating words
rather than simply reading them promotes subsequent re-
call of those words (Slamecka and Graf, 1978). (Chi
et al, 1994) notes that there is a general momentum in
the science education literature toward the importance
of talking, reflecting and explaining as ways to learn
(Lemke, 1990). Moreover, encouraging student self-
explanation, which includes both generating inferences
from material they have read and relating new material to
old material, has been shown to correlate with learning
(Chi et al, 1981; Chi et al, 1994; Renkl, 1997; Press-
ley et al, 1992). In a further study, prompting students
with zero content prompts to encourage them to self-
explain was also associated with student learning (Chi et
al., 2001). A second important advantage to dialogue is
that it affords the tutor the opportunity to tailor instruc-
tion to the needs of the student. While human tutors may
not always choose to tailor their instruction to the indi-
vidual characteristics of the knowledge state of their stu-
dents, tutors who ignore signs of student confusion may
run the risk of preventing learning (Chi, 1996). (Rose? et
al., submitted) explore the benefits of tutor adaptation by
comparing learning gains for naive learners and review
learners in a human tutoring condition and a non-adaptive
reading condition.
In recent years tutorial dialogue systems have be-
come more and more prevalent, most of which are text
based (Evens et al, 2001; Rose and Aleven, 2002; Zinn
et al, 2002; Aleven et al, 2001; VanLehn et al, 2002).
Many of these systems have yielded successful evalu-
ations with students (Rose? et al, 2001; Heffernan and
Koedinger, 2002; Ashley et al, 2002; Graesser et al,
2001a). However, while the majority of current tutorial
dialogue systems are text based, there is reason to believe
that speech based tutorial dialogue systems could be more
effective.
Prior studies have shown considerable benefits of
human-human tutoring through spoken interactions
(Lemke, 1990; Chi et al, 1994). (Hausmann and Chi,
2002) has shown that spontaneous self-explanation oc-
curs much more frequently in spoken tutoring then in
text based tutoring, suggesting that typing requires ad-
ditional cognitive capacity and thus reduces the cogni-
tive resources available for spontaneous self-explanation.
Other research projects (Mostow and Aist, 2001; Fry
et al, 2001) have shown that basic spoken natural lan-
guage capabilities can be implemented quite effectively
in computer tutoring systems. Moreover, speech con-
tains prosodic and acoustic information which has been
shown to improve the accuracy of predicting emotional
states (Ang et al, 2002; Batliner et al, 2000) and user
responses to system errors (Litman et al, 2001) that are
useful for triggering system adaptation. We are thus cur-
rently developing a speech based dialogue system that
uses a text based system (VanLehn et al, 2002) as its
?back-end?. These systems and their goals will be dis-
cussed in Section 2.
We expect that the different modalities used by these
systems (e.g. text based vs speech based) will display
interesting differences with respect to the characteristics
of dialogue interaction that may determine their relative
merits with respect to increasing student performance.
Although human-computer data from the speech based
system is not yet available for comparison, we have col-
lected parallel human-human corpora both for text based
and speech based tutoring, as discussed in Sections 3-4,
and these corpora already display similarities and differ-
ences with respect to features of their dialogue interac-
tions, as discussed in Section 5, that are wholly modality
based and that will likely be displayed to an even greater
extent in the comparable human-computer data.
2 Why2-Atlas and ITSPOKE Dialogue
Systems
Why2-Atlas is a text based intelligent tutoring dialogue
system (Rose? et al, 2002a; VanLehn et al, 2002). The
goal of Why2-Atlas is to provide a platform for testing
whether deep approaches to natural language process-
ing elicit more learning than shallower approaches, for
the task domain of qualitative physics explanation gen-
eration. Using Why2-Atlas, the activity in which stu-
dents engage is answering deep reasoning questions in-
volving topics in conceptual physics. One such question
that we used is, ?A lightweight car and a massive truck
have a head-on collision. On which vehicle is the im-
pact force greater? Which vehicle undergoes the greater
change in its motion? Explain.? This is an appropriate
task domain for pursuing questions about the benefits of
tutorial dialogue for learning because questions like this
one are known to elicit robust, persistent misconceptions
from students, such as ?heavier objects exert more force.?
(Hake, 1998; Halloun and Hestenes, 1985). We designed
a set of 10 essay questions to use as training problems.
Two physics professors and a computer science profes-
sor worked together to select a set of expectations (i.e.,
correct propositions that the tutors expected students to
include in their essays) and potential misconceptions as-
sociated with each question. Additionally, they agreed
on an ideal essay answer for each problem. In Why2-
Atlas, a student first types an essay answering a qualita-
tive physics problem. A computer tutor then engages the
student in a natural language dialogue to provide feed-
back, correct misconceptions, and to elicit more complete
explanations. The first version of Why2-Atlas was de-
ployed and evaluated with undergraduate students in the
spring of 2002; the system is continuing to be actively
developed (Graesser et al, 2002).
We are currently developing a speech-enabled version
of Why2-ATLAS, called ITSPOKE (Intelligent Tutor-
ing SPOKEn dialogue system), that uses the Why2-Atlas
system as its ?back-end?. To date we have interfaced
the Sphinx2 speech recognizer (Huang et al, 1993) with
stochastic language models trained from example user ut-
terances, and the Festival speech synthesizer (Black and
Taylor, 1997) for text-to-speech, to the Why2-Atlas back-
end. The rest of the needed natural language process-
ing components, e.g. the sentence-level syntactic and
semantic analysis modules (Rose?, 2000), discourse and
domain level processors (Makatchev et al, 2002), and a
finite-state dialogue manager (Rose? et al, 2001), are pro-
vided by a toolkit that is part of the Why2-Atlas back-
end. The student speech is digitized from microphone
input, while the tutor?s synthesized speech is played to
the student using a speaker and/or headphone. We are
now in the process of adapting the knowledge sources
needed by the spoken language components to our ap-
plication domain. For example, we have developed a set
of dialogue dependent language models using the exper-
imental human-computer typed corpus (4551 student ut-
terances) obtained during the Why2-Atlas 2002 evalua-
tion. Our language models will soon be enhanced using
student utterances from our parallel human-human spo-
ken language corpus.
One goal of the ITSPOKE system is simply replacing
text based dialogue interaction with spoken dialogue in-
teraction and leaving the rest of the Why2-Atlas back-end
unchanged, in order to test the hypothesis that student
self-explanation (which leads to greater learning (Haus-
mann and Chi, 2002)) might be easier to achieve in spo-
ken dialogues. This hypothesis is discussed further in
Section 5. Although not the focus of this paper, an-
other goal of the ITSPOKE system is to take full ad-
vantage of the speech modality. For example, speech
contains rich acoustic and prosodic information about
the speaker?s current emotional state that isn?t present
in typed dialogue. Connections between learning and
emotion have been well documented (Coles, 1999), so it
seems likely that the success of computer-based tutoring
systems could be greatly increased if they were capable
of predicting and adapting to student emotional states,
e.g. reinforcing positive states, while rectifying nega-
tive states (Evens, 2002). Preliminary machine learn-
ing experiments involving emotion annotation and au-
tomatic feature extraction from our corpus suggest that
ITSPOKE can indeed be enhanced to automatically pre-
dict and adapt to student emotional states (Litman et al,
2003).
3 Typed Human-Human Tutoring Corpus
The Why2-Atlas Human-Human Typed Tutoring Cor-
pus is a collection of typed tutoring dialogues between
(human) tutor and student collected via typed interface,
which the tutor plays the same role that Why2-Atlas
is designed to perform. The experimental procedure is
as follows: 1) students are given a pretest measuring
their knowledge of physics, 2) students are asked to read
through a small document of background material, 3) stu-
dents work through a set of up to 10 Why2-Atlas training
problems with the human tutor, and 4) students are given
a post-test that is similar to the pretest. The entire ex-
periment takes no more than 15 hours per student, and is
usually performed in 1-3 sessions of no more than 4 hours
each. Data collection began in the Fall 2002 semester and
is continuing in the Spring 2003 semester. The subjects
are all University of Pittsburgh students who have never
taken any college physics courses. One tutor currently
participates in the study.
As in the Why2-Atlas system, when the tutoring ses-
sion then begins, the student first types an essay answer-
ing a qualitative physics problem. Once the student sub-
mits his/her essay, the tutor then engages the student in a
typed natural language dialogue to provide feedback and
correct misconceptions, and to elicit more complete ex-
planations. This instruction is in the form of a dialogue
between the student and the tutor through a text based
chat interface with student and tutor in separate rooms.
At key points in the dialogue, the tutor asks the student
to revise the essay. This cycle of instruction and revision
continues until the tutor is satisfied with the student?s es-
say. A sample tutoring dialogue from the Why2-Atlas
typed human-human tutoring corpus is displayed in Fig-
ure 1.
The tutor was instructed to cover the expectations for
each problem, to watch for the specific set of expectations
and misconceptions associated with the problem, and to
end the discussion of each problem by showing the ideal
essay to the student. He was encouraged to avoid lectur-
ing the student and to attempt to draw out the student?s
own reasoning. He knew that transcripts of his tutoring
would be analyzed. Nevertheless, he was not required to
follow any prescribed tutoring strategies. So his tutoring
style was much more naturalistic than in previous stud-
ies such as the BEE study (Rose? et al, 2001) in which
two specific tutoring styles, namely Socratic and Didac-
tic, were contrasted. The results of that study revealed a
trend for students in the Socratic condition to learn more
than those in the Didactic condition. A further analysis
of the corpus collected during the BEE study (Core et
al., 2002) verified that the Socratic dialogues from the
BEE study were more interactive than the Didactic ones.
The biggest reliable difference between the two sets of
tutoring dialogues was the percentage of words spoken
by the student, i.e, number of student words divided by
total number of words. The Didactic dialogues contained
on average 26% student words, whereas the Socratic di-
alogues contained 33% student words. On average with
respect to percentage of student words, the dialogues in
our text based human tutoring corpus were more like the
Didactic dialogues from the BEE study, with average per-
centage of student text being 27%. Nevertheless, because
the tutor was not constrained to follow a prescribed tutor-
ing style, the level of interactivity varied widely through-
out the transcripts, at times being highly Socratic, and at
other times being highly Didactic.
Pre and post tests were used to measure learning gains
to be used for evaluating the effectiveness of various fea-
tures of tutorial dialogue found in our corpora. Thus, we
developed two tests: versions A and B, which were iso-
morphic to one another. That is, the problems on test A
and B differed only in the identities of the objects (e.g.,
cars vs. trucks) and other surface features that should not
affect the reasoning required to solve them. Each ver-
sion of the test (A and B) consisted of 40 multiple choice
questions. Each multiple choice question was written to
address a single expectation covered in the training prob-
lems. Some students were not able to complete all 10
problems before they reached the end of their participa-
tion time. Thus, they took the post-test after only working
through a subset of the training problems.
4 Spoken Human-Human Tutoring
Corpus
The ITSPOKE Human-Human Spoken Tutoring Corpus
is a parallel collection of spoken tutoring dialogues col-
lected via a web interface supplemented with a high qual-
ity audio link, where a human tutor performs the same
task that our ITSPOKE system is being designed to per-
form. The experimental procedure used to collect the cor-
pus is exactly the same as the procedure used to gather
the Why2-Atlas Human-Human Corpus: the same tutor
is used, the same subject pool1 is used, the same pre-test
and post-test are used, and the same set of physics prob-
1We assigned a greater percentage of students to the text
based condition as part of a separate experiment. Thus, the text
based corpus is larger than the speech based corpus.
PROBLEM: Suppose that you released 3 identical balls of clay in a vacuum at exactly the same instant. They would all hit the
ground at the same instant. Now you stick two of the balls together, forming one ball that is twice as heavy as the remaining,
untouched clay ball. Both balls are released in a vacuum at exactly the same instant. Which ball hits the ground first?
ESSAY: Both balls will hit the ground at the same time. The balls are in free fall (only gravitational forces). The ratio of the
masses and weight are equal.
. . . excerpt from 2 minutes into a typed dialogue . . .
Tutor1: You have correctly said the balls are in free fall. What do you conclude from this fact?
Student1: There is no air resistance and the balls? accelerations are constant in regards to one another
Tutor2: Right, so the conclusion is that they both have the same acceleration, the free fall acceleration. Now you have to show
that time taken by them to reach the ground will be the same. How do you do that?
Student2: F (net forces) / mass = f/m because F = m*a therefore ratio should equal one another
Tutor3: But that does not tell you anything about the time taken. What physical quantities will determine the time taken. Will the
initial velocity of the balls matter?
Student3: yes, assuming starting at rest? sorry, kind of at a loss at the moment
Tutor4: The initial velocity will matter, and here they both have the same initial velocity, zero. What else, will the height through
wich they fall matter?
Student4: not so much as long as they are both dropped from the same height
Tutor5: Height of fall will determine the time of fall, and here the height of fall is the same for both the balls. Now on the basis of
these facts can you conclude about the time taken by them to reach the ground?
Student5: since initial velocities are equal and the height of falls are equal, then the time taken is equal
Tutor6: How about acceleration, if they had different accelerations, even if they fall through the same height and have same initial
velocity , will they reach the ground at the same time?
Student6: no...
Tutor7: Right...
Figure 1: Excerpt from Human-Human Typed Dialogue Corpus
lems are used. Only the modality differs. In particular,
once the tutoring session begins and the student submits
his/her typed essay, the tutor and student then discuss the
student?s essay using spoken English. In contrast to the
text condition, where strict turn-taking is enforced, in the
spoken condition, interruptions and overlapping speech
are common. An example excerpt from the corpus is
shown in Figure 2. Note that turns ending in ?-? indi-
cate speech overlapping with the following turn. Even-
tually, the student will edit his/her typed explanation. As
in the text condition, the tutor then either expresses satis-
faction and ends the tutoring for the current problem, or
continues with another round of spoken dialogue interac-
tion and typed essay revision. As in the text condition,
students are presented with the ideal essay answer for a
problem upon completing that problem.
5 Differences between Typed and Spoken
Human-Tutoring
(Rose? et al, submitted) presents an analysis to uncover
which aspects of the tutorial dialogue were responsible
for its effectiveness in the text based condition. Longer
student answers to tutor questions reveal more of a stu-
dent?s reasoning. Very short answers, i.e., 10 words or
less, are normally composed of a single clause at most.
Longer, multi-clausal answers have the potential to com-
municate many more inter-connections between ideas.
Thus, if a tutor is attending to and responding directly
to the student?s revealed knowledge state, it would be
expected that the effectiveness of the tutor?s instruction
would increase as average student turn length increases.
To test this prediction, we computed a linear regression
of the sequence of student turn lengths over time for each
student in the text based condition in order to obtain an in-
tercept and a slope, since student turn lengths have been
observed to decline on average over the course of their
PROBLEM: If a car is able to accelerate at 2 m/s2, what acceleration can it attain if it is towing another car of equal mass?
ESSAY: If the car is towing another car of equal mass, the maximum acceleration would be the same because the car would be
towed behind and the friction caused would only be by the front of the first car.
. . . excerpt from 6.5 minutes into spoken dialogue . . .
Tutor1: So twice the mass multiplied by the acceleration should be equal to the force which you have already determined as the
mass of the first car times the acceleration. So essentially you are dividing it by two and that gives you the uh acceleration by
twice the because mass has become twice. Now this law that force is equal to mass times acceleration, what?s this law called?
This is uh since this it is a very important basic uh fact uh it is it is a law of physics. Um you have you have read it in the
background material. Can you recall it?
Student1: Um no it was one of Newton?s laws-
Tutor2: Right, right-
Student2: but I don?t remember which one. (laugh)
Tutor3: That-
Student3: he I-
Tutor4: is Newton?s second law of motion.
Student4: Ok, because I remember one, two, and three, but I didn?t know if there was a different name
Tutor5: Yeah that?s right you know Newton was a genius and-
Student5: (laugh)-
Tutor6: uh he looked at a large number of experiments and experimental data that was available and from that he could come to
this general law and it is known as Newton?s second law of motion. Um many many other scientists before him had seen all
this data which was collected by scientists but had not concluded this. Now it looks very simple but to come to a conclusion
from a mass of data was something which required the genius of Newton.
Student6: mm hm
Tutor7: So now you will give Newton full credit isn?t it? (laugh)
Student7: (laugh)
Figure 2: Excerpt from Human-Human Spoken Dialogue Corpus.
interaction with the turn. We then computed a multiple
regression with pre-test score, intercept, and gradient as
independent variables and post test score as the depen-
dent variable. We found a reliable correlation between
intercept and learning, with pre-test scores and gradients
regressed out (R=.836; p   .05). This result is consistent
with (Core et al, 2002) where percentage of student talk
is strongly correlated with learning. Consistent with this,
we found a strong and reliable correlation between ra-
tio of student words to tutor words and learning2. We
computed a correlation between ratio of student words to
tutor words and post-test score after pre-test scores were
regressed out (R=.866, p   .05).
One of our current research objectives is to compare
2Note that ratio of student words to tutor words is number
of student words divided by number of tutor words, whereas
percentage of student words is number of student words divided
by total number of words
the relative effectiveness of speech based and text based
tutoring. Thus, when we have enough speech data, we
would like to compare learning gains between the speech
and text based conditions to test whether or not speech
based tutoring is more effective than text based tutoring.
We also plan to test whether the same features that cor-
relate with learning in the text based condition also cor-
relate with learning in the speech based condition. Since
both average student turn length and overall ratio of stu-
dent words to tutor words correlated strongly with learn-
ing gains in the text based condition, in this paper we
compare these two measures between the text based tutor-
ing condition and the speech based tutoring condition, but
not yet in connection with learning gains in the speech-
based corpus.
Since strict turn taking was not enforced in the speech
condition, turn boundaries were manually annotated
(based on consensus labellings from two coders) when ei-
ther (1) the speaker stopped speaking and the other party
in the dialogue began to speak, (2) when the speaker
asked a question and stopped speaking to wait for an an-
swer, or (3) when the other party in the dialogue inter-
rupted the speaker and the speaker paused to allow the
other party to speak.
Currently, 13 students have started the typed human-
human tutoring experiment, 7 of whom have finished.
We have so far collected 78 typed dialogues from the
text based condition, 69 of which were used in our anal-
ysis. 9 students have started the spoken human-human
tutoring experiment, 6 of whom have finished. Thus, we
have collected 63 speech based dialogues (1290 minutes
of speech from 4 female and 4 male subjects), and have
transcribed 25 of them. We hope to have an analysis cov-
ering all of our data in both conditions by the time of the
workshop.
As shown in Table 1, analysis of the data that has
been collected and transcribed to date is already show-
ing interesting differences between the ITSPOKE (spo-
ken) and WHY2-ATLAS (text) corpora of human-human
dialogues. The #trns columns show mean and standard
deviation for the total number of turns taken by the stu-
dents or tutor in each problem dialogue, while the next
pair of columns show the mean and standard deviation
for the total number of words spoken or typed by the stu-
dents or tutor (#wds) in each problem dialogue. The last
pair of columns show mean and standard deviation for the
average number of student or tutor words per turn in each
problem dialogue.
Due to the fact that data is still being collected for both
corpora (and the fact that the speech corpus also requires
manual transcription), the sizes of the two data sets rep-
resented in the table differ somewhat. However, even at
this early stage in the development of both corpora, these
figures already show that the style of the interactions are
very different in each modality. In particular, in spoken
tutoring, both student and tutor take more turns on av-
erage than in text based tutoring, but these spoken turns
are on average shorter. Moreover, in spoken tutoring both
student and tutor on average use more words to commu-
nicate than in text based tutoring. Another interesting dif-
ference is that although in the speech condition both stu-
dent and tutor take more turns, students finish the speech
condition in less time. In particular, on average, students
in the text based tutoring condition require 370.58 min-
utes to finish the training problems, with a standard devi-
ation of 134.29 minutes, students in the speech condition
require only 159.9 minutes on average, with a standard
deviation of 58.6 minutes. We measured the statistical re-
liability of the difference between the two measures that
correlated reliably with learning in the text-based condi-
tion. A 2-tailed unpaired t-test indicates that this differ-
ence is significant (t(30)=8.99, p   .01). There are also
similarities across the two conditions. In particular, a 2-
tailed unpaired t-test shows that the relative proportion
of student and tutor word or turns do not differ signifi-
cantly on average across the two modalities (t(13)=1.225,
p=.242). As an illustration, Table 2 shows mean and stan-
dard deviation for the ratios of the total number of student
and tutor words (#Swds/#Twds) and turns (#Strns/#Ttrns)
in each problem dialogue3.
Average student turn length is significantly lower in
the speech based condition (t(13) = 4.5, p   .001). This
might predict that speech based tutoring may be less ef-
fective than text based tutoring. However, since ratio of
student words to tutor words does not differ significantly,
this would predict that learning will also not differ sig-
nificantly between conditions. Total number of words
uttered in the speech condition is larger than in the text
based condition as are total number of turns. This dif-
ference between the two conditions will likely be even
more pronounced in the human-computer comparison,
due to noisy student input that results from use of au-
tomatic speech recognition. For example, clarifications
and corrections made necessary by this will likely lead
to an increase in dialogue length. More careful analy-
sis is required to determine whether this means that more
self-explanation took place overall in the speech based
condition. If so, this would predict that the speech based
condition would be more effective for learning than the
text based condition. Thus, much interesting exploration
is left to be done after we have collected enough speech
data to compute a reliable comparison between the two
conditions.
6 Current Directions
Currently we are continuing to collect data both in the
speech and text based human tutoring conditions. Since
human tutors differ with each other with respect to both
their tutoring styles and their conversational styles, we
plan to collect data using several different human tutors
in order to test the robustness of our comparisons between
speech and text based human tutoring. Another possible
direction for further inquiry would be to contrast natural-
istic speech (where strict turn taking is not enforced, as
in this data collection effort), with a speech condition in
which strict turn taking is enforced, in order to separate
the effects of speech on learning from the effects of alter-
native turn taking policies.
As discussed in Section 2, we are currently develop-
ing both text based and speech based human-computer
tutorial systems. Our ultimate goal is to test the relative
effectiveness of speech versus text based computer tutors.
We expect differences both between text and speech con-
3Note that strict-turn taking is enforced in the text condition,
but not in the speech condition.
Table 1: Student and Tutor Characteristics in Human-Human Speech and Text Conditions
#trns #wds Avg#wds/trn
Condition Participant Mean STD Mean STD Mean STD
Speech Student 47.49 25.95 264.18 125.47 5.72 1.35
Text Student 9.71 6.79 146.72 57.96 13.39 5.55
Speech Tutor 46.94 20.90 1199.14 605.87 26.78 14.20
Text Tutor 11.03 7.04 391.85 136.89 39.04 6.23
Table 2: Student-Tutor Word and Turn Ratios in Speech and Text Conditions
Speech Condition Text Condition
#Swds/#Twds #Strns/#Ttrns #Swds/#Twds #Strns/#Ttrns
Mean STD Mean STD Mean STD Mean STD
0.29 0.15 0.99 0.15 0.37 0.08 0.81 0.15
ditions in the human-computer data and between human-
human and human-computer data. One of our first tasks
will thus be to use the baseline version of ITSPOKE
described in Section 2 to generate a corpus of human-
computer spoken dialogues, using a process comparable
to the human-human corpus collection described here.
This will allow us to 1) compare the ITSPOKE human-
human and human-computer corpora 2) compare the IT-
SPOKE human-computer spoken corpus with a compa-
rable Why2-Atlas text corpus, e.g. by expanding on the
just described pilot study of the two human-human cor-
pora, and 3) use the ITSPOKE human-computer corpus
to guide the development of a new version of ITSPOKE
that will attempt to increase its performance, by taking
advantage of information that is only available in speech,
and modifying its behavior in other ways to respect the
interaction differences in item 2.
7 Acknowledgments
This research was supported by the Office of Naval Re-
search, Cognitive Science Division under grant number
N00014-0-1-0600 and by NSF grant number 9720359 to
CIRCLE.
References
V. Aleven, O. Popescu, and K. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In J. D. Moore, C. L. Redfield, and W. L. John-
son, editors, Proceedings of Articial Intelligence in
Education, pages 246?255.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP.
K. D. Ashley, R. Desai, and J. M. Levine. 2002. Teach-
ing case-based argumentation concepts using didactic
arguments vs. didactic explanations. In Proceedings
of the Intelligent Tutoring Systems Conference, pages
585?595.
A. Batliner, R. Huber, H. R. Niemann, E. No?th, J. Spilker,
and K. Fischer. 2000. The recognition of emotion. In
Proc. of the ISCA Workshop on Speech and Emotion.
A. Black and P. Taylor. 1997. Festival speech synthesis
system: system documentation (1.1.1). Human Com-
munication Research Centre Technical Report 83, Uni-
versity of Edinburgh.
M. Chi, N. de Leeuw, M. Chiu, and C. LaVancher.
1981. Eliciting self-explanations improves under-
standing. Cognitive Science, 18(3).
Michelene Chi, Nicholas De Leeuw, Mei-Hung Chiu, and
Christian Lavancher. 1994. Eliciting self-explanations
improves understanding. Cognitive Science, 18:439?
477.
M. T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi, and
R. G. Hausmann. 2001. Learning from human tutor-
ing. Cognitive Science, (25):471?533.
M. T. H. Chi. 1996. Learning processes in tutoring. Ap-
plied Cognitive Psychology, 10:S33?S49.
G. Coles. 1999. Literacy, emotions, and the brain. Read-
ing Online, March 1999.
M. Core, J. D. Moore, and C. Zinn. 2002. Initiative in
tutorial dialogue. In Proceedings of the ITS Workshop
on Empirical Methods for Tutorial Dialogue Systems,
pages 46?55.
M. Evens, S. Brandle, R. Chang, R. Freedman, M. Glass,
Y. H. Lee, L. S. Shim, C. W. Woo, Y. Zhang, Y. Zhou,
J. A. Michaeland, and A. A. Rovick. 2001. Circsim-
tutor: An intelligent tutoring system using natural lan-
guage dialogue. In Proceedings of the Twelfth Midwest
AI and Cognitive Science Conference, MAICS 2001,
pages 16?23, Oxford, OH.
M. Evens. 2002. New questions for Circsim-Tutor. Pre-
sentation at the 2002 Symposium on Natural Language
Tutoring, University of Pittsburgh.
J. Fry, M. Ginzton, S. Peters, B. Clark, and H. Pon-Barry.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proc. 2nd SigDial Work-
shop on Discourse and Dialogue.
A. Graesser, N. Person, and D. Harter et al 2001a.
Teaching tactics and dialog in Autotutor. International
Journal of Articial Intelligence in Education.
A. Graesser, K. Vanlehn, TRG, and NLT Group. 2002.
Why2 report: Evaluation of why/atlas, why/autotutor,
and accomplished human tutors on learning gains for
qualitative physics problems and explanations. Tech-
nical report, LRDC Tech Report, University of Pitts-
burgh.
R. R. Hake. 1998. Interactive-engagement versus tra-
ditional methods: A six-thousand student survey of
mechanics test data for introductory physics students.
American Journal of Physics, 66(64).
I. A. Halloun and D. Hestenes. 1985. The initial knowl-
edge state of college physics students. American Jour-
nal of Physics, 53(11):1043?1055.
Robert Hausmann and Michelene Chi. 2002. Can a com-
puter interface support self-explaining? The Interna-
tional Journal of Cognitive Technology, 7(1).
N. T. Heffernan and K. R. Koedinger. 2002. An intelli-
gent tutoring system incorporating a model of an expe-
rienced tutor. In Proceedings of the Intelligent Tutor-
ing Systems Conference, pages 596?608.
X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F.
Lee, and R. Rosenfeld. 1993. The SphinxII speech
recognition system: An overview. Computer, Speech
and Language.
J. L. Lemke. 1990. Talking Science: Language, Learn-
ing and Values. Ablex, Norwood, NJ.
D. Litman, J. Hirschberg, and M. Swerts. 2001. Predict-
ing user reactions to system error. In Proc.of ACL.
Diane Litman, Kate Forbes, and Scott Silliman. 2003.
Towards emotion prediction in spoken tutoring dia-
logues. Submitted.
M. Makatchev, P. Jordan, and K. VanLehn. 2002. Dis-
course processing for explanatory essays in tutorial ap-
plications. In Proceedings of the 3rd SIGdial Work-
shop on Discourse and Dialogue.
J. Mostow and G. Aist. 2001. Evaluating tutors that lis-
ten: An overview of Project LISTEN. In K. Forbus and
P. Feltovich, editors, Smart Machines in Education.
M. Pressley, E. Wood, V. E. Woloshyn, V. Martin,
A. King, and D. Menke. 1992. Encouraging mind-
ful use of prior knowledge: Attempting to construct
explanatory answers facilitates learning. Educational
Psychologist, 27:91?109.
A. Renkl. 1997. Learning from worked-out examples:
A study on individual differences. Cognitive Science,
21(1):1?29.
C. P. Rose and V. Aleven. 2002. Proc. of the ITS 2002
workshop on empirical methods for tutorial dialogue
systems. Technical report, San Sebastian, Spain, June.
C. P. Rose?, J. D. Moore, K. VanLehn, and D. Allbritton.
2001. A comparative evaluation of socratic versus di-
dactic tutoring. In Proceedings of the 23rd Annual
Conference of the Cognitive Science Society, pages
869?874.
C. P. Rose?, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. Interactive conceptual
tutoring in atlas-andes. In Proceedings of Articial In-
telligence in Education, pages 256?266.
C. P. Rose?, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. Vanlehn. 2002a. A hybrid language un-
derstanding approach for robust selection of tutoring
goals. In Proceedings of the Intelligent Tutoring Sys-
tems Conference, pages 552?561.
C. P. Rose?, K. VanLehn, and The Natural Language Tu-
toring Group. Submitted. Is human tutoring always
more effective than reading. In Annual Meeting of the
Cognitive Science Society.
C. P. Rose?. 2000. A framework for robust sentence
level interpretation. In Proceedings of the First Meet-
ing of the North American Chapter of the Association
for Computational Linguistics, pages 1129?1135.
N. J. Slamecka and P. Graf. 1978. The generation ef-
fect: Delineation of a phenomenon. Journal of Exper-
imental Psychology: Human Learning and Memory,
(4):592?604.
K. VanLehn, P. Jordan, C. Rose?, D. Bhembe, M. Bo?ttner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, R. Srivastava, and R. Wil-
son. 2002. The architecture of Why2-Atlas: A coach
for qualitative physics essay writing. In Proc. of ITS.
Claus Zinn, Johanna D. Moore, and Mark G. Core. 2002.
A 3-tier planning architecture for managing tutorial di-
alogue. In Proceedings Intelligent Tutoring Systems,
Sixth International Conference (ITS 2002), Biarritz,
France, June.
Exceptionality and Natural Language Learning 
Mihai Rotaru Diane J. Litman 
Computer Science Department 
University of Pittsburgh 
Pittsburgh, PA 15260 
mrotaru, litman @cs.pitt.edu 
 
 
Abstract 
Previous work has argued that memory-based 
learning is better than abstraction-based learn-
ing for a set of language learning tasks. In this 
paper, we first attempt to generalize these re-
sults to a new set of language learning tasks 
from the area of spoken dialog systems and to 
a different abstraction-based learner. We then 
examine the utility of various exceptionality 
measures for predicting where one learner is 
better than the other. Our results show that 
generalization of previous results to our tasks 
is not so obvious and some of the exceptional-
ity measures may be used to characterize the 
performance of our learners. 
1 Introduction 
2 
                                                          
Our paper is a follow-up of the study done by Daele-
mans et al (1999) in which the authors show that keep-
ing exceptional training instances is useful for 
increasing generalization accuracy when natural lan-
guage learning tasks are involved. The tasks used in 
their experiments are: grapheme-phoneme conversion, 
part of speech tagging, prepositional phrase attachment 
and base noun phrase chunking. Their study provides 
empirical evidence that editing exceptional instances 
leads to a decrease in memory-based learner perform-
ance. Next, the memory-based learner is compared on 
the same tasks with a decision-tree learner and their 
results favor the memory-based learner. Moreover, the 
authors provide evidence that the performance of their 
memory-based learner is linked to its property of hold-
ing all instances (including exceptional ones) and gen-
eral properties of language learning tasks (difficultness 
in discriminating between noise and valid exceptions 
and sub-regularities for those tasks). 
We continue on the same track by investigating if 
their results hold on a different set of tasks. Our tasks 
come from the area of spoken dialog systems and have 
smaller datasets and more features (with many of the 
features being numeric, in contrast with the previous 
study that had none). We observe in our experiments 
with these tasks a much smaller exceptionality measure 
range compared with the previous study. Our results 
indicate that the previous results do not generalize to all 
our tasks. 
An additional goal of our research is to investigate a 
new topic by looking into whether exceptionality meas-
ures can be used to characterize the performance of our 
learners: a memory-based learner (IB1-IG) and a rule-
based learner (Ripper). Our results indicate that for 
some of the exceptionality measures we will examine, 
IB1-IG is better for predicting typical instances while 
Ripper is better for predicting exceptional instances. 
We will use the following conventions throughout 
the paper. The term ?exceptional? will be used to label 
instances that do not follow the rules that characterize 
the class they are part of (in language learning terms, 
they are ?bad? examples of their class rules). We will 
use ?typical? as the antonym of this term; it will label 
instances that are good examples of their class rules. 
The fact that an instance is typical should not be con-
fused with an exceptionality measure we will use that 
has the same name (typicality measure). 
Learning methods 
We will use in our study the same memory-based 
learner that was used in the previous study: IB1-IG. The 
abstraction-based learner used in the previous study was 
C5.0 (a commercial implementation of the C4.5 deci-
sion tree learner). In our study we will use a rule-based 
learner, Ripper. Although the two abstraction-based 
learners are different, they share many features (many 
techniques used in rule-based learning have been 
adapted from decision tree learning (Cohen, 1995))1.  
1 We used Ripper because its implementation was available 
and previous studies on our language learning tasks were per-
formed using Ripper  
2.1 IB1-IG 
Our memory-based learner is called IB1-IG and is part 
of TiMBL, a software package developed by the ILK 
Research Group, Tilburg University and the CNTS Re-
search Group, University of Antwerp. TiMBL is a col-
lection of memory-based learners that sit on top of the 
classic k-NN classification kernel with added metrics, 
algorithms, and extra functions.  
Memory-based reasoning is based on the hypothesis 
that humans, in order to react to a new situation, first 
compare the new situation with previously encountered 
situations (which reside in their memory), pick one or 
more similar situations, and react to the new one based 
on how they reacted to those similar situations.  This 
type of learning is also called lazy learning because the 
learner does not build a model from the training data. 
Instead, typically, the whole training set is stored. To 
predict the class for a new instance, the lazy learner 
compares it with stored instances using a similarity met-
ric and the new instance class is determined based on 
the classes of the most similar training instances. At the 
algorithm level, lazy learning algorithms are versions of 
k-nearest neighbor (k-NN) classifiers. 
IB1-IG is a k-NN classifier that uses a weighted 
overlap metric, where a feature weight is automatically 
computed as the Information Gain (IG) of that feature. 
The weighted overlap metric for two instances X and Y 
is defined as: 
?
=
=?
n
i
iii yxwYX
1
),(),( ?  (1) 
where: 
ii
ii
ii
ii
ii
yx
yx
minmax
yx
abs
yx
?
=
???
???
?
?
?
=
 if
 if
else numeric, if
1
0
)(
),(?  
Information gain is computed for every feature in 
isolation by computing the difference in uncertainty 
between situations with or without knowledge of the 
feature value (for more information, see Daelemans et 
al., 2001). These values describe the importance of that 
feature in predicting the class of an instance and are 
used as feature weights. 
 
2.2 
3 
3.1 
Ripper 
Ripper is a fast and effective rule-based learner devel-
oped by William Cohen (Cohen, 1995). The algorithm 
has an overfit-and-simplify learning strategy: first an 
initial rule set is devised by overfitting a part of the 
training set (called the growing set) and then this rule 
set is repeatedly simplified by applying pruning opera-
tors and testing the error reduction on another part of the 
training set (called the pruning set). Ripper produces a 
model consisting of an ordered set of if-then rules. 
There are several advantages to using rule-based 
learners. The most important one is the fact that people 
can understand relatively easy the model learned by a 
rule-based learner compared with the one learned by a 
decision-tree learner, neural network or memory-based 
learner. Also, domain knowledge can be incorporated in 
a rule-based learner by altering the type of rules it can 
learn. Finally, rule-based learners are relatively good at 
filtering the potential noise from the training set. But in 
the context of natural language learning tasks where 
distinguishing between noise and exceptions and sub-
regularities is very hard, this filtering may result in a 
decrease in accuracy. In contrast, memory-based learn-
ers, by keeping all instances around (including excep-
tional ones), may have higher classification accuracy for 
such tasks. 
Exceptionality measures 
One of the main disadvantages of memory-based learn-
ing is the fact that the entire training set is kept. This 
leads to serious time and memory performance draw-
backs if the training set is big enough. Moreover, to 
improve accuracy, one may want to have noisy in-
stances present in the training set pruned. To address 
these problems there has been a lot of work on trying to 
edit part of the training set without hampering the accu-
racy of the predictor. Two types of editing can be done. 
One can edit redundant regular instances (because the 
training set contains a lot of similar instances for that 
class) and/or unproductive instances (the ones that pre-
sent irregularities with respect to the training set space). 
There are many measures that capture both types of 
instances. We will use the ones from the previous study 
(typicality and class prediction strength) and a new one 
called local typicality. Even though these measures were 
devised with the purpose of editing part of the training 
set, they are used in our study and the previous study to 
point out instances that should not be removed, at least 
for language learning tasks. 
Typicality 
We will use the typicality definition from Daelemans et 
al. (1999) which is similar to the definition from Zhang 
(1992). In both cases, a typicality function is defined 
whose extremes correspond to exceptional and typical 
instances. The function requires a similarity measure 
which is defined in both cases as the inverse of the dis-
tance between two instances. The difference between 
the two implementations of typicality is that Zhang 
(1992) defines the distance as the Euclidian distance 
while Daelemans et al (1999) use the normalized 
weighted Manhattan distance from (1). Thus, our simi-
larity measure will be defined as: 
?
=
?=
n
i
iii yxwYXsim
1
)),(1(),( ?  
For every instance X, a subset of the dataset called 
family of X, Fam(X), is defined as being all instances 
from the dataset that have the same class as X. All re-
maining instances form the unrelated instances subset, 
Unr(X). Then, intra-concept similarity is defined as the 
average similarity between X and instances from 
Fam(X) and inter-concept similarity as the average 
similarity between X and instances from Unr(X). 
?
=
=
|)(|
1
))(,(
|)(|
1)(
XFam
i
iXFamXsimXFam
XIntra  
?
=
=
|)(|
1
))(,(
|)(|
1)(
XUnr
i
iXUnrXsimXUnr
XInter  
Finally, typicality of an instance X is defined as the 
ratio of its intra-concept and inter-concept similarity.  
)(
)()(
XInter
XIntraXTypicality =  
The typicality values are interpreted as follows: if 
the value is higher than 1, then that instance has an in-
tra-concept similarity higher than inter-concept similar-
ity, thus one can say that the instance is a good example 
of its class (it is a typical instance). A value less than 1 
implies the opposite: the instance is not a good example 
of its class (it is an exceptional instance). Values around 
1 are called by Zhang boundary instances since they 
seem to reside at the border between concepts. 
3.2 
3.3 
Class prediction strength 
Another measure used in the previous study is the class 
prediction strength (CPS). This measure tries to capture 
the ability of an instance to predict correctly the class of 
a new instance. We will employ the same CPS defini-
tion used in the previous study (the one proposed by 
Salzberg (1990)). In the context of k-NN, predicting the 
class means, typically, that the instance is the closest 
neighbor for a new instance. Thus the CPS function is 
defined as the ratio of the number of times our instance 
is the closest neighbor for an instance of the same class 
and the number of times our instance is the closest 
neighbor for another instance regardless of its class. A 
CPS value of 1 means that if our instance is to influence 
another instance class (by being its closest neighbor) its 
influence is good (in the sense that predicting the class 
using our instance class will result in an accurate predic-
tion). Thus our instance is a good predictor for our class, 
i.e. it is a typical instance. In contrast, a value of 0 indi-
cates a bad predictor for the class and thus labels an 
exception instance. A value of 0.5 will correspond to 
instances at the border between concepts. 
Unlike typicality, when computing CPS, we can en-
counter situations when its value is undefined (zero di-
vided by zero). This means that the instance is not the 
closest neighbor for any other instance. Since there is no 
clear interpretation of instance properties in this case, 
we will set its CPS value to a constant higher than 1 (no 
particular meaning of the value, just to recognize it in 
our graphs).  
Local typicality 
While CPS captures information very close to an in-
stance, typicality as defined by Zhang captures informa-
tion from the entire dataset. But this may not be the 
most desirable measure in cases such as those when a 
concept is made of at least two disjunctive clusters. 
Consider the example from Figure 1. For an instance in 
the center of cluster A1, its similarity with instances 
from the same cluster is very high but very low with 
instances from cluster A2. At the same time, its similar-
ity with instances from class B is somewhere between 
abo in-
stan ave 
com
arou
clus
of F
stan
we 
inst
sets
ity, 
inst
valu
stan
thei
of m
vici
neig
to th
sets
nish
and
puti
Fve two values. When everything is averaged, 
ce intra-concept and inter-concept similarity h
parable values thus leading to a typicality value 
nd 1 even if the instance is highly typical for the 
ter A1. 
A1
To address this problem, we changed the definition 
am(X) and Unr(X). Instead of considering all in-
ces from the dataset when building the two subsets, 
will be using only instances from a vicinity of our 
ance. The typicality computed using these new sub-
 will be called local typicality. To define the vicin-
we used again the similarity metric. When two 
ances are identical, their similarity has the maximum 
e which is the sum of all feature weights. An in-
ce is in the vicinity of another instance if and only if 
r similarity has a value higher than a given percent 
aximum similarity value (using this definition of 
nity instead of a specified number of nearest 
hbors, makes our exceptionality measure adaptive 
e density of the local neighborhood). For our data-
, a percent value of 90% yields the best results fur-
ing a measure that is different from both typicality 
 CPS. 
Like CPS, division by zero can appear when com-
ng local typicality. This means that inter-concept 
B
A2 
igure 1. Class distribution that causes flattening in typicality 
distribution  
similarity is zero and this can only happen if there is no 
instance with a different class in the vicinity of our in-
stance. In this case, if the intra-concept similarity is 
higher than 0 (there is at least one instance from the 
same class in the vicinity) we set the local typicality to a 
maximum value, while if the intra-concept similarity is 
0, then we set the typicality to a minimum value (no one 
in the vicinity of this instance is a good indication of an 
exceptional instance). When inter-concept similarity is 
higher than 0, we will set the local typicality to a mini-
mum value if its intra-concept similarity is 0 (so that we 
will not have a big gap between local typicality values). 
Minimum and maximum values are computed as values 
to the left and right of the local typicality interval for 
non-exceptional cases.   
We can rank our exceptionality measures by the 
level of information they capture (from most general to 
most local): typicality, local typicality and CPS. 
4 Language learning tasks 
The tasks we will be using in our study come from the 
area of spoken dialog systems (SDS). They were all 
designed as methods for potentially improving the dia-
log manager of a SDS system called TOOT (Litman and 
Pan, 2002).  This system provides access to train infor-
mation from the web via telephone and it was developed 
for the purpose of comparing differences in dialog strat-
egy. 
Our tasks are: (1) Identifying user corrections 
(ISCORR), (2) Identifying correction-aware sites 
(STATUS), (3) Identifying concept-level speech recog-
nition errors (CABIN) and (4) Identifying word-level 
speech recognition errors (WERBIN). The first task is a 
binary classification task that labels each user turn as to 
whether or not it is an attempt from the user to correct a 
prior system recognition failure. The second task is a 4-
way classification task that extends the previous one 
with whether or not the user is aware the system made a 
recognition error. The four classes are: normal user turn, 
user only tries to correct the system, user is only aware 
of a system recognition error, and user is both aware of 
and tries to correct the system error. The third and the 
fourth tasks are binary classification tasks that try to 
predict the system speech recognition accuracy when 
recognizing a user turn. CABIN measures a binary ver-
sion of the Concept Accuracy (percent of semantic con-
cepts recognized correctly) while WERBIN measures a 
binary version of the Word Error Rate (percent of words 
recognized incorrectly). 
Data for our tasks was gathered from a corpus of 
2,328 user turns from 152 dialogues between human 
subjects and TOOT. The features used to represent each 
user turn include prosodic information, information 
from the automatic speech recognizer, system condi-
tions and dialog history. Then, each user turn was la-
beled with respect to every classification task. Even 
though our classification tasks share the same data, 
there are clear differences between them. ISCORR and 
STATUS both deal with user corrections which is quite 
different from predicting speech recognition errors 
(handled in WERBIN and CABIN). Moreover, one will 
expect very little noise or no noise at all when manually 
annotating WERBIN and CABIN. For more information 
on our tasks and features, see (Litman et al, 2000; 
Hirschberg et al, 2001; Litman et al, 2001). 
There are a number of dimensions where our tasks 
differ from the tasks from the previous study. First of all 
our datasets are smaller (2,328 instances compared with 
at least 23,898). Second, the number of features used is 
much bigger than the previous study (141 compared 
with 4-11). Moreover, many features from our datasets 
are numeric while the previous study had none. These 
differences will also reflect on our exceptionality meas-
ures values. For example, the smallest range for typical-
ity in the previous study was between 0.43 and 10.57 
while for our tasks it is between 0.9 and 1.1. To explore 
these differences we varied the feature set used. Instead 
of using all the available features (this feature set is 
called All), we restricted the feature set by using only 
non-numeric features (Nonnum ? 22 features). The typi-
cality range increased when using this feature set (0.77-
1.45), but the number of features used was still larger 
than the previous study. For this reason, we next de-
vised two set of features with only 9 (First9) and 15 
features (First15). The features were selected based on 
their information gain (see section 2.1). 
Before proceeding with our results, there is one 
more thing we want to mention. At least half of our in-
stances have one or more missing values and while the 
Ripper implementation offered a way to handle them, 
there was no default handling of missing values in the 
IB1-IG implementation. Thus, we decided to replace 
missing values ourselves before presenting the datasets 
to our learners. In particular there are two types of miss-
ing values: genuine missing values (no value was pro-
vided; we will refer to them as missing values) and 
undefined values. Undefined values come from features 
that are not defined in that user turn (for example, in the 
first user turn, most of the dialog history features were 
undefined because there was no previous user turn).  
For symbolic features, we replaced missing and un-
defined values with a given string for missing values 
and another one for undefined values. For numeric fea-
tures, the problem was more complicated since the dis-
tance metric uses the difference between two numeric 
values and thus, the values used to fix the problem can 
influence the distance between instances. We experi-
mented with different replacement values: to the left and 
right of the interval boundaries for that features, both 
replacement values on one side of the interval or very 
far from the interval boundaries. All experiments with 
the values provided comparable results. For our experi-
ments, missing values were replaced with a value to the 
right of the interval for that feature and undefined val-
ues were replaced with a value to the left of that inter-
val. 
5 Results 
5.1 
In 5.1 we reproduce the editing and comparison experi-
ments from the previous study to see if their results gen-
eralize to our tasks. In 5.2, we move to our next goal: 
characterizing learners? performance using exceptional-
ity measures. Both learners were run using default pa-
rameters2. 
Natural language learning and memory-
based learning 
First, we performed the editing experiments from the 
previous study. The purpose of those experiments was 
to see the impact of editing exceptional and typical in-
stances on the accuracy of the memory-based learner. 
Since our datasets were small, unlike the previous study 
which performed editing only on the first train-test par-
tition of a 10-fold cross validation, we performed the 
editing experiment on all partitions of a 10-fold cross 
validation. For every fold, we edited 0, 1, 2, 5, 10, 20, 
30, 40 and 50% of the training set based on extreme 
values of all our exceptionality criteria. Accuracy after 
editing a given percent was averaged among all folds 
(there is a significant difference in accuracies among 
folds but all folds exhibit a similar trend with the aver-
age). Figure 2 shows our results for the ISCORR dataset 
79.0%
79.5%
80.0%
80.5%
81.0%
81.5%
82.0%
82.5%
83.0%
0 1 2 5 10 20 30 40 50
Percentage of instances removed
A
ve
ra
ge
 a
cc
ur
ac
y
High CPS
Low CPS
High Local Typ.
Low Local Typ.
High Typicality
Low Typicality
 
Figure 2. IB1-IG average accuracy after editing a given percent of the 
training set based on high and low extremes of all exceptionality 
measures (ISCORR dataset with all features) 
                                                          
2 We performed parameter tuning experiments for both predic-
tors: for every fold of a 10-fold cross validation, part of the 
training set was used as a validation set (for tuning parame-
ters). Our results indicate that the tuned parameters depend on 
the fold used and there was no clear gain to accuracy from 
tuning (in some cases there was even loss in accuracy). Inte-
grating tuned parameters with our leave-one-out experiments 
presents additional problems.  
using six types of editing (editing based on low and high 
value for all three criteria). In contrast with the previous 
study, where for all tasks even the smallest editing led 
to significant accuracy decreases, for our task there was 
no clear decrease in performance. Moreover, for some 
criteria (like low local-typicality) we can even see an 
initial increase in performance. Only after editing half 
of the training set is there a clear decrease in perform-
ance for all editing criteria on this task. 
Editing experiments for the other dataset-feature set 
combinations yield similar results. 
Next, we compared the memory-based learner with 
our abstraction-based learner on all tasks. Since the 
datasets were relatively small, we performed leave-one-
out cross validations. Table 1 summarizes our results. 
The baseline used is the majority class baseline. First, 
we run the predictors on all tasks using all features. In 
contrast with the previous study which favored the 
memory-based learner for almost all their tasks, our 
results favor IB1-IG for only two of the four tasks 
(ISCORR and STATUS). In Section 4, we mentioned 
that the typicality range for our tasks was very small 
compared with the previous study. Contrary to what we 
expected, the tasks where IB1-IG performed better were 
the ones with smaller typicality range. To investigate 
the typicality range impact on our predictors, we tried to 
make our datasets similar to the datasets from the previ-
ous study by tackling the feature set. We eliminated all 
numeric features (since the tasks from the previous 
study had none) and performed experiments on the tasks 
that had the less typicality range (again, ISCORR and 
STATUS). Again, when typicality range was increased, 
even though there were no numeric features, IB1-IG 
performed worse than Ripper. IB1-IG error rate in-
creased when using only non-numeric features for both 
tasks compared with the error rate when using all fea-
tures. This observation led us to assume that, at least for 
IB1-IG, some of the relevant features for classification 
were numeric and they were not present in our feature 
set. Thus, we selected two sets of features (First9 and 
First15) based on the features? relevance and performed 
the experiments again on the ISCORR dataset. We can 
 Error rate 
Data-Feat. set IB1-IG Ripper Baseline
Typicality 
range 
Iscorr-All 14.99% 16.15% 28.99% 0.94 - 1.06
Status-All 22.25% 23.71% 43.04% 0.96 - 1.10
Cabin-All 13.10% 12.11% 30.50% 0.90 - 1.12
Werbin-All 17.65% 11.90% 39.22% 0.90 - 1.10
Iscorr-Nonnum 17.01% 16.24% 28.99% 0.81 - 1.49
Status-Nonnum 23.93% 21.99% 43.04% 0.88 - 1.62
Iscorr-First9 17.78% 16.07% 28.99% 0.86 - 1.17
Iscorr-First15 14.69% 14.95% 28.99% 0.88 - 1.14
Table 1. IB1-IG, Ripper and majority class baseline error 
rate on some of our dataset-feature set combinations 
observe that as the number of relevant features is in-
creased, the error rate for both predictors and the typi-
cality range are decreasing and IB1-IG takes the lead 
when the First15 feature set is used. Our results indicate 
that the predictor that performs better depends on the 
task, the number of features and the type of features we 
use.  
To explore why the previous study?s results do not 
generalize in our case, we are planning to replicate these 
experiments on the dialog-act tagging task on the 
Switchboard corpus (a task more similar in size and 
feature types with the previous study than our tasks but 
still in the area of spoken dialog systems ? see Shriberg 
et al (1998)). 
5.2 Characterizing learners? performance 
using exceptionality measures 
The next goal of our study was to see if we can charac-
terize the performance of our predictors on various 
classes of instances defined by our exceptionality crite-
ria. In other words, we wanted to try to answer ques-
tions like: is IB1-IG better at predicting exceptional 
instances than Ripper? How about typical instances? 
Can we combine the two learners and select between 
them based on the instance exceptionality? 
To answer these questions, we performed the leave-
one-out experiments described above and recorded for 
every instance whether our predictors predicted it cor-
rectly or incorrectly. Next, we computed the exception-
ality of every instance using all three measures. Figure 3 
shows the exceptionality distribution using the typicality 
measure for the ISCORR dataset with all features3. The 
0
50
100
150
200
250
300
350
400
450
0.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06
Typicality
Fr
eq
ue
nc
y
IB1-IG
Ripper
Full dataset
 
Figure 3. Typicality distribution for all instances, instances correctly 
predicted by IB1-IG and instances correctly predicted by Ripper 
(ISCORR dataset with all features) 
typicality distributions of all instances from the 
ISCORR dataset, of instances correctly predicted by 
IB1-IG, and of instances correctly predicted by Ripper 
are plotted in the figure. The graph shows that for this 
dataset there are a lot of boundary instances, very few 
exceptional instances and few typical instances. The 
typicality range for all our datasets (usually between 
0.85 and 1.15) is far less than the one from the previous 
study (0.43 up to 10 or even 3500). According to Zhang 
(1992) hard concepts are often characterized by small 
typicality spread. Moreover, small typicality spread is 
associated with low accuracy in predicting. 
                                                          
3 For other dataset-feature set combination graphs see: 
http://www.cs.pitt.edu/~mrotaru/exceptionality 
Figure 4 shows the same information as Figure 3, 
but instead of plotting the count, we plot the percentage 
of the instances with typicality between a given interval 
that have been correctly classified by one of the predic-
tors. We can observe that accuracy of both predictors 
increases with typicality. That is, the more typical the 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06
Typicality
Co
rr
ec
tly
 p
re
di
ct
ed
 - 
pe
rc
en
ta
ge
 
IB1-IG
Ripper
 
Figure 4. Percent of instances predicted correctly by IB1-IG and Rip-
per based on instance typicality (ISCORR dataset with all features) 
instance, the more reliable the prediction; the more ex-
ceptional the instance, the more unreliable the predic-
tion. This observation holds for all our dataset-feature 
set combinations. It is not clear for the ISCORR dataset 
whether one predictor is better than the other based on 
the typicality. But for datasets CABIN and WERBIN 
where, overall, IB1-IG did worse than Ripper, the same 
graph (see Figure 5) shows that IB1-IG?s accuracy is 
worse than Ripper?s accuracy when predicting low typi-
cality instances4. Given the problems with typicality if 
the concepts we want to learn are clustered, we decided 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.89 0.91 0.93 0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.07 1.09 1.11
T ypicality
Co
rr
ec
tly
 p
re
di
ct
ed
 - 
pe
rc
en
ta
ge
IB1-IG
Ripper
 
Figure 5. Percent of instances predicted correctly by IB1-IG and Rip-
per based on instance typicality (CABIN dataset with all features) 
                                                          
4 It was not our point to investigate statistical significance of 
this trend. As we will see later, this trend is powerful enough 
to yield interesting results when combining the predictors 
based on exceptionality measures. 
to investigate if this observation holds for other excep-
tionality measures. 
We continued the experiments on the other excep-
tionality measures hoping to get more insight into the 
trend observed for typicality. Indeed, Figure 6 (same as 
Figure 4  but using the CPS instead of typicality) shows 
the same trend: IB1-IG is worse than Ripper when pre-
dicting exceptional instances and it is better when pre-
dicting typical instances. The accuracy curves of the 
two predictors seem to cross at a CPS value of 0.5, 
which corresponds to boundary instances. Undefined 
CPS values (0/0) are assigned a value above 1 (the 
rightmost point on the graph). Ripper was the one that 
offered higher accuracy in predicting instances with 
undefined CPS value for almost all datasets (although 
not in Figure 6). The result holds for all our dataset-
feature set combinations. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.00 0.08 0.17 0.25 0.34 0.42 0.51 0.59 0.68 0.76 0.85 0.93 1.02
CPS
Pr
ed
ic
te
d 
co
rr
ec
tly
 - 
pe
rc
en
ta
ge
IB1-IG
Ripper
 
Figure 6. Percent of instances predicted correctly by IB1-IG and Rip-
per based on instance CPS (ISCORR dataset with all features)5 
The experiments with local typicality yield the same 
results: Ripper constantly outperforms IB1-IG for ex-
ceptional instances and they switch places for typical 
instances (see Figure 7). Again, the accuracy curves 
cross at boundary instances (local typicality value of 1) 
and the same observation holds for all dataset-feature 
set combinations. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0.91 0.92 0.93 0.95 0.96 0.97 0.99 1.00 1.01 1.03 1.04 1.05 1.07
Local Typicality
Co
rr
ec
tly
 p
re
di
ct
ed
 - 
pe
rc
en
ta
ge
IB1-IG
Ripper
 
Figure 7. Percent of instances predicted correctly by IB1-IG and  
Ripper based on instance local typicality  
(ISCORR dataset with all features) 
                                                          
5 Abrupt movements in curves are caused by small number of 
instances in that class. We expect that a larger dataset will 
smooth our graphs. 
We computed what could be the reduction in error 
rate if we were to employ both predictors and decide 
between them based on the instance exceptionality 
measure. In other words, Ripper prediction was used for 
exceptional instances and for the left-hand side bound-
ary instances (CPS less than 0.5; typicality less than 1; 
local typicality less than 1); otherwise IB1-IG prediction 
was used. The lower bound of this reduction is when we 
perfectly know which of the predictors offer the correct 
prediction (in other words the error rate is the number of 
times both learners furnished wrong predictions). Figure 
8 plots the reduction in error rate achieved when decid-
ing between predictors based on typicality, CPS, local 
typicality and perfect discrimination. The reduction is 
relative to the best performer on that task. While dis-
criminating based on typicality offered no improvement 
relative to the best performer, CPS was able to con-
stantly achieve improvement and local typicality im-
proved in six out of eight cases. CPS improved the error 
rate of the best performer by decreasing it by 1.33% to 
3.18% (absolute percentage). In contrast with CPS, local 
typicality offered, for the cases when it improved the 
accuracy, more improvement decreasing the error rate 
by up to 4.94% (absolute percentage). A possible expla-
nation of this difference can be the fact that local typi-
cality captures much more information than CPS 
(vicinity-level information compared with information 
very close to the instance). 
-20%
-10%
0%
10%
20%
30%
40%
50%
60%
Is
co
rr
-
A
ll
St
at
us
-
A
ll
Ca
bi
n-
A
ll
W
er
bi
n-
A
ll
Is
co
rr
-
N
on
nu
m
St
at
us
-
N
on
nu
m
Is
co
rr
-
Fi
rs
t9
Is
co
rr
-
Fi
rs
t1
5
Typicality
CPS
Local Typicality
Perfect Discr.
 
Figure 8. Reduction in error rate relative to the best performer for 
typicality, CPS, local typicality and prefect discrimination 
In summary, all our exceptionality measures show 
the same trend in predicting ability: Ripper performs 
better than IB1-IG on exceptional instances while IB1-
IG performs better than Ripper on typical instances. 
While the fact that IB1-IG does better on typical in-
stances may be linked to its ability to handle sub-
regularities, we have no interpretation for the fact that 
Ripper does better on exceptional instances. We plan to 
address this by future work that will look at the distance 
between exceptional instances and the instances that 
generated the rule that made the correct prediction for 
those exceptional instances. 
5.3 Current directions 
The previous section showed that we can improve the 
overall accuracy on our datasets if we combine the pre-
diction generated by our learners based on the excep-
tionality measure of the new instance. Unfortunately, all 
our exceptionality measures require the class of the in-
stance. Moreover, for binary classification tasks, since 
all exceptionality criteria are a ratio, changing the in-
stance class will turn an exceptional instance into a 
typical instance. 
To move our results from offline to online, we con-
sidered interpolating the exceptionality value for an 
instance based on its neighbors? exceptionality values 
(the neighbors from the training set). We performed a 
very simple interpolation by using the exceptionality 
value of the closest neighbor (relative to equation (1)). 
While previous observations are not obvious anymore in 
online graphs (there is no clear crossing at boundary 
instances), there is a small improvement over the best 
predictor. Figure 9 shows that even for this simple in-
terpolation there is a small reduction in almost all cases 
in error rate relative to the best performer when using 
online CPS (interpolated CPS). 
-15%
-10%
-5%
0%
5%
10%
15%
20%
Is
co
rr
-A
ll
St
at
us
-A
ll
Ca
bi
n-
A
ll
W
er
bi
n-
A
ll
Is
co
rr
-
N
on
nu
m
St
at
us
-
N
on
nu
m
Is
co
rr
-F
irs
t9
Is
co
rr
-
Fi
rs
t1
5
Offline CPS
Online CPS
 
Figure 9. Reduction in error rate relative to the best performer for 
offline CPS and online CPS 
We are currently investigating more complicated in-
terpolation strategies like learning of a model from the 
training set that will predict the exceptionality value of 
an instance based on its closest neighbors. 
6 Conclusions 
In this paper we attempted to generalize the results of a 
previous study to a new set of language learning tasks 
from the area of spoken dialog systems. Our experi-
ments indicate that previous results do not generalize so 
obviously to the new tasks. Next, we showed that some 
exceptionality measures can be used as means to im-
prove the prediction accuracy on our tasks by combin-
ing the prediction of our learners based on measures of 
instance exceptionality. We observed that our memory-
based learner performs better than the rule-based learner 
on typical instances and they exchange places for excep-
tional instances. We also showed that there is potential 
for moving these results from offline to online by per-
forming a simple interpolation. Future work needs to 
address more complicated methods of interpolation, 
comparison between our method and other attempts to 
combine rule-based learning and memory-based learn-
ing (Domingos, 1996; Golding and Rosenbloom, 1991), 
comparison with ensemble methods, and whether the 
results from this paper generalize to other spoken dialog 
corpora. 
Acknowledgements 
We would like to thank Walter Daelemans and Antal 
van den Bosch for starting us on this work. 
References 
William Cohen. 1995. Fast effective rule induction. ICML. 
Walter Daelemans, Antal van den Bosch, and Jakub Zavrel. 
1999. Forgetting exceptions is harmful in language 
learning. Machine Learning 1999, 34 :11-43. 
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal 
van den Bosch. 2001. TiMBL: Tilburg Memory Based 
Learner, version 4.1, Reference Guide. ILK Technical 
Report ? ILK 01-04. 
Pedro Domingos. 1996. Unifying Instance-Based and Rule-
Based Induction. Machine Learning 1996, 24:141-168 
Andrew R. Golding and Paul S. Rosenbloom. 1991. Improving 
Rule-Based Systems Through Case-Based Reasoning. Proc. 
AAAI. 
Julia Hirschberg, Diane J. Litman, and Marc Swerts. 2001. 
Identifying User Corrections Automatically in Spoken 
Dialogue Systems. Proc. NAACL. 
Diane J. Litman, Julia Hirschberg, and Marc Swerts. 2000. 
Predicting Automatic Speech Recognition Performance 
Using Prosodic Cues. Proc. NAACL. 
Diane J. Litman, Julia Hirschberg, and Marc Swerts. 2001. 
Predicting User Reactions to System Error. Proc. ACL. 
Diane J. Litman, Shimei Pan. 2002. Designing and Evaluating 
an Adaptive Spoken Dialogue System. User Modeling and 
User-Adapted Interaction, 12(2/3):111-137. 
Salzberg, S. 1990. Learning with nested generalised 
exemplars. Kluwer Academic Publishers. 
Elizabeth Shriberg, Rebecca Bates, Paul Taylor, Andreas 
Stolcke, Klaus Ries, Daniel Jurafsky, Noah Coccaro, 
Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema. 
1998. Can prosody aid the automatic classification of 
dialog acts in conversational speech?. Language and 
Speech 41:439?487. 
Jianping Zhang. 1992. Selecting typical instances in 
instance-based learning. Proc. ICML, 470-479. 
Annotating Student Emotional States in Spoken Tutoring Dialogues
Diane J. Litman
University of Pittsburgh
Department of Computer Science
Learning Research and Development Center
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Kate Forbes-Riley
University of Pittsburgh
Learning Research and Development Center
Pittsburgh PA, 15260, USA
forbesk@pitt.edu
Abstract
We present an annotation scheme for stu-
dent emotions in tutoring dialogues. Analy-
ses of our scheme with respect to interannota-
tor agreement and predictive accuracy indicate
that our scheme is reliable in our domain, and
that our emotion labels can be predicted with
a high degree of accuracy. We discuss issues
concerning the implementation of emotion pre-
diction and adaptation in the computer tutoring
dialogue system we are developing.
1 Introduction
This paper describes a coding scheme for annotating stu-
dent emotional states in spoken dialogue tutoring cor-
pora, and analyzes the scheme not only for its reliabil-
ity, but also for its utility in developing a spoken dia-
logue tutoring system that can model and respond to stu-
dent emotions. Motivation for this work comes from the
performance discrepancy between human tutors and cur-
rent machine tutors: typically, students tutored by hu-
man tutors achieve higher learning gains than students
tutored by computer tutors. The development of com-
putational tutorial dialogue systems (Rose? and Aleven,
2002) represents one method of closing this performance
gap, e.g. it is hypothesized that dialogue-based tutors al-
low greater adaptivity to students? beliefs and misconcep-
tions. Another method for closing this performance gap
involves incorporating emotion prediction and adaptation
into computer tutors (Kort et al, 2001; Evens, 2002).
For example (Aist et al, 2002) have shown that adding
human-provided emotional scaffolding to an automated
reading tutor increases student persistence. This suggests
that the success of computer dialogue tutors could be in-
creased by responding to both what a student says and
how s/he says it, e.g. with condence or uncertainty.
To assess the impact of adding emotion modeling to
dialogue tutoring systems, we are building ITSPOKE
(Intelligent Tutoring SPOKEn dialogue system), a spo-
ken dialogue system that uses the Why2-Atlas concep-
tual physics tutoring system (VanLehn et al, 2002) as its
?back-end.?1 Our first step towards incorporating emo-
tion processing into ITSPOKE is to develop a reliable
annotation scheme for student emotions. Our next step
will be to use the data that has been annotated accord-
ing to this scheme to enhance ITSPOKE to dynamically
predict and adapt to student emotions. This adds addi-
tional constraints on our annotation scheme besides good
reliability, namely that our annotations are predictable by
ITSPOKE with a high degree of accuracy (automatically
and in real-time), and that they are expressive enough to
support the range of desired system adaptations.
In Section 2 we review previous work in emotion anno-
tation for spoken dialogue systems. In Section 3 we dis-
cuss our tutoring research project and corpora. In Section
4 we present an emotion annotation scheme for this do-
main. In Section 5 we analyze our scheme with respect to
interannotator agreement and predictive accuracy, using a
corpus of human tutoring dialogues. Our agreement indi-
cates that our scheme is reliable, while machine learning
experiments on annotated data indicate that our emotion
labels can be predicted with a high degree of accuracy.
In Section 6 we analyze more expressive versions of our
scheme, and discuss differences between annotating hu-
man and computer spoken tutoring dialogues.
2 Prior Research on Emotion
Developing a descriptive theory of emotion is a com-
plex research topic, viewed from either a theoretical or
an empirical standpoint (Cowie et al, 2001). Some re-
searchers have proposed a variety of ?fundamental? hu-
man emotions, while others have argued that emotions
1We also use ITSPOKE to examine the utility of building
spoken dialogue tutors (e.g. (Litman and Forbes, 2003)).
are best represented componentially, in terms of multiple
dimensions. Despite this lack of a well-defined descrip-
tive framework, there has been great recent interest in
predicting emotional states, using information extracted
from a person?s text, speech, physiology, facial expres-
sions, eye gaze, etc. (Pantic and Rothkrantz, 2003).
In the area of emotional speech, most research has
used databases of speech read by actors or native speak-
ers as training data for developing emotion predic-
tors (Holzapfel et al, 2002; Liscombe et al, 2003). In
this work the set of emotions to be read is predefined be-
fore the utterance is spoken, rather than annotated after
the fact. One problem with this approach is that such
prototypical emotional speech does not necessarily re-
flect natural speech (Batliner et al, 2003), e.g. the way
one acts an emotion is not necessarily the same as the
way one naturally expresses an emotion. Moreover, ac-
tors repeatedly reading the same sentence are restricted
to conveying different emotions using only acoustic and
prosodic features, while in natural interactions a much
wider feature variety is available (e.g., lexical, dialogue).
As a result of these problems, researchers motivated by
spoken dialogue applications have instead started to train
emotion predictors using naturally-occurring speech that
has been hand-annotated for various emotions (Ang et al,
2002; Batliner et al, 2003; Lee et al, 2001; Litman and
Forbes, 2003). However, this requires researchers to first
develop a scheme for annotating emotions in naturally-
occurring spoken dialogue corpora. Although emotion
annotation of natural corpora (typically at the turn or ut-
terance level) has been addressed in various domains, lit-
tle has yet been done in the educational setting. Although
not yet tested, (Evens, 2002) has hypothesized adaptive
strategies; for example, if detecting frustration, the sys-
tem should respond to hedges and self-deprecation, by
supplying praise and restructuring the problem. A com-
parison of our annotation scheme and prior non-tutoring
schemes is presented in Section 4.4.
3 The ITSPOKE System and Corpora
In ITSPOKE, a student types an essay answering a qual-
itative physics problem. The ITSPOKE computer tutor
then engages the student in spoken dialogue to correct
misconceptions and elicit more complete explanations,
after which the student revises the essay, thereby ending
the tutoring or causing another round of tutoring/essay re-
vision. Student speech is digitized from microphone in-
put and sent to the Sphinx2 recognizer, whose most prob-
able ?transcription? output is then sent to the Why2-Atlas
back-end for syntactic, semantic and dialogue analysis.
The text response produced by Why2-Atlas is sent to the
Cepstral text-to-speech system. A formal evaluation of
ITSPOKE began in November 2003; to date we have col-
lected 50 dialogues from 10 students. A corpus example
is shown in Figure 4, Appendix A. Corpus collection uses
the same experimental procedure as our human-human
tutoring corpus, described next.
Our Human-Human Spoken Dialogue Tutoring Corpus
contains spoken dialogues collected via a web interface
supplemented with a high-quality audio link, where the
human tutor performs the same task as ITSPOKE. The
experimental procedure for collecting both corpora is as
follows: 1) students are given a pre-test measuring their
physics knowledge, 2) students read through a small doc-
ument of background material, 3) students use the web
and voice interface to work through a set of training prob-
lems (dialogues) with the tutor, and 4) students are given
a post-test that is similar to the pre-test. Subjects are Uni-
versity of Pittsburgh students who have never taken col-
lege physics and who are native English speakers. One tu-
tor currently participates. To date we have collected 149
dialogues from 17 students. Annotated (see Section 4)
corpus examples are shown in Figure 1 and Figure 2 (Ap-
pendix A) (punctuation added for clarity).
. . . dialogue excerpt at 5.2 minutes into session. . .
TUTOR   : Suppose you apply equal force by pushing them.
Then uh what will happen to their motion?
STUDENT  : Um the one that?s heavier...uh, the acc- accelera-
tion won?t be as great. (NEGATIVE, UNCERTAIN)
TUTOR  : The one which is...
STUDENT  : Heavier? (NEGATIVE, UNCERTAIN)
TUTOR  : Mm, well, uh, is that your common-
STUDENT  : Er I?m sorry- I?m sorry- the one with most mass.
You- (POSITIVE, CONFIDENT)
TUTOR  : (lgh) Yeah, the one with more mass will- if you-
if the mass is more and force is the same then which one will
accelerate more?
STUDENT
 : Which one will move more? (NEGATIVE,
CONFUSED)
TUTOR

: Mm, which one will accelerate more?
STUDENT  : The- the one with the least amount of mass?
(NEGATIVE, UNCERTAIN)
TUTOR

: Yeah, but what you said was different isn?t it? So
uh you are applying Newton?s law of uh second law of motion:
F is equal to M times A. And uh you apply equal force on both
the containers, then the one which is less massive will accelerate
more.
STUDENT  : Right. (WEAK POSITIVE, CONFIDENT)
Figure 1: Annotated Excerpt (Human Spoken Corpus)
4 Annotation Scheme
In our spoken dialogue tutoring corpora, student emo-
tional states can only be identified indirectly ? via what a
student says and/or how s/he says it. Furthermore, such
evidence is not always obvious, unambiguous, or consis-
tent. For example, a student may express anger through
the use of swear words, or through a particular tone of
voice, or via a combination of signals, or not at all. More-
over, another student may present some of these same sig-
nals even when s/he does not feel anger.
Our objective is nevertheless to develop a reliable an-
notation scheme across annotators, for manually labeling
the student turns in our spoken tutoring dialogues for per-
ceived expressions of emotion.
4.1 Emotion Classes
In our current annotation scheme, perceived expressions
of emotion are viewed along a linear scale, as shown and
defined below: negative 	
 neutral 
 positive
Negative: a student turn that strongly expresses emo-
tions such as confused, bored, irritated, uncertain, sad.
Examples in Figure 1 include student  and student  .
Evidence2 for the negative emotions in these turns in-
cludes syntax (constructions such as questions), disflu-
encies, and acoustic-prosodic features.
Positive: a student turn that strongly expresses emo-
tions such as condent, enthusiastic. An example is
student  in Figure 1, where evidence of a positive emo-
tion comes primarily from acoustic-prosodic features.
Neutral: a student turn not strongly expressing a neg-
ative or positive emotion.
In addition to these three main emotion classes, we
also distinguish three minor emotion classes:
Weak Negative: a student turn that weakly expresses
negative emotions.
Weak Positive: a student turn that weakly expresses
positive emotions. An example is student  in Figure
1, where evidence is primarily lexical (?right?).
Mixed: a student turn that strongly expresses both posi-
tive and negative emotions: Case 1) multi-utterance turns
where one utterance is judged positive and another, nega-
tive. Case 2) turns where the simultaneous strong expres-
sion of negative and positive emotions is perceived. Case
2 is often due to conflicting domains (Section 4.2), e.g.
boredom with tutoring but confidence about physics.
4.2 Relativity and Domains of Emotion Classes
Our emotion annotation is relative to both context and
task. By context-relative we mean that a student turn in
our tutoring dialogues is identified as expressing emotion
relative to the other student turns in that dialogue. By
task-relative we mean that a student turn perceived during
tutoring as expressing an emotion might not be perceived
as expressing the same emotion with the same strength
in another situation. For example, consider the context
of a tutoring session, where a student has been answer-
ing tutor questions with apparent ease. If the tutor then
asks another question, and the student responds slowly,
2Determined in post-annotation discussion (see Section 4.4).
saying ?Um, now I?m confused?, this turn would likely
be labeled negative. However, in the context of a heated
argument between two people, this same turn might be
labeled as a weak negative, or even weak positive.
We also annotate emotion with respect to multiple do-
mains. One focus of our annotation scheme is expres-
sions of emotion that pertain to the physics material be-
ing learned (?PHYS? domain). For example, a student
may express confusion or confidence about the physics
material. Another focus of our scheme is expressions of
emotion that pertain to the tutoring process, including at-
titudes towards the tutor, the dialogue, and/or being tu-
tored (?TUT? domain). For example, a student may ex-
press boredom or amusement with the tutoring.
4.3 Specific Annotation Instructions
Our annotation scheme is detailed in an online, audio-
enhanced emotion labeling manual. As shown in Figure 3
(Appendix A), the emotion annotation is performed using
(our customization of) Wavesurfer, an open source sound
visualization and manipulation tool. The ?Tutor Speech?
and ?Student Speech? panes show a portion of the tutor
and student speech files, while the ?Tutor Text? and ?Stu-
dent Text? show the associated transcriptions, where ver-
tical lines correspond to turn segmentations.3 There are
three additional panes for emotion annotation:
The EMOa pane records the annotator?s judgment of
the expressed emotion class for each turn, e.g. the six
emotion classes described in Section 4.1: negative, weak
negative, neutral, weak positive, positive, mixed. An-
notators are instructed to focus on expressed emotions in
the PHYS domain. If an additional expressed emotion in
the TUT domain is perceived, this is noted in the NOTES
pane (e.g. ?amused/TUT?). If no expressed emotion is
perceived in the PHYS domain, any expressed emotion in
the TUT domain is labeled in the EMOa pane, and noted
(e.g. ?TUT?) in the NOTES pane. Domain indecision is
also noted (e.g. ?TUT/PHYS??) in the NOTES pane.
The EMOb pane further specifies the annotations in
the EMOa pane, by recording a specific expressed emo-
tion for each turn. Our current list of specific emotions
contains those that we believe will be useful for trigger-
ing ITSPOKE adaptation. Specific negative emotions are:
uncertain, confused, sad, bored, irritated. Specific pos-
itive emotions are: condent, enthusiastic. Our manual
includes glosses for these specific emotions, formulated
using synonyms and/or hyponyms that are currently not
distinguished. For example, our gloss for enthusiastic in-
cludes interested, pleased, amused. There are also com-
plex labels combining multiple specific emotions within
a class (e.g. uncertain+sad, condent+enthusiastic). If
3Transcription and turn-segmentation of the human-human
dialogues were also done within Wavesurfer, by a paid tran-
scriber prior to emotion annotation.
the annotator judges a specific emotion that is not listed
(or lacks a close substitute), s/he selects the label other,
and lists the alternative(s) in the NOTES pane. If the an-
notator selected mixed (case 1) in the EMOa pane, s/he
subdivides the turn into utterances in the EMOb pane and
provides a specific emotion label for each utterance. If
the annotator selected mixed (case 2) in the EMOa pane,
s/he selects the label other in the EMOb pane, and com-
ments on the indecision in the NOTES pane.
The NOTES pane records any additional annotator
comments concerning their judgment, the annotation, etc.
Because our annotation is student-, context-, and task-
specific, our manual first instructs the annotator to listen
to each dialogue at least once before annotating, to se-
cure an intuition of how and with what range emotional
expression is displayed. S/he is also instructed to not as-
sume that all dialogues will begin with neutral student
turns. S/he is however reminded that it is not necessary
to assign a non-neutral label to every turn. Finally, s/he
is told to ignore correctness when annotating, because a
correct answer to a tutor question can express uncertainty,
and an incorrect answer can express confidence.
Our manual also describes two default conventions for
our annotation scheme, which can however be overridden
by the annotator?s intuitive judgment and/or other extenu-
ating considerations (e.g. irony, etc), as described below:
1) By definition, a question expresses strong uncertainty
or confusion. Thus if a student turn consists only of a
question, its default label is negative. However:
a) If the turn consists of multiple utterances, one of
which is a question, and the other(s) expresses a positive
emotion, then the turn should be labeled mixed and sub-
divided (e.g. ?What directions are the forces acting in?
Gravity is only acting in the down direction?).
b) The domain must be considered. For example, de-
faults in one domain can be overridden if the turn ex-
presses a contrasting emotion in the other domain.
2) Many student turns in our dialogues are very short,
containing only grounding phrases such as ?yeah?, ?ok?,
?mm-hm?, ?uh-huh?, etc. By default, such turns are la-
beled neutral, because groundings serve mainly to en-
courage another speaker to continue speaking. However:
a) Groundings may occasionally strongly express an
emotion (e.g. ?yeah!?, (sigh) ?ok?), thereby overriding
the default label.
b) The semantics of certain groundings is associated
with weakly expressed understanding, (e.g. ?right? and
?sure?), and default to weak positive.
c) Certain phrases are associated with strongly ex-
pressed uncertainty or confusion (e.g. ?um? (silence)),
and default to negative.
Our annotation manual concludes with 8 examples of
annotated student turns (as in Figure 1), with links to cor-
responding audio files. The variety exemplifies how dif-
ferent students express emotions differently at different
points in the dialogue, and cover all 6 emotion labels at
least once (there are 2 negatives and 2 positives). Also
provided is a lengthy audio-enhanced transcript from a
single student tutoring dialogue, to exemplify how stu-
dent emotion changes throughout a single tutoring ses-
sion. This transcript is shown in part in Figure 2, Ap-
pendix A. The transcript is organized in terms of tutor
and student turn start and end times. For each student
turn, the four Wavesurfer panes are shown.
4.4 Comparison with Prior Schemes
Studies of actor-read speech often make a large num-
ber of emotion distinctions, e.g. the LDC Emotional
Prosody corpus distinguishes 15 classes. Our work,
like other studies of naturally occurring dialogues, uses
a more restricted set of emotions, due to the need to
first manually annotate such emotions reliably across an-
notators. As discussed above, our annotation scheme
distinguishes negative, neutral, and positive emotions,
as well as ?weak? and ?mixed? classes. Other stud-
ies of naturally occurring data have annotated only two
emotion classes (e.g. emotional/non-emotional (Bat-
liner et al, 2000), negative/non-negative (Lee et al,
2001)). The study of (Ang et al, 2002) annotates six
emotion classes, but collapses most of these for the
purposes of emotion prediction.4 In Section 5, we
will similarly explore the impact of collapsing some
of our 6 distinctions, to produce simpler 3-way (neg-
ative/positive/neutral) and 2-way (negative/non-negative
and emotional/non-emotional) schemes.
In further contrast to (Lee et al, 2001), our annotations
are context- and task-relative, because like (Ang et al,
2002; Batliner et al, 2003), we are interested in detect-
ing emotional changes across our dialogues. But unlike
(Batliner et al, 2003), we allow annotators to be guided
by their intuition rather than a set of expected features,
to avoid restricting or otherwise influencing their intu-
itive understanding of emotion expression, and because
such features are not used consistently or unambiguously
across speakers. Instead, our manual contains annotated
audio-enhanced corpus examples (as in Figures 1-2).
5 Analysis of the Annotation Scheme
Given our complete annotation scheme in Section 4, we
now explore both the reliability of the scheme at three
levels of granularity that have been proposed in prior
work, and the accuracy of automatically predicting these
variations. These analyses give insight into the tradeoff
4(Ang et al, 2002) also discusses the use of an ?uncertainty?
label, although it did not improve inter-annotator agreement.
Our ?weak? labels are more similar to an ?intensity? dimension
found in studies of elicited speech (see (Cowie et al, 2001)).
between interannotator reliability, annotation granularity,
and predictive accuracy.
For the purposes of these analyses, we randomly se-
lected 10 transcribed and turn-annotated dialogues from
our human-human tutoring corpus (Section 3), yielding
453 student turns from 9 subjects. The turns were sep-
arately annotated by two annotators, using the emotion
annotation instructions in Section 4. For our machine-
learning experiments we follow the methodology in (Lit-
man and Forbes, 2003), instantiated with the learning
method (boosted decision trees) and feature set (acoustic-
prosodic, lexical, dialogue and contextual) that has given
us our best results in ongoing studies.
5.1 Agreed Student Turns
Conflating Minor and Neutral Classes
For our first analysis, only our three main emotion
classes were distinguished: negative, neutral, positive.
Our three minor classes, weak negative, mixed, weak pos-
itive, were conflated with the neutral class. A confusion
matrix summarizing the resulting inter-annotator agree-
ment is shown in Table 1. The rows correspond to the
labels assigned by annotator 1, and the columns corre-
spond to the labels assigned by annotator 2. For example,
90 negatives were agreed upon by both annotators, while
6 negatives assigned by annotator 1 were labeled as neu-
tral by annotator 2. The two annotators agreed on the an-
notations of 385/453 turns, achieving 84.99% agreement
(Kappa = 0.68 (Carletta, 1996)). Such agreement is ex-
pected given the difficulty of the task, and exceeds that
of prior studies of emotion annotation in naturally occur-
ring speech; (Ang et al, 2002), for example, achieved
agreement of 71% (Kappa 0.47), while (Lee et al, 2001)
averaged around 70% agreement.
As in (Lee et al, 2001), we next performed a machine
learning experiment on the 385 student turns where the
two annotators agreed on the emotion label. Our predic-
tive accuracy for this data was 84.75% (using 10 x 10
cross-validation as in (Litman and Forbes, 2003)). Com-
pared to a baseline accuracy of 72.74% achieved by al-
ways predicting the majority (neutral) class, our result
yields a relative improvement of 44.06%.5
negative neutral positive
negative 90 6 4
neutral 23 280 30
positive 0 5 15
Table 1: Confusion Matrix 1: Minor  Neutral
5Relative improvement of x over y = Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 21?24, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Predicting Learning in Tutoring with the Landscape Model of Memory
Arthur Ward
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, Pa., 15260, USA
artward@cs.pitt.edu
Diane Litman
Learning Research and Development Center
University of Pittsburgh
Pittsburgh, Pa., 15260, USA
litman@cs.pitt.edu
Abstract
A Landscape Model analysis, adopted
from the text processing literature, was
run on transcripts of tutoring sessions, and
a technique developed to count the occur-
rence of key physics points in the result-
ing connection matrices. This point-count
measure was found to be well correlated
with learning.
1 Introduction
Human one-to-one tutoring often yields significantly
higher learning gains than classroom instruction
(Bloom, 1984). This difference motivates natu-
ral language tutoring research, which hopes to dis-
cover which aspects of tutorial dialogs correlate with
learning. Much of this research focuses on various
dialog characteristics. For example, (Graesser et al,
1995) argue that the important components of tutor-
ing include question answering and explanatory rea-
soning. In other work (Litman et al, 2004) examine
dialog characteristics that can be identified automat-
ically, such as ratio of student to tutor words, and
average turn length.
In this paper, rather than look at characteristics
of the tutoring dialog itself, we feed the dialog
into a computational model of student memory, in
which we then find a measure correlated with learn-
ing. This ?Landscape Model? (van den Broek et al,
1996) proves useful for predicting how much stu-
dents remember from tutoring sessions, as measured
by their learning gains.
We will first briefly describe the Landscape
Model. Then we will describe the tutoring experi-
ments from which we draw a corpus of dialogs, and
how the model was applied to this corpus. Finally,
we cover the model?s success in predicting learning.
2 The Landscape Model
The Landscape Model was designed by van den
Broek et al (1996) to simulate human reading com-
prehension. In this model, readers process a text
sentence-by-sentence. Each sentence contains ex-
plicitly mentioned concepts which are added into
working memory. In addition, the reader may
re-instantiate concepts from earlier reading cycles
or from world knowledge in an effort to maintain
a coherent representation. Concepts are entered
into working memory with initial activation values,
which then decay over subsequent reading cycles.
After concepts are entered, the model calculates
connection strengths between them. Two concepts
that are active in working memory at the same time
will be given a link. The higher the levels of con-
cept activation, the stronger the link will be. Van den
Broek et al (1996) give this formula for calculating
link strengths:
 
	




This defines the strength of the connection be-
tween concepts x and y as the product of their acti-
vations (A) at each cycle i, summed over all reading
cycles.
Two matrices result from these calculations. The
first is a matrix of activation strengths, showing all
the active concepts and their values for each reading
cycle. The second is a square matrix of link val-
ues showing the strength of the connection between
21
each pair of concepts. Van den Broek et al (1996)
demonstrate a method for extracting a list of indi-
vidual concepts from these matrices in order of their
link strengths, starting with the strongest concept.
They show a correlation between this sequence and
the order in which subjects name concepts in a free-
recall task.
In van den Broek?s original implementation, this
model was run on short stories. In the current work,
the model is extended to cover a corpus of transcripts
of physics tutoring dialogs. In the next section we
describe this corpus.
3 Corpus of Tutoring Transcripts
Our corpus was taken from transcripts collected
for the ITSPOKE intelligent tutoring system project
(Litman and Silliman, 2004). This project has col-
lected tutoring dialogs with both human and com-
puter tutors. In this paper, we describe results using
the human tutor corpus.
Students being tutored are first given a pre-test to
gauge their physics knowledge. After reading in-
structional materials about physics, they are given a
qualitative physics problem and asked to write an es-
say describing its solution. The tutor (in our case, a
human tutor), examines this essay, identifies points
of the argument that are missing or wrong, and en-
gages the student in a dialog to remediate those
flaws. When the tutor is satisfied that the student
has produced the correct argument, the student is al-
lowed to read an ?ideal? essay which demonstrates
the correct physics argument. After all problems
have been completed, the student is given a post-test
to measure overall learning gains. Fourteen students
did up to ten problems each. The final data set con-
tained 101,181 student and tutor turns, taken from
128 dialogs.
4 Landscape Model & Tutoring Corpus
Next we generated a list of the physics concepts nec-
essary to represent the main ideas in the target solu-
tions. Relevant concepts were chosen by examining
the ?ideal? essays, representing the complete argu-
ment for each problem. One hundred and twelve
such concepts were identified among the 10 physics
problems. Simple keyword matching was used to
identify these concepts as they appeared in each line
Concept Name Keywords
above above, over
acceleration acceleration,accelerating
action action, reaction
affect experience,experienced
after after, subsequent
air friction air resistance, wind resistance
average mean
ball balls, sphere
before before, previous
beside beside, next to
Table 1: Examples of concepts and keywords
of the dialog. A small sample of these concepts and
their keywords is shown in Table 1.
Each concept found was entered into the working
memory model with an initial activation level, which
was made to decay on subsequent turns using a for-
mula modeled on van den Broek (1996). Concept
strengths are assumed to decay by 50% every turn
for three turns, after which they go to zero. A sam-
ple portion of a transcript showing concepts being
identified, entering and decaying is shown in Table
2. Connections between concepts were then calcu-
lated as described in section two. A portion of a
resulting concept link matrix is shown in Table 3.
It should be noted that the Landscape model has
some disadvantages in common with other bag-of-
words methods. For example, it loses information
about word order, and does not handle negation well.
As mentioned in section two, van den Broek et al
created a measure that predicted the order in which
individual concepts would be recalled. For our task,
however, such a measure is less appropriate. We
are less interested, for example, in the specific or-
der in which a student remembers the concepts ?car?
and ?heavier,? than we are in whether the student re-
members the whole idea that a heavier car acceler-
ates less. To measure these constellations of con-
cepts, we created a new measure of idea strength.
5 Measuring Idea Strength
The connection strength matrices described above
encode data about which concepts are present in
each dialog, and how they are connected. To extract
useful information from these matrices, we used the
idea of a ?point.? Working from the ideal essays,
we identified a set of key points important for the
solution of each physics problem. These key points
22
Turn Text Concepts
car heavier acceleration cause
Student I don?t know how to answer this it?s got to be slower, cause,
it?s the car is heavier but
5 5 0 0
Tutor yeah, just write whatever you think is appropriate 2.5 2.5 0 0
Student ok, 1.25 1.25 0 0
Essay The rate of acceleration will decrease if the first car is towing
a second, because even though the force of the car?s engine
is the same, the weight of the car is double
5 0.625 5 5
Student ok 2.5 0 2.5 2.5
Tutor qualitatively,um, what you say is right, you have correctly
recognized that the force, uh, exerted will be the same in
both cases,uh, now, uh, how is force related to acceleration?
1.25 0 5 1.25
Table 2: Portion of a transcript, showing activation strengths per turn
car heavier acceleration cause decelerates decrease
car 0 35.9375 115.234375 102.34375 33.203125 33.2
heavier 0 0 3.125 3.125 3.125 3.13
acceleration 0 0 0 107.8125 42.1875 42.19
cause 0 0 0 0 33.203125 33.2
decelerates 0 0 0 0 0 66.41
decrease 0 0 0 0 0 0
Table 3: Portion of link value table, showing connection strengths between concepts
are modeled after the points the tutor looks for in
the student?s essay and dialog. For example, in
the ?accelerating car? problem, one key point might
be that the car?s acceleration would decrease as the
car got heavier. The component concepts of this
point would be ?car,? ?acceleration,? ?decrease,? and
?heavier.? If this point were expressed in the di-
alog or essay, we would expect these concepts to
have higher-than-average connection strengths be-
tween them. If this point were not expressed, or only
partially expressed, we would expect lower connec-
tion strengths among its constituent concepts.
The strength of a point, then, was defined as the
sum of strengths of all the links between its compo-
nent concepts. Call the point in the example above
? 

.? point    has n = 4 constituent concepts, and
to find its strength we would sum the link strengths
between their pairs: ?car-acceleration,? ?car-
decrease,? ?car-heavier,? ?acceleration-decrease,?,
?acceleration-heavier,? and ?decrease-heavier.? Us-
ing values from Table 3, the total strength for the
point would therefore be:
 
	ffProceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 54?61,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Manual Annotation of Opinion Categories in Meetings  
 
 
Swapna Somasundaran1,    Janyce Wiebe1,    Paul Hoffmann2,    Diane Litman1 
            1Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260 
                          2Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260 
{swapna,wiebe,hoffmanp,litman}@cs.pitt.edu 
 
 
  
 
Abstract 
This paper applies the categories from an 
opinion annotation scheme developed for 
monologue text to the genre of multiparty 
meetings. We describe modifications to 
the coding guidelines that were required 
to extend the categories to the new type 
of data, and present the results of an in-
ter-annotator agreement study. As re-
searchers have found with other types of 
annotations in speech data, inter-
annotator agreement is higher when the 
annotators both read and listen to the data 
than when they only read the transcripts.   
Previous work exploited prosodic clues 
to perform automatic detection of speaker 
emotion (Liscombe et al 2003). Our 
findings suggest that doing so to recog-
nize opinion categories would be a prom-
ising line of work. 
1 Introduction 
Subjectivity refers to aspects of language that 
express opinions, beliefs, evaluations and specu-
lations (Wiebe et al 2005).  Many natural lan-
guage processing applications could benefit from 
being able to distinguish between facts and opin-
ions of various types, including speech-oriented 
applications such as meeting browsers, meeting 
summarizers, and speech-oriented question an-
swering (QA) systems. Meeting browsers could 
find instances in meetings where opinions about 
key topics are expressed. Summarizers could in-
clude strong arguments for and against issues, to 
make the final outcome of the meeting more un-
derstandable.  A preliminary user survey 
(Lisowska 2003) showed that users would like to 
be able to query meeting records with subjective 
questions like ?Show me the conflicts of opin-
ions between X and Y? , ?Who made the highest 
number of positive/negative comments? and 
?Give me all the contributions of participant X in 
favor of alternative A regarding the issue I.?  A 
QA system with a component to recognize opin-
ions would be able to help find answers to such 
questions. 
Consider the following example from a meet-
ing about an investment firm choosing which car 
to buy1. (In the examples, the words and phrases 
describing or expressing the opinion are under-
lined): 
(1)2 OCK: Revenues of less 
than a million and losses of 
like five million you know 
that's pathetic 
Here, the speaker, OCK, shows his strong nega-
tive evaluation by using the expression ?That?s 
pathetic.? 
(2) OCK: No it might just be 
a piece of junk cheap piece 
of junk that's not a good 
investment 
In (2), the speaker uses the term ?just a piece of 
junk? to express his negative evaluation and uses 
this to argue for his belief that it is ?not a good 
investment.? 
(3) OCK: Yeah I think that's 
the wrong image for an in-
vestment bank he wants sta-
bility and s safety and you 
don't want flashy like zip-
                                                 
1 Throughout this paper we take examples from a meeting 
where a group of people are deciding on a new car for an 
investment bank. The management wants to attract younger 
investors with a sporty car.  
2 We have presented the examples the way they were ut-
tered by the speaker. Hence they may show many false 
starts and repetitions. Capitalization was added to improve 
readability. 
54
ping around the corner kind 
of thing you know 
The example above shows that the speaker has a 
negative judgment towards the suggestion of a 
sports car (that was made in the previous turn) 
which is indicated by the words ?wrong image.? 
The speaker then goes on to positively argue for 
what he wants. He further argues against the cur-
rent suggestion by using more negative terms 
like ?flashy? and ?zipping around the corner.? 
The speaker believes that ?zipping around the 
corner? is bad as it would give a wrong impres-
sion of the bank to the customers. In the absence 
of such analyses, the decision making process 
and rationale behind the outcomes of meetings, 
which form an important part of the organiza-
tion?s memory, might remain unavailable. 
In this paper, we perform annotation of a 
meeting corpus to lay the foundation for research 
on opinion detection in speech. We show how 
categories from an opinion (subjectivity) annota-
tion scheme, which was developed for news arti-
cles, can be applied to the genre of multi-party 
meetings. The new genre poses challenges as it is 
significantly different from the text domain, 
where opinion analysis has traditionally been 
applied. Specifically, differences arise because:  
1) There are many participants interacting with 
one another, each expressing his or her own 
opinion, and eliciting reactions in the process. 
2) Social interactions may constrain how openly 
people express their opinions; i.e., they are often 
indirect in their negative evaluations. 
We also  explore the influence of speech on hu-
man perception of opinions.  
Specifically, we annotated some meeting data 
with the opinion categories Sentiment and Argu-
ing as defined in Wilson and Wiebe (2005). In 
our annotation we first distinguish whether a 
Sentiment or Arguing is being expressed. If one 
is, we then mark the polarity (i.e., positive or 
negative) and the intensity (i.e., how strong the 
opinion is). Annotating the individual opinion 
expressions is useful in this genre, because we 
see many utterances that have more than one 
type of opinion (e.g. (3) above). To investigate 
how opinions are expressed in speech, we divide 
our annotation into two tasks, one in which the 
annotator only reads the raw text, and the other 
in which the annotator reads the raw text and also 
listens to the speech. We measure inter-annotator 
agreement for both tasks.  
We found that the opinion categories apply 
well to the multi-party meeting data, although 
there is some room for improvement: the Kappa 
values range from 0.32 to 0.69.  As has been 
found for other types of annotations in speech, 
agreement is higher when the annotators both 
read and listen to the data than when they only 
read the transcripts. Interestingly, the advantages 
are more dramatic for some categories than oth-
ers.  And, in both conditions, agreement is higher 
for the positive than for the negative categories.  
We discuss possible reasons for these disparities. 
Prosodic clues have been exploited to perform 
automatic detection of speaker emotion (Lis-
combe et al 2003).  Our findings suggest that 
doing so to recognize opinion categories is a 
promising line of work.  
The rest of the paper is organized as follows: 
In Section 2 we discuss the data and the annota-
tion scheme and present examples. We then pre-
sent our inter-annotator agreement results in Sec-
tion 3, and in Section 4 we discuss issues and 
observations. Related work is described in Sec-
tion 5. Conclusions and Future Work are pre-
sented in Section 6. 
2 Annotation  
2.1 Data 
The data is from the ISL meeting corpus (Bur-
ger et al 2002).  We chose task oriented meet-
ings from the games/scenario and discussion 
genres, as we felt they would be closest to the 
applications for which the opinion analysis will 
be useful. The ISL speech is accompanied by 
rich transcriptions, which are tagged according to 
VERBMOBIL conventions. However, since real-
time applications only have access to ASR out-
put, we gave the annotators raw text, from which 
all VERBMOBIL tags, punctuation, and capitali-
zations were removed.  
In order to see how annotations would be af-
fected by the presence or absence of speech, we 
divided each raw text document into 2 segments. 
One part was annotated while reading the raw 
text only. For the annotation of the other part, 
speech as well as the raw text was provided.   
2.2 Opinion Category Definitions  
We base our annotation definitions on the 
scheme developed by Wiebe et al (2005) for 
news articles. That scheme centers on the notion 
of subjectivity, the linguistic expression of pri-
vate states. Private states are internal mental 
states that cannot be objectively observed or veri-
fied (Quirk et al 1985) and include opinions, 
beliefs, judgments, evaluations, thoughts, and 
feelings. Amongst these many forms of subjec-
55
tivity, we focus on the Sentiment and Arguing 
categories proposed by Wilson and Wiebe 
(2005). The categories are broken down by po-
larity and defined as follows:  
Positive Sentiments: positive emotions, 
evaluations, judgments and stances. 
(4) TBC: Well ca How about 
one of the the newer Cadil-
lac the Lexus is good 
In (4), taken from the discussion of which car to 
buy, the speaker uses the term ?good? to express 
his positive evaluation of the Lexus . 
Negative Sentiments: negative emotions, 
evaluations, judgments and stances. 
(5) OCK: I think these are 
all really bad choices 
In (5), the speaker expresses his negative evalua-
tion of the choices for the company car. Note that 
?really? makes the evaluation more intense.  
Positive Arguing:  arguing for something, ar-
guing that something is true or is so, arguing that 
something did happen or will happen, etc. 
(6) ZDN: Yeah definitely 
moon roof  
In (6), the speaker is arguing that whatever car 
they get should have a moon roof. 
Negative Arguing: arguing against some-
thing, arguing that something is not true or is not 
so, arguing that something did not happen or will 
not happen, etc. 
(7) OCK: Like a Lexus or 
perhaps a Stretch Lexus 
something like that but that 
might be too a little too 
luxurious 
In the above example, the speaker is using the 
term ?a little too luxurious? to argue against a 
Lexus for the car choice.  
In an initial tagging experiment, we applied 
the above definitions, without modification, to 
some sample meeting data. The definitions cov-
ered much of the arguing and sentiment we ob-
served. However, we felt that some cases of Ar-
guing that are more prevalent in meeting than in 
news data needed to be highlighted more, namely 
Arguing opinions that are implicit or that under-
lie what is explicitly said. Thus we add the fol-
lowing to the arguing definitions. 
Positive Arguing: expressing support for or 
backing the acceptance of an object, viewpoint, 
idea or stance by providing reasoning, justifica-
tions, judgment, evaluations or beliefs. This sup-
port or backing may be explicit or implicit. 
(8) MHJ: That's That's why I 
wanna What about the the 
child safety locks I think I 
think that would be a good 
thing because if our custom-
ers happen to have children  
Example (8) is marked as both Positive Arguing 
and Positive Sentiment. The more explicit one is 
the Positive Sentiment that the locks are good. 
The underlying Argument is that the company 
car they choose should have child safety locks. 
Negative Arguing: expressing lack of support 
for or attacking the acceptance of an object, 
viewpoint, idea or stance by providing reasoning, 
justifications, judgment, evaluations or beliefs. 
This may be explicit or implicit. 
(9) OCK: Town Car But it's a 
little a It's a little like 
your grandf Yeah your grand-
father would drive that 
Example (9) is explicitly stating who would drive 
a Town Car, while implicitly arguing against 
choosing the Town Car (as they want younger 
investors). 
2.3 Annotation Guidelines 
Due to genre differences, we also needed to 
modify the annotation guidelines. For each Argu-
ing or Sentiment the annotator perceives, he or 
she identifies the words or phrases used to ex-
press it (the text span), and then creates an anno-
tation consisting of the following. 
? Opinion Category and Polarity 
? Opinion Intensity 
? Annotator Certainty 
Opinion Category and Polarity: These are 
defined in the previous sub-section. Note that the 
target of an opinion is what the opinion is about. 
For example, the target of ?John loves baseball? 
is baseball.   An opinion may or may not have a 
separate target.  For example, ?want stability? in 
?We want stability? denotes a Positive Senti-
ment, and there is no separate target.  In contrast, 
?good? in ?The Lexus is good? expresses a Posi-
tive Sentiment and there is a separate target, 
namely the Lexus. 
In addition to Sentiments toward a topic of 
discussion, we also mark Sentiments toward 
other team members (e.g. ?Man you guys 
are so limited?). We do not mark 
agreements or disagreements as Sentiments, as 
these are different dialog acts (though they some-
times co-occur with Sentiments and Arguing).  
Intensity: We use a slightly modified version 
of Craggs and Wood's (2004) emotion intensity 
56
annotation scheme. According to that scheme, 
there are 5 levels of intensity. Level ?0? denotes 
a lack of the emotion (Sentiment or Arguing in 
our case), ?1? denotes traces of emotion, ?2? de-
notes a low level of emotion, ?3? denotes a clear 
expression while ?4? denotes a strong expres-
sion. Our intensity levels mean the same, but we 
do not mark intensity level 0 as this level implies 
the absence of opinion. 
If a turn has multiple, separate expressions 
marked with the same opinion tag (category and 
polarity), and all expressions refer to the same 
target, then the annotators merge all the expres-
sions into a larger text span, including the sepa-
rating text in between the  expressions. This re-
sulting text span has the same opinion tag as its 
constituents, and it has an intensity that is greater 
than or equal to the highest intensity of the con-
stituent expressions that were merged. 
Annotator Certainty: The annotators use this 
tag if they are not sure that a given opinion is 
present, or if, given the context, there are multi-
ple possible interpretations of the utterance and 
the annotator is not sure which interpretation is 
correct. This attribute is distinct from the Inten-
sity attribute, because the Intensity attribute indi-
cates the strength of the opinion, while the Anno-
tator Certainty attribute indicates whether the 
annotator is sure about a given tag (whatever the 
intensity is). 
2.4 Examples 
We conclude this section with some examples 
of annotations from our corpus.  
(10) OCK: So Lexun had reve-
nues of a hundred and fifty 
million last year and prof-
its of like six million.  
That's pretty good 
Annotation: Text span=That's 
pretty good Cate-
gory=Positive Sentiment In-
tensity=3 Annotator Cer-
tainty=Certain  
The annotator marked the text span ?That?s 
pretty good? as Positive Sentiment because this 
this expression is used by OCK to show his fa-
vorable judgment towards the company reve-
nues. The intensity is 3, as it is a clear expression 
of Sentiment.  
(11) OCK: No it might just 
be a piece of junk Cheap 
piece of junk that?s not a 
good investment 
Annotation1: Text span=it 
might just be a piece of 
junk Cheap piece of junk 
that?s not a good investment 
Category=Negative Sentiment 
Intensity=4 Annotator Cer-
tainty=Certain 
Annotation2: Text span=Cheap 
piece of junk that?s not a 
good investment Category 
=Negative Arguing Inten-
sity=3 Annotator Certainty 
=Certain  
In the above example, there are multiple expres-
sions of opinions. In Annotation1, the expres-
sions ?it might just be a piece of junk?, ?cheap 
piece of junk? and ?not a good investment? ex-
press negative evaluations towards the car choice 
(suggested by another participant in a previous 
turn). Each of these expressions is a clear case of 
Negative Sentiment (Intensity=3). As they are all 
of the same category and polarity and towards 
the same target, they have been merged by the 
annotator into one long expression of Inten-
sity=4. In Annotation2, the sub-expression 
?cheap piece of junk that is not a good invest-
ment? is also used by the speaker OCK to argue 
against the car choice. Hence the annotator has 
marked this as Negative Arguing.  
3 Guideline Development and Inter-
Annotator Agreement 
3.1 Annotator Training 
Two annotators (both co-authors) underwent 
three rounds of tagging. After each round, dis-
crepancies were discussed, and the guidelines 
were modified to reflect the resolved ambiguities. 
A total of 1266 utterances belonging to sections 
of four meetings (two of the discussion genre and 
two of the game genre) were used in this phase. 
3.2 Agreement  
The unit for which agreement was calculated 
was the turn. The ISL transcript provides demar-
cation of speaker turns along with the speaker ID. 
If an expression is marked in a turn, the turn is 
assigned the label of that expression. If there are 
multiple expressions marked within a turn with 
different category tags, the turn is assigned all 
those categories. This does not pose a problem 
for our evaluation, as we evaluate each category 
separately. 
A previously unseen section of a meeting con-
taining 639 utterances was selected and divided 
57
into 2 segments. One part of 319 utterances was 
annotated using raw text as the only signal, and 
the remaining 320 utterances were annotated us-
ing text and speech. Cohen?s Kappa (1960) was 
used to calculate inter-annotator agreement. We 
calculated inter-annotator agreement for both 
conditions: raw-text-only and raw-text+speech. 
This was done for each of the categories: Posi-
tive Sentiment, Positive Arguing, Negative Sen-
timent, and Negative Arguing. To evaluate a 
category, we did the following:  
? For each turn, if both annotators tagged 
the turn with the given category, or both 
did not tag the turn with the category, then 
it is a match.  
? Otherwise it is a mismatch 
Table 1 shows the inter-annotator Kappa val-
ues on the test set. 
 
Agreement (Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.54 0.60 
Negative Arguing 0.32 0.65 
Positive Sentiment 0.57 0.69 
Negative Sentiment 0.41 0.61 
Table 1 Inter-annotator agreement on different 
categories. 
 
With raw-text-only annotation, the Kappa 
value is in the moderate range according to 
Landis and Koch (1977), except for Negative 
Arguing for which it is 0.32. Positive Arguing 
and Positive Sentiment were more reliably de-
tected than Negative Arguing and Negative Sen-
timent. We believe this is because participants 
were more comfortable with directly expressing 
their positive sentiments in front of other partici-
pants.  Given only the raw text data, inter-
annotator reliability measures for Negative Argu-
ing and Negative Sentiment are the lowest. We 
believe this might be due to the fact that partici-
pants in social interactions are not very forthright 
with their Negative Sentiments and Arguing. 
Negative Sentiments and Arguing towards some-
thing may be expressed by saying that something 
else is better. For example, consider the follow-
ing response of one participant to another par-
ticipant?s suggestion of aluminum wheels for the 
company car 
(12) ZDN: Yeah see what kind 
of wheels you know they have 
to look dignified to go with 
the car 
The above example was marked as Negative Ar-
guing by one annotator (i.e., they should not get 
aluminum wheels) while the other annotator did 
not mark it at all. The implied Negative Arguing 
toward getting aluminum wheels can be inferred 
from the statement that the wheels should look 
dignified. However the annotators were not sure, 
as the participant chose to focus on what is desir-
able (i.e., dignified wheels). This utterance is 
actually both a general statement of what is de-
sirable, and an implication that aluminum wheels 
are not dignified. But this may be difficult to as-
certain with the raw text signal only.  
When the annotators had speech to guide their 
judgments, the Kappa values go up significantly 
for each category. All the agreement numbers for 
raw text+speech are in the substantial range ac-
cording to Landis and Koch (1977). We observe 
that with speech, Kappa for Negative Arguing 
has doubled over the Kappa obtained without 
speech. The Kappa for Negative Sentiment 
(text+speech) shows a 1.5 times improvement 
over the one with only raw text. Both these ob-
servations indicate that speech is able to help the 
annotators tag negativity more reliably. It is quite 
likely that a seemingly neutral sentence could 
sound negative, depending on the way words are 
stressed or pauses are inserted. Comparing the 
agreement on Positive Sentiment, we get a 1.2 
times improvement by using speech. Similarly, 
agreement improves by 1.1 times for Positive 
Arguing when speech is used. The improvement 
with speech for the Positive categories is not as 
high as compared to negative categories, which 
conforms to our belief that people are more 
forthcoming about their positive judgments, 
evaluations, and beliefs.  
In order to test if the turns where annotators 
were uncertain were the places that caused mis-
match, we calculated the Kappa with the annota-
tor-uncertain cases removed. The corresponding 
Kappa values are shown in Table 2 
 
Agreement ( Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.52 0.63 
Negative Arguing 0.36 0.63 
Positive Sentiment 0.60 0.73 
Negative Sentiment 0.50 0.61 
Table-2 Inter-annotator agreement on different 
categories, Annotator Uncertain cases removed. 
 
The trends observed in Table 1 are seen in Ta-
ble 2 as well, namely annotation reliability im-
proving with speech. Comparing Tables 1 and 2, 
58
we see that for the raw text, the inter-annotator 
agreement goes up by 0.04 points for Negative 
Arguing and goes up by 0.09 points for Negative 
Sentiment. However, the agreement for Negative 
Arguing and Negative Sentiment on raw-text+ 
speech between Tables 1 and 2 remains almost 
the same. We believe this is  because we had 
20% fewer Annotator Uncertainty tags in the 
raw-text+speech annotation as compared to raw-
text-only, thus indicating that some types of un-
certainties seen in raw-text-only were resolved in 
the raw-text+speech due to the speech input. The 
remaining cases of Annotator Uncertainty could 
have been due to other factors, as discussed in 
the next section 
Table 3 shows Kappa with the low intensity 
tags removed. The hypothesis was that low in-
tensity might be borderline cases, and that re-
moving these might increase inter-annotator reli-
ability.  
 
Agreement ( Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.53 0.66 
Negative Arguing 0.26 0.65 
Positive Sentiment 0.65 0.74 
Negative Sentiment 0.45 0.59 
Table-3 Inter-annotator agreement on different 
categories, Intensity 1, 2 removed. 
 
Comparing Tables 1 and 3 (the raw-text col-
umns), we see that there is an improvement in 
the agreement on sentiment (both positive and 
negative) if the low intensity cases are removed.  
The agreement for Negative Sentiment (raw-text) 
goes up marginally by 0.04 points.  Surprisingly, 
the agreement for Negative Arguing (raw-text) 
goes down by 0.06 points. Similarly in raw-
text+speech results, removal of low intensity 
cases does not improve the agreement for Nega-
tive Arguing while hurting Negative Sentiment 
category (by 0.02 points). One possible explana-
tion is that it may be equally difficult to detect 
Negative categories at both low and high intensi-
ties. Recall that in (12) it was difficult to detect if 
there is  Negative Arguing at all. If the annotator 
decided that it is indeed a Negative Arguing, it is 
put at intensity level=3 (i.e., a clear case). 
4 Discussion 
There were a number of interesting subjectiv-
ity related phenomena in meetings that we ob-
served during our annotation. These are issues 
that will need to be addressed for improving in-
ter-annotator reliability. 
Global and local context for arguing: In the 
context of a meeting, participants argue for (posi-
tively) or against (negatively) a topic. This may 
become ambiguous when the participant uses an 
explicit local Positive Arguing and an implicit 
global Negative Arguing. Consider the following 
speaker turn, at a point in the meeting when one 
participant has suggested that the company car 
should have a moon roof and another participant 
has opposed it, by saying that a moon roof would 
compromise the headroom. 
(13) OCK: We wanna make sure 
there's adequate headroom 
for all those six foot six 
investors 
In the above example, the speaker OCK, in the 
local context of the turn, is arguing positively 
that headroom is important. However, in the 
global context of the meeting, he is arguing 
against the idea of a moon roof that was sug-
gested by a participant. Such cases occur when 
one object (or opinion) is endorsed which auto-
matically precludes another, mutually exclusive 
object (or opinion).  
Sarcasm/Humor: The meetings we analyzed 
had a large amount of sarcasm and humor. Issues 
arose with sarcasm due to our approach of mark-
ing opinions towards the content of the meeting 
(which forms the target of the opinion). Sarcasm 
is difficult to annotate because sarcasm can be 
1) On topic: Here the target is the topic of dis-
cussion and hence sarcasm is used as a Negative 
Sentiment. 
2) Off topic: Here the target is not a topic un-
der discussion, and the aim is to purely elicit 
laughter. 
3) Allied topic: In this case, the target is re-
lated to the topic in some way, and it?s difficult 
to determine if the aim of the sarcasm/humor was 
to elicit laughter or to imply something negative 
towards the topic.  
Multiple modalities: In addition to text and 
speech, gestures and visual diagrams play an im-
portant role in some types of meetings. In one 
meeting that we analyzed, participants were 
working together to figure out how to protect an 
egg when it is dropped from a long distance, 
given the materials they have. It was evident they 
were using some gestures to describe their ideas 
(?we can put tape like this?) and that they drew 
diagrams to get points across. In the absence of 
visual input, annotators would need to guess 
59
what was happening. This might further hurt the 
inter-annotator reliability. 
5 Related Work  
Our opinion categories are from the subjectiv-
ity schemes described in Wiebe et al (2005) and 
Wilson and Wiebe (2005). Wiebe et al (2005) 
perform expression level annotation of opinions 
and subjectivity in text. They define their annota-
tions as an experiencer having some type of atti-
tude (such as Sentiment or Arguing), of a certain 
intensity, towards a target. Wilson and Wiebe 
(2005) extend this basic annotation scheme to 
include different types of subjectivity, including 
Positive Sentiment, Negative Sentiment, Positive 
Arguing, and Negative Arguing. 
Speech was found to improve inter-annotator 
agreement in discourse segmentation of mono-
logs (Hirschberg and Nakatani 1996). Acoustic 
clues have been successfully employed for the 
reliable detection of the speaker?s emotions, in-
cluding frustration, annoyance, anger, happiness, 
sadness, and boredom (Liscombe et al 2003).  
Devillers et al (2003) performed perceptual tests 
with and without speech in detecting the 
speaker?s fear, anger, satisfaction and embar-
rassment.  Though related, our work is not con-
cerned with the speaker?s emotions, but rather 
opinions toward the issues and topics addressed 
in the meeting. 
Most annotation work in multiparty conversa-
tion has focused on exchange structures and dis-
course functional units like common grounding 
(Nakatani and Traum, 1998). In common ground-
ing research, the focus is on whether the partici-
pants of the discourse are able to understand each 
other, and not their opinions towards the content 
of the discourse. Other tagging schemes like the 
one proposed by Flammia and Zue (1997) focus 
on information seeking and question answering 
exchanges where one participant is purely seek-
ing information, while the other is providing it. 
The SWBD DAMSL (Jurafsky et al, 1997) an-
notation scheme over the Switchboard telephonic 
conversation corpus labels shallow discourse 
structures. The SWBD-DAMSL had a label ?sv? 
for opinions. However, due to poor inter-
annotator agreement, the authors discarded these 
annotations. The ICSI MRDA annotation scheme 
(Rajdip et al, 2003) adopts the SWBD DAMSL 
scheme, but does not distinguish between the 
opinionated and objective statements. The ISL 
meeting corpus (Burger and Sloane, 2004) is an-
notated with dialog acts and discourse moves like 
initiation and response, which in turn consist of 
dialog tags such as query, align, and statement. 
Their statement dialog category would not only 
include Sentiment and Arguing tags discussed in 
this paper, but it would also include objective 
statements and other types of subjectivity. 
?Hot spots? in meetings closely relate to our 
work because they find sections in the meeting 
where participants are involved in debates or 
high arousal activity (Wrede and Shriberg 2003). 
While that work distinguishes between high 
arousal and low arousal, it does not distinguish 
between  opinion or non-opinion or the different 
types of opinion. However, Janin et al (2004) 
suggest that there is a relationship between dia-
log acts and involvement, and that involved ut-
terances contain significantly more evaluative 
and subjective statements as well as extremely 
positive or negative answers. Thus we believe it 
may be beneficial for such works to make these 
distinctions. 
Another closely related work that finds par-
ticipants? positions regarding issues is argument 
diagramming (Rienks et al 2005). This ap-
proach, based on the IBIS system (Kunz and Rit-
tel 1970), divides a discourse into issues, and 
finds lines of deliberated arguments. However 
they do not distinguish between subjective and 
objective contributions towards the meeting. 
6 Conclusions and Future Work 
In this paper we performed an annotation 
study of opinions in meetings, and investigated 
the effects of speech. We have shown that it is 
possible to reliably detect opinions within multi-
party conversations. Our consistently better 
agreement results with text+speech input over 
text-only input suggest that speech is a reliable 
indicator of opinions. We have also found that 
Annotator Uncertainty decreased with speech 
input. Our results also show that speech is a more 
informative indicator for negative versus positive 
categories. We hypothesize that this is due to the 
fact the people express their positive attitudes 
more explicitly. The speech signal is thus even 
more important for discerning negative opinions. 
This experience has also helped us gain insights 
to the ambiguities that arise due to sarcasm and 
humor. 
Our promising results open many new avenues 
for research. It will be interesting to see how our 
categories relate to other discourse structures, 
both at the shallow level (agree-
ment/disagreement) as well as at the deeper level 
60
(intentions/goals). It will also be interesting to 
investigate how other forms of subjectivity like 
speculation and intention are expressed in multi-
party discourse. Finding prosodic correlates of 
speech as well as lexical clues that help in opin-
ion detection would be useful in building subjec-
tivity detection applications for multiparty meet-
ings.  
References 
Susanne Burger and Zachary A Sloane. 2004. The ISL 
Meeting Corpus: Categorical Features of Commu-
nicative Group Interactions. NIST Meeting Recog-
nition Workshop 2004, NIST 2004, Montreal, Can-
ada, 2004-05-17 
Susanne Burger, Victoria MacLaren and Hua Yu. 
2002. The ISL Meeting Corpus: The Impact of 
Meeting Type on Speech Style. ICSLP-2002. Den-
ver, CO: ISCA, 9 2002. 
Jacob Cohen. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Meas., 20:37?46. 
Richard Craggs and Mary McGee Wood. 2004. A 
categorical annotation scheme for emotion in the 
linguistic content of dialogue. Affective Dialogue 
Systems. 2004. 
Laurence Devillers, Lori Lamel and Ioana Vasilescu. 
2003. Emotion detection in task-oriented spoken 
dialogs. IEEE International Conference on Multi-
media and Expo (ICME). 
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey and 
Elizabeth Shriberg. 2003. ?Meeting Recorder Pro-
ject: Dialog Act Labeling Guide,? ICSI Technical 
Report TR-04-002, Version 3, October 2003 
Giovanni Flammia and Victor Zue. 1997. Learning 
The Structure of Mixed Initiative Dialogues Using 
A Corpus of Annotated Conversations. Eurospeech 
1997, Rhodes, Greece 1997, p1871?1874 
Julia Hirschberg and Christine Nakatani. 1996. A Pro-
sodic Analysis of Discourse Segments in Direction-
Giving Monologues Annual Meeting- Association 
For Computational Linguistics 1996, VOL 34, 
pages 286-293 
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhil-
lon, Jane Edwards, Javier Mac??as-Guarasa, Nelson 
Morgan, Barbara Peskin, Elizabeth Shriberg, An-
dreas Stolcke, Chuck Wooters and Britta Wrede. 
2004. ?The ICSI Meeting Project: Resources and 
Research,?  ICASSP-2004 Meeting Recognition 
Workshop. Montreal; Canada: NIST, 5 2004 
Daniel Jurafsky, Elizabeth Shriberg and Debra Biasca, 
1997. Switchboard-DAMSL Labeling Project 
Coder?s Manual. 
http://stripe.colorado.edu/?jurafsky/manual.august1 
Werner Kunz and Horst W. J. Rittel. 1970. Issues as 
elements of information systems. Working Paper 
WP-131, Univ. Stuttgart, Inst. Fuer Grundlagen der 
Planung, 1970 
Richard Landis and Gary Koch. 1977. The Measure-
ment of Observer Agreement for Categorical Data 
Biometrics, Vol. 33, No. 1 (Mar., 1977) , pp. 159-
174 
Agnes Lisowska. 2003. Multimodal interface design 
for the multimodal meeting domain: Preliminary 
indications from a query analysis study. Technical 
Report IM2.  Technical report, ISSCO/TIM/ETI. 
Universit de Genve, Switserland, November 2003. 
Jackson Liscombe, Jennifer Venditti and Julia 
Hirschberg. 2003. Classifying Subject Ratings of 
Emotional Speech Using Acoustic Features. Eu-
rospeech 2003. 
Christine Nakatani and David Traum. 1998. Draft: 
Discourse Structure Coding Manual version 
2/27/98 
Randolph Quirk, Sidney Greenbaum, Geoffry Leech 
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New 
York.s 
Rutger Rienks, Dirk Heylen and Erik van der Wei-
jden. 2005. Argument diagramming of meeting 
conversations. In Vinciarelli, A. and Odobez, J., 
editors, Multimodal Multiparty Meeting Process-
ing, Workshop at the 7th International Conference 
on Multimodal Interfaces, pages 85?92, Trento, It-
aly 
Janyce Wiebe, Theresa Wilson and Claire Cardie. 
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and 
Evaluation (formerly Computers and the Humani-
ties), volume 39, issue 2-3, pp. 165-210.  
Theresa Wilson and Janyce Wiebe. 2005. Annotating 
attributions and private states. ACL Workshop on 
Frontiers in Corpus Annotation II: Pie in the Sky.  
Britta Wrede and Elizabeth Shriberg. 2003. Spotting 
"Hotspots" in Meetings: Human Judgments and 
Prosodic Cues.  Eurospeech 2003, Geneva 
 
61
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, page 152,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Invited Talk
Discourse and Dialogue Processing in Spoken
Intelligent Tutoring Systems
Diane J. Litman
Computer Science Department &
Learning Research and Development Center
University of Pittsburgh
Pittsburgh, PA USA 15260
email: litman@cs.pitt.edu
Abstract
In recent years, the development of intelligent tutoring dialogue systems
has become more prevalent, in an attempt to close the performance gap be-
tween human and computer tutors. Tutoring applications differ in many
ways, however, from the types of applications for which spoken dialogue
systems are typically developed. This talk will illustrate some of the op-
portunities and challenges in this area, focusing on issues such as affective
reasoning, discourse and dialogue analysis, and performance evaluation.
152
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 85?93,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploiting Discourse Structure for  
Spoken Dialogue Performance Analysis 
 
 
Mihai Rotaru 
University of Pittsburgh 
Pittsburgh, USA 
mrotaru@cs.pitt.edu 
Diane J. Litman 
University of Pittsburgh 
Pittsburgh, USA 
litman@cs.pitt.edu 
 
  
 
Abstract 
In this paper we study the utility of dis-
course structure for spoken dialogue per-
formance modeling. We experiment with 
various ways of exploiting the discourse 
structure: in isolation, as context infor-
mation for other factors (correctness and 
certainty) and through trajectories in the 
discourse structure hierarchy. Our corre-
lation and PARADISE results show that, 
while the discourse structure is not useful 
in isolation, using the discourse structure 
as context information for other factors 
or via trajectories produces highly predic-
tive parameters for performance analysis. 
1 Introduction 
Predictive models of spoken dialogue system 
(SDS) performance are an important tool for re-
searchers and practitioners in the SDS domain. 
These models offer insights on what factors are 
important for the success of a SDS and allow 
researchers to assess the performance of future 
system improvements without running additional 
costly user experiments. 
One of the most popular models of perform-
ance is the PARADISE framework proposed by 
(Walker et al, 2000). In PARADISE, a set of 
interaction parameters are measured in a SDS 
corpus, and then used in a multivariate linear 
regression to predict the target performance met-
ric. A critical ingredient in this approach is the 
relevance of the interaction parameters for the 
SDS success. A number of parameters that 
measure the dialogue efficiency (e.g. number of 
system/user turns, task duration) and the dia-
logue quality (e.g. recognition accuracy, rejec-
tions, helps) have been shown to be successful in 
(Walker et al, 2000). An extensive set of pa-
rameters can be found in (M?ller, 2005a). 
In this paper we study the utility of discourse 
structure as an information source for SDS per-
formance analysis. The discourse structure hier-
archy has been shown to be useful for other 
tasks: understanding specific lexical and pro-
sodic phenomena (Hirschberg and Nakatani, 
1996; Levow, 2004), natural language generation 
(Hovy, 1993), predictive/generative models of 
postural shifts (Cassell et al, 2001), and essay 
scoring (Higgins et al, 2004). 
We perform our analysis on a corpus of 
speech-based tutoring dialogues. A tutoring SDS 
(Litman and Silliman, 2004; Pon-Barry et al, 
2004) has to discuss concepts, laws and relation-
ships and to engage in complex subdialogues to 
correct student misconceptions. As a result, dia-
logues with such systems have a rich discourse 
structure. 
We perform three experiments to measure 
three ways of exploiting the discourse structure. 
In our first experiment, we test the predictive 
utility of the discourse structure in itself. For ex-
ample, we look at whether the number of pop-up 
transitions in the discourse structure hierarchy 
predicts performance in our system. 
The second experiment measures the utility of 
the discourse structure as contextual information 
for two types of student states: correctness and 
certainty. The intuition behind this experiment is 
that interaction events should be treated differ-
ently based on their position in the discourse 
structure hierarchy. For example, we test if the 
number of incorrect answers after a pop-up tran-
sition has a higher predictive utility than the total 
number of incorrect student answers. In contrast, 
the majority of the previous work either ignores 
this contextual information (M?ller, 2005a; 
Walker et al, 2000) or makes limited use of the 
85
discourse structure hierarchy by flattening it 
(Walker et al, 2001) (Section 5). 
As another way to exploit the discourse struc-
ture, in our third experiment we look at whether 
specific trajectories in the discourse structure are 
indicative of performance. For example, we test 
if two consecutive pushes in the discourse struc-
ture are correlated with higher learning. 
To measure the predictive utility of our inter-
action parameters, we focus primarily on corre-
lations with our performance metric (Section 4). 
There are two reasons for this. First, a significant 
correlation between an interaction parameter and 
the performance metric is a good indicator of the 
parameter?s relevance for PARADISE modeling. 
Second, correlations between factors and the per-
formance metric are commonly used in tutoring 
research to analyze the tutoring/learning process 
(Chi et al, 2001). 
Our correlation and PARADISE results show 
that, while the discourse structure is not useful in 
isolation, using the discourse structure as context 
information for other factors or via trajectories 
produces highly predictive parameters for per-
formance analysis. 
2 Annotation 
Our annotation for discourse structure and stu-
dent state has been performed on a corpus of 95 
experimentally obtained spoken tutoring dia-
logues between 20 students and our system 
ITSPOKE (Litman and Silliman, 2004).  
ITSPOKE is a speech-enabled version of the 
text-based Why2-Atlas conceptual physics tutor-
ing system (VanLehn et al, 2002). When inter-
acting with ITSPOKE, students first type an es-
say answering a qualitative physics problem us-
ing a graphical user interface. ITSPOKE then 
engages the student in spoken dialogue (using 
head-mounted microphone input and speech out-
put) to correct misconceptions and elicit more 
complete explanations, after which the student 
revises the essay, thereby ending the tutoring or 
causing another round of tutoring/essay revision. 
Each student went through the same procedure: 
1) read a short introductory material, 2) took a 
pretest to measure the initial physics knowledge, 
3) work through a set of 5 problems with 
ITSPOKE, and 4) took a posttest similar to the 
pretest. The resulting corpus had 2334 student 
turns and a comparable number of system turns. 
2.1 Discourse structure 
We base our annotation of discourse structure on 
the Grosz & Sidner theory of discourse structure 
(Grosz and Sidner, 1986). A critical ingredient of 
this theory is the intentional structure. According 
to the theory, each discourse has a discourse pur-
pose/intention. Satisfying the main discourse 
purpose is achieved by satisfying several smaller 
purposes/intentions organized in a hierarchical 
structure. As a result, the discourse is segmented 
in discourse segments each with an associated 
discourse segment purpose/intention. This theory 
has inspired several generic dialogue managers 
for spoken dialogue systems (Bohus and Rud-
nicky, 2003). 
 
Figure 1. The discourse structure and transition anno-
tation 
We automate our annotation of the discourse 
structure by taking advantage of the structure of 
the tutored information. A dialogue with 
ITSPOKE follows a question-answer format (i.e. 
system initiative): ITSPOKE asks a question, the 
student provides the answer and then the process 
is repeated. Deciding what question to ask, in 
what order and when to stop is hand-authored 
beforehand in a hierarchical structure that resem-
bles the discourse segment structure (see Figure 
1). Tutor questions are grouped in segments 
which correspond roughly to the discourse seg-
ments. Similarly to the discourse segment pur-
pose, each question segment has an associated 
tutoring goal or purpose. For example, in 
86
ITSPOKE there are question segments discuss-
ing about forces acting on the objects, others dis-
cussing about objects? acceleration, etc. 
In Figure 1 we illustrate ITSPOKE?s behavior 
and our discourse structure annotation. First, 
based on the analysis of the student essay, 
ITSPOKE selects a question segment to correct 
misconceptions or to elicit more complete expla-
nations. This question segment will correspond 
to the top level discourse segment (e.g. DS1). 
Next, ITSPOKE asks the student each question 
in DS1. If the student answer is correct, the sys-
tem moves on to the next question (e.g. Tu-
tor1?Tutor2). If the student answer is incorrect, 
there are two alternatives. For simple questions, 
the system will simply give out the correct an-
swer and move on to the next question (e.g. Tu-
tor3?Tutor4). For complex questions (e.g. apply-
ing physics laws), ITSPOKE will engage into a 
remediation subdialogue that attempts to reme-
diate the student?s lack of knowledge or skills. 
The remediation subdialogue is specified in an-
other question segment and corresponds to a new 
discourse segment (e.g DS2). The new discourse 
segment is dominated by the current discourse 
segment (e.g. DS2 dominated by DS1). Tutor2 
system turn is a typical example; if the student 
answers it incorrectly, ITSPOKE will enter dis-
course segment DS2 and go through its questions 
(Tutor3 and Tutor4). Once all the questions in 
DS2 have been answered, a heuristic determines 
whether ITSPOKE should ask the original ques-
tion again (Tutor2) or simply move on to the next 
question (Tutor5). 
To compute interaction parameters from the 
discourse structure, we focus on the transitions in 
the discourse structure hierarchy. For each sys-
tem turn we define a transition feature. This fea-
ture captures the position in the discourse struc-
ture of the current system turn relative to the 
previous system turn. We define six labels (see 
Table 1). NewTopLevel label is used for the first 
question after an essay submission (e.g. Tutor1). 
If the previous question is at the same level with 
the current question we label the current question 
as Advance (e.g. Tutor2,4). The first question in a 
remediation subdialogue is labeled as Push (e.g. 
Tutor3). After a remediation subdialogue is com-
pleted, ITSPOKE will pop up and it will either 
ask the original question again or move on to the 
next question. In the first case, we label the sys-
tem turn as PopUp. Please note that Tutor2 will 
not be labeled with PopUp because, in such 
cases, an extra system turn will be created be-
tween Tutor4 and Tutor5 with the same content as 
Tutor2. In addition, variations of ?Ok, back to the 
original question? are also included in the new 
system turn to mark the discourse segment 
boundary transition. If the system moves on to 
the next question after finishing the remediation 
subdialogue, we label the system turn as  
PopUpAdv (e.g. Tutor5). Note that while the 
sum of PopUp and PopUpAdv should be equal 
with Push, it is smaller in our corpus because in 
some cases ITSPOKE popped up more than one 
level in the discourse structure hierarchy. In case 
of rejections, the system question is repeated us-
ing variations of ?Could you please repeat that??. 
We label such cases as SameGoal (e.g. Tutor6). 
Discourse structure transitions 
  
Advance 
NewTopLevel 
PopUp 
PopUpAdv 
Push 
SameGoal 
53.4% 
13.5% 
9.2% 
3.5% 
14.5% 
5.9% 
Certainty 
  
Certain 
Uncertain 
Mixed 
Neutral 
41.3% 
19.1% 
2.4% 
37.3% 
Correctness 
  
Correct 
Incorrect 
Partially Correct 
Unable to Answer 
63.3% 
23.3% 
6.2% 
7.1% 
Table 1: Transition and student state distribution. 
Please note that each student dialogue has a 
specific discourse structure based on the dialogue 
that dynamically emerges based on the correct-
ness of her answers. For this reason, the same 
system question in terms of content may get a 
different transition label for different students. 
For example, in Figure 1, if the student would 
have answered Tutor2 correctly, the next tutor 
turn would have had the same content as Tutor5 
but the Advance label. Also, while a human an-
notation of the discourse structure will be more 
complex but more time consuming (Hirschberg 
and Nakatani, 1996; Levow, 2004), its advan-
tages are outweighed by the automatic nature of 
our discourse structure annotation. 
We would like to highlight that our transition 
annotation is domain independent and automatic. 
Our transition labels capture behavior like start-
ing a new dialogue (NewTopLevel), crossing 
discourse segment boundaries (Push, PopUp, 
PopUpAdv) and local phenomena inside a dis-
course segment (Advance, SameGoal). If the dis-
course structure information is available, the 
87
transition information can be automatically com-
puted using the procedure described above. 
2.2 Student state 
Because for our tutoring system student learning 
is the relevant performance metric, we hypothe-
size that information about student state in each 
student turn, in terms of correctness and cer-
tainty, will be an important indicator. For exam-
ple, a student being more correct and certain dur-
ing her interaction with ITSPOKE might be 
indicative of a higher learning gain. Also, 
previous studies have shown that tutoring 
specific parameters can improve the quality of 
SDS performance models that model the learning 
gain (Forbes-Riley and Litman, 2006).  
In our corpus, each student turn was manually 
labeled for correctness and certainty (Table 1). 
While our system assigns a correctness label to 
each student turn to plan its next move, we 
choose to use a manual annotation of correctness 
to eliminate the noise introduced by the auto-
matic speech recognition component and the 
natural language understanding component. A 
human annotator used the human transcripts and 
his physics knowledge to label each student turn 
for various degrees of correctness: correct, par-
tially correct, incorrect and unable to answer. 
?Unable to Answer? label was used for turns 
where the student did not answer the system 
question or used variants of ?I don?t know?. 
Previous work has shown that certainty plays 
an important role in the learning and tutoring 
process (Pon-Barry et al, 2006; VanLehn et al, 
2003). A human annotator listened to the dia-
logues between students and ITSPOKE and la-
beled each student turn for its perceived degree 
of certainness. Four labels were used: certain, 
uncertain, neutral and mixed (both certain and 
uncertain). To date, one annotator has labeled all 
student turns in our corpus1. 
3 Interaction parameters 
For each user, interaction parameters measure 
specific aspects of the dialogue with the system. 
We use our transition and student state annota-
tion to create two types of interaction parame-
                                                 
1 The agreement between the manual correctness an-
notation and the correctness assigned by ITSPOKE is 
90% (kappa of 0.79). In a preliminary agreement 
study, a second annotator labeled our corpus for a 
binary version of certainty (uncertainty versus other), 
resulting in a 90% inter-annotator agreement and a 
kappa of 0.68. 
ters: unigrams and bigrams. The difference be-
tween the two types of parameters is whether the 
discourse structure context is used or not. For 
each of our 12 labels (4 for correctness, 4 for 
certainty and 6 for discourse structure), we de-
rive two unigram parameters per student over the 
5 dialogues for that student: a total parameter 
and a percentage parameter. For example, for the 
?Incorrect? unigram we compute, for each stu-
dent, the total number of student turns labeled 
with ?Incorrect? (parameter Incorrect) and the 
percentage of such student turns out of all stu-
dent turns (parameter Incorrect%). For example, 
if we consider only the dialogue in Figure 1, In-
correct = 3 (Student2,3,5) and Incorrect% = 60% 
(3 out of 5). 
Bigram parameters exploit the discourse struc-
ture context. We create two classes of bigram 
parameters by looking at transition?student state 
bigrams and transition?transition bigrams. The 
transition?student state bigrams combine the in-
formation about the student state with the transi-
tion information of the previous system turn. Go-
ing back to Figure 1, the three incorrect answers 
will be distributed to three bigrams: Advance?
Incorrect (Tutor2?Student2), Push?Incorrect (Tu-
tor3?Student3) and PopUpAdv?Incorrect (Tutor5?
Student5). The transition?transition bigram looks 
at the transition labels of two consecutive system 
turns. For example, the Tutor4?Tutor5 pair will 
be counted as an Advance?PopUpAdv bigram. 
Similar to the unigrams, we compute a total 
parameter and a percentage parameter for each 
bigram. The percentage denominator is number 
of student turns for the transition?student state 
bigrams and the number of system turns minus 
one for the transition?transition bigram. In addi-
tion, for each bigram we compute a relative per-
centage parameter (bigram followed by %rel) by 
computing the percentage relative to the total 
number of times the transition unigram appears 
for that student. For example, we will compute 
the Advance?Incorrect %rel parameter by divid-
ing the number of Advance?Incorrect bigrams 
with the number of Advance unigrams (1 divided 
by 2 in Figure 1); this value will capture the per-
centage of times an Advance transition is fol-
lowed by an incorrect student answer. 
4 Results 
We use student learning as our evaluation metric 
because it is the primary metric for evaluating 
the performance of tutoring systems. Previous 
work (Forbes-Riley and Litman, 2006) has suc-
88
cessfully used student learning as the perform-
ance metric in the PARADISE framework. Two 
quantities are used to measure student learning: 
the pretest score and the posttest score. Both tests 
consist of 40 multiple-choice questions; the test?s 
score is computed as the percentage of correctly 
answered questions. The average score and stan-
dard deviation for each test are: pretest 0.47 
(0.17) and posttest 0.68 (0.17). 
We focus primarily on correlations between 
our interaction parameters and student learning. 
Because in our data the pretest score is signifi-
cantly correlated with the posttest score, we 
study partial Pearson?s correlations between our 
parameters and the posttest score that account for 
the pretest score. This correlation methodology is 
commonly used in the tutoring research (Chi et 
al., 2001). For each trend or significant correla-
tion we report the unigram/bigram, its average 
and standard deviation over all students, the 
Pearson?s Correlation Coefficient (R) and the 
statistical significance of R (p). 
First we report significant correlations for uni-
grams to test our first hypothesis. Next, for our 
second and third experiment, we report correla-
tions for transition?student state and transition?
transition parameters. Finally, we report our pre-
liminary results on PARADISE modeling. 
4.1 Unigram correlations 
In our first proposed experiment, we want to test 
the predictive utility of discourse structure in 
isolation. We compute correlations between our 
transition unigram parameters and learning. We 
find no trends or significant correlations. This 
result suggests that discourse structure in isola-
tion has no predictive utility. 
Here we also report all trends and significant 
correlations for student state unigrams as the 
baseline for contextual correlations to be pre-
sented in Section 4.2. We find only one signifi-
cant correlation (Table 2): students with a higher 
percentage of neutral turns (in terms of certainty) 
are negatively correlated with learning. We hy-
pothesize that this correlation captures the stu-
dent involvement in the tutoring process: more 
involved students will try harder thus expressing 
more certainty or uncertainty. In contrast, less 
involved students will have fewer certain/uncer-
tain/mixed turns and, in consequence, more neu-
tral turns. Surprisingly, student correctness does 
not significantly correlate with learning. 
Parameter Mean (SD) R. p 
Neutral % 37% (8%) -.47 .04 
Table 2: Trend and significant unigram correlations 
4.2 Transition?student state correlations 
For our second experiment, we need to determine 
the predictive utility of transition?student state 
bigram parameters. We find a large number of 
correlations for both transition?correctness bi-
grams and transition?certainty bigrams. 
Transition?correctness bigrams 
This type of bigram informs us whether ac-
counting for the discourse structure transition 
when looking at student correctness has any pre-
dictive value. We find several interesting trends 
and significant correlations (Table 3).  
The student behavior, in terms of correctness, 
after a PopUp or a PopUpAdv transition is very 
informative about the student learning process. 
In both situations, the student has just finished a 
remediation subdialogue and the system is pop-
ping up either by reasking the original question 
again (PopUp) or by moving on to the next ques-
tion (PopUpAdv). We find that after PopUp, the 
number of correct student answers is positively 
correlated with learning. In contrast, the number, 
the percentage and the relative percentage of in-
correct student answers are negatively correlated 
with learning. We hypothesize that this correla-
tion indicates whether the student took advantage 
of the additional learning opportunities offered 
by the remediation subdialogue. By answering 
correctly the original system question (PopUp?
Correct), the student demonstrates that she has 
absorbed the information from the remediation 
dialogue. This bigram is an indication of a suc-
cessful learning event. In contrast, answering the 
original system question incorrectly (PopUp?
Incorrect) is an indication of a missed learning 
opportunity; the more events like this happen the 
less the student learns. 
Parameter Mean (SD) R. p 
PopUp?Correct 7 (3.3) .45 .05 
PopUp?Incorrect 2 (1.8) -.42 .07 
PopUp?Incorrect % 1.6% (1.2%) -.46 .05 
PopUp?Incorrect %rel 17% (13%) -.39 .10 
PopUpAdv?Correct 2.5 (2) .43 .06 
PopUpAdv?Correct % 2% (1.3%) .52 .02 
NewTopLevel?Incorrect 2.3 (1.8) .56 .01 
NewTopLevel?Incorrect % 1.9% (1.4%) .49 .03 
NewTopLevel?Incorrect %rel 15% (12%) .51 .02 
Advance?Correct 40.5 (9.8) .45 .05 
Table 3: Trend and significant transition?correctness 
bigram correlations 
Similarly, being able to correctly answer the 
tutor question after popping up from a remedia-
tion subdialogue (PopUpAdv?Correct) is posi-
tively correlated with learning. Since in many 
cases, these system questions will make use of 
89
the knowledge taught in the remediation subdia-
logues, we hypothesize that this correlation also 
captures successful learning opportunities. 
Another set of interesting correlations is pro-
duced by the NewTopLevel?Incorrect bigram. 
We find that the number, the percentage and the 
relative percentage of times ITSPOKE starts a 
new essay revision dialogue that results in an 
incorrect student answer is positively correlated 
with learning. The content of the essay revision 
dialogue is determined based on ITSPOKE?s 
analysis of the student essay. We hypothesize 
that an incorrect answer to the first tutor question 
is indicative of the system?s picking of a topic 
that is problematic for the student. Thus, we see 
more learning in students for which more knowl-
edge gaps are discovered and addressed by 
ITSPOKE. 
Finally, we find the number of times the stu-
dent answers correctly after an advance transition 
is positively correlated with learning (the Ad-
vance?Correct bigram). We hypothesize that this 
correlation captures the relationship between 
students that advance without having major prob-
lems and a higher learning gains. 
Transition?certainty bigrams 
Next we look at the combination between the 
transition in the dialogue structure and the stu-
dent certainty (Table 4). These correlations offer 
more insight on the negative correlation between 
the Neutral % unigram parameter and student 
learning. We find that out of all neutral student 
answers, those that follow an Advance transi-
tions are negatively correlated with learning. 
Similar to the Neutral % correlation, we hy-
pothesize that Advance?Neutral correlations cap-
ture the lack of involvement of the student in the 
tutoring process. This might be also due to 
ITSPOKE engaging in teaching concepts that the 
student is already familiar with.  
Parameter Mean (SD) R. p 
Advance?Neutral 27 (8.3) -.40 .08 
Advance?Neutral % 21% (6%) -.62 .00 
Advance?Neutral %rel 38% (10%) -.73 .00 
SameGoal?Neutral %rel 35% (31%) .46 .05 
Table 4: Trend and significant transition?certainty 
bigram correlations 
In contrast, staying neutral in terms of cer-
tainty after a system rejection is positively corre-
lated with learning. These correlations show that 
based on their position in the discourse structure, 
neutral student answers will be correlated either 
negatively or positively with learning. 
Unlike student state unigram parameters 
which produce only one significant correlation, 
transition?student state bigram parameters pro-
duce a large number of trend and significant cor-
relations (14). This result suggests that exploiting 
the discourse structure as a contextual informa-
tion source can be beneficial for performance 
modeling. 
4.3 Transition?transition bigrams 
For our third experiment, we are looking at the 
transition?transition bigram correlations (Table 
5). These bigrams help us find trajectories of 
length two in the discourse structure that are as-
sociated with better student learning. Because 
our student state is domain dependent, translating 
the transition?student state bigrams to a new 
domain will require finding a new set of relevant 
factors to replace the student state. In contrast, 
because our transition information is domain in-
dependent, transition?transition bigrams can be 
easily implemented in a new domain.  
The Advance?Advance bigram covers situa-
tions where the student is covering tutoring ma-
terial without major knowledge gaps. This is be-
cause an Advance transition happens when the 
student either answers correctly or his incorrect 
answer can be corrected without going into a 
remediation subdialogue. Just like with the Ad-
vance?Correct correlation (recall Table 3), we 
hypothesize that these correlations links higher 
learning gains to students that cover a lot of ma-
terial without many knowledge gap.  
Parameter Mean (SD) R. p 
Advance?Advance 35 (9.1) .47 .04 
Push?Push 2.2 (1.7) .50 .03 
Push?Push % 1.8% (1.3%) .52 .02 
Push?Push %rel 11% (7%) .52 .02 
SameGoal?Push %rel 18% (23%) .49 .03 
Table 5: Trend and significant transition?transition 
bigram correlations 
The Push?Push bigrams capture another inter-
esting behavior. In these cases, the student incor-
rectly answers a question, entering a remediation 
subdialogue; she also incorrectly answers the 
first question in the remediation dialogue enter-
ing an even deeper remediation subdialogue. We 
hypothesize that these situations are indicative of 
big student knowledge gaps. In our corpus, we 
find that the more such big knowledge gaps are 
discovered and addressed by the system the 
higher the learning gain. 
The SameGoal?Push bigram captures another 
type of behavior after system rejections that is 
positively correlated with learning (recall the 
SameGoal?Neutral bigram, Table 4). In our pre-
vious work (Rotaru and Litman, 2006), we per-
90
formed an analysis of the rejected student turns 
and studied how rejections affect the student 
state. The results of our analysis suggested a new 
strategy for handling rejections in the tutoring 
domain: instead of rejecting student answers, a 
tutoring SDS should make use of the available 
information. Since the recognition hypothesis for 
a rejected student turn would be interpreted most 
likely as an incorrect answer thus activating a 
remediation subdialogue, the positive correlation 
between SameGoal?Push and learning suggests 
that the new strategy will not impact learning. 
Similar to the second experiment, the results 
of our third experiment are also positive: in con-
trast to transition unigrams, our domain inde-
pendent trajectories can produce parameters with 
a high predictive utility. 
4.4 PARADISE modeling 
Here we present our preliminary results on ap-
plying the PARADISE framework to model 
ITSPOKE performance. A stepwise multivariate 
linear regression procedure (Walker et al, 2000) 
is used to automatically select the parameters to 
be included in the model. Similar to (Forbes-
Riley and Litman, 2006), in order to model the 
learning gain, we use posttest as the dependent 
variable and force the inclusion of the pretest 
score as the first variable in the model. 
For the first experiment, we feed the model all 
transition unigrams. As expected due to lack of 
correlations, the stepwise procedure does not 
select any transition unigram parameter. The 
only variable in the model is pretest resulting in a 
model with a R2 of .22. 
For the second and third experiment, we first 
build a baseline model using only unigram pa-
rameters. The resulting model achieves an R2 of 
.39 by including the only significantly correlated 
unigram parameter: Neutral %. Next, we build a 
model using all unigram parameters and all sig-
nificantly correlated bigram parameters. The new 
model almost doubles the R2 to 0.75. Besides the 
pretest, the parameters included in the resulting 
model are (ordered by the degree of contribution 
from highest to lowest): Advance?Neutral %rel, 
and PopUp?Incorrect %. These results strengthen 
our correlation conclusions: discourse structure 
used as context information or as trajectories in-
formation is useful for performance modeling. 
Also, note that the inclusion of student certainty 
in the final PARADISE model provides addi-
tional support to a hypothesis that has gained a 
lot of attention lately: detecting and responding 
to student emotions has the potential to improve 
learning (Craig et al, 2004; Forbes-Riley and 
Litman, 2005; Pon-Barry et al, 2006). 
The performance of our best model is compa-
rable or higher than training performances re-
ported in previous work (Forbes-Riley and Lit-
man, 2006; M?ller, 2005b; Walker et al, 2001). 
Since our training data is relatively small (20 
data points) and overfitting might be involved 
here, in the future we plan to do a more in-depth 
evaluation by testing if our model generalizes on 
a larger ITSPOKE corpus we are currently anno-
tating. 
5 Related work 
Previous work has proposed a large number of 
interaction parameters for SDS performance 
modeling (M?ller, 2005a; Walker et al, 2000; 
Walker et al, 2001). Several information sources 
are being tapped to devise parameters classified 
by (M?ller, 2005a) in several categories: dia-
logue and communication parameters (e.g. dia-
logue duration, number of system/user turns), 
speech input parameters (e.g. word error rate, 
recognition/concept accuracy) and meta-
communication parameters (e.g. number of help 
request, cancel requests, corrections). 
But most of these parameters do not take into 
account the discourse structure information. A 
notable exception is the DATE dialogue act an-
notation from (Walker et al, 2001). The DATE 
annotation captures information on three dimen-
sions: speech acts (e.g. acknowledge, confirm), 
conversation domain (e.g. conversation- versus 
task-related) and the task model (e.g. subtasks 
like getting the date, time, origin, and destina-
tion). All these parameters can be linked to the 
discourse structure but flatten the discourse 
structure. Moreover, the most informative of 
these parameters (the task model parameters) are 
domain dependent. Similar approximations of the 
discourse structure are also common for other 
SDS tasks like predictive models of speech rec-
ognition problems (Gabsdil and Lemon, 2004). 
We extend over previous work in several ar-
eas. First, we exploit in more detail the hierarchi-
cal information in the discourse structure. We 
quantify this information by recording the dis-
course structure transitions. Second, in contrast 
to previous work, our usage of discourse struc-
ture is domain independent (the transitions). 
Third, we exploit the discourse structure as a 
contextual information source. To our knowl-
edge, previous work has not employed parame-
ters similar with our transition?student state bi-
91
gram parameters. Forth, via the transition?
transition bigram parameters, we exploit trajecto-
ries in the discourse structure as another domain 
independent source of information for perform-
ance modeling. Finally, similar to (Forbes-Riley 
and Litman, 2006), we are tackling a more prob-
lematic performance metric: the student learning 
gain. While the requirements for a successful 
information access SDS are easier to spell out, 
the same can not be said about tutoring SDS due 
to the current limited understanding of the hu-
man learning process. 
6 Conclusion 
In this paper we highlight the role of discourse 
structure for SDS performance modeling. We 
experiment with various ways of using the dis-
course structure: in isolation, as context informa-
tion for other factors (correctness and certainty) 
and through trajectories in the discourse structure 
hierarchy. Our correlation and PARADISE re-
sults show that, while the discourse structure is 
not useful in isolation, using the discourse struc-
ture as context information for other factors or 
via trajectories produces highly predictive pa-
rameters for performance analysis. Moreover, the 
PARADISE framework selects in the final model 
only discourse-based parameters ignoring pa-
rameters that do not use the discourse structure 
(certainty and correctness unigrams are ignored). 
Our significant correlations also suggest ways 
we should modify our system. For example, the 
PopUp?Incorrect negative correlations suggest 
that after a failed learning opportunity the system 
should not give out the correct answer but en-
gage in a secondary remediation subdialogue 
specially tailored for these situations. 
In the future, we plan to test the generality of 
our PARADISE model on other corpora and to 
compare models built using our interaction pa-
rameters against models based on parameters 
commonly used in previous work (M?ller, 
2005a). Testing if our results generalize to a hu-
man annotation of the discourse structure and 
automated models of certainty and correctness is 
also of importance. We also want to see if our 
results hold for performance metrics based on 
user satisfaction questionnaires; in the new 
ITSPOKE corpus we are currently annotating, 
each student also completed a user satisfaction 
survey (Forbes-Riley and Litman, 2006) similar 
to the one used in the DARPA Communicator 
multi-site evaluation (Walker et al, 2002).  
Our work contributes to both the computa-
tional linguistics domain and the tutoring do-
main. For the computational linguistics research 
community, we show that discourse structure is 
an important information source for SDS per-
formance modeling. Our analysis can be ex-
tended easily to other SDS. First, a similar auto-
matic annotation of the discourse structure can 
be performed in SDS that rely on dialogue man-
agers inspired by the Grosz & Sidner theory of 
discourse (Bohus and Rudnicky, 2003). Second, 
the transition?transition bigram parameters are 
domain independent. Finally, for the other suc-
cessful usage of discourse structure (transition?
student state bigrams) researchers have only to 
identify relevant factors and then combine them 
with the discourse structure information. In our 
case, we show that instead of looking at the user 
state in isolation (Forbes-Riley and Litman, 
2006), combining it with the discourse structure 
transition can generate informative interaction 
parameters. 
For the tutoring research community, we show 
that discourse structure, an important concept in 
computational linguistics theory, can provide 
useful insights regarding the learning process. 
The correlations we observe in our corpus have 
intuitive interpretations (successful/failed learn-
ing opportunities, discovery of deep student 
knowledge gaps, providing relevant tutoring). 
Acknowledgements 
This work is supported by NSF Grant No. 
0328431. We would like to thank Kate Forbes-
Riley, Joel Tetreault and our anonymous review-
ers for their helpful comments. 
References 
D. Bohus and A. Rudnicky. 2003. RavenClaw: 
Dialog Management Using Hierarchical Task 
Decomposition and an Expectation Agenda. In Proc. 
of Eurospeech. 
J. Cassell, Y. I. Nakano, T. W. Bickmore, C. L. 
Sidner and C. Rich. 2001. Non-Verbal Cues for 
Discourse Structure. In Proc. of ACL. 
M. T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi 
and R. G. Hausmann. 2001. Learning from human 
tutoring. Cognitive Science, 25. 
S. D. Craig, A. C. Graesser, J. Sullins and B. 
Gholson. 2004. Affect and learning: an exploratory 
look into the role affect in learning with AutoTutor. 
Journal of Educational Media, 29. 
92
K. Forbes-Riley and D. Litman. 2005. Using 
Bigrams to Identify Relationships Between Student 
Certainness States and Tutor Responses in a Spoken 
Dialogue Corpus. In Proc. of SIGdial. 
K. Forbes-Riley and D. Litman. 2006. Modelling 
User Satisfaction and Student Learning in a Spoken 
Dialogue Tutoring System with Generic, Tutoring, 
and User Affect Parameters. In Proc. of 
HLT/NAACL. 
M. Gabsdil and O. Lemon. 2004. Combining 
Acoustic and Pragmatic Features to Predict 
Recognition Performance in Spoken Dialogue 
Systems. In Proc. of ACL. 
B. Grosz and C. L. Sidner. 1986. Attentions, 
intentions and the structure of discourse. 
Computational Lingustics, 12(3). 
D. Higgins, J. Burstein, D. Marcu and C. Gentile. 
2004. Evaluating Multiple Aspects of Coherence in 
Student Essays. In Proc. of HLT-NAACL. 
J. Hirschberg and C. Nakatani. 1996. A prosodic 
analysis of discourse segments in direction-giving 
monologues. In Proc. of ACL. 
E. Hovy. 1993. Automated discourse generation 
using discourse structure relations. Articial 
Intelligence, 63(Special Issue on NLP). 
G.-A. Levow. 2004. Prosodic Cues to Discourse 
Segment Boundaries in Human-Computer Dialogue. 
In Proc. of SIGdial. 
D. Litman and S. Silliman. 2004. ITSPOKE: An 
intelligent tutoring spoken dialogue system. In Proc. 
of HLT/NAACL. 
S. M?ller. 2005a. Parameters for Quantifying the 
Interaction with Spoken Dialogue Telephone Services. 
In Proc. of SIGDial. 
S. M?ller. 2005b. Towards Generic Quality 
Prediction Models for Spoken Dialogue Systems - A 
Case Study. In Proc. of Interspeech. 
H. Pon-Barry, B. Clark, E. O. Bratt, K. Schultz and 
S. Peters. 2004. Evaluating the effectiveness of Scot:a 
spoken conversational tutor. In Proc. of ITS 
Workshop on Dialogue-based Intelligent Tutoring 
Systems. 
H. Pon-Barry, K. Schultz, E. O. Bratt, B. Clark and 
S. Peters. 2006. Responding to Student Uncertainty in 
Spoken Tutorial Dialogue Systems. International 
Journal of Artificial Intelligence in Education, 16. 
M. Rotaru and D. Litman. 2006. Dependencies 
between Student State and Speech Recognition 
Problems in Spoken Tutoring Dialogues. In Proc. of 
ACL. 
K. VanLehn, P. W. Jordan, C. P. Ros?, D. Bhembe, 
M. B?ttner, A. Gaydos, M. Makatchev, U. 
Pappuswamy, M. Ringenberg, A. Roque, S. Siler and 
R. Srivastava. 2002. The Architecture of Why2-Atlas: 
A Coach for Qualitative Physics Essay Writing. In 
Proc. of Intelligent Tutoring Systems (ITS). 
K. VanLehn, S. Siler, C. Murray, T. Yamauchi and 
W. B. Baggett. 2003. Why do only some events cause 
learning during human tutoring? Cognition and 
Instruction, 21(3). 
M. Walker, D. Litman, C. Kamm and A. Abella. 
2000. Towards Developing General Models of 
Usability with PARADISE. Natural Language 
Engineering. 
M. Walker, R. Passonneau and J. Boland. 2001. 
Quantitative and Qualitative Evaluation of Darpa 
Communicator Spoken Dialogue Systems. In Proc. of 
ACL. 
M. Walker, A. Rudnicky, R. Prasad, J. Aberdeen, 
E. Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom, A. 
Potamianos, R. Passonneau, S. Roukos, G. Sanders, S. 
Seneff and D. Stallard. 2002. DARPA Communicator: 
Cross-System Results for the 2001 Evaluation. In 
Proc. of ICSLP. 
 
 
93
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 208?215,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Humor: Prosody Analysis and Automatic Recognition
for F * R * I * E * N * D * S *
Amruta Purandare and Diane Litman
Intelligent Systems Program
University of Pittsburgh
famruta,litmang@cs.pitt.edu
Abstract
We analyze humorous spoken conversa-
tions from a classic comedy television
show, FRIENDS, by examining acoustic-
prosodic and linguistic features and their
utility in automatic humor recognition.
Using a simple annotation scheme, we au-
tomatically label speaker turns in our cor-
pus that are followed by laughs as hu-
morous and the rest as non-humorous.
Our humor-prosody analysis reveals sig-
nificant differences in prosodic character-
istics (such as pitch, tempo, energy etc.)
of humorous and non-humorous speech,
even when accounted for the gender and
speaker differences. Humor recognition
was carried out using standard supervised
learning classifiers, and shows promising
results significantly above the baseline.
1 Introduction
As conversational systems are becoming preva-
lent in our lives, we notice an increasing need for
adding social intelligence in computers. There has
been a considerable amount of research on incor-
porating affect (Litman and Forbes-Riley, 2004)
(Alm et al, 2005) (D?Mello et al, 2005) (Shroder
and Cowie, 2005) (Klein et al, 2002) and person-
ality (Gebhard et al, 2004) in computer interfaces,
so that, for instance, user frustrations can be rec-
ognized and addressed in a graceful manner. As
(Binsted, 1995) correctly pointed out, one way to
alleviate user frustrations, and to make human-
computer interaction more natural, personal and
interesting for the users, is to model HUMOR.
Research in computational humor is still in
very early stages, partially because humorous lan-
guage often uses complex, ambiguous and incon-
gruous syntactic and semantic expressions (At-
tardo, 1994) (Mulder and Nijholt, 2002) which re-
quire deep semantic interpretation. Nonetheless,
recent studies have shown a feasibility of auto-
matically recognizing (Mihalcea and Strapparava,
2005) (Taylor and Mazlack, 2004) and generating
(Binsted and Ritchie, 1997) (Stock and Strappar-
ava, 2005) humor in computer systems. The state
of the art research in computational humor (Bin-
sted et al, 2006) is, however, limited to text (such
as humorous one-liners, acronyms or wordplays),
and to our knowledge, there has been no work to
date on automatic humor recognition in spoken
conversations.
Before we can model humor in real application
systems, we must first analyze features that char-
acterize humor. Computational approaches to hu-
mor recognition so far primarily rely on lexical
and stylistic cues such as alliteration, antonyms,
adult slang (Mihalcea and Strapparava, 2005). The
focus of our study is, on the other hand, on ana-
lyzing acoustic-prosodic cues (such as pitch, in-
tensity, tempo etc.) in humorous conversations
and testing if these cues can help us to auto-
matically distinguish between humorous and non-
humorous (normal) utterances in speech. We hy-
pothesize that not only the lexical content but
also the prosody (or how the content is expressed)
makes humorous expressions humorous.
The following sections describe our data collec-
tion and pre-processing, followed by the discus-
sion of various acoustic-prosodic as well as other
types of features used in our humorous-speech
analysis and classification experiments. We then
present our experiments, results, and finally end
with conclusions and future work.
208
2 FRIENDS Corpus
(Scherer, 2003) discuss a number of pros and cons
of using real versus acted data, in the context of
emotional speech analysis. His main argument is
that while real data offers natural expressions of
emotions, it is not only hard to collect (due to eth-
ical issues) but also very challenging to annotate
and analyze, as there are very few instances of
strong expressions and the rest are often very sub-
tle. Acted data (also referred to as portrayed or
simulated), on the other hand, offers ample of pro-
totypical examples, although these are criticized
for not being natural at times. To achieve some
balance between naturalness and strength/number
of humorous expressions, we decided to use di-
alogs from a comedy television show FRIENDS,
which provides classical examples of casual, hu-
morous conversations between friends who often
discuss very real-life issues, such as job, career,
relationships etc.
We collected a total of 75 dialogs (scenes) from
six episodes of FRIENDS, four from Season I
(Monica Gets a New Roommate, The One with
Two Parts: Part 1 and 2, All the Poker) and two
from Season II (Ross Finds Out, The Prom Video),
all available on The Best of Friends Volume I
DVD. This gave us approximately 2 hrs of audio.
Text transcripts of these episodes were obtained
from: http://www.friendscafe.org/scripts.shtml,
and were used to extract lexical features (used later
in classification).
Figure 1 shows an excerpt from one of the di-
alogs in our corpus.
3 Audio Segmentation and Annotation
We segmented each audio file (manually) by mark-
ing speaker turn boundaries, using Wavesurfer
(http://www.speech.kth.se/wavesurfer). We apply
a fairly straightforward annotation scheme to au-
tomatically identify humorous and non-humorous
turns in our corpus. Speaker turns that are fol-
lowed by artificial laughs are labeled as Humor-
ous, and all the rest as Non-Humorous. For ex-
ample, in the dialog excerpt shown in figure 1,
turns 3, 7, 9, 11 and 16 are marked as humor-
ous, whereas turns 1, 2, 5, 6, 13, 14, 15 are
marked as non-humorous. Artificial laughs, si-
lences longer than 1 second and segments of au-
dio that contain purely non-verbal sounds (such
as phone rings, door bells, music etc.) were ex-
cluded from the analysis. By considering only
[1] Rachel: Guess what?
[2] Ross: You got a job?
[3] Rachel: Are you kidding? I am trained for
nothing!
[4] <Laughter>
[5] Rachel: I was laughed out of twelve inter-
views today.
[6] Chandler: And yet you?re surprisingly up-
beat.
[7] Rachel: You would be too if you found John
and David boots on sale, fifty percent off!
[8] <Laughter>
[9] Chandler: Oh, how well you know me...
[10] <Laughter>
[11] Rachel: They are my new, I don?t need a job,
I don?t need my parents, I got great boots, boots!
[12] <Laughter>
[13] Monica: How?d you pay for them?
[14] Rachel: Uh, credit card.
[15] Monica: And who pays for that?
[16] Rachel: Um... my... father.
[17] <Laughter>
Figure 1: Dialog Excerpt
speaker turns that are followed by laughs as hu-
morous, we also automatically eliminate cases of
pure visual comedy where humor is expressed us-
ing only gestures or facial expressions. In short,
non-verbal sounds or silences followed by laughs
are not treated as humorous. Henceforth, by
turn, we mean proper speaker turns (and not non-
verbal turns). We currently do not apply any spe-
cial filters to remove non-verbal sounds or back-
ground noise (other than laughs) that overlap with
speaker turns. However, if artificial laughs overlap
with a speaker turn (there were only few such in-
stances), the speaker turn is chopped by marking a
turn boundary exactly before/after the laughs be-
gin/end. This is to ensure that our prosody anal-
ysis is fair and does not catch any cues from the
laughs. In other words, we make sure that our
speaker turns are clean and not garbled by laughs.
After segmentation, we got a total of 1629
speaker turns, of which 714 (43.8%) are humor-
ous, and 915 (56.2%) are non-humorous. We also
made sure that there is a 1-to-1 correspondence be-
tween speaker turns in text transcripts that were
obtained online and our audio segments, and cor-
rected few cases where there was a mis-match (due
to turn-chopping or errors in online transcripts).
209
Figure 2: Audio Segmentation, Transcription and Feature Extraction using Wavesurfer
4 Speaker Distributions
There are 6 main actors/speakers (3 male and 3 fe-
male) in this show, along with a number of (in our
data 26) guest actors who appear briefly and rarely
in some of our dialogs. As the number of guest
actors is quite large, and their individual contribu-
tion is less than 5% of the turns in our data, we
decided to group all the guest actors together in
one GUEST class.
As these are acted (not real) conversations,
there were only few instances of speaker turn-
overlaps, where multiple speakers speak together.
These turns were given a speaker label MULTI. Ta-
ble 1 shows the total number of turns and humor-
ous turns for each speaker, along with their per-
centages in braces. Percentages for the Humor col-
umn show, out of the total (714) humorous turns,
how many are by each speaker. As one can notice,
the distribution of turns is fairly balanced among
the six main speakers. We also notice that even
though each guest actors? individual contribution
is less than 5% in our data, their combined contri-
bution is fairly large, almost 16% of the total turns.
Table 2 shows that the six main actors together
form a total of 83% of our data. Also, of the to-
tal 714 humorous turns, 615 (86%) turns are by
the main actors. To study if prosody of humor dif-
fers across males and females, we also grouped
the main actors into two gender classes. Table
2 shows that the gender distribution is fairly bal-
Speaker #Turns(%) #Humor (%)
Chandler (M) 244 (15) 163 (22.8)
Joey (M) 153 (9.4) 57 (8)
Monica (F) 219 (13.4) 74 (10.4)
Phoebe (F) 180 (11.1) 104 (14.6)
Rachel (F) 273 (16.8) 90 (12.6)
Ross (M) 288 (17.7) 127 (17.8)
GUEST (26) 263 (16.1) 95 (13.3)
MULTI 9 (0.6) 4 (0.6)
Table 1: Speaker Distribution
anced among the main actors, with 50.5% male
and 49.5% female turns. We also see that of the
685 male turns, 347 turns (almost 50%) are hu-
morous, and of the 672 female turns, 268 (ap-
proximately 40%) are humorous. Guest actors and
multi-speaker turns are not considered in the gen-
der analysis.
Speaker #Turns #Humor
Male 685 347
(50.5% of Main) (50.6% of Male)
Female 672 268
(49.5% Of Main) (39.9% of Female)
Total 1357 615
Main (83.3% of Total) (86.1% of Humor)
Table 2: Gender Distribution for Main Actors
210
5 Features
Literature in emotional speech analysis (Liscombe
et al, 2003)(Litman and Forbes-Riley, 2004)
(Scherer, 2003)(Ang et al, 2002) has shown that
prosodic features such as pitch, energy, speak-
ing rate (tempo) are useful indicators of emotional
states, such as joy, anger, fear, boredom etc. While
humor is not necessarily considered as an emo-
tional state, we noticed that most humorous ut-
terances in our corpus (and also in general) often
make use of hyper-articulations, similar to those
found in emotional speech.
For this study, we use a number of acoustic-
prosodic as well as some non acoustic-prosodic
features as listed below:
Acoustic-Prosodic Features:
 Pitch (F0): Mean, Max, Min, Range, Stan-
dard Deviation
 Energy (RMS): Mean, Max, Min, Range,
Standard Deviation
 Temporal: Duration, Internal Silence, Tempo
Non Acoustic-Prosodic Features:
 Lexical
 Turn Length (#Words)
 Speaker
Our acoustic-prosodic features make use of
the pitch, energy and temporal information in
the speech signal, and are computed using
Wavesurfer. Figure 2 shows Wavesurfer?s energy
(dB), pitch (Hz), and transcription (.lab) panes.
The transcription interface shows text correspond-
ing to the dialog turns, along with the turn bound-
aries. All features are computed at the turn level,
and essentially measure the mean, maximum, min-
imum, range (maximum-minimum) and standard
deviation of the feature value (F0 or RMS) over
the entire turn (ignoring zeroes). Duration is mea-
sured in terms of time in seconds, from the be-
ginning to the end of the turn including pauses
(if any) in between. Internal silence is measured
as the percentage of zero F0 frames, and essen-
tially account for the amount of silence in the turn.
Tempo is computed as the total number of sylla-
bles divided by the duration of the turn. For com-
puting the number of syllables per word, we used
the General Inquirer database (Stone et al, 1966).
Our lexical features are simply all words (alpha-
numeric strings including apostrophes and stop-
words) in the turn. The value of these features is
integral and essentially counts the number of times
a word is repeated in the turn. Although this indi-
rectly accounts for alliterations, in the future stud-
ies, we plan to use more stylistic lexical features
like (Mihalcea and Strapparava, 2005).
Turn length is measured as the number of words
in the turn. For our classification study, we con-
sider eight speaker classes (6 Main actors, 1 for
Guest and Multi) as shown in table 1, whereas for
the gender study, we consider only two speaker
categories (male and female) as shown in table 2.
6 Humor-Prosody Analysis
Feature Humor Non-Humor
Mean-F0 206.9 208.9
Max-F0* 299.8 293.5
Min-F0* 121.1 128.6
Range-F0* 178.7 164.9
StdDev-F0 41.5 41.1
Mean-RMS* 58.3 57.2
Max-RMS* 76.4 75
Min-RMS* 44.2 44.6
Range-RMS* 32.16 30.4
StdDev-RMS* 7.8 7.5
Duration* 3.18 2.66
Int-Sil* 0.452 0.503
Tempo* 3.21 3.03
Length* 10.28 7.97
Table 3: Humor Prosody: Mean feature values for
Humor and Non-Humor groups
Table 3 shows mean values of various acoustic-
prosodic features over all speaker turns in our data,
across humor and non-humor groups. Features
that have statistically (p<=0.05 as per indepen-
dent samples t-test) different values across the two
groups are marked with asterisks. As one can
see, all features except Mean-F0 and StdDev-F0
show significant differences across humorous and
non-humorous speech. Table 3 shows that humor-
ous turns in our data are longer, both in terms of
the time duration and the number of words, than
non-humorous turns. We also notice that humor-
ous turns have smaller internal silence, and hence
rapid tempo. Pitch (F0) and energy (RMS) fea-
tures have higher maximum, but lower minimum
211
values, for humorous turns. This in turn gives
higher values for range and standard deviation for
humor compared to the non-humor group. This re-
sult is somewhat consistent with previous findings
of (Liscombe et al, 2003) who found that most of
these features are largely associated with positive
and active emotional states such as happy, encour-
aging, confident etc. which are likely to appear in
our humorous turns.
7 Gender Effect on Humor-Prosody
To analyze prosody of humor across two genders,
we conducted a 2-way ANOVA test, using speaker
gender (male/female) and humor (yes/no) as our
fixed factors, and each of the above acoustic-
prosodic features as a dependent variable. The
test tells us the effect of humor on prosody ad-
justed for gender, the effect of gender on prosody
adjusted for humor and also the effect of interac-
tion between gender and humor on prosody (i.e.
if the effect of humor on prosody differs accord-
ing to gender). Table 4 shows results of 2-way
ANOVA, where Y shows significant effects, and
N shows non-significant effects. For example, the
result for tempo shows that tempo differs signifi-
cantly only across humor and non-humor groups,
but not across the two gender groups, and that
there is no effect of interaction between humor
and gender on tempo. As before, all features ex-
cept Mean-F0 and StdDev-F0 show significant dif-
ferences across humor and no-humor conditions,
even when adjusted for gender differences. The
table also shows that all features except inter-
nal silence and tempo show significant differences
across two genders, although only pitch features
(Max-F0, Min-F0, and StdDev-F0) show the ef-
fect of interaction between gender and humor. In
other words, the effect of humor on these pitch fea-
tures is dependent on gender. For instance, if male
speakers raise their pitch while expressing humor,
female speakers might lower. To confirm this,
we computed means values of various features for
males and females separately (See Tables 5 and
6). These tables indeed suggest that male speak-
ers show higher values for pitch features (Mean-
F0, Min-F0, StdDev-F0), while expressing humor,
whereas females show lower. Also for male speak-
ers, differences in Min-F0 and Min-RMS values
are not statistically significant across humor and
non-humor groups, whereas for female speakers,
features Mean-F0, StdDev-F0 and tempo do not
show significant differences across the two groups.
One can also notice that the differences in the
mean pitch feature values (specifically Mean-F0,
Max-F0 and Range-F0) between humor and non-
humor groups are much higher for males than for
females.
In summary, our gender analysis shows that al-
though most acoustic-prosodic features are differ-
ent for males and females, the prosodic style of ex-
pressing humor by male and female speakers dif-
fers only along some pitch-features (both in mag-
nitude and direction).
Feature Humor Gender Humor
x Gender
Mean-F0 N Y N
Max-F0 Y Y Y
Min-F0 Y Y Y
Range-F0 Y Y N
StdDev-F0 N Y Y
Mean-RMS Y Y N
Max-RMS Y Y N
Min-RMS Y Y N
Range-RMS Y Y N
StdDev-RMS Y Y N
Duration Y Y N
Int-Sil Y N N
Tempo Y N N
Length Y Y N
Table 4: Gender Effect on Humor Prosody: 2-Way
ANOVA Results
8 Speaker Effect on Humor-Prosody
We then conducted similar ANOVA test to account
for the speaker differences, i.e. by considering hu-
mor (yes/no) and speaker (8 groups as shown in ta-
ble 1) as our fixed factors and each of the acoustic-
prosodic features as a dependent variable for a 2-
Way ANOVA. Table 7 shows results of this analy-
sis. As before, the table shows the effect of humor
adjusted for speaker, the effect of speaker adjusted
for humor and also the effect of interaction be-
tween humor and speaker, on each of the acoustic-
prosodic features. According to table 7, we no
longer see the effect of humor on features Min-
F0, Mean-RMS and Tempo (in addition to Mean-
F0 and StdDev-F0), in presence of the speaker
variable. Speaker, on the other hand, shows sig-
nificant effect on prosody for all features. But
212
Feature Humor Non-Humor
Mean-F0* 188.14 176.43
Max-F0* 276.94 251.7
Min-F0 114.54 113.56
Range-F0* 162.4 138.14
StdDev-F0* 37.83 34.27
Mean-RMS* 57.86 56.4
Max-RMS* 75.5 74.21
Min-RMS 44.04 44.12
Range-RMS* 31.46 30.09
StdDev-RMS* 7.64 7.31
Duration* 3.1 2.57
Int-Sil* 0.44 0.5
Tempo* 3.33 3.1
Length* 10.27 8.1
Table 5: Humor Prosody for Male Speakers
surprisingly, again only pitch features Mean-F0,
Max-F0 and Min-F0 show the interaction effect,
suggesting that the effect of humor on these pitch
features differs from speaker to speaker. In other
words, different speakers use different pitch varia-
tions while expressing humor.
9 Humor Recognition by Supervised
Learning
We formulate our humor-recognition experiment
as a classical supervised learning problem, by
automatically classifying spoken turns into hu-
mor and non-humor groups, using standard ma-
chine learning classifiers. We used the decision
tree algorithm ADTree from Weka, and ran a
10-fold cross validation experiment on all 1629
turns in our data1. The baseline for these ex-
periments is 56.2% for the majority class (non-
humorous). Table 8 reports classification results
for six feature categories: lexical alone, lexical +
speaker, prosody alone, prosody + speaker, lexical
+ prosody and lexical + prosody + speaker (all).
Numbers in braces show the number of features
in each category. There are total 2025 features
which include 2011 lexical (all word types plus
turn length), 13 acoustic-prosodic and 1 for the
speaker information. Feature Length was included
in the lexical feature group, as it counts the num-
ber of lexical items (words) in the turn.
1We also tried other classifiers like Naive Bayes and Ad-
aBoost, although since the results were equivalent to ADTree,
we do not report those here.
Feature Humor Non-Humor
Mean-F0 235.79 238.75
Max-F0* 336.15 331.14
Min-F0* 133.63 143.14
Range-F0* 202.5 188
StdDev-F0 46.33 46.6
Mean-RMS* 58.44 57.64
Max-RMS* 77.33 75.57
Min-RMS* 44.08 44.74
Range-RMS* 33.24 30.83
StdDev-RMS* 8.18 7.59
Duration* 3.35 2.8
Int-Sil* 0.47 0.51
Tempo 3.1 3.1
Length* 10.66 8.25
Table 6: Humor Prosody for Female Speakers
All results are significantly above the baseline
(as measured by a pair-wise t-test) with the best
accuracy of 64% (8% over the baseline) obtained
using all features. We notice that the classifica-
tion accuracy improves on adding speaker infor-
mation to both lexical and prosodic features. Al-
though these results do not show a strong evidence
that prosodic features are better than lexical, it is
interesting to note that the performance of just a
few (13) prosodic features is comparable to that
of 2011 lexical features. Figure 3 shows the deci-
sion tree produced by the classifier in 10 iterations.
Numbers indicate the order in which the nodes are
created, and indentations mark parent-child rela-
tions. We notice that the classifier primarily se-
lected speaker and prosodic features in the first
10 iterations, whereas lexical features were se-
lected only in the later iterations (not shown here).
This seems consistent with our original hypothe-
sis that speech features are better at discriminating
between humorous and non-humorous utterances
in speech than lexical content.
Although (Mihalcea and Strapparava, 2005) ob-
tained much higher accuracies using lexical fea-
tures alone, it might be due to the fact that our data
is homogeneous in the sense that both humorous
and non-humorous turns are extracted from the
same source, and involve same speakers, which
makes the two groups highly alike and hence chal-
lenging to distinguish. To make sure that the
lower accuracy we get is not simply due to using
smaller data compared to (Mihalcea and Strappar-
213
Feature Humor Speaker Humor
x Speaker
Mean-F0 N Y Y
Max-F0 Y Y Y
Min-F0 N Y Y
Range-F0 Y Y N
StdDev-F0 N Y N
Mean-RMS N Y N
Max-RMS Y Y N
Min-RMS Y Y N
Range-RMS Y Y N
StdDev-RMS Y Y N
Duration Y Y N
Int-Sil Y Y N
Tempo N Y N
Length Y Y N
Table 7: Speaker Effect on Humor Prosody: 2-
Way ANOVA Results
Feature -Speaker +Speaker
Lex 61.14 (2011) 63.5 (2012)
Prosody 60 (13) 63.8 (14)
Lex + Prosody 62.6 (2024) 64 (2025)
Table 8: Humor Recognition Results (% Correct)
ava, 2005), we looked at the learning curve for the
classifier (see figure 4) and found that the classi-
fier performance is not sensitive to the amount of
data.
Table 9 shows classification results by gender,
using all features. For the male group, the base-
line is 50.6%, as the majority class humor is 50.6%
(See Table 2). For females, the baseline is 60%
(for non-humorous) as only 40% of the female
turns are humorous.
Gender Baseline Classifier
Male 50.6 64.63
Female 60.1 64.8
Table 9: Humor Recognition Results by Gender
As Table 9 shows, the performance of the classi-
fier is somewhat consistent cross-gender, although
for male speakers, the relative improvement is
much higher (14% above the baseline), than for
females (only 5% above the baseline). Our earlier
observation (from tables 5 and 6) that differences
in pitch features between humor and non-humor
j (1)SPEAKER = chandler: 0.469
j (1)SPEAKER != chandler: -0.083
j j (4)SPEAKER = phoebe: 0.373
j j (4)SPEAKER != phoebe: -0.064
j (2)DURATION < 1.515: -0.262
j j (5)SILENCE < 0.659: 0.115
j j (5)SILENCE >= 0.659: -0.465
j j (8)SD F0 < 9.919: -1.11
j j (8)SD F0 >= 9.919: 0.039
j (2)DURATION >= 1.515: 0.1
j j (3)MEAN RMS < 56.117: -0.274
j j (3)MEAN RMS >= 56.117: 0.147
j j j (7)come < 0.5: -0.056
j j j (7)come >= 0.5: 0.417
j j (6)SD F0 < 57.333: 0.076
j j (6)SD F0 >= 57.333: -0.285
j j (9)MAX RMS < 86.186: 0.011
j j j (10)MIN F0 < 166.293: 0.047
j j j (10)MIN F0 >= 166.293: -0.351
j j (9)MAX RMS >= 86.186: -0.972
Legend: +ve = humor, -ve = non-humor
Figure 3: Decision Tree (only the first 10 iterations
are shown)
groups are quite higher for males than for females,
may explain why we see higher improvement for
male speakers.
10 Conclusions
In this paper, we presented our experiments on
humor-prosody analysis and humor recognition
in spoken conversations, collected from a clas-
sic television comedy, FRIENDS. Using a sim-
ple automated annotation scheme, we labeled
speaker turns in our corpus that are followed
by artificial laughs as humorous, and the rest as
non-humorous. We then examined a number of
acoustic-prosodic features based on pitch, energy
and temporal information in the speech signal,
that have been found useful by previous studies in
emotion recognition.
Our prosody analysis revealed that humorous
and non-humorous turns indeed show significant
differences in most of these features, even when
accounted for the speaker and gender differences.
Specifically, we found that humorous turns tend
to have higher tempo, smaller internal silence, and
higher peak, range and standard deviation for pitch
and energy, compared to non-humorous turns.
On the humor recognition task, our classifier
214
Figure 4: Learning Curve: %Accuracy versus
%Fraction of Data
achieved the best performance when acoustic-
prosodic features were used in conjunction with
lexical and other types of features, and in all ex-
periments attained the accuracy statistically signif-
icant over the baseline. While prosody of humor
shows some differences due to gender, the perfor-
mance on the humor recognition task is equiva-
lent for males and females, although the relative
improvement over the baseline is much higher for
males than for females.
Our current study focuses only on lexical and
speech features, primarily because these features
can be computed automatically. In the future, we
plan to explore more sophisticated semantic and
pragmatic features such as incongruity, ambiguity,
expectation-violation etc. We also like to inves-
tigate if our findings generalize to other types of
corpora besides TV-show dialogs.
References
C. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: Machine learning for text-based emotion pre-
diction. In Proceedings of HLT/EMNLP, Vancou-
ver, CA.
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and
A. Stolcke. 2002. Prosody-based automatic de-
tection of annoyance and frustration in human-
computer dialog. In Proceedings of ICSLP.
S. Attardo. 1994. Linguistic Theory of Humor. Moun-
ton de Gruyter, Berlin.
K. Binsted and G. Ritchie. 1997. Computational rules
for punning riddles. Humor, 10(1).
K. Binsted, B. Bergen, S. Coulson, A. Nijholt,
O. Stock, C. Strapparava, G. Ritchie, R. Manurung,
H. Pain, A. Waller, and D. O?Mara. 2006. Com-
putational humor. IEEE Intelligent Systems, March-
April.
K. Binsted. 1995. Using humour to make natural lan-
guage interfaces more friendly. In Proceedings of
the AI, ALife and Entertainment Workshop, Mon-
treal, CA.
S. D?Mello, S. Craig, G. Gholson, S. Franklin, R. Pi-
card, and A. Graesser. 2005. Integrating affect sen-
sors in an intelligent tutoring system. In Proceed-
ings of Affective Interactions: The Computer in the
Affective Loop Workshop.
P. Gebhard, M. Klesen, and T. Rist. 2004. Color-
ing multi-character conversations through the ex-
pression of emotions. In Proceedings of Affective
Dialog Systems.
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-
sults. Interacting with Computers, 14.
J. Liscombe, J. Venditti, and J. Hirschberg. 2003.
Classifying subject ratings of emotional speech us-
ing acoustic features. In Proceedings of Eurospeech,
Geneva, Switzerland.
D. Litman and K. Forbes-Riley. 2004. Predicting
student emotions in computer-human tutoring dia-
logues. In Proceedings of ACL, Barcelona, Spain.
R. Mihalcea and C. Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of HLT/EMNLP, Van-
couver, CA.
M. Mulder and A. Nijholt. 2002. Humor research:
State of the art. Technical Report 34, CTIT Techni-
cal Report Series.
Scherer. 2003. Vocal communication of emotion: A
review of research paradigms. Speech Communica-
tion, 40(1-2):227?256.
M. Shroder and R. Cowie. 2005. Toward emotion-
sensitive multimodal interfaces: the challenge of the
european network of excellence humaine. In Pro-
ceedings of User Modeling Workshop on Adapting
the Interaction Style to Affective Factors.
O. Stock and C. Strapparava. 2005. Hahaacronym:
A computational humor system. In Proceedings of
ACL Interactive Poster and Demonstration Session,
pages 113?116, Ann Arbor, MI.
P. Stone, D. Dunphy, M. Smith, and D. Ogilvie. 1966.
The General Inquirer: A Computer Approach to
Content Analysis. MIT Press, Cambridge, MA.
J. Taylor and L. Mazlack. 2004. Computationally rec-
ognizing wordplay in jokes. In Proceedings of the
CogSci 2004, Chicago, IL.
215
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 178?187,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Discourse Structure and Performance Analysis: 
Beyond the Correlation 
 
Mihai Rotaru 
Textkernel B.V. 
Amsterdam, The Netherlands 
 
mich.rotaru@gmail.com 
Diane J. Litman 
University of Pittsburgh 
Pittsburgh, USA 
 
litman@cs.pitt.edu 
 
  
 
Abstract 
This paper is part of our broader investi-
gation into the utility of discourse struc-
ture for performance analysis. In our pre-
vious work, we showed that several in-
teraction parameters that use discourse 
structure predict our performance metric. 
Here, we take a step forward and show 
that these correlations are not only a sur-
face relationship. We show that redesign-
ing the system in light of an interpreta-
tion of a correlation has a positive impact. 
1 Introduction 
The success of a spoken dialogue system (SDS) 
depends on a large number of factors and the 
strategies employed to address them. Some of 
these factors are intuitive. For example, problems 
with automated speech recognition can derail a 
dialogue from the normal course: e.g. non-
understandings, misunderstandings, end-
pointing, etc. (e.g. (Bohus, 2007; Raux and Es-
kenazi, 2008)). The strategies used to handle or 
avoid these situations are also important and re-
searchers have experimented with many such 
strategies as there is no clear winner in all con-
texts (e.g. (Bohus, 2007; Singh et al, 2002)). 
However, other factors can only be inferred 
through empirical analyses. 
A principled approach to identifying important 
factors and strategies to handle them comes from 
performance analysis. This approach was pio-
neered by the PARADISE framework (Walker et 
al., 2000). In PARADISE, the SDS behavior is 
quantified in the form of interaction parameters: 
e.g. speech recognition performance, number of 
turns, number of help requests, etc. (M?ller, 
2005).These parameters are then used in a multi-
variate linear regression to predict a SDS per-
formance metric (e.g. task completion, user satis-
faction: (Singh et al, 2002)). Finally, SDS redes-
ign efforts are informed by the parameters that 
make it in the regression model. 
Conceptually, this equates to investigating two 
properties of interaction parameters: predictive-
ness and informativeness1. Predictiveness looks 
at the connection between the parameter and sys-
tem performance via predictive models (e.g. mul-
tivariate linear regression in PARADISE). Once 
the predictiveness is established, it is important 
to look at the parameter informativeness. Infor-
mally, informativeness looks at how much the 
parameter can help us improve the system. We 
already know that the parameter is predictive of 
performance. But this does not tell us if there is a 
causal link between the two. In fact, the main 
drive is not to prove a causal link but to show 
that the interaction parameter will inform a modi-
fication of the system and that this modification 
will improve the system. 
This paper is part of our broader investigation 
into the utility of discourse structure for per-
formance analysis. Although each dialogue has 
an inherent structure called the discourse struc-
ture (Grosz and Sidner, 1986), this information 
has received little attention in performance 
analysis settings. In our previous work (Rotaru 
and Litman, 2006), we established the predic-
tiveness of several interaction parameters derived 
from discourse structure. Here we take a step 
further and demonstrate the informativeness of 
these parameters. 
We show that one of the predictive discourse 
structure-based parameters (PopUp-Incorrect) 
informs a promising modification of our system. 
                                                 
1 Although this terminology is not yet established in the 
SDS community, the investigations behind these properties 
are a common practice in the field. 
178
We implement this modification and we compare 
it with the original version of the system through 
a user study. Our analyses indicate that the modi-
fication leads to objective improvements for our 
system (e.g. performance improvements for cer-
tain users but not at the population level and 
fewer system turns). 
2 Background 
ITSPOKE (Intelligent Tutoring Spoken Dia-
logue System) (Litman et al, 2006) is a speech-
enabled version of the text-based Why2-Atlas 
conceptual physics tutoring system (VanLehn et 
al., 2007). The interaction between ITSPOKE 
and users is mediated through a graphical web 
interface supplemented with a headphone-
microphone unit. ITSPOKE first analyzes a user 
typed essay response to a physics problem for 
mistakes and omissions. Then it engages in a 
spoken dialogue to remediate the identified prob-
lems. Finally, users revise their essay and 
ITSPOKE either does another round of tutor-
ing/essay revision if needed or moves on to the 
next problem. 
While for most information access SDS per-
formance is measured using task completion or 
user satisfaction, for the tutoring SDS the pri-
mary performance metric is learning. To measure 
learning, users take a knowledge test before and 
after interacting with ITSPOKE. The Normalized 
Learning Gain (NLG) is defined as (posttest-
pretest)/(1-pretest) and measures the percentage 
improvement relative to the perfect improve-
ment: an NLG of 0.0 means no improvement 
while an NLG of 1.0 means maximum improve-
ment. 
2.1 Discourse structure 
We use the Grosz & Sidner theory of discourse 
(Grosz and Sidner, 1986). According to this the-
ory, dialogue utterances naturally aggregate into 
discourse segments, with each segment having an 
associated purpose or intention. These segments 
are hierarchically organized forming the dis-
course structure hierarchy. This hierarchical as-
pect of dialogue has inspired several generic dia-
logue management frameworks (e.g. RavenClaw 
(Bohus, 2007)). We briefly describe our auto-
matic annotation of this hierarchy and its use 
through discourse transitions. A sample example 
is shown in Appendix 1. For more details see 
(Rotaru and Litman, 2006). 
Since dialogues with ITSPOKE follow a ?tu-
tor question - user answer - tutor response? for-
mat, which is hand-authored beforehand in a hi-
erarchical structure, we can easily approximate 
the discourse structure hierarchy. After the essay 
analysis, ITSPOKE selects a group of questions 
which are asked one by one. These questions 
form the top-level discourse segment (e.g. DS1 
in Appendix 1). For incorrect answers to more 
complex questions (e.g. applying physics laws), 
ITSPOKE will engage in a remediation subdia-
logue that attempts to remediate the student?s 
lack of knowledge or skills. These subdialogues 
form the embedded discourse segments (e.g. DS2 
in Appendix 2). 
We define six discourse transitions in the dis-
course structure hierarchy and use them to label 
each system turn. A NewTopLevel label is used 
for the first question after an essay submission. If 
the previous question is at the same level with 
the current question we label the current question 
as Advance. The first question in a remediation 
subdialogue is labeled as Push. After a remedia-
tion subdialogue is completed, ITSPOKE will 
pop up and a heuristic determines whether to ask 
again the question that triggered the remediation 
dialogue. Reasking is labeled as a PopUp, while 
moving on to the next question is labeled as 
PopUpAdv. Rejections due to speech problems or 
timeouts are labeled as SameGoal. 
Our transitions partially encode the hierarchi-
cal information of discourse structure: they cap-
ture the position of each system turn in this hier-
archy relative to the previous system turn. 
2.2 Discourse structure-based interaction  
parameters 
To derive interaction parameters, we look at 
transition?phenomena and transition?transition 
bigrams. The first type of bigrams is motivated 
by our intuition that dialogue phenomena related 
to performance are not uniformly important but 
have more weight depending on their position in 
the dialogue. For example, it is more important 
for users to be correct at specific places in the 
dialogue rather than overall in the dialogue. We 
use two phenomena related to performance in our 
system/domain: user correctness (e.g. correct, 
incorrect) and user certainty (e.g. uncertain, neu-
tral, etc.). For example, a PopUp-Incorrect event 
occurs whenever users are incorrect after being 
reasked the question that initially triggered the 
remediation dialogue. The second type of bi-
grams is motivated by our intuition that ?good? 
and ?bad? dialogues have different discourse 
structures. To compare two dialogues in terms of 
179
the discourse structure we look at consecutive 
transitions: e.g. Push-Push. 
For each bigram we compute 3 interaction pa-
rameters: a total (e.g. the number of PopUp-
Incorrect events), a percentage (e.g. the number 
of PopUp-Incorrect relative to the number of 
turns) and a relative percentage (e.g. the percent-
age of times a PopUp is followed by an incorrect 
answer). 
3 Predictiveness 
In (Rotaru and Litman, 2006), we demonstrate 
the predictiveness of several discourse structure-
based parameters. Here we summarize the results 
for parameters derived from the PopUp?Correct 
and PopUp?Incorrect bigrams (Table 1). These 
bigrams caught our attention as their predictive-
ness has intuitive interpretations and generalizes 
to other corpora. Predictiveness was measured by 
looking at correlations (i.e. univariate linear re-
gression) between our interaction parameters and 
learning2. We used a corpus of 95 dialogues from 
20 users (2334 user turns). For brevity, we report 
in Table 1 only the bigram, the best Pearson?s 
Correlation Coefficient (R) associated with pa-
rameters derived from that bigram and the statis-
tical significance of this coefficient (p).  
R p
PopUp-Correct 0.45 0.05
PopUp-Incorrect -0.46 0.05
Bigram
 
Table 1. Several discourse structure-based parameters 
significantly correlated with learning  
(for complete results see (Rotaru and Litman, 2006)) 
The two bigrams shed light into user?s learn-
ing patterns. In both cases, the student has just 
finished a remediation subdialogue and the sys-
tem is popping up by reasking the original ques-
tion again (a PopUp transition). We find that cor-
rect answers after a PopUp are positively corre-
lated with learning. In contrast, incorrect answers 
after a PopUp are negatively correlated with 
learning. We hypothesize that these correlations 
indicate whether the user took advantage of the 
additional learning opportunities offered by the 
remediation subdialogue. By answering correctly 
the original system question (PopUp?Correct), 
the user demonstrates that he/she has absorbed 
the information from the remediation dialogue. 
This bigram is an indication of a successful 
learning event. In contrast, answering the origi-
                                                 
2 As it is commonly done in the tutoring research (e.g. (Lit-
man et al, 2006)), we use partial Pearson?s correlations 
between our parameters and the posttest score that account 
for the pretest score. 
nal system question incorrectly (PopUp?
Incorrect) is an indication of a missed learning 
opportunity; the more such events happen the 
less the user learns. 
In  (Rotaru and Litman, 2006) we also demon-
strate that discourse structure is an important 
source for producing predictive parameters. In-
deed, we found that simple correctness parame-
ters (e.g. number of incorrect answers) are sur-
prisingly not predictive in our domain. In con-
trast, parameters that look at correctness at spe-
cific places in the discourse structure hierarchy 
are predictive (e.g. PopUp?Incorrect). 
4 Informativeness 
We investigate the informativeness of the 
PopUp?Incorrect bigram as in (Rotaru, 2008) we 
also show that its predictiveness generalizes to 
two other corpora. We need 3 things for this: an 
interpretation of the predictiveness (i.e. an inter-
pretation of the correlation), a new system strat-
egy derived from this interpretation and a valida-
tion of the strategy. 
As mentioned in Section 3, our interpretation 
of the correlation between PopUp?Incorrect 
events and learning is that these events signal 
failed learning opportunities. The remediation 
subdialogue is the failed learning opportunity: 
the system had a chance to correct user?s lack of 
knowledge and failed to achieve that. The more 
such events we see, the lesser the system per-
formance. 
How can we change the system in light of this 
interpretation? We propose to give additional 
explanations after a PopUp?Incorrect event as 
the new strategy. To arrive at this strategy, we 
hypothesized why the failed opportunity has oc-
curred. The simplest answer is that the user has 
failed to absorb the information from the reme-
diation dialogue. It is possible that the user did 
not understand the remediation dialogue and/or 
failed to make the connection between the reme-
diation dialogue and the original question. The 
current ITSPOKE strategy after a PopUp?
Incorrect is to give away the correct answer and 
move on. The negative correlations indicate that 
this strategy is not working. Thus, maybe it 
would be better if the system will engage in addi-
tional explanations to correct the user. If we can 
make the user understand, then we transform the 
failed learning opportunity into a successful 
learning opportunity. This will be equivalent to a 
PopUp?Correct event which we have seen is 
positively correlated with learning (Section 3). 
180
While other interpretation and hypotheses 
might also be true, our results (Section 5) show 
that the new strategy is successful. This validates 
the interpretation, the strategy and consequently 
the informativeness of the parameter. 
 
4.1 Modification 
To modify the system, we had to implement the 
new PopUp?Incorrect strategy: provide addi-
tional explanations rather than simply giving 
away the correct answer and moving on. But how 
to deliver the additional explanations? One way 
is to engage in an additional subdialogue. How-
ever, this was complicated by the fact that we did 
not know exactly what information to convey 
and/or what questions to ask. It was crucial that 
the information and/or the questions were on tar-
get due to the extra burden of the new subdia-
logue. 
Instead, we opted for a different implementa-
tion of the strategy: interrupt the conversation at 
PopUp?Incorrect events and offer the additional 
explanations in form of a webpage that the user 
will read (recall that ITSPOKE uses in addition a 
graphical web interface ? Section 2). Each poten-
tial PopUp?Incorrect event had an associated 
webpage that is displayed whenever the event 
occurs. Because the information was presented 
visually, users can choose which part to read, 
which meant that we did not have to be on target 
with our explanations. To return to the spoken 
dialogue, users pressed a button when done read-
ing the webpage. 
All webpages included several pieces of in-
formation we judged to be helpful. We included 
the tutor question, the correct answer and a text 
summary of the instruction so far and of the 
remediation subdialogue. We also presented a 
graphical representation of the discourse struc-
ture, called the Navigation Map. Our previous 
work (Rotaru and Litman, 2007) shows that users 
prefer this feature over not having it on many 
subjective dimensions related to understanding. 
Additional information not discussed by the sys-
tem was also included if applicable: intuitions 
and examples from real life, the purpose of the 
question with respect to the current problem and 
previous problems and/or possible pitfalls. See 
Appendix 2 for a sample webpage. 
The information we included in the PopUp?
Incorrect webpages has a ?reflective? nature. For 
example, we summarize and discuss the relevant 
instruction. We also comment on the connection 
between the current problem and previous prob-
lems. The value of ?reflective? information has 
been established previously e.g. (Katz et al, 
2003). 
All webpages and their content were created 
by one of the authors. All potential places for 
PopUp?Incorrect events (i.e. system questions) 
were identified and a webpage was authored for 
each question. There were 24 such places out of 
a total of 96 questions the system may ask during 
the dialogue. 
5 Results 
There are several ways to demonstrate the suc-
cess of the new strategy. First, we can investigate 
if the correlation between PopUp?Incorrect and 
learning is broken by the new strategy. Our re-
sults (5.2) show that this is true. Second, we can 
show that the new system outperforms the old 
system. However, this might not be the best way 
as the new PopUp?Incorrect strategy directly 
affects only people with PopUp?Incorrect events. 
In addition, its effect might depend on how many 
times it was activated. Indeed, we find no sig-
nificant effect of the new strategy in terms of 
performance at the population level. However, 
we find that the new strategy does produce a per-
formance improvement for users that ?needed? it 
the most: users with more PopUp?Incorrect 
events (5.3). 
We begin by describing the user study and 
then we proceed with our quantitative evalua-
tions. 
5.1 User study 
To test the effect of the new PopUp?Incorrect 
strategy, we designed and performed a between-
subjects study with 2 conditions. In the control 
condition (R) we used the regular version of 
ITSPOKE with the old PopUp?Incorrect strategy 
(i.e. give the current answer and move on). In the 
experimental condition (PI), we had the regular 
version of ITSPOKE with the new PopUp?
Incorrect strategy (i.e. give additional informa-
tion). 
The resulting corpus has 22 R users and 25 PI 
users and it is balanced for gender. There are 235 
dialogues and 3909 user turns. The experiment 
took 2? hours per user on average. 
5.2 Breaking the correlation 
The predictiveness of the PopUp?Incorrect bi-
gram (i.e. its negative correlation with learning) 
means that PopUp?Incorrect events signal lower 
performance. One way to validate the effective-
181
ness of the new PopUp?Incorrect strategy is to 
show that it breaks down this correlation. In 
other words, PopUp?Incorrect events no longer 
signal lower performance. Simple correlation 
does not guarantee that this is true because corre-
lation does not necessarily imply causality. 
In our experiment, this translates to showing 
that that PopUp?Incorrect bigram parameters are 
still correlated with learning for R students but 
the correlations are weaker for PI students. 
Table 2 shows these correlations. As in Table 1, 
we show only the bigrams for brevity. 
R p R p
PopUp-Correct 0.60 0.01 0.18 0.40
PopUp-Incorrect -0.65 0.01 -0.18 0.40
Bigram
R  users PI users
 
Table 2. Correlation with learning in each condition 
We find that the connection between user be-
havior after a PopUp transition and learning con-
tinues to be strong for R users. PopUp?Incorrect 
events continue to signal lower performance (i.e. 
a strong significant negative correlation of           
-0.65). PopUp?Correct events signal increased 
performance (i.e. a strong significant positive 
correlation of +0.60). The fact that these correla-
tions generalize across experiments/corpora fur-
ther strengthens the predictiveness of the 
PopUp?Incorrect parameters. 
PopUp-Incorrect (rel %)
N
LG
0% 20% 40% 60% 80%
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
 PI
 R
 
Figure 1. Correlations between a PopUp-Incorrect 
parameter and NLG 
In contrast, for PI users these correlations are 
much weaker with non-significant correlation 
coefficients of -0.18 and 0.18 respectively. In 
other words the new PopUp?Incorrect strategy 
breaks down the observed correlation: PopUp?
Incorrect events are no longer a good indicator of 
lower performance. 
It is interesting to visualize these correlations 
graphically. Figure 1 shows a scatter plot of the 
PopUp?Incorrect relative percentage parameter 
and NLG for each PI and R user. The regression 
lines for the correlation between PopUp?
Incorrect and NLG for PI and R are shown. The 
graph shows that users with less PopUp?
Incorrect events (e.g. less than 30% relative) tend 
to have a higher NLG (0.5 or higher) regardless 
of the condition. However, for users with more 
PopUp?Incorrect events, the behavior depends 
on the condition: R users (crosses) tend to have 
lower NLG (0.5 or lower) while PI users (cir-
cles) tend to cover the whole NLG spectrum (0.2 
to 0.73). Our next analysis will provide objective 
support for this observation. 
5.3 Performance improvements 
The simplest way to investigate the effect of the 
new PopUp?Incorrect strategy is to compare the 
two systems in terms of performance (i.e. learn-
ing). Table 3 shows in the second column the 
learning (NLG) in each condition. We find that 
the new strategy provides a small 0.02 perform-
ance improvement (0.48 vs. 0.46), but this effect 
is far from being significant. A one-way 
ANOVA test finds no significant effect of the 
condition on the NLG (F(1,45)=0.12, p<0.73). 
All Low High
PI 0.48 (0.19) 0.49 (0.21) 0.48 (0.17)
R 0.46 (0.19) 0.56 (0.13) 0.30 (0.18)
PI Split
 
Table 3. System performance (NLG) in each condi-
tion  
(averages and standard deviation in parentheses) 
There are several factors that contribute to this 
lack of significance. First, the new PopUp?
Incorrect strategy is only activated by users that 
have PopUp?Incorrect events. Including users 
without such events in our comparison could 
weaken the effect of the new strategy. Second, 
the impact of the new strategy might depend on 
how many times it was activated. This relates 
back to our hypothesis that that a PopUp?
Incorrect is an instance of a failed learning op-
portunity. If this is true and our new PopUp?
Incorrect strategy is effective, then we should see 
a stronger impact on PI users with a higher 
number of PopUp?Incorrect events compared 
with the similar R users. 
To test if the impact of the strategy depends on 
how many times it was engaged, we split users 
based on their PopUp?Incorrect (PISplit) behav-
ior into two subsets: Low and High. We used the 
182
mean split based on the PopUp?Incorrect relative 
percentage parameter (see the X axis in Figure 
1): users with a parameter value less than 30% go 
into the Low subset (15 PI and 14 R users) while 
the rest go into the High subset (10 PI and 8 R 
users). 
Results are shown in the third and the fourth 
columns in Table 3. To test the significance of 
the effect, we run a two-way factorial ANOVA 
with NLG as the dependent variable and two fac-
tors: PISplit (Low vs. High) and Condition (PI 
vs. R). We find a significant effect of the combi-
nation PISplit ? Condition (F(1,43)=5.13, 
p<0.03). This effect and the results of the post-
hoc tests are visualized in Figure 2. We find that 
PI users have a similar NLG regardless of their 
PopUp?Incorrect behavior while for R, High PI-
Split users learn less than Low PISplit users. 
Posthoc tests indicate that High PISplit R users 
learn significantly less than Low PISplit R users 
(p<0.01) and both categories of PI users 
(p<0.05). In other words, there is an inherent and 
significant performance gap between R users in 
the two subsets. The effect of the new PopUp?
Incorrect strategy is to bridge this gap and bring 
High PISplit users to the performance level of 
the Low PISplit users. This confirms that the new 
PopUp?Incorrect strategy is effective where it is 
most needed (i.e. High PISplit users). 
pi r
condition
0.1
0.2
0.3
0.4
0.5
0.6
0.7
N
LG
 L
 H
 
Figure 2. PISplit ? Condition effect on NLG 
(bars represent 95% confidence intervals) 
It is interesting to note that Low PISplit R us-
ers learn better than both categories of PI users 
although the differences are not significant. We 
hypothesize this happens because not all learning 
issues are signaled by PopUp?Incorrect events: a 
user might still have low learning even if he/she 
does not exhibit any PopUp?Incorrect events. 
Indeed, there are two PI users with a single 
PopUp?Incorrect event but with very low learn-
ing (NLG of 0.00 and 0.14 respectively). It is 
very likely that other things went wrong for these 
users rather than the activation of the new 
PopUp?Incorrect strategy (e.g. they might have 
other misconceptions that are not addressed by 
the remediation subdialogues). In fact, removing 
these two users results in identical NLG averages 
for the two low PISplit subsets. 
5.4 Dialogue duration 
We also wanted to know if the new PopUp?
Incorrect strategy has an effect on measures of 
dialogue duration. The strategy delivers addi-
tional explanations which can result in an in-
crease in the time users spend with the system 
(due to reading of the new instruction). Also, 
when designing tutoring systems researchers 
strive for learning efficiency: deliver increased 
learning as fast as possible.  
Total time
(min)
No. of sys. 
turns
PI 44.2 (6.2) 86.4 (6.8)
R 45.5 (5.7) 90.9 (9.3)  
Table 4. Dialogue duration metrics  
(averages and standard deviation in parentheses) 
We look at two shallow dialogue metrics: dia-
logue time and number of turns. Table 4 shows 
that, in fact, the dialogue duration is shorter for 
PI users on both metrics. A one way ANOVA 
finds a non-significant effect on dialogue time 
(F(1,45)=0.57, p<0.45) but a trend effect for 
number of system turns (F(1,45)=3.72, p<0.06). 
We hypothesize that 2 factors are at play here. 
First, the additional information activated by the 
new PopUp?Incorrect strategy might have a 
positive effect on users? correctness for future 
system questions especially on questions that 
discuss similar topics. As a result, the system has 
to correct the user less and, consequently, finish 
faster. Second, the average total time PI users 
spend reading the additional information is very 
small (about 2 minutes) compared to the average 
dialogue time. 
6 Related work 
Designing robust, efficient and usable spoken 
dialogue systems (SDS) is a complex process 
that is still not well understood by the SDS re-
search community (M?ller and Ward, 2008). 
Typically, a number of evaluation/performance 
183
metrics are used to compare multiple (versions 
of) SDS. But what do these metrics and the re-
sulting comparisons tell us about designing SDS? 
There are several approaches to answering this 
question, each requiring a different level of su-
pervision.  
One approach that requires little human super-
vision is to use reinforcement learning. In this 
approach, the dialogue is modeled as a (partially 
observable) Markov Decision Process (Levin et 
al., 2000; Young et al, 2007). A reward is given 
at the end of the dialogue (i.e. the evaluation 
metric) and the reinforcement learning process 
propagates back the reward to learn what the best 
strategy to employ at each step is. Other semi-
automatic approaches include machine learning 
and decision theoretic approaches (Levin and 
Pieraccini, 2006; Paek and Horvitz, 2004). How-
ever, these semi-automatic approaches are feasi-
ble only in small and limited domains though 
recent work has shown how more complex do-
mains can be modeled (Young et al, 2007). 
An approach that works on more complex 
domains but requires more human effort is 
through performance analysis: finding and tack-
ling factors that affect the performance (e.g. 
PARADISE (Walker et al, 2000)). Central to 
this approach is the quality of the interaction pa-
rameters in terms of predicting the performance 
metric (predictiveness) and informing useful 
modifications of the system (informativeness). 
An extensive set of parameters can be found in 
(M?ller, 2005). 
Our use of discourse structure for performance 
analysis extends over previous work in two im-
portant aspects. First, we exploit in more detail 
the hierarchical information in the discourse 
structure through the domain-independent con-
cept of discourse structure transitions. Most pre-
vious work does not use this information (e.g. 
(M?ller, 2005)) or, if used, it is flattened (Walker 
et al, 2001). Also, to our knowledge, previous 
work has not employed parameters similar to our 
transition?phenomena (transition?correctness in 
this paper) and transition?transition bigram pa-
rameters. In addition, several of these parameters 
are predictive (Rotaru and Litman, 2006). 
Second, in our work we also look at the in-
formativeness while most of the previous work 
stops at the predictiveness step. A notable excep-
tion is the work by (Litman and Pan, 2002). The 
factor they look at is user?s having multiple 
speech recognition problems in the dialogue. 
This factor is well known in the SDS field and it 
has been shown to be predictive of system per-
formance by previous work (e.g. (Walker et al, 
2000)). To test the informativeness of this factor, 
Litman and Pan propose a modification of the 
system in which the initiative and confirmation 
strategies are changed to more conservative set-
tings whenever the event is detected. Their re-
sults show that the modified version leads to im-
provements in terms of system performance (task 
completion). We extend over their work by look-
ing at a factor (PopUp?Incorrect) that was not 
known to be predictive of performance before-
hand. We discover this factor through our em-
pirical analyses of existing dialogues and we 
show that by addressing it (the new PopUp?
Incorrect strategy) we also obtain performance 
improvements (at least for certain users). In addi-
tion, we are looking at a performance metric for 
which significant improvements are harder to 
obtain with small system changes (e.g. (Graesser 
et al, 2003)). 
7 Conclusions 
In this paper we finalize our investigation into 
the utility of discourse structure for SDS per-
formance analysis (at least for our system). We 
use the discourse structure transition information 
in combination with other dialogue phenomena 
to derive a number of interaction parameters (i.e. 
transition?phenomena and transition?transition). 
Our previous work (Rotaru and Litman, 2006) 
has shown that these parameters are predictive of 
system performance. Here we take a step further 
and show that one of these parameters (the 
PopUp?Incorrect bigram) is also informative. 
From the interpretation of its predictiveness, we 
inform a promising modification of our system: 
offer additional explanations after PopUp?
Incorrect events. We implement this modifica-
tion and we compare it with the original system 
through a user study. We find that the modifica-
tion breaks down the negative correlation be-
tween PopUp?Incorrect and system performance. 
In addition, users that need the modification the 
most (i.e. users with more PopUp?Incorrect 
events) show significant improvement in per-
formance in the modified system over corre-
sponding users in the original system. However, 
this improvement is not strong enough to gener-
ate significant differences at the population level. 
Even though the additional explanations add ex-
tra time to the dialogue, overall we actually see a 
small reduction in dialogue duration. 
Our work has two main contributions. First, 
we demonstrate the utility of discourse structure 
184
for performance analysis. In fact, our other work 
(Rotaru and Litman, 2007) shows that discourse 
structure is also useful for other SDS tasks. Sec-
ond, to our knowledge, we are the first to show a 
complete application of the performance analysis 
methodology. We discover a new set of predic-
tive interaction parameters in our system and we 
show how our system can be improved in light of 
these findings. Consequently, we validate per-
formance analysis as an iterative, ?debugging? 
approach to dialogue design. By analyzing cor-
pora collected with an initial version of the sys-
tem, we can identify semi-automatically prob-
lems in the dialogue design. These problems in-
form a new version of the system which will be 
tested for performance improvements. In terms 
of design methodology for tutoring SDS, our re-
sults suggest the following design principle: ?do 
not give up but try other approaches?. In our 
case, we do not give up after a PopUp-Incorrect 
but give additional explanations. 
In the future, we would like to extend our 
work to other systems and domains. This should 
be relatively straightforward as the main ingredi-
ents, the discourse transitions, are domain inde-
pendent. 
Acknowledgments 
This work is supported by the NSF grants 
0328431 and 0428472. We would like to thank 
the ITSPOKE group. 
References 
D. Bohus. 2007. Error Awareness and Recovery in 
Conversational Spoken Language Interfaces. Ph.D. 
Dissertation, Carnegie Mellon University, School 
of Computer Science 
A. Graesser, K. Moreno, J. Marineau, A. Adcock, A. 
Olney and N. Person. 2003. AutoTutor improves 
deep learning of computer literacy: Is it the dialog 
or the talking head? In Proc. of Artificial Intelli-
gence in Education (AIED). 
B. Grosz and C. L. Sidner. 1986. Attentions, inten-
tions and the structure of discourse. Computational 
Linguistics, 12(3). 
S. Katz, D. Allbritton and J. Connelly. 2003. Going 
Beyond the Problem Given: How Human Tutors 
Use Post-Solution Discussions to Support Transfer. 
International Journal of Artificial Intelligence in 
Education (IJAIED), 13. 
E. Levin and R. Pieraccini. 2006. Value-based opti-
mal decision for dialogue systems. In Proc. of 
IEEE/ACL Workshop on Spoken Language Tech-
nology (SLT). 
E. Levin, R. Pieraccini and W. Eckert. 2000. A Sto-
chastic Model of Human Machine Interaction for 
Learning Dialog Strategies. IEEE Transactions on 
Speech and Audio Processing, 8:1. 
D. Litman and S. Pan. 2002. Designing and Evaluat-
ing an Adaptive Spoken Dialogue System. User 
Modeling and User-Adapted Interaction, 12(2/3). 
D. Litman, C. Rose, K. Forbes-Riley, K. VanLehn, D. 
Bhembe and S. Silliman. 2006. Spoken Versus 
Typed Human and Computer Dialogue Tutoring. 
International Journal of Artificial Intelligence in 
Education, 16. 
S. M?ller. 2005. Parameters for Quantifying the In-
teraction with Spoken Dialogue Telephone Services. 
In Proc. of SIGDial. 
S. M?ller and N. Ward. 2008. A Framework for 
Model-based Evaluation of Spoken Dialog Systems. 
In Proc. of Workshop on Discourse and Dialogue 
(SIGDial). 
T. Paek and E. Horvitz. 2004. Optimizing Automated 
Call Routing by Integrating Spoken Dialog Models 
with Queuing Models. In Proc. of HLT-NAACL. 
A. Raux and M. Eskenazi. 2008. Optimizing End-
pointing Thresholds using Dialogue Features in a 
Spoken Dialogue System. In Proc. of 9th SIGdial 
Workshop on Discourse and Dialogue. 
M. Rotaru. 2008. Applications of Discourse Structure 
for Spoken Dialogue Systems. Ph.D. Dissertation, 
University of Pittsburgh, Department of Computer 
Science 
M. Rotaru and D. Litman. 2006. Exploiting Discourse 
Structure for Spoken Dialogue Performance Analy-
sis. In Proc. of EMNLP. 
M. Rotaru and D. Litman. 2007. The Utility of a 
Graphical Representation of Discourse Structure 
in Spoken Dialogue Systems. In Proc. of ACL. 
S. Singh, D. Litman, M. Kearns and M. Walker. 2002. 
Optimizing Dialogue Management with Rein-
forcement Learning: Experiments with the NJFun 
System. Journal of Artificial Intelligence Research, 
(16). 
K. VanLehn, A. C. Graesser, G. T. Jackson, P. Jordan, 
A. Olney and C. P. Rose. 2007. When are tutorial 
dialogues more effective than reading? Cognitive 
Science, 31(1). 
M. Walker, D. Litman, C. Kamm and A. Abella. 2000. 
Towards Developing General Models of Usability 
with PARADISE. Natural Language Engineering. 
M. Walker, R. Passonneau and J. Boland. 2001. 
Quantitative and Qualitative Evaluation of Darpa 
Communicator Spoken Dialogue Systems. In Proc. 
of ACL. 
S. Young, J. Schatzmann, K. Weilhammer and H. Ye. 
2007. The Hidden Information State Approach to 
Dialog Management. In Proc. of ICASSP. 
185
 
 
 
DS 1 
TUTOR1: Consider Newton's laws applied to two 
objects that move together.  What three 
quantities does Newton's Second Law 
describe the relationship between? 
 Student answer1: correct (e.g. force, mass, accel.) 
TUTOR2: If two bodies are connected so that they move 
together and you know the acceleration of the 
first body, what is the acceleration of the 
second body? 
 Student answer2: incorrect (e.g. zero) 
 
 
 
 
 
 
 
 
 
 
 
 
 
TUTOR5: If a force acts on one body such 
that it moves, what happens to the second 
body? 
 Student answer5: incorrect but rejected (e.g. stays) 
TUTOR6: Could you please repeat that? 
? 
DS 2 
TUTOR3: If the two bodies always move 
together and one body speeds up, 
what happens to the other? 
 Student answer3: incorrect (e.g. lags behind) 
TUTOR4: The second body will speed up too. If 
the first body accelerates at a 
particular rate, will the second body 
accelerate at an equal or different 
rate? 
 Student answer4: correct (e.g. equal) 
ESSAY SUBMISSION & ANALYSIS 
 
Appendix 1. Automatic annotation of discourse structure hierarchy and of discourse structure transitions 
 
Discourse structure hierarchy annotation: DS1 is the top level discourse segment. Its purpose is 
to correct misconceptions in user?s essay and/or to elicit more complete explanations for the 
essay. DS2 is an embedded discourse segment which corresponds to the remediation subdia-
logue for question Tutor2. 
 
Discourse structure transition annotation: Each transition labels the system turn at the tip of the 
arrow (e.g. Tutor2 is labeled with Advance). Please note that Tutor2 will not be labeled with 
PopUp because, in such cases, an extra system turn will be created between Tutor4 and Tutor5 
with the same content as Tutor2. This extra turn also includes variations of ?Ok, back to the 
original question? to mark the discourse segment boundary transition. 
 
186
 
 
 
 
 
Appendix 2. Sample additional instructions webpage 
 
Problem discussed by ITSPOKE: Suppose a man is running in a straight line at constant speed. 
He throws a pumpkin straight up. Where will it land? Explain. 
 
Location in the dialogue: For this problem, ITSPOKE discusses what happens during three 
time frames: before pumpkin toss, during pumpkin toss and after pumpkin toss. ITSPOKE is 
currently discussing the forces and the net force on the pumpkin during the toss.  
 
187
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 286?289,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Spoken Tutorial Dialogue and the Feeling of Another?s Knowing
Diane Litman
University of Pittsburgh
Pittsburgh, PA 15260 USA
litman@cs.pitt.edu
Kate Forbes-Riley
University of Pittsburgh
Pittsburgh, PA 15260 USA
forbesk@cs.pitt.edu
Abstract
We hypothesize that monitoring the accu-
racy of the ?feeling of another?s know-
ing? (FOAK) is a useful predictor of tu-
torial dialogue system performance. We
test this hypothesis in the context of a
wizarded spoken dialogue tutoring system,
where student learning is the primary per-
formance metric. We first present our cor-
pus, which has been annotated with re-
spect to student correctness and uncer-
tainty. We then discuss the derivation of
FOAK measures from these annotations,
for use in building predictive performance
models. Our results show that monitoring
the accuracy of FOAK is indeed predictive
of student learning, both in isolation and in
conjunction with other predictors.
1 Introduction
Detecting and exploiting knowledge of a speaker?s
uncertainty has been studied in several research
communities. Spoken language researchers have
identified statistically significant relationships be-
tween speaker uncertainty and linguistic proper-
ties of utterances such as prosody and lexical con-
tent (Liscombe et al, 2005; Dijkstra et al, 2006;
Pon-Barry, 2008). Spoken dialogue researchers
in turn are studying whether responding to user
states such as uncertainty can improve system
performance as measured by usability and effi-
ciency (Tsukahara and Ward, 2001; Pon-Barry et
al., 2006; Forbes-Riley and Litman, 2009a). In
the psycholinguistics community, uncertainty has
been studied in the context of metacognitive abil-
ities, e.g. the ability to monitor the accuracy
of one?s own knowledge (?Feeling of Knowing?
(FOK)), and the ability to monitor the FOK of
someone else (?Feeling of Another?s Knowing?
(FOAK)) (Smith and Clark, 1993; Brennan and
Williams, 1995).
Here we take a spoken dialogue systems per-
spective on FOAK, and investigate whether mon-
itoring the accuracy of FOAK is a useful con-
struct for predictive performance modeling. Our
study uses data previously collected with a wiz-
arded spoken dialogue tutoring system, where stu-
dent learning is the primary performance metric.
Section 2 reviews several relevant constructs and
measures from the area of metacognition. Sec-
tion 3 introduces our dialogue corpus and its user
correctness and uncertainty annotations. Section 4
presents our method for measuring monitoring ac-
curacy of FOAK from these annotations, while
Section 5 shows how we use these measures to
build predictive performance models. Our results
show that monitoring the accuracy of FOAK is in-
deed a significant positive predictor of learning,
both in isolation and over and above other predic-
tors. As discussed in Section 6, increasing mon-
itoring accuracy of FOAK is thus one avenue for
also potentially increasing performance, which we
plan to explore in future versions of our system.
2 Feeling of Another?s Knowing
?Feeling of knowing? (FOK) refers to peoples?
ability to accurately monitor their own knowl-
edge, e.g. to know whether they have answered
a question correctly. Psycholinguistics research
has shown that speakers display FOK in conver-
sation using linguistic cues such as filled pauses
and prosody (Smith and Clark, 1993). Of perhaps
more relevance to dialogue systems, research has
also shown that listeners can use the same cues
to monitor the FOK of someone else, i.e. ?feel-
286
ing of another?s knowing? (FOAK) (Brennan and
Williams, 1995).
To quantify knowledge monitoring, measures of
monitoring accuracy have been proposed. For ex-
ample, consider an FOK experimental paradigm,
where subjects 1) respond to a set of general
knowledge questions, 2) take a FOK survey, judg-
ing whether or not1 they think they would rec-
ognize the answer to each question in a multiple
choice test, and 3) take such a recognition test. As
shown in Figure 1, such data can be summarized in
an array where each cell represents a mutually ex-
clusive option: the row labels represent the possi-
ble FOK judgments (Y/N), while the columns rep-
resent the possible results of the multiple choice
test (Y/N).
Recognition=Y Recognition=N
Judgment=Y a b
Judgment=N c d
Gamma = (a)(d)?(b)(c)(a)(d)+(b)(c) HC =
(a+d)?(b+c)
(a+d)+(b+c)
Figure 1: Measuring Monitoring Accuracy.
Given such an array, the relationship between
the correctness and the judgment of FOK for an-
swers can be measured using the standard formu-
las in Figure 1: Gamma and the Harmann coef-
ficient (HC) measure relative and absolute knowl-
edge monitoring accuracy, respectively. We use
both metrics in the performance modeling exper-
iments described in Section 5, as both measures
have been well studied and there is a lack of con-
sensus regarding their relative benefits (e.g. (Niet-
feld et al, 2006)). Note that both range in value
from -1 (no monitoring accuracy) to 1 (perfect
monitoring accuracy).
3 Data
This research uses a corpus of 405 dialogues be-
tween 81 students and ITSPOKE-WOZ. The cor-
pus was collected in an experiment showing that
performance improved when ITSPOKE-WOZ re-
sponded to the uncertainty in student answers,
over and above correctness (Forbes-Riley and Lit-
man, 2009a).
ITSPOKE-WOZ is a semi-automatic version
of ITSPOKE (Intelligent Tutoring SPOKEn di-
alogue system), which is a speech-enhanced ver-
sion of the Why2-Atlas qualitative physics tu-
tor (VanLehn et al, 2002). Recent versions of
1Likert scale rating schemes are also possible.
ITSPOKE have also been enhanced to detect and
respond to student uncertainty, by parameterizing
the dialogue manager so that it can adapt con-
tingently based on student correctness and/or un-
certainty (to test various ways of responding to
uncertainty). In ITSPOKE-WOZ, a few compo-
nents (speech recognition, language understand-
ing, and uncertainty detection) have been replaced
by a human ?wizard?, to test the upper-bound per-
formance of adapting to uncertainty in a spoken
dialogue system (i.e. without the noise introduced
by speech and language processing).
The experimental procedure was as follows:
Students (1) read background material, (2) took
a multiple-choice pretest, (3) worked through five
problems with ITSPOKE-WOZ (1 per dialogue),
(4) took a survey, and (5) took a posttest simi-
lar to the pretest. Each student turn in the cor-
pus was annotated2 in real-time by the wizard dur-
ing the experiment, using binary tags representing
student Uncertainty (uncertain (U), nonuncertain
(nonU)) and student Correctness (correct (C), in-
correct (I)). Figure 2 shows an annotated dialogue
excerpt.
ITSPOKE-WOZ6: Now let?s find the forces exerted on the
car in the VERTICAL direction during the collision. First,
what vertical force is always exerted on an object near the
surface of the earth?
USER6: gravity (nonU,C)
ITSPOKE-WOZ7: Fine. In addition to the downward force
of gravity, one other force is exerted on the car in the VER-
TICAL direction during the collision. We haven?t discussed
this force yet, because we only dealt with freefalling objects
before this problem. But you may recall this other vertical
force from the reading. What is it?
USER7: um ... the friction from the air ... or the wind (U,I)
ITSPOKE-WOZ8: Great try, but I think you know there?s
a mistake in your answer. Let?s work it out. We know that
gravity can?t be the only vertical force on the car, because . . .
Figure 2: Annotated Corpus Excerpt.
4 Predictors
To derive our performance models, we use
posttest score as the dependent measure, and in-
clude pretest score among the independent mea-
sures. This allows us to measure learning gain,
not just final test score. We also include addi-
tional independent measures computed from the
wizard?s uncertainty and correctness annotations,
2In prior ITSPOKE corpora, this wizard displayed in-
terannotator agreement of 0.85 and 0.62 Kappa on correct-
ness and uncertainty, respectively (Forbes-Riley and Litman,
2008).
287
as detailed below. Note that all measures are com-
puted on a per student basis (over all 5 dialogues).
Table 1 shows means and standard deviations of
all measures across all 81 students.
Measure Mean Std. Dev.
pretest .51 .15
posttest .75 .14
%C .79 .09
%U .23 .11
Gamma .77 .17
HC .59 .16
Table 1: Descriptive Corpus Statistics.
The percentage of student turns annotated as
correct (%C) and as uncertain (%U) normalize
the raw counts of the wizard?s C and U annota-
tions. Similar measures predict learning in prior
experiments by ourselves and others (e.g (Litman
et al, 2009)) and thus serve as useful baselines.
In our corpus, 79% of a student?s turns are an-
swered correctly on average, while 77% are an-
swered without uncertainty.
The monitoring accuracy measures Gamma
and HC were introduced in Section 2. To con-
struct an array like that shown in Figure 1, we
map the first and second rows to our uncertainty
annotations NonU and U, and map the columns to
our correctness annotations C and I. In (Dijkstra
et al, 2006), high and low FOK/FOAK judgments
are similarly associated with speaker certainty and
uncertainty, respectively. Note that in our annota-
tion scheme, NonU answers are either certain or
neutral.
5 Results: Predicting Student Learning
Given the above measures, our first prediction ex-
periment measures the partial Pearson?s correla-
tion between each of the independent measures
and posttest, after first controlling for pretest to
account for learning gain. Our goal here is exam-
ine the predictive utility of the correctness, uncer-
tainty, and monitoring dimensions in isolation.
Table 2 shows the statistically significant results
of the partial correlations. The table shows the in-
dependent measure, the corresponding Pearson?s
Correlation Coefficient (R), and the significance of
the correlation (p). As can be seen, both monitor-
ing measures are positively correlated with learn-
ing, with HC providing better predictive utility
than Gamma. However, %C is even more pre-
dictive of learning than either monitoring measure.
Interestingly, the uncertainty measure %U in and
of itself does not show predictive utility in this
data.
Measure R p
%C .52 .00
Gamma .36 .00
HC .42 .00
Table 2: Partial Correlations with Posttest (p <
.05).
Our second prediction experiment uses PAR-
ADISE to build a learning model that can po-
tentially include multiple independent measures.
As in prior PARADISE applications (e.g. (Mo?ller,
2005)), we train the models using stepwise mul-
tiple linear regression, which automatically deter-
mines the measures to include in the model. Our
goal here is to explore whether monitoring accu-
racy provides any added value to our correctness
and uncertainty measures.
When all measures are made available for pre-
dicting learning, we see that monitoring accuracy
as measured byHC does add value over and above
correctness: the stepwise procedure includes HC
in the model, as it significantly accounts for more
variance than just including %C and pretest. In
particular, the application of PARADISE shows
that the following performance function provides
the best significant training fit to our data (R2 =
.71, p < .01):
postest = .44?%C+ .21?pretest+ .20?HC
The equation shows each selected measure and its
(standardized) weight; larger weights indicate pa-
rameters with greater relative predictive power in
accounting for posttest variance. %C is signifi-
cant at p < .01, while pretest and HC are each
significant at p < .05, with the coefficients all pos-
itive. Like the correlations, our regression demon-
strates the predictive utility of the accuracy and
monitoring measures, but not the uncertainty mea-
sure. The model further shows that while correctly
answering the system?s questions (%C) is predic-
tive of learning, also including FOAK monitoring
accuracy (HC) significantly increases the model?s
predictive power.
6 Conclusion and Future Directions
This paper explores whether knowledge monitor-
ing accuracy is a useful construct for understand-
ing dialogue system performance. In particular,
288
we demonstrate the utility of combining previ-
ously studied correctness and uncertainty annota-
tions, using a measure of FOAK monitoring ac-
curacy. Our results show that while the correct-
ness of a user?s response predicts learning, the un-
certainty with which a user conveys a response
does not. In contrast, the ability to monitor FOAK
accuracy predicts learning, in isolation and over
and above correctness. We believe that monitor-
ing accuracy will be a relevant construct for other
dialogue applications involving knowledge asym-
metry, such as problem solving, instruction giv-
ing, and trouble shooting (e.g. (Janarthanam and
Lemon, 2008)).
In future work we plan to use our results to in-
form a modification of our system aimed at im-
proving inferred user knowledge monitoring abil-
ities; we will better measure such improvements
by incorporating FOK ratings into our testing. In
addition, we recently found interactions between
learning and both user domain expertise and gen-
der (Forbes-Riley and Litman, 2009b); we will
investigate whether similar interactions extend to
knowledge monitoring metrics. Since our corpus
contains dialogues with both uncertainty-adaptive
and non-adaptive versions of ITSPOKE-WOZ, we
also plan to examine whether differing dialogue
strategies influence the learned predictive models.
Finally, we plan to replicate our analyses in a di-
alogue corpus we recently collected using a fully
automated version of our system.
Acknowledgements
NSF #0631930 supports this work. We thank H.
Ai, P. Jordan, and the reviewers for helpful com-
ments.
References
S. E. Brennan and M. Williams. 1995. The feeling
of another?s knowing: Prosody and filled pauses as
cues to listeners about the metacognitive states of
speakers. Journal of Memory and Language.
C. Dijkstra, E. Krahmer, and M. Swerts. 2006. Ma-
nipulating uncertainty: The contribution of different
audiovisual prosodic cues to the perception of confi-
dence. In Proc. Speech Prosody.
K. Forbes-Riley and D. J. Litman. 2008. Analyz-
ing dependencies between student certainness states
and tutor responses in a spoken dialogue corpus. In
L. Dybkjaer and W. Minker, editors, Recent Trends
in Discourse and Dialogue. Springer.
K. Forbes-Riley and D. Litman. 2009a. Adapting to
student uncertainty improves tutoring dialogues. In
Proc. Intl. Conf. on Artificial Intelligence in Educa-
tion.
K. Forbes-Riley and D. Litman. 2009b. A user
modeling-based performance analysis of a wizarded
uncertainty-adaptive dialogue system corpus. In
Proc. Interspeech, Brighton, UK, September.
S. Janarthanam and O. Lemon. 2008. User simulations
for online adaptation and knowledge-alignment in
troubleshooting dialogue systems. In Proc. SEM-
dial.
J. Liscombe, J. Venditti, and J. Hirschberg. 2005. De-
tecting certainness in spoken tutorial dialogues. In
Proc. Interspeech.
D. Litman, J. Moore, M. Dzikovska, and E. Farrow.
2009. Using natural language processing to analyze
tutorial dialogue corpora across domains and modal-
ities. In Proc. Intl. Conf. on Artificial Intelligence in
Education.
S. Mo?ller. 2005. Parameters for quantifying the in-
teraction with spoken dialogue telephone services.
In Proc. SIGdial Workshop on Discourse and Dia-
logue.
J. L. Nietfeld, C. K. Enders, and G. Schraw. 2006. A
Monte Carlo comparison of measures of relative and
absolute monitoring accuracy. Educational and Psy-
chological Measurement.
H. Pon-Barry, K. Schultz, E. O. Bratt, B. Clark, and
S. Peters. 2006. Responding to student uncertainty
in spoken tutorial dialogue systems. Intl. Journal of
Artificial Intelligence in Education.
H. Pon-Barry. 2008. Prosodic manifestations of confi-
dence and uncertainty in spoken language. In Proc.
Interspeech.
V. L. Smith and H. H. Clark. 1993. On the course of
answering questions. Journal of Memory and Lan-
guage.
W. Tsukahara and N. Ward. 2001. Responding to sub-
tle, fleeting changes in the user?s internal state. In
Proc. SIG-CHI on Human factors in computing sys-
tems.
K. VanLehn, P. W. Jordan, C. Rose?, D. Bhembe,
M. Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intl. Conf. on Intelligent Tutoring
Systems.
289
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1985?1995, Dublin, Ireland, August 23-29 2014.
Empirical analysis of exploiting review helpfulness for extractive
summarization of online reviews
Wenting Xiong
University of Pittsburgh
Department of Computer Science
wex12@cs.pitt.edu
Diane Litman
University of Pittsburgh
Department of Computer Science & LRDC
litman@cs.pitt.edu
Abstract
We propose a novel unsupervised extractive approach for summarizing online reviews by ex-
ploiting review helpfulness ratings. In addition to using the helpfulness ratings for review-level
filtering, we suggest using them as the supervision of a topic model for sentence-level content
scoring. The proposed method is metadata-driven, requiring no human annotation, and generaliz-
able to different kinds of online reviews. Our experiment based on a widely used multi-document
summarization framework shows that our helpfulness-guided review summarizers significantly
outperform a traditional content-based summarizer in both human evaluation and automated eval-
uation.
1 Introduction
Multi-document summarization has great potential in online reviews, as manually reading comments
provided by other users is time consuming if not impossible. While extractive techniques are generally
preferred over abstractive ones (as abstraction can introduce disfluency), existing extractive summarizers
are either supervised or based on heuristics of certain desired characteristics of the summarization result
(e.g., maximize n-gram coverage (Nenkova and Vanderwende, 2005), etc.). However, when it comes
to online reviews, there are problems with both approaches: the first one requires manual annotation
and is thus less generalizable; the second one might not capture the salient information in reviews from
different domains (camera reviews vs. movie reviews), because the heuristics are designed for traditional
genres (e.g., news articles) while the utility of reviews might vary with the review domain.
We propose to exploit review metadata, that is review helpfulness ratings
1
, to facilitate review sum-
marization. Because this is user-provided feedback on review helpfulness which naturally reflects users?
interest in online review exploration, our approach captures domain-dependent salient information adap-
tively. Furthermore, as this metadata is widely available online (e.g., Amazon.com, IMDB.com)
2
, our
approach is unsupervised in the sense that no manual annotation is needed for summarization purposes.
Therefore, we hypothesize that summarizers guided by review helpfulness will outperform systems based
on textual features/heuristics designed for traditional genres. To build such helpfulness-guided summa-
rizers, we introduce review helpfulness during content selection in two ways: 1) using the review-level
helpfulness ratings directly to filter out unhelpful reviews, 2) using sentence-level helpfulness features
derived from review-level helpfulness ratings for sentence scoring. As we observe in our pilot study
that supervised LDA (sLDA) (Blei and McAuliffe, 2010) trained with review helpfulness ratings has
potential in differentiating review helpfulness at the sentence level, we develop features based on the
inferred hidden topics from sLDA to capture the helpfulness of a review sentence for summarization pur-
poses. We implement our helpfulness-guided review summarizers based on an widely used open-source
multi-document extractive summarization framework (MEAD (Radev et al., 2004)). Both human and
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
This is the percentage of readers who found the review to be helpful (Kim et al., 2006).
2
If it is not available, the review helpfulness can be assessed fully automatically (Kim et al., 2006; Liu et al., 2008).
1985
automated evaluations show that our helpfulness-guided summarizers outperform a strong baseline that
MEAD provides across multiple review domains. Further analysis on the human summaries shows that
some effective heuristics proposed for traditional genres might not work well for online reviews, which
indirectly supports our use of review metadata as supervision. The presented work also extrinsically
demonstrates that the helpfulness-related topics learned from the review-level supervision can capture
review helpfulness at the sentence-level.
2 Related Work
In multi-document extractive summarization, various unsupervised approaches have been proposed to
avoid manual annotation. A key task in extractive summarization is to identify important text units.
Prior successful extractive summarizers score a sentence based on n-grams within the sentence: by the
word frequency (Nenkova and Vanderwende, 2005), bigram coverage (Gillick and Favre, 2009), topic
signatures (Lin and Hovy, 2000) or latent topic distribution of the sentence (Haghighi and Vanderwende,
2009), which all aim to capture the ?core? content of the text input. Other approaches regard the n-
gram distribution difference (e.g., Kullback-Lieber (KL) divergence) between the input documents and
the summary (Lin et al., 2006), or based on a graph-representation of the document content (Erkan and
Radev, 2004; Leskovec13 et al., 2005), with an implicit goal to maximize the output representativeness.
In comparison, while our approach follows the same extractive summarization paradigm, it is metadata
driven, identifying important text units through the guidance of user-provided review helpfulness assess-
ment.
When it comes to online reviews, the desired characteristics of a review summary are different from
traditional text genres (e.g., news articles), and could vary from one review domain to another. Thus
different review summarizers have been proposed to focus on different desired properties of review sum-
maries, primarily based on opinion mining and sentiment analysis (Carenini et al., 2006; Lerman et al.,
2009; Lerman and McDonald, 2009; Kim and Zhai, 2009). Here the desired property varies from the
coverage of product aspects (Carenini et al., 2006; Lerman et al., 2009) to the degree of agreement on
aspect-specific sentiment (Lerman et al., 2009; Lerman and McDonald, 2009; Kim and Zhai, 2009).
While there is a large overlap between text summarization and review opinion mining, most work fo-
cuses on sentiment-oriented aspect extraction and the output is usually a set of topics words plus their
representative text units (Hu and Liu, 2004; Zhuang et al., 2006). However, such a topic-based summa-
rization framework is beyond the focus of our work, as we aim to adapt traditional extractive techniques
to the review domain by introducing review helpfulness ratings as guidance.
In this paper, we utilize review helpfulness via using sLDA. The idea of using sLDA in text summa-
rization is not new. However, the model is previously applied at the sentence level (Li and Li, 2012),
which requires human annotation on the sentence importance. In comparison, our use of sLDA is at
the document (review) level, using existing metadata of the document (review helpfulness ratings) as the
supervision, and thus requiring no annotation at all. With respect to the use of review helpfulness ratings,
early work of review summarization (Liu et al., 2007) only consider it as a filtering criteria during input
preprocessing. Other researchers use it as the gold-standard for automated review helpfulness prediction,
a predictor of product sales (Ghose and Ipeirotis, 2011), a measurement of reviewers? authority in social
network analysis (Lu et al., 2010), etc.
3 Helpfulness features for sentence scoring
While the most straightforward way to utilize review helpfulness for content selection is through filter-
ing (Liu et al., 2007) (further discussed in Section 4.3), we also propose to take into account review
helpfulness during sentence scoring by learning helpfulness-related review topics in advance. Because
sLDA learns the utility of the topics for predicting review-level helpfulness ratings (decomposing review
helpfulness ratings by topics), we develop novel features (rHelpSum and sHelpSum) based on the in-
ferred topics of the words in a sentence to capture its helpfulness in various perspectives. We later use
them for sentence scoring in a helpfulness-guided summarizer (Section 4.3).
Compared with LDA (Blei et al., 2003), sLDA (Blei and McAuliffe, 2010) introduces a response
1986
variable y
i
? Y to each document D
i
during topic discovery. The model not only learns the topic
assignment z
1:N
for words w
1:N
in D
i
, it also learns a function from the posterior distribution of z in D
to Y . When Y is the review-level helpfulness gold-standard, the model leans a set of topics predictive of
review helpfulness, as well as the utility of z in predicting review helpfulness y
i
, denoted as ?. (Both z
and ? are K-dimensional.)
At each inference step, sLDA assigns a topic ID to each word in every review. z
l
= k means that
the topic ID for word at position l in sentence s is k. Given the topic assignments z
1:L
to words w
1:L
in a review sentence s, we estimate the contribution of s to the helpfulness of the review it belongs to
(Formula 1), as well as the average topic importance in s (Formula 2). While rHelpSum is sensitive to
the review length, sHelpSum is sensitive to the sentence length.
rHelpSum(s) =
1
N
l=L
?
l=1
?
k
?
k
p(z
l
= k) (1)
sHelpSum(s) =
1
L
l=L
?
l=1
?
k
?
k
p(z
l
= k) (2)
As the topic assignment in each inference iteration might not be the same, Riedl and Biemann (Riedl
and Biemann, 2012) proposed the mode method in their application of LDA for text segmentation ? use
the most frequently assigned topic for each word in all iterations as the final topic assignment ? to address
the instability issue. Inspired by their idea, we also use the mode method to infer the topic assignment in
our task, but only apply the mode method to the last 10 iterations, because the topic distribution might
not be well learned at the beginning.
4 Experimental setup
To investigate the utility of exploiting user-provided review helpfulness ratings for content selection in
extractive summarization, we develop two helpfulness-guided summarizers based on the MEAD frame-
work (HelpfulFilter and HelpfulSum). We compare our systems? performance against a strong unsu-
pervised extractive summarizer that MEAD supports as our baseline (MEAD+LexRank). To focus
on sentence scoring only, we use the same MEAD word-based MMR (Maximal Marginal Relevance)
reranker (Carbonell and Goldstein, 1998) for all summarizers, and set the length of the output to be 200
words.
4.1 Data
Our data consists of two kinds of online reviews: 4050 Amazon camera reviews provided by Jindal and
Liu (2008) and 280 IMDB movie reviews that we collected by ourselves. Both corpora were used in
our prior work of automatically predicting review helpfulness, in which every review has at least three
helpfulness votes. On average, the helpfulness of camera reviews is .80 and that of movie reviews is .74.
Summarization test sets. Because the proposed approach method is purely unsupervised, and we do
not optimize our summarization parameters during learning, we evaluate our approach based on a subset
of review items directly: we randomly sample 18 reviews for each review item (a camera or movie) and
randomly select 3 items for each review domain. In total there are 6 summarization test sets (3 items ?
2 domains), where each contains 18 reviews to be summarized (i.e. ?summarizing 18 camera reviews
for Nikon D3200?). In the summarization test sets, the average number of sentences per review is 9
for camera reviews, and 18 for movie reviews; the average number of words per sentence in the camera
reviews and movie reviews are 25 and 27, respectively.
4.2 sLDA training
We implement sLDA based on the topic modeling framework of Mallet (McCallum, 2002) using 20
topics (K = 20) and the best hyper-parameters (topic distribution priors ? and word distribution priors
1987
?) that we learned in our pilot study on LDA.
3
Since our summarization approach is unsupervised, we learn the topic assignment for each review
word using the corresponding sLDA model trained on all reviews of that domain (4050 reviews for
camera and 280 reviews for movie).
4
4.3 Three summarizers
Baseline (MEAD+LexRank): The default feature set of MEAD includes Position, Length, and Centroid.
Here Length is a word-count threshold, which gives score 0 to sentences shorter than the threshold. As
we observe that short review sentences sometimes can be very informative as well (e.g., ?This camera is
so amazing!?, ?The best film I have ever seen!?), we adjust Length to 5 from its default value 9. MEAD
also provides scripts to compute LexRank (Erkan and Radev, 2004), which is a more advanced feature
using graph-based algorithm for computing relative importance of textual units. We supplement the
default feature set with LexRank to get the best summarizer from MEAD, yielding the sentence scoring
function F
baseline
(s), in which s is a given sentence and all features are assigned equal weights (same as
in the other two summarizers).
F
baseline
(s) =
{
Position+ Centroid+ LexRank if Length ? 5
0 if Length < 5
(3)
HelpfulFilter: This summarizer is a direct extension of the baseline, which considers review-level help-
fulness ratings (hRating) as an additional filtering criteria in its sentence scoring function F
HelpfulF ilter
.
(In our study, we omit the automated prediction (Kim et al., 2006; Liu et al., 2008) and filter reviews
by their helpfulness gold-standard directly.) We set the cutting threshold to be the average helpful-
ness rating of all the reviews that we used to train the topic model for the corresponding domain
(hRatingAve(domain)).
F
HelpfulF ilter
(s) =
{
F
baseline
(s) if hRating(s) ? hRatingAve(domain)
0 if hRating(s) < hRatingAve(domain)
(4)
HelpfulSum: To isolate the contribution of review helpfulness, the second summarizer only uses help-
fulness related features in its sentence scoring function F
HelpfulSum
. The features are rHelpSum ? the
contribution of a sentence to the overall helpfulness of its corresponding review, sHelpSum ? the average
topic weight in a sentence for predicting the overall helpfulness of the review (Formula 1 and 2), plus
hRating for filtering. Note that there is no overlap between features used in the baseline and Helpful-
Sum, as we wonder if the helpfulness information alone is good enough for discovering salient review
sentences.
F
HelpfulSum
(s) =
{
rHelpSum(s) + sHelpSum(s) if hRating(s) ? hRatingAve(domain)
0 if hRating(s) < hRatingAve(domain)
(5)
5 Evaluation
For evaluation, we will first present our human evaluation user study and then present the automated
evaluation result based on human summaries collected from the user study.
3
In our pilot study, we experimented with various hyper-parameter settings, and trained the model with 100 sampling
iterations in both the Estimation and the Maximization steps. As we found the best results are more likely to be achieved when
? = 0.5, ? = 0.1, we use this setting to train the sLDA model in our summarization experiment.
4
In practice, this means that we need to (re)train the topic model after given the summarization test set.
1988
5.1 Human evaluation
The goal of our human evaluation is to compare the effectiveness of 1) using a traditional content selec-
tion method (MEAD+LexRank), 2) using the traditional method enhanced by review-level helpfulness
filtering (HelpfulFilter), and 3) using sentence helpfulness features estimated by sLDA plus review-level
helpfulness filtering (HelpfulSum) for building an extractive multi-document summarization system for
online reviews. Therefore, we use a within-subject design in our user study for each review domain,
considering the summarizer as the main effect on human evaluation results.
The user study is carried out in the form of online surveys (one survey per domain) hosted by Quadrics.
In total, 36 valid users participated in our online-surveys.
5
We randomly assigned 18 of them to the
camera reviews, and the rest 18 to the movie reviews.
5.1.1 Experimental procedures
Each online survey contains three summarization sets. The human evaluation on each one is taken in
three steps:
Step 1: We first require users to perform manual summarization, by selecting 10 sentences from the
input reviews (displayed in random order for each visit). This ensures that users are familiar with the
input text so that they can have fair judgement on machine-generated results. To help users select the
sentences, we provide an introductory scenario at the beginning of the survey to illustrate the potential
application in accordance with the domain (e.g., Figure 1).
Figure 1: Scenario for summarizing camera reviews
Figure 2: Content evaluation
Step 2: We then ask users to perform pairwise comparison on summaries generated by the three sys-
tems. The three pairs are generated in random order; and the left-or-right display position (in Figure 3)
of the two summaries in each pair is also randomly selected. Here we use the same 5-level preference
ratings used in (Lerman et al., 2009), and translate them into integers from -2 to 2 in our result analysis.
Step 3: Finally, we ask users to evaluate the three summaries in isolation regarding the summary quality
in three content-related aspects: recall, precision and accuracy (top, middle and bottom in Figure 2,
respectively), which were used in (Carenini et al., 2006). In this content evaluation, the three summaries
are randomly visited and the users rate the proposed statements (one for each aspect) on a 5-point scale.
5.1.2 Results
Pairwise comparison. We use a mixed linear model to analyze user preference over the three summary
pairs separately, in which ?summarizer? is a between-subject factor, ?review item? is the repeated factor,
and ?user? is a random effect. Results are summarized in Table 1. (Positive preference ratings on ?A
over B? means A is preferred over B; negative ratings means B is preferred over A.) As we can see,
HelpfulSum is the best: it is consistently preferred over the other two summarizers across domains and
the preference is significant throughout conditions except when compared with HelpfulFilter on movie
reviews. HelpfulFilter is significantly preferred over the baseline (MEAD+LexRank) for movie reviews,
while it does not outperform the baseline on camera reviews. A further look at the compression rate
(cRate) of the three systems (Table 2) shows that on average HelpfulFilter generates shortest summaries
5
All participants are older than eighteen, recruited via university mailing lists, on-campus flyers as well as social networks
online. While we also considered educational peer reviews as a third domain, about half of the participants dropped out in the
middle of the survey. Thus we only consider the two e-commerce domains in this paper.
1989
Figure 3: Example of pairwise comparison for summarizing camera reviews (left:HelpfulSum, right: the
baseline).
among the three summarizers on camera reviews
6
, which makes it naturally harder for HelpfulFilter to
beat the other two (Napoles et al., 2011).
Pair Domain Est. Mean Sig.
HelpfulFilter Camera -.602 .001
over MEAD+LexRank Movie .621 .000
HelpfulSum Camera .424 .011
over MEAD+LexRank Movie .601 .000
HelpfulSum Camera 1.18 .000
over HelpfulFilter Movie .160 .310
Table 1: Mixed-model analysis of user preference ratings
in pairwise comparison across domains. Confidence inter-
val = 95%. The preference rating is ranged from -2 to 2.
Summarizer Camera Movie
MEAD+LexRank 6.07% 2.64%
HelpfulFilter 3.25% 2.39%
HelpfulSum 5.94% 2.69%
Human (Ave.) 6.11% 2.94%
Table 2: Compression rate of the three
systems across domains.
Content evaluation. We summarize the average quality ratings (Figure 2) received by each summarizer
across review items and users for each review domain in Table 3. We carry out paired T-tests for every
pair of summarizers on each quality metric. While no significant difference is found among the three
summarizers on any quality metric for movie reviews, there are differences for camera reviews. In terms
of both accuracy and recall, HelpfulSum is significantly better than HelpfulFilter (p=.008 for accuracy,
p=.034 for recall) and the baseline is significantly better than HelpfulFilter (p=.005 for accuracy, p=.005
for recall), but there is no difference between HelpfulSum and the baseline. For precision, no significant
6
While we limit the summarization output to be 200 words in MEAD, as the content selection is at the sentence level, the
summaries can have different number of words in practice. Considering that word-based MMR controls the redundancy in the
selected summary sentences (? = 0.5 as suggested), there might be enough content to select using F
HelpfulFilter
.
1990
difference is observed in either domain.
Summarizer Camera Movie
Metric Precision Recall Accuracy Precision Recall Accuracy
MEAD+LexRank 2.63 3.24 3.57 2.50 2.59 2.93
HelpfulFilter 2.78 2.74 3.11 2.44 2.61 2.96
HelpfulSum 2.41 3.19 3.69 2.52 2.67 3.02
Table 3: Human ratings for content evaluation. The best result on each metric is bolded for every review
domain (the higher the better).
With respect to pairwise evaluation, content evaluation yields consistent results on camera reviews
between HelpfulFilter vs. the baseline and HelpfulSum vs. HelpfulFilter. However, only pairwise com-
parison (preference ratings) shows significant difference between HelpfulSum vs. the baseline and the
difference in the summarizers? performance on movie reviews. This confirms that pairwise comparison
is more suitable than content evaluation for human evaluation (Lerman et al., 2009).
5.2 Automated evaluation based on ROUGE metrics
Although human evaluation is generally preferred over automated metrics for summarization evaluation,
we report our automated evaluation results based on ROUGE scores (Lin, 2004) using references col-
lected from the user study. For each summarization test set, we have 3 machine generated summaries
and 18 human summaries. We compute the ROUGE scores in a leave-1-out fashion: for each machine
generated summary, we compare it against 17 out of the 18 human summaries and report the score aver-
age across the 17 runs; for each human summary, we compute the score using the other 17 as references,
and report the average human summarization performance.
Evaluation results are summarized in Table 4 and Table 5, in which we report the F-measure for R-
1 (unigram), R-2 (bigram) and R-SU4 (skip-bigram with maximum gap length of 4)
7
, following the
convention in the summarization community. Here we observe slightly different results with respect
to human evaluation: for camera reviews, no significant result is observed, while HelpfulSum achieves
the best R-1 score and HelpfulFilter works best regarding R-2 and R-SU4. In both cases the baseline
is never the best. For movie reviews, HelpfulSum significantly outperforms the other summarizers on
all ROUGE measurements, and the improvement is over 100% on R-2 and R-SU4, almost the same as
human does. This is consistent with the result of pairwise comparison in that HelpfulSum works better
than both HelpfulFilter and the baseline on movie reviews.
Summarizer R-1 R-2 R-SU4
MEAD+LexRank .333 .117 .110
HelpfulFilter .346 .121 .111
HelpfulSum .350 .110 .101
Human .360 .138 .126
Table 4: ROUGE evaluation on camera reviews
Summarizer R-1 R-2 R-SU4
MEAD+LexRank .281 .044 .047
HelpfulFilter .273 .040 .041
HelpfulSum .325 .095 .090
Human .339 .093 .093
Table 5: ROUGE evaluation on movie reviews
6 Human summary analysis
To get a comprehensive understanding of the challenges in extractive review summarization, we analyze
the agreement in human summaries collected in our user study at different levels of granularity, regarding
heuristics that are widely used in existing extractive summarizers.
Average word/sentence counts. Figure 4 illustrates the trend of average number of words and sentences
shared by different number of users across review items for each domain. As it shows, no sentence is
7
Because ROUGE requires all summaries to have equal length (word counts), we only consider the first 100 words in every
summary.
1991
agreed by over 10 users, which suggests that it is hard to make humans agree on the informativeness of
review sentences.
Figure 4: Average number of words (w) and sentences (s) in agreed
human summaries
Figure 5: Average probability of
words used in human summaries
Word frequency. We then compute the average probability of word (in the input) used by different
number of human summarizers to see if the word frequency pattern found in news articles (words that
human summarizers agreed to use in their summaries are of high frequency in the input text (Nenkova
and Vanderwende, 2005)) holds for online reviews. Figure 5 confirms this. However, the average word
probability is below 0.01 in those shared by 14 out of 18 summaries
8
; the flatness of the curve seems to
suggest that word frequency alone is not enough for capturing the salient information in input reviews.
KL-divergence. Another widely used heuristic in multi-document summarization is minimizing the
distance of unigram distribution between the summary and the input text (Lin et al., 2006). We wonder
if this applies to online review summarization. For each testing set, we group review sentences by the
number of users who selected them in their summaries, and compute the KL-divergence (KLD) between
each sentence group and the input. The average KL-divergence of each group across review items are
visualized in Figure 6, showing that this intuition is incorrect for our review domains. Actually, the
pattern is quite the opposite, especially when the number of users who share the sentences is less than 8.
Thus traditional methods that aim to minimize KL-divergence might not work well for online reviews.
Figure 6: Average KL-Divergence between
input and sentences used in human summaries
Figure 7: Average BigramSum of sentences
used in human summaries
Bigram coverage. Recent studies proposed a simple but effective criteria for extractive summarization
based on bigram coverage (Nenkova and Vanderwende, 2005; Gillick and Favre, 2009). The coverage
of a given bigram in a summary is defined as the number of input documents the bigram appears in, and
presumably good summaries should have larger sum of bigram coverage (BigramSum). However, as
shown in Figure 7, this criteria might not work well in our case either. For instance, the BigramSum of
the sentences that are shared by 3 human judges is smaller than those shared by 1 or 2 judges.
8
The average probability of words used by all 4 human summarizers are 0.01 across the 30 DUC03 sets (Nenkova and
Vanderwende, 2005).
1992
7 Conclusion and future work
We propose a novel unsupervised extractive approach for summarizing online reviews by exploiting re-
view helpfulness ratings for content selection. We demonstrate that the helpfulness metadata can not
only be directly used for review-level filtering, but also be used as the supervision of sLDA for sentence
scoring. This approach leverages the existing metadata of online reviews, requiring no annotation and
generalizable to multiple review domains. Our experiment based on the MEAD framework shows that
HelpfulFilter is preferred over the baseline (MEAD+LexRank) on camera reviews in human evaluation.
HelpfulSum, which utilizes review helpfulness at both the review and sentence level, significantly out-
performs the baseline in both human and automated evaluation. Our analysis on the collected human
summaries reveals the limitation of traditional summarization heuristics (proposed for news articles) for
being used in review domains.
In this study, we consider the ground truth of review helpfulness as the percentage of helpful votes
over all votes, where the helpfulness votes could be biased in various ways (Danescu-Niculescu-Mizil et
al., 2009). In the future, we would like to explore more sophisticated models of review helpfulness to
eliminate such biases, or even automatic review helpfulness predictions based on just review text. We also
would like to build a fully automated summarizer by replacing the review helpfulness gold-standard with
automated predictions as the filtering criteria. Given the collected human summaries, we will experiment
with different feature combinations for sentence scoring and we will compare our helpfulness features
with other content features as well. Finally, we want to further analyze the impact of the number of
human judges on our automated evaluation results based on ROUGE scores.
Acknowledgements
This research is supported by the Institute of Education Sciences, U.S. Department of Education, through
Grant R305A120370 to the University of Pittsburgh. The opinions expressed are those of the authors and
do not necessarily represent the views of the Institute or the U.S. Department of Education. We thank
Dr. Jingtao Wang and Dr. Christian Schunn for giving us suggestions on the user study design.
References
David M Blei and Jon D McAuliffe. 2010. Supervised topic models. arXiv preprint arXiv:1003.0783.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research
and development in information retrieval, pages 335?336. ACM.
Giuseppe Carenini, Raymond T Ng, and Adam Pauls. 2006. Multi-document summarization of evaluative text. In
In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguis-
tics.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinber g, and Lillian Lee. 2009. How opinions are
received by online communities: A case study on Amazon .com helpfulness votes. In Proceedings of the 18th
International Conference on World Wide Web, pages 141?150.
G?unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summa-
rization. J. Artif. Intell. Res.(JAIR), 22(1):457?479.
Anindya Ghose and Panagiotis G Ipeirotis. 2011. Estimating the helpfulness and economic impact of product
reviews: Mining text and reviewer characteristics. IEEE Transactions on Knowledge and Data Engineering,
23(10):1498?1512.
Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Langauge Processing, pages 10?18. Association for Computational
Linguistics.
1993
Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 362?370. Association for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 168?177. ACM.
Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of the international conference on
Web search and web data mining, pages 219?230.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generating comparative summaries of contradictory opinions in
text. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 385?394.
ACM.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,
pages 423?430. Association for Computational Linguistics.
Kevin Lerman and Ryan McDonald. 2009. Contrastive summarization: an experiment with consumer reviews.
In Proceedings of human language technologies: The 2009 annual conference of the North American chapter
of the association for computational linguistics, companion volume: Short papers, pages 113?116. Association
for Computational Linguistics.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan McDonald. 2009. Sentiment summarization: Evaluating and
learning user preferences. In Proceedings of the 12th Conference of the European Chapter of the Association
for Computational Linguistics, pages 514?522.
Jure Leskovec13, Natasa Milic-Frayling, and Marko Grobelnik. 2005. Impact of linguistic analysis on the seman-
tic graph coverage and learning of document extracts.
Jiwei Li and Sujian Li. 2012. A novel feature-based bayesian model for query focused multi-document summa-
rization. arXiv preprint arXiv:1212.2006.
Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational linguistics, volume 1 of COLING ?00, pages 495?501.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2006. An information-theoretic approach to
automatic evaluation of summaries. In Proceedings of the main conference on Human Language Technology
Conference of the North American Chapter of the Association of Computational Linguistics, pages 463?470.
Association for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?81.
Jingjing Liu, Yunbo Cao, Chin yew Lin, Yalou Huang, and Ming zhou. 2007. Low-quality product review
detection in opinion summarization. In Proceedings of the 2007 Conference on Empirical Methods in Natural
Language Processing.
Yang Liu, Xiangji Huang, Aijun An, and Xiaohui Yu. 2008. Modeling and predicting the helpfulness of online
reviews. In Data Mining, 2008. ICDM?08. Eighth IEEE International Conference on, pages 443?452. IEEE.
Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas, and Livia Polanyi. 2010. Exploiting social context for review
quality prediction. In Proceedings of the 19th international conference on World wide web, pages 691?700.
Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.
Courtney Napoles, Benjamin Van Durme, and Chris Callison-Burch. 2011. Evaluating sentence compression:
Pitfalls and suggested remedies. In Proceedings of the Workshop on Monolingual Text-To-Text Generation,
pages 91?97. Association for Computational Linguistics.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda Celebi, Stanko Dimitrov, Elliott
Drabek, Ali Hakim, Wai Lam, Danyu Liu, et al. 2004. Mead-a platform for multidocument multilingual text
summarization. Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC
2004).
1994
Martin Riedl and Chris Biemann. 2012. How text segmentation algorithms gain from topic models. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 553?557. Association for Computational Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the
15th ACM international conference on Information and knowledge management, pages 43?50. ACM.
1995
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91?102,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement
Detector for an Uncertainty-Adaptive Spoken Dialogue System
Kate Forbes-Riley and Diane Litman and Heather Friedberg and Joanna Drummond?
University of Pittsburgh
Pittsburgh, PA 15260, USA
forbesk@pitt.edu, litman@pitt.edu, haf13@pitt.edu
Abstract
We present a model for detecting user dis-
engagement during spoken dialogue interac-
tions. Intrinsic evaluation of our model (i.e.,
with respect to a gold standard) yields results
on par with prior work. However, since our
goal is immediate implementation in a sys-
tem that already detects and adapts to user un-
certainty, we go further than prior work and
present an extrinsic evaluation of our model
(i.e., with respect to the real-world task). Cor-
relation analyses show crucially that our au-
tomatic disengagement labels correlate with
system performance in the same way as the
gold standard (manual) labels, while regres-
sion analyses show that detecting user disen-
gagement adds value over and above detecting
only user uncertainty when modeling perfor-
mance. Our results suggest that automatically
detecting and adapting to user disengagement
has the potential to significantly improve per-
formance even in the presence of noise, when
compared with only adapting to one affective
state or ignoring affect entirely.
1 Introduction
Spoken dialogue systems that can detect and adapt
to user affect1 are fast becoming reality (Schuller
et al, 2009b; Batliner et al, 2008; Prendinger and
Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee
?Now at Univ. Toronto: jdrummond@cs.toronto.edu
1We use affect for emotions and attitudes that affect how
users communicate. Other speech researchers also combine
concepts of emotion, arousal, and attitudes where emotion is
not full-blown (Cowie and Cornelius, 2003).
and Narayanan, 2005; Shafran et al, 2003). The
benefits are clear: affect-adaptive systems have been
shown to increase task success (Forbes-Riley and
Litman, 2011a; D?Mello et al, 2010; Wang et al,
2008) or improve other system performance met-
rics such as user satisfaction (Liu and Picard, 2005;
Klein et al, 2002). However, to date most affec-
tive systems researchers have focused either only on
affect detection, or only on detecting and adapting
to a single affective state. The next step is thus to
develop and evaluate spoken dialogue systems that
detect and respond to multiple affective states.
We previously showed that detecting and re-
sponding to user uncertainty during spoken dialogue
computer tutoring significantly improves task suc-
cess (Forbes-Riley and Litman, 2011a). We are
now taking the next step: incorporating automatic
detection and adaptation to user disengagement as
well, with the goal of further improving task suc-
cess. We targeted user uncertainty and disengage-
ment because manual annotation showed them to be
the two most common user affective states in our
system and both are negatively correlated with task
success (Litman and Forbes-Riley, 2009; Forbes-
Riley and Litman, 2011b). Thus, we hypothesize
that providing appropriate responses to these states
would reduce their frequency, consequently improv-
ing task success. Although we address these user
states in the tutoring domain, spoken dialogue re-
searchers across domains and applications have in-
vestigated the automatic detection of both user un-
certainty (e.g. (Drummond and Litman, 2011; Pon-
Barry and Shieber, 2011; Paek and Ju, 2008; Alwan
et al, 2007)) and user disengagement (e.g., (Schuller
91
et al, 2010; Wang and Hirschberg, 2011; Schuller
et al, 2009a)), to improve system performance.
The detection of user disengagement in particular
has received substantial attention in recent years,
due to growing awareness of its potential for neg-
atively impacting commercial applications (Wang
and Hirschberg, 2011; Schuller et al, 2009a).
In this paper we present a model for automati-
cally detecting user disengagement during spoken
dialogue interactions. Intrinsic evaluation of our
model yields results on par with those of prior work.
However, we argue that while intrinsic evaluations
are necessary, they aren?t sufficient when immedi-
ate implementation is the goal, because there is no a
priori way to know when the model?s performance is
acceptable to use in a working system. This problem
is particularly relevant to affect detection because it
is such a difficult task, where no one achieves near-
perfect results. We argue that for such tasks some
extrinsic evaluation is also necessary, to show that
the automatic labels are useful and/or are a reason-
able substitute for a gold standard before undertak-
ing a labor-intensive and time-consuming evaluation
with real users. Here we use correlational analy-
ses to show that our automatic disengagement la-
bels are related to system performance in the same
way as the gold standard (manual) labels. We fur-
ther show through regression analyses that detecting
user disengagement adds value over and above de-
tecting only user uncertainty when modeling perfor-
mance. These results provide strong evidence that
enhancing a spoken dialogue system to detect and
adapt to multiple affective states (specifically, user
disengagement and uncertainty) has the potential to
significantly improve performance even in the pres-
ence of noise due to automatic detection, when com-
pared with only adapting to one affective state or ig-
noring affect entirely.
2 Related Work
Our focus in this paper is on first using machine
learning to develop a detector of user disengagement
for spoken dialogue systems, and then evaluating its
usefulness as fully as possible prior to its implemen-
tation and deployment with real users.
Disengaged users are highly undesirable in
human-computer interaction because they increase
the potential for user dissatisfaction and task fail-
ure; thus over the past decade there has already been
substantial prior work focused on detecting user dis-
engagement and the closely related states of bore-
dom, motivation and lack of interest (e.g., (Schuller
et al, 2010; Wang and Hirschberg, 2011; Jeon et
al., 2010; Schuller et al, 2009a; Bohus and Horvitz,
2009; Martalo et al, 2008; Porayska-Pomsta et al,
2008; Kapoor and Picard, 2005; Sidner and Lee,
2003; Forbes-Riley and Litman, 2011b)).
Within this work, specific affect definitions vary
slightly with the intention of being coherent within
the application and domain and being relevant to the
specific adaptation goal (Martalo et al, 2008). How-
ever, affective systems researchers generally agree
that disengaged users show little involvement in the
interaction, and often display facial, gestural and lin-
guistic signals such as gaze avoidance, finger tap-
ping, humming, sarcasm, et cetera.
The features used to detect disengagement also
vary depending on system domain and applica-
tion. For example, Sidner & Lee (2003) are in-
terested in modeling more natural and collabora-
tive human-robot interactions during basic conver-
sations. They define an algorithm for the engage-
ment process that involves appropriate eye gaze and
turn-taking. Martalo et al (2008) study how user
engagement influences dialogue patterns during in-
teractions with an embodied agent that gives ad-
vice about healthy dieting. They model engage-
ment using manually coded dialogue acts based on
the SWBDL-DAMSL scheme (Stolcke et al, 2000).
Bohus and Horvitz (2009) study systems that attract
and engage users for dynamic, multi-party dialogues
in open-world settings. They model user intentions
to engage the system with cues from facial sensors
and the dialogue. Within recent spoken dialogue
research, acoustic-prosodic, lexical and contextual
features have been found to be effective detectors
of disengagement (Schuller et al, 2010; Wang and
Hirschberg, 2011; Jeon et al, 2010); we will briefly
compare our own results with these in Section 5.
While all of the above-mentioned research has
presented intrinsic evaluations of their disengage-
ment modeling efforts that indicate a reasonable de-
gree of accuracy as compared to a gold standard
(e.g., manual coding), only a few have yet demon-
strated that the model?s detected values are useful
92
in practice and/or are a reasonable substitute for
the gold standard with respect to some practical
objective (e.g., a relationship to performance). In
particular, two studies (Bohus and Horvitz, 2009;
Schuller et al, 2009a) have gone directly from in-
trinsic evaluation of (dis)engagement models to per-
forming user studies with the implemented model,
thereby bypassing other less expensive and less
labor-intensive means of extrinsic evaluation to
quantify their model?s usefulness?and potentially in-
dicate its need to be further improved?before de-
ployment with real users. Neither study reports sta-
tistically significant improvements in system perfor-
mance as a result of detecting user (dis)engagement.
Finally, while substantial spoken dialogue and af-
fective systems research has shown that users dis-
play a range of affective states while interacting with
a system (e.g. (Schuller et al, 2009b; Conati and
Maclaren, 2009; Batliner et al, 2008; Devillers and
Vidrascu, 2006; Lee and Narayanan, 2005; Shafran
et al, 2003; Ang et al, 2002)), to date only a few af-
fective systems have been built that detect and adapt
to multiple user affective states (e.g., (D?Mello et al,
2010; Aist et al, 2002; Tsukahara andWard, 2001)),
and most of these have been deployed with cru-
cial natural language processing components ?wiz-
arded? by a hidden human agent (e.g., who performs
speech recognition or affect annotation on the user
turns); moreover, none have yet shown significant
improvements in system performance as a result of
adapting to multiple user affective states.
3 ITSPOKE: Spoken Dialogue Tutor
We develop and evaluate our disengagement detec-
tor using a corpus of spoken dialogues from a 2008
controlled experiment evaluating our uncertainty-
adaptive spoken dialogue tutoring system, IT-
SPOKE (Intelligent Tutoring SPOKEn dialog sys-
tem) (Forbes-Riley and Litman, 2011a).2
ITSPOKE tutors 5 Newtonian physics problems
(one per dialogue), using a Tutor Question - Stu-
dent Answer - Tutor Response format. After
each tutor question, the student speech is digi-
tized from head-mounted microphone input and sent
2ITSPOKE is a speech-enhanced and otherwise modified
version of the Why2-Atlas text-based qualitative physics tu-
tor (VanLehn et al, 2002).
to the Sphinx2 recognizer, which yields an auto-
matic transcript (Huang et al, 1993). This an-
swer?s (in)correctness is then automatically classi-
fied based on this transcript, using the TuTalk se-
mantic analyzer (Jordan et al, 2007), and the an-
swer?s (un)certainty is automatically classified by
inputting features of the speech signal, the automatic
transcript, and the dialogue context into a logistic
regression model. We will discuss these features
further in Section 5. All natural language process-
ing components were trained using prior ITSPOKE
corpora. The appropriate tutor response is deter-
mined based on the answer?s automatically labeled
(in)correctness and (un)certainty and then sent to the
Cepstral text-to-speech system3, whose audio output
is played through the student headphones and is also
displayed on a web-based interface.
The experimental procedure was as follows: col-
lege students with no college-level physics (1) read
a short physics text, (2) took a pretest, (3) worked
5 ?training? problems with ITSPOKE, where each
user received a varying level of uncertainty adapta-
tion based on condition, (4) took a user satisfaction
survey, (5) took a posttest isomorphic to the pretest,
and (6) worked a ?test? problem with ITSPOKE that
was isomorphic to the 5th training problem, where
no user received any uncertainty adaptation.
The resulting corpus contains 432 dialogues (6
per student) and 7216 turns from 72 students, 47
female and 25 male. All turns are used in the dis-
engagement detection experiments described next.
However, only the training problem dialogues (360,
5 per student, 6044 student turns) are used for the
performance analyses in Sections 6-7, because the
final test problem was given after the instruments
measuring performance (survey and posttest).
Our survey and tests are the same as those used in
multiple prior ITSPOKE experiments (c.f., (Forbes-
Riley and Litman, 2011a)). The pretest and posttest
each contain 26 multiple choice questions querying
knowledge of the topics covered in the dialogues.
Average pretest and posttest scores in the corpus
were 51.0% and 73.1% (out of 100%) with stan-
dard deviations of 14.5% and 13.8%, respectively.
The user satisfaction survey contains 16 statements
rated on a 5-point Likert scale. Average total sur-
3an outgrowth of Festival (Black and Taylor, 1997).
93
vey score was 60.9 (out of 80), with a standard de-
viation of 8.5. While the statements themselves are
listed elsewhere (Forbes-Riley and Litman, 2009),
9 statements concern the tutoring domain (e.g., The
tutor was effective/precise/useful), 7 of which were
taken from (Baylor et al, 2003) and 2 of which
were created for our system. 3 statements concern
user uncertainty levels and were created for our sys-
tem. 4 statements concern the spoken dialogue in-
teraction (e.g., It was easy to understand the tutor?s
speech) and were taken from (Walker et al, 2002).
Our survey has also been incorporated into other re-
cent work exploring user satisfaction in spoken dia-
logue computer tutors (Dzikovska et al, 2011). In
Section 6 we discuss how user scores on these in-
struments are used to measure system performance.
See (Forbes-Riley and Litman, 2011a) for further
details of ITSPOKE and the 2008 experiment.
Following the experiment, the entire corpus
was manually labeled for (in)correctness (cor-
rect, incorrect), (un)certainty (CER, UNC) and
(dis)engagement (ENG, DISE) by one trained an-
notator. Table 1 shows the distribution of the la-
beled turns in the 2008 ITSPOKE corpus. In prior
ITSPOKE corpora, our annotator displayed interan-
notator agreement of 0.85 and 0.62 Kappa on cor-
rectness and uncertainty, respectively (Forbes-Riley
and Litman, 2011a). For the disengagement label,
a reliability analysis was performed over several an-
notation rounds on subsets of the 2008 ITSPOKE
corpus by this and a second trained annotator, yield-
ing 0.55 Kappa (this analysis is described in detail
elsewhere (Forbes-Riley et al, 2011)). Our Kap-
pas indicate that user uncertainty and disengage-
ment can both be annotated with moderate reliabil-
ity in our dataset, on par with prior emotion anno-
tation work (c.f., (Pon-Barry and Shieber, 2011)).
Note however that the best way to label users? in-
ternal affective state(s) is still an open question.
Many system researchers (including ourselves) rely
on trained labelers (e.g., (Pon-Barry et al, 2006;
Porayska-Pomsta et al, 2008)) while others use self-
reports (e.g., (Conati and Maclaren, 2009; Gratch et
al., 2009; McQuiggan et al, 2008)). Both meth-
ods are problematic; for example both can be ren-
dered inaccurate when users mask their true feel-
ings. Two studies that have compared self-reports,
peer labelers, trained labelers, and combinations of
labelers (Afzal and Robinson, 2011; D?Mello et al,
2008) both illustrate the common finding that hu-
man annotators display low to moderate interannota-
tor reliability for affect annotation, and both studies
show that trained labelers yield the highest reliabil-
ity on this task. Despite the lack of high interan-
notator reliability, responding to affect detected by
trained human labels has still been shown to improve
system performance (see Section 1).
Table 1: 2008 ITSPOKE Corpus Description (N=7216)
Turn Label Total Percent
Disengaged 1170 16.21%
Correct 5330 73.86%
Uncertain 1483 20.55%
Uncertain+Disengaged 373 5.17%
4 Automatically Detecting User
Disengagement (DISE) in ITSPOKE
As noted in Section 1, we have developed a user dis-
engagement detector to incorporate into our existing
uncertainty-adaptive spoken dialogue system. The
result will be a state of the art system that adapts to
multiple affective states during the dialogue.
4.1 Binary DISE Label
Our disengagement annotation scheme (Forbes-
Riley et al, 2011) was derived from empirical ob-
servations in our data but draws on prior work,
including work mentioned in Section 2, appraisal
theory-based emotion models (e.g., Conati and Ma-
claren (2009))4, and prior approaches to annotating
disengagement or related states in tutoring (Lehman
et al, 2008; Porayska-Pomsta et al, 2008).
Briefly, our overall Disengagement label (DISE)
is used for turns expressing moderate to strong dis-
engagement towards the interaction, i.e., responses
given without much effort or without caring about
appropriateness. Responses might also be accompa-
nied by signs of inattention, boredom, or irritation.
Clear examples include answers spoken quickly in
leaden monotone, with sarcastic or playful tones,
or with off-task sounds such as rhythmic tapping or
4Appraisal theorists distinguish emotional behaviors from
their underlying causes, arguing that emotions result from an
evaluation of a context.
94
electronics usage.5 Note that our DISE label is de-
fined independently of the tutoring domain and thus
should generalize across spoken dialogue systems.
Figure 1 illustrates the DISE, (in)correctness, and
(un)certainty labels across 3 tutor/student turn pairs.
U1 is labeled DISE and UNC because the student
gave up immediately and with irritation when too
much prior knowledge was required. U2 is labeled
DISE and UNC because the student avoided giv-
ing a specific numerical value, offering instead a
vague (and obviously incorrect) answer. U3 is la-
beled DISE and CER because the student sang the
correct answer, indicating a lack of interest in the
larger purpose of the material being discussed.6
T1: What is the definition of Newton?s Second Law?
U1: I have no idea <sigh>. (DISE, incorrect, UNC)
. . .
T2: What?s the numerical value of the man?s accelera-
tion? Please specify the units too.
U2: The speed of the elevator. Meters per second. (DISE,
incorrect, UNC)
. . .
T3: What are the forces acting on the keys after the man
releases them?
U3: graaa-vi-tyyyyy <sings the answer> (DISE, cor-
rect, CER)
Figure 1: Corpus Example Illustrating the User Turn La-
bels ((Dis)Engagement, (In)Correctness, (Un)Certainty)
4.2 DISE Detection Method
Machine learning classification was done at the turn
level using WEKA software7 and 10-fold cross val-
idation. A J48 decision tree was chosen because of
its easily read output and the fact that previous ex-
periments with our data showed little variance be-
5Affective systems research has found total disengagement
rare in laboratory settings (Lehman et al, 2008; Martalo et al,
2008). As in that research, we equate the DISE label with no
or low engagement. Since total disengagement is common in
real-world unobserved human-computer interactions (deleting
unsatisfactory software being an extreme example) it remains
an open question as to how well laboratory findings generalize.
6Our original scheme distinguished six DISE subtypes
that trained annotators distinguished with a reliability of .43
Kappa (Forbes-Riley et al, 2011). However, pilot experiments
indicated that our models cannot accurately distinguish them,
thus our DISE detector focuses on the DISE label.
7http://www.cs.waikato.ac.nz/ml/weka/
tween different machine learning algorithms (Drum-
mond and Litman, 2011). We also use a cost matrix,
which heavily penalizes classifying a true DISE in-
stance as false, because our class distributions are
highly skewed (16.21% DISE turns) and the cost
matrix successfully mitigated the skew?s effect in
our prior work, where the uncertainty distribution is
also skewed (20.55% UNC turns) (Drummond and
Litman, 2011).
To train our DISE model, we first extracted the set
of speech and dialogue features shown in Figure 2
from the user turns in our corpus. As shown, the
acoustic-prosodic features represent duration, paus-
ing, pitch, and energy, and were normalized by the
first user turn, as well as totaled and averaged over
each dialogue. The lexical and dialogue features
consist of the current dialogue name (i.e., one of the
six physics problems) and turn number, the current
ITSPOKE question?s name (e.g.,T3 in Figure 1 has
a unique identifier) and depth in the discourse struc-
ture (e.g., an ITSPOKE remediation question after
an incorrect user answer would be at one greater
depth than the prior question), a word occurrence
vector for the automatically recognized text of the
user turn, an automatic (in)correctness label, and
lastly, the number of user turns since the last cor-
rect turn (?incorrect runs?). We also included two
user-based features, gender and pretest score.
? Acoustic-Prosodic Features
temporal features: turn duration, prior pause dura-
tion, turn-internal silence
fundamental frequency (f0) and energy (RMS) fea-
tures: maximum, minimum, mean, std. deviation
running totals and averages for all features
? Lexical and Dialogue Features
dialogue name and turn number
question name and question depth
ITSPOKE-recognized lexical items in turn
ITSPOKE-labeled turn (in)correctness
incorrect runs
? User Identifier Features:
gender and pretest score
Figure 2: Features Used to Detect Disengagement (DISE)
for each User Turn
95
Table 2: Results of 10-fold Cross-Validation Experiment with J48 Decision Tree Algorithm Detecting the Binary DISE
Label in the 2008 ITSPOKE Corpus (N=7216 user turns)
Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE
Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25
Majority Label 83.8% 41.9% 50.0% 45.6% ? 0.27
Note that although our feature set was drawn pri-
marily from our prior uncertainty detection exper-
iments (Forbes-Riley and Litman, 2011a; Drum-
mond and Litman, 2011), we have also experi-
mented with other features, including state-of-the-
art acoustic-prosodic features used in the last Inter-
speech Challenges (Schuller et al, 2010; Schuller et
al., 2009b) and made freely available in the openS-
MILE Toolkit (Florian et al, 2010). To date, how-
ever, these features have only decreased the cross-
validation performance of our models.8 While some
of our features are tutoring-specific, these have sim-
ilar counterparts in other applications (i.e., answer
(in)correctness corresponds to a more general no-
tion of ?response appropriateness? in other domains,
while pretest score corresponds to the general no-
tion of domain expertise). Moreover, all of our fea-
tures are fully automatic and available in real-time,
so that the model can be directly implemented and
deployed. To that end, we now describe the results
of our intrinsic and extrinsic evaluations of our DISE
model, aimed at determining whether it is ready to
be evaluated with real users.
5 Intrinsic Evaluation: Cross-Validation
Table 2 shows the averaged results of the cross-
validation with the J48 decision tree algorithm. In
addition to accuracy, we use Unweighted Aver-
age (UA) Precision9, Recall, and F-measure be-
cause they are the standard measures used to eval-
uate current affect recognition technology, particu-
larly for unbalanced two-class problems (Schuller
et al, 2009b). In addition, we use the cross corre-
lation (CC) measure and mean linear error (MLE)
because these metrics were used in recent work for
evaluating disengagement (level of interest) detec-
tors for the Interspeech 2010 challenge (Schuller et
8We also tried using our automatic UNC label as a feature in
our DISE model, but our results weren?t significantly improved.
9simply ((Precision(DISE) + Precision(ENG))/2)
al., 2010; Wang and Hirschberg, 2011; Jeon et al,
2010)).10 Note however that the Interspeech 2010
task differs from ours not only in the corpus and fea-
tures, but also in the learning task: they used regres-
sion to detect a continuous level of interest ranging
from 0 to 1, while we detect a binary class. Thus
comparison between our results and those are only
suggestive rather than conclusive.
As shown in Table 2, we also compare our results
with those of majority class (ENG) labeling of the
same turns. Since (7216-1170)/7216 user turns in
the corpus are engaged (recall Table 1), always se-
lecting the majority class (ENG) label for these turns
thus yields 83.8% accuracy (with 0% precision and
recall for DISE, and 83.8% precision and 100% re-
call for ENG). While our DISE model does not out-
perform majority class labeling with respect to ac-
curacy, this is not surprising given the steep skew
in class distribution, and our learned model signif-
icantly outperforms the baseline with respect to all
the other measures (p<.001).11
Our CC and MLE results are on par with the best
results from the state-of-the-art systems competing
in the 2010 Interspeech Challenge, where the task
was to detect level of interest. In particular, the win-
ner obtained a CC of 0.428 (higher numbers are bet-
ter) and an MLE of 0.146 (lower numbers are bet-
ter) (Jeon et al, 2010), while a subsequent study
yielded a CC of 0.480 and an MLE of 0.131 on
the same corpus (Wang and Hirschberg, 2011). Our
results are also on par with the best results of the
other prior research on detecting disengagement dis-
cussed in Section 2 that detects a small number of
disengagement classes and reports accuracy and/or
recall and precision. For example, (Martalo et al,
2008) report average precision of 75% and recall
10Pearson product-moment correlation coefficient (CC) is a
measure of the linear dependence that is widely used in regres-
sion settings. MLE is a regression performance measure for the
mean absolute error between an estimator and the true value.
11CC is undefined for majority class labeling.
96
of 74% (detecting three levels of disengagement),
while (Kapoor and Picard, 2005) report an accuracy
of 86% for detecting binary (dis)interest.
Our final DISE model was produced by running
the J48 algorithm over our entire corpus. The re-
sulting decision tree contains 141 nodes and 75
leaves. Inspection of the tree reveals that all of the
feature types in Figure 2 (acoustic-prosodic, lexi-
cal/dialogue, user identifier) are used as decision
nodes in the tree, although not all variations on these
types were used. The upper-level nodes of the tree
are usually considered to be more informative fea-
tures as compared to lower-level nodes, since they
are queried for more leaves. The upper level of
the DISE model consists entirely of temporal, lex-
ical, pitch and energy features as well as question
name and depth and incorrect runs, while features
such as gender, turn number, and dialogue name
appear only near the leaves, and pretest score and
turn (in)correctness don?t appear at all. The amount
of pausing prior to the start of the user turn is the
most important feature for determining disengage-
ment, with pauses shorter than a quarter second be-
ing labeled DISE, suggesting that fast answers are a
strong signal of disengagement in our system. Users
who answer quickly may do so without taking the
time to think it through; the more engaged user, in
contrast, takes more time to prepare an answer.
Three lexical items from the student turns, ?fric-
tion?, ?light?, and ?greater?, are the next most im-
portant features in the tree, suggesting that particular
concepts and question types can be typically associ-
ated with user disengagement in a system. For ex-
ample, open-ended system questions may lead users
to disengage due to frustration from not knowing
when their answer is complete. One common case
in ITSPOKE involves asking users to name all the
forces on an object; some users don?t know how
many to list, so they start listing random forces, such
as ?friction.? On the other hand, multiple choice
questions can also lead users to disengage; they be-
gin with a reasonable chance of being correct and
thus don?t take the time to think through their an-
swer. One common case in ITSPOKE involves ask-
ing users to determine which of two objects has the
greater or lesser force, acceleration, and velocity.
While our feature set is highly generalizable to
other domains, it is an empirical question as to
whether the feature values we found maximally ef-
fective for predicting disengagement also general-
ize to other domains. Intuition is often unreliable,
and it has been widely shown in affect prediction
that the answer can depend on domain, dataset, and
learning algorithm employed. Moreover, there are
many types of spoken dialogue systems with dif-
ferent styles and no single type can represent the
entire field. That said, it is also important to note
that there are lessons to be learned from the features
selected for one particular domain, in terms of the
take-home message for other domains. For example,
the fact that ?prior pause? is selected as a strong sig-
nal of disengagement in ITSPOKE dialogues may
indicate that the feature itself (regardless of its se-
lected value) could be transferred to different do-
mains, alone or in the demonstrated combinations
with the other selected features.
6 Extrinsic Evaluation: Correlation
Next we use extrinsic evaluation to confirm that our
final DISE model is both useful and a reasonable
substitute for our gold standard manual DISE la-
bels. With respect to showing the utility of detecting
DISE, we use a correlational analysis to show that
the gold standard (manual) DISE values are signif-
icantly predictive of two different measures of sys-
tem performance.12 With respect to showing the ad-
equacy of our current level of detection performance
for the learned DISE model, we demonstrate that af-
ter replacing the manual DISE labels with the au-
tomatic DISE labels when running our correlations,
the automatic labels are related to performance in
the same way as the gold standard labels.
Thus for both our automatically detected DISE la-
bels (auto) and our gold standard DISE labels (man-
ual), we first computed the total number of occur-
rences for each student, and then computed a bivari-
ate Pearson?s correlation between this total and two
different metrics of performance: learning gain (LG)
and user satisfaction (US). In the tutoring domain,
learning is the primary performance metric and as is
common in this domain we compute it as normal-
ized learning gain ((posttest score-pretest score)/(1-
12Spoken dialogue research has shown that redesigning a sys-
tem in light of such correlational analysis can indeed yield per-
formance improvements (Rotaru and Litman, 2009).
97
Table 3: Correlations between Disengagement and both Satisfaction and Learning in ITSPOKE Corpus (N=72 users)
Measure Mean (SD) User Satisfaction Learning Gain
R p R p
Total Manual DISE 12.3 (7.3) -0.25 0.031 -0.35 0.002
Total Automatic DISE 12.6 (7.4) -0.26 0.029 -0.31 0.009
pretest score)). In spoken dialogue systems, user sat-
isfaction is the primary performance metric and as
is common in this domain we compute it by totaling
over the user satisfaction survey scores.13
Table 3 shows first the mean and standard devia-
tion for the DISE label over all students, the Pear-
son?s Correlation coefficient (R) and its significance
(p). As shown, both our manual and automatic DISE
labels are significantly related to performance, re-
gardless of whether we measure it as user satisfac-
tion or learning gain.14 Moreover, in both cases the
correlations are nearly identical between the man-
ual and automatic labels. These results indicate that
the detected DISE values are a useful substitute for
the gold standard, and suggest that redesigning IT-
SPOKE to recognize and respond to DISE can sig-
nificantly improve system performance.
7 Extrinsic Evaluation: Affective State
Multiple Regression
Because we are adding our disengagement detector
to a spoken dialogue system that already detects and
adapts to user uncertainty, we argue that it is also
necessary to evaluate whether greater performance
benefits are likely to be obtained by adapting to a
second state. In other words, given how difficult it is
to effectively detect and adapt to one user affective
state, is performance likely to improve by detecting
and adapting to multiple affective states?
To answer this question, we performed a multi-
ple linear regression analysis aimed at quantifying
the relative usefulness of the automatically detected
13Identical results were obtained by using an average instead
of a total, and only slightly weaker results were obtained when
normalizing the DISE totals as the percentages of total turns.
14We previously found a related correlation between different
DISE and learning measures, during the analysis of our DISE
annotation scheme (Forbes-Riley and Litman, 2011b). In par-
ticular, we showed a significant partial correlation between the
percentage of manual DISE labels and posttest controlled for
pretest score.
disengagement and uncertainty labels when predict-
ing our system performance metrics. We ran four
stepwise linear regressions. The first regression pre-
dicted learning gain, and gave the model two possi-
ble inputs: the total number of automatic DISE la-
bels and UNC labels per user. We then ran the same
regression again, this time predicting user satisfac-
tion. For comparison, we ran the same two regres-
sions using the manual DISE and UNC labels.
As the trained regression models in Figure 3 show,
when predicting learning gain, selecting both auto-
matically detected affective state metrics as inputs
significantly increases the model?s predictive power
as compared to only selecting one.15 The (stan-
dardized) feature weights indicate relative predic-
tive power in accounting for the variance in learn-
ing gain. As shown, both automatic affect metrics
have the same weight in the final model. This re-
sult suggests that adapting to our automatically de-
tected disengagement and uncertainty labels can fur-
ther improve learning over and above adapting to un-
certainty alone. Although the final model?s predic-
tive power is low (R2=0.15), our interest here is only
in investigating whether the two affective states are
more useful in combination than in isolation for pre-
dicting performance. In similar types of stepwise re-
gressions on prior ITSPOKE corpora, we?ve shown
that more complete models of system performance
incorporating many predictors of learning (i.e. af-
fective states in conjunction with other dialogue fea-
tures) can yield R2 values of over .5 (Forbes-Riley
et al, 2008).16
15Using the stepwise method, Automatic DISE was the first
feature selected, and Automatic UNC the second. However,
note that a model consisting of only the Automatic UNC metric
also yields significantly worse predictive power than selecting
both affective state metrics. Further note that almost identical
models were produced using percentages rather than totals.
16R2 is the standard reported metric for linear regressions.
However, for consistency with Table 3, note that the two models
in Figure 3 yield R values of -.31 and -.38, respectively.
98
Learning Gain = -.31 * Total Automatic DISE (R2=.09, p=.009)
Learning Gain = -.24 * Total Automatic DISE - .24 * Total Automatic UNC (R2=.15, p=.004)
Figure 3: Performance Model?s Predictive Power Increases Significantly with Multiple Affective Features
Interestingly, for the regression models of learn-
ing gain that used manual affect metrics, only the
DISE metric was selected as an input. This indi-
cates that the automatic affective state labels are use-
ful in combination for predicting performance in a
way that is not reflected in their gold standard coun-
terparts. Detecting multiple affective states might
thus be one way to compensate for the noise that is
introduced in a fully-automated affective spoken di-
alogue system.
Similarly, only the DISE metric was selected
for inclusion in the regression model of user sat-
isfaction, regardless of whether manual or auto-
matic labels were used. A separate correlation
analysis showed that user uncertainty is not sig-
nificantly correlated with user satisfaction in our
system, though we previously found that multiple
uncertainty-related metrics do significantly correlate
with learning (Litman and Forbes-Riley, 2009).
8 Summary and Current Directions
In this paper we used extrinsic evaluations to pro-
vide evidence for the utility of a new system de-
sign involving the complex task of user affect de-
tection, prior to undertaking an expensive and time-
consuming evaluation of an affect-adaptive system
with real users. In particular, we first presented a
novel model for automatically detecting user disen-
gagement in spoken dialogue systems. We showed
through intrinsic evaluations (i.e., cross-validation
experiments using gold-standard labels) that the
model yields results on par with prior work. We
then showed crucially through novel extrinsic eval-
uation that the resulting automatically detected dis-
engagement labels correlate with two primary per-
formance metrics (user satisfaction and learning) in
the same way as gold standard (manual) labels. This
suggests that adapting to the automatic disengage-
ment labels has the potential to significantly improve
performance even in the presence of noise from the
automatic labeling. Finally, further extrinsic anal-
yses using multiple regression suggest that adapt-
ing to our automatic disengagement labels can im-
prove learning (though not user satisfaction) over
and above the improvement achieved by only adapt-
ing to automatically detected user uncertainty.
We have already developed and implemented an
adaptation for user disengagement in ITSPOKE.
The disengagement adaptation draws on empiri-
cal analyses of our data and effective responses
to user disengagement presented in prior work
(c.f., (Forbes-Riley and Litman, 2011b)), We are
currently evaluating our disengagement adaptation
in the ?ideal? environment of a Wizard of Oz exper-
iment, where user disengagement, uncertainty, and
correctness are labeled by a hidden human during
user interactions with ITSPOKE.
Based on the evaluations here, we believe our dis-
engagement model is ready for implementation in
ITSPOKE. We will then evaluate the resulting spo-
ken dialogue system for detecting and adapting to
multiple affective states in an upcoming controlled
experiment with real users.
Acknowledgments
This work is funded by NSF award 0914615. We
thank Scott Silliman for systems support.
References
S. Afzal and P. Robinson. 2011. Natural affect data:
Collection and annotation. In Sidney D?Mello and
Rafael Calvo, editors, Affect and Learning Technolo-
gies. Springer.
G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Pi-
card. 2002. Experimentally augmenting an intelli-
gent tutoring system with human-supplied capabili-
ties: Adding human-provided emotional scaffolding to
an automated reading tutor that listens. In Proc. In-
telligent Tutoring Systems Conference (ITS) Workshop
on Empirical Methods for Tutorial Dialogue Systems,
pages 16?28, San Sebastian, Spain.
A. Alwan, Y. Bai, M. Black, L. Caseyz, M. Gerosa,
M. Heritagez, M. Iseliy, M. Jonesz, A. Kazemzadeh,
S. Lee, S. Narayanan, P. Pricex, J. Tepperman, and
99
S. Wangy. 2007. A system for technology based as-
sessment of language and literacy in young children:
the role of multiple information sources. In Proceed-
ings of the 9th IEEE International Workshop on Multi-
media Signal Processing (MMSP), pages 26?30, Cha-
nia, Greece, October.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
J. H. L. Hansen and B. Pellom, editors, Proceedings
of the International Conference on Spoken Language
Processing (ICSLP), pages 2037?2039, Denver, USA.
A. Batliner, S. Steidl, C. Hacker, and E. Noth. 2008.
Private emotions vs. social interaction - a data-driven
approach towards analysing emotion in speech. User
Modeling and User-Adapted Interaction: The Journal
of Personalization Research, 18:175?206.
A. L. Baylor, J. Ryu, and E. Shen. 2003. The effect
of pedagogical agent voice and animation on learning,
motivation, and perceived persona. In Proceedings of
the ED-MEDIA Conference, Honolulu, Hawaii, June.
A. Black and P. Taylor. 1997. Festival speech synthe-
sis system: system documentation (1.1.1). The Centre
for Speech Technology Research, University of Edin-
burgh, http://www.cstr.ed.ac.uk/projects/festival/.
D. Bohus and E. Horvitz. 2009. Models for multiparty
engagement in open-world dialog. In Proceedings of
SIGdial, London, UK.
C. Conati and H. Maclaren. 2009. Empirically build-
ing and evaluating a probabilistic model of user af-
fect. User Modeling and User-Adapted Interaction,
19(3):267?303.
R. Cowie and R. R. Cornelius. 2003. Describing the
emotional states that are expressed in speech. Speech
Communication, 40(1-2):5?32.
L. Devillers and L. Vidrascu. 2006. Real-life emo-
tions detection with lexical and paralinguistic cues on
human-human call center dialogs. In Ninth Inter-
national Conference on Spoken Language Processing
(ICSLP, pages 801?804, Pittsburgh, PA, September.
S. D?Mello, S. Craig, A. Witherspoon, B. McDaniel, and
A. Graesser. 2008. Automatic detection of learner?s
affect from conversational cues. User Modeling and
User-Adapted Interaction: The Journal of Personal-
ization Research, 18:45?80.
S. D?Mello, B. Lehman, J. Sullins, R. Daigle, R. Combs,
K. Vogt, L. Perkins, and A. Graesser. 2010. A time
for emoting: When affect-sensitivity is and isn?t effec-
tive at promoting deep learning. In Intelligent Tutoring
Systems Conference, pages 245?254, Pittsburgh, PA,
USA, June.
J. Drummond and D. Litman. 2011. Examining the im-
pacts of dialogue content and system automation on
affect models in a spoken tutorial dialogue system.
In Proc. 12th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), pages
312?318, Portland, Oregon, June.
M. Dzikovska, J. Moore, N. Steinhauser, and G. Camp-
bell. 2011. Exploring user satisfaction in a tutorial
dialogue system. In Proc. 12th Annual Meeting of
the Special Interest Group on Discourse and Dialogue
(SIGDIAL), pages 162?172, Portland, Oregon, June.
E. Florian, M. Wollmer, and B. Schuller. 2010. The Mu-
nich versatile and fast open-source audio feature ex-
tractor. In Proc. ACM Multimedia (MM), pages 1459?
1462, Florence, Italy.
K. Forbes-Riley and D. Litman. 2009. A user modeling-
based performance analysis of a wizarded uncertainty-
adaptive dialogue system corpus. In Proc. Inter-
speech, Brighton, UK, September.
K. Forbes-Riley and D. Litman. 2011a. Benefits and
challenges of real-time uncertainty detection and adap-
tation in a spoken dialogue computer tutor. Speech
Communication, 53(9?10):1115?1136.
K. Forbes-Riley and D. Litman. 2011b. When does
disengagement correlate with learning in spoken dia-
log computer tutoring? In Proceedings 15th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Auckland, NZ, June.
K. Forbes-Riley, M. Rotaru, and D. Litman. 2008. The
relative impact of student affect on performance mod-
els in a spoken dialogue tutoring system. User Model-
ing and User-Adapted Interaction, 18(1-2):11?43.
K. Forbes-Riley, D. Litman, and H. Friedberg. 2011. An-
notating disengagement for spoken dialogue computer
tutoring. In Sidney D?Mello and Rafael Calvo, editors,
Affect and Learning Technologies. Springer.
Jonathan Gratch, Stacy Marsella, Ning Wang, and
Brooke Stankovic. 2009. Assessing the validity of
appraisal-based models of emotion. In Proceedings of
ACII, Amsterdam, Netherlands.
X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F.
Lee, and R. Rosenfeld. 1993. The SphinxII speech
recognition system: An Overview. Computer, Speech
and Language.
J. H. Jeon, R. Xia, and Y. Liu. 2010. Level of interest
sensing in spoken dialog using multi-level fusion of
acoustic and lexical evidence. In INTERSPEECH?10,
pages 2802?2805.
P. Jordan, B. Hall, M. Ringenberg, Y. Cui, and C.P. Rose.
2007. Tools for authoring a dialogue agent that par-
ticipates in learning studies. In Proc. Artificial Intelli-
gence in Education (AIED), pages 43?50.
A. Kapoor and R. W. Picard. 2005. Multimodal affect
recognition in learning environments. In 13th Annual
ACM International Conference on Multimedia, pages
677?682, Singapore.
100
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-
sults. Interacting with Computers, 14:119?140.
C. M. Lee and S. Narayanan. 2005. Towards detect-
ing emotions in spoken dialogs. IEEE Transactions
on Speech and Audio Processing, 13(2), March.
B. Lehman, M. Matthews, S. D?Mello, and N. Per-
son. 2008. What are you feeling? Investigating
student affective states during expert human tutoring
sessions. In Intelligent Tutoring Systems Conference
(ITS), pages 50?59, Montreal, Canada, June.
D. Litman and K. Forbes-Riley. 2009. Spoken tutorial
dialogue and the feeling of another?s knowing. In Pro-
ceedings 10th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), Lon-
don, UK, September.
K. Liu and R. W. Picard. 2005. Embedded empathy
in continuous, interactive health assessment. In CHI
Workshop on HCI Challenges in Health Assessment.
A. Martalo, N. Novielli, and F. de Rosis. 2008. Attitude
display in dialogue patterns. In Proc. AISB 2008 Sym-
posium on Affective Language in Human andMachine,
pages 1?8, Aberdeen, Scotland, April.
S. McQuiggan, B. Mott, and J. Lester. 2008. Model-
ing self-efficacy in intelligent tutoring systems: An in-
ductive approach. User Modeling and User-Adapted
Interaction (UMUAI), 18(1-2):81?123, February.
T. Paek and Y.-C. Ju. 2008. Accommodating explicit
user expressions of uncertainty in voice search or
something like that. In Proceedings of the 9th Annual
Conference of the International Speech Communica-
tion Association (INTERSPEECH 08), pages 1165?
1168, Brisbane, Australia, September.
H. Pon-Barry and S. Shieber. 2011. Recognizing uncer-
tainty in speech. EURASIP Journal on Advances in
Signal Processing.
H. Pon-Barry, K. Schultz, E. Owen Bratt, B. Clark, and
S. Peters. 2006. Responding to student uncertainty in
spoken tutorial dialogue systems. International Jour-
nal of Artificial Intelligence in Education, 16:171?194.
K. Porayska-Pomsta, M. Mavrikis, and H. Pain. 2008.
Diagnosing and acting on student affect: the tutor?s
perspective. User Modeling and User-Adapted In-
teraction: The Journal of Personalization Research,
18:125?173.
H. Prendinger and M. Ishizuka. 2005. The Empa-
thetic Companion: A character-based interface that ad-
dresses users? affective states. International Journal of
Applied Artificial Intelligence, 19(3):267?285.
M. Rotaru and D. Litman. 2009. Discourse structure and
performance analysis: Beyond the correlation. In Pro-
ceedings 10th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), Lon-
don, UK.
B. Schuller, R. Muller, F. Eyben, J. Gast, B. Hrnler,
M. Wollmer, G. Rigoll, A. Hthker, and H. Konosu.
2009a. Being bored? recognising natural interest by
extensive audiovisual integration for real-life applica-
tion. Image and Vision Computing Journal, Special
Issue on Visual and Multimodal Analysis of Human
Spontaneous Behavior, 27:1760?1774.
B. Schuller, S. Steidl, and A. Batliner. 2009b. The
Interspeech 2009 Emotion Challenge. In Proceed-
ings of the 10th Annual Conference of the Inter-
national Speech Communication Association (Inter-
speech), ISCA, Brighton, UK, September.
B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev-
illers, C. Muller, and S. Narayanan. 2010. The
Interspeech 2010 Paralinguistic Challenge. In Pro-
ceedings of the 11th Annual Conference of the In-
ternational Speech Communication Assocation (Inter-
speech), pages 2794?2797, Chiba, Japan, September.
I. Shafran, M. Riley, and M. Mohri. 2003. Voice signa-
tures. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 31?36, St. Thomas, US Virgin Islands.
C. Sidner and C. Lee. 2003. An architecture for engage-
ment in collaborative conversations between a robot
and a human. Technical Report TR2003-12, MERL.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3).
W. Tsukahara and N. Ward. 2001. Responding to subtle,
fleeting changes in the user?s internal state. In Pro-
ceedings of the SIG-CHI on Human factors in comput-
ing systems, pages 77?84, Seattle, WA. ACM.
K. VanLehn, P. W. Jordan, C. Rose?, D. Bhembe,
M. Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intl. Conf. on Intelligent Tutoring
Systems.
L. Vidrascu and L. Devillers. 2005. Detection of real-
life emotions in dialogs recorded in a call center. In
Proceedings of INTERSPEECH, Lisbon, Portugal.
M.Walker, A. Rudnicky, R. Prasad, J. Aberdeen, E. Bratt,
J. Garofolo, H. Hastie, A. Le, B. Pellom, A. Potami-
anos, R. Passonneau, S. Roukos, G. Sanders, S. Sen-
eff, and D. Stallard. 2002. DARPA communicator:
Cross-system results for the 2001 evaluation. In Proc.
ICSLP.
W. Wang and J. Hirschberg. 2011. Detecting levels of
interest from spoken dialog with multistream predic-
tion feedback and similarity based hierarchical fusion
101
learning. In Proc. 12th Annual Meeting of the Spe-
cial Interest Group on Discourse and Dialogue (SIG-
DIAL), pages 152?161, Portland, Oregon, June.
N. Wang, W.L. Johnson, R. E. Mayer, P. Rizzo, E. Shaw,
and H. Collins. 2008. The politeness effect: Peda-
gogical agents and learning outcomes. International
Journal of Human-Computer Studies, 66(2):98?112.
102
Proceedings of NAACL-HLT 2013, pages 796?801,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Differences in User Responses to a Wizard-of-Oz versus Automated System
Jesse Thomason
University of Pittsburgh
Pittsburgh, PA 15260, USA
jdt34@pitt.edu
Diane Litman
University of Pittsburgh
Pittsburgh, PA 15260, USA
litman@cs.pitt.edu
Abstract
Wizard-of-Oz experimental setup in a dia-
logue system is commonly used to gather data
for informing an automated version of that
system. Previous work has exposed depen-
dencies between user behavior towards sys-
tems and user belief about whether the sys-
tem is automated or human-controlled. This
work examines whether user behavior changes
when user belief is held constant and the sys-
tem?s operator is varied. We perform a post-
hoc experiment using generalizable prosodic
and lexical features of user responses to a dia-
logue system backed with and without a hu-
man wizard. Our results suggest that user
responses are different when communicating
with a wizarded and an automated system, in-
dicating that wizard data may be less reliable
for informing automated systems than gener-
ally assumed.
1 Introduction
In a Wizard-of-Oz (WOZ) experimental setup, some
or all of the automated portions of a dialogue sys-
tem are replaced with a hidden, human evaluator.
This setup is often used to gather data from users
who believe they are interacting with an automated
system (Wolska et al, 2004; Andrews et al, 2008;
Becker et al, 2011). This data can inform a down-
stream, real automated system. A WOZ experimen-
tal protocol calls for holding ?all other input and out-
put . . . constant so that the only unknown variable
is who does the internal processing? (Paek, 2001).
Thus, hiding the human wizard?s input by layers of
system interface can render that system believably
automated.
An assumption of this WOZ data-gathering strat-
egy is that user behavior will not vary substantially
between the WOZ and automated (AUT) experimen-
tal setups. However, it was shown in a dialogue sys-
tem that training with a small set of data from an au-
tomated system gave rise to better performance than
training with a large set of data from an analogous
wizarded system (Drummond and Litman, 2011).
There, it was suggested that differences in system
automation may be responsible for the performance
gap. It is possible that user responses to these dia-
logue systems differed substantially.
This paper aims to investigate this possibility by
comparing data between a wizarded and automated
version of a tutoring dialogue system. We hypoth-
esize that what users say and how they say it will
differ when the only change is whether the system?s
speech recognition and correctness evaluation com-
ponents are wizarded or automated.
2 Dialogue System
The data for this study is provided by the baseline
conditions (one wizarded (WOZ) and one automated
(AUT)) of two prior experiments with a spoken tu-
torial dialogue system. Users of this system were
students recruited at our university, and each was
a native speaker of American English. Users were
novices and were tutored in basic Newtonian physics
by the system. Each was engaged by a set of di-
alogues that illustrated one or more basic physics
concepts. Those dialogues included remedial sub-
dialogues that were accessed when the users pro-
796
Tutor: So what are the forces acting on the
packet after it?s dropped from the plane?
Student: um gravity then well air resistance is
negligible just gravity
Tutor: Fine. So what?s the direction of the force
of gravity on the packet?
Student: vertically down
Figure 1: Tutor text is shown on a screen and read aloud
via text-to-speech. The user responds verbally to the tu-
tor?s queries.
vided incorrect or off-topic answers. These prior
experiments were examining the effects of system
adaptation in response to detected student uncer-
tainty (Forbes-Riley and Litman(a), 2011; Forbes-
Riley and Litman(b), 2011). However, in this study
we consider only the baseline, non-adaptive condi-
tions of those experiments. Figure 1 shows a sample
dialogue excerpt between the student and tutor.
In the baseline conditions of the WOZ and AUT
system past experiments, as shown in Figure 2, the
setups varied only by the system component respon-
sible for understanding and evaluating a user?s ver-
bal response. Each student participated in only one
of the two setups, and students were not informed
when the system was wizarded. In the WOZ setup a
human wizard marked student responses to prompts
as correct or incorrect. In the AUT setup, automatic
speech recognition was performed on student re-
sponses1, and (in)correctness of answers was deter-
mined using natural language understanding models
trained from the WOZ experiment?s data.
3 Post-Hoc Experiment
Using both lexical and prosodic features, we aimed
to determine whether there exist significant differ-
ences in users? turn-level responses to the WOZ and
AUT systems.
It was suspected that the imperfect accuracy2
(87%) of the AUT system?s evaluations of the
(in)correctness of user responses may have led to
remedial sub-dialogues being accessed by the AUT
system more often, since false-negatives accounted
1The average word-error rate for these AUT responses was
19%.
2Agreement of ? = 0.7 between the system and human.
Figure 2: The workflow of the tutoring dialogue system
with the WOZ setup component shown in solid, blue and
the AUT setup component shown in dashed, red.
System #Users #Qu #Turns
WOZ 21 111 1542
AUT 25 111 2034
Table 1: Counts for users, unique questions, and user
turns in each data set.
for 72% of inaccurate evaluations. To correct for this
imbalance, rather than comparing user responses to
all questions, we compared the features of user re-
sponses (turns) to each question individually. We
omitted questions which were presented in only one
setup3 as well as turns for which a human transcriber
found no user speech. Table 1 gives the numbers of
users, number of unique questions asked, and total
number of user responses contained in the remain-
ing data and used in our investigations.
For prosodic features, we considered duration,
pitch, and energy (RMS), each extracted using
openSMILE (Eyben et al, 2010). From pitch and
energy, which contain many samples during a single
turn, we extracted features for maximum, minimum,
mean, and standard deviation of these readings. We
also considered speech duration and the length of
the pause before speech began. This gave us a total
of 10 prosodic features. To account for any differ-
ences in recording environment and users? voices,
we normalized each prosodic feature by dividing its
value on each turn by its value in the first turn of the
current problem dialogue for that user. This normal-
3There were 3 such questions containing 6 user responses;
each question was a remedial sub-dialogue accessed in the AUT
but not WOZ setup.
797
ization scheme was chosen for our analysis because
it is used in the live system, though we note that al-
ternative methods considering more user responses
could be explored in the future.
For lexical features, we used the Linguistic In-
quiry and Word Count (LIWC). LIWC (Pennebaker
et al, 2001), a word-count dictionary, provides fea-
tures representing the percentage of words in an ut-
terance falling under particular categories. Though
still a counting strategy, these categories capture
higher-level concepts than would simple unigrams.
For example, one category is Tentative(T), which
includes words such as ?maybe?, ?perhaps?, and
?guess?. Less abstract categories, such as Prepo-
sitions(P), with words such as ?to?, ?with?, and
?above?, are also generated by LIWC. Using these
example categories, the utterance ?Maybe above?
would receive feature vector:
?0, . . . , 0, T = 50, 0, . . . , 0, P = 50, 0, . . . , 0? (1)
Human transcriptions of users? speech were made
available post-hoc for both system versions. We ex-
tracted 69 LIWC categories as lexical features from
these human transcriptions of each user turn.
Between the WOZ and AUT setups, we looked
for user response feature differences in two ways.
First, a Welch?s two-tailed t-test was used to com-
pare the distributions of each feature?s values be-
tween WOZ and AUT user responses per question.
We noted the features found to be significantly dif-
ferent. Second, we built classification models to dis-
tinguish between user responses per question from
the WOZ and AUT experiments. For each question,
a J484 decision tree model was trained and tested
using 10-fold cross validation via the Weka5 toolkit.
Only questions with at least 10 responses between
both setups were considered. Each model was com-
pared against majority-class baseline for its respec-
tive question by checking for statistically significant
differences in the model?s accuracy.
4We tried Logistic Regression and Support Vector classifiers
but these were consistently outperformed by J48.
5http://www.cs.waikato.ac.nz/ml/weka
4 Results
4.1 Statistical Comparison of Features
The number of questions for which at least one
feature differed statistically significantly was calcu-
lated. Since distinct sets of students were involved in
the WOZ and AUT setups, it is possible that some of
these differences are inherent between the students
and not resulting from the presence or absence of a
human wizard. To control for this possibility, we as-
signed students randomly into two new groups (pre-
serving the original class distribution in each new
group) and tested for feature differences between
these new groups. Table 2 summarizes the differ-
ences found by each feature set. We report only
questions for which at least one feature differed be-
tween WOZ and AUT but not between these two
random groups6. Table 2 also shows the percentage
of turns that those questions comprised in the cor-
pus. Prosodic and lexical features each differ for a
substantial portion of the corpus of turns, and when
both sets are considered about 67% of the corpus is
captured.
Feature set #Qu % Corpus by Turns
Prosodic 42 46.22%
Lexical 33 35.46%
Either 61 66.86%
Table 2: Number of questions for which at least one fea-
ture from the feature set was found to differ with signif-
icance p < 0.05 between WOZ and AUT responses and
the percentage the corpus represented by those questions,
weighted by the speech turns they comprise.
After controlling for possible between-student
differences, all 10 prosodic features and 29 out of
69 lexical features differed significantly (p < 0.05)
for at least one question. Table 3 gives the features
which were able to differentiate at least 10% of the
corpus by turns.
These t-tests show there exist features which dif-
fer for a substantial number of questions between
the two experimental setups. Examination of Table
6We repeated this random split procedure 10 times and
found, after omitting features found significant in any of the 10
splits, that 58.08% of the corpus was still captured. Less than
2% of the turns belonged to questions with at least one feature
different through all 10 splits.
798
Feature % CbT #Qu #W>A
Duration 22.15% 19 1
RMS Min 16.86% 15 14
Dictionary Words 15.13% 13 11
pronoun 12.56% 10 10
social 11.35% 9 8
funct 10.99% 9 9
Six Letter Words 10.91% 9 0
Table 3: Features shown to differ with significance p <
0.05 between WOZ and AUT responses in questions
comprising at least 10% of the corpus by turns (CbT).
The numbers of questions these turns comprised and of
questions with greater (W)OZ than (A)UT mean are also
given.
Tutor: So how do these two forces? directions
compare?
Top two most common responses:
WOZ(9),AUT(2): they are opposite
WOZ(3),AUT(8): opposite
Longest responses per tutor setup:
WOZ Student: the relationship between the two
forces? directions are towards each other since
the sun is pulling the gravitational force of the
earth
AUT Student: they are opposite directions
Figure 3: The tutor question and select user responses to
a question for which the Dictionary Words feature was
greater for WOZ responses.
3 in addition suggests that users used more words
with the wizarded system. For example, the fourth
row shows that all of the questions showing differ-
ences for the LIWC category pronoun (the words
?they?, ?he?, and ?it? are popular in this corpus) ex-
posed higher percentage of pronouns in the WOZ
utterances. The usual dominance of the third row,
Dictionary Words, by the WOZ utterances also re-
flects this trend. Figure 3 gives common and charac-
teristic student responses for each setup on a ques-
tion for which Dictionary Words differed signifi-
cantly. We next applied machine learning to clas-
sify the experiment-of-origin of responses based on
these features.
Figure 4: The J48 tree for the question ?Would you like to
do another problem??. Classification nodes are marked in
blue and red for WOZ and AUT, respectively, and specify
(#Instances:#Incorrect).
4.2 Response Classification Experiments
After removing questions with less than 10 re-
sponses between the two setups, there remained 97
questions totaling 2980 turns. Of the J48 models
built and tested on each question, 21 of 97 out-
performed the majority-class baseline accuracies for
those questions with significance p < 0.05. These
21 questions represented 32.79% of the corpus by
turns. We present in detail the two of these 21 ques-
tions with the most turns.
The question ?Would you like to do another prob-
lem?? represented 6.11% of the corpus by turns and
the J48 model built for it, shown in Figure 4, out-
performed the baseline accuracy with p < 0.001.
While the Duration feature was the root node, a big-
ger decision was made by Word Count ? 1, for
which most responses were from AUT data. This
result is consistent with literature (Schechtman and
Horowitz, 2003; Rose? and Torrey, 2005) that sug-
gests that users interacting with automated systems
will be more curt.
The question ?Now let?s find the forces exerted
on the car in the vertical direction during the colli-
sion. First, what vertical force is always exerted on
an object near the surface of the earth?? represented
1.54% of the corpus by turns and the J48 model built
for it, shown in Figure 5, outperformed the baseline
accuracy with p < 0.01. Again, Duration emerged
as the tree root, but here the biggest decision fell to
RMS mean. Student responses approximately louder
than the initial response to the tutor in this question
dialogue were marked, almost entirely accurately, as
AUT.
Since both trees were rooted at Duration, we sam-
799
Figure 5: The J48 tree for the question ?Now let?s find the
forces exerted on the car in the vertical direction during
the collision. First, what vertical force is always exerted
on an object near the surface of the earth??. Classification
nodes are marked in blue and red for WOZ and AUT,
respectively, and specify (#Instances:#Incorrect).
pled common responses from each experiment for
both problems. We noticed that hyper-articulation
(speaking slowly, loudly, and enunciating each syl-
lable) was more common in the AUT responses. For
example, one user answering ?Would you like to do
another problem?? took almost 4 seconds to clearly
and slowly pronounce the word ?yes?. We suspect
that these hyper-articulations may have contributed
to the classifiers? ability to detect WOZ responses
based on their brevity.
The performance of the per-question J48 models
shows, for a non-trivial portion of the turns, that the
experiment-of-origin can be classified based on gen-
eralizable prosodic and lexical features alone. The
two trees discussed above demonstrate the simplic-
ity of the models needed to perform this separation.
5 Discussion and Future Work
We demonstrate that there exist significant differ-
ences between user responses to a wizarded and an
automatic dialogue system?s questions, even when
the contribution of the wizard is as atomic as speech
recognition and correctness evaluation. Our gen-
eralizable features are derived exclusively from the
recordings of the users? responses and human tran-
scriptions of their speech.
Because the role of the wizard in the WOZ setup
was limited to evaluating users? spoken response to a
prompt, our results suggest that user speech changes
as a result of user confidence in the system?s ac-
curacy. For example, Figure 3 demonstrates that
users in the WOZ setup used complete sentences
and gave long responses, where AUT users, possi-
bly anticipating system error, used shorter (some-
times one word) responses. This relationship be-
tween user confidence and user speech may be anal-
ogous to observed differences like users? longer
speech and typed responses to systems when told
those systems are human-operated (Schechtman and
Horowitz, 2003; Rose? and Torrey, 2005). Our re-
sults suggest ways in which raw wizarded data may
fall short of ideal for training an automated system.
Having established that differences exist, our fu-
ture work will focus on deeper exploration of the na-
ture of these differences in users? responses. We sus-
pect users become less confident in the automated
system over time, so one direction of study will be to
measure how the observed differences change over
the course of the dialogue. We expect that they are
minimal early on and become more pronounced in
the automated setup as users? confidence is shaken.
Additionally, some technical aspects of our method-
ology may impact these and future results: using dif-
ferent methods of normalization for user speech val-
ues than the one from this paper may affect visibility
of observed differences between the setups.
Future work may also attempt to address these
differences directly. Intentional wizard error could
be introduced to frustrate the user into responding
as she would to a less accurate system, analogous
to intentional errors produced in user simulation in
spoken dialogue systems (Lee and Eskenazi, 2012).
This strategy would be further informed by stud-
ies of the relationship between the system?s eval-
uation accuracy and student responses? deviation
from wizarded responses. Alternatively, post-hoc
domain adaptation could be used to adjust the WOZ
data. Generalizable statistical classification domain
adaptation (Daume? and Marcu, 2006) and adapta-
tion demonstrated to work well in NLP-specific do-
mains (Jiang and Zhai, 2007) both have the potential
to adjust WOZ data to better match that seen by au-
tomated systems.
Acknowledgments
This work is funded by NSF award 0914615. We
thank Scott Silliman for his support and Pamela Jor-
dan, Wenting Xiong, and the anonymous reviewers
for their helpful suggestions and commentary.
800
References
Pierre Andrews, Suresh Manandhar, and Marco De Boni.
2008. Argumentative Human Computer Dialogue for
Automated Persuasion. Proceedings of the 9th SIG-
DIAL Workshop on Discourse and Dialogue, pages
138-147, Columbus, June 2008. Association for Com-
putational Linguistics.
Lee Becker, Wayne Ward, Sarel van Vuuren, Martha
Palmer. 2011. DISCUSS: A dialogue move taxonomy
layered over semantic representations. International
Workshop on Computational Semantics (IWCS), Main
Conference. Association for Computational Linguis-
tics.
Hal Daume? III and Daniel Marcu. 2006. Domain Adap-
tation for Statistical Classifiers. Journal of Artificial
Intelligence Research, 26:101-126. AI Access Foun-
dation.
Joanna Drummond and Diane Litman. 2011. Examining
the Impacts of Dialogue Content and System Automa-
tion on Affect Models in a Spoken Tutorial Dialogue
System. Proceedings of the SIGDIAL 2011 Confer-
ence, Portland, Oregon, June. Association for Com-
putational Linguistics.
Florian Eyben, Martin Wo?llmer, and Bjo?rn Schuller.
2010. Opensmile: The Munich Versatile and Fast
Open-Source Audio Feature Extractor. MM ?10 Pro-
ceedings of the International Conference on Multime-
dia, 1459-1462.
Kate Forbes-Riley and Diane Litman. 2011. Designing
and Evaluating a Wizarded Uncertainty-Adaptive Spo-
ken Dialogue Tutoring System. Computer Speech and
Language, 25(1): 105-126.
Kate Forbes-Riley and Diane Litman. 2011. Benefits
and Challenges of Real-Time Uncertainty Detection
and Adaptation in a Spoken Dialogue Computer Tutor.
Speech Communication 2011, 53(9-10): 1115-1136.
Jing Jiang and ChengXiang Zhai. 2007. Instance
Weighting for Domain Adaptation in NLP. Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 264-271, Prague,
Czech Republic, June 2007.
Sungjin Lee and Maxine Eskenazi. 2012. An Unsu-
pervised Approach to User Simulation: toward Self-
Improving Dialog Systems. Proceedings of the 13th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue (SIGDIAL), pages 50-59, Seoul,
South Korea, 5-6 July 2012. Association for Compu-
tational Linguistics.
Tim Paek. 2001. Empirical Methods for Evaluating Dia-
log Systems. SIGDIAL ?01 Proceedings of the Second
SIGdial Workshop on Discourse and Dialogue, 16:1-9.
James Pennebaker, Martha Francis, and Roger Booth.
2001. Linguistic Inquiry and Word Count (LIWC):
LIWC2001. Lawrence Erlbaum Associates, Mahwah,
NJ.
Carolyn P. Rose? and Cristen Torrey. 2005. Inter-
activity and Expectation: Eliciting Learning Ori-
ented Behavior with Tutorial Dialogue Systems.
Human-Computer Interaction-INTERACT 2005, 323-
336. Springer Berlin/Heidelberg.
Nicole Schechtman and Leonard M. Horowitz. 2003.
Media Inequality in Conversation: How People Be-
have Differently When Interacting with Computers
and People. CHI ?03 Proceedings of the SIGCHI con-
ference on Human factors in computing systems, 281-
288.
Magdalena Wolska, Ivana Kruijff-Korbayova?, Helmut
Horacek. 2004. Lexical-Semantic Interpretation of
Language Input in Mathematical Dialogs. Proceed-
ings of the ACL 2nd Workshop on Text Meaning and
Interpretation, 81-88.
801
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 8?15,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Reducing Annotation Effort on Unbalanced Corpus based on Cost Matrix
Wencan Luo, Diane Litman
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
{wencan,litman}@cs.pitt.edu
Joel Chan
Department of Psychology
University of Pittsburgh
Pittsburgh, PA 15260, USA
chozen86@gmail.com
Abstract
Annotated corpora play a significant role in
many NLP applications. However, annota-
tion by humans is time-consuming and costly.
In this paper, a high recall predictor based
on a cost-sensitive learner is proposed as a
method to semi-automate the annotation of
unbalanced classes. We demonstrate the ef-
fectiveness of our approach in the context of
one form of unbalanced task: annotation of
transcribed human-human dialogues for pres-
ence/absence of uncertainty. In two data
sets, our cost-matrix based method of uncer-
tainty annotation achieved high levels of re-
call while maintaining acceptable levels of ac-
curacy. The method is able to reduce human
annotation effort by about 80% without a sig-
nificant loss in data quality, as demonstrated
by an extrinsic evaluation showing that results
originally achieved using manually-obtained
uncertainty annotations can be replicated us-
ing semi-automatically obtained uncertainty
annotations.
1 Introduction
Annotated corpora are crucial for the development
of statistical-based NLP tools. However, the annota-
tion of corpora is most commonly done by humans,
which is time-consuming and costly. To obtain a
higher quality annotated corpus, it is necessary to
spend more time and money on data annotation. For
this reason, one often has to accept some tradeoff
between data quality and human effort.
A significant proportion of corpora are unbal-
anced, where the distribution of class categories are
heavily skewed towards one or a few categories. Un-
balanced corpora are common in a number of dif-
ferent tasks, such as emotion detection (Ang et
al., 2002; Alm et al, 2005), sentiment classifica-
tion (Li et al, 2012), polarity of opinion (Carvalho
et al, 2011), uncertainty and correctness of student
answers in tutoring dialogue systems (Forbes-Riley
and Litman, 2011; Dzikovska et al, 2012), text
classification (Forman, 2003), information extrac-
tion (Hoffmann et al, 2011), and so on1.
In this paper, we present a semi-automated anno-
tation method that can reduce annotation effort for
the class of binary unbalanced corpora. Here is our
proposed annotation scheme: the first step is to build
a high-recall classifier with some initial annotated
data with an acceptable accuracy via a cost-sensitive
approach. The second step is to apply this classifier
to the rest of the unlabeled data, where the data are
then classified with positive or negative labels. The
last step is to manually check every positive label
and correct it if it is wrong.
To apply this method to work in practice, two re-
search questions have to be addressed. The first one
is how to get a high-recall classifier. High recall
means only a low proportion of true positives are
misclassified (false negatives). This property allows
for only positive labels to be corrected by human an-
notators in the third step, so that annotation effort
may be reduced. A related and separate research
question concerns the overall quality of data when
false negatives are not corrected: will a dataset anno-
tated with this method produce the same results as a
1The unbalanced degrees - proportion of minority class cat-
egory, of these corpora range from 3% to 24%.
8
fully manually annotated version of the same dataset
when analyzed for substantive research questions?
In this paper, we will answer the two research
questions in the context of one form of binary un-
balanced task2: annotation of transcribed human-
human dialogue for presence/absence of uncertainty.
The contribution of this paper is twofold. First,
an extrinsic evaluation demonstrates the utility of
our approach, by showing that results originally
achieved using manually-obtained uncertainty anno-
tations can be replicated using semi-automatically
obtained uncertainty annotations. Second, a high
recall predictor based on a cost-sensitive learner is
proposed as a method to semi-automate the annota-
tion of unbalanced classes such as uncertainty.
2 Related Work
2.1 Reducing Annotation Effort
A number of semi-supervised learning methods have
been proposed in the literature for reducing annota-
tion effort, such as active learning (Cohn et al, 1994;
Zhu and Hovy, 2007; Zhu et al, 2010), co-training
(Blum and Mitchell, 1998) and self-training (Mihal-
cea, 2004). Active learning reduces annotation by
carefully selecting more useful samples. Co-training
relies on several conditional independent classifiers
to tag new unlabeled data and self-training takes
the advantage of full unlabeled data. These semi-
supervised learning methods demonstrate that with
a small proportion of annotated data, a classifier can
achieve comparable performance with all annotated
data. However, these approaches still need consid-
erable annotation effort when a large corpus has to
be annotated. In that case, all predicted labels have
to be rechecked by humans manually. In addition,
none of them take advantage of unbalanced data.
Another class of effort reduction techniques is
pre-annotation, which uses supervised machine-
learning systems to automatically assign labels to
the whole data and subsequently lets human anno-
tators correct them (Brants and Plaehn, 2000; Chiou
et al, 2001; Xue et al, 2002; Ganchev et al, 2007;
Chou et al, 2006; Rehbein et al, 2012).
Generally speaking, our annotation method be-
longs to the class of pre-annotation methods. How-
2This annotation scheme can also benefit other kinds of
tasks.
ever, our method improves pre-annotation for unbal-
anced data in two ways. Firstly, we lower the thresh-
old for achieving a high recall classifier. Secondly,
with pre-annotation, although people only perform
a binary decision of whether the automatic classifier
is either right or wrong, they have to go through all
the unlabeled data one by one. In contrast, in our
scheme, people go through only the positive predic-
tions, which are much less than the whole unlabeled
data, due to the unbalanced structure of the data.
What?s more, reducing the annotation effort is the
goal of this paper but not building a high recall clas-
sifier such as Prabhakaran et al (2012) and Ambati
et al (2010).
The approach proposed by Tetreault and
Chodorow (2008) is similar to us. However, they
assumed they had a high recall classifier but did not
explicitly show how to build it. In addition, they
did not provide extrinsic evaluation to see whether a
corpus generated by pre-annotation is good enough
to be used in real applications.
2.2 Uncertainty Prediction
Uncertainty is a lack of knowledge about internal
state (Pon-Barry and Shieber, 2011). In this paper,
we only focus on detection of uncertainty on text.
Commonly used features are lexical features such as
unigram (Forbes-Riley and Litman, 2011). More-
over, energy, dialogue features such as turn number,
tutor goal, and metadata like gender are also con-
sidered by Forbes-Riley and Litman (2011). Un-
certainty prediction is both substantively interesting
(Chan et al, 2012; Forbes-Riley and Litman, 2009)
and pragmatically expeditious for our purposes, due
to its binary classification and typical unbalanced
class structure.
CoNLL 2010 has launched a shared task to de-
tect hedges and their scope in natural language text
on two data sets: BioScope and Wikipedia (CoNLL,
2010). This first task to detect whether there is a
hedge present or not present in a sentence is very
similar to our uncertainty prediction task. 23 teams
participated in the shared task with the best re-
call of 0.8772 on the BioScope, and 0.5528 on the
Wikipedia. As we can see, uncertainty detection is
not trivial and it can be hard to get a high recall clas-
sifier.
In this paper, we focus on lexical features for our
9
purpose because lexical features are simple to ex-
tract and sufficient for our scheme. Even though
other features may improve uncertainty prediction
performance, with the goal of reducing annotation
effort, such lexical features are shown to be good
enough for our task.
3 The Corpora
We examine the following two data sets: the Mars
Exploration Rover (MER) mission (Tollinger et al,
2006; Paletz and Schunn, 2011) and the student
engineering team (Eng) dataset (Jang and Schunn,
2012). The MER scientists are evaluating data
downloaded from the Rover, discussing their work
process, and/or making plans for the Rovers. They
come from a large team of about 100+ scien-
tists/faculty, graduate students, and technicians. At
any one time, conversations are between 2-10 peo-
ple. The Eng teams are natural teams of college un-
dergraduates working on their semester-long prod-
uct design projects. The conversations involve 2-6
individuals. Audio and video are available for both
data sets and transcripts are obtained with human an-
notators.
Our task is to annotate the transcribed human-
human dialogues for presence/absence of uncer-
tainty in each utterance. There are 12,331 tran-
scribed utterances in the MER data set, and 44,199
transcribed utterances in the Eng data set. Both data
sets are unbalanced: in the MER data, 1641 of all
the 12,331 (13.3%) utterances are annotated as un-
certain by trained human annotators; in the Eng data,
only 1558 utterances are annotated, 221 of which are
annotated as uncertain (14.2%). 96.5% of the utter-
ances in the Eng data set have not been annotated
yet, raising the need for an efficient annotated tech-
nique. Both data sets are annotated by two trained
coders with high inter-rater agreement, at Cohen?s
kappa of 0.75 (Cohen, 1960). A sample dialogue
snippet from the MER corpus is shown in Table 1.
The last column indicates whether the utterance is
labeled as uncertainty or not: ?1? means uncertainty
and ?0? means certainty.
The MER data serves as the initial annotated set
and a high recall classifier will be trained on it; the
Eng data3 serves as a simulated unlabeled data set to
3The Eng data in this paper denotes the annotated subset of
speaker utterance uncertainty?
S6 You can?t see the forest through the trees. 0
S1 Yea, we never could see the [missing words] 1
S6 No we had to get above it 0
S4 We just went right through it 0
S6 Yea 0
S1 I still don?t, 0
I?m not quite sure 1
Table 1: Sample dialogue from the MER corpus
test the performance of our annotation scheme.
4 High Recall Classifier
4.1 Basic Classifier
The uncertainty prediction problem can be viewed
as a binary classification problem. It involves two
steps to build a high recall classifier for unbalanced
data. The first step is to build up a simple classifier;
the second step is to augment this classifier to favor
high recall.
Aiming for a simple classifier with high recall,
only some lexical words/phrases are used as fea-
tures here. There are several resources for the
words/phrases of uncertainty prediction. The main
resource is a guideline book used by our annotators
showing how to distinguish uncertainty utterance. It
gives three different kinds of words/phrases, shown
in Table 2 indicated by three superscripts ?+?, ?-?
and ?*?. The words/phrases with ?+? show some
evidence of uncertainty; ones with ?-? mean that
they show no evidence of uncertainty; others with
?*? may or may not show uncertainty. The second
source is from existing literature. The words/phrases
with ?1? are from (Hiraishi et al, 2000) and ones
with ?2? are from (Holms, 1999).
For each word/phrase w, a binary feature is used
to indicate whether the word/phrase w is in the ut-
terance or not.
A Naive Bayes classifier is trained on the MER
data using these features and tested on the Eng data.
The performances of the model on the train set and
test set are shown in Table 3. Both weighted and un-
weighted false positive (FP) Rate, Precision, Recall
and F-Measure are reported. However, in later ex-
periments, we will focus on only the positive class
(the uncertainty class). A 0.689 recall means that
510 out of 1641 positive utterances are missed using
this model.
the original Eng corpus.
10
as far as+ i hope+ somehow+ it will? don?t remember? maybe? tends to? doubtful1
as far as i know+ i think+ something+ it wont? essentially? most? that can vary? good chance1
as far as we know+ i thought+ something like this+ it would? fairly? mostly? typically? improbable1
believe+ i wont+ worried that+ would it be? for the most part? normally? uh? possible1
could+ im not sure+ you cannot tell+ about? frequently? pretty much? um? probable1
guess+ may+ can? almost? generally? quite? usually? relatively1
guessed+ might+ i am? any nonprecise amount? hes? should? very? roughly1
guessing+ not really+ i can? basically? hopefully? sometimes? virtually? tossup1
i believe+ not sure+ i will? believed? i assumed that? somewhat? whatever? unlikely1
i cant really+ possibly+ i would? cannot remember? it sounds as? somewhere? you know? of course2
i feel+ probably+ it can? can?t remember? kind of? stuff? almost certain1 sort of2
i guess+ really+ it is? do not remember? likely? tend to? almost impossible1
Table 2: Words/phrases for uncertainty prediction.
Data Set FP Rate Precision Recall F-Measure Class
MER
.311 .954 .989 .971 0
.011 .908 .689 .784 1
.271 .948 .949 .946 (Weighted)
Eng
.475 .926 .981 .952 0
.019 .817 .525 .639 1
.41 .91 .916 .803 (Weighted)
Table 3: Naive Bayes classifier performance on the MER
(train set) and Eng (test set) with only the words/phrases
assume I didn?t know more or less some kind
couldn?t i don?t even know no idea suppose
don?t know if not clear suspect
don?t think if it or think
don?t understand if we perhaps thought
doubt if you possibility unclear
either imagine potential what i understood
figured kinda presumably wondering
i bet kinds of seem
i can try like some
Table 4: New words/phrases for uncertainty prediction
After error analysis, a few new words/phrases are
added to the feature set, shown in Table 4. By sup-
plementing the original feature set in this way, we
reran the training yielding our final baseline, the
performance on the training data (MER) and test-
ing data (Eng) is shown in Table 5. This time, we
compare different classifiers including Naive Bayes
(NB), Decision Tree (DT) and Support Vector Ma-
chine (SVM). All of them are implemented using the
open source platform Weka (Hall et al, 2009) with
default parameters.
As we can see, test recall is worse than train recall.
Data Set Method TP FP Precision Recall F-Measure
MER
NB .732 .016 .875 .732 .797
DT .831 .013 .908 .831 .868
SVM .811 .013 .905 .811 .855
Eng
NB .679 .014 .888 .679 .769
DT .665 .021 .84 .665 .742
SVM .674 .022 .832 .674 .745
Table 5: Performance with original and new
words/phrases as a feature set: train on the MER
and test on the Eng data for class ?1?. TP is true positive;
FP is false positive
In addition, although DT and SVM perform better
than NB on train data set, they have similar perfor-
mance on the test set. Thus, the performance of the
baseline is not unacceptable, but neither is it stellar.
In advance, it is not hard to build such a model, since
only simple features and classifiers are used here.
4.2 Augmenting the Classifier using a Cost
Matrix
In our annotation framework, if the classifier
achieves 100% recall, the annotated data will be per-
fect because all the wrong predictions can be cor-
rected. That?s the reason why we are seeking for a
high recall classifier. A confusion matrix, is a com-
mon way to represent classifier performance. High
recall is indexed by a low false negative (FN) rate;
therefore, we aim to minimize FNs to achieve high
recall.
Following this idea, we employ a cost-sensitive
model, where the cost of FN is more than false pos-
itive (FP).
Following the same notation, we represent our
cost-sensitive classifier as a cost matrix. In our cost
matrix, classifying an actual class ?1? as ?1? costs
Ctp, an actual class ?0? as ?1? costs Cfp, an actual
class ?1? to ?0? costs Cfn, and ?0? to ?0? costs Ctn.
To achieve a high recall, Cfn should be more than
Cfp.
We can easily achieve 100% recall by classifying
all samples to ?1?, but this would defeat our goal of
reducing human annotation effort, since all utterance
uncertainty predictions would need to be manually
corrected. Thus, at the same time of a high recall,
we should also balance the total ratio of TP and FP.
In our experiment, Ctp and Ctn are set to 0 since
they are perfectly correct. Additionally, Cfp = 1 all
the time and Cfn changes with different scales. FPs
11
Cfn FP Rate Precision Recall F-Measure (TP + FP )/N
1 .022 .831 .67 .742 .114
2 .024 .825 .683 .748 .117
3 .037 .771 .747 .759 .138
5 .052 .726 .828 .774 .162
10 .071 .674 .887 .766 .187
15 .091 .622 .91 .739 .207
20 .091 .622 .91 .739 .207
Table 6: Test performance with cost matrix
mean wrong predictions, but we can correct them
during the second pass to check them. However, we
cannot correct FNs without going through the whole
data set, so they are a more egregious detriment to
the quality of the annotated data. During the exper-
iment, Cfn varies from 1 to 20. With increases in
Cfn, the cost of FN increases compared to FP.
The cost-sensitive classifier is relying on Weka
with reweighting training instances. In this task,
SVM performed better than NB and DT. Only SVM
results are included here due to space constraint.
The test results are shown in Table 64. The last col-
umn in the two tables is the total proportion of pos-
itive predictions (FP + TP ). This value indicates
the total amount of data that humans have to check
in the second pass to verify whether positive predic-
tions are correct. To reduce human annotation effort,
we would like this value to be as low as possible.
As shown in Table 6, with the increase ofCfn, the
recall increases; however, the proportion of positive
predictions also increases. Therefore, it is a tradeoff
to achieve a high recall and a low ratio of TP and FP.
For the test set, the recall increases with larger
Cfn, even with a small increase of Cfn from 1 to
3. Remarkably, the classifier gives us a high recall
while keeping the proportion of positive predictions
at an acceptably low level. When Cfn = 20 for the
test set, only 20.7% of the data need to be manually
checked by humans, and less than 10% uncertain ut-
terances (19 out of 221 for the Eng data) are missed.
Now, we have achieved a high recall classifier
with an acceptable ratio of positive predictions.
5 Extrinsic Evaluation of Semi-Automated
Annotation
Even with a high recall classifier, some of the true
positive data are labeled incorrectly in the final an-
4Only Cfn = 1, 2, 3, 5, 10, 15, 20 are reported here due to
page limits
notated corpus. In addition, it also changes the dis-
tribution of class labels.
To test whether it hurts the overall data quality,
we performed an analysis, which demonstrates that
this annotation scheme is sufficient to produce qual-
ity data. We attempted to replicate an analysis on the
Eng data set, which examines the use of analogy, a
cognitive strategy where a source and target knowl-
edge structure are compared in terms of structural
correspondences as a strategy for solving problems
under uncertainty. The analysis we attempt to repli-
cate here focuses on examining how uncertainty lev-
els change relative to baseline before, during, and
after the use of analogies.
The overall Eng transcripts were segmented into
one of 5 block types: 1) pre-analogy (Lag -1) blocks,
10 utterances just prior to an analogy episode, 2)
during-analogy (Lag 0) blocks, utterances from the
beginning to end of an analogy episode, 3) post-
analogy (Lag 1) blocks, 10 utterances immediately
following an analogy episode, 4) post-post-analogy
(Lag 2) blocks, 10 utterances immediately follow-
ing post-analogy utterances, and 5) baseline blocks,
each block of 10 utterances at least 25 utterances
away from the other block types. The measure of un-
certainty in each block was the proportion of uncer-
tain utterances. The sampling strategy for the base-
line blocks was designed to provide an estimate of
uncertainty levels when the speakers were engaged
in pre-analogy, during-analogy, or post-analogy con-
versation, with the logic being that a certain amount
of lag or spillover of uncertainty was assumed to
take place surrounding analogy episodes.
Figure 1 shows the relationship of block type to
mean levels of uncertainty, comparing the pattern
with human vs. classifier-supported uncertainty la-
bels. The classifier-generated labels were first pre-
processed such that all FPs were removed, but FNs
remain. This re-analysis comparison thus provides
a test of whether the recall rate is high enough that
known statistical effects are not substantially altered
or removed. To examine how different settings of
Cfn might impact overall performance, we used la-
bels (corrected for false positives) for 4 different lev-
els of Cfn (1, 5, 10, 20) from the Table 6.
In the Eng data analyses, the main findings were
that analogy was triggered by local spikes in un-
certainty levels (Lag -1 > baseline), replicating re-
12
Baseline Lag ?1 Lag 0 Lag 1 Lag 2
0
0.05
0.1
0.15
0.2
0.25
Block type
M
ea
n 
%
 u
nc
er
ta
in
ty
 in
 b
lo
ck
 
 
Human
Cfn=20
Cfn=10
Cfn=5
Cfn=1
Figure 1: Mean % uncertainty by block type and label
source (Eng data set)
Table 7: Standardized mean difference (Cohen?s d) from
baseline by block type and label source (the Eng data set)
(Note: ?*? denotes p < .05, ?**? denotes p < .01)
Block type
Lag -1 Lag 0 Lag 1 Lag 2
Human 0.54? 0.4 0.79?? 0.46?
Cfn = 20 0.57? 0.3 0.78?? 0.44
Cfn = 10 0.58?? 0.32 0.73?? 0.47?
Cfn = 5 0.57? 0.34 0.66?? 0.48?
Cfn = 1 0.42 0.25 0.54? 0.40
sults from prior work with the MER dataset (Chan
et al, 2012); in contrast to the findings in MER,
uncertainty did not reduce to baseline levels follow-
ing analogy (Lags 1 and 2 > baseline). Figure 1
plots the relationship of block type to mean levels
of uncertainty in this data set, comparing the pat-
tern with human vs. classifier-generated uncertainty
labels. Table 7 shows the standardized mean differ-
ence (Cohen?s d) (Cohen, 1988) from baseline by
block type and label source. The pattern of effects
(Lag -1 > baseline, Lags 1 and 2 > baseline) re-
mains substantially unchanged with the exception of
the Lag 2 vs. baseline comparison falling short of
statistical significance (although note that the stan-
dardized mean difference remains very similar) for
Cfn ranging from 20 to 5, although we can observe
a noticeable attenuation of effect sizes from Cfn of
5 and below, and a loss of statistical significance
for the main effect of uncertainty being significantly
higher than baseline for Lag -1 blocks when Cfn =
1.
The re-analysis clearly demonstrates that the re-
call rate of the classifier is sufficient to not substan-
tially alter or miss known statistical effects. We can
reasonably extrapolate that using this classifier for
uncertainty annotation in other datasets should be
satisfactory.
6 Conclusion and Discussion
In this paper, a simple high recall classifier is pro-
posed based on a cost matrix to semi-automate the
annotation of corpora with unbalanced classes. This
classifier maintains a good balance between high re-
call and high FP and NP ratio. In this way, humans
can employ this classifier to annotate new data with
significantly reduced effort (approximately 80% less
effort, depending on the degree of imbalance in the
data). Although the classifier does introduce some
misclassified samples to the final annotation, an ex-
trinsic evaluation demonstrates that the recall rate is
high enough and the performance does not sacrifice
data quality.
Like other semi-supervised or supervised meth-
ods for supporting annotation, our annotation
scheme has some limitations that should be noted.
Firstly, an initial annotated data set is needed to de-
rive a good performance classifier and the amount
of annotated data is dependent on the specific task5.
Secondly, the features and machine learning algo-
rithms used in semi-supervised annotation are also
domain specific. At the same time, there are some
unique challenges and opportunities that can be fur-
ther investigated for our annotation scheme on un-
balanced data. For example, even though the cost
matrix method can achieve a high recall for binary
classification problem, whether it can be generalized
to other tasks (e.g., multi-class classification tasks)
is an unanswered question. Another open question
is how the degree of unbalance between classes in
the corpora affects overall annotation quality. We
suggest that if the data is not unbalanced, the total
amount of effort that can be reduced will be lower.
Acknowledgments
The collection of the engineering data was supported
by NSF grants SBE-0738071, SBE-0823628, and
SBE-0830210. Analogy analysis was supported by
NSF grant SBE-1064083.
5For a new task, a new feature set is usually derived.
13
References
Cecilia Ovesdotter Alm, Dan Roth and Richard Sproat.
2005. Emotions from text: Machine learning for
text-based emotion prediction. In Proceedings of
HLT/EMNLP 2005.
Bharat Ram Ambati, Mridul Gupta, Samar Husain and
Dipti Misra Sharma. 2010. A high recall error identi-
fication tool for Hindi treebank validation. In Proceed-
ings of The 7th International Conference on Language
Resources and Evaluation (LREC), Valleta, Malta.
Jeremy Ang, Rajdip Dhillon, Ashley Krupski, Elizabeth
Shriberg and Andreas Stolcke. 2002. Prosody-based
automatic detection of annoyance and frustration in
human-computer Dialog. In INTERSPEECH-02.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the eleventh annual conference on Compu-
tational learning theory, p.92-100, July 24-26, Madi-
son, Wisconsin, United States
Thorsten Brants and Oliver Plaehn. 2000. Interactive
corpus annotation. In Proceedings of LREC-2000.
Paula Carvalho, Lu??s Sarmento, Jorge Teixeira and Ma?rio
J. Silva. 2011. Liars and saviors in a sentiment an-
notated corpus of comments to political debates. In
Proceedings of the Association for Computational Lin-
guistics (ACL 2011), Portland, OR.
Joel Chan, Susannah B. F. Paletz and Christian D.
Schunn. 2012. Analogy as a strategy for supporting
complex problem solving under uncertainty. Memory
& Cognition, 40, 1352-1365.
Fu-Dong Chiou, David Chiang andMartha Palmer. 2001.
Facilitating treebank annotation using a statistical
parser. In HLT?01. ACL.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,
Wei Ku1, Ting-Yi Sung and Wen-Lian Hsu. 2006.
A semi-automatic method for annotating a biomedical
proposition bank. In Proceedings of FLAC-2006.
David Cohn, Richard Ladner and Alex Waibel. 1994.
Improving generalization with active learning. Ma-
chine Learning, 15 (2), 201-221.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20, 37-46.
Jacob Cohen. 1988. Statistical power analysis for the
behavioral sciences (2nd ed.). Lawrence Erlbaum.
CoNLL-2010 Shared Task. 2010. In Fourteenth Con-
ference on Computational Natural Language Learning,
Proceedings of the Shared Task.
Myroslava Dzikovska, Peter Bell, Amy Isard and Jo-
hanna D. Moore. 2012. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. EACL 2012: 471-481.
Kate Forbes-Riley and Diane Litman. 2009. Adapting
to student uncertainty improves tutoring dialogues. In
Proceedings 14th International Conference on Artifi-
cial Intelligence in Education (AIED2009), pp. 33-40.
Kate Forbes-Riley and Diane Litman. 2011. Bene-
fits and challenges of real-time uncertainty detection
and adaptation in a spoken dialogue computer tutor.
Speech Communication, v53, pp. 1115-1136.
George Forman 2003. An Extensive empirical study of
feature selection metrics for text classification. Jour-
nal of Machine Learning Research, 3, 1289-1305.
Kuzman Ganchev, Fernando Pereira, Mark Mandel,
Steven Carroll and Peter White. 2007. Semi-
automated named entity annotation. In Proceedings
of the linguistic annotation workshop, pp. 53-56
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann and Ian H.Witten. 2009.
The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Taka Hiraishi, Buruhani Nyenzi, Jim Penman and Semere
Habetsion. 2000. Quantifying uncertainties in prac-
tice. In Revised 1996 IPCC guidelines for national
greenhouse gas inventories.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL.
Janet Holmes. 1999. Women, men, and politeness. Lon-
don, SAGE publications, pp:86-96
Jooyoung Jang and Christian Schunn. 2012. Physical
design tools support and hinder innovative engineer-
ing design. Journal of Mechanical Design, vol. 134,
no. 4, pp. 041001-1-041001-9.
Shoushan Li, Shengfeng Ju, Guodong Zhou and Xiaojun
Li. 2012. Active learning for imbalanced sentiment
classification. EMNLP-CoNLL 2012: 139-148
Rada Mihalcea. 2004. Co-training and self-training for
word sense disambiguation. In Proceedings of the
8th Conference on Computational Natural Language
Learning (CoNLL, Boston, MA). 33-40.
Susannah B. F. Paletz and Christian D. Schunn. 2011.
Assessing group-level participation in fluid teams:
Testing a new metric. Behav Res 43:522-536.
Heather Pon-Barry and Stuart M. Shieber 2011. Rec-
ognizing uncertainty in speech. EURASIP Journal on
Advances in Signal Processing.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow and Benjamin Van Durme 2012 Sta-
tistical modality tagging from rule-based annotations
and crowdsourcing. In Proceedings of ACL Workshop
on Extra-propositional aspects of meaning in compu-
tational linguistics (ExProM).
14
Ines Rehbein, Josef Ruppenhofer and Caroline Sporleder.
2012. Is it worth the effort? Assessing the benefits of
partial automatic pre-labeling for frame-semantic an-
notation. Language Resources and Evaluation, Vol.46,
No.1. pp. 1-23
Joel R. Tetreault and Martin Chodorow. Native judg-
ments of non-native usage: experiments in preposi-
tion error detection. In Proceedings of the Workshop
on Human Judgements in Computational Linguistics,
p.24-32, Manchester, United Kingdom.
Irene V. Tollinger, Christian D. Schunn and Alonso H.
Vera. 2006. What changes when a large team becomes
more expert? Analyses of speedup in the Mars Explo-
ration Rovers science planning process. In Proceed-
ings of the 28th Annual Conference of the Cognitive
Science Society (pp. 840-845). Mahwah, NJ: Erlbaum.
Nianwen Xue, Fu-Dong Chiou andMartha Palmer. 2002.
Building a large-scale annotated chinese corpus. In
Proceedings of the 19th international conference on
Computational linguistics. ACL.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of
the 2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Nat-
ural Language Learning, 783-790.
Jingbo Zhu, Huizhen Wang, Eduard H. Hovy and
Matthew Y. Ma. 2010. Confidence-based stopping
criteria for active learning for data annotation. ACM
Transactions on Speech and Language Processing, 6,
124.
15
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 502?507,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatically Predicting Peer-Review Helpfulness
Wenting Xiong
University of Pittsburgh
Department of Computer Science
Pittsburgh, PA, 15260
wex12@cs.pitt.edu
Diane Litman
University of Pittsburgh
Department of Computer Science &
Learning Research and Development Center
Pittsburgh, PA, 15260
litman@cs.pitt.edu
Abstract
Identifying peer-review helpfulness is an im-
portant task for improving the quality of feed-
back that students receive from their peers. As
a first step towards enhancing existing peer-
review systems with new functionality based
on helpfulness detection, we examine whether
standard product review analysis techniques
also apply to our new context of peer reviews.
In addition, we investigate the utility of in-
corporating additional specialized features tai-
lored to peer review. Our preliminary results
show that the structural features, review uni-
grams and meta-data combined are useful in
modeling the helpfulness of both peer reviews
and product reviews, while peer-review spe-
cific auxiliary features can further improve
helpfulness prediction.
1 Introduction
Peer reviewing of student writing has been widely
used in various academic fields. While existing
web-based peer-review systems largely save instruc-
tors effort in setting up peer-review assignments and
managing document assignment, there still remains
the problem that the quality of peer reviews is of-
ten poor (Nelson and Schunn, 2009). Thus to en-
hance the effectiveness of existing peer-review sys-
tems, we propose to automatically predict the help-
fulness of peer reviews.
In this paper, we examine prior techniques that
have been used to successfully rank helpfulness for
product reviews, and adapt them to the peer-review
domain. In particular, we use an SVM regression al-
gorithm to predict the helpfulness of peer reviews
based on generic linguistic features automatically
mined from peer reviews and students? papers, plus
specialized features based on existing knowledge
about peer reviews. We not only demonstrate that
prior techniques from product reviews can be suc-
cessfully tailored to peer reviews, but also show the
importance of peer-review specific features.
2 Related Work
Prior studies of peer review in the Natural Lan-
guage Processing field have not focused on help-
fulness prediction, but instead have been concerned
with issues such as highlighting key sentences in pa-
pers (Sandor and Vorndran, 2009), detecting impor-
tant feedback features in reviews (Cho, 2008; Xiong
and Litman, 2010), and adapting peer-review assign-
ment (Garcia, 2010). However, given some simi-
larity between peer reviews and other review types,
we hypothesize that techniques used to predict re-
view helpfulness in other domains can also be ap-
plied to peer reviews. Kim et al (2006) used re-
gression to predict the helpfulness ranking of prod-
uct reviews based on various classes of linguistic
features. Ghose and Ipeirotis (2010) further exam-
ined the socio-economic impact of product reviews
using a similar approach and suggested the useful-
ness of subjectivity analysis. Another study (Liu
et al, 2008) of movie reviews showed that helpful-
ness depends on reviewers? expertise, their writing
style, and the timeliness of the review. Tsur and
Rappoport (2009) proposed RevRank to select the
most helpful book reviews in an unsupervised fash-
ion based on review lexicons. However, studies of
Amazon?s product reviews also show that the per-
502
Class Label Features
Structural STR review length in terms of tokens, number of sentences, percentage of sentences
that end with question marks, number of exclamatory sentences.
Lexical UGR, BGR tf-idf statistics of review unigrams and bigrams.
Syntactic SYN
percentage of tokens that are nouns, verbs, verbs conjugated in the
first person, adjectives / adverbs and open classes, respectively.
Semantic
TOP, counts of topic words,
posW, negW counts of positive and negative sentiment words.
Meta-data MET
the overall ratings of papers assigned by reviewers, and the absolute
difference between the rating and the average score given by all reviewers.
Table 1: Generic features motivated by related work of product reviews (Kim et al, 2006).
ceived helpfulness of a review depends not only on
its review content, but also on social effects such as
product qualities, and individual bias in the presence
of mixed opinion distribution (Danescu-Niculescu-
Mizil et al, 2009).
Nonetheless, several properties distinguish our
corpus of peer reviews from other types of reviews:
1) The helpfulness of our peer reviews is directly
rated using a discrete scale from one to five instead
of being defined as a function of binary votes (e.g.
the percentage of ?helpful? votes (Kim et al, 2006));
2) Peer reviews frequently refer to the related stu-
dents? papers, thus review analysis needs to take into
account paper topics; 3) Within the context of edu-
cation, peer-review helpfulness often has a writing
specific semantics, e.g. improving revision likeli-
hood; 4) In general, peer-review corpora collected
from classrooms are of a much smaller size com-
pared to online product reviews. To tailor existing
techniques to peer reviews, we will thus propose
new specialized features to address these issues.
3 Data and Features
In this study, we use a previously annotated peer-
review corpus (Nelson and Schunn, 2009; Patchan
et al, 2009), collected using a freely available web-
based peer-review system (Cho and Schunn, 2007)
in an introductory college history class. The corpus
consists of 16 papers (about six pages each) and 267
reviews (varying from twenty words to about two
hundred words). Two experts (a writing instructor
and a content instructor) (Patchan et al, 2009) were
asked to rate the helpfulness of each peer review
on a scale from one to five (Pearson correlation
r = 0.425, p < 0.01). For our study, we consider
the average ratings given by the two experts (which
roughly follow a normal distribution) as the gold
standard of review helpfulness. Two example rated
peer reviews (shown verbatim) follow:
A helpful peer review of average-rating 5:
The support and explanation of the ideas could use
some work. broading the explanations to include all
groups could be useful. My concerns come from some
of the claims that are put forth. Page 2 says that the
13th amendment ended the war. is this true? was there
no more fighting or problems once this amendment was
added? ...
The arguments were sorted up into paragraphs,
keeping the area of interest clear, but be careful about
bringing up new things at the end and then simply leaving
them there without elaboration (ie black sterilization at
the end of the paragraph).
An unhelpful peer review of average-rating 1:
Your paper and its main points are easy to find and to
follow.
As shown in Table 1, we first mine generic
linguistic features from reviews and papers based
on the results of syntactic analysis of the texts,
aiming to replicate the feature sets used by Kim et
al. (2006). While structural, lexical and syntactic
features are created in the same way as suggested
in their paper, we adapt the semantic and meta-data
features to peer reviews by converting the mentions
of product properties to mentions of the history
topics and by using paper ratings assigned by peers
instead of product scores.1
1We used MSTParser (McDonald et al, 2005) for syntactic
analysis. Topic words are automatically extracted from all stu-
503
In addition, the following specialized features are
motivated by an empirical study in cognitive sci-
ence (Nelson and Schunn, 2009), which suggests
that students? revision likelihood is significantly cor-
related with certain feedback features, and by our
prior work (Xiong and Litman, 2010; Xiong et
al., 2010) for detecting these cognitive science con-
structs automatically:
Cognitive-science features (cogS): For a given
review, cognitive-science constructs that are signifi-
cantly correlated with review implementation likeli-
hood are manually coded for each idea unit (Nel-
son and Schunn, 2009) within the review. Note,
however, that peer-review helpfulness is rated for
the whole review, which can include multiple idea
units.2 Therefore in our study, we calculate the dis-
tribution of feedbackType values (praise, problem,
and summary) (kappa = .92), the percentage of
problems that have problem localization ?the pres-
ence of information indicating where the problem is
localized in the related paper? (kappa = .69), and
the percentage of problems that have a solution ?
the presence of a solution addressing the problem
mentioned in the review? (kappa = .79) to model
peer-review helpfulness. These kappa values (Nel-
son and Schunn, 2009) were calculated from a sub-
set of the corpus for evaluating the reliability of hu-
man annotations3. Consider the example of the help-
ful review presented in Section 3 which was manu-
ally separated into two idea units (each presented in
a separate paragraph). As both ideas are coded as
problem with the presence of problem localization
and solution, the cognitive-science features of this
review are praise%=0, problem%=1, summary%=0,
localization%=1, and solution%=1.
Lexical category features (LEX2): Ten cate-
gories of keyword lexicons developed for automat-
ically detecting the previously manually annotated
feedback types (Xiong et al, 2010). The categories
are learned in a semi-supervised way based on syn-
tactic and semantic functions, such as suggestion
dents? papers using topic signature (Lin and Hovy, 2000) soft-
ware kindly provided by Annie Louis. Positive and negative
sentiment words are extracted from the General Inquirer Dic-
tionaries (http://www.wjh.harvard.edu/ inquirer/homecat.htm).
2Details of different granularity levels of annotation can be
found in (Nelson and Schunn, 2009).
3These annotators are not the same experts who rated the
peer-review helpfulness.
modal verbs (e.g. should, must, might, could, need),
negations (e.g. not, don?t, doesn?t), positive and neg-
ative words, and so on. We first manually created
a list of words that were specified as signal words
for annotating feedbackType and problem localiza-
tion in the coding manual; then we supplemented
the list with words selected by a decision tree model
learned using a Bag-of-Words representation of the
peer reviews. These categories will also be helpful
for reducing the feature space size as discussed be-
low.
Localization features (LOC): Five features de-
veloped in our prior work (Xiong and Litman, 2010)
for automatically identifying the manually coded
problem localization tags, such as the percentage of
problems in reviews that could be matched with a
localization pattern (e.g. ?on page 5?, ?the section
about?), the percentage of sentences in which topic
words exist between the subject and object, etc.
4 Experiment and Results
Following Kim et al (2006), we train our helpful-
ness model using SVM regression with a radial ba-
sis function kernel provided by SVMlight (Joachims,
1999). We first evaluate each feature type in iso-
lation to investigate its predictive power of peer-
review helpfulness; we then examine them together
in various combinations to find the most useful fea-
ture set for modeling peer-review helpfulness. Per-
formance is evaluated in 10-fold cross validation
of our 267 peer reviews by predicting the absolute
helpfulness scores (with Pearson correlation coeffi-
cient r) as well as by predicting helpfulness rank-
ing (with Spearman rank correlation coefficient rs).
Although predicted helpfulness ranking could be di-
rectly used to compare the helpfulness of a given set
of reviews, predicting helpfulness rating is desirable
in practice to compare helpfulness between existing
reviews and new written ones without reranking all
previously ranked reviews. Results are presented re-
garding the generic features and the specialized fea-
tures respectively, with 95% confidence bounds.
4.1 Performance of Generic Features
Evaluation of the generic features is presented in
Table 2, showing that all classes except syntac-
tic (SYN) and meta-data (MET) features are sig-
504
nificantly correlated with both helpfulness rating
(r) and helpfulness ranking (rs). Structural fea-
tures (bolded) achieve the highest Pearson (0.60)
and Spearman correlation coefficients (0.59) (al-
though within the significant correlations, the dif-
ference among coefficients are insignificant). Note
that in isolation, MET (paper ratings) are not sig-
nificantly correlated with peer-review helpfulness,
which is different from prior findings of product re-
views (Kim et al, 2006) where product scores are
significantly correlated with product-review help-
fulness. However, when combined with other fea-
tures, MET does appear to add value (last row).
When comparing the performance between predict-
ing helpfulness ratings versus ranking, we observe
r ? rs consistently for our peer reviews, while Kim
et al (2006) reported r < rs for product reviews.4
Finally, we observed a similar feature redundancy
effect as Kim et al (2006) did, in that simply com-
bining all features does not improve the model?s per-
formance. Interestingly, our best feature combina-
tion (last row) is the same as theirs. In sum our
results verify our hypothesis that the effectiveness
of generic features can be transferred to our peer-
review domain for predicting review helpfulness.
Features Pearson r Spearman rs
STR 0.60? 0.10* 0.59? 0.10*
UGR 0.53? 0.09* 0.54? 0.09*
BGR 0.58? 0.07* 0.57? 0.10*
SYN 0.36? 0.12 0.35? 0.11
TOP 0.55? 0.10* 0.54? 0.10*
posW 0.57? 0.13* 0.53? 0.12*
negW 0.49? 0.11* 0.46? 0.10*
MET 0.22? 0.15 0.23? 0.12
All-combined 0.56? 0.07* 0.58? 0.09*
STR+UGR+MET
0.61? 0.10* 0.61? 0.10*
+TOP
STR+UGR+MET 0.62? 0.10* 0.61? 0.10*
Table 2: Performance evaluation of the generic features
for predicting peer-review helpfulness. Significant results
are marked by * (p ? 0.05).
4.2 Analysis of the Specialized Features
Evaluation of the specialized features is shown in
Table 3, where all features examined are signifi-
4The best performing single feature type reported (Kim et
al., 2006) was review unigrams: r = 0.398 and rs = 0.593.
cantly correlated with both helpfulness rating and
ranking. When evaluated in isolation, although
specialized features have weaker correlation coeffi-
cients ([0.43, 0.51]) than the best generic features,
these differences are not significant, and the special-
ized features have the potential advantage of being
theory-based. The use of features related to mean-
ingful dimensions of writing has contributed to va-
lidity and greater acceptability in the related area of
automated essay scoring (Attali and Burstein, 2006).
When combined with some generic features, the
specialized features improve the model?s perfor-
mance in terms of both r and rs compared to
the best performance in Section 4.1 (the baseline).
Though the improvement is not significant yet, we
think it still interesting to investigate the potential
trend to understand how specialized features cap-
ture additional information of peer-review helpful-
ness. Therefore, the following analysis is also pre-
sented (based on the absolute mean values), where
we start from the baseline feature set, and gradually
expand it by adding our new specialized features:
1) We first replace the raw lexical unigram features
(UGR) with lexical category features (LEX2), which
slightly improves the performance before rounding
to the significant digits shown in row 5. Note that
the categories not only substantially abstract lexical
information from the reviews, but also carry simple
syntactic and semantic information. 2) We then add
one semantic class ? topic words (row 6), which en-
hances the performance further. Semantic features
did not help when working with generic lexical fea-
tures in Section 4.1 (second to last row in Table 2),
but they can be successfully combined with the lexi-
cal category features and further improve the perfor-
mance as indicated here. 3) When cognitive-science
and localization features are introduced, the predic-
tion becomes even more accurate, which reaches a
Pearson correlation of 0.67 and a Spearman correla-
tion of 0.67 (Table 3, last row).
5 Discussion
Despite the difference between peer reviews and
other types of reviews as discussed in Section 2,
our work demonstrates that many generic linguistic
features are also effective in predicting peer-review
helpfulness. The model?s performance can be alter-
505
Features Pearson r Spearman rs
cogS 0.43? 0.09 0.46? 0.07
LEX2 0.51? 0.11 0.50? 0.10
LOC 0.45? 0.13 0.47? 0.11
STR+MET+UGR
0.62? 0.10 0.61? 0.10
(Baseline)
STR+MET+LEX2 0.62? 0.10 0.61? 0.09
STR+MET+LEX2+
0.65? 0.10 0.66? 0.08
TOP
STR+MET+LEX2+
0.66? 0.09 0.66? 0.08
TOP+cogS
STR+MET+LEX2+
0.67? 0.09 0.67? 0.08
TOP+cogS+LOC
Table 3: Evaluation of the model?s performance (all sig-
nificant) after introducing the specialized features.
natively achieved and further improved by adding
auxiliary features tailored to peer reviews. These
specialized features not only introduce domain ex-
pertise, but also capture linguistic information at an
abstracted level, which can help avoid the risk of
over-fitting. Given only 267 peer reviews in our
case compared to more than ten thousand product
reviews (Kim et al, 2006), this is an important con-
sideration.
Though our absolute quantitative results are
not directly comparable to the results of Kim et
al. (2006), we indirectly compared them by ana-
lyzing the utility of features in isolation and com-
bined. While STR+UGR+MET is found as the best
combination of generic features for both types of
reviews, the best individual feature type is differ-
ent (review unigrams work best for product reviews;
structural features work best for peer reviews). More
importantly, meta-data, which are found to signif-
icantly affect the perceived helpfulness of product
reviews (Kim et al, 2006; Danescu-Niculescu-Mizil
et al, 2009), have no predictive power for peer re-
views. Perhaps because the paper grades and other
helpfulness ratings are not visible to the reviewers,
we have less of a social dimension for predicting
the helpfulness of peer reviews. We also found that
SVM regression does not favor ranking over predict-
ing helpfulness as in (Kim et al, 2006).
6 Conclusions and Future Work
The contribution of our work is three-fold: 1) Our
work successfully demonstrates that techniques used
in predicting product review helpfulness ranking can
be effectively adapted to the domain of peer reviews,
with minor modifications to the semantic and meta-
data features. 2) Our qualitative comparison shows
that the utility of generic features (e.g. meta-data
features) in predicting review helpfulness varies be-
tween different review types. 3) We further show
that prediction performance could be improved by
incorporating specialized features that capture help-
fulness information specific to peer reviews.
In the future, we would like to replace the man-
ually coded peer-review specialized features (cogS)
with their automatic predictions, since we have al-
ready shown in our prior work that some impor-
tant cognitive-science constructs can be successfully
identified automatically.5 Also, it is interesting to
observe that the average helpfulness ratings assigned
by experts (used as the gold standard in this study)
differ from those given by students. Prior work on
this corpus has already shown that feedback fea-
tures of review comments differ not only between
students and experts, but also between the writing
and the content experts (Patchan et al, 2009). While
Patchan et al (2009) focused on the review com-
ments, we hypothesize that there is also a difference
in perceived peer-review helpfulness. Therefore, we
are planning to investigate the impact of these dif-
ferent helpfulness ratings on the utilities of features
used in modeling peer-review helpfulness. Finally,
we would like to integrate our helpfulness model
into a web-based peer-review system to improve the
quality of both peer reviews and paper revisions.
Acknowledgements
This work was supported by the Learning Research
and Development Center at the University of Pitts-
burgh. We thank Melissa Patchan and Christian D.
Schunn for generously providing the manually an-
notated peer-review corpus. We are also grateful to
Christian D. Schunn, Janyce Wiebe, Joanna Drum-
mond, and Michael Lipschultz who kindly gave us
valuable feedback while writing this paper.
5The accuracy rate is 0.79 for predicting feedbackType, 0.78
for problem localization, and 0.81 for solution on the same his-
tory data set.
506
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. In Michael Russell, editor,
The Journal of Technology, Learning and Assessment
(JTLA), volume 4, February.
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A web-
based reciprocal peer review system. In Computers
and Education, volume 48, pages 409?426.
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings of the First In-
ternational Conference on Educational Data Mining
(EDM2008), pages 192?196.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinber g, and Lillian Lee. 2009. How opin-
ions are received by online communities: A case study
on Amazon .com helpfulness votes. In Proceedings of
WWW, pages 141?150.
Raquel M. Crespo Garcia. 2010. Exploring document
clustering techniques for personalized peer assessment
in exploratory courses. In Proceedings of Computer-
Supported Peer Review in Education (CSPRED) Work-
shop in the Tenth International Conference on Intelli-
gent Tutoring Systems (ITS 2010).
Anindya Ghose and Panagiotis G. Ipeirotis. 2010. Esti-
mating the helpfulness and economic impact of prod-
uct reviews: Mining text and reviewer characteristics.
In IEEE Transactions on Knowledge and Data Engi-
neering, volume 99, Los Alamitos, CA, USA. IEEE
Computer Society.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press, Cambridge, MA,
USA.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP2006), pages 423?430, Sydney, Australia,
July.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th conference on
Computational linguistics, volume 1 of COLING ?00,
pages 495?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu.
2008. Modeling and predicting the helpfulness of on-
line reviews. In Proceedings of the Eighth IEEE Inter-
national Conference on Data Mining, pages 443?452,
Los Alamitos, CA, USA.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 91?98, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. In Instructional Sci-
ence, volume 37, pages 375?401.
Melissa M. Patchan, Davida Charney, and Christian D.
Schunn. 2009. A validation study of students? end
comments: Comparing comments by students, a writ-
ing instructor, and a content instructor. In Journal of
Writing Research, volume 1, pages 124?152. Univer-
sity of Antwerp.
Agnes Sandor and Angela Vorndran. 2009. Detect-
ing key sentences for automatic assistance in peer-
reviewing research articles in educational sciences. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP), pages 36?44.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most helpful
book reviews. In Proceedings of the Third Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM2009), pages 36?44.
Wenting Xiong and Diane J. Litman. 2010. Identifying
problem localization in peer-review feedback. In Pro-
ceedings of Tenth International Conference on Intelli-
gent Tutoring Systems (ITS2010), volume 6095, pages
429?431.
Wenting Xiong, Diane J. Litman, and Christian D.
Schunn. 2010. Assessing reviewers performance
based on mining problem localization in peer-review
data. In Proceedings of the Third International Con-
ference on Educational Data Mining (EDM2010),
pages 211?220.
507
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?19,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Understanding Differences in Perceived Peer-Review Helpfulness using
Natural Language Processing
Wenting Xiong
University of Pittsburgh
Department of Computer Science
Pittsburgh, PA, 15260
wex12@cs.pitt.edu
Diane Litman
University of Pittsburgh
Department of Computer Science &
Learning Research and Development Center
Pittsburgh, PA, 15260
litman@cs.pitt.edu
Abstract
Identifying peer-review helpfulness is an im-
portant task for improving the quality of feed-
back received by students, as well as for help-
ing students write better reviews. As we tailor
standard product review analysis techniques to
our peer-review domain, we notice that peer-
review helpfulness differs not only between
students and experts but also between types
of experts. In this paper, we investigate how
different types of perceived helpfulness might
influence the utility of features for automatic
prediction. Our feature selection results show
that certain low-level linguistic features are
more useful for predicting student perceived
helpfulness, while high-level cognitive con-
structs are more effective in modeling experts?
perceived helpfulness.
1 Introduction
Peer review of writing is a commonly recommended
technique to include in good writing instruction. It
not only provides more feedback compared to what
students might get from their instructors, but also
provides opportunities for students to practice writ-
ing helpful reviews. While existing web-based peer-
review systems facilitate peer review from the logis-
tic aspect (e.g. collecting papers from authors, as-
signing reviewers, and sending reviews back), there
still remains the problem that the quality of peer
reviews varies, and potentially good feedback is
not written in a helpful way. To address this is-
sue, we propose to add a peer-review helpfulness
model to current peer-review systems, to automat-
ically predict peer-review helpfulness based on fea-
tures mined from textual reviews using Natural Lan-
guage Processing (NLP) techniques. Such an intel-
ligent component could enable peer-review systems
to 1) control the quality of peer reviews that are sent
back to authors, so authors can focus on the help-
ful ones; and 2) provide feedback to reviewers with
respect to their reviewing performance, so students
can learn to write better reviews.
In our prior work (Xiong and Litman, 2011), we
examined whether techniques used for predicting the
helpfulness of product reviews (Kim et al, 2006)
could be tailored to our peer-review domain, where
the definition of helpfulness is largely influenced by
the educational context of peer review. While previ-
ously we used the average of two expert-provided
ratings as our gold standard of peer-review help-
fulness1, there are other types of helpfulness rating
(e.g. author perceived helpfulness) that could be the
gold standard, and that could potentially impact the
features used to build the helpfulness model. In fact,
we observe that peer-review helpfulness seems to
differ not only between students and experts (exam-
ple 1), but also between types of experts (example
2).
In the following examples, students judge helpful-
ness with discrete ratings from one to seven; experts
judge it using a one to five scale. Higher ratings on
both scales correspond to the most helpful reviews.
Example 1:
Student rating = 7, Average expert rating = 2 The
1Averaged ratings are considered more reliable since they
are less noisy.
10
author also has great logic in this paper. How can
we consider the United States a great democracy
when everyone is not treated equal. All of the main
points were indeed supported in this piece.
Student rating = 3, Average expert rating = 5 I
thought there were some good opportunities to
provide further data to strengthen your argument.
For example the statement ?These methods of
intimidation, and the lack of military force offered
by the government to stop the KKK, led to the
rescinding of African American democracy.?
Maybe here include data about how . . . (126 words)
Example 2:
Writing-expert rating = 2, Content-expert rating = 5
Your over all arguements were organized in some
order but was unclear due to the lack of thesis in
the paper. Inside each arguement, there was no
order to the ideas presented, they went back and
forth between ideas. There was good support to
the arguements but yet some of it didnt not fit your
arguement.
Writing-expert rating = 5, Content-expert rating = 2
First off, it seems that you have difficulty writing
transitions between paragraphs. It seems that you
end your paragraphs with the main idea of each
paragraph. That being said, . . . (173 words) As a
final comment, try to continually move your paper,
that is, have in your mind a logical flow with every
paragraph having a purpose.
To better understand such differences and inves-
tigate their impact on automatically assessing peer-
review helpfulness, in this paper, we compare help-
fulness predictions using our many different pos-
sibilities for gold standard ratings. In particular,
we compare the predictive ability of features across
gold standard ratings by examining the most use-
ful features and feature ranks using standard feature
selection techniques. We show that paper ratings
and lexicon categories that suggest clear transitions
and opinions are most useful in predicting helpful-
ness as perceived by students, while review length
is generally effective in predicting expert helpful-
ness. While the presence of praise and summary
comments are more effective in modeling writing-
expert helpfulness, providing solutions is more use-
ful in predicting content-expert helpfulness.
2 Related Work
To our knowledge, no prior work on peer review
from the NLP community has attempted to auto-
matically predict peer-review helpfulness. Instead,
the NLP community has focused on issues such as
highlighting key sentences in papers (Sandor and
Vorndran, 2009), detecting important feedback fea-
tures in reviews (Cho, 2008; Xiong and Litman,
2010), and adapting peer-review assignment (Gar-
cia, 2010). However, many NLP studies have been
done on the helpfulness of other types of reviews,
such as product reviews (Kim et al, 2006; Ghose
and Ipeirotis, 2010), movie reviews (Liu et al,
2008), book reviews (Tsur and Rappoport, 2009),
etc. Kim et al (2006) used regression to predict the
helpfulness ranking of product reviews based on var-
ious classes of linguistic features. Ghose and Ipeiro-
tis (2010) further examined the socio-economic im-
pact of product reviews using a similar approach
and suggested the usefulness of subjectivity analy-
sis. Another study (Liu et al, 2008) of movie re-
views showed that helpfulness depends on review-
ers? expertise, their writing style, and the timeliness
of the review. Tsur and Rappoport (2009) proposed
RevRank to select the most helpful book reviews in
an unsupervised fashion based on review lexicons.
To tailor the utility of this prior work on help-
fulness prediction to educational peer reviews, we
will draw upon research on peer review in cognitive
science. One empirical study of the nature of peer-
review feedback (Nelson and Schunn, 2009) found
that feedback implementation likelihood is signif-
icantly correlated with five feedback features. Of
these features, problem localization ?pinpointing
the source of the problem and/or solution in the orig-
inal paper? and solution ?providing a solution to
the observed problem? were found to be most im-
portant. Researchers (Cho, 2008; Xiong and Lit-
man, 2010) have already shown that some of these
constructs can be automatically learned from tex-
tual input using Machine Learning and NLP tech-
niques. In addition to investigating what proper-
ties of textual comments make peer-review helpful,
researchers also examined how the comments pro-
duced by students versus by different types of ex-
perts differ (Patchan et al, 2009). Though focusing
on differences between what students and experts
11
produce, such work sheds light on our study of stu-
dents? and experts? helpfulness ratings of the same
student comments (i.e. what students and experts
value).
Our work in peer-review helpfulness prediction
integrates the NLP techniques and cognitive-science
approaches mentioned above. We will particularly
focus on examining the utility of features motivated
by related work from both areas, with respect to dif-
ferent types of gold standard ratings of peer-review
helpfulness for automatic prediction.
3 Data
In this study, we use a previously annotated peer-
review corpus (Nelson and Schunn, 2009; Patchan
et al, 2009) that was collected in an introduc-
tory college history class using the freely available
web-based peer-review SWoRD (Scaffolded Writ-
ing and Rewriting in the Discipline) system (Cho
and Schunn, 2007). The corpus consists of 16 pa-
pers (about six pages each) and 189 reviews (vary-
ing from twenty words to about two hundred words)
accompanied by numeric ratings of the papers. Each
review was manually segmented into idea units (de-
fined as contiguous feedback referring to a single
topic) (Nelson and Schunn, 2009), and these idea
units were then annotated by two independent an-
notators for various coding categories, such as feed-
back type (praise, problem, and summary), problem
localization, solution, etc. For example, the sec-
ond case in Example 1, which only has one idea
unit, was annotated as feedbackType = problem,
problemlocalization = True, and solution =
True. The agreement (Kappa) between the two an-
notators is 0.92 for FeedbackType, 0.69 for localiza-
tion, and 0.89 for solution.2
Our corpus also contains author provided back
evaluations. At the end of the peer-review assign-
ment, students were asked to provide back evalu-
ation on each review that they received by rating
review helpfulness using a discrete scale from one
to seven. After the corpus was collected, one writ-
2For Kappa value interpretation, Landis and Koch (1977)
propose the following agreement standard: 0.21-0.40 = ?Fair?;
0.41-0.60 = ?Moderate?; 0.61-0.80 = ?Substantial?; 0.81-1.00
= ?Almost Perfect?. Thus, while localization signals are more
difficult to annotate, the inter-annotator agreement is still sub-
stantial.
ing expert and one content expert were also asked to
rate review helpfulness with a slightly different scale
from one to five. For our study, we will also com-
pute the average ratings given by the two experts,
yielding four types of possible gold-standard ratings
of peer-review helpfulness for each review. Figure 1
shows the rating distribution of each type. Interest-
ingly, we observed that expert ratings roughly follow
a normal distribution, while students are more likely
to give higher ratings (as illustrated in Figure 1).
4 Features
Our features are motivated by the prior work in-
troduced in Section 2, in particular, NLP work on
predicting product-review helpfulness (Kim et al,
2006), as well as work on automatically learning
cognitive-science constructs (Nelson and Schunn,
2009) using NLP (Cho, 2008; Xiong and Litman,
2010). The complete list of features is shown in Ta-
ble 3 and described below. The computational lin-
guistic features are automatically extracted based
on the output of syntactic analysis of reviews and
papers3. These features represent structural, lexi-
cal, syntactic and semantic information of the tex-
tual content, and also include information for identi-
fying certain important cognitive constructs:
? Structural features consider the general struc-
ture of reviews, which includes review length in
terms of tokens (reviewLength), number of sen-
tences (sentNum), the average sentence length
(sentLengthAve), percentage of sentences that
end with question marks (question%), and
number of exclamatory sentences (exclams).
? Lexical features are counts of ten lexical cat-
egories (Table 1), where the categories were
learned in a semi-supervised way from review
lexicons in a pilot study. We first manually cre-
ated a list of words that were specified as signal
words for annotating feedbackType and prob-
lem localization in the coding manual; then
we supplemented the list with words selected
by a decision tree model learned using a Bag-
of-Words representation of the peer reviews.
3We used MSTParser (McDonald et al, 2005) for syntactic
analysis.
12
Figure 1: Distribution of peer-review helpfulness when rated by students and experts
Tag Meaning Word list
SUG suggestion should, must, might, could, need, needs, maybe, try, revision, want
LOC location page, paragraph, sentence
ERR problem error, mistakes, typo, problem, difficulties, conclusion
IDE idea verb consider, mention
LNK transition however, but
NEG negative words fail, hard, difficult, bad, short, little, bit, poor, few, unclear, only, more
POS positive words great, good, well, clearly, easily, effective, effectively, helpful, very
SUM summarization main, overall, also, how, job
NOT negation not, doesn?t, don?t
SOL solution revision specify correction
Table 1: Ten lexical categories
Compared with commonly used lexical uni-
grams and bigrams (Kim et al, 2006), these
lexical categories are equally useful in model-
ing peer-review helpfulness, and significantly
reduce the feature space.4
? Syntactic features mainly focus on nouns and
verbs, and include percentage of tokens that are
nouns, verbs, verbs conjugated in the first per-
son (1stPVerb%), adjectives/adverbs, and open
classes, respectively.
? Semantic features capture two important peer-
4Lexical categories help avoid the risk of over-fitting, given
only 189 peer reviews in our case compared to more than ten
thousand Amazon.com reviews used for predicting product re-
view helpfulness (Kim et al, 2006).
review properties: their relevance to the main
topics in students? papers, and their opinion
sentiment polarities. Kim et al (2006) ex-
tracted product property keywords from exter-
nal resources based on their hypothesis that
helpful product reviews refer frequently to cer-
tain product properties. Similarly, we hypothe-
size that helpful peer reviews are closely related
to domain topics that are shared by all students
papers in an assignment. Our domain topic set
contains 288 words extracted from the collec-
tion of student papers using topic-lexicon ex-
traction software5; our feature (domainWord)
5The software extracts topic words based on topic signa-
tures (Lin and Hovy, 2000), and was kindly provided by Annie
Louis.
13
Feature Description
regTag% The percentage of problems in reviews that could be matched with a localization pattern.
soDomain% The percentage of sentences where any domain word appears between the subject and the object.
dDeterminer The number of demonstrative determiners.
windowSize For each review sentence, we search for the most likely referred window of words in the relatedpaper, and windowSize is the average number of words of all windows.
Table 2: Localization features
counts how many words of a given review be-
long to the extracted set. For sentiment po-
larities, we extract positive and negative sen-
timent words from the General Inquirer Dictio-
naries 6, and count their appearance in reviews
in terms of their sentiment polarity (posWord,
negWord).
? Localization features are motivated by lin-
guistic features that are used for automatically
predicting problem localization (an important
cognitive construct for feedback understand-
ing and implementation) (Nelson and Schunn,
2009), and are presented in Table 2. To illus-
trate how these features are computed, consider
the following critique:
The section of the essay on African
Americans needs more careful at-
tention to the timing and reasons
for the federal governments decision
to stop protecting African American
civil and political rights.
The review has only one sentence, in which one
regular expression is matched with ?the section
of? thus regTag% = 1; no demonstrative de-
terminer, thus dDeterminer = 0; ?African?
and ?Americans? are domain words appearing
between the subject ?section? and the object
?attention?, so soDomain is true for this sen-
tence and thus soDomain% = 1 for the given
review.
In addition to the low-level linguistic features pre-
sented above, we also examined non-linguistic fea-
tures that are derived from the ratings and prior
manual annotations of the corpus, described in Sec-
tion 3.
6http://www.wjh.harvard.edu/ inquirer/homecat.htm
? Cognitive-science features are motivated by
an empirical study (Nelson and Schunn, 2009)
which suggests significant correlation between
certain cognitive constructs (e.g. feedbackType,
problem localization, solution) and review im-
plementation likelihood. Intuitively, helpful
reviews are more likely to get implemented,
thus we introduced these features to capture
desirable high-level characteristics of peer re-
views. Note that in our corpus these cogni-
tive constructs are manually coded at the idea-
unit level (Nelson and Schunn, 2009), how-
ever, peer-review helpfulness is rated at the re-
view level.7 Our cognitive-science features ag-
gregate the annotations up to the review-level
by reporting the percentage of idea-units in
a review that exhibit each characteristic: the
distribution of review types (praise%, prob-
lem%, summary%), the percentage of problem-
localized critiques (localization%), as well as
the percentage of solution-provided ones (solu-
tion%).
? Social-science features introduce elements re-
flecting interactions between students in a peer-
review assignment. As suggested in related
work on product review helpfulness (Kim
et al, 2006; Danescu-Niculescu-Mizil et al,
2009), some social dimensions (e.g. customer
opinion on related product quality) are of great
influence in the perceived helpfulness of prod-
uct reviews. Similarly, in our case, we intro-
duced related paper ratings (pRating) ? to con-
sider whether and how helpfulness ratings are
affected by the rating that the paper receives8
? and the absolute difference between the rat-
7Details of different granularity levels of annotation can be
found in (Nelson and Schunn, 2009).
8That is, to examine whether students give higher ratings to
peers who gave them higher paper ratings in the first place.
14
ing and the average score given by all review-
ers (pRatingDiff ) ? to measure the variation in
perceived helpfulness of a given review.
5 Experiments
We take a machine learning approach to model dif-
ferent types of perceived helpfulness (student help-
fulness, writing-expert helpfulness, content-expert
helpfulness, average-expert helpfulness) based on
combinations of linguistic and non-linguistic fea-
tures extracted from our peer-review corpus. Then
we compare the different helpfulness types in terms
of the predictive power of features used in their cor-
responding models. For comparison purpose, we
consider the linguistic and non-linguistic features
both separately and in combination, which generates
three set of features: 1) linguistic features, 2) non-
linguistic features, and 3) all features. For each set
of features, we train four models, each correspond-
ing to a different kind of helpfulness rating. For each
learning task (three by four), we use two standard
feature selection algorithms to find the most useful
features based on 10-fold cross validation. First, we
perform Linear Regression with Greedy Stepwise
search (stepwise LR) to select the most useful fea-
tures when testing in each of the ten folds, and count
how many times each features is selected in the ten
trials. Second, we use Relief Feature Evaluation9
with Ranker (Relief) (Kira and Rendell, 1992; Wit-
ten and Frank, 2005) to rank all used features based
on their average merits (the ability of the given fea-
ture to differentiate between two example pairs) of
ten trials.10
Although both methods are supervised, the wrap-
per is ?more aggressive? because its feature evalu-
ation is based on the performance of the regression
model and thus the resulting feature set is tailored
to the learning algorithm. In contrast, Relief does
not optimize feature sets directly for classifier per-
formance, thus it takes into account class informa-
tion in a ?less aggressive? manner than the Wrapper
method. We use both methods in our experiment to
9Relief evaluates the worth of an attribute by repeatedly
sampling an instance and changing the value of the given at-
tribute based on the nearest instance of the same and different
class.
10Both algorithms are provided by Weka
(http://www.cs.waikato.ac.nz/ml/weka/).
provide complementary perspectives. While the for-
mer can directly tell us what features are most use-
ful, the latter gives feature ranks which provide more
detailed information about differences between fea-
tures. To compare the feature selection results, we
examine the four kind of helpfulness models for
each of the three feature sets separately, as presented
below. Note that the focus of this paper is compar-
ing feature utilities in different helpfulness models
rather than predicting those types of helpfulness rat-
ings. (Details of how the average-expert model per-
forms can be found in our prior work (Xiong and
Litman, 2011).)
5.1 Feature Selection of Linguistic Features
Table 4 presents the feature selection results of com-
putational linguistic features used in modeling the
four different types of peer-review helpfulness. The
first row lists the four sources of helpfulness ratings,
and each column represents a corresponding model.
The second row presents the most useful features
in each model selected by stepwise LR, where ?#
of folds? refers to the number of trials in which the
given feature appears in the resulting feature set dur-
ing the 10-fold cross validation. Here we only report
features that are selected by no less than five folds
(half the time). The third row presents feature ranks
computed using Relief, where we only report the top
six features due to the space limit. Features are or-
dered in descending ranks, and the average merit and
its standard deviation is reported for each one of the
features.
The selection result of stepwise LR shows that
reviewLength is most useful for predicting expert
helpfulness in general, while specific lexicon cate-
gories (i.e. LNK, and NOT) and positive words (pos-
Word) are more useful in predicting student helpful-
ness. When looking at the ranking result, we observe
that transition cues (LNK) and posWord are also
ranked high in the student-helpfulness model, al-
though question% and suggestion words (SUG) are
ranked highest. For expert-helpfulness models, win-
dowSize and posWord, which are not listed in the se-
lected features for expert helpfulness (although they
are selected for students), are actually ranked high
for modeling average-expert helpfulness. While ex-
clamatory sentence number (exclams) and summa-
rization cues are ranked top for the writing expert,
15
Type Features
Structural reviewLength, sentNum, sentLengthAve, question%, exclams
Lexical SUG, LOC, ERR, IDE, LNK, NEG, POS, SUM, NOT, SOL (Table 1)
Syntactic noun%, verb%, 1stPVerb%, adj+adv%, opClass%
Semantic domainWord, posWord, negWord
Localization regTag%, soDomain%, dDeterminer, windowSize (Table 2)
Cognitive-science praise%, problem%, summary%, localization%, solution%
Social-science pRating, pRatingDiff
Table 3: Summary of features
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
LNK 9 reviewLength 8 reviewLength 10 reviewLength 10
Stepwise posWord 8 question% 6 sentNum 8
LR NOT 6 sentNum 5 question% 8
windowSize 6 1stPVerb% 5
POS 5
Relief
Feature Merit Feature Merit Feature Merit Feature Merit
question% .019? .002 exclams .010? .003 question% .010? .004 exclams .010? .003
SUG .015? .003 SUM .008? .004 ERR .009? .003 question% .011? .004
LNK .014? .003 NEG .006? .004 SUG .009? .004 windowSize .008? .002
sentLengthAve .012? .003 negWord .005? .002 posWord .007? .002 posWord .006? .002
POS .011? .002 windowSize .004? .002 exclams .006? .001 reviewLength .004? .001
posWord .010? .001 sentNum .003? .001 1stPVerb% .007? .004 sentLengthAve .004? .001
Table 4: Feature selection based on linguistic features
the percentage of questions (question%) and error
cues (ERR) are ranked top for the content-expert. In
addition, the percentage of words that are verbs con-
jugated in the first person (1stPVerb%) is both se-
lected and ranked high in the content-expert helpful-
ness model. Out of the four models, SUG are ranked
high for predicting both students and content-expert
helpfulness. These observations indicate that both
students and experts value questions (question%)
and suggestions (SUG) in reviews, and students par-
ticularly favor clear signs of logic flow in review ar-
guments (LNK), positive words (posWord), as well
as reference of their paper content which provides
explicit context information (windowSize). In addi-
tion, experts in general prefer longer reviews (re-
viewLength), and the writing expert thinks clear
summary signs (SUM) are important indicators of
helpful peer reviews.
5.2 Feature Selection of non-Linguistic
Features
When switching to the high-level non-linguistic fea-
tures (Table 5), we find that solution% is always se-
lected (in all ten trials) as a most useful feature for
predicting all four kind of helpfulness, and is also
ranked high for content-expert and student helpful-
ness. Especially for the content-expert, solution%
has a much higher merit (0.013) compared to all the
other features (? 0.03). This agrees with our ob-
servation in section 5.1 that SUG are ranked high in
both cases. localization% is selected as one of the
most useful features in the content-expert helpful-
ness model, which is also ranked top in the student
model (though not selected frequently by stepwise
LR). For modeling the writing-expert helpfulness,
praise (praise%) is more important than problem
and summary, and the paper rating (pRating) loses
its predictive power compared to how it works in the
other models. In contrast, pRating is both selected
and ranked high for predicting students? perceived
helpfulness.
5.3 Feature Selection of All Features
When considering all features together as reported
in Table 6, pRating is only selected in the student-
helpfulness model, and still remains to be the most
important feature for predicting students? perceived
helpfulness. As for experts, the structural feature
16
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
Stepwise pRating 10 solution% 10 localization% 10 solution% 10
LR solution% 10 solution% 10 pRating 10
problem% 9 pRating 10 localization% 9
Relief
Feature Merit Feature Merit Feature Merit Feature Merit
localization% .012? .003 praise% .008? .002 solution% .013? .005 problem% .004? .002
pRatingDiff .010? .002 problem% .007? .002 pRating .003? .002 localization% .004? .006
pRating .007? .002 summary% .001? .004 praise% .001? .002 praise% .003? .003
solution% .006? .005 localization% .001? .005 localization% .001? .004 solution% .002? .004
problem% .004? .002 pRating .004? .004 problem% .001? .002 pRating .005? .003
summary% .004? .003 pRatingDiff .007? .002 pRating .002? .003 pRatingDiff .006? .005
Table 5: Feature selection based on non-linguistic features
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
Stepwise pRating 10 reviewLength 10 reviewLength 10 reviewLength 10
LR dDeterminer 7 problem% 8 problem% 6
pRatingDiff 5
sentNum 5
Relief
Feature Merit Feature Merit Feature Merit Feature Merit
pRating .030? .006 exclams .016? .003 solution% .025? .003 exclams .015? .004
NOT .019? .004 praise% .015? .003 domainWord .012? .002 question% .012? .004
pRatingDiff .019? .005 SUM .013? .004 regTag% .012? .007 LOC .007? .002
sentNum .014? .002 summary% .008? .003 reviewLength .009? .002 sentNum .007? .002
question% .014? .003 problem% .009? .003 question% .010? .003 reviewLength .007? .001
NEG .013? .002 reviewLength .004? .001 sentNum .008? .002 praise% .008? .004
Table 6: Feature selection based on all features
reviewLength stands out from all other features in
both the writing-expert and the content-expert mod-
els. Interestingly, it is the number of sentences (sent-
Num) rather than review length of structure features
that is useful in the student-helpfulness model. And
demonstrative determiners (dDeterminer) is also se-
lected, which indicates that having a clear sign of
comment targets is considered important from the
students? perspective. When examining the model?s
ranking result, we find that more lexicon categories
are ranked high for students compared to other kind
of helpfulness. Specifically, NOT appears high
again, suggesting clear expression of opinion is im-
portant in predicting student-helpfulness. Across
four types of helpfulness, again, we observed that
the writing expert tends to value praise and summary
(indicated by both SUM and summary%) in reviews
while the content-expert favors critiques, especially
solution provided critiques.
5.4 Discussion
Based on our observations from the above three
comparisons, we summarize our findings with re-
spect to different feature types and provide inter-
pretation: 1) review length (in tokens) is generally
effective in predicting expert perceived helpfulness,
while number of sentences is more useful in mod-
eling student perceived helpfulness. Interestingly,
there is a strong correlation between these two fea-
tures (r = 0.91, p ? 0.001), and why one is selected
over the other in different helpfulness models needs
further investigation. 2) Lexical categories such as
transition cues, negation, and suggestion words are
of more importance in modeling student perceived
helpfulness. This might indicate that students pre-
fer clear expression of problem, reference and even
opinion in terms of specific lexicon clues, the lack of
which is likely to result in difficulty in their under-
standing of the reviews. 3) As for cognitive-science
features, solution is generally an effective indica-
tor of helpful peer reviews. Within the three feed-
back types of peer reviews, praise is valued high
by the writing expert. (It is interesting to notice
that although praise is shown to be more impor-
tant than problem and summary for modeling the
writing-expert helpfulness, positive sentiment words
do not appear to be more predictive than negative
sentiments.) In contrast, problem is more desirable
17
from the content expert?s point of view. Although
students assign less importance to the problem them-
selves, solution provided peer reviews could be help-
ful for them with respect to the learning goal of peer-
review assignments. 4) Paper rating is a very ef-
fective feature for predicting review helpfulness per-
ceived by students, which is not the case for either
expert. This supports the argument of social aspects
in people?s perception of review helpfulness, and it
also reflects the fact that students tend to be nice to
each other in such peer-review interactions. How-
ever, this dimension might not correspond with the
real helpfulness of the reviews, at least from the per-
spective of both the writing expert and content ex-
pert.
6 Conclusion and Future Work
We have shown that the type of helpfulness to be
predicted does indeed influence the utility of dif-
ferent feature types for automatic prediction. Low-
level general linguistic features are more predic-
tive when modeling students? perceived helpfulness;
high-level theory supported constructs are more use-
ful in experts? models. However, in the related area
of automated essay scoring (Attali and Burstein,
2006), others have suggested the need for the use
of validated features related to meaningful dimen-
sions of writing, rather than low-level (but easy to
automate) features. In this perspective, our work
similarly poses challenge to the NLP community in
terms of how to take into account the education-
oriented dimensions of helpfulness when applying
traditional NLP techniques of automatically pred-
icating review helpfulness. In addition, it is im-
portant to note that predictive features of perceived
helpfulness are not guaranteed to capture the nature
of ?truly? helpful peer reviews (in contrast to the
perceived ones).
In the future, we would like to investigate how
to integrate useful dimensions of helpfulness per-
ceived by different audiences in order to come up
with a ?true? helpfulness gold standard. We would
also like to explore more sophisticated features and
other NLP techniques to improve our model of peer-
review helpfulness. As we have already built models
to automatically predict certain cognitive constructs
(problem localization and solution), we will replace
the annotated cognitive-science features used here
with their automatic predictions, so that we can build
our helpfulness model fully automatically. Finally,
we would like to integrate our helpfulness model
into a real peer-review system and evaluate its per-
formance extrinsically in terms of improving stu-
dents? learning and reviewing performance in future
peer-review assignments.
Acknowledgments
This work was supported by the Learning Research
and Development Center at the University of Pitts-
burgh. We thank Melissa Patchan and Chris Schunn
for generously providing the manually annotated
peer-review corpus. We are also grateful to Michael
Lipschultz and Chris Schunn for their feedback
while writing this paper.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning and Assessment (JTLA), 4(3), February.
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A web-
based reciprocal peer review system. Computers and
Education, 48:409?426.
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings of the First In-
ternational Conference on Educational Data Mining
(EDM2008), pages 192?196.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opin-
ions are received by online communities: A case study
on Amazon.com helpfulness votes. In Proceedings of
WWW, pages 141?150.
Raquel M. Crespo Garcia. 2010. Exploring document
clustering techniques for personalized peer assessment
in exploratory courses. In Proceedings of Computer-
Supported Peer Review in Education (CSPRED) Work-
shop in the Tenth International Conference on Intelli-
gent Tutoring Systems (ITS 2010).
Anindya Ghose and Panagiotis G. Ipeirotis. 2010. Esti-
mating the helpfunless and economic impact of prod-
uct reviews: Mining text and reviewer characteristics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 99.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference
18
on Empirical Methods in Natural Language Process-
ing (EMNLP2006), pages 423?430, Sydney, Australia,
July.
Kenji Kira and Larry A. Rendell. 1992. A practical
approach to feature selection. In Derek H. Sleeman
and Peter Edwards, editors, ML92: Proceedings of
the Ninth International Conference on Machine Learn-
ing, pages 249?256, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th conference on
Computational linguistics, volume 1 of COLING ?00,
pages 495?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu.
2008. Modeling and predicting the helpfulness of on-
line reviews. In Proceedings of the Eighth IEEE Inter-
national Conference on Data Mining, pages 443?452,
Los Alamitos, CA, USA.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 91?98, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. Instructional Sci-
ence, 37(4):375?401.
Melissa M. Patchan, Davida Charney, and Christian D.
Schunn. 2009. A validation study of students? end
comments: Comparing comments by students, a writ-
ing instructor, and a content instructor. Journal of
Writing Research, 1(2):124?152.
Agnes Sandor and Angela Vorndran. 2009. Detect-
ing key sentences for automatic assistance in peer-
reviewing research articles in educational sciences. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP), pages 36?44.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most helpful
book reviews. In Proceedings of the Third Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM2009), pages 36?44.
IH Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, San Francisco, CA.
Wenting Xiong and Diane Litman. 2010. Identifying
problem localization in peer-review feedback. In Pro-
ceedings of Tenth International Conference on Intelli-
gent Tutoring Systems (ITS2010), volume 6095, pages
429?431.
Wenting Xiong and Diane Litman. 2011. Automatically
predicting peer-review helpfulness. In Proceedings
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL/HLT), Portland, Oregon, June.
19
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 136?141,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Predicting Change in Student Motivation by Measuring Cohesion between
Tutor and Student
Arthur Ward
Department of Biomedical
Informatics
University of Pittsburgh
Pittsburgh, Pa., 15232
akw13@pitt.edu
Diane Litman
Department of Computer
Science, LRDC
University of Pittsburgh
Pittsburgh, Pa., 15260
litman@cs.pitt.edu
Maxine Eskenazi
Language Technologies
Institute
Carnegie Mellon University
Pittsburgh, Pa., 15213
max@cmu.edu
Abstract
We apply a previously reported measure of di-
alog cohesion to a corpus of spoken tutoring
dialogs in which motivation was measured.
We find that cohesion significantly predicts
changes in student motivation, as measured
with a modified MSLQ instrument. This sug-
gests that non-intrusive dialog measures can
be used to measure motivation during tutoring.
1 Introduction
Motivation is widely believed to be an important fac-
tor in learning, and many studies have found rela-
tionships between motivation and educational out-
comes. For example Pintrich and DeGroot (1990)
found that students? motivational state was a signif-
icant predictor of classroom performance. In ad-
dition, pedagogically significant behaviors such as
dictionary lookup in the REAP (Brown and Eske-
nazi, 2004) vocabulary tutor have been shown to
be positively correlated with motivation assessments
(DelaRosa and Eskenazi, 2011). Also, in a separate
study with the REAP tutor, attempts to manipulate
reading motivation by presenting more interesting
stories were shown to improve vocabulary learning
(Heilman et al, 2010).
In addition to influencing learning outcomes, mo-
tivational state may also affect which interventions
will be effective during tutoring. For example, Ward
and Litman (2011) have shown that motivation can
significantly affect which students benefit from a re-
flective reading following interactive tutoring with a
the Itspoke (Litman and Silliman, 2004) tutor.
An accurate way to measure student motivation
during tutoring could therefore be valuable to Intel-
ligent Tutoring System (ITS) researchers. Several
self-report instruments have been developed which
measure various aspects of motivation (e.g. (Pintrich
and DeGroot, 1990; McKenna and Kear, 1990)).
However, these instruments are too intrusive to be
administered during tutoring, for fear of fatally dis-
rupting learning. We would prefer a non-intrusive
measure which would allow an ITS to detect when
student motivation is decreasing so as to launch a
motivational intervention. Similarly, the ITS should
be able to detect when motivation is increasing
again, to determine if the intervention worked. As
mentioned above, such a measure might also allow
an ITS to determine when it would be effective to
use certain instructional tactics.
In this work we investigate cohesion as a non-
intrusive measure of motivation for natural language
dialog based ITS. As defined more precisely below,
our measure of cohesion quantifies lexical and se-
mantic similarity between tutor and student dialog
utterances. We hypothesize that this measure of lexi-
cal similarity may be related to motivation in part be-
cause other measures of dialog similarity have been
shown to be related to task success. For example,
there is evidence that perceived similarity between a
student?s own speech rate and that of a recorded task
request increases the student?s feelings of immedi-
acy, which are in turn linked to greater compliance
with the request to perform a task (Buller and Aune,
1992). 1 In addition, Ward and Litman (2006; 2008)
investigated a measure of lexical similarity between
1In this experiment, the task was to watch a series of videos.
136
the tutor and student partners in a tutoring dialog
which was shown to be correlated with task success
in several corpora of tutorial dialogs.
Measures of cohesion have also been used in a va-
riety of NLP tasks such as measuring text readability
(e.g. (Pitler and Nenkova, 2008)), measuring stylis-
tic differences in text (Mccarthy et al, 2006), and
for topic segmentation in tutorial dialog (Olney and
Cai, 2005).
Given the previously mentioned results relating
motivation to educational task success, these links
between task success and cohesion lead us to hy-
pothesize a direct correlation between motivation
and cohesion when using the Itspoke tutor.
We will first briefly describe the Itspoke tutor, and
the corpus of tutoring dialogs used in this study. We
will then describe the instrument we used to mea-
sure motivation both before and immediately after
tutoring, then we will describe the algorithm used
to measure cohesion in the tutoring dialogs. Finally,
we show results of correlations between the measure
of motivation and the measure of cohesion. We will
find that the change in motivation is significantly
correlated with dialog cohesion.
2 Itspoke System and Corpus
Itspoke (Intelligent Tutoring SPOKEn dialog sys-
tem) is a spoken dialog tutoring system which
teaches qualitative physics. It provides a spoken di-
alog interface to the Why2-Atlas (VanLehn et al,
2002) tutor, and has recently been re-implemented
using the TuTalk (Jordan et al, 2007) dialog plat-
form. The Itspoke tutor presents a problem in qual-
itative physics, and asks the student an initial ques-
tion. The student answers the question, and the di-
alog continues until all points have been covered to
the tutor?s satisfaction.
The corpus used in the current work was collected
in a previous study (Ward and Litman, 2011), us-
ing novice subjects who had never taken a college
physics course. Before tutoring, students were given
a motivation survey which will be described in Sec-
tion 3. They then engaged Itspoke in five tutoring
dialogs as described above. Immediately after tu-
toring they were given the motivation questionnaire
again, with tenses changed as appropriate.
166 subjects were recruited by flyer, by advertise-
Speaker Utterance
Tutor To see which vehicle?s change in motion
is greater, we use the definition of accel-
eration. What is the definition of accel-
eration?
Student Change in velocity.
Tutor Excellent. Acceleration is defined as the
amount velocity changes per unit time.
Table 1: Dialog turns, with Token, Stem, and Semantic
Similarity Matches in bold (as discussed in Section 4).
ment during an undergraduate psychology course, or
from the University of Pittsburgh?s psychology sub-
ject pool. Of these, 40 were dismissed after pretest
as ?middle third? knowledge students, following ex-
treme groups design (Feldt, 1961). Extreme groups
design was adopted to increase the power of a ?high?
vs ?low? knowledge comparison, which is reported
elsewhere (Ward, 2010). Another 27 students were
not used for various reasons including incomplete
data. This left a corpus of 99 subjects who each par-
ticipated in 5 tutorial dialogs.
Table 1 shows an exchange from one of these di-
alogs. The tutor asks a question about the current
problem, which the student then answers. The tutor
restates the answer, and (later in the dialog) proceeds
on to the next point of discussion.
3 Motivation Measure
In this study we measure motivation using a reduced
version of the ?Motivated Strategies for Learning
Questionnaire (MSLQ)? developed by Pintrich and
DeGroot (1990). This version of the MSLQ is also
based on work by Roll (2009), who adapted it for
use in an IPL (Invention as Preparation for Learning
(Schwartz and Martin, 2004)) tutoring environment.
Our motivational survey is shown in Figure 1.
Questions one and two address ?self-regulation,?
particularly the students? tendency to manage and
control their own effort. Question one is on a re-
versed scale relative to the other questions, so re-
sponses to it were inverted. Question three addresses
?self-efficacy,? the students? expectation of success
on the task. Questions four and five address ?intrin-
sic value,? the students? beliefs about the importance
and interest of the task.
These dimensions of motivation are theoretically
137
Please read the following statements and then
click a number on the scale that best matches
how true it is of you. 1 means ?not at all true
of me? whereas 7 means ?very true of me?.
1. I think that when the tutor is talking
I will be thinking of other things and
won?t really listen to what is being said.
2. If I could take as much time as I want,
I would spend a lot of time on physics
tutoring sessions.
3. I think I am going to find the physics tu-
tor activities difficult.
4. I think I will be able to use what I learn
in the physics tutor sessions in my other
classes.
5. I think that what I will learn in the
physics tutor sessions is useful for me to
know.
Figure 1: Pre-tutoring Motivational Survey
distinct. However, except for question three (the
self-efficacy question), responses to these questions
were all very significantly correlated with each other
in our survey (p < .01).
Table 2 shows values of Cronbach?s Alpha (Cron-
bach, 1951) for various subsets of the motivation
questions. Alpha measures the internal consistency
of responses to a multi-point questionnaire, and is a
function of the number of test items and the corre-
lation between them. Higher values are thought to
indicate that the various test items are measuring the
same underlying latent construct. For this study we
omit Question 3, maximizing Alpha at .716. This
is just above the commonly accepted (Gliem and
Gliem, 2003) threshold for reliability in such an in-
strument.
Questions Alpha
1, 2, 3, 4, 5 0.531
1, 2, 4, 5 0.716
2, 4, 5 0.703
4, 5 0.683
Table 2: Alpha reliability
scores for various subsets
of questions.
As mentioned above,
this instrument was ad-
ministered both before
and (with suitable tense
changes) immediately
after tutoring. We will
report correlations using
mean scores on the
pre- and post-tutoring
measures, as well as
for the change-in-motivation score, calculated as
post-minus-pre.
4 Semantic Cohesion Measure
In this work we measure cohesion between tutor and
student using the ?semantic cohesion? measure first
reported by Ward and Litman (2008). This measure
counts the number of ?cohesive ties? (Halliday and
Hasan, 1976) between adjacent tutor and student di-
alog turns. A cohesive tie can be the repetition of an
exact word or word stem, or the use of two words
with similar meanings in adjacent turns. Stop words
are excluded, and cohesive ties are counted in both
the student-to-tutor and the tutor-to-student direc-
tions. For example, in the dialog shown in Table 1,
the final tutor turn repeats the word ?velocity? from
the previous student turn. This repetition would be
counted as an exact cohesive tie. Similarly, the tu-
tor uses the word ?changes? following the student?s
use of ?change.? This would be counted as a stem
repetition cohesive tie.
Finally, the student?s use of ?velocity? will be
counted as a cohesive tie because of its semantic
similarity to ?acceleration,? from the preceding turn.
The algorithm therefore counts four ties in Table 1.
As described more completely in (Ward and Litman,
2008), semantic similarity cohesive ties are counted
by measuring two words? proximity in the Word-
Net (Miller et al, 1990) hierarchy. We use a simple
path distance similarity measure, as implemented in
NLTK (Loper and Bird, 2002). This measure counts
the number of edges N in the shortest path between
two words in WordNet, and calculates similarity as 1
/ (1 + N). Our implementation of this semantic simi-
larity measure allows setting a threshold ?, such that
only word pairs with stronger-than-threshold simi-
larity are counted. Table 3 shows some semantic
similarity pairs counted with a threshold of 0.3.
motion-contact
man-person
decrease-acceleration
acceleration-change
travel-flying
Table 3: Example Se-
mantic ties: ? = 0.3
We obtain a normal-
ized cohesion score for
each dialog by dividing
the tie count by the num-
ber of turns in the dialog.
We then sum the line nor-
malized counts over all
the dialogs for each stu-
dent, resulting in a per-
student cohesion measure.
138
5 Results
We ran correlations between the change-in-
motivation score described in Section 3 and the
semantic similarity measure of cohesion described
in Section 4. We report results for a semantic
similarity threshold of .3 for consistency with
(Ward and Litman, 2008), however the pattern of
results is not sensitive to this threshold. Significant
results were obtained for all thresholds between .2
and .5, in .1 increments. 2 In addition, we report
results for the motivation measure with the third
question removed for consistency with (Ward and
Litman, 2011). However the pattern of results is not
sensitive to this exclusion, either. Significant results
were also obtained using the entire questionnaire.
Motivation
Measure Cor. pValue
pre-Tutoring 0.02 0.86
Change 0.21 0.03
post-Tutoring 0.19 0.055
Table 4: Cohesion - Motivation
Correlations. N = 99. ? = 0.3
In all cases,
the change in
motivation was
found to be
significantly
and positively
correlated with
the cohesive-
ness of the
tutoring dialog. More lexical similarity between
tutor and student was predictive of increased student
motivation. As shown in the middle row of Table
4, the correlation with motivational change, using a
threshold of .3 and the reduced motivation measure
was r(97)= .21, p = 0.03.
Interestingly, as shown in the top and bottom rows
of Table 4, neither motivation before tutoring r(97)
= .02, p=.86, nor after tutoring r(97) = .19, p = .055,
was significantly correlated with cohesion, although
the post-tutoring measure achieves a strong trend.
Pre- and post-tutoring mean motivation levels
were, however, significantly correlated with each
other (R(97) = .69, p < .0001). Mean motivation
levels also showed a non-significant improvement
from 4.31 before tutoring to 4.44 after tutoring.
6 Discussion and Future Work
We have brought forward evidence that cohesion in
tutorial dialog, as measured in this paper, is corre-
lated with changes in student motivation. This sug-
2Note from the path distance formula that thresholds be-
tween .5 and 1 are impossible
gests that dialog cohesion may be useful as a non-
intrusive measure of motivational fluctuations.
As discussed in Section 1, other researchers have
investigated various types of cohesion, and their re-
lationship to things such as task success and learn-
ing. In addition, work has been done investigating
the role of motivation in learning. However, we be-
lieve ours is the first work relating dialog cohesion
directly to user motivation.
The presence of a correlation between cohesion
and motivation leaves open the possibility that more
motivated students are experiencing greater task
success in the tutor, and so generating more cohe-
sive dialogs. 3 Note, however, that the very non-
significant correlation between pre-dialog motiva-
tion and dialog cohesion argues against this pos-
sibility. Instead, it seems that some process is
both creating dialog cohesion and improving student
motivation. The lack of significance in the post-
dialog/motivation correlation may be due to data
sparsity.
In future work, we hope to investigate other dia-
log features which may be even better predictors of
student motivation. As mentioned in Section 1, we
became interested in dialog similarity metrics partly
because of their association with task success. These
kinds of associations between task success and dia-
log have also been shown for dialog entrainment.
In this discussion we will use the term ?entrain-
ment? for the phenomenon in which conversational
partners? speech features become more similar to
each other at many levels, including word choice,
over the course of a dialog. 4 As mentioned above,
we use the term ?cohesion? for overall similarity of
word choice between speakers in a dialog, perhaps
resulting from entrainment.
Users appear to entrain strongly with dialog sys-
tems. For example, Brennan (1996) has found that
users are likely to adopt the terms used by a WOZ
dialog system, and that this tendency is at least as
strong as with human dialog partners. Similarly, Par-
ent and Eskenazi (2010) showed that users of the
Let?s Go (Raux et al, 2005) spoken dialog system
quickly entrain to its lexical choices.
3We thank an anonymous reviewer for prompting this dis-
cussion.
4This definition conflates studies of priming, alignment,
convergence and accommodation.
139
As with measures of dialog similarity, dialog en-
trainment has been found to be related to satisfac-
tion and success in task oriented dialogs. For ex-
ample, Reitter and Moore (2007) found that lexi-
cal and syntactic repetition predicted task success
in the MapTask corpus. Similarly, Ward and Lit-
man (2007) found that lexical and acoustic-prosodic
entrainment are correlated with task success in the
Itspoke dialog system. Interestingly, in that work
entrainment was more strongly correlated with task
success than a measure of dialog cohesion similar
to the one used in the current paper. This raises the
question of whether such a measure of dialog en-
trainment might also be a better predictor of motiva-
tion than the current measure of cohesion. We hope
in future work to further investigate this possibility.
Finally, because we are interested in predicting
motivation during tutoring, our dialog metrics may
be improved by making them sensitive to the educa-
tional domain. For example, exploratory work with
our tutor has suggested that a measure of cohesion
which only counts cohesive ties between physics
terms is better correlated with certain measures of
learning than a measure which counts non-physics
terms. This suggests that measures of cohesion
or entrainment which recognize educational domain
words may also improve correlations with motiva-
tion.
Acknowledgments
This work was supported by the ONR (N00014-07-
1-0039), by the NSF (0631930 and 0914615), and
by the LRDC and ISP at the University of Pittsburgh.
References
Susan E. Brennan. 1996. Lexical entrainment in sponta-
neous dialog. In International Symposium on Spoken
Dialog, pages 41?44.
Jonathan Brown and Maxine Eskenazi. 2004. Re-
trieval of authentic documents for reader-specific lexi-
cal practice. In In Proceedings of InSTIL/ICALL Sym-
posium.
David Buller and R.Kelly Aune. 1992. The effects of
speech rate similarity on compliance: Application of
communication accommodation theory. Western Jour-
nal of Communication, 56:37?53.
Lee Cronbach. 1951. Coefficient alpha and the internal
structure of tests. Psychometrika, 16(3):297?334.
Kevin DelaRosa and Maxine Eskenazi. 2011. Self-
assessment of motivation: Explicit and implicit indi-
cators of l2 vocabulary learning. Proceedings 15th In-
ternational Conference on Artificial Intelligence Edu-
cation (AIED).
Leonard Feldt. 1961. The use of extreme groups to
test for the presence of a relationship. Psychometrika,
26(3):307?316.
Joesph Gliem and Rosemary Gliem. 2003. Calculating,
interpreting, and reporting cronbach?s alpha reliability
coefficient for likert-type scales. Midwest Research to
Practice in Adult, Continuing and Community Educa-
tion.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. English Language Series. Pearson Educa-
tion Limited.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, Maxine Eskenazi, Alan Juffs, and Lois Wil-
son. 2010. Personalization of reading passages im-
proves vocabulary acquisition. International Journal
of Artificial Intelligence in Education, 20:73?98, Jan-
uary.
Pamela Jordan, Brian Hall, Michael Ringenberg, Yui Cui,
and Carolyn Rose?. 2007. Tools for authoring a di-
alogue agent that participates in learning studies. In
Proc. of Artificial Intelligence in Ed., AIED, pages 43?
50.
D. Litman and S. Silliman. 2004. ITSPOKE: An intelli-
gent tutoring spoken dialogue system. In Companion
Proc. of the Human Language Technology Conf: 4th
Meeting of the North American Chap. of the Assoc. for
Computational Linguistics.
Edward Loper and Steven Bird. 2002. Nltk: The natural
language toolkit. In In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia: Association for Computa-
tional Linguistics.
Philip M. Mccarthy, Gwyneth A. Lewis, David F. Dufty,
and Danielle S. Mcnamara. 2006. Analyzing writ-
ing styles with coh-metrix. In In Proceedings of the
Florida Artificial Intelligence Research Society Inter-
national Conference (FLAIRS.
M.C. McKenna and D.J. Kear. 1990. Measuring attitude
toward reading: A new tool for teachers. The Reading
Teacher, 43(8):626?639.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography (special issue),
3 (4):235?312.
Andrew Olney and Zhiqiang Cai. 2005. An orthonormal
basis for topic segmentation in tutorial dialog. In Pro-
140
ceedings of the Human Language Technology Confer-
ence and Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP), pages 971?978.
Vancouver, October.
Gabriel Parent and Maxine Eskenazi. 2010. Lexical en-
trainment of real users in the let?s go spoken dialog
system. In Proceedings Interspeech-2010, pages 3018
? 3021, Makuhari, Chiba, Japan.
Paul Pintrich and Elisabeth DeGroot. 1990. Motivational
and self-regulated learning components of classroom
academic performance. Journal of Educational Psy-
chology, 82(1):33?40.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP), pages 186 ? 195.
Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go public!
taking a spoken dialog system to the real world. In
Proceedings Interspeech-2005, pages 885 ? 888, Lis-
bon, Portugal.
David Reitter and Johanna D. Moore. 2007. Predict-
ing success in dialogue. In In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 808 ? 815, Prague, Czech Re-
public.
Ido Roll. 2009. Structured Invention Tasks to Pre-
pare Students for Future Learning: Means, Mecha-
nisms, and Cognitive Processes. Doctor of philoso-
phy, Carnegie Mellon University, 5000 Forbes Ave.
Pittsburgh, Pa.
Daniel Schwartz and Taylor Martin. 2004. Inventing
to prepare for future learning: The hidden efficiency
of encouraging original student production in statistics
instruction. Cognition and Instruction, 22:129 ? 184.
K. VanLehn, P. Jordan, C. Rose, D. Bhembe, M. Boettner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, and Srivastava R. 2002.
The architecture of why2-atlas: A coach for qualita-
tive physics essay writing. In Proc. 6th Int. Conf. on
Intelligent Tutoring Systems, pages 158?167.
Arthur Ward and Diane Litman. 2006. Cohesion and
learning in a tutorial spoken dialog system. In Pro-
ceedings of the 19th International FLAIRS Conference
(FLAIRS-19), pages 533?538, May.
Arthur Ward and Diane Litman. 2007. Dialog con-
vergence and learning. In Proceedings 13th Interna-
tional Conference on Artificial Intelligence Education
(AIED), Los Angeles, Ca.
Arthur Ward and Diane Litman. 2008. Semantic
cohesion and learning. In Proceedings 9th Inter-
national Conference on Intelligent Tutoring Systems
(ITS), pages 459?469, Ann Arbor, June.
Arthur Ward and Diane Litman. 2011. Adding abstrac-
tive reflection to a tutorial dialog system. Proceedings
24th International FLAIRS (Florida Artificial Intelli-
gence Research Society) Conference.
Arthur Ward. 2010. Reflection and Learning Robust-
ness in a Natural Language Conceptual Physics Tutor-
ing System. Doctor of philosophy, University of Pitts-
burgh, Pittsburgh, PA. 15260.
141
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 216?226,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Using Performance Trajectories to Analyze the Immediate Impact of User
State Misclassification in an Adaptive Spoken Dialogue System
Kate Forbes-Riley
Learning Research & Development Ctr (LRDC)
University of Pittsburgh
Pittsburgh, PA 15260
forbesk@cs.pitt.edu
Diane Litman
Dept. Computer Science & LRDC
University of Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Abstract
We present a method of evaluating the imme-
diate performance impact of user state mis-
classifications in spoken dialogue systems.
We illustrate the method with a tutoring sys-
tem that adapts to student uncertainty over and
above correctness. First we define a rank-
ing of user states representing local perfor-
mance. Second, we compare user state trajec-
tories when the first state is accurately clas-
sified versus misclassified. Trajectories are
quantified using a previously proposed met-
ric representing the likelihood of transitioning
from one user state to another. Comparison of
the two sets of trajectories shows whether user
state misclassifications change the likelihood
of subsequent higher or lower ranked states,
relative to accurate classification. Our tutoring
system results illustrate the case where user
state misclassification increases the likelihood
of negative performance trajectories as com-
pared to accurate classification.
1 Introduction
Spoken dialogue systems research has shown that
natural language processing errors can negatively
impact global system performance. For exam-
ple, automatic speech recognition errors have been
shown to negatively correlate with user satisfaction
surveys taken after the system interaction is over
(e.g., (Walker et al, 2000a; Pon-Barry et al, 2004)).
Automatic user state classification errors have
also been shown to negatively impact global per-
formance in spoken dialogue systems (e.g., (Pon-
Barry et al, 2006)). For example, in our prior work
with an uncertainty-adaptive spoken dialogue com-
puter tutoring system, we found that recognizing and
adapting to the user?s state of uncertainty, over and
above his/her state of correctness, significantly im-
proved global learning over all users (as measured
by tests taken before and after the system interac-
tion). However, this was only true when the user
uncertainty was manually labeled during the inter-
action by an unseen human ?wizard of oz? (Forbes-
Riley and Litman, 2011b); it was not true when the
uncertainty was automatically labeled by the system.
Further analysis showed that uncertainty classifica-
tion errors largely accounted for the global perfor-
mance decrease in our fully automated system. In
particular, only a small proportion of users? actual
uncertainty was being accurately classified by the
system (Forbes-Riley and Litman, 2011a).1
The question we address in this study is how to
analyze the impact of automatic user state classifi-
cation errors when analyzing performance at a local
level. In particular, is there a measurable local per-
formance difference when one compares what hap-
pens in a dialogue after a turn is accurately classi-
fied versus misclassified? We show here how user
state trajectories can be used to answer this ques-
tion. First, a ranking of user states is defined (Sec-
tion 3.1). Second, user state trajectories are com-
puted from two sets of system dialogue: one in
1In natural language processing (NLP) research, the terms
?(in)correct? and ?(un)certain? can have multiple interpreta-
tions. To avoid confusion, we reserve these terms in this paper
only to refer to the semantic content and affective/attitudinal ex-
pression of user answers (respectively). When referring to the
NLP performance of our system, we use the terms ?accurately
classified? and ?misclassified?.
216
which the user state of interest is accurately clas-
sified in the first turn in the trajectory, and another
in which it is misclassified (Section 3.2). Trajec-
tories are quantified as the likelihood of transition-
ing from one user state to another (D?Mello et al,
2007). Comparison of the two sets of trajectories in-
dicates how user state misclassifications change the
relative likelihood of subsequent states. Transitions
to higher ranked states indicate improved local per-
formance while transitions to lower ranked states in-
dicate decreased local performance.
In our research, we are interested in this question
because we hypothesize that accurate and inaccurate
user state classification in our uncertainty-adaptive
system yielded immediate differences in user behav-
ior. We further hypothesize that our uncertainty-
adaptive system had a negative immediate impact
on the user?s state when (un)certainty was misclas-
sified, as compared to when (un)certainty was ac-
curately classified. Our user state trajectory results
support these hypotheses. We find that (un)certainty
misclassifications increased the likelihood of transi-
tioning to the lowest ranked user state in the next
turn. In contrast, accurate (un)certainty classifica-
tion yielded an increased likelihood of more positive
performance trajectories (Section 4).
More generally, this question is relevant to other
automatically classified user states and other types
of dialogue systems, whenever the goal is to un-
derstand the immediate impact of user state classi-
fication errors on user behavior during the dialogue
(Sections 3.1 and Section 5).
2 The System and Dialogues
We apply this local performance analysis to dia-
logues between college students and our fully auto-
mated spoken dialogue tutoring system, ITSPOKE.2
Two sets of dialogues are used here, which come
from two versions of ITSPOKE: the uncertainty-
adaptive and non-adaptive versions. Both ver-
sions automatically classify user (un)certainty and
(in)correctness for each turn. However, the non-
adaptive version?s responses are based only on
(in)correctness, while the uncertainty-adaptive ver-
sion provides an uncertainty adaptation to uncer-
2ITSPOKE(Intelligent Tutoring SPOKEn dialogue) is a
spoken, modified version ofWhy2-Atlas (VanLehn et al, 2002).
tain+correct answers. All dialogues were collected
in our prior experiment comparing global learn-
ing across the uncertainty-adaptive and non-adaptive
system versions (see Section 1). The uncertainty-
adaptive system yielded 120 dialogues (1957 student
turns) from 24 subjects. The non-adaptive system
yielded 125 dialogues (2065 student turns) from 25
subjects. Our analysis will focus on the dialogues
from the uncertainty-adaptive system (Section 4.1);
the dialogues from the non-adaptive system will be
used for comparison (Section 4.2).
Briefly, ITSPOKE tutors 5 physics problems (one
per dialogue), in a Tutor Question - Student Answer
- Tutor Response format. The tutor questions seri-
ally present topics needed to solve each problem; ad-
jacent questions concern identical or closely related
topics. After each tutor question, the student answer
is digitized from head-mounted microphone input
and sent to the Sphinx2 recognizer, which yields an
automatic transcript. The answer?s (in)correctness is
then automatically classified based on this transcript,
using the TuTalk semantic analyzer (Jordan et al,
2007). Simultaneously, the answer?s (un)certainty
is automatically classified by inputting features of
the speech signal, the automatic transcript, and the
dialogue context into a logistic regression model.3
All natural language processing components were
trained using prior ITSPOKE corpora.
The dialogue manager determines the appropriate
tutor response and sends it to the Cepstral text-to-
speech system. The uncertainty-adaptive and non-
adaptive system versions give the same response to
incorrect answers (regardless of (un)certainty): in-
correctness feedback (e.g., ?That?s not right?) fol-
lowed by additional remedial content to help stu-
dents understand why their answer was incorrect.
Both versions also give the same response to cer-
tain+correct answers: correctness feedback only
(e.g., ?That?s right?). The two versions differ in their
response to uncertain+correct answers. The non-
adaptive version responds with correctness feed-
back only, while the uncertainty-adaptive version re-
sponds with correctness feedback followed by addi-
tional remedial content to help students resolve their
uncertainty. This uncertainty adaptation is based on
3The uncertainty model includes lexical, pitch, temporal,
and energy features as well as tutor question and user gender,
and is described further in (Forbes-Riley and Litman, 2011a).
217
tutor1: Let?s begin by looking at the motion of the man and his keys while he?s holding them. How does his velocity
compare to that of his keys?
student-automatic1: his also the is the same as that of his keys points force (certain+incorrect)
student-manual1: his velocity is the same as that of his keys while he?s holding them (uncertain+correct)
tutor2: Actually... Their velocities must be the same, because the man?s holding his keys, so they?re not moving with
respect to each other. [...] What are the forces exerted on the man after he releases his keys?
student-automatic2: gravity (uncertain+correct)
student-manual2: gravity?? (uncertain+correct)
non-adaptive-tutor3: Right. So what?s the direction of the force of gravity on the man (e.g., vertically up)?
uncertainty-adaptive-tutor3: Right. Gravity is the only force acting on the man. This force is also called the weight
of the man. It?s always present when an object is in earth?s gravitational field. So what?s the direction of the force of
gravity on the man (e.g., vertically up)?
Figure 1: Annotated Dialogue Example Showing Uncertainty Adaptation and User State Classifications
prior tutoring research, which argues that incorrect-
ness and uncertainty are both signals of a ?learn-
ing impasse?, i.e., an opportunity to better learn the
material (VanLehn et al, 2003). Our uncertainty
adaptation only provides additional remedial con-
tent after the uncertain+correct impasse, because in-
correctness impasses (uncertain and certain) already
receive remedial content from the non-adaptive sys-
tem. It is assumed that this content helps resolve
both incorrectness and uncertainty (when present).
After the experiment, each student answer
(turn) was manually transcribed and labeled for
(un)certainty and (in)correctness. One labeler
performed the annotation based on schemes devel-
oped and evaluated on prior ITSPOKE corpora,
where this labeler and another labeler displayed
interannotator reliability of 0.85 and 0.62 Kappa
on (in)correctness and (un)certainty, respec-
tively (Forbes-Riley and Litman, 2011a).4 Com-
parison of the automatic and manual labels yielded
84.7% accuracy for automatic (in)correctness
classification and 80.3% accuracy for auto-
matic (un)certainty classification. However, the
(un)certainty model had an uncertainty recall of
only about 20%, while the (in)correctness model
had a correctness recall of about 80% (Forbes-Riley
and Litman, 2011a).5
4Because these evaluations showed that this trained labeler
could reliably annotate (un)certainty and (in)correctness in IT-
SPOKE dialogues, no further evaluations were performed.
5The lower recall for predicting uncertainty is neverthe-
Figure 1 illustrates ITSPOKE?s natural language
processing components and the two system versions.
The first answer is classified as certain+incorrect
(student-automatic1) but manually labeled as un-
certain+correct (student-manual1); the manual and
automatic transcripts are also substantially differ-
ent. Because this answer was misclassified as in-
correct, both versions give the same response (tu-
tor2). The second answer is accurately classified as
uncertain+correct. The non-adaptive system thus ig-
nores the uncertainty and only provides correctness
feedback (non-adaptive-tutor3), while the adaptive
system responds with correctness feedback and ad-
ditional remedial content to help resolve the uncer-
tainty (uncertainty-adaptive-tutor3).
3 Local Performance Evaluation
Here we discuss how to evaluate the local impact of
user state misclassification in dialogue systems.
3.1 Defining a User State Severity Ranking
Building on tutoring research that views both uncer-
tainty and incorrectness as signals of learning im-
passes (Section 2), we previously defined a sever-
ity ranking for the four impasse states correspond-
ing to all combinations of binary (in)correctness
less higher than always predicting no uncertainty (a majority
class baseline has 0% recall), and is on par with prior work in
affect-adaptive tutoring systems, e.g. (Walonoski and Heffer-
nan, 2006); in general affective systems research has found it
difficult to accurately predict positive occurrences of affect.
218
Impasse State: certain+incorrect uncertain+incorrect uncertain+correct certain+correct
Severity: most less least none
Figure 2: User Impasse State Severity Ranking
and (un)certainty (Forbes-Riley and Litman, 2011a).
This ranking, shown in Figure 2, reflects the as-
sumption that a student must perceive an impasse in
order to resolve it. A state of uncertainty reflects this
awareness. Therefore, the most severe type of learn-
ing impasse occurs when a student is incorrect but
not aware of it. Impasse states of decreasing sever-
ity occur when the student is incorrect but aware that
s/he might be, and correct but believes s/he may not
be, respectively. No impasse exists when a student
is correct and not uncertain about it.
In our prior work, this ranking of user states was
independently validated by showing that average im-
passe state severity negatively correlates with global
learning gain in our system dialogues (Forbes-Riley
and Litman, 2011a). In other words, a higher pro-
portion of user states with less severe or no impasses
directly relates to higher global learning gain.
More generally, the idea of ranking user states in
terms of those that do or do not represent communi-
cation impasses applies to other dialogue system do-
mains and other user state dimensions as well. For
example, in information-seeking domains, frustra-
tion and anger are common affective states whose
occurrence during the dialogue signals severe com-
munication problems (Batliner et al, 2003), while
hang-ups and turns requesting a human operator are
other types of user states whose occurrence dur-
ing the dialogue signals severe communication prob-
lems (Walker et al, 2000b).
Moreover, state trajectories can be used to repre-
sent abstractions over other types of user (or system)
behaviors. In our tutoring system analysis, repre-
senting user states in terms of only (un)certainty and
(in)correctness is an abstraction that we find useful
for analyzing impasse trajectories. However, dur-
ing run-time, a finite-state dialogue manager con-
sisting of 142 states actually controls the system?s
operation, and uses many other features besides user
uncertainty and incorrectness to determine the sys-
tem?s response (e.g. the physics concepts related to
the current system question, the history of prior stu-
dent answers to similar questions, etc.). Any of these
states could be analyzed as well to understand their
local performance impact, as could their analogs in
other system domains. For example, in a train di-
alogue system, while the actual state representation
used during operation could be quite complex, for
a trajectory analysis a simpler representation could
be suitable, one which tracks whether the system
knows the values of the n attributes needed to query
the database. The state ranking in this case would
be over equivalence classes of states: states with n
attributes known > states with n-1 attributes known
> ... > initial state with 0 attributes known.
3.2 Computing User State Trajectories
Local trajectories of user states during a dialogue
can be computed as the likelihood of transitioning
from the user state in turn n to the user state in turn
n+1. Here we use D?Mello et al?s metric, transition
likelihood L (D?Mello et al, 2007).
Transition likelihood L is computed as shown be-
low, where n refers to the impasse state in turn n
and n+1 refers to the impasse state in turn n+1. As
shown, L is computed as the conditional probabil-
ity that the user state in turn n+1 will occur given
that the user state in turn n has occurred, adjusted
for the base rate of occurrence of the user state in
turn n+1. The denominator normalizes the result so
that L ranges from -? to 1. L=1 indicates that n+1
always follows n over and above the probability of
n+1 occurring. L=0 indicates that n+1 follows n at
the chance level. L<0 indicate that the likelihood of
n+1 following n is much lower than the base rate of
n+1 occurring.6
L(n?n+1) = P (n+1|n)?P (n+1)1?P (n+1)
Transition likelihood L has previously been used
to compute the likelihood of transitioning from one
affective state to another (e.g., from confusion to
6Note that this metric, which assesses the adjusted probabil-
ity of one user state following another, is equivalent to Kappa
in computing agreement among annotators after adjusting for
chance (D?Mello et al, 2007).
219
frustration) in a single set of dialogues between
student and computer tutor (D?Mello et al, 2007).
Transition likelihood L has also been used to com-
pare how the likelihoods of transitioning from one
affective state to another vary across two differ-
ent sets of dialogues collected with two different
versions of an affect-adaptive tutoring system (Mc-
Quiggan et al, 2008). Our analysis is based on
this analysis, but extends it in three ways: 1) our
transitions involve complex user states composed of
two dimensions ((un)certainty and (in)correctness),
2) the user states in our transitions are ranked to
enable a local performance analysis, 3) our perfor-
mance analysis is applied to the question of how user
state misclassification impacts local performance,
by comparing transition likelihoods after accurate
and inaccurate user state classifications.
In this prior work and in our work, likelihoods
for each transition are computed for each user (over
all dialogues of a user). ANOVAs with post-hoc
pairwise tests can then determine if there were sig-
nificant differences between all possible transitions
from the current user state in turn n.
To investigate how user state misclassifications
impact local performance, two user trajectories are
computed per user for each n?n+1 transition: one
when the manual and automatic user state labels
for turn n agreed, and another when they did not
agree. In both cases, using the manual label for turn
n+1 enables the true final user state to be compared
across the two sets of trajectories. Comparison of
the final state in the two sets of trajectories indicates
how user state misclassifications change the relative
likelihood of the subsequent user states. Transitions
to higher ranked states indicate improved local per-
formance while transitions to lower ranked states in-
dicate decreased local performance.
4 Impact of User State Misclassifications
in Uncertainty-Adaptive ITSPOKE
We now apply this analysis to the uncertainty-
adaptive ITSPOKE dialogues, to investigate how
user state misclassification impacts the local perfor-
mance of the uncertainty adaptation.
Since the complex user state of uncertain+correct
triggers the uncertainty adaptation, misclassifying
(un)certainty or (in)correctness can potentially im-
pact the local performance of the adaptation. How-
ever, as noted in Section 2, we previously found
that uncertainty misclassifications in our system
were more severe than correctness misclassifica-
tions. Thus, to streamline our analysis and avoid
data skew issues, we focus on how (un)certainty
misclassifications in manually labeled correct an-
swers impact our local performance trajectories.
There are 1270 manually labeled correct turns in
the dialogues collected with uncertainty-adaptive
ITSPOKE. In the dialogues collected with non-
adaptive ITSPOKE (which we will use for compari-
son), there are 1353 manually labeled correct turns.
We hypothesize that when (un)certainty misclas-
sification in correct answers causes the uncertainty
adaptation to be erroneously triggered or blocked,
we will see a negative performance impact, in terms
of an increased likelihood of transitioning to a more
severe impasse state when uncertainty is misclassi-
fied as compared to when it is accurately classified.
4.1 Uncertainty-Adaptive ITSPOKE Results
Accurate Uncertainty Classification: Figure 3
presents descriptive statistics for the likelihood (L)
that a manually labeled uncertain+correct answer
accurately classified as uncertain in turn n will tran-
sition to each of the four manually labeled impasse
states in turn n+1. As noted in Section 3.2, L=0 indi-
cates that the transition likelihood is equal to chance,
while L>0 and L<0 indicate likelihoods greater and
less than chance, respectively.
An ANOVA indicated that there were statistically
significant differences among the likelihoods in Fig-
ure 3 (F(3,56)=3.87, p=.02). The most likely transi-
tions are shown with stripes. Specifically, post-hoc
pairwise tests showed that in turn n+1, an uncer-
tain+incorrect answer (p<.01) or uncertain+correct
answer (p=.02) is significantly more likely than a
certain+correct answer (but are themselves equally
likely). In addition, an uncertain+incorrect answer
is significantly more likely than a certain+incorrect
answer (p=.05), in turn n+1. A dialogue example of
the most likely transition after accurately classified
uncertainty is shown in Figure 5, where it is com-
pared with the misclassified minimal pair in Figure 6
(see Appendix).
These results indicate that accurately classifying
(and thus accurately adapting to) uncertain+correct
220
Figure 3: Turn n? Turn n+1 Transition Likelihoods (L)
after a manually labeled uncertain+correct answer in turn
n is accurately classified as uncertain and receives the un-
certainty adaptation
answers is most likely to yield continued uncertainty
(regardless of correctness) in turn n+1. Prior re-
search (Craig et al, 2004; Kort et al, 2001) has
shown that uncertainty and questioning are positive
and crucial aspects of the learning process. The
continued uncertainty suggests that the uncertainty
adaptation keeps the student engaged in the learn-
ing process, and the equal likelihood of correctness
or incorrectness accompanying this uncertainty sug-
gests that they have not yet unreservedly adopted ei-
ther the correct or incorrect line of reasoning about
the topic under discussion.
To determine whether any of these transitions are
directly tied to global performance, we computed
Pearson?s correlations over all students between the
percentage of each transition and global learning
gain.7 Interestingly, transitioning from an accu-
rately classified correct+uncertain answer to a cor-
rect+certain answer is negatively related to global
learning gain (R=-.458, p=.025). This indicates that
continued uncertainty after the uncertainty adapta-
tion is provided is more beneficial, in the long run,
than no uncertainty. No other trajectories are di-
rectly related to global learning. Although our prior
result, that average impasse severity negatively cor-
relates with global learning gain (Section 3.1), indi-
cates it is better from a global perspective for a stu-
dent to be in a state of no impasse (correct+certain),
it does not tell us the best way for the student to at-
7normalized learning gain = (posttest-pretest)/(1-pretest).
tain this state. The results of our transition correla-
tions shed light on this - they tell us that transitioning
directly from correct+uncertain is not the best way
to attain the no impasse state. We hypothesize that
looking at wider transition windows (e.g., trigrams)
will shed light on what is the best way to attain this
state. For example, it may be that the best way to
transition to a state of no impasse is to do so after
sustained uncertainty (as in Figure 3).
Uncertainty Misclassification: Considering now
user state misclassifications, our results for accu-
rately classified uncertain+correct answers are in
sharp contrast to those for manually labeled uncer-
tain+correct answers misclassified as certain in turn
n. In particular, an ANOVA indicated that all manu-
ally labeled impasse states are equally likely in n+1
(F(3,88)=1.22, p=.32) after a misclassified uncer-
tain+correct answer.8
These results indicate that misclassifying (and
erroneously not adapting to) uncertain+correct an-
swers is as likely to have an immediate negative im-
pact on learning as it is to have a neutral or positive
impact. In particular, the misclassification is likely
to cause some students to transition from the least
severe impasse about the concept in turn n to the
most severe impasse about the concept in turn n+1.9
When they do not receive the uncertainty adaptation,
these students adopt an incorrect line of reasoning in
turn n+1, without any uncertainty about it at all.
As illustration, compare the example in Figure 5,
where uncertainty is accurately classified, with the
example in Figure 6, where uncertainty is misclas-
sified (see Appendix). As shown, the uncertainty in
student-manual1 signals that further explanation is
needed. When received (Figure 5) the student still
makes a math error on the next question, but s/he
appears to understand the task. In contrast, when the
uncertainty adaptation is erroneously not received
(Figure 6), there is no indication that the student?s
understanding has increased; s/he appears to be sim-
ply repeating the number 9.8 (a number which ap-
pears frequently in Newtonian physics). User uncer-
tainty misclassification in other domains could have
8Since the ANOVA results were non-significant, no figure
or correlations are discussed.
9As noted in Section 2, adjacent turns within a dialogue will
either address the same or closely related topics.
221
similar effects; in general, if a user is uncertain in
turn n about how to perform a task, and the system
moves on without supplying information to resolve
this uncertainty, there may be an immediate negative
impact if that knowledge is required or presupposed
again in turn n+1.
Accurate Certainty Classification: Turning now
to manually labeled certain+correct answers, Fig-
ure 4 presents descriptive statistics for the likelihood
that when accurately classified as certain in turn
n, certain+correct answers will transition to each
of the four manually labeled impasse states in turn
n+1. An ANOVA indicated that there were statisti-
cally significant differences among these likelihoods
(F(3,92)=17.96, p<.01). The most likely transitions
are shown with stripes. More specifically, post-hoc
pairwise tests showed that in turn n+1, a manually
labeled certain+correct answer is significantly more
likely than any other impasse state (p<.01), and all
other impasse states were equally likely. A dialogue
example of the most likely transition after accurately
classified certainty is shown in Figure 7, where it is
compared with the misclassified minimal pair in Fig-
ure 8 (see Appendix).
These results indicate that accurately classifying
and not adapting to certain+correct answers has an
immediate positive impact on the learning process,
by not introducing learning impasses about concepts
already understood. Note however that Pearson?s
correlations for these transitions showed no signif-
icant relation to global performance.
Certainty Misclassification: Again, our results
for accurately classified certain+correct answers are
in sharp contrast with those found for manually la-
beled certain+correct answers misclassified as un-
certain in turn n. An ANOVA indicated that all man-
ually labeled impasse states are equally likely in turn
n+1 (F(3,72)=0.33, p=.80). These results indicate
that misclassifying and erroneously adapting to cer-
tain+correct answers is as likely to have an imme-
diate negative impact on learning as it is to have a
neutral or positive impact. In particular, the misclas-
sification is likely to cause some students to tran-
sition from no impasse to the most severe impasse
state. When they erroneously receive the uncertainty
adaptation, these students go from no impasse at all
in turn n to an incorrect line of reasoning in turn n+1,
Figure 4: Turn n? Turn n+1 Transition Likelihoods (L)
after a manually labeled certain+correct answer in turn n
is accurately classified as certain and does not receive the
uncertainty adaptation
without any uncertainty about it at all.
As illustration, compare the example in Figure 7,
where certainty is accurately classified, with the ex-
ample in in Figure 8, where certainty is misclas-
sified (see Appendix). As shown, the certainty in
student-manual1 signals that no further explanation
is needed so the system can move on (Figure 7).
When the uncertainty adaptation is erroneously re-
ceived even though the student is certain (Figure 8),
this appears to have caused the student to stop pay-
ing close attention and thus provide an obviously in-
correct answer to an easy question. User certainty
misclassification in other domains could have simi-
lar effects; in general, if a user is already certain in
turn n about how to perform a task, and the system
?wastes? his/her time by resupplying information
that is already understood, there may be an imme-
diate negative impact in terms of loss of focus, dis-
engagement, or even decreased understanding, that
cause the task in turn n+1 to be performed incor-
rectly.
4.2 Comparing Non-Adaptive ITSPOKE
As a sanity check, we performed the same trajec-
tory analysis on the dialogues from the non-adaptive
version of the system. The purpose here was to con-
firm the presupposition of the above analysis, that
uncertainty-adaptive ITSPOKE was actually pro-
ducing different local behaviors than non-adaptive
ITSPOKE. In other words, since the non-adaptive
222
system ignores uncertainty, there should be no dif-
ference in transition likelihoods when uncertainty is
accurately classified versus when it is misclassified.
This expectation was borne out. ANOVAs indi-
cated that in the non-adaptive system, a manually
labeled uncertain+correct answer is equally likely
to transition to any of the four manually labeled
impasse states in turn n+1, regardless of whether
it was accurately classified as uncertain in turn n
(F(3,48)=0.25, p=.86) or misclassified as certain in
turn n (F(3,92)=0.07, p=.98). Thus as expected, un-
certain+correct answers in the non-adaptive system
pattern like uncertain+correct answers misclassified
as certain in the uncertainty-adaptive system. In
both cases, we see the same negative immediate per-
formance impact of not giving uncertain+correct an-
swers the uncertainty adaptation.
ANOVAs with post-hoc pairwise tests further in-
dicated that in the non-adaptive system, a manually
labeled certain+correct answer is significantly more
likely to transition to a certain+correct answer than
to any other manually labeled impasse state, regard-
less of whether it was accurately classified as certain
in turn n (ANOVA:(F(3,96)=20.81, p<.001), post-
hoc tests: p<.001) or misclassified as uncertain in
turn n (ANOVA:(F(3,80)=14.00, p<.001), post-hoc
tests: p<.001). Thus as expected, certain+correct
answers in the non-adaptive system pattern like ac-
curately classified certain+correct answers in the
uncertainty-adaptive system. In both cases, we see
the same positive immediate performance impact of
not giving manually labeled certain+correct answers
the uncertainty adaptation.
4.3 Comparing Local and Global Performance
Results
Finally, in analyses such as this one, comparing lo-
cal and global performance results can help pinpoint
specific areas for future system redesign. In our
case, this comparison suggests the most important
aspect to focus on with respect to improving our un-
certainty model.
In particular, as noted in Section 1, we previ-
ously found that the low uncertainty recall of our
system (approximately 20%) had a negative global
performance impact; mistaking so much true uncer-
tainty for certainty substantially reduced the amount
users learned (Forbes-Riley and Litman, 2011a).
We also showed in this prior work that mistaking
certainty for uncertainty did not negatively impact
the amount users learned. These results suggested
that the system should be less cautious in applying
the uncertainty-adaptive behavior; i.e., applying it
whenever there is some possibility that the user is ac-
tually uncertain, even if it means applying it to some
turns that are actually certain.
On the other hand, our local performance analy-
sis in this paper showed that (un)certainty misclas-
sification increased the likelihood of an immediate
negative impact on learning. These results suggest
that the system should be more cautious in applying
the uncertainty-adaptive behavior; i.e., only apply-
ing it when there is a high probability that the user
is actually uncertain.
Together these local and global results suggest
that we should focus on improving uncertainty re-
call without decreasing uncertainty precision, in our
uncertainty model. With this goal in mind, we are
currently exploring the use of features and methods
from recent INTERSPEECH emotion and paralin-
guistic challenges (Schuller et al, 2009; Schuller et
al., 2010).
5 Conclusion and Future Directions
This paper presents an approach for analyzing the
immediate impact of user state misclassifications in
dialogue systems. A ranking of user states is de-
fined, and then user state trajectories are compared
when the first state is accurately classified versus
misclassified. Trajectories are quantified using a
previously proposed metric representing the likeli-
hood of transitioning between states. Comparison
of the two sets of trajectories shows whether mis-
classifications change the likelihood of subsequent
higher or lower ranked states, relative to accurate
classification. We illustrated the approach with an
adaptive tutoring system that automatically detects
and adapts to student uncertainty.
As our results indicate, the approach can be used
to answer questions which global performance anal-
yses overlook. First, the analysis shows whether
user state misclassifications actually matter locally
- whether these errors have an immediate effect on
user behavior or not. Moreover, the analysis can de-
termine whether this effect is positive or negative or
223
neutral. In our tutoring system data, we found that
misclassifying user uncertainty had a negative im-
mediate impact on user behavior, relative to accurate
classification.
The analysis can also confirm that a dialogue in-
tervention actually changes user behaviors. In our
tutoring system data, we found that the adaptive sys-
tem yielded significantly different user state trajec-
tories than the non-adaptive system, even though,
as noted in Section 1, our prior global performance
analysis did not show any overall differences among
the global performance metrics that we examined
across the adaptive and non-adaptive systems.
In addition, the analysis can confirm that a dia-
logue intervention shifts user behaviors in the de-
sired direction. In our tutoring system data, we
found that the immediate effect of accurately adapt-
ing to uncertainty was most likely to be continued
uncertainty. Although the adaptation does not yield
an immediate transition to the highest ranked user
state, the outcome is clearly more positive than that
of ignoring uncertainty, which increases the likeli-
hood of transitioning to the lowest ranked user state.
Finally, the local performance results can shed
light on the steps needed to improve global perfor-
mance, by investigating how the two are related. In
our tutoring system data, we found that there is not a
one-to-one relationship between the most beneficial
local and global outcomes. In particular, transition-
ing directly to the highest ranked (no impasse) state
after receiving the uncertainty adaptation was neg-
atively correlated to global learning gain. We hy-
pothesized that looking at wider transition windows
(e.g., trigrams) will shed light on what is the best
local path to the highest ranked state.
We conclude by emphasizing that state trajecto-
ries can be used to represent abstractions over var-
ious types of user (or system) behaviors, in various
domains, whenever their local performance impact
is viewed as important to understand.
Acknowledgments
This work is funded by NSF awards #0914615 and
#0631930. We thank Reva Freedman and the IT-
SPOKE group for comments.
References
A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. Noth.
2003. How to find trouble in communication. Speech
Communication, 40:117?143.
S. Craig, A. Graesser, J. Sullins, and B. Gholson. 2004.
Affect and learning: an exploratory look into the role
of affect in learning with AutoTutor. Journal of Edu-
cational Media, 29(3).
S. D?Mello, R. S. Taylor, and A. Graesser. 2007. Mon-
itoring affective trajectories during complex learning.
In Proc. Cognitive Science Society.
K. Forbes-Riley and D. Litman. 2011a. Benefits and
challenges of real-time uncertainty detection and adap-
tation in a spoken dialogue computer tutor. Speech
Communication. In Press.
K. Forbes-Riley and D. Litman. 2011b. Designing and
evaluating a wizarded uncertainty-adaptive spoken di-
alogue tutoring system. Computer Speech and Lan-
guage, 25(1):105?126.
P. Jordan, B. Hall, M. Ringenberg, Y. Cui, and C.P. Rose.
2007. Tools for authoring a dialogue agent that par-
ticipates in learning studies. In Proc. Artificial Intelli-
gence in Education.
B. Kort, R. Reilly, and R. Picard. 2001. An affective
model of interplay between emotions and learning :
Reengineering educational pedagogy-building a learn-
ing companion. In Proc. IEEE Conference on Ad-
vanced Learning Technology.
S. W. McQuiggan, J. L Robison, and J. C. Lester. 2008.
Affective transitions in narrative-centered learning en-
vironments. In Proc. Intelligent Tutoring Systems
Conference.
Heather Pon-Barry, Brady Clark, Elizabeth Owen Bratt,
Karl Schultz, and Stanley Peters. 2004. Evaluating the
effectiveness of SCoT:a spoken conversational tutor.
In Proceedings of ITS Workshop on Dialogue-based
Intelligent Tutoring Systems.
H. Pon-Barry, K. Schultz, E. Bratt, B. Clark, and S. Pe-
ters. 2006. Responding to student uncertainty in spo-
ken tutorial dialogue systems. International Journal
of Artificial Intelligence in Education, 16.
B. Schuller, S. Steidl, and A. Batliner. 2009. The Inter-
speech 2009 emotion challenge. In Proc. Interspeech.
B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev-
illers, C. Muller, and S. Narayanan. 2010. The IN-
TERSPEECH 2010 paralinguistic challenge. In Proc.
Interspeech.
K. VanLehn, P. Jordan, C. Rose?, D. Bhembe, M. Bo?ttner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, R. Srivastava, and R. Wil-
son. 2002. The architecture of Why2-Atlas: A coach
for qualitative physics essay writing. In Proc. Intelli-
gent Tutoring Systems Conference.
224
K. VanLehn, S. Siler, and C. Murray. 2003. Why do
only some events cause learning during human tutor-
ing? Cognition and Instruction, 21(3).
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000a. Towards developing general models of
usability with PARADISE. Natural Language Engi-
neering, 6(3).
Marilyn A. Walker, Irene Langkilde, Jerry Wright, Allen
Gorin, and Diane Litman. 2000b. Learning to pre-
dict problematic situations in a spoken dialogue sys-
tem: Experiments with How May I Help You? In
Proceedings of the 1st Meeting of the North American
Chapter of the Association of Computational Linguis-
tics (NAACL).
J. A. Walonoski and N. T. Heffernan. 2006. Detec-
tion and analysis of off-task gaming behavior in in-
telligent tutoring systems. In Proceedings of the 8th
International Conference on Intelligent Tutoring Sys-
tems (ITS), pages 382?391, Jhongli, Taiwan.
225
APPENDIX
tutor1: If an object in freefall begins with a velocity of zero, what?ll its velocity be a second later? Please specify the
units too.
student-automatic1: 9.8 meters per second (uncertain+correct)
student-manual1: 9.8 meters per second?? (uncertain+correct)
tutor2: Fine. Because it?s in freefall, its acceleration is 9.8m/s2. This tells us that every second, its velocity increases
by 9.8 m/s. Therefore, at one second its velocity is 0 m/s, and a second later, its velocity is 9.8m/s. So, what?ll its
velocity be a second after that?
student-manual2: 18.6 meters per second?? (uncertain+incorrect)
Figure 5: Annotated Dialogue Example of Most Likely Transition after Accurate Uncertainty Classification
tutor1: If an object in freefall begins with a velocity of zero, what?ll its velocity be a second later? Please specify the
units too.
student-automatic1: 9.8 meters per second (certain+correct)
student-manual1: 9.8 meters per second?? (uncertain+correct)
tutor2: Fine. So, what?ll its velocity be a second after that?
student-manual2: 9.8 meters per second (certain+incorrect)
Figure 6: Annotated Dialogue Example of Negative Impact of Uncertainty Misclassification
tutor1: There?s only one horizontal force exerted on the truck during the collision with the car. What is it?
student-automatic1: impact force (certain+correct)
student-manual1: the impact force (certain+correct)
tutor2: Right. Now let?s find the forces exerted on the truck in the vertical direction during the collision. First, what
vertical force is always exerted on an object near the surface of the earth?
student-manual2: gravity (certain+correct)
Figure 7: Annotated Dialogue Example Showing Most Likely Transition after Accurate Certainty Classification
tutor1: There?s only one horizontal force exerted on the truck during the collision with the car. What is it?
student-automatic1: impact force (uncertain+correct)
student-manual1: the impact force (certain+correct)
tutor2: Right. The problem statement says the car and the truck are both traveling in a straight horizontal line when
they collide. So the impact force of the truck on the truck must be in the horizontal direction. It also says they are
driving on a patch of frictionless ice when they collide. So there is no friction to take into account. This means that
the impact force is the only horizontal force on the truck. Now let?s find the forces exerted on the truck in the vertical
direction during the collision. First, what vertical force is always exerted on an object near the surface of the earth?
student-manual2: normal force (certain+incorrect)
Figure 8: Annotated Dialogue Example Showing Negative Impact of Certainty Misclassification
226
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 312?318,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Examining the Impacts of Dialogue Content and System Automation on
Affect Models in a Spoken Tutorial Dialogue System
Joanna Drummond
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
jmd73@cs.pitt.edu
Diane Litman
Department of Computer Science
Learning Research & Development Ctr.
University of Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Abstract
Many dialogue system developers use data
gathered from previous versions of the dia-
logue system to build models which enable the
system to detect and respond to users? affect.
Previous work in the dialogue systems com-
munity for domain adaptation has shown that
large differences between versions of dialogue
systems affect performance of ported models.
Thus, we wish to investigate how more mi-
nor differences, like small dialogue content
changes and switching from a wizarded sys-
tem to a fully automated system, influence the
performance of our affect detection models.
We perform a post-hoc experiment where we
use various data sets to train multiple mod-
els, and compare against a test set from the
most recent version of our dialogue system.
Analyzing these results strongly suggests that
these differences do impact these models? per-
formance.
1 Introduction
Many dialogue system developers use data gathered
from previous versions of a system to train models
for analyzing users? interactions with later versions
of the system in new ways, e.g. detecting users? af-
fect enables the system to respond more appropri-
ately. However, this training data does not always
accurately reflect the current version of the system.
In particular, differences in the levels of automa-
tion and the presentation of dialogue content com-
monly vary between versions. For example, Raux et
al (2006) changed dialogue strategies for their Let?s
Go bus information system after real-world testing.
Previous work in dialogue systems with regards to
analyzing the impact of using differing training data
has primarily been in the domain adaptation field,
and has focused on two areas. First, previous work
empirically analyzed the need for domain adapta-
tion, i.e. methods for porting existing classifiers
to unrelated domains. For example, Webb and Liu
(2008) developed a cue-phrase-based dialogue act
classifier using the Switchboard corpus, and tested
on call center data. While this performed reason-
ably, training on the call center corpus and testing
on Switchboard performed poorly.
The second research direction involves propos-
ing methods for domain adaptation. Margolis et
al. (2010) observed similar poor performance when
porting their dialogue act classifier between three
corpora: Switchboard, the Meeting Recorder Dia-
log Act corpus, and a machine-translated version of
the Spanish Callhome corpus. They report promis-
ing results through varying their feature set. Blitzer
et al (2007) also observed poor performance and
the need for adaptation when porting product review
sentiment classifiers. They used four review corpora
from Amazon (books, DVDs, electronics, and small
appliances), which yielded 12 cross-domain train-
ing/testing pairs. Their algorithmic adaptation meth-
ods showed promising results.
Our work is in the first direction, as we also em-
pirically analyze the impact of differences in train-
ing and testing corpora to demonstrate the need for
adaptation methods. However, our work differs from
domain adaptation, as the corpora in this experiment
all come from one intelligent spoken physics tutor.
Instead, we analyze differences resulting from vary-
312
ing levels of automation and small changes in dia-
logue content between versions of our system.
With respect to analyzing automation, we em-
pirically compare the impact of differences in train-
ing on data from wizarded (WOZ) versus fully au-
tomated systems. Though many systems use data
from a WOZ version of the system to train models
which are then used in fully automated versions of
the system, the effectiveness of this method of dia-
logue system development has not been tested. We
hypothesize that models built with automated data
will outperform models built with wizarded data.
Additionally, minor dialogue content changes
typically exist between versions of systems. While
large changes, like changing domains, have been
shown to affect model performance, no work has in-
vestigated the impact of these more minute changes.
We hypothesize that these differences in dialogue
content presentation will also affect the models.
Finally, the amount of training data is a well
known factor which affects performance of models
built using supervised machine learning. We hy-
pothesize that combining some, but not all, types of
training corpora will improve the performance of the
trained models, e.g. adding automated data to WOZ
data will improve performance, as this provides fully
automated examples. We hypothesize only provid-
ing more WOZ data will not be as useful.
2 Data
The data used for this work comes from two prior
experiments using ITSPOKE, a spoken tutorial dia-
logue system, which tutors physics novices. Table
1 describes all data used, displaying the number of
users per data set, the number of dialogues between
the system and each user, the total number of user
turns per corpus, and the percentage of turns labeled
uncertain. See Appendix A for more information.
The first experiment, in 2007, compared two
dialogue-based strategies for remediating user un-
certainty over and above correctness (Forbes-Riley
and Litman, 2011b). The goal of this work was to
not only test the hypothesis that this uncertainty re-
mediation would improve users? learning, but to in-
vestigate what types of dialogue remediation would
improve users? learning the most. Since this experi-
ment, WOZ-07, was designed to be a gold-standard
case of uncertainty remediation, all natural language
understanding and uncertainty annotation was per-
formed by a human wizard, in real time (WOZ). All
annotations were made at the turn-level.
For WOZ-07, users? dialogue interactions with
the system would change based on which remedia-
tion strategy they were assigned to. There were two
different dialogue-based remediation strategies. In
addition to varying the strategies, the two control
conditions in this experiment also varied when the
remediation strategy was applied.
The simple remediation dialogue strategy pro-
vided additional information about the physics con-
cept the user was struggling with, or asked them
further questions about the concept. Both control
conditions used the simple remediation strategy; one
only applied the strategy when the user was incor-
rect, the other applied it if the user was incorrect and
randomly when the user was correct. The simple re-
mediation experimental condition applied the reme-
diation when the user was incorrect, or correct but
uncertain about their answer. The fourth condition
in WOZ-07 used the second dialogue strategy, com-
plex remediation. This strategy changed the way the
remediation was presented, depending on a combi-
nation of the user?s correctness and certainty in their
answer. Only users in the simple remediation exper-
imental condition learned more than users in other
conditions. Figure 1 shows an example of simple re-
mediation; the tutor acknowledges that the user is in-
correct, saying ?Well...?, and then explains the con-
cept the previous question tested. Appendix B com-
pares simple and complex remediation strategies.
Another experiment was performed in 2008,
where users interacted with either a fully automated
(ASR) version of ITSPOKE or a wizarded version.
The goal of this experiment was to see if the learn-
ing gains found in the 2007 experiment would hold
in the ASR version of the system. To mimic the
WOZ-07 experiment, the wizarded version (WOZ-
08) only used the simple remediation experimental
condition found in WOZ-07, while the ASR ver-
sion contained the simple remediation experimental
condition and both simple remediation control con-
ditions. The complex remediation strategy was not
included due to its poor performance in WOZ-07.
Thus, WOZ-08 and ASR-08 used identical dialogue
strategies, with minor differences in where the reme-
313
TUTORp5: (Response to an incorrect answer) Well...
We just discussed that by Newton?s Third law, when two
objects collide, the forces they exert on each other are
equal in magnitude and opposite in direction. This is true
regardless of the objects? differing masses. So the first
question?s answer is that the impact forces on the truck
and the car will have the same magnitude but opposite
direction. Now, the second question asks about the ve-
hicles? change in motion. We can use Newton?s Second
law to answer this. What does this law say?
TUTORp6: (Response to a correct, certain answer) Fine.
So the first question?s answer is that the impact forces on
the bus and the motorbike will have the same magnitude,
but opposite direction. Now, the second question asks
about the vehicles? change in motion. We can use New-
ton?s second law to answer this. What does this law say?
Figure 1: Corpus Excerpt: Remediation in Dialogue 5,
and No Remediation in Isomorphic Dialogue 6
diation would be applied. For the ASR conditions,
all models were trained on WOZ-07 data; users were
randomly assigned to the WOZ-08 or ASR-08 con-
dition as they participated.
In addition to eliminating the complex remedia-
tion condition, a sixth dialogue, completely isomor-
phic to the fifth dialogue, was added to all condi-
tions. See Appendix B dialogue examples, high-
lighting their content differences. Figure 1 displays
two ASR-08 tutor turns with the same user. These
turns are from the fifth problem, and the isomorphic
sixth problem. Note that two things change between
these two answers. First, the system responds to the
user?s incorrectness in the first example. Had the
user been correct and uncertain, this is also the di-
alogue s/he would have seen. Second, notice that
problem five discusses a car, while problem six dis-
cusses a motorcycle. To create a completely iso-
morphic problem, the scenario for the dialogue was
changed from a car to a motorcycle.
For both the 2007 and 2008 corpora, all gold-
standard uncertainty annotations were performed by
a trained human annotator. Development and pre-
vious testing of the annotation scheme between this
annotator and another trained annotator resulted in
kappa = 0.62. All wizarded conditions were an-
notated in real-time; all ASR conditions were anno-
Data Set #Usr #Dia #Turn %Unc
WOZ-07 81 5 6561 22.73
WOZ-08 19 6 1812 21.85
ASR-08 72 6 7216 20.55
ASR-08-Train 19 6 1911 21.51
ASR-08-Test 53 6 5305 20.21
Table 1: Description of data sets
tated in a post-hoc manner.
In sum, the main differences between the two sys-
tems? data are differences in automation (i.e. WOZ
and ASR) and content (i.e. presentation of content,
reflected by differing dialogue strategies, and num-
ber of physics dialogues).
3 Post-Hoc Experiment
In this post-hoc analysis, we will analyze the im-
pact of content differences by comparing the perfor-
mance of models built with WOZ-07 and WOZ-08,
and automation differences by comparing models
built with WOZ-08 and ASR-08 data. Instead of the
original study design, where WOZ-08 and ASR-08
subjects were run in parallel, we could have gathered
the WOZ data first, and used the WOZ data and the
first few ASR users for system evaluation and devel-
opment purposes. Thus, for the post-hoc analysis,
we mimic this by using WOZ-08 as a training set,
and splitting ASR-08 into two data sets?ASR-08-
Train (the first few users), and ASR-08-Test. (Please
see the last two rows of Table 1.) We held out the
first 19 users for ASR-08-Train, since this approx-
imates the amount of data used to train the model
built with WOZ-08. For our post-hoc study, the re-
maining 53 ASR users were used as a test set for
all training sets, to mimic an authentic development
lifestyle for a dialogue system. Additionally, this
guaranteed that no users appear in both the training
and testing set given any training set.
As all uncertainty remediation happens at the
turn-level, we classified uncertainty at the turn-level,
and compared these automated results with the gold-
standard annotations. We used all the features that
were designed for the original model. Since previ-
ous experiments with our data showed little variance
between different machine learning algorithms, we
chose a J48 decision tree, implemented by WEKA,1
1http://www.cs.waikato.ac.nz/ml/weka/
314
for all experiments due to its easy readability. Since
our class distribution is skewed (see Table 1), we
also used a cost matrix which heavily penalizes clas-
sifying an uncertain instance as certain.
We use simple lexical, prosodic and system-
specific features described in (Forbes-Riley and Lit-
man, 2011a) to build our models. These features
were kept constant through all experiments, so the
results could be directly comparable. For all lexical
features for all data sets, ASR text was used.2 For all
WOZ conditions, we gathered ASR text post-hoc.
We trained models on individual training sets, to
inspect the impact of content and automation dif-
ferences. We then trained new models on combi-
nations of these original training sets, to investigate
possible interactions. To allow for direct compari-
son, we used ASR-08-Test to evaluate all models.
Since detecting uncertainty is related to detecting
affective user states, we use the evaluation measures
Unweighted Average (UA) Recall and UA Precision,
presented in (Schuller et al, 2009).We also use UA
F-measure. Note that because only one hold-out
evaluation set was used, rather than using multiple
sets for cross-fold validation, we do not test for sta-
tistical significance between models? results.
4 Results
The first three rows of Table 2 present the results of
training a model on each possible training set indi-
vidually. Note that the number of instances per train-
ing set varies. WOZ-07 simply has more users in the
training set than WOZ-08 or ASR-08-Train. While
WOZ-08 and ASR-08-Train have the same number
of users, the number of turns slightly varies, since
dialogues vary depending on users? answers.
When comparing WOZ-08 to WOZ-07, first no-
tice that WOZ-08 outperforms WOZ-07 with a
much smaller amount of data. Both are wiz-
arded versions, but content differences exist be-
tween these experiments; WOZ-08 only used the
simple remediation strategy, and added a dialogue.
When comparing ASR-08-Train to the other two
individual training sets, note that it best approxi-
mates the test set. This training condition outper-
forms all others, while using less data than WOZ-
2We used ASR instead of manual transcriptions, to better
approximate automated data.
07. While WOZ-08 and ASR-08 have the same
content, the system changes from wizarded to au-
tomated language recognition. This allows us to di-
rectly compare how differences due to automation
(e.g. errors in detecting correct answers) can affect
performance of the models. Note that even though
we used ASR transcriptions of WOZ-08 turns, the
effects of ASR errors on later utterances are only
propagated in ASR-08-Train. As ASR-08-Train no-
ticeably outperforms WOZ-08, with approximately
the same amount of training data, we conclude that
using automated data for training better prepares the
model for the data it will be classifying.
As we also wish to investigate how incorporat-
ing more diverse training data would alter the per-
formance of the model, we combined ASR-08-Train
and WOZ-08 with the WOZ-07 training set, shown
in Table 2. We combined these sets practically, as
we wish to test how our model could have performed
if we had used our first few 2008 users to train the
model in the actual 2008 experiment.
First, note that all combination training sets out-
perform individual training sets. As ASR-08-Train
outperformed WOZ-08 for individual training sets,
it is not surprising that WOZ-07+ASR-08-Train out-
performs WOZ-07+WOZ-08.
However, we could have used WOZ-07 for feature
development only, and trained on WOZ-08 + ASR-
08-Train. Since the training and testing sets contain
identical content, it is unsurprising that the preci-
sion for this classifier is high. This classifier does
not perform as well with respect to recall, perhaps
since its training data is not as varied. Also note,
while this model trained on few data points, we used
additional data for feature development purposes.
Combining all three possible training sets does
not outperform WOZ-07+ASR-08-Train; it per-
forms equivalently, and uses much more data. We
hypothesize that, since WOZ-07 constitutes the ma-
jority of the training set, the benefit of including
WOZ-08 may be mitigated. Downsampling WOZ-
07 could test this hypothesis. Alternatively, the ben-
efit of combining WOZ-07+ASR-08-Train could be
that we provide many varied examples in this com-
bined training set. Since WOZ-07 already accounts
for differences in both content and automation,
WOZ-08 doesn?t introduce novel examples for the
classifier, and adding it may not be beneficial.
315
Training Set n UA Rec. UA Prec. UA F1
WOZ-07 6561 54.6% 53.0% 53.79%
WOZ-08 1812 58.0% 55.4% 56.67%
ASR-08-Train 1911 60.5% 57.2% 58.80%
WOZ-07 + WOZ-08 8373 66.1% 61.0% 63.45%
WOZ-07 + ASR-08-Train 8472 68.3% 63.5% 65.81%
WOZ-08 + ASR-08-Train 3723 64.0% 73.4% 68.38%
WOZ-07 + WOZ-08 + ASR-08-Train 10284 68.3% 63.6% 65.86%
Table 2: Results; Testing on ASR-08-Test (n = 5305). Bold denotes best performance per metric.
In sum, different training set combinations pro-
vide different benefits. With respect to UA F1 and
UA Precision, WOZ-08 + ASR-08-Train outper-
forms all other training sets. Using only 3723 turns
to train the model, this configuration uses the least
amount of training data. However, this requires pre-
viously collected data, such as WOZ-07, for fea-
ture development purposes. Alternatively, WOZ-
07 + ASR-08-Train performs better than WOZ-08 +
ASR-08-Train with respect to UA Recall, and does
not require a separate feature development set. Thus,
the ?best? training set would depend on both the ex-
perimental design, and the preferred metric.
5 Discussion and Future Work
In this paper, we provided evidence that the degree
of automation of a system used to collect training
data can impact the performance of a model when
used in a fully automated system. Since one com-
mon technique of building fully automated dialogue
systems uses a semi-automated wizarded version,
this result suggests incorporating a small amount of
automated data could greatly improve performance
of the models. Our results also suggest that the type
of data is more important than the quantity when
building these models, since well-performing mod-
els were built with small amounts of data. We also
investigated the impact of building models trained
with different dialogue content, another common
method of developing dialogue systems. As the
WOZ-08 model outperforms the WOZ-07 model, it
appears that this has a noticeable impact.
However, the WOZ-08 and WOZ-07 experiments
may not have had identical user population, due to
the timing differences between studies. We wish
to perform further post hoc-experiments to analyze
the impact of population differences in our data. To
do so, we will eliminate all dialogue strategy dif-
ferences between WOZ-07 and WOZ-08. To fur-
ther support our results regarding content differ-
ences, we wish to split WOZ-08 into two training
sets, one including the sixth problem, and one ex-
cluding it. After controlling for differences in quan-
tity of data, we will analyze the resulting models.
To further strength our results regarding automa-
tion differences, we will eliminate all differences in
when the remediation dialogue strategy was applied
between the WOZ-08 and ASR-08-Test corpus, and
try to replicate the results found in this paper.
As our results suggest the need for applying do-
main adaptation methods to improve models? per-
formance when there are differences in automation
and content, future work could investigate applying
already existing methods for domain adaptation, and
developing new ones for this problem. In particular,
the results we presented suggest a method for build-
ing a dialogue system that could mitigate the effects
of changes in automation and content. A small wiz-
arded condition, with changes in dialogue content,
could be used for feature development. This data, or
data from another small wizarded condition, could
then be used to train a preliminary model. This pre-
liminary model could be tested with a small num-
ber of users using an automated version. Then, the
data from the preliminary conditions could be used
to build the final model, which would be used for the
current, fully automated version of the system.
Acknowledgments
We thank Michal Valko, Michael Lipschultz,
Wenting Xiong, and the ITSPOKE group for helpful
comments and suggestions, and an REU supplement
to NSF Grant #0631930 for funding this work.
316
References
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Annual
Meeting-Association For Computational Linguistics,
volume 45, page 440.
K. Forbes-Riley and D. Litman. 2011a. Benefits and
challenges of real-time uncertainty detection and adap-
tation in a spoken dialogue computer tutor. Speech
Communication.
K. Forbes-Riley and D. Litman. 2011b. Designing and
evaluating a wizarded uncertainty-adaptive spoken di-
alogue tutoring system. Computer Speech & Lan-
guage, 25(1):105?126.
A. Margolis, K. Livescu, and M. Ostendorf. 2010. Do-
main adaptation with unlabeled data for dialog act tag-
ging. In Proceedings of the 2010 Workshop on Do-
main Adaptation for Natural Language Processing,
pages 45?52. Association for Computational Linguis-
tics.
A. Raux, D. Bohus, B. Langner, A.W. Black, and M. Es-
kenazi. 2006. Doing research on a deployed spoken
dialogue system: One year of Lets Go! experience. In
Proc. Interspeech, pages 65?68. Citeseer.
B. Schuller, S. Steidl, and A. Batliner. 2009. The in-
terspeech 2009 emotion challenge. In Tenth Annual
Conference of the International Speech Communica-
tion Association.
N. Webb and T. Liu. 2008. Investigating the portability
of corpus-derived cue phrases for dialogue act classifi-
cation. In Proceedings of the 22nd International Con-
ference on Computational Linguistics-Volume 1, pages
977?984. Association for Computational Linguistics.
317
Data Set Dialogue #5 Dialogue #6Avg. Avg. Avg. Avg.
#Turn %Unc #Turn %Unc
WOZ-07 15.21 26.87 N/A N/A
WOZ-08 14.37 28.21 13.42 9.02
ASR-08 16.26 19.90 16.28 9.73
ASR-08
-Train 16.05 19.67 16.00 11.84
ASR-08
Test 16.38 19.98 16.38 8.99
Table 3: Dialogue-level description of corpora
Appendix A Differences between corpora
We note that one possible difficulty in using dia-
logues with differing content could be differing lev-
els of users? uncertainty between those dialogues.
Thus, Table 3 depicts the average percent of uncer-
tain turns over all users, per dialogue. Only Dia-
logues #5 and #6 are presented; average number of
turns and average percent uncertainty do change be-
tween all problems. While the average number of
turns are similar, the percentage of uncertain turns
drops greatly between the two isomorphic problems.
Appendix B Dialogue Examples
We also present dialogue examples from our cor-
pora to illustrate various content differences be-
tween versions of ITSPOKE. The changes between
Dialogues #5 and #6 are as follows: every instance
of ?car? in Dialogue #5 with ?motorbike,? and ev-
ery instance of ?truck? is replaced with ?PAT bus.?
Since users discuss Problem #6 with ITSPOKE im-
mediately after discussing Problem #5, the drop in
percentage of user uncertainty between Dialogue #5
and Dialogue #6 show in Table 3 might be explained
by this. However, all of the domain-specific words
and all of the knowledge concepts will remain the
same between these two problems.
Figures 1 (in the main paper) and 2 give examples
of the two types of remediation found in the 2007
corpus. ITSPOKE can change its dialogue based on
not only how it presents the content, but also what
feedback it gives. Complex remediation changes
both of these attributes. Table 4 displays possible
feedbacks given to the user, depending on their lev-
els of correctness and certainty. In Figure 2, the seg-
Tutor: What?s the overall net force on the truck equal to?
User: Zero?? [Incorrect & Uncertain]
Tutor: That?s not correct, but don?t worry. You seem
to be aware of your mistake. Let?s resolve it. [FEED-
BACK] The net force on the truck is equal to the impact
force on it. Let?s walk through this answer step by step.
[NEW SHORT ANSWER] We can derive the net force on
the truck by summing the individual forces on it, just like
we did for the car. First, what horizontal force is exerted
on the truck during the collision? [EXISTING SUBDIA-
LOGUE]
Figure 2: Example of Complex uncertainty remediation.
User Answer Examples of
Feedback Phrases
Simple Complex
Correct & That?s That?s right.
Certain right.
Correct & That?s That?s right, but you don?t
Uncertain right. sound very certain, so let?s
recap.
Incorrect & Well... Good try, but that?s not
Uncertain right. It sounds like you
knew there might be an
error in your answer.
Let?s fix it.
Incorrect & Well... I?m sorry, but there?s a
Certain mistake in your answer that
we need to work out.
Table 4: Example Feedback Phrases used in Simple and
Complex Remediation
ment of the tutor?s turn is labeled after that segment
is completed (e.g. the Feedback is ?That?s not cor-
rect... resolve it.?). The type of remediation can also
change. While Figure 1 depicts the normal remedi-
ation path as if the user had answered incorrectly or
correct but uncertain, complex remediation, shown
in Figure 2, first gives the user a short version of the
answer that they should have given, before moving
down the normal remediation path.
318
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), page 197,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Cohesion, Entrainment and Task Success in Educational Dialog
Diane Litman
Department of Computer Science and LRDC
University of Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Researchers often study dialog corpora to better understand what makes some dialogs more successful than
others. In this talk I will examine the relationship between coherence/entrainment and task success, in
several types of educational dialog corpora: 1) one-on-one tutoring, where students use dialog to interact
with a human tutor in the physics domain, 2) one-on-one tutoring, where students instead interact with
a spoken dialog system, and 3) engineering design, where student teams engage in multi-party dialog to
complete a group project. I will first introduce several corpus-based measures of both lexical and acoustic-
prosodic dialog cohesion and entrainment, and extend them to handle multi-party conversations. I will then
show that the amount of cohesion and/or entrainment positively correlates with measures of educational task
success in all of our corpora. Finally, I will discuss how we are using our findings to build better tutorial
dialog systems.
197
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 217?226,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Adapting to Multiple Affective States in Spoken Dialogue
Kate Forbes-Riley
Learning R&D Ctr (LRDC)
University of Pittsburgh
Pittsburgh, PA 15260, USA
forbesk@cs.pitt.edu
Diane Litman
LRDC and Dept. Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
litman@cs.pitt.edu
Abstract
We evaluate a wizard-of-oz spoken dialogue
system that adapts to multiple user affective
states in real-time: user disengagement and
uncertainty. We compare this version with the
prior version of our system, which only adapts
to user uncertainty. Our analysis investigates
how iteratively adding new affect adaptation
to an existing affect-adaptive system impacts
global and local performance. We find a sig-
nificant increase in motivation for users who
most frequently received the disengagement
adaptation. Moreover, responding to disen-
gagement breaks its negative correlations with
task success and user satisfaction, reduces un-
certainty levels, and reduces the likelihood of
continued disengagement.
1 Introduction
State of the art spoken dialogue system research fo-
cuses on responding not only to the literal content
of users? speech but also to their affective state1,
such that the same literal content may receive one
system response when the user is frustrated, and
another when the user is confused, etc. The po-
tential benefits are clear: affect-adaptive systems
can increase task success (Forbes-Riley and Litman,
2011a; Wang et al, 2008) and other global perfor-
mance metrics such as user satisfaction (Liu and Pi-
card, 2005; Klein et al, 2002) and motivation (Aist
1We use affect for emotions and attitudes that affect how
users communicate. Other speech researchers also combine
concepts of emotion, arousal, and attitudes where emotion is
not full-blown (Cowie and Cornelius, 2003).
et al, 2002). However, to date most researchers have
focused on adapting to a single affective state. The
next step is thus to develop and evaluate spoken dia-
logue systems that respond to multiple user affective
states. The problem of how to develop effective af-
fect adaptations is a complex one even as applied
to a single affective state, and it multiples with ev-
ery new state added. For example, it is not clear
a priori how responding to one affective state may
impact another?s frequency and relationship to per-
formance. In this paper we examine this problem
in the context of the computer tutoring domain. We
previously showed that adapting to user uncertainty
during spoken dialogue computer tutoring improves
task success, both in a wizard-of oz version where
a hidden human performed the affect detection and
natural language understanding (Forbes-Riley and
Litman, 2011b), as well as in a fully automated sys-
tem version (Forbes-Riley and Litman, 2011a).
We are now taking the next step by incorporating
adaptation to a second user affective state: user dis-
engagement. We target user disengagement for two
reasons: first, our prior manual annotation showed
disengagement and uncertainty to be the most fre-
quent user affective states that occur in our system,
and second, our prior analyses show that the occur-
rence of disengagement is negatively correlated with
task success and user satisfaction (Forbes-Riley and
Litman, 2012).2 Thus, we hypothesized that provid-
ing appropriate system responses to both affective
states could have multiple benefits: 1) reduce the
frequency of one or both states, 2) ?break? the nega-
2Redesigning a system in light of correlational analyses can
improve performance (Rotaru and Litman, 2009).
217
tive correlations with performance, and 3) yield fur-
ther improvements in global and local performance.
In this paper, we test these hypotheses, present-
ing the results of a controlled experiment evaluating
a wizard-of-oz version of our spoken dialogue com-
puter tutor that adapts to both user uncertainty and
user disengagement (Section 3). Although we ad-
dress these states within the tutoring domain, speech
researchers from other domains and applications are
also focusing on detecting and adapting to user dis-
engagement (e.g., (Schuller et al, 2010; Wang and
Hirschberg, 2011)) and uncertainty (e.g. (Pon-Barry
and Shieber, 2011; Paek and Ju, 2008)) to improve
system performance. Our results should be of in-
terest not only to these researchers but also more
generally to any researchers working towards com-
prehensive affect-adaptive spoken dialogue systems.
In particular, our results show that iteratively adding
new affect adaptations to an existing affect-adaptive
system can yield performance improvements. We
find no increase (but also no decrease) in task suc-
cess or user satisfaction, but we do find an increase
in motivation for users who most frequently received
the disengagement adaptation (Section 4). Further-
more, we find that responding to disengagement
?breaks? negative correlations with task success and
user satisfaction (Section 5), and also yields a reduc-
tion both in uncertainty levels (Section 4) and in the
likelihood of continued disengagement (Section 6).
2 Related Work
User disengagement is highly undesirable because
of its potential to increase dissatisfaction and task
failure, and there is a growing awareness of its
potential to negatively impact commercial applica-
tions; thus there has been substantial prior work
focused on detecting disengagement (along with
the closely related states of boredom and lack of
interest) (e.g., (Schuller et al, 2010; Wang and
Hirschberg, 2011; Bohus and Horvitz, 2009)). To
date, however, only a few disengagement-adaptive
systems have been evaluated, and within the tutoring
domain these have focused on only one disengage-
ment behavior: gaming. For example, responding to
gaming with supplementary material reduced gam-
ing and improved task success for users who most
frequently gamed (Baker et al, 2006), while adding
progress reports and productive learning tips at the
end of problems (i.e., without specifically targeting
gaming instances) increased task success, engage-
ment, and user satisfaction (Arroyo et al, 2007).
Our research builds on this work but is novel in that
we focus on speech and dialogue-based disengage-
ment and on adapting to multiple affective states.
More generally, while substantial spoken dia-
logue and affective systems research has shown
that users display a range of affective states when
interacting with a system (e.g. (Schuller et al,
2009; Conati and Maclaren, 2009)), to date only
a few systems adapt to multiple affective states
(e.g., (D?Mello et al, 2010; Aist et al, 2002; Tsuka-
hara and Ward, 2001)). Most have been deployed
with wizard-of-oz components, and none have yet
shown significant improvements in task success,
though other benefits have been shown, including
increased user satisfaction (Tsukahara and Ward,
2001), rapport (Acosta and Ward, 2011) and mo-
tivation (Aist et al, 2002). Recently, D?Mello et
al. (2010) showed that performance can depend on
when and to whom the adaptations are provided;
higher expertise users never benefited from system
responses to their frustration, boredom and confu-
sion, while lower expertise users only benefited after
multiple system interactions. While this prior work
showed the benefits of adapting to multiple affec-
tive states as compared to not adapting to affect at
all, it did not test whether these benefits were due
to having multiple adaptations, or if any one would
have sufficed. Our work is novel in explicitly mea-
suring the value of having multiple adaptations as
compared to one.
3 The Experiment
Our prior work showed that our uncertainty-adaptive
spoken dialogue system improves performance over
not adapting to affect (Forbes-Riley and Litman,
2011b; Forbes-Riley and Litman, 2011a); this sys-
tem serves as our baseline in the current work.
3.1 Baseline System: UNC ADAPT ITSPOKE
UNC ADAPT ITSPOKE (Intelligent Tutoring
SPOKEn dialog system)3 tutors 5 Newtonian
3ITSPOKE is a speech-enhanced and modified version of
the Why2-Atlas text-based tutor (VanLehn et al, 2002).
218
physics problems (one per dialogue), using a Tutor
Question - User Answer - Tutor Response format.
In the fully automated system, the speech from
the user?s answer is digitized from head-mounted
microphone input and sent to a speech recognizer.
The answer?s (in)correctness is then automatically
classified based on the recognizer?s transcription us-
ing a semantic analysis component, and the answer?s
(un)certainty is automatically classified by inputting
features of the speech signal (e.g. prosody), the au-
tomatic transcript, and the dialogue context into a
logistic regression model. The (in)correctness and
(un)certainty detection components comprising our
system?s user model are described in detail else-
where (Forbes-Riley and Litman, 2011a).
For the present experiment, the affect and
(in)correctness labeling are performed by a hidden
human wizard. As in our prior work, this allows us
to first analyze the impact of an affect adaptation
separately from the noise introduced by automat-
ing affect and semantic analysis (see Section 7).
Figures 1-3 illustrate the binary (dis)engagement
(ENG, DISE), (in)correctness (COR, INC), and
(un)certainty (CER, UNC) labels.
Finally, the system automatically determines the
appropriate response based on the answer?s labeled
(in)correctness and (un)certainty and this response
is sent to the Cepstral text-to-speech system4, whose
audio output is played through the headphones and
displayed on a web-based interface (see Figure 4).
The uncertainty label and system adaptation are
described in detail elsewhere (Forbes-Riley and Lit-
man, 2011b; Forbes-Riley and Litman, 2011a).
Briefly, the uncertain (UNC) label is used for turns
expressing uncertainty or confusion about the topic
being discussed, and the non-uncertain (CER) label
is used otherwise. The wizard in this experiment
displayed interannotator agreement of 0.85 and 0.62
Kappa on correctness and uncertainty, respectively,
in prior ITSPOKE corpora. Our uncertainty adapta-
tion is based on the hypothesis that uncertainty and
incorrectness are both points of impasse in a dia-
logue, and that providing additional knowledge can
help resolve them. In UNC ADAPT ITSPOKE, in-
correct answers and uncertain answers both receive
(in)correctness feedback (e.g., ?Right? or ?I don?t
4an outgrowth of Festival (Black and Taylor, 1997).
think so?), followed by a (re)statement of the cor-
rect answer. Depending on topic difficulty, the sys-
tem then either provides a brief explanation of rea-
soning (?Bottom Out?) or a more lengthy dialogue
exchange that walks the user through the steps of
the reasoning (?Remediation Subdialogue?). An ex-
ample is shown in Figure 1.
3.2 UNC-DISE ADAPT ITSPOKE
UNC-DISE ADAPT ITSPOKE adds disengage-
ment detection and adaptation to UNC ADAPT IT-
SPOKE. Our disengagement annotation scheme is
described in detail elsewhere (Forbes-Riley and Lit-
man, 2011c). It was derived from empirical obser-
vation of our data and from prior work, including
that mentioned in Section 2 and appraisal theory-
based emotion models, which distinguish emotional
behaviors from their underlying causes (e.g., (Conati
and Maclaren, 2009)). Briefly, the Disengaged
(DISE) label is used for turns expressing moderate
to strong disengagement towards the interaction, i.e.,
responses given without much effort or caring about
appropriateness, and might include signs of bore-
dom or irritation. Clear examples include turns spo-
ken in leaden monotone, with sarcasm, or off-task
sounds such as electronics usage. The wizard in
this experiment displayed interannotator agreement
of 0.55 Kappa on the DISE label in prior ITSPOKE
corpora, which is on par with prior affect research,
where moderate agreement is common given the dif-
ficulty of the task (Forbes-Riley and Litman, 2011c).
Based on the results of the prior research dis-
cussed in Section 2 and our own prior research,
we have developed one class of system responses
for correct+disengaged (COR-DISE) answers and
another for incorrect+disengaged (INC-DISE) an-
swers (Forbes-Riley and Litman, 2011c)5.
Our INC-DISE adaptation builds on the prior
finding that supplementary information can help re-
duce some types of disengagement for highly dis-
engaged users (Baker et al, 2006). We hypothe-
sized that our UNC ADAPT response to incorrect-
ness (a Bottom Out or Remediation Subdialogue)
was insufficient for an INC-DISE turn because the
5Originally we distinguished six DISE types, but found this
too many to be reliably detected automatically and thus reduced
the distinction to two using correctness. Our automatic disen-
gagement detector is discussed further in Section 7.
219
user had already disengaged. To benefit from this
supplementary knowledge, the user first had to reen-
gage. Thus, the UNC-DISE ADAPT system re-
sponds to INC-DISE answers with ?productive in-
teraction feedback?6 followed by an easier ?fill in
the blank? version of the original question. The pur-
pose of this two-pronged response is to regain the
user?s attention with the feedback and then provide
a path through the impasse with the easier ques-
tion, thereby keeping the user engaged. An ex-
ample is shown in Figure 2, where USER-1 is la-
beled INC-DISE because the user gives an irrelevant
(and obviously incorrect) answer. Note that while
most knowledge asymmetry spoken dialogue sys-
tems (e.g., problem-solving and troubleshooting (Ja-
narthanam and Lemon, 2008)) use the concept of
response (in)correctness, a more general version is
response (in)appropriateness, which can be realized
differently across applications, including as the user
turn?s speech recognition score (Kamm et al, 1998).
Since misrecognitions are also a type of dialogue
impasse, a similar version of our INC-DISE adap-
tation could be provided by other spoken dialogue
systems for turns where users disengage and their
response isn?t recognized by the system.
Our COR-DISE adaptation builds on the prior
findings that progress reports and productive learn-
ing tips can positively impact multiple performance
metrics when used without specifically targeting dis-
engagement (Arroyo et al, 2007), but not when
used after every user turn (Walonoski and Heffer-
nan, 2006). We hypothesized that these responses
might be most beneficial if they targeted COR-DISE
turns. Thus, the UNC-DISE ADAPT system re-
sponds to COR-DISE answers with ?productive in-
teraction feedback? followed by a progress report
graphing the user?s correctness both in the current
dialogue and over all prior dialogues. Examples
are shown in Figures 3-4, where USER-1 is labeled
COR-DISE because the user unnecessarily repeats
himself, signaling his lack of interest. As shown,
we distinguish two classes of productive interaction
feedback. That in ?2a? shows the feedback given
when the progress report indicates improvement on
the current dialogue relative to the prior ones, while
6This is our generalization of the concept of ?productive
learning tip? used in prior work (Arroyo et al, 2007).
?2b? shows the feedback given when there is a de-
cline. Note that a similar combination of productive
interaction feedback and progress reports tailored to
the domain (e.g., graphs showing subtasks accom-
plished so far) could be provided by most spoken di-
alogue systems on turns where users disengage and
their response is recognized by the system.7
3.3 Experimental Procedure
College students with no college-level physics were
recruited and randomly assigned to either the
UNC ADAPT or UNC-DISE ADAPT condition af-
ter balancing for user expertise (pretest score) and
gender. Users: (1) read a short physics text, (2) took
a pretest and a pre-motivation survey, (3) worked 5
?training? problem dialogues with the system from
their condition, (4) took a post-motivation survey
and a user satisfaction survey, (5) took a posttest iso-
morphic to the pretest, and (6) worked a ?test? prob-
lem dialogue with UNC ADAPT.
The pre/post tests are the same as those used in
multiple prior ITSPOKE experiments (c.f., (Forbes-
Riley and Litman, 2011a)). The tests are isomor-
phic, each containing 26 multiple choice questions
querying knowledge of the topics covered in the di-
alogues. Average pretest and posttest scores were
53% and 81% (out of 100%), respectively.
The pre/post motivation surveys are a reduced
version of a widely used motivation survey in the
tutoring domain (Pintrich and DeGroot, 1990); our
selected questions were relevant to our system and
also selected in other recent research (Ward, 2010;
Roll, 2009). The two surveys are isomorphic, each
containing 19 statements rated on a 7-point Likert
scale. Average pre and post scores were 68% and
70% (out of 100%), respectively.
The user satisfaction survey was recently devel-
oped and validated for use with spoken dialogue
computer tutors (Dzikovska et al, 2011). It con-
tains 40 statements rated on a 5-point Likert scale.
Average score was 68% (out of 100%).
The ?test? dialogue is isomorphic to the fifth
training dialogue, such that all questions are identi-
cal except for the identities of the objects discussed.
In this way, we can measure how the disengagement
7Note that our DISE and UNC adaptations are combined if
the two states occur simultaneously.
220
adaptations from the fifth dialogue impact user turns
when the questions are repeated in the test dialogue
(where no disengagement adaptation is given). We
have also used this test dialogue in our prior work
(c.f., (Forbes-Riley and Litman, 2011a)).
3.4 Corpus
The resulting corpus contains 228 dialogues (6 per
user) and 3518 turns from 38 users, 22 female and 16
male, with 19 subjects per condition.8 Table 1 shows
the distribution of the labeled turns in the corpus.
Table 1: Corpus Description (N=3518)
Turn Label Total Percent
Disengaged 622 17.7%
Correct 2825 80.3%
Disengaged+Correct 247 7.0%
Uncertain 537 15.3%
4 Global Performance Evaluation
We use the test and survey instruments described in
Section 3.3 to evaluate global performance in UNC-
DISE ADAPT. We measure task success via learn-
ing gain; as is typical in the tutoring community,
we compute normalized learning gain as (posttest-
pretest)/(1-pretest). We compute percent user satis-
faction from the survey as (user score)/(maximum
possible score). We compute raw motivation gain
from the surveys as (post score-pre score).9 For each
metric, we ran a one-way ANOVA with condition as
the between-subjects factor. The first two rows of
Table 2 show the number of users (N), means (Mn)
and standard deviations (sd) for these metrics across
condition. Although UNC-DISE ADAPT shows a
small decrease in means for learning gain and user
satisfaction, there were no significant differences
(p?.05) or trends (p?.10) for differences between
conditions for any global metric.
As a further comparison, we compared the perfor-
mance of UNC-DISE ADAPT to our non-adaptive
wizard-of-oz version of ITSPOKE (NO ADAPT),
using the corpus collected from our prior user
8One outlier with negative learning was removed from each
condition, because our goal is to investigate the role of affect
adaptation when learning is successful.
9Total, average or percent satisfaction yielded comparable
results, as did raw or normalized motivation and learning gains.
study comparing UNC ADAPT and NO ADAPT;
that study showed UNC ADAPT had signifi-
cantly higher learning gain than NO ADAPT
(p=.001) (Forbes-Riley and Litman, 2011b).10 The
goal here was to ascertain in a post-hoc way whether
adapting to multiple affective states yielded higher
task success than not adapting to affect at all.
As shown last in Table 2, UNC-DISE ADAPT
and UNC ADAPT both significantly outperform
NO ADAPT (p?.003), suggesting that while itera-
tively adding new affect adaptations to an existing
affect-adaptive system does not necessarily yield ad-
ditive improvements to global performance, it also
does not decrease performance.
Table 2: Global Performance Metrics Across Conditions
(All UNC vs. UNC-DISE Differences Yield p>=.274;
All NO-ADAPT Differences Yield p?.003)
Cond N LearnGain UserSat MotGain
Mn sd Mn sd Mn sd
Unc 19 .65 .20 .69 .11 .01 .07
Unc-Dise 19 .58 .19 .66 .09 .01 .07
NoAdapt 21 .38 .20 - - - -
The frequency of disengagement and other af-
fective states can vary widely across system users.
In our case, some users showed disengagement on
the majority of turns in later dialogues while oth-
ers showed almost none at all; the average and stan-
dard deviation of per user %DISE over conditions
are 17.7% and 10.1%, respectively (Table 5 breaks
this down by condition). Thus we hypothesized
that the global performance improvements of UNC-
DISE ADAPT might have been weakened by in-
cluding users with low or no disengagement who
rarely received the adaptation and thus could not be
expected to show improvement. To test this hypoth-
esis, we split users into high and low DISE based
on the median %DISE in the corpus. We ran a
two-way ANOVA for each global metric with DISE
split and condition as factors. We found a signifi-
cant interaction effect between condition and DISE
10Because this prior corpus was collected in a different exper-
iment, the conclusions here are tenuous. However, both exper-
iments had similar subject populations (local college students)
and mean pretest scores (p=.84). The prior experiment used a
smaller satisfaction survey and no motivational surveys, so we
can only compare learning.
221
split (F(1,38) = 4.84, p=0.035) for motivation gain.
Means for these groups are shown in Table 3. As
shown, low DISE users had higher motivation gain
in UNC ADAPT, while high DISE users had higher
motivation gain in UNC-DISE ADAPT.
Table 3: Motivation Gain Differences Across Condition
for High and Low DISE Users (p=.035)
Condition Split N MotGain
Mn sd
UNC high DISE 9 -.01 .04
UNC-DISE high DISE 7 .04 .07
UNC low DISE 10 .03 .08
UNC-DISE low DISE 12 -.01 .06
In contrast to the tests and surveys, which do
not necessarily reflect user performance during the
dialogues, the ?test? dialogue enables us to mea-
sure global performance using dialogue-based met-
rics. The test dialogue was isomorphic with the fi-
nal training dialogue, except that the disengagement
adaptation was not given; moreover, different sys-
tem questions could appear in the test dialogue if the
user answered a question differently.11 We hypoth-
esized that responding to the user?s disengagement
during the training dialogue (UNC-DISE ADAPT)
would yield increased correctness as well as reduced
uncertainty and disengagement in the test dialogue.
We tested this hypothesis by computing per-
cent correctness, disengagement, and uncertainty
for each user, both alone and in combination, over
user answers to tutor questions that were repeated
between the training and test dialogues. We ran
ANOVAs comparing these metrics across the two
conditions. Table 4 presents our results. Interest-
ingly, no differences between conditions were found
for transitions from DISE turns. However, the dis-
engagement adaptation did impact other turns in the
dialogues apart from the (DISE) ones that triggered
it. The first row shows that uncertain answers are
more likely to remain uncertain in UNC ADAPT
than in UNC-DISE ADAPT. The second row shows
that incorrect+uncertain+engaged answers are more
likely to become correct and certain in UNC-
11For example, if a user answered a question incorrectly dur-
ing training and then answered its isomorph correctly during
testing, s/he would not receive the remediation during the test
dialogue that s/he received during training.
DISE ADAPT. By more fully engaging users, the
disengagement adaptation may thereby enable them
to benefit more from the uncertainty adaptation.
However, the third row suggests that the adaptation
can have a negative impact when users are origi-
nally certain about their incorrect answers: incor-
rect+certain+engaged users turns are more likely
to become disengaged in UNC-DISE ADAPT. This
suggests that the disengagement adaptation does not
more fully engage certain users (particularly those
whose certainty does not reflect correctness).
Table 4: Differences Across Condition for Test Dialogue
Metric Condition Mn sd p
UNC? UNC UNC .06 .09 .05
UNC-DISE .01 .04
INC+UNC+ENG? UNC .01 .03 .10
COR+CER+ENG UNC-DISE .03 .05
INC+CER+ENG? UNC .00 .00 .04
INC+CER+DISE UNC-DISE .02 .03
5 Breaking Negative Correlations
As noted in Section 1, in our prior ITSPOKE
corpora we found that user disengagement was
negatively correlated with task success (measured
as learning gain) (p=.01) and user satisfaction
(p=.03) (Forbes-Riley and Litman, 2011c; Forbes-
Riley and Litman, 2012). Thus, one important stan-
dard of evaluation for our disengagement adapta-
tion is to determine whether or not it ?breaks? these
negative correlations when it is employed with real
users (Rotaru and Litman, 2009). A broken corre-
lation would mean that even though disengagement
may still occur, it no longer relates to decreased per-
formance.
UNC-DISE ADAPT responds differently to cor-
rect and incorrect DISE turns (Section 3.2). To
compare the impacts of these responses both com-
bined and individually, we computed %DISE, %cor-
rectDISE (CDISE) and %incorrectDISE (IDISE) for
each user (over all five training problems). We then
computed bivariate Pearson?s correlations within
each condition between each DISE metric and both
learning and user satisfaction.
Table 5 shows the mean (Mn) and standard de-
viations (sd) for the DISE metrics within each con-
222
dition, the coefficient (R) for each correlation, and
its significance (p). Consider first task success.
The first pair of rows shows that the negative cor-
relation between DISE and learning is still present
whether or not the disengagement adaptation is re-
ceived. However, the second pair of rows shows that
the negative correlation between %correctDISE and
learning is broken when the disengagement adap-
tation is received (UNC-DISE), but is still present
when not received (UNC). The third pair of rows
shows that the disengagement adaptation does not
break the negative correlation between %incorrect-
DISE and learning. Consider next user satisfaction.
The first pair of rows shows that the negative cor-
relation between DISE and user satisfaction is bro-
ken when the disengagement adaptation is received
(UNC-DISE), but is still present when not received
(UNC). The third pair of rows shows that the the
negative correlation between %incorrectDISE and
user satisfaction is also broken when the disengage-
ment adaptation is received (UNC-DISE), but is still
present when not received (UNC). These results sug-
gest that for improving task success, adapting to dis-
engagement is more effective for correct turns than
incorrect turns12, while for improving user satisfac-
tion, adapting to disengagement is effective for in-
correct turns and for the dialogue as a whole with-
out considering correctness. Finally, Table 5 shows
that while %correctDISE is reduced in UNC-DISE
as compared to UNC, %incorrectDISE actually in-
creases in UNC-DISE. This suggests that while a re-
duction in disengagement due to the adaptation par-
tially explains the broken correlations, the adapta-
tion may also ameliorate the negative performance
impact of user disengagement.
6 Local Affect Transition Analyses
In addition to global performance analyses, the im-
pact of affect adaptation can also be evaluated lo-
cally, i.e., in terms of its immediate impact in the di-
alogue. We investigate this local effect by comput-
ing the likelihoods of transitioning from each user
12Users who are more often correct may also be predisposed
to learn more. This may explain why %correctDISE has a lesser
negative impact on learning than %DISE and %incorrectDISE
in UNC and UNC-DISE. However, only the disengagement
adaptation can explain why %correctDISE has a lesser negative
impact on learning in UNC-DISE than in UNC.
Table 5: Disengagement-Performance Correlations
Across Conditions (Bold Indicates ?Broken? Correlation)
Mn sd LGain UserSat
R p R p
%DISE in:
UNC 17.2 12.1 -.77 .01 -.48 .04
UNC-DISE 16.9 7.9 -.65 .01 -.16 .51
%CDISE in:
UNC 7.7 7.6 -.45 .05 -.14 .56
UNC-DISE 6.1 3.3 .25 .31 -.27 .27
%IDISE in:
UNC 9.5 7.7 -.76 .01 -.61 .01
UNC-DISE 10.8 7.7 -.78 .01 -.05 .83
disengagement state in turn n (DISE or ENG) to
each user disengagement state in turn n+1 (DISE
or ENG). We use the transition likelihood L met-
ric (D?Mello et al, 2007), which has also previously
been used by ourselves and others to compute the
likelihood of transitioning from one affective state
to another in a dialogue corpus and to compare these
likelihoods across different system versions (Forbes-
Riley and Litman, 2011a; McQuiggan et al, 2008;
D?Mello et al, 2007). As in this prior work, we com-
pute the transition likelihoods for each user (over all
5 training dialogues), then use ANOVAs to deter-
mine if there were differences in the likelihoods of
all possible transitions from the user state in turn n.
Transition likelihood L is computed as shown be-
low, where n refers to the disengagement state in
turn n and n+1 refers to the state in turn n+1. As
shown, L computes the likelihood that the n?n+1
transition will occur. L=1 indicates that n+1 always
follows n, while L=0 and L<0 indicate that the like-
lihood of transitioning from n to n+1 is equal to
chance, and less than chance, respectively.
L(n?n+1) = P (n+1|n)?P (n+1)1?P (n+1)
We hypothesized that users in the UNC-
DISE ADAPT condition would be less likely to
transition into disengagement in turn n+1. Mean L
values across users for each transition are shown in
Table 6 for the two conditions, where the rows repre-
sent each turn n state and the columns represent each
turn n+1 state. The p-value from the ANOVA for
each transition likelihood comparison is also shown.
The table shows that in both conditions, an engaged
223
user in turn n is significantly more likely to remain
engaged in turn n+1 than s/he is to become disen-
gaged. However, in UNC ADAPT, a disengaged
user is more likely (as a trend, p=.06) to remain dis-
engaged than to become engaged in turn n+1. In
contrast, in UNC-DISE ADAPT, a disengaged user
is equally likely (p=.14) to become disengaged or
remain engaged in turn n+1. This analysis thus in-
dicates that the disengagement adaptation also has a
benefit at the local performance level, in that it re-
duces the likelihood of continued disengagement.
Table 6: Mean L Values for Disengagement State Transi-
tions
Condition Turn n Turn n+1
ENG DISE p
UNC-DISE ENG .06 -.01 .04
DISE -.35 .06 .14
UNC ENG .09 -.03 .01
DISE -.41 .09 .06
7 Summary and Current Directions
We investigated how iteratively adding new affect
adaptation to an affect-adaptive spoken dialogue
system impacts global and local performance. We
presented a disengagement adaptation that can gen-
eralize across domains, and discussed its incorpo-
ration into our uncertainty-adaptive computer tutor.
We then presented a controlled evaluation compar-
ing these multiply and singly adaptive systems. Our
results showed that while the disengagement adap-
tation did not increase (or decrease) task success or
user satisfaction, it demonstrated a slight but sig-
nificant increase in motivation gain for users with
high disengagement. Future analyses will shed fur-
ther light on how disengagement mediates the ef-
fect of condition on motivation. The adaptation also
reduced user uncertainty and increased correctness
for uncertain answers when repeated in the test dia-
logue, but increased disengagement for repeated an-
swers that were originally certain and incorrect. It
also broke negative correlations between disengaged
turns and performance, when measured both as task
success and user satisfaction, and showed a trend to
reduce disengagement at the local dialogue level.
Our next step is to repeat the experiment with
fully automated versions of our affect-adaptive spo-
ken dialogue systems, to determine the impact of
adding new affect adaptation when the system per-
forms the affect detection and natural language un-
derstanding tasks. We are currently in the last
stages of building an automatic disengagement de-
tector that will then be implemented in UNC-
DISE ITSPOKE. Interestingly, our prior work sug-
gests that the fully automated UNC-DISE ADAPT
system may yield greater global performance im-
provements relative to UNC ADAPT (Forbes-Riley
and Litman, 2012) than the wizard-of-oz version of
the system; it may be that users are more responsive
to the disengagement adaptation when the affect de-
tection and natural language understanding outputs
are ?noisier?. Future work will also consider other
experimental designs to help determine the separate
and joint effects of the two affect adaptations.
Acknowledgments
This work is funded by NSF award 0914615. We
thank Scott Silliman for experimental support.
References
J. C. Acosta and N. G. Ward. 2011. Achieving rapport
with turn-by-turn, user-responsive emotional coloring.
Speech Communication, 53(9-10):1137?1148.
G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Pi-
card. 2002. Experimentally augmenting an intel-
ligent tutoring system with human-supplied capabil-
ities: Adding human-provided emotional scaffolding
to an automated reading tutor that listens. In Proc.
IEEE International Conference on Multimodal Inter-
faces (ICMI), pages 483?492, Washington, DC.
I. Arroyo, K. Ferguson, J. Johns, T. Dragon, H. Merhera-
nian, D. Fisher, A. Barto, S. Mahadevan, and B.Woolf.
2007. Repairing disengagement with non-invasive in-
terventions. In Proc. Artificial Intelligence in Educa-
tion (AIED), pages 195?202.
R. S. Baker, A. Corbett, K. Koedinger, S. Evenson,
I. Roll, A. Wagner, M. Naim, J. Raspat, D. Baker, and
J. Beck. 2006. Adapting to when students game an
intelligent tutoring system. In Proceedings Intelligent
Tutoring Systems, pages 392?401.
A. Black and P. Taylor. 1997. Festival speech synthe-
sis system: system documentation (1.1.1). The Centre
for Speech Technology Research, University of Edin-
burgh, http://www.cstr.ed.ac.uk/projects/festival/.
D. Bohus and E. Horvitz. 2009. Models for multiparty
engagement in open-world dialog. In Proceedings of
SIGdial, pages 225?234, London, UK.
224
C. Conati and H. Maclaren. 2009. Empirically build-
ing and evaluating a probabilistic model of user af-
fect. User Modeling and User-Adapted Interaction,
19(3):267?303.
R. Cowie and R. R. Cornelius. 2003. Describing the
emotional states that are expressed in speech. Speech
Communication, 40(1-2):5?32.
S. D?Mello, R. S. Taylor, and A. Graesser. 2007. Mon-
itoring affective trajectories during complex learning.
In Proc. Cognitive Science Society, pages 203?208.
S. D?Mello, B. Lehman, J. Sullins, R. Daigle, R. Combs,
K. Vogt, L. Perkins, and A. Graesser. 2010. A time
for emoting: When affect-sensitivity is and isn?t effec-
tive at promoting deep learning. In Proc. Intelligent
Tutoring Systems Conference, pages 245?254, June.
M. Dzikovska, J. Moore, N. Steinhauser, and G. Camp-
bell. 2011. Exploring user satisfaction in a tutorial
dialogue system. In Proc. SIGDIAL, pages 162?172,
Portland, Oregon, June.
K. Forbes-Riley and D. Litman. 2011a. Benefits and
challenges of real-time uncertainty detection and adap-
tation in a spoken dialogue computer tutor. Speech
Communication, 53(9?10):1115?1136.
K. Forbes-Riley and D. Litman. 2011b. Designing and
evaluating a wizarded uncertainty-adaptive spoken di-
alogue tutoring system. Computer Speech and Lan-
guage (CSL), 25(1):105?126.
K. Forbes-Riley and D. Litman. 2011c. When does dis-
engagement correlate with learning in spoken dialog
computer tutoring? In Proceedings of AIED, Auck-
land, NZ, June.
K. Forbes-Riley and D. Litman. 2012. Intrinsic and ex-
trinsic evaluation of an automatic user disengagement
detector for an uncertainty-adaptive spoken dialogue
system. In Proc. NAACL-HLT, Montreal, June.
S. Janarthanam and O. Lemon. 2008. User simula-
tions for online adaptation and knowledge-alignment
in troubleshooting dialogue systems. In Proc. SEM-
dial.
C. Kamm, D. Litman, and M. Walker. 1998. From
novice to expert: The effect of tutorials on user exper-
tise with spoken dialogue systems. In Proceedings of
the 5th International Conference on Spoken Language
Processing, pages 1211?1214.
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-
sults. Interacting with Computers, 14:119?140.
K. Liu and R. W. Picard. 2005. Embedded empathy
in continuous, interactive health assessment. In CHI
Workshop on HCI Challenges in Health Assessment.
S. W. McQuiggan, J. L Robison, and J. C. Lester. 2008.
Affective transitions in narrative-centered learning en-
vironments. In Proc. Intelligent Tutoring Systems
Conference, pages 490?499.
T. Paek and Y.-C. Ju. 2008. Accommodating ex-
plicit user expressions of uncertainty in voice search
or something like that. In Proceedings Interspeech),
pages 1165?1168, Brisbane, Australia, September.
P. Pintrich and E. DeGroot. 1990. Motivational and
self-regulated learning components of classroom aca-
demic performance. Journal of Educational Psychol-
ogy, 82(1):33?40.
H. Pon-Barry and S. Shieber. 2011. Recognizing uncer-
tainty in speech. EURASIP Journal on Advances in
Signal Processing.
I. Roll. 2009. Structured Invention Tasks to Prepare Stu-
dents for Future Learning: Means, Mechanisms, and
Cognitive Processes. Ph.D. thesis, Carnegie Mellon
University.
M. Rotaru and D. Litman. 2009. Discourse structure
and performance analysis: Beyond the correlation. In
Proc. SIGDIAL, pages 178?187, London, UK.
B. Schuller, S. Steidl, and A. Batliner. 2009. The Inter-
speech 2009 emotion challenge. In Proc. Interspeech,
pages 312?315, ISCA, Brighton, UK, September.
B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev-
illers, C. Muller, and S. Narayanan. 2010. The Inter-
speech 2010 paralinguistic challenge. In Proc. Inter-
speech, pages 2794?2797, Chiba, Japan, September.
W. Tsukahara and N. Ward. 2001. Responding to subtle,
fleeting changes in the user?s internal state. In Pro-
ceedings of the SIG-CHI on Human factors in comput-
ing systems, pages 77?84, Seattle, WA. ACM.
K. VanLehn, P. Jordan, C. Rose?, D. Bhembe, M. Bo?ttner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, R. Srivastava, and R. Wil-
son. 2002. The architecture of Why2-Atlas: A coach
for qualitative physics essay writing. In Proc. Intelli-
gent Tutoring Systems Conference, pages 158?167.
J. Walonoski and N. Heffernan. 2006. Prevention of
off-task gaming behavior in intelligent tutoring sys-
tems. In Proc. Intelligent Tutoring Systems Confer-
ence (ITS), pages 722?724.
W. Wang and J. Hirschberg. 2011. Detecting levels of
interest from spoken dialog with multistream predic-
tion feedback and similarity based hierarchical fusion
learning. In Proc. SIGDIAL, pages 152?161, Portland,
Oregon, June.
N. Wang, W.L. Johnson, R. E. Mayer, P. Rizzo, E. Shaw,
and H. Collins. 2008. The politeness effect: Peda-
gogical agents and learning outcomes. International
Journal of Human-Computer Studies, 66(2):98?112.
A. Ward. 2010. Reflection and Learning Robustness in
a Natural Language Conceptual Physics Tutoring Sys-
tem. Ph.D. thesis, University of Pittsburgh.
APPENDIX
225
UNC ADAPT ITSPOKE: What are the forces exerted on the man after he releases his keys?
USER: gravity??? (COR, UNC, ENG)
UNC ADAPT ITSPOKE: Right. [Correctness Feedback] Gravity is the only force acting on the man. This force is
also called the weight of the man. It?s always present when an object is in earth?s gravitational field. [Bottom Out]. So
what?s the direction of the force of gravity on the man (e.g., vertically up)? [Next Topic Question]
Figure 1: Annotated Dialogue Example Showing Uncertainty Adaptation
UNC-DISE ADAPT ITSPOKE-1: Let?s begin by looking at the motion of the man and his keys while he?s holding
them. How does his velocity compare to that of his keys?
USER-1: vertical (INC, UNC, DISE)
UNC-DISE ADAPT ITSPOKE-2: That doesn?t sound right. Let?s think about this a little more. [ Productive
Interaction Feedback] Since the man is holding his keys, they aren?t moving relative to each other. So their velocities
must be WHAT? [Supplementary Question].
Figure 2: Dialogue Example Showing Adaptation for Disengaged+Incorrect Turns
UNC-DISE ADAPT ITSPOKE-1: Let?s begin by looking at the motion of the man and his keys while he?s holding
them. How does his velocity compare to that of his keys?
USER-1: same same same (COR, CER, DISE)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
UNC-DISE ADAPT ITSPOKE-2a: Well done. Here?s a quick progress update. Good effort so far!![Productive
Interaction Feedback When Progress Report Shows Improvement] Now let?s see what happens after the man releases
his keys. [...] [Next Topic Question]
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
UNC-DISE ADAPT ITSPOKE-2b: Right. Here?s a quick progress update. It might help to remember we will build
on the topics we?re discussing now.[Productive Interaction Feedback When Progress Report Shows Decline] Now let?s
see what happens after the man releases his keys. [...] [Next Topic Question]
Figure 3: Dialogue Example Showing Adaptation for Disengaged+Correct Users
Figure 4: Example Progress Report after Disengaged+Correct Turn
226
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 174?179,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
An Interactive Analytic Tool for Peer-Review Exploration
Wenting Xiong1,2, Diane Litman1,2, Jingtao Wang1,2 and Christian Schunn2
1 Department of Computer Science & 2 Learning Research and Development Center
University of Pittsburgh, Pittsburgh, PA, 15260
wex12@cs.pitt.edu
Abstract
This paper presents an interactive analytic tool
for educational peer-review analysis. It em-
ploys data visualization at multiple levels of
granularity, and provides automated analytic
support using clustering and natural language
processing. This tool helps instructors dis-
cover interesting patterns in writing perfor-
mance that are reflected through peer reviews.
1 Introduction
Peer review is a widely used educational approach
for coaching writing in many domains (Topping,
1998; Topping, 2009). Because of the large number
of review comments to examine, instructors giving
peer review assignments find it difficult to examine
peer comments. While there are web-based peer-
review systems that help instructors set up peer-
review assignments, no prior work has been done
to support instructors? comprehension of the textual
review comments.
To address this issue, we have designed and devel-
oped an interactive analytic interface (RevExplore)
on top of SWoRD1 (Cho and Schunn, 2007), a web-
based peer-review reciprocal system that has been
used by over 12,000 students over the last 8 years.
In this paper, we show how RevExplore visualizes
peer-review information in multiple dimensions and
various granularity levels to support investigative
exploration, and applies natural language process-
ing (NLP) techniques to facilitate review compre-
hension and comparison.
1https://sites.google.com/site/swordlrdc/
2 Design Goals
Instructors face challenges when they try to make
sense of the peer-review data collected by SWoRD
for their assignments. Instructors we have inter-
viewed have complained that peer reviews are time-
consuming to read and almost ?impossible? to in-
terpret: 1) to understand the pros and cons of one
student?s paper, they need to synthesize all the peer
reviews received by that student by reading them one
by one; 2) furthermore, if instructors would like to
discover general patterns regarding students? writ-
ing performance, they have to additionally compare
peer reviews across multiple students which requires
their simultaneously remembering various opinions
for many students; 3) in the initial stage of peer re-
view analysis, instructors have no clear idea of what
potential patterns they should be looking for (?cold
start?).
These challenges motivate our design of RevEx-
plore, a peer-review analytic tool that is a plugin
to SWoRD. We set our design goals to address the
challenges mentioned above, respectively: 1) cre-
ate a simple and informative representation of peer-
review data which automatically aggregates peer-
reviews at the level of student; 2) provide intelligent
support of text mining and semantic abstraction for
the purpose of comparison; 3) enable an overview of
key characteristics of peer reviews for initial explo-
ration.
To fulfill our design goals, we design an inter-
active visualization system to ease the exploration
process, following the pattern of overview plus de-
tail (Card et al, 1999). In the overview, RevExplore
174
provides a high level of visualization of overall peer-
review information at the student level for initial ex-
ploration. In the detail-view, RevExplore automati-
cally abstracts the semantic information of peer re-
views at the topic-word level, with the original texts
visible on demand. In addition, we introduce clus-
tering and NLP techniques to support automated an-
alytics.
3 Related Work
One major goal of peer review studies in educational
research is to understand how to better improve stu-
dent learning, directly or indirectly. Empirical stud-
ies of textual review comments based on manual
coding have discovered that certain review features
(e.g., whether the solution to a problem is explicitly
stated in a comment) can predict both whether the
problem will be understand and the feedback imple-
mented (Nelson and Schunn, 2009). Our previous
studies used machine learning and NLP techniques
to automatically identify the presence of such useful
features in review comments (Xiong et al, 2010);
similar techniques have also been used to determine
review comment helpfulness (Xiong and Litman,
2011; Cho, 2008). With respect to paper analysis,
Sa?ndor and Vorndran (2009) used NLP to highlight
key sentences, in order to focus reviewer attention
on important paper aspects. Finally, Giannoukos et
al. (2010) focused on peer matching based on stu-
dents? profile information to maximize learning out-
comes, while Crespo Garcia and Pardo (2010) ex-
plored the use of document clustering to adaptively
guide the assignment of papers to peers. In contrast
to the prior work above, the research presented here
is primarily motivated by the needs of instructors,
instead of the needs of students. In particular, the
goal of RevExplore is to utilize the information in
peer reviews and papers, to help instructors better
understand student performance in the peer-review
assignments for their courses.
Many computer tools have already been de-
veloped to support peer review activities in var-
ious types of classrooms, from programming
courses (Hyyrynen et al, 2010) to courses involving
writing in the disciplines (Nelson and Schunn, 2009;
Yang, 2011). Within the writing domain, systems
such as SWoRD (Cho and Schunn, 2007) mainly as-
sist instructors by providing administrative manage-
ment support and/or (optional) automatic grading
services. While peer review systems especially de-
signed for instructors do exist, their goal is typically
to create a collaborative environment for instructors
to improve their professional skills (Fu and Hawkes,
2010). In terms of artificial intelligence support, to
our knowledge no current peer review system has the
power to provide instructors with insights about the
semantic content of peer reviews, due to the diver-
sity and complexity of the textual review comments.
For example, SWoRD currently provides teachers a
numerical summary view that includes the number
of reviews received for each paper, and the mean
and standard deviation of numerical reviewing rat-
ings for each paper. SWoRD also allows instruc-
tors to automatically compute a grade based on a
student?s writing and reviewing quality; the grading
algorithm uses the numerical ratings but not the as-
sociated text comments. In this work, we attempted
to address the lack of semantic insight both by hav-
ing humans in the loop to identify points of interest
for interactive data exploration, and by adapting ex-
isting natural language processing techniques to the
peer review domain to support automated analytics.
4 RevExplore
As an example for illustration, we will use data col-
lected in a college level history class (Nelson and
Schunn, 2009): the instructor created the writing
assignment through SWoRD and provided a peer-
review rubric which required students to assess a
history paper?s quality on three dimensions (logic,
flow and insight) separately, by giving a numeric
rating on a scale of 1-7 in addition to textual com-
ments. While reviewing dimensions and associated
guidelines (see below) are typically created by an in-
structor for a particular assignment, instructors can
also set up their rubric using a library provided by
SWoRD.
For instance, the instructor created the following
guidance for commenting on the ?logic? dimension:
?Provide specific comments about the logic of the
author?s argument. If points were just made without
support, describe which ones they were. If the sup-
port provided doesn?t make logical sense, explain
what that is. If some obvious counter-argument was
175
not considered, explain what that counter-argument
is. Then give potential fixes to these problems if you
can think of any. This might involve suggesting that
the author change their argument.?
Instructor guidance for numerically rating the log-
ical arguments of the paper based on the comments
was also given. For this history assignment, the
highest rating of 7 (?Excellent?) was described as
?All arguments strongly supported and no logical
flaws in the arguments.? The lowest rating of 1
(?Disastrous?) was described as ?No support pre-
sented for any arguments, or obvious flaws in all
arguments.?
24 students submitted their papers online through
SWoRD and then reviewed 6 peers? papers assigned
to them in a ?double blind? manner (review exam-
ples are available in Figure 2). When peer review
is finished, RevExplore loads all papers and peer
reviews, both textual comments and numeric rat-
ings, and then goes through several text processing
steps to prepare for interactive analytics. This pre-
processing includes computing the domain words,
sentence simplification, domain-word masking, syn-
tactic analysis, and key noun-phrase extraction.
4.1 Overview ? Student Clustering
RevExplore starts with a student-centric visualiza-
tion overview. It uses a visual node of a bar chart
to represent each student, visualizing the average of
the student?s peer ratings in gray, as well as the rat-
ing histogram with gradient colors (from red to blue)
that are mapped to the rating scale from 1 to 7 (de-
noted by the legend in Figure 1).
To investigate students? writing performance, in-
structors can manually group similar nodes together
into one stacked bar chart, or use automatic group-
ing options that RevExplore supports to inform ini-
tial hypotheses about peer review patterns. In the
auto-mode, RevExplore can group students regard-
ing a certain property (e.g. rating average); it can
also cluster students using standard clustering algo-
rithms2 based on either rating statistics or Bag-Of-
Words extracted from the relevant peer reviews.
If a instructor is curious about the review content
for certain students during exploration, the instruc-
2RevExplore implements both K-Means and a hierarchical
clustering algorithm.
Figure 1: RevExplore overview. Stacked bar charts rep-
resent student groups. The tooltip shows the ID of the
current student, writing performance (average peer rat-
ings), review helpfulness (average helpfulness ratings), as
well as the main issues in the descending order of their
frequency, which are extracted from the peer reviews re-
ceived by a highlighted student using NLP techniques.
tor can read the main issues, in the form of noun
phrases (NPs) of a student?s peer reviews in a tooltip
by mouse hovering on the bar squares which the stu-
dent corresponds to. For example, Figure 1 shows
that the peer reviews received by this student are
mainly focused on the argumentation and the intro-
duction part of the paper.
To extract peer-review main issues, RevExplore
syntactically simplifies each review sentence (Heil-
man and Smith, 2010), parses each simplified sen-
tence using the Stanford dependency parser (de
Marneffe et al, 2006), and then traverses each de-
pendency tree to find the key NP in a rule-based
manner.3 Due to reviewers? frequent references to
the relevant paper, most of the learned NPs are do-
main related facts used in the paper, rather than eval-
uative texts that suggest problems or suggestions. To
avoid the interference of the domain content, we ap-
ply domain-word masking (explained in Section 4.2)
to the simplified sentences before parsing, and elim-
inate any key NP that contains the mask.
4.2 Detail-View ? Topic Comparison
When two groups of students are selected in the
overview, their textual peer reviews can be further
3Rules are constructed purely based on our intuition.
176
Figure 2: Peer-review exploration using RevExplore, for mining differences between strong and weak students.
compared with respect to specific reviewing dimen-
sions using a list of topic words that are automati-
cally computed in real-time.
Extracting topic words of peer reviews for com-
parison purposes is different from most traditional
topic-word extraction tasks that are commonly in-
volved in text summarization. In traditional text
summarization, the informativeness measurement
is designed to extract the common themes, while
in our case of comparison, instructors are more
concerned with the uniqueness of each target set
of peer reviews compared to the others. Thus a
topic-signature acquisition algorithm (Lin and Hovy,
2000), which extracts topic words through compar-
ing the vocabulary distribution of a target corpus
against that of a generic background corpus using
a statistic metric, suits our application better than
other approaches, such as probabilistic graphical
models (e.g. LDA) and frequency based methods.
Therefore, RevExplore considers topic signatures as
the topic words for a group of reviews, using all peer
reviews as the background corpus.4 Again, to min-
imize the impact of the domain content of the rele-
vant papers, we apply topic-masking which replaces
all domain words5 with ?ddd? before computing the
topic signatures.
As the software outputs topic signatures together
with their associated weights which reflect signature
importance, RevExplore uses this weight informa-
tion to order the topic words as a list, and visualizes
the weight as the font size and foreground color of
the relevant topic word. These lists are placed in
two rows regarding their group membership dimen-
sion by dimension. For each dimension, the cor-
responding lists of both rows are aligned vertically
with the same background color to indicate that di-
mension (e.g. Topic-list detail view of Figure 2).
To further facilitate the comparison within a dimen-
sion, RevExplore highlights the topic words that are
unique to one group with a darker background color.
4We use TopicS (Nenkova and Louis, 2008) provided by An-
nie Louis.
5learned from all student papers against 5000 documents
from the English Gigaword Corpus using TopicS.
177
If the user cannot interpret the topic that an ex-
tracted word might imply, the user can click on the
word to read the relevant original reviews, with that
word highlighted in red (e.g. Original reviews pane
of Figure 2).
5 Analysis Example
Figure 2 shows how RevExplore is used to discover
the difference between strong and weak students
with respect to their writing performance on ?logic?
in the history peer-review assignment introduced in
Section 4.
First we group students into strong versus weak
regarding their writing performance on logic by se-
lecting the K-Means algorithm to cluster students
into two groups based on their rating histogram on
logic. As shown in the Overview pane of Figure 2,
we then label them as A and B for further topic com-
parison.
Next, in the topic-list detail view, we check
?praise? and ?problem?6, and fire the ?enter? but-
ton to start extracting topic words for group A and B
on every selected dimension. Note that ?logic? will
be automatically selected since the focus has already
been narrowed down to logic in the overview.
To first compare the difference in general logic is-
sues between these two groups, we refer to the two
lists on ?logic? (in the middle of the topic-list de-
tail view, Figure 2). As we can see, the weak stu-
dents? reviews (Group A) are more about the logic
of statements and the usage of facts (indicated by the
unique words ?examples? and ?details?); the strong
students? peer reviews (group B) focus more on ar-
gumentation (noted by ?counter? and ?supporting?).
To further compare the two groups regarding dif-
ferent review sentiment, we look at the lists corre-
sponding to ?problem? and ?praise? (left and right
columns). For instance, we can see that strong stu-
dents? suffer more from context specific problems,
which is indicated by the bigger font size of the
domain-word mask. Meanwhile, to understand what
a topic word implies, say, ?logic? in group A?s topic
list on ?problem?, we can click the word to bring out
the relevant peer reviews, in which all occurrences
6Although ?praise? and ?problem? are manually annotated
in this corpus (Nelson and Schunn, 2009), Xiong et al (2010)
have shown that they can be automatically learned in a data-
driven fashion.
of ?logic? are colored in red (original reviews pane
in Figure 2).
6 Ongoing Evaluation
We are currently evaluating our work along two di-
mensions. First, we are interested in examining
the utility of RevExplore for instructors. After re-
ceiving positive feedback from several instructors
at the University of Pittsburgh, as an informal pilot
study, we deployed RevExplore for some of these
instructors during the Spring 2012 semester and let
them explore the peer reviews of their own ongo-
ing classes. Instructors did observe interesting pat-
terns using this tool after a short time of exploration
(within two or three passes from the overview to the
topic-word detail view). In addition, we are con-
ducting a formal user study of 40 subjects to validate
the topic-word extraction component for comparing
reviews in groups. Our preliminary result shows that
our use of topic signatures is significantly better than
a frequency-based baseline.
7 Summary and Future work
RevExplore demonstrates the usage of data visual-
ization in combination with NLP techniques to help
instructors interactively make sense of peer review
data, which was almost impracticable before. In the
future we plan to further analyze the data collected
in our formal user study, to validate the helpful-
ness of our proposed topic-word approach for mak-
ing sense of large quantities of peer reviews. We
also plan to incorporate NLP information beyond the
word and NP level, to support additional types of re-
view comparisons. In addition, we plan to summa-
rize the interview data that we informally collected
from several instructors, and will mine the log files
of their interactions with RevExplore to understand
how the tool would (and should) be used by instruc-
tors in general. Last but not least, we will continue
revising our design of RevExplore based on instruc-
tor feedback, and plan to conduct a more formal
evaluation with instructors.
Acknowledgments
Thanks to Melissa Patchan for providing the history
peer-review corpus. We are also grateful to LRDC
for financial support.
178
References
Stuart K. Card, Jock D. Mackinlay, and Ben Shneider-
man. 1999. Readings in information visualization:
using vision to think. San Francisco, CA, USA.
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A web-
based reciprocal peer review system. Computers and
Education, 48(3):409?426.
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings First Interna-
tional Conference on Educational Data Mining, pages
192?196.
Raquel M Crespo Garcia and Abelardo Pardo. 2010. A
supporting system for adaptive peer review based on
learners? profiles. In Proceedings of Computer Sup-
ported Peer Review in Education Workshop, pages 22?
31.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006.
Hongxia Fu and Mark Hawkes. 2010. Technology-
supported peer assessment as a means for teacher
learning. In Proceedings of the 2010 Workshop on
Computer Supported Peer Review in Education.
Ioannis Giannoukos, Ioanna Lykourentzou, Giorgos
Mpardis, Vassilis Nikolopoulos, Vassilis Loumos, and
Eleftherios Kayafas. 2010. An adaptive mechanism
for author-reviewer matching in online peer assess-
ment. In Semantics in Adaptive and Personalized Ser-
vices, pages 109?126.
Michael Heilman and Noah A. Smith. 2010. Extracting
simplified statements for factual question generation.
In Proceedings of the 3rd Workshop on Question Gen-
eration.
Ville Hyyrynen, Harri Ha?ma?la?inen, Jouni Ikonen, and
Jari Porras. 2010. Mypeerreview: an online peer-
reviewing system for programming courses. In Pro-
ceedings of the 10th Koli Calling International Con-
ference on Computing Education Research, pages 94?
99.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings COLING.
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. Instructional Sci-
ence, 37:375?401.
Ani Nenkova and Annie Louis. 2008. Can you summa-
rize this? Identifying correlates of input difficulty for
generic multi-document summarization. In Proceed-
ings of Association for Computational Linguistics.
?Agnes Sa?ndor and Angela Vorndran. 2009. Detecting
key sentences for automatic assistance in peer review-
ing research articles in educational sciences. In Pro-
ceedings of the 2009 Workshop on Text and Citation
Analysis for Scholarly Digital Libraries, pages 36?44,
Suntec City, Singapore, August. Association for Com-
putational Linguistics.
Keith Topping. 1998. Peer assessment between students
in colleges and universities. Review of Educational
Research, 68(3):249?276.
Keith J. Topping. 2009. Peer assessment. Theory Into
Practice, 48(1):20?27.
Wenting Xiong and Diane Litman. 2011. Automatically
predicting peer-review helpfulness. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, HLT ?11, pages 502?
507.
Wenting Xiong, Diane J. Litman, and Christian D.
Schunn. 2010. Assessing reviewers performance
based on mining problem localization in peer-review
data. In Proceedings Third International Conference
on Educational Data Mining.
Yu-Fen Yang. 2011. A reciprocal peer review system to
support college students? writing. British Journal of
Educational Technology, 42(4):687?700.
179
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 99?108,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Improving Peer Feedback Prediction: The Sentence Level is Right
Huy V. Nguyen
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
hvn3@pitt.edu
Diane J. Litman
Department of Computer Science & LRDC
University of Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Abstract
Recent research aims to automatically pre-
dict whether peer feedback is of high qual-
ity, e.g. suggests solutions to identified
problems. While prior studies have fo-
cused on peer review of papers, simi-
lar issues arise when reviewing diagrams
and other artifacts. In addition, previous
studies have not carefully examined how
the level of prediction granularity impacts
both accuracy and educational utility. In
this paper we develop models for predict-
ing the quality of peer feedback regard-
ing argument diagrams. We propose to
perform prediction at the sentence level,
even though the educational task is to la-
bel feedback at a multi-sentential com-
ment level. We first introduce a corpus
annotated at a sentence level granularity,
then build comment prediction models us-
ing this corpus. Our results show that ag-
gregating sentence prediction outputs to
label comments not only outperforms ap-
proaches that directly train on comment
annotations, but also provides useful infor-
mation for enhancing peer review systems
with new functionality.
1 Introduction
Peer review systems are increasingly being used
to facilitate the teaching and assessment of student
writing. Peer feedback can complement and even
be as useful as teacher feedback; students can also
benefit by producing peer feedback. Past research
has shown that feedback implementation is sig-
nificantly correlated to the presence of desirable
feedback features such as the description of so-
lutions to problems (Nelson and Schunn, 2009).
Since it would be very time-consuming for in-
structors to identify feedback of low quality post-
hoc, recent research has used natural language
processing (NLP) to automatically predict whether
peer feedback contains useful content for guiding
student revision (Cho, 2008; Ramachandran and
Gehringer, 2011; Xiong et al., 2012). Such real-
time predictions have in turn been used to enhance
existing online peer-review systems, e.g. by trig-
gering tutoring that is designed to improve feed-
back quality (Nguyen et al., June 2014).
While most prior research of peer review qual-
ity has focused on feedback regarding papers, sim-
ilar issues arise when reviewing other types of ar-
tifacts such as program code, graphical diagrams,
etc. (Nguyen and Litman, July 2013). In addi-
tion, previous studies have not carefully examined
how the level of prediction granularity (e.g. multi-
sentential review comments versus sentences) im-
pacts both the accuracy and the educational utility
of the predictive models. For example, while the
tutoring intervention of (Nguyen et al., June 2014)
highlighted low versus high quality feedback com-
ments, such a prediction granularity could not sup-
port the highlighting of specific text spans that also
might have been instructionally useful.
In this paper, we first address the problem of
predicting feedback type (i.e. problem, solution,
non-criticism) in peer reviews of student argument
diagrams. In problem feedback, the reviewer de-
scribes what is wrong or needs to be improved in
the diagram. In solution feedback, the reviewer
provides a way to fix a problem or to improve the
diagram quality. Feedback is non-criticism when
it is neither a problem nor a solution (e.g. when it
provides only positive feedback or summarizes).
Examples are shown in Figure 1.
1
The second goal of our research is to design our
prediction framework so that it can support real-
time tutoring about feedback quality. We hypoth-
1
Our peer review corpus comes from a system that uses
an end-comment feedback approach as shown in Figure 1.
While it is possible to instead directly annotate a reviewed
artifact, this has been shown to encourage feedback on low-
level issues, and is not good for more global feedback.
99
#3. Are the relevance, validity, and reason fields in the supportive arcs complete and 
convincing? If not, indicate where the argument for relevance or validity is missing or 
unclear. Suggest ways to make the validity or relevance argument more convincing or 
sensible. 
Not all of these field are filled out, which makes it hard to get a clear idea of how legit 
these studies are. Also, some are unclear. An example is 24-supports where the reason 
is a question. I think there should be a substantial reason there instead of a question 
to convince me why it is relevant. 
#5. Is at least one credible opposing Finding, Study, or Theory connected to each 
Hypothesis? If there is no opposition, suggest a spot for a potential counterargument. 
If there is opposition, is it credible? If the opposition is not credible, explain why. 
There is a good piece of credible opposition, though it is hard to tell from the diagram 
what the study exactly did. 
#1. Are any parts of the diagram hard to understand because they are unclear? If so, 
describe any particularly confusing parts of the diagram and suggest ways to increase 
clarity. 
The argument diagram was easy to follow. I was able to effortlessly go through the 
diagram and connect each part. 
Your comments need to suggest solution. 
For every comment below, if you point out a problem make sure that 
you provide a solution to fix that problem. 
Your comments which are highlighted in GREEN already have solutions 
provided, while the RED comments mention only problem. Examples 
of problem and solution text are formatted in ITALIC and BOLD. 
I?ve revised my 
comments. 
Please check 
again. 
Could you show 
me some 
examples of 
problem and 
solution 
comments? 
My comments 
don?t have the 
issue that you 
describe. Please 
submit 
comments. 
Figure 1: A mock-up interface of a peer review system where the prediction of feedback type triggers
a system tutoring intervention. Left: three sample feedback comments including a non-criticism (top),
a solution (middle), and a problem (bottom). Right-top: a system tutoring intervention to teach the
student reviewer to provide a solution whenever a problem is mentioned. Right-bottom: possible student
responses to the system?s tutoring.
esize that using a student?s own high-quality re-
views during tutoring, and identifying the explicit
text that makes the review high quality, will help
students learn how to improve their lower quality
reviews. To facilitate this goal, we develop pre-
diction models that work at the sentence level of
granularity.
Figure 1 presents a mock-up of our envisioned
peer review interface. To tutor the student about
solutions (figure right), the system uses live ex-
amples taken from the student?s current review
(figure left). Color is used to display the feed-
back type predictions: here a non-criticism is dis-
played in black, while the criticisms that are pos-
itive and negative examples of solution are dis-
played in green and red, respectively. In addition,
to help the student focus on the important aspect of
the (green) positive example, the sentence that ac-
tually specifies the solution is highlighted in bold.
This paper presents our first results towards re-
alizing this vision. The contributions of our work
are two-fold. First, we develop a sentence-level
model for predicting feedback type in a diagram
review corpus. While our peer review system
works at the level of feedback comments (text of
each box in Figure 1), we find it is more accu-
rate to annotate and predict at finer-grained gran-
ularity levels, then use these predictions to infer
the comment?s feedback type. By introducing a
small overhead to annotate peer feedback, we cre-
ated a phrase level-annotated corpus of argument
diagram reviews. Our experimental results show
that our learned prediction models using labeled
sentences outperform models trained and tested
at comment level. In addition, our models out-
perform models previously developed for paper
rather than diagram feedback, and also show po-
tential generality by avoiding the use of domain-
specific features. Second, we demonstrate that
our sentence-level prediction can be used to sup-
port visualizations useful for tutoring. Particular
sentences that are predicted to express the com-
ment?s feedback type are highlighted for instruc-
tional purposes (e.g. the bold highlighting in Fig-
ure 1).
2 Related work
In instructional science, research has been con-
ducted to understand what makes peer feedback
helpful. At the secondary school level, Gielen et
al. (2010) found that the presence of justification
in feedback significantly improved students? writ-
ing performance. At the university level, Nelson
and Schunn (2009) found that feedback on papers
was more likely to be implemented when the feed-
back contained solutions or pinpointed problem
locations. Lippman et al. (2012) found that similar
feedback properties led to greater implementation
100
of feedback on diagrams as well.
Building on such findings, researchers have be-
gun to develop automated methods to identify
helpful feedback. Cho (2008) was the first to
take a machine learning approach. Peer feedback,
i.e. comments, were manually segmented into
idea units
2
and human-coded for various features
including problem detection, solution suggestion,
praise, criticism, and summary. Feedback was
then labeled as helpful or not-helpful based on the
presence of such features. The study showed that
feedback could be classified regarding helpfulness
with up to 67% accuracy using simple NLP tech-
niques including ngrams and part-of-speech. Our
work is different from (Cho, 2008) in that we focus
on predicting particular feedback types (i.e. solu-
tion and problem) rather than helpfulness in gen-
eral. Also, as the raw feedback to peer-review sys-
tems is typically at the comment-level, and being
aware that idea-units are difficult to automatically
segment, we instead predict at the sentence-level
to make model deployment more practical.
Our work is more similar to (Xiong and Litman,
2010; Xiong et al., June 2010; Xiong et al., 2012),
in which NLP and machine learning were used to
automatically predict whether peer reviews of stu-
dent papers contained specific desirable feedback
features. Xiong and Litman used NLP-based fea-
tures including paper ngrams, predefined keyword
lists, and dependency parses to predict feedback
type. For feedback of type criticism, they also
developed models to further predict problem lo-
calization and solution. Following (Cho, 2008),
Xiong and Litman evaluated their models on peer
review data that had been manually segmented
into idea units. As noted above, the difficulty of
automatically segmenting raw comments into idea
units makes deployment of such models less prac-
tical than our sentence-level approach. Also like
Cho (2008), while their models predicted a label
for each idea unit, the relevant text that led to the
prediction was not identified. We will address this
limitation by introducing a more fine-grained an-
notated corpus.
Regarding peer reviews of student argument
diagrams rather than papers, Nguyen and Lit-
man (July 2013) developed a rule-based algorithm
for predicting feedback that contained localization
text (e.g. ?Hypothesis 4?). Their approach was to
2
Cf. (Cho, 2008) ?a self-contained message on a single
piece of strength or weakness found in peer writing.?
first identify common words between a peer com-
ment and its diagram, then classify phrases con-
taining these words into different localization pat-
terns. Although we similarly focus on diagram
rather than paper feedback, our work addresses a
different prediction task (namely, predicting feed-
back type rather than localization). We also use
statistical machine learning rather than a rule-
based approach, in conjunction with more general
linguistic features, to allow us to ultimately use
our models for papers as well as diagrams with
minimal modification or training.
Outside of peer review, research has been per-
formed recently to mine wishes and suggestions
in product reviews and political discussion. Gold-
berg et al. (2009) analyzed the WISH
3
corpus and
built wish detectors based on simple word cues
and templates. Focusing on product reviews only,
Ramanand et al. (2010) created two corpora of
suggestion wishes (wishes for a change in an exist-
ing product or service) and purchasing wishes (ex-
plicit expressions of a desire to purchase a prod-
uct), and developed rules for identifying wish sen-
tences from non-wish ones. Both (Goldberg et al.,
2009; Ramanand et al., 2010) created rules manu-
ally by examining the data. Although we hypoth-
esize that wishes are related to solutions in peer
review, our educational data makes direct applica-
tion of product-motivated rules difficult. We thus
currently use statistical machine learning for our
initial research, but plan to explore incorporating
expression rules to enhance our model.
Sub-sentence annotation has gained much inter-
est in sentiment analysis and opinion mining. One
notable work is (Wilson et al., 2005) in which the
author addressed the problem that the contextual
polarity (i.e. positive, negative, or neutral) of the
phrase in which a word appears may be different
from the word?s prior polarity. We will also use a
phrase-level annotation, as described below.
3 Argument diagram review corpus
Diagramming software tools such as LASAD
(Scheuer et al., 2010) are increasingly being
used to teach student argumentation skills through
graphical representations. Graphical argument en-
vironments typically allow students to create di-
agrams in which boxes represent statements and
links represent argumentative or rhetorical rela-
tions. This helps students focus on abstract argu-
3
http://www.timessquarenyc.org/nye/nye interactive.html
101
Figure 2: Part of a student argument diagram.
ment schemes before learning how to write argu-
mentative essays. To further help students create
good argument diagrams, it has recently been sug-
gested that receiving and providing feedback on
argument diagrams might yield useful pedagogi-
cal benefits (Falakmassir et al., July 2013), anal-
ogously to improving writing via peer review of
papers.
Our corpus consists of a subset of comments
from diagram reviews collected from nine separate
sections of an undergraduate psychology course.
Student argument diagrams were created using an
instructor-defined diagram ontology. The diagram
ontology defines five different types of nodes:
Current study, Hypothesis, Theory, Finding, and
Study (for reference). The ontology also defines
four different types of arcs that connect nodes:
Supports, Opposes, Part-of, and Undecided. Fig-
ure 2 shows part of a student argument diagram
that includes two studies, each of which supports
a finding which in turn supports or opposes a hy-
pothesis. In the course that generated our corpus,
students first created graphical argument diagrams
using LASAD to justify given hypotheses. Student
argument diagrams were then distributed, using
the SWoRD (Cho and Schunn, 2007) web-based
peer-review system, to other students in the class
for reviewing. Student authors potentially revised
their argument diagrams based on peer feedback,
then used the diagrams to write the introduction
of associated papers. Diagram reviews consist of
multiple written feedback comments in response
<IU> <Pr>Not all of these field are filled out,
which makes it hard to get a clear idea of how
legit these studies are.</Pr> </IU> <IU>
<Pr>Also, some are unclear. An example is 24-
supports where the reason is a question.</Pr>
<Sl>I think there should be a substantial
reason there instead of a question to convince
me why it is relevant.</Sl> </IU>
Table 1: Example of an annotated comment.
Markers<IU>: idea unit,<Sl>: solution,<Pr>:
problem. Problem text is italic and solution text is
bold for illustration purpose.
to rubric prompts, i.e. review dimensions. Student
reviewers were required to provide at least one but
no more than three comments for each of five re-
view dimensions. Figure 1 shows three sample
peer comments for three review dimensions (i.e.
dimensions 1, 3 and 5).
Following prior work on peer review analy-
sis (Lippman et al., 2012; Nguyen and Litman,
July 2013), the first author composed a coding
manual for peer reviews of argument diagrams.
An annotator first segments each comment into
idea units (defined as contiguous feedback re-
ferring to a single topic). Note that idea-unit
segmentation is necessary to make coding reli-
able. We however do not exploit idea unit in-
formation for our current prediction tasks. Then
the annotator codes each idea unit for different
features among which solution and problem are
102
Label Number of comments
Solution 178
Problem 194
Combined 135
Non-criticism 524
Total 1031
Table 2: Comment label distribution.
the two labels used in this study. These la-
bels are then used to assign a feedback type
(i.e. solution, problem, combined, and
non-criticism) to the comment as a whole.
The comment is labeled Solution if at least
one of its idea units presents a solution but no
problem unit is explicitly present. If no solution
idea is found, the comment is labeled Problem
if at least one of its idea units presents a prob-
lem. The comment is labeled Combined if
it has both solution and problem idea units, or
Non-criticism if it does not have solution or
problem. Non-criticism units can be praise,
summary or text that does not express any idea,
e.g. ?Yes, it is.? Table 1 shows an example anno-
tated feedback comment that consists of two idea
units. The first idea unit is about empty fields, and
the second is about reason is a question. Based on
the annotations shown, the comment as a whole
has the label Combined.
We had one undergraduate psychology major
annotate the 1031 comments in our corpus, yield-
ing the label distribution shown in Table 2. The
first author also annotated 244 randomly selected
comments, solely to evaluate inter-coder agree-
ment. The obtained agreement of comment labels
was high, with accuracy 0.81 and kappa 0.74.
In addition to comment labeling, the annotator
also highlighted
4
text spans that explain the labels.
The marked text span must either express solution
or problem information but cannot express both.
Therefore we require the annotator to highlight at
the phrase (i.e. sub-sentence) level, and that each
marked text must be completely within an idea
unit. Generally speaking this requirement does
not increase cognitive workload because annota-
tors already have to read the comment and notice
any solution or problem mentioned before label-
ing.
4
Highlighting was made possible using macros in Mi-
crosoft Word. Annotators select the text of interest, then click
a button corresponding to the relevant label, e.g. problem.
Label Sentence
Problem Not all of these field are filled out,
which makes it hard to get a clear
idea of how legit these studies are.
Problem Also, some are unclear.
Problem An example is 24-supports where
the reason is a question.
Solution I think there should be a substantial
reason there instead of a question
to convince me why it is relevant.
Table 3: Examples of labeled sentences extracted
from the annotated comment.
Category Number of sentences
Solution 389
Problem 458
Non-criticism 1061
Total 1908
Table 4: Sentence label distribution.
Although we asked the annotator to mark text
spans which convey problem or solution informa-
tion, we did not ask the annotator to break each
text span into sentences. The first reason is that
the problem or solution text might only be part
of a sentence and highlighting only the informa-
tive part will give us more valuable data. Second,
sentence segmentation can be performed automat-
ically with high accuracy. After the corpus was
annotated, we ran a sentence segmentation proce-
dure using NLTK
5
to create a labeled corpus at
the sentence level as follows. Each comment is
broken into three possible parts: solution includ-
ing all solution text marked in the comment, prob-
lem including all problem text, and other for non-
criticism text. Each part is then segmented into
sentences and each sentence is assigned the label
of the part to which it belongs. It may happen
that the segmented text is a phrase rather than a
complete sentence. We consider such phrases as
reduced sentential-like text, and we use the term
sentence(s) to cover such sub-sentence forms, as
well. Labeled sentences of the comment in Table 1
are shown in Table 3. After discarding empty sen-
tences and those of length 1 (all of those are in the
non-criticism category), there are 1908 sentences
remaining, distributed as shown in Table 4.
5
www.nltk.org
103
4 Experimental setup
Sections 6 and 7 report the results of two different
experiments involving the prediction of feedback
types at the comment level. While each experi-
ment differs in the exact classes to be predicted,
both compare the predictive utility of the same two
different model-building approaches:
? Trained using comments (CTRAIN): our
baseline
6
approach learns comment predic-
tion models using labeled feedback com-
ments for training.
? Trained using sentences (STRAIN): our pro-
posed approach learns sentence prediction
models using labeled sentences, then aggre-
gates sentence prediction outputs to create
comment labels. For example, the aggrega-
tion used for the experiment in Section 6 is as
follows: if at least one sentence is predicted
as Solution/Problem then the comment is as-
signed Solution/Problem.
We hypothesize that the proposed approach will
yield better predictive performance than the base-
line because the former takes advantage of cleaner
and more discriminative training data.
To make the features of the two approaches
comparable, we use the same set of generic lin-
guistic features:
? Ngrams to capture word cues: word un-
igrams, POS/word bigrams, POS/word tri-
grams, word and POS pairs, punctuation,
word count.
? Dependency parse to capture structure cues.
We skip domain and course-specific features (e.g.
review dimensions, diagram keywords like hy-
pothesis) in order to make the learned model more
applicable to different diagram review data. In-
stead, we search for diagram keywords in com-
ments and replace them with the string ?KEY-
WORD?. The keyword list can be extracted au-
tomatically from LASAD?s diagram ontology.
Adding metadata features such as comment and
sentence ordering did not seem to improve perfor-
mance so we do not include such features in the
experiments below.
6
The use of comment-level annotations for training and
testing is similar to (Nguyen and Litman, July 2013).
Following (Xiong et al., 2012), we learn predic-
tion models using logistic regression. However, in
our work both feature extraction and model learn-
ing are performed using the LightSide
7
toolkit. As
our data is collected from nine separate sections
of the same course, to better evaluate the models,
we perform cross-section evaluation in which for
each fold we train the model using data from 8
sections and test on the remaining section. Re-
ported results are averaged over 9-fold cross vali-
dations. Four metrics are used to evaluate predic-
tion performance. Accuracy (Acc.) and Kappa (?)
are used as standard performance measurements.
Since our annotated corpus has imbalanced data
which makes the learned models bias to the ma-
jority classes, we also report the Precision (Prec.)
and Recall (Recl.) of predicting the minor classes.
5 Sentence prediction performance
We first evaluate models for predicting binary ver-
sions of the sentence labels from Table 4 (e.g. so-
lution or not), as this output will be aggregated in
our proposed STRAIN approach. The results of
using sentence training (STr) and sentence testing
(STe) are shown in the STR/STE row of Table 5.
For comparison, the first row of the table shows
the performance of a majority baseline approach
(MAJOR), which assigns all sentences the label of
the relevant major class in each prediction task.
To confirm that a sentence-level annotated corpus
is necessary to train sentence prediction models,
a third approach that uses labeled comment data
for training (CTr) but sentences for testing (STe)
is included in the CTR/STE row. As we can see,
STR/STE models outperform those of CTR/STE
and MAJOR for all 4 metrics
8
. The comment ver-
sus sentence training yields significant differences
for predicting Problem and Criticism sentences.
6 Three feedback type prediction tasks
In this experiment we evaluate our hypothesis that
STRAIN outperforms CTRAIN by comparing per-
formance on three feedback type prediction tasks
at the comment level (derived from Table 2):
? Problem v. Non-problem. The Problem class
includes problem and combined comments.
7
http://ankara.lti.cs.cmu.edu/side/download.html
8
Note that ? in general, and precision and recall of minor
classes, are not applicable when evaluating MAJOR.
104
Solution Problem Criticism
Model Acc. ? Prec. Recl. Acc. ? Prec. Recl. Acc. ? Prec. Recl.
MAJOR 0.80 - - - 0.76 - - - 0.56 - - -
CTR/STE 0.87 0.57 0.70 0.62 0.75 0.22 0.48 0.29 0.75 0.48 0.76 0.63
STR/STE 0.88 0.61 0.76 0.63 0.81 0.44 0.62 0.51 0.80 0.59 0.79 0.74
Table 5: Prediction performance of three tasks at the sentence level. Comparing STR/STE to CTR/STE:
Italic means higher with p < 0.05, Bold means higher with p < 0.01.
? Solution v. Non-solution. The Solution class
includes solution and combined comments.
? Criticism v. Non-criticism. The Criticism
class includes problem, solution and com-
bined comments.
The two approaches are also compared to majority
baselines (MAJOR) and a hybrid approach (HY-
BRD) that trains models using labeled sentence
data but tests on labeled comments.
As shown in Table 6, both MAJOR and HYBRD
perform much worse than CTRAIN and STRAIN.
We note that while HYBRD gives comparably high
precision, its kappa and recall do not match those
of CTRAIN and STRAIN. Comparing CTRAIN
and STRAIN, the results confirm our hypothesis
that STRAIN outperforms CTRAIN. The major ad-
vantage of STRAIN is that it only needs one cor-
rectly predicted sentence to yield the correct com-
ment label. This is particularly beneficial for pre-
dicting problem comments, where the improve-
ment is significant for 3 of 4 metrics.
As our evaluation is cross-section, folds do not
have identical label distributions. Therefore we
look at prediction performance for each of the nine
individual sections. We find that the sentence level
approach yields higher performance on all four
metrics in six sections when predicting both So-
lution and Problem task, but only two sections for
Criticism. For the Criticism task ? where it is not
necessary to exclusively differentiate between So-
lution and Problem, training prediction models us-
ing labeled sentences does not yield higher perfor-
mance than the traditional approach.
Roughly comparing predicting at the sentence
level (Table 5) versus the comment level (Table 6),
we note that the sentence level tasks are more dif-
ficult (e.g. lower absolute kappas) despite an in-
tuition that the labeled sentence corpus is cleaner
and more discriminative compared to the labeled
comment corpus. The observed performance dis-
parity shows the necessity of developing better
sentence prediction models, which we leave to fu-
ture work.
7 A case study experiment
To the best of our knowledge, (Xiong et al.,
June 2010; Xiong et al., 2012) contain the only
published models developed for predicting feed-
back types. A comment-level solution prediction
model has since been deployed in their peer re-
view software to evaluate student reviewer com-
ments in classroom settings, using the following 3-
way classification algorithm
9
. Each student com-
ment is classified as either a criticism (i.e. presents
problem/solution information) or a non-criticism.
The non-criticism comment is labeled NULL. The
criticism comment is labeled SOLUTION if it con-
tains solution information, and labeled PROBLEM
otherwise.
To evaluate our proposed STRAIN approach in
their practically-motivated setting, we follow the
description above to relabel peer feedback com-
ments in our corpus to new labels: NULL, PROB-
LEM, and SOLUTION. We also asked the authors
of (Xiong et al., 2012) for access to their current
model and we were able to run their model on our
feedback comment data. While it is not appro-
priate to directly compare model performance as
Xiong et al. were working with paper (not dia-
gram) review data, we report their model output,
named PAPER, to provide a reference baseline. We
expect the PAPER model to work on our diagram
review data to some extent, particularly due to its
predefined seed words for solution and problem
cues. Our CTRAIN baseline, in contrast, trains
models regarding the new label set using relabeled
diagram comment data, with the same features and
learning algorithm from the prior sections. The
majority baseline, MAJOR, assigns all comments
the major class label (which is now NULL).
Regarding our STRAIN sentence level ap-
9
Personal communication.
105
Solution Problem Criticism
Model Acc. ? Prec. Recl. Acc. ? Prec. Recl. Acc. ? Prec. Recl.
MAJOR 0.70 - - - 0.68 - - - 0.51 - - -
HYBRD 0.82 0.52 0.87 0.48 0.75 0.36 0.68 0.41 0.78 0.56 0.84 0.68
CTRAIN 0.87 0.67 0.84 0.71 0.76 0.43 0.65 0.55 0.83 0.66 0.85 0.80
STRAIN 0.88 0.71 0.86 0.74 0.81 0.55 0.71 0.66 0.85 0.70 0.84 0.85
Table 6: Prediction performance of three tasks at comment level. Comparing STRAIN to CTRAIN: Italic
means higher with p < 0.1, Bold means higher with p < 0.05.
1. For each sentence, label it SOLUTION if it is
predicted as Solution by the Solution model.
2. For a predicted Non-solution sentence, label
it NULL if it is predicted as Non-criticism by the
Criticism model.
3. For a predicted Criticism sentence, label it
PROBLEM if it is predicted as Problem by the
Problem model.
4. For a predicted Non-problem sentence, label
it SOLUTION
Table 7: Relabel procedure.
proach, we propose two aggregation procedures
to infer comment labels given sentence prediction
output. In the first procedure, RELABELFIRST, we
infer new sentence labels regarding NULL, PROB-
LEM, and SOLUTION using a series of condi-
tional statements. The order of statements is cho-
sen heuristically given the performance of indi-
vidual models (see Table 5) and is described in
Table 7. Given the sentences? inferred labels,
the comment is labeled SOLUTION if it has at
least one SOLUTION sentence. Else, it is labeled
PROBLEM if at least one of its sentences is PROB-
LEM, and labeled NULL otherwise. Our second
aggregation procedure, called INFERFIRST, fol-
lows an opposite direction in which we infer com-
ment labels regarding Solution, Problem, and Crit-
icism before re-labeling the comment regarding
SOLUTION, PROBLEM, and NULL following the
order of conditional statements in the relabel pro-
cedure.
As shown in Table 8, the MAJOR and PAPER
models perform much worse than the other three
models. While the PAPER model has accuracy
close to that of the other models, its kappa is far
lower. Regarding the three models trained on di-
agram review data, the two sentence level models
outperform the CTRAIN model. Particularly, kap-
Model Acc. ?
MAJOR 0.51 -
PAPER 0.71 0.49
CTRAIN 0.76 0.60
RELABELFIRST 0.79 0.66
INFERFIRST 0.79 0.66
Table 8: Prediction performance of different ap-
proaches in a case study.
pas of the two sentence level models are either sig-
nificantly higher (for INFERFIRST) or marginally
higher (for RELABELFIRST) compared to kappa
of CTRAIN. To further investigate performance
disparity between models, we report in Table 9
precision and recall of different models for each
class. The PAPER model achieves high preci-
sion but low recall for SOLUTION and PROBLEM
classes. We reason that the model?s seed words
help its precision, but its ngram features, which
were trained using paper review data, cannot ad-
equately cover positive instances in our corpus.
The two sentence level models perform better for
the PROBLEM class than the other two models,
which is consistent with what is reported in Ta-
ble 6. Comparing the two sentence level models,
INFERFIRST better balances precision and recall
than RELABELFIRST.
8 The sentence level is right
The experimental results in the previous two sec-
tions have demonstrated that sentence prediction
output helps improve prediction performance at
the comment level. This supports our hypothesis
that sentence prediction is the right level for en-
hancing peer review systems to detect and respond
to multi-sentence review comments of low qual-
ity. In our labeled sentence corpus, each instance
either expresses a solution, a problem, or is a non-
criticism, so the data is cleaner and more discrim-
106
SOLUTION PROBLEM NULL
Model Prec. Recl. Prec. Recl. Prec. Recl.
MAJOR - - - - 0.51 1.00
PAPER 0.81 0.62 0.58 0.29 0.70 0.92
CTRAIN 0.84 0.75 0.55 0.41 0.78 0.90
RELABELFIRST 0.72 0.90 0.66 0.48 0.88 0.84
INFERFIRST 0.75 0.86 0.61 0.55 0.88 0.84
Table 9: Precision and recall of different models in a case study.
inative than the labeled comment corpus. This is
a nice property that helps reduce feature colloca-
tion across exclusive classes, Problem vs. Solution
for example, which is a danger of training on feed-
back comments due to Combined instances. More-
over, our annotated comment corpus has solution
and problem text marked at the sub-sentence level,
which is a valuable resource for learning solution
and problem patterns and linguistic cues.
Improving peer feedback prediction accuracy is
not the only reason we advocate for the sentence
level. We envision that the sentence level is the
necessary lower bound that a peer review system
needs to handle new advanced functionalities such
as envisioned in Figure 1. Being able to highlight
featured text in a peer comment is a useful visu-
alization function that should help peer reviewers
learn from live examples, and may also help stu-
dent authors quickly notice the important point of
the comment.
Sentence and phrase level annotation is made
easy with the availability of many text annota-
tion toolkits; BRAT
10
(Stenetorp et al., 2012) is
an example. From our work, marking text spans
by selecting and clicking requires a minimal ad-
ditional effort from annotators and does not cause
more cognitive workload. Moreover, we hypoth-
esize that through highlighting the text, an anno-
tator has to reason about why she would choose a
label, which in turn makes the annotation process
more reliable. We plan to test whether annotation
performance does indeed improve in future work.
9 Conclusions and future work
In this paper we present a sentence-level anno-
tated corpus of argument diagram peer review
data, which we use to develop comment-level pre-
dictions of peer feedback types. Our work is the
first of its kind in building an automated feed-
10
http://brat.nlplab.org/
back type assessment component for reviews of
argument diagrams rather than papers. We have
demonstrated that using sentence prediction out-
puts to label the corresponding comments outper-
forms the traditional approach that learns mod-
els using labeled comments. The improvement
of using sentence prediction outputs is more sig-
nificant for more difficult tasks, i.e. Problem vs.
Non-problem, in which textual expression varies
greatly from explicit to implicit. In a case study
mimicking a real application setting to experiment
with the proposed models, we achieved a simi-
lar verification of the utility of sentence models.
Given our imbalanced training data labels and our
avoidance of using domain-specific features, these
first results of our two experiments are promising.
In these first studies, our models were trained
using generic prediction procedures, e.g., using
basic linguistic features without feature selection
or tuning. Thus our next step is to analyze pre-
diction features for their predictiveness. We also
plan to incorporate human-engineered rules for so-
lution and problem text. We aim to improve per-
formance while keeping feature generality. An in-
teresting experiment we may conduct is to test our
learned models on paper review data to evaluate
performance and generality in an extreme setting.
Acknowledgments
This work is supported by NFS Grant No.
1122504. We are grateful to our colleagues for
sharing the data. We thank Kevin Ashley, Wencan
Luo, Fan Zhang, other members of the Argument-
Peer and ITSPOKE groups as well as the anony-
mous reviewers for their valuable feedback.
References
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A
web-based reciprocal peer review system. Comput-
ers and Education, 48(3):409?426.
107
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings 1st inter-
national conference on Educational Data Mining
(EDM), pages 192?196.
Mohammad Falakmassir, Kevin Ashley, and Christian
Schunn. July 2013. Using argument diagramming
to improve peer grading of writing assignments. In
Proceedings of the 1st Workshop on Massive Open
Online Courses at 16th International Conference on
Artificial Intelligence in Education (AIED), Mem-
phis, TN, pages 41?48.
Sarah Gielen, Elien Peeters, Filip Dochy, Patrick
Onghena, and Katrien Struyven. 2010. Improv-
ing the effectiveness of peer feedback for learning.
Learning and Instruction, 20(4):304?315.
Andrew B. Goldberg, Nathanael Fillmore, David An-
drzejewski, Zhiting Xu, Bryan Gibson, and Xiao-
jin Zhu. 2009. May all your wishes come true: A
study of wishes and how to recognize them. In Pro-
ceedings Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL-HTL, pages 263?271.
Jordan Lippman, Mike Elfenbein, Matthew Diabes,
Cori Luchau, Collin Lynch, Kevin Ashley, and Chris
Schunn. 2012. To revise or not to revise: What
influences undergrad authors to implement peer cri-
tiques of their argument diagrams? In International
Society for the Psychology of Science and Technol-
ogy 2012 Conference. Poster.
Melissa M. Nelson and Christian D. Schunn. 2009.
The nature of feedback: how different types of peer
feedback affect writing performance. Instructional
Science, 37(4):375?401.
Huy V. Nguyen and Diane J. Litman. July 2013. Iden-
tifying localization in peer reviews of argument di-
agrams. In Proceedings 16th International Confer-
ence on Artificial Intelligence in Education (AIED),
Memphis, TN, pages 91?100.
Huy Nguyen, Wenting Xiong, and Diane Litman. June
2014. Classroom evaluation of a scaffolding inter-
vention for improving peer review localization. In
Proceedings 12th International Conference on Intel-
ligent Tutoring Systems (ITS), Honolulu, HI, pages
272?282.
Lakshmi Ramachandran and Edward F. Gehringer.
2011. Automated assessment of review quality us-
ing latent semantic analysis. In Proceedings 11th
IEEE International Conference on Advanced Learn-
ing Technologies (ICALT), pages 136?138.
J. Ramanand, Krishna Bhavsar, and Niranjan
Pedanekar. 2010. Wishful thinking: finding
suggestions and ?buy? wishes from product reviews.
In Proceedings the NAACL-HLT 2010 Workshop
on Computational Approaches to Analysis and
Generation of Emotion in Text, pages 54?61.
Oliver Scheuer, Frank Loll, Niels Pinkwart, and
Bruce M. McLaren. 2010. Computer-supported ar-
gumentation: A review of the state of the art. In-
ternational Journal of Computer-Supported Collab-
orative Learning, 5(1):43?102.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: a web-based tool for nlp-assisted
text annotation. In Proceedings the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102?107.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Joint Human Language
Technology Conference and the Conference on Em-
pirical Methods in Natural Language Processing
(HLT-EMNLP), pages 347?354.
Wenting Xiong and Diane Litman. 2010. Identify-
ing problem localization in peer-review feedback. In
Proceedings 10th International Conference on Intel-
ligent Tutoring System (ITS), Pittsburgh, PA. Poster.
Wenting Xiong, Diane Litman, and Christian Schunn.
2012. Natural language processing techniques for
researching and improving peer feedback. Journal
of Writing Research, 4(2):155?176.
Wenting Xiong, Diane Litman, and Christian Schunn.
June 2010. Assessing reviewer?s performance based
on mining problem localization in peer-review data.
In Proceedings 3rd International Conference on
Educational Data Mining (EDM), Pittsburgh, PA,
pages 211?220.
108
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 149?154,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Sentence-level Rewriting Detection
Fan Zhang
University of Pittsburgh
Pittsburgh, PA, 15260
zhangfan@cs.pitt.edu
Diane Litman
University of Pittsburgh
Pittsburgh, PA, 15260
litman@cs.pitt.edu
Abstract
Writers usually need iterations of revisions
and edits during their writings. To bet-
ter understand the process of rewriting,
we need to know what has changed be-
tween the revisions. Prior work mainly fo-
cuses on detecting corrections within sen-
tences, which is at the level of words
or phrases. This paper proposes to de-
tect revision changes at the sentence level.
Looking at revisions at a higher level al-
lows us to have a different understanding
of the revision process. This paper also
proposes an approach to automatically de-
tect sentence revision changes. The pro-
posed approach shows high accuracy in an
evaluation using first and final draft essays
from an undergraduate writing class.
1 Introduction
Rewriting is considered to be an important process
during writing. However, conducting successful
rewriting is not an easy task, especially for novice
writers. Instructors work hard on providing sug-
gestions for rewriting (Wells et al., 2013), but usu-
ally such advice is quite general. We need to un-
derstand the changes between revisions better to
provide more specific and helpful advice.
There has already been work on detecting cor-
rections in sentence revisions (Xue and Hwa,
2014; Swanson and Yamangil, 2012; Heilman
and Smith, 2010; Rozovskaya and Roth, 2010).
However, these works mainly focus on detecting
changes at the level of words or phrases. Ac-
cording to Faigley?s definition of revision change
(Faigley and Witte, 1981), these works could help
the identification of Surface Changes (changes
that do not add or remove information to the orig-
inal text). However, Text Changes (changes that
add or remove information) will be more difficult
to identify if we only look at revisions within sen-
tences. According to Hashemi and Schunn (2014),
when instructors were presented a comparison of
differences between papers derived from words,
they felt the information regarding changes be-
tween revisions was overwhelming.
This paper proposes to look at the changes be-
tween revisions at the level of sentences. Com-
paring to detecting changes at the word level, de-
tecting changes at the sentence level contains less
information, but still keeps enough information
to understand the authors? intention behind their
modifications to the text. The sentence level edits
could then be grouped and classified into differ-
ent types of changes. The long-term goal of this
project is to allow us to be able to identify both
Text Changes and Surface Changes automatically.
Students, teachers, and researchers could then per-
form analysis on the different types of changes and
have a better understanding of the rewriting pro-
cess. As a preliminary work, this paper explores
steps toward this goal: First, automatically gener-
ate the description of changes based on four prim-
itives: Add, Delete, Modify, Keep; Second, merge
the primitives that come from the same purpose.
2 Related work
Hashemi and Schunn (2014) presented a tool
to help professors summarize students? changes
across papers before and after peer review. They
first split the original documents into sentences
and then built on the output of Compare Suite
(CompareSuite, 2014) to count and highlight
changes in different colors. Figure 1 shows a
screenshot of their work. As we can see, the mod-
ifications to the text are misinterpreted. Line 66
in the final draft should correspond to line 55 and
line 56 in the first draft, while line 67 and line 68
should be a split of line 57 in the first draft. How-
ever, line 67 is aligned to line 56 wrongly in their
work. This wrong alignment caused many mis-
149
recognized modifications. According to Hashemi,
the instructors who use the system think that the
overwhelming information of changes make the
system less useful. We hypothesize that since their
work is based on analysis at the word level, al-
though their approach might work for identifying
differences within one sentence, it makes mistakes
when sentence analysis is the primary concern.
Our work avoids the above problem by detect-
ing differences at the sentence level. Sentence
alignment is the first step of our method; fur-
ther inferences about revision changes are then
based on the alignments generated. We borrow
ideas from the research on sentence alignment for
monolingual corpora. Existing research usually
focuses on the alignment from the text to its sum-
marization or its simplification (Jing, 2002; Barzi-
lay and Elhadad, 2003; Bott and Saggion, 2011).
Barzilay and Elhadad (2003) treat sentence align-
ment as a classification task. The paragraphs are
clustered into groups, and a binary classifier is
trained to decide whether two sentences should be
aligned or not. Nelken (2006) further improves
the performance by using TF*IDF score instead of
word overlap and also utilizing global optimiza-
tion to take sentence order information into con-
sideration. We argue that summarization could
be considered as a special form of revision and
adapted Nelken?s approach to our approach.
Edit sequences are then inferred based on the
results of sentence alignment. Fragments of ed-
its that come from the same purpose will then be
merged. Related work to our method is sentence
clustering (Shen et al., 2011; Wang et al., 2009).
While sentence clustering is trying to find and
cluster sentences similar to each other, our work
is to find a cluster of sentences in one document
that is similar to one sentence in the other docu-
ment after merging.
3 Sentence-level changes across revisions
3.1 Primitives for sentence-level changes
Previous work in educational revision analysis
(Faigley and Witte, 1981; Connor and Asenav-
age, 1994) categorized revision changes to be ei-
ther surface changes or text-based changes. With
both categories, six kinds of changes were defined
as shown in Table 1.
Different from Faigley?s definition, we define
only 4 primitives for our first step of edit sequence
generation: Add, Delete, Modify and Keep. This
Code Explanation
Addition Adding a word or phrase
Deletion Omitting a word or phrase
Substitutions exchange words with synonyms
Permutation rearrange of words or phrases
Distribution one segment divided into two
Consolidation combine two segments into one
Table 1: Code Definition by L.Faigley and S.Witte
definition is similar to Bronner?s work (Bronner
and Monz, 2012). We choose this definition be-
cause these 4 primitives only correspond to one
sentence at a time. Add, Delete, Modify indicates
that the writer has added/deleted/modified a sen-
tence. Keep means the original sentence is not
modified. We believe Permutation, Distribution
and Consolidation as defined by Faigley could be
described with these four primitives, which could
be recognized in the later merge step.
3.2 Data and annotation
The corpus we choose consists of paired first and
final drafts of short papers written by undergradu-
ates in a course ?Social Implications of Comput-
ing Technology?. Students are required to write
papers on one topic and then revise their own pa-
pers. The revisions are guided by other students?
feedback based on a grading rubric, using a web-
based peer review system. Students first submitted
their original paper into the system, and then were
randomly assigned to review and comment others?
work according to the writing rubric. The authors
would receive the others? anonymous comments,
and then could choose to revise their work based
on others? comments as well as their own insights
obtained by reviewing other papers.
The papers in the corpus contain two topics.
In the first topic, the students discussed the role
that Big Data played in Obama?s presidential cam-
paign. This topic contains 11 pairs of first and final
drafts of short papers. We name this C1. The other
topic, named C2, talks about intellectual property
and contains 10 pairs of paper drafts. The students
involved in these two topics are from the same
class. Students make more modifications to their
papers in C2. More details can be seen in Table 2.
Our revision change detection approach con-
tains three steps: sentence alignment, edit se-
quence generation and merge of edit sequences.
Thus we annotated for these three steps.
150
(a) first draft (b) final draft
(c) Revision detection using Hashemi?s approach
Figure 1: Fragments of a paper in corpus C2 discussing intellectual property, (c) is Hashemi?s work,
green for recognized modifications, blue for insertions and red for deletion
For sentence alignment, each sentence in the fi-
nal draft is assigned the index of its aligned sen-
tence in the original draft. If a sentence is newly
added, it will be annotated as ADD. Sentence
alignment is not necessarily one-to-one. It can
also be one-to-many (Consolidation) and many-
to-one (Distribution). Table 3 shows a fragment
of the annotation for the text shown in Figure 1.
For edit sequences, the annotators do the anno-
tation based on the initial draft. For the same frag-
ment in Table 3, the annotated sequence is: Keep,
Modify, Delete, Modify, Add
1
.
For edit sequence merging, we further annotate
Consolidation and Distribution based on the edit
sequences. In our example, 66 consolidates 55 and
56, while 57 distributes to 67 and 68.
pairs #D1 #D2 Avg1 Avg2
C1 11 761 791 22.5 22.7
C2 10 645 733 24.7 24.5
Table 2: Detailed information of corpora. #D1 and
#D2 are the number of sentences in the first and
final draft, Avg1 and Avg2 are the average number
of words in one sentence in the first and final draft
As a preliminary work, we only have one anno-
tator doing all the annotations. But for the anno-
tation of sentence alignments, we have two anno-
1
66 consolidates 55, 56; while 57 distributes to 67, 68.
Notice that Consolidation is illustrated as Modify, Delete and
Distribution is illustrated as Modify, Add. As the annotators
annotate based on the first draft, Modify always appears be-
fore Add or Delete
tators annotating on one pair of papers. The paper
contains 76 sentences, and the annotators only dis-
agree in one sentence. The kappa is 0.794
2
, which
suggests that the annotation is reliable based on
our annotation scheme.
4 Automatic detection of revision
changes
The detection of revision changes contains three
parts: sentence alignment, edit sequence genera-
tion and edit sequence merging. The first two parts
generate edit sequences detected at the sentence
level, while the third part groups edit sequences
and classifies them into different types of changes.
Currently the third step only covers the identifica-
tion of Consolidation and Distribution.
Sentence Index (Final) 65 66 67 68
Sentence Index (First) 54 55,56 57 57
Table 3: An example of alignment annotation
Sentence alignment We adapted Nelken?s ap-
proach to our problem.
Alignment based on sentence similarity
The alignment task goes through three stages.
1. Data preparation: for each sentence in the an-
notated final draft, if it is not a new sentence, cre-
ate a sentence pair with its aligned sentence in the
2
We calculate the Kappa value following Macken?s idea
(Macken, 2010), where the aligned sentences are categorized
as direct-link, while new added sentences are categorized as
null-link (ADD).
151
first draft. The pair is considered to be an aligned
pair. Also, randomly select another sentence from
the first draft to make a negative sentence pair.
Thus we ensure there are nearly equal numbers of
positive and negative cases in the training data.
2. Training: according to the similarity met-
ric defined, calculate the similarity of the sentence
pairs. A logistic regression classifier predicting
whether a sentence pair is aligned or not is trained
with the similarity score as the feature. In addi-
tion to classification, the classifier is also used to
provide a similarity score for global alignment.
3. Alignment: for each pair of paper drafts, con-
struct sentence pairs using the Cartesian product
of sentences in the first draft and sentences in the
final. Logistic regression classifier is used to deter-
mine whether the sentence pair is aligned or not.
We added Levenshtein distance (LD) (Leven-
shtein, 1966) as another similarity metric in ad-
dition to Nelken?s metrics. Together three similar-
ity metrics were compared: Levenshtein Distance,
Word Overlap(WO), and TF*IDF.
Global alignment
Sentences are likely to preserve the same or-
der between rewritings. Thus, sentence or-
dering should be an important feature in sen-
tence alignment. Nelken?s work modifies the
Needleman-Wunsch alignment (Needleman and
Wunsch, 1970) to find the sentence alignments and
goes in the following steps.
Step1: The logistic regression classifier previ-
ously trained assigns a probability value from 0 to
1 for each sentence pair s(i, j). Use this value as
the similarity score of sentence pair: sim(i, j).
Step2: Starting from the first pair of sen-
tences, find the best path to maximize the likeli-
hood between sentences according to the formula
s(i, j) = max{s(i ? 1, j ? 1) + sim(i, j), s(i ?
1, j) + sim(i, j) , s(i, j ? 1) + sim(i, j)}
Step3: Infer the sentence alignments by back
tracing the matrix s(i, j).
We found out that changing bolded parts in the
formula to s(i, j) = max{s(i ? 1, j ? 1) +
sim(i, j), s(i ? 1, j) + insertcost , s(i, j ? 1) +
deletecost} shows better performance in our prob-
lem. According to our experiment with C1, insert-
cost and deletecost are both set to 0.1 as they are
found to be the most effective during practice.
Edit sequence generation This step is an inter-
mediate step, which tries to generate the edit se-
quence based on the sentence alignment results
from the previous step. The edit sequences gen-
erated would later be grouped together and clas-
sified into different types. In our current work, a
rule-based method is proposed for this step.
Step1: The index of original document i and the
index of the modified document j both start from
0. If sentence i in the original document is aligned
to sentence j in the modified one, go to step 2, if
not go to step 3.
Step2: If the two sentences are exactly the same,
add Keep to the edit sequence, if not, add Modify.
Increase i and j by 1, go to step 1.
Step3: Check the predicted alignment index of
sentence j, if the predicted index is larger than sen-
tence i in the original document, add Delete and
increase i by 1, otherwise, mark as Add and in-
crease j by 1, go to step 1.
Edit sequence merging Distribution means
splitting one sentence into two or more sentences,
while Consolidation means merging two or more
sentences into one sentence. These two operations
can be derived with primitives Modify, Add and
Delete. They follow the following patterns:
Consolidation: Modify-Delete-Delete-...
Distribution: Modify-Add-Add-...
These sequences both start with Modify fol-
lowed with a repetitive number of Delete or Add.
A group of edit sequences can be merged if they
can be merged to a sentence close to the sentence
in the other draft. We applied a rule-based ap-
proach based on our observations.
We first scan through the sequence generated
above. Sequences with Modify-Add-... or Mod-
ify-Delete-... are extracted. For each sequence ex-
tracted, if there are n consecutive Add or Delete
following Modify, create n groups, Group
i
(i ?
n) contains sentences from the modified sentence
to the next consecutive i sentences. For each
group, merge all the sentences, and use the clas-
sifier trained above to get the similarity score
Sim
group
i
between the merged sentence and the
original one. If there are multiple groups classi-
fied as aligned, choose group i that has the largest
Sim
group
i
, merge the basic edit operations into
Consolidation or Distribution. If none of the
groups are classified as aligned, do not merge.
5 Evaluation
Sentence alignment We use accuracy as the
evaluation metric. For each pair of drafts, we
count the number of sentences in the final draft
152
N1
. For each sentence in the final draft, we count
the number of sentences that get the correct align-
ment as N
2
. The accuracy of the sentence align-
ment is
N
2
N
1
.
3
We use Hashemi?s approach as the baseline.
Compare Suite colors the differences out, as
shown in Figure 1. We treat the green sentences
as Modify and aligned to the original sentence.
For our method, we tried four groups of set-
tings. Group 1 and group 2 perform leave-one-out
cross validation on C1 and C2 (test on one pair of
paper drafts and train on the others). Group 3 and
group 4 train on one corpus and test on the other.
Group LD WO TF*IDF Baseline
1 0.9811 0.9863 0.9931 0.9427
2 0.9649 0.9593 0.9667 0.9011
3 0.9727 0.9700 0.9727 0.9045
4 0.9860 0.9886 0.9798 0.9589
Table 4: Accuracy of our approach vs. baseline
Table 4 shows that all our methods beat the
baseline
4
. Among the three similarity metrics,
TF*IDF is the most predictive.
Edit sequence generation We use WER (Word
Error Rate) from speech recognition for evaluat-
ing the generated sequence by comparing the gen-
erated sequence to the gold standard.
WER is calculated based on edit distances be-
tween sequences. The ratio is calculated as:
WER =
S+D+I
N
, where S means the number of
modifications, D means the number of deletes, I
means the number of inserts.
We apply our method on the gold standard of
sentence alignment. The generated edit sequence
is then compared with the gold standard edit se-
quence to calculate WER. Hashemi?s approach is
chosen as the baseline. The WER of our method is
0.035 on C1 and 0.017 on C2, comparing to 0.091
on C1 and 0.153 on C2 for the baseline, which
shows that our rule-based method has promise.
3
Notice that we have the case that one sentence is aligned
to two sentences (i.e. Consolidation, as sentence 66 in Table
3). In our evaluation, an alignment is considered to be correct
only if the alignment covers all the sentences that should be
covered. For example, if Sentence 66 in Table 3 is aligned to
Sentence 55 in the first draft, it is counted as an error.
4
For Groups 1 and 2, we calculate the accuracy of
Hashemi?s approach under a leave-one-out setting, each time
remove one pair of document and calculate the accuracy. A
significance test is also conducted, the worst metric LD in
Group 1 and WO in Group 2 both beat the baseline signifi-
cantly ( p
1
= 0.025,p
2
= 0.017) in two-tailed T-test.
Applying our method on the predicted alignment
on the first step gets 0.067 on C1 and 0.025 on C2,
which although degraded still beats the baseline.
Edit sequence merging There are only a limited
number of Consolidation and Distribution exam-
ples in our corpus. Together there are 9 Consolida-
tion and 5 Distribution operations. In our current
data, the number of sentences involved in these
operations is always 2. Our rule-based method
achieved 100% accuracy in the identification of
these operations. It needs further work to see if
this method would perform equally well in more
complicated corpora.
6 Conclusion
This paper presents a preliminary work in the ef-
fort of describing changes across revisions at a
higher level than words, motivated by a long term
goal to build educational applications to support
revision analysis for writing. Comparing to revi-
sion analysis based on words or phrases, our ap-
proach is able to capture higher level revision op-
erations. We also propose algorithms to detect re-
vision changes automatically. Experiments show
that our method has a reliable performance.
Currently we are investigating applying se-
quence merging on the automatic generated edit
sequences based on edit distances directly. Our
next plan is to develop a tool for comparing drafts,
and conduct user studies to have extrinsic evalua-
tions on whether our method would provide more
useful information to the user. We are also plan-
ning to do further analysis based on the revisions
detected, and ultimately be able to distinguish be-
tween surface changes and text-based changes.
Acknowledgments
We would like to thank W. Wang, W. Luo, H. Xue,
and the ITSPOKE group for their helpful feedback
and all the anonymous reviewers for their sugges-
tions.
This research is supported by the Institute of
Education Sciences, U.S. Department of Educa-
tion, through Grant R305A120370 to the Univer-
sity of Pittsburgh. The opinions expressed are
those of the authors and do not necessarily repre-
sent the views of the Institute or the U.S. Depart-
ment of Education.
153
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?
32. Association for Computational Linguistics.
Stefan Bott and Horacio Saggion. 2011. An un-
supervised alignment algorithm for text simplifica-
tion corpus construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
pages 20?26. Association for Computational Lin-
guistics.
Amit Bronner and Christof Monz. 2012. User edits
classification using document revision histories. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 356?366. Association for Computa-
tional Linguistics.
CompareSuite. 2014. Compare suite, feature-rich
file and folder compare tool. http://www.
comparesuite.com.
Ulla Connor and Karen Asenavage. 1994. Peer re-
sponse groups in esl writing classes: How much im-
pact on revision? Journal of Second Language Writ-
ing, 3(3):257?276.
Lester Faigley and Stephen Witte. 1981. Analyzing
revision. College composition and communication,
pages 400?414.
Homa B. Hashemi and Christian D. Schunn. 2014.
A tool for summarizing students? shanges across
drafts. In International Conference on Intelligent
Tutoring Systems(ITS).
Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011?1019.
Association for Computational Linguistics.
Hongyan Jing. 2002. Using hidden markov modeling
to decompose human-written summaries. Computa-
tional linguistics, 28(4):527?543.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Lieve Macken. 2010. An annotation scheme and
gold standard for dutch-english word alignment.
In 7th conference on International Language Re-
sources and Evaluation (LREC 2010), pages 3369?
3374. European Language Resources Association
(ELRA).
Saul B Needleman and Christian D Wunsch. 1970.
A general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of molecular biology, 48(3):443?453.
Rani Nelken and Stuart M Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In EACL.
Alla Rozovskaya and Dan Roth. 2010. Annotating
esl errors: Challenges and rewards. In Proceedings
of the NAACL HLT 2010 fifth workshop on innova-
tive use of NLP for building educational applica-
tions, pages 28?36. Association for Computational
Linguistics.
Chao Shen, Tao Li, and Chris HQ Ding. 2011. Inte-
grating clustering and multi-document summariza-
tion by bi-mixture probabilistic latent semantic anal-
ysis (plsa) with sentence bases. In AAAI.
Ben Swanson and Elif Yamangil. 2012. Correction
detection and error type selection as an esl educa-
tional aid. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 357?361. Association for Computa-
tional Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 297?300. Association for Computational Lin-
guistics.
Jaclyn M. Wells, Morgan Sousa, Mia Martini, and
Allen Brizee. 2013. Steps for revising your pa-
per. http://owl.english.purdue.edu/
owl/resource/561/05.
Huichao Xue and Rebecca Hwa. 2014. Improved cor-
rection detection in revised esl sentences. In Pro-
ceedings of The 52nd Annual Meeting of the Associ-
ation for Computational Linguistics(ACL).
154
Proceedings of the First Workshop on Argumentation Mining, pages 24?28,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Ontology-Based Argument Mining and Automatic Essay Scoring
Nathan Ong, Diane Litman, and Alexandra Brusilovsky
Department of Computer Science, University of Pittsburgh
Pittsburgh, PA 15260 USA
nro5,dlitman,apb27@pitt.edu
Abstract
Essays are frequently used as a medium
for teaching and evaluating argumentation
skills. Recently, there has been interest in
diagrammatic outlining as a replacement
to the written outline that often precedes
essay writing. This paper presents a pre-
liminary approach for automatically iden-
tifying diagram ontology elements in es-
says, and demonstrates its positive corre-
lation with expert scores of essay quality.
1 Introduction
Educators tend to favor students providing a
minimal-writing structure, or an outline, before
writing a paper. This allows teachers to give
early feedback to students to reduce the amount
of structural editing that might be needed later
on. However, there is evidence to suggest that
standard text-based outlines do not necessarily im-
prove writing quality (Torrance et al., 2000). Re-
cently, there has been growing interest in graph-
ical outline representations, especially for argu-
mentative essays in various domains (Scheuer et
al., 2009; Scheuer et al., 2010; Peldszus and Stede,
2013; Reed and Rowe, 2004; Reed et al., 2007).
Not only do they provide a different outlining for-
mat, but they also allow students to concretely vi-
sualize their argumentation structure. Our work
is part of the ArgumentPeer project (Falakmassir
et al., 2013), which combines computer-supported
argument diagramming and peer-review with the
goal of improving students? writing skills.
In this paper, we follow the lead of others in dis-
course parsing for essay scoring (Burstein et al.,
2001), and we preliminarily attempt to answer two
questions: Q1) Can an argument mining system
be developed to automatically recognize the ar-
gument ontology used during diagramming, when
processing a student?s later written essay? Q2) If
so, is the number of ontological elements that can
be recognized in a student?s essay correlated with
the essay?s argumentation quality? Potentially, an-
swering these questions in the affirmative would
allow us to assist students with their writing by al-
lowing computer tutors to label sentences with the
ontology, determine which elements are missing,
and suggest adding these missing elements to im-
prove essay quality.
2 Corpus
Our corpus for argument mining consists of 52 es-
says written in two University of Pittsburgh un-
dergraduate psychology courses. In both courses,
students were asked to write an argumentative es-
say supporting two separate hypotheses that they
created based on data they were given. The aver-
age essay contains 5.2 paragraphs, 28.6 sentences,
and 592.1 words.
Before writing the essay, students were first re-
quired to generate an argument diagram justify-
ing their hypotheses using the LASAD argumen-
tation system
1
. LASAD argument diagrams con-
sist of nodes and arcs from an instructor-defined
ontology, as shown in Figure 1. Next, students
were required to turn their diagrams into writ-
ten argumentative essays. Automatically tagging
these essays according to the 4 node types (Cur-
rent Study, Hypothesis, Claim, Citation) and 2
arc types (Supports, Opposes) common to both
courses is the argument mining goal of this pa-
per. The tagged essay corresponding to Figure 1 is
shown in Table 1.
2
While the diagram is required
to be completed by students, this work does not
utilize the student diagrams.
1
http://lasad.dfki.de
2
Both diagrams and papers were distributed to other stu-
dents in the class for peer review. While the diagrams were
not required to be revised, students needed to revise their es-
says to address peer feedback. To maximize diagram and es-
say similarity, here we work with only the first drafts.
24
Figure 1: An argument diagram from a research methods course.
After the courses, expert graders were asked to
score all essays on a 5-point Likert scale (with 1
being the lowest and 5 being the highest) without
the diagrams, using a rubric with multiple crite-
ria. For the essay as a whole, graders not only
checked for correct grammar usage, but also for
flow and organization. In addition, essays were
graded based on the logic behind their argumen-
tation of their hypotheses, as well as addressing
claims that both supported and opposed their hy-
potheses. While not an explicit category, many of
the criteria required students to present multiple
citations backing their hypotheses. The average
expert score for the 52 essays is 3.03, and the me-
dian is 3, with the scores distributed as shown in
column four of Table 2.
3 Methodology
Essay Discourse Processing. Firstly, raw essays
are parsed for discourse connectives. Explicit dis-
course connectives are then tagged with their sense
(i.e. Expansion, Contingency, Comparison, or
Temporal) using the Discourse Connectives Tag-
ger
3
, as shown in Table 1.
Mining the Argument Ontology. We devel-
oped a rule-based algorithm to label each sentence
3
http://www.cis.upenn.edu/
?
epitler/
discourse.html
in an essay with at most one label from our tar-
get argument ontology. Our rules were developed
using our intuition and informal examination of 9
essays from the corpus of 52. The algorithm con-
sists of the following ordered
4
rules:
Rule 1: If the sentence begins with a Compar-
ison discourse connective, or if the sentence con-
tains any string prefixes from {conflict, oppose}
and a four-digit number (intended as a year for a
citation), then tag with Opposes.
Rule 2: If the sentence begins with a Contin-
gency connective and does not contain a four-digit
number, then tag with Supports.
Rule 3: If the sentence contains a four-digit
number, then tag with Citation.
Rule 4: If the sentence contains string prefixes
from {suggest, evidence, shows, Essentially, indi-
cate} (case-sensitive), then tag with Claim.
Rule 5: If the sentence is in the first, second, or
last paragraph, and contains string prefixes from
{hypothes, predict}, or if the sentence contains the
word ?should? and contains no Contingency con-
nectives, and does not contain a four-digit number
and does not contain string prefixes from {conflict,
oppose}, then tag with Hypothesis.
Rule 6: If the previous sentence was tagged
with Hypothesis, and this sentence begins with an
Expansion connective and does not contain a four-
4
When multiple rules apply, the tag of the earliest is used.
25
# Essay Sentence Label Rule
1 The ultimate goal of this study is to investigate the relationship between
stop-sign violations and traffic activity.
Current
Study
7
2 To do this we analyzed two different variables on traffic activity: time of day
and location.
None 8
... ... ... ...
6 Stop-signs indicate that the driver must come to a complete stop before the
sign and check for oncoming and opposing traffic before[-Temporal] pro-
ceeding on.
Claim 4
7 For a stop to be considered complete the car must completely stop moving. None 8
... ... ... ...
16 The first hypothesis was: If[-Contingency] it is a high activity time of day at
an intersection then[-Contingency], there will be a higher ratio of complete
stops made than during a low activity time at the intersection.
Hypothesis 5
17 The second hypothesis was: If[-Contingency] there is a busy intersection
then[-Contingency], there will be a higher ratio of complete stops made than
at an intersection that is less busy.
Hypothesis 5
18 So[-Contingency] essentially, it was expected that when[-Temporal] there
was a higher traffic activity level, either due to location or time of day, there
were to be less stop-sign violations.
Supports 2
19 There have been many studies which indicate that people do drive differently
at different times of day and[-Expansion] that it does have an impact on
driving risk.
Claim 4
20 Reimer et al (2007) found that time of day did influence driving speed, reac-
tion time, and speed variability measures.
Citation 3
... ... ... ...
24 However[-Comparison], McGarva & Steiner (2000) oppose the second hy-
pothesis because[-Contingency] they found that provoked driver aggression
through honking horns, increased the rate of acceleration at a stop sign.
Opposes 1
... ... ... ...
Table 1: Essay sentences, their mined ontological labels, and rules used to determine the labels, for the
essay associated with Figure 1. Inferred discourse connective senses are italicized in square brackets.
digit number, then tag with Hypothesis.
Rule 7: If the sentence is in the first or last para-
graph and contains at least one word from {study,
research} and does not contain the words {past,
previous, prior} (first letter case-insensitive) and
does not contain string prefixes from {hypothes,
predict} and does not contain a four-digit number,
then tag with Current Study.
Rule 8: Do not assign a tag to the sentence.
Some sample output can be found on Table 1.
Note that sentence 24 could have been tagged as
Citation using Rule 3, but because it fits the crite-
ria for Rule 1, it is tagged as Opposes.
Ontology-Based Essay Scoring. We also devel-
oped a rule-based algorithm to score each essay in
the corpus. These rules were developed using our
intuition in conjunction with the examination of
the expert grading rubric. These rules take a la-
beled essay from the argument mining algorithm
and outputs a score in the continuous range [0,5]
using the following procedure:
5
1: Assign one point to essays that have at least
one sentence tagged with Current Study (CS).
2: Assign one point to essays that have at least
one sentence tagged with Hypothesis (H).
3: Assign one point to essays that have at least
one sentence tagged with Opposes (O).
4: Assign points based on the sum of the num-
ber of sentences tagged with Claim (Cl) and the
number of sentences tagged with Supports (S), all
divided by the number of paragraphs (#?). If this
5
Score 0 occurs when no labels are assigned to the essay.
26
value exceeds 1, assign only one point.
5: Assign points based on the number of sen-
tences tagged with Citation (Ci) divided by the
number of paragraphs (#?). If this value exceeds
1, assign only one point.
6: Sum all of the previously computed points.
For the three paragraph essay excerpted in Ta-
ble 1 (assigned expert score 3), there were three
sentences tagged with Current Study, three with
Hypothesis, one with Opposes, one with Sup-
ports, two with Claim and three with Citation.
The score is computed as follows:
1
CS
+ 1
H
+ 1
O
+
2
Cl
+ 1
S
3
#?
+
3
Ci
3
#?
= 5
4 Results
Since our essays do not have gold-standard on-
tology labels yet, we cannot intrinsically evaluate
the argument mining algorithm. We instead per-
formed an extrinsic evaluation via our use of the
mined argument labels for essay scoring.
The average automatic score for the corpus is
3.42 and the median is 3.5, while the correspond-
ing expert values are 3.03 and 3, respectively. A
paired t-test of the means has a significance of p <
0.01, suggesting that our algorithm over-scores the
essays. We also ran a one-sample t-test on each ex-
pert score value to see if the automatic scores were
similar to the expert scores. We hypothesized that
within each expert score category predicted accu-
rately, we should not see a significant difference (p
? 0.05). Table 2 shows that while the automatic
score is not significantly different for expert score
4, the scores are significantly different for scores 2
and 3.
We also examined the Spearman?s rank corre-
lation between the computed and expert scores.
6
We see that the Spearman?s rank correlation shows
significance of p < 0.0001 with a rho value of
0.997. Together these metrics suggest that our au-
tomated scores are currently useful for ranking but
not for rating.
5 Conclusion and Future Work
We have presented simple rule-based algorithms
for argumentation mining in student essays and
essay scoring using argument mining. Based on
preliminary extrinsic evaluation, our pattern-based
recognition of a basic argumentation ontology
6
A Pearson correlation did not give significant results.
expert avg. auto t n p
score score
1 4.33 ? 1 ?
2 3.23 3.21 8 0.013
3 3.30 2.10 31 0.044
4 3.80 -1.00 12 0.337
Table 2: One-sample t-test results for scores.
seems to provide some insight into essay scores
across two courses. While the automatic scores
did not necessarily reflect the expert scores, the
ranking correlation demonstrated that more argu-
mentative elements were related to higher scores.
Even with the limitations of this study (e.g. no in-
trinsic evaluation, a small essay corpus, a limited
argument ontology, a scoring algorithm using only
ontology features, application of discourse con-
nector for a different genre), our results suggest
the promise of using argument mining to trigger
feedback in a writing tutoring system.
To develop a more linguistically sophisticated
and accurate argument mining algorithm, our fu-
ture plans include exploiting discourse informa-
tion beyond connectives, e.g., by parsing our es-
says in terms of PDTB (Lin et al., 2011) or RST
relations (Feng and Hirst, 2012). We also plan to
look at the helpfulness of argumentation schemes
(Feng and Hirst, 2011), and other linguistic and
essay features for automatic evaluation (Crossley
and McNamara, 2010). In addition, our essays
are being annotated with diagram ontology labels,
which will enable us to use machine learning to
conduct intrinsic argument mining evaluations and
to learn the weights for each rule or determine new
rules. Finally, we plan to explore using the dia-
grams to bootstrap the essay annotation process.
While some sentences in an essay can easily be
mapped to the corresponding diagram (e.g. sen-
tence 1 in Table 1 to node 1 in Figure 1), the com-
plication is that essays tend to be more fleshed-out
than diagrams, and at least in our corpus, also con-
tain argument changes motivated by diagram peer-
review. While sentence 6 in Table 1 is correctly
tagged as a Claim, this content is not in Figure 1.
Acknowledgments
This work is supported by NSF Award 1122504.
We thank Huy Nguyen, Wenting Xiong, and
Michael Lipschultz.
27
References
[Burstein et al.2001] Jill Burstein, Karen Kukich, Su-
sanne Wolff, Ji Lu, and Martin Chodorow. 2001.
Enriching automated essay scoring using discourse
marking. ERIC Clearinghouse.
[Crossley and McNamara2010] Scott A Crossley and
Danielle S McNamara. 2010. Cohesion, coher-
ence, and expert evaluations of writing proficiency.
In Proceedings of the 32nd annual conference of the
Cognitive Science Society, pages 984?989. Austin,
TX: Cognitive ScienceSociety.
[Falakmassir et al.2013] Mohammad Falakmassir,
Kevin Ashley, and Christian Schunn. 2013. Using
argument diagramming to improve peer grading
of writing assignments. In Proceedings of the 1st
Workshop on Massive Open Online Courses at the
16th Annual Conference on Artificial Intelligence in
Education, Memphis, TN.
[Feng and Hirst2011] Vanessa Wei Feng and Graeme
Hirst. 2011. Classifying arguments by scheme. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 987?996. As-
sociation for Computational Linguistics.
[Feng and Hirst2012] Vanessa Wei Feng and Graeme
Hirst. 2012. Text-level discourse parsing with rich
linguistic features. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 60?68.
Association for Computational Linguistics.
[Lin et al.2011] Ziheng Lin, Hwee Tou Ng, and Min-
Yen Kan. 2011. Automatically evaluating text
coherence using discourse relations. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 997?1006. Associa-
tion for Computational Linguistics.
[Peldszus and Stede2013] Andreas Peldszus and Man-
fred Stede. 2013. From argument diagrams to argu-
mentation mining in texts: A survey. International
Journal of Cognitive Informatics and Natural Intel-
ligence (IJCINI), 7(1):1?31.
[Reed and Rowe2004] Chris Reed and Glenn Rowe.
2004. Araucaria: Software for argument analy-
sis, diagramming and representation. International
Journal on Artificial Intelligence Tools, 13(04):961?
979.
[Reed et al.2007] Chris Reed, Douglas Walton, and
Fabrizio Macagno. 2007. Argument diagramming
in logic, law and artificial intelligence. The Knowl-
edge Engineering Review, 22(01):87?109.
[Scheuer et al.2009] Oliver Scheuer, Bruce M.
McLaren, Frank Loll, and Niels Pinkwart. 2009.
An analysis and feedback infrastructure for argu-
mentation learning systems. In Proceedings of the
2009 Conference on Artificial Intelligence in Educa-
tion: Building Learning Systems That Care: From
Knowledge Representation to Affective Modelling,
pages 629?631, Amsterdam, The Netherlands, The
Netherlands. IOS Press.
[Scheuer et al.2010] Oliver Scheuer, Frank Loll, Niels
Pinkwart, and Bruce M. McLaren. 2010. Computer-
supported argumentation: A review of the state
of the art. International Journal of Computer-
Supported Collaborative Learning, 5(1):43?102.
[Torrance et al.2000] Mark Torrance, Glyn V. Thomas,
and Elizabeth J. Robinson. 2000. Individual differ-
ences in undergraduate essay-writing strategies: A
longitudinal study. Higher Education, 39(2):181?
200.
28
Proceedings of the SIGDIAL 2014 Conference, pages 181?185,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Evaluating a Spoken Dialogue System that Detects and Adapts to User
Affective States
Diane Litman
Computer Science Dept. & LRDC
University of Pittsburgh
Pittsburgh, PA 15260 USA
dlitman@pitt.edu
Kate Forbes-Riley
Learning Research & Development Center
University of Pittsburgh
Pittsburgh, PA 15260 USA
forbesk@pitt.edu
Abstract
We present an evaluation of a spoken
dialogue system that detects and adapts
to user disengagement and uncertainty in
real-time. We compare this version of our
system to a version that adapts to only user
disengagement, and to a version that ig-
nores user disengagement and uncertainty
entirely. We find a significant increase in
task success when comparing both affect-
adaptive versions of our system to our non-
adaptive baseline, but only for male users.
1 Introduction
There is increasing interest in building dialogue
systems that can detect and adapt to user affec-
tive states.
1
However, while this line of research is
promising, there is still much work to be done. For
example, most research has focused on detecting
user affective states, rather than on developing di-
alogue strategies that adapt to such states once de-
tected. In addition, when affect-adaptive dialogue
systems have been developed, most systems detect
and adapt to only a single user state, and typically
assume that the same affect-adaptive strategy will
be equally effective for all users.
In this paper we take a step towards examin-
ing these issues, by presenting an evaluation of
three versions of an affect-adaptive spoken tuto-
rial dialogue system: one that detects and adapts
to both user disengagement and uncertainty, one
that adapts to only disengagement, and one that
doesn?t adapt to affect at all. Our evaluation exam-
ines the impact of adapting to differing numbers of
affective states on task success, and also examines
interactions with user gender. We target disen-
gagement and uncertainty because these were the
1
We use the term affect to describe emotions and attitudes
that impact how people communicate. Other researchers also
combine concepts of emotion, arousal, and attitudes where
emotion is not full-blown (Cowie and Cornelius, 2003).
most frequent affective states in prior studies with
our system and their presence was negatively cor-
related with task success
2
(Forbes-Riley and Lit-
man, 2011; Forbes-Riley and Litman, 2012). The
detection of these and similar states is also of in-
terest to the larger speech and language processing
communities, e.g. (Wang and Hirschberg, 2011;
Bohus and Horvitz, 2009; Pon-Barry and Shieber,
2011). Our results suggest that while adapting
to affect increases task success compared to not
adapting at all, the utility of our current methods
varies with user gender. Also, we find no differ-
ence between adapting to one or two states.
2 Related Work
2.1 Adapting to Multiple Affective States
While prior research has shown that users display
a range of affective states during spoken dialogue
(e.g. (Schuller et al., 2009)), only a few dialogue
systems have been developed that can adapt to
more than one user affective state (e.g., (D?Mello
et al., 2010; Acosta and Ward, 2011)). Further-
more, prior evaluations have compared adapting
to at least one affective state to not adapting to af-
fect at all, but have not examined the benefits of
adapting to one versus multiple affective states.
In a first evaluation comparing singly and mul-
tiply affect-adaptive dialogue systems, we com-
pared an existing system that adapted to uncer-
tainty to a new version that also adapted to disen-
gagement (Forbes-Riley and Litman, 2012). The
multiply-adaptive system increased motivation for
users with high disengagement, and reduced both
uncertainty and the likelihood of continued dis-
engagement. However, this evaluation was only
conducted in a ?Wizard-of-Oz? scenario, where a
hidden human replaced the speech recognition, se-
mantic analysis, and affect detection components
of our dialogue system. We also conducted a post-
2
Our success measure is learning gain (Section 4).
181
hoc correlational (rather than causal) study, using
data from an earlier fully-automated version of the
uncertainty-adaptive system. Regressions demon-
strated that using both automatically labeled dis-
engagement and uncertainty to predict task suc-
cess significantly outperformed using only disen-
gagement (Forbes-Riley et al., 2012). However, if
manual labels were instead used, only disengage-
ment was predictive of learning, and adding un-
certainty didn?t help. This suggests that detecting
multiple affective states might compensate for the
noise that is introduced in a fully-automated sys-
tem. In this paper we further investigate this hy-
pothesis, by evaluating the utility of adapting to
zero, one, or two affective states in a controlled
experiment involving fully-automated systems.
2.2 Gender Effects in Dialogue
Differences in dialogue structure have been found
between male and female students talking to a hu-
man tutor (Boyer et al., 2007). Studies have also
shown gender differences in conversational en-
trainment patterns, for acoustic-prosodic features
in human-human dialogues (Levitan et al., 2012)
and articles in movie conversations (Danescu-
Niculescu-Mizil and Lee, 2011). For dialogue sys-
tems involving embodied conversational agents,
gender effects have been found for facial dis-
plays, with females preferring more expressive
agents (Foster and Oberlander, 2006). When used
for tutoring, females report more positive affect
when a learning companion is used, while males
are more negative (Woolf et al., 2010).
In our own prior work, we compared two
uncertainty-adaptive and one non-adaptive ver-
sions of a wizarded dialogue system. Our results
demonstrated that only one method of adapting to
user uncertainty increased task success, and only
for female users (Forbes-Riley and Litman, 2009).
In this paper we extend this line of research, by
adding an affective dialogue system that adapts
to two rather than just one user state to our eval-
uation, and by moving from wizarded to fully-
automated systems.
3 System, Experiment and Corpus
Our corpus consists of dialogues between
users and three different versions of ITSPOKE
(Intelligent Tutoring SPOKEn dialog sys-
tem) (Forbes-Riley and Litman, 2011; Forbes-
Riley and Litman, 2012). ITSPOKE is a
speech-enhanced and otherwise modified version
of the Why2-Atlas text-based qualitative physics
tutor (VanLehn et al., 2002) that interacts with
users using a system initiative dialogue strategy.
User speech is first digitized from head-mounted
microphone input and sent to the PocketSphinx
recognizer.
3
The recognition output is then clas-
sified as (in)correct with respect to the anticipated
physics content via semantic analysis (Jordan
et al., 2007). Simultaneously, user uncertainty
(UNC) and disengagement (DISE) are classified
from prosodic, lexical and contextual features
using two binary classification models (Forbes-
Riley et al., 2012). All statistical components of
the speech recognizer, the semantic analyzer, and
the uncertainty and disengagement detectors were
trained using prior ITSPOKE corpora.
4
Finally,
ITSPOKE?s response is determined based on the
answer?s automatically labeled (in)correctness,
(un)certainty, and (dis)engagement and then sent
to the Cepstral text-to-speech system,
5
as well as
displayed on a web-based interface.
Our corpus was collected in an experiment con-
sisting of three conditions (CONTROL, DISE,
DISE+UNC), where ITSPOKE used a different
method of affect-adaptation in each condition.
The experiment was designed to compare the ef-
fectiveness of not adapting to user affect in IT-
SPOKE (CONTROL), adapting to user disengage-
ment (DISE), and adapting to user disengagement
as well as user uncertainty (DISE+UNC).
6
In CONTROL, ITSPOKE?s responses to user
utterances were based on only the correctness of
user answers. This version of the system thus ig-
nored any automatically detected user disengage-
ment or uncertainty. In particular, after each cor-
rect answer, ITSPOKE provided positive feedback
then moved on to the next topic. After incor-
rect answers, ITSPOKE instead provided negative
3
http://www.speech.cs.cmu.edu/pocketsphinx
4
We have not yet performed the manual annotations
needed to evaluate our current versions of these components
in isolation. However, earlier versions of our affect detec-
tors yielded FMeasures of 69% and 68% for disengagement
and uncertainty, respectively, on par with the best perform-
ing affect detectors in the wider literature (Forbes-Riley and
Litman, 2011; Forbes-Riley et al., 2012).
5
http://www.cepstral.com
6
We did not include an uncertainty-only condition (UNC)
because in previous work we compared UNC versus CON-
TROL (Forbes-Riley and Litman, 2011) and DISE+UNC
versus UNC (Forbes-Riley and Litman, 2012). Further de-
tails and motivation for all experimental conditions can be
found in the description of our earlier Wizard-of-Oz experi-
ment (Forbes-Riley and Litman, 2012).
182
feedback, then provided remediation tutoring be-
fore moving on to the next topic.
In DISE, two adaptive responses were devel-
oped to allow ITSPOKE?s responses to consider
user disengagement as well as the correctness of
the user?s answer;
7
however, this system version
still ignored user uncertainty. In particular, af-
ter each disengaged+correct answer, ITSPOKE
provided correctness feedback, a progress chart
showing user correctness on prior problems and
the current problem, and a brief re-engagement
tip. After each disengaged+incorrect answer, IT-
SPOKE provided incorrectness feedback, a brief
re-engagement tip, and an easier supplemental ex-
ercise, which consisted of an easy fill-in-the-blank
type question to reengage the user, followed by re-
mediation targeting the material on which the user
disengaged and answered incorrectly. Examples
of both types of adaptive responses are shown in
A.1 and A.2 of Appendix A, respectively.
In DISE+UNC, ITSPOKE responded to dis-
engagement as just described, but also adapted
to uncertainty. In particular, after each uncer-
tain+correct answer, ITSPOKE provided positive
correctness feedback, but then added the remedia-
tion designed for incorrect answers with the goal
of reducing the user?s uncertainty. A dialogue ex-
cerpt illustrating this strategy is shown in A.3 of
Appendix A. Note that when a single utterance is
predicted to be both disengaged and uncertain, the
DISE and UNC adaptations are combined.
Finally, our experimental procedure was as fol-
lows. College students who were native English
speakers and who had no college-level physics
read a short physics text, took a pretest, worked
5 physics problems (one problem per dialogue)
with the version of ITSPOKE from their experi-
mental condition, and took a posttest isomorphic
to the pretest. The pretest and posttest were taken
from our Wizard-of-Oz experiment and each con-
tained 26 multiple choice physics questions. Our
experiment yielded a corpus of 335 dialogues (5
per user) from 67 users (39 female and 28 male).
Average pretest
8
and posttest scores were 50.4%
and 74.7% (out of 100%), respectively.
4 Performance Analysis
Based on the prior research discussed in Section 2,
we had two experimental hypotheses:
7
Engaged answers were treated as in CONTROL.
8
Pretest did not differ across conditions (p = .92).
Condition Learning Gain N
Mean (%) Std Err
DISE+UNC 53.2 5.0 23
DISE 51.4 4.8 22
CONTROL 46.6 4.7 22
Gender Learning Gain N
Male 53.2 4.3 28
Female 47.6 3.6 39
Table 1: No effect of experimental condition
(p=.62) or gender (p=.32) on learning gain.
Gender Condition Learning Gain N
Mn (%) Std Err
Male DISE+UNC 58.8 8.4 7
DISE 62.2 7.0 10
CONTROL 38.7 6.7 11
Female DISE+UNC 47.5 5.6 16
DISE 40.6 6.4 12
CONTROL 54.6 6.7 11
Table 2: Significant interaction between the ef-
fects of gender and condition on learning (p=.02).
H1: Responding to multiple affective states will
yield greater task success than responding to only
a single state (DISE+UNC > DISE), which in
turn will outperform not responding to affect at all
(DISE > CONTROL).
H2: The effects of ITSPOKE?s affect-
adaptation method and of gender will interact.
A two-way analysis of variance (ANOVA) was
thus conducted to examine the effect of ex-
perimental condition (DISE+UNC, DISE, CON-
TROL) and user gender (Male, Female) on task
success. As is typical in the tutoring domain, task
success was computed as (normalized) learning
gain:
posttest?pretest
100?pretest
.
Table 1 shows that although our results pat-
terned as hypothesized when considering all users,
the differences in learning gains were not statisti-
cally different across experimental conditions, F
(2, 61) = .487, p = .617. There were also no main
effects of gender, F (1, 61) = 1.014, p = .318.
In contrast, as shown in Table 2, there was a
statistically significant interaction between the ef-
fects of user gender and experimental condition on
learning gains, F (2, 61) = 4.141, p = .021. We
thus tested the simple effects of condition within
each level of gender to yield further insights.
For males, simple main effects analysis showed
183
that there were statistically significant differences
in learning gains between experimental conditions
(p = .042). In particular, males in the DISE con-
dition had significantly higher learning gains than
males in the CONTROL condition (p = .019).
Males in the DISE+UNC condition also showed
a trend for higher learning gains than males in the
CONTROL condition (p = .066). However, males
in the DISE and DISE+UNC conditions showed
no difference in learning gains (p= .760).
For females, in contrast, simple main effects
analysis showed no statistically significant differ-
ences in learning gains between any experimental
conditions (p = .327).
In sum, hypothesis H1 regarding the utility of
affect adaptations was only partially supported by
our results, where (DISE+UNC = DISE) > CON-
TROL, and only for males. That is, adapting to
affect was indeed better than not adapting at all,
but only for males (supporting hypothesis H2).
Contrary to H1, adapting to uncertainty over and
above disengagement did not provide any bene-
fit compared to adapting to disengagement alone
(DISE+UNC = DISE), for both genders.
5 Discussion and Future Directions
Our results contribute to the increasing body
of literature demonstrating the utility of adding
fully-automated affect-adaptation to existing spo-
ken dialogue systems. In particular, males in
our two affect-adaptive conditions (DISE+UNC
and DISE) learned more than males in the
non-adaptive CONTROL. While our prior work
demonstrated the benefits of adapting to uncer-
tainty, the current results demonstrate the impor-
tance of adapting to disengagement either alone
or in conjunction with uncertainty. However, we
also predicted that DISE+UNC should outperform
DISE, which was not the case. In future work we
will examine other performance measures besides
learning, and will manually annotate true disen-
gagement and uncertainty in order to group stu-
dents by amount of disengagement. Furthermore,
since the motivating prior studies discussed in Sec-
tion 2 were based on older versions of our system,
annotation could identify problematic differences
in training and testing data. A final potential is-
sue is that the re-engagement tips do not convey
exactly the same information.
Second, our results contribute to the literature
suggesting that gender effects should be consid-
ered when designing dialogue systems. We see
similar results as in our prior work; namely our
current results continue to suggest that males don?t
benefit from adapting to their uncertainty as com-
pared to ignoring it, but our current results also
suggest that males do benefit from adapting to
their disengagement. On the other hand, our cur-
rent results suggest that females do not benefit
from our disengagement adaptation and moreover,
combining it with our uncertainty adaptation re-
duces the benefit of the uncertainty adaptation for
them. This suggests the possibility of a differ-
ing affective hierarchy, in terms of how affective
states may impact the learning process of the two
genders differently. Our results yield an empirical
basis for future investigations into whether adap-
tive system performance can improve by adapting
to affect differently based on gender. However,
further research is needed to determine more ef-
fective combinations of disengagement and uncer-
tainty adaptations for both males and females, and
to investigate whether gender differences might be
related to other types of measurable user factors.
Acknowledgments
This work is funded by NSF 0914615. We thank S.
Silliman for experimental support, and H. Nguyen,
W. Xiong, and the reviewers for feedback.
References
J. C. Acosta and N. G. Ward. 2011. Achieving rapport
with turn-by-turn, user-responsive emotional color-
ing. Speech Communication, 53(9-10):1137?1148.
D. Bohus and E. Horvitz. 2009. Models for multiparty
engagement in open-world dialog. In Proceedings
of SIGdial, pages 225?234, London, UK.
K. Boyer, M. Vouk, and J. Lester. 2007. The influ-
ence of learner characteristics on task-oriented tuto-
rial dialogue. Frontiers in Artificial Intelligence and
Applications, 158:365.
R. Cowie and R. R. Cornelius. 2003. Describing
the emotional states that are expressed in speech.
Speech Communication, 40(1-2):5?32.
C. Danescu-Niculescu-Mizil and L. Lee. 2011.
Chameleons in imagined conversations: A new ap-
proach to understanding coordination of linguistic
style in dialogs. In Proceedings of the 2nd Workshop
on Cognitive Modeling and Computational Linguis-
tics, pages 76?87.
S. D?Mello, B. Lehman, J. Sullins, R. Daigle,
R. Combs, K. Vogt, L. Perkins, and A. Graesser.
184
2010. A time for emoting: When affect-sensitivity
is and isn?t effective at promoting deep learning. In
Intelligent Tutoring Systems Conference, pages 245?
254, Pittsburgh, PA, USA.
K. Forbes-Riley and D. Litman. 2009. A user
modeling-based performance analysis of a wizarded
uncertainty-adaptive dialogue system corpus. In
Proceedings Interspeech, pages 2467?2470.
K. Forbes-Riley and D. Litman. 2011. Benefits
and challenges of real-time uncertainty detection
and adaptation in a spoken dialogue computer tutor.
Speech Communication, 53(9?10):1115?1136.
K. Forbes-Riley and D. Litman. 2012. Adapting to
multiple affective states in spoken dialogue. In Pro-
ceedings of the 13th Annual Meeting of the Special
Interest Group on Discourse and Dialogue, SIG-
DIAL ?12, pages 217?226.
K. Forbes-Riley, D. Litman, H. Friedberg, and J. Drum-
mond. 2012. Intrinsic and extrinsic evaluation of
an automatic user disengagement detector for an
uncertainty-adaptive spoken dialogue system. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 91?102.
M. Foster and J. Oberlander. 2006. Data-driven gen-
eration of emphatic facial displays. Proceedings of
EACL, 2006:353?363.
P. Jordan, B. Hall, M. Ringenberg, Y. Cui, and C.P.
Rose. 2007. Tools for authoring a dialogue agent
that participates in learning studies. In Proc. Artifi-
cial Intelligence in Education (AIED), pages 43?50.
R. Levitan, A. Gravano, L. Willson, S. Benus,
J. Hirschberg, and A. Nenkova. 2012. Acoustic-
prosodic entrainment and social behavior. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 11?19.
H. Pon-Barry and S. Shieber. 2011. Recognizing un-
certainty in speech. EURASIP Journal on Advances
in Signal Processing.
B. Schuller, S. Steidl, and A. Batliner. 2009. The In-
terspeech 2009 emotion challenge. In Proc. Inter-
speech, pages 312?315, ISCA, Brighton, UK.
K. VanLehn, P. W. Jordan, C. Ros?e, D. Bhembe,
M. B?ottner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intl. Conf. on Intelligent Tutoring
Systems.
W. Wang and J. Hirschberg. 2011. Detecting levels of
interest from spoken dialog with multistream predic-
tion feedback and similarity based hierarchical fu-
sion learning. In Proc. SIGDIAL, pages 152?161,
Portland, Oregon.
B. Woolf, I. Arroyo, D. Cooper, W. Burleson, and
K. Muldner. 2010. Affective tutors: Automatic de-
tection of and response to student emotion. In Ad-
vances in Intelligent Tutoring Systems, pages 207?
227. Springer.
Appendix A. Affect-Adaptive Examples.
A.1 After a Predicted Disengaged+Correct
Turn
ITSPOKE: How does his velocity compare to
that of his keys?
USER: same same same
ITSPOKE: Right. [Feedback] Here?s a quick
progress update. [See Screenshot] It might help
to remember we will build on the topics we?re
discussing now. [Re-engagement Tip] Now let?s
see what happens after the man releases his keys.
[Next Topic]
A.2 After Predicting Disengaged+Incorrect
ITSPOKE: How does his velocity compare to
that of his keys?
USER: vertical
ITSPOKE: That doesn?t sound right. [Feed-
back] Let?s think about this a little more. [Re-
engagement Tip] Since the man is holding his
keys, they aren?t moving relative to each other.
So their velocities must be WHAT? [Supplemen-
tal Exercise].
A.3 After a Predicted Uncertain+Correct Turn
ITSPOKE: What are the forces exerted on the
man after he releases his keys?
USER: gravity???
ITSPOKE: Right. [Feedback] Gravity is the
only force acting on the man. This force is also
called the weight of the man. It?s always present
when an object is in earth?s gravitational field.
[Remediation]. So what?s the direction of the
force of gravity on the man ? [Next Topic]
185
