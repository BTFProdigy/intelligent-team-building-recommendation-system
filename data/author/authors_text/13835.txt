Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 53?61,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Reinforcement Learning to Create Communication Channel
Management Strategies for Diverse Users
Rebecca Lunsford
Center for Spoken Lang. Understanding
Oregon Health & Science University
Beaverton, OR, USA
lunsforr@ohsu.edu
Peter Heeman
Center for Spoken Lang. Understanding
Oregon Health & Science University
Beaverton, OR, USA
heemanp@ohsu.edu
Abstract
Spoken dialogue systems typically do not
manage the communication channel, instead
using fixed values for such features as the
amplitude and speaking rate. Yet, the qual-
ity of a dialogue can be compromised if the
user has difficulty understanding the system.
In this proof-of-concept research, we explore
using reinforcement learning (RL) to create
policies that manage the communication chan-
nel to meet the needs of diverse users. To-
wards this end, we first formalize a prelimi-
nary communication channel model, in which
users provide explicit feedback regarding is-
sues with the communication channel, and the
system implicitly alters its amplitude to ac-
commodate the user?s optimal volume. Sec-
ond, we explore whether RL is an appropri-
ate tool for creating communication channel
management strategies, comparing two differ-
ent hand-crafted policies to policies trained
using both a dialogue-length and a novel an-
noyance cost. The learned policies performed
better than hand-crafted policies, with those
trained using the annoyance cost learning an
equitable tradeoff between users with differ-
ing needs and also learning to balance finding
a user?s optimal amplitude against dialogue-
length. These results suggest that RL can be
used to create effective communication chan-
nel management policies for diverse users.
Index Terms: communication channel, spoken di-
alogue systems, reinforcement learning, amplitude,
diverse users
1 Introduction
Both Spoken Dialog Systems (SDS) and Assistive
Technology (AT) tend to have a narrow focus, sup-
porting only a subset of the population. SDS typ-
ically aim to support the ?average man?, ignoring
wide variations in potential users? ability to hear and
understand the system. AT aims to support peo-
ple with a recognized disability, but doesn?t sup-
port those whose impairment is not severe enough
to warrant the available devices or services, or those
who are unaware or have not acknowledged that they
need assistance. However, SDS should be able to
meet the needs of users whose abilities fall within,
and between, the extremes of severly impaired and
perfectly abled.
When aiming to support users with widely differ-
ing abilities, the cause of a user?s difficulty is less
important than adapting the communication channel
in a manner that aids understanding. For example,
speech that is presented more loudly and slowly can
help a hearing-impaired elderly person understand
the system, and can also help a person with no hear-
ing loss who is driving in a noisy car. Although one
user?s difficulty is due to impairment and the other
due to an adverse environment, a similar adaptation
may be appropriate to both.
During human-human communication, speakers
manage the communication channel; implicitly al-
tering their manner of speech to increase the likeli-
hood of being understood while concurrently econo-
mizing effort (Lindblom, 1990). In addition to these
implicit actions, speakers also make statements re-
ferring to breakdowns in the communication chan-
53
nel, explicitly pointing out potential problems or
corrections, (e.g. ?Could you please speak up??)
(Jurafsky et al, 1997).
As for human-computer dialogue, SDS are prone
to misrecognition of users? spoken utterances. Much
research has focused on developing techniques for
overcoming or avoiding system misunderstandings.
Yet, as the quality of automatic speech recognition
improves and SDS are deployed to diverse popula-
tions and in varied environments, systems will need
to better attend to possible human misunderstand-
ings. Future SDS will need to manage the commu-
nication channel, in addition to managing the task,
to aid in avoiding these misunderstandings.
Researchers have explored the use of reinforce-
ment learning (RL) to create dialogue policies that
balance and optimize measures of task success (e.g.,
see (Scheffler and Young, 2002; Levin et al, 2000;
Henderson et al, 2008; Walker, 2000)). Along these
lines, RL is potentially well suited to creating poli-
cies for the subtask of managing the communica-
tion channel, as it can learn to adapt to the user
while continuing the dialogue. In doing so, RL may
choose actions that appear costly at the time, but lead
to better overall dialogues.
Our long term goal is to learn how to manage the
communication channel along with the task, moving
away from just ?what? to say and also focusing on
?how? to say it. For this proof-of-concept, our goals
are twofold: 1) to formalize a communication chan-
nel model that encompasses diverse users, initially
focusing just on explicit user actions and implicit
system actions, and 2) to determine whether RL is
an appropriate tool for learning an effective commu-
nication channel management strategy for diverse
users. To explore the above issues, we use a simple
communication channel model in which the system
needs to determine and maintain an amplitude level
that is pleasant and effective for users with differ-
ing amplitude preferences and needs. As our goal
includes decreasing the amount of potentially an-
noying utterances (i.e., those in which the system?s
amplitude setting is in discord with the user?s op-
timal amplitude), we introduce a user-centric cost
metric, which we have termed annoyance cost. We
then compare hand-crafted policies against policies
trained using both annoyance and more traditional
dialogue-length cost components.
2 Related Work
2.1 How People Manage the Channel
When conversing, speakers implicitly adjust fea-
tures of their speech (e.g., speaking rate, loudness)
to maintain the communication channel. For ex-
ample, speakers produce Lombard speech when in
noisy conditions, produce clear speech to better ac-
commodate a hard of hearing listener, and alter their
speech to more closely resemble the interlocutor?s
(Junqua, 1993; Lindblom, 1990). These changes in-
crease the intelligibility of the speech, thus helping
to maintain the communication channel (Payton et
al., 1994). Research has also shown that speakers
adjust their speaking style when addressing a com-
puter; exhibiting the same speech adaptations seen
during human-human communication (Bell et al,
2003; Lunsford et al, 2006).
In addition to altering their speech implicitly,
speakers also explicitly point out communication
channel problems (Jurafsky et al, 1997). Exam-
ples include; requesting a change in speaking rate or
amplitude (?Could you please speak up??), explain-
ing sources of communication channel interference
(?Oh, that noise is the coffee grinder.?), or asking
their interlocutor to repeat an utterance (?What was
that??). These explicit utterances identify some is-
sue with the communication channel that must be
remedied before continuing the dialogue. In re-
sponse, interlocutors will rewind to a previous point
in the dialogue and alter their speech to ensure they
are understood. This approach, of adapting ones
speech in response to a communication problem, oc-
curs even when conversing with a computer (Stent et
al., 2008).
Both implicit speech alterations and explicit ut-
terances regarding the communication channel of-
ten address issues of amplitude. This is to be
expected, as speaking at an appropriate amplitude
is critical to maintaining an effective communica-
tion channel, with sub-optimal amplitude affecting
listeners? understanding and performance (Baldwin
and Struckman-Johnson, 2002). In addition, Bald-
win (2001) found that audible, but lowered, ampli-
tude can negatively affect both younger and older
subjects? reaction time and ability to respond cor-
rectly while multitasking, and that elderly listeners
are likely to need higher amplitudes than younger
54
listeners to maintain similar performance. Just as
low amplitude can be difficult to understand, high
amplitude can be annoying, and, in the extreme,
cause pain.
2.2 How Systems Manage the Channel
Towards improving listener understanding in a po-
tentially noisy environment, Martinson and Brock
(2007) take advantage of the mobility and sensory
capabilities of a robot. To determine the best course
of action, the robot maintains a noise map of the en-
vironment, measuring the environmental noise prior
to each TTS utterance. The robot then rotates to-
ward the listener, changes location, alters its am-
plitude, or pauses until the noise abates. A similar
technique, useful for remote listeners who may be
in a noisy environment or using a noisy communica-
tion medium, could analyze the signal-to-noise ratio
to ascertain the noise level in the listener?s environ-
ment. Although these techniques may be useful for
adjusting amplitude to compensate for noise in the
listener?s environment, they do not address speech
alterations needed to accommodate users with dif-
ferent hearing abilities or preferences.
Given the need to adapt to individual users, it
seems reasonable that users themselves would sim-
ply adjust volume on their local device. However,
there are issues with this approach. First, man-
ual adjustment of the volume would prove problem-
atic when the user?s hands and eyes are busy, such
as when driving a car. Second, during an ongo-
ing dialogue speakers tend to minimize pauses, re-
sponding quickly when given the turn (Sacks et al,
1974). Stopping to alter the amplitude could re-
sult in longer than natural pauses, which systems
often respond to with increasingly lengthy ?time-
out? responses (Kotelly, 2003), or repeating the same
prompt endlessly (Villing et al, 2008). Third, al-
though we focus on amplitude adaptations in this
paper, amplitude is only one aspect of the commu-
nication channel. A fully functional communication
channel management solution would also incorpo-
rate adaptations of features such as speaking rate,
pausing, pitch range, emphasis, etc. This extended
set of features, because of their number and interac-
tion between them, do not readily lend themselves
to listener manipulation.
3 Reinforcement Learning
RL has been used to create dialogue strategies that
specify what action to perform in each possible
system state so that a minimum dialogue cost is
achieved (Walker, 2000; Levin et al, 2000). To ac-
complish this, RL starts with a policy, namely what
action to perform in each state. It then uses this pol-
icy, with some exploration, to estimate the cost of
getting from each state with each possible action to
the final state. As more simulations are run, RL re-
fines its estimates and its current policy. RL will
converge to an optimal solution as long as assump-
tions about costs and state transitions are met. RL is
particularly well suited for learning dialogue strate-
gies as it will balance opposing goals (e.g., minimiz-
ing excessive confirmations vs. ensuring accurate
information).
RL has been applied to a number of dialogue
scenarios. For form-filling dialogues, in which the
user provides parameters for a database query, re-
searchers have used RL to decide what order to use
when prompting for the parameters and to decrease
resource costs such as database access (Levin et al,
2000; Scheffler and Young, 2002). System misun-
derstanding caused by speech recognition errors has
also been modeled to determine whether, and how,
the system should confirm information (Scheffler
and Young, 2002). However, there is no known work
on using RL to manage the communication channel
so as to avoid user misunderstanding.
User Simulation: To train a dialogue strategy us-
ing RL, some method must be chosen to emulate
realistic user responses to system actions. Training
with actual users is generally considered untenable
since RL can require millions of runs. As such, re-
searchers create simulated users that mimic the re-
sponses of real users. The approach employed to
create these users varies between researchers; rang-
ing from simulations that employ only real user data
(Henderson et al, 2008), to those that model users
with probabilistic simulations based on known re-
alistic user behaviors (Levin et al, 2000). Ai et
al. suggest that less realistic user simulations that al-
low RL to explore more of the dialogue state space
may perform as well or better than simulations that
statistically recreate realistic user behavior (Ai et al,
2007). For this proof-of-concept work, we employ a
55
hand-crafted user simulation that allows full explo-
ration of the state space.
Costs: Although it is agreed that RL is a viable
approach to creating optimal dialogue policies, there
remains much debate as to what cost functions result
in the most useful policies. Typically, these costs in-
clude a measure of efficiency (e.g., number of turns)
and a measure of solution quality (e.g., the user suc-
cessfully completed the transaction) (Scheffler and
Young, 2002; Levin et al, 2000). For manag-
ing the communication channel, it is unclear how
the cost function should be structured. In this work
we compare two cost components, a more traditional
dialogue-length cost versus a novel annoyance cost,
to determine which best supports the creation of use-
ful policies.
4 Communication Channel Model
Based on the literature reviewed in Section 2.1, we
devised a preliminary model that captures essential
elements of how users manage the communication
channel. For now, we only include explicit user ac-
tions, in which users directly address issues with
the communication channel, as noted by Jurafsky
et al (1997). In addition, the users modeled are
both consistent and amenable; they provide feed-
back every time the system?s utterances are too loud
or too soft, and abandon the interaction only when
the system persists in presenting utterances outside
the user?s tolerance (either ten utterances that are too
loud or ten that are too soft).
For this work, we wish to create policies that treat
all users equitably. That is, we do not want to train
polices that give preferential treatment to a subset of
users simply because they are more common. To ac-
complish this, we use a flat rather than normal distri-
bution of users within the simulation, with both the
optimal amplitude and the tolerance range randomly
generated for each user. To represent users with dif-
fering amplitude needs, simulated users are modeled
to have an optimal amplitude between 2 and 8, and
a tolerance range of 1, 3 or 5. For example, a user
may have a optimal amplitude of 4, but be able to
tolerate an amplitude between 2 and 6.
When interacting with the computer, the user re-
sponds with: (a) the answer to the system?s query if
the amplitude is within their tolerance range; (b) too
soft (TS) if below their range; or (c) too loud (TL)
if the amplitude is above their tolerance range. As
a simplifying assumption, TS and TL represent any
user responses that address communication channel
issues related to amplitude. For example, the user
response ?Pardon me?? would be represented by TS
and ?There?s no need to shout!? by TL. With this
user model, the user only responds to the domain
task when the system employs an amplitude setting
within the user?s tolerance range.
For the system, we need to ensure that the sys-
tem?s amplitude range can accommodate any user-
tolerable amplitude. For this reason, the system?s
amplitude can vary between 0 and 10, and is ini-
tially set to 5 prior to each dialogue. In addition to
performing domain actions, the system specifies the
amount the amplitude should change: -2, -1, +0, +1,
+2. Each system communication to the user consists
of both a domain action and the system?s amplitude
change. Thus, the system manages the communica-
tion channel using only implicit actions. If the user
responds with TS or TL, the system will then restate
what it just said, perhaps altering the amplitude prior
to re-addressing the user.
5 Hand-crafted Policies
To help in determining whether RL is an appropriate
tool for learning communication channel manage-
ment strategies, we designed two hand-crafted poli-
cies for comparison. The first handcrafted policy,
termed no-complaints, finds a tolerable amplitude
as quickly as possible, then holds that amplitude for
the remainder of the dialogue. As such, this policy
only changes the amplitude in response to explicit
complaints from the user. Specifically, the policy in-
creases the amplitude by 2 after a TS response, and
drops it by 2 after a TL. If altering the amplitude by
2 would cause the system to return to a setting al-
ready identified as too soft or too loud, the system
uses an amplitude change of 1.
The second policy, termed find-optimal, searches
for the user?s optimal amplitude, then maintains that
amplitude for the remainder of the dialogue. For
this policy, the system first increases the amplitude
by 1 until the user responds with TL (potentially in
response to the system?s first utterance), then de-
creases the amplitude by 1 until the user either re-
56
sponds with TS or the optimal amplitude is clearly
identified based on the previous feedback. An am-
plitude change of 2 is used only when both the op-
timal amplitude is obvious and a change of 2 will
bring the amplitude setting to the optimal ampli-
tude.
6 RL and System Encoding
To learn communication channel management poli-
cies we use RL with system and user actions spec-
ified using Information State Update rules (Hender-
son et al, 2008). Following Heeman (2007), we en-
code commonsense preconditions rather than trying
to learn them, and only use a subset of the informa-
tion state for RL.
Domain Task: We use a domain task that requires
the user to supply 9 pieces of information, excluding
user feedback relating to the communication chan-
nel. The system has a deterministic way of selecting
its actions, thus no learning is needed for the domain
task.
State Variables: For RL, each state is represented
by two variables; AmpHistory and Progress. Am-
pHistory models the user by tracking all previ-
ous user feedback. In addition, it tracks the cur-
rent amplitude setting. The string contains one
slot for each potential amplitude setting (0 through
10), with the current setting contained within ?[]?.
Thus, at the beginning of each interaction, the string
is ?-----[-]-----?, where ?-? represents no
known data. Each time the user responds, the string
is updated to reflect which amplitude settings are too
soft (?<?), too loud (?>?), or within the user?s toler-
ance (?O?). When the user responds with TL/TS,
the system also updates all settings above/below the
current setting. The Progress variable is required
to satisfy the Markov property needed for RL. This
variable counts the number of successful informa-
tion exchanges (i.e., the user did not respond with
TS or TL). As the domain task requires 9 pieces of
information, the Progress variable ranged from 1 to
9.
Costs: Our user model only allows up to 10 utter-
ances that are too soft or too loud. If the cutoff is
reached, the domain task has not been completed, so
a solution quality cost of 100 is incurred. Cutting
off dialogues in this way has the additional benefit
of preventing a policy from looping forever during
testing. During training, to allow the system to bet-
ter model the cost of choosing the same action re-
peatedly, we use a longer cutoff of 1000 utterances
rather than 10.
In addition to solution quality, two different cost
components are utilized. The first, a dialogue-length
cost (DC), assigns a cost of 1 for each user utterance.
The second, an annoyance cost (AC), assigns a cost
calculated as the difference between the system?s
amplitude setting and the user?s optimal amplitude.
This difference is multiplied by 3 when the sys-
tem?s amplitude setting is below the user?s optimal.
This multiplier was chosen based on research that
demonstrated increased response times and errors
during cognitively challenging tasks when speech
was presented below, rather than above, typical con-
versational levels (Baldwin and Struckman-Johnson,
2002). Thus, only utterances at the optimal ampli-
tude have no cost.
7 Results
With the above system and user models, we trained
policies using the two cost functions discussed
above, eight with the DC component and eight us-
ing the AC component. All used Q-Learning and
the ?-greedy method to explore the state space with
? set at 20% (Sutton and Barto, 1998). Dialogue runs
were grouped into epochs of 100; after each epoch,
the current dialogue policy was updated. We trained
each policy for 60,000 epochs. After certain epochs,
we tested the policy on 5000 user tasks.
For our simple domain, the solution quality cost
remained 0 after about the 100th epoch, as all poli-
cies learned to avoid user abandonment. Because of
this, only the dialogue-length cost(DC) and annoy-
ance cost(AC) components are reflected in the fol-
lowing analyses.
7.1 DC-Trained Policies
By 40,000 epochs, all eight DC policies converged
to one common optimal policy. Dialogues resulting
from the DC policies average 9.76 user utterances
long. DC policies start each dialogue using the de-
fault amplitude setting of 5. After receiving the ini-
tial user response, they aggressively explore the am-
plitude range. If the initial user response is TL (or
57
DC AC
AmpHistory System Amp User AmpHistory System Amp User
-----[-]----- Query1 +0 5 TS -----[-]----- Query1 +1 6 TS
<<<<<[<]----- Query1 +2 7 Answer <<<<<<[<]---- Query1 +1 7 Answer
<<<<<<-[0]--- Query2 +0 7 Answer <<<<<<<[0]--- Query2 +1 8 Answer
<<<<<<-[0]--- Query3 +0 7 Answer <<<<<<<0[0]-- Query3 +1 9 Answer
<<<<<<-[0]--- Query4 +0 7 Answer <<<<<<00[0]- Query4 +1 10 TL
<<<<<<-[0]--- Query5 +0 7 Answer <<<<<<<000[>] Query4 -2 8 Answer
<<<<<<-[0]--- Query6 +0 7 Answer <<<<<<<0[0]0> Query5 +0 8 Answer
. . . . . . . . . . . . . . . . . . . . . . . .
dialogue length cost = 10 annoyance cost = 12
Table 1: Comparison of DC (left) and AC (right) interactions with a user who has an optimal amplitude of 8 and a
tolerance range of 3. The policies continue as shown, without changing the amplitude level, until all 9 queries are
answered.
TS), they continue by decreasing (or increasing) the
amplitude by -2 (or +2) until they find a tolerable
volume, in which case they stop. Table 1 illustrates
the above noted aspects of the policy. Additionally,
if the policy receives user feedback that is contrary
to the last feedback (i.e., TS after TL, or TL after
TS), the policy backtracks one amplitude setting. In
addition, if the current amplitude is near the bound-
ary (3 or 7), the policy will change the volume by
-1 or +1 as changing it by -2 or +2 would cause it
to move outside users? amplitude range of 2-8. In
essence, the DC policies are quite straightforward;
aggressively changing the amplitude if the user com-
plains, and assuming the amplitude is correct if the
user does not complain.
7.2 AC-Trained Policies
By 55,000 epochs, AC policies converged to one of
two optimal solutions, with an average annoyance
cost of 7.49. As illustrated in Table 1, the behav-
ior of the AC policies is substantially more complex
than the DC policies. First, the AC policies start
by increasing the amplitude, delivering the first ut-
terance at a setting of 6 or 7. Second, the policies
do not stop exploring after they find a tolerable set-
ting, instead attempting to bracket the user?s toler-
ance range, thus identifying the user?s optimal am-
plitude. Third, AC policies sometimes avoid lower-
ing the amplitude, even when doing so would con-
cretely identify the user?s optimal amplitude. By do-
ing so, the policies potentially incur a cost of 1 for
all following turns, but avoid incurring a one time
cost of 3 or 6. In essence, the AC policies attempt to
find the user?s optimal amplitude but may stop short
as they approach the end of the dialogue, favoring a
slightly too high amplitude over one that might be
too low.
7.3 Comparing AC- and DC- Trained Policies
The costs for the AC and DC trained policy sets can-
not be directly compared as each set used a different
cost function. However, we can compare them using
each others? cost function.
First, we compare the two sets of policies in terms
of average dialogue-length. For example, in Table 1,
following a DC policy results in a dialogue-length
of 10. However, for the same user, following the AC
policy results in a dialogue-length of 11, one utter-
ance longer due to the TL response to Query4.
The average dialogue-length of the DC and AC
policies, averaged across users, is shown in the right-
most two columns of Figure 1. As expected, the DC
policies perform better in terms of dialogue-length,
averaging 9.76 utterances long. However, the AC
policies average 10.32 utterances long, only 0.52 ut-
terances longer. This similarity in length is to be ex-
pected, as system communication outside the user?s
tolerance range impedes progress and is costly using
either cost component.
We also compared the AC and DC policies? aver-
age dialogue-length for users with the same optimal
amplitude (i.e., each column shows the average cost
across users with tolerance ranges of 1, 3 and 5), as
shown in Figure 1. From this figure it is clear that
there is little difference in dialogue-length between
AC and DC policies for users with the same optimal
58
amplitude. In addition, for both policies, the lengths
are similar between users with differing optimal am-
plitudes.
 0
 2
 4
 6
 8
 10
 12
 14
2 3 4 5 6 7 8 Ave
A
ve
ra
ge
 D
ia
lo
gu
e 
Le
ng
th
User?s Optimal Amplitude
AC Policies
DC Policies
Figure 1: Comparison of the dialogue-length between AC
and DC policies for users with differing optimal ampli-
tudes.
Second, we compare the two sets of polices in
terms of annoyance costs. For example, in Table 1,
following the AC policy results in an annoyance cost
of 12. For the same user, following the DC policy re-
sults in an annoyance cost of 36; 9 for Query1 as it is
three below the user?s optimal amplitude, and 3 for
each of the following nine utterances as they are all
one below optimal.
As shown in the rightmost columns of Figure 2,
DC policies average annoyance cost was 13.35, a
substantial 78% increase over the average cost of
7.49 for AC policies. Figure 2 also illustrates that
the AC and DC policies perform quite differently for
users with differing optimal amplitudes. For exam-
ple, users of the DC policies whose optimal is at (5),
or slightly below (4), the system?s default setting (5)
average lower annoyance costs than those using the
AC policies. However, these lowered costs for users
in the mid-range is gained at the expense of users
whose optimal amplitude is farther afield, especially
those users requiring higher amplitude settings. This
substantial difference between users with different
optimal amplitudes is because, for DC policies, the
interaction is often conducted at the very edge of the
users? tolerance. In contrast, the AC policies risk
more intolerable utterances, but use this information
to decrease overall costs by better meeting users?
amplitude needs. As such, users of the AC policies
can expect the majority of the task to be conducted
at, or only one setting above, their optimal ampli-
tude.
 0
 5
 10
 15
 20
 25
 30
 35
2 3 4 5 6 7 8 Ave
A
ve
ra
ge
 A
nn
oy
an
ce
 C
os
t
User?s Optimal Amplitude
AC Policies
DC Policies
Figure 2: Comparison of the annoyance cost between AC
and DC policies for users with differing optimal ampli-
tudes.
7.4 Comparing Hand-crafted and Learned
Policies
Each of the two hand-crafted policies were run with
each user simulation (i.e., optimal amplitude from
2-8 and tolerance ranges of 1, 3, or 5). In addition,
we varied the domain task size, requiring between 4
and 10 pieces of information. DC and AC policies
were also trained for these domain task sizes.
As shown in Figure 3, The no-complain policy?s
annoyance costs ranged from 7.81 for dialogues re-
quiring four pieces of information to 14.67 for those
requiring ten pieces. The cost increases linearly with
the amount of information required, because the no-
complain policy maintains the first amplitude setting
found that does not result in a user response of TS
or TL. This ensures the amplitude setting is toler-
able to the user, but may not be the user?s optimal
amplitude.
In contrast, the find-optimal policy?s annoyance
costs initially increase from 9.67 for four pieces of
information to 12.24 for seven through ten pieces.
The cost does not continue to increase when the
amount of information required is greater than seven
because, for dialogues long enough to allow the sys-
tem to concretely identify the user?s optimal ampli-
tude, the cost is zero for all subsequent utterances.
Figure 3 also includes the mean annoyance cost
for the DC and AC policies. Although one might
expect the DC trained policies to resemble the
no-complain policy, the learned policy performs
slightly better. This difference is because the DC
policies learn the range of users? optimal amplitude
settings (2-8), and do not move the amplitude below
2 or above 8. In contrast, the no-complain policies
59
Figure 3: Average user annoyance costs for hand-crafted,
DC and AC policies across dialogues requiring differing
amounts of information.
behave consistently regardless of the current setting,
and thus will incur costs for exploring settings out-
side the range of users? optimal amplitudes. Simi-
larly, AC policies could be anticipated to closely re-
semble the find-optimal policy. However, the AC
policies average cost is lower than the costs for ei-
ther hand-crafted policy, regardless of the amount
of information required. This difference is, in part,
due to differences in behavior at the ends of the
users? optimal amplitude range, like the DC poli-
cies. However, additional factors include the AC
policies? more varied use of amplitude changes and
their balancing of the remaining duration of the di-
alogue against the cost to perform additional explo-
ration, as discussed in subsection 7.2.
8 Discussion and Future Work
The first objective of this work was to create a model
of the communication channel that takes into ac-
count the abilities and preferences of diverse users.
In this model, each user has an optimal amplitude,
but will answer a system query delivered within a
range around that amplitude, although they find non-
preferred, especially too soft, amplitudes annoying.
When outside the user?s tolerance, the user pro-
vides explicit feedback regarding the communica-
tion channel breakdown. For the system, the model
specifies a composite system action, pairing a do-
main action with a possible communication chan-
nel management action to change the amplitude. By
modeling explicit user actions, and implicit system
actions, this model captures some essential elements
of how people manage the communication channel.
The second objective was to determine whether
RL is appropriate for learning communication chan-
nel management. As expected, the learned policies
found and maintained a tolerable amplitude setting
and eliminated user abandonment. We also com-
pared the learned policies with handcrafted solu-
tions, and found that the learned policies performed
better. This is primarily due to RL?s ability to auto-
matically balance the opposing goals of finding the
user?s optimal amplitude and minimizing dialogue-
length.
An added benefit of RL is that it optimizes the sys-
tem?s behavior for the users on which it is trained.
In this work, we purposely used a flat distribution of
users, which caused RL to find a policy (especially
when using annoyance costs) that does not penal-
ize the outliers, which are usually those with special
needs. In fact, we could modify the user distribution,
or the simulated users? behavior, and RL would op-
timize the system?s behavior automatically.
In this work, we contrasted dialogue length (DC)
against annoyance cost (AC) components. We found
that the AC and DC policies share the objective of
finding an amplitude setting within the user?s tol-
erance range because both incur stepwise costs for
intolerable utterances. But, AC policies further re-
fine this objective by incurring costs for tolerable,
but non-optimal, amplitudes as well. AC policies
are using information that is not explicitly commu-
nicated to the system, but which none-the-less RL
can use while learning a policy.
As this was exploratory work, the user model does
not yet fully reflect expected user behavior. For ex-
ample, as the system?s amplitude decreases, users
may misunderstand the system?s query or fail to re-
spond at all. In future work we will use an enhanced
user model that includes more natural user behavior.
In addition, because we wanted the system to focus
on learning a communication channel management
strategy, the domain task was fixed. In future work,
we will use RL to learn policies that both accom-
plish a more complex domain task, and model con-
nections between domain tasks and communication
channel management. Ultimately, we need to con-
duct user-testing to measure the efficacy of the com-
munication channel management policies. We feel
confident that learned policies trained using a com-
munication channel model which reflects the range
of users? abilities and preferences will prove effec-
tive for supporting all users.
60
References
Hua Ai, Joel R. Tetreault, and Diane J. Litman. 2007.
Comparing user simulation models for dialog strategy
learning. In NAACL-HLT, April.
Carryl L. Baldwin and David Struckman-Johnson. 2002.
Impact of speech presentation level on cognitive task
performance: implications for auditory display design.
Ergonomics, 45(1):62?74.
Carryl L. Baldwin. 2001. Impact of age-related hearing
impairment on cognitive task performance: evidence
for improving existing methodologies. In Human Fac-
tors and Ergonomics Society Annual Meeting; Aging,
pages 245?249.
Linda Bell, Joakim Gustafson, and Mattias Heldner.
2003. Prosodic adaptation in humancomputer inter-
action. In Proceedings of ICPhS 03, volume 1, pages
833?836.
Peter Heeman. 2007. Combining reinforcement learning
with information-state update rules. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics, pages
268?275, Rochester, NY, April.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid reinforcement/supervised learning of
dialogue policies from fixed data sets. Comput. Lin-
guist., 34(4):487?511.
J. C. Junqua. 1993. The lombard reflex and its role
on human listeners and automatic speech recogniz-
ers. The Journal of the Acoustical Society of America,
93(1):510?524, January.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard: SWBD-DAMSL Coders Manual.
Blade Kotelly. 2003. The Art and Business of Speech
Recognition. Addison-Wesley, January.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing, 8(1):11?23.
Bjorn Lindblom, 1990. Explaining phonetic variation: A
sketch of the H & H theory, pages 403?439. Kluwer
Academic Publishers.
Rebecca Lunsford, Sharon Oviatt, and Alexander M.
Arthur. 2006. Toward open-microphone engagement
for multiparty interactions. In Proceedings of the 8th
International Conference on Multimodal Interfaces,
pages 273?280, New York, NY, USA. ACM.
Eric Martinson and Derek Brock. 2007. Improv-
ing human-robot interaction through adaptation to the
auditory scene. In HRI ?07: Proceedings of the
ACM/IEEE international conference on Human-robot
interaction, pages 113?120, New York, NY, USA.
ACM.
K. L. Payton, R. M. Uchanski, and L. D. Braida. 1994.
Intelligibility of conversational and clear speech in
noise and reverberation for listeners with normal and
impaired hearing. The Journal of the Acoustical Soci-
ety of America, 95(3):1581?1592, March.
Harvey Sacks, Emanuel A. Schlegoff, and Gail Jefferson.
1974. A simplest sytsematic for the organization of
turn-taking for conversation. Language, 50(4):696?
735, December.
K. Scheffler and S. J. Young. 2002. Automatic learning
of dialogue strategy using dialogue simulation and re-
inforcement learning. In Proceedings of Human Lan-
guage Technology, pages 12?18, San Diego CA.
A. Stent, M. Huffman, and S. Brennan. 2008. Adapt-
ing speaking after evidence of misrecognition: Local
and global hyperarticulation. Speech Communication,
50(3):163?178, March.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction.
Jessica Villing, Cecilia Holtelius, Staffan Larsson, An-
ders Lindstro?m, Alexander Seward, and Nina Aaberg.
2008. Interruption, resumption and domain switching
in in-vehicle dialogue. In GoTAL ?08: Proceedings of
the 6th international conference on Advances in Natu-
ral Language Processing, pages 488?499, Berlin, Hei-
delberg. Springer-Verlag.
Marilyn A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Aritificial
Intelligence Research, 12:387?416.
61
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 249?252,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Autism and Interactional Aspects of Dialogue
Peter A. Heeman, Rebecca Lunsford, Ethan Selfridge, Lois Black, and Jan van Santen
Center for Spoken Language Understanding
Oregon Health & Science University
heemanp@ohsu.edu
Abstract
Little research has been done to explore
differences in the interactional aspects of
dialogue between children with Autis-
tic Spectrum Disorder (ASD) and those
with typical development (TD). Quantify-
ing the differences could aid in diagnosing
ASD, understanding its nature, and better
understanding the mechanisms of dialogue
processing. In this paper, we report on a
study of dialogues with children with ASD
and TD. We find that the two groups differ
substantially in how long they pause be-
fore speaking, and their use of fillers, ac-
knowledgments, and discourse markers.
1 Introduction
Autism Spectrum Disorders (ASD) form a group
of severe neuropsychiatric conditions whose fea-
tures can include impairments in reciprocal social
interaction and in communication (APA, 2000).
These impairments may take different forms,
ranging from individuals with little or no com-
munication to fully verbal individuals with fluent,
grammatically correct speech. In this latter verbal
group, shortcomings in communication have been
noted, including using and processing social cues
during conversations. This is no surprise, since
negotiating a conversation requires many abilities,
several of which are generally impaired in ASD,
such as generating appropriate prosody (Kanner,
1943) and ?theory of mind? (Baron-Cohen, 2000).
We make a distinction between transactional
and interactional aspects of dialogue (Brown and
Yule, 1983). The transactional aspect refers to
message content and interactional focuses on ex-
pressing social relations and personal attitudes.
In this paper, we focus on surface behaviors
that speakers use to help manage the interaction,
namely turn-taking, and the use of fillers, dis-
course markers, and acknowledgments. One ad-
vantage of these behaviors is that they do not re-
quire complete understanding of the dialogue, and
thus lend themselves to automatic analysis. In
addition, these behaviors are under the speaker?s
control and should be robust to what the other
speaker is doing. We hypothesize that just as in-
teractional aspects in general are affected in ASD,
so are these surface behaviors. However, to our
knowledge, little or no work has been done on this.
Investigating how the interactional aspects of
dialogue are affected in ASD serves several pur-
poses. First, it can help in the diagnostic process.
Currently, diagnosing ASD is subjective. Objec-
tive measures based on dialogue interaction could
improve the reliability of the diagnostic process.
Second, it can help us refine the behavioral phe-
notypes of ASD, which is critical for progress on
the basic science front. Third, it can help us re-
fine therapy for people with ASD to address di-
alogue interaction deficits. Fourth, understand-
ing what dialogue aspects are affected in high-
functioning verbal children with ASD can help de-
termine which aspects of dialogue are primarily
social in nature. For example, do speakers use
fillers to signal that there is a communication prob-
lem, or are fillers a symptom of it (cf. Clark and
Fox Tree, 2002)?
In this paper, we report on a study of interac-
tional aspects of dialogues between clinicians and
children with ASD. The dialogues were recorded
during administration of the Autism Diagnostic
Observation Schedule (Lord et al, 2000), which
is an instrument used to assist in diagnosing ASD.
We compare the performance of these children
with a group of children with typical development
(TD).
2 Data
The data used in this paper was collected dur-
ing administration of the ADOS on 22 TD chil-
dren and 26 with ASD, ranging in age from 4 to
8 years old. The children with ASD were high-
functioning and verbal. The speech of the clini-
cian and child was transcribed into utterance-like
units, with a start and an end time. Activities were
annotated in a separate tier. The transcriptions in-
cluded the punctuation marks ?.?, ?!?, and ??? to
mark syntactically and semantically complete sen-
249
tences, and ?>? to mark incomplete ones. As a sin-
gle audio channel was used, the timing of overlap-
ping speech was marked as best as possible. Each
child on average said 2221 words, 574 utterances,
and 316 turns.
3 Results
Pauses between Turns: We first examine how
long children wait before starting their turn. We
hypothesized that children with ASD would wait
longer on average to respond, either because they
are less aware of (a) the turn-taking cues, (b) the
social obligation to minimize inter-turn pauses, or
(c) they have a slower processing and response
times. For this analysis, we look at all turns in
which there is no overlap between the beginning of
the child?s turn and the clinician?s speech. Data is
available on 4412 pauses for the TD children and
5676 for the children with ASD. The grand means
of the children?s pauses are shown in Table 1 along
with the standard deviations. The TD children?s
average pause length is 0.876s. For the children
with ASD, it is 1.115s, 27.3% longer. This dif-
ference is significant, a-priori independent t-test
t=2.34 (df=39), p<.02 one-tailed.
TD ASD
all 0.876 (0.24) 1.115 (0.45)
after question 0.748 (0.25) 1.005 (0.40)
after non-question 1.076 (0.37) 1.329 (0.74)
Table 1: Pauses before new turns.
We also examine the pauses following ques-
tions by the clinician versus non-questions. Ques-
tions are interesting as they impose a social obli-
gation for the child to respond, and they have
strong prosodic cues at their ending. We identified
questions as utterances transcribed with a ques-
tion mark, which might include rhetorical ques-
tions. After a non-question (e.g., a statement), the
average pause is 1.076s for the TD children and
1.329s for children with ASD. This difference is
not statistically significant by independent t-test,
t<1.6, NS. After a question, the average pause
is 0.748s for the TD children and 1.005s for the
children with ASD, a significant difference by a-
priori independent t-test t=2.72 (df=42), p<.005
one-tailed. The ASD children on average take
34.4% longer to respond. Thus, after a question,
the difference between children with TD and ASD
is more pronounced.
Pauses by Activity: The ADOS includes hav-
ing the child engage in different activities. For
this research, we collapse the activities into three
types: converse is when there is no non-speech
task; describe is when the child is doing a men-
tal task, such as describing a picture; and play is
when the child is interacting with the clinician in
a play session. To better understand the difference
between questions and non-questions, we examine
the pauses in each activity (Table 2).
TD ASD
question non-ques. question non-ques.
converse 0.730 0.30 0.656 0.27 0.890 0.34 0.932 0.88
describe 0.853 0.44 0.879 0.37 1.056 0.51 1.282 1.21
play 0.720 0.34 1.825 0.78 1.289 1.51 1.887 1.37
Table 2: Pauses for each type of activity.
After a question, the TD children tend to re-
spond with similar pauses in each activity (the dif-
ferences in column 2 between activities are not
significant by pairwise paired t-test, all t?s<1.6,
NS). After a question, the child has a social obli-
gation to respond, and this does not seem to be
overridden by whether there is a separate task
they are involved in. Even after a non-question,
conversants have a social obligation to keep the
speaking floor occupied and so to minimize inter-
utterance pauses (Sacks et al, 1974). However, as
seen in the third column, the pauses are affected
by the type of activity, and the differences are
statistically significant by pairwise paired t-test,
(df=21), two-tailed: converse-describe t=2.24,
p<.04; describe-play t=5.68, p<.0001; converse-
play t=6.87, p<.0001. The biggest difference is
with play. Here, it seems that the conversants
physical interaction lessens the social obligation
of maintaining the speaking floor. These findings
are interesting for social-linguistics as it suggests
that the social obligations of turn-taking are al-
tered by the presence of a non-speech task.
We next compare the children with ASD to the
TD children. For the converse activity, we see that
the children with ASD take longer to respond, af-
ter questions and non-questions. The difference
after questions is significant by independent t-test,
t=1.74 (df=46) p<.05, one-tailed, whereas the dif-
ference after non-questions is marginal, t=1.47
(df=28) p<.08. This result could be explained by
the slower processing and response times associ-
ated with ASD.
Just as with the TD children, we see that after
a non-question, the children with ASD take longer
to respond when there is another task. The differ-
ences in pause lengths between converse and play
are significant, by paired t-test, t=2.89 (df=23)
250
p<.009, two-tailed. The difference between de-
scribe and play is marginal, t=2.03 (df=25) p<.06,
and there was no significant difference between
converse and describe, t<1, NS.
After a question, the children with ASD take
longer to respond when there is another task, espe-
cially for play, although the pairwise differences in
pause length between activities are not significant.
This suggests that the children with ASD become
distracted when there is another task, and so be-
come less sensitive to either the question prosody
or the social obligation of questions.
Fillers: We next examine the rate of fillers, at
the beginning of turns, beginning of utterances,
and in the middle of utterances. We look at these
contexts individually as fillers can serve different
roles, such as turn-taking, stalling for time or as
part of a disfluency, and their role is correlated
to their position in a turn. The rates are reported
in Table 3, along with the total number of fillers
within each category. Interestingly, the rate of ?uh?
between children with TD and ASD is similar for
all positions (independent t-test, all g?s<1, NS).
uh um
TD ASD TD ASD
turn init. 1.70% 112 1.84% 159 3.86% 243 1.65% 146
utt. init. 1.31% 43 1.20% 33 2.29% 73 0.52% 10
utt. medial 0.25% 103 0.31% 137 1.03% 492 0.21% 123
Table 3: Rate of fillers.
The more interesting finding, though, is in the
usage of ?um?. Children with ASD use it signifi-
cantly less than the TD children in every position,
from 1/2 the rate in turn-initial position to 1/5 in
utterance-medial position, independent two-tailed
t-test: turn initial t=2.74 (df=38), p<.01; utterance
initial t=2.53 (df=31), p<.02; and utterance me-
dial t=3.94 (df=24), p<.001.
TD ASD
converse 1.76% 569 0.56% 190
describe 1.15% 115 0.33% 31
play 0.96% 124 0.45% 58
Table 4: Use of ?um? by activity.
We also examined the overall usage of ?um? in
each activity (Table 4). The TD children use ?um?
more often in each activity than the children with
ASD, and the differences are statistically signif-
icant by independent two-tailed t-test: converse
t=3.62 (df=29), p<.002; describe t=2.83 (df=27),
p<.01; play t=2.42 (df=33), p<.03. This result
supports the robustness of the findings about ?um?.
Many researchers have speculated on the role
of ?um? and ?uh?. In recent work, Clark and Fox
Tree (2002) argued that they signal a delay, and
that ?um? signals more delay than ?uh?. They view
both as linguistic devices that are planned for, just
as any other word is. Our work suggests that ?um?
and ?uh? arise from different cognitive processes,
and that the process that accounts for ?uh? is not
affected by ASD, while the process for ?um? is.1
Acknowledgments: We next look at the rate of
acknowledgments: single word utterances that are
used to show agreement or understanding. Thus,
the use of acknowledgments requires awareness of
the other person?s desire to ensure mutual under-
standing. As the corpus did not have these words
explicitly marked, we identify a word as an ac-
knowledgment if it meets the following criteria:
(a) it is one of the words listed in Table 5 (based
on Heeman and Allen, 1999); (b) it is first in the
speaker?s turn; and (c) it does not follow a question
by the clinician. The TD children used acknowl-
edgments in 17.42% of their turns that did not fol-
low a question, while the children with ASD did
this only 13.39% of the time (Table 5), a statis-
tically significant difference by a-priori indepen-
dent t-test t=1.78 (df=46), p<.05 one-tailed.
TD ASD
total 17.42% 568 13.39% 459
yeah 7.49% 248 5.87% 215
no 2.78% 78 2.06% 63
mm-hmm 2.06% 75 1.07% 35
mm 0.99% 29 1.35% 42
ok 1.87% 65 0.83% 27
yes 0.92% 32 0.88% 32
right 0.14% 5 0.23% 8
hm 0.73% 21 0.69% 20
uh-huh 0.44% 15 0.42% 17
Table 5: Use of acknowledgments.
Discourse Markers: We next examine dis-
course markers, which are words such as ?well?
and ?oh? that express how the current utterance
relates to the discourse context (Schiffrin, 1987).
We classified a word as a discourse marker if it
was the first word in an utterance and is one of
the words in Table 6 (Heeman and Allen, 1999).
As shown in Table 6, the children with ASD use
discourse markers significantly less than the TD
children in both conditions by a-priori indepen-
dent, one-tailed t-test: turn-initial t=3.24 (df=43)
p<.002; utterance-initial t=4.01 (df=44) p<.0001.
1In Lunsford et al (2010) we investigate the rate and
length of pauses after ?uh? and ?um?. In addition, we veri-
fied the t-tests using Wilcoxon rank sum tests.
251
As can be seen, most of the difference is in the use
of ?and?. The data for the other discourse markers
was sparse, so we compared ?and? against all of the
others combined. The decreased usage of ?and?
in the ASD children is statistically significant
for both conditions by a-priori independent, one-
tailed t-test: turn-initial t=4.47 (df=30), p<.0001;
utterance-initial t=3.79 (df=43), p<.0002. There
is little difference in the use of all of the other dis-
course markers combined, and the difference is not
statistically significant.
Turn Initial Utterance Initial
TD ASD TD ASD
all 19.2% 1290 12.8% 1196 28.7% 2053 19.4% 1330
and 10.7% 731 5.0% 471 19.5% 1419 12.0% 844
then 0.6% 38 1.0% 89 1.5% 97 1.4% 79
but 2.1% 144 1.3% 113 3.6% 238 2.7% 194
well 2.2% 143 2.7% 271 1.1% 74 1.2% 79
oh 2.0% 135 1.8% 160 1.0% 67 1.3% 68
so 1.2% 75 0.7% 60 1.6% 129 0.7% 49
wait 0.2% 9 0.2% 21 0.2% 17 0.2% 15
actually 0.2% 15 0.1% 11 0.2% 12 0.0% 2
not and 8.5% 559 7.8% 725 9.2% 634 7.4% 486
Table 6: Use of discourse markers.
The use of ?and? is also lower in each activity
for the ASD children (Table 7), a significant dif-
ference by a-priori independent one-tailed t-test:
converse t=3.00 (df=41), p<.003; describe t=4.79
(df=38), p<.0001, play t=4.07 (df=30), p<.0002.
TD ASD
converse 13.36% 1139 7.95% 755
describe 21.77% 587 10.76% 339
play 12.97% 424 5.18% 221
Table 7: Use of ?and? in each activity.
One explanation for the decreased usage of
?and? and not the other discourse markers might
be that, of all the discourse markers, ?and? seems
to have the least meaning. It simply signifies
that there is some continuation between the new
speech and the previous context. This might make
it difficult for children with ASD to learn its use. A
second explanation is that the children with ASD
are using ?and? correctly, but simply do not pro-
duce as many utterances that are related to the pre-
vious context (cf. Bishop et al, 2000).
4 Conclusion
In this paper, we examined a number of interac-
tional aspects of dialogue in the speech of children
with ASD and TD. We found that children with
ASD have a lower rate of the filler ?um?, acknowl-
edgments, and the discourse marker ?and.? We also
found that in certain situations, they take longer to
respond. These deficits might prove useful for im-
proved diagnosis of ASD. We also found that chil-
dren with ASD have a lower rate of ?um? but not
of ?uh?, and that only the discourse marker ?and?
seems to be affected. This might prove useful for
both better understanding the nature of ASD as
well as better understanding the role of these phe-
nomena in dialogue. Although the results reported
in this work are preliminary, they do show the po-
tential of our approach. More work is needed to
ensure that our automatic identification of turn-
taking events, discourse markers, and acknowl-
edgments is correct and to explore alternate expla-
nations for the results that we observed.
Acknowledgments
Funding gratefully received from the National In-
stitute of Heath under grants IR21DC010239 and
5R01DC007129, and the National Science Foun-
dation under IIS-0713698. The views herein are
those of the authors and reflect the views neither
of the funding agencies.
References
American Psychiatric Association, Washington DC,
2000. Diagnostic and Statistical Manual of Mental
Disorders, 4th Edition, Text Revision (DSM-IV-TR).
S. Baron-Cohen. 2000. Theory of mind and autism:
A review. In L. M. Glidden, editor, International
Review of Research in Mental Retardation, volume
23: Autism, pages 170?184. Academic Press.
D. Bishop et al 2000. Conversational responsive-
ness in specific language impairment: Evidence of
disproportionate pragmatic difficulties in a subset
of children. Development and Psychopathology,
12(2):177?199.
G. Brown and G. Yule. 1983. Discourse Analysis.
Cambridge University Press.
H. Clark and J. Fox Tree. 2002. Using uh and um in
spontaneous speaking. Cognition, 8:73?111.
P. Heeman and J. Allen. 1999. Speech repairs, in-
tonational phrases and discourse markers: Model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?572.
L. Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
C. Lord et al 2000. The autism diagnostic observa-
tion schedule-generic: a standard measure of social
and communication deficits associted with the spec-
trum of autium. Journal of Austism Developmental
Disorders, 30(3):205?223, June.
R. Lunsford et al 2010. Autism and the use of fillers:
differences between ?um? and ?uh?. In 5th Workshop
on Disfluency in Spontaneous Speech, Tokyo.
H. Sacks, E. Schegloff, and G. Jefferson. 1974. A sim-
plest systematics for the organization of turn-taking
for conversation. Language, 50(4):696?735.
D. Schiffrin. 1987. Discourse Markers. Cambridge
University Press, New York.
252
