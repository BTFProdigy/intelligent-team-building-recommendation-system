Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 529?539,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Random Walk Inference and Learning in A Large Scale Knowledge Base
Ni Lao
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
nlao@cs.cmu.edu
Tom Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
tom.mitchell@cs.cmu.edu
William W. Cohen
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
wcohen@cs.cmu.edu
Abstract
We consider the problem of performing learn-
ing and inference in a large scale knowledge
base containing imperfect knowledge with
incomplete coverage. We show that a soft
inference procedure based on a combination
of constrained, weighted, random walks
through the knowledge base graph can be
used to reliably infer new beliefs for the
knowledge base. More specifically, we
show that the system can learn to infer
different target relations by tuning the weights
associated with random walks that follow
different paths through the graph, using a
version of the Path Ranking Algorithm (Lao
and Cohen, 2010b). We apply this approach to
a knowledge base of approximately 500,000
beliefs extracted imperfectly from the web
by NELL, a never-ending language learner
(Carlson et al, 2010). This new system
improves significantly over NELL?s earlier
Horn-clause learning and inference method:
it obtains nearly double the precision at rank
100, and the new learning method is also
applicable to many more inference tasks.
1 Introduction
Although there is a great deal of recent research
on extracting knowledge from text (Agichtein and
Gravano, 2000; Etzioni et al, 2005; Snow et
al., 2006; Pantel and Pennacchiotti, 2006; Banko
et al, 2007; Yates et al, 2007), much less
progress has been made on the problem of drawing
reliable inferences from this imperfectly extracted
knowledge. In particular, traditional logical
inference methods are too brittle to be used to make
complex inferences from automatically-extracted
knowledge, and probabilistic inference methods
(Richardson and Domingos, 2006) suffer from
scalability problems. This paper considers the
problem of constructing inference methods that can
scale to large knowledge bases (KB?s), and that are
robust to imperfect knowledge. The KB we consider
is a large triple store, which can be represented as a
labeled, directed graph in which each entity a is a
node, each binary relation R(a, b) is an edge labeled
R between a and b, and unary concepts C(a) are
represented as an edge labeled ?isa? between the
node for the entity a and a node for the concept
C. We present a trainable inference method that
learns to infer relations by combining the results of
different random walks through this graph, and show
that the method achieves good scaling properties and
robust inference in a KB containing over 500,000
triples extracted from the web by the NELL system
(Carlson et al, 2010).
1.1 The NELL Case Study
To evaluate our approach experimentally, we study
it in the context of the NELL (Never Ending
Language Learning) research project, which is an
effort to develop a never-ending learning system that
operates 24 hours per day, for years, to continuously
improve its ability to read (extract structured facts
from) the web (Carlson et al, 2010). NELL began
operation in January 2010. As of March 2011,
NELL had built a knowledge base containing several
million candidate beliefs which it had extracted from
the web with varying confidence. Among these,
529
NELL had fairly high confidence in approximately
half a million, which we refer to as NELL?s
(confident) beliefs. NELL had lower confidence in a
few million others, which we refer to as its candidate
beliefs.
NELL is given as input an ontology that defines
hundreds of categories (e.g., person, beverage,
athlete, sport) and two-place typed relations among
these categories (e.g., atheletePlaysSport(?athlete?,
?sport?)), which it must learn to extract from the
web. It is also provided a set of 10 to 20 positive
seed examples of each such category and relation,
along with a downloaded collection of 500 million
web pages from the ClueWeb2009 corpus (Callan
and Hoy, 2009) as unlabeled data, and access to
100,000 queries each day to Google?s search engine.
Each day, NELL has two tasks: (1) to extract
additional beliefs from the web to populate its
growing knowledge base (KB) with instances of the
categories and relations in its ontology, and (2) to
learn to perform task 1 better today than it could
yesterday. We can measure its learning competence
by allowing it to consider the same text documents
today as it did yesterday, and recording whether it
extracts more beliefs, more accurately today.1
NELL uses a large-scale semi-supervised multi-
task learning algorithm that couples the training
of over 1500 different classifiers and extraction
methods (see (Carlson et al, 2010)). Although
many of the details of NELL?s learning method
are not central to this paper, two points should
be noted. First, NELL is a multistrategy learning
system, with components that learn from different
?views? of the data (Blum and Mitchell, 1998): for
instance, one view uses orthographic features of
a potential entity name (like ?contains capitalized
words?), and another uses free-text contexts in
which the noun phrase is found (e.g., ?X frequently
follows the bigram ?mayor of? ?). Second, NELL
is a bootstrapping system, which self-trains on its
growing collection of confident beliefs.
1.2 Knowledge Base Inference: Horn Clauses
Although NELL has now grown a sizable knowl-
edge base, its ability to perform inference over this
1NELL?s current KB is available online at
http://rtw.ml.cmu.edu.
Eli Manning Giants
AthletePlays
ForTeam
HinesWard Steelers
AthletePlays
ForTeam NFL
TeamPlays
InLeague
MLBTeamPlays
InLeague
TeamPlays
InLeague
Figure 1: An example subgraph.
knowledge base is currently very limited. At present
its only inference method beyond simple inheritance
involves applying first order Horn clause rules to
infer new beliefs from current beliefs. For example,
it may use a Horn clause such as
AthletePlaysForTeam(a, b) (1)
? TeamPlaysInLeague(b, c)
? AthletePlaysInLeague(a,c)
to infer that AthletePlaysInLeague(HinesWard,NFL),
if it has already extracted the beliefs in the
preconditions of the rule, with variables a, b and c
bound to HinesWard, PittsburghSteelers and NFL
respectively as shown in Figure 1. NELL currently
has a set of approximately 600 such rules, which
it has learned by data mining its knowledge base
of beliefs. Each learned rule carries a conditional
probability that its conclusion will hold, given that
its preconditions are satisfied.
NELL learns these Horn clause rules using
a variant of the FOIL algorithm (Quinlan and
Cameron-Jones, 1993), henceforth N-FOIL.
N-FOIL takes as input a set of positive and
negative examples of a rule?s consequent
(e.g., +AthletePlaysInLeague(HinesWard,NFL),
?AthletePlaysInLeague(HinesWard,NBA)), and
uses a ?separate-and-conquer? strategy to learn a
set of Horn clauses that fit the data well. Each
Horn clause is learned by starting with a general
rule and progressively specializing it, so that it
still covers many positive examples but covers few
negative examples. After a clause is learned, the
examples covered by that clause are removed from
the training set, and the process repeats until no
positive examples remain.
Learning first-order Horn clauses is computation-
ally expensive?not only is the search space large,
but some Horn clauses can be costly to evaluate
(Cohen and Page, 1995). N-FOIL uses two tricks
to improve its scalability. First, it assumes that
the consequent predicate is functional?e.g., that
530
each Athlete plays in at most one League. This
means that explicit negative examples need not
be provided (Zelle et al, 1995): e.g., if Ath-
letePlaysInLeague(HinesWard,NFL) is a positive
example, then AthletePlaysInLeague(HinesWard,c?)
for any other value of c? is negative. In general,
this constraint guides the search algorithm toward
Horn clauses that have fewer possible instantiations,
and hence are less expensive to match. Second,
N-FOIL uses ?relational pathfinding? (Richards
and Mooney, 1992) to produce general rules?i.e.,
the starting point for a predicate R is found
by looking at positive instances R(a, b) of the
consequent, and finding a clause that corresponds
to a bounded-length path of binary relations that
link a to b. In the example above, a start clause
might be the clause (1). As in FOIL, the clause
is then (potentially) specialized by greedily adding
additional conditions (like ProfessionalAthlete(a))
or by replacing variables with constants (eg,
replacing c with NFL).
For each N-FOIL rule, an estimated conditional
probability P? (conclusion|preconditions) is calcu-
lated using a Dirichlet prior according to
P? = (N+ +m ? prior)/(N+ +N? +m) (2)
where N+ is the number of positive instances
matched by this rule in the FOIL training data,
N? is the number of negative instances matched,
m = 5 and prior = 0.5. As the results below
show, N-FOIL generally learns a small number of
high-precision inference rules. One important role
of these inference rules is that they contribute to
the bootstrapping procedure, as inferences made by
N-FOIL increase either the number of candidate
beliefs, or (if the inference is already a candidate)
improve NELL?s confidence in candidate beliefs.
1.3 Knowledge Base Inference: Graph
Random Walks
In this paper, we consider an alternative approach,
based on the Path Ranking Algorithm (PRA) of Lao
and Cohen (2010b), described in detail below. PRA
learns to rank graph nodes b relative to a query
node a. PRA begins by enumerating a large set of
bounded-length edge-labeled path types, similar to
the initial clauses used in NELL?s variant of FOIL.
These path types are treated as ranking ?experts?,
each performing a random walk through the graph,
constrained to follow that sequence of edge types,
and ranking nodes b by their weights in the resulting
distribution. Finally, PRA combines the weights
contributed by different ?experts? using logistic
regression to predict the probability that the relation
R(a, b) is satisfied.
As an example, consider a path from a to b via
the sequence of edge types isa, isa?1 (the inverse of
isa), and AthletePlaysInLeague, which corresponds
to the Horn clause
isa(a, c) ? isa?1(c, a?) (3)
? AthletePlaysInLeague(a?, b)
? AthletePlaysInLeague(a, b)
Suppose a random walk starts at a query node a
(say a=HinesWard). If HinesWard is linked to the
single concept node ProfessionalAthlete via isa, the
walk will reach that node with probability 1 after
one step. If A is the set of ProfessionalAthlete?s
in the KB, then after two steps, the walk will have
probability 1/|A| of being at any a? ? A. If L is
the set of athletic leagues and ` ? L, let A` be the
set of athletes in league `: after three steps, the walk
will have probability |A`|/|A| of being at any point
b ? L. In short, the ranking associated with this
path gives the prior probability of a value b being an
athletic league for a?which is useful as a feature in
a combined ranking method, although not by itself a
high-precision inference rule.
Note that the rankings produced by this ?expert?
will change as the knowledge base evolves?for
instance, if the system learns about proportionally
more soccer players than hockey players over time,
then the league rankings for the path of clause (3)
will change. Also, the ranking is specific to the
query node a. For instance, suppose the KB contains
facts which reflect the ambiguity of the team name
?Giants?2 as in Figure 1. Then the path for clause (1)
above will give lower weight to b = NFL for a =
EliManning than to b = NFL for a = HinesWard.
The main contribution of this paper is to introduce
and evaluate PRA as an algorithm for making
probabilistic inference in large KBs. Compared to
Horn clause inference, the key characteristics of this
new inference method are as follows:
2San Francisco?s Major-League Baseball and New York?s
National Football League teams are both called the ?Giants?.
531
? The evidence in support of inferring a relation
instance R(a, b) is based on many existing
paths between a and b in the current KB,
combined using a learned logistic function.
? The confidence in an inference is sensitive to
the current state of the knowledge base, and the
specific entities being queried (since the paths
used in the inference have these properties).
? Experimentally, the inference method yields
many more moderately-confident inferences
than the Horn clauses learned by N-FOIL.
? The learning and inference are more efficient
than N-FOIL, in part because we can exploit
efficient approximation schemes for random
walks (Lao and Cohen, 2010a). The resulting
inference is as fast as 10 milliseconds per query
on average.
The Path Ranking Algorithm (PRA) we use is
similar to that described elsewhere (Lao and Cohen,
2010b), except that to achieve efficient model
learning, the paths between a and b are determined
by the statistics from a population of training
queries rather than enumerated completely. PRA
uses random walks to generate relational features
on graph data, and combine them with a logistic
regression model. Compared to other relational
models (e.g. FOIL, Markov Logic Networks), PRA
is extremely efficient at link prediction or retrieval
tasks, in which we are interested in identifying top
links from a large number of candidates, instead of
focusing on a particular node pair or joint inferences.
1.4 Related Work
The TextRunner system (Cafarella et al, 2006)
answers list queries on a large knowledge base
produced by open domain information extrac-
tion. Spreading activation is used to measure
the closeness of any node to the query term
nodes. This approach is similar to the random
walk with restart approach which is used as a
baseline in our experiment. The FactRank system
(Jain and Pantel, 2010) compares different ways of
constructing random walks, and combining them
with extraction scores. However, the shortcoming
of both approaches is that they ignore edge type
information, which is important for achieving high
accuracy predictions.
The HOLMES system (Schoenmackers et al,
2008) derives new assertions using a few manually
written inference rules. A Markov network
corresponding to the grounding of these rules to
the knowledge base is constructed for each query,
and then belief propagation is used for inference.
In comparison, our proposed approach discovers
inference rules automatically from training data.
Similarly, the Markov Logic Networks (Richard-
son and Domingos, 2006) are Markov networks
constructed corresponding to the grounding of rules
to knowledge bases. In comparison, our proposed
approach is much more efficient by avoiding the
harder problem of joint inferences and by leveraging
efficient random walk schemes (Lao and Cohen,
2010a).
Below we describe our approach in greater detail,
provide experimental evidence of its value for
performing inference in NELL?s knowledge base,
and discuss implications of this work and directions
for future research.
2 Approach
In this section, we first describe how we formulate
link (relation) prediction on a knowledge base as
a ranking task. Then we review the Path Ranking
Algorithm (PRA) introduced by Lao and Cohen
(2010b; 2010a). After that, we describe two
improvements to the PRA method to make it more
suitable for the task of link prediction in knowledge
bases. The first improvement helps PRA deal
with the large number of relations typical of large
knowledge bases. The second improvement aims at
improving the quality of inference by applying low
variance sampling.
2.1 Learning with NELL?s Knowledge Base
For each relationR in the knowledge base we train a
model for the link prediction task: given a concept a,
find all other concepts b which potentially have the
relationR(a, b). This prediction is made based on an
existing knowledge base extracted imperfectly from
the web. Although a model can potentially benefit
from predicting multiple relations jointly, such joint
inference is beyond the scope of this work.
532
To ensure a reasonable number of training
instances, we generate labeled training example
queries from 48 relations which have more than
100 instances in the knowledge base. We create
two tasks for each relation?i.e., predicting b given
a and predicting a given b? yielding 96 tasks in
all. Each node a which has relation R in the
knowledge base with any other node is treated as a
training query, the actual nodes b in the knowledge
base known to satisfy R(a, b) are treated as labeled
positive examples, and any other nodes are treated
as negative examples.
2.2 Path Ranking Algorithm Review
We now review the Path Ranking Algorithm
introduced by Lao and Cohen (2010b). A relation
path P is defined as a sequence of relations
R1 . . . R`, and in order to emphasize the types
associated with each step, P can also be written as
T0 R1??? . . . R`?? T`, where Ti = range(Ri) =
domain(Ri+1), and we also define domain(P ) ?
T0, range(P ) ? T`. In the experiments in this
paper, there is only one type of node which we call
a concept, which can be connected through different
types of relations. In this notation, relations like ?the
team a certain player plays for?, and ?the league a
certain player?s team is in? can be expressed by the
paths below (respectively):
P1 : concept
AtheletePlayesForTeam??????????????? concept
P2 : concept
AtheletePlayesForTeam??????????????? concept
TeamPlaysInLeagure?????????????? concept
For any relation path P = R1 . . . R` and a
seed node s ? domain(P ), a path constrained
random walk defines a distribution hs,P recursively
as follows. If P is the empty path, then define
hs,P (e) =
{
1, if e = s
0, otherwise (4)
If P = R1 . . . R` is nonempty, then let P ? =
R1 . . . R`?1, and define
hs,P (e) =
?
e??range(P ?)
hs,P ?(e?) ? P (e|e?;R`), (5)
where P (e|e?;R`) = R`(e?,e)|R`(e?,?)| is the probability ofreaching node e from node e? with a one step random
walk with edge type R`. R(e?, e) indicates whether
there exists an edge with type R that connect e? to e.
More generally, given a set of paths P1, . . . , Pn,
one could treat each hs,Pi(e) as a path feature for
the node e, and rank nodes by a linear model
?1hs,P1(e) + ?2hs,P2(e) + . . . ?nhs,Pn(e)
where ?i are appropriate weights for the paths. This
gives a ranking of nodes e related to the query node
s by the following scoring function
score(e; s) =
?
P?P`
hs,P (e)?P , (6)
where P` is the set of relation paths with length? `.
Given a relation R and a set of node pairs
{(si, ti)} for which we know whether R(si, ti) is
true or not, we can construct a training dataset
D = {(xi, yi)}, where xi is a vector of all the
path features for the pair (si, ti)?i.e., the j-th
component of xi is hsi,Pj (ti), and where yi is a
boolean variable indicating whetherR(si, ti) is true.
We then train a logistic function to predict the
conditional probability P (y|x; ?). The parameter
vector ? is estimated by maximizing a regularized
form of the conditional likelihood of y given x. In
particular, we maximize the objective function
O(?) =
?
i
oi(?)? ?1|?|1 ? ?2|?|2, (7)
where ?1 controls L1-regularization to help struc-
ture selection, and ?2 controls L2-regularization
to prevent overfitting. oi(?) is the per-instance
weighted log conditional likelihood given by
oi(?) = wi[yi ln pi + (1? yi) ln(1? pi)], (8)
where pi is the predicted probability p(yi =
1|xi; ?) = exp(?
Txi)
1+exp(?Txi) , and wi is an importanceweight to each example. A biased sampling
procedure selects only a small subset of negative
samples to be included in the objective (see (Lao and
Cohen, 2010b) for detail).
2.3 Data-Driven Path Finding
In prior work with PRA, P` was defined as all
relation paths of length at most `. When the number
of edge types is small, one can generate P` by
533
Table 1: Number of paths in PRA models of maximum
path length 3 and 4. Averaged over 96 tasks.
`=3 `=4
all paths up to length L 15, 376 1, 906, 624
+query support? ? = 0.01 522 5016
+ever reach a target entity 136 792
+L1 regularization 63 271
enumeration; however, for domains with a large
number of edge types (e.g., a knowledge base), it is
impractical to enumerate all possible relation paths
even for small `. For instance, if the number of
edge types related to each node type is 100, even
the number of length three paths types easily reaches
millions. For other domains like parsed natural
language sentences, useful relation paths can be as
long as ten relations (Minkov and Cohen, 2008). In
this case, even with smaller number of possible edge
types, the total number of relation paths is still too
large for systematic enumeration.
In order to apply PRA to these domains, we
modify the path generation procedure in PRA to
produce only relation paths which are potentially
useful for the task. Define a query s to be supporting
a path P if hs,P (e) 6= 0 for any entity e. We require
that any path node created during path finding needs
to be supported by at least a fraction ? of the training
queries si, as well as being of length no more than
` (In the experiments, we set ? = 0.01) We also
require that in order for a relation path to be included
in the PRA model, it must retrieve at least one target
entity ti in the training set. As we can see from
Table 1, together these two constraints dramatically
reduce the number of relation paths that need to be
considered, relative to systematically enumerating
all possible relation paths. L1 regularization reduces
the size of the model even more.
The idea of finding paths that connects nodes in a
graph is not new. It has been embodied previously in
first-order learning systems (Richards and Mooney,
1992) as well as N-FOIL, and relational database
searching systems (Bhalotia et al, 2002). These
approaches consider a single query during path
finding. In comparison, the data-driven path finding
method we described here uses statistics from a
population of queries, and therefore can potentially
determine the importance of a path more reliably.
Table 2: Comparing PRA with RWR models. MRRs and
training times are averaged over 96 tasks.
`=2 `=3
MRR Time MRR Time
RWR(no train) 0.271 0.456
RWR 0.280 3.7s 0.471 9.2s
PRA 0.307 5.7s 0.516 15.4s
2.4 Low-Variance Sampling
Lao and Cohen (2010a) previously showed that
sampling techniques like finger printing and particle
filtering can significantly speedup random walk
without sacrificing retrieval quality. However, the
sampling procedures can induce a loss of diversity
in the particle population. For example, consider a
node in the graph with just two out links with equal
weights, and suppose we are required to generate
two walkers starting from this node. A disappointing
result is that with 50 percent chance both walkers
will follow the same branch, and leave the other
branch with no probability mass.
To overcome this problem, we apply a technique
called Low-Variance Sampling (LVS) (Thrun et
al., 2005), which is commonly used in robotics
to improve the quality of sampling. Instead of
generating independent samples from a distribution,
LVS uses a single random number to generate all
samples, which are evenly distributed across the
whole distribution. Note that given a distribution
P (x), any number r in [0, 1] points to exactly one
x value, namely x = argminj
?
m=1..j P (m) ?
r. Suppose we want to generate M samples from
P (x). LVS first generates a random number r in
the interval [0,M?1]. Then LVS repeatedly adds
the fixed amount M?1 to r and chooses x values
corresponding to the resulting numbers.
3 Results
This section reports empirical results of applying
random walk inference to NELL?s knowledge base
after the 165th iteration of its learning process. We
first investigate PRA?s behavior by cross validation
on the training queries. Then we compare PRA and
N-FOIL?s ability to reliably infer new beliefs, by
leveraging the Amazon Mechanical Turk service.
534
3.1 Cross Validation on the Training Queries
Random Walk with Restart (RWR) (also called
personalized PageRank (Haveliwala, 2002)) is a
general-purpose graph proximity measure which
has been shown to be fairly successful for many
types of tasks. We compare PRA to two versions
of RWR on the 96 tasks of link prediction with
NELL?s knowledge base. The two baseline methods
are an untrained RWR model and a trained RWR
model as described by Lao and Cohen (2010b). (In
brief, in the trained RWR model, the walker will
probabilistically prefer to follow edges associated
with different labels, where the weight for each edge
label is chosen to minimize a loss function, such as
Equation 7. In the untrained model, edge weights
are uniform.) We explored a range of values for
the regularization parameters L1 and L2 using cross
validation on the training data, and we fix both
L1 and L2 parameters to 0.001 for all tasks. The
maximum path length is fixed to 3.3
Table 2 compares the three methods using
5-fold cross validation and the Mean Reciprocal
Rank (MRR)4 measure, which is defined as the
inverse rank of the highest ranked relevant result
in a set of results. If the the first returned
result is relevant, then MRR is 1.0, otherwise,
it is smaller than 1.0. Supervised training can
significantly improve retrieval quality (p-value=9 ?
10?8 comparing untrained and trained RWR), and
leveraging path information can produce further
improvement (p-value=4? 10?4 comparing trained
RWR with PRA). The average training time for a
predicate is only a few seconds.
We also investigate the effect of low-variance
sampling on the quality of prediction. Figure 2 com-
pares independent and low variance sampling when
applied to finger printing and particle filtering (Lao
and Cohen, 2010a). The horizontal axis corresponds
to the speedup of random walk compared with
exact inference, and the vertical axis measures the
quality of prediction by MRR with three fold cross
validation on the training query set. Low-variance
3Results with maximum length 4 are not reported here.
Generally models with length 4 paths produce slightly better
results, but are 4-5 times slower to train
4For a set of queries Q,
MRR = 1|Q|
?
q?Q
1rank of the first correct answer for q
 
0.4
0.5
0 1 2 3 4 5
M
R
R
Random Walk Speedup
Exact
Independent Fingerprinting
Low Variance Fingerprinting
Independent Filtering
Low Variance Filtering
10k
1k
100
10k
1k
100k
Figure 2: Compare inference speed and quality over 96
tasks. The speedup is relative to exact inference, which is
on average 23ms per query.
sampling can improve prediction for both finger
printing and particle filtering. The numbers on the
curves indicate the number of particles (or walkers).
When using a large number of particles, the particle
filtering methods converge to the exact inference.
Interestingly, when using a large number of walkers,
the finger printing methods produce even better
prediction quality than exact inference. Lao and
Cohen noticed a similar improvement on retrieval
tasks, and conjectured that it is because the sampling
inference imposes a regularization penalty on longer
relation paths (2010a).
3.2 Evaluation by Mechanical Turk
The cross-validation result above assumes that the
knowledge base is complete and correct, which
we know to be untrue. To accurately compare
PRA and N-FOIL?s ability to reliably infer new
beliefs from an imperfect knowledge base, we
use human assessments obtained from Amazon
Mechanical Turk. To limit labeling costs, and
since our goal is to improve the performance of
NELL, we do not include RWR-based approaches
in this comparison. Among all the 24 functional
predicates, N-FOIL discovers confident rules for
8 of them (it produces no result for the other 16
predicates). Therefore, we compare the quality
of PRA to N-FOIL on these 8 predicates only.
Among all the 72 non-functional predicates?which
535
Table 3: The top two weighted PRA paths for tasks on which N-FOIL discovers confident rules. c stands for concept.
ID PRA Path (Comment)
athletePlaysForTeam
1 c athletePlaysInLeague???????????????? c leaguePlayers?????????? c athletePlaysForTeam???????????????? c (teams with many players in the athlete?s league)
2 c athletePlaysInLeague???????????????? c leagueTeams?????????? c teamAgainstTeam????????????? c (teams that play against many teams in the athlete?s league)
athletePlaysInLeague
3 c athletePlaysSport????????????? c players?????? c athletePlaysInLeague???????????????? c (the league that players of a certain sport belong to)
4 c isa??? c isa?1????? c athletePlaysInLeague???????????????? c (popular leagues with many players)
athletePlaysSport
5 c isa??? c isa?1????? c athletePlaysSport????????????? c (popular sports of all the athletes)
6 c athletePlaysInLeague???????????????? c superpartOfOrganization?????????????????? c teamPlaysSport???????????? c (popular sports of a certain league)
stadiumLocatedInCity
7 c stadiumHomeTeam?????????????? c teamHomeStadium?????????????? c stadiumLocatedInCity???????????????? c (city of the stadium with the same team)
8 c latitudeLongitude????????????? c latitudeLongitudeOf??????????????? c stadiumLocatedInCity???????????????? c (city of the stadium with the same location)
teamHomeStadium
9 c teamPlaysInCity????????????? c cityStadiums?????????? c (stadiums located in the same city with the query team)
10 c teamMember?????????? c athletePlaysForTeam???????????????? c teamHomeStadium?????????????? c (home stadium of teams which share players with the query)
teamPlaysInCity
11 c teamHomeStadium?????????????? c stadiumLocatedInCity???????????????? c (city of the team?s home stadium)
12 c teamHomeStadium?????????????? c stadiumHomeTeam?????????????? c teamPlaysInCity????????????? c (city of teams with the same home stadium as the query)
teamPlaysInLeague
13 c teamPlaysSport???????????? c players?????? c athletePlaysInLeague???????????????? c (the league that the query team?s members belong to)
14 c teamPlaysAgainstTeam????????????????? c teamPlaysInLeague?????????????? c (the league that the query team?s competing team belongs to)
teamPlaysSport
15 c isa??? c isa?1????? c teamPlaysSport???????????? c (sports played by many teams)
16 c teamPlaysInLeague?????????????? c leagueTeams?????????? c teamPlaysSport???????????? c (the sport played by other teams in the league)
Table 4: Amazon Mechanical Turk evaluation for the promoted knowledge. Using paired t-test at task level, PRA is
not statistically different from N-FOIL for p@10 (p-value=0.3), but is significantly better for p@100 (p-value=0.003)
PRA N-FOIL
Task Pmajority #Paths p@10 p@100 p@1000 #Rules #Query p@10 p@100 p@1000
athletePlaysForTeam 0.07 125 0.4 0.46 0.66 1(+1) 7 0.6 0.08 0.01
athletePlaysInLeague 0.60 15 1.0 0.84 0.80 3(+30) 332 0.9 0.80 0.24
athletePlaysSport 0.73 34 1.0 0.78 0.70 2(+30) 224 1.0 0.82 0.18
stadiumLocatedInCity 0.05 18 0.9 0.62 0.54 1(+0) 25 0.7 0.16 0.00
teamHomeStadium 0.02 66 0.3 0.48 0.34 1(+0) 2 0.2 0.02 0.00
teamPlaysInCity 0.10 29 1.0 0.86 0.62 1(+0) 60 0.9 0.56 0.06
teamPlaysInLeague 0.26 36 1.0 0.70 0.64 4(+151) 30 0.9 0.18 0.02
teamPlaysSport 0.42 21 0.7 0.60 0.62 4(+86) 48 0.9 0.42 0.02
average 0.28 43 0.79 0.668 0.615 91 0.76 0.38 0.07
teamMember 0.01 203 0.8 0.64 0.48
companiesHeadquarteredIn 0.05 42 0.6 0.54 0.60
publicationJournalist 0.02 25 0.7 0.70 0.64
producedBy 0.19 13 0.5 0.58 0.68 N-FOIL does not produce results
competesWith 0.19 74 0.6 0.56 0.72 for non-functional predicates
hasOfficeInCity 0.03 262 0.9 0.84 0.60
teamWonTrophy 0.24 56 0.5 0.50 0.46
worksFor 0.13 62 0.6 0.60 0.74
average 0.11 92 0.650 0.620 0.615
536
N-FOIL cannot be applied to?PRA exhibits a wide
range of performance in cross-validation. The are 43
tasks for which PRA obtains MRR higher than 0.4
and builds a model with more than 10 path features.
We randomly sampled 8 of these predicates to be
evaluated by Amazon Mechanical Turk.
Table 3 shows the top two weighted PRA features
for each task on which N-FOIL can successfully
learn rules. These PRA rules can be categorized into
broad coverage rules which behave like priors over
correct answers (e.g. 1-2, 4-6, 15), accurate rules
which leverage specific relation sequences (e.g. 9,
11, 14), rules which leverage information about the
synonyms of the query node (e.g. 7-8, 10, 12),
and rules which leverage information from a local
neighborhood of the query node (e.g. 3, 12-13, 16).
The synonym paths are useful, because an entity
may have multiple names on the web. We find
that all 17 general rules (no specialization) learned
by N-FOIL can be expressed as length two relation
paths such as path 11. In comparison, PRA explores
a feature space with many length three paths.
For each relation R to be evaluated, we generate
test queries s which belong to domain(R). Queries
which appear in the training set are excluded. For
each query node s, we applied a trained model
(either PRA or N-FOIL) to generate a ranked list
of candidate t nodes. For PRA, the candidates
are sorted by their scores as in Eq. (6). For
N-FOIL, the candidates are sorted by the estimated
accuracies of the rules as in Eq. (2) (which generate
the candidates). Since there are about 7 thousand
(and 13 thousand) test queries s for each functional
(and non-functional) predicate R, and there are
(potentially) thousands of candidates t returned for
each query s, we cannot evaluate all candidates of
all queries. Therefore, we first sort the queries s for
each predicate R by the scores of their top ranked
candidate t in descending order, and then calculate
precisions at top 10, 100 and 1000 positions for the
list of result R(sR,1, tR,11 ), R(sR,2, tR,21 ), ..., where
sR,1 is the first query for predicate R, tR,11 is its first
candidate, sR,2 is the second query for predicate R,
tR,21 is its first candidate, so on and so forth. To
reduce the labeling load, we judge all top 10 queries
for each predicate, but randomly sample 50 out of
the top 100, and randomly sample 50 out of the
Table 5: Comparing Mechanical Turk workers? voted
assessments with our gold standard labels based on 100
samples.
AMT=F AMT=T
Gold=F 25% 15%
Gold=T 11% 49%
top 1000. Each belief is evaluated by 5 workers
at Mechanical Turk, who are given assertions like
?Hines Ward plays for the team Steelers?, as well
as Google search links for each entity, and the
combination of both entities. Statistics shows
that the workers spend on average 25 seconds to
judge each belief. We also remove some workers?
judgments which are obviously incorrect5. We
sampled 100 beliefs, and compared their voted result
to gold-standard labels produced by one author of
this paper. Table 5 shows that 74% of the time the
workers? voted result agrees with our judgement.
Table 4 shows the evaluation result. The
Pmajority column shows for each predicate the
accuracy achieved by the majority prediction: given
a query R(a, ?), predict the b that most often
satisfies R over all possible a in the knowledge
base. Thus, the higher Pmajority is, the simpler
the task. Predicting the functional predicates
is generally easier predicting the non-functional
predicates. The #Query column shows the number
of queries on which N-FOIL is able to match any
of its rules, and hence produce a candidate belief.
For most predicates, N-FOIL is only able to produce
results for at most a few hundred queries. In
comparison, PRA is able to produce results for 6,599
queries on average for each functional predicate, and
12,519 queries on average for each non-functional
predicate. Although the precision at 10 (p@10) of
N-FOIL is comparable to that of PRA, precision
at 10 and at 1000 (p@100 and p@1000) are much
lower6.
The #Path column shows the number of paths
learned by PRA, and the #Rule column shows the
number of rules learned by N-FOIL, with the num-
bers before brackets correspond to unspecialized
rules, and the numbers in brackets correspond to
5Certain workers label all the questions with the same
answer
6If a method makes k predictions, and k < n, then p@n is
the number correct out of the k predictions, divided by n
537
specialized rules. Generally, specialized rules have
much smaller recall than unspecialized rules. There-
fore, the PRA approach achieves high recall partially
by combining a large number of unspecialized paths,
which correspond to unspecialized rules. However,
learning more accurate specialized paths is part of
our future work.
A significant advantage of PRA over N-FOIL is
that it can be applied to non-functional predicates.
The last eight rows of Table 4 show PRA?s
performance on eight of these predicates. Compared
to the result on functional predicates, precisions
at 10 and at 100 of non-functional predicates
are slightly lower, but precisions at 1000 are
comparable. We note that for some predicates
precision at 1000 is better than at 100. After
some investigation we found that for many relations,
the top portion of the result list is more diverse:
i.e. showing products produced by different com-
panies, journalist working at different publications.
While the lower half of the result list is more
homogeneous: i.e. showing relations concentrated
on one or two companies/publications. On the
other hand, through the process of labeling the
Mechanical Turk workers seem to build up a prior
about which company/publication is likely to have
correct beliefs, and their judgments are positively
biased towards these companies/publications. These
two factors combined together result in positive bias
towards the lower portion of the result list. In future
work we hope to design a labeling strategy which
avoids this bias.
4 Conclusions and Future Work
We have shown that a soft inference procedure based
on a combination of constrained, weighted, random
walks through the knowledge base graph can be
used to reliably infer new beliefs for the knowledge
base. We applied this approach to a knowledge
base of approximately 500,000 beliefs extracted
imperfectly from the web by NELL. This new
system improves significantly over NELL?s earlier
Horn-clause learning and inference method: it
obtains nearly double the precision at rank 100. The
inference and learning are both very efficient?our
experiment shows that the inference time is as fast
as 10 milliseconds per query on average, and the
training for a predicate takes only a few seconds.
There are several prominent directions for future
work. First, inference starting from both the query
nodes and target nodes (Richards and Mooney,
1992) can be much more efficient in discovering
long paths than just inference from the query nodes.
Second, inference starting from the target nodes
of training queries is a potential way to discover
specialized paths (with grounded nodes). Third,
generalizing inference paths to inference trees or
graphs can produce more expressive random walk
inference models. Overall, we believe that random
walk is a promising way to scale up relational
learning to domains with very large data sets.
Acknowledgments
This work was supported by NIH under grant
R01GM081293, by NSF under grant IIS0811562,
by DARPA under awards FA8750-08-1-0009 and
AF8750-09-C-0179, and by a gift from Google.
We thank Geoffrey J. Gordon for the suggestion
of applying low variance sampling to random walk
inference. We also thank Bryan Kisiel for help with
the NELL system.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries - DL ?00, pages 85?94, New York, New York,
USA. ACM Press.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
Information Extraction from the Web. In IJCAI, pages
2670?2676.
Gaurav Bhalotia, Arvind Hulgeri, Charuta Nakhe,
Soumen Chakrabarti, and S. Sudarshan. 2002.
Keyword searching and browsing in databases using
banks. ICDE, pages 431?440.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the eleventh annual conference on
Computational learning theory - COLT? 98, pages
92?100, New York, New York, USA. ACM Press.
MJ Cafarella, M Banko, and O Etzioni. 2006. Relational
Web Search. In WWW.
Jamie Callan and Mark Hoy. 2009. Clueweb09 data set.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
538
Mitchell. 2010. Toward an Architecture for
Never-Ending Language Learning. In AAAI.
William W. Cohen and David Page. 1995. Polyno-
mial learnability and inductive logic programming:
Methods and results. New Generation Comput.,
13(3&4):369?409.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.
Weld, and Er Yates. 2005. Unsupervised named-
entity extraction from the web: An experimental study.
Artificial Intelligence, 165:91?134.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW, pages 517?526.
Alpa Jain and Patrick Pantel. 2010. Factrank: Random
walks on a web of facts. In COLING, pages 501?509.
Ni Lao and William W. Cohen. 2010a. Fast query exe-
cution for retrieval models based on path-constrained
random walks. KDD.
Ni Lao and William W. Cohen. 2010b. Relational
retrieval using a combination of path-constrained
random walks. Machine Learning.
Einat Minkov and William W. Cohen. 2008. Learning
graph walk based similarity measures for parsed text.
EMNLP, pages 907?916.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In ACL.
J. Ross Quinlan and R. Mike Cameron-Jones. 1993.
FOIL: A Midterm Report. In ECML, pages 3?20.
Bradley L. Richards and Raymond J. Mooney. 1992.
Learning relations by pathfinding. In Proceedings
of the Tenth National Conference on Artificial Intel-
ligence (AAAI-92), pages 50?55, San Jose, CA, July.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling Textual Inference to the Web.
In EMNLP, pages 79?88.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic Taxonomy Induction from Heterogenous
Evidence. In ACL.
Sebastian Thrun, Wolfram Burgard, and Dieter Fox.
2005. Probabilistic Robotics (Intelligent Robotics and
Autonomous Agents). The MIT Press.
Alexander Yates, Michele Banko, Matthew Broadhead,
Michael J. Cafarella, Oren Etzioni, and Stephen
Soderland. 2007. TextRunner: Open Information
Extraction on the Web. In HLT-NAACL (Demonstra-
tions), pages 25?26.
John M. Zelle, Cynthia A. Thompson, Mary Elaine
Califf, and Raymond J. Mooney. 1995. Inducing
logic programs without explicit negative examples.
In Proceedings of the Fifth International Workshop
on Inductive Logic Programming (ILP-95), pages
403?416, Leuven, Belgium.
539
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1017?1026, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Reading The Web with Learned Syntactic-Semantic Inference Rules
Ni Lao1?, Amarnag Subramanya2, Fernando Pereira2, William W. Cohen1
1Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA
2Google Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA
nlao@cs.cmu.edu, {asubram, pereira}@google.com, wcohen@cs.cmu.edu
Abstract
We study how to extend a large knowledge
base (Freebase) by reading relational informa-
tion from a large Web text corpus. Previous
studies on extracting relational knowledge
from text show the potential of syntactic
patterns for extraction, but they do not exploit
background knowledge of other relations
in the knowledge base. We describe a
distributed, Web-scale implementation of a
path-constrained random walk model that
learns syntactic-semantic inference rules for
binary relations from a graph representation
of the parsed text and the knowledge base.
Experiments show significant accuracy im-
provements in binary relation prediction over
methods that consider only text, or only the
existing knowledge base.
1 Introduction
Manually-created knowledge bases (KBs) often lack
basic information about some entities and their
relationships, either because the information was
missing in the initial sources used to create the
KB, or because human curators were not confident
about the status of some putative fact, and so they
excluded it from the KB. For instance, as we will
see in more detail later, many person entries in
Freebase (Bollacker et al 2008) lack nationality
information. To fill those KB gaps, we might use
general rules, ideally automatically learned, such as
?if person was born in town and town is in country
?This research was carried out during an internship at
Google Research
then the person is a national of the country.? Of
course, rules like this may be defeasible, in this case
for example because of naturalization or political
changes. Nevertheless, many such imperfect rules
can be learned and combined to yield useful KB
completions, as demonstrated in particular with the
Path-Ranking Algorithm (PRA) (Lao and Cohen,
2010; Lao et al 2011), which learns such rules on
heterogenous graphs for link prediction tasks.
Alternatively, we may attempt to fill KB gaps by
applying relation extraction rules to free text. For
instance, Snow et al(2005) and Suchanek et al
(2006) showed the value of syntactic patterns in
extracting specific relations. In those approaches,
KB tuples of the relation to be extracted serve as
positive training examples to the extraction rule
induction algorithm. However, the KB contains
much more knowledge about other relations that
could potentially be helpful in improving relation
extraction accuracy and coverage, but that is not
used in such purely text-based approaches.
In this work, we use PRA to learn weighted
rules (represented as graph path patterns) that
combine both semantic (KB) and syntactic infor-
mation encoded respectively as edges in a graph-
structured KB, and as syntactic dependency edges
in dependency-parsed Web text. Our approach can
easily incorporate existing knowledge in extraction
tasks, and its distributed implementation scales to
the whole of the Freebase KB and 60 million parsed
documents. To the best of our knowledge, this is the
first successful attempt to apply relational learning
methods to heterogeneous data with this scale.
1017
1.1 Terminology and Notation
In this study, we use a simplified KB consisting of a
set C of concepts and a set R of labels. Each label r
denotes some binary relation partially represented in
the KB. The concrete KB is a directed, edge-labeled
graph G = (C, T ) where T ? C ? R ? C is the
set of labeled edges (also known as triples) (c, r, c?).
Each triple represents an instance r(c, c?) of the
relation r ? R. The KB may be incomplete, that
is, r(c, c?) holds in the real world but (c, r, c?) 6? T .
Our method will attempt to learn rules to infer such
missing relation instances by combining the KB
with parsed text.
We denote by r?1 the inverse relation of r:
r(c, c?) ? r?1(c?, c). For instance Parent?1 is
equivalent to Children. It is convenient to take G
as containing triple (c?, r?1, c) whenever it contains
triple (c, r, c?).
A path type in G is a sequence pi = ?r1, . . . , rm?.
An instance of the path type is a sequence of nodes
c0, . . . , cm such that ri(ci?1, ci). For instance, ?the
persons who were born in the same town as the
query person?, and ?the nationalities of persons who
were born in the same town as the query person? can
be reached respectively through paths matching the
following types
pi1 :
?
BornIn,BornIn?1
?
pi2 :
?
BornIn,BornIn?1,Nationality
?
1.2 Learning Syntactic-Semantic Rules with
Path-Constrained Random Walks
Given a query concept s ? C and a relation
r ? R, PRA begins by enumerating a large set of
bounded-length path types. These path types are
treated as ranking ?experts,? each generating some
random instance of the path type starting from s, and
ranking end nodes t by their weights in the resulting
distribution. Finally, PRA combines the weights
contributed by different ?experts? by using logistic
regression to predict the probability that the relation
r(s, t) holds.
In this study, we test the hypothesis that PRA can
be used to find useful ?syntactic-semantic patterns?
? that is, patterns that exploit both semantic
and syntactic relationships, thereby using semantic
knowledge as background in interpreting syntactic
 
wrote
She
Mention
dobj
Charlotte
was
nsubj
nsubj
Jane Eyre
Charlotte
Bronte
Mention
Jane Eyre
Mention
Coreference Resolution
Entity 
Resolution
Freebase
News Corpus
Dependency Trees
Write
Patrick Bront?HasFather
?
Profession
Writer
Figure 1: Knowledge base and parsed text as a labeled
graph. For clarity, some word nodes are omitted.
relationships. As shown in Figure 1, we extend the
KB graph G with nodes and edges from text that
has been syntactically analyzed with a dependency
parser1 and where pronouns and other anaphoric
referring expressions have been clustered with their
antecedents. The text nodes are word/phrase
instances, and the edges are syntactic dependencies
labeled by the corresponding dependency type.
Mentions of entities in the text are linked to KB
concepts by mention edges created by an entity
resolution process.
Given for instance the query
Profession(CharlotteBronte, ?), PRA produces
a ranked list of answers that may have the relation
Profession with the query node CharlotteBronte.
The features used to score answers are the
random walk probabilities of reaching a certain
profession node from the query node by paths
with particular path types. PRA can learn path
types that combine background knowledge in
the database with syntactic patterns in the text
corpus. We now exemplify some path types
involving relations described in Table 3. Type
?
M, conj,M?1,Profession
?
is active (matches
paths) for professions of persons who are mentioned
in conjunction with the query person as in
?collaboration between McDougall and Simon
1Stanford dependencies (de Marneffe and Manning, 2008).
1018
Philips?. For a somewhat subtler example, type
?
M,TW,CW?1,Profession?1,Profession
?
is active
for persons who are mentioned by their titles as in
?President Barack Obama?. The type subsequence
?
Profession?1,Profession
?
ensures that only
profession concepts are activated. The features
generated from these path types combine syntactic
dependency relations (conj) and textual information
relations (TW and CW) with semantic relations in
the KB (Profession).
Experiments on three Freebase relations (profes-
sion, nationality and parents) show that exploiting
existing background knowledge as path features
can significantly improve the quality of extraction
compared with using either Freebase or the text
corpus alone.
1.3 Related Work
Information extraction from varied unstructured and
structured sources involves both complex relational
structure and uncertainty at all levels of the extrac-
tion process. Statistical Relational Learning (SRL)
seeks to combine statistical and relational learning
methods to address such tasks. However, most SRL
approaches (Friedman et al 1999; Richardson and
Domingos, 2006) suffer the complexity of inference
and learning when applied to large scale problems.
Recently, Lao and Cohen (2010) introduced Path
Ranking algorithm, which is applicable to larger
scale problems such as literature recommendation
(Lao and Cohen, 2010) and inference on a large
knowledge base (Lao et al 2011).
Much of the previous work on automatic relation
extraction was based on certain lexico-syntactic
patterns. Hearst (1992) first noticed that patterns
such as ?NP and other NP? and ?NP such as NP?
often imply hyponym relations (NP here refers to
a noun phrase). However, such approaches to
relation extraction are limited by the availability of
domain knowledge. Later systems for extracting
arbitrary relations from text mostly use shallow
surface text patterns (Etzioni et al 2004; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002).
The idea of using sequences of dependency edges
as features for relation extraction was explored by
Snow et al(2005) and Suchanek et al(2006). They
define features to be shortest paths on dependency
trees which connect pairs of NP candidates.
This study is most closely related to work of
Mintz et al(2009), who also study the problem of
extending Freebase with extraction from parsed text.
As in our work, they use a logistic regression model
with path features. However, their approach does not
exploit existing knowledge in the KB. Furthermore,
their path patterns are used as binary-values features.
We show experimentally that fractional-valued
features generated by random walks provide much
higher accuracy than binary-valued ones.
Culotta et al(2006)?s work is similar to our
approach in the sense of relation extraction by
discovering relational patterns. However while
they focus on identifying relation mentions in text
(microreading),this work attempts to infer new
tuples by gathering path evidence over the whole
corpus (macroreading). In addition, their work
involves a few thousand examples, while we aim for
Web-scale extraction.
Do and Roth (2010) use a KB (YAGO) to
aid the generation of features from free text.
However their method is designed specifically for
extracting hierarchical taxonomic structures, while
our algorithm can be used to discover relations for
general general graph-based KBs.
In this paper we extend the PRA algorithm along
two dimensions: combining syntactic and semantic
cues in text with existing knowledge in the KB;
and a distributed implementation of the learning and
inference algorithms that works at Web scale.
2 Path Ranking Algorithm
We briefly review the Path Ranking algorithm
(PRA), described in more detail by Lao and Cohen
(2010). Each path type pi = ?r1, r2, ..., r`? specifies
a real-valued feature. For a given query-answer node
pair (s, t), the value of the feature pi is P (s? t;pi),
the probability of reaching t from s by a random
walk that instantiates the type. More specifically,
suppose that the random walk has just reached vi by
traversing edges labeled r1, . . . , ri with s=v0. Then
vi+1 is drawn at random from all nodes reachable
from vi by edges labeled ri+1. A path type pi is
active for pair (s, t) if P (s? t;pi) > 0.
Let B = {?, pi1, ..., pin} be the set of all path
types of length no greater than ` that occur in
the graph together with the dummy type ?, which
1019
represents the bias feature. For convenience, we set
P (s ? t;?) = 1 for any nodes s, t. The score for
whether query node s is related to another node t by
relation r is given by
score(s, t) =
?
pi?B
P (s? t;pi)?pi ,
where ?pi is the weight of feature pi. The model
parameters to be learned are the vector ? =
??pi?pi?B . The procedures used to discover B and
estimate ? are described in the following. Finally,
note that we train a separate PRA model for each
relation r.
Path Discovery: Given a graph and a target
relation r, the total number of path types is an
exponential function of the maximum path length
` and considering all possible paths would be
computationally very expensive. As a result, B is
constructed using only path types that satisfy the
following two constraints:
1. the path type is active for more than K training
query nodes, and
2. the probability of reaching any correct target
node t is larger than a threshold ? on average
for the training query nodes s.
We will discuss how K, ? and the training queries
are chosen in Section 5. In addition to making the
training more efficient, these constraints are also
helpful in removing low quality path types.
Training Examples: For each relation r of inter-
est, we start with a set of node pairs Sr = {(si, ti)}.
From Sr, we create the training setDr = {(xi, yi)},
where xi = ?P (si ? ti;pi)?pi?B is the vector
of path feature values for the pair (si, ti), and yi
indicates whether r(si, ti) holds.
Following previous work (Lao and Cohen, 2010;
Mintz et al 2009), node pairs that are in r in
the KB are legitimate positive training examples2.
One can generate negative training examples by
considering all possible pairs of concepts whose
type is compatible with r (as given by the schema)
and are not present in the KB. However this
2In our experiments we subsample the positive examples.
See section 3.2 for more details.
procedure leads to a very large number of negative
examples (e.g., for the parents relation, any pair of
person concepts which are related by this relation
would be valid negative examples) which not only
makes training very expensive but also introduces
an incorrect bias in the training set. Following
Lao and Cohen (2010) we use a simple biased
sampling procedure to generate negative examples:
first, the path types discovered in the previous (path
discovery) step are used to construct an initial PRA
model (all feature weights are set to 1.0); then, for
each query node si, this model is used to retrieve
candidate answer nodes, which are then sorted in
descending order by their scores; finally, nodes at
the k(k + 1)/2-th positions are selected as negative
samples, where k = 0, 1, 2, ....
Logistic Regression Training: Given a training
set D, we estimate parameters ? by maximizing the
following objective
F(?) =
1
|D|
?
(x,y)?D
f(x, y;?)? ?1???1 ? ?2???22
where ?1 and ?2 control the strength of the L1-
regularization which helps with structure selection
and L22-regularization which prevents overfitting.
The log-likelihood f(x, y;?) of example (x, y) is
given by
f(x, y,?) = y ln p(x,?) + (1? y) ln(1? p(x,?))
p(x,?) =
exp(?Tx)
1 + exp(?Tx)
.
Inference: After a model is trained for a relation
r in the knowledge base, it can be used to produce
new instances of r. We first generate unlabeled
queries s which belong to the domain of r. Queries
which appear in the training set are excluded. For
each unlabeled query node s, we apply the trained
PRA model to generate a list of candidate t nodes
together with their scores. We then sort all the
predictions (s, t) by their scores in descending order,
and evaluate the top ones.
3 Extending PRA
As described in the previous section, the PRA model
is trained on positive and negative queries generated
from the KB. As Freebase contains millions of
1020
concepts and edges, training on all the generated
queries is computationally challenging. Further,
we extend the Freebase graph with parse paths of
mentions of concepts in Freebase in millions of Web
pages. Yet another issue is that the training queries
generated using Freebase are inherently biased
towards the distribution of concepts in Freebase
and may not reflect the distribution of mentions of
these concepts in text data. As one of the goals of
our approach is to learn relation instances that are
missing in Freebase, training on such a set biased
towards the distribution of concepts in Freebase may
not lead to good performance. In this section we
explain how we modified the PRA algorithm to
address those issues.
3.1 Scaling Up
Most relations in Freebase have a large set of
existing triples. For example, for the profession
relation, there are around 2 million persons in
Freebase, and about 0.3 million of them have known
professions. This results in more than 0.3 million
training queries (persons), each with one or more
positive answers (professions), and many negative
answers, which make training computationally
challenging. Generating all the paths for millions
of queries over a graph with millions of concepts
and edges further complicates the computational
issues. Incorporating the parse path features from
the text only exacerbates the matter. Finally once we
have trained a PRA model for a given relation, say
profession, we would like to infer the professions for
all the 1.7 million persons whose professions are not
known to Freebase (and possibly predict changes to
the profession information of the 0.3 million people
whose professions were given).
We use distributed computing to deal with the
large number of training and prediction queries
over a large graph. A key observation is that the
different stages of the PRA algorithm are based
on independent computations involving individual
queries. Therefore, we can use the MapReduce
framework to distribute the computation (Dean and
Ghemawat, 2008). For path discovery, we modify
Lao et als path finding (2011) approach to decouple
the queries: instead of using one depth-first search
that involves all the queries, we first find all paths
up to certain length for each query node in the
map stage, and then collect statistics for each path
from all the query nodes in the reduce stage. We
used a 500-machine, 8GB/machine cluster for these
computations.
Another challenge associated with applying PRA
to a graph constructed using a large amounts of
text is that we cannot load the entire graph on a
single machine. To circumvent this problem, we first
index all parsed sentences by the concepts that they
mention. Therefore, to perform a random walk for a
query concept s, we only load the sentences which
mention s.
3.2 Sampling Training Data
Using the r-edges in the KB as positive examples
distorts the training set. For example, for the
profession relation, there are 0.3 million persons
for whom Freebase has profession information, and
amongst these 0.24 million are either politicians
or actors. This may not reflect the distribution
of professions of persons mentioned in Web data.
Using all of these as training queries will most
certainly bias the trained model towards these
professions as PRA is trained discriminatively. In
other words, training directly with this data would
lead to a model that is more likely to predict
professions that are popular in Freebase. To avoid
this distortion, we use stratified sampling. For each
relation r and concept t ? C, we count the number
of r edges pointing to t
Nr,t = |{(s, r, t) ? T}| .
Given a training query (s, r, t) we sample it
according to
Pr,t = min
(
1,
?
m+Nr,t
Nr,t
)
We fix m = 100 in our experiments. If we take the
profession relation as an example, the above implies
that for popular professions, we only sample about
?
Nr,t out of the Nr,t possible queries that end in t,
whereas for the less popular professions we would
accept all the training queries.
3.3 Text Graph Construction
As we are processing Web text data (see following
section for more detail), the number of mentions
1021
of a concept follows a somewhat heavy-tailed
distribution: there are a small number of very
popular concepts (head) and a large number of not
so popular concepts (tail). For instance the concept
BarackObama is mentioned about 8.9 million times
in our text corpus. To prevent the text graph from
being dominated by the head concepts, for each
sentence that mentions concept c ? C, we accept
it as part of the text graph with probability:
Pc = min
(
1,
?
k + Sc
Sc
)
where Sc is the number of sentences in which c is
mentioned in the whole corpus. In our experiments
we use k = 105. This means that if Sc  k, then we
only sample about
?
Sc of the sentences that contain
a mention of the concept, while if Sc  k, then all
mentions of that concept will likely be included.
4 Datasets
We use Freebase as our knowledge base. Freebase
data is harvested from many sources, including
Wikipedia, AMG, and IMDB.3 As of this writing,
it contains more than 21 million concepts and 70
million labeled edges. For a large majority of con-
cepts that appear both in Freebase and Wikipedia,
Freebase maintains a link to the Wikipedia page of
that concept.
We also collect a large Web corpus and identify
60 million pages that mention concepts relevant
to this study. The free text on those pages
are POS-tagged and dependency parsed with an
accuracy comparable to that of the current Stanford
dependency parser (Klein and Manning, 2003). The
parser produces a dependency tree for each sentence
with each edge labeled with a standard dependency
tag (see Figure 1).
In each of the parsed documents, we use POS tags
and dependency edges to identify potential referring
noun phrases (NPs). We then use a within-document
coreference resolver comparable to that of Haghighi
and Klein (2009) to group referring NPs into
co-referring clusters. For each cluster that contains a
proper-name mention, we find the Freebase concept
or concepts, if any, with a name or alias that matches
3www.wikipedia.org, www.allmusic.com, www.
imdb.com.
Table 1: Size of training and test sets for each relation.
Task Training Set Test Set
Profession 22,829 15,219
Nationality 14,431 9,620
Parents 21,232 14,155
the mention. If a cluster has multiple possible
matching Freebase concepts, we choose a single
sense based on the following simple model. For
each Freebase concept c ? C, we computeN(c,m),
the number of times the concept c is referred by
mention m by using both the alias information
in Freebase and the anchors of the corresponding
Wikipedia page for that concept. Based on N(c,m)
we can calculate the empirical probability p(c|m) =
N(c,m)/
?
c? N(c
?,m). If u is a cluster with
mention set M(u) in the document, and C(m) the
set of concepts in KB with name or alias m, we
assign u to concept c? = argmax
c?C(m),m?M(u)
p(c|m),
provided that there exists at least one c ? C(m) and
m ? M(u) such that p(c|m) > 0. Note that M(c)
only contains the proper-name mentions in cluster c.
5 Results
We use three relations profession, nationality and
parents for our experiments. For each relation, we
select its current set of triples in Freebase, and apply
the stratified sampling (Section 3.2) to each of the
three triple sets. The resulting triple sets are then
randomly split into training (60% of the triples) and
test (the remaining triples). However, the parents
relation yields 350k triples after stratified sampling,
so to reduce experimental effort we further randomly
sub-sample 10% of that as input to the train-test
split. Table 1 shows the sizes of the training and
test sets for each relation.
To encourage PRA to find paths involving the
text corpus, we do not count relation M (which
connects concepts to their mentions) or M?1 when
calculating path lengths. We use L1/L22-regularized
logistic regression to learn feature weights. The
PRA hyperparameters (? and K as defined in
Section 2) and regularizer hyperparameters are
tuned by threefold cross validation (CV) on the
training set. We average the models across all
the folds and choose the model that gives the best
1022
Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and
KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and
text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results
shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions
significance test.
Task KB Text KB+Text KB+Text[b]
Profession 0.532 0.516 0.583 0.453
Nationality 0.734 0.729 0.812 0.693
Parents 0.329 0.332 0.392 0.319
performance on the training set for each relation.
We report results of two evaluations. First, we
evaluate the performance of the PRA algorithm
when trained on a subset of existing Freebase facts
and tested on the rest. Second, we had human
annotators verify facts proposed by PRA that are not
in Freebase.
5.1 Evaluation with Existing Knowledge
Previous work in relation extraction from parsed
text (Mintz et al 2009) has mostly used binary
features to indicate whether a pattern is present in
the sentences where two concepts are mentioned.
To investigate the benefit of having fractional valued
features generated by random walks (as in PRA), we
also evaluate a binarized PRA approach, for which
we use the same syntactic-semantic pattern features
as PRA does, but binarize the feature values from
PRA: if the original fractional feature value was
zero, the feature value is set to zero (equivalent to
not having the feature in that example), otherwise it
is set to 1.
Table 2 shows a comparison of the results
obtained using the PRA algorithm trained using
only Freebase (KB), using only the text corpus
graph (Text), trained with both Freebase and the
text corpus (KB+Text) and the binarized PRA
algorithm using both Freebase and the text corpus
(KB+Text[b]). We report Mean Reciprocal Rank
(MRR) where, given a set of queries Q,
MRR =
1
|Q|
?
q?Q
1
rank of q?s first correct answer
.
Comparing the results of first three columns we
see that combining Freebase and text achieves
significantly better results than using either Freebase
or text alone. Further comparing the results of last
two columns we also observe a significant drop in
MRR for the binarized version of PRA. This clearly
shows the importance of using the random walk
probabilities. It can also be seen that the MRR for
the parents relation is lower than those for other
relations. This is mainly because there are larger
number of potential answers for each query node of
Parent relation than for each query node of the other
two relations ? all persons in Freebase versus all
professions or nationalities. Finally, it is important
to point out that our evaluations are actually lower
bounds of actual performance, because, for instance,
a person might have a profession besides the ones in
Freebase and in such cases, this evaluation does not
give any credit for predicting those professions ?
they are treated as errors. We try to address this issue
with the manual evaluations in the next section.
Table 2 only reports results for the maximum path
length ` = 4 case. We found that shorter maximum
path lengths give worse results: for instance, with
` = 3 for the profession relation, MRR drops to
0.542, from 0.583 for ` = 4 when using both
Freebase and text. This difference is significant
at the 0.0001 level according to a difference of
proportions test. Further we find that using longer
path length takes much longer time to train and test,
but does not lead to significant improvements over
the ` = 4 case. For example, for profession, ` = 5
gives a MRR of 0.589.
Table 3 shows the top weighted features that
involve text edges for PRA models trained on both
Freebase and the text corpus. To make them
easier to understand, we group them based on their
functionality. For the profession and nationality
tasks, the conjunction dependency relation (in group
1,4) plays an important role: these features first find
persons mentioned in conjunction with the query
1023
Table 3: Top weighted path types involving text edges for each task grouped according to functionality. M relations
connect each concept in knowledge base to its mentions in the corpus. TW relations connect each token in a sentence to
the words in the text representation of this token. CW relations connect each concept in knowledge base to the words
in the text representation of this concept. We use lower case names to denote dependency edges, word capitalized
names to denote KB edges, and ??1 ? to denote the inverse of a relation.
Profession Top Weighted Features Comments
1
?
M, conj,M?1,Profession
?
Professions of persons mentioned in conjunction
with the query person: ?McDougall and Simon
Phillips collaborated ...?
?
M, conj?1,M?1,Profession
?
2
?
M,TW,CW?1,Profession?1,Profession
?
Active if a person is mentioned by his profession:
?The president said ...?
3
?
M,TW,TW?1,M?1,Children,Profession
?
First find persons with similar names or
mentioned in similar ways, then aggregate the
professions of their children/parents/advisors:
starting from the concept BarackObama, words
such as ?Obama?, ?leader?, ?president?, and
?he? are reachable through path ?M,TW?
?
M,TW,TW?1,M?1,Parents,Profession
?
?
M,TW,TW?1,M?1,Advisors,Profession
?
Nationality Top Weighted Features Comments
4
?
M, conj,TW,CW?1,Nationality
?
The nationalities of persons mentioned in
conjunction with the query person: ?McDougall
and Simon Phillips collaborated ...?
?
M, conj?1,TW,CW?1,Nationality
?
5
?
M, nc?1,TW,CW?1,Nationality
?
The nationalities of persons mentioned close to
the query person through other dependency
relations.
?
M, tmod?1,TW,CW?1,Nationality
?
?
M, nn,TW,CW?1,Nationality
?
6
?
M, poss, poss?1,M?1,PlaceOfBirth,ContainedBy
?
The birth/death places of the query person with
restrictions to different syntactic constructions.
?
M, title, title?1,M?1,PlaceOfDeath,ContainedBy
?
Parents Top Weighted Features Comments
7
?
M,TW,CW?1,Parents
?
The parents of persons with similar names or
mentioned in similar ways: starting from the
concept CharlotteBronte words such as
?Bronte?, ?Charlotte?, ?Patrick??, and ?she? are
reachable through path ?M,TW?.
8
?
M, nsubj, nsubj?1,TW,CW?1
?
Persons with similar names or mentioned in
similar ways to the query person with various
restrictions or expansions.
?
nsubj, nsubj?1
?
and
?
nc?1, nc
?
require the query to be subject and
noun compound respectively.
?
TW?1,TW
?
expands further by word similarities.
?
M, nsubj, nsubj?1,M?1,CW,CW?1
?
?
M, nc?1, nc,TW,CW?1
?
?
M,TW,CW?1
?
?
M,TW,TW?1,TW,CW?1
?
1024
person, and then find their professions or nation-
alities. The features in group 2 capture the fact
that sometimes people are mentioned by their pro-
fessions. The subpath
?
Profession?1,Profession
?
ensures that only profession related concepts are
activated. Features in group 3 first find persons
with similar names or mentioned in similar ways
to the query person, and then aggregate the
professions of their children, parents, or advisors.
Features in group 6 can be seen as special
versions of feature ?PlaceOfBirth,ContainedBy?
and ?PlaceOfDeath,ContainedBy?. The subpaths
?
M, poss, poss?1,M?1
?
and
?
M, title, title?1,M?1
?
return the random walks back to the query node only
if the mentions of the query node have poss (stands
for possessive modifier, e.g. ?Bill?s clothes?) or title
(stands for person?s title, e.g. ?President Obama?)
edges in text; otherwise these features are inactive.
Therefore, these features are active only for specific
subsets of queries. Features in group 8 generally find
persons with similar names or mentioned in similar
ways to the query person. However, they further
expand or restrict this person set in various ways.
Typically, each trained model includes hundreds
of paths with non-zero weights, so the bulk of
classifications are not based on a few high-precision-
recall patterns, but rather on the combination of
a large number of lower-precision high-recall or
high-precision lower-recall rules.
5.2 Manual Evaluation
We performed two sets of manual evaluations. In
each case, an annotator is presented with the triples
predicted by PRA, and asked if they are correct. The
annotator has access to the Freebase and Wikipedia
pages for the concepts (and is able to issue search
queries about the concepts).
In the first evaluation, we compared the perfor-
mance of two PRA models, one trained using the
stratified sampled queries and another trained using
a randomly sampled set of queries for the profession
relation. For each model, we randomly sample 100
predictions from the top 1000 predictions (sorted by
the scores returned by the model). We found that the
PRA model trained with stratified sampled queries
has 0.92 precision, while the other model has only
0.84 precision (significant at the 0.02 level). This
shows that stratified sampling leads to improved
Table 4: Human judgement for predicted new beliefs.
Task p@100 p@1k p@10k
Profession 0.97 0.92 0.84
Nationality 0.98 0.97 0.90
Parents 0.86 0.81 0.79
performance.
We also evaluated the new beliefs proposed by
the models trained for all the three relations using
stratified sampled queries. We estimated precision
for the top 100 predictions and randomly sampled
100 predictions each from the top 1,000 and 10,000
predictions. Here we use the PRA model trained
using both KB and text. The results of this
evaluation are shown in Table 4. It can be seen
that the PRA model is able to produce very high
precision predications even when one considers the
top 10,000 predictions.
Finally, note that our model is inductive. For
instance, for the profession relation, we are able to
predict professions for the around 2 million persons
in Freebase. The top 1000 profession facts extracted
by our system involve 970 distinct people, the top
10,000 facts involve 8,726 distinct people, and the
top 100,000 facts involve 79,885 people.
6 Conclusion
We have shown that path constrained random walk
models can effectively infer new beliefs from a
large scale parsed text corpus with background
knowledge. Evaluation by human annotators shows
that by combining syntactic patterns in parsed
text with semantic patterns in the background
knowledge, our model can propose new beliefs
with high accuracy. Thus, the proposed random
walk model can be an effective way to automate
knowledge acquisition from the web.
There are several interesting directions to con-
tinue this line of work. First, bidirectional search
from both query and target nodes can be an efficient
way to discover long paths. This would especially
useful for parsed text. Second, relation paths that
contain constant nodes (lexicalized features) and
conjunction of random walk features are potentially
very useful for extraction tasks.
1025
Acknowledgments
We thank Rahul Gupta, Michael Ringgaard, John
Blitzer and the anonymous reviewers for helpful
comments. The first author was supported by a
Google Research grant.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries, DL ?00, pages 85?94, New York, NY, USA.
ACM.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a
collaboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, SIGMOD ?08, pages 1247?1250, New York, NY,
USA. ACM.
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
296?303, New York City, USA, June. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Chris Manning.
2008. Stanford dependencies. http:
//www.tex.ac.uk/cgi-bin/texfaq2html?
label=citeURL.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113, January.
Quang Do and Dan Roth. 2010. Constraints based
taxonomic relation classification. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1099?1109, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in knowitall:
(preliminary results). In Proceedings of the 13th
international conference on World Wide Web, WWW
?04, pages 100?110, New York, NY, USA. ACM.
Nir Friedman, Lise Getoor, Daphne Koller, and Avi
Pfeffer. 1999. Learning Probabilistic Relational
Models. In IJCAI, volume 16, pages 1300?1309.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1152?1161, Singapore, August. Association for
Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proceedings
of COLING-92, pages 539?545. Association for
Computational Linguistics, August.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics, pages
423?430. Association for Computational Linguistics,
July.
Ni Lao and William Cohen. 2010. Relational retrieval
using a combination of path-constrained random
walks. Machine Learning, 81:53?67.
Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large
scale knowledge base. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 529?539, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel
Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a question answering
system. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 41?47, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107?
136.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
Le?on Bottou, editors, Advances in Neural Information
Processing Systems 17, pages 1297?1304, Cambridge,
MA. NIPS Foundation, MIT Press.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?06, pages 712?717, New York, NY, USA.
ACM.
1026
