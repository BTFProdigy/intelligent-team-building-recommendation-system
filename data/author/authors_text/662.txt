Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 601?608,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Comparison of Document, Sentence, and Term Event Spaces 
 
 
Catherine Blake 
School of Information and Library Science 
University of North Carolina at Chapel Hill 
North Carolina, NC 27599-3360 
cablake@email.unc.edu 
 
 
  
Abstract 
The trend in information retrieval sys-
tems is from document to sub-document 
retrieval, such as sentences in a summari-
zation system and words or phrases in 
question-answering system. Despite this 
trend, systems continue to model lan-
guage at a document level using the in-
verse document frequency (IDF). In this 
paper, we compare and contrast IDF with 
inverse sentence frequency (ISF) and in-
verse term frequency (ITF). A direct 
comparison reveals that all language 
models are highly correlated; however, 
the average ISF and ITF values are 5.5 
and 10.4 higher than IDF. All language 
models appeared to follow a power law 
distribution with a slope coefficient of 
1.6 for documents and 1.7 for sentences 
and terms. We conclude with an analysis 
of IDF stability with respect to random, 
journal, and section partitions of the 
100,830 full-text scientific articles in our 
experimental corpus.  
1 Introduction 
The vector based information retrieval model 
identifies relevant documents by comparing 
query terms with terms from a document corpus. 
The most common corpus weighting scheme is 
the term frequency (TF) x inverse document fre-
quency (IDF), where TF is the number of times a 
term appears in a document, and IDF reflects the 
distribution of terms within the corpus (Salton 
and Buckley, 1988). Ideally, the system should 
assign the highest weights to terms with the most 
discriminative power. 
One component of the corpus weight is the 
language model used. The most common lan-
guage model is the Inverse Document Fre-
quency (IDF), which considers the distribution 
of terms between documents (see equation (1)). 
IDF has played a central role in retrieval systems 
since it was first introduced more than thirty 
years ago (Sparck Jones, 1972).  
IDF(ti)=log2(N)?log2(ni)+1   (1) 
N is the total number of corpus 
documents; ni is the number of docu-
ments that contain at least one oc-
currence of the term ti; and ti is a 
term, which is typically stemmed. 
 
Although information retrieval systems are 
trending from document to sub-document re-
trieval, such as sentences for summarization and 
words, or phrases for question answering, sys-
tems continue to calculate corpus weights on a 
language model of documents. Logic suggests 
that if a system identifies sentences rather than 
documents, it should use a corpus weighting 
scheme based on the number of sentences rather 
than the number documents.  That is, the system 
should replace IDF with the Inverse Sentence 
Frequency (ISF), where N in (1) is the total 
number of sentences and ni is the number of sen-
tences with term i. Similarly, if the system re-
trieves terms or phrases then IDF should be re-
placed with the Inverse Term Frequency (ITF), 
where N in (1) is the vocabulary size, and ni is 
the number of times a term or phrases appears in 
the corpus. The challenge is that although docu-
ment language models have had unprecedented 
empirical success, language models based on a 
sentence or term do not appear to work well 
(Robertson, 2004).  
Our goal is to explore the transition from the 
document to sentence and term spaces, such that 
we may uncover where the language models start 
601
to break down. In this paper, we explore this goal 
by answering the following questions: How cor-
related are the raw document, sentence, and term 
spaces? How correlated are the IDF, ISF, and 
ITF values? How well does each language mod-
els conform to Zipf?s Law and what are the slope 
coefficients? How sensitive is IDF with respect 
to sub-sets of a corpus selected at random, from 
journals, or from document sections including 
the abstract and body of an article?  
This paper is organized as follows: Section 2 
provides the theoretical and practical implica-
tions of this study; Section 3 describes the ex-
perimental design we used to study document, 
sentence, and term, spaces in our corpora of 
more than one-hundred thousand full-text docu-
ments; Section 4 discusses the results; and Sec-
tion 5 draws conclusions from this study. 
2 Background and Motivation 
The transition from document to sentence to 
term spaces has both theoretical and practical 
ramifications. From a theoretical standpoint, the 
success of TFxIDF is problematic because the 
model combines two different event spaces ? the 
space of terms in TF and of documents in IDF. In 
addition to resolving the discrepancy between 
event spaces, the foundational theories in infor-
mation science, such as Zipf?s Law (Zipf, 1949) 
and Shannon?s Theory (Shannon, 1948) consider 
only a term event space. Thus, establishing a di-
rect connection between the empirically success-
ful IDF and the theoretically based ITF may en-
able a connection to previously adopted informa-
tion theories.  
 
0
5
10
15
20
25
0 5 10 15 20 25
log(Vocababulary Size (n))
lo
g(
C
or
pu
s 
S
iz
e 
(N
))
SS
SM
SL
MS
MM
ML
LS
LM
LL
first IDF 
paper
this 
paper
Document space dominates
Vocabulary space dominates
the web 
over time ?
 
Figure 1. Synthetic data showing IDF trends 
for different sized corpora and vocabulary. 
Understanding the relationship among docu-
ment, sentence and term spaces also has practical 
importance. The size and nature of text corpora 
has changed dramatically since the first IDF ex-
periments. Consider the synthetic data shown in 
Figure 1, which reflects the increase in both vo-
cabulary and corpora size from small (S), to me-
dium (M), to large (L). The small vocabulary 
size is from the Cranfield corpus used in Sparck 
Jones (1972), medium is from the 0.9 million 
terms in the Heritage Dictionary (Pickett 2000) 
and large is the 1.3 million terms in our corpus. 
The small number of documents is from the 
Cranfield corpus in Sparck Jones (1972), me-
dium is 100,000 from our corpus, and large is 1 
million 
As a document corpus becomes sufficiently 
large, the rate of new terms in the vocabulary 
decreases. Thus, in practice the rate of growth on 
the x-axis of Figure 1 will slow as the corpus size 
increases. In contrast, the number of documents 
(shown on the y-axis in Figure 1) remains un-
bounded. It is not clear which of the two compo-
nents in equation (1), the log2(N), which re-
flects the number of documents, or the 
log2(ni),which reflects the distribution of 
terms between documents within the corpus will 
dominate the equation. Our strategy is to explore 
these differences empirically. 
In addition to changes in the vocabulary size 
and the number of documents, the average num-
ber of terms per document has increased from 
7.9, 12.2 and 32 in Sparck Jones (1972), to 20 
and 32 in Salton and Buckley (1988), to 4,981 in 
our corpus. The transition from abstracts to full-
text documents explains the dramatic difference 
in document length; however, the impact with 
respect to the distribution of terms and motivates 
us to explore differences between the language 
used in an abstract, and that used in the body of a 
document.  
One last change from the initial experiments is 
a trend towards an on-line environment, where 
calculating IDF is prohibitively expensive. This 
suggests a need to explore the stability of IDF so 
that system designers can make an informed de-
cision regarding how many documents should be 
included in the IDF calculations. We explore the 
stability of IDF in random, journal, and docu-
ment section sub-sets of the corpus.   
3 Experimental Design 
Our goal in this paper is to compare and contrast 
language models based on a document with those 
based on a sentence and term event spaces. We 
considered several of the corpora from the Text 
Retrieval Conferences (TREC, trec.nist.gov); 
however, those collections were primarily news 
602
articles. One exception was the recently added 
genomics track, which considered full-text scien-
tific articles, but did not provide relevance judg-
ments at a sentence or term level. We also con-
sidered the sentence level judgments from the 
novelty track and the phrase level judgments 
from the question-answering track, but those 
were news and web documents respectively and 
we had wanted to explore the event spaces in the 
context of scientific literature. 
Table 1 shows the corpus that we developed 
for these experiments. The American Chemistry 
Society provided 103,262 full-text documents, 
which were published in 27 journals from 2000-
20041. We processed the headings, text, and ta-
bles using Java BreakIterator class to identify 
sentences and a Java implementation of the Por-
ter Stemming algorithm (Porter, 1980) to identify 
terms. The inverted index was stored in an Ora-
cle 10i database.  
 
 Docs Avg Tokens  
Journal # % Length Million   %  
ACHRE4 548 0.5 4923 2.7 1 
ANCHAM 4012 4.0 4860 19.5 4 
BICHAW 8799 8.7 6674 58.7 11 
BIPRET 1067 1.1 4552 4.9 1 
BOMAF6 1068 1.1 4847 5.2 1 
CGDEFU 566 0.5 3741 2.1 <1 
CMATEX 3598 3.6 4807 17.3 3 
ESTHAG 4120 4.1 5248 21.6 4 
IECRED 3975 3.9 5329 21.2 4 
INOCAJ 5422 5.4 6292 34.1 6 
JACSAT 14400  14.3 4349 62.6 12 
JAFCAU 5884 5.8 4185 24.6 5 
JCCHFF 500 0.5 5526 2.8 1 
JCISD8 1092 1.1 4931 5.4 1 
JMCMAR 3202 3.2 8809 28.2 5 
JNPRDF 2291 2.2 4144 9.5 2 
JOCEAH 7307 7.2 6605 48.3 9 
JPCAFH 7654 7.6 6181 47.3 9 
JPCBFK 9990 9.9 5750 57.4 11 
JPROBS 268 0.3 4917 1.3 <1 
MAMOBX 6887 6.8 5283 36.4 7 
MPOHBP 58 0.1 4868 0.3 <1 
NALEFD 1272 1.3 2609 3.3 1 
OPRDFK 858 0.8 3616 3.1 1 
ORLEF7 5992 5.9 1477 8.8 2 
Total 100,830    526.6  
Average 4,033 4.0 4,981 21.1
Std Dev 3,659 3.6 1,411 20.3
Table 1. Corpus summary. 
 
                                                 
1 Formatting inconsistencies precluded two journals and 
reduced the number of documents by 2,432. 
We made the following comparisons between 
the document, sentence, and term event spaces. 
(1) Raw term comparison 
A set of well-correlated spaces would enable 
an accurate prediction from one space to the 
next. We will plot pair-wise correlations between 
each space to reveal similarities and differences.  
This comparison reflects a previous analysis 
comprising a random sample of 193 words from 
a 50 million word corpus of 85,432 news articles 
(Church and Gale 1999). Church and Gale?s 
analysis of term and document spaces resulted in 
a p value of -0.994. Our work complements their 
approach by considering full-text scientific arti-
cles rather than news documents, and we con-
sider the entire stemmed term vocabulary in a 
526 million-term corpus. 
(2) Zipf Law comparison  
Information theory tells us that the frequency 
of terms in a corpus conforms to the power law 
distribution K/j? (Baeza-Yates and Ribeiro-Neto 
1999). Zipf?s Law is a special case of the power 
law, where ? is close to 1 (Zipf, 1949). To pro-
vide another perspective of the alternative 
spaces, we calculated the parameters of Zipf?s 
Law, K and ? for each event space and journal 
using the binning method proposed in (Adamic 
2000). By accounting for K, the slope as defined 
by ? will provide another way to characterize 
differences between the document, sentence and 
term spaces. We expect that all event spaces will 
conform to Zipf?s Law. 
(3) Direct IDF, ISF, and ITF comparison 
The log2(N) and  log2(ni) should allow a 
direct comparison between IDF, ISF and ITF. 
Our third experiment was to provide pair-wise 
comparisons among these the event spaces. 
(4) Abstract versus full-text comparison 
Language models of scientific articles often 
consider only abstracts because they are easier to 
obtain than full-text documents. Although his-
torically difficult to obtain, the increased avail-
ability of full-text articles motivates us to under-
stand the nature of language within the body of a 
document. For example, one study found that 
full-text articles require weighting schemes that 
consider document length (Kamps, et al 2005). 
However, controlling the weights for document 
lengths may hide a systematic difference be-
tween the language used in abstracts and the lan-
guage used in the body of a document. For ex-
ample, authors may use general language in an 
603
abstract and technical language within a docu-
ment. 
Transitioning from abstracts to full-text docu-
ments presents several challenges including how 
to weigh terms within the headings, figures, cap-
tions, and tables. Our forth experiment was to 
compare IDF between the abstract and full text 
of the document. We did not consider text from 
headings, figures, captions, or tables. 
(5) IDF Sensitivity 
In a dynamic environment such as the Web, it 
would be desirable to have a corpus-based 
weight that did not change dramatically with the 
addition of new documents. An increased under-
standing of IDF stability may enable us to make 
specific system recommendations such as if the 
collection increases by more than n% then up-
date the IDF values. 
To explore the sensitivity we compared the 
amount of change in IDF values for various sub-
sets of the corpus. IDF values were calculated 
using samples of 10%, 20%, ?, 90% and com-
pared with the global IDF. We stratified sam-
pling such that the 10% sample used term fre-
quencies in 10% of the ACHRE4 articles, 10% 
of the BICHAW articles, etc. To control for 
variations in the corpus, we repeated each sample 
10 times and took the average from the 10 runs. 
To explore the sensitivity we compared the 
global IDF in Equation 1 with the local sample, 
where N was the average number of documents 
in the sample and ni was the average term fre-
quency for each stemmed term in the sample. 
In addition to exploring sensitivity with re-
spect to a random subset, we were interested in 
learning more about the relationship between the 
global IDF and the IDF calculated on a journal 
sub-set. To explore these differences, we com-
pared the global IDF with local IDF where N 
was the number of documents in each journal 
and ni was the number of times the stemmed 
term appears in the text of that journal. 
4 Results and Discussion 
The 100830 full text documents comprised 
2,001,730 distinct unstemmed terms, and 
1,391,763 stemmed terms. All experiments re-
ported in this paper consider stemmed terms. 
4.1 Raw frequency comparison 
The dimensionality of the document, sentence, 
and terms spaces varied greatly, with 100830 
documents, 16.5 million sentences, and 2.0 mil-
lion distinct unstemmed terms (526.0 million in 
total), and 1.39 million distinct stemmed terms. 
Figure 2A shows the correlation between the fre-
quency of a term in the document space (x) and 
the average frequency of the same set of terms in 
the sentence space (y). For example, the average 
number of sentences for the set of terms that ap-
pear in 30 documents is 74.6. Figure 2B com-
pares the document (x) and average term freq- 
 
 
Frequency 
A - Document vs. Sentence 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.0E+7
1.0E+8
1.0E+00 1.0E+01 1.0E+02 1.0E+03 1.0E+04 1.0E+05 1.0E+06
Document Frequency (Log scale)
A
ve
ra
g
e 
S
en
te
n
ce
 F
re
q
ue
n
cy
 (
Lo
g
 s
ca
le
)
B - Document vs. Term 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.0E+7
1.0E+8
1.00E+00 1.00E+01 1.00E+02 1.00E+03 1.00E+04 1.00E+05 1.00E+06
Document Frequency (Log scale)
A
ve
ra
ge
 T
er
m
 F
re
q
u
en
cy
 (L
og
 s
ca
le
)
C - Sentence vs.Term 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.0E+7
1.0E+8
1.0E+00 1.0E+01 1.0E+02 1.0E+03 1.0E+04 1.0E+05 1.0E+06 1.0E+07
Sentence Frequency (Log scale)
A
ve
ra
ge
 T
er
m
 F
re
q
u
en
cy
 (L
og
 s
ca
le
)
Standard Deviation Error 
D - Document vs. Sentence 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.0E+0 1.0E+1 1.0E+2 1.0E+3 1.0E+4 1.0E+5
Document Frequency (Log scale)
S
en
te
nc
e 
S
ta
n
da
rd
 D
ev
ia
ti
o
n 
(L
o
g 
sc
al
e)
 
E - Document vs. Term 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.0E+0 1.0E+1 1.0E+2 1.0E+3 1.0E+4 1.0E+5
Document Frequency (Log scale)
T
er
m
 S
ta
n
da
rd
 D
ev
ia
tio
n
 (L
o
g 
sc
al
e)
F - Sentence vs. Term 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.0E+0 1.0E+1 1.0E+2 1.0E+3 1.0E+4 1.0E+5
Sentence Frequency (Log scale)
Te
rm
 S
ta
nd
ar
d
 D
ev
ia
ti
on
 (
Lo
g
 s
ca
le
)
Figure 2. Raw frequency correlation between document, sentence, and term spaces. 
 
604
A ? JACSAT Document Space 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.E+0 1.E+1 1.E+2 1.E+3 1.E+4 1.E+5 1.E+6 1.E+7 1.E+8
Word Rank (log scale)
W
o
rd
 F
re
q
ue
nc
y 
(lo
g 
sc
al
e)
Actual
Predicted(K=89283, m=1.6362)
 
B ? JACSAT Sentence Space 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.E+0 1.E+1 1.E+2 1.E+3 1.E+4 1.E+5 1.E+6 1.E+7 1.E+8
Word Rank (log scale)
W
or
d 
F
re
qu
en
cy
 (
lo
g 
sc
al
e) Actual
Predicted (K=185818, m=1.7138)
 
C ? JACSAT Term Space 
1.0E+0
1.0E+1
1.0E+2
1.0E+3
1.0E+4
1.0E+5
1.0E+6
1.E+0 1.E+1 1.E+2 1.E+3 1.E+4 1.E+5 1.E+6 1.E+7 1.E+8
Word Rank (log scale)
W
or
d 
Fr
eq
ue
nc
y 
(l
og
 s
ca
le
)
Actual
Predicted(K=185502, m=1.7061)
 
D - Slope Coefficients between document, sen-
tence, and term spaces for each journal, when fit 
to the power law K=jm 
-1.85
-1.80
-1.75
-1.70
-1.65
-1.60
-1.55
-1.80 -1.70 -1.60 -1.50
Document Slope
S
en
te
nc
e 
or
 T
er
m
 S
lo
pe
 
Sentence
Term JACSAT
 
Figure 3. Zipf?s Law comparison. A through C show the power law distribution for the journal JAC-
SAT in the document (A), sentence (B), and term (C) event spaces. Note the predicted slope coeffi-
cients of 1.6362, 1.7138 and 1.7061 respectively). D shows the document, sentence, and term slope 
coefficients for each of the 25 journals when fit to the power law K=jm, where j is the rank. 
 
quency (y) These figures suggest that the docu-
ment space differs substantially from the sen-
tence and term spaces. Figure 2C shows the sen-
tence frequency (x) and average term frequency 
(y), demonstrating that the sentence and term 
spaces are highly correlated.  
Luhn proposed that if terms were ranked by 
the number of times they occurred in a corpus, 
then the terms of interest would lie within the 
center of the ranked list (Luhn 1958). Figures 
2D, E and F show the standard deviation be-
tween the document and sentence space, the 
document and term space and the sentence and 
term space respectively. These figures suggest 
that the greatest variation occurs for important 
terms.  
4.2 Zipf?s Law comparison 
Zipf?s Law states that the frequency of terms 
in a corpus conforms to a power law distribution 
K/j? where ? is close to 1 (Zipf, 1949). We calcu-
lated the K and ? coefficients for each journal 
and language model combination using the 
binning method proposed in (Adamic, 2000). 
Figures 3A-C show the actual frequencies, and 
the power law fit for the each language model in 
just one of the 25 journals (jacsat). These and the 
remaining 72 figures (not shown) suggest that 
Zipf?s Law holds in all event spaces.  
Zipf Law states that ? should be close to -1. In 
our corpus, the average ? in the document space 
was -1.65, while the average ? in both the sen-
tence and term spaces was -1.73.  
Figure 3D compares the document slope (x) 
coefficient for each of the 25 journals with the 
sentence and term spaces coefficients (y). These 
findings are consistent with a recent study that 
suggested ? should be closer to 2 (Cancho 2005). 
Another study found that term frequency rank 
distribution was a better fit Zipf?s Law when the 
term space comprised both words and phrases 
(Ha et al 2002). We considered only stemmed 
terms. Other studies suggest that a Poisson mix-
ture model would better capture the frequency 
rank distribution than the power model (Church 
and Gale, 1995). A comprehensive overview of 
using Zipf?s Law to model language can be 
found in (Guiter and Arapov, 1982). 
605
4.3 Direct IDF, ISF, and ITF comparison 
Our third experiment was to compare the three 
language models directly. Figure 4A shows the 
average, minimum and maximum ISF value for 
each rounded IDF value. After fitting a regres-
sion line, we found that ISF correlates well with 
IDF, but that the average ISF values are 5.57 
greater than the corresponding IDF. Similarly, 
ITF correlates well with IDF, but the ITF values 
are 10.45 greater than the corresponding IDF. 
  
A 
y = 1.0662x + 5.5724
R2 = 0.9974
0
5
10
15
20
25
30
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
IDF
IS
F
Avg
Min
Max
B 
y = 1.0721x + 10.452
R2 = 0.9972
0
5
10
15
20
25
30
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
IDF
IT
F
Avg
Min
Max
C 
y = 1.0144x + 4.6937
R2 = 0.9996
0
5
10
15
20
25
30
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
ISF
IT
F
Avg
Min
Max
Figure 4. Pair-wise IDF, ISF, and ITF com-
parisons. 
It is little surprise that Figure 4C reveals a 
strong correlation between ITF and ISF, given 
the correlation between raw frequencies reported 
in section 4.1. Again, we see a high correlation 
between the ISF and ITF spaces but that the ITF 
values are on average 4.69 greater than the 
equivalent ISF value. These findings suggests 
that simply substituting ISF or ITF for IDF 
would result in a weighting scheme where the 
corpus weights would dominate the weights as-
signed to query in the vector based retrieval 
model. The variation appears to increase at 
higher IDF values. 
Table 2 (see over) provides example stemmed 
terms with varying frequencies, and their corre-
sponding IDF, ISF and ITF weights. The most 
frequent term ?the?, appears in 100717 docu-
ments, 12,771,805 sentences and 31,920,853 
times. In contrast, the stemmed term ?electro-
chem? appeared in only six times in the corpus, 
in six different documents, and six different sen-
tences. Note also the differences between ab-
stracts, and the full-text IDF (see section 4.4).  
4.4 Abstract vs full text comparison 
Although abstracts are often easier to obtain, the 
availability of full-text documents continues to 
increase. In our fourth experiment, we compared 
the language used in abstracts with the language 
used in the full-text of a document. We com-
pared the abstract and non-abstract terms in each 
of the three language models.  
Not all of the documents distinguished the ab-
stract from the body. Of the 100,830 documents, 
92,723 had abstracts and 97,455 had sections 
other than an abstract. We considered only those 
documents that differentiated between sections. 
Although the number of documents did not differ 
greatly, the vocabulary size did. There were 
214,994 terms in the abstract vocabulary and 
1,337,897 terms in the document body, suggest-
ing a possible difference in the distribution of 
terms, the log(ni) component of IDF. 
Figure 5 suggests that language used in an ab-
stract differs from the language used in the body 
of a document. On average, the weights assigned 
to stemmed terms in the abstract were higher 
than the weights assigned to terms in the body of 
a document (space limitations preclude the inclu-
sion of the ISF and ITF figures).  
0
2
4
6
8
10
12
14
16
18
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Global IDF
A
ve
ra
ge
 a
bs
tr
ac
t/
N
on
-a
b
st
ra
ct
 ID
F
Abstract
Non-Abstract
 
Figure 5. Abstract and full-text IDF compared 
with global IDF. 
606
 Document (IDF) Sentence (ISF) Term (ITF) 
Word Abs NonAbs All Abs NonAbs All Abs NonAbs All 
the 1.014 1.004 1.001 1.342 1.364 1.373 4.604 9.404 5.164
chemist 11.074 5.957 5.734 13.635 12.820 12.553 22.838 17.592 17.615
synthesis 14.331 11.197 10.827 17.123 18.000 17.604 26.382 22.632 22.545
eletrochem 17.501 15.251 15.036 20.293 22.561 22.394 29.552 26.965 27.507
Table 2. Examples of IDF, ISF and ITF for terms with increasing IDF. 
 
4.5 IDF sensitivity 
The stability of the corpus weighting scheme is 
particularly important in a dynamic environment 
such as the web. Without an understanding of 
how IDF behaves, we are unable to make a prin-
cipled decision regarding how often a system 
should update the corpus-weights.  
To measure the sensitivity of IDF we sampled 
at 10% intervals from the global corpus as out-
lined in section 3. Figure 6 compares the global 
IDF with the IDF from each of the 10% samples. 
The 10% samples are almost indiscernible from 
the global IDF, which suggests that IDF values 
are very stable with respect to a random subset of 
articles. Only the 10% sample shows any visible 
difference from the global IDF values, and even 
then, the difference is only noticeable at higher 
global IDF values (greater than 17 in our cor-
pus).  
 
0
2
4
6
8
10
12
14
16
18
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
IDF of Total Corpus
A
ve
ra
ge
 ID
F
 o
f S
te
m
m
ed
 T
er
m
s
10
20
30
40
50
60
70
80
90
% of Total Corpus
 
Figure 6 ? Global IDF vs random sample IDF. 
 
In addition to a random sample, we compared 
the global based IDF with IDF values generated 
from each journal (in an on-line environment, it 
may be pertinent to partition pages into academic 
or corporate URLs or to calculate term frequen-
cies for web pages separately from blog and 
wikis). In this case, N in equation (1) was the 
number of documents in the journal and ni was 
the distribution of terms within a journal. 
If the journal vocabularies were independent, 
the vocabulary size would be 4.1 million for un-
stemmed terms and 2.6 million for stemmed 
terms. Thus, the journals shared 48% and 52% of 
their vocabulary for unstemmed and stemmed 
terms respectively. 
Figure 7 shows the result of this comparison 
and suggests that the average IDF within a jour-
nal differed greatly from the global IDF value, 
particularly when the global IDF value exceeds 
five. This contrasts sharply with the random 
samples shown in Figure 6. 
 
0
5
10
15
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Global IDF
A
ve
ra
ge
 L
oc
al
 ID
F
ACHRE4
ANCHAM
BICHAW
BIPRET
BOMAF6
CGDEFU
CMATEX
ESTHAG
IECRED
INOCAJ
JACSAT
JAFCAU
JCCHFF
JCISD8
JMCMAR
JNPRDF
JOCEAH
JPCAFH
JPCBFK
JPROBS
MAMOBX
MPOHBP
NALEFD
OPRDFK
ORLEF7
 
Figure 7 ? Global IDF vs local journal IDF. 
 
At first glance, the journals with more articles 
appear to correlated more with the global IDF 
than journals with fewer articles. For example, 
JACSAT has 14,400 documents and is most cor-
related, while MPOHBP with 58 documents is 
least correlated. We plotted the number of arti-
cles in each journal with the mean squared error 
(figure not shown) and found that journals with 
fewer than 2,000 articles behave differently to 
journals with more than 2,000 articles; however, 
the relationship between the number of articles in 
the journal and the degree to which the language 
in that journal reflects the language used in the 
entire collection was not clear. 
5 Conclusions  
We have compared the document, sentence, and 
term spaces along several dimensions. Results 
from our corpus of 100,830 full-text scientific 
articles suggest that the difference between these 
alternative spaces is both theoretical and practi-
607
cal in nature. As users continue to demand in-
formation systems that provide sub-document 
retrieval, the need to model language at the sub-
document level becomes increasingly important. 
The key findings from this study are:  
(1) The raw document frequencies are con-
siderably different to the sentence and 
term frequencies. The lack of a direct 
correlation between the document and 
sub-document raw spaces, in particular 
around the areas of important terms, sug-
gest that it would be difficult to perform 
a linear transformation from the docu-
ment to a sub-document space. In con-
trast, the raw term frequencies correlate 
well with the sentence frequencies. 
(2) IDF, ISF and ITF are highly correlated; 
however, simply replacing IDF with the 
ISF or ITF would result in a weighting 
scheme where the corpus weight domi-
nated the weights assigned to query and 
document terms.  
(3) IDF was surprisingly stable with respect 
to random samples at 10% of the total 
corpus. The average IDF values based on 
only a 20% random stratified sample 
correlated almost perfectly to IDF values 
that considered frequencies in the entire 
corpus. This finding suggests that sys-
tems in a dynamic environment, such as 
the Web, need not update the global IDF 
values regularly (see (4)).  
(4) In contrast to the random sample, the 
journal based IDF samples did not corre-
late well to the global IDF. Further re-
search is required to understand these 
factors that influence language usage. 
(5) All three models (IDF, ISF and ITF) sug-
gest that the language used in abstracts is 
systematically different from the lan-
guage used in the body of a full-text sci-
entific document. Further research is re-
quired to understand how well the ab-
stract tested corpus-weighting schemes 
will perform in a full-text environment.  
References 
Lada A. Adamic 2000 Zipf, Power-laws, and Pareto - 
a ranking tutorial. [Available from 
http://www.parc.xerox.com/istl/groups/iea/papers/r
anking/ranking.html] 
Ricardo Baeza-Yates, and Berthier Ribeiro-Neto 1999 
Modern Information Retrieval: Addison Wesley. 
Cancho, R. Ferrer 2005 The variation of Zipfs Law in 
human language. The European Physical Journal B 
44 (2):249-57. 
Kenneth W Church and William A. Gale 1999 Inverse 
document frequency: a measure of deviations from 
Poisson. NLP using very large corpora, Kluwer 
Academic Publishers. 
Kenneth W Church.and William A. Gale 1995 Pois-
son mixtures. Natural Language Engineering, 1 
(2):163-90. 
H. Guiter and M Arapov 1982. Editors Studies on 
Zipf's Law. Brochmeyer, Bochum. 
Jaap Kamps, Maarten De Rijke, and Borkur 
Sigurbjornsson 2005 The Importance of lenght 
normalization for XML retrieval. Information Re-
trieval 8:631-54. 
Le Quan Ha, E.I. Sicilia-Garcia, Ji Ming, and F.J. 
Smith 2002 Extension of Zipf's Law to words and 
phrases. 19th International Conference on Compu-
tational linguistics. 
Hans P. Luhn 1958 The automatic creation of litera-
ture abstracts IBM Journal of Research and Devel-
opment 2 (1):155-64. 
Joseph P Pickett et al 2000 The American Heritage? 
Dictionary of the English Language. Fourth edi-
tion. Edited by H. Mifflin. 
Martin F. Porter 1980 An Algorithm for Suffix Strip-
ping. Program, 14 (3). 130-137. 
Stephen Robertson 2004 Understanding inverse 
document frequency: on theoretical arguments for 
IDF. Journal of Documentation 60 (5):503-520. 
Gerard Salton and Christopher Buckley 1988 Term-
weighting approaches in automatic text retrieval. 
Information Processing & Management, 24 
(5):513-23. 
Claude E. Shannon 1948 A Mathematical Theory of 
Communication Bell System Technical Journal. 27 
379?423 & 623?656. 
Karen Sparck Jones, Steve Walker, and Stephen 
Robertson 2000 A probabilistic model of informa-
tion retrieval: development and comparative ex-
periments Part 1. Information Processing & Man-
agement, 36:779-808. 
Karen Sparck Jones 1972 A statistical interpretation 
of term specificity and its application in retrieval. 
Journal of Documentation, 28:11-21. 
George Kingsley Zipf 1949 Human behaviour and the 
principle of least effort. An introduction to human 
ecology, 1st edn. Edited by Addison-Wesley. Cam-
bridge, MA. 
608
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 101?106,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Role of Sentence Structure in Recognizing Textual Entailment  
 
Catherine Blake 
School of Information and Library Science 
University of North Carolina at Chapel Hill  
Chapel Hill, NC 27599-3360 
cablake@email.unc.edu 
 
 
Abstract 
Recent research suggests that sentence 
structure can improve the accuracy of 
recognizing textual entailments and 
paraphrasing. Although background 
knowledge such as gazetteers, WordNet 
and custom built knowledge bases are 
also likely to improve performance, our 
goal in this paper is to characterize the 
syntactic features alone that aid in 
accurate entailment prediction. We 
describe candidate features, the role of 
machine learning, and two final decision 
rules. These rules resulted in an accuracy 
of 60.50 and 65.87% and average 
precision of 58.97 and 60.96% in 
RTE3Test and suggest that sentence 
structure alone can improve entailment 
accuracy by 9.25 to 14.62% over the 
baseline majority class.  
1 Introduction 
Understanding written language is a non-trivial 
task. It takes years for children to read, and 
ambiguities of written communication remain long 
after we learn the basics. Despite these apparent 
complexities, the bag-of-words (BOW) approach, 
which ignores structure both within a sentence and 
within a document, continues to dominate 
information retrieval, and to some extent document 
summarization and paraphrasing and entailment 
systems.  
The rational behind the BOW approach is in part 
simplicity (it is much easier and less 
computationally expensive to compare terms in 
one sentence with terms in another, than to 
generate the sentence structure); and in part 
accuracy, the BOW approach continues to achieve 
similar if not improved performance than 
information retrieval systems employing deep 
language or logical based representations. This 
performance is surprising when you consider that a 
BOW approach could not distinguish between the 
very different meaning conveyed by: (1)Slow 
down so that you don?t hit the 
riders on the road and (2)Don?t 
slow down so you hit the riders on 
the road. A system that employed a syntactic 
representation of these sentences however, could 
detect that the don?t modifier applies to hit in 
first sentence and to slow second. 
In contrast to information retrieval, researchers 
in paraphrase and entailment detection have 
increased their use of sentence structure. Fewer 
than half of the submissions in the first 
Recognizing Textual Entailment challenge (RTE1) 
employed syntax (13/28, 46%) (Dagan, Glickman, 
& Magnini, 2005), but more than two-thirds (28/ 
41, 68%) of the second RTE challenge (RTE2) 
submissions employed syntax (Bar-Haim et al, 
2006). Furthermore, for the first time, the RTE2 
results showed that systems employing deep 
language features, such as syntactic or logical 
representations of text, could outperform the 
purely semantic overlap approach typified by 
BOW. Earlier findings such as (Vanderwende, 
Coughlin, & Dolan, 2005) also suggest that 
sentence structure plays an important role in 
recognizing textual entailment and paraphrasing 
accurately.  
Our goal in this paper is to explore the degree to 
which sentence structure alone influences the 
accuracy of entailment and paraphrase detection. 
101
 Other than a lexicon (which is used to identify the 
base form of a term), our approach uses no 
background knowledge, such as WordNet (Miller, 
1995), extensive dictionaries (Litkowski, 2006) or 
custom-built knowledge-bases (Hickl et al, 2006) 
that have been successfully employed by other 
systems. While such semantic knowledge should 
improve entailment performance, we deliberately 
avoid these sources to isolate the impact of 
sentence structure alone. 
2 System Architecture 
2.1 Lexical Processing 
Our approach requires an explicit representation of 
structure in both the hypothesis (HSent) and test 
(TSent) sentence(s). Systems in RTE challenges 
employ a variety of parsers. In RTE2 the most 
popular sentence structure was generated by 
Minipar (Lin, 1998), perhaps because it is also one 
of the fastest parsers. Our system uses the typed 
dependency tree generated by the Stanford Parser 
(Klein & Manning, 2002). A complete set of parser 
tags and the method used to map from a 
constituent to a typed dependency grammar can be 
found in (de Marneffe et al, 2006). Figure 1 shows 
an example typed dependency grammar for pair id 
355 in the RTE3Test set.  
2.2 Lexicon 
Our proposed approach requires the base form of 
each term. We considered two lexicons for this 
purpose: WordNet (Miller, 1995) and the 
SPECIALIST lexicon (National Library of 
Medicine, 2000). The latter is part of the National 
Library of Medicine?s (NLM) Unified Medical 
Language System (UMLS) and comprises terms 
drawn from medical abstracts, and dictionaries, 
both medical and contemporary. 
With 412,149 entries, the SPECIALIST lexicon 
(version 2006AA) is substantially larger than the 
5,947 entries in WordNet (Version 3.0). To 
understand the level of overlap between the 
lexicons we loaded both into an oracle database. 
Our subsequent analysis revealed that of the 
WordNet entries, 5008 (84.1%) had a 
morphological base form in the SPECIALIST 
lexicon. Of the 548 distinct entries that differed 
between the two lexicons, 389 differed because 
either the UMLS (214 terms) or WordNet (11 
terms) did not have a base form. These results 
suggest that although the NLM did not develop 
their lexicon for news articles, the entries in the 
SPECIALIST lexicon subsumes most terms found 
in the more frequently used WordNet lexicon. 
Thus, our system uses the base form of terms from 
the SPECIALIST lexicon. 
2.3 Collapsing Preposition Paths 
Previous work (Lin & Pantel, 2001) suggests the 
utility of collapsing paths through prepositions. 
The type dependency does have a preposition tag, 
prep, however, we found that the parser typically 
assigns a more general tag, such as dep (see the 
dep tag in Figure 1 between wrapped and by). 
Instead of using the prep tag, the system collapses 
paths that contain a preposition from the 
SPECIALIST lexicon. For example, the system 
 
Figure 1. Dependency grammar tree for pair identifier 355 in the RTE3Test
102
 collapses four paths in TSentEG millions of 
visitors, wrapped in 1995, wrapped by Christo, 
and wrapped began before.  
2.4 Base Level Sentence Features 
The typed dependency grammar, such as that 
shown in Figure 1, can produce many different 
features that may indicate entailment. Our current 
implementation uses the following four base level 
features.  
(1) Subject: The system identifies the subject(s) 
of a sentence using heuristics and the parser 
subject tags nsubjpass and nsubj.  
(2) Object: The system uses the parser tag dobj to 
identify the object(s) in each sentence.  
(3) Verb: The system tags all terms linked with 
either the subject or the object as a verb. For 
example, wrapped is tagged as the verb wrap 
from the link wrapped nsubjpass 
Reichstag shown in Figure 1. 
(4) Preposition: As described in section 2.3 the 
system collapses paths that include a 
preposition.  
 
The subject feature had the most coverage of the 
base level features and the system identified at 
least one subject for 789 of the 800 hypotheses 
sentences in RTE3Devmt. We wrote heuristics that 
use the parser tags to identify the subject of the 
remaining 11 sentences. The system found subjects 
for seven of those eight remaining hypothesis 
sentences (3 were duplicate sentences). In contrast, 
the object feature had the least coverage, with the 
system identifying objects for only 480 of the 800 
hypotheses in the RTE3 revised development set 
(RTE3Devmt).  
In addition to the head noun of a subject, 
modifying nouns can also be important to 
recognize entailment. Consider the underlined 
section of TSentEG: which was later bought by 
the Russian state-owned oil company 
Rosneft. This sentence would lend support to 
hypotheses sentences that start with The 
Baikalfinasgroup was bought by ? and end 
with any of the following phrases an oil 
company, a company, Rosneft, the Rosneft 
Company, the Rosneft oil company, a Russian 
company, a Russian Oil company, a state-
owned company etc. Our system ensures the 
detection of these valid entailments by adding 
noun compounds and all modifiers associated with 
the subject and object term.  
2.5 Derived Sentence Features 
We reviewed previous RTE challenges and a 
subset of RTE3Devmt sentences before arriving at 
the following derived features that build on the 
base level features described in 2.4. The features 
that use ?opposite? approximate the difference 
between passive and active tense. For each 
hypothesis sentence, the system records both the 
number of matches (#match), and the percentage of 
matches (%match) that are supported by the test 
sentence(s).  
(1) Triple: The system compares the subject-verb-
objects in HSent with the corresponding triple 
in TSent. 
(2) Triple Opposite: The system matches the 
verbs in both HSent and TSent, but matches 
the subject in HSent with the object in TSent.  
(3) Triple Subject Object: This feature 
approximates the triple in (1) by comparing 
only the subject and the object in HSent with 
TSent, but ignoring the verb.  
(4) Triple Subject Object Opposite: The system 
compares the objects in HSent with the 
subjects in TSent. 
(5) Subject Subject: In addition to the triples used 
in the derived features 1-4, the system stores 
subject-verb and object-verb pairs. This 
feature compares the distinct number of 
subjects in HSent with those in TSent. 
(6) Verb Verb: The system compares only the 
verb in the subject-verb, object-verb tuples in 
HSent with those in TSent. 
(7) Subject Verb: The system compares the 
distinct subjects in HSent with the distinct 
verbs in TSent. 
(8) Verb Subject: The system compares the verb 
in HSent with the subject in TSent. 
(9) Verb Preposition: The system compares both 
the preposition and verb in HSent with those 
in TSent.  
(10) Subject Preposition: The system compares 
both the subject and preposition in HSent with 
those in TSent. 
(11) Subject Word: The system compares the 
distinct subjects in HSent with the distinct 
words in TSent. This is the most general of all 
11 derived features used in the current system 
103
 2.6 Combining Features 
A final decision rule requires a combination of the 
derived features in section 2.5. We used both 
previous RTE challenges and machine learning 
over the derived features to inform the final 
decision rules. For the latter, we chose a decision 
tree classifier because in addition to classification 
accuracy, we are also interested in gaining insight 
into the underlying syntactic features that produce 
the highest predictive accuracy. 
The decision trees shown in Figure 2 were 
generated using the Oracle Data Miner 10.2.0.2. 
Tree (A) suggests that if there is less than a 
63.33% similarity between the number of subjects 
in the hypothesis sentence and the words in any of 
the test sentences (feature 11), that the hypothesis 
sentence is not entailed by the test sentence(s). The 
NO prediction from this rule would be correct in 
71% cases, and assigning NO would apply to 42% 
of sentences in the development set. A YES 
prediction would be correct in 69% of sentences, 
and a YES prediction would take place in 57% of 
sentences in the development set. Tree (B) also 
suggests that an increase in the number of matches 
between the subject in the hypothesis sentence and 
the words used in the test sentence(s) is indicative 
of an entailment.  
Decision Tree (A) 
 
 
 If Subject-Word match <= 63.3% 
 NO YES 
Accuracy=71% Accuracy=69% 
Coverage=43% Coverage= 57% 
 
Decision Tree (B) 
   If Subject-Word match <= 1.5 
 
 NO  YES 
   Accuracy=58% Accuracy=64% 
   Coverage=52%  Coverage=48% 
 
Figure 2. Decision trees generated for the revised 
RTE3Devmt set during decision rule development. 
 
Although tempting to implement the decision 
tree with the highest accuracy, we should first 
consider the greedy search employed by this 
algorithm. At each level of recursion, a decision 
tree algorithm selects the single feature that best 
improves performance (in this case, the purity of 
the resulting leaves, i.e. so that sentences in each 
leaf have all YES or all NO responses).  
Now consider feature 1, where the subject, verb 
and object triple in the hypothesis sentence 
matches the corresponding triple in a test sentence. 
Even though the predictive accuracy of this feature 
is high (74.36%), it is unlikely that this feature will 
provide the best purity because only a small 
number of sentences (39 in RTE3Devmt) match. 
Similarly, a subject-object match has the highest 
predictive accuracy of any feature in RTE3Devmt 
(78.79%), but again few sentences (66 in 
RTE3Devm) match. 
2.7 Final Decision Rules 
We submitted two different decision rules to RTE3 
based on thresholds set to optimize performance in 
RTE3Devmt set. The thresholds do not consider the 
source of a sentence, i.e. from information 
extraction, summarization, information retrieval or 
question answering activities. 
The first decision rule adds the proportion of 
matches for each of the derived features described 
in section 2.5 and assigns YES when the total 
proportion is greater than or equal to a threshold 
2.4. Thus, the first decision rule overly favors 
sentences where the subject, verb and object match 
both HSent and TSent because if a sentence pair 
matches on feature 1, then the system also counts a 
match for features 3, 4, 5, and 8. This lack of 
feature independence is intentional, and consistent 
with our intuition that feature 1 is a good indicator 
of entailment.  
To arrive at the second decision rule, we 
considered the features proposed by decision trees 
with a non-greedy search strategy that favors high 
quality features even when only a small percentage 
of sentences match. The second rule predicts YES 
under the following conditions: when the subject, 
verb, and object of HSent match those in any 
TSent (feature 1), in either order (feature 2) or 
when the subject and object from the HSent triple 
match any TSent (feature 3), or when the TSent 
subject matches >= 80% of the HSent subject 
terms (feature 5) or when the TSent subject and 
preposition matches >=70% of those in HSent 
(feature 10) or when TSent word matches >= 70% 
of the subject terms in the HSent  sentence (feature 
11). 
104
 RTE3Devmt RTE3Test RTE2AllFeature 
Total Pos %Accy Total Pos %Accy Total Pos %Accy
1 Triple 35 26 74.29 37 24 64.86 47 35 74.47
2 Triple Opposite 4 3 75.00 9 4 44.44 2 1 50.00
3 Triple Subj Obj 66 52 78.79 76 47 61.84 102 69 67.65
4 Triple Subj Obj Opp. 9 4 44.44 16 7 43.75 10 5 50.00
5 Subject-Subject 750 397 52.93 760 404 53.16 777 391 50.32
6 Verb-Verb 330 196 59.39 345 181 52.46 395 208 52.66
7 Subject-Verb 297 178 59.93 291 168 57.73 292 154 52.74
8 Verb-Subject 348 196 56.32 369 207 56.10 398 212 53.27
9 Verb-Preposition 303 178 58.75 312 167 53.53 355 190 53.52
10 Subject-Preposition 522 306 58.62 540 310 57.41 585 303 51.79
11 Subject-Word 771 406 52.66 769 407 52.93 790 395 50.00
Table 1. Coverage and accuracy of each derived feature for RTE3 revised development collection 
(RTE3Devmt), the RTE3 Test collection (RTE3Test ) and the entire RTE2 collection (RTE2All).
3 Results 
The experiments were completed using the revised 
RTE3 development set (RTE3Devmt) before the 
RTE3Test results were released. The remaining 
RTE2 and RTE3Test analyses were then conducted. 
3.1 Accuracy of Derived Features 
Table 1 shows the accuracy of any match between 
the derived features described in section 2.5. 
Complete matching triples (feature 1), and 
matching subjects and objects in the triple (feature 
2) provide the highest individual accuracy. 
0
10
20
30
40
50
60
70
80
90
100
10 20 30 40 50 60 70 80 90 100
Match (%)
A
cc
u
ra
cy
 (
%
)
 
Figure 3. Correlation between accuracy and the 
percentage subjects in HSent that have a 
corresponding subject in TSent (feature 5). 
 
The results in Table 1 do not consider the degree 
of feature match. For example, only one of the 
words from TSent in sentence 525?s (RTE3Devmt) 
matched the eight subject terms in corresponding 
HSent. If the derived features outlined in section 
2.7 did capture the underlying structure of an 
entailment, you would expect an increased match 
would correlate with increased accuracy. We 
explored the correlations for each of the derived 
features. Figure 3 suggests entailment accuracy 
increases with an increase in the percentage of 
TSent subject terms that match HSent terms. 
(feature 5) and demonstrates why we set the 80% 
threshold for feature 5 in the second decision rule. 
3.2 Accuracy of Decision Rules 
Of the 800 sentences in RTE3Devmt, the annotators 
labeled 412 as an entailment. Thus, without any 
information about HSent or TSent, the system 
would assign YES (the majority class) to each 
sentence, which would result in 51.50% accuracy. 
The first decision rule considers the total 
percentage match of all features defined in section 
2.5. We arrived at a threshold of 2.4 by ranking the 
development set in decreasing order the total 
percentage match and identifying where the 
threshold would lead to an accuracy of around 
65%. Many sentences had a threshold of around 
2.4, and the overall accuracy of the first decision 
on the RTE3Devmt set was 62.38%, compared to 
60.50% in RTE3Test. We consider the first decision 
rule a baseline and the second rule is our real 
submission. 
The second rule uses only a sub-set of the 
derived features (1, 2, 3, 5, 10, and 11) and 
includes thresholds for features 5, 10 and 11. The 
accuracy of the second decision rule on RTE3Devmt 
105
 set was 71.50%, compared with an accuracy of 
65.87 % on RTE3Test. 
Our results are consistent with previous RTE2 
findings (Bar-Haim et al, 2006) where task 
performance varies with respect to the sentence 
source. Both rules had similar (poor) performance 
for information extraction (50.00 vs. 50.50%). 
Both rules had moderate performance for 
summarization (56.50 vs. 60.50%) and good 
performance for information retrieval (70.00 vs. 
75.50%). The second decision rule constantly out-
performed the first, with the largest increase of 
11.5% in the question answering activity (65.50 vs. 
77.00%). 
Both decision rules lend themselves well to 
ranking sentences in decreasing order from the 
most to the least certain entailment. Average 
precision is calculated using that ranking and 
produces a perfect score when all sentence pairs 
that are entailments (+ve) are listed before all the 
sentence pairs that are not (-ve) (Voorhees & 
Harman., 1999). The average precision of the first 
and second decision rules was 58.97% and 60.96% 
respectively. The variation in precision also varied 
with respect to the sentence source (IE, IR, QA and 
SUM) of 48.52, 65.93, 72.38, and 56.04% for the 
first decision rule and 48.32, 72.71, 78.75 and 
56.69% for the second decision rule. 
4 Conclusions 
Although most systems include both syntax and 
semantics to detect entailment and paraphrasing, 
our goal in this paper was to measure the impact of 
sentence structure alone. We developed two 
decision rules that each use features from a typed 
dependency grammar representation the hypothesis 
and test sentences. The first decision rule considers 
all features and the second considers only a sub-set 
of features, and adds thresholds to ensure that the 
system does not consider dubious matches. 
Thresholds for both rules were established using 
sentences in RTE3Devmt only. The second rule out-
performed the first on RTE3Test, both with respect 
to accuracy (60.50% vs. 65.87%) and average 
precision (58.97% vs. 60.96%).  
These results are particularly encouraging given 
that our approach requires no background 
knowledge (other than the lexicon) and that this 
was the first time we participated in RTE. The 
results suggest that sentence structure alone can 
improve entailment prediction by between 9.25-
14.62% alone, over the majority class baseline 
(51.52% in RTE3Test) and they provided additional 
support to the growing body of evidence that 
sentence structure will continue to play a role in 
the accurate detection of textual entailments and 
paraphrasing. 
References  
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., 
Giampiccolo, D., Magnini, B., et al (2006). The 
Second PASCAL Recognising Textual Entailment 
Challenge. Venice, Italy. 
Dagan, I., Glickman, O., & Magnini, B. (2005). The 
PASCAL Recognising Textual Entailment Challenge. 
Southampton, U.K. 
de Marneffe, M.-C., MacCartney, B., Grenager, T., Cer, 
D., Rafferty, A., & Manning, C. D. (2006). Learning 
to distinguish valid textual entailments, In The 
Second PASCAL Challenges Workshop on 
Recognising Textual Entailment, Venice, Italy. 
Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink, 
B., & Shi, Y. (2006). Recognizing Textual 
Entailment with LCC?s GROUNDHOG System In 
The Second PASCAL Recognising Textual 
Entailment Challenge, Venice, Italy. 
Klein, D., & Manning, C. D. (2002). Fast Exact 
Inference with a Factored Model for Natural 
Language Parsing. Paper presented at the Advances 
in Neural Information Processing Systems. 
Lin, D. (1998). Dependency-based Evaluation of 
MINIPAR. In Workshop on the Evaluation of 
Parsing Systems, First International Conference on 
Language Resources and Evaluation, Granada,Spain. 
Lin, D., & Pantel, P. (2001). Induction of semantic 
classes from natural language text. In The 7th 
International conference on Knowledge discovery 
and Data Mining, San Francisco, CA. 
Litkowski, K. (2006). Componential Analysis for 
Recognizing Textual Entailment. In The Second 
PASCAL Challenges Workshop on Recognising 
Textual Entailment, Venice, Italy. 
Miller, G. (1995). WordNet: A Lexical Database for 
English. Communications of the ACM, 38(11), 39-
41. 
National Library of Medicine. (2000). The SPECIALIST 
Lexicon, from 
www.nlm.nih.gov/pubs/factsheets/umlslex.html
Vanderwende, L., Coughlin, D., & Dolan, B. (2005). 
What Syntax can Contribute in Entailment Task. The 
PASCAL Challenges Workshop on Recognising 
Textual Entailment. Southhampton, UK. 
Voorhees, E. M., & Harman., D. (1999). Overview of 
the seventh text retrieval conference. In The Seventh 
Text REtrieval Conference (TREC-7). 
106
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536?542,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Sbdlrhmn: A Rule-based Human Interpretation System for  
Semantic Textual Similarity Task 
 
Samir AbdelRahman 
sbdlrhmn@illinois.edu, 
 s.abdelrahman@fci-cu.edu.eg 
Catherine Blake 
clblake@illinois.edu 
 
 The Graduate School of Library and Information Science 
University Of Illinois at Urbana-Champaign 
 
 
 
 
Abstract 
In this paper, we describe the system architec-
ture used in the Semantic Textual Similarity 
(STS) task 6 pilot challenge. The goal of this 
challenge is to accurately identify five levels 
of semantic similarity between two sentences: 
equivalent, mostly equivalent, roughly equiva-
lent, not equivalent but sharing the same topic 
and no equivalence. Our participations were 
two systems. The first system (rule-based) 
combines both semantic and syntax features to 
arrive at the overall similarity. The proposed 
rules enable the system to adequately handle 
domain knowledge gaps that are inherent 
when working with knowledge resources. As 
such one of its main goals, the system sug-
gests a set of domain-free rules to help the 
human annotator in scoring semantic equiva-
lence of two sentences. The second system is 
our baseline in which we use the Cosine Simi-
larity between the words in each sentence 
pair.       
1 Introduction 
Accurately establishing sentence semantic similari-
ty would provide one of the key ingredients for 
solutions to many text-related applications, such as 
automatic grading systems (Mohler and Mihalcea, 
2009), paraphrasing (Fernando and Stevenson, 
2008), text entailment (Corley et al, 2005) and 
summarization (Erkan and Radev, 2004). Current 
approaches for computing semantic similarity be-
tween a pair of sentences focus on analyzing their 
shared words (Salton, 1989), structures (Hu et al 
2011;Mandreoli et al 2002), semantics (Mihalcea 
et al 2006; Le el al. 2006; Hatzivassiloglou, 1999) 
or any of their combinations (Liu et al 2008; Foltz 
et al 1998).  The goal is to arrive at a score which 
increases proportionally with the relatedness be-
tween the two sentences.  Yet, they are not con-
cerned with scoring the interpretations of such 
relatedness (Zhang et al 2011; Jesus et al 2011; 
Wenyin et al 2010; Liu et al 2008).  
Semantic Textual Similarity (STS), SEMEVAL-
12 Task 6 (Agirre et al 2012), measures the degree 
of semantic equivalence between a pair of sentenc-
es by comparing meaningful contents within a sen-
tence. The assigned scores range from 0 to 5 for 
each sentence pair with the following interpreta-
tions: (5) completely equivalent, (4) mostly 
equivalent pair with missing unimportant infor-
mation, (3) roughly equivalent with missing im-
portant information, (2) not equivalent, but sharing 
some details, (1) not equivalent but sharing the 
same topic and (0) not equivalent and on different 
topics. The goal of developing our rule-based sys-
tem was to identify knowledge representations 
which have possibly all task human interpretations. 
Meanwhile, the system domain-free rules aim to 
help the human annotator in scoring semantic 
equivalence of sentence pair. 
The proposed rule-based solution exploits both 
sentence syntax and semantics. First, it uses Stan-
ford parser (Klein and Manning, 2002) to expose 
the sentence structure, part-of-speech (POS) word 
tags, parse tree and Subject-Verb-Object (S-V-O) 
dependencies. Second, Illinois Coreference Pack-
age (Bengtson and Roth, 2008) is used to extract 
sentence named entities resolving possible men-
536
tions. Third, WordNet (Miller, 1995) and Adapted 
Lesk Algorithm for word sense disambiguation 
(Banerjee and Pedersen, 2010) are used to compute 
each sentence word semantic relatedness to the 
other sentence.  ReVerb (Etzioni et al 2011) aug-
ments WordNet in case of uncovered words and 
helps us to discriminate the topics of sentences. 
We use (Blake, 2007) thought to compare the sen-
tence pair words with each other.  Finally, we 
evolve a rule-based module to present the human 
heuristics when he interprets the relatedness of the 
sentence pair meaningful contents. 
Throughout our training and testing experi-
ments, we used Task6 corpora (Agirre et al 2012) 
namely MSRpar, MSRvid, SMTeuroparl, OnWN 
and SMTnews; where: 
- MSRpar is 1500 pairs of sentences of MSR-
Paraphrase, Microsoft Research Paraphrase Cor-
pus; 750 for training and 750 for testing. 
- MSRvid is 1500 pairs of sentences of MSR-
Video, Microsoft Research Video Description 
Corpus; 750 for training and 750 for testing. 
- SMTeuroparl is 918 pairs of sentences of 
WMT2008 development dataset (Europarl sec-
tion); 459 for training and 459 for testing. 
-  OnWn is 750 pairs of sentences pairs of sen-
tences where the first sentence comes from On-
tonotes and the second sentence from a WordNet 
definition; it is only a testing corpus. 
-  SMTnews is 399 pairs of sentences of news 
conversation sentence pairs from WMT; it is on-
ly a testing corpus. 
 
The reminder of this paper is organized as fol-
lows: Section 2 describes our two participations; 
Section 3 discusses their official results; Section 4 
draws our conclusion for both systems.   
2 The Proposed Systems  
In this section, we focus on the rule-based system, 
Sections 2.1, 2.2, 2.3 and 2.4, as our main task 
contribution. Further, the section describes our se-
cond run, Sections 2.5, to shed light on the role of 
cosine similarity for solving the task problem. To 
establish the task semantic textual similarity, we 
show how the rule-based system exploits the sen-
tence semantic, syntax and heuristics; also, we de-
scribe how our base-line system uses the sentence 
syntax only. 
2.1 Definitions 
We say the two sentences are on different topics, if 
all their verbs are mostly (> 50%) unrelated (Table 
1). Otherwise, they are on the same topic. For ex-
ample, the two sentences ?A woman is putting on 
makeup.?, ?A band is singing.? are on different 
topics as ?putting?, ?singing? are not equivalent. 
However, the two sentences ?A baby is talking.?, 
A boy is trying to say firetruck.? are on the same 
topics  as ?talking? and ?trying to say? are seman-
tically equivalent.  
We define the sentence important information as 
its head nouns, named entities or main verbs; 
where the main verbs are all verbs except auxilia-
ry, model and infinitive ones.  Hence, we say that 
two sentences miss important information if either 
loses at least one of these mentions from the other. 
Otherwise, they are candidates to be semantically 
equivalent. For example, the sentence ?Besides 
Hampton and Newport News, the grant funds wa-
ter testing in Yorktown, King George County, Nor-
folk and Virginia Beach.? misses ?Hampton and 
Newport News? compared to the sentence ?The 
grant also funds beach testing in King George 
County, Norfolk and Virginia Beach.? However, 
?on a table? is unimportant information which ?A 
woman is tapping her fingers.? misses compared 
to ?A woman is tapping her fingers on a table.? 
Finally, we deploy a list of stop words and non-
verbs as unimportant information. However, if any 
exists in both sentences, we match them with each 
other; otherwise we ignore any occurrences. 
2.2  The Syntactic Module  
This syntactic module is a preprocessing module in 
which the system calls Stanford parser, Version 
2.0.1, and the Illinois coreference package, Version 
1.3.2, to result in the sentence four type representa-
tions: 1) part of speech (POS) tags, 2) Subject-
Verb-Object (S-V-O), Subject-Verb (S-V) and 
Verb-Object (V-O) dependencies, 3) parse tree and 
4) coreference resolutions.  All sentences are lem-
matized based on their POSs. Also, verbs and CDs 
are utilized to determine topics/important infor-
mation and numbers respectively. All noun  and 
verb phrases are used to boost the sentence word 
semantic scores (Section 2.3). We consider all oc-
currences of S-V-O, S-V and V-O to distinguish 
537
the topic compatibility between two comparable 
sentences (Section 2.3 and 2.4).  
The coreference package is used to match the 
equivalent discourse entities between two sentenc-
es which improve the matching steps. For example, 
in the pair of   ?Mrs Hillary Clinton explains her 
plan towards the Middle East countries? and ?Mrs 
Clinton meets their ambassadors?, ?Mrs Hillary 
Clinton?, ?her? and ?Mrs Clinton? refer to the 
same entity where ?the Middle East countries? and 
?their? are equivalent. Moreover, we consider the 
second sentence doesn?t lose ?Hillary? as missing 
important information since the related mentions 
are labeled equivalent.  
2.3 The Semantic Matching Module 
WordNet, Version 3.0, has approximately 5,947 
entries covering around 85% of training corpora 
words (Agirre et al 2012). Most of the remaining 
15% words are abbreviations, named entities and 
incorrect POS tags. We use WordNet shortest path 
measure to compute the semantic similarity be-
tween two words. Also, we use Adapted Lesk algo-
rithm to obtain the best WordNet word sense. The 
disambiguation algorithm compares each pair of 
words through their contexts (windows) of words 
coupled with their all overlapping glosses of all 
WordNet relation types. 
The semantic matching module inputs are the 
sentence pair (S1, S2), their lemmatized words, 
parse trees, S-V-O/S-V/V-O dependencies and co-
reference mentions (Section 2.2). It matches syn-
tactically the words with each other. For any 
uncovered WordNet word, the module calls Re-
Verb (Section 2.4) and it assigns the returned value 
to the word score. All numbers, e.g. million, 
300,45.6, are mathematically compared with each 
other. This module compares the noun phrases 
with single words to handle the compound words, 
e.g. ?shot gun? with ?shotgun? or ?part-of-
speech? with ?part of speech?. For those words 
whose scores are not equal to 1, it compares each 
pair of words from the sentence pair within their 
Subject-VP (subject with its verb phrase) contexts 
using Adapted Lesk algorithm to find best sense 
for each included word. Then, it applies WordNet 
shortest path measure to score such words. In our 
disambiguation algorithm implementation, we 
found that the runtime requirement is directly pro-
portional to the input sentence length. So, we 
shortened the sentence length to Subject-VP which 
includes the underlying comparable words.   
 
Relatedness Score (S1, S2) 
unrelated  0 <= Ws <0.3 
weakly related 0.3 <= Ws <0.85 
strongly related Ws >= 0.85 
 
Table 1 ? Mapping relatedness to wordnet similarity 
 
Table 1 describes the proposed system WordNet 
thresholds through our relatedness definitions. The 
thresholds were thoroughly selected depending on 
our analysis for the WordNet hierarchary and 
semantic similarity measures (Pedersen et al, 
2004). We obsereved that while most of the nearest 
tree sibilings and parent-child nodes scores have 
more than 0.85 Wordnet semantic scores, most of 
the fartherest ones have scores less than 0.3. In 
between these extremes, there is a group of 
scattered tree nodes which ranges from 0.3 to 0.85. 
The number of nodes per each mentioned group is 
related to the semantic simlarity measure 
technique.   
2.4 Semantics ? Using ReVerb  
Our working hypothesis is that verbs that use the 
same arguments are more likely to be similar. To 
estimate verb usage, the system uses frequencies 
from the ReVerb (http://openie.cs.washington. 
edu/) online interface to count the number of times 
a verb is used with two arguments. For example, 
consider the sentence pair ?The man fires rifle? 
and ?The man cuts lemon?. The number of sen-
tences in ReVerb that contain the verb fires with 
the argument rifle is 538 and the number of sen-
tences for the verb cuts with the argument lemon is 
45, which tell us that you are more likely to find 
sentences that describe firing a rifle than cutting a 
lemon on the web. However, there a no ReVerb 
sentences for the verb fires with the argument lem-
on or the verb cuts with the argument rifle. Which 
tells us that people generally don?t fire lemons or 
cut rifles.  
     Reverb provides the system with information 
about the suitability of using argument in one sen-
tence with verbs from another. Specifically, fre-
quencies from Reverb are retrieved for each 
subject-verb-object triple in each sentence, e.g. 
?S1-V1-O1? and ?S2-V2-O2?. The system then 
retrieves ReVerb frequencies for the verb-object in 
538
each sentence of ?V2-O1? and ?V1-O2?. If at least 
one of all of these scores equals to 0, they are con-
sidered to be weakly similar.  
ReVerb is also called for any sentence word that 
WordNet doesn?t cover.  The system retrieves the 
Reverb frequency for is-a relation using the word 
missing from Wordnet, as Argument1, and each 
word from the other sentence as Argument2. The 
largest Reverb retrieved score is taken. Consider 
the pair of ?A group of girls are exiting a taxi? and 
?A video clip of Rihanna leaving a taxi.?. Since 
?Rihanna? is not a WordNet word, our ReVerb 
interface hits the web for ?Rihanna is-a girl?, ?Ri-
hanna is-a group?, ?Rihanna is-a taxi? and ?Ri-
hanna is-a existing? and it returns ?Rihanna is-a 
girl? as the best candidate with strength score 
equals 0.2. 
We explored several relatedness scores which 
specifically equal to 0, 0.2, 0.4, 0.6, 0.8 or 1 if the 
frequencies are less than to 10, 50, 100, 500, 1000 
or 1000+ respectively. 
2.5 The Rule-Based Module  
Rule-based module aims at defining human-like 
rules to interpret how the pair similar or dissimilar 
from each other. Pair Similarity (P) is based on the 
strong relatedness values (Table 1) and the Dissim-
ilarity (D) is based on the other types of related-
ness values. As we believe that strong and not 
strong are proportional to the pair similarity and 
dissimilarity respectively 
Rule-based module input is sentence pair S1, S2 
word semantic scores, i.e. Ws1s and Ws2s (Table 
1). Then, it calculates: 1) their three types of aver-
ages for S1 and S2 semantic scores, i.e. all word 
semantic scores, weakly related only and unrelated 
values; 2) P as the minimum percentage of strong 
Wss in (S1 and S1); 3) D as, 100-P, the percentage 
of not strong Wss in S1 and S1  
This module outputs the semantic textual simi-
larity semantic (STS) score which ranges from 0 to 
5. Throughout this section, when we use ?unrelat-
ed?, ?weak? and strong terminologies, we use Ta-
ble 1 Relatedness definitions. Also, when we use 
?important? term, we refer to our definition (Sec-
tion 2.1) 
 Human judgments for computing STS score of 
the sentence pair are based on word similarities 
and dissimilarities. They consider that two sen-
tences are similar if most (> 50%) of their words 
are strongly related, otherwise the sentences are 
candidates to be dissimilar. Since all Wss range 
from 0 to 1, the average of strong scores is more 
than the average of weak scores. Likewise, the av-
erage of weak scores is more than the average of 
non-related scores.  
 
Score(Sentence Ws1s, Sentence Ws2s) 
AllAvg = (Ws1s+ Ws1s)/2 
WeakAvg= the averaged weakly related scores of 
Ws1s and Ws1s 
UnRElAvg=the average of unrelated scores of 
Ws1s and Ws1s 
P = minimum (% Ws1s strong scores, % Ws2s 
strong scores) 
D=100-P 
Value=0 
If 95 <= P <=100 then  Value = 5; 
If 80 <= P < 95 then Value = 4; 
If 50 <= P < 80  then Value = 3; 
If 20 <= P < 50  then Value = 2; 
If 0 <= P < 20 then  
    If all verbs are strongly related then Value=1 
    Else Value= 0.0001; 
If (Value in [4, 5]) then 
    If all Ds for important words then Value=   3 
If (Value ==3) then 
    If all Ds for not important words then Value= 2 
If (Value <> 5 AND Value <> 0) then 
    If all Ds for weakly related words 
Value= Value+ AllAvg 
    Else if at least half Ds for weakly related words 
Value= Value+ WeakAvg 
Otherwise 
Value = Value + UnRelAvg 
Return Value 
 
When we call Score(Ws1s,Ws2s), we take care 
of the following two special cases where it goes 
directly to Value 3: 1) if missing some words leads 
to missing the whole verb/noun phrases and 2) if 
one sentence has all past tense verbs and the other 
has present verbs. 
When we design P inequalities, we make them 
have relaxed boundaries conformed with human 
grading values. For example, we choose P between 
95 and 100 in Value (5); where 95 and 100 equal 
to grades 4.5 and 5 respectively. Value (3) interval 
are values between more than or equal 2.5 and less 
than 4. Then, we utilize the important information 
539
and verb constraints to direct classifications 
through different groups.  
When we design range conditions between val-
ues, we select D to present the distance between 
the sentence pair. As D weak values increase, the 
two sentences become closer.  As D unrelated val-
ues increase, the two sentences become distant.   
We carefully analyzed the training corpora to 
assure that the above thresholds satisfy most of the 
training sentence pairs. Each threshold output was 
manually checked and adjusted to satisfy around 
55% to 75% of the training corpora.   
Applying the above module, the pair of ?A man 
is playing football? and ?The man plays football? 
STS score equals 5.00. The pair of ?A man is sing-
ing and playing? and ?The man plays? STS score 
equals 3.00 since the first one misses ?singing?. 
The pair of ?The cat is drinking milk.? and ?A 
white cat is licking and drinking milk kept on a 
plate.? STS scores equals to 3.4 since they have 
P=0.66, ?white? as unimportant information but 
?licking?, ? kept?, ?plate? as important infor-
mation words. 
2.6 Our Baseline System Description 
Our goal in the second run is to evaluate the relat-
edness of the two sentences using only the words 
in the sentence. Sentences are represented as a vec-
tor (i.e. based on the Vector Space Model) and the 
similarity between the two sentences S1 and S2 is 
(5* cosine similarity). We take into account all 
sentence words such that they are lower-case and 
non-stemmed.    
3 Results and Discussion 
3.1 Rule-based System Analysis 
Our system was implemented in Python and used 
the Natural Language Toolkit (NLTK, 
www.nltk.org/), WordNet and lemmatization mod-
ules. Table 2 provides in the official results of our 
system Pearson-Correlation measure.   
 
D Para Vid Europ OnWn News 
Tr 0.6011 0.7021 0.4528  
Te 0.5440 0.7335 0.3830 0.5860 0.2445 
 
Table 2. Run1 Official Person-Correlation measure  
 
In Table 2, the first row shows the proposed sys-
tem results namely 0.6011, 0.7021 and 0.4528 for 
MSRpar, MSRvid and SMTeuropel training corpo-
ra respectively. The second row shows the test re-
sults, namely 0.5440, 0.7335 and 0.3830, 0.5860 
and 0.2445 for MSRpar, MSRvid and SMTeuro-
pel, On-Wn and SMTnews testing corpora respec-
tively. 
In the Task-6 results (Agirre et al 2012), our 
system was ranked 21th out of 85 participants with 
0.6663 Pearson-Correlation ALL competition rank.  
We tested two WordNet measures, namely the 
shortest path and WUP, the path length to the root 
node from the least common subsumer (LCS) of 
the two concepts, measures on the training corpora. 
In contrast to the shortest path measure, WUP 
measure increased the P versus the D scores on the 
three corpora.  This overestimated many training 
STS scores and negatively affected the correlation 
with the gold standard corpora. Using WUP meas-
ure, the correlations of MSRpar, MSRvid and 
SMTeuropel corpora were 0.5553, 0.3488 and 
0.4819 respectively. We decided to use WordNet 
shortest path measure due to its better correlation 
results. When we used WUP measure on testing 
corpora, the correlations were 0.5103, 0.4617, 
0.4810, 0.6422 and 0.4400 for MSRpar, MSRvid 
and SMTeuropel, On-Wn and SMTnews testing 
corpora respectively. We observed that when we 
used WUP measure on MSRvid corpora, the corre-
lations were degraded. This is because most of 
MSRvid corpus pair sentences talking about hu-
man genders which have high WUP scores when 
comparing with each other. Unfortunately, Word-
Net shortest path measure underestimated SMT-
news pair sentence similarities which affected 
dramatically the related correlation measure. 
Hence, the choice of the suitable WordNet metric 
for the whole corpora is still under our considera-
tion. 
Thresholds and Semantic Pattern: Our current 
efforts are directed towards statistical modeling of 
the system thresholds.  We intend also to use some 
web semantic patterns or phrases, such as ReVerb 
patterns, to boost the semantic scores of single 
words.       
3.2 Baseline System Analysis 
In Table 3, the first row shows the proposed sys-
tem results namely 0.4688, 0.4175 and 0.5349 for 
540
MSRpar, MSRvid and SMTeuropel training corpo-
ra respectively. The second row shows the pro-
posed system results, namely 0.4617, 0.4489 and 
0.4719, 0.6353 and 0.4353 for MSRpar, MSRvid 
and SMTeuropel, On-Wn and SMTnews testing 
corpora respectively. 
 
D Para Vid Europ OnWn News 
Tr 0.4688 0.4175 0.5349  
Te 0.4617 0.4489 0.4719 0.6353 0.4353 
 
Table 3. Run 2 Official Person-Correlation measure  
 
In the Task-6 results (Agirre et al 2012), Run2 
was ranked 72th out of 85 participants with 0.4169 
Pearson-Correlation ALL competition rank. As 
anticipated, Run2 released fair results. Its perfor-
mance is penalized or awarded proportionally to 
the number of exact matching pair words. Accord-
ingly, it may record considerable scores for pairs 
which have highly percentage exact matching 
words. For example, it provides competitive corre-
lation scores compared to other participants on On-
Wn and SMTnews testing corpora. Though, this 
doesn?t imply that it is an ideal solution for STS 
task.  It usually indicates that many corpus pairs 
may have some substantial exact matching words.   
4 Conclusions 
In this paper, we presented systems developed for 
SEMEVAL12- Task6. The first run used both se-
mantics and syntax. The second run, our baseline, 
uses only the words in the initial two sentences and 
defines similarity as the cosine similarity between 
the two sentences. The official task results suggest 
that semantics and syntax (Run1) supersedes the 
words alone (Run 2) with 0.2494 which indicates 
that the words alone are not sufficient to capture 
semantic similarity.  
Acknowledgment 
This material is based upon work supported by the 
National Science Foundation under Grant No. 
(1115774). Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the author(s) and do not 
necessarily reflect the views of the National Sci-
ence Foundation. 
 
References  
Catherine Blake. 2007. The Role of Sentence Structure 
in Recognizing Textual Entailment. RTE Proceedings 
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing:101-106. 
Courtney Corley and Andras Csomai and Rada Mihal-
cea. 2005. Text Semantic Similarity, with Applica-
tions. Proceedings of the Conference on Recent 
Advances in Natural Language Processing (RANLP), 
Borovetz, Bulgaria.  
Dan Klein and Christopher D. Manning. 2002. Fast 
Exact Inference with a Factored Model for Natural 
Language Parsing. In Advances in Neural Infor-
mation Processing Systems 15 (NIPS), Cambridge, 
MA: MIT Press:3-10. 
Dong-bin Hu and  Jun Ding. 2011. Study on Similar 
Engineering Decision Problem Identification Based 
On Combination of Improved Edit-Distance and 
Skeletal Dependency Tree with POS. Systems Engi-
neering Procedia 1: 406?413. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on 
Semantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evaluation 
(SemEval 2012), in conjunction with the First Joint 
Conference on Lexical and Computational Semantics 
(*SEM 2012) 
Eric Bengtson and Dan Roth. 2008. Understanding the 
Value of Features for Coreference Resolution. 
EMNLP:294-303. 
Federica Mandreoli and Riccardo Martoglia and Paolo 
Tiberio. 2002 . A Syntactic Approach for Searching 
Similarities within Sentences. Proceeding of Interna-
tional Conference on Information and Knowledge 
Management:656?637. 
George A. Miller. 1995. WordNet: A Lexical Database 
for English. Communications of the ACM, 38(11): 39- 
41. 
Gerard Salton. 1989. Automatic Text Processing. The 
Transformation, Analysis, and Retrieval of Infor-
mation by Computer. Wokingham, Mass.Addison-
Wesley. 
Gunes Erkan and Dragomir R. Radev. 2004. LexRank: 
Graph-based Lexical Centrality as Salience in Text 
Summarization. Journal of Artificial Intelligence Re-
search 22:457-479. 
Junsheng Zhang, Yunchuan Sun, Huilin Wang, Yanqing 
He. 2011. Calculating Statistical Similarity between 
Sentences. Journal of Convergence Information 
Technology, Volume 6, Number 2: 22-34. 
Liu Wenyin and Xiaojun Quan and Min Feng and Bite 
Qiu. 2010. A Short Text Modeling Method Combin-
ing Semantic and Statistical Information. Information 
Sciences 180: 4031?4041. 
541
Michael Mohler and Rada Mihalcea. 2009. Text-to-text 
Semantic Similarity for Automatic Short Answer 
Grading. Proceedings of the European Chapter of the 
Association for Computational Linguistics (EACL). 
Oliva Jesus and Serrano I. Jose and Mar?a D. del Cas-
tillo and ?ngel Iglesias .2011. SyMSS: A Syntax-
based Measure for Short-Text Semantic Similarity. 
Data and Knowledge Engineering 70: 390?405. 
Oren Etzioni, Anthony Fader, Janara Christensen, Ste-
phen Soderland, and Mausam.  2011. Open Infor-
mation Extraction: The Second Generation.  
Proceedings of the 22nd International Joint Confer-
ence on Artificial Intelligence (IJCAI). 
Rada Mihalcea and Courtney Corley and Carlo Strap-
parava. 2006. Corpus-based and knowledge-based 
measures of text semantic similarity. Proceeding of 
the Twenty-First National Conference on Artificial 
Intelligence and the Eighteenth Innovative Applica-
tions of Artificial Intelligence Conference. 
Peter W. Foltz and Walter Kintsch and  Thomas K Lan-
dauer. 1998. The measurement of textual coherence 
with latent semantic analysis. Discourse Processes 
Vol. 25, No. 2-3: 285-307. 
Samuel Fernando and Mark Stevenson. 2008. A Seman-
tic Similarity Approach to Paraphrase Detection. 
Computational Linguistics (CLUK) 11th Annual Re-
search Colloquium, 2008. 
Satanjeev Banerjee and Ted Pedersen. 2010. An 
Adapted Lesk Algorithm for Word Sense Disambigu-
ation Using WordNet. CICLING:136-145. 
Ted Pedersen, Siddharth Patwardhan, and Jason 
Michelizzi. 2004. WordNet::Similarity-measuring the 
relatedness of concepts. In Proceedings of NAACL, 
2004. 
Vasileios Hatzivassiloglou , Judith L. Klavans , Eleazar 
Eskin.  1999. Detecting text similarity over short 
passages: Exploring linguistic feature combinations 
via machine learning. Proceeding of Empirical 
Methods in natural language processing and Very 
Large Corpora.  
Xiao-Ying Liu and Yi-Ming Zhou and Ruo-Shi Zheng. 
2008. Measuring Semantic Similarity within Sentenc-
es. Proceedings of the Seventh International Confer-
ence on Machine Learning and Cybernetics. 
Yuhua Li, David McLean, Zuhair A. Bandar, James D. 
O?Shea, and Keeley Crockett. Sentence Similarity 
based on Semantic Nets and Corpus statistics. 2006. 
IEEE Transactions on Knowledgeand Data Engineer-
ing Vol. 18, No. 8: 1138-1150. 
 
 
 
 
 
542
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 101?102,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
In Search of Protein Locations 
Catherine Blake1,2 
clblake@illinois.edu 
Wu Zheng1 
wuzheng2@illinois.edu 
1 Graduate School of Library and Information Science 
2 Computer Science and Medical Information Science 
University of Illinois, Urbana Champaign, IL, USA 
 
Abstract 
We present a bootstrapping approach to infer 
new proteins, locations and protein-location 
pairs by combining UniProt seed protein-
location pairs with dependency paths from a 
large collection of text. Of the top 20 system 
proposed protein-location pairs, 18 were in 
UniProt or supported by online evidence. In-
terestingly, 3 of the top 20 locations identified 
by the system were in the UniProt description, 
but missing from the formal ontology.  
1 Introduction 
Identifying subcellular protein locations is an im-
portant problem because the protein location can 
shed light on the protein function. Our goal is to 
identify new proteins, new locations and new pro-
tein-location relationships directly from full-text 
scientific articles. As with many ontological rela-
tions, location relations can be described as a bina-
ry predicate comprising two arguments, 
Location(X, Y) indicates that X is located in Y, 
such as Location (CIC-5, luminal membrane) from 
the sentence: ClC-5 specific signal also appeared 
to be localized close to the luminal membrane of 
the intestinal crypt. 
Identifying protein subcellular locations has 
been framed as a classification task, where features 
include sequences, motifs and amino acid compo-
sition (H?glund, et al 2006) and protein networks 
(Lee et al, 2008). The SherLoc system (Shatkay et 
al., 2007) includes text features the EpiLoc system 
(Brady & Shatkay, 2008) represents text from 
Medline abstracts as a vector of terms and uses a 
support vector machine to predict the most likely 
location for a new protein. Classification accuracy 
varies between species, locations, and datasets.  
We take an alternative strategy in this paper and 
propose a bootstrapping algorithm similar to 
(Gildea & Jurafsky, 2001). The proposed system 
builds on earlier work (Zheng & Blake, 2010) by 
considering a larger set of seed terms and by re-
moving syntactic path constraints. 
2 Approach 
The proposed bootstrapping algorithm is depicted 
in Figure 1. The system identifies lexico-syntactic 
patterns from sentences that include a given set of 
seed terms. Those the patterns are then used to in-
fer new proteins, new locations, and new protein-
location relationships. The system thus requires (a) 
an existing collection of known entity pairs that 
participate in a location relationship (called the 
seed terms) (b) a corpora of texts that report loca-
tion relationships and (c) a syntactic path represen-
tation.  
Our experiments use seed protein-location rela-
tionships from the UniProt knowledge base 
(www.uniprot.org). The complete knowledge base 
comprises more than 80,000 protein names for a 
range of species. The system uses the location and 
the location synonyms from the UniProt controlled 
vocabulary of subcellular locations and membrane 
topologies and orientations (www.uniprot.org/ 
docs/subcell release 2011_2). The system also used 
a list of protein terms that were created by identify-
ing words that immediately precede the word pro-
tein or proteins in the TREC collection. Two-thirds 
of the top 100 proteins in the TREC collection 
were used as seed terms and the remaining 1/3 
were used to evaluate system performance. 
The system was developed and evaluated using 
different subsets of the Genomics Text Retrieval 
(TREC) collection (Hersh, & Voorhees, 2009). 
Specifically 5533 articles in JBC 2002 were used 
for development and ~11,000 articles in JBC 2004 
and 2005 were used in the evaluation. 
The syntactic paths used the dependency tree 
representation produced by the Stanford Parser 
(Klein & Manning., 2003) (version 1.6.4). 
101
Figure 1 ? The Bootstrapping approach used to generate new proteins, subcellular locations and 
protein location pairs. Inferred proteins and locations are depicted with a dashed line. 
3 Results 
The system identified 792 new proteins in the first 
iteration. All but 3 of the most frequent 20 proteins 
were in UniProt. All proteins in the test set were 
identified, but only 10 were in the top 100 proteins.  
The system identified just over 1,200 new pro-
tein-location pairs after the first bootstrapping step. 
We evaluated the twenty most frequent pairs. Two 
erroneous proteins in the previous step caused two 
protein-location pair errors. UniProt reported 13 of 
the remaining 18 protein-location pairs. The five 
remaining pairs, were supported by online sources 
and in sentences within the collection. 
The system identified 493 new locations after 
the second bootstrapping step and we evaluated the 
top 20. Sentences in the collection suggest that 9 of 
the new locations are in fact locations, but that they 
may not be subcellular locations and that 8 pro-
posed locations are too general. Interestingly, 3 of 
the top 20 locations identified by the system are 
mentioned in the UniProt definitions, but are not 
included in the control vocabulary as a synonym, 
which suggests the need for automated approaches 
such as this to supplement  manual efforts.  
Acknowledgments 
This material is based in part upon work sup-
ported by the National Science Foundation under 
Grant IIS-0812522. Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the author(s) and do not nec-
essarily reflect the views of the National Science 
Foundation. 
References 
Brady, S., & Shatkay, H. 2008. EpiLoc: a (working) 
text-based system for predicting protein subcellular 
location., Pac Symp Biocomput (pp. 604-615). 
Gildea, D., & Jurafsky, D. 2001. Automatic labeling of 
semantic roles. Computational Linguistics, 99(9): 1-
43. 
Hersh, W., & Voorhees, E. (2009). TREC genomics 
special issue overview. Information Retrieval, 12(1), 
1-15. 
H?glund, A., D?nnes, P., Blum, T., Adolph, H.W., & 
Kohlbacher, O. 2006. MultiLoc: prediction of protein 
subcellular localization using N-terminal targeting 
sequences, sequence motifs and amino acid composi-
tion. Bioinformatics, 22(10):1158-1165. 
Klein, D., & Manning., C.D. 2003. In Accurate Unlexi-
calized Parsing (pp. 423-430). Paper presented at the 
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-2003). 
Lee, K., Chuang, H.-Y., Beyer, A., Sung, M.-K., Huh, 
W.-K., Lee, B., et al 2008. Protein networks marked-
ly improve prediction of subcellular localization in 
multiple eukaryotic species. Nucleic Acids Research, 
36(20), e136. 
Shatkay, H., H?glund, A., Brady, S., Blum, T., D?nnes, 
P., & Kohlbacher, O. 2007. SherLoc: high-accuracy 
prediction of protein subcellular localization by inte-
grating text and protein sequence data Bioinformat-
ics, 23(11), 1410-1417. 
Zheng, W., & Blake, C. 2010. Bootstrapping Location 
Relations from Text. American Society for Infor-
mation Science and Technology, Pittsburgh, PA. 
Protein
Protein-subcellular 
locations
Augmented protein 
subcellular locations
TREC sentences 
with UniProt 
protein-locations Given path and 
locations find 
sentences with 
new proteins
Path
Given path and 
new protein find 
sentences with 
new locations
Locations
Path
UniProt
TREC
Location
Protein
Protein 
names
Seed
102
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?9,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Identifying Comparative Claim Sentences in Full-Text Scientific Articles 
 
 
Dae Hoon Parka Catherine Blakea,b 
aDepartment of Computer Science bCenter for Informatics Research in Science 
and Scholarship at the Graduate School of 
Library and Information Science 
University of Illinois at Urbana-Champaign University of Illinois at Urbana-Champaign 
Urbana, IL 61801, USA Champaign, IL 61820-6211, USA  
dpark34@illinois.edu clblake@illinois.edu 
 
  
 
Abstract 
Comparisons play a critical role in scientific 
communication by allowing an author to situate 
their work in the context of earlier research 
problems, experimental approaches, and results. 
Our goal is to identify comparison claims 
automatically from full-text scientific articles. 
In this paper, we introduce a set of semantic 
and syntactic features that characterize a 
sentence and then demonstrate how those 
features can be used in three different 
classifiers: Na?ve Bayes (NB), a Support Vector 
Machine (SVM) and a Bayesian network (BN). 
Experiments were conducted on 122 full-text 
toxicology articles containing 14,157 sentences, 
of which 1,735 (12.25%) were comparisons. 
Experiments show an F1 score of 0.71, 0.69, 
and 0.74 on the development set and 0.76, 0.65, 
and 0.74 on a validation set for the NB, SVM 
and BN, respectively.  
1 Introduction 
Comparisons provide a fundamental building block 
in human communication. We continually compare 
products, strategies, and political candidates in our 
daily life, but comparisons also play a central role 
in scientific discourse and it is not a surprise that 
comparisons appear in several models of scientific 
rhetoric. The Create a Research Space (CARS) 
model includes counter-claiming and establishing a 
gap during the ?establishing a niche? phase 
(Swales, 1990), and the Rhetorical Structure 
Theory includes a contrast schema and antithesis 
relation that is used between different nucleus and 
satellite clauses (Mann & Thompson, 1988). 
However, neither of these models identify where 
scientists make these comparisons. In contrast, 
Kircz?s (1991) study of physics articles only 
mentions comparisons with respect to the use of 
data to compare with other experimental results 
(sections 4.3 and 8.1, respectively) with earlier 
work. Similarly, Teufel and Moen?s contrast 
category (which includes the action lexicon s 
better_solution, comparison and contrast) is also 
restricted to contrasts with other work (Teufel & 
Moens, 2002). Lastly the Claim Framework  (CF) 
includes a comparison category, but in contrast to 
the earlier comparisons that reflect how science is 
situated within earlier work, the CF captures 
comparisons between entities (Blake, 2010).  
    Identifying comparisons automatically is 
difficult from a computational perspective 
(Friedman, 1989). For example, the following 
sentence is not a comparison even though it 
contains two words (more than) which are 
indicative of comparisons. More than five methods 
were used. Bresnan claimed that ?comparative 
clause construction in English is almost notorious 
for its syntactic complexity? (Bresnan, 1973), 
p275. Perhaps due to this complexity, several 
instructional books have been written to teach such 
constructs to non-native speakers.  
    Our goal in this paper is to automatically 
identify comparison claims from full-text scientific 
articles, which were first defined in Blake?s Claim 
Framework (Blake, 2010). Comparisons capture a 
binary relationship between two concepts within a 
sentence and the aspect on which the comparison is 
made. For example, ?patients with AML? (a type of 
1
leukemia) and ?normal controls? are being 
compared in the following sentence, and the aspect 
on which the comparison is made is ?the plasma 
concentration of nm23-H1?. The plasma 
concentration of nm23-H1 was higher in patients 
with AML than in normal controls (P = .0001). In 
this paper, we focus on identifying comparison 
sentences and leave extraction of the two concepts 
and the aspect on which the comparison is made as 
future work. Similar to earlier comparison 
sentences in biomedicine, we consider the sentence 
as the unit of analysis (Fiszman, et al 2007). 
    To achieve this goal, we cast the problem as a 
classification activity and defined both semantic 
and syntactic features that are indicative of 
comparisons based on comparison sentences that 
were kindly provided by Fiszman (2007) and 
Blake (2010). With the features in place, we 
conducted experiments using the Na?ve Bayes 
(NB) and Support Vector Machine (SVM) 
classifiers, which both work well on text. We then 
introduce a Bayesian Network (BN) that removes 
some of the independence assumptions made in 
NB model. The subsequent evaluation considers 
more than 1,735 comparison claim sentences that 
were identified in 122 full text toxicology articles. 
    Although automatically detecting comparison 
sentences in full-text articles is challenging, we 
believe that the information conveyed from such 
sentences will provide a powerful new way to 
organize scientific findings. For example, a student 
or researcher could enter a concept of interest and 
the system would provide all the comparisons that 
had been made. Such a system would advance our 
general knowledge of information organization by 
revealing what concepts can be compared. Such a 
strategy could also be used for query expansion in 
information retrieval, and comparisons have 
already been used for question answering (Ballard, 
1989). 
2 Related Work 
Comparisons play an important role in models of 
scientific discourse (see Introduction), because 
authors can compare research hypotheses, data 
collection methods, subject groups, and findings. 
Comparisons are similar to the antithesis in the 
CARS model (Swales, 1990), the contrast schema 
in RST (Mann & Thompson, 1988) and in (Teufel 
& Moens, 2002) and the comparisons category of 
the CF model (Blake, 2010).  
   From a computational linguistic perspective, 
Bresnan (1973) described the comparative clause 
construction in English as ?almost notorious for its 
syntactic complexity?.  Friedman (1989) also 
pointed out that comparative structure is very 
difficult to process by computer since comparison 
can occur in a variety of forms pervasively 
throughout the grammar and can occur almost 
anywhere in a sentence. In contrast to the syntax 
description of comparison sentences, Staab and 
Hahn (1997) provided a description logic 
representation of comparative sentences. Each of 
these linguists studied the construction of 
comparative sentence, but did not distinguish 
comparatives from non-comparative sentences.  
    Beyond the linguistic community, Jindal and 
Liu (2006) have explored comparisons between 
products and proposed a comparative sentence 
mining method based on sequential rule mining 
with words and the neighboring words? Part-of-
Speech tags. The sequential rules are then used 
as features in machine learning algorithms. They 
report that their method achieved a precision of 
79% and a recall of 81% on their data set. We 
too frame the problem as a classification 
activity, but Jindal and Liu use Part-of-Speech 
tags and indicator words as features while we 
use a dependency tree representation to capture 
sentence features. We also constructed a 
Bayesian Network to remove the independence 
assumption of Na?ve Bayes classifier. The 
comparison definition used here also reflects the 
work of Jindal and Liu (2006).  
    The work on product review comparisons was 
subsequently extended to identify the preferred 
product; for example, camera X would be 
extracted from the sentence ?the picture quality 
of Camera X is better than that of Camera Y.? 
(Ganapathibhotla and Liu, 2008). Features used 
for this subsequent work included a comparative 
word, compared features, compared entities, and 
a comparison type. Most recently, Xu et al 
(2011) explored comparative opinion mining 
using Conditional Random Fields (CRF) to 
identify different types of comparison relations 
where two product names must be present in a 
sentence. They report that their approach 
achieved a higher F1 score than the Jindal and 
Liu?s method on mobile phone review data. 
2
Yang and Ko (2011) used maximum entropy 
method and Support Vector Machines (SVM) to 
identify comparison sentences from the web 
based on keywords and Part-of-Speech tags of 
their neighboring words. They achieved an F1-
score of 90% on a data set written in Korean. 
    The experiments reported here consider 
articles in biomedicine and toxicology which are 
similar to those used by Fiszman et al who 
identified comparisons between drugs reported 
in published clinical trial abstracts (Fiszman et 
al., 2007). However, their definition of 
comparative sentence is narrower than ours in 
that non-gradable comparative sentences are not 
considered. Also, the goal is to classify type of 
comparative sentences which is different from 
identifying comparative sentences from a full-
text article that contains non-comparative 
sentences as well. 
    From a methodological standpoint, Na?ve 
Bayes (NB), Support Vector Machines (SVM), 
and Bayesian Network (BN) have been explored 
for variety of text classification problems 
(Sebastiani, 2002). However, we are not aware 
of any studies that have explored these methods 
to identify comparison sentences in full-text 
scientific articles.  
3 Method 
Our goal is to automatically identify comparison 
sentences from full text articles, which can be 
framed as a classification problem. This section 
provides the definitions used in this paper, a 
description of the semantic and syntactic 
features, and the classifiers used to achieve the 
goal. Stated formally: Let S = {S1, S2, ?, SN} be 
a set of sentences in a collection D. The features 
extracted automatically from those sentences will 
be X = {X1, X2, ?, XM}. Each feature Xj is a 
discrete random variable and has a value Xij for 
each sentence Si. Let Ci be a class variable that 
indicates whether a sentence Si is a comparative. 
Thus, the classifier will predict Ci based on the 
feature values Xi1, Xi2, ?, XiM of Si. 
3.1 Definitions 
A comparative sentence describes at least one 
similarity or difference relation between two 
entities. The definition is similar to that in (Jindal 
& Liu, 2006). A sentence may include more than 
one comparison relation and may also include an 
aspect on which the comparison is made. We 
require that the entities participating in the 
comparison relation should be non-numeric and 
exist in the same sentence.   
    A comparison word expresses comparative 
relation between entities. Common comparison 
words include ?similar?, ?different?, and adjectives 
with an ?-er? suffix. A compared entity is an object 
in a sentence that is being compared with another 
object. Objects are typically noun phrases, such as 
a chemical name or biological entity. Other than 
being non-numeric, no other constraints apply to 
the compared entities. A compared aspect captures 
the aspect on which two comparison entities are 
compared. The definition is similar to a feature in 
(Jindal & Liu, 2006). For example: the level of 
significance differed greatly between the first and 
second studies. A compared aspect is optional in 
comparative sentence. 
    There are two comparative relation types: 
gradable and non-gradable (Jindal & Liu, 2006), 
and we further partition the latter into non-
gradable similarity comparison and non-
gradable difference comparison. Also, we 
consider equative comparison (Jindal & Liu, 
2006) as non-gradable. Gradable comparisons 
express an ordering of entities with regard to a 
certain aspect. For example, sentences with 
phrases such as ?greater than?, ?decreased 
compared with?, or ?shorter length than? are 
typically categorized into this type. The 
sentence ?The number of deaths was higher for 
rats treated with the Emulphor vehicle than with 
corn oil and increased with dose for both 
vehicles? is a gradable difference comparison 
where ?higher? is a comparison word, ?rats 
treated with the Emulphor vehicle? and ?rats 
treated with corn oil? are compared entities, and 
?the number of deaths? is a compared aspect.  
    Non-gradable similarity comparisons state 
the similarity between entities. Due to nature of 
similarity, it has a non-gradable property. 
Phrases such as ?similar to?, ?the same as?, ?as ~ 
as?, and ?similarly? can indicate similarity 
comparison in the sentence. The sentence ?Mean 
maternal body weight was similar between 
controls and treated groups just prior to the 
beginning of dosing.? is an example of 
similarity comparison where ?similar? is a 
comparison word, ?controls? and ?treated 
3
groups? are compared entities, and ?Mean 
maternal body weight? is a compared aspect. 
    Non-gradable difference comparisons 
express the difference between entities without 
stating the order of the entities. For example, 
comparison phrases such as ?different from? and 
?difference between? are present in non-gradable 
difference comparison sentences. In the 
sentence ?Body weight gain and food 
consumption were not significantly different 
between groups? there is a single term entity 
?groups?, and a comparison word ?different?. 
With the entity and comparison word, this 
sentence has two comparative relations: one 
with a compared aspect ?body weight gain? and 
another with ?food consumption?. 
3.2 Feature representations 
Feature selection can have significant impact on 
classification performance (Mitchell, 1997). We 
identified candidate features in a pilot study that 
considered 274 comparison sentences in 
abstracts (Fiszman et al, 2007) and 164 
comparison claim sentences in full text articles 
(Blake, 2010). Thirty-five features were 
developed that reflect both lexical and syntactic 
characteristics of a sentence. Lexical features 
explored in these experiments include: 
L1: The first lexical feature uses terms from the 
SPECIALIST lexicon (Browne, McCray, & 
Srinivasan, 2000), a component of the Unified 
Medical Language System (UMLS1, 2011AB) 
and is set to true when the sentence contains 
any inflections that are marked as 
comparisons. We modified the lexicon by 
adding terms in {?better?, ?more?, ?less?, 
?worse?, ?fewer?, ?lesser?} and removing 
terms in {?few?, ?good?, ?ill?, ?later?, ?long-
term?, ?low-dose?, ?number?, ?well?, ?well-
defined?}, resulting in 968 terms in total. 
L2: The second lexical feature captures 
direction. A lexicon of 104 words was created 
using 82 of 174 direction verbs in (Blake, 
2010) and an additional 22 manually compiled 
words. Selections of direction words were 
based on how well the individual word 
predicted a comparison sentence in the 
development set. This feature is set to true 
when a sentence contains any words in the 
lexicon. 
                                                          
1 http://www.nlm.nih.gov/research/umls/quickstart.html 
L3: Set to true when a sentence includes any of 
the following words: from, over or above.  
L4: Set to true when the sentence includes 
either versus or vs.  
L5: Set to true when the sentence includes the 
phrase twice the.  
L6: Set to true when the sentence includes any 
of the following phrases times that of, half 
that of, third that of, fourth that of 
The 27 syntactic features use a combination of 
semantics (words) and syntax. Figure 1 shows a 
dependency tree that was generated using the 
Stanford Parser (version 1.6.9) (Klein & 
Manning, 2003). The tree shown in Figure 1 
would be represented as: 
ROOT [root orders [nsubj DBP, cop is, amod 
several, prep of [pobj magnitude [amod 
mutagenic/carcinogenic [advmod more], prep 
than [pobj BP]], punct .]] 
where dependencies are shown in italics and the 
tree hierarchy is captured using []. The word 
ROOT depicts the parent node of the tree.   
 
Figure 1. Dependency tree for the sentence 
?DBP is several orders of magnitude more 
mutagenic/carcinogenic than BP.? 
 
    We compiled a similarity and difference 
lexicon (SIMDIF), which includes 31 words 
such as similar, different, and same. Words were 
selected in the same way as the direction words 
(see L2). Each term in the SIMDIF lexicon has a 
corresponding set of prepositions that were 
4
collected from dictionaries. For example, the 
word different in the SIMDIF lexicon has two 
corresponding prepositions: ?from? and ?than?. 
    The first four syntactic rules capture 
comparisons containing words in SIMDIF, and 
rules 5 through 24 capture comparisons related 
to the features L1, L2, or both. Each of the rules 
25 and 26 consists of a comparative phrase and 
its syntactic dependency. Each rule is reflected 
as a Boolean feature that is set to true when the 
rule applies and false otherwise. For example, 
rule S1 would be true for the sentence ?X is 
similar to Y?.  
    Subscripts in the templates below depict the 
word identifier and constraints applied to a word. 
For example W2_than means that word 2 is drawn 
from the domain of (than), where numeric 
values such as 2 are used to distinguish between 
words. Similarly, W4_SIMDIF means that the word 
4 is drawn from terms in the SIMDIF lexicon. 
The symbols |, ?, ?, and * depict disjunctions, 
negations, optional, and wildcard operators 
respectively. 
S1: [root W1_SIMDIF [nsubj|cop W2, (prep 
W3)?]] 
S2: [?root W1_SIMDIF [nsubj|cop W2, (prep 
W3)?]] 
    Syntactic rules 3 and 4 capture other forms of 
non-gradable comparisons with connected 
prepositions.  
S3: [(prep W1)?, (* W2)? [ (prep W3)?, 
(acomp|nsubjpass|nsubj|dobj|conj) W4_SIMDIF 
[(prep W5)?]]] 
S4: [(prep W1)?, (* W2)? [ (prep W3)?, 
?(acomp|nsubjpass|nsubj|dobj|conj) 
W4_SIMDIF [(prep W5)?]]] 
    The following syntactic rules capture other 
non-gradable comparisons and gradable 
comparisons. For example, the comparative 
sentence example in Figure 1 has the component 
[prep than], which is satisfied by rule S5. One 
additional rule (rule S27) uses a construct of ?as 
? as?, but it?s not included here due to space 
limitations. 
S5: [ prep W1_than ] 
S6: [ advmod W1_than ] 
S7: [ quantmod|mwe W1_than ] 
S8: [ mark W1_than ] 
S9: [ dep W1_than ] 
S10: [ ?(prep|advmod|quantmod|mwe|mark 
|dep) W1_than ] 
S11: [ advcl|prep W1_compared ] 
S12: [ dep W1_compared ] 
S13: [ ? (advcl|prep|dep) W1_compared ] 
S14: [ advcl W1_comparing ] 
S15: [ partmod|xcomp W1_comparing ] 
S16: [ pcomp W1_comparing ] 
S17: [ nsubj W1_comparison ] 
S18: [ pobj W1_comparison ] 
S19: [ ? (nsubj|pobj) W1_comparison ] 
S20: [ dep W1_contrast ] 
S21: [ pobj W1_contrast ] 
S22: [ advmod W1_relative ] 
S23: [ amod W1_relative ] 
S24: [ ?(advmod|amod) W1_relative ] 
S25: W1_compare [ advmod W2_(well|favorably)] 
S26: W1_% [ nsubj W2 [prep W3_of]] 
 
    Two additional general features were used. 
The preposition feature (PREP) captures the 
most indicative preposition among connected 
prepositions in the rules 1 through 4. It is a 
nominal variable with six possible values, and the 
value assignment is shown in Table 1. When more 
than two values are satisfied, the lowest value is 
assigned. The plural feature (PLURAL) for the 
rules 1 through 4 is set to true when the subject 
of a comparison is in the plural form and false 
otherwise. These two features provide 
information on if the sentence contains 
compared entities which are required in a 
comparison sentence. 
 
Value Preposition connected to SIMDIF word 
1 between, among, or across 
2 proper preposition provided in SIMDIF 
3 between, among, or across, but may not be 
connected to SIMDIF word 
4 in or for 
5 any other prepositions or no preposition 
6 no SIMDIF word is found 
Table 1: PREP value assignment 
3.3 Classifiers 
The Na?ve Bayes (NB), Support Vector Machine 
(SVM) and Bayesian Network (BN) classifiers 
were used in these experiments because they 
work well with text (Sebastiani, 2002).  
   
5
 
 
Figure 2: Bayesian Network for comparative sentences. Multiple features having the same 
connections are placed in a big circle node for the purpose of simple representation. C is a class 
variable (comparative). 
 
    The Bayesian Network model was developed 
to remove the independence assumption in the 
NB model. BN is a directed acyclic graph that 
can compactly represent a probability 
distribution because only the conditional 
probabilities (rather than the joint probabilities) 
need to be maintained. Each node in the BN 
represents a random variable Xi and each 
directed edge reflects influence from the parent 
node to the child node.  
    In order to improve Na?ve Bayes classifier, 
we designed our Bayesian Network model by 
capturing proper conditional dependencies 
among features. Figure 2 shows the BN model 
used in our experiments. The relationships 
between features in BN were determined 
heuristically. Based on our observation, most 
gradable comparisons contain both comparison 
words and corresponding prepositions, so we 
connected such pairs. Also, most non-gradable 
comparisons contained comparison words and 
different kinds of prepositions depending on 
syntactic structure and plurality of subjects, and 
these relations are captured in the network. For 
example, features S5 through S10 depend on L1 
because a preposition ?than? can be a good 
indicative word only if there is a comparison 
word of L1 in the same sentence. Parameters for 
the BN were estimated using maximum 
likelihood estimation (MLE) with additive 
smoothing. Exact inference is feasible because 
all nodes except for the class node are observed. 
4 Results and Discussion 
A pilot study was conducted using 297 and 165 
sentences provided by (Fiszman et al, 2007) 
and (Blake, 2010) respectively to identify an 
initial set of features. Features were then refined 
based on the development set described below 
(section 3 reports the revised features). The BN 
model was also created based on results in the 
development set.   
 
Sentence Type Develop-
ment 
Valid-
ation 
Comparative 
Sentences 
1659 
(12.15%) 
76 
(15.2%) 
Non-comparative 
sentences 
11998 
(87.85%) 
424 
(84.8%) 
Total 13657 
(100%) 
500 
(100%) 
Table 2: Distribution of comparative and non-
comparative sentences. 
 
    Experiments reported in this paper consider 
122 full text articles on toxicology. Figures, 
tables, citations, and references were removed 
from the corpus, and a development set 
comprising 83 articles were drawn at random 
which included 13,657 headings and sentences 
(the development set). Articles in the 
development set were manually inspected by 
three annotators to identify comparison claim 
sentences. Annotators met weekly to discuss 
problematic sentences and all comparison 
sentences were subsequently reviewed by the 
first author and updated where required to 
ensure consistency. Once the feature 
refinements and BN were complete, a random 
6
sample of 500 sentences was drawn from the 
remaining 39 articles (the validation set) which 
were then annotated by the first author. Table 2 
shows that the number of comparison and non-
comparison sentences are similar between the 
development and validation sets. 
    The NB, SVM (LIBSVM package), and BN 
implementations from WEKA were used with 
their default settings (Hall et al, 2009; Chang 
and Lin, 2011). Classifier performance was 
measured using stratified 10-fold cross 
validation and a paired t-test was performed 
(using two-tail p-values 0.05 and 0.01) to 
determine if the performance of the BN model 
was significantly different from the NB and 
SVM. 
    We measured accuracy, the proportion of 
correct predictions, and the area under a ROC 
curve (ROC AUC), which is a plot of true 
positive rate vs. false positive rate. Given the 
skewed dataset (only 12% of the development 
sentences are comparisons), we recorded 
precision, recall, and F1 score of each class, 
where F1 score is a harmonic mean of precision 
and recall. 
 
     NB SVM BN 
Accuracy 0.923 0.933 0.940++++ 
ROC AUC 0.928 0.904 0.933++++ 
Comp. Precision 0.653 0.780 0.782++ 
Comp. Recall 0.778 0.621 0.706--++ 
Comp. F1 score 0.710 0.691 0.742++++ 
Non-comp. Precision 0.968 0.949 0.960--++ 
Non-comp. Recall 0.943 0.976 0.973++- 
Non-comp. F1 score 0.955 0.962 0.966++++ 
Table 3: Development set results. Superscripts 
and subscripts depict statistical significance for 
BN vs. NB and BN vs. SVM respectively. +/- is 
significant at p=0.05 and ++/-- is significant at 
p=0.01. Bold depicts the best performance for 
each metric. 
    Table 3 shows the development set results. 
The accuracy and area under the ROC curve was 
significantly higher in BN compared to the NB 
and SVM models. For comparative sentences, 
recall was the highest with NB, but F1 score 
was significantly higher with BN. Although the 
difference was small, the F1 score for non-
comparative sentences was significantly highest 
in the BN model.  
    Table 4 shows the validation set results, 
which are similar to the development set in that 
the BN model also achieved the highest 
accuracy and area under the ROC curve. The 
BN model had the highest non-comparative F1 
score, but NB had a higher F1 score on 
comparatives.  
 NB SVM BN 
Accuracy 0.924 0.916 0.932 
ROC AUC 0.948 0.883 0.958 
Comp. Precision 0.726 0.886 0.875 
Comp. Recall 0.803 0.513 0.645 
Comp. F1 score 0.763 0.650 0.742 
Non-comp. Precision 0.964 0.919 0.939 
Non-comp. Recall 0.946 0.988 0.983 
Non-comp. F1 score 0.955 0.952 0.961 
Table 4: Validation set results.  
The results suggest that capturing 
dependencies between features helped to 
improve the BN performance in some cases. For 
example, unlike the BN, the NB and SVM 
models incorrectly classified the following 
sentence as comparative: ?The method of 
forward difference was selected for calculation 
of sensitivity coefficients.? The words ?forward? 
and ?difference? would activate features L2 and 
S4, respectively, and 5 would be assigned for 
PREP. Since the BN model captures 
dependencies between L and S features and 
between S and the PREP feature, the probability 
in the BN model would not increase as much as 
in the NB model. To better understand the 
features, we conducted an error analysis of the 
BN classifier on validation set (see Table 5).  
 
 
 Predicted 
 
Class 0 1 
Actual 
Non-comparative (0)  417 7 
Comparative (1) 27 49 
Table 5. Validation confusion matrix for BN. 
    We conducted a closer inspection of the seven 
false positives (i.e. the non-comparative 
sentences that were predicted comparative). In 
four cases, sentences were predicted as 
comparative because two or more independent 
7
weak features were true. For example, in the 
sentence below, the features related to 
?compared? (rule S11) and ?different? (rule S4) 
were true and produced an incorrect 
classification. ?Although these data cannot be 
compared directly to those in the current study 
because they are in a different strain of rat 
(Charles River CD), they clearly illustrate the 
variability in the incidence of glial cell tumors 
in rats.? This sentence is not comparative for 
compared since there is no comparison word 
between these data and current study. Similarly, 
this sentence is not comparative for different 
since only one compared entity is present for it.  
Two of the remaining false positive sentences 
were misclassified because the sentence had a 
comparison word and comparison entities, but 
the sentence was not a claim. The last incorrect 
sentence included a comparison with a numeric 
value. 
 
Reason of misclassification # errors 
Probability is estimated poorly 10 
Comparison is partially covered by 
dependency features 
7 
Comparison word is not in lexicon 7 
Dependency parse error 3 
Total 27 
Table 6. Summary of false negative errors. 
We also investigated false negatives (i.e. 
comparative sentences that were predicted as 
non-comparative by the BN). The reasons of 
errors are summarized in Table 6. Out of 27 
errors, poor estimation was responsible for ten 
errors. These errors mostly come from the 
sparse feature space. For example, in the 
sentence below, the features related to 
?increased? (rule L2) and ?comparison? (rule 
S18) were active, but the probability of 
comparison is 0.424 since the feature space of 
?comparison? feature is sparse, and the feature is 
not indicative enough. ?Mesotheliomas of the 
testicular tunic were statistically ( p < 0.001) 
increased in the high-dose male group in 
comparison to the combined control groups.? 
    Seven of the false negative errors were 
caused by poor dependency features. In this case, 
the comparison was covered by either the parent 
or the child feature node, not by both. Other 
seven errors were caused by missing terms in 
the lexicons, and the last three were caused by a 
dependency parse error. 
5 Conclusion 
Comparison sentences play a critical role in 
scientific discourse as they enable an author to 
fully engage the reader by relating work to earlier 
research hypotheses, data collection methods, 
subject groups, and findings. A review scientific 
discourse models reveals that comparisons have 
been reported as the thesis/antithesis in CARS 
(Swales, 1990), the contrast category in RST 
(Mann & Thompson, 1988) in Teufel & Moens 
(2002) and as a comparisons category in CF 
(Blake, 2010).  
    In this paper, we introduce 35 features that 
capture both semantic and syntactic characteristics 
of a sentence. We then use those features with 
three different classifiers, Na?ve Bayes, Support 
Vector Machines, and Bayesian Networks to 
predict comparison sentences. Experiments 
consider 122 full text documents and 14,157 
sentences, of which 1,735 express at least one 
comparison. To our knowledge, this is the largest 
experiment on comparison sentences expressed in 
full-text scientific articles. 
    Results show that the accuracy and F1 scores 
of the BN were statistically (p<=0.05) higher 
than those of both the NB and SVM classifiers. 
Results also suggest that scientists report claims 
using a comparison sentence in 12.24% of the 
full-text sentences, which is consistent with, but 
more prevalent than in an earlier Claim 
Framework study which reported a rate of 
5.11%. Further work is required to understand 
the source of this variation and the degree to 
which the comparison features and classifiers 
used in this paper can also be used to capture 
comparisons of scientific papers in other 
domains.  
 
Acknowledgement 
This material is based upon work supported by the 
National Science Foundation under Grant No. 
(1115774). Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the author(s) and do not 
necessarily reflect the views of the National 
Science Foundation. 
8
References 
Ballard, B.W. (1989). A General Computational 
Treatment of Comparatives for Natural Language 
Question Answering, Association of 
Computational Linguistics. Vancouver, British 
Columbia, Canada. 
Blake, C. (2010). Beyond genes, proteins, and 
abstracts: Identifying scientific claims from full-
text biomedical articles. Journal of Biomedical 
Informatics, 43, 173-189. 
Bresnan, J.W. (1973). Syntax of the Comparative 
Clause Construction in English. Linguistic Inquiry , 
4(3), 275-343. 
Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a 
library for support vector machines. ACM 
Transactions on Intelligent Systems and 
Technology, 2:27:1--27:27, 2011. 
Browne, A.C., McCray, A.T., & Srinivasan, S. (2000). 
The SPECIALIST LEXICON. Bethesda, Maryland. 
Fiszman, M., Demner-Fushman, D., Lang, F.M., 
Goetz, P., & Rindflesch, T.C. (2007). In 
Interpreting Comparative Constructions in 
Biomedical Text. (pp. 37-144).  
Friedman, C. (1989). A General Computational 
Treatment Of The Comparative, Association of 
Computational Linguistics (pp. 161-168). 
Stroudsburg, PA. 
Ganapathibhotla, M., & Liu, B. (2008). Mining 
Opinions in Comparative Sentences. International 
Conference on Computational Linguistics (Coling). 
Manchester, UK. 
Hall, M., Frank, E., Holmes, G., Pfahringer, B., 
Reutemann, P., & Witten, I.H. (2009). The WEKA 
Data Mining Software: An Update. SIGKDD 
Explorations, 11(1). 
Jindal, N., & Liu, B. (2006). Identifying 
Comparative Sentences in Text Documents, 
Special Interest Group in Information Retrieval 
(SIGIR) Seattle Washington USA, 244-251. 
Jindal, N., & Liu, B. (2006). Mining Comparative 
Sentences and Relations, American Association for 
Artificial Intelligence Boston, MA. 
Kircz, J.G. (1991). Rhetorical structure of scientific 
articles: the case for argumentation analysis in 
information retrieval. Journal of Documentation , 
47(4), 354-372. 
Klein, D., & Manning, C.D. (2003). In Fast Exact 
Inference with a Factored Model for Natural 
Language Parsing. Advances in Neural 
Information Processing Systems, 3-10. 
Mann, W.C., & Thompson, S.A. (1988). Rhetorical 
Structure Theory: Toward a Functional Theory of 
Text Organization. Text, 8(3), 243-281. 
Mitchell, T.M. (1997). Machine Learning: McGraw-
Hill. 
Sebastiani, F. (2002). Machine learning in automated 
text categorization. ACM Computing Surveys, 
34(1), 1 - 47. 
Staab, S., & Hahn, U. (1997). Comparatives in 
Context. National Conference on AI. National 
Conference on Artificial Intelligence 616-621. 
Swales, J. (1990). Genre Analysis: English in 
Academic and Research Settings: Cambridge 
Applied Linguistics. 
Teufel, S., & Moens, M. (2002). Summarizing 
Scientific Articles -- Experiments with Relevance 
and Rhetorical Status. Computational Linguistics , 
28(4), 409-445. 
Xu, K., Liao, S., Li, J., & Song, Y. (2011). Mining 
Comparative Opinions from Customer Reviews for 
Competitive Intelligence. Decision Support 
Systems, 50(4), 743-754. 
Yang, S., & Ko, Y. (2011). Extracting Comparative 
Entities and Predicates from Texts Using 
Comparative Type Classification, Association of 
Computation Linguistics. Portland, OR.
 
9
