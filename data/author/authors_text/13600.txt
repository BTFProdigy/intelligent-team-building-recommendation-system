Coling 2010: Poster Volume, pages 1265?1273,
Beijing, August 2010
Citation Author Topic Model in Expert Search
Yuancheng Tu, Nikhil Johri, Dan Roth, Julia Hockenmaier
University of Illinois at Urbana-Champaign
{ytu,njohri2,danr,juliahmr}@illinois.edu
Abstract
This paper proposes a novel topic model,
Citation-Author-Topic (CAT) model that
addresses a semantic search task we define
as expert search ? given a research area as
a query, it returns names of experts in this
area. For example, Michael Collins would
be one of the top names retrieved given the
query Syntactic Parsing.
Our contribution in this paper is two-fold.
First, we model the cited author informa-
tion together with words and paper au-
thors. Such extra contextual information
directly models linkage among authors
and enhances the author-topic association,
thus produces more coherent author-topic
distribution. Second, we provide a prelim-
inary solution to the task of expert search
when the learning repository contains ex-
clusively research related documents au-
thored by the experts. When compared
with a previous proposed model (Johri
et al, 2010), the proposed model pro-
duces high quality author topic linkage
and achieves over 33% error reduction
evaluated by the standard MAP measure-
ment.
1 Introduction
This paper addresses the problem of searching for
people with similar interests and expertise, given
their field of expertise as the query. Many existing
people search engines need people?s names to do a
?keyword? style search, using a person?s name as
a query. However, in many situations, such infor-
mation is insufficient or impossible to know be-
forehand. Imagine a scenario where the statistics
department of a university invited a world-wide
known expert in Bayesian statistics and machine
learning to give a keynote speech; how can the
organizer notify all the people on campus who
are interested without spamming those who are
not? Our paper proposes a solution to the afore-
mentioned scenario by providing a search engine
which goes beyond ?keyword? search and can re-
trieve such information semantically. The orga-
nizer would only need to input the research do-
main of the keynote speaker, i.e. Bayesian statis-
tics, machine learning, and all professors and stu-
dents who are interested in this topic will be re-
trieved and an email agent will send out the infor-
mation automatically.
Specifically, we propose a Citation-Author-
Topic (CAT) model which extracts academic re-
search topics and discovers different research
communities by clustering experts with similar in-
terests and expertise. CAT assumes three steps of
a hierarchical generative process when producing
a document: first, an author is generated, then that
author generates topics which ultimately generate
the words and cited authors. This model links
authors to observed words and cited authors via
latent topics and captures the intuition that when
writing a paper, authors always first have topics
in their mind, based on which, they choose words
and cite related works.
Corpus linguists or forensic linguists usually
1265
identify authorship of disputed texts based on
stylistic features, such as vocabulary size, sen-
tence length, word usage that characterize a spe-
cific author and the general semantic content is
usually ignored (Diederich et al, 2003). On the
other hand, graph-based and network based mod-
els ignore the content information of documents
and only focus on network connectivity (Zhang
et al, 2007; Jurczyk and Agichtein, 2007). In
contrast, the model we propose in this paper fully
utilizes the content words of the documents and
combines them with the stylistic flavor contex-
tual information to link authors and documents to-
gether to not only identify the authorship, but also
to be used in many other applications such as pa-
per reviewer recommendation, research commu-
nity identification as well as academic social net-
work search.
The novelty of the work presented in this pa-
per lies in the proposal of jointly modeling the
cited author information and using a discrimi-
native multinomial distribution to model the co-
author information instead of an artificial uni-
form distribution. In addition, we apply and eval-
uate our model in a semantic search scenario.
While current search engines cannot support in-
teractive and exploratory search effectively, our
model supports search that can answer a range of
exploratory queries. This is done by semantically
linking the interests of authors to the topics of the
collection, and ultimately to the distribution of the
words in the documents.
In the rest of this paper, we first present some
related work on author topic modeling and expert
search in Sec. 2. Then our model is described in
Sec. 3. Sec. 4 introduces our expert search system
and Sec. 5 presents our experiments and the evalu-
ation. We conclude this paper in Sec. 6 with some
discussion and several further developments.
2 Related Work
Author topic modeling, originally proposed
in (Steyvers et al, 2004; Rosen-Zvi et al, 2004),
is an extension of Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), a probabilistic genera-
tive model that can be used to estimate the proper-
ties of multinomial observations via unsupervised
learning. LDA represents each document as a
mixture of probabilistic topics and each topic as
a multinomial distribution over words. The Au-
thor topic model adds an author layer over LDA
and assumes that the topic proportion of a given
document is generated by the chosen author.
Author topic analysis has attracted much atten-
tion recently due to its broad applications in ma-
chine learning, text mining and information re-
trieval. For example, it has been used to pre-
dict authors for new documents (Steyvers et al,
2004), to recommend paper reviewers (Rosen-Zvi
et al, 2004), to model message data (Mccallum et
al., 2004), to conduct temporal author topic anal-
ysis (Mei and Zhai, 2006), to disambiguate proper
names (Song et al, 2007), to search academic so-
cial networks (Tang et al, 2008) and to generate
meeting status analyses for group decision mak-
ing (Broniatowski, 2009).
In addition, there are many related works on
expert search at the TREC enterprise track from
2005 to 2007, which focus on enterprise scale
search and discovering relationships between enti-
ties. In that setting, the task is to find the experts,
given a web domain, a list of candidate experts
and a set of topics 1. The task defined in our paper
is different in the sense that our topics are hid-
den and our document repositories are more ho-
mogeneous since our documents are all research
papers authored by the experts. Within this set-
ting, we can explore in depth the influence of the
hidden topics and contents to the ranking of our
experts. Similar to (Johri et al, 2010), in this pa-
per we apply CAT in a semantic retrieval scenario,
where searching people is associated with a set of
hidden semantically meaningful topics instead of
their personal names.
In recent literature, there are three main lines of
work that extend author topic analyses. One line
of work is to relax the model?s ?bag-of-words?
assumption by automatically discovering multi-
word phrases and adding them into the original
model (Johri et al, 2010). Similar work has also
been proposed for other topic models such as
Ngram topic models (Wallach, 2006; Wang and
McCallum, 2005; Wang et al, 2007; Griffiths et
al., 2007).
1http://trec.nist.gov/pubs.html
1266
Another line of work models authors informa-
tion as a general contextual information (Mei and
Zhai, 2006) or associates documents with network
structure analysis (Mei et al, 2008; Serdyukov et
al., 2008; Sun et al, 2009). This line of work
aims to propose a general framework to deal with
collections of texts with an associated networks
structure. However, it is based on a different topic
model than ours; for example, Mei?s works (Mei
and Zhai, 2006; Mei et al, 2008) extend proba-
bilistic latent semantic analysis (PLSA), and do
not have cited author information explicitly.
Our proposal follows the last line of work
which extends author topic modeling with spe-
cific contextual information and directly captures
the association between authors and topics to-
gether with this contextual information (Tang et
al., 2008; Mccallum et al, 2004). For exam-
ple, in (Tang et al, 2008), publication venue is
added as one extra piece of contextual informa-
tion and in (Mccallum et al, 2004), email recip-
ients, which are treated as extra contextual infor-
mation, are paired with email authors to model an
email message corpus. In our proposed method,
the extra contextual information consists of the
cited authors in each documents. Such contextual
information directly captures linkage among au-
thors and cited authors, enhances author-topic as-
sociations, and therefore produces more coherent
author-topic distributions.
3 The Citation-Author-Topic (CAT)
Model
CAT extends previously proposed author topic
models by explicitly modelling the cited author
information during the generative process. Com-
pared with these models (Rosen-Zvi et al, 2004;
Johri et al, 2010), whose plate notation is shown
in Fig. 1, CAT (shown in Fig. 2) adds cited au-
thor information and generates authors according
to the observed author distribution.
Four plates in Fig. 1 represent topic (T ), au-
thor (A), document (D) and words in each doc-
ument (Nd) respectively. CAT (Fig. 2) has one
more plate, cited-author topic plate, in which each
topic is represented as a multinomial distribution
over all cited authors (?c).
Within CAT, each author is associated with a
 
D
A
N d
Figure 1: Plate notation of the previously pro-
posed author topic models (Rosen-Zvi et al,
2004; Johri et al, 2010).

D
A
N d

Figure 2: Plate notation of our current model:
CAT generates words W and cited authors C in-
dependently given the topic.
multinomial distribution over all topics, ~?a, and
each topic is a multinomial distribution over all
words, ~?t, as well as a multinomial distribution
over all cited authors ~?c. Three symmetric Dirich-
let conjugate priors, ?, ? and ?, are defined for
each of these three multinomial distributions in
CAT as shown in Fig. 2.
The generative process of CAT is formally de-
fined in Algorithm 1. The model first samples
the word-topic, cited author-topic and the author-
topic distributions according to the three Dirich-
let hyperparameters. Then for each word in each
document, first the author k is drawn from the
observed multinomial distribution and that author
chooses the topic zi, based on which word wi and
cited author ci are generated independently.
CAT differs from previously proposed MAT
(Multiword-enhanced Author Topic) model (Johri
et al, 2010) in two aspects. First of all, CAT uses
1267
Algorithm 1: CAT: A, T ,D,N are four
plates as shown in Fig. 2. The generative pro-
cess of CAT modeling.
Data: A, T ,D,N
for each topic t ? T do
draw a distribution over words:
~?t ? DirN (?) ;
draw a distribution over cited authors:
~?c ? DirC(?) ;
for each author a ? A do
draw a distribution over topics:
~?a ? DirT (?) ;
for each document d ? D and k authors ? d
do
for each word w ? d do
choose an author
k ? Multinomial(Ad) ;
assign a topic i given the author:
zk,i|k ? Multinomial(?a) ;
draw a word from the chosen topic:
wd,k,i|zk,i ? Multinomial(?zk,i) ;
draw a cited author from the topic:
cd,k,i|zk,i ? Multinomial(?zk,i)
cited author information to enhance the model
and assumes independence between generating
the words and cited authors given the topic. Sec-
ondly, instead of an artificial uniform distribution
over all authors and co-authors, CAT uses the ob-
served discriminative multinomial distribution to
generate authors.
3.1 Parameter Estimation
CAT includes three sets of parameters. The T
topic distribution over words, ?t which is similar
to that in LDA. The author-topic distribution ?a as
well as the cited author-topic distribution ?c. Al-
though CAT is a relatively simple model, finding
its posterior distribution over these hidden vari-
ables is still intractable due to their high dimen-
sionality. Many efficient approximate inference
algorithms have been used to solve this problem
including Gibbs sampling (Griffiths and Steyvers,
2004; Steyvers and Griffiths, 2007; Griffiths et al,
2007) and mean-field variational methods (Blei et
al., 2003). Gibbs sampling is a special case of
Markov-Chain Monte Carlo (MCMC) sampling
and often yields relatively simple algorithms for
approximate inference in high dimensional mod-
els.
In our CAT modeling, we use a collapsed Gibbs
sampler for our parameter estimation. In this
Gibbs sampler, we integrated out the hidden vari-
ables ?, ? and ? using the Dirichlet delta func-
tion (Heinrich, 2009). The Dirichlet delta func-
tion with an M dimensional symmetric Dirichlet
prior ? is defined as:
?M (?) =
?
(
?M
)
? (M?)
Based on the independence assumptions de-
fined in Fig. 2, the joint distribution of topics,
words and cited authors given all hyperparame-
ters which originally represented by integrals can
be transformed into the delta function format and
formally derived in Equation 1.
P (~z, ~w,~c|?, ?, ?) (1)
= P (~z|?, ?, ?)P (~w,~c|~z, ?, ?, ?)
= P (~z)P (~w|~z)P (~c|~z)
=
A?
a=1
?(nA+?)
?(?)
T?
z=1
?(nZw+?)
?(?)
T?
z=1
?(nZc+?)
?(?)
The updating equation from which the Gibbs
sampler draws the hidden variable for the current
state j, i.e., the conditional probability of drawing
the kth author Kkj , the ith topic Zij , and the cth
cited author Ccj tuple, given all the hyperparame-
ters and all the observed documents and authors,
cited authors except the current assignment (the
exception is denoted by the symbol ??j), is de-
fined in Equation 2.
P (Zij ,Kkj , Ccj |Wwj ,??j, Ad, ?, ?, ?) (2)
? ?(nZ+?)?(nZ,?j+?)
?(nK+?)
?(nK,?j+?)
?(nC+?)
?(nC,?j+?)
= n
w
i,?j+?w
V
P
w=1
nwi,?j+V ?w
nik,?j+?i
T
P
i=1
nik,?j+T?i
nci,?j+?c
C
P
c=1
nci,?j+C?c
The parameter sets ? and ?, ? can be interpreted
as sufficient statistics on the state variables of
the Markov Chain due to the Dirichlet conjugate
priors we used for the multinomial distributions.
1268
These three sets of parameters are estimated based
on Equations 3 , 4 and 5 respectively, in which nwi
is defined as the number of times the word w is
generated by topic i; nik is defined as the number
of times that topic i is generated by author k and
nic is defined as the number of times that the cited
author c is generated by topic i. The vocabulary
size is V , the number of topics is T and the cited-
author size is C.
?w,i =
nwi + ?w
V?
w=1
nwi + V ?w
(3)
?k,i =
nik + ?i
T?
i=1
nik + T?i
(4)
?c,i =
nci + ?c
C?
c=1
nci + C?c
(5)
The Gibbs sampler used in our experiments is
adapted from the Matlab Topic Modeling Tool-
box 2.
4 Expert Search
In this section, we describe a preliminary re-
trieval system that supports expert search, which
is intended to identify groups of research experts
with similar research interests and expertise by in-
putting only general domain key words. For ex-
ample, we can retrieve Michael Collins via search
for natural language parsing.
Our setting is different from the standard TREC
expert search in that we do not have a pre-defined
list of experts and topics, and our documents are
all research papers authored by experts. Within
this setting, we do not need to identify the status of
our experts, i.e., a real expert or a communicator,
as in TREC expert search. All of our authors and
cited authors are experts and the task amounts to
ranking the experts according to different topics
given samples of their research papers.
The ranking function of this retrieval model is
derived through the CAT parameters. The search
2http://psiexp.ss.uci.edu/research/programs data/
aims to link research topics with authors to by-
pass the proper names of these authors. Our re-
trieval function ranks the joint probability of the
query words (W ) and the target author (a), i.e.,
P (W,a). This probability is marginalized over all
topics, and the probability that an author is cited
given the topic is used as an extra weight in our
ranking function. The intuition is that an author
who is cited frequently should be more prominent
and ranked higher. Formally, we define the rank-
ing function of our retrieval system in Equation 6.
ca denotes when the author is one of the cited au-
thors in our corpus. CAT assumes that words and
authors, and cited authors are conditionally inde-
pendent given the topic, i.e., wi ? a ? ca.
P (W,a) =
?
wi
?i
?
t
P (wi, a|t, ca)P (t, ca)
=
?
wi
?i
?
t
P (wi|t)P (a|t)P (ca|t)P (t)
(6)
W is the input query, which may contain one or
more words. If a multiword is detected within the
query, it is added into the query. The final score
is the sum of all words in this query weighted by
their inverse document frequency ?i.
In our experiments, we chose ten queries which
cover several popular research areas in computa-
tional linguistics and natural language processing
and run the retrieval system based on three mod-
els: the original author topic model (Rosen-Zvi
et al, 2004), the MAT model (Johri et al, 2010)
and the CAT model. In the original author topic
model, query words are treated token by token.
Both MAT and CAT expand the query terms with
multiwords if they are detected inside the original
query. For each query, top 10 authors are returned
from the system. We manually label the relevance
of these 10 authors based on the papers collected
in our corpus.
Two standard evaluation metrics are used to
measure the retrieving results. First we evaluate
the precision at a given cut-off rank, namely pre-
cision at rank k with k ranging from 1 to 10. We
then calculate the average precision (AP) for each
query and the mean average precision (MAP) for
1269
the queries. Unlike precision at k, MAP is sensi-
tive to the ranking and captures recall information
since it assumes the precision of the non-retrieved
documents to be zero. It is formally defined as
the average of precisions computed at the point of
each of the relevant documents in the ranked list
as shown in Equation 7.
AP =
?n
r=1(Precision(r)? rel(r))
| relevant documents | (7)
To evaluate the recall of our system, we col-
lected a pool of authors for six of our queries re-
turned from an academic search engine, Arnet-
Miner (Tang et al, 2008)3 as our reference author
pool and evaluate our recall based on the number
of authors we retrieved from that pool.
5 Experiments and Analysis
In this section, we describe the empirical evalua-
tion of our model qualitatively and quantitatively
by applying our model to the expert search we de-
fined in Sec. 4. We compare the retrieving results
with two other models: Multiword- enhanced Au-
thor Topic (MAT) model (Johri et al, 2010) and
the original author topic model (Rosen-Zvi et al,
2004).
5.1 Data set and Pre-processing
We crawled the ACL anthology website and col-
lected papers from ACL, EMNLP and CONLL
over a period of seven years. The ACL anthol-
ogy website explicitly lists each paper together
with its title and author information. Therefore,
the author information of each paper can be ob-
tained accurately without extracting it from the
original paper. However, many author names are
not represented consistently. For example, the
same author may have his/her middle name listed
in some papers, but not in others. We therefore
normalized all author names by eliminating mid-
dle names from all authors.
Cited authors of each paper are extracted from
the reference section and automatically identified
by a named entity recognizer tuned for citation ex-
traction (Ratinov and Roth, 2009). Similar to reg-
ular authors, all cited authors are also normalized
3http://www.arnetminer.org
Conf. Year Paper Author uni. Vocab.
ACL 03-09 1,326 2,084 34,012 205,260
EMNLP 93-09 912 1,453 40,785 219,496
CONLL 97-09 495 833 27,312 123,176
Total 93-09 2,733 2,911 62,958 366,565
Table 1: Statistics about our data set. Uni. denotes
unigram words and Vocab. denotes all unigrams
and multiword phrases discovered in the data set.
with their first name initial and their full last name.
We extracted about 20,000 cited authors from our
corpus. However, for the sake of efficiency, we
only keep those cited authors whose occurrence
frequency in our corpus is above a certain thresh-
old. We experimented with thresholds of 5, 10 and
20 and retained the total number of 2,996, 1,771
and 956 cited authors respectively.
We applied the same strategy to extract mul-
tiwords from our corpus and added them into
our vocabulary to implement the model described
in (Johri et al, 2010). Some basic statistics about
our data set are summarized in Table 1 4.
5.2 Qualitative Coherence Analysis
As shown by other previous works (Wallach,
2006; Griffiths et al, 2007; Johri et al, 2010),
our model also demonstrates that embedding mul-
tiword tokens into the model can achieve more co-
hesive and better interpretable topics. We list the
top 10 words from two topics of CAT and compare
them with those from the unigram model in Ta-
ble 2. Unigram topics contain more general words
which can occur in every topic and are usually less
discriminative among topics.
Our experiments also show that CAT achieves
better retrieval quality by modeling cited authors
jointly with authors and words. The rank of an
author is boosted if that author is cited more fre-
quently. We present in Table 3 the ranking of one
of our ten query terms to demostrate the high qual-
ity of our proposed model. When compared to the
model without cited author information, CAT not
only retrieves more comprehensive expert list, its
ranking is also more reasonable than the model
without cited author information.
Another observation in our experiments is that
4Download the data and the software package at:
http://L2R.cs.uiuc.edu/?cogcomp/software.php.
1270
Query term: parsing
Proposed CAT Model Model without cited authors
Rank Author Prob. Author Prob.
1 J. Nivre 0.125229 J. Nivre 0.033200
2 C. Manning 0.111252 R. Barzilay 0.023863
3 M. Johnson 0.101342 M. Johnson 0.023781
4 J. Eisner 0.063528 D. Klein 0.018937
5 M. Collins 0.047347 R. McDonald 0.017353
6 G. Satta 0.042081 L. Marquez 0.016003
7 R. McDonald 0.041372 A. Moschitti 0.015781
8 D. Klein 0.041149 N. Smith 0.014792
9 K. Toutanova 0.024946 C. Manning 0.014040
10 E. Charniak 0.020843 K. Sagae 0.013384
Table 3: Ranking for the query term: parsing. CAT achieves more comprehensive and reasonable rank
list than the model without cited author information.
CAT Uni. AT Model
TOPIC 49 Topic 27
pronoun resolution anaphor
antecedent antecedents
coreference resolution anaphoricity
network anphoric
resolution is
anaphor anaphora
pronouns soon
anaphor antecedent determination
semantic knowledge pronominal
proper names salience
TOPIC 14 Topic 95
translation quality hypernym
translation systems seeds
source sentence taxonomy
word alignments facts
paraphrases hyponym
decoder walk
parallel corpora hypernyms
translation system page
parallel corpus logs
translation models extractions
Table 2: CAT with embedded multiword com-
ponents achieves more interpretable topics com-
pared with the unigram Author Topic (AT) model.
some experts who published many papers, but on
heterogeneous topics, may not be ranked at the
very top by models without cited author infor-
mation. However, with cited author information,
those authors are ranked higher. Intuitively this
makes sense since many of these authors are also
the most cited ones.
5.3 Quantitative retrieval results
One annotator labeled the relevance of the re-
trieval results from our expert search system. The
annotator was also given all the paper titles of each
Precision@K
K CAT Model Model w/o Cited Authors
1 0.80 0.80
2 0.80 0.70
3 0.73 0.60
4 0.70 0.50
5 0.68 0.48
6 0.70 0.47
7 0.69 0.40
8 0.68 0.45
9 0.73 0.44
10 0.70 0.44
Table 4: Precision at K evaluation of our proposed
model and the model without cited author infor-
mation.
corresponding retrieved author to help make this
binary judgment. We experiment with ten queries
and retrieve the top ten authors for each query.
We first used the precision at k for evaluation.
We calculate the precision at k for both our pro-
posed CAT model and the MAT model, which
does not have the cited author information. The
results are listed in Table 4. It can be observed
that at every rank position, our CAT model works
better. In order to focus more on relevant retrieval
results, we also calculated the mean average pre-
cision (MAP) for both models. For the given ten
queries, the MAP score for the CAT model is 0.78,
while the MAT model without cited author infor-
mation has a MAP score of 0.67. The CAT model
with cited author information achieves about 33%
error reduction in this experiment.
1271
Query ID Query Term
1 parsing
2 machine translation
3 dependency parsing
4 transliteration
5 semantic role labeling
6 coreference resolution
7 language model
8 Unsupervised Learning
9 Supervised Learning
10 Hidden Markov Model
Table 5: Queries and their corresponding ids we
used in our experiments.
Recall for each query
Query ID CAT Model Model w/o Cite
1 0.53 0.20
2 0.13 0.20
3 0.27 0.13
4 0.13 0.2
5 0.27 0.20
6 0.13 0.26
Average 0.24 0.20
Table 6: Recall comparison between our proposed
model and the model without cited author infor-
mation.
Since we do not have a gold standard experts
pool for our queries, to evaluate recall, we col-
lected a pool of authors returned from an aca-
demic search engine, ArnetMiner (Tang et al,
2008) as our reference author pool and evaluated
our recall based on the number of authors we re-
trieved from that pool. Specifically, we get the
top 15 returned persons from that website for each
query and treat them as the whole set of relevant
experts for that query and our preliminary recall
results are shown in Table 6.
In most cases, the CAT recall is better than that
of the compared model, and the average recall is
better as well. All the queries we used in our ex-
periments are listed in Table 5. And the average
recall value is based on six of the queries which
have at least one overlap author with those in our
reference recall pool.
6 Conclusion and Further Development
This paper proposed a novel author topic model,
CAT, which extends the existing author topic
model with additional cited author information.
We applied it to the domain of expert retrieval
and demonstrated the effectiveness of our model
in improving coherence in topic clustering and au-
thor topic association. The proposed model also
provides an effective solution to the problem of
community mining as shown by the promising re-
trieval results derived in our expert search system.
One immediate improvement would result from
extending our corpus. For example, we can ap-
ply our model to the ACL ARC corpus (Bird et
al., 2008) to check the model?s robustness and en-
hance the ranking by learning from more data. We
can also apply our model to data sets with rich
linkage structure, such as the TREC benchmark
data set or ACL Anthology Network (Radev et al,
2009) and try to enhance our model with the ap-
propriate network analysis.
Acknowledgments
The authors would like to thank Lev Ratinov for
his help with the use of the NER package and the
three anonymous reviewers for their helpful com-
ments and suggestions. The research in this pa-
per was supported by the Multimodal Information
Access & Synthesis Center at UIUC, part of CCI-
CADA, a DHS Science and Technology Center of
Excellence.
References
Bird, S., R. Dale, B. Dorr, B. Gibson, M. Joseph,
M. Kan, D. Lee, B Powley, D. Radev, and Y. Tan.
2008. The acl anthology reference corpus: A refer-
ence dataset for bibliographic research in computa-
tional linguistics. In Proceedings of LREC?08.
Blei, D., A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
Broniatowski, D. 2009. Generating status hierar-
chies from meeting transcripts using the author-
topic model. In In Proceedings of the Workshop:
Applications for Topic Models: Text and Beyond.
Diederich, J., J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with support
vector machines. Applied Intelligence, 19:109?123.
1272
Griffiths, T. and M. Steyvers. 2004. Finding scientific
topic. In Proceedings of the National Academy of
Science.
Griffiths, T., M. Steyvers, and J. Tenenbaum. 2007.
Topics in semantic representation. Psychological
Review.
Heinrich, G. 2009. Parameter estimation for text anal-
ysis. Technical report, Fraunhofer IGD.
Johri, N., D. Roth, and Y. Tu. 2010. Experts? retrieval
with multiword-enhanced author topic model. In
Proceedings of NAACL-10 Semantic Search Work-
shop.
Jurczyk, P. and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using
link analysis. In Proceedings of CIKM?07.
Mccallum, A., A. Corrada-emmanuel, and X. Wang.
2004. The author-recipient-topic model for topic
and role discovery in social networks: Experiments
with enron and academic email. Technical report,
University of Massachusetts Amherst.
Mei, Q. and C. Zhai. 2006. A mixture model for con-
textual text mining. In Proceedings of KDD-2006,
pages 649?655.
Mei, Q., D. Cai, D. Zhang, and C. Zhai. 2008. Topic
modeling with network regularization. In Proceed-
ing of WWW-08:, pages 101?110.
Radev, D., M. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis
of the field of Computational Linguistics. Journal
of the American Society for Information Science and
Technology.
Ratinov, L. and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL).
Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smyth.
2004. the author-topic model for authors and docu-
ments. In Proceedings of UAI.
Serdyukov, P., H. Rode, and D. Hiemstra. 2008. Mod-
eling multi-step relevance propagation for expert
finding. In Proceedings of CIKM?08.
Song, Y., J. Huang, and I. Councill. 2007. Efficient
topic-based unsupervised name disambiguation. In
Proceedings of JCDL-2007, pages 342?351.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.
Steyvers, M., P. Smyth, and T. Griffiths. 2004. Proba-
bilistic author-topic models for information discov-
ery. In Proceedings of KDD.
Sun, Y., J. Han, J. Gao, and Y. Yu. 2009. itopicmodel:
Information network-integrated topic modeling. In
Proceedings of ICDM-2009.
Tang, J., J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
2008. Arnetminer: Extraction and mining of aca-
demic social networks. In Proceedings of KDD-
2008, pages 990?998.
Wallach, H. 2006. Topic modeling; beyond bag of
words. In International Conference on Machine
Learning.
Wang, X. and A. McCallum. 2005. A note on topi-
cal n-grams. Technical report, University of Mas-
sachusetts.
Wang, X., A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discoery with an appli-
cation to information retrieval. In Proceedings of
ICDM.
Zhang, J., M. Ackerman, and L. Adamic. 2007. Ex-
pertise networks in online communities: Structure
and algorithms. In Proceedings of WWW 2007.
1273
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Experts? Retrieval with Multiword-Enhanced Author Topic Model
Nikhil Johri Dan Roth Yuancheng Tu
Dept. of Computer Science Dept. of Linguistics
University of Illinois at Urbana-Champaign
{njohri2,danr,ytu}@illinois.edu
Abstract
In this paper, we propose a multiword-
enhanced author topic model that clusters au-
thors with similar interests and expertise, and
apply it to an information retrieval system that
returns a ranked list of authors related to a key-
word. For example, we can retrieve Eugene
Charniak via search for statistical parsing.
The existing works on author topic model-
ing assume a ?bag-of-words? representation.
However, many semantic atomic concepts are
represented by multiwords in text documents.
This paper presents a pre-computation step as
a way to discover these multiwords in the cor-
pus automatically and tags them in the term-
document matrix. The key advantage of this
method is that it retains the simplicity and
the computational efficiency of the unigram
model. In addition to a qualitative evaluation,
we evaluate the results by using the topic mod-
els as a component in a search engine. We ex-
hibit improved retrieval scores when the docu-
ments are represented via sets of latent topics
and authors.
1 Introduction
This paper addresses the problem of searching peo-
ple with similar interests and expertise without in-
putting personal names as queries. Many existing
people search engines need people?s names to do a
?keyword? style search, using a person?s name as a
query. However, in many situations, such informa-
tion is impossible to know beforehand. Imagine a
scenario where the statistics department of a univer-
sity invited a world-wide known expert in Bayesian
statistics and machine learning to give a keynote
speech; how can the department head notify all the
people on campus who are interested without spam-
ming those who are not? Our paper proposes a solu-
tion to the aforementioned scenario by providing a
search engine which goes beyond ?keyword? search
and can retrieve such information semantically. The
department head would only need to input the do-
main keyword of the keynote speaker, i.e. Bayesian
statistics, machine learning, and all professors and
students who are interested in this topic will be
retrieved. Specifically, we propose a Multiword-
enhanced Author-Topic Model (MATM), a proba-
bilistic generative model which assumes two steps
of generation process when producing a document.
Statistical topical modeling (Blei and Lafferty,
2009a) has attracted much attention recently due to
its broad applications in machine learning, text min-
ing and information retrieval. In these models, se-
mantic topics are represented by multinomial distri-
bution over words. Typically, the content of each
topic is visualized by simply listing the words in or-
der of decreasing probability and the ?meaning? of
each topic is reflected by the top 10 to 20 words in
that list. The Author-Topic Model (ATM) (Steyvers
et al, 2004; Rosen-Zvi et al, 2004) extends the ba-
sic topical models to include author information in
which topics and authors are modeled jointly. Each
author is a multinomial distribution over topics and
each topic is a multinomial distribution over words.
Our contribution to this paper is two-fold. First
of all, our model, MATM, extends the original ATM
by adding semantically coherent multiwords into the
term-document matrix to relax the model?s ?bag-of-
10
words? assumption. Each multiword is discovered
via statistical measurement and filtered by its part of
speech pattern via an off-line way. One key advan-
tage of tagging these semantic atomic units off-line,
is the retention of the flexibility and computational
efficiency in using the simpler word exchangeable
model, while providing better interpretation of the
topics author distribution.
Secondly, to the best of our knowledge, this is
the first proposal to apply the enhanced author topic
modeling in a semantic retrieval scenario, where
searching people is associated with a set of hid-
den semantically meaningful topics instead of their
names. While current search engines cannot sup-
port interactive and exploratory search effectively,
search based on our model serves very well to an-
swer a range of exploratory queries about the doc-
ument collections by semantically linking the inter-
ests of the authors to the topics of the collection, and
ultimately to the distribution of the words in the doc-
uments.
The rest of the paper is organized as follows. We
present some related work on topic modeling, the
original author-topic model and automatic phrase
discovery methods in Sec. 2. Then our model is de-
scribed in Sec. 3. Sec. 4 presents our experiments
and the evaluation of our method on expert search.
We conclude this paper in Sec. 5 with some discus-
sion and several further developments.
2 Related Work
Author topic modeling, originally proposed
in (Steyvers et al, 2004; Rosen-Zvi et al, 2004), is
an extension of another popular topic model, Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), a
probabilistic generative model that can be used to
estimate the properties of multinomial observations
via unsupervised learning. LDA represents each
document as a mixture of probabilistic topics and
each topic as a multinomial distribution over words.
The Author topic model adds an author layer over
LDA and assumes that the topic proportion of a
given document is generated by the chosen author.
Both LDA and the author topic model assume
bag-of-words representation. As shown by many
previous works (Blei et al, 2003; Steyvers et al,
2004), even such unrealistic assumption can actu-
ally lead to a reasonable topic distribution with rel-
atively simple and computationally efficient infer-
ence algorithm. However, this unigram represen-
tation also poses major handicap when interpreting
and applying the hidden topic distributions. The
proposed MATM is an effort to try to leverage this
problem in author topic modeling. There have been
some works on Ngram topic modeling over the orig-
inal LDA model (Wallach, 2006; Wang and McCal-
lum, 2005; Wang et al, 2007; Griffiths et al, 2007).
However, to the best of our knowledge, this paper
is the first to embed multiword expressions into the
author topic model.
Many of these Ngram topic models (Wang and
McCallum, 2005; Wang et al, 2007; Griffiths et
al., 2007) improves the base model by adding a new
indicator variable xi to signify if a bigram should
be generated. If xi = 1, the word wi is gener-
ated from a distribution that depends only on the
previous word to form an Ngram. Otherwise, it is
generated from a distribution only on the topic pro-
portion (Griffiths et al, 2007) or both the previous
words and the latent topic (Wang and McCallum,
2005; Wang et al, 2007). However, these complex
models not only increase the parameter size to V
times larger than the size of the original LDA model
parameters (V is the size of the vocabulary of the
document collection) 1, it also faces the problem of
choosing which word to be the topic of the potential
Ngram. In many text retrieval tasks, the humongous
size of data may prevent us using such complicated
computation on-line. However, our model retains
the computational efficiency by adding a simple tag-
ging process via pre-computation.
Another effort in the current literature to interpret
the meaning of the topics is to label the topics via
a post-processing way (Mei et al, 2007; Blei and
Lafferty, 2009b; Magatti et al, 2009). For example,
Probabilistic topic labeling (Mei et al, 2007) first
extracts a set of candidate label phrases from a refer-
ence collection and represents each candidate label-
ing phrase with a multinomial distribution of words.
Then KL divergence is used to rank the most prob-
able labels for a given topic. This method needs not
only extra reference text collection, but also facing
1LDA collocation models and topic Ngram models also have
parameters for the binomial distribution of the indicator variable
xi for each word in the vocabulary.
11
the problem of finding discriminative and high cov-
erage candidate labels. Blei and Lafferty (Blei and
Lafferty, 2009b) proposed a method to annotate each
word of the corpus by its posterior word topic distri-
bution and then cast a statistical co-occurrence anal-
ysis to extract the most significant Ngrams for each
topic and visualize the topic with these Ngrams.
However, they only applied their method to basic
LDA model.
In this paper, we applied our multiword extension
to the author topic modeling and no extra reference
corpora are needed. The MATM, with an extra pre-
computing step to add meaningful multiwords into
the term-document matrix, enables us to retain the
flexibility and computational efficiency to use the
simpler word exchangeable model, while providing
better interpretation of the topics and author distri-
bution.
3 Multiword-enhanced Author-Topic
Model
The MATM is an extension of the original ATM
(Rosen-Zvi et al, 2004; Steyvers et al, 2004) by
semantically tagging collocations or multiword ex-
pressions, which represent atomic concepts in doc-
uments in the term-document matrix of the model.
Such tagging procedure enables us to retain compu-
tational efficiency of the word-level exchangeabil-
ity of the orginal ATM while provides more sensi-
ble topic distributions and better author topic coher-
ence. The details of our model are presented in Al-
gorithm 1.
3.1 Beyond Bag-of-Words Tagging
The first for loop in Algorithm 1 is the procedure
of our multiword tagging. Commonly used ngrams,
or statistically short phrases in text retrieval, or
so-called collocations in natural language process-
ing have long been studied by linguistics in vari-
ous ways. Traditional collocation discovery meth-
ods range from frequency to mean and variance,
from statistical hypothesis testing, to mutual infor-
mation (Manning and Schtze, 1999). In this pa-
per, we use a simple statistical hypothesis testing
method, namely Pearson?s chi-square test imple-
mented in Ngram Statistic Package (Banerjee and
Pedersen, 2003), enhanced by passing the candidate
phrases through some pre-defined part of speech
patterns that are likely to be true phrases. This
very simple heuristic has been shown to improve the
counting based methods significantly (Justenson and
Katz, 1995).
The ?2 test is chosen since it does not assume any
normally distributed probabilities and the essence
of this test is to compare the observed frequencies
with the frequencies expected for independence. We
choose this simple statistic method since in many
text retrieval tasks the volume of data we see al-
ways makes it impractical to use very sophisticated
statistical computations. We also focus on nominal
phrases, such as bigram and trigram noun phrases
since they are most likely to function as semantic
atomic unit to directly represent the concepts in text
documents.
3.2 Author Topic Modeling
The last three generative procedures described in Al-
gorithm 1 jointly model the author and topic infor-
mation. This generative model is adapted directly
from (Steyvers et al, 2004). Graphically, it can be
visualized as shown in Figure 1.
Figure 1: Plate notation of our model: MATM
The four plates in Fiture 1 represent topic (T), au-
thor (A), document (D) and Words in each document
(Nd) respectively. Each author is associated with a
multinomial distribution over all topics, ~?a and each
topic is a multinomial distribution over all words, ~?t.
Each of these distribution has a symmetric Dirichlet
prior over it, ~? and ~? respectively. When generat-
ing a document, an author k is first chosen according
to a uniform distribution. Then this author chooses
the topic from his/her associated multinomial distri-
bution over topics and then generates a word from
the multinomial distribution of that topic over the
12
words.
Algorithm 1: MATM: A,T ,D,N are four
plates as shown in Fig. 1. The first for loop is the
off-line process of multiword expressions. The
rest of the algorithm is the generative process of
the author topic modeling.
Data: A,T ,D,N
for all documents d ? D do
Part-of-Speech tagging ;
Bigram extraction ;
Part-of Speech Pattern Filtering ;
Add discovered bigrams into N ;
for each author a ? A do
draw a distribution over topics:
~?a ? DirT (~?) ;
for each topic t ? T do
draw a distribution over words:
~?t ? DirN (~?) ;
for each document d ? D and k authors ? d do
for each word w ? d do
choose an author k ? uniformly;
draw a topic assignment i given the
author: zk,i|k ? Multinomial(?a) ;
draw a word from the chosen topic:
wd,k,i|zk,i ? Multinomial(?zk,i) ;
MATM includes two sets of parameters. The T
topic distribution over words, ?t which is similar to
that in LDA. However, instead of a document-topic
distribution, author topic modeling has the author-
topic distribution, ?a. Using a matrix factorization
interpretation, similar to what Steyvers, Griffiths and
Hofmann have pointed out for LDA (Steyvers and
Griffiths, 2007) and PLSI (Hofmann, 1999), a word-
author co-occurrence matrix in author topic model
can be split into two parts: a word-topic matrix ?
and a topic-author matrix ?. And the hidden topic
serves as the low dimensional representation for the
content of the document.
Although the MATM is a relatively simple model,
finding its posterior distribution over these hidden
variables is still intractable. Many efficient ap-
proximate inference algorithms have been used to
solve this problem including Gibbs sampling (Grif-
fiths and Steyvers, 2004; Steyvers and Griffiths,
2007; Griffiths et al, 2007) and mean-field vari-
ational methods (Blei et al, 2003). Gibbs sam-
pling is a special case of Markov-Chain Monte Carlo
(MCMC) sampling and often yields relatively sim-
ple algorithms for approximate inference in high di-
mensional models.
In our MATM, we use a collapsed Gibbs sam-
pler for our parameter estimation. In this Gibbs
sampler, we integrated out the hidden variables ?
and ? as shown by the delta function in equation 2.
This Dirichlet delta function with a M dimentional
symmetric Dirichlet prior is defined in Equation 1.
For the current state j, the conditional probability
of drawing the kth author Kkj and the ith topic Zij
pair, given all the hyperparameters and all the obe-
served documents and authors except the current as-
signment (the exception is denoted by the symbol
?j), is defined in Equation 2.
?M (?) =
?
(
?M
)
? (M?) (1)
P (Zij ,Kkj |Wj = w,Z?j ,K?j ,W?j , Ad, ~?, ~?)
?
?(nZ+~?)
?(nZ,?j+~?)
?(nK+~?)
?(nK,?j+~?)
= n
w
i,?j+ ~?w
?V
w=1 nwi,?j+V ~?w
nik,?j+~?i
?T
i=1 nik,?j+T ~?i
(2)
And the parameter sets ? and ? can be interpreted
as sufficient statistics on the state variables of the
Markov Chain due to the Dirichlet conjugate priors
we used for the multinomial distributions. The two
formulars are shown in Equation 3 and Equation 4 in
which nwi is defined as the number of times that the
word w is generated by topic i and nik is defined as
the number of times that topic i is generated by au-
thor k. The Gibbs sampler used in our experiments
is from the Matlab Topic Modeling Toolbox 2.
?w,i =
nwi + ~?w
?V
w=1 nwi + V ~?w
(3)
?k,i =
nik + ~?i
?T
i=1 nik + T ~?i
(4)
2http://psiexp.ss.uci.edu/research/programs data/toolbox.htm
13
4 Experiments and Analysis
In this section, we describe the empirical evaluation
of our model qualitatively and quantitatively by ap-
plying our model to a text retrieval system we call
Expert Search. This search engine is intended to re-
trieve groups of experts with similar interests and ex-
pertise by inputting only general domain key words,
such as syntactic parsing, information retrieval.
We first describe the data set, the retrieval system
and the evaluation metrics. Then we present the em-
pirical results both qualitatively and quantitatively.
4.1 Data
We crawled from ACL anthology website and col-
lected seven years of annual ACL conference papers
as our corpus. The reference section is deleted from
each paper to reduce some noisy vocabulary, such
as idiosyncratic proper names, and some coding er-
rors caused during the file format conversion pro-
cess. We applied a part of speech tagger3 to tag
the files and retain in our vocabulary only content
words, i.e., nouns, verbs, adjectives and adverbs.
The ACL anthology website explicitly lists each
paper together with its title and author information.
Therefore, the author information of each paper can
be obtained accurately without extracting from the
original paper. We transformed all pdf files to text
files and normalized all author names by eliminating
their middle name initials if they are present in the
listed names. There is a total of 1,326 papers in the
collected corpus with 2, 084 authors. Then multi-
words (in our current experiments, the bigram collo-
cations) are discovered via the ?2 statistics and part
of speech pattern filtering. These multiwords are
then added into the vocabulary to build our model.
Some basic statistics about this corpus is summa-
rized in Table 1.
Two sets of results are evaluated use the retrieval
system in our experiments: one set is based on un-
igram vocabulary and the other with the vocabulary
expanded by the multiwords.
4.2 Evaluation on Expert Search
We designed a preliminary retrieval system to eval-
uate our model. The functionality of this search is
3The tagger is from:
http://l2r.cs.uiuc.edu/?cogcomp/software.php
ACL Corpus Statistics
Year range 2003-2009
Total number of papers 1,326
Total number of authors 2,084
Total unigrams 34,012
Total unigram and multiwords 205,260
Table 1: Description of the ACL seven-year collection in
our experiments
to associate words with individual authors, i.e., we
rank the joint probability of the query words and the
target author P (W,a). This probability is marginal-
ized over all topics in the model to rank all authors
in our corpus. In addition, the model assumes that
the word and the author is conditionally indepen-
dent given the topic. Formally, we define the ranking
function of our retrieval system in Equation 5:
P (W,a) =
?
wi
?i
?
t
P (wi, a|t)P (t)
=
?
wi
?i
?
t
P (wi|t)P (a|t)P (t) (5)
W is the input query, which may contain one or
more words. If a multiword is detected within the
query, it is added into the query. The final score is
the sum of all words in this query weighted by their
inverse document frequency ?i The inverse docu-
ment frequency is defined as Equation 6.
?i =
1
DF (wi)
(6)
In our experiments, we chose ten queries which
covers several most popular research areas in com-
putational linguistics and natural language process-
ing. In our unigram model, query words are treated
token by token. However, in our multiword model,
if the query contains a multiword inside our vocabu-
lary, it is treated as an additional token to expand the
query. For each query, top 10 authors are returned
from the system. We manually label the relevance
of these 10 authors based on the papers they submit-
ted to these seven-year ACL conferences collected
in our corpus. Two evaluation metrics are used to
measure the precision of the retrieving results. First
we evaluate the precision at a given cut-off rank,
namely precision at K with K ranging from 1 to 10.
14
We also calculate the average precision (AP) for
each query and the mean average precision (MAP)
for all the 10 queries. Average precision not only
takes ranking as consideration but also emphasizes
ranking relevant documents higher. Different from
precision at K, it is sensitive to the ranking and cap-
tures some recall information since it assumes the
precision of the non-retrieved documents to be zero.
It is defined as the average of precisions computed
at the point of each of the relevant documents in the
ranked list as shown in equation 7.
AP =
?n
r=1(Precision(r)? rel(r))
?
relevant documents
(7)
Currently in our experiments, we do not have a
pool of labeled authors to do a good evaluation of
recall of our system. However, as in the web brows-
ing activity, many users only care about the first sev-
eral hits of the retrieving results and precision at K
and MAP measurements are robust measurements
for this purpose.
4.3 Results and Analysis
In this section, we first examine the qualitative re-
sults from our model and then report the evaluation
on the external expert search.
4.3.1 Qualitative Coherence Analysis
As have shown by other works on Ngram topic
modeling (Wallach, 2006; Wang et al, 2007; Grif-
fiths et al, 2007), our model also demonstrated that
embedding multiword tokens into the simple author
topic model can always achieve more coherent and
better interpretable topics. We list top 15 words
from two topics of the multiword model and uni-
gram model respectively in Table 2. Unigram topics
contain more general words which can occur in ev-
ery topic and are usually less discriminative among
topics.
Our experiments also show that embedding the
multiword tokens into the model achieves better
clustering of the authors and the coherence between
authors and topics. We demonstrate this qualita-
tively by listing two examples respectively from the
multiword models and the unigram model in Table 3.
For example, for the topic on dependency pars-
ing, unigram model missed Ryan-McDonald and the
ranking of the authors are also questionable. Further
MultiWord Model Unigram Model
TOPIC 4 Topic 51
coreference-resolution resolution
antecedent antecedent
treesubstitution-grammars pronoun
completely pronouns
pronoun is
resolution information
angry antecedents
candidate anaphor
extracted syntactic
feature semantic
pronouns coreference
model anaphora
perceptual-cooccurrence definite
certain-time model
anaphora-resolution only
TOPIC 49 Topic 95
sense sense
senses senses
word-sense disambiguation
target-word word
word-senses context
sense-disambiguation ontext
nouns ambiguous
automatically accuracy
semantic-relatedness nouns
disambiguation unsupervised
provided target
ambiguous-word predominant
concepts sample
lexical-sample automatically
nouns-verbs meaning
Table 2: Comparison of the topic interpretation from the
multiword-enhanced and the unigram models. Qualita-
tively, topics with multiwords are more interpretable.
quantitative measurement is listed in our quantita-
tive evaluation section. However, qualitatively, mul-
tiword model seems less problematic.
Some of the unfamiliar author may not be easy to
make a relevance judgment. However, if we trace
all the papers the author wrote in our collected cor-
pus, many of the authors are coherently related to the
topic. We list all the papers in our corpus for three
authors from the machine translation topic derived
from the multiword model in Table 4 to demonstrate
the coherence between the author and the related
topic. However, it is also obvious that our model
missed some real experts in the corresponding field.
15
MultiWord Model Unigram Model
Topic 63 Topic 145 Topic 23 Topic 78
Word Word Word Word
translation dependency-parsing translation dependency
machine-translation dependency-tree translations head
language-model dependency-trees bilingual dependencies
statistical-machine dependency pairs structure
translations dependency-structures language structures
phrases dependency-graph machine dependent
translation-model dependency-relation parallel order
decoding dependency-relations translated word
score order monolingual left
decoder does quality does
Author Author Author Author
Shouxun-Lin Joakim-Nivre Hua-Wu Christopher-Manning
David-Chiang Jens-Nilsson Philipp-Koehn Hisami-Suzuk
Qun-Liu David-Temperley Ming-Zhou Kenji-Sagae
Philipp-Koehn Wei-He Shouxun-Lin Jens-Nilsson
Chi-Ho-Li Elijah-Mayfield David-Chiang Jinxi-Xu
Christoph-Tillmann Valentin-Jijkoun Yajuan-Lu Joakim-Nivre
Chris-Dyer Christopher-Manning Haifeng-Wang Valentin-Jijkoun
G-Haffari Jiri-Havelka Aiti-Aw Elijah-Mayfield
Taro-Watanabe Ryan-McDonald Chris-Callison-Burch David-Temperley
Aiti-Aw Andre-Martins Franz-Och Julia-Hockenmaier
Table 3: Two examples for topic and author coherece from multiword-enhanced model and unigram model. Top 10
words and authors are listed accordingly for each model.
For example, we did not get Kevin Knight for the
machine translation topic. This may be due to the
limitation of our corpus since we only collected pa-
pers from one conference in a limited time, or be-
cause usually these experts write more divergent on
various topics.
Another observation in our experiment is that
some experts with many papers may not be ranked
at the very top by our system. However, they have
pretty high probability to associate with several top-
ics. Intuitively this makes sense, since many of these
famous experts write papers with their students in
various topics. Their scores may therefore not be as
high as authors who have fewer papers in the corpus
which are concentrated in one topic.
4.3.2 Results from Expert Search
One annotator labeled the relevance of the re-
trieval results from our expert search system. The
annotator was also given all the paper titles of each
corresponding retrieved author to help make the bi-
nary judgment. We experimented with ten queries
and retrieved the top ten authors for each query.
We first used the precision at K for evaluation. we
calculate the precision at K for both of our multi-
word model and the unigram model and the results
are listed in Table 5. It is obvious that at every rank
position, the multiword model works better than the
unigram model. In order to focus more on relevant
retrieval results, we then calculate the average preci-
sion for each query and mean average precision for
both models. The results are in Table 6.
When only comparing the mean average precision
(MAP), the multiword model works better. How-
ever, when examining the average precision of each
query within these two models, the unigram model
also works pretty well with some queries. How the
query words may interact with our model deserves
further investigation.
5 Discussion and Further Development
In this paper, we extended the existing author topic
model with multiword term-document input and ap-
plied it to the domain of expert retrieval. Although
our study is preliminary, our experiments do return
16
Author Papers from ACL(03-09)
Shouxun-Lin
Log-linear Models for Word Alignment
Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation
Tree-to-String Alignment Template for Statistical Machine Translation
Forest-to-String Statistical Translation Rules
Partial Matching Strategy for Phrase-based Statistical Machine Translation
David-Chiang
A Hierarchical Phrase-Based Model for Statistical Machine Translation
Word Sense Disambiguation Improves Statistical Machine Translation
Forest Rescoring: Faster Decoding with Integrated Language Models
Fast Consensus Decoding over Translation Forests
Philipp-Koehn
Feature-Rich Statistical Translation of Noun Phrases
Clause Restructuring for Statistical Machine Translation
Moses: Open Source Toolkit for Statistical Machine Translation
Enriching Morphologically Poor Languages for Statistical Machine Translation
A Web-Based Interactive Computer Aided Translation Tool
Topics in Statistical Machine Translation
Table 4: Papers in our ACL corpus for three authors related to the ?machine translation? topic in Table 3.
Precision@K
K Multiword Model Unigram Model
1 0.90 0.80
2 0.80 0.80
3 0.73 0.67
4 0.70 0.65
5 0.70 0.64
6 0.72 0.65
7 0.71 0.64
8 0.71 0.66
9 0.71 0.66
10 0.70 0.64
Table 5: Precision at K evaluation of the multiword-
enhanced model and the unigram model.
promising results, demonstrating the effectiveness
of our model in improving coherence in topic clus-
ters. In addition, the use of the MATM for expert
retrieval returned some useful preliminary results,
which can be further improved in a number of ways.
One immediate improvement would be an exten-
sion of our corpus. In our experiments, we consid-
ered only ACL papers from the last 7 years. If we
extend our data to cover papers from additional con-
ferences, we will be able to strengthen author-topic
associations for authors who submit papers on the
same topics to different conferences. This will also
allow more prominent authors to come to the fore-
front in our search application. Such a modifica-
Average Precision (AP)
Query Multi. Mod. Uni. Mod.
Language Model 0.79 0.58
Unsupervised Learning 1.0 0.78
Supervised Learning 0.84 0.74
Machine Translation 0.95 1.0
Semantic Role Labeling 0.81 0.57
Coreference Resolution 0.59 0.72
Hidden Markov Model 0.93 0.37
Dependency Parsing 0.75 0.94
Parsing 0.81 0.98
Transliteration 0.62 0.85
MAP: 0.81 0.75
Table 6: Average Precision (AP) for each query and Mean
Average Precision (MAP) of the multiword-enhanced
model and the unigram model.
tion would require us to further increase the model?s
computational efficiency to handle huge volumes of
data encountered in real retrieval systems.
Another further development of this paper is the
addition of citation information to the model as a
layer of supervision for the retrieval system. For in-
stance, an author who is cited frequently could have
a higher weight in our system than one who isn?t,
and could occur more prominently in query results.
Finally, we can provide a better evaluation of our
system through a measure of recall and a simple
baseline system founded on keyword search of pa-
per titles. Recall can be computed via comparison to
a set of expected prominent authors for each query.
17
Acknowledgments
The research in this paper was supported by the Mul-
timodal Information Access & Synthesis Center at
UIUC, part of CCICADA, a DHS Science and Tech-
nology Center of Excellence.
References
S. Banerjee and T. Pedersen. 2003. The design, im-
plementation, and use of the Ngram Statistic Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381.
D. Blei and J. Lafferty. 2009a. Topic models. In A. Sri-
vastava and M. Sahami, editors, Text Mining: Theory
and Applications. Taylor and Francis.
D. Blei and J. Lafferty. 2009b. Visualiz-
ing topics with multi-word expressions. In
http://arxiv.org/abs/0907.1013.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topic. In Proceedings of the National Academy of Sci-
ence.
T. Griffiths, M. Steyvers, and J. Tenenbaum. 2007. Top-
ics in semantic representation. Psychological Review.
T. Hofmann. 1999. Probabilistic latent semantic index-
ing. In Proceedings of SIGIR.
J. Justenson and S. Katz. 1995. Technical terminology:
some linguistic properties and an algorithm for inden-
tification in text. Natural Language Engineering.
D. Magatti, S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In ISDA, pages 1227?
1232.
Christopher D. Manning and Hinrich Schtze. 1999.
Foundations of Statistical Natural Language Process-
ing. Cambridge, Massachusetts.
Q. Mei, X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proceedings
of the 13th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 490?
499.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. the author-topic model for authors and docu-
ments. In Proceedings of UAI.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.
M. Steyvers, P. Smyth, and T. Griffiths. 2004. Proba-
bilistic author-topic models for information discovery.
In Proceedings of KDD.
H. Wallach. 2006. Topic modeling; beyond bag
of words. In International Conference on Machine
Learning.
X. Wang and A. McCallum. 2005. A note on topical n-
grams. Technical report, University of Massachusetts.
X. Wang, A. McCallum, and X. Wei. 2007. Topical n-
grams: Phrase and topic discoery with an application
to information retrieval. In Proceedings of ICDM.
18
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 124?132,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
A Study of Academic Collaboration in Computational Linguistics with
Latent Mixtures of Authors
Nikhil Johri, Daniel Ramage
Department of Computer Science
Stanford University
Stanford, CA, USA
Daniel A. McFarland
School of Education
Stanford University
Stanford, CA, USA
{njohri2,dramage,dmcfarla,jurafsky}@stanford.edu
Daniel Jurafsky
Department of Linguistics
Stanford University
Stanford, CA, USA
Abstract
Academic collaboration has often been at
the forefront of scientific progress, whether
amongst prominent established researchers, or
between students and advisors. We suggest a
theory of the different types of academic col-
laboration, and use topic models to computa-
tionally identify these in Computational Lin-
guistics literature. A set of author-specific
topics are learnt over the ACL corpus, which
ranges from 1965 to 2009. The models are
trained on a per year basis, whereby only pa-
pers published up until a given year are used
to learn that year?s author topics. To determine
the collaborative properties of papers, we use,
as a metric, a function of the cosine similarity
score between a paper?s term vector and each
author?s topic signature in the year preceding
the paper?s publication. We apply this metric
to examine questions on the nature of collabo-
rations in Computational Linguistics research,
finding that significant variations exist in the
way people collaborate within different sub-
fields.
1 Introduction
Academic collaboration is on the rise as single au-
thored work becomes less common across the sci-
ences (Rawlings and McFarland, 2011; Jones et al,
2008; Newman, 2001). In part, this rise can be at-
tributed to the increasing specialization of individual
academics and the broadening in scope of the prob-
lems they tackle. But there are other advantages to
collaboration, as well: they can speed up produc-
tion, diffuse knowledge across authors, help train
new scientists, and are thought to encourage greater
innovation. Moreover, they can integrate scholarly
communities and foster knowledge transfer between
related fields. But all collaborations aren?t the same:
different collaborators contribute different material,
assume different roles, and experience the collabo-
ration in different ways. In this paper, we present
a new frame for thinking about the variation in col-
laboration types and develop a computational metric
to characterize the distinct contributions and roles of
each collaborator within the scholarly material they
produce.
The topic of understanding collaborations has at-
tracted much interest in the social sciences over the
years. Recently, it has gained traction in computer
science, too, in the form of social network analysis.
Much work focuses on studying networks formed
via citations (Radev et al, 2009; White and Mccain,
1998), as well as co-authorship links (Nascimento
et al, 2003; Liu et al, 2005). However, these works
focus largely on the graphical structure derived from
paper citations and author co-occurrences, and less
on the textual content of the papers themselves. In
this work, we examine the nature of academic col-
laboration using text as a primary component.
We propose a theoretical framework for determin-
ing the types of collaboration present in a docu-
ment, based on factors such as the number of es-
tablished authors, the presence of unestablished au-
thors and the similarity of the established authors?
past work to the document?s term vector. These col-
laboration types attempt to describe the nature of co-
authorships between students and advisors (e.g. ?ap-
prentice? versus ?new blood?) as well as those solely
between established authors in the field. We present
a decision diagram for classifying papers into these
types, as well as a description of the intuition behind
each collaboration class.
124
We explore our theory with a computational
method to categorize collaborative works into their
collaboration types using an approach based on topic
modeling, where we model every paper as a la-
tent mixture of its authors. For our system, we use
Labeled-LDA (LLDA (Ramage et al, 2009)) to train
models over the ACL corpus for every year of the
words best attributed to each author in all the papers
they write. We use the resulting author signatures
as a basis for several metrics that can classify each
document by its collaboration type.
We qualitatively analyze our results by examin-
ing the categorization of several high impact papers.
With consultation from prominent researchers and
textbook writers in the field, we demonstrate that our
system is able to differentiate between the various
types of collaborations in our suggested taxonomy,
based only on words used, at low but statistically
significant accuracy. We use this same similarity
score to analyze the ACL community by sub-field,
finding significant deviations.
2 Related Work
In recent years, popular topic models such as La-
tent Dirichlet Allocation (Blei et al, 2003) have
been increasingly used to study the history of sci-
ence by observing the changing trends in term based
topics (Hall et al, 2008), (Gerrish and Blei, 2010).
In the case of Hall et al, regular LDA topic mod-
els were trained over the ACL anthology on a per
year basis, and the changing trends in topics were
studied from year to year. Gerrish and Blei?s work
computed a measure of influence by using Dynamic
Topic Models (Blei and Lafferty, 2006) and study-
ing the change of statistics of the language used in a
corpus.
These models propose interesting ideas for utiliz-
ing topic modeling to understand aspects of scien-
tific history. However, our primary interest, in this
paper, is the study of academic collaboration be-
tween different authors; we therefore look to learn
models for authors instead of only documents. Pop-
ular topic models for authors include the Author-
Topic Model (Rosen-Zvi et al, 2004), a simple
extension of regular LDA that adds an additional
author variable over the topics. The Author-Topic
Model learns a distribution over words for each
topic, as in regular LDA, as well as a distribution
over topics for each author. Alternatively, Labeled
LDA (Ramage et al, 2009), another LDA variation,
offers us the ability to directly model authors as top-
ics by considering them to be the topic labels for the
documents they author.
In this work, we use Labeled LDA to directly
model probabilistic term ?signatures? for authors. As
in (Hall et al, 2008) and (Gerrish and Blei, 2010),
we learn a new topic model for each year in the cor-
pus, allowing us to account for changing author in-
terests over time.
3 Computational Methodology
The experiments and results discussed in this paper
are based on a variation of the LDA topic model run
over data from the ACL corpus.
3.1 Dataset
We use the ACL anthology from years 1965 to 2009,
training over 12,908 papers authored by over 11,355
unique authors. We train our per year topic mod-
els over the entire dataset; however, when evaluating
our results, we are only concerned with papers that
were authored by multiple individuals as the other
papers are not collaborations.
3.2 Latent Mixture of Authors
Every abstract in our dataset reflects the work, to
some greater or lesser degree, of all the authors of
that work. We model these degrees explicitly us-
ing a latent mixture of authors model, which takes
its inspiration from the learning machinery of LDA
(Blei et al, 2003) and its supervised variant La-
beled LDA (Ramage et al, 2009). These models
assume that documents are as a mixture of ?topics,?
which themselves are probability distributions over
the words in the vocabulary of the corpus. LDA
is completely unsupervised, assuming that a latent
topic layer exists and that each word is generated
from one underlying topic from this set of latent top-
ics. For our purposes, we use a variation of LDA in
which we assume each document to be a latent mix-
ture of its authors. Unlike LDA, where each docu-
ment draws a multinomial over all topics, the latent
mixture of authors model we use restricts a docu-
ment to only sample from topics corresponding to
125
its authors. Also, unlike models such as the Author-
Topic Model (Rosen-Zvi et al, 2004), where au-
thors are modeled as distributions over latent top-
ics, our model associates each author to exactly one
topic, modeling authors directly as distributions over
words.
Like other topic models, we will assume a genera-
tive process for our collection of D documents from
a vocabulary of size V . We assume that each docu-
ment d has Nd terms and Md authors from a set of
authors A. Each author is described by a multino-
mial distribution ?a over words V , which is initially
unobserved. We will recover for each document a
hidden multinomial ?(d) of length Md that describes
which mixture of authors? best describes the doc-
ument. This multinomial is in turn drawn from a
symmetric Dirichlet distribution with parameter ?
restrict to the set of authors ?(d) for that paper. Each
document?s words are generated by first picking an
author zi from ?(d) and then drawing a word from
the corresponding author?s word distribution. For-
mally, the generative process is as follows:
? For each author a, generate a distribution ?a over
the vocabulary from a Dirichlet prior ?
? For each document d, generate a multinomial mix-
ture distribution ?(d) ? Dir(?.1?(d))
? For each document d,
? For each i ? {1, ..., Nd}
? Generate zi ? {?
(d)
1 , ..., ?
(d)
Md
} ?
Mult(?(d))
? Generate wi ? {1, ..., V } ?Mult(?zi)
We use Gibbs sampling to perform inference in
this model. If we consider our authors as a label
space, this model is equivalent to that of Labeled
LDA (Ramage et al, 2009), which we use for in-
ference in our model, using the variational objec-
tive in the open source implementation1. After in-
ference, our model discovers the distribution over
terms that best describes that author?s work in the
presence of other authors. This distribution serves
as a ?signature? for an author and is dominated by
the terms that author uses frequently across collabo-
rations. It is worth noting that this model constrains
the learned ?topics? to authors, ensuring directly in-
terpretable results that do not require the interpreta-
1http://nlp.stanford.edu/software/tmt/
tion of a latent topic space, such as in (Rosen-Zvi et
al., 2004).
To imbue our model with a notion of time, we
train a separate LLDA model for each year in the
corpus, training on only those papers written before
and during the given year. Thus, we have separate
?signatures? for each author for each year, and each
signature only contains information for the specific
author?s work up to and including the given year.
Table 1 contains examples of such term signatures
computed for two authors in different years. The top
terms and their fractional counts are displayed.
4 Studying Collaborations
There are several ways one can envision to differen-
tiate between types of academic collaborations. We
focus on three factors when creating collaboration
labels, namely:
? Presence of unestablished authors
? Similarity to established authors
? Number of established authors
If an author whom we know little about is present
on a collaborative paper, we consider him or her to
be a new author. We threshold new authors by the
number of papers they have written up to the pub-
lication year of the paper we are observing. De-
pending on whether this number is below or above a
threshold value, we consider an author to be estab-
lished or unestablished in the given year.
Similarity scores are measured using the trained
LLDA models described in Section 3.2. For any
given paper, we measure the similarity of the pa-
per to one of its (established) authors by calculating
the cosine similarity of the author?s signature in the
year preceding the paper?s publication to the paper?s
term-vector.
Using the aforementioned three factors, we define
the following types of collaborations:
? Apprenticeship Papers are authored by one or
more established authors and one or more un-
established authors, such that the similarity of
the paper to more than half of the established
authors is high. In this case, we say that the
new author (or authors) was an apprentice of
126
Philipp Koehn, 2002 Philipp Koehn, 2009 Fernando Pereira, 1985 Fernando Pereira, 2009
Terms Counts Terms Counts Terms Counts Terms Counts
word 3.00 translation 69.78 grammar 14.99 type 40.00
lexicon 2.00 machine 34.67 phrase 10.00 phrase 30.89
noun 2.00 phrase 26.85 structure 7.00 free 23.14
similar 2.00 english 23.86 types 6.00 grammar 23.10
translation 1.29 statistical 19.51 formalisms 5.97 constraint 23.00
purely 0.90 systems 18.32 sharing 5.00 logical 22.41
accuracy 0.90 word 16.38 unification 4.97 rules 21.72
Table 1: Example term ?signatures? computed by running a Labeled LDA model over authors in the ACL corpus on a
per year basis: top terms for two authors in different years are shown alongside their fractional counts.
the established authors, continuing in their line
of work.
? New Blood Papers are authored by one estab-
lished author and one or more unestablished au-
thors, such that the similarity of the paper to the
established author is low. In this case, we say
that the new author (or authors) provided new
ideas or worked in an area that was dissimilar to
that which the established author was working
in.
? Synergistic Papers are authored only by es-
tablished authors such that it does not heavily
resemble any authors? previous work. In this
case, we consider the paper to be a product of
synergy of its authors.
? Catalyst Papers are similar to synergistic
ones, with the exception that unestablished au-
thors are also present on a Catalyst Paper. In
this case, we hypothesize that the unestablished
authors were the catalysts responsible for get-
ting the established authors to work on a topic
dissimilar to their previous work.
The decision diagram in Figure 1 presents an easy
way to determine the collaboration type assigned to
a paper.
5 Quantifying Collaborations
Following the decision diagram presented in Figure
1 and using similarity scores based on the values
returned by our latent author mixture models (Sec-
tion 3.2), we can deduce the collaboration type to
assign to any given paper. However, absolute cate-
gorization requires an additional thresholding of au-
thor similarity scores. To avoid the addition of an
arbitrary threshold, instead of directly categorizing
papers, we rank them based on the calculated sim-
ilarity scores on three different spectra. To facili-
tate ease of interpretation, the qualitative examples
we present are drawn from high PageRank papers as
calculated in (Radev et al, 2009).
5.1 The MaxSim Score
To measure the similarity of authors? previous work
to a paper, we look at the cosine similarity between
the term vector of the paper and each author?s term
signature. We are only interested in the highest co-
sine similarity score produced by an author, as our
categories do not differentiate between papers that
are similar to one author and papers that are sim-
ilar to multiple authors, as long as high similarity
to any single author is present. Thus, we choose
our measure, the MaxSim score, to be defined as:
max
a?est
cos(asig, paper)
We choose to observe the similarity scores only
for established authors as newer authors will not
have enough previous work to produce a stable term
signature, and we vary the experience threshold by
year to account for the fact that there has been a large
increase in the absolute number of papers published
in recent years.
Depending on the presence of new authors and
the number of established authors present, each pa-
per can be placed into one of the three spectra: the
Apprenticeship-New Blood spectrum, the Synergy
spectrum and the Apprenticeship-Catalyst spectrum.
Apprenticeship and Low Synergy papers are those
with high MaxSim scores, while low scores indicate
New Blood, Catalyst or High Synergy papers.
5.2 Examples
The following are examples of high impact papers
as they were categorized by our system:
127
Figure 1: Decision diagram for determining the collaboration type of a paper. A minimum of 1 established author is
assumed.
5.2.1 Example: Apprenticeship Paper
Improvements in Phrase-Based Statistical Ma-
chine Translation (2004)
by Richard Zens and Hermann Ney
This paper had a high MaxSim score, indicating high
similarity to established author Hermann Ney. This
categorizes the paper as an Apprenticeship Paper.
5.2.2 Example: New Blood Paper
Thumbs up? Sentiment Classification using
Machine Learning Techniques (2002)
by Lillian Lee, Bo Pang and Shivakumar
Vaithyanathan
This paper had a low MaxSim score, indicating
low similarity to established author Lillian Lee.
This categorizes the paper as a New Blood Pa-
per, with new authors Bo Pang and Shivakumar
Vaithyanathan. It is important to note here that new
authors do not necessarily mean young authors or
grad students; in this case, the third author on the
paper was experienced, but in a field outside of
ACL.
5.2.3 Example: High Synergy Paper
Catching the Drift: Probabilistic Content
Models, with Applications to Generation and
Summarization (2003)
by Regina Barzilay and Lillian Lee
This paper had low similarity to both established
authors on it, making it a highly synergistic paper.
Synergy here indicates that the work done on this
paper was mostly unlike work previously done by
either of the authors.
5.2.4 Example: Catalyst Paper
Answer Extraction (2000)
by Steven Abney, Michael Collins, Amit Singhal
This paper had a very low MaxSim score, as well
as the presence of an unestablished author, making
it a Catalyst Paper. The established authors (from
an ACL perspective) were Abney and Collins, while
Singhal was from outside the area and did not have
many ACL publications. The work done in this pa-
per focused on information extraction, and was un-
like that previously done by either of the ACL estab-
lished authors. Thus, we say that in this case, Sing-
hal played the role of the catalyst, getting the other
two authors to work on an area that was outside of
their usual range.
5.3 Evaluation
5.3.1 Expert Annotation
To quantitatively evaluate the performance of
our system, we prepared a subset of 120 papers
from among the highest scoring collaborative papers
based on the PageRank metric (Radev et al, 2009).
Only those papers were selected which had at least a
128
single established author. One expert in the field was
asked to annotate each of these papers as being ei-
ther similar or dissimilar to the established authors?
prior work given the year of publication, the title of
the publication and its abstract.
We found that the MaxSim scores of papers la-
beled as being similar to the established authors
were, on average, higher than those labeled as dis-
similar. The average MaxSim score of papers anno-
tated as low MaxSim collaboration types (High Syn-
ergy, New Blood or Catalyst papers) was 0.15488,
while that of papers labeled as high MaxSim types
(Apprentice or Low Synergy papers) had a mean
MaxSim score of 0.21312. The MaxSim scores of
the different sets were compared using a t-test, and
the difference was found to be statistically signifi-
cant with a two-tailed p-value of 0.0041.
Framing the task as a binary classification prob-
lem, however, did not produce very strong results.
The breakdown of the papers and success rates (as
determined by a tuned threshold) can be seen in Ta-
ble 3. The system had a relatively low success rate of
62.5% in its binary categorization of collaborations.
5.3.2 First Author Prediction
Studies have suggested that authorship order,
when not alphabetical, can often be quantified and
predicted by those who do the work (Sekercioglu,
2008). Through a survey of all authors on a sam-
ple of papers, Slone (1996) found that in almost all
major papers, ?the first two authors are said to ac-
count for the preponderance of work?. We attempt
to evaluate our similarity scores by checking if they
are predictive of first author.
Though similarity to previous work is only a small
contributor to determining author order, we find that
using the metric of cosine similarity between author
signatures and papers performs significantly better
at determining the first author of a paper than ran-
dom chance. Of course, this feature alone isn?t ex-
tremely predictive, given that it?s guaranteed to give
an incorrect solution in cases where the first author
of a paper has never been seen before. To solve the
problem of first author prediction, we would have
to combine this with other features. We chose two
other features - an alphabetical predictor, and a pre-
dictor based on the frequency of an author appearing
as first author. Although we don?t show the regres-
Predictor Feature Accuracy
Random Chance 37.35%
Author Signature Similarity 45.23%
Frequency Estimator 56.09%
Alphabetical Ordering 43.64%
Table 2: Accuracy of individual features at predicting the
first author of 8843 papers
sion, we do explore these two other features and find
that they are also predictive of author order.
Table 2 shows the performance of our prediction
feature alongside the others. The fact that it beats
random chance shows us that there is some infor-
mation about authorial efforts in the scores we have
computed.
6 Applications
A number of questions about the nature of collabo-
rations may be answered using our system. We de-
scribe approaches to some of these in this section.
6.1 The Hedgehog-Fox Problem
From the days of the ancient Greek poet
Archilochus, the Hedgehog-Fox analogy has
been frequently used (Berlin, 1953) to describe two
different types of people. Archilochus stated that
?The fox knows many things; the hedgehog one big
thing.? A person is thus considered a ?hedgehog?
if he has expertise in one specific area and focuses
all his time and resources on it. On the other hand,
a ?fox? is a one who has knowledge of several
different fields, and dabbles in all of them instead of
focusing heavily on one.
We show how, using our computed similarity
scores, one can discover the hedgehogs and foxes
of Computational Linguistics. We look at the top
100 published authors in our corpus, and for each
author, we compute the average similarity score the
author?s signature has to each of his or her papers.
Note that we start taking similarity scores into ac-
count only after an author has published 5 papers,
thereby allowing the author to stablize a signature
in the corpus and preventing the signature from be-
ing boosted by early papers (where author similarity
would be artificially high, since the author was new).
We present the authors with the highest average
similarity scores in Table 4. These authors can be
129
Collaboration Type True Positives False Positives Accuracy
New Blood, Catalyst or High Synergy Papers 43 23 65.15%
Apprentice or Low Synergy Papers 32 22 59.25%
Overall 75 45 62.50%
Table 3: Evaluation based on annotation by one expert
considered the hedgehogs, as they have highly sta-
ble signatures that their new papers resemble. On
the other hand, Table 5 shows the list of foxes, who
have less stable signatures, presumably because they
move about in different areas.
Author Avg. Sim. Score
Koehn, Philipp 0.43456
Pedersen, Ted 0.41146
Och, Franz Josef 0.39671
Ney, Hermann 0.37304
Sumita, Eiichiro 0.36706
Table 4: Hedgehogs - authors with the highest average
similarity scores
Author Avg. Sim. Score
Marcus, Mitchell P. 0.09996
Pustejovsky, James D. 0.10473
Pereira, Fernando C. N. 0.14338
Allen, James F. 0.14461
Hahn, Udo 0.15009
Table 5: Foxes - authors with the lowest average similar-
ity scores
6.2 Similarity to previous work by sub-fields
Based on the different types of collaborations dis-
cussed in, a potential question one might ask is
which sub-fields are more likely to produce appren-
tice papers, and which will produce new blood pa-
pers. To answer this question, we first need to deter-
mine which papers correspond to which sub-fields.
Once again, we use topic models to solve this prob-
lem. We first filter out a subset of the 1,200 highest
page-rank collaborative papers from the years 1980
to 2007. We use a set of topics built by running a
standard LDA topic model over the ACL corpus, in
which each topic is hand labeled by experts based on
the top terms associated with it. Given these topic-
term distributions, we can once again use the cosine
similarity metric to discover the highly associated
Topic Score
Statistical Machine Translation 0.2695
Prosody 0.2631
Speech Recognition 0.2511
Non-Statistical Machine Translation 0.2471
Word Sense Disambiguation 0.2380
Table 6: Topics with highest MaxSim scores (papers are
more similar to the established authors? previous work)
Topic Score
Question Answering 0.1335
Sentiment Analysis 0.1399
Dialog Systems 0.1417
Spelling Correction 0.1462
Summarization 0.1511
Table 7: Topics with lowest MaxSim scores (papers are
less similar to the established authors? previous work)
topics for each given paper from our smaller sub-
set, by choosing topics with cosine similarity above
a certain threshold ? (in this case 0.1).
Once we have created a paper set for each topic,
we can measure the ?novelty? for each paper by look-
ing at their MaxSim score. We can now find the av-
erage MaxSim score for each topic. This average
similarity score gives us a notion of how similar to
the established author (or authors) a paper in the sub
field usually is. Low scores indicate that new blood
and synergy style papers are more common, while
higher scores imply more non-synergistic or appren-
ticeship style papers. This could indicate that topics
with lower scores are more open ended, while those
with higher scores require more formality or train-
ing. The top five topics in each category are shown
in Tables 6 and 7. The scores of the papers from
the two tables were compared using a t-test, and the
difference in the scores of the two tables was found
to be very statistically significant with a two-tailed p
value << 0.01.
130
7 Discussion and Future Work
Once we have a robust way to score different kinds
of collaborations in ACL, we can begin to use these
scores as a quantitative tool to study phonemena in
the computational linguistics community. With our
current technique, we discovered a number of nega-
tive results; however, given that our accuracy in bi-
nary classification of categories is relatively low, we
cannot state for sure whether these are true negative
results or a limitation of our model.
7.1 Tentative Negative Results
Among the questions we looked into, we found the
following results:
? There was no signal indicating that authors
who started out as new blood authors were any
more or less likely to survive than authors who
started out as apprentices. Survival was mea-
sured both by the number of papers eventually
published by the author as well as the year of
the author?s final publication; however, calcu-
lations by neither measure correlated with the
MaxSim scores of the authors? early papers.
? Each author in the corpus was labeled for gen-
der. Gender didn?t appear to differentiate how
people collaborated. In particular, there was no
difference between men and women based on
how they started their careers. Women and men
are equally likely to begin as new blood authors
as they are to begin as apprentices.
? On a similar note, established male authors are
equally likely to partake in new blood or ap-
prentice collaborations as their female counter-
parts.
? No noticeable difference existed between aver-
age page rank scores of a certain categorization
of collaborative papers (e.g. high synergy pa-
pers vs. low synergy papers).
It is difficult to conclusively demonstrate negative
results, particularly given that our MaxSim scores
are by themselves not particularly strong discrimi-
nators in the binary classification tasks. We consider
these findings to be tentative and an opportunity to
explore in the future.
8 Conclusion
Not everything we need to know about academic
collaborations can be found in the co-authorship
graph. Indeed, as we have argued, not all types
of collaborations are equal, as embodied by differ-
ing levels of seniority and contribution from each
co-author. In this work, we have taken a first step
toward computationally modeling these differences
using a latent mixture of authors model and ap-
plied it to our own field, Computational Linguistics.
We used the model to examine how collaborative
works differ by authors and subfields in the ACL an-
thology. Our model quantifies the extent to which
some authors are more prone to being ?hedgehogs,?
whereby they heavily focus on certain specific ar-
eas, whilst others are more diverse with their fields
of study and may be analogized with ?foxes.?
We also saw that established authors in certain
subfields have more deviation from their previous
work than established authors in different subfields.
This could imply that the former fields, such as
?Sentiment Analysis? or ?Summarization,? are more
open to new blood and synergistic ideas, while other
latter fields, like ?Statistical Machine Translation?
or ?Speech Recognition? are more formal or re-
quire more training. Alternatively, ?Summarization?
or ?Sentiment Analysis? could just still be younger
fields whose language is still evolving and being in-
fluenced by other subareas.
This work takes a first step toward a new way of
thinking about the contributions of individual au-
thors based on their network of areas. There are
many design parameters that still exist in this space,
including alternative text models that take into ac-
count richer structure and, hopefully, perform bet-
ter at discriminating between the types of collabo-
rations we identified. We intend to use the ACL an-
thology as our test bed for continuing to work on tex-
tual models of collaboration types. Ultimately, we
hope to apply the lessons we learn on modeling this
familiar corpus to the challenge of answering large-
scale questions about the nature of collaboration as
embodied by large scale publication databases such
as ISI and Pubmed.
131
Acknowledgments
This research was supported by NSF grant NSF-
0835614 CDI-Type II: What drives the dynamic cre-
ation of science? We thank our anonymous review-
ers for their valuable feedback and the members of
the Stanford Mimir Project team for their insights
and engagement.
References
Isaiah Berlin. 1953. The hedgehog and the fox: An essay
on Tolstoy?s view of history. Simon & Schuster.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd international
conference on Machine learning, ICML ?06, pages
113?120, New York, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Sean M. Gerrish and David M. Blei. 2010. A language-
based approach to measuring scholarly impact. In Pro-
ceedings of the 26th International Conference on Ma-
chine Learning.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 363?371, Stroudsburg, PA, USA.
Association for Computational Linguistics.
B. F. Jones, S. Wuchty, and B. Uzzi. 2008. Multi-
university research teams: Shifting impact, geography,
and stratification in science. Science, 322:1259?1262,
November.
Xiaoming Liu, Johan Bollen, Michael L. Nelson, and
Herbert Van de Sompel. 2005. Co-authorship net-
works in the digital library research community. In-
formation Processing & Management, 41(6):1462 ?
1480. Special Issue on Infometrics.
Mario A. Nascimento, Jo?rg Sander, and Jeffrey Pound.
2003. Analysis of sigmod?s co-authorship graph. SIG-
MOD Rec., 32:8?10, September.
M. E. J. Newman. 2001. From the cover: The struc-
ture of scientific collaboration networks. Proceedings
of the National Academy of Science, 98:404?409, Jan-
uary.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network cor-
pus. In Proceedings of the 2009 Workshop on Text
and Citation Analysis for Scholarly Digital Libraries,
NLPIR4DL ?09, pages 54?61, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 248?256.
Craig M. Rawlings and Daniel A. McFarland. 2011. In-
fluence flows in the academy: Using affiliation net-
works to assess peer effects among researchers. Social
Science Research, 40(3):1001 ? 1017.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence, UAI
?04, pages 487?494.
Cagan H. Sekercioglu. 2008. Quantifying coauthor con-
tributions. Science, 322(5900):371.
RM Slone. 1996. Coauthors? contributions to major
papers published in the ajr: frequency of undeserved
coauthorship. Am. J. Roentgenol., 167(3):571?579.
Howard D. White and Katherine W. Mccain. 1998. Visu-
alizing a discipline: An author co-citation analysis of
information science. Journal of the American Society
for Information Science, 49:1972?1995.
132
