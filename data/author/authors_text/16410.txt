Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 787?797,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
An Unsupervised Dynamic Bayesian Network Approach to Measuring
Speech Style Accommodation
Mahaveer Jain1, John McDonough1, Gahgene Gweon2, Bhiksha Raj1, Carolyn Penstein Rose?1,2
1. Language Technologies Institute; 2. Human Computer Interaction Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{mmahavee,johnmcd,ggweon,bhiksha,cprose}@cs.cmu.edu
Abstract
Speech style accommodation refers to
shifts in style that are used to achieve strate-
gic goals within interactions. Models of
stylistic shift that focus on specific fea-
tures are limited in terms of the contexts
to which they can be applied if the goal of
the analysis is to model socially motivated
speech style accommodation. In this pa-
per, we present an unsupervised Dynamic
Bayesian Model that allows us to model
stylistic style accommodation in a way that
is agnostic to which specific speech style
features will shift in a way that resem-
bles socially motivated stylistic variation.
This greatly expands the applicability of the
model across contexts. Our hypothesis is
that stylistic shifts that occur as a result of
social processes are likely to display some
consistency over time, and if we leverage
this insight in our model,we will achieve
a model that better captures inherent struc-
ture within speech.
1 Introduction
Sociolinguistic research on speech style and its
resulting social interpretation has frequently fo-
cused on the ways in which shifts in style are
used to achieve strategic goals within interac-
tions, for example the ways in which speakers
may adapt their speaking style to suppress differ-
ences and accentuate similarities between them-
selves and their interlocutors in order to build
solidarity (Coupland, 2007; Eckert & Rickford,
2001; Sanders, 1987). We refer to this stylis-
tic convergence as speech style accommodation.
In the language technologies community, one tar-
geted practical benefit of such modeling has been
the achievement of more natural interactions with
speech dialogue systems (Levitan et al 2011).
Monitoring social processes from speech or
language data has other practical benefits as well,
such as enabling monitoring how beneficial an in-
teraction is for group learning (Ward & Litman,
2007; Gweon, 2011), how equal participation is
within a group (DiMicco et al 2004), or how
conducive an environment is for fostering a sense
of belonging and identification with a community
(Wang et al 2011).
Typical work on computational models of
speech style accommodation have focused on spe-
cific aspects of style that may be accommodated,
such as the frequency or timing of pauses or
backchannels (i.e., words that show attention like
?Un huh? or ?ok?), pitch, or speaking rate (Ed-
lund et al 2009; Levitan & Hirschberg, 2011). In
this paper, we present an unsupervised Dynamic
Bayesian Model that allows us to model speech
style accommodation in a way that does not re-
quire us to specify which linguistic features we
are targeting. We explore a space of models de-
fined by two independent factors, namely the di-
rect influence of one speaker?s style on another
speaker?s style and the influence of the relational
gestalt between the two speakers that motivates
the stylistic accommodation, and thus may keep
the accommodation moving consistently, with the
same momentum. Prior work has explored the in-
fluence of the first factor. However, because ac-
commodation reflects social processes that extend
over time within an interaction, one may expect a
certain consistency of motion within the stylistic
shift. Furthermore, we can leverage this consis-
tency of style shift to identify socially meaningful
variation without specifying ahead of time which
787
particular stylistic elements we are focusing on.
Our evaluation provides support for this hypothe-
sis.
When stylistic shifts are focused on specific
linguistic features, then measuring the extent of
the stylistic accommodation is simple since a
speaker?s style may be represented on a one or two
dimensional space, and movement can then be
measured precisely within this space using sim-
ple linear functions. However, the rich sociolin-
guistic literature on speech style accommodation
highlights a much greater variety of speech style
characteristics that may be associated with social
status within an interaction and may thus be bene-
ficial to monitor for stylistic shifts. Unfortunately,
within any given context, the linguistic features
that have these status associations, which we re-
fer to as indexicality, are only a small subset of
the linguistic features that are being used in some
way. Furthermore, which features carry this in-
dexicality are specific to a context. Thus, separat-
ing the socially meaningful variation from varia-
tion in linguistic features occurring for other rea-
sons is akin to searching for the proverbial needle
in a haystack. It is this technical challenge that we
address in this paper.
In the remainder of the paper we review the lit-
erature on speech style accommodation both from
a sociolinguistic perspective and from a techno-
logical perspective in order to motivate our hy-
pothesis and proposed model. We then describe
the technical details of our model. Next, we
present an experiment in which we test our hy-
pothesis about the nature of speech style accom-
modation and find statistically significant con-
firming evidence. We conclude with a discussion
of the limitations of our model and directions for
ongoing research.
2 Theoretical Framework
Our research goal is to model the structure of
speech in a way that allows us to monitor so-
cial processes through speech. One common goal
of prior work on modeling speech dynamics has
been for the purpose of informing the design of
more natural spoken dialogue systems (Levitan et
al., 2011). The practical goal of our work is to
measure the social processes themselves, for ex-
ample in order to estimate the extent to which
group discussions show signs of productive con-
sensus building processes (Gweon, 2011). Much
prior work on modeling emotional speech has
sought to identify features that themselves have
a social interpretation, such as features that pre-
dict emotional states like uncertainty (Liscombe
et al 2005), or surprise (Ang et al 2002), or
social strategies like flirting (Ranganath et al
2009). However, our goal is to monitor social pro-
cesses that evolve over time and are reflected in
the change in speech dynamics. Examples include
fostering trust, forming attachments, or building
solidarity.
2.1 Defining Speech Style Accommmodation
The concept of what we refer to as Speech
Style Accommodation has its roots in the field
of the Social Psychology of Language, where
the many ways in which social processes are re-
flected through language, and conversely, how
language influences social processes, are the ob-
jects of investigation (Giles & Coupland, 1991).
As a first step towards leveraging this broad range
of language processes, we refer to one very spe-
cific topic, which has been referred to as entrain-
ment, priming, accommodation, or adaptation in
other computational work (Levitan & Hirschberg,
2011). Specifically we refer to the finding that
conversational partners may shift their speaking
style within the interaction, either becoming more
similar or less similar to one another.
Our usage of the term accommodation specifi-
cally refers to the process of speech style conver-
gence within an interaction. Stylistic shifts may
occur at a variety of levels of speech or language
representation. For example, much of the early
work on speech style accommodation focused on
regional dialect variation, and specifically on as-
pects of pronunciation, such as the occurrence of
post-vocalic ?r? in New York City, that reflected
differences in age, regional identification, and so-
cioeconomic status (Labov, 2010a,b). Distribu-
tion of backchannels and pauses have also been
the target of prior work on accommodation (Lev-
itan & Hirschberg, 2011). These effects may be
moderated by other social factors. For example,
Bilous & Krauss (1988) found that females ac-
commodated to their male partners in conversa-
tion in terms of average number of words uttered
per turn. For example, Hecht et al(1989) re-
ported that extroverts are more listener adaptive
than introverts and hence extroverts converged
more in their data.
788
Accommodation could be measured either
from textual or speech content of a conversation.
The former relates to ?what? people say whereas
the latter to ?how? they say it. We are only inter-
ested in measuring accommodation from speech
in this work. There has been work on convergence
in text such as syntactic adaptation (Reitter et al
2006) and language similarity in online commu-
nities (Huffaker et al 2006).
2.2 Social Interpretation of Speech Style
Accommodation
It has long been established that while some
speech style shifts are subconscious, speakers
may also choose to adapt their way of speaking
in order to achieve social effects within an in-
teraction (Sanders, 1987). One of the main mo-
tives for accommodation is to decrease social dis-
tance. On a variety of levels, speech style accom-
modation has been found to affect the impression
that speakers give within an interaction. For ex-
ample, Welkowitz & Feldstein (1970) found that
when speakers become more similar to their part-
ners, they are liked more by partners. Another
study by Putman & Street Jr (1984) demonstrated
that interviewees who converge to the speaking
rate and response latency of their interviewers are
rated more favorably by the interviewers. Giles et
al. (1987) found that more accommodating speak-
ers were rated as more intelligent and supportive
by their partners. Conversely, social factors in
an interaction affect the extent to which speak-
ers engage in, and some times chose not to en-
gage in, accommodation. For example, Purcell
(1984) found that Hawaiian children exhibit more
convergence in interactions with peer groups that
they like more. Bourhis & Giles (1977) found that
Welsh speakers while answering to an English
surveyor broadened their Welsh accent when their
ethnic identity was challenged. Scotton (1985)
found that few people hesitated to repeat lexi-
cal patterns of their partners to maintain integrity.
Nenkova et al(2008) found that accommodation
on high frequency words correlates with natural-
ness, task success, and coordinated turn-taking
behavior.
2.3 Computational models of speech style
accommodation
Prior research has attempted to quantify accom-
modation computationally by measuring similar-
ity of speech and lexical features either over full
conversations or by comparing the similarity in
the first half and the second half of the conver-
sation. For example, Edlund et al(2009) mea-
sure accommodation in pause and gap length us-
ing measures such as synchrony and convergence.
Levitan & Hirschberg (2011) found that accom-
modation is also found in special social behaviors
within conversation such as backchannels. They
show that speakers in conversation tend to use
similar kinds of speech cues such as high pitch at
the end of utterance to invite a backchannel from
their partner. In order to measure accommodation
on these cues, they compute the correlation be-
tween the numerical values of these cues used by
partners.
In our work we measure accommodation using
Dynamic Bayesian Networks (DBNs). Our mod-
els are learnt in an unsupervised fashion. What
we are specifically interested in is the manner in
which the influence of one partner on the other is
modeled. What is novel in our approach is the
introduction of the concept of an accommodation
state, or relational gestalt variable, which essen-
tially models the momentum of the influence that
one partner is having on the other partner?s speak-
ing style. It allows us to represent structurally the
insight that accommodation occurs over time as a
reflection of a social process, and thus has some
consistency in the nature of the accommodation
within some span of time. The prior work de-
scribed in this section can be thought of as tak-
ing the influence of the partner?s style directly on
the speaker?s style within an instant as the floor
shifts from one speaker to the next. Thus, no con-
sistency in the manner in which the accommoda-
tion is occurring is explicitly encouraged by the
model. The major advantage of consistency of
motion within the style shift over time is that it
provides a sign post for identifying which style
variation within the speech is salient with respect
to social interpretation within a specific interac-
tion so that the model may remain agnostic and
may thus be applied to a variety of interactions
that differ with respect to which stylistic features
are salient in this respect.
3 A Dynamic Bayesian Network Model
for Conversation
Speech stylistic information is reflected in
prosodic features such as pitch, energy, speak-
789
ing rate etc. In this work, we leverage on sev-
eral of these speech features to quantify accom-
modation. We propose a series of models that
can be trained unsupervised from speech features
and can be used for predicting accommodation.
The models attempt to capture the dependence of
speech features on speaking style, as well as the
effect of persistence and accommodation on style.
We use a dynamic Bayesian network (DBN) for-
malism to capture these relationships. Below we
briefly review DBNs, and subsequently describe
the speech features used, and the proposed mod-
els.
3.1 Dynamic Bayesian Networks
The theory of Bayesian networks is well doc-
umented and understood (Jensen, 1996; Pearl,
1988). A Bayesian network is a probabilistic
model that represents statistical relationships be-
tween random variables via a directed acyclic
graph (DAG). Formally, it is a directed acyclic
graph whose nodes represent random variables
(which may be observable quantities, latent unob-
servable variables, or hypotheses to be estimated).
Edges represent conditional dependencies; nodes
which are connected by an edge represent ran-
dom variables that have a direct influence on one
another. The entire network represents the joint
probability of all the variables represented by the
nodes, with appropriate factoring of the condi-
tional dependencies between variables.
Consider, for instance, a joint distribution
over a set of random variables x1, x2, ? ? ? , xn,
modeled by a Bayesian network. Let V =
v1, v2, ? ? ? , vn represent the set of n nodes in
the network, representing the random variables
x1, x2, ? ? ? , xn respectively. Let ?(vi) represent
the set of parent nodes of vi, i.e. nodes in V
that have a directed edge into a node vi. Then,
by the dependencies specified by the network,
P (xi|x1, x2, ? ? ? , xn) = P (xi|xj : vj ? ?(vi)).
In other words, any variable xi is directly depen-
dent only on its parent variables, i.e. the random
variables represented by the nodes in ?(vi), and
is independent of all other variables given these
variables. The joint probability of x1, x2, ? ? ? , xn
is hence given by
p(x1, x2, ..., xn) =
?
i
p(xi|xpii) (1)
Where xpii represents {xj : vj ? ?(vi), i.e. the
Figure 1: An example Dynamic Bayesian Network
(DBN) showing the temporal relationship between
three random variables (A,B and C). A is observered
and dependent on two hidden variables B and C. Di-
rected edges across time (t? 1 ? t) indicate temporal
relationships between variables. In this example, the
variables At and Bt are both dependent on Bt?1 with
the relationship defined through conditional distribu-
tions P (At|Bt?1) and P (Bt|Bt?1).
parents of xi in the network. We note that not
all of these variables need to be observable; of-
ten in such models several of the variables are
unobservable, i.e. they are latent. In order
to obtain the joint distribution of the observable
variables the latent variables must be marginal-
ized out. I.e. if x1, ? ? ? , xm are observable
and xm+1, ? ? ? , xn are latent, P (x1, ? ? ? , xm) =
?
xm+1,??? ,xn P (x1, x2, ? ? ? , xn).
Dynamic Bayesian networks (DBNs) further
represent time-series data through a recurrent for-
mulation of a basic Bayesian network that repre-
sents the relationship between variables. Within
a DBN a set of random variables at each time in-
stance t is represented as a static Bayesian Net-
work with temporal dependencies to variables at
other instants. Namely, the distribution of a vari-
able xi,t at time t is dependent on other variables
at times t ? ? , xj,t?? through conditional prob-
abilities of the form Pr(xi,t|xj,t?? ). An exam-
ple DBN, consisting of three variables (A,B and
C), two of which have temporal dependencies is
shown in Figure 1.
One benefit of the DBN formalism is that in
addition to providing a compact graphical way
of representing statistical relationships between
variables in a process, the constrained, directed
network structure also allows for simplified in-
ference. Moreover, the conditional distributions
associated with the network are often assumed
not to vary over time, i.e. Pr(xi,t|xj,t?? ) =
Pr(xi,t? |xj,t??? ). This allows for a very com-
pact representation of DBNs and allows for ef-
ficient Expectation-Maximization (EM) learning
algorithms to be applied.
790
In the discussion that follows we do not explic-
itly specify the random variables and the form of
the associated probability distributions, but only
present them graphically. The joint distribution of
the variables should nevertheless be obvious from
the figures. We employ EM to learn the param-
eters of the models from training data, and the
junction tree algorithm (Lauritzen & Spiegelhal-
ter, 1988) to perform inference.
3.2 Speech Features
We characterize conversations as a series of spo-
ken turns by the partners. We characterize the
speech in each turn through a vector that cap-
tures several aspects of the signal that are salient
to style. We used the OPENSmile toolkit (opens-
mile, 2011) to compute the features. Specifi-
cally, within each turn the speech was segmented
into analysis windows of 50ms, where adjacent
windows overlapped by 40ms. From each anal-
ysis window a total of 7 features were com-
puted: voice probability, harmonic to noise ratio,
voice quality , three measures of pitch (F0, F raw0 ,
F env0 ), and loudness. A 10-bin histogram of fea-
ture values was computed for each of these fea-
tures, which was then normalized to sum to 1.0.
The normalized histogram effectively represents
both the values and the fluctuation in the features.
For instance, a histogram of loudness values cap-
tures the variation in the loudness of the speaker
within a turn. The logarithms of the normalized
10-bin histograms for the 7 features were concate-
nated to result in a single 70-dimensional obser-
vation vector for the turn. These 70 dimensional
observation vectors for each turn of any speaker
are represented in our model as oit where t is turn
index and i is speaker index.
3.3 Elements of the Models
In this section we formally describe the elements
of our model.
Speaking Style State: These states represent the
speaking styles of the partners in a conversation.
We represent these states as sit, where t represent
turn index and i represents speaker index. These
states are assumed to belong to a finite, discrete
set S = {s1, s2, ? ? ? , sk}, i.e. sit ? S ?(i, t).
Accommodation State: An accommodation state
represents the indirect influence of partners on
each other in a conversation. In our present de-
sign, it can take a value of either 1 or 0. These
Yt-1 Yt+1
O1t-1 O1t+1O1tS1t-1 S1t S1t+1
Figure 2: The basic generative model.
Yt-1 Yt+1
O1t-1 O1t+1O1tS1t-1 S1t S1t+1
S2t-1O2t-1
S2t
O2t O2t+1
S2t+1
Figure 3: ISM: The dynamics of each speaker are in-
dependent of the other speaker.
states are represented as At, where t is turn index.
Observation Vector: The observation vectors are
the feature vectors oit computed for each turn.
3.4 Models for Accommodation
Our models embody two premises. First, a per-
son?s speech in any turn is a function of his/her
speaking style in that turn. Second, a person?s
speaking style at any turn depends not only by
their own personal biases, but also by their ac-
commodation to their partner. We represent these
dependencies as a DBN.
Our basic model to represent the generation of
speech (i.e. speech features) by a speaker in the
absence of other influences is shown in Figure 2.
The speech features oit in any turn depend only on
the speaking style sit in that turn. The style sit in
any turn depends on the style sit?1 in the previ-
ous turn, to capture the speaker-specific patterns
of variation in speaking style. We note that this
is a rather simple model and patterns of variation
in style are captured only through the statistical
dependence between styles in consequent turns.
We now build our models for accommodation
on this basic model.
3.4.1 Style-based models
Our two first models assume that accommo-
dation is demonstrated as a direct dependence
of a person?s speaking sytle on their partner?s
style. Therefore the models only consider speak-
ing styles.
The Independent Speaker Model
Our simplest model for a conversation assumes
791
Yt-1 Yt+1
O1t-1 O1t+1O1tS1t-1 S1t S1t+1
S2t-1O2t-1
S2t
O2t O2t+1
S2t+1
Figure 4: CSDM: A speaker?s style depends on their
partner?s style at the previous turn.
Yt-1 Yt+1
O1t-1
Y Yt-1At At+1
O1t+1O1tS1t-1 S1t S1t+1
S2t-1O2t-1
S2t
O2t O2t+1
S2t+1
Figure 5: SASM: Both partners? styles depend on mu-
tual accommodation to one another.
that each person?s speaking style evolves indepen-
dently, uninfluenced by their partner. The DBN
for this is shown in Figure 3. We refer to this
model as the Independent Speaker Model (ISM).
Note that the set of values that the style states can
take is common for both speakers. The speaking
styles for the two speakers may be said to be con-
fluent in any turn if both of them are in the same
style state at that turn.
The Cross-speaker Dependence Model
Intuitively, in a conversation speakers are influ-
enced by their partners? speaking style in previ-
ous turns. The Cross-Speaker Dependence Model
(CSDM) represents this dependence as shown in
the DBN in Figure 4. In this model a person?s
speaking style depends on both their own and
their partner?s speaking styles in the previous turn.
3.4.2 Accommodation state models
Accommodation state models assume that con-
versations actually have an underlying state of ac-
commodation, and that speakers in fact vary their
speaking styles in response to it. We models this
through a binary-valued accommodation state that
is embedded into the DBN. We posit two types of
accommodation state models.
The Symmetric Accommodation State Model
In the symmetric accommodation state model
Yt-1 Yt+1
O1t
Y Yt-1A2t A1t
O1t+1S1t S1t+1
S2t
O2t O2t+1
S2t+1Yt-1A2t+1
Figure 6: AASM: Accommodation state associated
with every speaker turn
(SASM) we assume that accommodation is a
jointly experienced characteristic of the conversa-
tion at any time, which enjoys some persistence,
but is also affected by the speaking styles exhib-
ited by the speakers at each turn. The accom-
modation at any time in turn affects the speaking
styles of both speakers in the next turn. The DBN
for this model is shown in Figure 5.
The Asymmetric Accommodation State Model
The asymmetric accommodation state model
(AASM) represents accommodation as a speaker-
turn-specific characteristic. In any turn, the ac-
commodation for a speaker depends chiefly on
their partner?s most recent speaking style. The ac-
commodation state can change after each speaker
turn. Figure 6 shows the DBN for this model.
Note that this model captures the asymmetric na-
ture of accommodation, e.g. it may be the case
that only one of the speakers is accommodating.
For instance, if if a1t = 0 and a2t = 1, only
speaker2 is accommodating but not speaker1.
3.4.3 Accommodated style dependence
models
While accommodation state models explicitly
models accommodation, they do not explicitly
represent how it is expressed. In reality, accom-
modation is a process of convergence ? an ac-
commodating speaker?s speaking style may be ex-
pected to converge toward that of their partner. In
other words, the person?s speaking style depends
not only on whether they are accommodating or
not, but also on their partner?s style at the previ-
ous turn. Accommodated style dependence mod-
els explicitly represent this dependence.
The Symmetric Accommodated Style Depen-
dence Model
The Symmetric Accommodated Style Depen-
dence Model (SASDM) extends the SASM, to in-
792
Yt-1 Yt+1
O1t-1
Y Yt-1At At+1
O1t+1O1tS1t-1 S1t S1t+1
S2t-1O2t-1
S2t
O2t O2t+1
S2t+1
Figure 7: SASDM: A speaker?s style depends both on
mutual accommodation and the partner?s style in the
previous turn.
Yt-1 Yt+1
O1t
Y Yt-1A2t A1t
O1t+1S1t S1t+1
S2t
O2t O2t+1
S2t+1Yt-1A2t+1
Figure 8: AASDM: The accommodation state associ-
ated with every speaker and a speaker?s style depends
on the partner?s style.
dicate that a speaker?s style in any turn depends
both on accommodation and on their partner?s
style in the previous turn. Figure 7 shows the
DBN for this model.
Asymmetric Accommodated Style Dependence
Model
The Asymmetric Accommodated Style Depen-
dence Model (AASDM) extends the AASM by
adding a direct dependence between a speaker?s
style and their partner?s style in their most recent
turn. The DBN for this is shown in Figure 8.
3.5 Interpreting the states
We note that we have referred to the states in the
models above as ?style? states. In reality, in all
cases, we learn the parameters of the model in
an unsupervised manner, since the data we use to
train it do not have either speaking style or ac-
commodation indicated (although, if they were la-
beled, the labels could be employed within our
models). Consequently, we have no assurance
that the states learned will actually correspond to
speaking styles. They can only be considered a
proxy for speaking style. Nevertheless, if both
speakers are in the same state, they can both be
expected to be producing similar prosodic fea-
tures, as represented in the observation vectors.
It is hence reasonable to assume that they are both
speaking in similar style. Similarly, the accom-
modation state cannot be expected to actually de-
pict accommodation; nevertheless, it can capture
the dependencies that govern when the two speak-
ers are likely to be in the same state.
4 Evaluation
The model we have just described allows us to in-
vestigate two separate aspects of our concept of
speech style accommodation. The first aspect is
that style accommodation occurs as a local influ-
ence of one speaker?s style on the other speaker?s
style, as depicted by direct links between style
states. The second aspect is that although this is a
local phenomenon, because it is a reflection of a
social process that extends over a period of time,
there will be some persistence of accommodation
over longer periods of time, as characterized by
the accommodation state. We presented two dif-
ferent operationalizations of the accommodation
state above, namely Asymmetric and Symmetric.
Accommodation is a phenomenon that occurs
within interactions between speakers; we can ex-
pect not to observe accommodation occurring be-
tween individuals that have never met and are not
interacting. On average, then, we expect to see
more evidence of speech style accommodation in
pairs of individuals who are interacting (i.e., Real
Pairs) than in pairs of individuals who are not in-
teracting and have never met (i.e., Constructed
Pairs). Thus, we may evaluate the extent to which
our model is sensitive to social dynamics within
pairs by the extent to which it is able to distinguish
between true conversation between Real Pairs of
speaker and synthetic conversation between Con-
structed Pairs. A similar experimental paradigm
has been adopted in prior work on speech style
accommodation (Levitan et al 2011).
Hypothesis: Our hypothesis is that models that
explicitly represent the notion that accommoda-
tion occurs over a span of time with consistency
of momentum will achieve better success at dis-
tinguishing between Real Pairs and Constructed
Pairs than models that do not.
Experimental Manipulation: Thus, using the
model we have just described, we are able to
test our hypothesis using a 2 ? 3 factorial design
in which one factor is the inclusion of direct
links from the style of one speaker to the style
793
of the other speaker, which we refer to as the
DirectInfluence (DI) factor, with values True
(T) and False (F), and the second factor is the
inclusion of links from style states to and from
Accommodation states, which we refer to as the
IndirectInfluence (II) factor, with values False
(F), Asymmetric (A), and Symmetric (S). The
result of this 2 ? 3 factorial design are the 6
different models described in Section 3, namely
ISM (DI=False, II=False), CSDM (DI=True,
II=False), SASM (DI=False, II=Symmetric),
AASM (DI=False, II=Asymmetric), SASDM
(DI=True, II=Symmetric), and AASDM
(DI=True, II= Asymmetric).
Corpus: The success criterion in our experiment
is the extent to which models of speech style
accommodation are able to distinguish between
Real Pairs and Constructed pairs. In order to set
up this comparison, we began with a corpus of de-
bates between students about the reasons for the
fall of the Ottoman Empire. We obtained this cor-
pus from researchers who originally collected it
to investigate issues related to learning from con-
versational interactions (Nokes et al 2010). The
full corpus contains interactions between 76 pairs
of students who interacted for 8 minutes. Within
each pair, one student was assigned the role of ar-
guing that the fall of the Ottoman empire was due
to internal causes, whereas the other student was
assigned the role of arguing that the fall of the Ot-
toman empire was due to external causes. Each
student was given a 4 page packet of supporting
information for their side of the debate to draw
from in the interaction.
The speech from each participant was recorded
on a separate channel. As a first step, we aligned
the speech recordings automatically to their tran-
scriptions at the word and turn level. After align-
ing the corpus at the word level, we identify the
turn interval of each partner in the conversation.
We use 66 of the debates out of the complete set
of 76 for the experiments discussed in this paper.
We had to eliminate 10 dialogues where the seg-
mentation and alignment failed. For each of our
models, we used the same 3 fold cross-validation.
Participants: Participants were all male under-
graduate students between the ages of 18 and 25.
In prior studies, it has been shown that accommo-
dation varies based on gender, age and familiar-
ity between partners. This corpus is particularly
appropriate because it controls for most of these
factors. Furthermore, because the participants did
not know each other before the debate, we can
assume that if accommodation happened, it was
only during the conversation.
Real versus Constructed Pairs: In our analy-
sis below, we compare measured accommodation
between pairs of humans who had a real conver-
sation and a constructed pair in which one per-
son from that conversation is paired with a con-
structed partner, where the partner?s side of the
conversation was constructed from turns that oc-
curred in other conversations. We set up this com-
parison in order to isolate speech style conver-
gence from lexical convergence when we evalu-
ate the performance of our model. The difference
between the measured accommodation between
real and constructed pairs is treated as a weak op-
erationalization of model accuracy at measuring
speech style accommodation.
For each of the 20 Real pairs in the test corpus
we composed one Constructed Pair. Each Con-
structed Pair comprised one student from the cor-
responding Real Pair (i.e., the Real Student) and a
Constructed Partner that resembled the real part-
ner in content but not necessarily style. We did
this by iterating through the real partner?s turns,
replacing each with a turn that matched as well as
possible in terms of lexical content but came from
a different conversation. Lexical content match
was measured in terms of cosine similarity. Turns
were selected from the other Real pairs. Thus, the
Constructed Partner had similar content to the cor-
responding real partner on a turn by turn basis, but
the style of expression could not be influenced by
the Real Student. Thus, ideally we should not see
evidence of speech style accommodation within
the Constructed Pairs.
Experimental Procedure: For each of the four
models we computed an Accommodation Score
for each of the Real Pairs and Constructed Pairs.
In order to obtain a measure that can be used to
compute accommodation for all the models con-
sidered, we compute the accommodation value as
the fraction of turns in a session where partners
exhibited the same speaking style.
Results: In order to test our hypothesis we con-
structed an ANOVA model with Accommodation
Score as the dependent variable and DirectInflu-
ence, IndirectInfluence, RealVsConstructed as in-
dependent variables. Additionally we included
the interaction terms between all pairs of inde-
794
DI II Real Constructed
?(?) ?(?)
SASDM T S .54 (.23) .44 (.29)
SASM F S .54 (.23) .44 (.29)
CSDM T F .6 (.26) .52 (.3)
ISM F F .56 (.25) .51 (.32)
AASM F A .6 (.24) .51 (.3)
AASDM T A .61 (.24) .48 (.3)
Table 1: Accommodation measured using different
models. Legend: ?=mean, ? = standard deviation, DI
= ?Direct Influence?, II = ?Indirect Influence?.
pendent variables. Using this ANOVA model, we
find a highly significant main effect of the Re-
alVsConstructed factor that demonstrates the gen-
eral ability of the models to achieve separation be-
tween Real Pairs and Constructed Pairs; on aver-
age F(1,780) = 18.22, p < .0001.
However, when we look more closely, we find
that although the trend is consistently to find more
evidence of speech style accommodation in Real
Pairs than in Constructed Pairs, we see differen-
tiation among the models in terms of their abil-
ity to achieve this separation. When we exam-
ine the two way interactions between DirectIn-
fluence and RealVsConstructed as well as be-
tween IndirectInfluence and RealVsConstructed,
although we do not find significant interactions,
we do find some suggestive patterns when we
do the student T posthoc analysis. In particular,
when we explore just the interaction between In-
directInfluence links, we find a significant separa-
tion between Real vs Constructed pairs for models
with Accommodation states, but not for the cases
where no Accommodation states are included.
However, when we do the same for the interaction
between DirectInfluence links and RealVsCon-
structed, we find significant separation with or
without those links. This suggests that IndirectIn-
fluence links are more important than DirectInflu-
ence links. At a finer-grained level, when we ex-
amine the models individually, we only find a sig-
nificant separation between Real and Constructed
pairs with the model that includes both Direct-
Influence and Symmetric IndirectInfluence links.
These results suggest that Symmetric IndirectIn-
fluence links may be slightly better than Asym-
metric ones, and that combining DirectInfluence
links and Symmetric IndirectInfluence links may
be the best combination.
Based on this analysis, we find support for our
hypothesis. We find that the model that includes
Symmetric IndirectInfluence links and DirectIn-
fluence links is the best balance between represen-
tational power and simplicity. The support for the
inclusion of DirectInfluence links in the model is
weaker than that of IndirectInfluence links, how-
ever. On a larger dataset, we may have observed
stronger effects of both factors. Even on this small
dataset, we find evidence that adding that struc-
ture improves the performance of the model with-
out leading to overfitting.
5 Conclusions and Current Directions
In this paper we presented an unsupervised dy-
namic Bayesian modeling approach to modeling
speech style accommodation in face-to-face inter-
actions. Our model was motivated by the idea that
because accommodation reflects social processes
that extend over time within an interaction, one
may expect a certain consistency of motion within
the stylistic shift. Our evaluation demonstrated a
statistically significant advantage for the models
that embodied this idea.
An important motivation for our modeling ap-
proach was that it allows us to avoid targeting
specific linguistic style features in our measure
of accommodation. However, in our evaluation,
we only tested our approach on conversations be-
tween male undergraduate students discussing the
fall of the Ottoman Empire. Thus, while our eval-
uation provides evidence that we have taken a first
important step towards our ultimate goal, we can-
not yet claim that we have a model that performs
equally effectively across contexts. In our future
work, we plan to formally test the extent to which
this allows us to accurately measure accommoda-
tion within contexts in which very different stylis-
tic elements carry strategic social value.
Another important direction of our current re-
search is to explore how measures of speech style
accommodation may predict other important mea-
sures such as how positively partners view one an-
other, how successful partners perform tasks to-
gether, or how well students learn together.
6 Acknowledgments
We gratefully acknowledge John Levine and Tim-
othy Nokes for sharing their data with us. This
work was funded by NSF SBE 0836012.
795
References
Ang, J., Dhillon, R., Krupski, A., Shriberg, E., & Stol-
cke, A. (2002). Prosody-based automatic detection
of annoyance and frustration in human-computer di-
alog. In Proc. ICSLP, volume 3, pages 2037?2040.
Citeseer.
Bilous, F. & Krauss, R. (1988). Dominance and
accommodation in the conversational behaviours
of same-and mixed-gender dyads. Language and
Communication, 8(3), 4.
Bourhis, R. & Giles, H. (1977). The language of in-
tergroup distinctiveness. Language, ethnicity and
intergroup relations, 13, 119.
Coupland, N. (2007). Style: Language variation and
identity. Cambridge Univ Pr.
DiMicco, J., Pandolfo, A., & Bender, W. (2004). Influ-
encing group participation with a shared display. In
Proceedings of the 2004 ACM conference on Com-
puter supported cooperative work, pages 614?623.
ACM.
Eckert, P. & Rickford, J. (2001). Style and sociolin-
guistic variation. Cambridge Univ Pr.
Edlund, J., Heldner, M., & Hirschberg, J. (2009).
Pause and gap length in face-to-face interaction. In
Proc. Interspeech.
Giles, H. & Coupland, N. (1991). Language: Contexts
and consequences. Thomson Brooks/Cole Publish-
ing Co.
Giles, H., Mulac, A., Bradac, J., & Johnson, P. (1987).
Speech accommodation theory: The next decade
and beyond. Communication yearbook, 10, 13?48.
Gweon, G. A. P. U. M. R. B. R. C. P. (2011). The
automatic assessment of knowledge integration pro-
cesses in project teams. In Proceedings of Computer
Supported Collaborative Learning.
Hecht, M., Boster, F., & LaMer, S. (1989). The ef-
fect of extroversion and differentiation on listener-
adapted communication. Communication Reports,
2(1), 1?8.
Huffaker, D., Jorgensen, J., Iacobelli, F., Tepper, P., &
Cassell, J. (2006). Computational measures for lan-
guage similarity across time in online communities.
In In ACTS: Proceedings of the HLT-NAACL 2006
Workshop on Analyzing Conversations in Text and
Speech, pages 15?22.
Jensen, F. V. (1996). An introduction to Bayesian net-
works. UCL Press.
Labov, W. (2010a). Principles of linguistic change:
Internal factors, volume 1. Wiley-Blackwell.
Labov, W. (2010b). Principles of linguistic change:
Social factors, volume 2. Wiley-Blackwell.
Lauritzen, S. L. & Spiegelhalter, D. J. (1988). Local
computations with probabilities on graphical struc-
tures and their application to expert systems. Jour-
nal of the Royal Statistical Society, 50, 157?224.
Levitan, R. & Hirschberg, J. (2011). Measuring
acoustic-prosodic entrainment with respect to mul-
tiple levels and dimensions. In Proceedings of In-
terspeech.
Levitan, R., Gravano, A., & Hirschberg, J. (2011).
Entrainment in speech preceding backchannels. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies: short papers-Volume 2,
pages 113?117. Association for Computational Lin-
guistics.
Liscombe, J., Hirschberg, J., & Venditti, J. (2005). De-
tecting certainness in spoken tutorial dialogues. In
Proceedings of INTERSPEECH, pages 1837?1840.
Citeseer.
Nenkova, A., Gravano, A., & Hirschberg, J. (2008).
High frequency word entrainment in spoken dia-
logue. In In Proceedings of ACL-08: HLT. Asso-
ciation for Computational Linguistics.
opensmile (2011). http://opensmile.sourceforge.net/.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
Purcell, A. (1984). Code shifting hawaiian style: chil-
drens accommodation along a decreolizing contin-
uum. International Journal of the Sociology of Lan-
guage, 1984(46), 71?86.
Putman, W. & Street Jr, R. (1984). The conception
and perception of noncontent speech performance:
Implications for speech-accommodation theory. In-
ternational Journal of the Sociology of Language,
1984(46), 97?114.
Ranganath, R., Jurafsky, D., & McFarland, D. (2009).
It?s not you, it?s me: detecting flirting and its mis-
perception in speed-dates. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1-Volume 1, pages
334?342. Association for Computational Linguis-
tics.
Reitter, D., Keller, F., & Moore, J. D. (2006). Com-
putational modelling of structural priming in dia-
logue. In In Proc. Human Language Technology
conference - North American chapter of the Asso-
ciation for Computational Linguistics annual mtg,
pages 121?124.
Sanders, R. (1987). Cognitive foundations of calcu-
lated speech. State University of New York Press.
Scotton, C. (1985). What the heck, sir: Style shifting
and lexical colouring as features of powerful lan-
796
guage. Sequence and pattern in communicative be-
haviour, pages 103?119.
Wang, Y., Kraut, R., & Levine, J. (2011). To stay or
leave? the relationship of emotional and informa-
tional support to commitment in online health sup-
port groups. In Proceedings of the ACM conference
on computer-supported cooperative work. ACM.
Ward, A. & Litman, D. (2007). Automatically measur-
ing lexical and acoustic/prosodic convergence in tu-
torial dialog corpora. In Proceedings of the SLaTE
Workshop on Speech and Language Technology in
Education. Citeseer.
Welkowitz, J. & Feldstein, S. (1970). Relation of ex-
perimentally manipulated interpersonal perception
and psychological differentiation to the temporal
patterning of conversation. In Proceedings of the
78th Annual Convention of the American Psycho-
logical Association, volume 5, pages 387?388.
797
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 60?69,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Hierarchical Conversation Structure Prediction in Multi-Party Chat
Elijah Mayfield, David Adamson, and Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
{emayfiel, dadamson, cprose}@cs.cmu.edu
Abstract
Conversational practices do not occur at a sin-
gle unit of analysis. To understand the inter-
play between social positioning, information
sharing, and rhetorical strategy in language,
various granularities are necessary. In this
work we present a machine learning model
for multi-party chat which predicts conversa-
tion structure across differing units of analy-
sis. First, we mark sentence-level behavior us-
ing an information sharing annotation scheme.
By taking advantage of Integer Linear Pro-
gramming and a sociolinguistic framework,
we enforce structural relationships between
sentence-level annotations and sequences of
interaction. Then, we show that clustering
these sequences can effectively disentangle
the threads of conversation. This model is
highly accurate, performing near human accu-
racy, and performs analysis on-line, opening
the door to real-time analysis of the discourse
of conversation.
1 Introduction
When defining a unit of analysis for studying lan-
guage, one size does not fit all. Part-of-speech tag-
ging is performed on individual words in sequences,
while parse trees represent language at the sentence
level. Individual tasks can be performed at the lex-
ical, sentence, or document level, or even to arbi-
trary length spans of text (Wiebe et al, 2005), while
rhetorical patterns are annotated in a tree-like struc-
ture across sentences or paragraphs.
In dialogue, the most common unit of analysis is
the utterance, usually through dialogue acts. Here,
too, the issue of granularity and specificity of tags
has been a persistent issue, along with the inte-
gration of larger discourse structure. Both theory-
driven and empirical work has argued for a col-
lapsing of annotations into fewer categories, based
on either marking the dominant function of a given
turn (Popescu-Belis, 2008) or identifying a single
construct of interest and annotating only as nec-
essary to distinguish that construct. We take the
latter approach in this work, predicting conversa-
tion structure particularly as it relates to informa-
tion sharing and authority in dialogue. We use sys-
temic functional linguistics? Negotiation annotation
scheme (Mayfield and Rose?, 2011) to identify utter-
ances as either giving or receiving information. This
annotation scheme is of particular interest because in
addition to sentence-level annotation, well-defined
sequences of interaction are incorporated into the
annotation process. This sequential structure has
been shown to be useful in secondary analysis of
annotated data (Mayfield et al, 2012a), as well as
providing structure which improves the accuracy of
automated annotations.
This research introduces a model to predict infor-
mation sharing tags and Negotiation sequence struc-
ture jointly with thread disentanglement. We show
that performance can be improved using integer lin-
ear programming to enforce constraints on sequence
structure. Structuring and annotation of conversa-
tion is available quickly and with comparatively lit-
tle effort compared to manual annotation. More-
over, all of our results in this paper were obtained
using data a real-world, chat-based internet commu-
nity, with a mix of long-time expert and first-time
60
novice users, showing that the model is robust to the
challenges of messy data in natural environments.
The remainder of this paper is structured as fol-
lows. First, we review relevant work in annota-
tion at the levels of utterance, sequence, and thread,
and applications of each. We then introduce the
domain of our data and the framework we use for
annotation of conversation structure. In Section 4
we define a supervised, on-line machine learning
model which performs this annotation and structur-
ing across granularities. In Section 5, we evaluate
this model and show that it approaches or matches
human reliability on all tasks. We conclude with dis-
cussion of the utility of this conversation structuring
algorithm for new analyses of conversation.
2 Related Work
Research on multi-party conversation structure is
widely varied, due to the multifunctional nature of
language. These structures have been used in di-
verse fields such as computer-supported collabora-
tive work (O?Neill and Martin, 2003), dialogue sys-
tems (Bohus and Horvitz, 2011), and research on
meetings (Renals et al, 2012). Much work in an-
notation has been inspired by speech act theory and
dialogue acts (Traum, 1994; Shriberg et al, 2004),
which operate primarily on the granularity of indi-
vidual utterances. A challenge of tagging is the issue
of specificity of tags, as previous work has shown
that most utterances have multiple functions (Bunt,
2011). General tagsets have attempted to capture
multi-functionality through independent dimensions
which produce potentially millions of possible an-
notations, though in practice the number of varia-
tions remains in the hundreds (Jurafsky et al, 1998).
Situated work has jointly modelled speech act and
domain-specific topics (Laws et al, 2012).
Additional structure inspired by linguistics, such
as adjacency pairs (Schegloff, 2007) or dialogue
games (Carlson, 1983), has been used to build dis-
course relations between turns. This additional
structure has been shown to improve performance
of automated analysis (Poesio and Mikheev, 1998).
Identification of this fine-grained structure of an in-
teraction has been studied in prior work, with appli-
cations in agreement detection (Galley et al, 2004),
addressee detection (op den Akker and Traum,
2009), and real-world applications, such as cus-
tomer service conversations (Kim et al, 2010).
Higher-order structure has also been explored in dia-
logue, from complex graph-like relations (Wolf and
Gibson, 2005) to simpler segmentation-based ap-
proaches (Malioutov and Barzilay, 2006). Utterance
level-tagging can take into account nearby structure,
e.g. forward-looking and backward-looking func-
tions in DAMSL (Core and Allen, 1997), while dia-
logue management systems in intelligent agents of-
ten have a plan unfolding over a whole dialogue
(Ferguson and Allen, 1998).
In recent years, threading and maintaining of mul-
tiple ?floors? has grown in popularity (Elsner and
Charniak, 2010), especially in text-based media.
This level of analysis is designed with the goal of
separating out sub-conversations which are indepen-
dently coherent. There is a common ground emerg-
ing in the thread detection literature on best prac-
tices for automated prediction. Early work viewed
the problem as a time series analysis task (Bingham
et al, 2003). Treating thread detection as a cluster-
ing problem, with lines representing instances, was
given great attention in Shen et al (2006). Subse-
quent researchers have treated the thread detection
task as based in discourse coherence, and have pur-
sued topic modelling (Adams, 2008) or entity refer-
ence grids (Elsner and Charniak, 2011) to define that
concept of coherence.
Other work integrates local discourse structure
with the topic-based threads of discourse. Ai et al
(2007) utilizes information state, a dialogue man-
agement component which loosely parallels thread
structure, to improve dialogue act tagging. In the
context of Twitter conversations, Ritter et al (2010)
suggests using dialogue act tags as a middle layer to-
wards conversation reconstruction. Low-level struc-
ture between utterances has also been used as a
foundation for modelling larger-level sociological
phenomena between speakers in a dialogue, for in-
stance, identifying leadership (Strzalkowski et al,
2011) and rapport between providers and patients
in support groups (Ogura et al, 2008). These
works have all pointed to the utility of incorporat-
ing sentence-level annotations, low-level interaction
structure, and overarching themes into a unified sys-
tem. To our knowledge, however, this work is the
first to present a single system for simultaneous an-
61
Negotiation/Threads Seq User Text
K2 1 C [M], fast question, did your son have a biopsy?
K2 1 C or does that happen when he comes home
K1 2 V i have 3 dogs.
K1 2 V man?s best friend
f 2 S :-D
o 2 C and women
K2 3 J what kind of dogs????
K1 4 C [D], I keep seeing that you are typing and then it stops
K2 5 C how are you doing this week
K1 3 V the puppies are a maltese/yorkie mix and the full grown is a pomara-
nian/yorkie.
K1 1 M No, he did not have a biopsy.
K1 1 M The surgeon examined him and said that by feel, he did not think the
lump was cancerous, and he should just wait until he got home.
f 1 C that has to be very hard
o 7 M A question, however? [J], you would probably know.
K2 7 M He was told that they could not just do a needle biopsy, that he would
have to remove the whole lump in order to tell if it was malignant.
o 8 D Yes.
K1 8 D I was waiting for [M] to answer.
K1 7 J That sounds odd to me
Table 1: An example excerpt with Negotiation labels, sequences, and threads structure (columns) annotated.
notation and structuring at all three levels.
3 Data and Annotation
Our data comes from the Cancer Support Commu-
nity, which provides chatrooms, forums, and other
resources for support groups for cancer patients.
Each conversation took place in the context of a
weekly meeting, with several patient participants as
well as a professional therapist facilitating the dis-
cussion. In total, our annotated corpus consists of
45 conversations. This data was sampled from three
group sizes - 15 conversations from small groups (2
patients, in addition to the trained facilitator), 15
from medium-sized groups (3-4 patients), and 15
from large groups (5 or more patients).
3.1 Annotation
Our data is annotated at the three levels of granu-
larity described previously in this paper: sentences,
sequences, and threads. In this section we define
those annotations in greater detail. Sentence-level
and sequence-level annotations were performed us-
ing the Negotiation framework from systemic func-
tional linguistics (Martin and Rose, 2003). Once
sequences were identified, those sequences were
grouped together into threads based on shared topic.
We annotate our data using an adaptation of the
Negotiation framework. This framework has been
proven reliable and reproducible in previous work
(Mayfield and Rose?, 2011). By assigning aggregate
scores over a conversation, the framework also gives
us a notion of Authoritativeness. This metric, de-
fined later in Section 5, allows us to test whether
automated codes faithfully reproduce human judg-
ments of information sharing behavior at a per-user
level. This metric has proven to be a statistically
significant indicator of outcome variables in direc-
tion giving (Mayfield et al, 2011) and collaborative
learning domains (Howley et al, 2011).
In particular, Negotiation labels define whether
each speaker is a source or recipient of information.
Our annotation scheme has four turn-level codes
and a rigidly defined information sharing structure,
rooted in sociolinguistic observation. We describe
62
each in detail below.
Sentences containing new information are marked
as K1, as the speaker is the ?primary knower,? the
source of information. These sentences can be gen-
eral facts and world knowledge, but can also con-
tain opinions, retelling of narrative, or other contex-
tualized information, so long as the writer acts as
the source of that information. Sentences requesting
information, on the other hand, are marked K2, or
?secondary knower,? when the writer is signalling
that they want information from other participants
in the chat. This can be direct question asking, but
can also include requests for elaboration or indirect
illocutionary acts (e.g. ?I?d like to hear more.?).
In addition to these primary moves, we also use a
social feedback code, f, for sentences consisting of
affective feedback or sentiment, but which do not
contain new information. These moves can include
emoticons, fixed expressions such as ?good luck,? or
purely social banter. All other moves, such as typo
correction or floor grabbing, are labelled o.
This annotation scheme is highly flexible and
adaptive to new domains, and is not specific to med-
ical topics or chatroom-based media. It also gives us
a well-defined structure of an interaction: each se-
quence consists of exactly one primary knower (K1)
move, which can consist of any number of primary
knower sentences from a single speaker. If a K2
move occurs in the sequence, it occurs before any
K1 moves. Feedback moves (f) may come at any
time so long as the speaker is responding to another
speaker in the same sequence. Sentences labeled
o are idiosyncratic and may appear anywhere in a
sequence. In section 4.3, we represent these con-
straints formally.
In addition to grouping sentences together into se-
quences structurally, we also group those sequences
into threads. These threads are based on annotator
judgement, but generally map to the idea that a sin-
gle thread should be on a single theme, e.g. ?han-
dling visiting relatives at holidays.? These threads
are both intrinsically interesting for identifying the
topics of a conversation, as well as being a useful
preprocessing step for any additional, topic-based
annotation that may be desired for later analysis.
We iteratively developed a coding manual for
these layers of annotation; to test reliability at each
iteration of instructions, two annotators each inde-
Figure 1: Structured output at each phase of the two-
pass machine learning model. In pass one, utterances are
grouped into sequences with organizational structure; the
second pass groups sequences based on shared themes.
pendently annotated one full conversation. Inter-
annotator reliability is high for sentence-level an-
notation (? = 0.75). Following Elsner and Char-
niak (2010), we use micro-averaged f-score to eval-
uate inter-rater agreement on higher-level structure.
We find that inter-annotator agreement is high for
both sequence-level structure (f = 0.82) and thread-
level structure (f = 0.80). A detailed description
of the annotation process is available in Mayfield et
al. (2012b). After establishing reliability, our entire
corpus was annotated by one human coder.
4 Conversation Structure Prediction
In previous work, the Negotiation framework has
been automatically coded with high accuracy (May-
field and Rose?, 2011). However, that work restricted
the domain to a task-based, two-person dialogue,
and structure was viewed as a segmentation, rather
than threading, formulation. At each turn, a se-
quence could continue or a new sequence could be-
gin.
Here, we extend this automated coding to larger
groups speaking in unstructured, social chat, and we
extend the structured element of this coding scheme
to structure by sequence and thread. To our knowl-
edge, this is also the first attempt to utilize functional
sequences of interaction as a preprocessing step for
thread disentanglement in chat. We now present a
comprehensive machine learning model which an-
notates a conversation by utterance, groups utter-
ances topics by local structure into sequences, and
assigns sequences to threads.
63
4.1 On-Line Instance Creation
This is a two-pass algorithm. The first pass la-
bels sentences and detects sequences, and the second
pass groups these sequences into threads. We follow
Shen et al (2006) in treating the sequence detection
problem as a single-pass clustering algorithm. Their
model is equivalent to the Previous Cluster model
described below, albeit with more complex features.
In that work a threshold was defined in order for a
new message to be added to an existing cluster. If
that threshold is not passed, a new cluster is formed.
Modelling the probability that a new cluster should
be formed is similar to a context-sensitive threshold,
and because we do not impose a hard threshold, we
can pass the set of probabilities for cluster assign-
ments to a structured prediction system.
4.2 Model Definitions
At its core, our model relies on three probabilistic
classifiers. One of these models is a classification
model, and the other two treat sequence and thread
structure as clusters. All models use the LightSIDE
(Mayfield and Rose?, 2010) with the LibLinear algo-
rithm (Fan et al, 2008) for machine learning..
Negotiation Classifier (Neg)
The Negotiation model takes a single sentence as
input. The output of this model is a distribution over
the four possible sentence-level labels described in
section 3.1. The set of features for this model con-
sists of unigrams, bigrams, and part-of-speech bi-
grams. Part-of-speech tagging was performed using
the Stanford tagger (Toutanova et al, 2003) within
LightSIDE.
Cluster Classifiers (PC, NC)
We use two models of cluster assignment prob-
ability. The Previous Cluster (PC) classifier
takes as input a previous set of sentences C =
{c1, c2, . . . , cn} and set of new sentences N =
{N1, N2, . . . , Nm}. To evaluate whether c? should
be added to this cluster, we train a binary proba-
bilistic classifier that predicts the probability that the
sentences inN belong to the same cluster as the sen-
tences already inC. In the first pass, each inputN to
the PC classifier is a set containing a single sentence,
and each C is the set of sentences in a previously-
identified sequence. In the second pass, each N is a
sequence as predicted by the first pass.
The PC model uses two features. The first is a
time-based feature, measuring the amount of time
that has elapsed between the last sentence in C and
the first sentence in N . The time feature is repre-
sented differently between sequence prediction and
thread prediction. Elsner and Charniak (2010) rec-
ommends using bucketed nominal values based on
the log time, to group together very recent and very
distant posts. We follow this for sequence predic-
tion. Due to the more complex structure of the se-
quence grouping task in the second pass, we use a
raw numeric time feature. The second feature is a
coherence metric, the cosine similarity between the
centroid of C and the centroid of N . We define the
centroid based on TF-IDF weighted unigram vec-
tors.
We impose a threshold after which previous clus-
ters are no longer considered as options for the
PC classifier. Because sequences are shorter than
threads, we set these thresholds separately, at 90 sec-
onds for sequences and 120 seconds for threads. Ap-
proximately 1% of correct assignments are impossi-
ble due to these thresholds.
The New Cluster (NC) classifier takes as input
a set of sentences n = {n1, n2, . . . , nm}, and pre-
dicts the probability that a given sentence is initiat-
ing a new sequence (or, in the second pass, whether
a given sequence is initiating a new thread). This
model contains only unigram features.
At each sentence s we consider the set of possible
previous cluster assignments C = {c1, c2, . . . , cn},
and define psc(s, c) to be the probability that s
will be assigned to cluster c. We define pnc(s) =
?sNC(s). The addition of a weight parameter to
the output of the NC classifier allows us to tune the
likelihood of transitioning to a new cluster. This pre-
diction structure is illustrated in Figure 2. In the
first pass, these cluster probabilities are used in con-
junction with the output of the Negotiation classifier
to form a structured output; in the second pass, the
maximum cluster probability is chosen.
4.3 Constraining Sequence Structure with ILP
In past work the Negotiation framework has bene-
fited from enforced constraints of linguistically sup-
ported rules on sequence structure (Mayfield and
64
Figure 2: The output of the cluster classifier in either pass
is a set of probabilities corresponding to possible clus-
ter assignments, including that of creating a new cluster.
In the second pass, the input is a set of sentences (a se-
quence) rather than a single sentence, and output assign-
ments are to threads rather than sequences.
Rose?, 2011). Constraints on the structure of anno-
tations are easily defined using Integer Linear Pro-
gramming. Recent work has used boolean logic
(Chang et al, 2008) to allow intuitive rules about
a domain to be enforced at classification time. ILP
inference was performed using Learning-Based Java
(Rizzolo and Roth, 2010).
First, we define the classification task. Opti-
mization is performed given the set of probabilities
N (s) as the distribution output of the Neg classifier
given sentence s as input, and the set of probabilities
C(s) = pnc(s) ? psc(s, c), ?c ? C. Instance classi-
fication requires maximizing the objective function:
arg max
n?N (s),c?C(s)
n+ c
We impose constraints on sequence prediction. If
the most likely output from this function assigns
a label that is incompatible with the assigned se-
quence, either the label is changed or a new se-
quence is assigned so that constraints are met. For
each constraint, we give the intuition from sec-
tion 3.1, followed by our formulation of that con-
straint. us is shorthand for the user who wrote
sentence s; ns is shorthand for a proposed Ne-
gotiation label of sentence s; while cs is a pro-
posed sequence assignment for s, c? is shorthand
for assignment to a new sequence, and Sc =
{(nc,1, uc,1), (nc,2, uc,2), . . . , (nc,k, uc,k)} is the set
of Negotiation labels n and users u associated with
sentences (sc,1 . . . sc,k) already in sequence c.
1. K2 moves, if any, occur before K1 moves.
((cs = c) ? (ns = K2))
? (@i ? Sc s.t. nc,i = K1)
2. f moves may occur at any time but must be re-
sponding to a different speaker in the same se-
quence.
((cs = c) ? (ns = f))
? (?i ? Sc s.t. uc,i 6= us)
3. Functionally, therefore, f moves may not initi-
ate a sequence).
(cs = c?) ? (ns 6= f)
4. Speakers do not respond to their own requests
for information (the speakers of K2 and K1
moves in the same sequence must be different).
((cs = c) ? (ns = K1))
? (?i ? Sc, ((nc,i = K2) ? (uc,i 6= us)))
5. Each sequence consists of at most one continu-
ous series of K1 moves from the same speaker.
(cs = c) ? ((?i ? Sc s.t. (nc,i = K1))
? ( (uc,i = us) ? (?j > i,
(uc,j = us) ? (nc,i = K1)) )
Human annotators treated these rules as hard con-
straints, as the classifier does. In circumstances
where these rules would be broken (for instance, due
to barge-in or trailing off), a new sequence begins.
5 Evaluation
5.1 Methods
To evaluate the performance of this model, we wish
to know how it replicates human annotation at each
granularity. For Negotiation labels, agreement is
measured by terms of absolute accuracy and kappa
agreement above chance. We also include a measure
of aggregate information sharing behavior per user.
This score, which we term Information Authorita-
tiveness (Auth), is defined per user as the percentage
65
of their contentful sentences (K1 or K2) which were
giving information (K1). To measure performance
on this measure, we measure the r2 coefficient be-
tween user authoritativeness scores calculated from
the predicted labels compared to actual labels. This
is equivalent to measuring the variance explained by
our model, where each data point represents a single
user?s predicted and actual authoritativeness scores
over the course of a whole conversation (n = 215).
Sequence and thread agreement is evaluated by
micro-averaged f-score (MAF), defined in prior
work for a gold sequence i with size ni, and a pro-
posed sequence j with size nj , based on precision
and recall metrics:
P = nijnj R =
nij
ni F (i, j) =
2?P?R
P+R
MAF across an entire conversation is then a
weighted sum of f-scores across all sequences1:
MAF =
?
i
ni
n maxj F (i, j)
We implemented multiple baselines to test
whether our methods improve upon simpler ap-
proaches. For sequence and thread prediction, we
implement the following baselines. Speaker Shift
predicts a new thread every time a new writer adds a
line to the chat. Turn Windows predicts a new se-
quence or thread after every n turns. Pause Length
predicts a new sequence or thread every time that a
gap of n seconds has occurred between lines of chat.
For both of the previous two baselines, we vary the
parameter n to optimize performance and provide
a challenging baseline. None of these models use
any features or constraints, and are based on heuris-
tics. To compare to our model, we present both an
Unconstrained model, which uses machine learn-
ing and does not impose sequence constraints from
Section 4.3, as well as our full Constrained model.
Evaluation is performed using 15-fold cross-
validation. In each fold, one small, one medium,
and one large conversation are held out as a test set,
and classifiers are trained on the remaining 42 con-
versations. Significance is evaluated using a paired
student?s t-test per conversation (n = 45).
Sentence-Level (Human ? = 0.75)
Model Accuracy ? Auth r2
Unconstrained .7736 .5870 .7498
Constrained .7777 .5961 .7355
Sequence-Level (Human MAF = 0.82)
Model Precision Recall MAF
Speaker Shift .7178 .5140 .5991
Turn Windows .7207 .6233 .6685
Pause Length .8479 .6582 .7411
Unconstrained .7909 .7068 .7465
Constrained .8557 .7116 .7770
Thread-Level (Human MAF = 0.80)
Model Precision Recall MAF
Turn Windows .5994 .7173 .6531
Pause Length .6145 .6316 .6229
Unconstrained .7132 .5781 .6386
Constrained .6805 .6024 .6391
Table 2: Tuned optimal annotation performances of base-
line heuristics compared to our machine learning model.
5.2 Results
Results of experimentation show that all models
are highly accurate in their respective tasks. With
sentence-level annotation approaching 0.6 ?, the
output of the model is reliable enough to allow
automatically annotated data to be included reli-
ably alongside human annotations. Performance for
sequence-based modelling is even stronger, with no
statistically significant difference in f-score between
the machine learning model and human agreement.
Table 2 reports our best results after tuning to
maximize performance of baseline models, our orig-
inal machine learning model, and the model with
ILP constraints enforced between Negotiation labels
and sequence. In all three cases, we see machine
performance approaching, but not matching, human
agreement. Incorporating ILP constraints improves
per-sentence Negotiation label classification by a
small but significant amount (p < .001).
Clustering performance is highly robust, as
demonstrated in Figure 3, which shows the effect of
changing window sizes and pause lengths and values
of ?s for machine learned models. Our thread disen-
tanglement performance matches our baselines, and
1This metric extends identically to a gold thread i and pro-
posed thread j.
66
Figure 3: Parameter sensitivity on sequence-level (top)
and thread-level (bottom) annotation models.
is in line with heuristic-based assignments from El-
sner and Charniak (2010). In sequence clustering,
we observe improvement across all metrics. The
Constrained model achieves a higher f-score than all
other models (p < 0.0001). We determine through
a two-tailed confidence interval that sequence clus-
tering performance is statistically indistinguishable
from human annotation (p < 0.05).
Error analysis suggests that the constraints are too
punishing on the most constrained labels, K2 and f.
The differences in performance between constrained
and unconstrained models is largely due to higher
recall for both K1 and o move prediction, while
recall for K2 and f moves lowered slightly. One
possibility for future work may include compensat-
ing for this by artificially inflating the likelihood of
highly-constrained Negotiation labels. Additionally,
we see that the most common mistakes involve dis-
tinguishing between K1 and f moves. While many
f moves are obviously non-content-bearing (?Wow,
what fun!?), others, especially those based in humor,
may look grammatical and contentful (?We?ve got to
stop meeting this way.?). Better detection of humor
and a more well-defined definition of what informa-
tion is being shared will improve this aspect of the
model. Overall, these errors do not limit the efficacy
of the model for enabling future analysis.
6 Conclusion and Future Work
This work has presented a unified machine learn-
ing model for annotating information sharing acts
on a sentence-by-sentence granularity; grouping se-
quences of sentences based on functional structure;
and then grouping those sequences into topic-based
threads. The model performs at a high accuracy,
approaching human agreement at the sentence and
thread level. Thread-level accuracy matched but did
not exceed simpler baselines, suggesting that this
model could benefit from a more elaborate repre-
sentation of coherence and topic. At the level of se-
quences, the model performs statistically the same
as human annotation.
The automatic annotation and structuring of di-
alogue that this model performs is a vital prepro-
cessing task to organize and structure conversational
data in numerous domains. Our model allows re-
searchers to abstract away from vocabulary-based
approaches, instead working with interaction-level
units of analysis. This is especially important in
the context of interdisciplinary research, where other
representations may be overly specialized towards
one task, and vocabulary may differ for spurious rea-
sons across populations and cultures.
Our evaluation was performed on a noisy, real-
world chatroom corpus, and still performed very ac-
curately. Coherent interfacing between granularities
of analysis is always a challenge. Segmentation,
tokenization, and overlapping or inconsistent struc-
tured output are nontrivial problems. By incorpo-
rating sentence-level annotation, discourse-level se-
quence structure, and topical thread disentanglement
into a single model, we have shown one way to re-
duce or eliminate this interfacing burden and allow
greater structural awareness in real-world systems.
Future work will improve this model?s accuracy fur-
ther, test its generality in new domains such as spo-
ken multi-party interactions, and evaluate its useful-
ness in imposing structure for secondary analysis.
67
Acknowledgments
The research reported here was supported by Na-
tional Science Foundation grant IIS-0968485, Of-
fice of Naval Research grant N000141110221, and
in part by the Pittsburgh Science of Learning Center,
which is funded by the National Science Foundation
grant SBE-0836012.
References
Paige H. Adams. 2008. Conversation Thread Extraction
and Topic Detection in Text-based Chat. Ph.D. thesis.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007. Using information state to improve dia-
logue move identification in a spoken dialogue system.
In Proceedings of Interspeech.
Ella Bingham, Ata Kaban, and Mark Girolami. 2003.
Topic identification in dynamical text by complexity
pursuit. In Neural Processing Letters.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn tak-
ing in situated dialog. In Procedings of SIGDIAL.
Harry Bunt. 2011. Multifunctionality in dialogue. In
Computer Speech and Language.
Lauri Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. Massachussetts Institute of Tech-
nology.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In Proceedings of the Association for the Ad-
vancement of Artificial Intelligence.
Mark G Core and James F Allen. 1997. Coding dialogs
with the damsl annotation scheme. In AAAI Fall Sym-
posium on Communicative Action in Humans and Ma-
chines.
Micha Elsner and Eugene Charniak. 2010. Disentan-
gling chat. Computational Linguistics.
Micha Elsner and Eugene Charniak. 2011. Disentan-
gling chat with local coherence models. In Proceed-
ings of the Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification.
George Ferguson and James Allen. 1998. Trips: An in-
tegrated intelligent problem-solving assistant. In Pro-
ceedings of AAAI.
Michael Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL.
Iris Howley, Elijah Mayfield, and Carolyn Penstein Rose?.
2011. Missing something? authority in collaborative
learning. In Proceedings of Computer Supported Col-
laborative Learning.
Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel
Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,
Andreas Stolcke, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Switchboard discourse language
modelling final report. Technical report.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
M Barton Laws, Mary Catherine Beach, Yoojin Lee,
William H. Rogers, Somnath Saha, P Todd Korthuis,
Victoria Sharp, and Ira B Wilson. 2012. Provider-
patient adherence dialogue in hiv care: Results of a
multisite study. AIDS Behavior.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL/COLING.
J.R. Martin and David Rose. 2003. Working with Dis-
course: Meaning Beyond the Clause. Continuum.
Elijah Mayfield and Carolyn Penstein Rose?. 2010. An
interactive tool for supporting error analysis for text
mining. In NAACL Demonstration Session.
Elijah Mayfield and Carolyn Penstein Rose?. 2011. Rec-
ognizing authority in dialogue with an integer linear
programming constrained model. In Proceedings of
Association for Computational Linguistics.
Elijah Mayfield, Michael Garbus, David Adamson, and
Carolyn Penstein Rose?. 2011. Data-driven interac-
tion patterns: Authority and information sharing in di-
alogue. In Proceedings of AAAI Fall Symposium on
Building Common Ground with Intelligent Agents.
Elijah Mayfield, David Adamson, Alexander I Rudnicky,
and Carolyn Penstein Rose?. 2012a. Computational
representations of discourse practices across popula-
tions in task-based dialogue. In Proceedings of the
International Conference on Intercultural Collabora-
tion.
Elijah Mayfield, Miaomiao Wen, Mitch Golant, and Car-
olyn Penstein Rose?. 2012b. Discovering habits of ef-
fective online support group chatrooms. In ACM Con-
ference on Supporting Group Work.
Kanayo Ogura, Takashi Kusumi, and Asako Miura.
2008. Analysis of community development using
chat logs: A virtual support group of cancer patients.
In Proceedings of the IEEE Symposium on Universal
Communication.
Jacki O?Neill and David Martin. 2003. Text chat in ac-
tion. In Proceedings of the International Conference
on Supporting Group Work.
68
Rieks op den Akker and David Traum. 2009. A compari-
son of addressee detection methods for multiparty con-
versations. In Workshop on the Semantics and Prag-
matics of Dialogue.
Massimo Poesio and Andrei Mikheev. 1998. The pre-
dictive power of game structure in dialogue act recog-
nition: Experimental results using maximum entropy
estimation. In Proceedings of the International Con-
ference on Spoken Language Processing.
Andrei Popescu-Belis. 2008. Dimensionality of dialogue
act tagsets: An empirical analysis of large corpora. In
Language Resources and Evaluation.
Steve Renals, Herve? Bourlard, Jean Carletta, and Andrei
Popescu-Belis. 2012. Multimodal Signal Processing:
Human Interactions in Meetings.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of NAACL.
Nicholas Rizzolo and Dan Roth. 2010. Learning based
java for rapid development of nlp systems. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation.
E. Schegloff. 2007. Sequence organization in interac-
tion: A primer in conversation analysis. Cambridge
University Press.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In Proceedings of SIGIR.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The icsi meeting
recorder dialog act (mrda) corpus. In Proceedings of
SIGDIAL.
Tomek Strzalkowski, George Aaron Broadwell, Jennifer
Stromer-Galley, Samira Shaikh, Ting Liu, and Sarah
Taylor. 2011. Modeling socio-cultural phenomena in
online multi-party discourse. In AAAI Workshop on
Analyzing Microtext.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
David Traum. 1994. A computational theory of ground-
ing in natural language conversation. Ph.D. thesis.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics.
69
