Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 24?25,
Vancouver, October 2005.
A Flexible Conversational Dialog System for MP3 Player
Fuliang Weng
1
 Lawrence Cavedon
2
 Badri Raghunathan
1
 Danilo Mirkovic
2 
Ben Bei
1
Heather Pon-Barry
1
 Harry Bratt
3
 Hua Cheng
2
 Hauke Schmidt
1
 Rohit Mishra
4
 Brian Lathrop
4
Qi Zhang
1
   Tobias Scheideck
1
   Kui Xu
1
    Tess Hand-Bender
1
   Sandra Upson
1
     Stanley Peters
2
Liz Shriberg
3
 Carsten Bergmann
4
Research and Technology Center, Robert Bosch Corp., Palo Alto, California
1
Center for Study of Language and Information, Stanford University, Stanford, California
2
Speech Technology and Research Lab, SRI International, Menlo Park, California
3
Electronics Research Lab, Volkswagen of America, Palo Alto, California
4
{Fuliang.weng,badri.raghunathan,hauke.Schmidt}@rtc.bosch.com
{lcavedon,huac,peters}@csli.Stanford.edu
{harry,ees}@speech.sri.com
{rohit.mishra,carsten.bergmann}@vw.com
1 Abstract
In recent years, an increasing number of new de-
vices have found their way into the cars we drive.
Speech-operated devices in particular provide a
great service to drivers by minimizing distraction,
so that they can keep their hands on the wheel and
their eyes on the road. This presentation will dem-
onstrate our latest development of an in-car dialog
system for an MP3 player designed under a joint
research effort from Bosch RTC, VW ERL, Stan-
ford CSLI, and SRI STAR Lab funded by NIST
ATP [Weng et al2004] with this goal in mind.
This project has developed a number of new tech-
nologies, some of which are already incorporated
in the system.  These include: end-pointing with
prosodic cues, error identification and recovering
strategies, flexible multi-threaded, multi-device
dialog management, and content optimization and
organization strategies. A number of important
language phenomena are also covered in the sys-
tem?s functionality. For instance, one may use
words relying on context, such as ?this,? ?that,? ?it,?
and ?them,? to reference items mentioned in par-
ticular use contexts. Different types of verbal revi-
sion are also permitted by the system, providing a
great convenience to its users. The system supports
multi-threaded dialogs so that users can diverge to
a different topic before the current one is finished
and still come back to the first after the second
topic is done. To lower the cognitive load on the
drivers, the content optimization component orga-
nizes any information given to users based on on-
tological structures, and may also refine users?
queries via various strategies. Domain knowledge
is represented using OWL, a web ontology lan-
guage recommended by W3C, which should
greatly facilitate its portability to new domains.
The spoken dialog system consists of a number of
components (see Fig. 1 for details). Instead of the
hub architecture employed by Communicator pro-
jects [Senef et al 1998], it is developed in Java and
uses a flexible event-based, message-oriented mid-
dleware. This allows for dynamic registration of
new components. Among the component modules
in Figure 1, we use the Nuance speech recognition
engine with class-based ngrams and dynamic
grammars, and the Nuance Vocalizer as the TTS
engine. The Speech Enhancer removes noises and
echo. The Prosody module will provide additional
features to the Natural Language Understanding
(NLU) and Dialogue Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation.
Parallel to the deep analysis, a topic classifier as-
signs top n topics to the utterance, which are used
in the cases where the dialog manager cannot make
24
any sense of the parsed structure. The NLU mod-
ule also supports dynamic updates of the knowl-
edge base.
The CSLI DM module mediates and manages in-
teraction. It uses the dialogue-move approach to
maintain dialogue context, which is then used to
interpret incoming utterances (including fragments
and revisions), resolve NPs, construct salient re-
sponses, track issues, etc. Dialogue states can also
be used to bias SR expectation and improve SR
performance, as has been performed in previous
applications of the DM. Detailed descriptions of
the DM can be found in [Lemon et al2002; Mirk-
ovic & Cavedon 2005].
The Knowledge Manager (KM) controls access to
knowledge base sources (such as domain knowl-
edge and device information) and their updates.
Domain knowledge is structured according to do-
main-dependent ontologies. The current KM
makes use of OWL, a W3C standard, to represent
the ontological relationships between domain enti-
ties. Prot?g? (http://protege.stanford.edu), a do-
main-independent ontology tool, is used to
maintain the ontology offline. In a typical interac-
tion, the DM converts a user?s query into a seman-
tic frame (i.e. a set of semantic constraints) and
sends this to the KM via the content optimizer.
The Content Optimization module acts as an in-
termediary between the dialogue management
module and the knowledge management module
during the query process. It receives semantic
frames from the DM, resolves possible ambigui-
ties, and queries the KM. Depending on the items
in the query result as well as the configurable
properties, the module selects and performs an ap-
propriate optimization strategy.
Early evaluation shows that the system has a
task completion rate of 80% on 11 tasks of MP3
player domain, ranging from playing requests to
music database queries. Porting to a restaurant se-
lection domain is currently under way.
References
Seneff, Stephanie, Ed Hurley, Raymond Lau, Christine Pao,
Philipp Schmid, and Victor Zue, GALAXY-II: A Reference
Architecture for Conversational System Development, In-
ternational Conference on Spoken Language Processing
(ICSLP), Sydney, Australia, December 1998.
Lemon, Oliver, Alex Gruenstein, and Stanley Peters, Collabo-
rative activities and multi-tasking in dialogue systems,
Traitement Automatique des Langues (TAL), 43(2), 2002.
Mirkovic, Danilo, and Lawrence Cavedon, Practical Multi-
Domain, Multi-Device Dialogue Management, Submitted
for publication, April 2005.
Weng, Fuliang, Lawrence Cavedon, Badri Raghunathan, Hua
Cheng, Hauke Schmidt, Danilo Mirkovic, et al, Develop-
ing a conversational dialogue system for cognitively over-
loaded users, International Conference on Spoken
Language Processing (ICSLP), Jeju, Korea, October 2004.
25
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 185?188,
New York, June 2006. c?2006 Association for Computational Linguistics
A Maximum Entropy Framework that Integrates Word Dependencies and
Grammatical Relations for Reading Comprehension
Kui Xu1,2 and Helen Meng1
1Human-Computer Communications Laboratory
Dept. of Systems Engineering and
Engineering Management
The Chinese University of Hong Kong
Hong Kong SAR, China
{kxu, hmmeng}@se.cuhk.edu.hk
Fuliang Weng2
2Research and Technology Center
Robert Bosch Corp.
Palo Alto, CA 94304, USA
Fuliang.weng@rtc.bosch.com
Abstract
Automatic reading comprehension (RC)
systems can analyze a given passage and
generate/extract answers in response to
questions about the passage. The RC
passages are often constrained in their
lengths and the target answer sentence
usually occurs very few times. In order
to generate/extract a specific precise an-
swer, this paper proposes the integration
of two types of ?deep? linguistic features,
namely word dependencies and grammati-
cal relations, in a maximum entropy (ME)
framework to handle the RC task. The
proposed approach achieves 44.7% and
73.2% HumSent accuracy on the Reme-
dia and ChungHwa corpora respectively.
This result is competitive with other re-
sults reported thus far.
1 Introduction
Automatic reading comprehension (RC) systems
can analyze a given passage and generate/extract
answers in response to questions about the pas-
sage. The RC passages are often constrained in
their lengths and the target answer sentence usu-
ally occurs only once (or very few times). This
differentiates the RC task from other tasks such as
open-domain question answering (QA) in the Text
Retrieval Conference (Light et al, 2001). In order
to generate/extract a specific precise answer to a
given question from a short passage, ?deep? linguis-
tic analysis of sentences in a passage is needed.
Previous efforts in RC often use the bag-of-words
(BOW) approach as the baseline, which is further
augmented with techniques such as shallow syn-
tactic analysis, the use of named entities (NE) and
pronoun references. For example, Hirschman et
al. (1999) have augmented the BOW approach
with stemming, NE recognition, NE filtering, se-
mantic class identification and pronoun resolution
to achieve 36% HumSent1 accuracy in the Reme-
dia test set. Based on these technologies, Riloff
and Thelen (2000) improved the HumSent accuracy
to 40% by applying a set of heuristic rules that as-
sign handcrafted weights to matching words and NE.
Charniak et al (2000) used additional strategies for
different question types to achieve 41%. An exam-
ple strategy for why questions is that if the first word
of the matching sentence is ?this,? ?that,? ?these? or
?those,? the system should select the previous sen-
tence as an answer. Light et al (2001) also intro-
duced an approach to estimate the performance up-
per bound of the BOW approach. When we apply
the same approach to the Remedia test set, we ob-
tained the upper bound of 48.3% HumSent accuracy.
The state-of-art performance reached 42% with an-
swer patterns derived from web (Du et al, 2005).
This paper investigates the possibility of enhanc-
ing RC performance by applying ?deep? linguistic
analysis for every sentence in the passage. We
refer to the use of two types of features, namely
word dependencies and grammatical relations, that
1If the system?s answer sentence is identical to the corre-
sponding human marked answer sentence, the question scores
one point. Otherwise, the question scores no point. HumSent
accuracy is the average score across all questions.
185
are integrated in a maximum entropy framework.
Word dependencies refer to the headword depen-
dencies in lexicalized syntactic parse trees, together
with part-of-speech (POS) information. Grammat-
ical relations (GR) refer to linkages such as sub-
ject, object, modifier, etc. The ME framework
has shown its effectiveness in solving QA tasks (It-
tycheriah et al, 1994). In comparison with previ-
ous approaches mentioned earlier, the current ap-
proach involves richer syntactic information that
cover longer-distance relationships.
2 Corpora
We used the Remedia corpus (Hirschman et al,
1999) and ChungHwa corpus (Xu and Meng, 2005)
in our experiments. The Remedia corpus contains
55 training stories and 60 testing stories (about 20K
words). Each story contains 20 sentences on aver-
age and is accompanied by five types of questions:
who, what, when, where and why. The ChungHwa
corpus contains 50 training stories and 50 test stories
(about 18K words). Each story contains 9 sentences
and is accompanied by four questions on average.
Both the Remedia and ChungHwa corpora contain
the annotation of NE, anaphor referents and answer
sentences.
3 The Maximum Entropy Framework
Suppose a story S contains n sentences, C0, . . . , Cn,
the objective of an RC system can be described as:
A = arg maxCi?S P (Ci answers Q|Q). (1)
Let ?x? be the question (Q) and ?y? be the answer
sentence Ci that answers ?x?. Equation 1 can be
computed by the ME method (Zhou et al, 2003):
p(y|x) = 1Z(x) exp
?
j ?jfj(x,y), (2)
where Z(x) = ?y exp
?
j
?jfj(x,y) is a normalization
factor, fj(x, y) is the indicator function for feature
fj; fj occurs in the context x, ?j is the weight of
fj . For a given question Q, the Ci with the highest
probability is selected. If multiple sentences have
the maximum probability, the one that occurs
the earliest in the passage is returned. We used
the selective gain computation (SGC) algorithm
(Zhou et al, 2003) to select features and estimate
parameters for its fast performance.
Question: Who wrote the "Pledge of Allegiance"
Answer sentence: The pledge was written by Frances Bellamy.
PP(by)
by/IN
Frances/NNP Bellamy/NNP
was/VBD
NPB(Bellamy)
PP(of)
NP(Pledge)
VP(wrote)
Who/WP of/IN
WHNP(Who)
SBARQ(wrote)
wrote/VBD NP(Allegiance)
Allegiance/NNP "/??
NP(Pledge)
the/DT "/?? Pledge/NN
The/DT
NPB(pledge)
written/VBN
VP(written)
S(written)
VP(written)
pledge/NN
Figure 1. The lexicalized syntactic parse trees of a
question and a candidate answer sentence.
4 Features Used in the ?Deep? Linguistic
Analysis
A feature in the ME approach typically has binary
values: fj(x, y) = 1 if the feature j occurs; other-
wise fj(x, y) = 0. This section describes two types
of ?deep? linguistic features to be integrated in the
ME framework in two subsections.
4.1 POS Tags of Matching Words and
Dependencies
Consider the following question Q and sentence C ,
Q: Who wrote the ?Pledge of Allegiance?
C: The pledge was written by Frances Bellamy.
The set of words and POS tags2 are:
Q: {write/VB, pledge/NN, allegiance/NNP}
C: {write/VB, pledge/NN, by/IN, Frances/NNP,
Bellamy/NNP}.
Two matching words between Q and C (i.e. ?write?
and ?pledge?) activate two POS tag features:
fV B(x, y)=1 and fNN (x, y)=1.
We extracted dependencies from lexicalized
syntactic parse trees, which can be obtained accord-
ing to the head-rules in (Collins, 1999) (e.g. see
Figure 1). In a lexicalized syntactic parse tree, a
dependency can be defined as:
< hc ? hp > or < hr ? TOP >,
where hc is the headword of the child node, hp
is the headword of the parent node (hc 6= hp),
hr is the headword of the root node. Sample
2We used the MXPOST toolkit downloaded from
ftp://ftp.cis.upenn.edu/pub/adwait/jmx/ to generate POS
tags. Stop words including who, what, when, where, why,
be, the, a, an, and of are removed in all questions and story
sentences. All plural noun POS tags are replaced by their
single forms (e.g. NNS?NN); all verb POS tags are replaced
by their base forms (e.g. VBN?VB) due to stemming.
186
mod
be
write/V
subj
Question: Who wrote the "Pledge of Allegiance"
the/Det be/be
by/Prep
pcomp?n
Frances Bellamy/N
pledge/N
obj
det
write/V subj
Answer sentence: The pledge was written by Frances Bellamy.
Who/N the/Det
Pledge/N
det
punc
"/U of/Prep
Allegiance/N
punc
"/U
mod
obj
Figure 2. The dependency trees produced by MINI-
PAR for a question and a candidate answer sentence.
dependencies in C (see Figure 1) are:
<write?TOP> and <pledge?write>.
The dependency features are represented by the
combined POS tags of the modifiers and headwords
of (identical) matching dependencies3 . A matching
dependency between Q and C , <pledge?write>
activates a dependency feature: fNN?V B(x, y)=1.
In total, we obtained 169 and 180 word dependency
features from the Remedia and ChungHwa training
sets respectively.
4.2 Matching Grammatical Relationships (GR)
We extracted grammatical relationships from the de-
pendency trees produced by MINIPAR (Lin, 1998),
which covers 79% of the dependency relationships
in the SUSANNE corpus with 89% precision4 . IN
a MINIPAR dependency relationship:
(word1 CATE1:RELATION:CATE2 word2),
CATE1 and CATE2 represent such grammatical cat-
egories as nouns, verbs, adjectives, etc.; RELA-
TION represents the grammatical relationships such
as subject, objects, modifiers, etc.5 Figure 2 shows
dependency trees of Q and C produced by MINI-
PAR. Sample grammatical relationships in C are
pledge N:det:Det the, and write V:by-subj:Prep by.
GR features are extracted from identical matching
relationships between questions and candidate sen-
tences. The only identical matching relationship be-
tween Q and C , ?write V:obj:N pledge? activates a
grammatical relationship feature: fobj(x, y)=1. In
total, we extracted 44 and 45 GR features from the
Remedia and ChungHwa training sets respectively.
3We extracted dependencies from parse trees generated by
Collins? parser (Collins, 1999).
4MINIPAR outputs GR directly, while Collins? parser gives
better result for dependencies.
5Refer to the readme file of MINIPAR downloaded from
http://www.cs.ualberta.ca/ lindek/minipar.htm
5 Experimental Results
We selected the features used in Quarc (Riloff and
Thelen, 2000) to establish the reference performance
level. In our experiments, the 24 rules in Quarc are
transferred6 to ME features:
?If contains(Q,{start, begin}) and contains(S,{start,
begin, since, year}) Then Score(S)+=20? ?
fj(x, y) = 1 (0< j <25) if Q is a when question that
contains ?start? or ?begin? and C contains ?start,?
?begin,? ?since? or ?year?; fj(x, y) = 0 otherwise.
In addition to the Quarc features, we resolved five
pronouns (he, him, his, she and her) in the stories
based on the annotation in the corpora. The result
of using Quarc features in the ME framework is
38.3% HumSent accuracy on the Remedia test set.
This is lower than the result (40%) obtained by our
re-implementation of Quarc that uses handcrafted
scores. A possible explanation is that handcrafted
scores are more reliable than ME, since humans
can generalize the score even for sparse data.
Therefore, we refined our reference performance
level by combining the ME models (MEM) and
handcrafted models (HCM). Suppose the score of a
question-answer pair is score(Q,Ci), the conditional
probability that Ci answers Q in HCM is:
HCM(Q,Ci) = P (Ci answers Q|Q) = score(Q,Ci)?j?nscore(Q,Cj) .We combined the probabilities from MEM and
HCM in the following manner:
score?(Q, Ci) = ?MEM(Q, Ci) + (1 ? ?)HCM(Q, Ci).
To obtain the optimal ?, we partitioned the training
set into four bins. The ME models are trained on
three different bins; the optimal ? is determined
on the other bins. By trying different bins com-
binations and different ? such that 0 < ? < 1
with interval 0.1, we obtained the average optimal
? = 0.15 and 0.9 from the Remedia and ChungHwa
training sets respectively7 . Our baseline used the
combined ME models and handcrafted models to
achieve 40.3% and 70.6% HumSent accuracy in the
Remedia and ChungHwa test sets respectively.
We set up our experiments such that the linguistic
features are applied incrementally - (i) First , we use
only POS tags of matching words among questions
6The features in (Charniak et al, 2000) and (Du et al, 2005)
could have been included similarly if they were available.
7HCM are tuned by hand on Remedia, thus a bigger weight,
0.85 represents their reliability. For ChungHwa, a weight, 0.1
means that HCM are less reliable.
187
and candidate answer sentences. (ii) Then we add
POS tags of the matching dependencies. (iii) We ap-
ply only GR features from MINIPAR. (iv) All fea-
tures are used. These four feature sets are denoted
as ?+wp,? ?+wp+dp,? ?+mini? and ?+wp+dp+mini?
respectively. The results are shown in Figure 3 for
the Remedia and ChungHwa test sets.
With the significance level 0.05, the pairwise t-
test (for every question) to the statistical significance
of the improvements shows that the p-value is 0.009
and 0.025 for the Remedia and ChungHwa test sets
respectively. The ?deep? syntactic features signif-
icantly improve the performance over the baseline
system on the Remedia and ChungHwa test sets8.
Baseline +wp +wp+dp +mini +wp+dp+mini
Combinations of different features
H
um
Se
nt
 A
cc
ur
ac
y(%
)
30
40
50
60
70
80
90
Remedia
ChungHwa
40.3 41.7
43.3 43 44.7
70.6 71.1 72.7 72.2
73.2
Figure 3. Baseline and proposed feature results on
the Remedia and ChungHwa test sets.
6 Conclusions
This paper proposes the integration of two types of
?deep? linguistic features, namely word dependen-
cies and grammatical relations, in a ME framework
to handle the RC task. Our system leverages
linguistic information such as POS, word depen-
dencies and grammatical relationships in order to
extract the appropriate answer sentence for a given
question from all available sentences in the passage.
Our system achieves 44.7% and 73.2% HumSent
accuracy on the Remedia and ChungHwa test sets
respectively. This shows a statistically significant
improvement over the reference performance levels,
40.3% and 70.6% on the same test sets.
Acknowledgements
This work is done during the first author?s internship
8Our previous work about developing the ChungHwa corpus
(Xu and Meng, 2005) shows that most errors can only be solved
by reasoning with domain ontologies and world knowledge.
at RTC Bosch Corp. The work is also affiliated with
the CUHK Shun Hing Institute of Advanced Engi-
neering and partially supported by CUHK4237/03E
from RGC of HKSAR Government.
References
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. Workshop on the Evaluation of Parsing
Systems 1998.
Ellen Riloff and Michael Thelen. 2000. A Rule-based
Question Answering System for Reading Comprehen-
sion Test. ANLP/NAACL-2000 Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
Eugene Charniak, Yasemin Altun, Rofrigo D. Braz, Ben-
jamin Garrett, Margaret Kosmala, Tomer Moscovich,
Lixin Pang, Changhee Pyo, Ye Sun, Wei Wy, Zhongfa
Yang, Shawn Zeller, and Lisa Zorn. 2000. Reading
Comprehension Programs In a Statistical-Language-
Processing Class. ANLP-NAACL 2000 Work-
shop: Reading Comprehension Tests as Evaluation for
Computer-Based Language Understanding Systems.
Kui Xu and Helen Meng. 2005. Design and Develop-
ment of a Bilingual Reading Comprehension Corpus.
International Journal of Computational Linguistics &
Chinese Language Processing, Vol. 10, No. 2.
Lynette Hirschman, Marc Light, Eric Breck, and John D.
Burger. 1999. Deep Read: A Reading Comprehension
System. Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Marc Light, Gideon S. Mann, Ellen Riloff, and Eric
Breck. 2001. Analyses for Elucidating Current Ques-
tion Answering Technology. Journal of Natural Lan-
guage Engineering, No. 4 Vol. 7.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD thesis, University
of Pennsylvania.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu and
Adwait Ratnaparkhi. 2001. Question Answering Us-
ing Maximum-Entropy Components. Proceedings of
NAACL 2001.
Yaqian Zhou, Fuliang Weng, Lide Wu, Hauke Schmidt.
2003. A Fast Algorithm for Feature Selection in Con-
ditional Maximum Entropy Modeling. Proceedings of
EMNLP 2003.
Yongping Du, Helen Meng, Xuanjing Huang, Lide
Wu. 2005. The Use of Metadata, Web-derived An-
swer Patterns and Passage Context to Improve Read-
ing Comprehension Performance. Proceedings of
HLT/EMNLP 2005.
188
NAACL HLT Demonstration Program, pages 23?24,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
A Conversational In-car Dialog System
Baoshi Yan1 Fuliang Weng1 Zhe Feng1 Florin Ratiu2 Madhuri Raya1 Yao Meng1
Sebastian Varges2 Matthew Purver2 Annie Lien1 Tobias Scheideck1 Badri Raghunathan1
Feng Lin1 Rohit Mishra4 Brian Lathrop4 Zhaoxia Zhang4 Harry Bratt3 Stanley Peters2
Research and Technology Center, Robert Bosch LLC, Palo Alto, California1
Center for the Study of Language and Information, Stanford University, Stanford, California2
Speech Technology and Research Lab, SRI International, Menlo Park, California3
Electronics Research Lab, Volkswagen of America, Palo Alto, California4
Abstract
In this demonstration we present a con-
versational dialog system for automobile
drivers. The system provides a voice-
based interface to playing music, finding
restaurants, and navigating while driving.
The design of the system as well as the
new technologies developed will be pre-
sented. Our evaluation showed that the
system is promising, achieving high task
completion rate and good user satisfation.
1 Introduction
As a constant stream of electronic gadgets such as
navigation systems and digital music players en-
ters cars, it threatens driving safety by increasing
driver distraction. According to a 2005 report by
the National Highway Traffic Safety Administration
(NHTSA) (NHTSA, 2005), driver distraction and
inattention from all sources contributed to 20-25%
of police reported crashes. It is therefore impor-
tant to design user interfaces to devices that mini-
mize driver distraction, to which voice-based inter-
faces have been a promising approach as they keep
a driver?s hands on the wheel and eyes on the road.
In this demonstration we present a conversational
dialog system, CHAT, that supports music selection,
restaurant selection, and driving navigation (Weng
et al, 2006). The system is a joint research effort
from Bosch RTC, VWERL, Stanford CSLI, and SRI
STAR Lab funded by NIST ATP. It has reached a
promising level, achieving a task completion rate of
98%, 94%, 97% on playing music, finding restau-
rants, and driving navigation respectively.
Specifically, we plan to present a number of fea-
tures in the CHAT system, including end-pointing
with prosodic cues, robust natural language under-
standing, error identification and recovery strate-
gies, content optimization, full-fledged reponse gen-
eration, flexible multi-threaded, multi-device dialog
management, and support for random events, dy-
namic information, and domain switching.
2 System Descriptions
The spoken dialog system consists of a number of
components (see the figure on the next page). In-
stead of the hub architecture employed by Commu-
nicator projects (Seneff et al, 1998), it is devel-
oped in Java and uses flexible event-based, message-
oriented middleware. This allows for dynamic regis-
tration of new components. Among the component
modules in the figure, we use the Nuance speech
recognition engine with class-based n-grams and
dynamic grammars, and the Nuance Vocalizer as the
TTS engine. The Speech Enhancer removes noises
and echo. The Prosody module will provide addi-
tional features to the Natural Language Understand-
ing (NLU) and Dialog Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation. Par-
allel to the deep analysis, a topic classifier assigns
n-best topics to the utterance, which are used in the
cases where the dialog manager cannot make any
sense of the parsed structure. The NLU module also
supports dynamic updates of the knowledge base.
The DM module mediates and manages interac-
23
tion. It uses an information-state-update approach to
maintain dialog context, which is then used to inter-
pret incoming utterances (including fragments and
revisions), resolve NPs, construct salient responses,
track issues, etc. Dialog states can also be used to
bias SR expectation and improve SR performance,
as has been performed in previous applications of
the DM. Detailed descriptions of the DM can be
found in (Lemon et al, 2002) (Mirkovic and Cave-
don, 2005).
The Knowledge Manager (KM) controls access
to knowledge base sources (such as domain knowl-
edge and device information) and their updates. Do-
main knowledge is structured according to domain-
dependent ontologies. The current KMmakes use of
OWL, a W3C standard, to represent the ontological
relationships between domain entities.
The Content Optimization module acts as an in-
termediary between the dialog management module
and the knowledge management module and con-
trols the amount of content and provides recommen-
dations to user. It receives queries in the form of se-
mantic frames from the DM, resolves possible ambi-
guities, and queries the KM. Depending on the items
in the query result as well as configurable properties,
the module selects and performs an appropriate op-
timization strategy (Pon-Barry et al, 2006).
The Response Generation module takes query re-
sults from the KM or Content Optimizer and gener-
ates natural language sentences as system responses
to user utterances. The query results are converted
into natural language sentences via a bottom-up ap-
proach using a production system. An alignment-
based ranking algorithm is used to select the best
generated sentence.
The system supports random events and dy-
namic external information, for example, the system
prompts users for the next turn when they drive close
to an intersection and dialogs can be carried out in
terms of the current dynamic situation. The user can
also switch among the three different applications
easily by explicitly instructing the system which do-
main to operate in.
3 Acknowledgement
This work is partially supported by the NIST Ad-
vanced Technology Program.
References
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in
dialogue systems. In Traitement Automatique des
Langues (TAL), page 43(2).
Danilo Mirkovic and Lawrence Cavedon. 2005. Prac-
tical Plug-and-Play Dialogue Management. In Pro-
ceedings of the 6th Meeting of the Pacific Associa-
tion for Computational Linguistics (PACLING), page
43(2), Tokyo, Japan.
National Highway Traffic Safety Administration
NHTSA. 2005. NHTSA Vehicle Safety Rulemaking
and Supporting Research Priorities: Calendar Years
2005-2009. January.
Heather Pon-Barry, Fuliang Weng, and Sebastian Varges.
2006. Evaluation of content presentation strategies
for an in-car spoken dialogue system. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing (Interspeech/ICSLP), pages 1930?
1933, Pittsburgh, PA, September.
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
GALAXY-II: A Reference Architecture for Conversa-
tional System Development. In International Confer-
ence on Spoken Language Processing (ICSLP), page
43(2), Sydney, Australia, December.
Fuliang Weng, Sebastian Varges, Badri Raghunathan,
Florin Ratiu, Heather Pon-Barry, Brian Lathrop,
Qi Zhang, Tobias Scheideck, Harry Bratt, Kui Xu,
Matthew Purver, Rohit Mishra, Annie Lien, Mad-
huri Raya, Stanley Peters, Yao Meng, Jeff Russel,
Lawrence Cavedon, Liz Shriberg, and Hauke Schmidt.
2006. CHAT: A conversational helper for automo-
tive tasks. In Proceedings of the 9th International
Conference on Spoken Language Processing (Inter-
speech/ICSLP), pages 1061?1064, Pittsburgh, PA,
September.
24
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 561?568,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Progressive Feature Selection Algorithm for Ultra  
Large Feature Spaces 
 
 
Qi Zhang 
Computer Science Department 
Fudan University 
Shanghai 200433, P.R. China 
qi_zhang@fudan.edu.cn  
Fuliang Weng 
Research and Technology Center 
Robert Bosch Corp. 
Palo Alto, CA 94304, USA 
fuliang.weng@rtc.bosch.com 
 
Zhe Feng 
Research and Technology Center 
Robert Bosch Corp. 
Palo Alto, CA 94304, USA 
zhe.feng@rtc.bosch.com  
 
Abstract 
Recent developments in statistical modeling 
of various linguistic phenomena have shown 
that additional features give consistent per-
formance improvements. Quite often, im-
provements are limited by the number of fea-
tures a system is able to explore. This paper 
describes a novel progressive training algo-
rithm that selects features from virtually 
unlimited feature spaces for conditional 
maximum entropy (CME) modeling. Experi-
mental results in edit region identification 
demonstrate the benefits of the progressive 
feature selection (PFS) algorithm: the PFS 
algorithm maintains the same accuracy per-
formance as previous CME feature selection 
algorithms (e.g., Zhou et al, 2003) when the 
same feature spaces are used. When addi-
tional features and their combinations are 
used, the PFS gives 17.66% relative im-
provement over the previously reported best 
result in edit region identification on 
Switchboard corpus (Kahn et al, 2005), 
which leads to a 20% relative error reduction 
in parsing the Switchboard corpus when gold 
edits are used as the upper bound. 
1 Introduction 
Conditional Maximum Entropy (CME) modeling 
has received a great amount of attention within 
natural language processing community for the 
past decade (e.g., Berger et al, 1996; Reynar and 
Ratnaparkhi, 1997; Koeling, 2000; Malouf, 2002; 
Zhou et al, 2003; Riezler and Vasserman, 2004). 
One of the main advantages of CME modeling is 
the ability to incorporate a variety of features in a 
uniform framework with a sound mathematical 
foundation. Recent improvements on the original 
incremental feature selection (IFS) algorithm, 
such as Malouf (2002) and Zhou et al (2003), 
greatly speed up the feature selection process. 
However, like many other statistical modeling 
algorithms, such as boosting (Schapire and 
Singer, 1999) and support vector machine (Vap-
nik 1995), the algorithm is limited by the size of 
the defined feature space. Past results show that 
larger feature spaces tend to give better results. 
However, finding a way to include an unlimited 
amount of features is still an open research prob-
lem. 
In this paper, we propose a novel progressive 
feature selection (PFS) algorithm that addresses 
the feature space size limitation. The algorithm is 
implemented on top of the Selective Gain Com-
putation (SGC) algorithm (Zhou et al, 2003), 
which offers fast training and high quality mod-
els. Theoretically, the new algorithm is able to 
explore an unlimited amount of features. Be-
cause of the improved capability of the CME 
algorithm, we are able to consider many new 
features and feature combinations during model 
construction. 
To demonstrate the effectiveness of our new 
algorithm, we conducted a number of experi-
ments on the task of identifying edit regions, a 
practical task in spoken language processing. 
Based on the convention from Shriberg (1994) 
and Charniak and Johnson (2001), a disfluent 
spoken utterance is divided into three parts: the 
reparandum, the part that is repaired; the inter-
561
regnum, which can be filler words or empty; and 
the repair/repeat, the part that replaces or repeats 
the reparandum. The first two parts combined are 
called an edit or edit region. An example is 
shown below: 
 
interregnum 
It is, you know, this is a tough problem.
reparandum repair 
 
In section 2, we briefly review the CME mod-
eling and SGC algorithm. Then, section 3 gives a 
detailed description of the PFS algorithm. In sec-
tion 4, we describe the Switchboard corpus, fea-
tures used in the experiments, and the effective-
ness of the PFS with different feature spaces. 
Section 5 concludes the paper. 
2 Background 
Before presenting the PFS algorithm, we first 
give a brief review of the conditional maximum 
entropy modeling, its training process, and the 
SGC algorithm. This is to provide the back-
ground and motivation for our PFS algorithm. 
2.1 Conditional Maximum Entropy Model 
The goal of CME is to find the most uniform 
conditional distribution of y given observation 
x, ( )xyp , subject to constraints specified by a set 
of features ( )yxf i , , where features typically take 
the value of either 0 or 1 (Berger et al, 1996). 
More precisely, we want to maximize 
 ( ) ( ) ( ) ( )( )xypxypxppH
yx
log~
,
??=           (1) 
given the constraints:  
                  ( ) ( )ii fEfE ~=                         (2) 
where  
( ) ( ) ( )?=
yx
ii yxfyxpfE
,
,,~~  
is the empirical expected feature count from the 
training data and 
   ( ) ( ) ( ) ( )?=
yx
ii yxfxypxpfE
,
,~  
is the feature expectation from  the conditional 
model ( )xyp . 
This results in the following exponential 
model: 
              ( ) ( ) ( )???
?
???
?= ?
j
jj yxfxZ
xyp ,exp1 ?          (3) 
where ?j  is the weight corresponding to the fea-
ture fj, and Z(x) is a normalization factor. 
A variety of different phenomena, including 
lexical, structural, and semantic aspects, in natu-
ral language processing tasks can be expressed in 
terms of features. For example, a feature can be 
whether the word in the current position is a verb, 
or the word is a particular lexical item. A feature 
can also be about a particular syntactic subtree, 
or a dependency relation (e.g., Charniak and 
Johnson, 2005). 
2.2 Selective Gain Computation Algorithm 
In real world applications, the number of possi-
ble features can be in the millions or beyond. 
Including all the features in a model may lead to 
data over-fitting, as well as poor efficiency and 
memory overflow. Good feature selection algo-
rithms are required to produce efficient and high 
quality models. This leads to a good amount of 
work in this area (Ratnaparkhi et al, 1994; Ber-
ger et al, 1996; Pietra et al 1997; Zhou et al, 
2003; Riezler and Vasserman, 2004) 
In the most basic approach, such as Ratna-
parkhi et al (1994) and Berger et al (1996), 
training starts with a uniform distribution over all 
values of y and an empty feature set. For each 
candidate feature in a predefined feature space, it 
computes the likelihood gain achieved by includ-
ing the feature in the model. The feature that 
maximizes the gain is selected and added to the 
current model. This process is repeated until the 
gain from the best candidate feature only gives 
marginal improvement. The process is very slow, 
because it has to re-compute the gain for every 
feature at each selection stage, and the computa-
tion of a parameter using Newton?s method be-
comes expensive, considering that it has to be 
repeated many times.  
The idea behind the SGC algorithm (Zhou et 
al., 2003) is to use the gains computed in the 
previous step as approximate upper bounds for 
the subsequent steps. The gain for a feature 
needs to be re-computed only when the feature 
reaches the top of a priority queue ordered by 
gain. In other words, this happens when the fea-
ture is the top candidate for inclusion in the 
model. If the re-computed gain is smaller than 
that of the next candidate in the list, the feature is 
re-ranked according to its newly computed gain, 
and the feature now at the top of the list goes 
through the same gain re-computing process.  
This heuristics comes from evidences that the 
gains become smaller and smaller as more and 
more good features are added to the model. This 
can be explained as follows: assume that the 
Maximum Likelihood (ML) estimation lead to 
the best model that reaches a ML value. The ML 
value is the upper bound. Since the gains need to 
be positive to proceed the process, the difference 
562
between the Likelihood of the current and the 
ML value becomes smaller and smaller. In other 
words, the possible gain each feature may add to 
the model gets smaller. Experiments in Zhou et 
al. (2003) also confirm the prediction that the 
gains become smaller when more and more fea-
tures are added to the model, and the gains do 
not get unexpectively bigger or smaller as the 
model grows. Furthermore, the experiments in 
Zhou et al (2003) show no significant advantage 
for looking ahead beyond the first element in the 
feature list. The SGC algorithm runs hundreds to 
thousands of times faster than the original IFS 
algorithm without degrading classification per-
formance. We used this algorithm for it enables 
us to find high quality CME models quickly. 
The original SGC algorithm uses a technique 
proposed by Darroch and Ratcliff (1972) and 
elaborated by Goodman (2002): when consider-
ing a feature fi, the algorithm only modifies those 
un-normalized conditional probabilities: ( )( )? j jj yxf ,exp ?   
for (x, y) that satisfy fi (x, y)=1, and subsequently 
adjusts the corresponding normalizing factors 
Z(x) in (3). An implementation often uses a map-
ping table, which maps features to the training 
instance pairs (x, y).  
3 Progressive Feature Selection Algo-
rithm 
In general, the more contextual information is 
used, the better a system performs. However, 
richer context can lead to combinatorial explo-
sion of the feature space. When the feature space 
is huge (e.g., in the order of tens of millions of 
features or even more), the SGC algorithm ex-
ceeds the memory limitation on commonly avail-
able computing platforms with gigabytes of 
memory.  
To address the limitation of the SGC algo-
rithm, we propose a progressive feature selection 
algorithm that selects features in multiple rounds. 
The main idea of the PFS algorithm is to split the 
feature space into tractable disjoint sub-spaces 
such that the SGC algorithm can be performed 
on each one of them. In the merge step, the fea-
tures that SGC selects from different sub-spaces 
are merged into groups. Instead of re-generating 
the feature-to-instance mapping table for each 
sub-space during the time of splitting and merg-
ing, we create the new mapping table from the 
previous round?s tables by collecting those en-
tries that correspond to the selected features. 
Then, the SGC algorithm is performed on each 
of the feature groups and new features are se-
lected from each of them. In other words, the 
feature space splitting and subspace merging are 
performed mainly on the feature-to-instance 
mapping tables. This is a key step that leads to 
this very efficient PFS algorithm.  
At the beginning of each round for feature se-
lection, a uniform prior distribution is always 
assumed for the new CME model. A more pre-
cise description of the PFS algorithm is given in 
Table 1, and it is also graphically illustrated in 
Figure 1. 
Given:  
    Feature space F(0) = {f1(0), f2(0), ?, fN(0)},
step_num = m,  select_factor = s 
1. Split the feature space into N1 parts 
    {F1(1), F2(1), ?, FN1(1)} = split(F(0)) 
2. for k=1 to m-1 do 
      //2.1 Feature selection 
      for each feature space Fi(k) do 
           FSi(k) = SGC(Fi(k), s) 
      //2.2 Combine selected features 
      {F1(k+1), ?, FNk+1(k+1)}  =  
                      merge(FS1(k), ?, FSNk(k)) 
3. Final feature selection & optimization
F(m) = merge(FS1(m-1), ?, FSNm-1(m-1)) 
FS(m) = SGC(F(m), s) 
Mfinal = Opt(FS(m)) 
 
Table 1. The PFS algorithm. 
 
 
M
)2(
1F
)1(
1FS
)1(
1i
FS
M
M
)1(
2i
FS
M
)1(
1N
FS
L
select 
Step 1 Step m 
)1(
1F
)1(
1i
F
M
M
)1(
2i
F
M
)1(
1N
F
)2(
1FS
)2(
2N
FS
)(mFM
merge 
Step 2 
)0(F
Split 
select merge 
select 
)2(
2N
F
Mfinal 
)(mFS
optimize
Figure 1. Graphic illustration of PFS algorithm. 
 
In Table 1, SGC() invokes the SGC algorithm, 
and Opt() optimizes feature weights. The func-
tions split() and merge() are used to split and 
merge the feature space respectively.  
Two variations of the split() function are in-
vestigated in the paper and they are described 
below: 
1. random-split: randomly split a feature 
space into n- disjoint subspaces, and select 
an equal amount of features for each fea-
ture subspace.  
2. dimension-based-split: split a feature 
space into disjoint subspaces based on fea-
563
ture dimensions/variables, and select the 
number of features for each feature sub-
space with a certain distribution.  
We use a simple method for merge() in the 
experiments reported here, i.e., adding together 
the features from a set of selected feature sub-
spaces. 
One may image other variations of the split() 
function, such as allowing overlapping sub-
spaces. Other alternatives for merge() are also 
possible, such as randomly grouping the selected 
feature subspaces in the dimension-based split. 
Due to the limitation of the space, they are not 
discussed here. 
This approach can in principle be applied to 
other machine learning algorithms as well.  
4 Experiments with PFS for Edit Re-
gion Identification 
In this section, we will demonstrate the benefits 
of the PFS algorithm for identifying edit regions. 
The main reason that we use this task is that the 
edit region detection task uses features from sev-
eral levels, including prosodic, lexical, and syn-
tactic ones. It presents a big challenge to find a 
set of good features from a huge feature space.  
First we will present the additional features 
that the PFS algorithm allows us to include. 
Then, we will briefly introduce the variant of the 
Switchboard corpus used in the experiments. Fi-
nally, we will compare results from two variants 
of the PFS algorithm. 
4.1 Edit Region Identification Task 
In spoken utterances, disfluencies, such as self-
editing, pauses and repairs, are common phe-
nomena. Charniak and Johnson (2001) and Kahn 
et al (2005) have shown that improved edit re-
gion identification leads to better parsing accu-
racy ? they observe a relative reduction in pars-
ing f-score error of 14% (2% absolute) between 
automatic and oracle edit removal.  
The focus of our work is to show that our new 
PFS algorithm enables the exploration of much 
larger feature spaces for edit identification ? in-
cluding prosodic features, their confidence 
scores, and various feature combinations ? and 
consequently, it further improves edit region 
identification. Memory limitation prevents us 
from including all of these features in experi-
ments using the boosting method described in 
Johnson and Charniak (2004) and Zhang and 
Weng (2005). We couldn?t use the new features 
with the SGC algorithm either for the same rea-
son. 
The features used here are grouped according 
to variables, which define feature sub-spaces as 
in Charniak and Johnson (2001) and Zhang and 
Weng (2005). In this work, we use a total of 62 
variables, which include 16 1  variables from 
Charniak and Johnson (2001) and Johnson and 
Charniak (2004), an additional 29 variables from 
Zhang and Weng (2005), 11 hierarchical POS tag 
variables, and 8 prosody variables (labels and 
their confidence scores). Furthermore, we ex-
plore 377 combinations of these 62 variables, 
which include 40 combinations from Zhang and 
Weng (2005). The complete list of the variables 
is given in Table 2, and the combinations used in 
the experiments are given in Table 3. One addi-
tional note is that some features are obtained af-
ter the rough copy procedure is performed, where 
we used the same procedure as the one by Zhang 
and Weng (2005). For a fair comparison with the 
work by Kahn et al (2005), word fragment in-
formation is retained. 
4.2 The Re-segmented Switchboard Data 
In order to include prosodic features and be able 
to compare with the state-oft-art, we use the 
University of Washington re-segmented 
Switchboard corpus, described in Kahn et al 
(2005). In this corpus, the Switchboard sentences 
were segmented into V5-style sentence-like units 
(SUs) (LDC, 2004). The resulting sentences fit 
more closely with the boundaries that can be de-
tected through automatic procedures (e.g., Liu et 
al., 2005). Because the edit region identification 
results on the original Switchboard are not di-
rectly comparable with the results on the newly 
segmented data, the state-of-art results reported 
by Charniak and Johnson (2001) and Johnson 
and Charniak (2004) are repeated on this new 
corpus by Kahn et al (2005).  
The re-segmented UW Switchboard corpus is 
labeled with a simplified subset of the ToBI pro-
sodic system (Ostendorf et al, 2001).  The three 
simplified labels in the subset are p, 1 and 4, 
where p refers to a general class of disfluent 
boundaries (e.g., word fragments, abruptly short-
ened words, and hesitation); 4 refers to break 
level 4, which describes a boundary that has a 
boundary tone and phrase-final lengthening;
                                                 
1 Among the original 18 variables, two variables, Pf and Tf 
are not used in our experiments, because they are mostly 
covered by the other variables. Partial word flags only con-
tribute to 3 features in the final selected feature list. 
564
Categories Variable Name Short Description 
Orthographic 
Words W-5, ? , W+5 
Words at the current position and the left and right 5 
positions. 
Partial Word Flags P-3, ?, P+3 
Partial word flags at the current position and the left 
and right 3 positions 
Words 
Distance DINTJ, DW, DBigram, DTrigram Distance features 
POS Tags T-5, ?, T+5 
POS tags at the current position and the left and 
right 5 positions. Tags 
Hierarchical  
POS Tags (HTag) HT-5, ?, HT+5 
Hierarchical POS tags at the current position and the 
left and right 5 positions. 
HTag Rough Copy Nm, Nn, Ni, Nl, Nr, Ti Hierarchical POS rough copy features. 
Rough Copy 
Word Rough Copy WNm, WNi, WNl, WNr Word rough copy features. 
Prosody Labels PL0, ?, PL3 
Prosody label with largest post possibility at the 
current position and the right 3 positions. Prosody 
Prosody Scores PC0, ?, PC3 
Prosody confidence at the current position and the 
right 3 positions. 
Table 2. A complete list of variables used in the experiments. 
 
Categories Short Description Number of  Combinations 
Tags HTagComb Combinations among Hierarchical POS Tags  55 
Words OrthWordComb Combinations among Orthographic Words 55 
Tags 
WTComb 
WTTComb Combinations of Orthographic Words and POS Tags; Combination among POS Tags 176 
Rough Copy RCComb Combinations of HTag Rough Copy and Word Rough Copy 55 
Prosody PComb Combinations among Prosody, and with Words 36 
Table 3. All the variable combinations used in the experiments. 
 
and 1 is used to include the break index levels 
BL 0, 1, 2, and 3. Since the majority of the cor-
pus is labeled via automatic methods, the f-
scores for the prosodic labels are not high. In 
particular, 4 and p have f-scores of about 70% 
and 60% respectively (Wong et al, 2005). There-
fore, in our experiments, we also take prosody 
confidence scores into consideration. 
Besides the symbolic prosody labels, the cor-
pus preserves the majority of the previously an-
notated syntactic information as well as edit re-
gion labels.  
In following experiments, to make the results 
comparable, the same data subsets described in 
Kahn et al (2005) are used for training, develop-
ing and testing. 
4.3 Experiments 
The best result on the UW Switchboard for edit 
region identification uses a TAG-based approach 
(Kahn et al, 2005). On the original Switchboard 
corpus, Zhang and Weng (2005) reported nearly 
20% better results using the boosting method 
with a much larger feature space 2 . To allow 
comparison with the best past results, we create a 
new CME baseline with the same set of features 
as that used in Zhang and Weng (2005).  
We design a number of experiments to test the 
following hypotheses: 
1. PFS can include a huge number of new 
features, which leads to an overall per-
formance improvement. 
2. Richer context, represented by the combi-
nations of different variables, has a posi-
tive impact on performance. 
3. When the same feature space is used, PFS 
performs equally well as the original SGC 
algorithm. 
The new models from the PFS algorithm are 
trained on the training data and tuned on the de-
velopment data. The results of our experiments 
on the test data are summarized in Table 4. The 
first three lines show that the TAG-based ap-
proach is outperformed by the new CME base-
line (line 3) using all the features in Zhang and 
Weng (2005). However, the improvement from 
                                                 
2 PFS is not applied to the boosting algorithm at this time 
because it would require significant changes to the available 
algorithm.  
565
Results on test data Feature Space Codes number of features Precision Recall F-Value 
TAG-based result on UW-SWBD reported in Kahn et al (2005)    78.20 
CME with all the variables from Zhang and Weng (2005) 2412382 89.42 71.22 79.29 
CME with all the variables from Zhang and Weng (2005) + post 2412382 87.15 73.78 79.91 
+HTag +HTagComb +WTComb +RCComb 17116957 90.44 72.53 80.50 
+HTag +HTagComb +WTComb +RCComb +PL0 ? PL3 17116981 88.69 74.01 80.69 
+HTag +HTagComb +WTComb +RCComb +PComb: without cut 20445375 89.43 73.78 80.86 
+HTag +HTagComb +WTComb +RCComb +PComb: cut2 19294583 88.95 74.66 81.18 
+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau 19294583 90.37 74.40 81.61 
+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +post 19294583 86.88 77.29 81.80 
+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau 
+post  19294583 87.79 77.02 82.05 
Table 4. Summary of experimental results with PFS. 
 
CME is significantly smaller than the reported 
results using the boosting method. In other 
words, using CME instead of boosting incurs a 
performance hit. 
The next four lines in Table 4 show that addi-
tional combinations of the feature variables used 
in Zhang and Weng (2005) give an absolute im-
provement of more than 1%. This improvement 
is realized through increasing the search space to 
more than 20 million features, 8 times the maxi-
mum size that the original boosting and CME 
algorithms are able to handle.  
Table 4 shows that prosody labels alone make 
no difference in performance. Instead, for each 
position in the sentence, we compute the entropy 
of the distribution of the labels? confidence 
scores. We normalize the entropy to the range [0, 
1], according to the formula below: 
            ( ) ( )UniformHpHscore ?= 1        (4) 
Including this feature does result in a good 
improvement. In the table, cut2 means that we 
equally divide the feature scores into 10 buckets 
and any number below 0.2 is ignored. The total 
contribution from the combined feature variables 
leads to a 1.9% absolute improvement. This con-
firms the first two hypotheses. 
When Gaussian smoothing (Chen and 
Rosenfeld, 1999), labeled as +Gau, and post-
processing (Zhang and Weng, 2005), labeled as 
+post, are added, we observe 17.66% relative 
improvement (or 3.85% absolute) over the previ-
ous best f-score of 78.2 from Kahn et al (2005). 
To test hypothesis 3, we are constrained to the 
feature spaces that both PFS and SGC algorithms 
can process. Therefore, we take all the variables 
from Zhang and Weng (2005) as the feature 
space for the experiments. The results are listed 
in Table 5. We observed no f-score degradation 
with PFS. Surprisingly, the total amount of time 
PFS spends on selecting its best features is 
smaller than the time SGC uses in selecting its 
best features. This confirms our hypothesis 3. 
 
Results on test data Split / Non-split Precision Recall F-Value 
non-split 89.42 71.22 79.29 
split by 4 parts 89.67 71.68 79.67 
split by 10 parts 89.65 71.29 79.42 
Table 5. Comparison between PFS and SGC with 
all the variables from Zhang and Weng (2005). 
 
The last set of experiments for edit identifica-
tion is designed to find out what split strategies 
PFS algorithm should adopt in order to obtain 
good results. Two different split strategies are 
tested here. In all the experiments reported so far, 
we use 10 random splits, i.e., all the features are 
randomly assigned to 10 subsets of equal size. 
We may also envision a split strategy that divides 
the features based on feature variables (or dimen-
sions), such as word-based, tag-based, etc. The 
four dimensions used in the experiments are 
listed as the top categories in Tables 2 and 3, and 
the results are given in Table 6.  
 
Results on test data Split  
Criteria 
Allocation 
Criteria Precision Recall F-Value 
Random Uniform 88.95 74.66 81.18 
Dimension Uniform 89.78 73.42 80.78 
Dimension Prior 89.78 74.01 81.14 
Table 6. Comparison of split strategies using feature space 
+HTag+HTagComb+WTComb+RCComb+PComb: cut2 
 
In Table 6, the first two columns show criteria 
for splitting feature spaces and the number of 
features to be allocated for each group. Random 
and Dimension mean random-split and dimen-
sion-based-split, respectively. When the criterion 
566
is Random, the features are allocated to different 
groups randomly, and each group gets the same 
number of features. In the case of dimension-
based split, we determine the number of features 
allocated for each dimension in two ways. When 
the split is Uniform, the same number of features 
is allocated for each dimension. When the split is 
Prior, the number of features to be allocated in 
each dimension is determined in proportion to 
the importance of each dimension. To determine 
the importance, we use the distribution of the 
selected features from each dimension in the 
model ?+ HTag + HTagComb + WTComb + 
RCComb + PComb: cut2?, namely: Word-based 
15%, Tag-based 70%, RoughCopy-based 7.5% 
and Prosody-based 7.5%3. From the results, we 
can see no significant difference between the 
random-split and the dimension-based-split. 
To see whether the improvements are trans-
lated into parsing results, we have conducted one 
more set of experiments on the UW Switchboard 
corpus. We apply the latest version of Charniak?s 
parser (2005-08-16) and the same procedure as 
Charniak and Johnson (2001) and Kahn et al 
(2005) to the output from our best edit detector 
in this paper. To make it more comparable with 
the results in Kahn et al (2005), we repeat the 
same experiment with the gold edits, using the 
latest parser. Both results are listed in Table 7. 
The difference between our best detector and the 
gold edits in parsing (1.51%) is smaller than the 
difference between the TAG-based detector and 
the gold edits (1.9%). In other words, if we use 
the gold edits as the upper bound, we see a rela-
tive error reduction of 20.5%. 
 
Parsing F-score 
Methods Edit  F-score 
Reported 
in Kahn et 
al. (2005) 
Latest 
Charniak 
Parser 
Diff. 
with 
Oracle 
Oracle 100 86.9 87.92 -- 
Kahn et 
al. (2005) 78.2 85.0 -- 1.90 
PFS best 
results 82.05 -- 86.41 1.51 
Table 7. Parsing F-score various different edit 
region identification results. 
                                                 
3 It is a bit of cheating to use the distribution from the se-
lected model. However, even with this distribution, we do 
not see any improvement over the version with random-
split. 
5 Conclusion 
This paper presents our progressive feature selec-
tion algorithm that greatly extends the feature 
space for conditional maximum entropy model-
ing. The new algorithm is able to select features 
from feature space in the order of tens of mil-
lions in practice, i.e., 8 times the maximal size 
previous algorithms are able to process, and 
unlimited space size in theory. Experiments on 
edit region identification task have shown that 
the increased feature space leads to 17.66% rela-
tive improvement (or 3.85% absolute) over the 
best result reported by Kahn et al (2005), and 
10.65% relative improvement (or 2.14% abso-
lute) over the new baseline SGC algorithm with 
all the variables from Zhang and Weng (2005). 
We also show that symbolic prosody labels to-
gether with confidence scores are useful in edit 
region identification task. 
In addition, the improvements in the edit iden-
tification lead to a relative 20% error reduction in 
parsing disfluent sentences when gold edits are 
used as the upper bound.  
Acknowledgement 
This work is partly sponsored by a NIST ATP 
funding. The authors would like to express their 
many thanks to Mari Ostendorf and Jeremy Kahn 
for providing us with the re-segmented UW 
Switchboard Treebank and the corresponding 
prosodic labels. Our thanks also go to Jeff Rus-
sell for his careful proof reading, and the anony-
mous reviewers for their useful comments. All 
the remaining errors are ours.  
References 
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy 
Approach to Natural Language Processing. Com-
putational Linguistics, 22 (1): 39-71.  
Eugene Charniak and Mark Johnson. 2001. Edit De-
tection and Parsing for Transcribed Speech. In 
Proceedings of the 2nd Meeting of the North Ameri-
can Chapter of the Association for Computational 
Linguistics, 118-126, Pittsburgh, PA, USA. 
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best Parsing and MaxEnt Discriminative 
Reranking. In Proceedings of the 43rd Annual 
Meeting of Association for Computational Linguis-
tics, 173-180, Ann Arbor, MI, USA. 
Stanley Chen and Ronald Rosenfeld. 1999. A Gaus-
sian Prior for Smoothing Maximum Entropy Mod-
567
els. Technical Report CMUCS-99-108, Carnegie 
Mellon University. 
John N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models. In Annals 
of Mathematical Statistics, 43(5): 1470-1480. 
Stephen A. Della Pietra, Vincent J. Della Pietra, and 
John Lafferty. 1997. Inducing Features of Random 
Fields. In IEEE Transactions on Pattern Analysis 
and Machine Intelligence, 19(4): 380-393. 
Joshua Goodman. 2002. Sequential Conditional Gen-
eralized Iterative Scaling. In Proceedings of the 
40th Annual Meeting of Association for Computa-
tional Linguistics, 9-16, Philadelphia, PA, USA.  
Mark Johnson, and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In 
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, 33-39, 
Barcelona, Spain. 
Jeremy G. Kahn, Matthew Lease, Eugene Charniak, 
Mark Johnson, and Mari Ostendorf. 2005.  Effec-
tive Use of Prosody in Parsing Conversational 
Speech. In Proceedings of the 2005 Conference on 
Empirical Methods in Natural Language Process-
ing, 233-240, Vancouver, Canada. 
Rob Koeling. 2000. Chunking with Maximum En-
tropy Models. In Proceedings of the CoNLL-2000 
and LLL-2000, 139-141, Lisbon, Portugal. 
LDC. 2004. Simple MetaData Annotation Specifica-
tion. Technical Report of Linguistic Data Consor-
tium. (http://www.ldc.upenn.edu/Projects/MDE). 
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-
bara Peskin, Jeremy Ang, Dustin Hillard, Mari Os-
tendorf, Marcus Tomalin, Phil Woodland and Mary 
Harper. 2005. Structural Metadata Research in the 
EARS Program. In Proceedings of the 30th 
ICASSP, volume V, 957-960, Philadelphia, PA, 
USA. 
Robert Malouf. 2002. A Comparison of Algorithms 
for Maximum Entropy Parameter Estimation. In 
Proceedings of the 6th  Conference on Natural Lan-
guage Learning (CoNLL-2002), 49-55, Taibei, 
Taiwan. 
Mari Ostendorf, Izhak Shafran, Stefanie Shattuck-
Hufnagel, Leslie Charmichael, and William Byrne. 
2001. A Prosodically Labeled Database of Sponta-
neous Speech. In Proceedings of the ISCA Work-
shop of Prosody in Speech Recognition and Under-
standing, 119-121, Red Bank, NJ, USA. 
Adwait Ratnaparkhi, Jeff Reynar and Salim Roukos. 
1994. A Maximum Entropy Model for Preposi-
tional Phrase Attachment. In Proceedings of the 
ARPA Workshop on Human Language Technology, 
250-255, Plainsboro, NJ, USA. 
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A 
Maximum Entropy Approach to Identifying Sen-
tence Boundaries. In Proceedings of the 5th Con-
ference on Applied Natural Language Processing, 
16-19, Washington D.C., USA. 
Stefan Riezler and Alexander Vasserman. 2004. In-
cremental Feature Selection and L1 Regularization 
for Relaxed Maximum-entropy Modeling. In Pro-
ceedings of the 2004 Conference on Empirical 
Methods in Natural Language Processing, 174-
181, Barcelona, Spain. 
Robert E. Schapire and Yoram Singer, 1999. Im-
proved Boosting Algorithms Using Confidence-
rated Predictions. Machine Learning, 37(3): 297-
336. 
Elizabeth Shriberg. 1994. Preliminaries to a Theory 
of Speech Disfluencies. Ph.D. Thesis, University of 
California, Berkeley.  
Vladimir Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, New York, NY, USA. 
Darby Wong, Mari Ostendorf, Jeremy G. Kahn. 2005. 
Using Weakly Supervised Learning to Improve 
Prosody Labeling. Technical Report UWEETR-
2005-0003, University of Washington.  
Qi Zhang and Fuliang Weng. 2005. Exploring Fea-
tures for Identifying Edited Regions in Disfluent 
Sentences. In Proc. of the 9th International Work-
shop on Parsing Technologies, 179-185, Vancou-
ver, Canada. 
Yaqian Zhou, Fuliang Weng, Lide Wu, and Hauke 
Schmidt. 2003. A Fast Algorithm for Feature Se-
lection in Conditional Maximum Entropy Model-
ing. In Proceedings of the 2003 Conference on 
Empirical Methods in Natural Language Process-
ing, 153-159, Sapporo, Japan. 
568
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 217?220,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Computing Confidence Scores for All Sub Parse Trees 
 
Feng Lin Fuliang Weng 
Department of Computer Science and Engineering Research and Technology Center 
Fudan University Robert Bosch LLC 
Shanghai 200433, P.R. China Palo Alto, CA, 94303, USA 
fenglin@fudan.edu.cn fuliang.weng@us.bosch.com 
 
 
 
 
Abstract 
Computing confidence scores for applica-
tions, such as dialogue system, informa-
tion retrieving and extraction, is an active 
research area. However, its focus has been 
primarily on computing word-, concept-, 
or utterance-level confidences. Motivated 
by the need from sophisticated dialogue 
systems for more effective dialogs, we 
generalize the confidence annotation to all 
the subtrees, the first effort in this line of 
research. The other contribution of this 
work is that we incorporated novel long 
distance features to address challenges in 
computing multi-level confidence scores. 
Using Conditional Maximum Entropy 
(CME) classifier with all the selected fea-
tures, we reached an annotation error rate 
of 26.0% in the SWBD corpus, compared 
with a subtree error rate of 41.91%, a 
closely related benchmark with the 
Charniak parser from (Kahn et al, 2005). 
1 Introduction 
There has been a good amount of interest in ob-
taining confidence scores for improving word or 
utterance accuracy, dialogue systems, information 
retrieving & extraction, and machine translation 
(Zhang and Rudnicky, 2001; Guillevic et al, 2002; 
Gabsdil et al, 2003; Ueffing et al, 2007). 
However, these confidence scores are limited to 
relatively simple systems, such as command-n-
control dialogue systems. For more sophisticated 
dialogue systems (e.g., Weng et al, 2007), identi-
fication of reliable phrases must be performed at 
different granularity to ensure effective and 
friendly dialogues. For example, in a request of 
MP3 music domain ?Play a rock song by Cher?, if 
we want to communicate to the user that the sys-
tem is not confident of the phrase ?a rock song,? 
the confidence scores for each word, the artist 
name ?Cher,? and the whole sentence would not be 
enough. For tasks of information extraction, when 
extracted content has internal structures, confi-
dence scores for such phrases are very useful for 
reliable returns. 
As a first attempt in this research, we generalize 
confidence annotation algorithms to all sub parse 
trees and tested on a human-human conversational 
corpus, the SWBD. Technically, we also introduce 
a set of long distance features to address the chal-
lenges in computing multi-level confidence scores.  
This paper is organized as follows:  Section 2 in-
troduces the tasks and the representation for parse 
trees; Section 3 presents the features used in the 
algorithm; Section 4 describes the experiments in 
the SWBD corpus; Section 5 concludes the paper. 
2 Computing Confidence Scores for 
Parse Trees 
The confidence of a sub-tree is defined as the pos-
terior probability of its correctness, given all the 
available information. It is )|( xcorrectisspP  ? the 
posterior probability that the parse sub-tree sp is 
correct, given related information x. In real appli-
cations, typically a threshold or cutoff t is needed:  
 
??
?
<
?
txcorrectisspPifincorrec
txcorrectisspPifcorrect
issp
)|(,
)|(,
   (1) 
217
In this work, the probability )|( xcorrectisspP is 
calculated using CME modeling framework: 
( ) ( ) ( )???
?
???
?
= ?
j
jj yxfxZ
xyP ,exp
1
| ?   (2) 
where y?{sp is correct, sp is incorrect}, x is the 
syntactic context of the parse sub-tree sp, fj are the 
features, ?j are the corresponding weights, and Z(x) 
is the normalization factor.  
The parse trees used in our system are lexical-
ized binary trees. However, the confidence compu-
tation is independent of any parsing method used 
in generating the parse tree as long as it generates 
the binary dependency relations. An example of 
the lexicalized binary trees is given in Figure 1, 
where three important components are illustrated: 
the left sub-tree, the right sub-trees, and the 
marked head and dependency relation.  
Because the parse tree is already given, a bot-
tom-up left-right algorithm is used to traverse 
through the parse tree: for each subtree, compute 
its confidence, and annotate it as correct or wrong. 
3 Features 
Four major categories of features are used, includ-
ing, words, POS tags, scores and syntactic infor-
mation. Due to the space limitation, we only give a 
detailed description of the most important one1, 
lexical-syntactic features.  
The lexical-syntactic features include lexical, 
POS tag, and syntactic features. Word and POS tag 
features include the head and modifier words of the 
parse sub-tree and the two children of the root, as 
well as their combinations. The POS tags and hier-
archical POS tags of the corresponding words are 
                                                          
1 The other important one is the dependency score, which is 
the conditional probability of the last dependency relation in 
the subtree, given its left and right child trees 
also considered to avoid data sparseness. The 
adopted hierarchical tags are: Verb-related (V), 
Noun-related (N), Adjectives (ADJ), and Adverbs 
(ADV), similar to (Zhang et al 2006).  
Long distance structural features in statistical 
parsing lead to significant improvements (Collins 
et al, 2000; Charniak et al, 2005). We incorporate 
some of the reported features in the feature space 
to be explored, and they are enriched with different 
POS categories and grammatical types. Two eam-
ples are given below.  
 One example is the Single-Level Joint Head 
and Dependency Relation (SL-JHD). This feature 
is pairing the head word of a given sub-tree with its 
last dependency relation. To address the data 
sparseness problem, two additional SL-JHD fea-
tures are considered: a pair of the POS tag of the 
head of a given sub-tree and its dependency rela-
tion, a pair of the hierarchical POS tag of the head 
of a given sub-tree and its dependency relation. For 
example, for the top node in Figure 2, (restaurant 
NCOMP), (NN, NCOMP), and (N, NCOMP) are 
the examples for the three SL-JHD features. To 
compute the confidence score of the sub-tree, we 
include the three JHD features for the top node, 
and the JHD features for its two children. Thus, for 
the sub-tree in Figure 2, the following nine JHD 
features are included in the feature space, i.e., (res-
taurant NCOMP), (NN, NCOMP), (N, NCOMP), 
(restaurant NMOD), (NN NMOD), (N NMOD), 
(with POBJ), (IN POBJ), and (ADV POBJ).  
The other example feature is Multi-Level Joint 
Head and Dependency Relation (ML-JHD), which 
takes into consideration the dependency relations 
at multiple levels. This feature is an extension of 
SL-JHD. Instead of including only single level 
head and dependency relations, the ML-JHD fea-
ture includes the hierarchical POS tag of the head 
and dependency relations for all the levels of a 
given sub-tree. For example, given the sub-tree in 
Figure 3, (NCOMP, N, NMOD, N, NMOD, N, 
POBJ, ADV, NMOD, N) is the ML-JHD feature 
for the top node (marked by the dashed circle).  
In addition, three types of features are included:  
dependency relations, neighbors of the head of the 
current subtree, and the sizes of the sub-tree and its 
left and right children. The dependency relations 
include the top one in the subtree. The neighbors 
are typically within a preset distance from the head 
word. The sizes refer to the numbers of words or 
non-terminals in the subtree and its children. 
Figure 1. Example of parse sub-tree?s structure for 
phrase ?three star Chinese restaurant? 
star 
NN 
NP (star) 
NMOD 
NP (restaurant) 
NMOD 
Left Sub-tree 
three 
CD 
restaurant
NN 
NP (restaurant) 
NMOD 
Chinese 
NNP 
Right Sub-tree
218
Figure 3. ML-JHD Features 
a NNP 
CUISINENAME restaurant 
NN 
NP (restaurant) DT 
NP (restaurant) 
NMOD
with JJ 
good service
NN 
NP (service)IN 
PP (with) 
NMOD
NMOD POBJ
NP (restaurant) 
NCOMP 
 
4 Experiments 
Experiments were conducted to see the perform-
ance of our algorithm in human to human dialogs ? 
the ultimate goal of a dialogue system. In our work, 
we use a version of the Charniak?s parser from 
(Aug. 16, 2005) to parse the re-segmented SWBD 
corpus (Kahn et al, 2005), and extract the parse 
sub-trees from the parse trees as experimental data.   
The parser?s training procedure is the same as 
(Kahn et al, 2005). The only difference is that they 
use golden edits in the parsing experiments while 
we delete all the edits in the UW Switchboard cor-
pus. The F-score of the parsing result of the 
Charniak parser without edits is 88.24%.  
The Charniak parser without edits is used to 
parse the training data, testing data and tuning data. 
We remove the sentences with only one word and 
delete the interjections in the hypothesis parse trees. 
Finally, we extract parse sub-trees from these hy-
pothesis parse trees. Based on the gold parse trees, 
a parse sub-tree is labeled with 1 (correct), if it has 
all the words, their POS tags and syntactic struc-
tures correct. Otherwise, it is 0 (incorrect). Among 
the 424,614 parse sub-trees from the training data, 
316,182 sub-trees are labeled with 1; among the 
38,774 parse sub-trees from testing data, 22,521 
ones are labeled with 1; and among the 67,464 
parse sub-trees from the tuning data, 38,619 ones 
are labeled with 1. In the testing data, there are 
5,590 sentences, and the percentage of complete 
bracket match2 is 57.11%, and the percentage of 
parse sub-trees with correct labels at the sentence 
level is 48.57%. The percentage of correct parse 
sub-trees is lower than that of the complete bracket 
match due to its stricter requirements.  
Table 1 shows our analysis of the testing data. 
There, the first column indicates the phrase length 
categories from the parse sub-trees. Among all the 
parse trees in the test data, 82.84% (first two rows) 
have a length equal to or shorter than 10 words. 
We converted the original parse sub-trees from the 
Charniak parser into binary trees.  
 
Length Sub-tree Types Number Ratio 
Correct 21,593 55.70%<=10 
Incorrect 10,525 27.14%
Correct 928 2.39%>10 
Incorrect 5,728 14.77%
Table 1. The analysis of testing data. 
 
We apply the model (2) from section 2 on the 
above data for all the following experiments. The 
performance is measured based on the confidence 
annotation error rate (Zhang and Rudnicky, 2001).  
SubtreesOfNumberTotal
ncorrectnotatedAsISubtreesAnOfNumber
ErrorAnnot =.
 
Two sets of experiments are designed to demon-
strate the improvements of our confidence comput-
ing algorithm, as well as the newly introduced 
features (see Table 2 and Table 3). 
Experiments were conducted to evaluate the ef-
fectiveness of each feature category for the sub-
tree level confidence annotation on SWBD corpus 
(Table 2). The baseline system uses the conven-
tional features: words and POS tags. Additional 
feature categories are included separately. The syn-
tactic feature category shows the biggest improve-
ment among all the categories.  
To see the additive effect of the feature spaces 
for the multi-level confidence annotation, another 
set of experiments were performed (Table 3). 
Three feature spaces are included incrementally: 
dependency score, hierarchical tags and syntactic 
features. Each category provides sizable reduction 
in error rate. Totally, it reduces the error rate by  
                                                          
2 Complete bracket match is the percentage of sentences where 
bracketing recall and precision are both 100%. 
Figure 2. SL-JHD Features 
a NNP 
CUISINENAME restaurant
NN 
NP (restaurant) DT 
NP (restaurant) 
NMOD
with JJ 
good service
NN
NP (service)IN 
PP (with) 
NMOD
NMOD POBJ
NP (restaurant) 
NCOMP Relation_Head
219
 Feature Space Description Annot. Error Relative Error Decrease 
Baseline Base features: Words, POS tag 36.2% \ 
Set 1 Base features + Dependency score 32.8% 9.4% 
Set 2 Base features + Hierarchical tags 35.3% 2.5% 
Set 3 Base features + Syntactic features 29.3% 19.1% 
Table 2. Comparison of different feature space (on SWBD corpus). 
 
 Feature Space Description Annot. Error Relative Error Decrease 
Baseline Base features: Words, POS tag 36.2% \ 
Set 4 + Dependency score 32.8% 9.4% 
Set 5 + Dependency score + hierarchical tags   32.7% 9.7% 
Set 6 + Dependency score + hierarchical tags   + syntactic features 26.0% 28.2% 
Table 3. Summary of experiment results with different feature space (on SWBD corpus).
 
10.2%, corresponding to 28.2% of a relative error 
reduction over the baseline. The best result of an-
notation error rate is 26% for Switchboard data, 
which is significantly lower than the 41.91% sub-
tree parsing error rate (see Table 1: 41.91% = 
27.14%+14.77%). So, our algorithm would also 
help the best parsing algorithms during rescoring 
(Charniak et al, 2005; McClosky et al, 2006).  
We list the performance of the parse sub-trees 
with different lengths for Set 6 in Table 4, using 
the F-score as the evaluation measure.  
Length Sub-tree Category F-score
Correct 82.3% <=10 
Incorrect 45.9% 
Correct 33.1% >10 
Incorrect 86.1% 
Table 4. F-scores for various lengths in Set 15. 
 
The F-score difference between the ones with 
correct labels and the ones with incorrect labels are 
significant. We suspect that it is caused by the dif-
ferent amount of training data. Therefore, we sim-
ply duplicated the training data for the sub-trees 
with incorrect labels. For the sub-trees of length 
equal to or less than 10 words, this training method 
leads to a 79.8% F-score for correct labels, and a 
61.4% F-score for incorrect labels, which is much 
more balanced than those in the first set of results. 
5 Conclusion 
In this paper, we generalized confidence annota-
tion algorithms to multiple-level parse trees and 
demonstrated the significant benefits of using long  
 
distance features in SWBD corpora. It is foresee-
able that multi-level confidence annotation can be 
used for many other language applications such as 
parsing, or information retrieval.  
References  
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. Proc. 
ACL, pages 173?180. 
Michael Collins. 2000. Discriminative reranking for natural 
language parsing. Proc. ICML, pages 175?182. 
Malte Gabsdil and Johan Bos. 2003. Combining Acoustic Con-
fidence Scores with Deep Semantic Analysis for Clarifica-
tion Dialogues. Proc. IWCS, pages 137-150. 
Didier Guillevic, et al 2002.  Robust semantic confidence 
scoring. Proc. ICSLP,  pages 853-856. 
Jeremy G. Kahn, et al 2005. Effective Use of Prosody in Pars-
ing Conversational Speech. Proc. EMNLP, pages 233-240. 
David McClosky, Eugene Charniak and Mark Johnson. 2006. 
Reranking and Self-Training for Parser Adaptation. Proc. 
COLING-ACL, pages 337-344. 
Nicola Ueffing and Hermann Ney. 2007. Word-Level Confi-
dence Estimation for Machine Translation. Computational 
Linguistics, 33(1):9-40. 
Fuliang Weng, et al, 2007. CHAT to Your Destination. Proc. 
of the 8th SIGDial workshop on Discourse and Dialogue, 
pages 79-86. 
Qi Zhang, Fuliang Weng and Zhe Feng. 2006. A Pro-gressive 
Feature Selection Algorithm for Ultra Large Feature 
Spaces. Proc. COLING-ACL, pages 561-568. 
Rong Zhang and Alexander I. Rudnicky. 2001. Word level 
confidence annotation using combinations of features. Proc. 
Eurospeech, pages 2105-2108. 
220
A Fast Algorithm for Feature Selection in Conditional
Maximum Entropy Modeling
Yaqian Zhou
Computer Science Department
Fudan University
Shanghai 200433, P.R. China
archzhou@yahoo.com
Fuliang Weng
Research and Technology Center
Robert Bosch Corp.
Palo Alto, CA 94304, USA
Fuliang.weng@rtc.bosch.com
Lide Wu
Computer Science Department
Fudan University
Shanghai 200433, P.R. China
ldwu@fudan.edu.cn
Hauke Schmidt
Research and Technology Center
Robert Bosch Corp.
Palo Alto, CA 94304, USA
hauke.schmidt@rtc.bosch.com
Abstract
This paper describes a fast algorithm that se-
lects features for conditional maximum en-
tropy modeling. Berger et al (1996) presents
an incremental feature selection (IFS) algo-
rithm, which computes the approximate gains
for all candidate features at each selection
stage, and is very time-consuming for any
problems with large feature spaces. In this
new algorithm, instead, we only compute the
approximate gains for the top-ranked features
based on the models obtained from previous
stages. Experiments on WSJ data in Penn
Treebank are conducted to show that the new
algorithm greatly speeds up the feature selec-
tion process while maintaining the same qual-
ity of selected features. One variant of this
new algorithm with look-ahead functionality
is also tested to further confirm the good
quality of the selected features. The new algo-
rithm is easy to implement, and given a fea-
ture space of size F, it only uses O(F) more
space than the original IFS algorithm.
1 Introduction
Maximum Entropy (ME) modeling has received
a lot of attention in language modeling and natural
language processing for the past few years (e.g.,
Rosenfeld, 1994; Berger et al1996; Ratnaparkhi,
1998; Koeling, 2000). One of the main advantages
using ME modeling is the ability to incorporate
various features in the same framework with a
sound mathematical foundation. There are two
main tasks in ME modeling: the feature selection
process that chooses from a feature space a subset
of good features to be included in the model; and
the parameter estimation process that estimates the
weighting factors for each selected feature in the
exponential model. This paper is primarily con-
cerned with the feature selection process in ME
modeling.
While the majority of the work in ME modeling
has been focusing on parameter estimation, less
effort has been made in feature selection. This is
partly because feature selection may not be neces-
sary for certain tasks when parameter estimate al-
gorithms are fast. However, when a feature space
is large and complex, it is clearly advantageous to
perform feature selection, which not only speeds
up the probability computation and requires
smaller memory size during its application, but
also shortens the cycle of model selection during
the training.
Feature selection is a very difficult optimization
task when the feature space under investigation is
large. This is because we essentially try to find a
best subset from a collection of all the possible
feature subsets, which has a size of 2
|
W
|
, where |W|
is the size of the feature space.
In the past, most researchers resorted to a sim-
ple count cutoff technique for selecting features
(Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and
Ratnaparkhi, 1997; Koeling, 2000), where only the
features that occur in a corpus more than a pre-
defined cutoff threshold get selected. Chen and
Rosenfeld (1999) experimented on a feature selec-
tion technique that uses a c
2
 test to see whether a
feature should be included in the ME model, where
the c
2
 test is computed using the count from a prior
distribution and the count from the real training
data. It is a simple and probably effective tech-
nique for language modeling tasks. Since ME
models are optimized using their likelihood or
likelihood gains as the criterion, it is important to
establish the relationship between c
2
 test score and
the likelihood gain, which, however, is absent.
Berger et al (1996) presented an incremental fea-
ture selection (IFS) algorithm where only one fea-
ture is added at each selection and the estimated
parameter values are kept for the features selected
in the previous stages. While this greedy search
assumption is reasonable, the speed of the IFS al-
gorithm is still an issue for complex tasks. For
better understanding its performance, we re-
implemented the algorithm. Given a task of
600,000 training instances, it takes nearly four
days to select 1000 features from a feature space
with a little more than 190,000 features. Berger
and Printz (1998) proposed an f-orthogonal condi-
tion for selecting k features at the same time with-
out affecting much the quality of the selected
features. While this technique is applicable for
certain feature sets, such as word link features re-
ported in their paper, the f-orthogonal condition
usually does not hold if part-of-speech tags are
dominantly present in a feature subset. Past work,
including Ratnaparkhi (1998) and Zhou et al
(2003), has shown that the IFS algorithm utilizes
much fewer features than the count cutoff method,
while maintaining the similar precision and recall
on tasks, such as prepositional phrase attachment,
text categorization and base NP chunking. This
leads us to further explore the possible improve-
ment on the IFS algorithm.
In section 2, we briefly review the IFS algo-
rithm. Then, a fast feature selection algorithm is
described in section 3. Section 4 presents a number
of experiments, which show a massive speed-up
and quality feature selection of the new algorithm.
Finally, we conclude our discussion in section 5.
2  The Incremental Feature Selection Al-
gorithm
For better understanding of our new algorithm, we
start with briefly reviewing the IFS feature selec-
tion algorithm. Suppose the conditional ME model
takes the following form:
? 
p(y | x) =
1
Z (x )
exp( l
j
f
j
(x, y))
j
?
where f
j 
are the features, l
j 
are their corre-
sponding weights, and Z(x) is the normalization
factor.
The algorithm makes the approximation that the
addition of a feature f in an exponential model af-
fects only its associated weight a, leaving un-
changed the l-values associated with the other
features. Here we only present a sketch of the algo-
rithm in Figure 1. Please refer to the original paper
for the details.
In the algorithm, we use I for the number of
training instances, Y for the number of output
classes, and F for the number of candidate features
or the size of the candidate feature set.
0. Initialize: S = ?, sum[1..I, 1..Y] = 1,
z[1..I] = Y
1. Gain computation:
MaxGain = 0
for f in feature space F do
)(maxarg
? aa a fSG ?=
)(max
? aa fSGg ?=
if MaxGain < 
g
?  then
   MaxGain = 
g
?
   f
*
 = f
  a*=a?
2. Feature selection:
S = S ? { f
*
 }
3. if termination condition is met, then stop
4. Model adjustment:
for instance i such that there is y
and f
*
(x
i
, y) = 1 do
z[i] -=sum[i, y]
sum[i, y] ?= exp(a*)
z[i] += sum[i, y]
5. go to  step 1.
Figure 1: A Variant of the IFS Algorithm.
One difference here from the original IFS algo-
rithm is that we adopt a technique in (Goodman,
2002) for optimizing the parameters in the condi-
tional ME training. Specifically, we use array z to
store the normalizing factors, and array sum for all
the un-normalized conditional probabilities sum[i,
y]. Thus, one only needs to modify those sum[i, y]
that satisfy f
*
(x
i
, y)=1, and to make changes to their
corresponding normalizing factors z[i]. In contrast
to what is shown in Berger et al1996?s paper, here
is how the different values in this variant of the IFS
algorithm are computed.
Let us denote
?
=
j
jj
yxfxysum )),(exp()|( l
?
=
y
xysumxZ )|()(
Then, the model can be represented by sum(y|x)
and Z(x) as follows:
)(/)|()|( xZxysumxyp =
where sum(y|x
i
) and Z(x
i
) correspond to sum[i,y]
and z[i] in Figure 1, respectively.
Assume the selected feature set is S, and f is
currently being considered. The goal of each se-
lection stage is to select the feature f that maxi-
mizes the gain of the log likelihood, where the a
and gain of f are derived through following steps:
Let the log likelihood of the model be
?
?
-=
-?
yx
yx
xZxysumyxp
xypyxppL
,
,
))(/)|(log(),(
~
))|(log(),(
~
)(
and the empirical expectation of feature f be
? 
E
? 
p
( f ) =
? 
p (x,y) f (x, y)
x,y
?
With the approximation assumption in Berger
et al(1996)?s paper, the un-normalized component
and the normalization factor of the model have the
following recursive forms:
  )|()|(
aa
exysumxysum
SfS
?=
?
)|(                           
)|()()(
xysum
xysumxZxZ
fS
SSfS aa
?
?
+
-=
The approximate gain of the log likelihood is
computed by
 
? 
G
S? f
(a) ? L(p
S? f
a
) - L(p
S
)
             = -
? 
p (x)(logZ
S? f ,a (x)
x
?
/Z
S
(x))
                      + aE
? 
p
( f )                    (1)
The maximum approximate gain and its corre-
sponding a are represented as:
  )(max),(~ aa fSGfSL ?=D
  )(maxarg),(~ aa a fSGfS ?=
3 A Fast Feature Selection Algorithm
The inefficiency of the IFS algorithm is due to the
following reasons. The algorithm considers all the
candidate features before selecting one from them,
and it has to re-compute the gains for every feature
at each selection stage. In addition, to compute a
parameter using Newton?s method is not always
efficient. Therefore, the total computation for the
whole selection processing can be very expensive.
Let g(j, k) represent the gain due to the addition
of feature f
j
 to the active model at stage k. In our
experiments, it is found even if D (i.e., the addi-
tional number of stages after stage k) is large, for
most j, g(j, k+D) - g(j, k) is a negative number or at
most a very small positive number. This leads us to
use the g(j, k) to approximate the upper bound of
g(j, k+D).
The intuition behind our new algorithm is that
when a new feature is added to a model, the gains
for the other features before the addition and after
the addition do not change much. When there are
changes, their actual amounts will mostly be within
a narrow range across different features from top
ranked ones to the bottom ranked ones. Therefore,
we only compute and compare the gains for the
features from the top-ranked downward until we
reach the one with the gain, based on the new
model, that is bigger than the gains of the remain-
ing features. With a few exceptions, the gains of
the majority of the remaining features were com-
puted based on the previous models.
As in the IFS algorithm, we assume that the ad-
dition of a feature f only affects its weighting fac-
tor a. Because a uniform distribution is assumed as
the prior in the initial stage, we may derive a
closed-form formula for a(j, 0) and g(j, 0) as fol-
lows.
Let
? 
Ed ( f ) = ? p (x)max
y
{ f (x, y)}
x
?
? 
R
e
( f ) = E
? 
p
( f ) / Ed ( f )
Yp /1
0
=
Then
 )log()0,(
)(1
1)(
0
0
fR
p
p
fR
e
e
j
-
-
?=a
? 
g( j,0) = L(p
?? f
a( i,0)
) - L(p
?
)
          = Ed ( f )[Re ( f )log Re ( f )
p
0
                     + (1- R
e
( f ))log
1-R
e
( f )
1- p
0
] 
where ? denotes an empty set, p? is the uni-
form distribution. The other steps for computing
the gains and selecting the features are given in
Figure 2 as a pseudo code. Because we only com-
pute gains for a small number of top-ranked fea-
tures, we call this feature selection algorithm as
Selective Gain Computation (SGC) Algorithm.
In the algorithm, we use array g to keep the
sorted gains and their corresponding feature indi-
ces. In practice, we use a binary search tree to
maintain the order of the array.
The key difference between the IFS algorithm
and the SGC algorithm is that we do not evaluate
all the features for the active model at every stage
(one stage corresponds to the selection of a single
feature). Initially, the feature candidates are or-
dered based on their gains computed on the uni-
form distribution. The feature with the largest gain
gets selected, and it forms the model for the next
stage. In the next stage, the gain of the top feature
in the ordered list is computed based on the model
just formed in the previous stage. This gain is
compared with the gains of the rest features in the
list. If this newly computed gain is still the largest,
this feature is added to form the model at stage 3.
If the gain is not the largest, it is inserted in the
ordered list so that the order is maintained. In this
case, the gain of the next top-ranked feature in the
ordered list is re-computed using the model at the
current stage, i.e., stage 2.
This process continues until the gain of the top-
ranked feature computed under the current model
is still the largest gain in the ordered list. Then, the
model for the next stage is created with the addi-
tion of this newly selected feature. The whole fea-
ture selection process stops either when the
number of the selected features reaches a pre-
defined value in the input, or when the gains be-
come too small to be useful to the model.
0. Initialize: S = ?, sum[1..I, 1..Y] = 1,
z[1..I] = Y, g[1..F] = {g(1,0),?,g(F,0)}
1. Gain computation:
MaxGain = 0
Loop
    
]},...,1[{maxarg
in  
Fgf
Ff
j
=
    if g[j] ? MaxGain then go to step 2
    else
 
)(maxarg
? aa a fSG ?=
 
)(max
? aa fSGg ?=
       g[j]= 
g
?
       if MaxGain < 
g
?  then
          MaxGain = 
g
?
          f
*
 = f
j
         a*=a?
2. Feature selection:
S = S ? { f
*
 }
3. if termination condition is met, then stop
4. Model adjustment:
for instance i such that there is y
and f
*
(x
i
, y) = 1 do
z[i] -=sum[i, y]
sum[i, y] ?= exp(a*)
z[i] += sum[i, y]
5. go to  step 1.
Figure 2: Selective Gain Computation Algo-
rithm for Feature Selection
In addition to this basic version of the SGC al-
gorithm, at each stage, we may also re-compute
additional gains based on the current model for a
pre-defined number of features listed right after
feature f
*
 (obtained in step 2) in the ordered list.
This is to make sure that the selected feature f
*
 is
indeed the feature with the highest gain within the
pre-defined look-ahead distance. We call this vari-
ant the look-ahead version of the SGC algorithm.
4 Experiments
A number of experiments have been conducted to
verify the rationale behind the algorithm. In par-
ticular, we would like to have a good understand-
ing of the quality of the selected features using the
SGC algorithm, as well as the amount of speed-
ups, in comparison with the IFS algorithm.
The first sets of experiments use a dataset {(x,
y)}, derived from the Penn Treebank, where x is a
10 dimension vector including word, POS tag and
grammatical relation tag information from two ad-
jacent regions, and y is the grammatical relation
tag between the two regions. Examples of the
grammatical relation tags are subject and object
with either the right region or the left region as the
head. The total number of different grammatical
tags, i.e., the size of the output space, is 86. There
are a little more than 600,000 training instances
generated from section 02-22 of WSJ in Penn
Treebank, and the test corpus is generated from
section 23.
In our experiments, the feature space is parti-
tioned into sub-spaces, called feature templates,
where only certain dimensions are included. Con-
sidering all the possible combinations in the 10-
dimensional space would lead to 2
10
 feature tem-
plates. To perform a feasible and fair comparison,
we use linguistic knowledge to filter out implausi-
ble subspaces so that only 24 feature templates are
actually used. With this amount of feature tem-
plates, we get more than 1,900,000 candidate fea-
tures from the training data. To speed up the
experiments, which is necessary for the IFS algo-
rithm, we use a cutoff of 5 to reduce the feature
space down to 191,098 features. On average, each
candidate feature covers about 485 instances,
which accounts for 0.083% over the whole training
instance set and is computed through:
???
=
jj yx
j
yxfac 1/),(
,
The first experiment is to compare the speed of
the IFS algorithm with that of SGC algorithm.
Theoretically speaking, the IFS algorithm com-
putes the gains for all the features at every stage.
This means that it requires O(NF) time to select a
feature subset of size N from a candidate feature
set of size F. On the other hand, the SGC algorithm
considers much fewer features, only 24.1 features
on average at each stage, when selecting a feature
from the large feature space in this experiment.
Figure 3 shows the average number of features
computed at the selected points for the SGC algo-
rithm, SGC with 500 look-ahead, as well as the
IFS algorithm. The averaged number of features is
taken over an interval from the initial stage to the
current feature selection point, which is to smooth
out the fluctuation of the numbers of features each
selection stage considers. The second algorithm
looks at an additional fixed number of features,
500 in this experiment, beyond the ones considered
by the basic SGC algorithm. The last algorithm has
a linear decreasing number of features to select,
because the selected features will not be consid-
ered again. In Figure 3, the IFS algorithm stops
after 1000 features are selected. This is because it
takes too long for this algorithm to complete the
entire selection process. The same thing happens in
Figure 4, which is to be explained below.
0
1
2
3
4
5
6
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
A
v
e
r
a
g
e
 
C
o
n
s
i
d
e
r
e
d
 
F
e
a
t
u
r
e
 
N
u
m
b
e
r
Berger SGC-0 SGC-500
log
10
(Y)
Figure 3: The log number of features considered in
SGC algorithm, in comparison with the IFS algo-
rithm.
To see the actual amount of time taken by the
SGC algorithms and the IFS algorithm with the
currently available computing power, we use a
Linux workstation with 1.6Ghz dual Xeon CPUs
and 1 GB memory to run the two experiments si-
multaneously. As it can be expected, excluding the
beginning common part of the code from the two
algorithms, the speedup from using the SGC algo-
rithm is many orders of magnitude, from more than
100 times to thousands, depending on the number
of features selected. The results are shown in Fig-
ure 4.
-2
-1
0
1
2
3
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
A
v
e
r
a
g
e
 
T
i
m
e
f
o
r
 
e
a
c
h
 
s
e
l
e
c
t
i
o
n
 
s
t
e
p
(
s
e
c
o
n
d
)
Berger SGC-0
log
10
(Y)
Figure 4: The log time used by SGC algorithm, in
comparison with the IFS algorithm.
To verify the quality of the selected features
using our SGC algorithm, we conduct four experi-
ments: one uses all the features to build a condi-
tional ME model, the second uses the IFS
algorithm to select 1,000 features, the third uses
our SGC algorithm, the fourth uses the SGC algo-
rithm with 500 look-ahead, and the fifth takes the
top n most frequent features in the training data.
The precisions are computed on section 23 of the
WSJ data set in Penn Treebank. The results are
listed in Figure 5. Three factors can be learned
from this figure. First, the three IFS and SGC algo-
rithms perform similarly. Second, 3000 seems to
be a dividing line: when the models include fewer
than 3000 selected features, the IFS and SGC algo-
rithms do not perform as well as the model with all
the features; when the models include more than
3000 selected features, their performance signifi-
cantly surpass the model with all the features. The
inferior performance of the model with all the fea-
tures at the right side of the chart is likely due to
the data over-fitting problem. Third, the simple
count cutoff algorithm significantly under-
performs the other feature selection algorithms
when feature subsets with no more than 10,000
features are considered.
To further confirm the findings regarding preci-
sion, we conducted another experiment with Base
NP recognition as the task. The experiment uses
section 15-18 of WSJ as the training data, and sec-
tion 20 as the test data. When we select 1,160 fea-
tures from a simple feature space using our SGC
algorithm, we obtain a precision/recall of
92.75%/93.25%. The best reported ME work on
this task includes Koeling (2000) that has the pre-
cision/recall of 92.84%/93.18% with a cutoff of 5,
and Zhou et al (2003) has reached the perform-
ance of 93.04%/93.31% with cutoff of 7 and
reached a performance of 92.46%/92.74% with
615 features using the IFS algorithm. While the
results are not directly comparable due to different
feature spaces used in the above experiments, our
result is competitive to these best numbers. This
shows that our new algorithm is both very effective
in selecting high quality features and very efficient
in performing the task.
5 Comparison and Conclusion
Feature selection has been an important topic in
both ME modeling and linear regression. In the
past, most researchers resorted to count cutoff
technique in selecting features for ME modeling
(Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and
Ratnaparkhi, 1997; Koeling, 2000). A more refined
algorithm, the incremental feature selection algo-
rithm by Berger et al(1996), allows one feature
being added at each selection and at the same time
keeps estimated parameter values for the features
selected in the previous stages. As discussed in
(Ratnaparkhi, 1998), the count cutoff technique
works very fast and is easy to implement, but has
the drawback of containing a large number of re-
70
72
74
76
78
80
82
84
86
88
90
92
94
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
P
r
e
c
i
s
i
o
n
 
(
%
)
All (191098) IFS SGC-0
SGC-500 Count Cutoff
Figure 5: Precision results from models using the whole
feature set and the feature subsets through the IFS algo-
rithm, the SGC algorithm, the SGC algorithm with 500
look-ahead, and the count cutoff algorithm.
dundant features. In contrast, the IFS removes the
redundancy in the selected feature set, but the
speed of the algorithm has been a big issue for
complex tasks. Having realized the drawback of
the IFS algorithm, Berger and Printz (1998) pro-
posed an f-orthogonal condition for selecting k
features at the same time without affecting much
the quality of the selected features. While this
technique is applicable for certain feature sets,
such as link features between words, the f -
orthogonal condition usually does not hold if part-
of-speech tags are dominantly present in a feature
subset.
Chen and Rosenfeld (1999) experimented on a
feature selection technique that uses a c
2
 test to see
whether a feature should be included in the ME
model, where the c
2
 test is computed using the
counts from a prior distribution and the counts
from the real training data. It is a simple and
probably effective technique for language model-
ing tasks. Since ME models are optimized using
their likelihood or likelihood gains as the criterion,
it is important to establish the relationship between
c
2
 test score and the likelihood gain, which, how-
ever, is absent.
There is a large amount of literature on feature
selection in linear regression, where least mean
squared errors measure has been the primary opti-
mization criterion. Two issues need to be ad-
dressed in order to effectively use these techniques.
One is the scalability issue since most statistical
literature on feature selection only concerns with
dozens or hundreds of features, while our tasks
usually deal with feature sets with a million of
features. The other is the relationship between
mean squared errors and likelihood, similar to the
concern expressed in the previous paragraph.
These are important issues and require further in-
vestigation.
In summary, this paper presents our new im-
provement to the incremental feature selection al-
gorithm. The new algorithm runs hundreds to
thousands times faster than the original incre-
mental feature selection algorithm. In addition, the
new algorithm selects the features of a similar
quality as the original Berger et alalgorithm,
which has also shown to be better than the simple
cutoff method in some cases.
Acknowledgement
This work is done while the first author is visiting
the Center for Study of Language and Information
(CSLI) at Stanford University and the Research
and Technology Center of Robert Bosch Corpora-
tion. This project is sponsored by the Research and
Technology Center of Robert Bosch Corporation.
We are grateful to the kind support from Prof.
Stanley Peters of CSLI. We also thank the com-
ments from the three anonymous reviewers which
improve the quality of the paper.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistic, 22 (1): 39-71.
Adam L. Berger and Harry Printz. 1998. A Comparison
of Criteria for Maximum Entropy / Minimum Diver-
gence Feature Selection. Proceedings of the 3
rd
 con-
ference on Empirical Methods in Natural Language
Processing. Granda, Spain.
Stanley Chen and Ronald Rosenfeld. 1999. Efficient
Sampling and Feature Selection in Whole Sentence
maximum Entropy Language Models. Proceedings of
ICASSP-1999, Phoenix, Arizona.
Joshua Goodman. 2002. Sequential Conditional Gener-
alized Iterative Scaling. Association for Computa-
tional Linguistics, Philadelphia, Pennsylvania.
Rob Koeling. 2000. Chunking with Maximum Entropy
Models. In: Proceedings of CoNLL-2000 and LLL-
2000, Lisbon, Portugal, 139-141.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Ronald Rosenfeld. 1994. Adaptive Statistical Language
Modeling: A Maximum Entropy Approach. Ph.D.
thesis, Carnegie Mellon University, April.
J. Reynar and A. Ratnaparkhi. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries.
In: Proceedings of the Fifth Conference on Applied
Natural Language Processing, Washington D.C., 16-
19.
Zhou Ya-qian, Guo Yi-kun, Huang Xuan-jing, and Wu
Li-de. 2003. Chinese and English BaseNP Recog-
nized by Maximum Entropy. Journal of Computer
Research and Development. 40(3):440-446, Beijin
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 179?185,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Exploring Features for Identifying Edited Regions in Disfluent Sentences 
 
 
Qi Zhang Fuliang Weng 
Department of Computer Science Research and Technology Center 
Fudan University Robert Bosch Corp. 
Shanghai, P.R.China 200433 Palo Alto, CA 94304 
qi_zhang@fudan.edu.cn fuliang.weng@rtc.bosch.com 
 
 
 
 
Abstract 
This paper describes our effort on the task 
of edited region identification for parsing 
disfluent sentences in the Switchboard 
corpus. We focus our attention on 
exploring feature spaces and selecting 
good features and start with analyzing the 
distributions of the edited regions and 
their components in the targeted corpus. 
We explore new feature spaces of a part-
of-speech (POS) hierarchy and relaxed for 
rough copy in the experiments. These 
steps result in an improvement of 43.98% 
percent relative error reduction in F-score 
over an earlier best result in edited 
detection when punctuation is included in 
both training and testing data [Charniak 
and Johnson 2001], and 20.44% percent 
relative error reduction in F-score over the 
latest best result where punctuation is 
excluded from the training and testing 
data [Johnson and Charniak 2004]. 
1 Introduction 
Repairs, hesitations, and restarts are common in 
spoken language, and understanding spoken 
language requires accurate methods for identifying 
such disfluent phenomena. Processing speech 
repairs properly poses a challenge to spoken dialog 
systems. Early work in this field is primarily based 
on small and proprietary corpora, which makes the 
comparison of the proposed methods difficult 
[Young and Matessa 1991, Bear et al 1992, 
Heeman & Allen 1994]. Because of the availability 
of the Switchboard corpus [Godfrey et al 1992] 
and other conversational telephone speech (CTS) 
corpora, there has been an increasing interest in 
improving the performance of identifying the 
edited regions for parsing disfluent sentences 
[Charniak and Johnson 2001, Johnson and 
Charniak 2004, Ostendorf et al 2004, Liu et al 
2005].  
 
In this paper we describe our effort towards the 
task of edited region identification with the 
intention of parsing disfluent sentences in the 
Switchboard corpus. A clear benefit of having 
accurate edited regions for parsing has been 
demonstrated by a concurrent effort on parsing 
conversational speech [Kahn et al2005]. Since 
different machine learning methods provide similar 
performances on many NLP tasks, in this paper, 
we focus our attention on exploring feature spaces 
and selecting good features for identifying edited 
regions. We start by analyzing the distributions of 
the edited regions and their components in the 
targeted corpus. We then design several feature 
spaces to cover the disfluent regions in the training 
data. In addition, we also explore new feature 
spaces of a part-of-speech hierarchy and extend 
candidate pools in the experiments. These steps 
result in a significant improvement in F-score over 
the earlier best result reported in [Charniak and 
Johnson 2001], where punctuation is included in 
both the training and testing data of the 
Switchboard corpus, and a significant error 
reduction in F-score over the latest best result 
[Johnson and Charniak 2004], where punctuation 
is ignored in both the training and testing data of 
the Switchboard corpus.  
 
179
In this paper, we follow the definition of [Shriberg 
1994] and others for speech repairs: A speech 
repair is divided into three parts: the reparandum, 
the part that is repaired; the interregnum, the part 
that can be either empty or fillers; and the 
repair/repeat, the part that replaces or repeats the 
reparandum. The definition can also be 
exemplified via the following utterance: 
 
N
repeatreparanda int erregnum
 ,  , this is  a big problem.This is you know	
 	
  
 
This paper is organized as follows. In section 2, we 
examine the distributions of the editing regions in 
Switchboard data. Section 3, then, presents the 
Boosting method, the baseline system and the 
feature spaces we want to explore. Section 4 
describes, step by step, a set of experiments that 
lead to a large performance improvement. Section 
5 concludes with discussion and future work. 
2 Repair Distributions in Switchboard 
We start by analyzing the speech repairs in the 
Switchboard corpus. Switchboard has over one 
million words, with telephone conversations on 
prescribed topics [Godfrey et al 1992]. It is full of 
disfluent utterances, and [Shriberg 1994, Shriberg 
1996] gives a thorough analysis and categorization 
of them. [Engel et al 2002] also showed detailed 
distributions of the interregnum, including 
interjections and parentheticals. Since the majority 
of the disfluencies involve all the three parts 
(reparandum, interregnum, and repair/repeat), the 
distributions of all three parts will be very helpful 
in constructing patterns that are used to identify 
edited regions.  
 
For the reparandum and repair types, we include 
their distributions with and without punctuation. 
We include the distributions with punctuation is to 
match with the baseline system reported in 
[Charniak and Johnson 2001], where punctuation 
is included to identify the edited regions. Resent 
research showed that certain punctuation/prosody 
marks can be produced when speech signals are 
available [Liu et al 2003]. The interregnum type, 
by definition, does not include punctuation.  
 
The length distributions of the reparanda in the 
training part of the Switchboard data with and 
without punctuation are given in Fig. 1. The 
reparanda with lengths of less than 7 words make 
up 95.98% of such edited regions in the training 
data. When we remove the punctuation marks, 
those with lengths of less than 6 words reach 
roughly 96%. Thus, the patterns that consider only 
reparanda of length 6 or less will have very good 
coverage. 
 
Length distribution of reparanda
0%
10%
20%
30%
40%
50%
1 2 3 4 5 6 7 8 9 10
With punctation Without punctation
 
 
Figure 1. Length distribution of reparanda in 
Switchboard training data. 
 
Length distribution of 
repairs/repeats/restarts 
0%
10%
20%
30%
40%
50%
0 1 2 3 4 5 6 7 8 9
With punctation Without punctation
Figure 2. Length distribution of 
repairs/repeats/restarts in Switchboard training data. 
 
Length distribution of interregna
0%
20%
40%
60%
80%
100%
0 1 2 3 4 5 6 7 8 9 10
 
Figure 3. Length distribution of interregna in 
Switchboard training data. 
 
The two repair/repeat part distributions in the 
training part of the Switchboard are given in Fig. 2. 
The repairs/repeats with lengths less than 7 words 
180
make 98.86% of such instances in the training data. 
This gives us an excellent coverage if we use 7 as 
the threshold for constructing repair/repeat patterns. 
 
The length distribution of the interregna of the 
training part of the Switchboard corpus is shown in 
Fig. 3. We see that the overwhelming majority has 
the length of one, which are mostly words such as 
?uh?, ?yeah?, or ?uh-huh?. 
 
In examining the Switchboard data, we noticed that 
a large number of reparanda and repair/repeat pairs 
differ on less than two words, i.e. ?as to, you know, 
when to?1, and the amount of the pairs differing on 
less than two POS tags is even bigger. There are 
also cases where some of the pairs have different 
lengths. These findings provide a good base for our 
feature space. 
3 Feature Space Selection for Boosting 
We take as our baseline system the work by 
[Charniak and Johnson 2001]. In their approach, 
rough copy is defined to produce candidates for 
any potential pairs of reparanda and repairs. A 
boosting algorithm [Schapire and Singer 1999] is 
used to detect whether a word is edited. A total of 
18 variables are used in the algorithm. In the rest 
of the section, we first briefly introduce the 
boosting algorithm, then describe the method used 
in [Charniak and Johnson 2001], and finally we 
contrast our improvements with the baseline 
system. 
3.1 Boosting Algorithm 
Intuitively, the boosting algorithm is to combine a 
set of simple learners iteratively based on their 
classification results on a set of training data. 
Different parts of the training data are scaled at 
each iteration so that the parts of the data previous 
classifiers performed poorly on are weighted 
higher. The weighting factors of the learners are 
adjusted accordingly.  
 
We re-implement the boosting algorithm reported 
by [Charniak and Johnson 2001] as our baseline 
system in order to clearly identify contributing 
                                                          
1  ?as to?  is the edited region. Italicized words in the 
examples are edited words 
factors in performance.  Each word token is 
characterized by a finite tuple of random variables  
(Y, X1,..., Xm ). 
Y is  the conditioned variables and ranges from    
{-1,+1}, with Y = +1 indicating that the word is 
edited. X1,..., Xm  are the conditioning variables; 
each variable jX  ranges over a finite set j? . The 
goal of the classifer is to predict the value of Y 
given a value for X1,..., Xm .  
 
A boosting classifier is a linear combination of n 
features to define the prediction variable Z. 
                          ?
=
=
n
i
iiFZ
1
?                (1) 
where ?i is the weight to be estimated for feature ?i. 
?i is a set of variable-value pairs, and each Fi has 
the form of: 
                  Fi = (X j = x j )
<X j ,x j >?? i
?         (2) 
with X?s being conditioning variables and x?s being 
values.   
 
Each component in the production for Fi  is 
defined as: 
 
       (X j = x j ) =
1  < X j = x j >? ?i
0   otherwise
? ? ?      (3) 
 
In other words, Fi is 1 if and only if all the 
variable-value pairs for the current position belong 
to ?i.  
 
The prediction made by the classifier is 
|Z| Z/ sign(Z)= . Intuitively, our goal is to adjust 
the vector of feature weights 1( ,...., )n? ? ?=K  to 
minimize the expected misclassification rate 
]E[sign(Z) Y? . This function is difficult to 
minimize, so our boosting classifier minimizes the 
expected boost loss )][(exp(-YZE?t  as in [Collins 
2000], where ][E?t ?  is the expectation on the 
empirical training corpus distribution. In our 
implementation, each learner contains only one 
variable. The feature weights are adjusted 
iteratively, one weight per iteration. At each 
iteration, it reduces the boost loss on the training 
corpus. In our experiments, ?K is obtained after 
181
1500 iterations, and contains around 1350 non-zero 
feature weights. 
3.2 Charniak-Johnson approach 
In [Charniak and Johnson 2001], identifying edited 
regions is considered as a classification problem, 
where each word is classified either as edited or 
normal. The approach takes two steps. The first 
step is to find rough copy. Then, a number of 
variables are extracted for the boosting algorithm. 
In particular, a total of 18 different conditioning 
variables are used to predict whether the current 
word is an edited word or a non-edited word. The 
18 different variables listed in Table 1 correspond 
to the 18 different dimensions/factors for the 
current word position. Among the 18 variables, six 
of them, Nm, Nu, Ni, Nl, Nr and Tf , depend on the 
identification of a rough copy. 
 
For convenience, their definition of a rough copy is 
repeated here. A rough copy in a string of tagged 
words has the form of 21 ?? ?? , where: 
1. 1?  (the source) and 2?  (the copy) both 
begin   with    non-punctuation, 
2. the strings of non-punctuation POS tag 
of   1?  and 2?  are identical, 
3. ?  (the free final) consists of zero or 
more sequences of a free final word  (see 
below) followed by optional punctuation, 
4. ?  (the interregnum) consists of 
sequences of an interregnum string (see 
below) followed by optional punctuation. 
 
The set of free final words includes all partial 
words and a small set of conjunctions, adverbs and 
miscellanea. The set of interregnum strings 
consists of a small set of expressions such as uh, 
you know, I guess, I mean, etc.  
3.3 New Improvements 
Our improvements to the Charniak-Johnson 
method can be classified into three categories with 
the first two corresponding to the twp steps in their 
method. The three categories of improvements are 
described in details in the following subsections.  
3.3.1 Relaxing Rough Copy  
We relax the definition for rough copy, because 
more than 94% of all edits have both reparandum 
and repair, while the rough copy defined in 
[Charniak and Johnson 2001] only covers 77.66% 
of such instances.  
 
Two methods are used to relax the rough copy 
definition. The first one is to adopt a hierarchical 
POS tag set: all the Switchboard POS tags are 
further classified into four major categories: N 
(noun related), V (verb related), Adj (noun 
modifiers), Adv (verb modifiers). Instead of 
requiring the exact match of two POS tag 
sequences, we also consider two sequences as a 
Variables Name Short description 
X1 W0 The current orthographic word. 
X2 ? X5 P0,P1,P2,Pf Partial word flags for the current position, the next two to the right, and the first one 
in a sequence of free-final words (partial, conjunctions, etc.) to the right of the 
current position. 
X6 ? X10 T-1,T0,T1,T2,Tf Part of speech tags for the left position, the current position, the next two positions 
to the right, and the first free-final word position to the right of the current position.
X11 Nm Number of words in common in reparandum and repair 
X12 Nn Number of words in reparandum but not repair 
X13 Ni Number of words in interregnum 
X14 Nl Number of words to the left edge of reparandum 
X15 Nr Number of words to the right edge of reparandum 
X16 Ct The first non-punctuation tag to the right of the current position 
X17 Cw The first non-punctuation word to the right of the current position 
X18 Ti The tag of the first word right after the interregnum that is right after the current 
word.  
 
Table 1. Descriptions of the 18 conditioning variables from [Charniak and Johnson 2001] 
 
182
rough copy if their corresponding major categories 
match. This relaxation increases the rough copy 
coverage, (the percent of words in edited regions 
found through the definition of rough copy), from 
77.66% to 79.68%.  
 
The second is to allow one mismatch in the two 
POS sequences. The mismatches can be an 
addition, deletion, or substitution. This relaxation 
improves the coverage from 77.66% to 85.45%. 
Subsequently, the combination of the two 
relaxations leads to a significantly higher coverage 
of 87.70%. Additional relaxation leads to excessive 
candidates and worse performance in the 
development set. 
3.3.2 Adding New Features  
We also include new features in the feature set: 
one is the shortest distance (the number of words) 
between the current word and a word of the same 
orthographic form to the right, if that repeated 
word exists; another is the words around the 
current position. Based on the distributional 
analysis in section 2, we also increase the window 
sizes for POS tags ( 5 5,...,T T? ) and words 
( 5 5,...,W W? ) to ?5 and partial words ( 3 3,...,P P? ) 
to ?3, extending Ti and Pj.  
3.3.3 Post Processing Step 
In addition to the two categories, we try to use 
contextual patterns to address the independency of 
variables in the features. The patterns have been 
extracted from development and training data, to 
deal with certain sequence-related errors, e.g.,  
E N E ? E E E, 
which means that if the neighbors on both sides of 
a word are classified into EDITED, it should be 
classified into EDITED as well.  
4 Experimental Results  
We conducted a number of experiments to test the 
effectiveness of our feature space exploration. 
Since the original code from [Charniak and 
Johnson 2001] is not available, we conducted our 
first experiment to replicate the result of their 
baseline system described in section 3. We used 
the exactly same training and testing data from the 
Switchboard corpus as in [Charniak and Johnson 
2001]. The training subset consists of all files in 
the sections 2 and 3 of the Switchboard corpus. 
Section 4 is split into three approximately equal 
size subsets. The first of the three, i.e., files 
sw4004.mrg to sw4153.mrg, is the testing corpus. 
The files sw4519.mrg to sw4936.mrg are the 
development corpus. The rest files are reserved for 
other purposes.  When punctuation is included in 
both training and testing, the re-established 
baseline has the precision, recall, and F-score of 
94.73%, 68.71% and 79.65%, respectively. These 
results are comparable with the results from 
[Charniak & Johnson 2001], i.e., 95.2%, 67.8%, 
and 79.2% for precision, recall, and f-score, 
correspondingly. 
 
In the subsequent experiments, the set of additional 
feature spaces described in section 3 are added, 
step-by-step. The first addition includes the 
shortest distance to the same word and window 
size increases. This step gives a 2.27% 
improvement on F-score over the baseline. The 
next addition is the introduction of the POS 
hierarchy in finding rough copies. This also gives 
more than 3% absolute improvement over the 
baseline and 1.19% over the expanded feature set 
model. The addition of the feature spaces of 
relaxed matches for words, POS tags, and POS 
hierarchy tags all give additive improvements, 
which leads to an overall of 8.95% absolute 
improvement over the re-implemented baseline, or 
43.98% relative error reduction on F-score.  
 
When compared with the latest results from 
[Johnson and Charniak 2004], where no 
punctuations are used for either training or testing 
data, we also observe the same trend of the 
improved results. Our best result gives 4.15% 
absolute improvement over their best result, or 
20.44% relative error reduction in f-scores. As a 
sanity check, when evaluated on the training data 
as a cheating experiment, we show a remarkable 
consistency with the results for testing data.  
 
For error analysis, we randomly selected 100 
sentences with 1673 words total from the test 
sentences that have at least one mistake. Errors can 
be divided into two types, miss (should be edited) 
and false alarm (should be noraml). Among the 
207 misses, about 70% of them require some 
phrase level analysis or acoustic cues for phrases. 
183
For example, one miss is ?because of the friends 
because of many other things?, an error we would 
have a much better chance of correct identification, 
if we were able to identify prepositional phrases 
reliably. Another example is ?most of all my 
family?. Since it is grammatical by itself, certain 
prosodic information in between ?most of? and ?all 
my family? may help the identification. [Ostendorf 
et al 2004] reported that interruption point could 
help parsers to improve results.  [Kahn et al 2005] 
also showed that prosody information could help 
parse disfluent sentences. The second major class 
of the misses is certain short words that are not 
labeled consistently in the corpus. For example, 
?so?, ?and?, and ?or?, when they occur in the 
beginning of a sentence, are sometimes labeled as 
edited, and sometimes just as normal. The last 
category of the misses, about 5.3%, contains the 
ones where the distances between reparanda and 
repairs are often more than 10 words.  
 
Among the 95 false alarms, more than three 
quarters of misclassified ones are related to certain 
grammatical constructions. Examples include cases 
like, ?the more ? the more? and ?I think I 
should ??. These cases may be fixable if more 
elaborated grammar-based features are used.  
5 Conclusions  
This paper reports our work on identifying edited 
regions in the Switchboard corpus. In addition to a 
Results on testing data Results on training data 
with punctuation Punctuation on both  No punctuation on both 
Method codes 
Precision Recall f-score Precision Recall f-score Precision Recall f-score
CJ?01    95.2 67.8 79.2    
JC?04 p       82.0 77.8 79.7 
 R CJ?01 94.9 71.9 81.81 94.73 68.71 79.65 91.46 64.42 75.59 
+d 94.56 78.37 85.71 94.47 72.31 81.92 91.79 68.13 78.21 
+d+h 94.23 81.32 87.30 94.58 74.12 83.11 91.56 71.33 80.19 
+d+rh 94.12 82.61 87.99 92.61 77.15 84.18 89.92 72.68 80.39 
+d+rw 96.13 82.45 88.77 94.79 75.43 84.01 92.17 70.79 80.08 
+d+rw+rh 94.42 84.67 89.28 94.57 77.93 85.45 92.61 73.46 81.93 
+d+rw+rt+wt 94.43 84.79 89.35 94.65 76.61 84.68 92.08 72.61 81.19 
+d+rw+rh+wt 94.58 85.21 89.65 94.72 79.22 86.28 92.69 75.30 83.09 
+d+rw+rh+wt+ps 93.69 88.62 91.08 93.81 83.94 88.60 89.70 78.71 83.85 
 
Table 2. Result summary for various feature spaces. 
 
Method codes Method description 
CJ?01 Charniak and Johnson 2001 
JC?04 p Johnson and Charniak 2004, parser results 
R CJ?01 Duplicated results for Charniak and Johnson 2001 
+d Distance + window sizes 
+d+h Distance + window sizes + POS hierarchy in rough copy 
+d+rh Distance + window sizes + relaxed POS hierarchy in rough copy 
+d+rw Distance + window sizes + relaxed word in rough copy 
+d+rw+rh Distance + window sizes + relaxed word and POS hierarchy in rough copy 
+d+rw+rt+wt Distance + window sizes + word & tag pairs + relaxed word and POS in rough copy 
+d+rw+rh+wt Distance + window sizes + word & tag pairs + relaxed word and POS hierarchy in 
rough copy 
+d+rw+rh+wt+ps Distance + window sizes + word & tag pairs + relaxed word and POS hierarchy in 
rough copy + pattern substitution 
 
Table 3. Description of method codes used in the result table. 
184
distributional analysis for the edited regions, a 
number of feature spaces have been explored and 
tested to show their effectiveness. We observed a 
43.98% relative error reduction on F-scores for the 
baseline with punctuation in both training and 
testing [Charniak and Johnson 2001]. Compared 
with the reported best result, the same approach 
produced a 20.44% of relative error reduction on 
F-scores when punctuation is ignored in training 
and testing data [Johnson and Charniak 2004]. The 
inclusion of both hierarchical POS tags and the 
relaxation for rough copy definition gives large 
additive improvements, and their combination has 
contributed to nearly half of the gain for the test 
set with punctuation and about 60% of the gain for 
the data without punctuation.  
 
Future research would include the use of other 
features, such as prosody, and the integration of 
the edited region identification with parsing.   
6 Acknowledgement 
This work has been done while the first author is 
working at the Research and Technology Center of 
Robert Bosch Corp. The research is partly 
supported by the NIST ATP program. The authors 
would also like to express their thanks to Tess 
Hand-Bender for her proof-reading and Jeremy G. 
Kahn for many useful comments. Nevertheless, all 
the remaining errors are ours. 
References  
John Bear, John Dowding and Elizabeth Shriberg. 1992. 
Integrating Multiple Knowledge Sources for Detection 
and Correction of Repairs in Human-Computer Dialog. 
Proc. Annual Meeting of the Association for 
Computational Linguistics. 1992. 
 
Charniak, Eugene and Mark Johnson. 2001. Edit 
Detection and Parsing for Transcribed Speech. Proc. of 
the 2nd Meeting of the North American Chapter of the 
Association for Computational Linguistics, pp 118-126. 
 
Collins, M. 2000. Discriminative reranking for natural 
language parsing. Proc.  ICML 2000. 
 
Engel, Donald, Eugene Charniak, and Mark Johnson. 
2002. Parsing and Disfluency Placement. Proc.  
EMNLP, pp 49-54, 2002.  
 
Godfrey, J.J., Holliman, E.C. and McDaniel, J. 
SWITCHBOARD: Telephone speech corpus for 
research and development, Proc. ICASSP, pp 517-520, 
1992. 
 
Heeman, Peter, and James Allen. 1994.  Detecting and 
Correcting Speech Repairs. Proc. of the annual meeting 
of the Association for Computational Linguistics. Las 
Cruces, New Mexico,  pp 295-302, 1994.  
 
Johnson, Mark, and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. Proc. of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics. 
 
Kahn, Jeremy G., Mari Ostendorf, and Ciprian Chelba. 
2004. Parsing Conversational Speech Using Enhanced 
Segmentation. Proc. of HLT-NAACL, pp 125-138, 2004. 
 
Kahn, Jeremy G., Matthew Lease, Eugene Charniak, 
Mark Johnson and Mari Ostendorf 2005. Effective Use 
of Prosody in Parsing Conversational Speech. Proc. 
EMNLP, 2005. 
 
Liu, Yang, Elizabeth Shriberg, Andreas Stolcke, 
Barbara Peskin, Jeremy Ang, Dustin Hillard, Mari 
Ostendorf, Marcus Tomalin, Phil Woodland, Mary 
Harper. 2005. Structural Metadata Research in the 
EARS Program. Proc. ICASSP, 2005. 
 
Liu, Yang, Elizabeth Shriberg, Andreas Stolcke. 2003. 
Automatic disfluency identification in conversational 
speech using multiple knowledge sources Proc. 
Eurospeech, 2003 
 
Ostendorf, Mari, Jeremy G. Kahn, Darby Wong, Dustin 
Hillard, and William McNeill. Leveraging Structural 
MDE in Language Processing. EARS RT04 Workshop, 
2004. 
 
Robert E. Schapire and Yoram Singer, 1999. Improved 
Boosting Algorithms Using Confidence-rated 
Predictions. Machine Learning 37(3): 297-336, 1999. 
 
Shriberg, Elizabeth. 1994. Preliminaries to a Theory of 
Speech Disfluencies. Ph.D. Thesis. UC Berkeley,1994.  
 
Shriberg, Elizabeth. 1996. Disfluencies in Switchboard. 
Proc. of ICSLP. 1996. 
Young, S. R. and Matessa, M. (1991). Using pragmatic 
and semantic knowledge to correct parsing of spoken 
language utterances. Proc. Eurospeech 91, Genova, 
Italy. 
185
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 28?35,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Interactive Question Answering and Constraint Relaxation
in Spoken Dialogue Systems
Sebastian Varges
CSLI
Stanford University
Stanford, CA 94305, USA
varges@stanford.edu
Fuliang Weng, Heather Pon-Barry
Research and Technology Center
Robert Bosch Corporation
4009 Miranda Ave, Palo Alto, CA, USA
fuliang.weng, heather.pon-barry
@rtc.bosch.com
Abstract
We explore the relationship between ques-
tion answering and constraint relaxation in
spoken dialog systems. We develop dia-
logue strategies for selecting and present-
ing information succinctly. In particular,
we describe methods for dealing with the
results of database queries in information-
seeking dialogs. Our goal is to structure
the dialogue in such a way that the user is
neither overwhelmed with information nor
left uncertain as to how to refine the query
further. We present evaluation results ob-
tained from a user study involving 20 sub-
jects in a restaurant selection task.
1 Introduction
Information presentation is an important issue
when designing a dialogue system. This is espe-
cially true when the dialogue system is used in a
high-stress environment, such as driving a vehi-
cle, where the user is already occupied with the
driving task. In this paper, we explore efficient
dialogue strategies to address these issues, and
present implemented knowledge management, di-
alogue and generation components that allow cog-
nitively overloaded users ? see (Weng et al, 2004),
for example ? to obtain information from the di-
alogue system in a natural way. We describe a
knowledge manager that provides factual and on-
tological information, a content optimizer that reg-
ulates the amount of information, and a genera-
tor that realizes the selected content. The domain
data is divided between domain-specific ontolo-
gies and a database back-end. We use the system
for both restaurant selection and MP3 player tasks,
and conducted experiments with 20 subjects.
There has been substantial previous work on
information presentation in spoken dialogue sys-
tems. (Qu and Green, 2002) also present a
constraint-based approach to cooperative informa-
tion dialogue. Their experiments focus on over-
constrained queries, whereas we also deal with un-
derconstrained ones. Moreover, we guide the user
through the dialogue by making suggestions about
query refinements, which serve a similar ro?le to
the conditional responses of (Kruijff-Korbayova et
al., 2002). (Hardy et al, 2004) describe a dialogue
system that uses an error-correcting database man-
ager for matching caller-provided information to
database entries. This allows the system to se-
lect the most likely database entry, but, in contrast
to our approach, does not modify constraints at
a more abstract level. In contrast to all the ap-
proaches mentioned above, our language gener-
ator uses overgeneration and ranking techniques
(Langkilde, 2000; Varges and Mellish, 2001).
This facilitates variation and alignment with the
user utterance.
A long-standing strand of research in NLP is
in natural language access to databases (Androut-
sopoulos et al, 1995). It mainly focused on map-
ping natural language input to database queries.
Our work can be seen as an extension of this work
by embedding it into a dialogue system and al-
lowing the user to refine and relax queries, and
to engage in clarification dialogs. More recently,
work on question answering (QA) is moving to-
ward interactive question answering that gives the
user a greater role in the QA process (HLT, forth-
coming). QA systems mostly operate on free text
whereas we use a relational database. (Thus, one
needs to ?normalize? the information contained in
free text to use our implemented system without
further adaption.)
28
In the following section, we give an overview
of the dialogue system. We then describe the
knowledge management, dialogue and generation
components in separate sections. In section 6 we
present evaluation results obtained from a user
study. This is followed by a discussion section and
conclusions.
2 System architecture
Our dialogue system employs the following archi-
tecture: the output of a speech recognizer (Nu-
ance, using a statistical language model) is ana-
lyzed by both a general-purpose statistical depen-
dency parser and a (domain-specific) topic classi-
fier. Parse trees and topic labels are matched by
the ?dialogue move scripts? of the dialogue man-
ager (Mirkovic and Cavedon, 2005; Weng et al,
2005). The scripts serve to license the instantia-
tion of dialogue moves and their integration into
the ?dialogue move tree.? The use of dialogue
move scripts is motivated by the need to quickly
tailor the system to new domains: only the scripts
need to be adapted, not the underlying machinery
implemented in Java. The scripts define short se-
quences of dialog moves, for example a command
move (?play song X?) may be followed either by
a disambiguation question or a confirmation that
the command will be executed. A dialogue pro-
ceeds by integrating such scripted sequences into
the dialogue move tree, yielding a relatively ?flat?
dialogue structure.
Query constraints are built by dialogue move
scripts if the parse tree matches input patterns
specified in the scripts. These query constraints
are the starting point for the processing strategies
described in this paper. The dialogue system is
fully implemented and has been used in restau-
rant selection and MP3 player tasks. There are 41
task-independent, generic dialogue move scripts,
52 restaurant selection scripts and 89 MP3 player
scripts. The examples in this paper are mostly
taken from the restaurant selection task.
3 Knowledge and Content management
The Knowledge Manager (KM) controls access to
domain knowledge that is structured according to
domain-dependent ontologies. The KM makes use
of OWL, a W3C standard, to represent the onto-
logical relationships between domain entities. The
knowledge base can be dynamically updated with
new instances at any point. In a typical interac-
tion, the Dialog Manager converts a user?s query
into a semantic frame (i.e., a set of semantic con-
straints) and sends this to the KM via the content
optimizer. For example, in the Restaurant domain,
a request such as ?I want to find an inexpensive
Japanese restaurant that takes reservations? results
in the semantic frame below, where Category is a
system property, and the other constraints are in-
herited properties of the Restaurant class:
(1) system:Category = restaurant:Restaurant
restaurant:PriceLevel = 0-10
restaurant:Cuisine = restaurant:japanese
restaurant:Reservations = yes
In addition to the KM module, we employ a
Content Optimization (CO) module that acts as
an intermediary between dialogue and knowledge
management during the query process. It receives
semantic frames from the Dialogue Manager, re-
vises the semantic frames if necessary (see below),
and queries the Knowledge Manager.
The content optimizer also resolves remaining
ambiguities in the interpretation of constraints.
For example, if the user requests an unknown cui-
sine type, the otherwise often accurate classifier
will not be able to provide a label since it oper-
ates under a closed-world assumption. In contrast,
the general purpose parser may be able to pro-
vide an accurate syntactic analysis. However, the
parse still needs to be interpreted by the content
optimizer which has the domain-specific knowl-
edge to determine that ?Montenegrin restaurant?
is a cuisine constraint rather than a service level
constraint, for example. (See also section 7).
Depending on the items in the query result set,
configurable properties, and (potentially) a user
model, the CO module selects and performs an ap-
propriate optimization strategy. To increase porta-
bility, the module contains a library of domain-
independent strategies and makes use of external
configuration files to tailor it to specific domains.
The CO module can modify constraints de-
pending on the number of items in the result
set, the system ontology, and information from
a user model. Constraints can be relaxed, tight-
ened, added or removed. The manner in which
a constraint is modified depends on what kind of
values it takes. For example, for the Cuisine
constraint, values are related hierarchically (e.g.,
Chinese, Vietnamese, and Japanese are all sub-
types of Asian), whereas PriceLevel values are
linear (e.g., cheap, moderate, expensive), and
acceptsCreditCards values are binary (e.g., ac-
29
cepted or not accepted).
If the original query returns no results, the con-
tent optimizer selects a constraint to modify and
then attempts to relax the constraint value. If re-
laxation is impossible, it removes the constraint
instead. Constraint relaxation makes use of the
ontological relationships in the knowledge base.
For example, relaxing a Cuisine constraint entails
replacing it with its parent-concept in the domain
ontology. Relaxing a linear constraint entails re-
placing the current value with an adjacent value.
Relaxing a binary constraint entails replacing the
current value with its opposite value.
Based on the ontological structures, the content
optimizer also calculates statistics for every set of
items returned by the knowledge manager in re-
sponse to a user?s query. If the result set is large,
these figures can be used by the dialogue manager
to give meaningful responses (e.g., in the MP3 do-
main, ?There are 85 songs. Do you want to list
them by a genre such as Rock, Pop, or Soul??).
The content optimizer also produces constraints
that represent meta-knowledge about the ontology,
for example, in response to a user input ?What
cuisines are there??:
(2) rdfs:subClassOf = restaurant:Cuisine
The processing modules described in the next
sections can use meta-level constraints in similar
ways to object-level constraints (see (1)).
4 Dialogue strategies for dealing with
query results
In the following two sections, we describe how
our dialogue and generation strategies tie in with
the choices made by the content optimizer. Con-
sider the following discourse-initial interaction for
which the semantic frame (1) is constructed:
(3)
U: i want to find an inexpensive Japanese
restaurant that takes reservations
S: I found 9 inexpensive Japanese
restaurants that take reservations .
Here are the first few :
S: GINZA JAPANESE RESTAURANT
S: OKI SUSHI CAFE
S: YONA SUSHI
S: Should I continue?
The example query has a relatively small result
set which can be listed directly. This is not always
the case, and thus we need dialogue strategies that
deal with different result set sizes. For example, it
does not seem sensible to produce ?I found 2500
restaurants. Here are the first few: ...?. At what
point does it become unhelpful to list items? We
do not have a final answer to this question ? how-
ever, it is instructive that the (human) wizard in
our data collection experiments did not start list-
ing when the result set was larger than about 10
items. In the implemented system, we define di-
alogue strategies that are activated at adjustable
thresholds.
Even if the result set is large and the system
does not list any result items, the user may still
want to see some example items returned for the
query. This observation is based on comments
by subjects in experimental dry-runs that in some
cases it was difficult to obtain any query result at
all. For example, speech recognition errors may
make it difficult to build up a sufficiently complex
query. In response to this, we always give some
example items even if the result set is large. (An
alternative would be to start listing items after a
certain number of dialogue turns.) Furthermore,
the system should encourage the user to refine the
query by suggesting constraints that have not been
used yet. This is done by maintaining a list of con-
straints in the generator that is used up as the di-
alogue progresses. This list is roughly ordered by
how likely the constraint will be useful. For exam-
ple, using cuisine type is suggested before propos-
ing to ask for information about reservations or
credit cards.
In our architecture, information flows from the
CO module to the generator (see section 5) via the
dialogue move scripts of the dialogue manager.
These are conditioned on the size of the final re-
sult set and whether or not any modifications were
performed. Table 1 summarizes the main dialogue
strategies. These dialogue strategies represent im-
plicit confirmations and are used if NLU has a high
confidence in its analysis of the user utterance (see
(Varges and Purver, 2006) for more details on our
handling of robustness issues). Small result sets
up to a threshold t1 are listed in a single sentence.
For medium-sized result sets up to a threshold t2,
the system starts listing immediately. For large re-
sult sets, the generator shows example items and
makes suggestions as to what constraint the user
may use next. If the CO module performs any con-
straint modification, the first, constraint realizing
sentence of the system turns reflects the modifica-
tion. (?NP-original? and ?NP-optimized? in table 1
are used for brevity and are explained in the next
section.)
30
|resultfinal| mod example realization fexp
s1a 0 no I?m sorry but I found no restaurants on Mayfield Road that serve Mediterranean food. 0
s1b 0 yes I?m sorry but I found no [NP-original]. I did not even find any [NP-optimized]. 0
s2a small: no There are 2 cheap Thai restaurants in Lincoln in my database: Thai Mee Choke and 61
> 0, < t1 Noodle House.
s2b small yes I found no cheap Greek restaurants that have a formal dress code but there are 0
4 inexpensive restaurants that serve other Mediterranean food and have a formal
dress code in my database: ... .
s3a medium: no I found 9 restaurants with a two star rating and a formal dress code that are open 212
>= t1, < t2 for dinner and serve French food. Here are the first ones: ... .
s3b medium yes I found no [NP-original]. However, there are N [NP-optimized]. Here are the first few: ... . 5
s4a large: no I found 258 restaurants on Page Mill Road, for example Maya Restaurant , 300
>= t2 Green Frog and Pho Hoa Restaurant. Would you like to try searching by cuisine?
s4b large yes I found no [NP-original]. However, there are N [NP-optimized]. Would you like to try 16
searching by [Constraint]?
Table 1: Dialogue strategies for dealing with query results (last column explained in sec. 6)
5 Generation
The generator produces turns that verbalize the
constraints used in the database query. This is
important since the system may miss or misinter-
pret constraints, leading to uncertainty for the user
about what constraints were used. For this rea-
son, a generic system response such as ?I found 9
items.? is not sufficient.
The input to the generator consists of the name
of the dialogue move and the relevant instantiated
nodes of the dialogue move tree. From the in-
stantiated move nodes, the generator obtains the
database query result including information about
query modifications. The core of the generator is
a set of productions1 written in the Java Expert
System Shell (Friedman-Hill, 2003). We follow
the bottom-up generation approach for production
systems described in (Varges, 2005) and perform
mild overgeneration of candidate moves, followed
by ranking. The highest-ranked candidate is se-
lected for output.
Productions map individual database con-
straints to phrases such as ?open for lunch?,
?within 3 miles? and ?a formal dress code?, and
recursively combine them into NPs. This includes
the use of coordination to produce ?restaurants
with a 5-star rating and a formal dress code?,
for example. The NPs are integrated into sen-
tence templates, several of which can be combined
to form an output candidate turn. For example,
a constraint realizing template ?I found no [NP-
1Productions are ?if-then? rules that operate over a shared
knowledge base of facts.
original] but there are [NUM] [NP-optimized] in
my database? (see below for further explanation)
can be combined with a follow-up sentence tem-
plate such as ?You could try to look for [NP-
constraint-suggestion]?.
The selection of which sentence template to use
is determined by the dialogue move scripts. Typ-
ically, a move-realizing production produces sev-
eral alternative sentences. On the other hand, the
NP generation rules realize constraints regardless
of the specific dialogue move at hand. This al-
lows us to also use them for clarification ques-
tions based on constraints constructed from classi-
fier information if the parser and associated parse-
matching patterns fail; all that is required is a new
sentence template, for example ?Are you looking
for [NP]??. We currently use 102 productions
overall in the restaurant and MP3 domains, 38 of
them to generate NPs that realize 19 possible input
constraints (for both domains).
The decision of the CO module to relax or re-
move constraints also affects the generator: there
are two sets of constraints, an ?original? one di-
rectly constructed from the user utterance, and an
?optimized? one used by the KM module to obtain
the query result (see section 3). In case of con-
straint modifications, these two sets are not identi-
cal but often overlapping. To avoid generating sep-
arate sets of NPs independently for the two con-
straint sets, we assign unique indices to the con-
straints and hand the generator two index sets as
targets of NP generation. We overgenerate NPs
and check their index sets before integrating them
into sentence templates.
31
turn speaker utterance
1.1 User i need a restaurant with a five star food rating excellent service
1.2 System I found 425 restaurants with a five star rating and excellent service , for example
EL CAPITAN FISH MARKET , K-NINE COUNTRY CLUB , and KAISERHOF GERMAN
AMERICAN . You could try to look for cuisine type .
1.3 User which restaurants take reservations
1.4 System I found 353 restaurants that have a five star rating , have excellent service and
take reservations , for example GRAND CENTRAL CAFE , CHASIN CHICKEN , and
CHON WOO RESTAURANT . Would you like to try searching by price ?
Figure 1: Example dialogue 1 from system evaluation using restaurant selection task
Ranking of candidate output moves is done by
using a combination of factors. First, the ranker
computes an alignment score for each candidate,
based on its ngram-based overlap with the user
utterance. For example, this allows us to prefer
?restaurants that serve Chinese food? over ?Chi-
nese restaurants? if the user used a wording more
similar to the first. We note that the Gricean
Maxim of Brevity, applied to NLG in (Dale and
Reiter, 1995), suggests a preference for the sec-
ond, shorter realization. However, if the user
thought it necessary to use ?serves?, maybe to
avoid confusion of constraints or even to correct an
earlier mislabeling, then the system should make
it clear that it understood the user correctly by
using those same words, thus preferring the first
realization. Mild overgeneration combined with
alignment also allows us to map the constraint
PriceLevel=0-10 in example (1) above to both
?cheap? and ?inexpensive?, and use alignment to
?play back? the original word choice to the user.
As these examples show, using alignment for rank-
ing in NLG allows one to employ overgeneration
techniques even in situations where no corpus data
is available.2
Second, ranking uses a variation score to ?cycle?
over sentence-level paraphrases. In the extreme
case of repeated identical user inputs, the system
simply chooses one paraphrase after the other, and
starts over when all paraphrases have been used.
Third, we use an ngram filter based on bad
examples ngrams, removing, for example, ?Chi-
nese cheap restaurants? but keeping ?cheap Chi-
nese restaurant.? For generalization, we replace
constraint realizations with semantic tags derived
from the constraint names (except for the head
noun), for example the trigram ?CUISINE PRICE
restaurants?. An alternative is to use a more com-
2However, we do have wizard-of-oz data to inform the
system design (see section 7).
plex grammar formalism to prevent ungrammati-
cal candidate moves.
6 Evaluation
We conducted experimental studies involving 20
subjects in a MP3 player task and 20 subjects in a
restaurant selection task. In the following, we con-
centrate on the restaurant selection task because it
is more challenging for constraint handling and in-
formation presentation.
Each subject in the restaurant selection task
was given 9 scenario descriptions involving 3 con-
straints. Subjects were instructed to use their own
words to find a fitting restaurant. We use a back-
end database of 2500 restaurants containing the
following information for each restaurant: restau-
rant name, cuisine type, city and street names,
service level, rating, whether they accept credit
cards or reservations, price level, open hours, dress
code, additional information (e.g. vegetarian food)
and distance from current location. Some of these
constraints have a fixed number of values (e.g.
service level and dress code), whereas those tak-
ing named entities as values are obviously open-
ended.
Figures 1 and 2 show two of the dialogues from
the experiments. To exemplify the challenges
the system is faced with, user turns are shown
in the form of the output of the speech recog-
nizer (we currently use the best ASR hypothesis
as input to the parser). The task of the first dia-
logue (figure 1) was to find a restaurant that has
the following constraints: [Reservations=yes, Ser-
viceLevel:excellent, Rating=5star]. Despite some
minor speech recognition errors, the system cor-
rectly builds a query for the two constraints pro-
vided by the user in (1.1.) and verbalizes these in
(1.2) to show its understanding of the user input.
It also gives some example items and makes a sug-
gestion. This suggestion is not taken up by the user
32
turn speaker utterance
2.1 User breakfast
2.2 System Are you looking for a restaurant that is open for breakfast ?
2.3 User yes located on bower street
2.4 System I found 71 restaurants on bower street that serve breakfast , for example ,
SOUPLANTATION-TO-GO , RAMAYANI WESTWOOD , and , PHILLY WEST .
You could try to look for cuisine type .
2.5 User which
2.6 System I?m sorry, I do not understand what you mean by: which
2.7 System Please try rephrasing that.
2.8 User casual dress
2.9 System There are 51 restaurants on bower street that serve breakfast and have a casual
dress code in my database , for example , BERTY?S , LABOTETEGA , and ,
JAVAN RESTAURANT . Would you like to try searching by price ?
2.10 User british
2.11 System I found no restaurants on bower street that serve british food , have a casual
dress code and are open for breakfast but there are 8 restaurants on bower street that are open for
breakfast , serve European food and have a casual dress code in my database . Here are the first few :
2.12 System HOTEL BEL-AIR
Figure 2: Example dialogue 2 from system evaluation using restaurant selection task
? in fact, we find that suggestions are generally not
taken up by the user. We believe this is due to the
nature of the tasks, which specified exactly which
criteria to match. On the other hand, in more open
application scenarios, where users may not know
what questions can be asked, suggestions may be
useful. In (1.3) the user issues a sub-query that
further constrains the result set. By again summa-
rizing the constraints used, the system confirms in
(1.4) that it has interpreted the new constraint as a
revision of the previous query. The alternative is
to start a new query, which would be wrong in this
context.
The task of the second dialogue, figure 2, was to
find a restaurant that meets the constraints [Busi-
nessHours:breakfast, StreetName=?bower street?,
DressCode=casual]. This user tends to give
shorter, keyword-style input to the system (2.1,
2.8). In (2.3), the user reacts to a clarification
question and adds another constraint which the
system summarizes in (2.4). (2.5) is an ASR er-
ror which the system cannot handle (2.6, 2.7). The
user constraint of (2.8) is correctly used to revise
the query (2.9), but ?british? (2.10) is another ASR
error that leads to a cuisine constraint not intended
in the scenario/by the user. This additional con-
straint yields an empty result set, from which the
system recovers automatically by relaxing the hi-
erarchically organized cuisine constraint to ?Eu-
ropean food?. In (2.11) the system uses dialogue
strategy s3b for medium-sized result sets with con-
straint modifications (section 4). The result of
both dialogues is that all task constraints are met.
We conducted 20 experiments in the restaurant
domain, 2 of which were restarted in the middle.
Overall, 180 tasks were performed involving 1144
user turns and 1818 system turns. Two factors con-
tributing to the higher number of system turns are
a) some system turns are counted as two turns,
such as 2.6, 2.7 in figure 2, and b) restaurants in
longer enumerations of result items are counted as
individual turns. On average, user utterances are
significantly shorter than system utterances (4.9
words, standard deviation ? = 3.82 vs 15,4 words,
? = 13.53). This is a result of the ?constraint sum-
maries? produced by the generator. The high stan-
dard deviation of the system utterances can be ex-
plained by the above-mentioned listing of individ-
ual result items (e.g. utterance (2.12) in figure 2).
We collected usage frequencies for the dia-
logue strategies presented in section 4: there was
no occurrence of empty final result sets (strat-
egy s1a/b) because the system successfully re-
laxed constraints if it initially obtained no results.
Strategy s2a (small result sets without modifica-
tions) was used for 61 inputs, i.e. constraint sets
constructed from user utterances. Strategy s3a/b
(medium-sized result sets) was used for 217 times
and required constraint relaxations in 5 cases.
Strategy s4a/b (large result sets) was used for
33
316 inputs and required constraint relaxations in
16 cases. Thus, the system performed constraint
modifications in 21 cases overall. All of these
yielded non-empty final result sets. For 573 in-
puts, no modification was required. There were no
empty final result set despite modifications.
On average, the generator produced 16 output
candidates for inputs of two constraints, 160 can-
didates for typical inputs of 3 constraints and 320
candidates for 4 constraints. Such numbers can
easily be handled by simply enumerating candi-
dates and selecting the ?best? one.
Task completion in the experiments was high:
the subjects met al target constraints in 170 out of
180 tasks, i.e. completion rate was 94.44%. An
error analysis revealed that the reasons for only
partially meeting the task constraints were varied.
For example, in one case a rating constraint (?five
stars?) was interpreted as a service constraint by
the system, which led to an empty result set. The
system recovered from this error by means of con-
straint relaxation but the user seems to have been
left with the impression that there are no restau-
rants of the desired kind with a five star rating.
7 Discussion
Based on wizard-of-oz data, the system alter-
nates specific and unspecific refinement sugges-
tions (?You could search by cuisines type? vs ?Can
you refine your query??). Furthermore, many of
the phrases used by the generator are taken from
wizard-of-oz data too. In other words, the sys-
tem, including the generator, is informed by em-
pirical data but does not use this data directly (Re-
iter and Dale, 2000). This is in contrast to genera-
tion systems such as the ones described in (Langk-
ilde, 2000) and (Varges and Mellish, 2001).
Considering the fact that the domain ontology
and database schema are known in advance, it is
tempting to make a closed world assumption in
the generator (which could also help system de-
velopment and testing). However, this seems too
restrictive: assume, for example, that the user has
asked for Montenegrin food, which is an unknown
cuisine type, and that the statistical parser com-
bined with the parse-matching patterns in the di-
alogue manager has labeled this correctly. The
content optimization module will remove this con-
straint since there is no Montenegrin restaurant in
the database. If we now want to generate ?I did not
find any restaurants that serve Montenegrin food
...?, we do need to be able to use generation input
that uses unseen attribute-value pairs. The price
one has to pay for this increased robustness and
flexibility is, of course, potentially bad output if
NLU mislabels input words. More precisely, we
find that if any one of the interpretation modules
makes an open-world assumption, the generator
has to do as well, at least as long as we want to
verbalize the output of that module.
7.1 Future work
Our next application domain will be in-car naviga-
tion dialogues. This will involve dialogues that de-
fine target destinations and additional route plan-
ning constraints. It will allow us to explore the
effects of cognitive constraints due to changing
driving situations on dialogue behavior. The nav-
igation domain may also affect the point of inter-
action between dialogue system and external de-
vices: we may query a database to disambiguate
proper names such as street names as soon as these
are mentioned by the user, but start route planning
only when all planning constraints are collected.
An option for addressing the current lack of a
user model is to extend the work in (Cheng et al,
2004). They select the level of detail to be com-
municated to the user by representing the driver?s
route knowledge to avoid repeating known infor-
mation.
Another avenue of future research is to automat-
ically learn constraint relaxation strategies from
(appropriately annotated) evaluation data. User
modeling could be used to influence the order in
which refinement suggestions are given and deter-
mine the thresholds for the information presenta-
tion moves described in section 4.
One could handle much larger numbers of gen-
eration candidates either by using packing (Langk-
ilde, 2000) or by interleaving rule-based genera-
tion with corpus-based pruning (Varges and Mel-
lish, 2001) if complexity should become an issue
when doing overgeneration.
8 Conclusions
We described strategies for selecting and present-
ing succinct information in spoken dialogue sys-
tems. Verbalizing the constraints used in a query is
crucial for robustness and usability ? in fact, it can
be regarded as a special case of providing feed-
back to the user about what the system has heard
and understood (see (Traum, 1994), for example).
34
The specific strategies we use include ?backing-
off? to more general constraints (by the system)
or suggesting query refinements (to be requested
explicitly by the user). Our architecture is config-
urable and open: it can be parametrized by em-
pirically derived values and extended by new con-
straint handling techniques and dialogue strate-
gies. Constraint relaxation techniques have widely
been used before, of course, for example in syn-
tactic and semantic processing. The presented pa-
per details how these techniques, when used at the
content determination level, tie in with dialogue
and generation strategies. Although we focussed
on the restaurant selection task, our approach is
generic and can be applied across domains, pro-
vided that the dialogue centers around accessing
and selecting potentially large amounts of factual
information.
Acknowledgments This work is supported by
the US government?s NIST Advanced Technology
Program. Collaborating partners are CSLI, Robert
Bosch Corporation, VW America, and SRI Inter-
national. We thank the many people involved in
system design, development and evaluation, and
the reviewers of this paper.
References
Ion Androutsopoulos, G.D. Ritchie, and P. Thanisch.
1995. Natural Language Interfaces to Databases ?
An Introduction. Natural Language Engineering,
1(1):29?81.
Hua Cheng, Lawrence Cavedon, and Robert Dale.
2004. Generating Navigation Information Based on
the Driver?s Route Knowledge. In Proceedings of
the Coling 2004 Workshop on Robust and Adap-
tive Information Processing for Mobile Speech In-
terfaces, pages 31?38, Geneva, Switzerland.
Robert Dale and Ehud Reiter. 1995. Computational
Interpretations of the Gricean Maxims in the Gener-
ation of Referring Expressions. Cognitive Science,
19:233?263.
Ernest Friedman-Hill. 2003. Jess in Action: Java
Rule-Based Systems. Manning Publications.
Hilda Hardy, Tomek Strzalkowski, Min Wu, Cristian
Ursu, Nick Webb, Alan Biermann, R. Bryce Inouye,
and Ashley McKenzie. 2004. Data-driven strategies
for an automated dialogue system. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
71?78, Barcelona, Spain, July.
forthcoming. Proceedings of the workshop on Interac-
tive Question Answering at HLT-NAACL 2006.
Ivana Kruijff-Korbayova, Elena Karagjosova, and Stef-
fan Larsson. 2002. Enhancing collaboration with
conditional responses in information-seeking dia-
logues. In Proc. of 6th workshop on the semantics
and pragmatics of dialogue (EDILOG-02).
Irene Langkilde. 2000. Forest-based Statistical Sen-
tence Generation. In Proc NAACL-00, pages 170?
177.
Danilo Mirkovic and Lawrence Cavedon. 2005. Prac-
tical Plug-and-Play Dialogue Management. In Pro-
ceedings of the 6th Meeting of the Pacific Associa-
tion for Computational Linguistics (PACLING).
Yan Qu and Nancy Green. 2002. A Constraint-based
Approach for Cooperative Information-Seeking Di-
alogue. In Proceedings of the International Work-
shop on Natural Language Generation (INLG-02).
Ehud Reiter and Robert Dale. 2000. Building Applied
Natural Language Generation Systems. Cambridge
University Press, Cambridge, UK.
David Traum. 1994. A Computational Theory
of Grounding in Natural Language Conversation.
Ph.D. thesis, Computer Science Dept., U. Rochester.
Sebastian Varges and Chris Mellish. 2001. Instance-
based Natural Language Generation. In Proc.
NAACL-01.
Sebastian Varges and Matthew Purver. 2006. Ro-
bust language analysis and generation for spoken di-
alogue systems (short paper). In Proceedings of the
ECAI 06 Workshop on the Development and Evalu-
ation of Robust Spoken Dialogue Systems.
Sebastian Varges. 2005. Chart generation using pro-
duction systems (short paper). In Proc. of 10th Eu-
ropean Workshop On Natural Language Generation.
Fuliang Weng, L. Cavedon, B. Raghunathan,
D. Mirkovic, H. Cheng, H. Schmidt, H. Bratt,
R. Mishra, S. Peters, L. Zhao, S. Upson, E. Shriberg,
and C. Bergmann. 2004. Developing a conversa-
tional dialogue system for cognitively overloaded
users. In Proceedings of the International Congress
on Intelligent Transportation Systems (ICSLP).
Fuliang Weng, Lawrence Cavedon, Badri Raghu-
nathan, Danilo Mirkovic, Ben Bei, Heather Pon-
Barry, Harry Bratt, Hua Cheng, Hauke Schmidt, Ro-
hit Mishra, Brian Lathrop, Qi Zhang, Tobias Schei-
deck, Kui Xu, Tess Hand-Bender, Stanley Peters,
Liz Shriberg, and Carsten Bergmann. 2005. A Flex-
ible Conversational Dialog System for MP3 Player.
In demo session of HLT-EMNLP 2005.
35
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 164?171,
Columbus, June 2008. c?2008 Association for Computational Linguistics
 
Abstract 
We propose to use user simulation for testing 
during the development of a sophisticated dia-
log system. While the limited behaviors of the 
state-of-the-art user simulation may not cover 
important aspects in the dialog system testing, 
our proposed approach extends the functional-
ity of the simulation so that it can be used at 
least for the early stage testing before the sys-
tem reaches stable performance for evaluation 
involving human users. The proposed ap-
proach includes a set of evaluation measures 
that can be computed automatically from the 
interaction logs between the user simulator 
and the dialog system. We first validate these 
measures on human user dialogs using user 
satisfaction scores. We also build a regression 
model to estimate the user satisfaction scores 
using these evaluation measures. Then, we 
apply the evaluation measures on a simulated 
dialog corpus trained from the real user cor-
pus. We show that the user satisfaction scores 
estimated from the simulated corpus are not 
statistically different from the real users? satis-
faction scores.  
1 Introduction 
 Spoken dialog systems are being widely used in 
daily life. The increasing demands of such systems 
require shorter system development cycles and 
better automatic system developing techniques. As 
a result, machine learning techniques are applied to 
learn dialog strategies automatically, such as rein-
forcement learning (Singh et al, 2002; Williams & 
Young, 2007), supervised learning (Henderson et 
                                                          
* This study was conducted when the author was an intern at 
Bosch RTC. 
al., 2005), etc. These techniques require a signifi-
cant amount of training data for the automatic 
learners to sufficiently explore the vast space of 
possible dialog states and strategies. However, it is 
always hard to obtain training corpora that are 
large enough to ensure that the learned strategies 
are reliable. User simulation is an attempt to solve 
this problem by generating synthetic training cor-
pora using computer simulated users. The simu-
lated users are built to mimic real users' behaviors 
to some extent while allowing them to be pro-
grammed to explore unseen but still possible user 
behaviors. These simulated users can interact with 
the dialog systems to generate large amounts of 
training data in a low-cost and time-efficient man-
ner. Many previous studies (Scheffler, 2002; 
Pietquin, 2004) have shown that the dialog strate-
gies learned from the simulated training data out-
perform the hand-crafted strategies. There are also 
studies that use user simulation to train speech rec-
ognition and understanding components (Chung, 
2004). 
    While user simulation is largely used in dialog 
system training, it has only been used in limited 
scope for testing specific dialog system compo-
nents in the system evaluation phase (L?pez-C?zar 
et al, 2003; Filisko and Seneff, 2006). This is 
partly because the state-of-the-art simulated users 
have quite limited abilities in mimicking human 
users' behaviors and typically over-generate possi-
ble dialog behaviors. This is not a major problem 
when using simulated dialog corpus as the training 
corpus for dialog strategy learning because the 
over-generated simulation behaviors would only 
provide the machine learners with a broader dialog 
state space to explore (Ai et al, 2007). However, 
realistic user behaviors are highly desired in the 
testing phase because the systems are evaluated 
and adjusted based on the analysis of the dialogs 
generated in this phase. Therefore, we would ex-
User Simulation as Testing for Spoken Dialog Systems 
 
Hua Ai* Fuliang Weng 
Intelligent Systems Program Research and Technology Center 
University of Pittsburgh Robert Bosch LLC 
210 S. Bouquet St., Pittsburg, PA 15260 4009 Miranda Ave., Palo Alto, CA 94304 
Hua@cs.pitt.edu Fuliang.weng@us.bosch.com 
 
164
pect that these user behaviors are what we will see 
in the final evaluation with human users. In this 
case, any over-generated dialog behaviors may 
cause the system to be blamed for untargeted func-
tions. What is more, the simulated users cannot 
provide subjective user satisfaction feedback 
which is also important for improving the systems. 
Since it is expensive and time-consuming to test 
every version of the system with a significant 
amount of paid subjects, the testing during the de-
velopment is typically constrained to a limited 
number of users, and often, to repeated users who 
are colleagues or developers themselves. Thus, the 
system performance is not always optimized for 
the intended users.  
Our ultimate goal is to supplement human test-
ing with simulated users during the development to 
speed up the system development towards desired 
performance. This would be especially useful in 
the early development stage, since it would avoid 
conducting tests with human users when they may 
feel extremely frustrated due to the malfunction of 
the unstable system. 
As a first attempt, we try to extend the state-of-
the-art user simulation by incorporating a set of 
new but straightforward evaluation measures for 
automatically assessing the dialog system perform-
ance. These evaluation measures focus on three 
basic aspects of task-oriented dialog systems: un-
derstanding ability, efficiency, and the appropri-
ateness of the system actions. They are first 
applied on a corpus generated between a dialog 
system and a group of human users to demonstrate 
the validity of these measures with the human us-
ers' satisfaction scores. Results show that these 
measures are significantly correlated with the hu-
man users' satisfactions. Then, a regression model 
is built to predict the user satisfaction scores using 
these evaluation measures. We also apply the re-
gression model on a simulated dialog corpus 
trained from the above real user corpus, and show 
that the user satisfaction scores estimated from the 
simulated dialogs do not differ significantly from 
the real users? satisfaction scores. Finally, we con-
clude that these evaluation measures can be used to 
assess the system performance based on the esti-
mated user satisfaction. 
2 User Simulation Techniques  
Most user simulation models are trained from dia-
log corpora generated by human users. Earlier 
models predict user actions based on simple rela-
tions between the system actions and the following 
user responses. (Eckert et al, 1997) first suggest a 
bigram model to predict the next user's action 
based on the previous system's action. (Levin et al, 
2000) add constraints to the bigram model to ac-
cept the expected dialog acts only. However, their 
basic assumption of making the next user's action 
dependent only on the system's previous action is 
oversimplified. Later, many studies model more 
comprehensive user behaviors by adding user goals 
to constrain the user actions (Scheffler, 2002; Piet-
quin, 2004). These simulated users mimic real user 
behaviors in a statistical way, conditioning the user 
actions on the user goals and the dialog contexts. 
More recent research defines agenda for simulated 
users to complete a set of settled goals (Schatz-
mann et al, 2007). This type of simulated user up-
dates the agenda and the current goal based on the 
changes of the dialog states. 
In this study, we build a simulated user similar 
to (Schatzmann et al, 2007) in which the simulated 
user keeps a list of its goals and another agenda of 
actions to complete the goals. In our restaurant se-
lection domain, the users? tasks are to find a de-
sired restaurant based on several constraints 
specified by the task scenarios. We consider these 
restaurant constraints as the goals for the simulated 
user. At the beginning of the dialog, the simulated 
user randomly generates an agenda for the list of 
the ordered goals corresponding to the three con-
straints in requesting a restaurant. An agenda con-
tains multiple ordered items, each of which 
consists of the number of constraints and the spe-
cific constraints to be included in each user utter-
ance. During the dialog, the simulated user updates 
its list of goals by removing the constraints that 
have been understood by the system. It also re-
moves from its agenda the unnecessary actions that 
are related to the already filled goals while adding 
new actions. New actions are added according to 
the last system?s question (such as requesting the 
user to repeat the last utterance) as well as the 
simulated user?s current goals. The actions that 
address the last system?s question are given higher 
priorities then other actions in the agenda. For ex-
ample, if the dialog system fails to understand the 
last user utterance and thus requests a clarification, 
the simulated user will satisfy the system?s request 
165
before moving on to discuss a new constraint. The 
simulated user updated the agenda with the new 
actions after each user turn.  
The current simulated user interacts with the 
system on the word level. It generates a string of 
words by instantiating its current action using pre-
defined templates derived from previously col-
lected corpora with real users. Random lexical 
errors are added to simulate a spoken language 
understanding performance with a word error rate 
of 15% and a semantic error rate of 11% based on 
previous experience (Weng et al, 2006). 
3 System and Corpus  
CHAT (Conversational Helper for Automotive 
Tasks) is a spoken dialog system that supports na-
vigation, restaurant selection and mp3 player ap-
plications. The system is specifically designed for 
users to interact with devices and receive services 
while performing other cognitive demanding, or 
primary tasks such as driving (Weng et al, 2007). 
CHAT deploys a combination of off-the-shelf 
components, components used in previous lan-
guage applications, and components specifically 
developed as part of this project. The core compo-
nents of the system include a statistical language 
understanding (SLU) module with multiple under-
standing strategies for imperfect input, an informa-
tion-state-update dialog manager (DM) that 
handles multiple dialog threads and mixed initia-
tives (Mirkovic and Cavedon, 2005), a knowledge 
manager (KM) that controls access to ontology-
based domain knowledge, and a content optimizer 
that connects the DM and the KM for resolving 
ambiguities from the users' requests, regulating the 
amount of information to be presented to the user, 
as well as providing recommendations to users. In 
addition, we use Nuance 8.51 with dynamic gram-
mars and classbased n-grams, for speech recogni-
tion, and Nuance Vocalizer 3.0 for text-to-speech 
synthesis (TTS). However, the two speech compo-
nents, i.e., the recognizer and TTS are not used in 
the version of the system that interacts with the 
simulated users.  
The CHAT system was tested for the navigation 
domain, the restaurant selection and the MP3 mu-
sic player. In this study, we focus on the dialog 
corpus collected on the restaurant domain only. A 
                                                          
1 See http://www.nuance.com for details. 
small number of human users were used as dry-run 
tests for the system development from November, 
2005 to January, 2006. We group the adjacent dry-
runs to represent system improvement stages on a 
weekly basis. Table 1 shows the improvement 
stages, the dry-run dates which each stage in-
cludes, and the number of subjects tested in each 
stage. A final evaluation was conducted during 
January 19-31, 2006, without any further system 
modifications. This final evaluation involved 20 
paid subjects who were recruited via internet ad-
vertisement. 
Only the users in the final evaluation completed 
user satisfaction surveys after interacting with the 
system. In the survey, users were asked to rate the 
conversation from 6 perspectives, each on a 5-
point scale: whether the system was easy to use, 
whether the system understood the user well, 
whether the interaction pattern was natural, 
whether the system's actions were appropriate, 
whether the system acted as expected, and whether 
the user was willing to use the system on a regular 
base. A user satisfaction score was computed as 
the average of the 6 ratings. 
 
 
Nine tasks of restaurant selections were used in 
both dry-runs and the final evaluation using 12 
constraints in total (e.g., cuisine type, price level, 
location). These 12 constraints are spread across 
the nine tasks evenly with three constraints per 
task. In addition, each task is carefully worded 
based on the task-constrained and language-
unconstrained guideline. In other words, we want 
the users to form an intended mental context while 
trying to prevent them from copying the exact 
phrasing in the task description. During the dry-
runs, the users randomly pick three to four tasks to  
Stage Dry-run Dates Users
1 11/21/05, 11/22/05 2 
2 11/30/05, 12/1/05, 12/2/05 3 
3 12/7/05, 12/8/05 2 
4 12/13/05, 12/14/05, 12/15/05 5 
5 12/19/05, 12/20/05, 12/21/05 4 
6 12/27/05, 12/28/05 2 
7 1/4/06, 1/5/06 2 
8 1/10/06, 1/11/06, 1/13/06 4 
9 1/16/06, 1/17/06 3 
Table 1: Dry-runs 
166
test the system, while in the final evaluation each 
user is required to complete all of the 9 tasks. As a  
result of the final evaluation in the restaurant do-
main with 2500 restaurants, we reached a task 
completion rate of 94% with a word recognition 
rate of 85%, and a semantic accuracy rate of 89%. 
4 Evaluation Measures  
 In this section, we describe in detail the evaluation 
measures covering three basic aspects of task-
oriented dialog systems: understanding ability, ef-
ficiency, and the appropriateness of the system 
actions.  
4.1 Understanding Ability Measures 
Human-human dialog is a process to reach mutual 
understandings between the dialog partners by ex-
changing information through the dialog. This in-
formation exchanging process also takes place in 
the interaction between users and spoken dialog 
systems. In a task-oriented conversation, the dialog 
system's major task is to understand the users' 
needs in order to provide the right service. In the 
information-state update framework, the system 
continuously updates its information-states during 
the dialog while the users are conveying their re-
quirements. If a misunderstanding occurs, there 
would be a mismatch between the users? require-
ments and the system?s understandings. Thus, the 
error recovery dialog is needed to fix the mis-
matches. The error recovery dialog can be initiated 
either by the system by asking the user to rephrase 
or to repeat the previous utterance, or by the user 
to restate the previous request. 
We use the percent of agreement between the 
system's and the user's understandings (under-
standingAgreement) to measure how well the 
system understands the user. The computation of 
this measure is illustrated through the example dia-
log in Table 2. In this table, the first column shows 
the system utterances and the user utterances re-
ceived by the system. The correct words are shown 
in square brackets immediately after the misunder-
stood words (E.g., in Utterance ?User1?). The sec-
ond column represents semantic content from the 
users? utterances in the form of constraint-value 
pairs based on the system?s understandings. This 
information can be automatically retrieved from 
the system logs. The third column includes the ac-
tion types of the current system/user utterances. 
Since the dialog manager is an information-
updating dialog manager that manages information 
in the format of slots, this column also shows the 
number of slots that are exchanged in the utterance 
and the number of matched slots. In our task do-
main, the user can request information (req_info), 
request the same information again (rereq_info), 
answer an explicit confirmation (exp_confirm), 
and close a dialog (close_dialog). The system can 
provide information (pro_info) or explicitly con-
firms (exp_confirm) the information. Another 
Utterances Constraints Understood by the System 
Action Types & Under-
standing Status 
User1: I'm looking for a five star Thai [Italian] 
restaurant accept credit cards  
Action: req_info 
Slots conveyed: 3 slots 
System1: There are 7 restaurants that have 
a five star rating, accept credit cards, and 
serve Thai food in my database. Here are 
the first few: TeJUS, Slam Corner Cate, 
Setha's Restaurant. Should I continue? 
service level=five star 
cuisine type=Thai 
payment type=credit card 
Action: pro_info 
Slots accepted: 3 slots 
Completely matched: 2 slots 
 
User2: I need to look for Italian restaurant  Action: rereq_info Slot  conveyed: 1 slot 
System2: Are you looking for a restaurant 
that serves Italian food? 
service level=five star 
cuisine type=Italian (unconfirmed) 
payment type=credit card 
Action: exp_confirm 
Slot  updated: 1 slot 
Completely matched: 2.5 slot 
User3: yes  Action: exp_confirm Slot conveyed: 1 slot 
System3: I found 38 restaurants that serve 
Italian food, accept credit cards and have a 
five star rating, for example, Toto Caf? ? 
service level=five star 
cuisine type=Italian (confirmed) 
payment type=credit card 
Action: pro_info 
Slots accepted: 3 slots 
Completely matched: 3 slots 
User4: I'm done.  Action: close_dialog 
Table 2: A Sample Dialog with Annotations. 
      
167
available system action that is not shown in this 
example is to ask the user to repeat/rephrase (re-
phrase), where the user can respond by providing 
the information again (repro_info).  
In our experiment, we measure the understand-
ings between the users and the system by compar-
ing the values of the constraints that are specified 
by the users with their values understood by the 
system. In this dialog, the user specified all con-
straints in the first utterance: 
Service level = Five star 
           Cuisine type = Italian 
  Payment type = Credit card 
    The first system utterance shows that the system 
understood two constraints but misunderstood the 
cuisine type, thus the percent agreement of mutual 
understandings is 2/3 at this time. Then, the user 
restated the cuisine type and the second system 
utterance confirmed this information. Since the 
system only asks for explicit information when its 
confidence is low, we count the system's under-
standing on the cuisine type as a 50% match with 
the user's. Therefore, the total percent agreement is 
2.5/3. The user then confirmed that the system had 
correctly understood all constraints. Therefore, the 
system provided the restaurant information in the 
last utterance. The system's understanding matches 
100% with the user's at this point.  
    The percent agreement of system/user under-
standings over the entire dialog is calculated by 
averaging the percent agreement after each turn. In 
this example, understandingAgreement is (2/3 + 
2.5/3 + 1)/3 =83.3%. We hypothesize that the 
higher the understandingAgreement is, the better 
the system performs, and thus the more the user is 
satisfied. The matches of understandings can be 
calculated automatically from the user simulation 
and the system logs. However, since we work with 
human users' dialogs in the first part of this study, 
we manually annotated the semantic contents (e.g., 
cuisine name) in the real user corpus.  
Previous studies (E.g., Walker et al, 1997) use a 
corpus level semantic accuracy measure (semanti-
cAccuracy) to capture the system?s understanding 
ability. SemanticAccuracy is defined in the stan-
dard way as the total number of correctly under-
stood constraints divided by the total number of 
constraints mentioned in the entire dialog. The un-
derstandingAgreement measure we introduce here 
is essentially the averaged per-sentence semantic 
accuracy, which emphasizes the utterance level 
perception rather than a single corpus level aver-
age. The intuition behind this new measure is that 
it is better for the system to always understand 
something to keep a conversation going than for 
the system to understand really well sometimes but 
really bad at other times. We compute both meas-
ures in our experiments for comparison.  
4.2 Efficiency Measure 
Efficiency is another important measure of the sys-
tem performance. A standard efficiency measure is 
the number of dialog turns. However, we would 
like to take into account the user's dialog strategy 
because how the user specifies the restaurant selec-
tion constraints has a certain impact on the dialog 
pace. Comparing two situations where one user 
specifies the three constraints of selecting a restau-
rant in three separate utterances, while another user 
specifies all the constraints in one utterance, we 
will find that the total number of dialog turns in the 
second situation is smaller assuming perfect under-
standings. Thus, we propose to use the ratio be-
tween the number of turns in the perfect 
understanding situation and the number of turns in 
practice (efficiencyRatio) to measure the system 
efficiency. The larger the efficiencyRatio is, the 
closer the actual number of turns is to the perfect 
understanding situation. In the example in Table 2, 
because the user chose to specify all the constraints 
in one utterance, the dialog length would be 2 turns 
in perfect understanding situation (excluding the 
last user turn which is always "I'm done"). How-
ever, the actual dialog length is 6 turns. Thus, the 
efficiencyRatio is 2/6. 
Since our task scenarios always contain three 
constraints, we can calculate the length of the er-
ror-free dialogs based on the user?s strategy. When 
the user specifies all constraints in the first utter-
ance, the ideal dialog will have only 2 turns; when 
the user specifies two constraints in one utterance 
and the other constraints in a separate utterance, 
the ideal dialog will have 4 turns; when the user 
specifies all constraints one by one, the ideal dia-
log will have 6 turns. Thus, in the simulation envi-
ronment, the length of the ideal dialog can be 
calculated from the simulated users? agenda. Then, 
the efficiencyRatio can be calculated automati-
cally. We manually computed this measure for the 
real users? dialogs. 
168
Similarly, in order to compare with previous 
studies, we also investigate the total number of 
dialog turns (dialogTurns) proposed as the effi-
ciency measure (E.g., M?ller et al, 2007).  
4.3 Action Appropriateness Measure  
This measure aims to evaluate the appropriateness 
of the system actions. The definition of appropri-
ateness can vary on different tasks and different 
system design requirements. For example, some 
systems always ask users to explicitly confirm 
their utterances due to high security needs. In this 
case, an explicit confirmation after each user utter-
ance is an appropriate system action. However, in 
other cases, frequent explicit confirmations may be 
considered as inappropriate because they may irri-
tate the users. In our task domain, we define the 
only inappropriate system action to be providing 
information based on misunderstood user require-
ments. In this situation, the system is not aware of 
its misunderstanding error. Instead of conducting 
an appropriate error-recovering dialog, the system 
provides wrong information to the user which we 
hypothesize will decrease the user?s satisfaction.  
We use the percentage of appropriate system ac-
tions out of the total number of system actions 
(percentAppropriate) to measure the appropriate-
ness of system actions. In the example in Table 2, 
only the first system action is inappropriate in all 3 
system actions. Thus, the percent system action 
appropriateness is 2/3. Since we can detect the sys-
tem?s misunderstanding and the system?s action in 
the simulated dialog environment, this measure can 
be calculated automatically for the simulated dia-
logs. For the real user corpus, we manually coded 
the inappropriate system utterances.  
Note that the definition of appropriate action we 
use here is fairly loose. This is partly due to the 
simplicity of our task domain and the limited pos-
sible system/user actions. Nevertheless, there is 
also an advantage of the loose definition: we do 
not bias towards one particular dialog strategy 
since our goal here is to find some general and eas-
ily measurable system performance factors that are 
correlated with the user satisfaction. 
5 Investigating Evaluation Measures on 
the Real User Corpus  
In this section, we first validate the proposed 
measures using real users? satisfaction scores, and 
then show the differentiating power of these meas-
ures through the improvement curves plotted on 
the dry-run data. 
5.1 Validating Evaluation Measures 
To validate the evaluation measures introduced in 
Section 4, we use Pearson?s correlation to examine 
how well these evaluation measures can predict the 
user satisfaction scores. Here, we only look at the 
dialog corpus in final evaluation because only 
these users filled out the user satisfaction surveys. 
For each user, we compute the average value of the 
evaluation measures across all dialogs generated 
by that user. 
 
Table 3 lists the correlation between the evalua-
tion measures and the user satisfaction scores, as 
well as the p-value for each correlation. The corre-
lation describes a linear relationship between these 
measures and the user satisfaction scores. For the 
measures that describe the system?s understanding 
abilities and the measures that describe the sys-
tem?s efficiency, our newly proposed measures 
show higher correlations with the user satisfaction 
scores than their counterparts. Therefore, in the 
rest of the study, we drop the two measures used 
by the previous studies, i.e., semanticAccuracy and 
dialogTurns.  
We observe that the user satisfaction scores are 
significantly positively correlated with all the three 
proposed measures. These correlations confirms 
our expectations: user satisfaction is higher when 
the system?s understanding matches better with the 
users? requirements; when the dialog efficiency is 
closer to the situation of perfect understanding; or 
when the system's actions are mostly appropriate. 
We suggest that these measures can serve as indi-
cators for user satisfaction.  
    We further use all the measures to build a re-
gression model to predict the user satisfaction 
score. The prediction model is: 
Evaluation Measure Correlation P-value 
understandingAgreement 0.354 0.05 
semanticAccuracy 0.304 0.08 
efficiencyRatio 0.406 0.02 
dialogTurns -0.321 0.05 
percentAppropriate 0.454 0.01 
Table3: Correlations with User Satisfaction Scores. 
169
User Satisfaction  
   = 6.123*percentAppropriate 
  +2.854*efficiencyRatio                         --- (1) 
      +0.864*understandingAgreement - 4.67 
 
The R-square is 0.655, which indicates that 
65.5% of the user satisfaction scores can be ex-
plained by this model. While this prediction model 
has much room for improvement, we suggest that 
it can be used to estimate the users? satisfaction 
scores for simulated users in the early system test-
ing stage to quickly assess the system's perform-
ance. Since the weights are tuned based on the data 
from this specific application, the prediction model 
may not be used directly for other domains.  
5.2 Assessing the Differentiating Power of the 
Evaluation Measures 
Since this set of evaluation measures intends to 
evaluate the system's performance in the develop-
ment stage, we would like the measures to be able 
to reflect small changes made in the system and to 
indicate whether these changes show the right 
trend of increased user satisfaction in reality. A set 
of good evaluation measures should be sensible to 
subtle system changes. 
We assess the differentiating power of the eval-
uation measures using the dialog corpus collected 
during the dry-runs. The system was tested on a 
weekly basis as explained in Table 1. For each im-
provement stage, we compute the values for the 
three evaluation measures averaging across all dia-
logs from all users. Figure 1 shows the three im-
provement curves based on these three measures. 
The x-axis shows the first date of each improve-
ment stage; the y-axis shows the value of the eval-
uation measures. We observe that all three curves 
show the right trends that indicate the system?s 
improvements over the development stages.  
6 Applying the Evaluation Measures on 
the Simulated Corpus  
We train a goal and agenda driven user simulation 
model from the final evaluation dialog corpus with 
the real users. The simulation model interacts with 
the dialog system 20 times (each time the simula-
tion model represents a different simulated user), 
generating nine dialogs on all of the nine tasks 
each time. In each interaction, the simulated users 
generate their agenda randomly based on a uniform 
distribution. The simulated corpus consists of 180 
dialogs from 20 simulated users, which is of the 
same size as the real user corpus. The values of the 
evaluation measures are computed automatically at 
the end of each simulated dialog. 
   We compute the estimated user satisfaction score 
using Equation 1 for each simulated user. We then 
compare the user satisfaction scores of the 20 si-
mulated users with the satisfaction scores of the 20 
real users. The average and the standard deviation 
of the user satisfaction scores for real users are 
(3.79, 0.72), and the ones for simulated users are 
(3.77, 1.34). Using two-tailed t-test at significance 
level p<0.05, we observe that there are no statisti-
cally significant differences between the two pools 
of scores. Therefore, we suggest that the user satis-
faction estimated from the simulated dialog corpus 
can be used to assess the system performance. 
However, these average scores only offer us one 
perspective in comparing the real with the simu-
lated user satisfaction. In the future, we would like 
to look further into the differences between the 
distributions of these user satisfaction scores. 
7 Conclusions and Future Work  
User simulation has been increasingly used in gen-
erating large corpora for using machine learning 
techniques to automate dialog system design. 
However, user simulation has not been used much 
in testing dialog systems. There are two major con-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
11/21/05 11/30/05 12/05/05 12/13/05 12/19/05 12/27/05 01/04/06 01/10/06 01/16/06
understandingAgreement eff iciencyRatio percentAppropriate
Figure 1: The Improvement Curves on Dry-run Data 
170
cerns: 1. we are not sure how well the state-of-the-
art user simulation can mimic realistic user behav-
iors; 2. we do not get important feedback on user 
satisfaction when replacing human users with 
simulated users. In this study, we suggest that 
while the simulated users might not be mature to 
use in the final system evaluation stage, they can 
be used in the early testing stages of the system 
development cycle to make sure that the system is 
functioning in the desired way. We further propose 
a set of evaluation measures that can be extracted 
from the simulation logs to assess the system per-
formance. We validate these evaluation measures 
on human user dialogs and examine the differenti-
ating power of these measures. We suggest that 
these measures can be used to guide the develop-
ment of the system towards improving user satis-
faction. We also apply the evaluation measures on 
a simulation corpus trained from the real user dia-
logs. We show that the user satisfaction scores es-
timated on the simulated dialogs do not 
significantly differ statistically from the real users? 
satisfaction scores. Therefore, we suggest that the 
estimated user satisfaction can be used to assess 
the system performance while testing with simu-
lated users.  
In the future, we would like to confirm our pro-
posed evaluation measures by testing them on dia-
log systems that allows more complicated dialog 
structures and systems on other domains.  
Acknowledgments 
The authors would like to thank Zhongchao 
Fei, Zhe Feng, Junkuo Cao, and Baoshi Yan 
for their help during the simulation system de-
velopment and the three anonymous reviewers 
for their insightful suggestions. All the remain-
ing errors are ours.  
References  
H. Ai, J. Tetreault, and D. Litman. 2007. Comparing 
User Simulation Models for Dialog Strategy Learn-
ing. In Proc. NAACL-HLT (short paper session). 
G. Chung. 2004. Developing a Flexible Spoken Dialog 
System Using Simulation. In Proc. of ACL 04. 
W. Eckert, E. Levin, and R. Pieraccini. 1997. User 
Modeling for Spoken Dialogue System Evaluation. In 
Proc. of IEEE workshop on ASRU. 
E. Filisko and S. Seneff. 2006. Learning Decision Mod-
els in Spoken Dialogue Systems Via User Simulation. 
In Proc. of AAAI Workshop on Statistical and Em-
pirical Approaches for Spoken Dialogue Systems. 
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid 
Reinforcement/Supervised Learning for Dialogue 
Policies from COMMUNICATOR data. In IJCAI 
workshop on Knowledge and Reasoning in Practical 
Dialogue Systems. 
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochas-
tic Model of Human-Machine Interaction For learn-
ing Dialogue Strategies. IEEE Trans. On Speech and 
Audio Processing, 8(1):11-23. 
R. L?pez-C?zar, A. De la Torre, J. C. Segura and A. J. 
Rubio. (2003). Assessment of dialogue systems by 
means of a new simulation technique. Speech Com-
munication (40): 387-407. 
D. Mirkovic and L. Cavedon. 2005. Practical multi-
domain, multi-device dialogue management, 
PACLING'05: 6th Meeting of the Pacific Association 
for Computational Linguistics. 
Sebastian M?ller, Jan Krebber and Paula Smeele. 2006. 
Evaluating the speech output component of a smart-
home system. Speech Communication (48): 1-27. 
O. Pietquin, O. 2004. A Framework for Unsupervised 
Learning of Dialog Strategies. Ph.D. diss., Faculte 
Polytechnique de Mons. 
K. Scheffler. 2002. Automatic Design of Spoken Dialog 
Systems. Ph.D. diss., Cambridge University. 
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002. 
Optimizing DialogueManagement with Reinforce-
ment Learning: Experiments with the NJFun System. 
Journal of Artificial Intelligence Research (JAIR), 
vol. 16. 
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, 
and Young. S. 2007. Agenda-Based User Simulation 
for Bootstrapping a POMDP Dialogue System. In 
Proc. of NAACL-HLT (short paper session). 
F. Weng, S. Varges, B. Raghunathan, F. Ratiu, H. Pon-
Barry, B. Lathrop, Q. Zhang, H. Bratt, T. Scheideck, 
R. Mishra, K. Xu, M. Purvey, A. Lien, M. Raya, S. 
Peters, Y. Meng, J. Russell,  L. Cavedon, E. Shri-
berg, and H. Schmidt. 2006. CHAT: A Conversa-
tional Helper for Automotive Tasks. In Proc. of 
Interspeech. 
F. Weng, B. Yan, Z. Feng, F. Ratiu, M. Raya, B. Lath-
rop, A. Lien, S. Varges, R. Mishra, F. Lin, M. Purver, 
H. Bratt, Y. Meng, S. Peters, T. Scheideck, B. Rag-
hunathan and Z. Zhang. 2007. CHAT to your destina-
tion. In Proc. Of 8th SIGdial workshop on Discourse 
and Dialogue. 
J. Williams and S. Young. 2006. Partially Observable 
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language. 
M. Walker, D. Litman, C. Kamm, and A. Abella. 1997. 
PARADISE: A Framework for Evaluating Spoken 
Dialogue Agents. In Proceedings of the 35th ACL. 
171
Coling 2010: Poster Volume, pages 692?700,
Beijing, August 2010
Contextual Recommendation based on Text Mining
Yize Li, Jiazhong Nie, Yi Zhang
School of Engineering
University of California Santa Cruz
{yize,niejiazhong,yiz}@soe.ucsc.edu
Bingqing Wang
School of Computer Science Technology
Fudan University
wbq@fudan.edu.cn
Baoshi Yan, Fuliang Weng
Research and Technology Center
Robert Bosch LLC
Baoshi.Yan@us.bosch.com
Fuliang.Weng@us.bosch.com
Abstract
The potential benefit of integrating con-
textual information for recommendation
has received much research attention re-
cently, especially with the ever-increasing
interest in mobile-based recommendation
services. However, context based recom-
mendation research is limited due to the
lack of standard evaluation data with con-
textual information and reliable technol-
ogy for extracting such information. As
a result, there are no widely accepted con-
clusions on how, when and whether con-
text helps. Additionally, a system of-
ten suffers from the so called cold start
problem due to the lack of data for train-
ing the initial context based recommenda-
tion model. This paper proposes a novel
solution to address these problems with
automated information extraction tech-
niques. We also compare several ap-
proaches for utilizing context based on
a new data set collected using the pro-
posed solution. The experimental results
demonstrate that 1) IE-based techniques
can help create a large scale context data
with decent quality from online reviews,
at least for restaurant recommendations;
2) context helps recommender systems
rank items, however, does not help pre-
dict user ratings; 3) simply using context
to filter items hurts recommendation per-
formance, while a new probabilistic latent
relational model we proposed helps.
1 Introduction
In the information retrieval community, one ma-
jor research focus is developing proactive re-
trieval agent that acts in anticipation of informa-
tion needs of a user and recommends information
to the user without requiring him/her to issue an
explicit query. The most popular examples of such
kind of proactive retrieval agent are recommender
systems. Over the last several years, research
in standard recommender systems has been im-
proved significantly, largely due to the availability
of large scale evaluation data sets such as Netflix.
The current research focus goes beyond the stan-
dard user-item rating matrix. As researchers start
to realize that the quality of recommendations de-
pends on time, place and a range of other rele-
vant users? context, how to integrate contextual
information for recommendation is becoming an
ever increasingly important topic in the research
agenda (Adomavicius and Ricci, 2009).
One major challenge in context-aware recom-
mendation research is the lack of large scale an-
notated data set. Ideally, a good research data
set should contain contextual information besides
users? explicit ratings on items. However, such
kinds of data sets are not readily available for
researchers. Previous research work in context
based recommendation usually experiments on a
small data set collected through user studies. Al-
though undoubtedly useful, this approach is lim-
ited because 1) user studies are usually very ex-
pensive and their scales are small; 2) it is very hard
for the research community to repeat such study;
and 3) a personalized contextual system may not
692
1 I was very excited to try this place and my wife took me here on my birthday. . .
We ordered a side of the brussell sprouts and they were the highlight of the night.
2 A friend of mine suggested we meet up here for a night of drinks. . . This actually
a restaurant with a bar in it, but when we went it was 10pm and . . .
Table 1: Examples of the restaurant reviews
succeed until a user has interacted with it for a
long period of time to enable context based rec-
ommendation models well trained.
On the other hand, a large amount of re-
view documents from web sites such as tri-
padvisor.com, yelp.com, cnet.com, amazon.com,
are available with certain contextual information,
such as time and companion, implicitly in the re-
views (see Table 1 for examples). This offers us an
opportunity to apply information extraction tech-
niques for obtaining contextual information from
the review texts. Together with users? explicit rat-
ings on items, this might lead to a large research
data set for context based recommendation and
consequently address the cold start issue in the
recommender systems. This paper describes the
methods that extract the contextual information
from online reviews and their impact on the rec-
ommendation quality at different accuracy levels
of the extraction methods.
Another challenge is how to integrate contex-
tual information into existing recommendation al-
gorithms. Existing approaches can be classified
into three major categories: pre-filtering, post-
filtering and the modeling based approaches (Oku
et al, 2007; Adomavicius and Tuzhilin, 2008).
Pre-filtering approaches utilize contextual infor-
mation to select data for that context, and then pre-
dict ratings using a traditional recommendation
method on the selected data (Adomavicius et al,
2005). Post-filtering approaches first predict rat-
ings on the whole data using traditional methods,
then use the contextual information to adjust re-
sults. Both methods separate contextual informa-
tion from the rating estimation process and leads
to unsatisfying findings. For example, Adomavi-
cious et al (2005) found neither standard col-
laborative filtering nor contextual reduction-based
methods dominate each other in all the cases. In
the modeling based approaches, contextual infor-
mation is used directly in the rating prediction
process. For example, Oku et al (2007) propose
a context-aware SVM-based predictive model to
classify restaurants into ?positive? and ?negative?
classes, and contextual information is included as
additional input features for the SVM classifier.
However, treating recommendation as classifica-
tion is not a common approach, and does not take
advantage of the state of art collaborative filtering
techniques. In this paper, we propose a new prob-
abilistic model to integrate contextual information
into the state of art factorization based collabora-
tive filtering approach, and compare it with sev-
eral baselines.
2 Mining Contextual Information from
Textual Opinions
The context includes any information that can be
used to characterize the situation of entities. Ex-
amples of context are: location, identity and state
of people, companions, time, activities of the cur-
rent user, the devices being used etc. (Lee et
al., 2005). Without loss of generality, we looked
into widely available restaurant review data. More
specifically, we investigated four types of contex-
tual information for a dining event, as they might
affect users? dining decisions, and they have not
been studied carefully before. The four types of
contextual information are: Companion (whether
a dining event involves multiple people), Occa-
sion (for what occasions the event is), Time (what
time during the day) and Location (in which city
the event happens).
2.1 Text Mining Approaches
We developed a set of algorithms along with exist-
ing NLP tools (GATE (Cunningham et al, 2002)
etc.) for this task. More detailed description of
these algorithms is given below.
Time: we classified the meal time into the
following types: ?breakfast?, ?lunch?, ?dinner?,
?brunch?, ?morning tea?, ?afternoon tea?. We
693
compiled a list of lexicons for these different types
of meal times, and used a string matching method
to find the explicit meal times from reviews. Here,
the meal time with an expression, such as ?6pm?,
was extracted using ANNIE?s time named entity
recognition module from the GATE toolkit. For
example, if a user says, ?When we went there, it
was 10pm?, we infer that it was for dinner.
Occasion: The ANNIE?s time named en-
tity recognition module recognizes certain special
days from text. We augmented ANNIE?s lookup
function with a list of holidays in the United States
from Wikipedia1 as well as some other occasions,
such as birthdays and anniversaries.
Location: Ideally, a location context would be
a user?s departure location to the selected restau-
rant. However, such information rarely exists in
the review texts. Therefore, we used the location
information from a user?s profile to approximate.
Companion: Extracting a companion?s infor-
mation accurately from review data is more diffi-
cult. We utilized two methods to address the chal-
lenge:
Companion-Baseline: This is a string match-
ing based approach. First, we automatically gen-
erated a lexicon of different kinds of compan-
ion words/phrases by using prepositional patterns,
such as ?with my (our) NN (NNS)?. We extracted
the noun or noun phrases from the prepositional
phrases as the companion terms, which were then
sorted by frequency of occurrence and manually
verified. This led to a lexicon of 167 entries.
Next, we grouped these entries into 6 main cate-
gories of companions: ?family?, ?friend?, ?cou-
ple?, ?colleague?, ?food-buddy? and ?pet?. Fi-
nally, the review is tagged as one or more of the
companion categories if it contains a correspond-
ing word/phrase in that lexicon.
Companion-Classifier: In order to achieve bet-
ter precision, we sampled and annotated 1000
sentences with companion terms from the corpus
and built three classifiers: 1) a MaxEnt classi-
fier with bag-of-words features, 2) a rule-based
classifier, 3) a hybrid classifier. For the rule-
based classifier, we looked into the structural as-
pects of the window where companion terms oc-
1http://en.wikipedia.org/wiki/List of holidays by
country#United States of America
curred, specifically, the adjacent verbs and prepo-
sitions associated with those terms. We collected
high frequency structures including verbs, verb-
proposition combinations, and verb-genitive com-
binations from the whole corpus, and then con-
structed a list of rules to decide whether a compan-
ion context exists based on these structures. For
the hybrid classifier, we used the patterns identi-
fied by the rule-based classifier as features for the
MaxEnt model (Ratnaparkhi, 1998). To train the
classifier, we also included features such as POS
tags of the verb and of the candidate companion
term, the occurrence of a meal term (e.g. ?lunch?,
?dinner?), the occurrence of pronouns (e.g. ?we?
or ?us?) and the genitive of the companion term.
Based on the evaluation results (using 5-fold cross
validation) shown in Table 2, the hybrid classifier
is the best performing classifier and it is used for
the subsequent experiments in the paper.
Words Rule Hybrid
Precision 0.7181 0.7238 0.7379
Recall 0.8962 0.8947 0.9143
F-Score 0.7973 0.8003 0.8167
Table 2: Evaluation results for the bag-of-words-
based classifier (Words), the rule-based classifier
(Rule) and the hybrid classifier (Hybrid)
3 Recommendation based on Contextual
Information
Next we consider how to integrate various con-
textual information into recommender systems.
Assume there are N items and M users. Each
user reviews a set of items in the system. The
data set can be represented as a set of quadruplet
D = (y, i, j, c), where i is the index of user, j is
the index of item, c is a vector describing the con-
text of this rating data, and y is the rating value.
Let c = (c1, ..., ck), where each component ck
represents a type of context, such as ?dinner time?
or ?location=San Jose?. The observed features
(meta data) of user i and item j are represented
as vectors fi and fj respectively, where each com-
ponent in the vector represents a type of feature,
such as ?gender of the user? or ?price range of
the restaurant?. In the rest of this paper, we in-
694
tegrate context c into the user?s observed features
fi. This makes fi a dynamic feature vector, which
will change with different context. The goal is
to predict ratings for candidate items given user i
and context c, and recommend the top items. We
present two recommendation models for integrat-
ing contextual information in this section.
3.1 Boolean Model
The Boolean Model filters out items that do not
match the context. The Boolean model itself re-
turns an item set instead of a ranked list. We fur-
ther rank the items by predicted rating values. We
score items by the Boolean model as follows:
s(j) =
{
sm(j) if item j matches the context
?? otherwise
(1)
where sm(j) is the predicted rating computed us-
ing a rating prediction method m, such as a Col-
laborative Filtering model without using context.
3.2 Probabilistic Latent Relational Model
We propose a novel Probabilistic Latent Rela-
tional Model (PLRM) for integrating contextual
information. In a context-aware recommender
system, a user?s interest for item is influenced by
two factors: (1) the user?s long-term preference,
which can be learned from users? rating history;
(2) the current context (how the item matches the
current context). To capture the two factors si-
multaneously, we introduce a new probabilistic
model by assuming the rating value yi,j,c follows
a Gaussian distribution with mean ui,j,c and vari-
ance 1/?(y):
yi,j,c ? N (ui,j,c, 1/?(y)) (2)
ui,j,c = uTi Avj + (Wufi)T (Wvfj) (3)
where ui and vj are the hidden representations of
user i and item j to be learned from rating data,
and Wu and Wv are feature transformation matri-
ces for users and items respectively. In Equation
(3), the first term uTi Avj is the estimation based
on user? long term preferences, where A = {a} is
a matrix modeling the interaction between ui and
vj .2 The second term (Wufi)T (Wvfj) is the esti-
2We introduce A matrix so that the model can also
be used to model multiple different types of relation-
mation based on current context and the observed
features of users and items, since the context c is
integrated into user?s observed features fi.
{U, V,A,W} are the parameters of the model
to be estimated from the training data set D,
where W = {Wu,Wv} = {w} , U =
{u1,u2, ...uN} and V = {v1,v2, ...vM}. We as-
sume the prior distribution of the parameters fol-
low the Gaussian distributions centered on 0. We
use 1/?(u),1/?(v), 1/?(w) and 1/?(a) to represent
the variance of the corresponding Gaussian distri-
butions. The effect of the prior distribution is sim-
ilar to the ridge regression (norm-2 regularizer)
commonly used in machine learning algorithms to
control model complexity and avoid overfitting.
The proposed model is motivated by well per-
forming recommendation models in the literature.
It generalizes several existing models. If we set A
to the identity matrix and Wu,Wv to zero matri-
ces, the model presented in Equation (3) is equiv-
alent to the well known norm-2 regularized singu-
lar value decomposition, which performs well on
the Netflix competition(Salakhutdinov and Mnih,
2007). If we set A to zero matrix and Wu to iden-
tity matrix, the Model (3) becomes the bilinear
model that works well on Yahoo news recommen-
dation task (Chu and Park, 2009).
Based on the above model assumption, the joint
likelihood of all random variables (U , V , A, W
and D) in the system is:
P (U, V,A,W,D) =?
(i,j,c,y)?D
P (yi,j,c|ui,vj , fi, fj , A,Wu,Wv)
?
i
P (ui)
?
j
P (vj)P (A)P (Wu)P (Wv)(4)
3.3 Parameter Estimation
We use a modified EM algorithm for parame-
ter estimation to find the posterior distribution of
(U, V ) and max a posterior (MAP) of (A,W ).
The estimation can be used to make the final pre-
ships/interactions jointly, where each type of relationship
corresponds to a different A matrix. For the task in this pa-
per, A is not required and can be set to the identity matrix
for simplicity. However, we leave A as parameters to be es-
timated in the rest of this paper for generality.
695
dictions as follows:
y?i,j,c =
?
ui,vj
P (ui)P (vj)(uTi Avj
+(Wufi)TWvfj)duidvj
E Step: the Variational Bayesian approach is used
to estimate the posterior distributions of U and V .
Assuming (A,W ) are known, based on Equation
4, we have
P (U, V |A,W,D) ?
?
(y,i,j,c)?D
N (uTi Avj + (Wufi)TWvfj , 1/?(y))
?
M?
i=1
N (ui|0, 1/?(u)I)
N?
j=1
N (vj |0, 1/?(v)I)
Deriving the exact distribution and use it to predict
y will result in intractable integrals. Thus we ap-
proximate the posterior with a variational distribu-
tion Q(U, V ) = ?Mi=1Q(ui)
?N
j=1Q(vj). Q(ui)
and Q(vj) are restricted to Gaussian distributions
so that predicting y using Bayesian inference with
Q(U, V ) will be straightforward. Q(U, V ) can be
estimated by minimizing the KL-divergence be-
tween it and P (U, V |A,W,D). Since Q(U, V ) is
factorized into individual Q(ui) and Q(vj), we
can first focus on one Q(ui) (or Q(vj)) at a time
by fixing/ignoring other factors. For space consid-
erations, we omit the derivation in this paper. The
optimal Q(ui) is N (u?i,?i), where u?i = ?idi,
??1i =
?
(y,i,j,c)?D
?(y)A(v?jv?Tj + ?j)AT
+ ?(u)I
di =
?
(y,i,j,c)?D
?(y)y?Av?j
Similarly, the optimal Q(vj) isN (v?j,?j), where
v?j = ?jej ,
??1j =
?
(y,i,j,c)?D
?(y)AT (u?iu?Ti + ?i)A
+ ?(v)I
ej =
?
(y,i,j,c)?D
?(y)y?AT v?j
M Step: Based on the approximate pos-
terior estimation Q(U, V ) derived in the E
step, the maximum a posteriori estimation
of {A,W} can be found by maximizing
the expected posterior likelihood {A?, W?} =
argmaxA,W EQ(U,V )(logP (A,W,U, V |D)).
This can be done using the conjugate gradient
descent method, and the gradient of A,Wu,Wv
can be calculated as follows:
??
?A =
?
(y,i,j,c)?D
?(y)((y? ? y)u?iv?Tj
+ u?iu?Ti A?j + ?iAv?jv?Tj + ?iA?j)
+ ?(a)A
??
?Wu
=
?
(y,i,j,c)?D
?(y)(y? ? y)WvfjfTi
+ ?(w)Wu
??
?Wv
=
?
(y,i,j,c)?D
?(y)(y? ? y)WufifTj
+ ?(w)Wv
where ? = EQ(U,V )(logP (A,W,U, V |D)) and
y? = u?Ti Av?j + (Wufi)TWvfj .
4 Experimental Methodology
4.1 Data Collection
We collected an evaluation data set from a pop-
ular review web site where users review ser-
vices/products and provide integer ratings from 1
to 5. The user profile and the description of items,
such as user gender and the category of restau-
rants are also collected. The data set used in this
paper includes the restaurants in Silicon Valley
(Bay area) and the users who ever reviewed these
restaurants. We extract context from the review
texts. The four kinds of context considered in our
paper are described in Section 2.1. For each type
of context, we create a subset, in which all reviews
contain the corresponding contextual information.
Finally we construct four sub data sets and each
data set is described by the corresponding con-
text type: Time, Location, Occasion and Compan-
ion. We use ?All? to represent the whole data set.
Statistics about each data set are described in Ta-
ble 3.
696
(a) Time (b) Location (c) Occasion
(d) Companion (e) All
Figure 1: Performance on the top-K recommendation task. The plots focus on the top 20% ranking
region.
Dataset #Ratings #Users #Items
All 756,031 82,892 12,533
Location 583,051 56,026 12,155
Time 229,321 49,748 10,561
Occasion 22,732 12,689 4,135
Companion 196,000 47,545 10,246
Table 3: Statistics of data
4.2 Experimental Setup
We design the experiments to answer the follow-
ing questions: 1) Does including contextual in-
formation improve the recommendation perfor-
mance? 2) How does the probabilistic latent re-
lational modeling approach compare with pre-
filtering or post-filtering approaches? 3) How
does the extraction quality of the contextual infor-
mation affect the recommendation performance?
To answer the first question, we compare the
performance of the Probabilistic Latent Relational
Model on a standard collaborative filtering setting
where only rating information is considered, in-
dicated by Nocontext. We also evaluate the per-
formance of the Probabilistic Latent Relational
Model when integrating contextual information,
indicated by Context-X, where X represents the
type of contextual information considered. To
answer the second question, we compare the
performance of Context-X with the pre-filtering
Boolean Model, which first uses the context to se-
lect items and then ranks them using scores com-
puted by Nocontext. To answer the third question,
we compare the recommendation performance for
different extraction precision. The performance
on the following two recommendation tasks are
reported in this paper:
Top-K Recommendation: We rank the items
by the predicted rating values and retrieve the top
K items. This task simulates the scenario where
a real recommender system usually suggests a list
of ranked K items to a user. To simulate the sce-
nario that we only want to recommend the 5-star
items to users, we treat 5-star rating data in testing
data as relevant. Ideally, classic IR measures such
as Precision and Recall are used to evaluate the
recommendation algorithms. However, without
complete relevance judgements, standard IR eval-
uation is almost infeasible. Thus we use a varia-
tion of the evaluation method proposed by Koren
(Koren, 2008).
Rating Prediction: Given an active user i and a
target item j, the system predicts the rating of user
697
Training on Sub Data set Training on the Whole Data set
Testing Data ItemAvg Nocontext Context ItemAvg Nocontext Context
Time 1.1517 1.0067 1.0067 1.1052 0.9829 0.9822
Companion 1.2657 1.0891 1.0888 1.2012 1.0693 1.0695
Occasion 1.2803 1.1381 1.1355 1.2121 1.0586 1.0583
Location 1.1597 1.0209 1.0206 1.1597 1.0183 1.0183
All context - - - 1.1640 1.0222 1.0219
Table 4: RMSE on the rating prediction task
Time CompanionBaseline CompanionClassifier Occasion
#Reviews 300 300 300 200
#Contexts 115 148 114 207
Precision 84.4% 62.2% 77.1% -
Recall 80.2% 95.8% 91.7% -
F-Score 82.2% 75.4% 83.8% Accuracy 78.3%
Table 5: Performance of the context extraction module
i on item j. The prediction accuracy is measured
by Root Mean Square Error (RMSE), which is
commonly used in collaborative filtering research.
This task simulates the scenario that we need to
guess a user?s rating about an item, given that the
user has already purchased/selected the item.
For each data set (Time, Companion, Location,
Occasion and All), we randomly sample 10% for
testing, 80% for training and 10% for validation.
5 Experimental Results
5.1 Performance on Top-K Recommendation
Figure 1(a)-(e) shows the ranking performance on
each data set. The x-axis is the rank and the y-axis
is the portion of relevant products covered by this
level of rank. The results across all data sets are
consistent. With contextual information, PLRM
Context-X outperforms Nocontext, whereas using
context to pre-filter items (Boolean) does not help.
It means that contextual information can help if
used appropriately, however improperly utilizing
context, such as simply using it as a boolean filter,
may hurt the recommendation performance. Our
proposed PLRM is an effective way to integrate
contextual information.
5.2 Performance on Rating Prediction Task
Table 4 summaries the RMSE results of differ-
ent approaches on the rating prediction task. The
RMSE of simply using item?s average rating value
as the prediction is also reported as a reference
since it is a commonly used approach by non per-
sonalized recommender systems. For each con-
text, we can either train the model only on the sub-
set that consists of rating data with related context,
or train on a bigger data set by adding the rating
data without related context. The results on both
settings are reported here. Table 4 shows that uti-
lizing context does not affect the prediction accu-
racy. We may wonder why the effects of adding
context is so different on the rating task compared
with the ranking task. One possible explanation
is that the selection process of a user is influenced
by context, while how the user rates an item after
selecting it is less relevant to context. For exam-
ple, when a user wants to have a breakfast, he may
prefer a cafeteria rather than a formal restaurant.
However, how the user rates this cafeteria is more
based on user?s experiences in the cafeteria, such
as quality of services, food, price, environment,
etc.
5.3 How does Text Mining Accuracy Affect
Recommendation
To evaluate the extraction performance on ?Com-
panion?, ?Time? and ?Occasion?, we randomly
sample some reviews and evaluate the perfor-
698
mance on the samples3. The results are shown in
Table 5. Compared with other contexts, the ex-
traction of companion context is more challenging
and the string matching baseline algorithm pro-
duces significantly inferior results. However, by
using a MaxEnt classifier with features selection,
we can boost the precision of the companion con-
text extraction to a level comparable to other con-
texts.
To further investigate the relationship between
the quality of the extracted context and the perfor-
mance of the recommender system, we compare
the recommendation performance of Companion-
Baseline and Companion-Classifier in Figure
1(d). It shows that improving the quality of the
extraction task leads to a significant improvement
on the recommender systems? top-K ranking task.
6 Conclusions
Reviews widely available online contain a large
amount of contextual information. This paper
proposes to leverage information extraction tech-
niques to help recommender systems to train
better context-aware recommendation models by
mining reviews. We also introduce a probabilis-
tic latent relation model for integrating the cur-
rent context and the user?s long term preferences.
This model takes the advantages of traditional col-
laborative filtering approaches (CF). It also cap-
tures the interaction between contextual informa-
tion and item characteristics. The experimental
results demonstrate that context is an important
factor that affects user choices. If properly used,
contextual information helps ranking based rec-
ommendation systems, probably because context
influences users? purchasing decisions. Besides,
more accurate contextual information leads to bet-
ter recommendation models. However, contextual
information does not help the user rating predic-
tion task significantly, probably because context
doesn?t matter much given the user has already
chosen a restaurant.
As the first step towards using the information
3We sample 300 reviews for ?Time? and ?Companion?
evaluation. Due to the extremely low probability of occur-
rence of Occasion context, we futher sample 200 reviews
containing Occasion-related expressions and only evaluate
extraction accuracy on these samples
extraction techniques to help contextual recom-
mendation, the techniques used in this paper are
far from optimal. In the future, we will research
more effective text mining techniques for contex-
tual extraction(Mazur and Dale, 2008; McCallum
et al, 2000; Lafferty et al, 2001) at the same time
increasing the amount of annotated review data
for better classifier performance through actively
learning (Laws and Schu?tze, 2008). We also plan
to work towards a better understanding of con-
textual information in recommender systems, and
explore other types of contextual information in
different types of recommendation tasks besides
restaurant recommendations.
7 Acknowledgements
Part of this research is funded by National Sci-
ence Foundation IIS-0713111 and the Institute of
Education Science. Any opinions, findings, con-
clusions or recommendations expressed in this pa-
per are the authors?, and do not necessarily reflect
those of the sponsors. Bingqing Wang?s work is
done during his stay in the Research and Technol-
ogy Center, Robert Bosch LLC.
References
Adomavicius, Gediminas and Francesco Ricci. 2009.
Recsys?09 workshop 3: workshop on context-aware
recommender systems, cars-2009. In Proceedings
of the 3rd ACM Conference on Recommender Sys-
tems, RecSys 2009, pages 423?424.
Adomavicius, Gediminas and Alexander Tuzhilin.
2008. Context-aware recommender systems. In
Proceedings of the 2nd ACM Conference on Rec-
ommender Systems, RecSys 2008, pages 335?336.
Adomavicius, Gediminas, Ramesh Sankaranarayanan,
Shahana Sen, and Alexander Tuzhilin. 2005.
Incorporating contextual information in recom-
mender systems using a multidimensional approach.
ACM Transactions on Information Systems (TOIS),
23(1):103?145.
Chu, Wei and Seung-Taek Park. 2009. Personalized
recommendation on dynamic content using predic-
tive bilinear models. In Proceedings of the 18th In-
ternational Conference on World Wide Web, WWW
2009, pages 691?700.
Cunningham, Hamish, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. A frame-
work and graphical development environment for
699
robust nlp tools and applications. In Proceedings of
the 40th Anniversary Meeting of the Association for
Computational Linguistics, ACL 2002, pages 168?
175.
Koren, Yehuda. 2008. Factorization meets the
neighborhood: a multifaceted collaborative filtering
model. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, SIGKDD 2008, pages 426?434.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning, ICML
2001, pages 282?289.
Laws, Florian and Hinrich Schu?tze. 2008. Stopping
criteria for active learning of named entity recogni-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics, Coling 2008,
pages 465?472, August.
Lee, Hong Joo, Joon Yeon Choi, and Sung Joo Park.
2005. Context-aware recommendations on the mo-
bile web. In On the Move to Meaningful Internet
Systems 2005: OTM 2005 Workshops, pages 142?
151.
Mazur, Pawel and Robert Dale. 2008. What?s the
date? high accuracy interpretation of weekday
names. In Proceedings of the 22nd International
Conference on Computational Linguistics, Coling
2008, pages 553?560.
McCallum, Andrew, Dayne Freitag, and Fernando
C. N. Pereira. 2000. Maximum entropy markov
models for information extraction and segmenta-
tion. In Proceedings of the 17th International Con-
ference on Machine Learning, ICML 2000, pages
591?598.
Oku, Kenta, Shinsuke Nakajima, Jun Miyazaki, and
Shunsuke Uemura. 2007. Investigation for design-
ing of context-aware recommendation system using
svm. In Proceedings of the International MultiCon-
ference of Engineers and Computer Scientists 2007,
IMECS 2007, pages 970?975.
Ratnaparkhi, A. 1998. MAXIMUM ENTROPY MOD-
ELS FOR NATURAL LANGUAGE AMBIGUITY
RESOLUTION. Ph.D. thesis, University of Penn-
sylvania.
Salakhutdinov, Ruslan and Andriy Mnih. 2007. Prob-
abilistic matrix factorization. In Advances in Neural
Information Processing Systems 20, Proceedings of
the 21st Annual Conference on Neural Information
Processing Systems, NIPS 2007, pages 1257?1264.
700
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Document Summarization via Guided Sentence Compression
Chen Li1, Fei Liu2, Fuliang Weng2, Yang Liu1
1 Computer Science Department, The University of Texas at Dallas
Richardson, Texas 75080, USA
2 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{fei.liu, fuliang.weng@us.bosch.com}
Abstract
Joint compression and summarization has
been used recently to generate high quality
summaries. However, such word-based joint
optimization is computationally expensive. In
this paper we adopt the ?sentence compression
+ sentence selection? pipeline approach for
compressive summarization, but propose to
perform summary guided compression, rather
than generic sentence-based compression. To
create an annotated corpus, the human anno-
tators were asked to compress sentences while
explicitly given the important summary words
in the sentences. Using this corpus, we train
a supervised sentence compression model us-
ing a set of word-, syntax-, and document-
level features. During summarization, we use
multiple compressed sentences in the inte-
ger linear programming framework to select
salient summary sentences. Our results on the
TAC 2008 and 2011 summarization data sets
show that by incorporating the guided sen-
tence compression model, our summarization
system can yield significant performance gain
as compared to the state-of-the-art.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive sum-
marization. Extractive summarization focuses on
selecting the salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is generally
considered more difficult, involving sophisticated
techniques for meaning representation, content plan-
ning, surface realization, etc., and the ?true abstrac-
tive summarization remains a researcher?s dream?
(Radev et al, 2002).
There has been a surge of interest in recent
years on generating compressed document sum-
maries as a viable step towards abstractive sum-
marization. These compressive summaries often
contain more information than sentence-based ex-
tractive summaries since they can remove insignif-
icant sentence constituents and make space for more
salient information that is otherwise dropped due to
the summary length constraint. Two general strate-
gies have been used for compressive summarization.
One is a pipeline approach, where sentence-based
extractive summarization is followed or proceeded
by sentence compression (Knight and Marcu, 2000;
Lin, 2003; Zajic et al, 2007; Wang et al, 2013).
Another line of work uses joint compression and
summarization. They have been shown to achieve
promising performance (Daume?, 2006; Martins and
Smith, 2009; Berg-Kirkpatrick et al, 2011; Chali
and Hasan, 2012; Almeida and Martins, 2013; Qian
and Liu, 2013). One popular approach for such joint
compression and summarization is via integer lin-
ear programming (ILP). However, since words are
the units in the optimization framework, solving this
ILP problem can be expensive.
In this study, we use the pipeline compression
and summarization method because of its compu-
tational efficiency. Prior work using such pipeline
methods simply uses generic sentence-based com-
pression for each sentence in the documents, no mat-
ter whether compression is done before or after sum-
mary sentence extraction. We propose to use sum-
490
mary guided compression combined with ILP-based
sentence selection for summarization in this paper.
We create a compression corpus for this purpose.
Using human summaries for a set of documents, we
identify salient words in the sentences. During anno-
tation, the human annotators are given these salient
words and asked to generate compressed sentences.
We expect such ?guided? sentence compression is
beneficial for the pipeline compression and summa-
rization task. In addition, previous research on joint
modeling for compression and summarization sug-
gested that the labeled extraction and compression
data sets would be helpful for learning a better joint
model (Daume?, 2006; Martins and Smith, 2009).
We hope that our work on this guided compression
will also be of benefit to the future joint modeling
studies.
Using our created compression data, we train
a supervised compression model using a variety
of word-, sentence-, and document-level features.
During summarization, we generate multiple com-
pression candidates for each sentence, and use the
ILP framework to select compressed summary sen-
tences. In addition, we also propose to apply a pre-
selection step to select some important sentences,
which can both speed up the summarization system
and improve performance. We evaluate our pro-
posed summarization approach on the TAC 2008
and 2011 data sets using the standard ROUGE met-
ric (Lin, 2004). Our results show that by incorporat-
ing a guided sentence compression model, our sum-
marization system can yield significant performance
gain as compared to the state-of-the-art reported re-
sults.
2 Related Work
Summarization research has seen great development
over the last fifty years (Nenkova and McKeown,
2011). Compared to the abstractive counterpart, ex-
tractive summarization has received considerable at-
tention due to its clear problem formulation ? to ex-
tract a set of salient and non-redundant sentences
from the given document set. Both unsupervised and
supervised approaches have been explored for sen-
tence selection. The supervised approaches include
the Bayesian classifier (Kupiec et al, 1995), max-
imum entropy (Osborne, 2002), skip-chain condi-
tional random fields (CRF) (Galley, 2006), discrim-
inative reranking (Aker et al, 2010), among others.
The extractive summary sentence selection prob-
lem can also be formulated in an optimization
framework. Previous approaches include the inte-
ger linear programming (ILP) and submodular func-
tions, which are used to solve the optimization prob-
lem. In particular, Gillick et al (2009) proposed
a concept-based ILP approach for summarization.
Li et al (2013) improved it by using supervised
stragety to estimate concept weight in ILP frame-
work. In (Lin and Bilmes, 2010), the authors model
the sentence selection problem as maximizing a sub-
modular function under a budget constraint. A
greedy algorithm is proposed to efficiently approxi-
mate the solution to this NP-hard problem.
Compressive summarization receives increasing
attention in recent years, since it offers a viable
step towards abstractive summarization. The com-
pressed summaries can be generated through a joint
model of the sentence selection and compression
processes, or through a pipeline approach that in-
tegrates a generic sentence compression model with
a summary sentence pre-selection or post-selection
step.
Many studies explore the joint sentence compres-
sion and selection setting. Martins and Smith (2009)
jointly perform sentence extraction and compression
by solving an ILP problem; Berg-Kirkpatrick et al
(2011) propose an approach to score the candidate
summaries according to a combined linear model
of extractive sentence selection and compression.
They train the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) present a method
where the summary?s informativeness, succinctness,
and grammaticality are learned separately from data
but optimized jointly using an ILP setup; Yoshikawa
et al (2012) incorporate semantic role information
in the ILP model; Chali and Hasan (2012) investi-
gate three strategies in compressive summarization:
compression before extraction, after extraction, or
joint compression and extraction in one global op-
timization framework. These joint models offer a
promise for high quality summaries, but they often
have high computational cost. Qian and Liu (2013)
propose a graph-cut based method that improves the
speed of joint compression and summarization.
491
The pipeline approach, where sentence-based ex-
tractive summarization is followed or proceeded by
sentence compression, is also popular. Knight and
Marcu (2000) utilize the noisy channel and deci-
sion tree method to perform sentence compression;
Lin (2003) shows that pure syntactic-based com-
pression may not improve the system performance;
Zajic et al (2007) compare two sentence compres-
sion approaches for multi-document summarization,
including a ?parse-and-trim? and a noisy-channel ap-
proach; Galanis and Androutsopoulos (2010) use
the maximum entropy model to generate the candi-
date compressions by removing the branches from
the source sentences; Liu and Liu (2013) couple
the sentence compression and extraction approaches
for summarizing the spoken documents; Wang et al
(2013) design a series of learning-based compres-
sion models built on parse trees, and integrate them
in query-focused multi-document summarization.
Prior studies often rely heavily on the generic sen-
tence compression approaches (McDonald, 2006;
Nomoto, 2007; Clarke and Lapata, 2008; Thadani
and McKeown, 2013) for compressing the sentences
in the documents, yet a generic compression system
may not be the best fit for the summarization pur-
pose.
In this paper, we adopt the pipeline-based com-
pressive summarization framework, but propose a
novel guided compression method that is catered to
the summarization task. We expect this approach
to take advantage of the efficient pipeline process-
ing while producing satisfying results as the joint
models. We train a supervised guided compression
model to produce n-best compressions for each sen-
tence, and use an ILP formulation to select the best
set of summary sentences. In addition, we pro-
pose to apply a sentence pre-selection step to fur-
ther accelerate the processing and enhance the per-
formance.
3 Guided Compression Corpus
The goal of guided sentence compression is to create
compressed sentences that are grammatically cor-
rect and contain the important information that we
would like to preserve in the final summary. Fol-
lowing the compression literature (Clarke and Lap-
ata, 2008), the compression task is defined as a word
Original Sentence:
The gas leak was contained Monday afternoon , nearly 18
hours after it was reported , Statoil spokesman Oeivind
Reinertsen said .
Compression A:
The gas leak was contained
Compression B:
The gas leak was contained Monday afternoon
Compression C:
The gas leak was contained nearly 18 hours after it was
reported
Table 1: Example sentence and three compressions.
deletion problem, that is, the human annotators (and
also automatic compression systems) are allowed to
only remove words from the original sentence to
form a compression. The key difference between
our proposed guided compression with generic sen-
tence compression is that, we provide guidance to
the human compression process by specifying a set
of ?important words? that we wish to keep for each
sentence. We expect this kind of summary oriented
compression would benefit the ultimate summariza-
tion task. Take the sentence shown in Table 1 as an
example. For generic sentence compression, there
may be multiple ?good? human compressions for this
sentence, such as those listed in the table. Without
guidance, a human annotator (or automatic system)
is likely to use option A or B; however, if ?18 hours?
appears in the summary, then we want to provide this
guidance in the compression process, hence option
C may be the best compression choice. This guided
compression therefore avoids removing the salient
words that are important to the final summary.
To generate the guided compression corpus, we
use the TAC 2010 data set1 that was used for
the multi-document summarization task. There are
46 topics. Each has 10 news documents, and
also four human-created abstractive reference sum-
maries. Since annotating all the sentences in this
data set is time consuming and some sentences are
not very important for the summarization task, we
choose a set of sentences that are highly related to
the human abstracts for annotation. We compare
each sentence with the four human abstracts using
the ROUGE-2 metric (Lin, 2004), and the sentences
1http://www.nist.gov/tac/2010/
492
Original Sentence:
He said Vietnam veterans are presumed to have been ex-
posed to Agent Orange and veterans with any of the 10 dis-
eases is presumed to have contracted it from the exposure ,
without individual proof .
Guided Compression:
Vietnam veterans are presumed to have been exposed to
Agent Orange.
Original Sentence:
The province has limited the number of trees to be chopped
down in the forest area in northwest Yunnan and has stopped
building sugar factories in the Xishuangbanna region to
preserve the only tropical rain forest in the country located
there .
Guided Compression:
province has stopped building sugar factories in the
Xishuangbanna region to preserve tropical rain forest.
Table 2: Example original sentences and their guided
compressions. The ?guiding words? are italicized and
marked in red.
with the highest scores are selected.
In annotation, human annotators are provided
with important ?guiding words? (highlighted in the
annotation interface) that we want to preserve in the
sentences. We calculate the word overlap between a
sentence and each of those sentences in the human
abstracts, and use a set of heuristic rules to deter-
mine the ?guiding words? in a sentence: the longest
consecutive word overlaps (greater than 2 words) in
each sentence pair are first selected; the rest overlaps
that contain 2 or more words (excluding the stop-
words) are also selected. We suggest the human an-
notators to use their best judgment to keep the guid-
ing words as many as possible while compressing
the sentence.
We use the Amazon Mechanical Turk (AMT) for
data annotation2. In total, we select 1,150 sentences
from the TAC news documents. They are grouped
into about 230 human intelligence tasks (HITs) with
5 sentences in each HIT. A sentence was compressed
by 3 human annotatorsand we select the shortest
candidate as the goldstandard compression for each
sentence. In Table 2, we show two example sen-
tences, their guiding words (bold), and the human
compressions. The first example shows that giving
up some guiding words is acceptable, since more
2http://www.mturk.com
unnecessary words will be included in order to ac-
commodate all the guiding words; the second ex-
ample shows that the guided compression can lead
to more aggressive word deletions since the con-
stituents that are not important to the summary will
be deleted even though they contain salient informa-
tion by themselves.
For our compression corpus, which contains
1,150 sentences and their guided compressions, the
average compression rate, as measured by the per-
centage of dropped words, is about 50%. This com-
pression ratio is higher compared to other generic
sentence compression corpora, in which the word
deletion rate ranges from 24% to 34% depending
on different text genres and annotation guidelines
(Clarke and Lapata, 2008; Liu and Liu, 2009). This
suggests that the annotators can remove words more
aggressively when they are provided with a limited
set of guiding words.
4 Summarization System
Our summarization system consists of three key
components: we train a supervised guided compres-
sion model using our created compression data, with
a variety of features.then we use this model to gener-
ate n-best compressions for each sentence; we feed
the multiple compressed sentences to the ILP frame-
work to select the best summary sentences. In ad-
dition, we propose a sentence pre-selection step that
can both speed up the summarization system and im-
prove the performance.
4.1 Guided Sentence Compression
Sentence compression has been explored in previous
studies using both supervised and unsupervised ap-
proaches, including the noisy-channel and decision
tree model (Knight and Marcu, 2000; Turner and
Charniak, 2005), discriminative learning (McDon-
ald, 2006), integer linear programming (Clarke and
Lapata, 2008; Thadani and McKeown, 2013), con-
ditional random fields (CRF) (Nomoto, 2007; Liu
and Liu, 2013), etc. In this paper, we employ the
CRF-based compression approach due to its proved
performance and its flexibility to integrate differ-
ent levels of discriminative features. Under this
framework, sentence compression is formulated as
a sequence labeling problem, where each word is
493
labeled as either ?0? (retained) or ?1? (removed).
We develop different levels of features to capture
word-specific characteristics, sentence related infor-
mation, and document level importance. Most of the
features are extracted based only on the sentence to
be compressed. However, we introduce a few doc-
ument level features. These are designed to cap-
ture the word and sentence significance within the
given document collection and are thus expected to
be more summary related.
Word and sentence features:
? Word n-grams: identity of the current word
and two words before and after, as well as all
the bigrams and trigrams that can be formed by
the adjacent words and the current word.
? POS n-grams: same as the word n-grams, but
use the part-of-speech tags instead.
? Named entity tags: binary features represent-
ing whether the current word is a person, loca-
tion, or temporal expression. We use the Stan-
ford CoreNLP tools3 for named entity tagging.
? Stopwords: whether the current word is a stop-
word or not.
? Conjunction features: (1) conjunction of the
current word with its relative position in the
sentence; (2) conjunction of the NER tag with
its relative position.
? Syntactic features: We obtain the syntactic
parsing tree using the Berkeley Parser (Petrov
and Klein, 2007), then obtain the following fea-
tures: (1) the last sentence constituent tag in
the path from the root to the word; (2) depth:
length of the path starting from the root node
to the word; (3) normalized depth: depth di-
vided by the longest path in the parsing tree;
(4) whether the word is under an SBAR node;
(5) depth and normalized depth of the SBAR
node if the word is under an SBAR node;
? Dependency features: We employ the
Penn2Malt toolkit 4 to convert the parse re-
sult from the Berkeley parser to the depen-
dency parsing tree, and use these dependency
3http://nlp.stanford.edu/software/corenlp.shtml
4http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html
features: (1) dependency relations such as
?AMOD? (adjective modifier), ?NMOD? (noun
modifier), etc. (2) whether the word has a child,
left child, or right child in the dependency tree.
Document-level features:
? Sentence salience score: We use a simple re-
gression model to estimate a salience score for
each sentence (more details in Section 4.3),
which represents the importance of the sen-
tence in the document. This score is discretized
into four binary features according to the aver-
age sentence salience.
? Unigram document frequency: this is the
current word?s document frequency based on
the 10 documents associated with each topic.
? Bigram document frequency: document fre-
quency for the two bigrams, the current word
and its previous or next word.
Some of the above features were employed in re-
lated sentence compression studies (Nomoto, 2007;
Liu and Liu, 2013). In addition to these features, we
explored other related features, including the abso-
lute position of the current word, whether the word
appears in the corresponding topic title and descrip-
tions, conjunction of the syntactic tag with the tree
depth, etc.; however, these features did not lead to
improved performance. We train the CRF model
with the Pocket CRF toolkit5 using the guided com-
pression corpus collected in Section 3. During sum-
marization, we apply the model to a given sentence
to generate its n-best guided compressions and use
them in the following summarization step.
4.2 Summary Sentence Selection
The sentence selection process is similar to the stan-
dard sentence-based extractive summarization, ex-
cept that the input to the selection module is a list
of compressed sentences in our work. Many extrac-
tive summarization approaches can be applied for
this purpose. In this work, we choose the integer
linear programming (ILP) method, specifically, the
concept-based ILP framework introduced in (Gillick
5http://sourceforge.net/projects/pocket-crf-1/
494
et al, 2009), mainly because it yields best perfor-
mance in the TAC evaluation tasks. This ILP ap-
proach aims to extract sentences that can cover as
many important concepts as possible, while ensuring
the summary length is within a given constraint. We
follow the study in (Gillick et al, 2009) to use word
bi-grams as concepts, and assign a weight to each
bi-gram using its document frequency in the given
document collection for a test topic. Two differences
are between our ILP setup and that in (Gillick et al,
2009). First, since we use multiple compressions
for one sentence, we need to introduce an additional
constraint: for each sentence, only one of the n-best
compressions may be included in the summary. Sec-
ond, we optimize a joint score of the concept cover-
age and the sentence salience. The formal ILP for-
mulation is shown below:
max
?
i
wici +
?
j
vj
?
k
sjk (1)
s.t.
?
k
sjk ? 1?j (2)
sjkOcci jk ? ci (3)
?
jk
sjkOcci jk ? ci (4)
?
jk
ljksjk ? L (5)
ci ? {0, 1} ?i (6)
sjk ? {0, 1} ?j, k (7)
where ci and sjk are binary variables indicating the
presence of a concept and a sentence respectively;
sjk denotes the kth candidate compression of the
jth sentence; wi represents the weight of the con-
cept; vj is the sentence salience score of the jth
sentence, predicted using a regression model (Sec-
tion 4.3), and all of its compressed candidates share
this value. (1) is the new objective function we use
that combines the coverage of the concepts and the
sentence salience scores. (2) represents our addi-
tional constraint, which requires that for each sen-
tence j, only one candidate compression will be cho-
sen. Occi jk represents the occurrence of concept i
in the sentence sjk. Inequalities (3) and (4) associate
the sentences and the concepts. Constraint (5) con-
trols the summary length, as measured by the total
number of words in the summary. We use an open
source ILP solver6.
4.3 Sentence Pre-selection
The above ILP method can offer an exact solution
to the defined objective function. However, ILP is
computationally expensive when the formulation in-
volves large quantities of variables, i.e, when we
have many sentences and a large number of candi-
date compressions for each sentence. We therefore
propose to apply a sentence pre-selection step be-
fore the compression. This kind of selection step
has been used in previous ILP-based summarization
systems (Berg-Kirkpatrick et al, 2011; Gillick et al,
2009). In this work, we propose to use a simple su-
pervised support vector regression (SVR) model (Ng
et al, 2012) to predict a salience score for each sen-
tence and select the top ranked sentences for further
processing (compression and summarization).
To train the SVR model, the target value for each
sentence is the ROUGE-2 score between the sen-
tence and the four human abstracts (this same value
is used for sentence selection in corpus annotation
(Section 3)). We employ three commonly used fea-
tures: (1) sentence position in the document; (2) sen-
tence length as indicated by a binary feature: it takes
the value of 0 if the number of words in the sentence
is greater than 50 or less than 10, otherwise the fea-
ture value is 1; (3) interpolated n-gram document
frequency as introduced in (Ng et al, 2012), which
is a weighted linear combination of the document
frequency of the unigrams and bigrams contained in
the sentence:
f(s) =
?
?
wu?S
DF (wu) + (1? ?)
?
wb?S
DF (wb)
|S|
where wu and wb represent the unigrams and bi-
grams contained in the sentence S; ? is a balancing
factor; |S| denotes the number of words in the sen-
tence.
The SVR model was trained using the SVMlight
toolkit7. Using this model, we can predict a salience
score (Vj in Eq 1) for each sentence and only select
the top n sentences and supply them to the compres-
sion and summarization steps. In practice, using a
fixed n may not be a good choice since the number
6http://www.gnu.org/software/glpk/
7http://svmlight.joachims.org/
495
of sentences varies greatly for different topics. We
therefore set n heuristically based on the total num-
ber of sentencesm for each topic: n=15 ifm > 150;
n=10 if m < 100; n=0.1 ?m otherwise.
5 Experimental Results
5.1 Experimental Setup
For our experiments, we use the standard TAC data
sets8, which have been used in the NIST competi-
tions and in other summarization studies. In par-
ticular, we used the TAC 2010 data set for creating
the guided compression corpus and training the SVR
pre-selection model, the TAC 2009 data set as devel-
opment set for parameter tuning, and the TAC 2008
and 2011 data sets as the test set for reporting the
final summarization results.
We compare our pipeline summarization sys-
tem against three recent studies, which have re-
ported some of the highest published results on this
task. Berg-Kirkpatrick et al (2011) introduce a
joint model for sentence extraction and compres-
sion. The model is trained using a margin-based ob-
jective whose loss captures the end summary qual-
ity; Woodsend and Lapata (2012) learn individ-
ual summary aspects from data, e.g., informative-
ness, succinctness, grammaticality, stylistic writ-
ing conventions, and jointly optimize the outcome
in an integer linear programming framework. Ng
et al (2012) exploit category-specific information
for multi-document summarization. In addition to
the three previous studies, we also report the best
achieved results in the TAC competitions.
5.2 Summarization Results
In Table 3 and Table 4, we present the results of our
system and the aforementioned summarization stud-
ies. We use the ROUGE evaluation metrics (Lin,
2004), with R-2 measuring the bigram overlap be-
tween the system and reference summaries and R-
SU4 measuring the skip-bigram with the maximum
gap length of 4. ?Our System? uses the pipeline
setting including the three components described in
Section 4. We use the SVR-based approach to pre-
select a set of sentences from the document set; these
sentences are further fed to the guided compression
module that produces n-best compressions for each
8http://www.nist.gov/tac/data/index.html
System R-2 R-SU4 CompR
TAC?08 Best System 11.03 13.96 n/a
(Berg-Kirkpatrick et al, 2011) 11.70 14.38 n/a
(Woodsend et al, 2012) 11.37 14.47 n/a
Our System 12.35? 15.27? 43.06%
Our System w/o Pre-selection 12.02 14.98 55.69%
Our System w/ Generic Comp 10.88 13.79 30.90%
Table 3: Results on the TAC 2008 data set. ?Our Sys-
tem? uses the SVR-based sentence pre-selection + guided
compression + ILP-based summary sentence selection.
?Our System w/ Generic Comp? uses the pre-selection +
generic compression + ILP summary sentence selection
setting. ?CompR? represents the compression ratio, i.e.,
percentage of dropped words. ? represents our system
outperforms the best previous result at the 95% signifi-
cance level.
System R-2 R-SU4 CompR
TAC?11 Best System 13.44 16.51 n/a
(Ng et al, 2012) 13.93 16.83 n/a
Our System 14.40 16.89 39.90%
Our System w/o Pre-selection 13.74 16.5 53.81%
Our System w/ Generic Comp 13.08 16.23 30.10%
Table 4: Results on the TAC 2011 data set. The systems
use the same settings as for the TAC 2008 data set.
sentence; the ILP-based framework is then used to
select the summary sentences from these compres-
sions.
We can see from the table that in general, our sys-
tem achieves considerably better results compared to
the state-of-the-art on both the TAC 2008 and 2011
data sets. On the TAC 2008 data set, our system out-
performs the best reported result at the 95% signifi-
cance level; on the TAC 2011 data set, our system
also yields considerable performance gain though
not exceed the 95% significance level. In the fol-
lowing, we show more detailed analysis to study the
effect of different system parameters.
With or without sentence pre-selection. First
we evaluate the impact of sentence pre-selection
step. In Table 3 and Table 4, we include the
results when this step is not used (?Our System
w/o Pre-selection?). That is, all of the sentences
in the documents (excluding those containing less
than 5 words) are compressed and used in the ILP-
496
based summary sentence selection module. We can
see that although sentence pre-selection removes
some sentences from consideration in the later sum-
marization step, it actually significantly improves
system performance. In the TAC 2008 data set,
each topic contains averagely 210 sentences; while
the pre-selection step chooses 13 sentences among
them. These numbers are 185 and 12 for the TAC
2011 data set. Table 5 shows the average running
time of each topic in TAC 2011 data for the two sys-
tems, with or without the pre-selection step. Here
we fix the number of compressions to 100 in both
cases for fair comparison. We can see the selec-
tion step greatly accelerates the system processing.
When applying the pre-selection step, fewer sen-
tences are used in the compression and summariza-
tion, this means we are able to use more compres-
sion candidates for each sentence (considering the
complexity of ILP module). Using the TAC 2009
as development set, we tuned the number of can-
didate compressions generated for each sentence.
Without pre-selection, we used the 100-best candi-
dates generated from the compression model; with
pre-selection, we are able to increase the number
to 200-best candidate compressions and still main-
tain reasonable computational cost. These are the
numbers used in the results in Table 3 and 4. Us-
ing more compressions helps improve summariza-
tion performance. We also notice that the compres-
sion ratios are quite different when using sentence
pre-selection vs. not. This suggests that in the im-
portant sentences (those are kept after pre-selection),
there is more summary related information and thus
the compression model keeps more words in them
(lower compression ratio).
System
Compressed Number of Running
Sentences Compressions Time (sec)
w/o Pre-selection 185 100 3.9
w/ Pre-selection 12 100 0.85
Table 5: Average running time of our system, w/ or w/o
the sentence pre-selection step. Experiments conducted
on the TAC 2011 data set. Running time refers only to
the execution time of the ILP module for each topic.
Number of compression candidates. This pa-
rameter (denoted as n) also impacts system perfor-
mance. Figure 1 shows the R-2 scores of the two
systems (with and without the sentence pre-selection
step) when using different number of compressions
for each sentence. In general, we find that the R-2
scores do not change much when n is large enough.
For example, the ?with pre-selection? system can
achieve relatively stable R-2 scores on the TAC 2008
data set (ranging from 12.2 to 12.4) when m is
greater than 140; similarly, the R-2 scores on the
TAC 2011 data is over 14.2 when m is greater than
100. Without the pre-selection step, the scores are
less stable in regard to the changing of the m value,
since the large amount of sentences plus a high vol-
ume of the compression candidates may incur huge
computational cost to the ILP solver. This is also the
reason that in Figure 1, for the system without pre-
selection, we only vary n from 1 to 100. In general,
we also notice that given more compression candi-
dates, the R-2 score is still improving, as indicated
by Figure 1. The improved performance of ?with
pre-selection? over ?without pre-selection? is partly
because fewer sentences are used and thus we are
able to increase the number of compression candi-
dates for these sentences in the ILP sentence extrac-
tion module.
Quality of sentence compression training data.
In order to illustrate the contribution of our
summary-guided sentence compression component,
we train a generic sentence compression model
and use this in our compression and summariza-
tion pipeline. The generic compression model was
trained using the Edinburgh sentence compression
corpus (Clarke and Lapata, 2008), which contains
1370 sentences collected from news articles. This
data set has been widely used in other summariza-
tion studies (Martins and Smith, 2009). Each sen-
tence has 3 compressions and we choose the short-
est compression as the reference. The average com-
pression rate of this corpus is about 28%, lower than
that in our summary guided compression data. Note
that in generic sentence compression, we only use
those word and sentence features described in Sec-
tion 4.1, not the document-level features since they
are not available for the Edinburgh data set. Results
of our system using the generic compression model
(with sentence pre-selection) are shown in the last
row of Table 3 and Table 4. We can see that the sys-
tem with this generic compression model performs
497
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  20  40  60  80  100
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  50  100  150  200  250
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  20  40  60  80  100
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  50  100  150  200  250
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
Figure 1: R-2 scores of the two systems (without and
with the sentence pre-selection step) when using differ-
ent number of compressions for each sentence.
worse than ours, and is also inferior to the TAC best
performing system on both data sets, which signi-
fies the importance of our proposed summary guided
sentence compression approach. We can also see
there is a difference in the compression ratio in the
system generated compressions when using differ-
ent compression corpora to train the compression
models. The resulting compression ratio patterns are
consistent with those in the training data, that is, us-
ing our guided compression corpus our system com-
pressed sentences more aggressively.
Learning curve of guided compression. Since
we use a supervised compression model, we further
consider the relationship between the summarization
performance and the number of sentence pairs used
for training the guided compression model. In to-
tal, there are 1150 training sentence pairs in our cor-
pus. We incrementally add 100 sentence pairs each
time and plot the learning curve in Figure 2. In
the compression step, we generate only the 1-best
compression candidate in order to remove the im-
pact caused by the downstream summary sentence
selection module. As seen from Figure 2, increasing
the compression training data generally improves
summarization performance, although there are also
fluctuations. When adding more training sentence
pairs, the system performance is likely to further in-
crease.
 10.5
 11
 11.5
 12
 12.5
 200  400  600  800 1000 1200
RO
UG
E-
2
# Sentence Pairs in the Training Set
TAC 2011
TAC 2008
Figure 2: ROUGE-2 scores when using different number
of sentences to train the guided compression model.
6 Conclusion and Future Work
In this paper, we propose a pipeline summariza-
tion approach that combines a novel guided com-
pression model with ILP-based summary sentence
selection. We create a guided compression cor-
pus, where the human annotators were explicitly in-
formed about the important summary words during
the compression annotation. We then train a super-
vised compression model to capture the guided com-
pression process using a set of word-, sentence-, and
document-level features. We conduct experiments
on the TAC 2008 and 2011 summarization data sets
and show that by incorporating the guided sentence
compression model, our summarization system can
yield significant performance gain as compared to
the state-of-the-art. In future, we would like to
further explore the reinforcement relationship be-
tween keywords and summaries (Wan et al, 2007),
improve the readability of the sentences generated
from the guided compression system, and report re-
sults using multiple evaluation metrics (Nenkova et
al., 2007; Louis and Nenkova, 2012) as well as per-
forming human evaluations.
498
Acknowledgments
Part of this work was done during the first au-
thor?s internship in Bosch Research and Technol-
ogy Center. The work is also partially supported
by NSF award IIS-0845484 and DARPA Contract
No. FA8750-13-2-0041. Any opinions, findings,
and conclusions or recommendations expressed are
those of the author and do not necessarily reflect the
views of the funding agencies.
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using a* search and
discriminative training. In Proceedings of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In Proceedings
of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In Proceed-
ings of COLING.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. thesis,
University of Southern California.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd
summarization system at tac 2009. In Proceedings of
TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: Sentence compression.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
SIGIR.
Chen Li, Xian Qian, and Yang Liu. 2013. Using super-
vised bigram-based ilp for extractive summarization.
In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodular
functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression - A pilot study. In
Proceeding of the Sixth International Workshop on In-
formation Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP.
Fei Liu and Yang Liu. 2013. Towards abstractive speech
summarization: Exploring unsupervised and super-
vised approaches for spoken utterance compression.
IEEE Transactions on Audio, Speech, and Language
Processing.
Annie Louis and Ani Nenkova. 2012. Automati-
cally assessing machine summary content with a gold-
standard. Computational Linguistics.
Andre F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the ACL Workshop
on Integer Linear Programming for Natural Language
Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan,
and Chew-Lim Tan. 2012. Exploiting category-
specific information for multi-document summariza-
tion. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Information
Processing and Management.
Miles Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Work-
shop on Automatic Summarization.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
499
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings of
EMNLP.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. In Computational Linguistics.
Kapil Thadani and Kathleen McKeown. 2013. Sentence
compression with joint structural inference. In Pro-
ceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of ACL.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Multiple
aspect summarization using integer linear program-
ming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression with
semantic role constraints. In Proceedings of ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. In Information Processing and Management.
500
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691?701,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Multi-documents Summarization by Sentence Compression
based on Expanded Constituent Parse Trees
Chen Li1, Yang Liu1, Fei Liu2, Lin Zhao3, Fuliang Weng3
1 Computer Science Department, The University of Texas at Dallas
Richardson, TX 75080, USA
2 School of Computer Science, Carnegie Mellon University
Pittsburgh, PA 15213, USA
3 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{feiliu@cs.cmu.edu}
{lin.zhao,fuliang.weng@us.bosch.com}
Abstract
In this paper, we focus on the problem
of using sentence compression techniques
to improve multi-document summariza-
tion. We propose an innovative sentence
compression method by considering every
node in the constituent parse tree and de-
ciding its status ? remove or retain. In-
teger liner programming with discrimina-
tive training is used to solve the problem.
Under this model, we incorporate various
constraints to improve the linguistic qual-
ity of the compressed sentences. Then we
utilize a pipeline summarization frame-
work where sentences are first compressed
by our proposed compression model to ob-
tain top-n candidates and then a sentence
selection module is used to generate the
final summary. Compared with state-of-
the-art algorithms, our model has simi-
lar ROUGE-2 scores but better linguistic
quality on TAC data.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive
summarization. Extractive summarization focuses
on selecting salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is gener-
ally considered more difficult, involving sophisti-
cated techniques for meaning representation, con-
tent planning, surface realization, etc.
There has been a surge of interest in recent years
on generating compressed document summaries as
a viable step towards abstractive summarization.
These compressive summaries often contain more
information than sentence-based extractive sum-
maries since they can remove insignificant sen-
tence constituents and make space for more salient
information that is otherwise dropped due to the
summary length constraint. Two general strate-
gies have been used for compressive summariza-
tion. One is a pipeline approach, where sentence-
based extractive summarization is followed or pro-
ceeded by sentence compression (Lin, 2003; Zajic
et al., 2007; Vanderwende et al., 2007; Wang et al.,
2013). Another line of work uses joint compres-
sion and summarization. Such methods have been
shown to achieve promising performance (Daume?,
2006; Chali and Hasan, 2012; Almeida and Mar-
tins, 2013; Qian and Liu, 2013), but they are typi-
cally computationally expensive.
In this study, we propose an innovative sen-
tence compression model based on expanded con-
stituent parse trees. Our model uses integer lin-
ear programming (ILP) to search the entire space
of compression, and is discriminatively trained.
It is built based on the discriminative sentence
compression model from (McDonald, 2006) and
(Clarke and Lapata, 2008), but our method uses
an expanded constituent parse tree rather than only
the leaf nodes in previous work. Therefore we
can extract rich features for every node in the con-
stituent parser tree. This is an advantage of tree-
based compression technique (Knight and Marcu,
2000; Galley and McKeown, 2007; Wang et al.,
2013). Similar to (Li et al., 2013a), we use a
pipeline summarization framework where multi-
ple compression candidates are generated for each
pre-selected important sentence, and then an ILP-
691
based summarization model is used to select the
final compressed sentences. We evaluate our pro-
posed method on the TAC 2008 and 2011 data
sets using the standard ROUGE metric (Lin, 2004)
and human evaluation of the linguistic quality.
Our results show that using our proposed sentence
compression model in the summarization system
can yield significant performance gain in linguis-
tic quality, without losing much performance on
the ROUGE metric.
2 Related Work
Summarization research has seen great develop-
ment over the last fifty years (Nenkova and McKe-
own, 2011). Compared to the abstractive counter-
part, extractive summarization has received con-
siderable attention due to its clear problem for-
mulation: to extract a set of salient and non-
redundant sentences from the given document
set. Both unsupervised and supervised approaches
have been explored for sentence selection. Su-
pervised approaches include the Bayesian classi-
fier (Kupiec et al., 1995), maximum entropy (Os-
borne, 2002), skip-chain CRF (Galley, 2006), dis-
criminative reranking (Aker et al., 2010), among
others. The extractive summary sentence selec-
tion problem can also be formulated in an opti-
mization framework. Previous methods include
using integer linear programming (ILP) and sub-
modular functions to solve the optimization prob-
lem (Gillick et al., 2009; Li et al., 2013b; Lin and
Bilmes, 2010).
Compressive summarization receives increas-
ing attention in recent years, since it offers a vi-
able step towards abstractive summarization. The
compressed summaries can be generated through a
joint model of the sentence selection and compres-
sion processes, or through a pipeline approach that
integrates a sentence compression model with a
summary sentence pre-selection or post-selection
step.
Many studies have explored the joint sentence
compression and selection setting. Martins and
Smith (2009) jointly performed sentence extrac-
tion and compression by solving an ILP prob-
lem. Berg-Kirkpatrick et al. (2011) proposed an
approach to score the candidate summaries ac-
cording to a combined linear model of extrac-
tive sentence selection and compression. They
trained the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) presented an-
other method where the summary?s informative-
ness, succinctness, and grammaticality are learned
separately from data but optimized jointly using an
ILP setup. Yoshikawa et al. (2012) incorporated
semantic role information in the ILP model.
Our work is closely related with the pipeline
approach, where sentence-based extractive sum-
marization is followed or proceeded by sentence
compression. There have been many studies on
sentence compression, independent of the summa-
rization task. McDonald (2006) firstly introduced
a discriminative sentence compression model to
directly optimize the quality of the compressed
sentences produced. Clarke and Lapata (2008)
improved the above discriminative model by us-
ing ILP in decoding, making it convenient to
add constraints to preserve grammatical structure.
Nomoto (2007) treated the compression task as
a sequence labeling problem and used CRF for
it. Thadani and McKeown (2013) presented an
approach for discriminative sentence compression
that jointly produces sequential and syntactic rep-
resentations for output text. Filippova and Altun
(2013) presented a method to automatically build
a sentence compression corpus with hundreds of
thousands of instances on which deletion-based
compression algorithms can be trained.
In addition to the work on sentence compres-
sion as a stand-alone task, prior studies have also
investigated compression for the summarization
task. Knight and Marcu (2000) utilized the noisy
channel and decision tree method to perform sen-
tence compression in the summarization task. Lin
(2003) showed that pure syntactic-based compres-
sion may not significantly improve the summariza-
tion performance. Zajic et al. (2007) compared
two sentence compression approaches for multi-
document summarization, including a ?parse-and-
trim? and a noisy-channel approach. Galanis and
Androutsopoulos (2010) used the maximum en-
tropy model to generate the candidate compres-
sions by removing branches from the source sen-
tences. Woodsend and Lapata (2010) presented a
joint content selection and compression model for
single-document summarization. They operated
over a phrase-based representation of the source
document which they obtained by merging infor-
mation from PCFG parse trees and dependency
graphs. Liu and Liu (2013) adopted the CRF-
based sentence compression approach for summa-
692
rizing spoken documents. Unlike the word-based
operation, some of these models e.g (Knight and
Marcu, 2000; Siddharthan et al., 2004; Turner
and Charniak, 2005; Galanis and Androutsopou-
los, 2010; Wang et al., 2013), are tree-based ap-
proaches that operate on the parse trees and thus
the compression decision can be made for a con-
stituent, instead of a single word.
3 Sentence Compression Method
Sentence compression is a task of producing a
summary for a single sentence. The compressed
sentence should be shorter, contain important con-
tent from the original sentence, and be grammat-
ical. In some sense, sentence compression can
be described as a ?scaled down version of the
text summarization problem? (Knight and Marcu,
2002). Here similar to much previous work on
sentence compression, we just focus on how to re-
move/select words in the original sentence without
using operation like rewriting sentence.
3.1 Discriminative Compression Model by
ILP
McDonald (2006) presented a discriminative com-
pression model, and Clarke and Lapata (2008) im-
proved it by using ILP for decoding. Since our
proposed method is based upon this model, in
the following we briefly describe it first. Details
can be found in (Clarke and Lapata, 2008). In
this model, the following score function is used
to evaluate each compression candidate:
s(x, y) =
|y|
?
j=2
s(x, L(y
j?1
), L(y
j
)) (1)
where x = x
1
x
2
, ..., x
n
represents an original sen-
tence and y = y
1
y
2
, ..., y
m
denotes a compressed
sentence. Because the sentence compression prob-
lem is defined as a word deletion task, y
j
must oc-
cur in x. Function L(y
i
) ? [1...n] maps word y
i
in
the compression to the word index in the original
sentence x. Note that L(y
i
) < L(y
i+1
) is required,
that is, each word in x can only occur at most
once in compression y. In this model, a first or-
der Markov assumption is used for the score func-
tion. Decoding this model is to find the combina-
tion of bigrams that maximizes the score function
in Eq (1). Clarke and Lapata (2008) introduced the
following variables and used ILP to solve it:
?
i
=
{
1 if x
i
is in the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
starts the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
ends the compression
0 otherwise
?i ? [1..n]
?
ij
=
{
1 if x
i
, x
j
are in the compression
0 otherwise
?i ? [1..n ? 1]?j ? [i + 1..n]
Using these variables, the objective function can
be defined as:
max z =
n
?
i=1
?
i
? s(x, 0, i)
+
n?1
?
i=1
n
?
j=i+1
?
ij
? s(x, i, j)
+
n
?
i=1
?
i
? s(x, i, n + 1) (2)
The following four basic constraints are used to
make the compressed result reasonable:
n
?
i=1
?
i
= 1 (3)
?
j
? ?
j
?
j
?
i=1
?
ij
= 0 ?j ? [1..n] (4)
?
i
?
n
?
j=i+1
?
ij
? ?
i
= 0 ?i ? [1..n] (5)
n
?
i=1
?
i
= 1 (6)
Formula (3) and (6) denote that exactly one
word can begin or end a sentence. Formula (4)
means if a word is in the compressed sentence, it
must either start the compression or follow another
word; formula (5) represents if a word is in the
693
compressed sentence, it must either end the sen-
tence or be followed by another word.
Furthermore, discriminative models are used for
the score function:
s(x, y) =
|y|
?
j=2
w ? f(x, L(y
j?1
), L(y
j
)) (7)
High dimensional features are used and their cor-
responding weights are trained discriminatively.
Above is the basic supervised ILP formula-
tion for sentence compression. Linguistically and
semantically motivated constraints can be added
in the ILP model to ensure the correct grammar
structure in the compressed sentence. For exam-
ple, Clarke and Lapata (2008) forced the introduc-
ing term of prepositional phrases and subordinate
clauses to be included in the compression if any
word from within that syntactic constituent is also
included, and vice versa.
3.2 Compression Model based on Expanded
Constituent Parse Tree
In the above ILP model, variables are defined for
each word in the sentence, and the task is to pre-
dict each word?s status. In this paper, we propose
to adopt the above ILP framework, but operate di-
rectly on the nodes in the constituent parse tree,
rather than just the words (leaf nodes in the tree).
This way we can remove or retain a chunk of the
sentence rather than isolated words, which we ex-
pect can improve the readability and grammar cor-
rectness of the compressed sentences.
The top part of Fig1 is a standard constituent
parse tree. For some levels of the tree, the nodes
at that same level can not represent a sentence. We
extend the parse tree by duplicating non-POS con-
stituents so that leaf nodes (words and their corre-
sponding POS tags) are aligned at the bottom level
as shown in bottom of as Fig1. In the example tree,
the solid lines represent relationship of nodes from
the original parse tree, the long dot lines denote the
extension of the duplication nodes from the up-
per level to the lower level, and the nodes at the
same level are connected (arrowed lines) to repre-
sent that is a sequence. Based on this expanded
constituent parse tree, we can consider every level
as a ?sentence? and the tokens are POS tags and
parse tree labels. We apply the above compression
model in Section 3.1 on every level to decide every
node?s status in the final compressed sentence. In
order to make the compressed parsed tree reason-
able, we model the relationship of nodes between
PRP/
I 
VBP/ 
am 
DT/  
a 
NN/ 
worker 
IN/ 
from 
NNP/
USA
NP IN 
PRP 
PRP 
PRP 
DT/ 
the
 
NNP/
USA
 
IN/ 
from 
NN/ 
worker 
PP 
NP 
PRP/ 
I 
DT/ 
the
S 
VP NP 
VBP NP 
NP PP 
DT NN 
VBP 
VBP 
S 
VP NP 
VBP/ 
am 
NP 
NP 
DT/ 
a 
Figure 1: A regular constituent parse tree and its
Expanded constituent tree.
adjacent levels as following: if the parent node is
labeled as removed, all of its children will be re-
moved; one node will retain if at least one of its
children is kept.
Therefore, the objective function in the new ILP
formulation is:
max z =
height
?
l=1
(
n
l
?
i=1
?
l
i
? s(x, 0, l
i
)
+
n
l
?1
?
i=1
n
l
?
j=i+1
?
l
ij
? s(x, l
i
, l
j
)
+
n
l
?
i=1
?
l
i
? s(x, l
i
, n
l
+ 1) ) (8)
where height is the depth for a parse tree (starting
from level 1 for the tree), and n
l
means the length
of level l (for example, n
5
= 6 in the example
in Fig1). Then every level will have a set of pa-
rameters ?l
i
, ?
l
i
, ?
l
i
, and ?l
ij
, and the corresponding
constraints as shown in Formula (3) to (6). The re-
lationship between nodes from adjacent levels can
be expressed as:
?
l
i
? ?
(l+1)
j
(9)
?
l
i
?
?
?
(l+1)
j
(10)
in which node j at level (l+1) is the child of node
694
i at level l. In addition, 1 ? l ? height ? 1,
1 ? i ? n
l
and 1 ? j ? n
l+1
.
3.3 Linguistically Motivated Constraints
In our proposed model, we can jointly decide the
status of every node in the constituent parse tree
at the same time. One advantage is that we can
add constraints based on internal nodes or rela-
tionship in the parse tree, rather than only using
the relationship based on words. In addition to
the constraints proposed in (Clarke and Lapata,
2008), we introduce more linguistically motivated
constraints to keep the compressed sentence more
grammatically correct. The following describes
the constraints we used based on the constituent
parse tree.
? If a node?s label is ?SBAR?, its parent?s label
is ?NP? and its first child?s label is ?WHNP? or
?WHPP? or ?IN?, then if we can find a noun
in the left siblings of ?SBAR?, this subordi-
nate clause could be an attributive clause or
appositive clause. Therefore the found noun
node should be included in the compression
if the ?SBAR? is also included, because the
node ?SBAR? decorates the noun. For exam-
ple, the top part of Fig 2 is part of expanded
constituent parse tree of sentence ?Those who
knew David were all dead.? The nodes in el-
lipse should share the same status.
? If a node?s label is ?SBAR?, its parent?s label
is ?VP? and its first child?s label is ?WHNP?,
then if we can find a verb in the left siblings
of ?SBAR?, this subordinate clause could be
an objective clause. Therefore, the found
verb node should be included in the compres-
sion if the ?SBAR? node is also included, be-
cause the node ?SBAR? is the object of that
verb. An example is shown in the bottom part
of Fig 2. The nodes in ellipse should share the
same status.
? If a node?s label is ?SBAR?, its parent?s
label is ?VP? and its first child?s label is
?WHADVP?, then if the first leaf for this node
is a wh-word (e.g., ?where, when, why?) or
?how?, this clause may be an objective clause
(when the word is ?why, how, where?) or at-
tributive clause (when the word is ?where?) or
adverbial clause (when the word is ?when?).
Therefore, similar to above, if a verb or noun
is found in the left siblings of ?SBAR?, the
VBD/ 
knew 
NNP/ 
David 
NP 
DT 
DT 
DT 
VP 
WP 
WP/ 
who 
DT/ 
Those 
VBD 
S 
NP 
PRP/ 
he 
PRP/ 
    I 
VBP/ 
believe  
PRP VBP 
 WP/ 
what 
WHNP S PRP 
 
VBP 
VBD/ 
said 
SBAR 
VP NP 
NP VP WP PRP 
 
VBP 





















NP 
SBAR 
WHNP 
WHNP 
 
S 
Figure 2: Expanded constituent parse tree for ex-
amples.
found verb or noun node should be included
in the compression if the ?SBAR? node is also
included.
? If a node?s label is ?SBAR? and its parent?s la-
bel is ?ADJP?, then if we can find a ?JJ?, ?JJR?,
or ?JJS? in the left siblings of ?SBAR?, the
?SBAR? node should be included in the com-
pression if the found ?JJ?, ?JJR? or ?JJS? node
is also included because the node ?SBAR? is
decorated by the adjective.
? The node with a label of ?PRN? can be re-
moved without other constraints.
We also include some other constraints based on
the Stanford dependency parse tree. Table 1 lists
the dependency relations we considered.
? For type I relations, the parent and child node
with those relationships should have the same
value in the compressed result (both are kept
or removed).
? For type II relations, if the child node in
those relations is retained in the compressed
sentence, the parent node should be also re-
tained.
695
Dependency Relation Example
prt: phrase verb particle They shut down the station. prt(shut,down)
prep: prepositional modifier He lives in a small village. prep(lives,in)
I pobj: object of a preposition I sat on the chair. pobj(on,chair)
nsubj: nominal subject The boy is cute. nsubj(cute,boy)
cop: copula Bill is big. cop(big,is)
partmod: participial modifier Truffles picked during the spring are tasty. partmod(truffles,picked)
II nn: noun compound modifier Oil price futures. nn(futures,oil)
acomp: adjectival complement She looks very beautiful. acomp(looks,beautiful)
pcomp: prepositional complement He felt sad after learning that tragedy. pcomp(after,learning)
III ccomp: clausal complement I am certain that he did it. ccomp(certain,did)
tmod: temporal modifier Last night I swam in the pool. tmod(swam,night)
Table 1: Some dependency relations used for extra constraints. All the examples are from (Marneffe and
Manning, 2002)
? For type III relations, if the parent node in
these relations is retained, the child node
should be kept as well.
3.4 Features
So far we have defined the decoding process
and related constraints used in decoding. These
all rely on the score function s(x, y) = w ?
f(x, L(y
j?1
), L(y
j
)) for every level in the con-
stituent parse tree. We included all the features in-
troduced in (Clarke and Lapata, 2008) (those fea-
tures are designed for leaves). Table 2 lists the
additional features we used in our system.
General Features for Every Node
1. individual node label and concatenation of a pair of
nodes
2. distance of two nodes at the same level
3. is the node at beginning or end at that level?
4. do the two nodes have the same parent?
5. if two nodes do not have the same parent, then is the left
node the rightmost child of its parent? is the right node the
leftmost child of its parent?
6. combination of parent label if the node pair are not
under the same parent
7. number of node?s children: 1/0/>1
8. depth of nodes in the parse tree
Extra Features for Leaf nodes
1. word itself and concatenation of two words
2. POS and concatenation of two words? POS
3. whether the word is a stopword
4. node?s named entity tag
5. dependency relationship between two leaves
Table 2: Features used in our system besides those
used in (Clarke and Lapata, 2008).
3.5 Learning
To learn the feature weights during training, we
perform ILP decoding on every sentence in the
training set, to find the best hypothesis for each
node in the expanded constituent parse tree. If
the hypothesis is incorrect, we update the feature
weights using the structured perceptron learning
strategy (Collins, 2002). The reference label for
every node in the expanded constituent parse tree
is obtained automatically from the bottom to the
top of the tree. Since every leaf node (word) is
human annotated (removed or retain), we annotate
the internal nodes as removed if all of its children
are removed. Otherwise, the node is annotated as
retained.
During perceptron training, a fixed learning rate
is used and parameters are averaged to prevent
overfitting. In our experiment, we observe sta-
ble convergence using the held-out development
corpus, with best performance usually obtained
around 10-20 epochs.
4 Summarization System
Similar to (Li et al., 2013a), our summarization
system is , which consists of three key compo-
nents: an initial sentence pre-selection module
to select some important sentence candidates; the
above compression model to generate n-best com-
pressions for each sentence; and then an ILP sum-
marization method to select the best summary sen-
tences from the multiple compressed sentences.
The sentence pre-selection model is a simple su-
pervised support vector regression (SVR) model
that predicts a salience score for each sentence and
selects the top ranked sentences for further pro-
cessing (compression and summarization). The
target value for each sentence during training is
the ROUGE-2 score between the sentence and the
human written abstracts. We use three common
features: (1) sentence position in the document;
(2) sentence length; and (3) interpolated n-gram
document frequency as introduced in (Ng et al.,
2012).
The final sentence selection process follows the
696
ILP method introduced in (Gillick et al., 2009).
Word bi-grams are used as concepts, and their doc-
ument frequency is used as weights. Since we use
multiple compressions for one sentence, an addi-
tional constraint is used: for each sentence, only
one of its n-best compressions may be included in
the summary.
For the compression module, using the ILP
method described above only finds the best com-
pression result for a given sentence. To generate
n-best compression candidates, we use an iterative
approach ? we add one more constraints to prevent
it from generating the same answer every time af-
ter getting one solution.
5 Experimental Results
5.1 Experimental Setup
Summarization Data For summarization experi-
ments, we use the standard TAC data sets1, which
have been used in the NIST competitions. In par-
ticular, we used the TAC 2010 data set as train-
ing data for the SVR sentence pre-selection model,
TAC 2009 data set as development set for parame-
ter tuning, and the TAC 2008 and 2011 data as the
test set for reporting the final summarization re-
sults. The training data for the sentence compres-
sion module in the summarization system is sum-
mary guided compression corpus annotated by (Li
et al., 2013a) using TAC2010 data. In the com-
pression module, for each word we also used its
document level feature.2
Compression Data We also evaluate our com-
pression model using the data set from (Clarke
and Lapata, 2008). It includes 82 newswire arti-
cles with manually produced compression for each
sentence. We use the same partitions as (Martins
and Smith, 2009), i.e., 1,188 sentences for training
and 441 for testing.
Data Processing We use Stanford CoreNLP
toolkit3 to tokenize the sentences, extract name en-
tity tags, and generate the dependency parse tree.
Berkeley Parser (Petrov et al., 2006) is adopted
to obtain the constituent parse tree for every sen-
tence and POS tag for every token. We use Pocket
1http://www.nist.gov/tac/data/index.html
2Document level features for a word include information
such as the word?s document frequency in a topic. These
features cannot be extracted from a single sentence, as in the
standard sentence compression task, and are related to the
document summarization task.
3http://nlp.stanford.edu/software/corenlp.shtml
CRF4 to implement the CRF sentence compres-
sion model. SVMlight5 is used for the summary
sentence pre-selection model. Gurobi ILP solver6
does all ILP decoding.
5.2 Summarization Results
We compare our summarization system against
four recent studies, which have reported some of
the highest published results on this task. Berg-
Kirkpatrick et al. (2011) introduced a joint model
for sentence extraction and compression. Wood-
send and Lapata (2012) learned individual sum-
mary aspects from data, e.g., informativeness, suc-
cinctness, grammaticalness, stylistic writing con-
ventions, and jointly optimized the outcome in
an ILP framework. Ng et al. (2012) exploited
category-specific information for multi-document
summarization. Almeida and Martins (2013) pro-
posed compressive summarization method by dual
decomposition and multi-task learning. Our sum-
marization framework is the same as (Li et al.,
2013a), except they used a CRF-based compres-
sion model. In addition to the four previous stud-
ies, we also report the best achieved results in the
TAC competitions.
Table 3 shows the summarization results of our
method and others. The top part contains the re-
sults for TAC 2008 data and bottom part is for
TAC 2011 data. We use the ROUGE evaluation
metrics (Lin, 2004), with R-2 measuring the bi-
gram overlap between the system and reference
summaries and R-SU4 measuring the skip-bigram
with the maximum gap length of 4. In addition,
we evaluate the linguistic quality (LQ) of the sum-
maries for our system and (Li et al., 2013a).7 The
linguistic quality consists of two parts. One eval-
uates the grammar quality within a sentence. For
this, annotators marked if a compressed sentence
is grammatically correct. Typical grammar errors
include lack of verb or subordinate clause. The
other evaluates the coherence between sentences,
including the order of sentences and irrelevant sen-
tences. We invited 3 English native speakers to do
this evaluation. They gave every compressed sen-
tence a grammar score and a coherence score for
4http://sourceforge.net/projects/pocket-crf-1/
5http://svmlight.joachims.org/
6http://www.gurobi.com
7We chose to evaluate the linguistic quality for this system
because of two reasons: one is that we have an implementa-
tion of that method; the other more important one is that it
has the highest reported ROUGE results among the compared
methods.
697
System R-2 R-SU4 Gram Cohere
TAC?08 Best System 11.03 13.96 n/a n/a
(Berg-Kirk et al., 2011) 11.70 14.38 n/a n/a
(Woodsend et al., 2012) 11.37 14.47 n/a n/a
(Almeida et al.,2013) 12.30 15.18 n/a n/a
(Li et al., 2013a) 12.35 15.27 3.81 3.41
Our System 12.23 15.47 4.29 4.11
TAC?11 Best System 13.44 16.51 n/a n/a
(Ng et al., 2012) 13.93 16.83 n/a n/a
(Li et al., 2013a) 14.40 16.89 3.67 3.32
Our System 14.04 16.67 4.18 4.07
Table 3: Summarization results on the TAC 2008
and 2011 data sets.
each topic. The score is scaled and ranges from 1
(bad) to 5 (good). Therefore, in table 3, the gram-
mar score is the average score for each sentence
and coherence score is the average for each topic.
We measure annotators? agreement in the follow-
ing way: we consider the scores from each anno-
tator as a distribution and we find that these three
distributions are not statistically significantly dif-
ferent each other (p > 0.05 based on paired t-test).
We can see from the table that in general, our
system achieves better ROUGE results than most
previous work except (Li et al., 2013a) on both
TAC 2008 and TAC 2011 data. However, our
system?s linguistic quality is better than (Li et
al., 2013a). The CRF-based compression model
used in (Li et al., 2013a) can not well model the
grammar. Particularly, our results (ROUGE-2) are
statistically significantly (p < 0.05) higher than
TAC08 Best system, but are not statistically signif-
icant compared with (Li et al., 2013a) (p > 0.05).
The pattern is similar in TAC 2011 data. Our result
(R-2) is statistically significantly (p < 0.05) better
than TAC11 Best system, but not statistically (p >
0.05) significantly different from (Li et al., 2013a).
However, for the grammar and coherence score,
our results are statistically significantly (p < 0.05)
than (Li et al., 2013a). All the above statistics are
based on paired t-test.
5.3 Compression Results
The results above show that our summarization
system is competitive. In this section we focus
on the evaluation of our proposed compression
method. We compare our compression system
against four other models. HedgeTrimmer in Dorr
et al. (2003) applied a variety of linguistically-
motivated heuristics to guide the sentences com-
System C Rate (%) Uni-F1 Rel-F1
HedgeTrimmer 57.64 0.64 0.50
McDonald (2006) 70.95 0.77 0.55
Martins (2009) 71.35 0.77 0.56
Wang (2013) 68.06 0.79 0.59
Our System 71.19 0.77 0.58
Table 4: Sentence compression results. The hu-
man compression rate of the test set is 69%.
pression; McDonald (2006) used the output of two
parsers as features in a discriminative model that
decomposes over pairs of consecutive words; Mar-
tins and Smith (2009) built the compression model
in the dependency parse and utilized the relation-
ship between the head and modifier to preserve the
grammar relationship; Wang et al. (2013) devel-
oped a novel beam search decoder using the tree-
based compression model on the constituent parse
tree, which could find the most probable compres-
sion efficiently.
Table 4 shows the compression results of vari-
ous systems, along with the compression ratio (C
Rate) of the system output. We adopt the com-
pression metrics as used in (Martins and Smith,
2009) that measures the macro F-measure for the
retained unigrams (Uni-F1), and the one used
in (Clarke and Lapata, 2008) that calculates the
F1 score of the grammatical relations labeled by
(Briscoe and Carroll, 2002) (Rel-F1). We can see
that our proposed compression method performs
well, similar to the state-of-the-art systems.
To evaluate the power of using the expanded
parse tree in our model, we conducted another ex-
periment where we only consider the bottom level
of the constituent parse tree. In some sense, this
could be considered as the system in (Clarke and
Lapata, 2008). Furthermore, we use two differ-
ent setups: one uses the lexical features (about the
words) and the other does not. Table 5 shows the
results using the data in (Clarke and Lapata, 2008).
For a comparison, we also include the results us-
ing the CRF-based compression model (the one
used in (Nomoto, 2007; Li et al., 2013a)). We
report results using both the automatically calcu-
lated compression metrics and the linguistic qual-
ity score. Three English native speaker annotators
were asked to judge two aspects of the compressed
sentence compared with the gold result: one is the
content that looks at whether the important words
are kept and the other is the grammar score which
evaluates the sentence?s readability. Each of these
698
two scores ranges from 1(bad) to 5(good).
Table 5 shows that when using lexical features,
our system has statistically significantly (p < 0.05)
higher Grammar value and content importance
value than the CRF and the leaves only system.
When no lexical features are used, default system
can achieve statistically significantly (p < 0.01)
higher results than the CRF and the leaves only
system.
We can see that using the expanded parse tree
performs better than using the leaves only, espe-
cially when lexical features are not used. In ad-
dition, we observe that our proposed compression
method is more generalizable than the CRF-based
model. When our system does not use lexical
features in the leaves, it achieves better perfor-
mance than the CRF-based model. This is impor-
tant since such a model is more robust and may be
used in multiple domains, whereas a model rely-
ing on lexical information may suffer more from
domain mismatch. From the table we can see our
proposed tree based compression method consis-
tently has better linguistic quality. On the other
hand, the CRF compression model is the most
computationally efficient one among these three
compression methods. It is about 200 times faster
than our model using the expanded parse tree. Ta-
ble 6 shows some examples using different meth-
ods.
System C Rate(%) Uni-F1 Rel-F1 Gram Imp
Using lexical features
CRF 79.98 0.80 0.51 3.9 4.0
ILP(I) 80.54 0.79 0.57 4.0 4.2
ILP(II) 79.90 0.80 0.57 4.2 4.4
No lexical features
CRF 77.75 0.78 0.51 3.35 3.5
ILP(I) 77.77 0.78 0.56 3.7 3.9
ILP(II) 77.78 0.80 0.58 4.1 4.2
Table 5: Sentence compression results: effect of
lexical features and expanded parse tree. ILP(I)
represents the system using only bottom nodes in
constituent parse tree. ILP(II) is our system. Imp
means the content importance value.
6 Conclusion
In this paper, we propose a discriminative ILP sen-
tence compression model based on the expanded
constituent parse tree, which aims to improve the
linguistic quality of the compressed sentences in
the summarization task. Linguistically motivated
constraints are incorporated to improve the sen-
tence quality. We conduct experiments on the TAC
Using lexical features
Source:
Apart from drugs, detectives believe money is laun-
dered from a variety of black market deals involving
arms and high technology.
Human compress:
detectives believe money is laundered from a variety of
black market deals.
CRF result :
Apart from drugs detectives believe money is laundered
from a black market deals involving arms and technol-
ogy.
ILP(I) Result:
detectives believe money is laundered from a variety of
black deals involving arms.
ILP(II) Result:
detectives believe money is laundered from black mar-
ket deals.
No lexical features
Source:
Mrs Allan?s son disappeared in May 1989, after a party
during his back packing trip across North America.
Human compress:
Mrs Allan?s son disappeared in 1989, after a party dur-
ing his trip across North America.
CRF result :
Mrs Allan?s son disappeared May 1989, after during his
packing trip across North America.
ILP(I) Result:
Mrs Allan?s son disappeared in May, 1989, after a party
during his packing trip across North America .
ILP(II) Result:
Mrs Allan?s son disappeared in May 1989, after a party
during his trip.
Table 6: Examples of original sentences and their
compressed sentences from different systems.
2008 and 2011 summarization data sets and show
that by incorporating this sentence compression
model, our summarization system can yield signif-
icant performance gain in linguistic quality with-
out losing much ROUGE results. The analysis
of the compression module also demonstrates its
competitiveness, in particular the better linguistic
quality and less reliance on lexical cues.
Acknowledgments
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts
of this paper. The work is also partially sup-
ported by NSF award IIS-0845484 and DARPA
Contract No. FA8750-13-2-0041. Any opinions,
findings, and conclusions or recommendations ex-
pressed are those of the authors and do not neces-
sarily reflect the views of the funding agencies.
699
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013.
Fast and robust compressive summarization with
dual decomposition and multi-task learning. In Pro-
ceedings of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of LREC.
Yllias Chali and Sadid A. Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of COLING.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. the-
sis, University of Southern California.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of NAACL.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of EMNLP.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen McKeown. 2007. Lexi-
calized markov grammars for sentence compression.
In Processings of NAACL.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
icsi/utd summarization system at tac 2009. In Pro-
ceedings of TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of SIGIR.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of the EMNLP.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using su-
pervised bigram-based ilp for extractive summariza-
tion. In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression - A pilot study.
In Proceeding of the Sixth International Workshop
on Information Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2013. Towards abstractive
speech summarization: Exploring unsupervised and
supervised approaches for spoken utterance com-
pression. IEEE Transactions on Audio, Speech, and
Language Processing.
Marie-Catherine de Marneffe and Christopher D Man-
ning. 2002. Stanford typed dependencies manual.
Andre F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extrac-
tion and compression. In Proceedings of the ACL
Workshop on Integer Linear Programming for Natu-
ral Language Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew-Lim Tan. 2012. Exploiting
category-specific information for multi-document
summarization. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Informa-
tion Processing and Management.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
700
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of Coling.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information Processing &
Management.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceedings
of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of
ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. In Information Processing and Man-
agement.
701
Proceedings of NAACL-HLT 2013, pages 1152?1162,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Participant-based Approach for Event Summarization Using
Twitter Streams
Chao Shen1, Fei Liu2, Fuliang Weng2, Tao Li1
1School of Computing and Information Sciences, Florida International University
Miami, Florida 33199, USA
2Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{cshen001, taoli}@cs.fiu.edu
{fei.liu, fuliang.weng}@us.bosch.com
Abstract
Twitter offers an unprecedented advantage on
live reporting of the events happening around
the world. However, summarizing the Twit-
ter event has been a challenging task that was
not fully explored in the past. In this paper,
we propose a participant-based event summa-
rization approach that ?zooms-in? the Twit-
ter event streams to the participant level, de-
tects the important sub-events associated with
each participant using a novel mixture model
that combines the ?burstiness? and ?cohesive-
ness? properties of the event tweets, and gen-
erates the event summaries progressively. We
evaluate the proposed approach on different
event types. Results show that the participant-
based approach can effectively capture the
sub-events that have otherwise been shadowed
by the long-tail of other dominant sub-events,
yielding summaries with considerably better
coverage than the state-of-the-art.
1 Introduction
Twitter has increasingly become a critical source of
information. People report the events they are ex-
periencing or publish comments on a wide variety
of events happening around the world, ranging from
the unexpected natural disasters, regional riots, to
many scheduled events, such as sports games, po-
litical debates, local festivals, and even academic
conferences. The Twitter data streams thus cover
a broad range of events and broadcast these in-
formation in a live manner. Event summarization
in this paper aims to generate a representative and
concise textual description of the scheduled events
that are being lively reported on Twitter, providing
people with an alternative means of observing the
world beyond the traditional journalism. Specifi-
cally, we investigate scheduled events of different
types, including six of the NBA (National Basket-
ball Association) sports games and a representative
conference event, namely the Apple CEO?s keynote
speech in the Apple Worldwide Developers Confer-
ence (WWDC 2012)1. All these events have excited
great discussion among the Twitter community.
Summarizing the Twitter event is a challenging
task that has yet been fully explored in the past.
Most previous summarization studies focus on the
well-formatted news documents, as driven by the
annual DUC2 and TAC3 evaluations. In contrast,
the Twitter messages (a.k.a., tweets) are very short
and noisy, containing nonstandard terms such as ab-
breviations, acronyms, emoticons, etc. (Liu et al,
2011b; Liu et al, 2012; Eisenstein, 2013). The
noisy contents also cause great difficulties to the tra-
ditional NLP tools such as NER and dependency
parser (Ritter et al, 2011; Foster et al, 2011), lim-
iting the possibility of applying finer-grained event
analysis tools. In nature, the event tweets are closely
associated with the timeline and are drastically dif-
ferent from a static collection of news documents.
The tweets converge into text streams that pulse
along the timeline and cluster around the important
moments or sub-events. These ?sub-events? are of
crucial importance since they represent a surge of in-
terest from the Twitter audience and the correspond-
1https://developer.apple.com/wwdc/
2http://duc.nist.gov/
3http://www.nist.gov/tac/
1152
Figure 1: Example Twitter event stream (upper) and par-
ticipant stream (lower). Event stream contains tweets
related to an NBA basketball game (Spurs vs Thunder)
scheduled on May 31, 2012; participant stream contains
tweets corresponding to the player Russell Westbrook in
team Thunder. X-axis denotes the timeline and y-axis
represents the number of tweets per 10-second interval.
ing key information must be reflected in the event
summary. As such, event summarization research
has been focusing on developing accurate sub-event
detection systems and generating text descriptions
that can best summarize the sub-events in a progres-
sive manner (Chakrabarti and Punera, 2011; Nichols
et al, 2012; Zubiaga et al, 2012).
In Figure 1, we show an example Twitter event
stream and one of its ?participant? streams. The
event stream contains all the tweets related to an
NBA basketball game Spurs vs Thunder; while
the participant stream contains only tweets corre-
sponding to the player Russell Westbrook in this
game. Previous research on event summarization
focuses on identifying the important moments from
the coarse-level event stream. This may yield sev-
eral side effects: first, the spike patterns are not
clearly identifiable from the overall event stream,
though they are more clearly seen if we ?zoom-in? to
the participant level; second, it is arguable whether
the important sub-events can be accurately detected
based solely on the tweet volume change; third, a
popular participant or sub-event can elicit huge vol-
ume of tweets which dominant the event discussion
and shield less prominent sub-events. For example,
in the NBA games, discussions about the key players
(e.g., ?LeBron James?, ?Kobe Bryant?) can heavily
shadow other important participants or sub-events,
resulting in an event summary with repetitive de-
scriptions about the dominant players.
In this work, we propose a novel participant-
based event summarization approach, which dynam-
ically identifies the participants from data streams,
then ?zooms-in? the event stream to participant
level, detects the important sub-events related to
each participant using a novel time-content mixture
model, and generates the event summary progres-
sively by concatenating the descriptions of the im-
portant sub-events. Results show that the mixture
model-based sub-event detection approach can effi-
ciently incorporate the ?burstiness? and ?cohesive-
ness? of the participant streams, and the participant-
based event summarization can effectively capture
the sub-events that have otherwise been shadowed
by the long-tail of other dominant sub-events, yield-
ing summaries with considerably better coverage
than the state-of-the-art approach.
2 Related Work
Mining Twitter for event information has received
increasing attention in recent years. Many research
studies focus on identifying the trending events from
Twitter and providing a concise and dynamic visual-
ization of the information. The identified events are
often represented using a set of keywords. (Petro-
vic et al, 2010) proposed an algorithm based on
locality-sensitive hashing for detecting new events
from a stream of Twitter posts. (O?Connor et al,
2010; Becker et al, 2011b; Becker et al, 2011a;
Weng et al, 2011) proposed demo systems to dis-
play the event-related themes and popular tweets,
allowing the users to navigate through their topic
of interest. (Zhao et al, 2011) described an effort
to perform data collection and event recognition de-
spite various limits to the free access of Twitter data.
(Diao et al, 2012) integrated both temporal infor-
mation and users? personal interests for bursty topic
detection from the microblogs. (Ritter et al, 2012)
described an open-domain event-extraction and cat-
egorization system, which extracts an open-domain
calendar of significant events from Twitter.
With the identified events of interest, there is an
ever-increasing demand for event summarization,
which distills the huge volume of Twitter discus-
sions into a concise and representative textual de-
scription of the events. Many studies start with
the text summarization approaches that have been
shown to perform well on the news documents and
1153
develop adaptations to fit these methods to a col-
lection of event tweets. (Sharifi et al, 2010b) pro-
posed a graph-based phrase reinforcement algorithm
to build a one-sentence summary from a collection
of topic tweets. (Sharifi et al, 2010a; Inouye and
Kalita, 2011) presented a hybrid TF-IDF approach
to extract one- or multiple-sentence summary for
each topic. (Liu et al, 2011a) proposed to use
the concept-based ILP framework for summarizing
the Twitter trending topics, using both tweets and
the webpages linked from the tweets as input text
sources. (Harabagiu and Hickl, 2011) introduced a
generative framework that incorporates event struc-
ture and user behavior information in summarizing
multiple microblog posts related to the same topic.
Regarding summarizing the data streams, (Mar-
cus et al, 2011) introduced a ?TwitInfo? system to
visually summarize and track the events on Twit-
ter. They proposed an automatic peak detection and
labeling algorithm for the social streams. (Taka-
mura et al, 2011) proposed a summarization model
based on the facility location problem, which gener-
ates summary for a stream of short documents along
the timeline. (Chakrabarti and Punera, 2011) pro-
posed an event summarization algorithm based on
learning an underlying hidden state representation
of the event via hidden Markov models. (Louis and
Newman, 2012) presented a method for summariz-
ing a collection of tweets related to a business. The
proposed procedure aggregates tweets into subtopic
clusters which are then ranked and summarized
by a few representative tweets from each cluster.
(Nichols et al, 2012; Zubiaga et al, 2012) focused
on real-time event summarization, which detects the
sub-events by identifying those moments where the
tweet volume has increases sharply, then uses var-
ious weighting schemes to perform tweet selection
and finally generates the event summary.
Our work is different from the above research
studies in three folds: first, we propose to ?zoom-
in? the Twitter event streams to the participant
level, which allows us to clearly identify the im-
portant sub-events associated with each participant
and generate a balanced event summary with com-
prehensive coverage of all the important sub-events;
second, we propose a novel time-content mixture
model approach for sub-event detection, which ef-
fectively leverages the ?burstiness? and ?cohesive-
ness? of the event tweets and accurately detects
the participant-level sub-events. Third, we evalu-
ate the participant-based event summarization sys-
tem on different event types and demonstrate that the
proposed approach outperforms the state-of-the-art
method by a considerable margin.
3 Participant-based Event Summarization
We propose a novel participant-centered event sum-
marization approach that consists of three key com-
ponents: (1) ?Participant Detection? dynamically
identifies the event participants and divides the
entire event stream into a number of participant
streams (Section 3.1); (2) ?Sub-event Detection? in-
troduces a novel time-content mixture model ap-
proach to identify the important sub-events associ-
ated with each participant; these ?participant-level
sub-events? are then merged along the timeline to
form a set of ?global sub-events?4, which capture
all the important moments in the event stream (Sec-
tion 3.2); (3) ?Summary Tweet Extraction? extracts
the representative tweets from the global sub-events
and forms a comprehensive coverage of the event
progress (Section 3.3).
3.1 Participant Detection
We define event participants as the entities that play
a significant role in shaping the event progress. ?Par-
ticipant? is a general concept to denote the event
participating persons, organizations, product lines,
etc., each of which can be captured by a set of
correlated proper nouns. For example, the NBA
player ?LeBron Raymone James? can be represented
by {LeBron James, LeBron, LBJ, King James, L.
James}, where each proper noun represents a unique
mention of the participant. In this work, we automat-
ically identify the proper nouns from tweet streams,
filter out the infrequent ones using a threshold ?,
and cluster them into individual event participants.
This process allows us to dynamically identify the
key participating entities and provide a full-coverage
for these participants in the event summary.
4We use ?participant sub-events? and ?global sub-events?
respectively to represent the important moments happened on
the participant-level and on the entire event-level. A ?global
sub-event? may consist of one or more ?participant sub-events?.
For example., the ?steal? action in the basketball game typically
involves both the defensive and offensive players, and can be
generated by merging the two participant-level sub-events.
1154
We formulate the participant detection in a hier-
archical agglomerative clustering framework. The
CMU TweetNLP tool (Gimpel et al, 2011) was used
for proper noun tagging. The proper nouns (a.k.a.,
mentions) are grouped into clusters in a bottom-up
fashion. Two mentions are considered similar if they
share (1) lexical resemblance, and (2) contextual
similarity. For example, in the following two tweets
?Gotta respect Anthony Davis, still rocking the uni-
brow?, ?Anthony gotta do something about that uni-
brow?, the two mentions Anthony Davis and An-
thony are referring to the same participant and they
share both character overlap (?anthony?) and con-
text words (?unibrow?, ?gotta?). We use sim(ci, cj)
to represent the similarity between two mentions ci
and cj , defined as:
sim(ci, cj) = lex sim(ci, cj)?cont sim(ci, cj)
where the lexical similarity (lex sim(?)) is defined
as a binary function representing whether a mention
ci is an abbreviation, acronym, or part of another
mention cj , or if the character edit distance between
the two mentions is less than a threshold ?5:
lex sim(ci, cj)=
?
?
?
1 ci(cj) is part of cj(ci)
1 EditDist(ci, cj) < ?
0 Otherwise
We define the context similarity (cont sim(?)) of
two mentions as the cosine similarity between their
context vectors ~vi and ~vj . Note that on the tweet
stream, two temporally distant tweets can be very
different even though they are lexically similar, e.g.,
two slam dunk shots performed by the same player
at different time points are different. We there-
fore restrain the context to a segment of the tweet
stream |Sk| and then take the weighted average of
the segment-based similarity as the final context
similarity. To build the context vector, we use term
frequency (TF) as the term weight and remove all the
stopwords. We use |D| to represent the total tweets
in the event stream.
cont sim|Sk|(ci, cj) = cos(~vi, ~vj)
cont sim(ci, cj) =
?
k
|Sk|
|D|
? cont sim|Sk|(ci, cj)
5? was empirically set as 0.2?min{|ci|, |cj |}
t w
Wz? |D|
? ? ? ?'K B
Figure 2: Plate notation of the mixture model.
Similarity between two clusters of mentions are de-
fined as the maximum possible similarity between a
pair of mentions, each from one cluster:
sim(Ci, Cj) = max
ci?Ci,cj?Cj
sim(ci, cj)
We perform bottom-up agglomerative clustering on
the mentions until a stopping threshold ? has been
reached for sim(Ci, Cj). The clustering approach
naturally groups the frequent proper nouns into par-
ticipants. The participant streams are then formed
by gathering the tweets that contain one or more
mentions in the participant cluster.
3.2 Mixture Model-based Sub-event Detection
A sub-event corresponds to a topic that emerges
from the data stream, being intensively discussed
during a short period, and then gradually fades away.
The tweets corresponding to a sub-event thus de-
mand not only ?temporal burstiness? but also a cer-
tain degree of ?lexical cohesiveness?. To incorporate
both the time and content aspects of the sub-events,
we propose a mixture model approach for sub-event
detection. Figure 2 shows the plate notation.
In the proposed model, each tweet d in the data
stream D is generated from a topic z, weighted by
piz . Each topic is characterized by both its content
and time aspects. The content aspect is captured by
a multinomial distribution over the words, param-
eterized by ?; while the time aspect is character-
ized by a Gaussian distribution, parameterized by ?
and ?, with ? represents the average time point that
the sub-event emerges and ? determines the duration
of the sub-event. These distributions bear similari-
ties with the previous work (Hofmann, 1999; Allan,
2002; Haghighi and Vanderwende, 2009). In addi-
tion, there are often background or ?noise? topics
that are being constantly discussed over the entire
1155
event evolvement process and do not present the de-
sired ?burstiness? property. We use a uniform dis-
tribution U(tb, te) to model the time aspect of these
?background? topics, with tb and te being the event
beginning and end time points. The content aspect
of a background topic is modeled by similar multi-
nomial distribution, parameterized by ??. We use the
maximum likelihood parameter estimation. The data
likelihood can be represented as:
L(D) =
?
d?D
?
z
{pizpz(td)
?
w?d
pz(w)}
where pz(td) models the timestamp of tweet d under
the topic z; pz(w) corresponds to the word distribu-
tion in topic z. They are defined as:
pz(td) =
{
N(td;?z, ?z) if z is a sub-event topic
U(tb, te) if z is background topic
pz(w) =
{
p(w; ?z) if z is a sub-event topic
p(w; ??z) if z is background topic
where both p(w; ?z) and p(w; ??z) are multinomial
distributions over the words. Initially, we assume
there are K sub-event topics and B background top-
ics and use the EM algorithm for model fitting. The
EM equations are listed below:
E-step:
p(zd = j) ?
?
?
?
pijN(d;?j , ?j)
?
w?d
p(w; ?j) if j <= K
pijU(tb, te)
?
w?d
p(w; ??j) else
M-step:
pij ?
?
d
p(zd = j)
p(w; ?j) ?
?
d
p(zd = j)? c(w, d)
p(w; ??j) ?
?
d
p(zd = j)? c(w, d)
?j =
?
d p(zd = j)? td
?K
j=1
?
d p(zd = j)
?2j =
?
d p(zd = j)? (td ? ?j)
2
?K
j=1
?
d p(zd = j)
To process the data stream D, we divide the data
into 10-second bins and process each bin at a time.
The peak time of a sub-event was determined as
the bin that has the most tweets related to this sub-
event. During EM initialization, the number of sub-
event topics K was empirically decided by scanning
through the data stream and examine tweets in ev-
ery 3-minute stream segment. If there was a spike6,
we add a new sub-event to the model and use the
tweets in this segment to initialize the value of ?,
?, and ?. Initially, we use a fixed number of back-
ground topics with B = 4. A topic re-adjustment
was performed after the EM process. We merge two
sub-events in a data stream if they (1) locate closely
in the timeline, with peaks times within a 2-minute
window; and (2) share similar word distributions:
among the top-10 words with highest probability in
the word distributions, there are over 5 words over-
lap. We also convert the sub-event topics to back-
ground topics if their ? values are greater than a
threshold ?7. We then re-run the EM to obtain the
updated parameters. The topic re-adjustment pro-
cess continues until the number of sub-events and
background topics do not change further.
We obtain the ?participant sub-events? by ap-
plying this sub-event detection approach to each of
the participant streams. The ?global sub-events?
are obtained by merging the participant sub-events
along the timeline. We merge two participant sub-
events into a global sub-event if (1) their peaks are
within a 2-minute window, and (2) the Jaccard simi-
larity (Lee, 1999) between their associated tweets is
greater than a threshold (set to 0.1 empirically). The
tweets associated with each global sub-event are the
ones with p(z|d) greater than a threshold ?, where z
is one of the participant sub-events and ? was set to
0.7 empirically. After the sub-event detection pro-
cess, we obtain a set of global sub-events and their
associated event tweets.8
3.3 Summary Tweet Extraction
We extract a representative tweet from each of the
global sub-events and concatenate them to form an
informative event summary. Note that our goal in
this work is to identify all the important moments
6We use the algorithm described in (Marcus et al, 2011) as
a baseline and ad hoc spike detection algorithm.
7? was set to 5 minutes in our experiments.
8We empirically set some threshold values in the topic re-
adjustment and sub-event merging process. In future, we would
like to explore more principled way of parameter selection.
1156
Event Date Duration #Tweets
Lakers vs Okc 05/19/2012 3h10m 218,313
N Celtics vs 76ers 05/23/2012 3h30m 245,734
B Celtics vs Heat 05/30/2012 3h30m 345,335
A Spurs vs Okc 05/31/2012 3h 254,670
Heat vs Okc (1) 06/12/2012 3h30m 331,498
Heat vs Okc (2) 06/21/2012 3h30m 332,223
Apple?s WWDC?12 Conf. 06/11/2012 3h30m 163,775
Table 1: Statistics of the data set, including six NBA bas-
ketball games and the WWDC 2012 conference event.
for event summarization, but not on proposing new
methods for tweet selection. We thus use the Hybrid
TF-IDF approach (Sharifi et al, 2010a; Liu et al,
2011a) to extract the representative sentences from
a collection of tweets. In this approach, each tweet
was considered as a sentence. The sentences were
ranked according to the average TF-IDF score of the
consisting words; top weighted sentences were it-
eratively extracted, while excluding those that have
high cosine similarity with the existing summary
sentences. (Inouye and Kalita, 2011) showed the
Hybrid TF-IDF approach performs constantly better
than the phrase reinforcement algorithm and other
traditional summarization systems.
4 Data Corpus
We evaluate the proposed event summarization ap-
proach on six NBA basketball games and a repre-
sentative conference event, namely the Apple CEO?s
keynote speech in the Apple Worldwide Develop-
ers Conference (WWDC 2012)9. We use the het-
erogeneous event types to verify that the proposed
approach can robustly and efficiently produce sum-
maries on different event streams. The tweet streams
corresponding to these events are collected using
the Twitter Streaming API10 with pre-defined key-
word set. For NBA games, we use the team names,
first name and last name of the players and head
coaches as keywords for retrieving the event tweets;
for the WWDC conference, the keyword set contains
about 20 terms related to the Apple event, such as
?wwdc?, ?apple?, ?mac?, etc. We crawl the tweets
in real-time when these scheduled events are taking
place; nevertheless, certain non-event tweets could
be mis-included due to the broad coverage of the
used keywords. During preprocessing, we filter out
9https://developer.apple.com/wwdc/
10https://dev.twitter.com/docs/streaming-apis
Time Action (Sub-event) Score
9:22 Chris Bosh misses 10-foot two point shot 7-2
9:22 Serge Ibaka defensive rebound 7-2
9:11 Kevin Durant makes 15-foot two point shot 9-2
8:55 Serge Ibaka shooting foul (Shane Battier draws 9-2
the foul)
8:55 Shane Battier misses free throw 1 of 2 9-2
8:55 Miami offensive team rebound 9-2
8:55 Shane Battier makes free throw 2 of 2 9-3
Table 2: An example clip of the play-by-play live cov-
erage of an NBA game (Heat vs Okc). ?Time? corre-
sponds to the minutes left in the current quarter of the
game; ?Score? shows the score between the two teams.
the tweets containing URLs, non-English tweets,
and retweets since they are less likely containing
new information regarding the event progress. Ta-
ble 1 shows statistics of the event tweets after the
filtering process. In total, there are over 1.8 million
tweets used in the event summarization experiments.
We use the play-by-play live coverage collected
from the ESPN11 and MacRumors12 websites as ref-
erence, which provide detailed descriptions of the
NBA and WWDC events as they unfold. Table 2
shows an example clip of the play-by-play descrip-
tions of an NBA game. Ideally, each item in the live
coverage descriptions may correspond to a sub-event
in the tweet streams, but in reality, not all actions
would attract enough attention from the Twitter au-
dience. We use a human annotator to manually filter
out the actions that did not lead to any spike in the
corresponding participant stream. The rest items are
projected to the participant and event streams as the
goldstandard sub-events. The projection was man-
ually performed since the ?game clock? associated
with the goldstandard (first column in Table 2) does
not align well with the ?wall clock? due to the game
rules such as timeout and halftime rest. To evalu-
ate the participant detection performance, we ask the
annotator to manually group the proper noun men-
tions into clusters, each cluster corresponds to a par-
ticipant. The mentions that do not correspond to any
participant are discarded. The goldstandard event
summaries are generated by manually selecting one
representative tweet from each of the groundtruth
global sub-events. We choose not to use the play-
by-play descriptions as reference summaries since
their vocabulary is rather limited and do not overlap
with the tweet language.
11http://espn.go.com/nba/scoreboard
12http://www.macrumorslive.com/archive/wwdc12/
1157
Example Participants - NBA game
westbrook, russell westbrook
stephen jackson, steven jackson, jackson
james, james harden, harden
ibaka, serge ibaka
oklahoma city thunder, oklahoma
gregg popovich, greg popovich, popovich
kevin durant, kd, durant
thunder, okc, #okc, okc thunder, #thunder
Example Participants - WWDC Conference
macbooks, mbp, macbook pro, macbook air,...
google maps, google, apple maps
wwdc, apple wwdc, #wwdc
os, mountain, os x mountain, os x
iphone 4s, iphone 3gs, iphone
Table 3: Example participants automatically detected
from the NBA game Spurs vs Okc (2012-5-31) and the
WWDC?12 conference.
5 Experimental Results
We evaluate the participant-based event summariza-
tion in a cascaded fashion and present results for
each of the three components, including the par-
ticipant detection (Section 5.1), sub-event detection
(Section 5.2), and quantitative and qualitative evalu-
ation of example event summaries (Section 5.3).
5.1 Participant Detection Results
In Table 3, we show example participants that were
automatically detected by the proposed hierarchical
agglomerative clustering approach. We note that the
clusters include various mentions of the same event
participant, e.g., ?gregg popovich?, ?greg popovich?,
and ?popovich? are both referring to the head coach
of the team Spurs; ?macbooks?, ?macbook pro?,
?mbp? are referring to a line of products from Apple.
Quantitatively, we evaluate the participant detection
results on both participant- and mention-level. As-
sume the system-detected and the goldstandard par-
ticipant clusters are Ts and Tg respectively. We de-
fine a correct participant as a system detected par-
ticipant with more than half of its associated men-
tions are included in a goldstandard participant (re-
ferred to as the hit participant). As a result, we
can define the participant-level precision and recall
as below:
participant-prec = #correct-participants/|Ts|
participant-recall = #hit-participants/|Tg|
Note that a correct participant may include incor-
rect mentions, and that more than one correct par-
Figure 3: Participant detection performance. The upper
figures represent the participant-level precision and re-
call scores; while the lower figures represent the mention-
level precision and recall. X-axis corresponds to the six
NBA games and the WWDC conference.
ticipants may correspond to the same hit participant,
both of which are undesired. In the latter case, we
use representative participant to refer to the cor-
rect participant which contains the most mentions
in the hit participant. In this way, we build a 1-
to-1 mapping from the detected participants to the
groundtruth participants. Next, we define correct
mentions as the union of the overlapping mentions
between all pairs of representative and hit partici-
pants. Then we calculate the mention-level precision
and recall as the number of correct mentions divided
by the total mentions in the system or goldstandard
participant clusters.
Figure 3 shows the participant- and mention-level
precision and recall scores. We experimented with
different similarity measures for the agglomerative
clustering approach13. The ?global context? means
that the context vectors are created from the entire
data stream; this may not perform well since dif-
ferent participants can share similar global context.
E.g., the terms ?shot?, ?dunk?, ?rebound? can ap-
pear in the context of any NBA players and are not
13The stopping threshold ? was set to 0.15, local context
length is 3 minutes, and frequency threshold ? was set to 200.
1158
Participant-level Sub-event Detection Global Sub-event Detection
Event
#P #S
Spike MM
#S
Spike Participant + Spike Participant + MM
R P F R P F R P F R P F R P F
Lakers vs Okc 9 65 0.75 0.31 0.44 0.71 0.39 0.50 48 0.67 0.38 0.48 0.94 0.19 0.32 0.88 0.40 0.55
Celtics vs 76ers 10 88 0.52 0.39 0.45 0.53 0.43 0.47 60 0.65 0.51 0.57 0.72 0.18 0.29 0.78 0.39 0.52
Celtics vs Heat 14 152 0.53 0.29 0.37 0.50 0.38 0.43 67 0.57 0.41 0.48 0.97 0.21 0.35 0.91 0.28 0.43
Spurs vs Okc 12 98 0.78 0.46 0.58 0.84 0.57 0.68 81 0.41 0.42 0.41 0.88 0.35 0.50 0.91 0.54 0.68
Heat vs Okc (1) 15 123 0.75 0.27 0.40 0.72 0.35 0.47 85 0.41 0.47 0.44 0.94 0.20 0.33 0.96 0.34 0.50
Heat vs okc (2) 13 153 0.74 0.36 0.48 0.76 0.43 0.55 92 0.41 0.33 0.37 0.88 0.21 0.34 0.87 0.38 0.53
WWDC?12 10 56 0.64 0.14 0.23 0.59 0.33 0.42 43 0.53 0.26 0.35 0.77 0.14 0.24 0.70 0.31 0.43
Average 12 105 0.67 0.32 0.42 0.66 0.41 0.50 68 0.52 0.40 0.44 0.87 0.21 0.34 0.86 0.38 0.52
Table 4: Sub-event detection results on both participant and the event streams. ?Spike? corresponds to the spike
detection algorithm proposed in (Marcus et al, 2011); ?MM? represents our proposed time-content mixture model
approach. ?#P? and ?#S? list the number of participants and sub-events in each event stream.
discriminative enough. We found that adding the
lexical similarity measure greatly boosted the clus-
tering performance, especially on the mention-level,
and that combining the lexical similarity with the lo-
cal context is even more helpful for some events.
We notice that two events (celtics vs 76ers and
celtics vs heat) yield relatively low precision on both
participant- and mention-level. Taking a close look
at the data, we found that these two events acciden-
tally co-occurred with other popular events, namely
the TV program ?American Idol? finale and the NBA
Draft. The keyword based data crawler thus includes
many noisy tweets in the event streams, leading to
some false participants being detected.
5.2 Sub-event Detection Results
We compare our proposed time-content mixture
model (noted as ?MM?) against the spike detection
algorithm proposed in (Marcus et al, 2011) (noted
as ?Spike?) . The spike algorithm is based on the
tweet volume change. It uses 10 seconds as a time
unit, calculates the tweet arrival rate in each unit,
and identifies the rates that are significantly higher
than the mean tweet rate. For these rate spikes, the
algorithm finds the local maximum of tweet rate and
identify a window surrounding the local maximum.
We tune the parameter of the ?Spike? approach (set
? = 4) so that it yields similar recall values as the
mixture model approach. We then apply the ?MM?
and ?Spike? approaches to both the participant and
event streams and evaluate the sub-event detection
performance. Results are shown in Table 4. A sys-
tem detected sub-event is considered to match the
goldstandard sub-event if its peak time is within a
2-minute window of the goldstandard.
We first apply the ?Spike? and ?MM? approach to
the participant streams. The participant streams on
which we cannot detect any meaningful sub-events
have been excluded, the resulting number of partic-
ipants are listed in Table 4 and denoted as ?#P?.
In general, we found the ?MM? approach can per-
form better since it inherently incorporates both the
?burstiness? and ?lexical cohesiveness? of the event
tweets, while the ?Spike? approach relies solely on
the ?burstiness? property. Note that although we di-
vide the entire event stream into participant streams,
some key participants still own huge amount of dis-
cussion and the spike patterns are not always clearly
identifiable. The time-content mixture model gains
advantages in these cases.
We apply three settings to detect global sub-
events on the data streams. ?Spike? directly ap-
plies the spike algorithm on the entire event stream;
the ?Participant + Spike? and ?Participant + MM?
approaches first perform sub-event detection on the
participant streams and then merge the detected sub-
events along the timeline to generate global sub-
events. Note that there are fewer goldstandard
sub-events (?#S?) on the global streams since each
global sub-event may correspond to one or multiple
participant-level sub-events. Because of the averag-
ing effect, spike patterns on the entire event stream
is less obvious than those on the participant streams.
As a result, few spikes have been detected on the
event stream using the ?Spike? algorithm, which
leads to low recall as compared to other participant-
based approaches. It also indicates that, by dividing
the entire event stream into participant streams, we
have a better chance of identifying the sub-events
that have otherwise been shadowed by the domi-
nant sub-events or participants. The two participant-
based methods yield similar recall but ?Participant
1159
+ Spike? yields slightly worse precision, since it is
very sensitive to the spikes on the participant-level,
leading to the rise of false alarms. The ?Participant +
MM? approach is much better in precision, which is
consistent to our findings on the participant streams.
5.3 Summarization Results
Summarization evaluation has been a longstanding
issue in the literature (Nenkova and Mckeown, 2011;
Liu and Liu, 2010). There are even less studies fo-
cusing on evaluating the event summaries generated
from data streams. Since the summary annotation
takes quite some effort, we sample a 10-minute seg-
ment from each of the seven event streams and ask
a human annotator to select representative tweets
for each segment. We then compare the system
summaries against the manual summaries using the
ROUGE-1 (Lin, 2004) metric. The quantitative re-
sults and qualitative analysis are presented in Table 5
and Table 6 respectively. Note that the ROUGE
scores are based solely on the n-gram overlap be-
tween the system and reference summaries, which
may not be the most appropriate measure for eval-
uating the Twitter event summaries. However, we
do notice that the accurate sub-event detection per-
formance can successfully translate into a gain of
the ROUGE scores. Qualitatively, the participant-
based event summarization approach focus more on
extracting tweets associated with the targeted partic-
ipants, which could lead to better text coherence.
6 Conclusion and Future Work
In this work, we made an initial attempt to gen-
erate event summaries using Twitter data streams.
We proposed a participant-based event summariza-
tion approach which ?zooms-in? the Twitter event
streams to the participant level, detects the impor-
tant sub-events associated with each participant us-
ing a novel mixture model that incorporates both the
?burstiness? and ?cohesiveness? of tweets, and gen-
erates the event summaries progressively. Results
show that the proposed approach can effectively cap-
ture the sub-events that have otherwise been shad-
owed by the long-tail of other dominant sub-events,
yielding summaries with considerably better cover-
age. Without loss of generality, we report results
on the entire event streams, though the proposed ap-
proach can well be applied in an online fashion.
Event Method R(%) P(%) F(%)
NBA Average
Spike 14.73 23.24 16.87
Participant + Spike 54.60 14.65 22.40
Participant + MM 54.36 23.06 31.53
WWDC Conf.
Spike 26.58 39.62 31.82
Participant + Spike 49.37 25.16 33.33
Participant + MM 42.77 31.73 36.07
Table 5: ROUGE-1 scores of summarization
Method Summary
Manual
Good drive for durant
Pretty shot by Duncan
Good 3 point tony parker
Nice move westbrook
Good shot Westbrook
Spike
Game 3. Spurs vs. OKC
Okc and spurs game.
Participant
+ Spike
OKLAHOMA CITY THUNDER vs san antonio
spurs!! YA
I hope okc win the series. Ill hate too see the heat
play San Antonio
we aint in San Antonio anymore.
NBA: SA 0 OKC 8, 9:11 1st.#TeamOkc
San antonio spurs for 21 consecutive win? #nba
Somebody Should Stop Tim Duncan.
Pass the damn ball Westbrook
Good 3 pointer tony parker!
Participant
+ MM
Tim Duncan shot is so precise
Tim Duncan is gettin started
Good 3 pointer tony parker!
Sefalosa guarding tony parker. Good fucking move
coach brooks
Westbrook = 2 Fast 2 Furious
Niggas steady letting Tim Duncan shoot
Westbrook mid range shot is automatic
Table 6: Example summaries for an event segment. Par-
ticipants are marked using italicized text.
There are many challenges left in this line of re-
search. Having a standardized evaluation metric for
event summaries is one of them. In the current work,
we employed ROUGE-1 for summary evaluation,
since it has been shown to correlate well with the hu-
man judgements on noisy text genres (Liu and Liu,
2010). We would like to explore other evaluation
metrics (e.g., ROUGE-2, -SU4, Pyramid (Nenkova
et al, 2007)) and the human evaluation in future.
We will also explore better ways of integrating the
sub-event detection and summarization approaches.
Acknowledgments
Part of this work was done during the first author?s
internship in Bosch Research and Technology Cen-
ter. The work is also partially supported by NSF
grants DMS-0915110 and HRD-0833093.
1160
References
James Allan. 2002. Topic detection and tracking: Event-
based information organization. Kluwer Academic
Publishers Norwell, MA, USA.
Hila Becker, Feiyang Chen, Dan Iter, Mor Naaman, and
Luis Gravano. 2011a. Automatic identification and
presentation of twitter content for planned events. In
Proceedings of the Fifth International AAAI Confer-
ence on Weblogs and Social Media (ICWSM), pages
655?656.
Hila Becker, Mor Naaman, and Luis Gravano. 2011b.
Beyond trending topics: Real-world event identifica-
tion on twitter. In Proceedings of the Fifth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM), pages 438?441.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 66?73.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 536?544.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL/HLT).
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS tagging and parsing the twitterverse. In
Proceedings of the AAAI Workshop on Analyzing Mi-
crotext, pages 20?25.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 42?47.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-Document summariza-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL), pages 362?370.
Sanda Harabagiu and Andrew Hickl. 2011. Relevance
modeling for microblog summarization. In Proceed-
ings of the Fifth International AAAI Conference on We-
blogs and Social Media (ICWSM), pages 514?517.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence (UAI).
David Inouye and Jugal K. Kalita. 2011. Compar-
ing twitter summarization algorithms for multiple post
summaries. In Proceedings of 2011 IEEE Third Inter-
national Conference on Social Computing, pages 290?
306.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
25?32.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Workshop on Text Sum-
marization Branches Out.
Feifan Liu and Yang Liu. 2010. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. IEEE Transactions on Audio, Speech, and
Language Processing, 18(1):187?196.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is
?SXSW? trending? Exploring multiple text sources
for twitter topic summarization. In Proceedings of the
ACL Workshop on Language in Social Media (LSM),
pages 66?75.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011b. Insertion, deletion, or substitution? Normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1035?1044.
Annie Louis and Todd Newman. 2012. Summarization
of business-related tweets: A concept-based approach.
In Proceedings of the 24th International Conference
on Computational Linguistics (COLING).
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: Aggregating and visualizing
microblogs for event exploration. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, pages 227?236.
Ani Nenkova and Kathleen Mckeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval, 5(2?3):103?233.
Ani Nenkova, Rebecca Passonneau, and Kathleen Mcke-
own. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2).
1161
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.
2012. Summarizing sporting events using twitter. In
Proceedings of the 2012 ACM Interntional Conference
on Intelligent User Interfaces (IUI), pages 189?198.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic sum-
marization for twitter. In Proceedings of the Fourth
International AAAI Conference on Weblogs and Social
Media (ICWSM), pages 384?385.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of the 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL), pages
181?189.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1524?1534.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 1104?1112.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010a. Experiments in microblog summariza-
tion. In Proceedings of the 2010 IEEE Second Interna-
tional Conference on Social Computing, pages 49?56.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010b. Summarizing microblogs automati-
cally. In Proceedings of the 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL), pages 685?688.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. In
Proceedings of the 33rd European Conference on Ad-
vances in Information Retrieval (ECIR), pages 177?
188.
Jui-Yu Weng, Cheng-Lun Yang, Bo-Nian Chen, Yen-Kai
Wang, and Shou-De Lin. 2011. Imass: An intelli-
gent microblog analysis and summarization system. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 133?138.
Siqi Zhao, Lin Zhong, Jehan Wickramasuriya, and Venu
Vasudevan. 2011. Human as real-time sensors of so-
cial and physical events: A case study of twitter and
sports games. Technical Report TR0620-2011, Rice
University and Motorola Labs.
Arkaitz Zubiaga, Damiano Spina, Enrique Amigo?, and
Julio Gonzalo. 2012. Towards real-time summariza-
tion of scheduled events from twitter streams. In Pro-
ceedings of the 23rd ACM Conference on Hypertext
and Social Media, pages 319?320.
1162
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 71?76,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insertion, Deletion, or Substitution? Normalizing Text Messages without
Pre-categorization nor Supervision
Fei Liu1 Fuliang Weng2 Bingqing Wang3 Yang Liu1
1Computer Science Department, The University of Texas at Dallas
2Research and Technology Center, Robert Bosch LLC
3School of Computer Science, Fudan University
{feiliu, yangl}@hlt.utdallas.edu1
fuliang.weng@us.bosch.com2, wbq@fudan.edu.cn3
Abstract
Most text message normalization approaches
are based on supervised learning and rely on
human labeled training data. In addition, the
nonstandard words are often categorized into
different types and specific models are de-
signed to tackle each type. In this paper,
we propose a unified letter transformation ap-
proach that requires neither pre-categorization
nor human supervision. Our approach mod-
els the generation process from the dictionary
words to nonstandard tokens under a sequence
labeling framework, where each letter in the
dictionary word can be retained, removed, or
substituted by other letters/digits. To avoid
the expensive and time consuming hand label-
ing process, we automatically collected a large
set of noisy training pairs using a novel web-
based approach and performed character-level
alignment for model training. Experiments on
both Twitter and SMS messages show that our
system significantly outperformed the state-
of-the-art deletion-based abbreviation system
and the jazzy spell checker (absolute accuracy
gain of 21.69% and 18.16% over jazzy spell
checker on the two test sets respectively).
1 Introduction
Recent years have witnessed the explosive growth
of text message usage, including the mobile phone
text messages (SMS), chat logs, emails, and sta-
tus updates from the social network websites such
as Twitter and Facebook. These text message col-
lections serve as valuable information sources, yet
the nonstandard contents within them often degrade
2gether (6326) togetha (919) tgthr (250) togeda (20)
2getha (1266) togather (207) t0gether (57) toqethaa (10)
2gthr (178) togehter (94) togeter (49) 2getter (10)
2qetha (46) togethor (29) tagether (18) 2gtr (6)
Table 1: Nonstandard tokens originated from ?together?
and their frequencies in the Edinburgh Twitter corpus.
the existing language processing systems, calling
the need of text normalization before applying the
traditional information extraction, retrieval, senti-
ment analysis (Celikyilmaz et al, 2010), or sum-
marization techniques. Text message normalization
is also of crucial importance for building text-to-
speech (TTS) systems, which need to determine pro-
nunciation for nonstandard words.
Text message normalization aims to replace the
non-standard tokens that carry significant mean-
ings with the context-appropriate standard English
words. This is a very challenging task due to the
vast amount and wide variety of existing nonstan-
dard tokens. We found more than 4 million dis-
tinct out-of-vocabulary tokens in the English tweets
of the Edinburgh Twitter corpus (see Section 2.2).
Table 1 shows examples of nonstandard tokens orig-
inated from the word ?together?. We can see that
some variants can be generated by dropping let-
ters from the original word (?tgthr?) or substitut-
ing letters with digit (?2gether?); however, many
variants are generated by combining the letter in-
sertion, deletion, and substitution operations (?to-
qethaa?, ?2gthr?). This shows that it is difficult to
divide the nonstandard tokens into exclusive cate-
gories.
Among the literature of text normalization
71
(for text messages or other domains), Sproat et
al. (2001), Cook and Stevenson (2009) employed the
noisy channel model to find the most probable word
sequence given the observed noisy message. Their
approaches first classified the nonstandard tokens
into various categories (e.g., abbreviation, stylistic
variation, prefix-clipping), then calculated the pos-
terior probability of the nonstandard tokens based
on each category. Choudhury et al (2007) de-
veloped a hidden Markov model using hand anno-
tated training data. Yang et al (2009), Pennell and
Liu (2010) focused on modeling word abbreviations
formed by dropping characters from the original
word. Toutanova and Moore (2002) addressed the
phonetic substitution problem by extending the ini-
tial letter-to-phone model. Aw et al (2006), Kobus
et al (2008) viewed the text message normalization
as a statistical machine translation process from the
texting language to standard English. Beaufort et
al. (2010) experimented with the weighted finite-
state machines for normalizing French SMS mes-
sages. Most of the above approaches rely heavily
on the hand annotated data and involve categorizing
the nonstandard tokens in the first place, which gives
rise to three problems: (1) the labeled data is very
expensive and time consuming to obtain; (2) it is
hard to establish a standard taxonomy for categoriz-
ing the tokens found in text messages; (3) the lack of
optimized way to integrate various category-specific
models often compromises the system performance,
as confirmed by (Cook and Stevenson, 2009).
In this paper, we propose a general letter trans-
formation approach that normalizes nonstandard to-
kens without categorizing them. A large set of noisy
training word pairs were automatically collected via
a novel web-based approach and aligned at the char-
acter level for model training. The system was tested
on both Twitter and SMS messages. Results show
that our system significantly outperformed the jazzy
spell checker and the state-of-the-art deletion-based
abbreviation system, and also demonstrated good
cross-domain portability.
2 Letter Transformation Approach
2.1 General Framework
Given a noisy text message T , our goal is to nor-
malize it into a standard English word sequence S.
b - - - - d a y f - o t o z
h u b b i e
(1) birthday --> bday
(2) photos --> fotoz
(4) hubby --> hubbie
b i r t h d a y
p h o t o s
h u b b y
s o m e 1 - -
(6) someone --> some1
s o m e o n e
n u t h i n -
(3) nothing --> nuthin
n o t h i n g
4 - - e v a -
(5) forever --> 4eva
f o r e v e r
Figure 1: Examples of nonstandard tokens generated by
performing letter transformation on the dictionary words.
Under the noisy channel model, this is equivalent to
finding the sequence S? that maximizes p(S|T ):
S? = argmaxS p(S|T ) = argmaxS(
?
i
p(Ti|Si))p(S)
where we assume that each non-standard token Ti
is dependent on only one English word Si, that is,
we are not considering acronyms (e.g., ?bbl? for
?be back later?) in this study. p(S) can be cal-
culated using a language model (LM). We formu-
late the process of generating a nonstandard token
Ti from dictionary word Si using a letter transfor-
mation model, and use the model confidence as the
probability p(Ti|Si). Figure 1 shows several exam-
ple (word, token) pairs1. To form a nonstandard to-
ken, each letter in the dictionary word can be labeled
with: (a) one of the 0-9 digits; (b) one of the 26 char-
acters including itself; (c) the null character ?-?; (d)
a letter combination. This transformation process
from dictionary words to nonstandard tokens will be
learned automatically through a sequence labeling
framework that integrates character-, phonetic-, and
syllable-level information.
In general, the letter transformation approach will
handle the nonstandard tokens listed in Table 2 yet
without explicitly categorizing them. Note for the
tokens with letter repetition, we first generate a set
of variants by varying the repetitive letters (e.g. Ci =
{?pleas?, ?pleeas?, ?pleaas?, ?pleeaas?, ?pleeeaas?}
for Ti = {?pleeeaas?}), then select the maximum
posterior probability among all the variants:
p(Ti|Si) = max
T?i?Ci
p(T?i|Si)
1The ideal transform for example (5) would be ?for? to ?4?.
But in this study we are treating each letter in the English word
separately and not considering the phrase-level transformation.
72
(1) abbreviation tgthr, weeknd, shudnt
(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture
(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq
(4) typographic error thimg, macam
(5) stylistic variation betta, hubbie, cutie
(6) letter repetition pleeeaas, togtherrr
(7) any combination of (1) to (6) luvvvin, 2moro, m0rnin
Table 2: Nonstandard tokens that can be processed by the
unified letter transformation approach.
2.2 Web based Data Collection w/o Supervision
We propose to automatically collect training data
(annotate nonstandard words with the corresponding
English forms) using a web-based approach, there-
fore avoiding the expensive human annotation. We
use the Edinburgh Twitter corpus (Petrovic et al,
2010) for data collection, which contains 97 mil-
lion Twitter messages. The English tweets were
extracted using the TextCat language identification
toolkit (Cavnar and Trenkle, 1994), and tokenized
into a sequence of clean tokens consisting of letters,
digits, and apostrophe.
For the out-of-vocabulary (OOV) tokens consist-
ing of letters and apostrophe, we form n Google
queries for each of them in the form of either
?w1 w2 w3? OOV or OOV ?w1 w2 w3?, where w1
to w3 are consecutive context words extracted from
the tweets that contain this OOV. n is set to 6 in this
study. The first 32 returned snippets for each query
are parsed and the words in boldface that are differ-
ent from both the OOV and the context words are
collected as candidate normalized words. Among
them, we further select the words that have longer
common character sequence with the OOV than with
the context words, and pair each of them with the
OOV to form the training pairs. For the OOV tokens
consisting of both letters and digits, we use simple
rules to recover possible original words. These rules
include: 1 ? ?one?, ?won?, ?i?; 2 ? ?to?, ?two?,
?too?; 3 ? ?e?; 4 ? ?for?, ?fore?, ?four?; 5 ? ?s?;
6 ? ?b?; 8 ? ?ate?, ?ait?, ?eat?, ?eate?, ?ight?,
?aight?. The OOV tokens and any resulting words
from the above process are included in the noisy
training pairs. In addition, we add 932 word pairs
of chat slangs and their normalized word forms col-
lected from InternetSlang.com that are not covered
by the above training set.
These noisy training pairs were further expanded
and purged. We apply the transitive rule on these
initially collected training pairs. For example, if the
two pairs ?(cause, cauz)? and ?(cauz, coz)? are in the
data set, we will add ?(cause, coz)? as another train-
ing pair. We remove the data pairs whose word can-
didate is not in the CMU dictionary. We also remove
the pairs whose word candidate and OOV are simply
inflections of each other, e.g., ?(headed, heading)?,
using a set of rules. In total, this procedure generated
62,907 training word pairs including 20,880 unique
candidate words and 46,356 unique OOVs.2
2.3 Automatic Letter-level Alignment
Given a training pair (Si, Ti) consisting of a word Si
and its nonstandard variant Ti, we propose a proce-
dure to align each letter in Si with zero, one, or more
letters/digits in Ti. First we align the letters of the
longest common sequence between the dictionary
word and the variant (which gives letter-to-letter cor-
respondence in those common subsequences). Then
for the letter chunks in between each of the obtained
alignments, we process them based on the following
three cases:
(a) (many-to-0): a chunk in the dictionary word
needs to be aligned to zero letters in the variant.
In this case, we map each letter in the chunk to
?-? (e.g., ?birthday? to ?bday?), obtaining letter-
level alignments.
(b) (0-to-many): zero letters in the dictionary word
need to be aligned to a letter/digit chunk in the
variant. In this case, if the first letter in the
chunk can be combined with the previous letter
to form a digraph (such as ?wh? when aligning
?sandwich? to ?sandwhich?), we combine these
two letters. The remaining letters, or the entire
chunk when the first letter does not form a di-
graph with the previous letter, are put together
with the following aligned letter in the variant.
(c) (many-to-many): non-zero letters in the dictio-
nary word need to be aligned to a chunk in the
variant. Similar to (b), the first letter in the vari-
ant chunk is merged with the previous alignment
if they form a digraph. Then we map the chunk
in the dictionary word to the chunk in the vari-
ant as one alignment, e.g., ?someone? aligned to
?some1?.
2Please contact the first author for the collected word pairs.
73
The (b) and (c) cases above generate chunk-level
(with more than one letter) alignments. To elimi-
nate possible noisy training pairs, such as (?you?,
?haveu?), we keep all data pairs containing digits,
but remove the data pairs with chunks involving
three letters or more in either the dictionary word or
the variant. For the chunk alignments in the remain-
ing pairs, we sequentially align the letters (e.g., ?ph?
aligned to ?f-?). Note that for those 1-to-2 align-
ments, we align the single letter in the dictionary
word to a two-letter combination in the variant. We
limit to the top 5 most frequent letter combinations,
which are ?ck?, ?ey?, ?ie?, ?ou?, ?wh?, and the pairs
involving other combinations are removed.
After applying the letter alignment to the col-
lected noisy training word pairs, we obtained
298,160 letter-level alignments. Some example
alignments and corresponding word pairs are:
e ? ? ? (have, hav) q ? k (iraq, irak)
e ? a (another, anotha) q ? g (iraq, irag)
e? 3 (online, 0nlin3) w?wh (watch, whatch)
2.4 Sequence Labeling Model for P (Ti|Si)
For a letter sequence Si, we use the conditional ran-
dom fields (CRF) model to perform sequence tag-
ging to generate its variant Ti. To train the model,
we first align the collected dictionary word and its
variant at the letter level, then construct a feature
vector for each letter in the dictionary word, using
its mapped character as the reference label. This la-
beled data set is used to train a CRF model with L-
BFGS (Lafferty et al, 2001; Kudo, 2005). We use
the following features:
? Character-level features
Character n-grams: c?1, c0, c1, (c?2 c?1),
(c?1 c0), (c0 c1), (c1 c2), (c?3 c?2 c?1),
(c?2 c?1 c0), (c?1 c0 c1), (c0 c1 c2), (c1 c2 c3).
The relative position of character in the word.
? Phonetic-level features
Phoneme n-grams: p?1, p0, p1, (p?1 p0),
(p0 p1). We use the many-to-many letter-
phoneme alignment algorithm (Jiampojamarn
et al, 2007) to map each letter to multiple
phonemes (1-to-2 alignment). We use three bi-
nary features to indicate whether the current,
previous, or next character is a vowel.
? Syllable-level features
Relative position of the current syllable in the
word; two binary features indicating whether
the character is at the beginning or the end of
the current syllable. The English hyphenation
dictionary (Hindson, 2006) is used to mark all
the syllable information.
The trained CRF model can be applied to any En-
glish word to generate its variants with probabilities.
3 Experiments
We evaluate the system performance on both Twitter
and SMS message test sets. The SMS data was used
in previous work (Choudhury et al, 2007; Cook and
Stevenson, 2009). It consists of 303 distinct non-
standard tokens and their corresponding dictionary
words. We developed our own Twitter message test
set consisting of 6,150 tweets manually annotated
via the Amazon Mechanical Turk. 3 to 6 turkers
were required to convert the nonstandard tokens in
the tweets to the standard English words. We extract
the nonstandard tokens whose most frequently nor-
malized word consists of letters/digits/apostrophe,
and is different from the token itself. This results
in 3,802 distinct nonstandard tokens that we use as
the test set. 147 (3.87%) of them have more than
one corresponding standard English words. Similar
to prior work, we use isolated nonstandard tokens
without any context, that is, the LM probabilities
P (S) are based on unigrams.
We compare our system against three approaches.
The first one is a comprehensive list of chat slangs,
abbreviations, and acronyms collected by Internet-
Slang.com; it contains normalized word forms for
6,105 commonly used slangs. The second is the
word-abbreviation lookup table generated by the su-
pervised deletion-based abbreviation approach pro-
posed in (Pennell and Liu, 2010). It contains
477,941 (word, abbreviation) pairs automatically
generated for 54,594 CMU dictionary words. The
third is the jazzy spell checker based on the Aspell
algorithm (Idzelis, 2005). It integrates the phonetic
matching algorithm (DoubleMetaphone) and Leven-
shtein distance that enables the interchanging of two
adjacent letters, and changing/deleting/adding of let-
ters. The system performance is measured using the
n-best accuracy (n=1,3). For each nonstandard to-
ken, the system is considered correct if any of the
corresponding standard words is among the n-best
output from the system.
74
System Accuracy
Twitter (3802 pairs) SMS (303 pairs)
1-best 3-best 1-best 3-best
InternetSlang 7.94 8.07 4.95 4.95
(Pennell et al 2010) 20.02 27.09 21.12 28.05
Jazzy Spell Checker 47.19 56.92 43.89 55.45
LetterTran (Trim) 57.44 64.89 58.09 70.63
LetterTran (All) 59.15 67.02 58.09 70.96
LetterTran (All) + Jazzy 68.88 78.27 62.05 75.91
(Choudhury et al 2007) n/a n/a 59.9 n/a
(Cook et al 2009) n/a n/a 59.4 n/a
Table 3: N-best performance on Twitter and SMS data
sets using different systems.
Results of system accuracies are shown in Ta-
ble 3. For the system ?LetterTran (All)?, we first
generate a lookup table by applying the trained CRF
model to the CMU dictionary to generate up to
30 variants for each dictionary word.3 To make
the comparison more meaningful, we also trim our
lookup table to the same size as the deletion ta-
ble, namely ?LetterTran (Trim)?. The trimming was
performed by selecting the most frequent dictionary
words and their generated variants until the length
limit is reached. Word frequency information was
obtained from the entire Edinburgh corpus. For both
the deletion and letter transformation lookup tables,
we generate a ranked list of candidate words for each
nonstandard token, by sorting the combined score
p(Ti|Si)?C(Si), where p(Ti|Si) is the model con-
fidence and C(Si) is the unigram count generated
from the Edinburgh corpus (we used counts instead
of unigram probability P (Si)). Since the string sim-
ilarity and letter switching algorithms implemented
in jazzy can compensate the letter transformation
model, we also investigate combining it with our ap-
proach, ?LetterTran(All) + Jazzy?. In this configura-
tion, we combine the candidate words from both sys-
tems and rerank them according to the unigram fre-
quency; since the ?LetterTran? itself is very effective
in ranking candidate words, we only use the jazzy
output for tokens where ?LetterTran? is not very
confident about its best candidate ((p(Ti|Si)?C(Si)
is less than a threshold ? = 100).
We notice the accuracy using the InternetSlang
list is very poor, indicating text message normal-
ization is a very challenging task that can hardly
3We heuristically choose this large number since the learned
letter/digit insertion, substitution, and deletion patterns tend to
generate many variants for each dictionary word.
be tackled by using a hand-crafted list. The dele-
tion table has modest performance given the fact
that it covers only deletion-based abbreviations and
letter repetitions (see Section 2.1). The ?Letter-
Tran? approach significantly outperforms all base-
lines even after trimming. This is because it han-
dles different ways of forming nonstandard tokens
in an unified framework. Taking the Twitter test
set for an example, the lookup table generated by
?LetterTran? covered 69.94% of the total test to-
kens, and among them, 96% were correctly normal-
ized in the 3-best output, resulting in 67.02% over-
all accuracy. The test tokens that were not covered
by the ?LetterTrans? model include those generated
by accidentally switching and inserting letters (e.g.,
?absolotuely? for ?absolutely?) and slangs (?addy?
or ?address?). Adding the output from jazzy com-
pensates these problems and boosts the 1-best ac-
curacy, achieving 21.69% and 18.16% absolute per-
formance gain respectively on the Twitter and SMS
test sets, as compared to using jazzy only. We also
observe that the ?LetterTran? model can be easily
ported to the SMS domain. When combined with
the jazzy module, it achieved 62.05% 1-best accu-
racy, outperforming the domain-specific supervised
system in (Choudhury et al, 2007) (59.9%) and
the pre-categorized approach by (Cook and Steven-
son, 2009) (59.4%). Regarding different feature cat-
egories, we found the character-level features are
strong indicators, and using phonetic- and syllabic-
level features also slightly benefits the performance.
4 Conclusion
In this paper, we proposed a generic letter trans-
formation approach for text message normaliza-
tion without pre-categorizing the nonstandard to-
kens into insertion, deletion, substitution, etc. We
also avoided the expensive and time consuming hand
labeling process by automatically collecting a large
set of noisy training pairs. Results in the Twitter
and SMS domains show that our system can signif-
icantly outperform the state-of-the-art systems and
have good domain portability. In the future, we
would like to compare our method with a statistical
machine translation approach performed at the let-
ter level, evaluate the system using sentences by in-
corporating context word information, and consider
many-to-one letter transformation in the model.
75
5 Acknowledgments
The authors thank Deana Pennell for sharing the
look-up table generated using the deletion-based ab-
breviation approach. Thank Sittichai Jiampojamarn
for providing the many-to-many letter-phoneme
alignment data sets and toolkit. Part of this work
was done while Fei Liu was working as a research
intern in Bosch Research and Technology Center.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of the COLING/ACL, pages 33?
40.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of the ACL, pages
770?779.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of Third An-
nual Symposium on Document Analysis and Informa-
tion Retrieval, pages 161?175.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng.
2010. Probabilistic model-based sentiment analysis of
twitter messages. In Proceedings of the IEEE Work-
shop on Spoken Language Technology, pages 79?84.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157?174.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text messages normalization. In Pro-
ceedings of the NAACL HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78.
Matthew Hindson. 2006. En-
glish language hyphenation dictionary.
http://www.hindson.com.au/wordpress/2006/11/11/english-
language-hyphenation-dictionary/.
Mindaugas Idzelis. 2005. Jazzy: The java open source
spell checker. http://jazzy.sourceforge.net/.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proceedings of the HLT/NAACL, pages
372?379.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: Are two metaphors
better than one? In Proceedings of the COLING, pages
441?448.
Taku Kudo. 2005. CRF++: Yet another CRF took kit.
http://crfpp.sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the ICML, pages 282?289.
Deana L. Pennell and Yang Liu. 2010. Normalization
of text messages for text-to-speech. In Proceedings of
the ICASSP, pages 4842?4845.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25?26.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287?333.
Kristina Toutanova and Robert C. Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
Proceedings of the ACL, pages 144?151.
Dong Yang, Yi cheng Pan, and Sadaoki Furui. 2009.
Automatic chinese abbreviation generation using con-
ditional random field. In Proceedings of the NAACL
HLT, pages 273?276.
76
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035?1044,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Broad-Coverage Normalization System for Social Media Language
Fei Liu Fuliang Weng Xiao Jiang
Research and Technology Center
Robert Bosch LLC
{fei.liu, fuliang.weng}@us.bosch.com
{fixed-term.xiao.jiang}@us.bosch.com
Abstract
Social media language contains huge amount
and wide variety of nonstandard tokens, cre-
ated both intentionally and unintentionally by
the users. It is of crucial importance to nor-
malize the noisy nonstandard tokens before
applying other NLP techniques. A major
challenge facing this task is the system cov-
erage, i.e., for any user-created nonstandard
term, the system should be able to restore the
correct word within its top n output candi-
dates. In this paper, we propose a cognitively-
driven normalization system that integrates
different human perspectives in normalizing
the nonstandard tokens, including the en-
hanced letter transformation, visual priming,
and string/phonetic similarity. The system
was evaluated on both word- and message-
level using four SMS and Twitter data sets.
Results show that our system achieves over
90% word-coverage across all data sets (a
10% absolute increase compared to state-of-
the-art); the broad word-coverage can also
successfully translate into message-level per-
formance gain, yielding 6% absolute increase
compared to the best prior approach.
1 Introduction
The amount of user generated content has increased
drastically in the past few years, driven by the pros-
perous development of the social media websites
such as Twitter, Facebook, and Google+. As of June
2011, Twitter has attracted over 300 million users
and produces more than 2 billion tweets per week
(Twitter, 2011). In a broader sense, Twitter mes-
sages, SMS messages, Facebook updates, chat logs,
Emails, etc. can all be considered as ?social text?,
which is significantly different from the traditional
news text due to the informal writing style and the
conversational nature. The social text serves as a
very valuable information source for many NLP ap-
plications, such as the information extraction (Ritter
et al, 2011), retrieval (Subramaniam et al, 2009),
summarization (Liu et al, 2011a), sentiment analy-
sis (Celikyilmaz et al, 2010), etc. Yet existing sys-
tems often perform poorly in this domain due the
to extensive use of the nonstandard tokens, emoti-
cons, incomplete and ungrammatical sentences, etc.
It is reported that the Stanford named entity recog-
nizer (NER) experienced a performance drop from
90.8% to 45.8% on tweets (Liu et al, 2011c); the
part-of-speech (POS) tagger and dependency parser
degraded 12.2% and 20.65% respectively on tweets
(Foster et al, 2011). It is therefore of great impor-
tance to normalize the social text before applying the
standard NLP techniques. Text normalization is also
crucial for building robust text-to-speech (TTS) sys-
tems, which need to determine the pronunciations
for nonstandard words in the social text.
The goal of this work is to automatically con-
vert the noisy nonstandard tokens observed in the
social text into standard English words. We aim
for a robust text normalization system with ?broad
coverage?, i.e., for any user-created nonstandard to-
ken, the system should be able to restore the correct
word within its top n candidates (n = 1, 3, 10...).
This is a very challenging task due to two facts:
first, there exists huge amount and a wide variety
of nonstandard tokens. (Liu et al, 2011b) found
more than 4 million distinct out-of-vocabulary to-
kens in the Edinburgh Twitter corpus (Petrovic et
al., 2010); second, the nonstandard tokens consist
1035
2gether (6326) togetha (919) tgthr (250) togeda (20)
2getha (1266) togather (207) t0gether (57) toqethaa (10)
2gthr (178) togehter (94) togeter (49) 2getter (10)
u (3240535) ya (460963) yo (252274) yaa (17015)
yaaa (7740) yew (7591) yuo (467) youz (426)
yoooooou (186) youy (105) yoiu (128) yoooouuuu (82)
Table 1: Nonstandard tokens and their frequencies in the
Edinburgh Twitter corpus. The corresponding standard
words are ?together? and ?you?, respectively.
of a mixture of both unintentional misspellings and
intentionally-created tokens for various reasons1, in-
cluding the needs for speed, ease of typing (Crystal,
2009), sentiment expressing (e.g., ?coooool? (Brody
and Diakopoulos, 2011)), intimacy and social pur-
pose (Thurlow, 2003), etc., making it even harder to
decipher the social messages. Table 1 shows some
example nonstandard tokens.
Existing spell checkers and normalization sys-
tems rely heavily on lexical/phonetic similarity to
select the correct candidate words. This may not
work well since a good portion of the correct words
lie outside the specified similarity threshold (e.g.,
(tomorrow, ?tmrw?)2), yet the number of candidates
increases dramatically as the system strives to in-
crease the coverage by enlarging the threshold. (Han
and Baldwin, 2011) reported an average of 127 can-
didates per nonstandard token with the correct-word
coverage of 84%. The low coverage score also en-
forces an undesirable performance ceiling for the
candidate reranking approaches. Different from pre-
vious work, we tackle the text normalization prob-
lem from a cognitive-sensitive perspective and in-
vestigate the human rationales for normalizing the
nonstandard tokens. We argue that there exists a set
of letter transformation patterns that humans use to
decipher the nonstandard tokens. Moreover, the ?vi-
sual priming? effect may play an important role in
human comprehension of the noisy tokens. ?Prim-
ing? represents an implicit memory effect. For ex-
ample, if a person reads a list of words including the
word table, and is later asked to complete a word
starting with tab-, it is very likely that he answers
table since the person is primed.
In this paper, we propose a broad-coverage nor-
malization system by integrating three human per-
1For this reason, we will use the term ?nonstandard tokens?
instead of ?ill-formed tokens? throughout the paper.
2We use the form (standard word, ?nonstandard token?) to
denote an example nonstandard token and its corresponding
standard word.
spectives, including the enhanced letter transforma-
tion, visual priming, and the string and phonetic
similarity. For an arbitrary nonstandard token, the
three subnormalizers each suggest their most con-
fident candidates from a different perspective. The
candidates can then be heuristically combined or
reranked using a message-level decoding process.
We evaluate the system on both word- and message-
level using four SMS and Twitter data sets. Results
show that our system can achieve over 90% word-
coverage with limited number of candidates and the
broad word-coverage can be successfully translated
into message-level performance gain. In addition,
our system requires no human annotations, therefore
can be easily adapted to different domains.
2 Related work
Text normalization, in its traditional sense, is the
first step of a speech synthesis system, where the
numbers, dates, acronyms, etc. found in the real-
world text were converted into standard dictionary
words, so that the system can pronounce them cor-
rectly. Spell checking plays an important role in this
process. (Church and Gale, 1991; Mays et al, 1991;
Brill and Moore, 2000) proposed to use the noisy
channel framework to generate a list of corrections
for any misspelled word, ranked by the correspond-
ing posterior probabilities. (Sproat et al, 2001) en-
hanced this framework by calculating the likelihood
probability as the chance of a noisy token and its as-
sociated tag being generated by a specific word.
With the rapid growth of SMS and social me-
dia content, text normalization system has drawn in-
creasing attention in the recent decade, where the
focus is on converting the noisy nonstandard tokens
in the informal text into standard dictionary words.
(Choudhury et al, 2007) modeled each standard En-
glish word as a hidden Markov model (HMM) and
calculated the probability of observing the noisy-
token under each of the HMM models; (Cook and
Stevenson, 2009) calculated the sum of the probabil-
ities of a noisy token being generated by a specific
word and a word formation process; (Beaufort et al,
2010) employed the weighted finite-state machines
(FSMs) and rewriting rules for normalizing French
SMS; (Pennell and Liu, 2010) focused on tweets cre-
ated by handsets and developed a CRF tagger for
deletion-based abbreviation. The text normalization
problem was also tackled under the machine transla-
1036
tion (MT) or speech recognition (ASR) framework.
(Aw et al, 2006) adapted a phrase-based MT model
for normalizing SMS and achieved satisfying per-
formance. (Kobus et al, 2008) showed that using a
statistical MT system in combination with an anal-
ogy of the ASR system improved performance in
French SMS normalization. (Pennell and Liu, 2011)
proposed a two-phase character-level MT system for
expanding the abbreviations into standard text.
Recent work also focuses on normalizing the
Twitter messages, which is generally considered a
more challenging task. (Han and Baldwin, 2011) de-
veloped classifiers for detecting the ill-formed words
and generated corrections based on the morpho-
phonemic similarity. (Liu et al, 2011b) proposed
to normalize the nonstandard tokens without explic-
itly categorizing them. (Xue et al, 2011) adopted
the noisy-channel framework and incorporated or-
thographic, phonetic, contextual, and acronym ex-
pansion factors in calculating the likelihood proba-
bilities. (Gouws et al, 2011) revealed that different
populations exhibit different shortening styles.
Most of the above systems limit their processing
scope to certain categories (e.g., deletion-based ab-
breviations, misspellings) or require large-scale hu-
man annotated corpus for training, which greatly
hinders the scalability of the system. In this paper,
we propose a novel cognitively-driven text normal-
ization system that robustly tackle both the unin-
tentional misspellings and the intentionally-created
noisy tokens. We propose a global context-based
approach to purify the automatically collected train-
ing data and learn the letter transformation pat-
terns without human supervision. We also propose
a cognitively-grounded ?visual priming? approach
that leverages the ?priming? effect to suggest the
candidate words. By integrating different perspec-
tives, our system can successfully mimic the hu-
man rationales and yield broad word-coverage on
both SMS and Twitter messages. To the best of our
knowledge, we are the first to integrate these human
perspectives in the text normalization system.
3 Broad-Coverage Normalization System
In this section, we describe our broad-coverage nor-
malization system, which consists of four key com-
ponents. For a standard/nonstandard token, three
subnormalizers each suggest their most confident
b - - - - d a y f - o t o z
h u b b i e
(1) birthday --> bday
(2) photos --> fotoz
(4) hubby --> hubbie
b i r t h d a y
p h o t o s
h u b b y
s o m e 1 - -
(6) someone --> some1
s o m e o n e
n u t h i n -
(3) nothing --> nuthin
n o t h i n g
4 - - e v a -
(5) forever --> 4eva
f o r e v e r
Figure 1: Examples of nonstandard tokens generated by
performing letter transformation on the dictionary words.
candidates from a different perspective3: ?Enhanced
Letter Transformation? automatically learns a set
of letter transformation patterns and is most effec-
tive in normalizing the intentionally created non-
standard tokens through letter insertion, repetition,
deletion, and substitution (Section 3.1); ?Visual
Priming? proposes candidates based on the visual
cues and a primed perspective (Section 3.2); ?Spell
Checker? corrects the misspellings (Section 3.3).
The fourth component, ?Candidate Combination?
introduces various strategies to combine the candi-
dates with or without the local context (Section 3.4).
Note that it is crucial to integrate different human
perspectives so that the system is flexible in pro-
cessing both unintentional misspellings and various
intentionally-created noisy tokens.
3.1 Enhanced Letter Transformation
Given a noisy token ti seen in the text, the letter
transformation subnormalizer produces a list of cor-
rection candidates si under the noisy channel model:
s? = argmaxsip(si|ti) = argmaxsip(ti|si)p(si)
where we assume each nonstandard token ti is de-
pendent on only one English word si, that is, we
are not considering acronyms (e.g., ?bbl? for ?be
back later?) in this study. p(si) can be calculated
as the unigram count from a background corpus. We
formulate the process of generating a nonstandard
token ti from the dictionary word si using a letter
transformation model, and use the model confidence
as the probability p(ti|si). Figure 1 shows several
example (word, token) pairs.
To form a nonstandard token, each letter in the
dictionary word can be labeled with: (a) one of the
0-9 digits; (b) one of the 26 characters including it-
self; (c) the null character ?-?; (d) a letter combi-
nation4. This transformation process from dictio-
3For the dictionary word, we allow the subnormalizers to
either return the word itself or candidates that are the possibly
intended words in the given context (e.g., (with, ?wit?)).
4The set of letter combinations used in this work are {ah, ai,
aw, ay, ck, ea, ey, ie, ou, te, wh}
1037
nary words to nonstandard tokens will be learned
by a character-level sequence labeling system us-
ing the automatically collected (word, token) pairs.
Next, we create a large lookup table by applying the
character-level labeling system to the standard dic-
tionary words and generate multiple variations for
each word using the n-best labeling output, the la-
beling confidence is used as p(ti|si). During testing,
we search this lookup table to find the best candidate
words for the nonstandard tokens. For tokens with
letter repetition, we first generate a set of variants
by varying the repetitive letters (e.g. Ci = {?pleas?,
?pleeas?, ?pleaas?, ?pleeaas?, ?pleeeaas?} for ti =
{?pleeeaas?}), then select the maximum posterior
probability among all the variants:
p(ti|si) = max
t?i?Ci
p(t?i|si)
Different from the work in (Liu et al, 2011b), we
enhanced the letter transformation process with two
novel aspects: first, we devise a set of phoneme-,
syllable-, morpheme- and word-boundary based fea-
tures that effectively characterize the formation pro-
cess of the nonstandard tokens; second, we propose
a global context-aware approach to purify the auto-
matically collected training (word, token) pairs, re-
sulting system yielded similar performance but with
only one ninth of the original data. We name this
subnormalizer ?Enhanced Letter Transformation?.
3.1.1 Context-Aware Training Pair Selection
Manual annotation of the noisy nonstandard to-
kens takes a lot of time and effort. (Liu et al, 2011b)
proposed to use Google search engine to automati-
cally collect large amount of training pairs. Yet the
resulting (work, token) pairs are often noisy, con-
taining pairs such as (events, ?ents?), (downtown,
?downto?), etc. The ideal training data should con-
sist of the most frequent nonstandard tokens paired
with the corresponding corrections, so that the sys-
tem can learn from the most representative letter
transformation patterns.
Motivated by research on word sense disambigua-
tion (WSD) (Mihalcea, 2007), we hypothesize the
nonstandard token and the standard word share a lot
of common terms in their global context. For exam-
ple, ?luv? and ?love? share ?i?, ?you?, ?u?, ?it?, etc.
among their top context words. Based on this find-
ing, we propose to filter out the low-quality train-
ing pairs by evaluating the global contextual simi-
larity between the word and token. To the best of
our knowledge, we are the first to explore this global
contextual similarity for the text normalization task.
Given a noisy (word, token) pair, we construct
two context vectors vi and vj by collecting the
most frequent terms appearing before or after the
work/token. We consider two terms on each side
of the word/token as context and restrict the vector
length to the top 100 terms. The frequency informa-
tion were calculated using a large background cor-
pus; stopwords were not excluded from the context
vector. The contextual similarity of the (word, to-
ken) pair is defined as the cosine similarity between
the context vectors vi and vj :
ContextSim(vi, vj) =
Pn
k=1 wi,k ? wj,kqPn
k=1 w
2
i,k ?
qPn
k=1 w
2
j,k
where wi,k is the weight of term tk within the con-
text of term ti. The term weights are defined using a
normalized TF-IDF method:
wi,k =
TFi,k
TFi
? log(
N
DFk
)
where TFi,k is the count of term tk appearing within
the context of term ti; TFi is the total count of ti in
the corpus. TFi,kTFi is therefore the relative frequency
of tk appearing in the context of ti; log( NDFk ) de-
notes the inverse document frequency of tk, calcu-
lated as the logarithm of total tweets (N ) divided by
the number of tweets containing tk.
To select the most representative (word, token)
pairs for training, we rank the automatically col-
lected 46,288 pairs by the token frequency, filter
out pairs whose contextual similarity lower than a
threshold ? (set empirically at 0.0003), and retain
only the top portion (5,000 pairs) for experiments.
3.1.2 Character-level Sequence Labeling
For a dictionary word si, we use the conditional
random fields (CRF) model to perform character-
level labeling to generate its variant ti. In the train-
ing stage, we align the collected (word, token) pairs
at the character level (Liu et al, 2011b), then con-
struct a feature vector for each letter of the dictio-
nary word, using its mapped character as the ref-
erence label. This aligned data set is used to train
a CRF model (Lafferty et al, 2001; Kudo, 2005)
1038
Character a d v e r t i s e m e n t s
Phoneme AE D V ER ER T AY Z M AH N T S
Phoneme boundary O O O B1 L1 O O O O O O O O O
Syllable boundary B L B I L B I I L B I I I L
Morpheme boundary B I I I I I I I L B I I L U
Word boundary B I I I I I I I I I I I I L
Table 2: Example boundary tags for word ?advertise-
ments? on the phoneme-, syllable-, morpheme-, and
word-level, labeled with the ?BILOU? encoding scheme.
with L-BFGS optimization. We use the charac-
ter/phoneme n-gram and binary vowel features as in
(Liu et al, 2011b), but develop a set of boundary
features to effectively characterize the letter trans-
formation process.
We notice that in creating the nonstandard tokens,
humans tend to drop certain letter units from the
word or replace them with other letters. For exam-
ple, in abbreviating ?advertisements? to ?ads?, hu-
mans may first break the word into smaller units
?ad-ver-tise-ment-s?, then drop the middle parts.
This also conforms with the word construction the-
ory where a word is composed of smaller units and
construction rules. Based on this assumption, we
decompose the dictionary words on the phoneme-,
syllable-, morpheme-, and word-level5 and use the
?BILOU? tagging scheme (Ratinov and Roth, 2009)
to represent the unit boundary, where ?BILOU?
stands for B(egin), I(nside), L(ast), O(utside), and
U(nit-length) of the corresponding unit6. Example
?BILOU? boundary tags were shown in Table 2.
On top of the boundary tags, we develop a set of
conjunction features to accurately pinpoint the cur-
rent character position. We consider conjunction
features formed by concatenating character position
in syllable and current syllable position in the word
(e.g., conjunction feature ?L B? for the letter ?d? in
Table 2). A similar set of features are also devel-
oped on morpheme level. We consider conjunction
of character/vowel feature and their boundary tags
on the syllable/morpheme/word level; conjunction
of phoneme and phoneme boundary tags, and ab-
solute position of current character within the corre-
5Phoneme decomposition is generated using the (Jiampo-
jamarn et al, 2007) algorithm to map up to two letters to
phonemes (2-to-2 alignment); syllable boundary acquired by
the hyphenation algorithm (Liang, 1983); morpheme boundary
determined by toolkit Morfessor 1.0 (Creutz and Lagus, 2005).
6For phoneme boundary, we use ?B1? and ?L1? to represent
two different characters aligned to one phoneme and ?B2?, ?L2?
represent same characters aligned to one phoneme.
sponding syllable/morpheme/word.
We use the aforementioned features to train the
CRF model, then apply the model on dictionary
words si to generate multiple variations ti for each
word. When a nonstandard token is seen during test-
ing, we apply the noisy channel to generate a list of
best candidate words: s? = argmaxsip(ti|si)p(si).
3.2 Visual Priming Approach
A second key component of the broad-coverage nor-
malization system is a novel ?Visual Priming? sub-
normalizer. It is built on a cognitively-driven ?prim-
ing? effect, which has not been explored by other
studies yet was shown to be effective across all our
data sets.
?Priming?7 is an implicit memory effect caused
by spreading neural networks (Tulving and Stark,
1982). As an example, in the word-stem comple-
tion task, participants are given a list of study words,
and then asked to complete word ?stems? consisting
of first 3 letters. A priming effect is observed when
participants complete stems with words on the study
list more often than with the novel words. The study
list activates parts of the human brain right before
the stem completion task, later when a word stem is
seen, less additional activation is needed for one to
choose a word from the study list.
We argue that the ?priming? effect may play an
important role in human comprehension of the noisy
tokens. A person familiarized with the ?social talk?
is highly primed with the most commonly used
words; later when a nonstandard token shows only
minor visual cues or visual stimulus, it can still be
quickly recognized by the person. In this process,
the first letter or first few letters of the word serve
as a very important visual stimulus. Based on this
assumption, we introduce the ?priming? subnormal-
izer based only on the word frequency and the minor
visual stimulus. Concretely, this approach proposes
candidate words based on the following equation:
V isualPrim(si|ti) =
len(LCS(ti, si))
len(ti)
? log(TF (si))
Where TF (si) is the term frequency of si as in the
background social text corpus; log(TF (si)) primes
the system with the most common words in the so-
cial text; LCS(?) means the longest common char-
acter subsequence; len(?) denotes the length of the
7http://en.wikipedia.org/wiki/Priming (psychology)
1039
character sequence. Together len(LCS(ti,si))len(ti) pro-
vides the minor visual stimulus from ti. Note that
the first character has been shown to be a crucial vi-
sual cue for the brain to understand jumbled words
(Davis, ), we therefore consider as candidates only
those words si that start with the same character as
ti. In the case that the nonstandard token ti starts
with a digit (e.g., ?2moro?), we use the mostly likely
corresponding letter to search the candidates (those
starting with letter ?t?). This setting also effectively
reduces the candidate search space.
The ?visual priming? subnormalizer promotes the
candidate words that are frequently used in the so-
cial talk and also bear visual similarity with the
given noisy token. It slightly deviates from the tradi-
tional ?priming? notion in that the frequency infor-
mation were acquired from the global corpus rather
than from the prior context. This approach also in-
herently follows the noisy channel framework, with
p(ti|si) represents the visual stimulus and p(si) be-
ing the logarithm of frequency. The candidate words
are ranked by s? = argmaxsiV isualPrim(si|ti).
We show that the ?priming? subnormalizer is robust
across data sets abide its simplistic representation.
3.3 Spell Checker
The third subnormalizer is the spell checker, which
combines the string and phonetic similarity algo-
rithms and is most effective in normalizing the mis-
spellings. We use the Jazzy spell checker (Idzelis,
2005) that integrates the DoubleMetaphone phonetic
matching algorithm and the Levenshtein distance us-
ing the near-miss strategy, which enables the in-
terchange of two adjacent letters, and the replac-
ing/deleting/adding of letters.
3.4 Candidate Combination
Each of the three subnormalizers is a stand-alone
system and can suggest corrections for the nonstan-
dard tokens. Yet we show that each subnormal-
izer mimics a different perspective that humans use
to decode the nonstandard tokens, as a result, our
broad-coverage normalization system is built by in-
tegrating candidates from the three subnormalizers
using various strategies.
For a noisy token seen in the informal text, the
most convenient way of system combination is to
harvest up to n candidates from each of the sub-
normalizers, and use the pool of candidates (up to
3n) as the system output. This sets an upper bound
for other candidate combination strategies, and we
name this approach ?Oracle?.
A second combination strategy is to give higher
priority to candidates from high-precision subsys-
tems. Both ?Letter Transformation? and ?Spell
Checker? have been shown to have high precision in
suggesting corrections (Liu et al, 2011b), while ?Vi-
sual Priming? may not yield high precision due to
its definition. We therefore take the top-3 candidates
from each of the ?Letter Tran.? and ?Spell Checker?
subsystems, but put candidates from ?Letter Tran.?
ahead of ?Spell Checker? if the confidence of the
best candidate is greater than a threshold ? and vice
versa. The list of candidates is then compensated us-
ing the ?Visual Priming? output until the total num-
ber reaches n. We name this approach ?Word-level?
combination since no message-level context infor-
mation is involved.
Based on the ?Word-level? combination output,
we can further rerank all the candidates using a
message-level Viterbi decoding process (Pennell and
Liu, 2011) where the local context information is
used to select the best candidate. This approach is
named ?Message-level? combination.
4 Experiments
4.1 Experimental Setup
We use four SMS and Twitter data sets to evaluate
the system effectiveness. Statistics of these data sets
are summarized in Table 3. Data set (1) to (3) are
used for word-level evaluation; data set (4) for both
word- and message-level evaluation. In Table 3, we
also present the number of distinct nonstandard to-
kens found in each data set, and notice that only a
small portion of the nonstandard tokens correspond
to multiple standard words. We calculate the dic-
tionary coverage of the manually annotated words
since this sets an upper bound for any normaliza-
tion system. We use the Edinburgh Twitter corpus
(Petrovic et al, 2010) as the background corpus for
frequency calculation, and a dictionary containing
82,324 words.8 The nonstandard tokens may consist
of both numbers/characters and apostrophe.
8The dictionary is created by combining the CMU (CMU,
2007) and Aspell (Atkinson, 2006) dictionaries and dropping
words with frequency < 20 in the background corpus. ?rt? and
all single characters except ?a? and ?i? are excluded.
1040
Index Domain Time Period #Msgs
#Uniq Nonstan. %Nonstan. Tkns %Dict cov.
Reference
Tokens w/ Multi-cands of cands
(1) SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al, 2007)
(2) Twitter Nov 2009 ? Feb 2010 6150 3802 3.87% 99.34% (Liu et al, 2011)
(3) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011)
(4) Twitter Aug 2010 ? Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011)
Table 3: Statistics of different SMS and Twitter data sets.
The goal of word-level normalization is to convert
the list of distinct nonstandard tokens into standard
words. For each nonstandard token, the system is
considered correct if any of the corresponding stan-
dard words is among the n-best output from the sys-
tem. We adopt this word-level n-best accuracy to
make our results comparable to other state-of-the-art
systems. On message-level, we evaluate the 1-best
system output using precision, recall, and f-score,
calculated respective to the nonstandard tokens.
4.2 Word-level Results
The word-level results are presented in Table 4, 5,
and 6, evaluated on data set (1), (2), (3) respectively.
We present the n-best accuracy (n = 1, 3, 10, 20) of
the system as well as the ?Oracle? results generated
by pooling the top-20 candidates from each of the
three subnormalizers. The best prior results on the
data sets are also included in the tables.
We notice that the broad-coverage system outper-
forms all other systems on the reported data sets.
It achieves about 90% word-level accuracy on data
set (1) and (2) with the top-10 candidates (an aver-
age 10% performance gain compared to (Liu et al,
2011b)). This is of crucial importance to a normal-
ization system, since the high accuracy and limited
number of candidates will enable more sophisticated
reranking or supervised learning techniques to se-
lect the best candidate. We also observe the ?Ora-
cle? system has averagely only 5% gap to the dic-
tionary coverage. A detailed analysis shows that the
human annotators perform many semantic/grammar
corrections as well as inconsistent annotations, e.g.,
(sleepy, ?zzz?), (disliked, ?unliked?). These are out
of the capabilities of the current text normalization
system and partly explains the remaining 5% gap.
Regarding the subnormalizer performance, the
spell checker yields only 50% to 60% accuracy on
all data sets, indicating that the vast amount of the
intentionally created nonstandard tokens can hardly
be tackled by a system relies solely on the lexi-
cal/phonetic similarity. The ?Visual Priming? sub-
SMS Dataset Word Level Accuracy (%)
(303 pairs) 1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 43.89 55.45 56.77 56.77 n/a
Visual Priming 54.13 74.92 84.82 87.13 n/a
Enhanced Letter Tran. 61.06 74.92 80.86 82.51 n/a
Broad-Cov. System 64.36 80.20 89.77 91.75 94.06
(Pennell et al, 2011)? 60.39 74.58 75.57 75.57 n/a
(Liu et al, 2011) 62.05 75.91 81.19 81.19 n/a
(Cook et al, 2009) 59.4 n/a 83.8 87.8 n/a
(Choudhury et al, 2007)? 59.9 n/a 84.3 88.7 n/a
Table 4: Word-level results on data set (1). ? denotes
system requires human annotations for training.
Twitter Dataset Word Level Accuracy (%)
(3802 pairs) 1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 47.19 56.92 59.13 59.18 n/a
Visual Priming 54.34 70.59 80.83 84.74 n/a
Enhanced Letter Tran. 61.05 70.07 74.04 74.75 n/a
Broad-Cov. System 69.81 82.51 92.24 93.79 95.71
(Liu et al, 2011) 68.88 78.27 80.93 81.17 n/a
Table 5: Word-level results on data set (2).
normalizer performs surprisingly well and shows ro-
bust performance across all data sets. A minor side-
effect is that the candidates were restricted to have
the same first letter with the noisy token, this sets
the upper bound of the approach to 89.77%, 92.45%,
and 93.51%, respectively on data set (1), (2), and (3).
Compared to other subnormalizers, the ?Enhanced
Letter Tran.? is effective at normalizing intention-
ally created tokens and has better precision regard-
ing its top candidate (n = 1). We demonstrate the
context-aware training pair selection results in Fig-
ure 2, by plotting the learning curve using different
amounts of training data, ranging from 1,000 (word,
token) pairs to the total 46,288 pairs. We notice that
the system can effectively learn the letter transfor-
mation patterns from a small number of high quality
training pairs. The final system was trained using the
top 5,000 pairs and the lookup table was created by
generating 50 variations for each dictionary word.
4.3 Message-level Results
The goal of message-level normalization is to re-
place each occurrence of the nonstandard token with
the candidate word that best fits the local context.
1041
SMS/Twitter Dataset Word Level Accuracy (%)
(2404 pairs) 1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 39.89 46.51 48.54 48.67 n/a
Visual Priming 54.12 68.59 78.83 83.11 n/a
Enhanced Letter Tran. 57.65 67.18 71.01 71.88 n/a
Broad-Cov. System 64.39 78.29 86.56 88.69 91.60
(Pennell et al, 2011)? 37.40 n/a n/a 72.38 n/a
Table 6: Word-level results on data set (3). ? denotes
system requires human annotations for training.
64666870727476
1K
2K
5K
10K
20K
All (~45K)
Amoun
t of Tra
ining P
airs
Nonstandard Token Cov. (%)
Rando
m Sele
ction
Contex
t-aware
 Select
ion
Figure 2: Learning curve of the enhanced letter transfor-
mation system using random training pair selection or the
context-aware approach. Evaluated on data set (2).
We use the word-level ?Broad-Cov. System? for
candidate suggestion and the Viterbi algorithm for
message-level decoding. The system is evaluated on
data set (4) and results shown in Table 7. Following
research in (Han and Baldwin, 2011), we focus on
the the normalization task and assume perfect non-
standard token detection.
The ?Word-level w/o Context? results are gen-
erated by replacing each nonstandard token using
the 1-best word-level candidate. Although the re-
placement process is static, it results in 70.97% f-
score due to the high performance of the word-level
system. We explore two language models (LM)
for the Viterbi decoding process. First, a bigram
LM is trained using the Edinburgh Twitter corpus
(53,794,549 English tweets) with the SRILM toolkit
(Stolcke, 2002) and Kneser-Ney smoothing; second,
we retrieve the bigram probabilities from the Mi-
crosoft Web N-gram API (Wang et al, 2010) since
this represents a more comprehensive web-based
corpus. During decoding, we use the ?VisualPrim?
score as the emission probability, since this score
best fits the log scale and applies to all candidates.
For the Twitter LM, we apply a scaling factor of
0.5 to the ?VisualPrim? score to make it compara-
ble in scale to the LM probabilities. We use the 3-
best word-level candidates for Viterbi decoding. In
addition, we add the commonly used corrections for
Twitter Dataset Message-level P/R/F
(549 Tweets) Precision (%) Recall (%) F-score (%)
Word-level w/o Context 75.69 66.81 70.97
w/ Context
Web LM 79.12 77.11 78.10
Twitter LM 84.13 78.38 81.15
(Han and Baldwin, 2011)? 75.30 75.30 75.30
Table 7: Message-level results on data set (4). ? denotes
system requires human annotations for training.
16 single-characters, e.g., for ?r?, ?c?, we add ?are?,
?see? to the candidate list if they are not already pre-
sented. A default ?VisualPrim? score (? = 25) is
used for these candidates. As seen from Table 7,
both Web LM and Twitter LM achieve better perfor-
mance than the best prior results, with Twitter LM
outperforms the Web LM, yielding a f-score of 81%.
This shows that a vanilla Viterbi decoding process is
able to outperform the fine-tuned supervised system
given competitive word-level candidates. In future,
we will investigate other comprehensive message-
level candidate reranking process.
5 Conclusion
In this paper, we propose a broad-coverage normal-
ization system for the social media language with-
out using the human annotations. It integrates three
key components: the enhanced letter transformation,
visual priming, and string/phonetic similarity. The
system was evaluated on both word- and message-
level using four SMS and Twitter data sets. We show
that our system achieves over 90% word-coverage
across all data sets and the broad word-coverage can
be successfully translated into message-level perfor-
mance gain. We observe that the social media is an
emotion-rich language, therefore future normaliza-
tion system will need to address various sentiment-
related expressions, such as emoticons (?:d?, ?X-
8?), interjections (?bwahaha?, ?brrrr?), acronyms
(?lol?, ?lmao?), etc., whether and how these expres-
sions should be normalized is an unaddressed issue
and worths future investigation.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments and valuable input. We thank
Prof. Yang Liu, Deana Pennell, Bo Han, and Prof.
Tim Baldwin for sharing the annotated data and the
useful discussions. Part of this work was done while
Xiao Jiang was a research intern in Bosch Research.
1042
References
Kevin Atkinson. 2006. Gnu aspell. http://aspell.net/.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of COLING/ACL, pages 33?40.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of ACL, pages 770?
779.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of ACL.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using word
lengthening to detect sentiment in microblogs. In Pro-
ceedings of EMNLP, pages 562?570.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng.
2010. Probabilistic model-based sentiment analysis of
twitter messages. In Proceedings of the IEEE Work-
shop on Spoken Language Technology, pages 79?84.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157?174.
Kenneth W. Church and William A. Gale. 1991. Prob-
ability scoring for spelling correction. Statistics and
Computing, 1:93?103.
CMU. 2007. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text messages normalization. In Pro-
ceedings of the NAACL HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. In Computer
and Information Science, Report A81, Helsinki Uni-
versity of Technology.
David Crystal. 2009. Txtng: The gr8 db8. Oxford Uni-
versity Press.
Matt Davis. Reading jumbled texts. http://www.mrc-
cbu.cam.ac.uk/personal/matt.davis/Cmabrigde/.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS tagging and parsing the twitterverse. In
Proceedings of the AAAI Workshop on Analyzing Mi-
crotext, pages 20?25.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011. Contextual bearing on linguistic
variation in social media. In Proceedings of the ACL
Workshop on Language in Social Media, pages 20?29.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
Proceedings of ACL, pages 368?378.
Mindaugas Idzelis. 2005. Jazzy: The java open source
spell checker. http://jazzy.sourceforge.net/.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proceedings of HLT/NAACL, pages 372?
379.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: Are two metaphors
better than one? In Proceedings of COLING, pages
441?448.
Taku Kudo. 2005. CRF++: Yet another CRF took kit.
http://crfpp.sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282?289.
Franklin Mark Liang. 1983. Word hy-phen-a-tion by
com-put-er. In PhD Dissertation, Stanford University.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why
is ?sxsw? trending? Exploring multiple text sources
for twitter topic summarization. In Proceedings of the
ACL Workshop on Language in Social Media (LSM),
pages 66?75.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011b. Insertion, deletion, or substitution? Normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of ACL, pages 71?76.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011c. Recognizing named entities in tweets.
In Proceedings of ACL, pages 359?367.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Information
Processing and Management: An International Jour-
nal, 27(5):517?522.
Rada Mihalcea. 2007. Using wikipedia for auto-
matic word sense disambiguation. In Proceedings of
NAACL, pages 196?203.
Deana L. Pennell and Yang Liu. 2010. Normalization
of text messages for text-to-speech. In Proceedings of
ICASSP, pages 4842?4845.
Deana L. Pennell and Yang Liu. 2011. A character-
level machine translation approach for normalization
of sms abbreviations. In Proceedings of the 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 974?982.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
1043
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25?26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL, pages 147?155.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In Proceedings of EMNLP.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287?333.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904.
L. Venkata Subramaniam, Shourya Roy, Tanveer A.
Faruquie, and Sumit Negi. 2009. A survey of types
of text noise and techniques to handle noisy text. In
Proceedings of AND.
Crispin Thurlow. 2003. Generation txt? the sociolin-
guistics of young people?s text-messaging. Discourse
Analysis Online.
Endel Tulving and Daniel L. Schacter; Heather A. Stark.
1982. Priming effects in word fragment comple-
tion are independent of recognition memory. Journal
of Experimental Psychology: Learning, Memory and
Cognition, 8(4).
Twitter. 2011. http://en.wikipedia.org/wiki/Twitter.
Kuansan Wang, Christopher Thrasher, Evelyne Viegas,
Xiaolong Li, and Bo june (Paul) Hsu. 2010. An
overview of microsoft web n-gram corpus and appli-
cations. In Proceedings of NAACL-HLT, pages 45?48.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI
Workshop on Analyzing Microtext, pages 74?79.
1044
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 66?75,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Why is ?SXSW? trending? Exploring Multiple Text Sources for
Twitter Topic Summarization
Fei Liu1 Yang Liu1 Fuliang Weng2
1Computer Science Department, The University of Texas at Dallas
2Research and Technology Center, Robert Bosch LLC
{feiliu, yangl}@hlt.utdallas.edu1
fuliang.weng@us.bosch.com2
Abstract
User-contributed content is creating a surge on
the Internet. A list of ?buzzing topics? can
effectively monitor the surge and lead people
to their topics of interest. Yet a topic phrase
alone, such as ?SXSW?, can rarely present
the information clearly. In this paper, we
propose to explore a variety of text sources
for summarizing the Twitter topics, includ-
ing the tweets, normalized tweets via a ded-
icated tweet normalization system, web con-
tents linked from the tweets, as well as inte-
gration of different text sources. We employ
the concept-based optimization framework for
topic summarization, and conduct both au-
tomatic and human evaluation regarding the
summary quality. Performance differences are
observed for different input sources and types
of topics. We also provide a comprehensive
analysis regarding the task challenges.
1 Introduction
User contributed content has become a major source
of information in the Web 2.0 era. People follow
their topics of interest, share their experience or
opinions on a variety of interactive platforms, in-
cluding forums, blogs, microblogs, social network-
ing sites, etc. To keep track of the trends online
and suggest topics of interest to the general public,
many leading websites provide a ?buzzing? service
by publishing the current most popular topics on
their entrance page and update them regularly, such
as the ?popular now? column on Bing.com, ?trend-
ing topics? on Twitter.com, ?trending now? on Ya-
hoo.com, Google Trends, and so forth. Often pop-
ular topics are in the form of a list of keywords or
phrases1. Take Twitter.com as an example. Clicking
on a trending topic phrase will return a set of relevant
Twitter posts (tweets) or web pages. Nonetheless,
whether this is a convenient way for users to navi-
gate through the popular topic information is still ar-
guable. For example, when ?SXSW? was listed as a
trending topic, it seems difficult to understand at the
first glance. A condensed topic summary would be
extremely helpful for the users before diving into the
massive search results to figure out what this topic
phrase is about and why it is trending. In this paper,
our goal is to generate a short text summary for any
given topic phrase. Note that the proposed approach
is not limited to trending topics, but can be applied
to arbitrary Twitter topics.
There are a lot of differences between tweets and
traditional written text that has been widely used
for automatic summarization. In Table 1, we show
example tweets for the topic ?SXSW?. The tweets
were extracted by searching the Twitter site using
the topic phrase as a query. We also provide an ex-
cerpt of the linked web content to help understand
the topic. The tweets present some unique charac-
teristics:
? All tweets are limited to 140 characters. Some
tweets are news headlines from the official me-
dia, others are generated by users with vari-
ous degrees of familiarity with the social me-
dia. The resulting tweets can be very different
regarding the text quality and word usage.
1They are referred to as topic phrases hereafter, with no dis-
tinction between keywords and key phrases.
66
Twitter Topic: ?SXSW?
Twts
I wish I could go to SXSW... I will, one day!
http://sxsw.com/
RT @user123: SXSW Film
Round-Up: Documentaries http://bit.ly/fg033b
@user456 yo.whats good,i met u at sxsw, talkin
bout that feature.I was gonna see about sending
u a few beats.u lookin for only original?
The South by Southwest (SXSW) Conferences
Web & Festivals offer the unique convergence of
Cont original music, independent films, and
emerging technologies...(http://sxsw.com/)
Table 1: Example tweets and an excerpt of the linked web
content for Twitter topic ?SXSW?.
? Tweets lack structure information, contain var-
ious ill-formed sentences and grammatical er-
rors. There are lots of noisy nonstandard to-
kens, such as abbreviations (?feelin? for ?feel-
ing?), substitutions (?Pr1mr0se? for ?Prim-
rose?), emoticons, etc.
? Twitter invented its own markup language.
?@user? is used to reply to a specific user or
call for attentions. The hashtag ?#topic? aims
to assign a topic label to the tweet, and is fre-
quently employed by the twitter users.
? Tweets frequently contain embedded URLs
that direct users to other online content, such
as news web pages, blogs, organization home-
pages (Wu et al, 2011). According to Twitter?s
news release in September 2010 (Rao, 2010),
25% of tweets contain an URL. These linked
web pages provide a much richer source of in-
formation than is possible in the 140-character
tweet.
These Twitter-specific characteristics may pose
challenges to the automatic summarization systems
for identifying the essential information. In this pa-
per, we focus on two such characteristics that are
not studied in previous literature, the web content
link and the non-standard tokens in tweets. Specif-
ically, we ask two questions: (1) Is the web content
linked from the tweets useful for summarization?
Can we integrate different text sources, including
the tweets and linked web pages, to generate more
informative Twitter topic summaries? (2) what is
the effect of nonstandard tokens on summarization
performance? Will the summaries be improved if
the noisy tweets were pre-normalized into standard
English sentences? We investigate these two ques-
tions under a concept-based summarization frame-
work using integer linear programming (ILP). We
utilize text input that has various quality and is orig-
inated from multiple sources, and thoroughly ana-
lyze the resulting summaries using both automatic
and human evaluation metrics.
2 Related Work
There is not much previous work on summarizing
the Twitter topics. Most previous summarization lit-
erature focused on the written text domain, as driven
by the annual evaluation tracks of the DUC (Doc-
ument Understanding Conference) and TAC (Text
Analysis Conference). To some extent, Twitter topic
summarization is related to spoken document sum-
marization, since both tasks deal with the conver-
sational text that is contributed by multiple par-
ticipants and contains lots of ill-formed sentences,
colloquial expressions, nonstandard word tokens or
high word error rate, etc. To summarize the spo-
ken text, (Zechner, 2002) aimed to address prob-
lems related to disfluencies, extraction units, cross-
speaker coherence, etc. (Maskey and Hirschberg,
2005; Murray et al, 2006; Galley, 2006; Xie et
al., 2008; Liu and Liu, 2010a) incorporated lexical,
structural, speaker, and discourse cues to generate
textual summaries for broadcast news and meeting
conversations.
For microblog summarization, (Sharifi et al,
2010a) proposed a phrase reinforcement (PR) algo-
rithm to summarize the Twitter topic in one sen-
tence. The algorithm builds a word graph using the
topic phrase as the root node; each word node is
weighted in proportion to its distance to the root and
the corresponding phrase frequency. The summary
sentence is selected as one of the highest weighted
paths in the graph. (Sharifi et al, 2010b; Inouye,
2010) introduced a hybrid TF-IDF approach to ex-
tract one- or multiple-sentence summary for each
topic. Sentences were ranked according to the av-
erage TF-IDF score of the consisting words; top
weighted sentences were iteratively extracted, but
excluding those that have high cosine similarity with
the existing summary sentences. They showed the
Hybrid TF-IDF approach performs constantly bet-
67
ter than the PR algorithm and other traditional sum-
marization systems. Our approach of summarizing
the Twitter topics is different from the above stud-
ies in that, we focus on exploring richer informa-
tion sources (such as the online web content) and in-
vestigating effect of non-standard tokens. There are
also studies working on visualizing Twitter topics
by identifying a set of topic phrases and presenting
the related tweets to users (O?Connor et al, 2010;
Marcus et al, 2011). Our proposed approach can be
beneficial to these systems by providing informative
topic summaries generated from rich text sources.
3 Data Collection
We collected 5,537 topic phrases and the reference
topic descriptions by crawling the Twitter.com and
WhatTheTrend.com simultaneously during the pe-
riod of Aug 22th, 2010 to Oct 30th, 2010 (about 70
days). The Twitter API was queried every 5 min-
utes for the current top ten trending topics. For each
of these topics, a search query was submitted to the
Twitter Search API to retrieve only English tweets
related to this topic. If any tweet contains embedded
URLs linked to the other web pages, the contents
of these web pages were retrieved. For each topic,
we limit the maximum number of retrieved tweets to
5,000 and webpages to 100. An example is shown in
Table 1 for a topic phrase, some related tweets, and
an excerpt of the linked webpage. WhatTheTrend
API provides short topic descriptions contributed
and constantly updated by the Twitter users. There
is also a manually assigned category tag for each
topic phrase. We found the top categories among
the collected topics are ?Entertainment (29.26%)?,
?Sports (25.58%)?, and ?Meme (15.69%, pointless
babble)?. We divided the collected topics into two
groups: the general topics (e.g., ?Chilean miners?,
?MTV VMA?) and the hashtag topics that start with
the ?#? (e.g., ?#top10rappers?, ?#octoberwish?).
To generate reference summaries for the Twit-
ter topics, two human annotators were asked to
pick the topic descriptions/sentences (collected from
WhatTheTrend.com) that are appropriate and valu-
able to be included in the summary. This is per-
formed on a selected set of 1,511 topics with both
trending duration and number of tweets greater than
our predefined thresholds. For each of the topic sen-
tences, we ask the annotators to label its category:
(1) the sentence is a general description of the topic;
(2) the sentence is trying to explain why the topic is
trending; (3) it is hard to tell the difference. Over-
all, the two annotators have good agreement (Kappa
= 0.67) regarding whether or not to include a sen-
tence in the summary. Among the selected summary
sentences, 22.58% of them were assigned with con-
flicting purpose tags such as (1) or (2). To form
a reference summary, we concatenate all the topic
sentences selected by both annotators. Since some
reference descriptions are simply repetition of oth-
ers with very minor changes, we reduce the dupli-
cates by iteratively removing the oldest sentences if
all the consisting words are covered by the remain-
ing sentence collection, until no sentence can be re-
moved. On average, the reference summary for gen-
eral and hashtag topics contains 44 and 40 words
respectively.
4 Summarization System
For each of the topic phrases, our goal is to gener-
ate a short textual summary that can best convey the
main ideas of the topic contents. We explore and
compare multiple text sources as summarization in-
put, including the user-contributed tweets, web con-
tents linked from the tweets, as well as combination
of the two sources. The concept-based optimization
approach (Gillick et al, 2009; Xie et al, 2009; Mur-
ray et al, 2010) was employed for selecting informa-
tive summary sentences and minimizing the redun-
dancy. Note that our focus of this paper is not devel-
oping new summarization systems, but rather utiliz-
ing and integrating different text sources for gener-
ating more informative Twitter topic summaries.
4.1 Concept-based Optimization Framework
Concept-based summarization approach first ex-
tracts a set of important concepts for each topic, then
selects a collection of sentences that can cover as
many important concepts as possible, while within
the specified length limit. This idea is realized us-
ing the integer linear programming-based (ILP) op-
timization framework, with objective function set to
maximize the sum of the weighted concepts:
max
?
i
wici
68
where ci is a binary variable indicating whether the
concept i is covered by the summary; wi is the
weight assigned to ci.
We enforce two sets of length constraints to the
summary: sentence- or word-based. Sentence con-
straint requires the total number of selected sum-
mary sentences to not to exceed a length limit L1;
while word constraint requires the total words of
selected sentences not to exceed length limit L2.
These two constraints are:
?
j
sj < L1 or
?
j
ljsj < L2
where sj is a binary variable indicating whether sen-
tence j was selected in the summary; lj represents
the number of words in sj .
Further, we connect concept i with sentence j us-
ing two sets of constraints. For all the sentences that
contain concept i, if any sentence was selected in
the summary, the concept i should be covered by the
summary; reversely, if concept i was covered by the
summary, at least one of the sentences containing
concept i should be selected.
?i ci ?
?
j
oijsj
?i, j ci ? oijsj
where the binary variable oij is used to indicate
whether concept i exists in sentence j.
The concepts are selected by extracting n-grams
(n=1, 2, 3) from the input documents corresponding
to each topic. Similar to (Xie et al, 2009), we re-
move (1) n-grams that appear only once in the docu-
ments; (2) n-grams that have a consisting word with
inverse document frequency (IDF) value lower than
a threshold; (3) n-grams that are enclosed by higher
order n-grams with the same frequency. These fil-
ters are designed to exclude insignificant n-grams
from the concept set. The IDF scores were calcu-
lated from a large background corpus corresponding
to the input text source, using individual sentences
or tweets as pseudo-documents; words with low IDF
scores (such as stopwords) tend to appear in many
sentences and therefore should be removed from the
concept set. We assign a weight wi to an n-gram
concept as follows:
wi = tf(ngrami)? n?max
j
idf(wij)
where tf(ngrami) is the term frequency of ngrami
in the input document of the topic; n denotes the
order of ngrami; wij are the consisting words of
ngrami; idf(wij) represents IDF value of word
wij . This approach aims to extract n-grams that ap-
pear frequently in each topic, but do not appear fre-
quently in a large background corpus. The weights
are also biased towards longer n-grams since they
carry more information.
4.2 Summarization Input
In this section, we explore different text sources
as input to the summarization system. Different
from previous studies that take input from a sin-
gle text source, we propose to utilize both the
user-contributed tweets and the linked web con-
tents for Twitter topic summarization, since these
two sources provide very different text quality and
may contain complementary information regarding
the topic. These text sources also pose great chal-
lenges to the summarization system: the tweets are
short and extremely noisy; while the online contents
linked from the tweets may have vastly different lay-
outs and contain a variety of information.
4.2.1 Original Tweets
As shown in Table 1, the initially collected tweets
are very noisy. They are passed through a set of pre-
processors to remove non-ascii characters, HTML
special characters, URLs, emoticons, punctuation
marks, retweet tags (RT @user), etc. We also re-
move the reply (@) and hashtag (#) tokens that do
not carry important syntactic roles (such as in the
subject or object position) by using a set of regular
expressions. These preprocessed tweets are sorted
by date and taken as the first input source to the sum-
marization system (denoted by ?OrigTweets?).
4.2.2 Normalized Tweets
The original tweets contain various nonstandard
word tokens. In Table 2, we list the possible to-
ken categories and corresponding examples. We hy-
pothesize that normalizing these nonstandard tokens
into standard English words and using the normal-
ized tweets as input can help boost the summariza-
tion performance.
We developed a twitter message normalization
system based on the noisy-channel framework and
a proposed letter transformation model (Liu et al,
69
Category Example
(1) abbreviation tgthr, weeknd, shudnt
(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture
(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq
(4) typographic error thimg, macam
(5) stylistic variation betta, hubbie, cutie
(6) letter repetition pleeeaas, togtherrr
(7) any combination of (1) to (6) luvvvin, 2moro, m0rnin
Table 2: Nonstandard token categories and examples.
2011). Given a noisy tweet T , our goal is to nor-
malize it into a standard English word sequence S.
Under the noisy channel model, this is equivalent to
finding the sequence S? that maximizes p(S|T ):
S? = argmaxS p(S|T ) = argmaxS(
?
i
p(Ti|Si))p(S)
where we assume that each non-standard token Ti
is dependent on only one English word Si, that is,
we are not considering acronyms (e.g., ?bbl? for ?be
back later?) in this study. p(S) can be calculated
using a language model (LM). We formulate the
process of generating a nonstandard token Ti from
dictionary word Si using a letter transformation
model, and use the model confidence as the prob-
ability p(Ti|Si). This transformation process will be
learned automatically through a sequence labeling
framework. To form a nonstandard token, each let-
ter in the dictionary word can be labeled with: (a)
one of the 0-9 digits; (b) one of the 26 characters
including itself; (c) the null character ?-?; (d) a let-
ter combination. We integrate character-, phonetic-,
and syllable-level features in the model that can ef-
fectively characterize the formation process of non-
standard tokens. In general, the letter transforma-
tion approach will handle the nonstandard tokens
listed in Table 2 yet without explicitly categorizing
them. The proposed system also achieved robust
performance using the automatically collected train-
ing word pairs. On a test set of 3,802 distinct non-
standard tokens collected from Twitter, our system
achieved 68.88% 1-best normalization word accu-
racy and 78.27% 3-best accuracy.
We identify the nonstandard tokens that need to
be normalized using the following criteria: (1) it is
not in the CMU dictionary2; (2) it does not contain
capitalized letter; (3) it appears infrequently in the
2http://www.speech.cs.cmu.edu/cgi-bin/cmudict
topic (less than a threshold); (4) it is not a popular
chat acronyms (such as ?lol?, ?omg?); (5) it contains
letters/digits/apostrophe, but should not be numbers
only. These criteria are designed to avoid normaliz-
ing the named entities, frequently appearing out-of-
vocabulary terms (such as ?itunes?), chat acronyms,
usernames, and hashtags. The selected nonstandard
tokens in the original tweets will be replaced by the
system generated 1-best candidate word. Note that
we do not discriminate the context when replacing
each nonstandard token. This will be addressed in
the future work. We use these normalized tweets as
a second source of summarization input and name
them ?NormTweets?.
4.2.3 Linked Web Contents
For each Twitter topic, we collect a set of web
pages linked by the topic tweets and use them as
another source of summarization input. For each
topic, we select up to n (n = 10) URLs that appear
most frequently in the topic tweets and infrequently
across different Twitter topics. This scheme is sim-
ilar to the TF-IDF measure. This way we can se-
lect the salient URLs for each topic while avoiding
the spam URLs. The contents of these URLs were
collected and only distinct web pages were retained.
We use an HTML parser3 to extract the textual con-
tents, and perform sentence segmentation (Reynar
and Ratnaparkhi, 1997) on the parsed web pages.
All the pages corresponding to the same topic were
sorted by the date they were first cited in the tweets.
These web pages were taken as another input text
source for the summarization system, denoted as
?Web?.
4.2.4 Combining Tweets and Web Contents
We expect that taking advantage of both tweets
and linked web contents would benefit the topic
summarization system. Consolidating the distinct
text sources may help boost the weight of key con-
cepts and eliminate the spam information. As a pre-
liminary study, we investigate concatenating either
the original tweets or the normalized tweets with
the linked web pages as input to the concept-based
summarization system. This results in two inputs
?Web + OrigTweets? and ?Web + NormTweets?. We
will explore other ways of combining the two text
3http://jericho.htmlparser.net/docs/index.html
70
sources in future work.
5 Experiments
5.1 Experimental Setup
Among the collected topics, we select 500 general
topics (such as ?Chilean miners?) and 50 hashtag
topics (such as ?#octoberwish?, ?#wheniwasakid?)
for experimentation. On average, a general topic
contains 1673 tweets and 3.43 extracted linked web
pages; while a hashtag topic contains 3316 tweets
but does not have meaningful linked web pages.
The concept-based optimization system was con-
figured to extract a collection of sentences/tweets
for each topic, using either the sentence- or word-
constraint (denoted as ?#Sent? and ?#Word?). We
opt to set individual length constraint for each topic
rather than using a uniform length limit for all the
topics, since the topics can be very different in
length and duration. We use the number of sen-
tences/words in the reference summary as the sen-
tence/word constraint for each topic. Note that in
practice this reference summary length information
may not be available. We use the length constraints
obtained from the reference summary in this ex-
ploratory study, since our focus is to first evaluate if
twitter trending summarization is feasible, and what
are the effects of different information sources and
non-standard tokens. For a comparison to our ap-
proach, we implement the Hybrid TF-IDF approach
in (Sharifi et al, 2010b; Inouye, 2010) as a baseline
using ?OrigTweets? as input. For the baseline, the
summary length is altered according to the sentence-
or word-constraint. The last summary tweet is cut in
the middle if it exceeds the word limit.
The ROUGE-1 F-scores (Lin, 2004) are used to
measure the n-gram (n=1) overlap between the sys-
tem summaries and reference summaries. Since the
ROUGE scores may not correlate well with the hu-
man judgments (Liu and Liu, 2010b), we also per-
formed human evaluation by asking annotators to
score both the system and reference summaries re-
garding the linguistic quality and content respon-
siveness, in the hope this will benefit future research
in this direction.
5.2 Automatic Evaluation
We present the results (ROUGE-1 F-measure) for
the general topics in Table 3. ROUGE-2 and
General Topics R-1 F(%) RefSum
Input Source Render #Sent #Word Cov(%)
OrigTweets
Orig 29.53 30.21 94.81
Norm 29.41 30.21 94.81
NormTweets Norm 29.69 30.35 94.60
Web 24.32 25.07 63.74
Web + OrigTweets 29.58 30.44 95.37
Web + NormTweets 29.66 30.54 95.16
OrigTweets
(Sharifi et al, 2010b) 24.37 25.68 94.81
Table 3: ROUGE-1 F-measure and reference summary
coverage scores for general topics.
ROUGE-4 scores show similar trends and thus are
not presented. Five different text sources were ex-
ploited as the system inputs, as described in Sec-
tion 4.2. To measure the quality of the input for
summarization, we also include reference summary
coverage score in the table, defined as the percent-
age of words in the reference summary that are cov-
ered by the input text source. When using tweets
as input, we also investigate whether we should ap-
ply tweet normalization before or after the summa-
rization process, that is ?pre-normalization? (using
?NormTweets? as input), or ?post-normalization?
(using ?OrigTweets? as input, and rendering the nor-
malized summary tweets).
Compared to the Hybrid TF-IDF approach (Shar-
ifi et al, 2010b; Inouye, 2010), our system per-
forms significantly better (p < 0.05) according
to the paired t-test; however, we also notice the
ROUGE scores are lower compared to summariza-
tion in other text domains. This indicates that Twit-
ter topic summarization is very challenging. Com-
paring the two constraints used in the concept-based
optimization framework, we found that the word
constraint performs constantly better for the gen-
eral topics. This is natural since the word constraint
tightly bounds the length of the system output, while
the sentence constraint is relatively loose. For the
different sources, we notice using linked web pages
alone yields worse summarization performance, as
well as lower reference summary coverage; how-
ever, when combined with the tweets, there is a
slight increase in the coverage scores, and some-
times improved summarization results. This sug-
gests that the linked web pages can contain extra
71
useful information for generating summaries. Re-
garding normalization, results show that the ?pre-
normalization? (using normalized tweets as input)
can generally improve the summary tweet selec-
tion. For general topics, the best performance was
achieved by combining the normalized tweets and
linked web pages as input source and using the
word-level constraint.
Hashtag Topics R-1 F(%) RefSum
Input Source Render #Sent #Word Cov(%)
OrigTweets
Orig 9.08 7.19 93.93
Norm 9.09 7.16 93.93
NormTweets Norm 9.35 7.14 93.71
OrigTweets
(Sharifi et al, 2010b) 7.03 7.72 93.93
Table 4: ROUGE-1 F-measure and reference summary
coverage scores for hashtag topics.
Results for hashtag topics were shown in Table
4 using tweets as input (there are no linked web-
pages for these topics). We notice the reference cov-
erage scores are satisfying, yet the system output
barely matches the reference summaries (very low
ROUGE-1 scores). Looking at the reference and
system generated summaries for the hashtag top-
ics, we found the system output is more specific
(e.g., ?#octoberwish everything goes well.?), while
the reference summaries are often very general (e.g.,
?people tweeting about their wishes for October.?).
The human annotators also noted that most hashtag
topics (such as ?#octoberwish?, ?#wheniwasakid?)
are self-explainable and may require special atten-
tion to redefine an appropriate summary. Using
sentence constraints yields better performance than
word-based one, with larger performance difference
than that for the general topics. We found the
word-constraint summaries tend to include tweets
that are very short and noisy. Our system with
sentence-based length constraint also significantly
outperforms the Hybrid TF-IDF approach (Sharifi
et al, 2010b; Inouye, 2010). For hashtag topics,
the best performance was achieved using the ?pre-
normalization? with sentence constraint.
For an analysis, we generate oracle system per-
formance by using the reference summaries to ex-
tract a set of unweighted concepts to use in the ILP
optimization framework for sentences/tweets selec-
tion. This results in 61.76% ROUGE-1 F-score for
the general topics and 40.34% for the hashtag topics,
indicating abundant space for future improvement.
We also notice that though there is some perfor-
mance gain using normalized tweets and linked web
contents, the improvement is not statistically signifi-
cant as compared to using the original tweets. Upon
closer examination, we found the normalization sys-
tem replaced 1.08% and 1.8% of the total word to-
kens for the general and hashtag topics respectively;
these tokens spread in 13.12% and 16.85% of the
total tweets. The relatively small percentage of the
normalized tokens partly explains the marginal per-
formance gain when using the normalized tweets as
input. Similarly for linked web content, though it
contains some sentences that can provide more de-
tails of the topic, but they can also take more space
in the summary as compared to the short and con-
densed tweets. Therefore using the combined tweets
and linked webpages does not significantly outper-
form using just the tweets.
5.3 Human Evaluation
General Hashtag
Tweet Web Ref Tweet Ref
Gram. 3.13 3.42 4.52 3.04 4.24
NRedun. 3.93 4.64 4.30 4.82 3.62
Clarity 4.07 3.91 4.77 4.06 4.60
Focus 3.64 3.03 4.75 3.22 4.72
Content 2.82 2.55 n/a 2.60 n/a
ExtraInfo n/a 2.63 n/a n/a n/a
Table 5: Linguistic quality, content coverage, and useful-
ness scores judged by human assessors.
We ask two human annotators to manually evalu-
ate the system and reference summaries regarding
the readability and content coverage. Readability
includes grammaticality, non-redundancy, referen-
tial clarity, and focus; content coverage was eval-
uated for system summaries against the reference
summary. The annotators were also asked to rate
the ?Web? summaries regarding whether they pro-
vided extra useful topic information on top of the
?Tweet? summary. 50 general topics and 25 hash-
tag topics were randomly selected for assessment.
The ?Tweet? and ?Web? summaries were generated
using the original tweets and linked web pages with
word constraint for general topics, and sentence con-
straint for hashtag topics. Each of the assessors was
72
General Topic: ?3PAR?
RefSum
Dell Inc. and Hewlett-Packard Co. are both bidding for storage device maker 3Par Inc.
3Par jumped 21 percent after Hewlett- Packard Co. offered $30 a share for the company.
TweetSum
Dell ups 3Par offer yet again, to $27 per share
Dell Raises 3par Offer to Match HP Bid
Dell Matches HP?s Offer for 3Par, Boosting Bid to $1.8 Billion
WebSum
Dell Matches HP?s $27 Offer, Is Accepted by 3PAR.
3PAR has accepted an increased acquisition offer from Dell of US$27 per share, matching
Hewlett-Packard?s earlier raised bid.
Hashtag Topic: ?#wheniwasakid?
RefSum
when i was a kid.... people are sharing there best (good or bad) memories from childhood.
People reminise the wonderful times about being a kid.
TweetSum
#whenIwasakid getting wasted meant eating all the ice cream and candy you could until you puked!
#whenIWasAKid Apple & Blackberry were fruits not phones.
Table 6: Example system and reference summaries for both general and hashtag topics.
asked to judge all the summaries and assign a score
for each criterion on a 1 to 5 Likert scale (5 being
the best quality). The average scores of the two as-
sessors were presented in Table 5.
For general topics, the ?Web? summaries outper-
form the ?Tweet? summaries on both grammatical-
ity and non-redundancy, confirming the advantage
of using the high-quality linked web pages. The
referential clarity and focus scores of the ?Web?
summaries are not very high, since the summary
sentences were extracted simultaneously from sev-
eral web pages, and the system subjects to simi-
lar challenges as in multi-document summarization.
The content coverage scores of both system sum-
maries seem to correlate well with the ROUGE-1
F-measure, with a higher score for ?Tweet? sum-
maries. The assessors also rated that 48% of the
?Web? summaries contain ?Somewhat Useful? ex-
tra topic information, and 21% are ?Very Useful?.
Note that this could be just because of the inherent
difference of the two summaries, regardless of the
input source, but in general we believe the linked
web pages (such as the news documents) can pro-
vide more detailed and coherent stories as compared
to the 140-character tweets. For hashtag topics, the
?Tweet? summaries yield worse grammaticality and
focus scores, but have very high non-redundancy
score. On the contrary, the reference summaries
often contain redundant information. The content
match score between the system and reference sum-
maries (2.6) does not seem to reflect the ROUGE
scores. We hypothesize that even though the speci-
ficity of the two summaries is different, the asses-
sors may still think the system summaries match the
reference ones to some extent. A larger scale human
evaluation is needed to study the correlation between
human and automatic evaluation.
5.4 Discussions
We show an example of reference and system gen-
erated summaries for a general and a hashtag topic
in Table 6, and summarize some challenges for this
summarization task below:
? Gold standard summaries are difficult and
time-consuming to obtain. The reference de-
scriptions from WhatTheTrend.com were cre-
ated by Twitter users, which vary a lot in
word usage and would be unavoidably biased
to the information available in Twitter. The
user-contributed descriptions may also contain
spam descriptions, repetitions, nonstandard to-
kens, etc. It would be better to have a con-
cise non-redundant sentence collection for de-
veloping future summarization systems. In
particular, hashtag topics need special atten-
tion. They account for 40% of the total trend-
ing topics in 2010 according to the statistics
in WhatTheTrend.com4. Yet there still lacks
standard definition regarding a good hashtag
summary. From the example topic ?#wheni-
wasakid? in Table 6, we can see they are very
different in nature from general topics, thus fu-
ture efforts are needed to define an appropriate
summary.
4http://yearinreview.whatthetrend.com/
73
? Evaluation issues. Word based evaluation
measures will rarely consider semantic relat-
edness between concepts, or name entity vari-
ations, such as ?Hewlett-Packard? vs. ?HP?,
?Dell ups 3Par offer? vs. ?Dell Raises 3par
Offer?, etc. When comparing the system
summaries with short human-written reference
summaries, the word overlap varies a lot for
different human summarizers.
? Dynamically changing topics/events. Some
general topics are related to events that are con-
stantly changing. Take the ?3PAR? topic in
Table 6 as an example, where two companies
take turns to raise the bid for 3Par Inc. A good
topic summary should be able to develop a se-
ries of sub-events and show the topic evolving
process.
6 Conclusion
In this paper, we proposed to explore a variety of text
sources for summarizing the Twitter topics. We em-
ployed the concept-based optimization framework
with multiple input text sources to generate the sum-
maries. We conducted both automatic and human
evaluation regarding the summary quality. Better
performance is observed when using the normalized
tweets as input, indicating special treatment should
be performed before feeding the noisy tweets to the
summarization system. We also found the linked
web contents can provide extra useful topic infor-
mation. In future work, we will compare our sys-
tem with other dedicated microblog summarization
systems, as well as address some of the challenges
identified in this study.
Acknowledgments
This work is partly supported by NSF award IIS-
0845484. Any opinions expressed in this work are
those of the authors and do not necessarily reflect the
views of NSF.
References
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tu?r. 2009. A global optimization
framework for meeting summarization. In Proc. of
ICASSP.
David Inouye. 2010. Multiple post microblog summa-
rization. REU Research Final Report.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Workshop on Text Sum-
marization Branches Out.
Fei Liu and Yang Liu. 2010a. Exploring speaker char-
acteristics for meeting summarization. In Proc. of IN-
TERSPEECH.
Feifan Liu and Yang Liu. 2010b. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. IEEE Transactions on Audio, Speech, and
Language Processing, 18(1):187?196.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? Normaliz-
ing text messages without pre-categorization nor su-
pervision. In Proc. of ACL-HLT.
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. TwitInfo: Aggregating and visualizing
microblogs for event exploration. In Proc. of CHI.
Sameer Maskey and Julia Hirschberg. 2005. Compar-
ing lexical, acoustic/prosodic, structural and discourse
features for speech summarization. In Proc. of Eu-
rospeech.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proc. of HLT-
NAACL.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010. Interpretation and transformation for abstract-
ing conversations. In Proc. of NAACL.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. Tweetmotif: Exploratory search and topic sum-
marization for twitter. In Proc. of the International
AAAI Conference on Weblogs and Social Media.
Leena Rao. 2010. Twitter seeing 90 mil-
lion tweets per day, 25 percent contain links.
http://techcrunch.com/2010/09/14/twitter-seeing-90-
million-tweets-per-day/.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. of the Fifth Conference on Ap-
plied Natural Language Processing.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010a. Summarizing microblogs automatically. In
Proc. of HLT/NAACL.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010b. Experiments in microblog summariza-
tion. In Proc. of IEEE Second International Confer-
ence on Social Computing.
74
Shaomei Wu, Jake M. Hofman, Winter A. Mason, and
Duncan J. Watts. 2011. Who says what to whom on
twitter. In Proc. of WWW.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extractive
meeting summarization. In Proc. of IEEE Workshop
on Spoken Language Technology.
Shasha Xie, Benoit Favre, Dilek Hakkani-Tu?r, and Yang
Liu. 2009. Leveraging sentence weights in a concept-
based optimization framework for extractive meeting
summarization. In Proc. of INTERSPEECH.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
75
