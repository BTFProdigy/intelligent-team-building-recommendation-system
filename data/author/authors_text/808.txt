Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 189?198,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Inducing Temporal Graphs
Philip Bramsen Pawan Deshpande Yoong Keok Lee Regina Barzilay
MIT CSAIL MIT CSAIL DSO National Laboratories MIT CSAIL
bramsen@mit.edu pawan@mit.edu lyoongke@dso.org.sg regina@csail.mit.edu
Abstract
We consider the problem of constructing
a directed acyclic graph that encodes tem-
poral relations found in a text. The unit of
our analysis is a temporal segment, a frag-
ment of text that maintains temporal co-
herence. The strength of our approach lies
in its ability to simultaneously optimize
pairwise ordering preferences and global
constraints on the graph topology. Our
learning method achieves 83% F-measure
in temporal segmentation and 84% accu-
racy in inferring temporal relations be-
tween two segments.
1 Introduction
Understanding the temporal flow of discourse is
a significant aspect of text comprehension. Con-
sequently, temporal analysis has been a focus of
linguistic research for quite some time. Tem-
poral interpretation encompasses levels ranging
from the syntactic to the lexico-semantic (Re-
ichenbach, 1947; Moens and Steedman, 1987)
and includes the characterization of temporal dis-
course in terms of rhetorical structure and prag-
matic relations (Dowty, 1986; Webber, 1987; Pas-
sonneau, 1988; Lascarides and Asher, 1993).
Besides its linguistic significance, temporal
analysis has important practical implications. In
multidocument summarization, knowledge about
the temporal order of events can enhance both the
content selection and the summary generation pro-
cesses (Barzilay et al, 2002). In question an-
swering, temporal analysis is needed to determine
when a particular event occurs and how events re-
late to each other. Some of these needs can be
addressed by emerging technologies for temporal
analysis (Wilson et al, 2001; Mani et al, 2003;
Lapata and Lascarides, 2004; Boguraev and Ando,
2005).
This paper characterizes the temporal flow of
discourse in terms of temporal segments and their
ordering. We define a temporal segment to be
a fragment of text that does not exhibit abrupt
changes in temporal focus (Webber, 1988). A seg-
ment may contain more than one event or state, but
the key requirement is that its elements maintain
temporal coherence. For instance, a medical case
summary may contain segments describing a pa-
tient?s admission, his previous hospital visit, and
the onset of his original symptoms. Each of these
segments corresponds to a different time frame,
and is clearly delineated as such in a text.
Our ultimate goal is to automatically construct
a graph that encodes ordering between temporal
segments. The key premise is that in a coherent
document, temporal progression is reflected in a
wide range of linguistic features and contextual
dependencies. In some cases, clues to segment or-
dering are embedded in the segments themselves.
For instance, given a pair of adjacent segments,
the temporal adverb next day in the second seg-
ment is a strong predictor of a precedence relation.
In other cases, we can predict the right order be-
tween a pair of segments by analyzing their rela-
tion to other segments in the text. The interaction
between pairwise ordering decisions can easily be
formalized in terms of constraints on the graph
topology. An obvious example of such a con-
straint is prohibiting cycles in the ordering graph.
We show how these complementary sources of in-
formation can be incorporated in a model using
global inference.
We evaluate our temporal ordering algorithm on
a corpus of medical case summaries. Temporal
189
analysis in this domain is challenging in several re-
spects: a typical summary exhibits no significant
tense or aspect variations and contains few abso-
lute time markers. We demonstrate that humans
can reliably mark temporal segments and deter-
mine segment ordering in this domain. Our learn-
ing method achieves 83% F-measure in temporal
segmentation and 84% accuracy in inferring tem-
poral relations between two segments.
Our contributions are twofold:
Temporal Segmentation We propose a fully
automatic, linguistically rich model for temporal
segmentation. Most work on temporal analysis
is done on a finer granularity than proposed here.
Our results show that the coarse granularity of our
representation facilitates temporal analysis and is
especially suitable for domains with sparse tempo-
ral anchors.
Segment Ordering We introduce a new method
for learning temporal ordering. In contrast to ex-
isting methods that focus on pairwise ordering, we
explore strategies for global temporal inference.
The strength of the proposed model lies in its abil-
ity to simultaneously optimize pairwise ordering
preferences and global constraints on graph topol-
ogy. While the algorithm has been applied at the
segment level, it can be used with other temporal
annotation schemes.
2 Related Work
Temporal ordering has been extensively studied
in computational linguistics (Passonneau, 1988;
Webber, 1988; Hwang and Schubert, 1992; Las-
carides and Asher, 1993; Lascarides and Ober-
lander, 1993). Prior research has investigated
a variety of language mechanisms and knowl-
edge sources that guide interpretation of tempo-
ral ordering, including tense, aspect, temporal ad-
verbials, rhetorical relations and pragmatic con-
straints. In recent years, the availability of an-
notated corpora, such as TimeBank (Pustejovsky
et al, 2003), has triggered the use of machine-
learning methods for temporal analysis (Mani et
al., 2003; Lapata and Lascarides, 2004; Boguraev
and Ando, 2005). Typical tasks include identifica-
tion of temporal anchors, linking events to times,
and temporal ordering of events.
Since this paper addresses temporal ordering,
we focus our discussion on this task. Existing or-
dering approaches vary both in terms of the or-
dering unit ? it can be a clause, a sentence or
an event ? and in terms of the set of ordering
relations considered by the algorithm. Despite
these differences, most existing methods have the
same basic design: each pair of ordering units (i.e.,
clauses) is abstracted into a feature vector and a
supervised classifier is employed to learn the map-
ping between feature vectors and their labels. Fea-
tures used in classification include aspect, modal-
ity, event class, and lexical representation. It is im-
portant to note that the classification for each pair
is performed independently and is not guaranteed
to yield a globally consistent order.
In contrast, our focus is on globally optimal
temporal inference. While the importance of
global constraints has been previously validated
in symbolic systems for temporal analysis (Fikes
et al, 2003; Zhou et al, 2005), existing corpus-
based approaches operate at the local level. These
improvements achieved by a global model moti-
vate its use as an alternative to existing pairwise
methods.
3 TDAG: A representation of temporal
flow
We view text as a linear sequence of temporal
segments. Temporal focus is retained within a
segment, but radically changes between segments.
The length of a segment can range from a single
clause to a sequence of adjacent sentences. Fig-
ure 1 shows a sample of temporal segments from
a medical case summary. Consider as an example
the segment S13 of this text. This segment de-
scribes an examination of a patient, encompassing
several events and states (i.e., an abdominal and
neurological examination). All of them belong to
the same time frame, and temporal order between
these events is not explicitly outlined in the text.
We represent ordering of events as a temporal
directed acyclic graph (TDAG). An example of the
transitive reduction1 of a TDAG is shown in Fig-
ure 1. Edges in a TDAG capture temporal prece-
dence relations between segments. Because the
graph encodes an order, cycles are prohibited. We
do not require the graph to be fully connected ? if
the precedence relation between two nodes is not
specified in the text, the corresponding nodes will
not be connected. For instance, consider the seg-
ments S5 and S7 from Figure 1, which describe
her previous tests and the history of eczema. Any
1The transitive reduction of a graph is the smallest graph
with the same transitive closure.
190
S1 S12 S13 S14
S2 S10 S6
S8
S4
S3 S5
S7
S9S11
S1 A 32-year-old woman was admitted to the hospital because of left subcostal pain...
S2 The patient had been well until four years earlier,
S5 Three months before admission an evaluation elsewhere included an ultrasonographic ex-
amination, a computed tomographic (CT) scan of the abdomen...
S7 She had a history of eczema and of asthma...
S8 She had lost 18 kg in weight during the preceding 18 months.
S13 On examination the patient was slim and appeared well. An abdominal examination re-
vealed a soft systolic bruit... and a neurologic examination was normal...
Figure 1: An example of the transitive reduction of a TDAG for a case summary. A sample of segments
corresponding to the nodes marked in bold is shown in the table.
order between the two events is consistent with our
interpretation of the text, therefore we cannot de-
termine the precedence relation between the seg-
ments S5 and S7.
In contrast to many existing temporal represen-
tations (Allen, 1984; Pustejovsky et al, 2003),
TDAG is a coarse annotation scheme: it does not
capture interval overlap and distinguishes only a
subset of commonly used ordering relations. Our
choice of this representation, however, is not ar-
bitrary. The selected relations are shown to be
useful in text processing applications (Zhou et al,
2005) and can be reliably recognized by humans.
Moreover, the distribution of event ordering links
under a more refined annotation scheme, such as
TimeML, shows that our subset of relations cov-
ers a majority of annotated links (Pustejovsky et
al., 2003).
4 Method for Temporal Segmentation
Our first goal is to automatically predict shifts
in temporal focus that are indicative of segment
boundaries. Linguistic studies show that speakers
and writers employ a wide range of language de-
vices to signal change in temporal discourse (Best-
gen and Vonk, 1995). For instance, the presence of
the temporal anchor last year indicates the lack of
temporal continuity between the current and the
previous sentence. However, many of these pre-
dictors are heavily context-dependent and, thus,
cannot be considered independently. Instead of
manually crafting complex rules controlling fea-
ture interaction, we opt to learn them from data.
We model temporal segmentation as a binary
classification task. Given a set of candidate bound-
aries (e.g., sentence boundaries), our task is to se-
lect a subset of the boundaries that delineate tem-
poral segment transitions. To implement this ap-
proach, we first identify a set of potential bound-
aries. Our analysis of the manually-annotated cor-
pus reveals that boundaries can occur not only be-
tween sentences, but also within a sentence, at the
boundary of syntactic clauses. We automatically
segment sentences into clauses using a robust sta-
tistical parser (Charniak, 2000). Next, we encode
each boundary as a vector of features. Given a
set of annotated examples, we train a classifier2 to
predict boundaries based on the following feature
set:
Lexical Features Temporal expressions, such
as tomorrow and earlier, are among the strongest
markers of temporal discontinuity (Passonneau,
1988; Bestgen and Vonk, 1995). In addition to
a well-studied set of domain-independent tempo-
ral markers, there are a variety of domain-specific
temporal markers. For instance, the phrase ini-
tial hospital visit functions as a time anchor in the
medical domain.
To automatically extract these expressions, we
provide a classifier with n-grams from each of the
candidate sentences preceding and following the
candidate segment boundary.
Topical Continuity Temporal segmentation is
closely related to topical segmentation (Chafe,
1979). Transitions from one topic to another may
indicate changes in temporal flow and, therefore,
2BoosTexter package (Schapire and Singer, 2000).
191
identifying such transitions is relevant for tempo-
ral segmentation.
We quantify the strength of a topic change
by computing a cosine similarity between sen-
tences bordering the proposed segmentation. This
measure is commonly used in topic segmenta-
tion (Hearst, 1994) under the assumption that
change in lexical distribution corresponds to topi-
cal change.
Positional Features Some parts of the docu-
ment are more likely to exhibit temporal change
than others. This property is related to patterns in
discourse organization of a document as a whole.
For instance, a medical case summary first dis-
cusses various developments in the medical his-
tory of a patient and then focuses on his current
conditions. As a result, the first part of the sum-
mary contains many short temporal segments. We
encode positional features by recording the rela-
tive position of a sentence in a document.
Syntactic Features Because our segment
boundaries are considered at the clausal level,
rather than at the sentence level, the syntax sur-
rounding a hypothesized boundary may be indica-
tive of temporal shifts. This feature takes into ac-
count the position of a word with respect to the
boundary. For each word within three words of
the hypothesized boundary, we record its part-of-
speech tag along with its distance from the bound-
ary. For example, NNP+1 encodes the presence
of a proper noun immediately following the pro-
posed boundary.
5 Learning to Order Segments
Our next goal is to automatically construct a graph
that encodes ordering relations between tempo-
ral segments. One possible approach is to cast
graph construction as a standard binary classifica-
tion task: predict an ordering for each pair of dis-
tinct segments based on their attributes alone. If
a pair contains a temporal marker, like later, then
accurate prediction is feasible. In fact, this method
is commonly used in event ordering (Mani et al,
2003; Lapata and Lascarides, 2004; Boguraev and
Ando, 2005). However, many segment pairs lack
temporal markers and other explicit cues for order-
ing. Determining their relation out of context can
be difficult, even for humans. Moreover, by treat-
ing each segment pair in isolation, we cannot guar-
antee that all the pairwise assignments are consis-
tent with each other and yield a valid TDAG.
Rather than ordering each pair separately, our
ordering model relies on global inference. Given
the pairwise ordering predictions of a local clas-
sifier3, our model finds a globally optimal assign-
ment. In essence, the algorithm constructs a graph
that is maximally consistent with individual order-
ing preferences of each segment pair and at the
same time satisfies graph-level constraints on the
TDAG topology.
In Section 5.2, we present three global inference
strategies that vary in their computational and lin-
guistic complexity. But first we present our under-
lying local ordering model.
5.1 Learning Pairwise Ordering
Given a pair of segments (i, j), our goal is to as-
sign it to one of three classes: forward, backward,
and null (not connected). We generate the train-
ing data by using all pairs of segments (i, j) that
belong to the same document, such that i appears
before j in the text.
The features we consider for the pairwise order-
ing task are similar to ones used in previous re-
search on event ordering (Mani et al, 2003; Lapata
and Lascarides, 2004; Boguraev and Ando, 2005).
Below we briefly summarize these features.
Lexical Features This class of features cap-
tures temporal markers and other phrases indica-
tive of order between two segments. Represen-
tative examples in this category include domain-
independent cues like years earlier and domain-
specific markers like during next visit. To automat-
ically identify these phrases, we provide a classi-
fier with two sets of n-grams extracted from the
first and the second segments. The classifier then
learns phrases with high predictive power.
Temporal Anchor Comparison Temporal an-
chors are one of the strongest cues for the order-
ing of events in text. For instance, medical case
summaries use phrases like two days before ad-
mission and one day before admission to express
relative order between events. If the two segments
contain temporal anchors, we can determine their
ordering by comparing the relation between the
two anchors. We identified a set of temporal an-
chors commonly used in the medical domain and
devised a small set of regular expressions for their
comparison.4 The corresponding feature has three
3The perceptron classifier.
4We could not use standard tools for extraction and analy-
sis of temporal anchors as they were developed on the news-
paper corpora, and are not suitable for analysis of medical
192
values that encode preceding, following and in-
compatible relations.
Segment Adjacency Feature Multiple studies
have shown that two subsequent sentences are
likely to follow a chronological progression (Best-
gen and Vonk, 1995). To encode this information,
we include a binary feature that captures the adja-
cency relation between two segments.
5.2 Global Inference Strategies for Segment
Ordering
Given the scores (or probabilities) of all pairwise
edges produced by a local classifier, our task is
to construct a TDAG. In this section, we describe
three inference strategies that aim to find a con-
sistent ordering between all segment pairs. These
strategies vary significantly in terms of linguistic
motivation and computational complexity. Exam-
ples of automatically constructed TDAGs derived
from different inference strategies are shown in
Figure 2.
5.2.1 Greedy Inference in Natural Reading
Order (NRO)
The simplest way to construct a consistent
TDAG is by adding segments in the order of their
appearance in a text. Intuitively speaking, this
technique processes segments in the same order
as a reader of the text. The motivation underly-
ing this approach is that the reader incrementally
builds temporal interpretation of a text; when a
new piece of information is introduced, the reader
knows how to relate it to already processed text.
This technique starts with an empty graph and
incrementally adds nodes in order of their appear-
ance in the text. When a new node is added, we
greedily select the edge with the highest score that
connects the new node to the existing graph, with-
out violating the consistency of the TDAG. Next,
we expand the graph with its transitive closure.
We continue greedily adding edges and applying
transitive closure until the new node is connected
to all other nodes already in the TDAG. The pro-
cess continues until all the nodes have been added
to the graph.
5.2.2 Greedy Best-first Inference (BF)
Our second inference strategy is also greedy. It
aims to optimize the score of the graph. The score
of the graph is computed by summing the scores of
text (Wilson et al, 2001).
its edges. While this greedy strategy is not guar-
anteed to find the optimal solution, it finds a rea-
sonable approximation (Cohen et al, 1999).
This method begins by sorting the edges by their
score. Starting with an empty graph, we add one
edge at a time, without violating the consistency
constraints. As in the previous strategy, at each
step we expand the graph with its transitive clo-
sure. We continue this process until all the edges
have been considered.
5.2.3 Exact Inference with Integer Linear
Programming (ILP)
We can cast the task of constructing a globally
optimal TDAG as an optimization problem. In
contrast to the previous approaches, the method
is not greedy. It computes the optimal solu-
tion within the Integer Linear Programming (ILP)
framework.
For a document with N segments, each pair of
segments (i, j) can be related in the graph in one
of three ways: forward, backward, and null (not
connected). Let si?j , si?j , and si=j be the scores
assigned by a local classifier to each of the three
relations respectively. Let Ii?j , Ii?j , and Ii=j
be indicator variables that are set to 1 if the corre-
sponding relation is active, or 0 otherwise.
The objective is then to optimize the score of a
TDAG by maximizing the sum of the scores of all
edges in the graph:
max
N
X
i=1
N
X
j=i+i
si?jIi?j + si?jIi?j + si=jIi=j (1)
subject to:
Ii?j , Ii?j , Ii=j ? {0, 1} ? i, j = 1, . . . N, i < j (2)
Ii?j + Ii?j + Ii=j = 1 ? i, j = 1, . . . N, i < j (3)
We augment this basic formulation with two more
sets of constraints to enforce validity of the con-
structed TDAG.
Transitivity Constraints The key requirement
on the edge assignment is the transitivity of the
resulting graph. Transitivity also guarantees that
the graph does not have cycles. We enforce tran-
sitivity by introducing the following constraint for
every triple (i, j, k):
Ii?j + Ij?k ? 1 ? Ii?k (4)
If both indicator variables on the left side of the
inequality are set to 1, then the indicator variable
193
on the right side must be equal to 1. Otherwise, the
indicator variable on the right can take any value.
Connectivity Constraints The connectivity
constraint states that each node i is connected to
at least one other node and thereby enforces con-
nectivity of the generated TDAG. We introduce
these constraints because manually-constructed
TDAGs do not have any disconnected nodes. This
observation is consistent with the intuition that the
reader is capable to order a segment with respect
to other segments in the TDAG.
(
i?1
?
j=1
Ii=j +
N
?
j=i+1
Ij=i) < N ? 1 (5)
The above constraint rules out edge assignments
in which node i has null edges to the rest of the
nodes.
Solving ILP Solving an integer linear program
is NP-hard (Cormen et al, 1992). Fortunately,
there exist several strategies for solving ILPs. We
employ an efficient Mixed Integer Programming
solver lp solve5 which implements the Branch-
and-Bound algorithm. It takes less than five sec-
onds to decode each document on a 2.8 GHz Intel
Xeon machine.
6 Evaluation Set-Up
We first describe the corpora used in our experi-
ments and the results of human agreement on the
segmentation and the ordering tasks. Then, we in-
troduce the evaluation measures that we use to as-
sess the performance of our model.
6.1 Corpus Characteristics
We applied our method for temporal ordering to
a corpus of medical case summaries. The medical
domain has been a popular testbed for methods for
automatic temporal analyzers (Combi and Shahar,
1997; Zhou et al, 2005). The appeal is partly due
to rich temporal structure of these documents and
the practical need to parse this structure for mean-
ingful processing of medical data.
We compiled a corpus of medical case sum-
maries from the online edition of The New Eng-
land Journal of Medicine.6 The summaries are
written by physicians of Massachusetts General
5
http://groups.yahoo.com/group/lp_solve
6
http://content.nejm.org
Hospital. A typical summary describes an admis-
sion status, previous diseases related to the cur-
rent conditions and their treatments, family his-
tory, and the current course of treatment. For
privacy protection, names and dates are removed
from the summaries before publication.
The average length of a summary is 47 sen-
tences. The summaries are written in the past
tense, and a typical summary does not include in-
stances of the past perfect. The summaries do
not follow a chronological order. The ordering of
information in this domain is guided by stylistic
conventions (i.e., symptoms are presented before
treatment) and the relevance of information to the
current conditions (i.e., previous onset of the same
disease is summarized before the description of
other diseases).
6.2 Annotating Temporal Segmentation
Our approach for temporal segmentation requires
annotated data for supervised training. We first
conducted a pilot study to assess the human agree-
ment on the task. We employed two annotators to
manually segment a portion of our corpus. The an-
notators were provided with two-page instructions
that defined the notion of a temporal segment and
included examples of segmented texts. Each an-
notator segmented eight summaries which on av-
erage contained 49 sentences. Because annotators
were instructed to consider segmentation bound-
aries at the level of a clause, there were 877 po-
tential boundaries. The first annotator created 168
boundaries, while the second ? 224 boundaries.
We computed a Kappa coefficient of 0.71 indicat-
ing a high inter-annotator agreement and thereby
confirming our hypothesis about the reliability of
temporal segmentation.
Once we established high inter-annotator agree-
ment on the pilot study, one annotator seg-
mented the remaining 52 documents in the cor-
pus.7 Among 3,297 potential boundaries, 1,178
(35.7%) were identified by the annotator as seg-
ment boundaries. The average segment length is
three sentences, and a typical document contains
around 20 segments.
6.3 Annotating Temporal Ordering
To assess the inter-annotator agreement, we asked
two human annotators to construct TDAGs from
7It took approximately 20 minutes to segment a case sum-
mary.
194
five manually segmented summaries. These sum-
maries consist of 97 segments, and their transi-
tive closure contain a total of 1,331 edges. We
computed the agreement between human judges
by comparing the transitive closure of the TDAGs.
The annotators achieved a surprisingly high agree-
ment with a Kappa value of 0.98.
After verifying human agreement on this task,
one of the annotators constructed TDAGs for an-
other 25 summaries.8 The transitive reduction of
a graph contains on average 20.9 nodes and 20.5
edges. The corpus consists of 72% forward, 12%
backward and 16% null segment edges inclusive
of edges induced by transitive closure. At the
clause level, the distribution is even more skewed
? forward edges account for 74% edges, equal for
18%, backward for 3% and null for 5%.
6.4 Evaluation Measures
We evaluate temporal segmentation by consider-
ing the ratio of correctly predicted boundaries.
We quantify the performance using F-measure, a
commonly used binary classification metric. We
opt not to use the Pk measure, a standard topical
segmentation measure, because the temporal seg-
ments are short and we are only interested in the
identification of the exact boundaries.
Our second evaluation task is concerned with
ordering manually annotated segments. In these
experiments, we compare an automatically gener-
ated TDAG against the annotated reference graph.
In essence, we compare edge assignment in the
transitive closure of two TDAGs, where each edge
can be classified into one of the three types: for-
ward, backward, or null.
Our final evaluation is performed at the clausal
level. In this case, each edge can be classified into
one of the four classes: forward, backward, equal,
or null. Note that the clause-level analysis allows
us to compare TDAGs based on the automatically
derived segmentation.
7 Results
We evaluate temporal segmentation using leave-
one-out cross-validation on our corpus of 60 sum-
maries. The segmentation algorithm achieves a
performance of 83% F-measure, with a recall of
78% and a precision of 89%.
8It took approximately one hour to build a TDAG for each
segmented document.
To evaluate segment ordering, we employ leave-
one-out cross-validation on 30 annotated TDAGs
that overall contain 13,088 edges in their transi-
tive closure. In addition to the three global in-
ference algorithms, we include a majority base-
line that classifies all edges as forward, yielding
a chronological order.
Our results for ordering the manually annotated
temporal segments are shown in Table 1. All infer-
ence methods outperform the baseline, and their
performance is consistent with the complexity of
the inference mechanism. As expected, the ILP
strategy, which supports exact global inference,
achieves the best performance ? 84.3%.
An additional point of comparison is the accu-
racy of the pairwise classification, prior to the ap-
plication of global inference. The accuracy of the
local ordering is 81.6%, which is lower than that
of ILP. The superior performance of ILP demon-
strates that accurate global inference can further
refine local predictions. Surprisingly, the local
classifier yields a higher accuracy than the two
other inference strategies. Note, however, the local
ordering procedure is not guaranteed to produce a
consistent TDAG, and thus the local classifier can-
not be used on its own to produce a valid TDAG.
Table 2 shows the ordering results at the clausal
level. The four-way classification is computed
using both manually and automatically generated
segments. Pairs of clauses that belong to the same
segment stand in the equal relation, otherwise they
have the same ordering relation as the segments to
which they belong.
On the clausal level, the difference between the
performance of ILP and BF is blurred. When eval-
uated on manually-constructed segments, ILP out-
performs BF by less than 1%. This unexpected re-
sult can be explained by the skewed distribution of
edge types ? the two hardest edge types to clas-
sify (see Table 3), backward and null, account only
for 7.4% of all edges at the clause level.
When evaluated on automatically segmented
text, ILP performs slightly worse than BF. We hy-
pothesize that this result can be explained by the
difference between training and testing conditions
for the pairwise classifier: the classifier is trained
on manually-computed segments and is tested on
automatically-computed ones, which negatively
affects the accuracy on the test set. While all
the strategies are negatively influenced by this dis-
crepancy, ILP is particularly vulnerable as it relies
195
Algorithm Accuracy
Integer Linear Programming (ILP) 84.3
Best First (BF) 78.3
Natural Reading Order (NRO) 74.3
Baseline 72.2
Table 1: Accuracy for 3-way ordering classifica-
tion over manually-constructed segments.
Algorithm Manual Seg. Automatic Seg.
ILP 91.9 84.8
BF 91.0 85.0
NRO 87.8 81.0
Baseline 73.6 73.6
Table 2: Results for 4-way ordering classification
over clauses, computed over manually and auto-
matically generated segments.
on the score values for inference. In contrast, BF
only considers the rank between the scores, which
may be less affected by noise.
We advocate a two-stage approach for temporal
analysis: we first identify segments and then order
them. A simpler alternative is to directly perform
a four-way classification at the clausal level using
the union of features employed in our two-stage
process. The accuracy of this approach, however,
is low ? it achieves only 74%, most likely due
to the sparsity of clause-level representation for
four-way classification. This result demonstrates
the benefits of a coarse representation and a two-
stage approach for temporal analysis.
8 Conclusions
This paper introduces a new method for temporal
ordering. The unit of our analysis is a temporal
segment, a fragment of text that maintains tem-
poral coherence. After investigating several infer-
ence strategies, we concluded that integer linear
programming and best first greedy approach are
valuable alternatives for TDAG construction.
In the future, we will explore a richer set of con-
straints on the topology on the ordering graph. We
will build on the existing formal framework (Fikes
et al, 2003) for the verification of ordering con-
sistency. We are also interested in expanding our
framework for global inference to other temporal
annotation schemes. Given a richer set of temporal
relations, the benefits from global inference can be
even more significant.
Algorithm Forward Backward Null
ILP 92.5 45.6 76.0
BF 91.4 42.2 74.7
NRO 87.7 43.6 66.4
Table 3: Per class accuracy for clause classifica-
tion over manually computed segments.
Acknowledgments
The authors acknowledge the support of the Na-
tional Science Foundation and National Institute
of Health (CAREER grant IIS-0448168, grant IIS-
0415865). Thanks to Terry Koo, Igor Malioutov,
Zvika Marx, Benjamin Snyder, Peter Szolovits,
Luke Zettlemoyer and the anonymous reviewers
for their helpful comments and suggestions. Any
opinions, findings, conclusions or recommenda-
tions expressed above are those of the authors and
do not necessarily reflect the views of the NSF or
NIH.
References
James F. Allen. 1984. Towards a general theory of
action and time. Artificial Intelligence, 23(2):123?
154.
Regina Barzilay, Noemie Elhadad, and Kathleen McK-
eown. 2002. Inferring strategies for sentence order-
ing in multidocument news summarization. Journal
of Artificial Intelligence Research, 17:35?55.
Yves Bestgen and Wietske Vonk. 1995. The role
of temporal segmentation markers in discourse pro-
cessing. Discourse Processes, 19:385?406.
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal reason-
ing. In Proceedings of IJCAI, pages 997?1003.
Wallace Chafe. 1979. The flow of thought and the
flow of language. In Talmy Givon, editor, Syntax
and Semantics: Discourse and Syntax, volume 12,
pages 159?182. Academic Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the NAACL, pages
132?139.
William Cohen, Robert Schapire, and Yoram Singer.
1999. Learning to order things. Journal of Artificial
Intelligence, 10:243?270.
Carlo Combi and Yuval Shahar. 1997. Temporal rea-
soning and temporal data maintenance in medicine:
Issues and challenges. Computers in Biology and
Medicine, 27(5):353?368.
196
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1992. Intoduction to Algorithms.
The MIT Press.
David R. Dowty. 1986. The effects of aspectual class
on the temporal structure of discourse: Semantics or
Pragmatics? Linguistics and Philosophy, 9:37?61.
R. Fikes, J. Jenkins, and G. Frank. 2003. A system
architecture and component library for hybrid rea-
soning. Technical report, Stanford University.
Marti Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of the ACL, pages
9?16.
Chung Hee Hwang and Lenhart K. Schubert. 1992.
Tense trees as the ?fine structure? of discourse. In
Proceedings of the ACL, pages 232?240.
Mirella Lapata and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In Proceedings
of HLT-NAACL, pages 153?160.
Alex Lascarides and Nicholas Asher. 1993. Tem-
poral interpretation, discourse relations, and com-
monsense entailment. Linguistics and Philosophy,
16:437?493.
Alex Lascarides and John Oberlander. 1993. Temporal
connectives in a discourse context. In Proceeding of
the EACL, pages 260?268.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in news.
In Proceeding of HLT-NAACL, pages 55?57.
Mark Moens and Mark J. Steedman. 1987. Temporal
ontology in natural language. In Proceedings of the
ACL, pages 1?7.
Rebecca J. Passonneau. 1988. A computational model
of the semantics of tense and aspect. Computational
Linguistics, 14(2):44?60.
James Pustejovsky, Patrick Hanks, Roser Sauri,
Andrew See, David Day, Lissa Ferro, Robert
Gaizauskas, Marcia Lazo, Andrea Setzer, and Beth
Sundheim. 2003. The timebank corpus. Corpus
Linguistics, pages 647?656.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York, NY.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Bonnie L. Webber. 1987. The interpretation of tense
in discourse. In Proceedings of the ACL, pages 147?
154.
Bonnie L. Webber. 1988. Tense as discourse anaphor.
Computational Linguistics, 14(2):61?73.
George Wilson, Inderjeet Mani, Beth Sundheim, and
Lisa Ferro. 2001. A multilingual approach to anno-
tating and extracting temporal information. In Pro-
ceedings of the ACL 2001 Workshop on Temporal
and Spatial Information Processing, pages 81?87.
Li Zhou, Carol Friedman, Simon Parsons, and George
Hripcsak. 2005. System architecture for temporal
information extraction, representation and reason-
ing in clinical narrative reports. In Proceedings of
AMIA, pages 869?873.
197
   
 
 
   
	  
 
 	
 
		
 
	
 
 
	  
	
(a) Reference TDAG
 


 
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 773?782,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
 
 
Extracting Social Power Relationships from Natural Language 
 
Philip Bramsen 
Louisville, KY 
bramsen@alum.mit.edu* 
 
Ami Patel 
Massachusetts Institute of Technology 
Cambridge, MA 
ampatel@mit.edu* 
 
Martha Escobar-Molano 
San Diego, CA 
mescobar@asgard.com* 
 
Rafael Alonso 
SET Corporation 
Arlington, VA 
ralonso@setcorp.com 
 
 
 
 
 
Abstract 
Sociolinguists have long argued that social 
context influences language use in all manner 
of ways, resulting in lects 1 . This paper ex-
plores a text classification problem we will 
call lect modeling, an example of what has 
been termed computational sociolinguistics. In 
particular, we use machine learning techniques 
to identify social power relationships between 
members of a social network, based purely on 
the content of their interpersonal communica-
tion. We rely on statistical methods, as op-
posed to language-specific engineering, to 
extract features which represent vocabulary 
and grammar usage indicative of social power 
lect. We then apply support vector machines to 
model the social power lects representing su-
perior-subordinate communication in the En-
ron email corpus. Our results validate the 
treatment of lect modeling as a text classifica-
tion problem ? albeit a hard one ? and consti-
tute a case for future research in computational 
sociolinguistics. 
1 Introduction 
Linguists in sociolinguistics, pragmatics and re-
lated fields have analyzed the influence of social 
context on language and have catalogued countless 
phenomena that are influenced by it, confirming 
many with qualitative and quantitative studies. In-
                                                           
* This work was done while these authors were at SET Corpo-
ration, an SAIC Company. 
1 Fields that deal with society and language have inconsistent 
terminology; ?lect? is chosen here because ?lect? has no other 
English definitions and the etymology of the word gives it the 
sense we consider most relevant. 
deed, social context and function influence lan-
guage at every level ? morphologically, lexically, 
syntactically, and semantically, through discourse 
structure, and through higher-level abstractions 
such as pragmatics.  
Considered together, the extent to which speak-
ers modify their language for a social context 
amounts to an identifiable variation on language, 
which we call a lect. Lect is a backformation from 
words such as dialect (geographically defined lan-
guage) and ethnolect (language defined by ethnic 
context). 
In this paper, we describe lect classifiers for so-
cial power relationships. We refer to these lects as: 
 
? UpSpeak: Communication directed to 
someone with greater social authority.  
? DownSpeak: Communication directed to 
someone with less social authority.  
? PeerSpeak: Communication to someone of 
equal social authority.  
 
We call the problem of modeling these lects Social 
Power Modeling (SPM). The experiments reported 
in this paper focused primarily on modeling Up-
Speak and DownSpeak.  
Manually constructing tools that effectively 
model specific linguistic phenomena suggested by 
sociolinguistics would be a Herculean effort. 
Moreover, it would be necessary to repeat the ef-
fort in every language! Our approach first identi-
fies statistically salient phrases of words and parts 
of speech ? known as n-grams ? in training texts 
generated in conditions where the social power 
773
  
relationship is known. Then, we apply machine 
learning to train classifiers with groups of these n-
grams as features. The classifiers assign the Up-
Speak and DownSpeak labels to unseen text. This 
methodology is a cost-effective approach to model-
ing social information and requires no language- or 
culture-specific feature engineering, although we 
believe sociolinguistics-inspired features hold 
promise. 
When applied to the corpus of emails sent and 
received by Enron employees (CALO Project 
2009), this approach produced solid results, despite 
a limited number of training and test instances. 
This has many implications. Since manually de-
termining the power structure of social networks is 
a time-consuming process, even for an expert, ef-
fective SPM could support data driven socio-
cultural research and greatly aid analysts doing 
national intelligence work. Social network analysis 
(SNA) presupposes a collection of individuals, 
whereas a social power lect classifier, once trained, 
would provide useful information about individual 
author-recipient links. On networks where SNA 
already has traction, SPM could provide comple-
mentary information based on the content of com-
munications.  
If SPM were yoked with sentiment analysis, we 
might identify which opinions belong to respected 
members of online communities or lay the 
groundwork for understanding how respect is 
earned in social networks. 
More broadly, computational sociolinguistics is 
a nascent field with significant potential to aid in 
modeling and understanding human relationships. 
The results in this paper suggest that successes to 
date modeling authorship, sentiment, emotion, and 
personality extend to social power modeling, and 
our approach may well be applicable to other di-
mensions of social meaning. 
In the coming sections, we first establish the 
Related Work, primarily from Statistical NLP. 
We then cover our Approach, the Evaluation, 
and, finally, the Conclusions and Future Re-
search. 
2 Related Work 
The feasibility of Social Power Modeling is sup-
ported by sociolinguistic research identifying spe-
cific ways in which a person?s language reflects his 
relative power over others. Fairclough's classic 
work Language and Power explores how 
"sociolinguistic conventions . . . arise out of -- and 
give rise to ? particular relations of power" (Fair-
clough, 1989). Brown and Levinson created a the-
ory of politeness, articulating a set of strategies 
which people employ to demonstrate different lev-
els of politeness (Brown & Levinson, 1987). Mo-
rand drew upon this theory in his analysis of 
emails sent within a corporate hierarchy; in it, he 
quantitatively showed that emails from subordi-
nates to superiors are, in fact, perceived as more 
polite, and that this perceived politeness is corre-
lated with specific linguistic tactics, including ones 
set out by Brown and Levinson (Morand, 2000). 
Similarly, Erikson et alidentified measurable char-
acteristics of the speech of witnesses in a court-
room setting which were directly associated with 
the witness?s level of social power (Erikson, 1978). 
Given, then, that there are distinct differences 
among what we term UpSpeak and DownSpeak, 
we treat Social Power Modeling as an instance of 
text classification (or categorization): we seek to 
assign a class (UpSpeak or DownSpeak) to a text 
sample. Closely related natural language process-
ing problems are authorship attribution, sentiment 
analysis, emotion detection, and personality classi-
fication: all aim to extract higher-level information 
from language.  
Authorship attribution in computational linguis-
tics is the task of identifying the author of a text. 
The earliest modern authorship attribution work 
was (Mosteller & Wallace, 1964), although foren-
sic authorship analysis has been around much 
longer. Mosteller and Wallace used statistical lan-
guage-modeling techniques to measure the similar-
ity of disputed Federalist Papers to samples of 
known authorship. Since then, authorship identifi-
cation has become a mature area productively ex-
ploring a broad spectrum of features (stylistic, 
lexical, syntactic, and semantic) and many genera-
tive and discriminative modeling approaches (Sta-
matatos, 2009). The generative models of 
authorship identification motivated our statistically 
extracted lexical and grammatical features, and 
future work should consider these language model-
ing (a.k.a. compression) approaches.  
Sentiment analysis, which strives to determine 
the attitude of an author from text, has recently 
garnered much attention (e.g. Pang, Lee, & Vai-
thyanathan, 2002; Kim & Hovy, 2004; Breck, Choi 
774
  
& Cardie, 2007). For example, one problem is 
classifying user reviews as positive, negative or 
neutral. Typically, polarity lexicons (each term is 
labeled as positive, negative or neutral) help de-
termine attitudes in text (Hiroya & Takamura, 
2005, Ravichandran 2009, Choi & Cardie 2009).  
The polarity of an expression can be determined 
based on the polarity of its component lexical 
items (Choi & Cardie 2008). For example, the po-
larity of the expression is determined by the major-
ity polarity of its lexical items or by rules applied 
to syntactic patterns of expressions on how to de-
termine the polarity from its lexical components. 
McDonald et alstudied models that classify senti-
ment on multiple levels of granularity: sentence 
and document-level (McDonald, 2007). Their work 
jointly classifies sentiment at both levels instead of 
using independent classifiers for each level or cas-
caded classifiers. Similar to our techniques, these 
studies determine the polarity of text based on its 
component lexical and grammatical sequences. 
Unlike their works, our text classification tech-
niques take into account the frequency of occur-
rence of word n-grams and part-of-speech (POS) 
tag sequences, and other measures of statistical 
salience in training data. 
Text-based emotion prediction is another in-
stance of text classification, where the goal is to 
detect the emotion appropriate to a text (Alm, Roth 
& Sproat, 2005) or provoked by an author, for ex-
ample (Strapparava & Mihalcea, 2008). Alm, Roth, 
and Sproat explored a broad array of lexical and 
syntactic features, reminiscent of those of author-
ship attribution, as well as features related to story 
structure. A Winnow-based learning algorithm 
trained on these features convincingly predicted an 
appropriate emotion for individual sentences of 
narrative text. Strapparava and Mihalcea try to 
predict the emotion the author of a headline intends 
to provoke by leveraging words with known affec-
tive sense and by expanding those words? syno-
nyms. They used a Na?ve Bayes classifier trained 
on short blogposts of known emotive sense. The 
knowledge engineering approaches were generally 
superior to the Na?ve Bayes approach. Our ap-
proach is corpus-driven like the Na?ve Bayes ap-
proach, but we interject statistically driven feature 
selection between the corpus and the machine 
learning classifiers. 
In personality classification, a person?s lan-
guage is used to classify him on different personal-
ity dimensions, such as extraversion or neuroticism 
(Oberlander & Nowson, 2006; Mairesse & Walker; 
2006). The goal is to recover the more permanent 
traits of a person, rather than fleeting characteris-
tics such as sentiment or emotion. Oberlander and 
Nowson explore using a Na?ve Bayes and an SVM 
classifier to perform binary classification of text on 
each personality dimension. For example, one clas-
sifier might determine if a person displays a high 
or low level of extraversion. Their attempt to clas-
sify each personality trait as either ?high? or ?low? 
echoes early sentiment analysis work that reduced 
sentiments to either positive or negative (Pang, 
Lee, & Vaithyanathan, 2002), and supports ini-
tially treating Social Power Modeling as a binary 
classification task. Personality classification seems 
to be the application of text classification which is 
the most relevant to Social Power Modeling. As 
Mairesse and Walker note, certain personality 
traits are indicative of leaders. Thus, the ability to 
model personality suggests an ability to model so-
cial power lects as well.  
Apart from text classification, work from the 
topic modeling community is also closely related 
to Social Power Modeling. Andrew McCallum ex-
tended Latent Dirichlet Allocation to model the 
author and recipient dependencies of per-message 
topic distributions with an Author-Recipient-Topic 
(ART) model (McCallum, Wang, & Corrada-
Emmanuel, 2007). This was the first significant 
work to model the content and relationships of 
communication in a social network. McCallum et 
al applied ART to the Enron email corpus to show 
that the resulting topics are strongly tied to role. 
They suggest that clustering these topic distribu-
tions would yield roles and argue that the person-
to-person similarity matrix yielded by this ap-
proach has advantages over those of canonical so-
cial network analysis. The same authors proposed 
several Role-Author-Recipient-Topic (RART) 
models to model authors, roles and words simulta-
neously. With a RART modeling roles-per-word, 
they produced per-author distributions of generated 
roles that appeared reasonable (e.g. they labeled 
Role 10 as ?grant issues? and Role 2 as ?natural 
language researcher?). 
We have a similar emphasis on statistically 
modeling language and interpersonal communica-
775
  
tion. However, we model social power relation-
ships, not roles or topics, and our approach pro-
duces discriminative classifiers, not generative 
models, which enables more concrete evaluation.  
Namata, Getoor, and Diehl effectively applied 
role modeling to the Enron email corpus, allowing 
them to infer the social hierarchy structure of En-
ron (Namata et al, 2006). They applied machine 
learning classifiers to map individuals to their roles 
in the hierarchy based on features related to email 
traffic patterns. They also attempt to identify cases 
of manager-subordinate relationships within the 
email domain by ranking emails using traffic-based 
and content-based features (Diehl et al, 2007). 
While their task is similar to ours, our goal is to 
classify any case in which one person has more 
social power than the other, not just identify in-
stances of direct reporting. 
3 Approach 
3.1 Feature Set-Up 
Previous work in traditional text classification and 
its variants ? such as sentiment analysis ? has 
achieved successful results by using the bag-of-
words representation; that is, by treating text as a 
collection of words with no interdependencies, 
training a classifier on a large feature set of word 
unigrams which appear in the corpus. However, 
our hypothesis was that this approach would not be 
the best for SPM. Morand?s study, for instance, 
identified specific features that correlate with the 
direction of communication within a social hierar-
chy (Morand, 2000). Few of these tactics would be 
effectively encapsulated by word unigrams. Many 
would be better modeled by POS tag unigrams 
(with no word information) or by longer n-grams 
consisting of either words, POS tags, or a combina-
tion of the two. ?Uses subjunctive? and ?Uses past 
tense? are examples. Because considering such 
features would increase the size of the feature 
space, we suspected that including these features 
would also benefit from algorithmic means of se-
lecting n-grams that are indicative of particular 
lects, and even from binning these relevant n-
grams into sets to be used as features. 
Therefore, we focused on an approach where 
each feature is associated with a set of one or more 
n-grams. Each n-gram is a sequence of words, POS 
tags or a combination of words and POS tags 
(?mixed? n-grams). Let S represent a set {n1, ?, 
nk} of n-grams. The feature associated with S on 
text T would be: 
 
1
( , ) ( , )
k
i
i
f S T freq n T
=
=?  
   
where ( , )ifreq n T is the relative frequency (de-
fined later) of in  in text T. Let in  represent the 
sequence 1 ms s? where js  specifies either a word 
or a POS tag. Let T represent the text consisting of 
the sequence of tagged-word tokens 1 lt t? . 
( , )ifreq n T is then defined as follows: 
 
1( , ) ( , )i mfreq n T freq s s T= ?  
{ }1 1: ( )
1
b b m p m b p pt t t s
l m
+ + ? ? +? == ? +
?
 
 
where: 
 
( )
( )
i j j
i j
i j j
word t s if s is a word
t s
tag t s if s is a tag
=??= ? ? =??
 
 
To illustrate, consider the following feature set, a 
bigram and a trigram (each term in the n-gram ei-
ther has the form word or ^tag):  
 
{please ^VB, please ^?comma? ^VB}2  
 
The tag ?VB? denotes a verb. Suppose T consists 
of the following tokenized and tagged text (sen-
tence initial and final tokens are not shown):  
 
please^RB bring^VB the^DET report^NN 
to^TO our^PRP$ next^JJ weekly^JJ meet-
ing^NN .^.  
 
The first n-gram of the set, please ^VB, would 
match please^RB bring^VB from the text. The fre-
quency of this n-gram in T would then be 1/9, 
where 1 is the number of substrings in T that match 
                                                           
2 To distinguish a comma separating elements of a set with a 
comma as part of an ngram, we use ?comma? to denote the 
punctuation mark ?,? as part of the ngram.  
776
  
please ^VB and 9 is the number of bigrams in T, 
excluding sentence initial and final markers. The 
other n-gram, the trigram please ^?comma? ^VB, 
does not have any match, so the final value of the 
feature is 1/9. 
Defining features in this manner allows us to 
both explore the bag-of-words representation as 
well as use groups of n-grams as features, which 
we believed would be a better fit for this problem. 
3.2 N-Gram Selection 
To identify n-grams which would be useful fea-
tures, frequencies of n-grams in only the training 
set are considered. Different types of frequency 
measures were explored to capture different types 
of information about an n-gram?s usage. These are: 
 
? Absolute frequency: The total number of 
times a particular n-gram occurs in the text 
of a given class (social power lect).  
? Relative frequency: The total number of 
times a particular n-gram occurs in a given 
class, divided by the total number of n-
grams in that class. Normalization by the 
size of the class makes relative frequency a 
better metric for comparing n-gram usage 
across classes.  
  
We then used the following frequency-based met-
rics to select n-grams: 
 
? We set a minimum threshold for the abso-
lute frequency of the n-gram in a class. 
This helps weed out extremely infrequent 
words and spelling errors.  
? We require that the ratio of the relative 
frequency of the n-gram in one class to its 
relative frequency in the other class is also 
greater than a threshold. This is a simple 
means of selecting n-grams indicative of 
lect. 
 
In experiments based on the bag-of-words model, 
we only consider an absolute frequency threshold, 
whereas in later experiments, we also take into ac-
count the relative frequency ratio threshold.  
3.3 N-gram Binning  
In experiments in which we bin n-grams, selected 
n-grams are assigned to the class in which their 
relative frequency is highest. For example, an n-
gram whose relative frequency in UpSpeak text is 
twice that in DownSpeak text would be assigned to 
the class UpSpeak. 
N-grams assigned to a class are then partitioned 
into sets of n-grams. Each of these sets of n-grams 
is associated with a feature. This partition is based 
on the n-gram type, the length of n-grams and the 
relative frequency ratio of the n-grams. While the 
n-grams composing a set may themselves be in-
dicative of social power lects, this method of 
grouping them makes no guarantees as to how in-
dicative the overall set is. Therefore, we experi-
mented with filtering out sets which had a 
negligible information gain. Information gain is an 
information theoretic concept measuring how 
much the probability distributions for a feature dif-
fer among the different classes. A small informa-
tion gain suggests that a feature may not be 
effective at discriminating between classes. 
Although this approach to partitioning is simple 
and worthy of improvement, it effectively reduced 
the dimensionality of the feature space. 
3.4 Classification 
Once features are selected, a classifier is trained on 
these features. Many features are weak on their 
own; they either occur rarely or occur frequently 
but only hint weakly at social information. There-
fore, we experimented with classifiers friendly to 
weak features, such as Adaboost and Logistic Re-
gression (MaxEnt). However, we generally 
achieved the best results using support vector ma-
chines, a machine learning classifier which has 
been successfully applied to many previous text 
classification problems. We used Weka?s opti-
mized SVMs (SMO) (Witten 2005, Platt 1998) and 
default parameters, except where noted. 
4 Evaluation 
4.1 Data 
To validate our supervised learning approach, we 
sought an adequately large English corpus of per-
son-to-person communication labeled with the 
ground truth. For this, we used the publicly avail-
777
  
able Enron corpus. After filtering for duplicates 
and removing empty or otherwise unusable emails, 
the total number of emails is 245K, containing 
roughly 90 million words. However, this total in-
cludes emails to non-Enron employees, such as 
family members and employees of other corpora-
tions, emails to multiple people, and emails re-
ceived from Enron employees without a known 
corporate role. Because the author-recipient rela-
tionships of these emails could not be established, 
they were not included in our experiments. 
Building upon previous annotation done on the 
corpus, we were able to ascertain the corporate role 
(CEO, Manager, Employee, etc.) of many email 
authors and recipients. From this information, we 
determined the author-recipient relationship by 
applying general rules about the structure of a cor-
porate hierarchy (an email from an Employee to a 
CEO, for instance, is UpSpeak). This annotation 
method does not take into account promotions over 
time, secretaries speaking on behalf of their super-
visors, or other causes of relationship irregularities. 
However, this misinformation would, if anything, 
generally hurt our classifiers.   
The emails were pre-processed to eliminate text 
not written by the author, such as forwarded text 
and email headers. As our approach requires text to 
be POS-tagged, we employed Stanford?s POS tag-
ger (http://nlp.stanford.edu/software/tagger.shtml). 
In addition, text was regularized by conversion to 
lower case and tokenized to improve counts. 
To create training and test sets, we partitioned 
the authors of text from the corpus into two sets: A 
and B. Then, we used text authored by individuals 
in A as a training set and text authored by indi-
viduals in B as a test set. The training set is used to 
determine discriminating features upon which clas-
sifiers are built and applied to the test set. We  
 
Table 1. Author-based Training and Test partitions. The 
number of author-recipient pairs (links) and the number 
of words in text labeled as UpSpeak and DownSpeak 
are shown. 
 
found that partitioning by authors was necessary to 
avoid artificially inflated scores, because the clas-
sifiers pick up aspects of particular authors? lan-
guage (idiolect) in addition to social power lect 
information. It was not necessary to account for 
recipients because the emails did not contain text 
from the recipients. Table 1 summarizes the text 
partitions. 
Because preliminary experiments suggested that 
smaller text samples were harder to classify, the 
classifiers we describe in this paper were both 
trained and tested on a subset of the Enron corpus 
where at least 500 words of text was communi-
cated from a specific author to a specific recipient. 
This subset contained 142 links, 40% of which 
were used as the test set. 
Weighting for Cost-Sensitive Learning: The 
original corpus was not balanced: the number of 
UpSpeak links was greater than the number of 
DownSpeak links. Varying the weight given to 
training instances is a technique for creating a clas-
sifier that is cost-sensitive, since a classifier built 
on an unbalanced training set can be biased to-
wards avoiding errors on the overrepresented class 
(Witten, 2005). We wanted misclassifying Up-
Speak as DownSpeak to have the same cost as mis-
classifying DownSpeak as UpSpeak. To do this, 
we assigned weights to each instance in the train-
ing set. UpSpeak instances were weighted less than 
DownSpeak instances, creating a training set that 
was balanced between UpSpeak and DownSpeak. 
Balancing the training set generally improved re-
sults.  
Weighting the test set in the same manner al-
lowed us to evaluate the performance of the classi-
fier in a situation in which the numbers of 
UpSpeak and DownSpeak instances were equal. A 
baseline classifier that always predicted the major-
ity class would, on its own, achieve an accuracy of 
74% on UpSpeak/DownSpeak classification of 
unweighted test set instances with a minimum 
length of 500 words. However, results on the 
weighted test set are properly compared to a base-
line of 50%. We include both approaches to scor-
ing in this paper. 
4.2 UpSpeak/DownSpeak Classifiers 
In this section, we describe experiments on classi-
fication of interpersonal email communication into 
UpSpeak and DownSpeak. For these experiments, 
only emails exchanged between two people related 
by a superior/subordinate power relationship were  
 UpSpeak DownSpeak 
 Links Words Links Words 
Training 431 136K 328 63K 
Test 232 74K 148 27K 
778
  
Table 2. Experiment Results. Accuracies/F-Scores with an SVM classifier for 10-fold cross validation on the 
weighted training set and evaluation against the weighted and unweighted test sets. Note that the baseline accu-
racy against the unweighted test set is 74%, but 50% for the weighted test set and cross-validation.   
 
Human-Engineered Features: Before examin-
ing the data itself, we identified some features 
which we thought would be predictive of UpSpeak 
or DownSpeak, and which could be fairly accu-
rately modeled by mixed n-grams. These features 
included the use of different types of imperatives. 
We also thought that the type of greeting or sig-
nature used in the email might be reflective of 
formality, and therefore of UpSpeak and Down-
Speak. For example, subordinates might be more 
likely to use an honorific when addressing a supe-
rior, or to sign an email with ?Thanks.? We pre-
formed some preliminary experiments using these 
features. While the feature set was too small to 
produce notable results, we identified which fea-
tures actually were indicative of lect. One such 
feature was polite imperatives (imperatives pre-
ceded by the word ?please?). The polite imperative 
feature was represented by the n-gram set: 
 
{please ^VB, please ^?comma? ^VB}. 
 
Unigrams and Bigrams: As a different sort of 
baseline, we considered the results of a bag-of-
words based classifier. Features used in these ex-
periments consist of single words which occurred a 
minimum of four times in the relevant lects (Up-
Speak and DownSpeak) of the training set. The 
results of the SVM classifier, shown in line (1) of 
Table 2, were fairly poor. We then performed ex-
periments with word bigrams, selecting as features 
those which occurred at least seven times in the 
relevant lects of the training set. This threshold for 
bigram frequency minimized the difference in the 
number of features between the unigram and bi-
gram experiments. While the bigrams on their own 
were less successful than the unigrams, as seen in 
line (2), adding them to the unigram features im-
proved accuracy against the test set, shown in line 
(3).  
As we had speculated that including surface-
level grammar information in the form of tag n-
grams would be beneficial to our problem, we per-
formed experiments using all tag unigrams and all 
tag bigrams occurring in the training set as fea-
tures. The results are shown in line (4) of Table 2. 
The results of these experiments were not particu-
larly strong, likely owing to the increased sparsity 
of the feature vectors. 
Binning: Next, we wished to explore longer n-
grams of words or POS tags and to reduce the 
sparsity of the feature vectors. We therefore ex-
perimented with our method of binning the indi-
vidual n-grams to be used as features. We binned 
features by their relative frequency ratios. In addi-
tion to binning, we also reduced the total number 
of n-grams by setting higher frequency thresholds 
and relative frequency ratio thresholds.  
When selecting n-grams for this experiment, we 
considered only word n-grams and tag n-grams ? 
not mixed n-grams, which are a combination of 
words and tags. These mixed n-grams, while useful 
for specifying human-defined features, largely in-
creased the dimensionality of the feature search 
space and did not provide significant benefit in 
preliminary experiments. For the word sequences, 
Cross-Validation Test Set  
(weighted) 
Test Set  
(unweighted) 
 Features # of  
features 
# of  
n-grams 
Acc (%) F-score Acc (%) F-score Acc (%) F-score 
(1) Word unigrams 3899 3899 55.4 .481 62.1 .567 78.9 .748 
(2) Word bigrams 3740 3740 54.5 .457 56.4 .498 73.7 .693 
(3) Word unigrams + 
word bigrams 
7639 7639 51.8 .398 63.3 .576 80.7 .762 
(4) (3) + tag unigrams 
+ tag bigrams 
9014 9014 51.8 .398 58.8 .515 77.2 .719 
(5) Binned n-grams 8 106 83.0 .830 78.1 .781 77.2 .783 
(6) N-grams from (5), 
separated 
106 106 83.0 .828 60.5 .587 70.2 .698 
(7) (5) + polite  
imperatives 
9 108 83.9 .839 77.1 .771 78.9 .797 
779
  
we set an absolute frequency threshold that de-
pended on class. The frequency of a word n-gram 
in a particular class was required to be 0.18 * 
nrlinks / n, where nrlinks is the number of links in 
each class (431 for UpSpeak and 328 for Down-
Speak), and n is the number of words in the class. 
The relative frequency ratio was required to be at 
least 1.5. The tag sequences were required to meet 
an absolute frequency threshold of 20, but the 
same relative frequency ratio of 1.5.  
Binning the n-grams into features was done 
based on both the length of the n-gram and the rel-
ative frequency ratio. For example, one feature 
might represent the set of all word unigrams which 
have a relative frequency ratio between 1.5 and 
1.6.  
We explored possible feature sets with cross va-
lidation. Before filtering for low information gain, 
we used six word n-gram bins per class (relative 
frequency ratios of 1.5, 1.6 ..., 1.9 and 2.0+), one 
tag n-gram bin for UpSpeak (2.0+), and three tag 
n-gram bins for DownSpeak (2.0+, 5.0+, 10.0+). 
Even with the weighted training set, DownSpeak 
instances were generally harder to identify and 
likely benefited from additional representation. 
Grouping features by length was a simple but arbi-
trary method for reducing dimensionality, yet 
sometimes produced small bins of otherwise good 
features.  Therefore, as we explored the feature 
space, small bins of different n-gram lengths were 
merged. We then employed Weka?s InfoGain fea-
ture selection tool to remove those features with a 
low information gain3, which removed all but eight 
features. The results of this experiment are shown 
in line (5) of Table 2. It far outperforms the bag-of-
words baselines, despite significantly fewer fea-
tures. 
To ascertain which feature reduction method had 
the greatest effect on performance ? binning or 
setting a relative frequency ratio threshold ? we 
performed an experiment in which all the n-grams 
that we used in the previous experiment were their 
own features. Line (6) of Table 2 shows that while 
this approach is an improvement over the basic 
bag-of-words method, grouping features still im-
proves results. 
                                                           
3 In Weka, features (?attributes?) with a sufficiently low in-
formation gain have this value rounded down to ?0?; these are 
the features we removed. 
Our goal was to have successful results using 
only statistically extracted features; however, we 
examined the effect of augmenting this feature set 
with the most indicative of the human-identified 
feature ? polite imperatives. The results, in line (7), 
show a slight improvement in both the cross vali-
dation accuracy, and the accuracy against the un-
weighted test set increases to 78.9%4. However, 
among the weighted test sets, the highest accuracy 
was 78.1%, with the features in line (5). 
We report the scores for cross-validation on the 
training set for these features; however, because 
the features were selected with knowledge of their 
per-class distribution in the training set, these 
cross-validation scores should not be seen as the 
classifier?s true accuracy. 
Self-Training: Besides sparse feature vectors, 
another factor likely to be hurting our classifier 
was the limited amount of training data. We at-
tempted to increase the training set size by per-
forming exploratory experiments with self-
training, an iterative semi-supervised learning me-
thod (Zhu, 2005) with the feature set from (7). On 
the first iteration, we trained the classifier on the 
labeled training set, classified the instances of the 
unlabeled test set, and then added the instances of 
the test set alng with their predicted class to the 
training set to be used for the next iteration. After 
three iterations, the accuracy of the classifier when 
evaluated on the weighted test set improved to 
82%, suggesting that our classifiers would benefit 
from more data. 
Impact of Cost-Sensitive Learning: Without 
cost-sensitive learning, the classifiers were heavily 
biased towards UpSpeak, tending to classify both 
DownSpeak and UpSpeak test instances as Up-
Speak.  With cost-sensitive training, overall per-
formance improved and classifier performance on 
DownSpeak instances improved dramatically.  In 
(5) of Table 2, DownSpeak  classifier accuracy 
even edged out the accuracy for UpSpeak.  We 
expect that on a larger dataset behavior with un-
weighted training and test data would improve. 
5 Conclusions and Future Research 
We presented a corpus-based statistical learning 
approach to modeling social power relationships 
and experimental results for our methods. To our 
                                                           
4 The associated p-value is 6.56E-6. 
780
  
knowledge, this is the first corpus-based approach 
to learning social power lects beyond those in di-
rect reporting relationships. 
Our work strongly suggests that statistically ex-
tracted features are an efficient and effective ap-
proach to modeling social information. Our 
methods exploit many aspects of language use and 
effectively model social power information while 
using statistical methods at every stage to tease out 
the information we seek, significantly reducing 
language-, culture-, and lect-specific engineering 
needs. Our feature selection method picks up on 
indicators suggested by sociolinguistics, and it also 
allows for the identification of features that are not 
obviously characteristic of UpSpeak or Down-
Speak. Some easily recognizable features include: 
 
Lect Ngram Example 
UpSpeak if you ?Let me know if you need any-
thing.? 
  ?Please call me if you have any 
questions.? 
Down-
Speak 
give me ?Read this over and give me a 
call.? 
  ?Please give me your comments 
next week.? 
 
On the other hand, other features are less intuitive: 
 
Lect Ngram Example 
UpSpeak I?ll, we?ll ?I?ll let you know the final re-
sults soon? 
  ?Everyone is very excited [?] 
and we?re confident we?ll be 
successful? 
DownSpeak that is, 
this is 
?Neither does any other group 
but that is not my problem? 
  ?I think this is an excellent let-
ter? 
We hope to improve our methods for selecting 
and binning features with information theoretic 
selection metrics and clustering algorithms. 
We also have begun work on 3-way, UpSpeak/ 
DownSpeak/PeerSpeak classification. Training a 
multiclass SVM on the binned n-gram features 
from (5) produces 51.6% cross-validation accu-
racy on training data and 44.4% accuracy on the 
weighted test set (both numbers should be com-
pared to a 33% baseline). That classifier contained 
no n-gram features selected from the PeerSpeak 
class. Preliminary experiments incorporating 
PeerSpeak n-grams yield slightly better numbers. 
However, early results also suggest that the three-
way classification problem is made more tractable 
with cascaded two-way classifiers; feature selec-
tion was more manageable with binary problems. 
For example, one classifier determines whether an 
instance is UpSpeak; if it is not, a second classifier 
distinguishes between DownSpeak and PeerSpeak. 
Our text classification problem is similar to senti-
ment analysis in that there are class dependencies; 
for example, DownSpeak is more closely related to 
PeerSpeak than to UpSpeak. We might attempt to 
exploit these dependencies in a manner similar to 
Pang and Lee (2005) to improve three-way classi-
fication. 
In addition, we had promising early results for 
classification of author-recipient links with 200 to 
500 words, so we plan to explore performance im-
provements for links of few words. 
In early, unpublished work, we had promising 
results with generative model-based approach to 
SPM, and we plan to revisit it; language models 
are a natural fit for lect modeling. Finally, we hope 
to investigate how SPM and SNA can enhance one 
another, and explore other lect classification prob-
lems for which the ground truth can be found. 
Acknowledgments  
Dr. Richard Sproat contributed time, valuable in-
sights, and wise counsel on several occasions dur-
ing the course of the research. Dr. Lillian Lee and 
her students in Natural Language Processing and 
Social Interaction reviewed the paper, offering 
valuable feedback and helpful leads. 
Our colleague, Diane Bramsen, created an ex-
cellent graphical interface for probing and under-
standing the results. Jeff Lau guided and advised 
throughout the project.  
We thank our anonymous reviewers for prudent 
advice. 
This work was funded by the Army Studies 
Board and sponsored by Col. Timothy Hill of the 
United Stated Army Intelligence and Security 
Command (INSCOM) Futures Directorate under 
contract W911W4-08-D-0011. 
References 
Cecilia Ovesdotter Alm, Dan Roth and Richard Sproat. 
2005. Emotions from text: machine learning for text-
based emotion prediction. HLT/EMNLP 2005. Octo-
ber 6-8, 2005, Vancouver. 
781
  
Penelope Brown and Stephen C. Levinson. 1987. Po-
liteness: Some universals in language usage. Cam-
bridge: Cambridge University Press. 
Eric Breck, Yejin Choi and Claire Cardie. 2007. Identi-
fying expressions of opinion in context. 
In Proceedings of the Twentieth International Joint 
Conference on Artificial Intelligence (IJCAI-2007) 
CALO Project. 2009. Enron E-Mail Dataset. 
http://www.cs.cmu.edu/~enron/. 
Yejin Choi and Claire Cardie. 2008. Learning with 
compositional semantics as structural inference for 
subsentential sentiment analysis. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing. Honolulu, Hawaii: ACM. 793-801. 
Yejin Choi and Claire Cardie. 2009. Adapting a polarity 
lexicon using integer linear programming for domain-
specific sentiment classification. Empirical Methods 
in Natural Language Processing (EMNLP).  
Christopher P. Diehl, Galileo Namata, and Lise Getoor. 
2007. Relationship identification for social network 
discovery. AAAI '07: Proceedings of the 22nd Na-
tional Conference on Artificial Intelligence. 
Bonnie Erickson, et al 1978. Speech style and impres-
sion formation in a court setting: The effects of 'pow-
erful? and 'powerless' speech. Journal of Experimental 
Social Psychology 14: 266-79. 
Norman Fairclough. 1989. Language and power. Lon-
don: Longman. 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, and Ian H. Witten. 
2009. The WEKA data mining software: An update. 
SIGKDD Exploration (1): Issue 1. 
JHU Center for Imaging Science. 2005. Scan Statistics 
on Enron Graphs. http://cis.jhu.edu/~parky/Enron/ 
Soo-min Kim and Eduard Hovy. 2004. Determining the 
Sentiment of Opinions. Proceedings of the COLING 
Conference. Geneva, Switzerland. 
Francois Mairesse and Marilyn Walker. 2006. Auto-
matic recognition of personality in conversation. Pro-
ceedings of HLT-NAACL. New York City, New York.  
Galileo Mark S. Namata Jr., Lise Getoor, and Christo-
pher P. Diehl. 2006. Inferring organizational titles in 
online communication. ICML 2006, 179-181. 
Andrew McCallum, Xuerui Wang, and Andres Corrada-
Emmanuel. 2007. Topic and role discovery in social 
networks with experiments on Enron and academic e-
Mail. Journal of Artificial Intelligence Research 29. 
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike 
Wells, and Jeff Reynar. 2007. Structured models for 
fine-to-coarse sentiment analysis. Proceedings of the 
ACL.  
David Morand. 2000. Language and power: An empiri-
cal analysis of linguistic strategies used in supe-
rior/subordinate communication. Journal of 
Organizational Behavior, 21:235-248. 
Frederick Mosteller and David L. Wallace. 1964. Infer-
ence and disputed authorship: The Federalist. Addi-
son-Wesley, Reading, Mass. 
Jon Oberlander and Scott Nowson. 2006. Whose thumb 
is it anyway? Classifying author personality from we-
blog text. Proceedings of CoLing/ACL. Sydney, Aus-
tralia.  
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. Proceedings of EMNLP, 
79?86. 
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting 
class relationships for sentiment categorization with 
respect to rating scales. Proceedings of the ACL. 
John Platt. 1998. Sequential minimal optimization: A 
fast algorithm for training support vector machines. In 
Technical Report MST-TR-98-14. Microsoft Re-
search. 
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. European 
Chapter of the Association for Computational Lin-
guistics.  
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. JASIST 60(3): 538-556. 
Carol Strapparava and Rada Mihalcea. 2008. Learning 
to identify emotions in text. SAC 2008: 1556-1560 
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 
2005. Semantic Orientations of Words using Spin 
Model. Annual Meeting of the Association for Com-
putational Linguistics. 
Ian H. Witten and Eibe Frank. 2005. Data Mining: 
Practical Machine Learning Tools and Techniques. 
Morgan Kauffman.  
Xiaojin Zhu. 2005. Semi-supervised learning literature 
survey. Technical Report 1530, Department of Com-
puter Sciences, University of Wisconsin, Madison. 
 
 
782
