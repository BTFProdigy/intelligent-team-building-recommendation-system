Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
A THAI SPEECH TRANSLATION SYSTEM  FOR MEDICAL DIALOGS
Tanja Schultz, Dorcas Alexander, Alan W Black, Kay Peterson, Sinaporn Suebvisai, Alex Waibel
Language Technologies Institute, Carnegie Mellon University
E-mail: tanja@cs.cmu.edu
1. Introduction
In this paper we present our activities towards a Thai
Speech-to-Speech translation system. We investigated in
the design and implementation of a prototype system. For
this purpose we carried out research on bootstrapping a
Thai speech recognition system, developing a translation
component, and building an initial Thai synthesis system
using our existing tools.
2. Speech Recognition
The language adaptation techniques developed in our lab
[5] enables us to rapidly bootstrap a speech recognition
system in a new target language given very limited amount
of training data. The Thailand?s National Electronics and
Technology Center gave us the permission to use their
Thai speech data collected in the hotel reservation domain.
They provided us with a 6 hours text and speech database
recorded from native Thai speakers. We divided the data
into three speaker disjoint sets, 34 speakers were used for
training, 4 speakers for development, and another 4
speakers for evaluation. The provided transcriptions were
manually pre-segmented and given in Thai script. We
transformed the Thai script into a Roman script
representation by concatenating the phoneme
representation of the Thai word given in the pronunciation
dictionary. The motivation for this romanization step was
threefold: (1) it makes it easier for non-Thai researchers to
work with the Roman representation like in the grammar
development, (2) the romanized output basically provides
the pronunciation which makes things easier for the speech
synthesis component, and (3) our speech engine currently
does not handle Thai characters.
In our first Thai speech engine we decided to disregard the
tone information. Since tone is a distinctive feature in the
Thai language, disregarding the tone increases the number
of homographs. In order to limit this number, we
distinguished those word candidates by adding a tag that
represents the tone. The resulting dictionary consists of
734 words which cover the given 6-hours database.
Building on our earlier studies which showed that
multilingual seed models outperform monolingual ones
[5], we applied phonemes taken from seven languages,
namely Chinese, Croatian, French, German, Japanese,
Spanish, and Turkish as seed models for the Thai phone
set. Table 1 describes the performance of the Thai speech
recognition component for different acoustic model sizes
(context-independent vs. 500 and 1000 tri-phone models).
The results indicate that a Thai speech recognition engine
can be built by using the bootstrapping approach with a
reasonable amount of speech data. Even the very initial
system bootstrapped from multilingual seed models gives
a performance above 80% word accuracy. The good
performance might be an artifact from the very limited
domain with a compact and closed vocabulary.
System Dev Test Eval Test
Context-Independent 85.62% 83.63%
Context-Dependent (500) 86.99% 84.44%
Context-Dependent (1000) 84.63% 82.71%
Table1: Word accuracy [%] in Thai language
3. Machine Translation
The Machine Translation (MT) component of our current
Thai system is based on an interlingua called the
Interchange Format (IF). The IF developed by CMU has
been expanded and now encompasses concepts in both the
travel and medical domains, as well as many general-use
or cross-domain concepts in many different languages [4].
Interlingua-based MT has several advantages, namely: (1)
it abstracts away from variations in syntax across
languages, providing potentially deep analysis of meaning
without relying on information pertinent only to one
particular language pair, (2) modules for analysis and
generation can be developed monolingually, with
additional reference only to the second "language" of the
interlingua, (3) the speaker can be given a paraphrase in
his or her own language, which can help verify the
accuracy of the analysis and be used to alert the listener to
inaccurate translations, and (4) translation systems can be
extended to new languages simply by hooking up new
monolingual modules for analysis and/or generation,
eliminating the need to develop a completely new system
for each new language pair.
Thai has some particular characteristics which we
addressed in IF and appear in the grammars as follows:
1) The use of a term to indicate the gender of the person:
Thai: zookhee kha1
Eng: okay (ending)
s[acknowledge] (zookhee *[speaker=])
2) An affirmation that means more than simply "yes."
Thai: saap khrap
Eng: know (ending)
s[affirm+knowledge](saap *[speaker=])
3) The separation from the main verb of terms for
feasibility and other modalities.
Thai: rvv khun ca paj dooj thxksii
kyydaaj
Eng: or you will go by taxi [can too]
s[give-information+feasibility+trip]
(*DISC-RHET [who=] ca paj
[locomotion=] [feasibility=])
4. Language Generation
For natural language generation from interlingua for Thai
and English, we are currently investigating two options: a
knowledge-based generation with the pseudo-unification
based GenKit generator developed at CMU, which
employs manually written semantic/syntactic grammars
and lexicons, and a statistical generation operating on a
training corpus of aligned interlingua and natural language
correspondences. Performance tests as well as the amount
and quality of training data will decide which approach
will be pursued in the future.
5. Speech Synthesis
First, we built a limited domain Thai voice in the Festival
Speech Synthesis System [1]. Limited Domain voices can
achieve very high quality voice output [2], and can be easy
to construct if the domain is constrained. Our initial voice
targeted the Hotel Reservation domain and we constructed
235 sentence that covered the aspects of our immediate
interest. Using the tools provided in FestVox [1], we
recorded, auto-labeled, and built a synthetic voice.
In supporting any new language in synthesis, a number of
language specific issues first had to be addressed. As with
our other speech-to-speech translation projects we share
the phoneme set between the recognizer and the
synthesizer. The second important component is the
lexicon. The pronunciation of Thai words from Thai script
is not straightforward, but there is a stronger relationship
between the orthography and pronunciation than in
English. For this small set of initial words we constructed
an explicit lexicon by hand with the output vocabulary of
522 words. The complete Thai limited domain voice uses
unit selection concatenative synthesis. Unlike our other
limited domain synthesizers, where they have a limited
vocabulary, we tag each phone with syllable and tone
information in selection making the result more fluent, and
a little more general.
Building on our previous Thai work in pronunciation of
Thai words [3], we have used the lexicon and statistically
trained letter to sound rules to bootstrap the required word
coverage. With a pronunciation model we can select
suitable phonetically balanced text (both general and in-
domain) from which we are able to record and build a
more general voice.
6. Demonstration Prototype System
Our current version is a two-way speech-to-speech
translation system between Thai and English for dialogs in
the medical domain where the English speaker is a doctor
and the Thai speaker is a patient. The translated speech
input will be spoken using the built voice. At the moment,
the coverage is very limited due to the simplicity of the
used grammars. The figure shows the interface of our
prototype system.
Acknowledgements
This work was partly funded by LASER-ACTD. The
authors thank Thailand?s National Electronics and
Computer Technology Center for giving the permission to
use their database and dictionary for this task.
References
[1] Black, A. and Lenzo, K. (2000) "Building Voices in the
Festival Speech Synthesis System", http://festvox.org
[2] Black, A. and Lenzo, K. (2000) "Limited Domain Synthesis",
ICSLP2000, Beijing, China.
[3] Chotmongkol, A. and Black, A. (2000) "Statistically trained
orthographic to sound models for Thai", ICSLP2000,
Beijing, China.
[4] Lavie A. and Levin L. and Schultz T. and Langley C. and
Han B., Tribble, A., Gates D., Wallace D. and Peterson K.
(2001) ?Domain Portability in Speech-to-speech
Translation?,  HLT, San Diego, March 2001.
[5] Schultz, T. and Waibel, A. (2001) ?Language Independent
and Language Adaptive Acoustic Modeling for Speech
Recognition?, Speech Communication, Volume 35, Issue 1-
2, pp. 31-51, August 2001.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 232?239,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning Pronunciation Dictionaries
Language Complexity and Word Selection Strategies
John Kominek Alan W Black
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jkominek,awb}@cs.cmu.edu
Abstract
The  speed  with  which  pronunciation  dictio-
naries can be bootstrapped depends on the ef-
ficiency of learning algorithms and on the or-
dering of words presented to the user. This pa-
per presents an active-learning word selection
strategy that is mindful of human limitations.
Learning rates approach that of an oracle sys-
tem that knows the final LTS rule set. 
1 Introduction
The  construction  of  speech-to-speech  translation
systems is difficult, complex, and prohibitively ex-
pensive for all but handful of major languages. De-
veloping  systems  for  new  languages  is  a  highly
skilled job requiring considerable effort, as is the
process of training people to acquire the necessary
technical knowledge. 
Ideally, a native speaker of a (minor) language ?
with the right tools ? should be able to develop a
speech  system with  little  or  no technical  knowl-
edge  of  speech  recognition,  machine  translation,
dialog management, or speech synthesis. Rapid de-
velopment of machine translation, for example, is
the goal  of  (Lavie  et  al.,  2003).  Similarly,  com-
bined  development  of  speech  recognition  and
speech synthesis is the stated goal of (Engelbrecht
and Schultz, 2005). 
Here  we  concentrate  on  lexicon  creation  for
synthesis and recognition tasks, with the affiliated
problem  of  letter-to-sound  rule  inference.  Two
central  questions of dictionary  building are: what
letter-to-sound rule representation lends itself well
to incremental learning? ? and which words should
be presented to the user,  in what order? 
In this paper we investigate various approaches
to the word ordering problem, including an active
learning algorithm. An ?active learner? is a class
of machine learning algorithms that choose the or-
der  in  which  it  is  exposed  to  training  examples
(Auer,  2000).  This is valuable when there isn't a
pre-existing set of training data and when the cost
of  acquiring such data is high. When humans are
adding dictionary entries the time and accuracy de-
pends on the selected word (short words are easier
than long; familiar are easier than unfamiliar), and
on how quickly the learner's error rate drops (long
words  are  more  informative  than  short).  Also,
mindful  that  no  answer  key  exists  for  new lan-
guages ? and that humans easily become impatient
? we would like to know when a language's letter
to sound rule system is, say, 90% complete. This
turns out to be surprising elusive to pin down.
The next section outlines our working assump-
tions and issues we seek to address. Section 3 de-
scribes our LTS learning framework, an elabora-
tion  of (Davel and Barnard,  2003).  The learning
behavior on multiple test languages is documented
in Section 4, followed in Section 5 by a compari-
son of several word selection strategies.
2 Assumptions and Issues
In  designing  language  technology  development
tools we find it helpful to envision our target user,
whom may  be  characterized  as  ?non-technical.?
Such a person speaks, reads, and writes the target
language, is able to enumerate the character set of
that  language,  distinguish  punctuation  from
whitespace,  numerals,  and  regular  letters  or
graphemes,  and  specify  if  the  language  distin-
guishes upper and lower casing.  When presented
232
with the pronunciation of a word (as a synthesized
wavefile),  the user can say whether it  is  right or
wrong. In addition, such a person has basic com-
puter fluency, can record sound files, and can navi-
gate the  HTML interface of our software tools. If
these latter requirements present a barrier then we
assume the  availability of a field agent to config-
ure the computer, familiarize the user, plus trans-
late the English instructions, if necessary.
Ideally, our  target  user  need not  have explicit
knowledge of their  own language's phoneme set,
nor even be aware that a word can be transcribed
as a sequence of phonemes (differently from let-
ters).  The ability to reliably discover a workable
phoneme set from an unlabeled corpus of speech
is not yet at hand, however. Instead we elicit a lan-
guage's phoneme set during an initialization stage
by presenting examples  of  IPA wavefiles  (Wells
and House, 1995). 
Currently, pronunciations are spelled out using
a romanized phonetic alphabet. Following the rec-
ommendation of (Davel and Barnard, 2005) a can-
didate pronunciation is accompanied with a wave-
file generated from a phoneme-concatenation syn-
thesizer. Where possible, more than one pronunci-
ation is generated for each word presented, under
that assumption that it is easier for a listener to se-
lect from among a small  number of choices than
correct a wrong prediction.
2.1 Four Questions to Address
1. What  is  our  measure  of  success? Ultimately,
the time to build a lexicon of a certain coverage
and correctness. As a proxy for time we use the
number of characters presented. (Not words, as
is typically the case, since long words contain
more information than short, and yet are harder
for a human to verify.)
2. For  a  given  language,  how many words  (let-
ters) are needed to learn its LTS rule system?
The true,  yet not too useful  answer  is  ?it  de-
pends.?   The  complexity  of  the  relation  be-
tween  graphemic  representation  and  acoustic
realization varies greatly across languages. That
being the case, we seek a useful measure of a
language's degree of  complexity.
3. Can the asymptote  of the LTS system be esti-
mated,  so  that  one  can  determine  when  the
learned rules are 90 or 95% complete? In Sec-
tion 4 we present evidence that this may not be
possible.  The  fall-back position  is  percentage
coverage of the supplied corpus.
4. Which words should be presented to the user,
and  in  what  order? Each  additional  word
should maximize the marginal information gain
to the system. However, short words are easier
for humans to contend with than long. Thus a
length-based weighting needs to be considered.
3 LTS Algorithm Basics 
A wide variety of approaches have been applied to
the problem of letter-to-sound rule induction. Due
to simplicity of representation and ease of manipu-
lation, our LTS rule learner follows the Default &
Refine  algorithm  of  Davel  (Davel  and  Barnard,
2004). In this framework, each letter  c is assigned
a default production p1-p2... denoting the sequence
of zero or  more phonemes most  often associated
with that letter. Any exceptions to a letter's default
rule is explained in terms of the surrounding con-
text  of  letters.  The  default  rules  have  a  context
width of one (the letter itself), while each addition-
al letter increases the width of the context window.
For example, if we are considering the first occur-
rence of  's'  in  the  word  basics, the  context  win-
dows are as listed in Table 1. By convention, the
underscore  character  denotes  the  predicted  posi-
tion, while the hash represents word termination.
width context sets ordered by increasing width
1 {_}
2 {a_ , _i}
3 {ba_ , a_i , _ic}
4 (#ba_ , ba_i , a_ic , _ics}
5 {#ba_i , ba_ic , a_ics , _ics#}
6 {#ba_ic , ba_ics , a_ics#}
7 {#ba_ics , ba_ics#}
8 {#ba_ics#}
Table 1. Letter contexts for the first 's' in basics.
In this position there are 20 possible explanatory
contexts. The order in which they are visited de-
fines an algorithm's search strategy. In the class of
algorithms knows as ?dynamically expanding con-
text (DEC)?, contexts are considered top-down as
depicted in Table 1. Within one row, some algo-
rithms follow a fixed order (e.g. center, left, right).
Another variant tallies the instances of productions
233
associated  with  a  candidate  context  and  chooses
the  one  with  the  largest  count.  For  example,  in
Spanish the letter 'c' may generate K (65%), or TH
when followed by e or i (32%), or CH when fol-
lowed by h (3%). These are organized by frequen-
cy into a ?rule chain.?
Rule rank RHS Context Frequency
1 K _ 65.1%
2 TH _i 23.6%
3 TH _e 8.5%
4 CH _h 2.8%
If desired,  rules  2 and 3 in  this  example  can be
condensed into 'c' ? TH /_{i,e}, but in general are
left separated for sake of simplicity.
In our variant, before adding a new rule all pos-
sible contexts  of all  lengths are considered when
selecting the best one. Thus the rule chains do not
obey a strict order of expanding windows, though
shorter  contexts generally  precede longer ones in
the rule chains.
One  limitation  of  our  representation is  that  it
does not support gaps in the letter context. Consid-
er  the  word  pairs  tom/tome,  top/tope,  tot/tote.  A
CART tree can represent this pattern with the rule:
if (c-1 = 't' and c0='o' and c2='e') then ph=OW. In prac-
tice, the inability to skip letters is not a handicap. 
3.1 Multiple Pronunciation Predictions
Given a word, finding the predicted pronunciation
is easy. Rule chains are indexed by the letter to be
predicted, and possible contexts are scanned start-
ing from the most specific until a match is found.
Continuing  our  example,  the  first  letter  in  the
Spanish word ciento fails rule 4, fails rule 3, then
matches rule 2 to yield TH. For additional pronun-
ciations the search continues until another match is
found: here, the default rule 'c' ? K /_. This proce-
dure  is  akin  to  predicting  from  progressively
smoother models. In a complex language such as
English,  a  ten  letter  word  can  readily  generate
dozens  of  alternate  pronunciations,  necessitating
an ordering policy to keep the total manageable.
4 Language Characterization
English is notorious for having a highly irregular
spelling  system.  Conversely,  Spanish  is  admired
for its simplicity. Most others lie somewhere in be-
tween.  To estimate  how many words  need to  be
seen in order  to acquire 90% coverage of a lan-
guage's LTS rules, it helps to have a quantitative
measure.  In  this  section  we  offer  a  perplexity-
based measure of LTS regularity and present mea-
surements of several  languages with varying cor-
pus  size.  These  measurements  establish,  surpris-
ingly,  that  a  rule  system's  perplexity  increases
without bound as the number of training words in-
creases.  This  holds  true  whether  the  language is
simple  or  complex.  In  response,  we  resort  to  a
heuristic  measure  for  positioning languages  on a
scale of relative difficulty.
4.1 A Test Suite of Seven Languages
Our test suite consists of pronunciation dictionar-
ies from seven languages, with English considered
under two manifestations.
English.  Version  0.6d of  CMU-DICT,  consid-
ered without stress (39 phones) and with two level
stress marking (58 phones).  German. The Celex
dictionary of 321k entries (Burnage, 1990). Dutch.
The  Fonilex  dictionary  of  218k entries  (Mertens
and  Vercammen,  1998).  Fonilex  defines  an  ab-
stract  phonological  level  from which  specific di-
alects  are specified.  We tested on the ?standard?
dialect. Afrikaans. A 37k dictionary developed lo-
cally. Afrikaans is a language of South Africa and
is  a  recent  derivative  of  Dutch.  Italian.  A 410k
dictionary  distributed  as  part  of  a  free  Festival-
based  Italian  synthesizer  (Cosi,  2000).  Spanish.
Generated by applying a set of hand written rules
to a 52k lexicon. The LTS rules are a part of the
standard Festival Spanish distribution. Telugu. An
8k locally  developed dictionary.  In its  native or-
thography, this language of India possess a highly
regular  syllabic  writing system. We've adopted  a
version  of  the  Itrans-3  transliteration  scheme
(Kishore 2003) in which sequences of two to four
English letters map onto Telugu phonemes.
4.2 Perplexity as a Measure of Difficulty
A useful  way of considering letter  to sound pro-
duction is as a Markov process in which the gener-
ator passes through a sequence of  states (letters),
each  probabilistically  emitting  observation  sym-
bols  (phonemes)  before  transitioning  to  the  next
state  (following letter).  For a letter  c,  the unpre-
dictability  of  phoneme  emission  is  its  entropy
H ?c?=?? ? pi log pi? or equivalently its perplexity
P ?c?=eH ?c? . The perplexity can be interpreted as
234
the average number of  output  symbols  generated
by a letter. The production perplexity of the char-
acter set is the sum of each individual letter's per-
plexity weighted by its unigram probability pc.
   (1)
Continuing with our Spanish example, the letter 'c'
emits the observation symbols (K, TH, CH) with a
probability distribution of (.651, .321, .028), for a
perplexity  of  2.105.  This  computation applies
when each letter is assigned a single probabilistic
state. The process of LTS rule discovery effective-
ly splits the state 'c' into four context-defined sub-
states:  (-,c,-),  (-,c,i),  (-,c,e),  (-,c,h).  Each of these
states emits only a single symbol. Rule addition is
therefore an entropy reduction process;  when the
rule set is complete the letter-to-sound system has
a perplexity of 1, i.e. it is perfectly predictable.
The ?price paid? for perfect  predictability is a
complex set of rule chains. To measure rule com-
plexity we again associate a single state with each
letter. But, instead of phonemes, the  rules  are the
emission  symbols.  Thus  the  letter  'c'  emits  the
symbols (K/_, TH/_i, TH/_e, CH/_h) with a distri-
bution of (.651, .236, .085, .028), for a perplexity
of 2.534. Applying equation (1) to the full set of
rules defines the LTS system's average perplexity. 
4.3 Empirical Measurements
In  the  Default  & Refine  representation,  the  rule
chain for each letter is is initialized with its most
probably  production.  Additional  context-depen-
dent rules are appended to cover additional letter
productions, with the rule offering the greatest in-
cremental  coverage  being  added  first.  (Ties  are
broken in an implementation-dependent way.)
Figure 1 uses Spanish to illustrate a characteris-
tic  pattern:  the  increase  in  coverage as  rules  are
added one at  a time. Since the figure of merit  is
letter-based, the upper curve (% letters correct) in-
creases monotonically, while the middle curve (%
words correct) can plateau or decrease briefly. 
In the lower curve of Figure 1 the growth proce-
dure is constrained such that all width 1 rules are
added before width 2 rules, which in turn must be
exhausted  before  width  3  rules  are  considered.
This  constraint  leads  to  its  distinctive  scalloped
shape. The upper limit of the W=1 region shows
the performance of the unaided default rules (68%
words correct).
Figure 1. Coverage of Spanish (52k corpus) as a
function of rule size. For the lower curve, W indi-
cates the rule context window width. The middle
(blue) curve tracks near-optimal performance im-
provement with the introduction of new rules.
For more complex languages the majority of rules
have a context width in the range of 3 to 6. This is
seen in Figure 2 for English, Dutch, Afrikaans, and
Italian. However, a larger rule set does not mean
that the average context width is greater. In Table
2, below, compare Italian to Dutch.
Language Number of Rules Average Width
English 40k  19231 5.06
Dutch 40k  10071 4.35
Afrikaans 37k  5993 4.66
Italian 40k  3385 4.78
Spanish 52k  76 1.66
Table 2.  Number of LTS rules for five language
and their average context width.
Figure  2.  Distribution  of  LTS  rules  by  context
window width for four languages: English, Dutch,
Afrikaans, and Italian.
Perave=?
c
pc e
??
i
pi log pi
Window Width
2 4 6 8 10
Nu
mb
er
 of
 R
ule
s
0
1000
2000
3000
4000
5000
6000 LTS Rule Count vs Window Width
Legend
English 40k
Dutch 40k
Afrikaans 37k
Italian 40k
Legend
Chars Correct
Words Correct
Words Correct
Number of Rules
0 10 20 30 40 50 60
Pe
rce
nt
 C
or
re
ct
0
20
40
60
80
100 Spanish LTS Ruleset Performance
W=3W=2W=1
235
Beyond a window width of 7, rule growth tapers
off  considerably.  In  this  region  most  new  rules
serve  to  identify  particular  words  of  irregular
spelling, as it is uncommon for long rules to gener-
alize beyond a single instance. Thus when training
a  smoothed  LTS rule  system it  is  fair  to  ignore
contexts larger than 7, as is done for example in
the Festival synthesis system (Black, 1998).
Figure 2 contrasts four languages with training
data of around 40k words, but says nothing of how
rule sets grow as the corpus size increases. Figure
3 summarizes measurements taken on eight encod-
ings of seven languages (English twice, with and
without stress marking), tested from a range of 100
words  to  over  100,000.  Words  were  subsampled
from each alphabetized lexicon at equal spacings.
The results are interesting, and for us, unexpected.
Figure 3. Rule system growth as the corpus size is
increased,  for  seven languages.  From top to bot-
tom:  English  (twice),  Dutch,  German,  Afrikaans,
Italian, Telugu, Spanish. The Telugu lexicon uses
an Itrans-3 encoding into roman characters, not the
native  script,  which  is  a  nearly  perfect  syllabic
transcription. The context window has a maximum
width of 9 in these experiments.
Within  this  experimental  range  none  of  the  lan-
guages  reach  an  asymptotic  limit,  though  some
hint  at  slowed  growth  near  the  upper  end.  A
straight line on a log-log graph is characteristic of
geometric growth, to which a power law function
y=axb+c is an appropriate parametric fit. For diffi-
cult  languages the growth rates  (power  exponent
b) vary between 0.5 and 0.9, as summarized in Ta-
ble 3. The language with the fastest growth is En-
glish, followed, not by Dutch, but Italian. Italian is
nonetheless the simpler of these two, as indicated
by the smaller multiplicative factor a.
Language a b
English (stressed) 2.97 0.88
English (plain)  3.27 0.85
Dutch  12.6 0.64
German  39.86 0.49
  Afrikaans  15.34 0.57
Italian  2.16 0.69
Table 3. Parameters a and b for the power law fit
y=axb+c to the growth of LTS system size. 
It would be good if a tight ceiling could be estimat-
ed from partial data in order to know (and report to
the lexicon builder) that  with  n rules defined the
system is m percent complete. However, this trend
of  geometric  growth  suggests  that  asking  ?how
many letter-to-sound rules does a given  language
have?? is an ill-posed question. 
In light of this, two questions are worth asking.
First, is the geometric trend particular to our rule
representation?  And  second,  is  ?total  number  of
rules?  the  right  measure  of  LTS complexity?  To
answer the first  question we repeated the experi-
ments with the  CART tree builder available from
the Festival  speech  synthesis  toolkit.  As it  turns
out  ?  see  Table  4  ?  a  comparison  of  contextual
rules and node counts for Italian  demonstrate that
a CART tree representation also exhibits geometric
growth with respect to lexicon size.
Num Words
in Lexicon
Contextual
LTS Rules
CART Tree
Nodes
100 80 145
250 131 272
500 198 399
1000 283 601
2500 506 1169
5000 821 1888
10,000 1306 2840
20,000 2109 4642
40,000 3385 7582
80,000 5524 13206
Table 4. A comparison of rule system growth for
Italian as the corpus size is increased. CART tree
nodes (i.e. questions) are the element comparable
to LTS rules used in letter context chains. The fit-
ted parameters to the  CART data are  a=2.29 and
b=0.765. This compares to  a=2.16 and b=0.69.
Num Words in Lexicon
100 1000 10000 100000
Nu
m 
LT
S 
Ru
les
100
1000
10000
LTS Rules vs. Lexicon Size
Legend
English (w/stress)
English (no stress)
Dutch 
German 
Afrikaans
Italian
Telugu (itrans-3)
Spanish
236
If geometric growth and lack of an obvious asymp-
tote  is  not  particular  to  expanding  context  rule
chains,  then  what  of  the  measure?  The  measure
proposed in Section 4.2 is average chain perplexi-
ty. The hypothesis is that a system close to satura-
tion will still add new rules, but that the average
perplexity levels off. Instead, the data shows little
sign of saturation (Figure 4). In contrast, the aver-
age  perplexity  of  the  letter-to-phoneme  distribu-
tions remains level with corpus size (Figure 5). 
Figure 4. Growth of average rule perplexity as a
function  of lexicon size.  Except  for  Spanish and
Telugu,  the  average  rule  system  perplexity  not
only grows, but grows at an accelerating rate. 
Figure  5.  Growth  of  average  letter-to-phoneme
production perplexity as a function of lexicon size.
Considering  these  observations  we've resorted  to
the following heuristic to measure language com-
plexity: a) fix the window width to 5, b) measure
the average rule perplexity at lexicon sizes of 10k,
20k,  and  40k,  then  c)  take  the  average  of  these
three  values.  Fixing  the  window  width  to  5  is
somewhat arbitrary, but is intended to prevent the
system from learning an unbounded suite of excep-
tions. Available values are contained in Table 5.
Language Ave Letter
Perplexity
Heuristic
Perplexity
Perplexity
Ratio
English 3.25 50.11 15.42
Dutch  2.73 16.80 6.15
German  2.41 16.70 6.93
Afrikaans  2.32 11.48 8.32
Italian  1.38 3.52 2.55
Spanish  1.16 1.21 1.04
 Table 5.  Perplexity measures  for  six languages.
The  third  (rightmost)  column is  the  ratio  of  the
second divided by the first.  A purely phonetic sys-
tem has a heuristic perplexity of one.
From these measurements we conclude, for exam-
ple, that Dutch and German are equally difficult,
that English is 3 times more complex than either of
these, and that English is 40 times more complex
than Spanish.
5 Word Selection Strategies
A selection strategy is  a method for choosing an
ordered  list  of  words  from a lexicon.  It  may be
based on an estimate of expected maximum return,
or be as simple as random selection. A good strate-
gy should enable rapid learning, avoid repetition,
be robust, and not overtax the human verifier. 
This  section  compares  competing  selection
strategies on a single lexicon. We've chosen a 10k
Italian lexicon as a problem of intermediate diffi-
culty, and focus on early stage learning. To pro-
vide a useful frame of reference, Figure 6 shows
the results of running 5000 experiments in which
the word sequence has been chosen randomly. The
x-axis is number of letters examined.
Figure 6. Random sampling of Italian 10k corpus.
Legend
English (w/stress)
English (no stress)
Dutch 
German 
Afrikaans
Italian
Telugu
Spanish
Num Words in Lexicon
100 1000 10000 100000
LT
S 
Ru
le 
Pe
rp
lex
ity
0.0
5.0
10.0
15.0
20.0
25.0
30.0 LTS Rule Perplexity vs Lexicon Size
Legend
English (no stress)
Dutch 
German 
Afrikaans
Italian
Telugu (itrans)
Spanish
37k4k
170k1k
Ave Productions per Letter
0 2 4 6 8 10 12
Av
e P
ro
du
cti
on
 Pe
rp
lex
ity
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0 Letter to Phoneme Perplexity
Spanish
Iraqi
Phonetic alphabet
1k 40k
80k
Telugu
Italian
Num Letters Examined
0 1000 2000 3000 4000 5000 6000
Wo
rd
s C
or
re
ct 
(%
)
10
20
30
40
50
60
70
80 Word Accuracy, Random SelectionItalian, 10k dict, maxwin=5
237
Figure 7 compares average random performance to
four deterministic strategies.  They are:  alphabeti-
cal word ordering, reverse alphabetical, alphabeti-
cal sorted by word length (groups of single charac-
ter  words first,  followed by two character  words,
etc.), and a greedy ngram search. Of the first three,
reverse alphabetical performs best because it intro-
duces  a  greater  variety  of  ngrams  more  quickly
than the others. Yet, all of these three are substan-
tially  worse  than  random.  Notice  that  grouping
words  from short  to  long degrades  performance.
This implies that strategies tuned to the needs of
humans will incur a machine learning penalty.
Figure 7. Comparison of three simple word order-
ings  to  the  average  random  curve,  as  well  as
greedy ngram search. 
It might be expected that selecting words contain-
ing the most popular ngrams first  would out-per-
forms random, but as is seen in Figure 7, greedy
selection  closely  tracks  the  random  curve.  This
leads  us to investigate  active  leaning algorithms,
which we treat as variants of ngram selection. 
5.1 Algorithm Description
Let W = {w1,w2,...} be the lexicon word set, having A =
{'a', 'b',...} as the alphabet of letters. We seek an ordered
list V = (... wi ...) s.t. score(wi) ? score (wi+1). V is initial-
ly empty and is extended one word at a time with wb, the
?best? new word. Let g=c1c2...cn ` A* be an ngram of
length n, and Gw={gi}, gi ` w are all the ngrams found in
word w. Then GW =  5 Gw,  w  `  W, is  the set  of  all
ngrams in the lexicon W, and GV = 5 Gw, w ` Vis the set
of all ngrams in the selected word list V. The number of
occurrences of g in W is score(g), while score(w) =  ?
score(g) st.  g  `  w and g  v GV. The scored ngrams are
segmented  into separately sorted  lists,  forming an  or-
dered list of queues Q = (q1,q2,...qN) where qn contains
ngram of length n and only n. 
Algorithm
for q in Q
g = pop(q)
for L = 1 to |longest word in W|
Wg,L = {wi} s.t. |wi| = L, g ` wi and wi v V
wb = argmax score(Wg,L)
if score (wb) > 0 then
V = V + wb
GV = GV 4 Gwb
return wb
In this search the outer loop orders ngrams by length,
while the inner loop orders words by length. For selec-
tion based on ngram coverage, the queue Q is computed
only once for the given lexicon W. In our active learner,
Q is re-evaluated after each word is selected, based on
the ngrams present in the current LTS rule contexts. Let
GLTS = {gi} s.t. gi ` some letter context in the LTS rules.
Initially GLTS,0 = {}. Then, at any iteration k, GLTS,k are
the ngrams present in the rules, and G'LTS,k+1 is an ex-
panded set of candidate ngrams that constitute the ele-
ments of Q. G' is formed by prepending each letter c of
A to each g in G, plus appending each c to g. That is,
G'LTS,k+1 = A%GLTS,k 4 GLTS,k%A where % is the Cartesian
product. Executing the algorithm returns wb and yields
GLTS,k+1 the set of ngrams covered by the expanded rule
set.  In  this  way knowledge  of  the  current  LTS  rules
guides the search for maximally informative new words.
5.2 Active Learner Performance
Figure  8  displays  the  performance  of  our  active
learner  on  the  Italian  10k corpus,  shown  as  the
blue  curve.  For  the  first  500  characters  encoun-
tered,  the  active  learner's  performance  is  almost
everywhere better  than average random, typically
one half to one standard deviation above this refer-
ence level.
Two  other  references  are  shown.  Immediately
above the active learner curve is ?Oracle? word se-
lection. The Oracle has access to the final LTS sys-
tem  and  selects  words  that  maximally  increases
coverage of the known rules. The topmost curve is
for  a  ?Perfect  Oracle.?  This  represents  an  even
more unrealistic  situation in which each letter  of
each  word  carries  with  it  information  about  the
corresponding production rule.  For example,  that
'g' yields /F/ 10% of the time, when followed by
the letter 'h' (as in ?laugh?) . Carrying complete in-
formation with each letter allows the  LTS system
to be constructed directly and without mistake. In
contrast,  the  non-perfect  oracle  makes  mistakes
sequencing rules  in each  letter's  rule  chain.  This
decreases performance.
Italian, 10k dict, maxwin=5
Num Letters Examined
0 1000 2000 3000 4000 5000 6000
W
or
ds
 C
or
re
ct 
(%
)
10
20
30
40
50
60
70
80 Word Accuracy, Simple Strategies
Legend
Average random
n-gram coverage
Reverse alphabetic
Alphabetic order
Length, alpha order
238
Figure 8. From top to bottom: a perfect Oracle, a
word selection Oracle, our active learner, and av-
erage random performance. The perfect Oracle de-
marcates  (impossibly  high)  optimal  performance,
while  Oracle  word  selection  suggests  near-opti-
mality.  For  comparison,  standard  deviation  error
bars are added to the random curve. 
Encouragingly, the active learning algorithm strad-
dles  the  range  in  between  average  random  (the
baseline) and Oracle word selection (near-optimal-
ity). Less favorable is the non-monotonicity of the
performance curve; for example, when the number
of  letters  examined  is  135,  and  210.  Analysis
shows that these drops occur when a new letter-to-
sound  production  is  encountered  but  more  than
one  context  offers  an  equally  likely  explanation.
Faced  with  a  tie,  the  LTS  learner  sometimes
chooses incorrectly. Not being aware of this mis-
take  it  does  not  seek  out  correcting  words.  Flat
plateaus occur when  additional words (containing
the next most popular ngrams) do not contain pre-
viously unseen letter-to-sound productions. 
6 Conclusions
While this work does not definitively answer the
question of ?how may words to learn the rules,?
we  have  developed  ways  of  characterizing  lan-
guage  complexity,  which  can  guide  developers.
We've devised a word selection  strategy that  ap-
pears to perform better than the (surprisingly high)
standard  set  by  randomly  selection.  Further  im-
provements  are  possible  by incorporating knowl-
edge of  word  alignment  and rule  sequencing  er-
rors.  By  design,  our  strategy  is  biased  towards
short words over long, thereby being ?nice? to lex-
icon developers ? our original objective. 
Acknowledgments 
This material is in part based upon work supported by
the  National  Science Foundation  under  Grant  No.
0415201. Any opinions, findings, and conclusions or
recommendations expressed in this material are those
of the authors and do not necessarily reflect the views
of the National Science Foundation.
References 
Peter  Auer, 2000.  Using upper confidence bounds for
online learning. Proceedings of the 41st Annual Sym-
posium on Foundations of Computer Science, pp.
Alan W Black, Kevin Lenzo, and Vincent Pagel, 1998.
Issues in Building General Letter to Sound Rules. 3rd
ESCA Workshop on Speech Synthesis, Australia.
Gavin Burnage, 1990. CELEX ? A Guide for Users. Hi-
jmegen: Centre for Lexical Information, University of
Nijmegen.
Piero Cosi,  Roberto Gretter, Fabio Tesser, 2000.  Festi-
val  parla  italiano.  Proceedings of  GFS2000,  Gior-
nate del Gruppo di Fonetica Sperimentale, Padova.
Marelie Davel and Etienne Barnard,  2003.  Bootstrap-
ping in Language Resource Generation. Proceedings
of the 14th Symposium of the Pattern Recognition As-
sociation of South Africa, pp. 97-100.
Marelie Davel and Etienne Barnard,  2004.  A default-
and-refine  approach  to  pronunciation  prediction,
Proceedings of  the 15th  Symposium of the Pattern
Recognition Association of South Africa.
Marelie Davel and Etienne Barnard,  2005.  Bootstrap-
ping  Pronunciation  Dictionaries:  Practical  Issues.
Proceedings  of  the  9th International  Conference  on
Spoken Language Processing, Lisbon, Portugal.
Herman Engelbrecht,  Tanja  Schultz,  2005.  Rapid  De-
velopment of an Afrikaans-English Speech-to-Speech
Translator, International Workshop on  Spoken Lan-
guage Translation, Pittsburgh, PA. pp.169-176.
S P Kishore and Alan W Black, 2003. Unit Size in Unit
Selection Speech Synthesis. Proceedings of the 8th Eu-
ropean Conference on Spoken Language Processing,
Geneva, Switzerland.
Alon Lavie, et al 2003.  Experiments with a Hindi-to-
English Transfer-based MT System under a Miserly
Data  Scenario,  ACM Transactions  on  Asian  Lan-
guage Information Processing, 2(2).
Piet Mertens and Filip Vercammen, 1998. Fonilex Man-
ual, Technical Report, K. U. Leuven CCL.
John Wells  and Jill  House,  1995.  Sounds of  the IPA.
http://www.phon.ucl.ac.uk/shop/soundsipa.php.
Italian, 10k dict, maxwin=5
Legend
Perfect Oracle
Oracle word selection
Active learner
Averge random
Num Letters Examined
0 100 200 300 400 500
Wo
rd
s C
or
re
ct 
(%
)
0
20
40
60
80
100 Word Accuracy, Active Learner
239
Proceedings of NAACL HLT 2009: Short Papers, pages 149?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incremental Adaptation of Speech-to-Speech Translation
Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,
Tanja Schultz, Ian Lane, Alex Waibel and Alan W. Black
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.edu
Abstract
In building practical two-way speech-to-speech
translation systems the end user will always wish
to use the system in an environment different from
the original training data. As with all speech sys-
tems, it is important to allow the system to adapt
to the actual usage situations. This paper investi-
gates how a speech-to-speech translation system can
adapt day-to-day from collected data on day one to
improve performance on day two. The platform is
the CMU Iraqi-English portable two-way speech-
to-speech system as developed under the DARPA
TransTac program. We show how machine transla-
tion, speech recognition and overall system perfor-
mance can be improved on day 2 after adapting from
day 1 in both a supervised and unsupervised way.
1 Introduction
As speech-to-speech translation systems move from the
laboratory into field deployment, we quickly see that mis-
match in training data with field use can degrade the per-
formance of the system. Retraining based on field us-
age is a common technique used in all speech systems
to improve performance. In the case of speech-to-speech
translation we would particularly like to be able to adapt
the system based on its usage automatically without hav-
ing to ship data back to the laboratory for retraining. This
paper investigates the scenario of a two-day event. We
wish to improve the system for the second day based on
the data collected on the first day.
Our system is designed for eyes-free use and hence
provides no graphical user interface. This allows the user
to concentrate on his surrounding environment during an
operation. The system only provides audio control and
feedback. Additionally the system operates on a push-to-
talk method. Previously the system (Hsiao et al, 2006;
Bach et al, 2007) needed 2 buttons to operate, one for the
English speaker and the other one for the Iraqi speaker.
W i i c o n t r o l l e r
M i c & L i g h t
L o u d  s p e a k e r
Figure 1: The users interact with the system
To make the system easier and faster to use, we propose
to use a single button which can be controlled by the En-
glish speaker. We mounted a microphone and a Wii re-
mote controller together as shown in 1.
Since the Wii controller has an accelerometer which
can be used to detect the orientation of the controller, this
feature can be applied to identify who is speaking. When
the English speaker points towards himself, the system
will switch to English-Iraqi translation. However, when
the Wii is pointed towards somebody else, the system will
switch to Iraqi-English translation. In addition, we attach
a light on the Wii controller providing visual feedback.
This can inform an Iraqi speaker when to start speaking.
The overall system is composed of five major compo-
nents: two automatic speech recognition (ASR) systems,
a bidirectional statistical machine translation (SMT) sys-
tem and two text-to-speech (TTS) systems.
2 Data Scenario
The standard data that is available for the TransTac
project was collected by recording human interpreter
mediated dialogs between war fighters and Iraqi native
speakers in various scenarios. The dialog partners were
aware that the data was being collected for training ma-
chine based translation devices, but would often talk di-
rectly to the human interpreter rather than pretending it
was an automatic device. This means that the dialog
149
partners soon ignored the recording equipment and used
a mostly natural language, using informal pronunciation
and longer sentences with more disfluencies than we find
in machine mediated translation dialogs.
Most users mismatch their language when they com-
municate using an automatic speech-to-speech transla-
tion system. They often switch to a clearer pronuncia-
tion and use shorter and simpler sentences with less dis-
fluency. This change could have a significant impact on
speech recognition and machine translation performance
if a system was originally trained on data from the inter-
preter mediated dialogs.
For this reason, additional data was collected during
the TransTac meeting in June of 2008. This data was
collected with dialog partners using the speech-to-speech
translation systems from 4 developer participants in the
TransTac program. The dialog partners were given a de-
scription of the specific scenario in form of a rough script
and had to speak their sentences into the translation sys-
tems. The dialog partners were not asked to actually react
to the potentially incorrect translations but just followed
the script, ignoring the output of the translation system.
This has the effect that the dialog partners are no longer
talking to a human interpreter, but to a machine, press-
ing push-to-talk buttons etc. and will change their speech
patterns accordingly.
The data was collected over two days, with around 2
hours of actual speech per day. This data was transcribed
and translated, resulting in 864 and 824 utterance pairs
on day 1 and 2, respectively.
3 ASR LM Adaptation
This section describes the Iraqi ASR system and how we
perform LM adaptation on the day 1 data to improve ASR
performance on day 2. The CMU Iraqi ASR system is
trained with around 350 hours of audio data collected un-
der the TransTac program. The acoustic model is speaker
independent but incremental unsupervised MLLR adap-
tation is performed to improve recognition. The acous-
tic model has 6000 codebooks and each codebook has
at most 64 Gaussian mixtures determined by merge-and-
split training. Semi-tied covariance and boosted MMI
discriminative training is performed to improve the model
(Povey et al, 2009). The features for the acoustic model
is the standard 39-dimension MFCC and we concatenate
adjacent 15 frames and perform LDA to reduce the di-
mension to 42 for the final feature vectors. The language
model of the ASR system is a trigram LM trained on the
audio transcripts with around three million words with
Kneser-Ney smoothing (Stolcke, 2002).
To perform LM adaptation for the ASR system, we use
the ASR hypotheses from day 1 to build a LM. This LM
is then interpolated with the original trigram LM to pro-
duce an adapted LM for day 2. We also evaluate the effect
of having transcribers provide accurate transcription ref-
erences for day 1 data, and see how it may improve the
performance on day 2. We compare unigram, bigram and
trigram LMs for adaptation. Since the amount of day 1
data is much smaller than the whole training set and we
do not assume transcription of day 1 is always available,
the interpolation weight is chosen of be 0.9 for the orig-
inal trigram LM and 0.1 for the new LM built from the
day 1 data. The WER of baseline ASR system on day 1
is 32.0%.
Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref
31.3 30.9 31.2 31.1 30.6 30.5 30.4
Table 1: Iraqi ASR?s WER on day 2 using different adaptation
schemes for day 1 data
The results in Table 1 show that the ASR benefits from
LM adaptation. Adapting day 1 data can slightly improve
the performance of day 2. The improvement is larger
when day 1 transcript is available which is expected. The
result also shows that the unigram LM is the most robust
model for adaptation as it works reasonably well when
transcripts are not available, whereas bigram and trigram
LM are more sensitive to the ASR errors made on day 1.
Day 1 Day 2
No ASR adaptation 29.39 27.41
Unsupervised ASR adaptation 31.55 27.66
Supervised ASR adaptation 32.19 27.65
Table 2: Impact of ASR adaptation to SMT
Table 2 shows the impact of ASR adaptation on the
performance of the translation system in BLEU (Papineni
et al, 2002). In these experiments we only performed
adaptation on ASR and still using the baseline SMT com-
ponent. There is no obvious difference between unsuper-
vised and supervised ASR adaptation on performance of
SMT on day 2. However, we can see that the difference
in WER on day 2 of unsupervised and supervised ASR
adaptation is relatively small.
4 SMT Adaptation
The Iraqi-English SMT system is trained with around
650K sentence pairs collected under the TransTac pro-
gram. We used PESA phrase extraction (Vogel, 2005)
and a suffix array language model (Zhang and Vogel,
2005). To adapt SMT components one approach is to op-
timize LM interpolation weights by minimizing perplex-
ity of the 1-best translation output (Bulyko et al, 2007).
Related work including (Eck et al, 2004) attempts to use
information retrieval to select training sentences similar
to those in the test set. To adapt the SMT components
we use a domain-specific LM on top of the background
150
language models. This approach is similar to the work
in (Chen et al, 2008). sThe adaptation framework is 1)
create a domain-specific LM via an n-best list of day 1
machine translation hypothesis, or day 1 translation ref-
erences; 2) re-tune the translation system on day 1 via
minimum error rate training (MERT) (Venugopal and Vo-
gel, 2005).
Use Day 1 Day 2
Baseline 29.39 27.41
500 Best 1gramLM 29.18 27.23
MT Hypos 2gramLM 29.53 27.50
3gramLM 29.36 27.23
Table 3: Performance in BLEU of unsupervised adaptation.
The first question we would like to address is whether
our adaptation obtains improvements via an unsupervised
manner. We take day 1 baseline ASR hypothesis and use
the baseline SMT to get the MT hypothesis and a 500-
best list. We train a domain LM using the 500-best list
and use the MT hypotheses as the reference in MERT. We
treat day 1 as a development set and day 2 as an unseen
test set. In Table 3 we compare the performance of four
systems: the baseline which does not have any adaptation
steps; and 3 adapted systems using unigram, bigram and
trigram LMs build from 500-best MT hypotheses.
Use Day 1 Day 2
Baseline (no tune) 29.39 27.41
Baseline (tune) 29.49 27.30
500 Best 1gramLM 30.27 28.29
MT Hypos 2gramLM 30.39 28.30
3gramLM 28.36 24.64
MT Ref 1gramLM MT Ref 30.53 28.35
Table 4: Performance in BLEU of supervised adaptation.
Experimental results from unsupervised adaptation did
not show consistent improvements but suggest we may
obtain gains via supervised adaptation. In supervised
adaptation, we assume we have day 1 translation refer-
ences. The references are used in MERT. In Table 4 we
show performances of two additional systems which are
the baseline system without adaptation but tuned toward
day 1, and the adapted system which used day 1 trans-
lation references to train a unigram LM (1gramLM MT
Ref). The unigram and bigram LMs from 500-best and
unigram LM from MT day 1 references perform rela-
tively similar on day 2. Using a trigram 500-best LM
returned a large degradation and this LM is sensitive to
the translation errors on day1
5 Joint Adaptation
In Sections 3 and 4 we saw that individual adaptation
helps ASR to reduce WER and SMT to increase BLEU
ASR SMT Day 1 Day 2
No adaptation No adaptation 29.39 27.41
Unsupervised ASR 1gramLM 500-Best 32.07 28.65
adaptation with MT Hypo
1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83
Supervised ASR 1gramLM 500-Best 32.48 28.59
adaptation with MT Hypo
1gramLM transcription 1gramLM MT Ref 32.68 28.60
Table 5: Performance in BLEU of joint adaptation.
score. The next step in validating the adaptation frame-
work was to check if the joint adaptation of ASR and
SMT on day 1 data will lead to improvements on day
2. Table 5 shows the combination of ASR and SMT
adaptation methods. Improvements are obtained by us-
ing both ASR and SMT adaptation. Joint adaptation con-
sistently gained more than one BLEU point improvement
on day 2. Our best system is unsupervised ASR adapta-
tion via 1gramLM of ASR day 1 transcription coupled
with supervised SMT adaptation via 1gramLM of day
1 translation references. An interesting result is that to
have a better result on day 2 our approach only requires
translation references on day 1. We selected 1gramLM
of 500-best MT hypotheses to conduct the experiments
since there is no significant difference between 1gramLM
and 2gramLM on day 2 as showed in Table 3.
6 Selective Adaptation
The previous results indicate that we require human
translation references on day 1 data to get improved per-
formance on day 2. However, our goal is to make a better
system on day 2 but try to minimize human efforts on day
1. Therefore, we raise two questions: 1) Can we still ob-
tain improvements by not using all of day 1 data? and 2)
Can we obtain more improvements?
To answer these questions we performed oracle exper-
iments when we take the translation hypotheses on day
1 of the baseline SMT and compare them with transla-
tion references, then select sentences which have BLEU
scores higher than a threshold. The subset of day 1 sen-
tences is used to perform supervised adaptation in a sim-
ilar way showed in section 5. These experiments also
simulate the situation when we have a perfect confidence
score for machine translation hypothesis selection. Table
6 shows results when we use various portions of day 1 to
perform adaptation. By using day 1 sentences which have
smoothed sentence BLEU scores higher than 10 or 20 we
have very close performance with adaptation by using all
day 1 data. The results also show that by using 416 sen-
tences which have sentence BLEU score higher than 40
on day 1, our adapted translation components outperform
the baseline. Performance starts degrading after 50. Ex-
perimental results lead to the answer for question 1) that
151
by using less day 1 data our adapted translation compo-
nents still obtain improvements compare with the base-
line, and 2) we did not see that using less data will lead
us to a better performance compare with using all day 1
data.
No. sents Day 1 Day 2
Baseline 29.39 27.41
? 0 864 30.27 28.29
? 10 797 31.15 28.27
? 20 747 30.81 28.24
? 30 585 30.04 27.71
? 40 416 29.72 27.65
? 50 296 30.06 27.04
Correct 98 29.18 27.19
Table 6: Performance in BLEU of selective adaptation
W i c o n t r o l e M & L g
h r n u
d
r c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  
h r n u c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  Tutorial Abstracts of ACL-08: HLT, page 2,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Building Practical Spoken Dialog Systems 
Antoine Raux1, Brian Langner2, Alan W Black3, Maxine Eskenazi4 Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA {antoine,blangner,awb,max}@cs.cmu.edu 
                                                           1 http://www.cs.cmu.edu/~antoine 2 http://www.cs.cmu.edu/~blangner 3 http://www.cs.cmu.edu/~awb 4 http://www.cs.cmu.edu/~max 
 
 
1 Abstract This tutorial will give a practical description of the free software Carnegie Mellon Olympus 2 Spoken Dialog Architecture. Building real working dialog systems that are robust enough for the general pub-lic to use is difficult. Most frequently, the func-tionality of the conversations is severely limited - down to simple question-answer pairs. While off-the-shelf toolkits help the development of such simple systems, they do not support more ad-vanced, natural dialogs nor do they offer the trans-parency and flexibility required by computational linguistic researchers.  However, Olympus 2 offers a complete dialog system with automatic speech recognition (Sphinx) and synthesis (SAPI, Festi-val) and has been used, along with previous ver-sions of Olympus, for teaching and research at Carnegie Mellon and elsewhere for some 5 years. Overall, a dozen dialog systems have been built using various versions of Olympus, handling tasks ranging from providing bus schedule information to guidance through maintenance procedures for complex machinery, to personal calendar manage-ment. In addition to simplifying the development of dialog systems, Olympus provides a transparent platform for teaching and conducting research on all aspects of dialog systems, including speech rec-ognition and synthesis, natural language under-standing and generation, and dialog and interaction management. The tutorial will give a brief introduction to spoken dialog systems before going into detail 
about how to create your own dialog system within Olympus 2, using the Let's Go bus information system as an example. Further, we will provide guidelines on how to use an actual deployed spo-ken dialog system such as Let's Go to validate re-search results in the real world. As a possible testbed for such research, we will describe Let's Go Lab, which provides access to both the Let's Go system and its genuine user population for research experiments. 2 Outline Part 1 1.1 Introduction 1.2 Overview of current spoken dialog  system architectures 1.3 Description of the Olympus2 dialog  architecture 1.4 How to build an Olympus2 spoken  dialog system Part 2 2.1 Advanced Topics a. Improving ASR b. Improving TTS c. Dealing with ASR Errors d. Logs and Tools 2.2 Using Olympus2 for research and  applications 2.3 Final summary 
2
 	
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 337?340,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
?Vm?????m??%<S?j?m??V<SSm?jm??
? ??
? ?
?
???"<9?
P??Vm?g%mS~??g???mm9V?<?~? S<?j?<jm??"?9?m??%?j? Vm? %?"?~?9%??? ?g? ??V<SSm?jm???V<??VmS?m~?g?9???Vm?g?9???<? g%mS~??<SS???%?j?~m<%Sm~? 9??<"%??????g? ??m?? <?~?m9V?%6?m??? ?"%?j%?j? ?m?? m?m"?? %???Vm? g%mS~?? <?~? g<9%S%<%?j? <~M<?9mm???%?? 9?"m?"m?m<"9Vd? ??SV??jV? Vm? %~m<??g? <?????m?? ~%<S?j?m? 9V<SSm?jm? V<?? ?mm?? ~%??9???m~? g?"???m?%m?? V%?? %?? Vm? g%"??<?m?? ???"%?j? Vm?m?~%?9???%???? ?jmVm"?<?~?<?m?9??9"mm?<9%??d???
{? ??%M<%??? g?"? <? 9m?"<S%?m~? ?V<S?Sm?jm?
?Vm? %~m<? ?g? <? 9m?"<S%?m~? 9V<SSm?jm? %?? Vm? g%mS~???g? ??mm9V? <?~? S<?j?<jm??Vm"m? ~%ggm"m?? ??m??<?~? m9V?%6?m?? <"m? <??S%m~? ?? Vm? ?<m? ~<<?V<???mm???%V? ??? g?"? ??m? %md? ??"??<?S? Vm????Sm?jV?<?~???99m??g?S?9V<SSm?jm?%??Vm??????g??~m~? ??mm9V? "m9?j?%%??? ?m? ?g? 9V<SSm?jm?? +B??V%9V??<"m~?%??Vm?{Yp???<?~?9<m???<??m<??%??Vm?{YY??d? ??Vm??%SS?9??%??m? V"??jV?~%ggm"m???"?j"<???<?~?%????%m??g??m%?j?9"%%9%?m~?g?"? g??9??%?j? ???<%9? ??mm9V? ?"?9m??%?j? ?????? ????%?Sm? m<??"m?? ?g? ??99m???? %? %?? 9Sm<"? V<? Vm?V<Mm?VmS?m~?<?m?????g??~<m?<SS??mm"d????Vm? %??"<?? ??%??? %?? <? ??99m??g?S? 9V<S?Sm?jm?<"m??<??mSS?~mg%?m~?<???V<?Vm?9???%?9???%~m"?? ?? ?m? 9V<SSm?j%?j? g?"? Vm? 9?""m?? ?<m??g? Vm?<"????gg%9%m?? ???m"??g??<"%9%?<????%V?M<"%m~???m?????V<?Vm??m"g?"<?9m??g?~%ggm"?m?? m9V?%6?m?? 9<?? ?m? mM<S?<m~?? jm?m"<SS? <9?9m?m~?mM<S?<%?????99m???9"%m"%<??<?~?<??m??m??g?9?SS<??"<%????m?mm???<"%9%?<??????V<?~m<%S???g? Vm%"? m?"%m?? <? ?m? "m<???<?S? ?V<"m~d? ? P? %??%??"<??V<?Vm?9V<SSm?jm?~?m??????m9?m?<????"%jV? 9??m%%???? ??? ?"%M%?j? ???"?~?9m? Vm???m?????m?m?9??"<jm??m?S?"%?j??m??%~m<?d??
(? P?"?~?9%???
 %V%?? Vm? g%mS~? ?g? ????m?? ~%<S?j?m? ??m???m"V<??? Vm? ??? ?%%S<"? 9??"~%?<m~? mM<S?<%????g? ?S%?Sm? ??m?? ?<?? ?<"? ?g? Vm? ?????g??~m~? ????%9<?"? ?"?j"<d? ? ?V%?? ?"?j"<?j<Mm? ????m?? ~%<S?j?m? ??m? <99m??? ?? <? gS%jV?%?g?"<%???<?~?????%?j???md??P??(????(??{?<????m"? ?g? ??m?? ~mMmS??m~? <?? ?<"? ?g? Vm??"?j"<? ?m"m? 9m?"<SS? mM<S?<m~? ??P??d? ? ?????m"??g? mm"?<S?9<SSm"??<99m??m~? m<9V???m?<?~? Vm? "m??S?? ?m"m? ???S%?Vm~? %?? YBd? ? ?%?9m?????m?? ~%<S?j?m? ??m?? ~?? ??? V<Mm? <?? 9Sm<"? <?m<??"m? ?g? ??99m??? <?? ?????? <? ???m"? ?g?~%ggm"m??m<??"m???m"m?~m?%j?m~????V???m9%Mm?<?~? ???m9%Mmd? ? ??~? ?Vm"? <?<S?m?? ?g? Vm?mM<S?<%??? ?m"m? <~m? ?? <? ???m"? ?g? j"?????%?9S?~%?j?{Bd??
Vm"? 9??m%%???? V<Mm? ?mm?? ~mMmS??m~?%?9S?~%?j?Vm??m??m"??"%?m?B???V%9V?<~~"m??m??Vm? %???m?? ?g? ?<? Sm<?? ?"%j%?<SS?? m??<?m~?9??Mm"?<%???? V<? <m?? ?? ?<??? Vm? ??"%?j??m?d? ? ?V%?? m<"?? V??mMm"?? Vm??m??m"? ?"%?m? %???m%?j?VmS~? %?? 9????9%????%V? P?m"??mm9V?(??Y?<?~??%SS?V<Mm?<???mm9V?9????m?d?
+? ?"????m~?????m??%<S?j?m??V<SSm?jm?
?SV??jV? %? %?? 9Sm<"? Vm"m? <"m? <?? ????%?Sm?9V?%9m??g?"?V%??9V<SSm?jm??m?gmmS?Vm?g%"??"???~??g? Vm? 9V<SSm?jm? ?V??S~? ?m? ?%?Sm? <?~? 9Sm<"?? <S?S??%?j? g?"? Vm? ?m?? ?g? <???? ?? j"??? %?? M<"%m??Mm"?Vm?m<"?d???"?%?%%<S??"????<S???%S~???????"?????m?m"%m?9m?%????%S~%?j?~m?S?m~?????m??~%<?S?j?m???m?d?? ?"???m?V<??<S"m<~?9?SSm9m~?<?S<"jm????m"??g?m<?Sm?~%<S?j?m??V<?9<???m???m~? g?"? "<%?%?j? <?~?<~m? <M<%S<?Sm? %?? Vm? g%"??m<"????<"%9%?<??d????Vm? <"jm? ~?<%?? %?? ???? %?g?"<%??d? ? ?V%???<??9V??m???m9<??m?%?%??<?????S<"?~?<%?????m~????mMm"<S?~%ggm"m?? ~%<S?j?m???m?d? ? P? %????m?g?S???Vm????S%9?<?~?~?m???"m6?%"m?~m<S%?j??%V??m??%%Mm? %?g?"<%??d? ? ?Vm? ??m? %?? ?%?Sm?m???jV? V<? %? 9<?? ?m? "m?%?Smm?m~? ?%V???
?S<?? ? S<9??<?~??<%?m? ??m?<?%?<?j?<jm??m9V??S?j%m??P??%?m??<"?mj%m??mSS??? ?%Mm"?%???%???"jV?????? ???<???< 9?d9?dm~???<mB?
?
337
mm??%Mm?<~~%%??<S???"?d??m?V<Mm?<S"m<~?9?S?Sm9m~? ?Mm"?	
????? ~m%~m?%g%m~? ~%<S?j?m?? %?? Vm?~?<%?? V<? ?m? 9<?? ~%?"%??m? g?"? "<%?%?j? ??"????m?d??Vm? ?m9??~? %??"<??~m<%S? ??~mg%?m? %???V<??m?m?m9??g?Vm??<"%9%?<????Vm?<???d???Vm"m?%??<??%~m?M<"%m??g?"m?m<"9V?%?m"m???%??Vm?9?""m??????m??~%<S?j?m? g%mS~d? P? %?? %????%?Sm? g?"?<??%??jSm?<??????"?M%~m???%<?Sm?9V<SSm?jm??g?"?mMm"???m?%??%??g%"??%m"<%???????%?%??%??"<????g%?~?<?<???%???V%9V?<?????<?%<S?????m?9<??9??mmd?%Mm?? Vm? ???? %?g?"<%??? <??? <?? Vm? ????%~m?"m<9V%?j?g?"?Vm?g%"??m<"???m??mm?V"mm?SmM?mS???g??<"%9%?<%????{d??%S~? <? ???? %?g?"<%??? ??m? ?%V???"?????m??~%<S?j?m?<"9V%m9?"m?(d? ?%S~? <? ???? %?g?"<%??? ??m? ?%V?g"mm? ??g?<"m? ??S?? ??9V? <??S????PPd?+d? ?<?m? Vm? m%?%?j?m?? ?????P?g?"?<%??? ??m? <?~? <~<?? %??%V? ??"?9????m??d??Vm?m?V"mm?SmMmS???ggm"?<?9m"<%??<?????g?g"mm?~?? ?? Vm??<"%9%?<??d? ? Pg? ?<"%9%?<??? <"m???S?%?m"m?m~? %?? ??m? ?<SS? ?<"? ?g? Vm? ~%<S?j?m? <???Vm?9<??<~~m~??m??9????m?????Vm?m?? ??
??? P?g?"<%??? ??m? 	B?? ??9V? <?? <? ?m?? "m9??j?%?m"?? <? ?m?? ??Vm?%?m"?? ?"? <~~"m??? 9?"m? ~%<?S?j?9????m???S%?m?m""?"?"m9?Mm"?m9d???Vm?%?%?%<S? S<?j?<jm? ???S~? ?m? ?????jS%?V? ??? ?m????S~?S%?m???mm?~?V%??%??S<m"?m<"?d?
+d{? M<S?<%???
M<S?<%??? ?g? ????m?? ~%<S?j?m? ??m?? %?? ?%SS?Mm"? ?9V? <? "m?m<"9V? %???m? <?~? ?m? ?mm? Vm?????m?? %<S?j?m? ?V<SSm?jm? <?? <? m9V<?%?? ??<%~?V<?<"m<??g?"m?m<"9Vd???SS??%?j??"?9?"m??%??Vm???mm9V???Vm?%??S%??<"~??V<SSm?jm?(B???m??"????m? ??V<Mm?<????m"??g? ~%ggm"m??j"??????g???m"??9<SS?Vm????%m~???m?d?"????{???%<S?j??m?m<"9Vm"???m<9V??<"%9%?<??j"?????%SS??"?M%~m~?<????m"??g?9<SSm"???V???%SS??m?j%Mm???9m?<"%???<?~?<??m~? ??9<SS???m??g?Vm??<"%9%?<%?j???m?d? ?%<S?j?m??m?m<"9Vm"??<"m??"??<?S? Vm??m????m"???g? <???m?<?~?<"m? <S???Vm???? ?m"???<SS? %?m"m?m~? %?? 9??Sm%?j? Vm?<??d?"???? (? ? <%Mm? ??m<?m"? ?~m"j"<~?<m???V%?? j"???? ?%SS? ?m? ?<%~? <?~? %?m?~m~? ?? ?m? Vm????V??jm?m????j"?????g?9<SSm"?d???"????+??? ?S??mm"?????"m6?m?%?j?g?"?M?S???mm"?? V"??jV?<%S%?j? S%??? <?~? Vm? ?m?? ?m? ?%SS?9?SSm9?Vm?V%"~??m??g?9<SSm"?d??
<M%?j? V"mm? ?m?? ?g? 9<SSm"???%SS? m?<?Sm? ??? ???m"g?"? 9?""mS<%??? ?m?mm?? Vm? j"????? <?~?Vm"mg?"m?"???g%?~?"mS%<?Sm??<%?%9?d??
m?<S????"????m? ??"??? Vm??V<SSm?jm???? ???SmMmS?d???SS??<"%9%?<%?j???m???%SS?<?m??<"?%??Vm?m?%?%%<S?mM<S?<%???d???Vm??m????"?????<?Sm????m?? 9<?? Vm?? ?m? ~m?S?m~? ??? Vm?m????
%Mm? ??m? ?V%9V? ?"?M%~m?? ???? %?g?"<%??? ??Vm? ?m??Sm? ?g? ?%???"jVd? ? ?V??? "m<S? ??m"??? ?V??<"m? %?m"m?m~? %?? Vm? %m? ?g? Vm? ?m? ???? "<Vm"?V<?? Vm? ??99m??? ?g? Vm? ~%<S?j?m? ??m?? ?%SS??"?M%~m?<??<~~%%??<S??m??g?9<SSm"??<%?%9?d??Vm? g%"?? j"????? ?g? ??m"?? ?%SS? ?m? j%Mm???9m?<"%??? ?Vm? <"m? ??S%?mS? ?? ?m? g<%S%<"? ?%V??%???"jV? ????m???? <?~? ?%SS? g%SS? %?? <? ?m??6?m?%???<%"m? <gm"? m<9V? 9<SSd? ??"? Vm? S%Mm? m???%V?"m<S???m"?????6?m?%???<%"m??%SS??m?????%?Smd??m<S? ??m"?? <"m? ??%?m"m?m~? %?? <???m"%?j? <??g?"Vm"? 6?m?%????? <?~? <? ~m?%j?? V<? jm?? ??m?????m? ?? g%SS? %?? 6?m?%???<%"m?? ???S~? ?"??<?S?%mS~?<?~%ggm"m??"m??S?V<???<?~<"~?"m<S???m"?d?
M<S?<%????%SS??m? V"??jV???SPEECHALATOR: TWO-WAY SPEECH-TO-SPEECH TRANSLATION IN YOUR HAND
Alex Waibel
 
, Ahmed Badran
 
, Alan W Black
 
, Robert Frederking
 
, Donna Gates
 
Alon Lavie
 
, Lori Levin
 
, Kevin Lenzo

, Laura Mayfield Tomokiyo
Juergen Reichert

, Tanja Schultz   , Dorcas Wallace   , Monika Woszczyna , Jing Zhang
 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA

Cepstral, LLC,

Multimodal Technologies Inc,

Mobile Technologies Inc.
speechalator@speechinfo.org
ABSTRACT
This demonstration involves two-way automatic speech-
to-speech translation on a consumer off-the-shelf PDA. This
work was done as part of the DARPA-funded Babylon project,
investigating better speech-to-speech translation systems for
communication in the field. The development of the Speecha-
lator software-based translation system required addressing
a number of hard issues, including a new language for the
team (Egyptian Arabic), close integration on a small device,
computational efficiency on a limited platform, and scalable
coverage for the domain.
1. BACKGROUND
The Speechalator was developed in part as the next genera-
tion of automatic voice translation systems. The Phrasalator
is a one-way device that can recognize a set of pre-defined
phrases and play a recorded translation, [1]. This device
can be ported easily to new languages, requiring only a
hand translation of the phrases and a set of recorded sen-
tences. However, such a system severely limits communica-
tion as the translation is one way, thus reducing one party?s
responses to simple pointing and perhaps yes and no.
The Babylon project addresses the issues of two-way
communication where either party can use the device for
conversation. A number of different groups throughout the
US were asked to address specific aspects of the task, such
as different languages, translation techniques and platform
specifications. The Pittsburgh group was presented with
three challenges. First, we were to work with Arabic, a lan-
guage with which the group had little experience, to test our
capabilities in moving to new languages quickly. Second,
we were instructed to use an interlingua approach to trans-
lation, where the source language is translated into an in-
termediate form that is shared between all languages. This
step streamlines expansion to new languages, and CMU has
a long history in working with interlingua based translation
systems. Third, we were constrained to one portable PDA-
class device to host the entire two-way system: two recog-
nizers, two translation engines, and two synthesizers.
2. RECOGNITION
We used an HMM-based recognizer, developed by Multi-
modal Technologies Inc, which has been specifically tuned
for PDAs. The recognizer allows a grammar to be tightly
coupled with the recognizer, which offers important effi-
ciencies considering the limited computational power of the
device. With only minor modification we were able to gen-
erate our interlingua interchange format (IF) representation
directly as output from the recognizer, removing one mod-
ule from the process.
MTI?s recognizer requires under 1M of memory with
acoustic models of around 3M per language. Special op-
timizations deal with the slow processor and ensure low
use of memory during decoding. The Arabic models were
bootstrapped from the GlobalPhone [2] Arabic collections
as well as data collected as part of this project.
3. TRANSLATION
As part of this work we investigated two different tech-
niques for translation, both interlingua based. The first was
purely knowledge-based, following our previous work [3].
The engine developed for this was too large to run on the
device, although we were able to run the generation part off-
line seamlessly connected by a wireless link from the hand-
held device. The second technique we investigated used
a statistical training method to build a model to translate
structured interlingua IF to text in the target language. Be-
cause this approach was developed with the handheld in
mind, it is efficient enough to run directly on the device,
and is used in this demo.
4. SYNTHESIS
The synthesis engine is Cepstral?s Theta system. As the
Speechalator runs on very small hardware devices (at least
small compared to standard desktops), it was important that
the synthesis footprint remained as small as possible.
The speechalator is to be used for people with little ex-
posure to synthetic speech, and the output quality must be
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 29-30
                                                         Proceedings of HLT-NAACL 2003
very high. Cepstral?s unit selection voices, tailored to the
domain, meet the requirements for both quality and size.
Normal unit selection voices may take hundreds of megabytes,
but the 11KHz voices developed by Cepstral were around 9
megabytes each.
5. ARABIC
The Arabic language poses a number of challenges for any
speech translation system. The first problem is the wide
range of dialects of the language. Just as Jamaican and
Glaswegian speakers may find it difficult to understand each
other?s dialect of English, Arabic speakers of different di-
alects may find it impossible to communicate.
Modern Standard Arabic (MSA) is well-defined and widely
understood by educated speakers across the Arab world.
MSA is principally a written language and not a spoken lan-
guage, however. Our interest was in dealing with a normal
spoken dialect, and we chose Egyptian Arabic; speakers of
that dialect were readily accessible to us, and media influ-
ences have made it perhaps the most broadly understood of
the regional dialects.
Another feature of Arabic is that the written form, ex-
cept in specific rare cases, does not include vowels. For
speech recognition and synthesis, this makes pronunciations
hard. Solutions have been tested for recognition where the
vowels are not explicitly modeled, but implicitly modeled
by context. This would not work well for synthesis; we have
defined an internal romanization, based on the CallHome
[4] romanization, from which full phonetic forms can easily
be derived. This romanization is suitable for both recog-
nizer and synthesis systems, and can easily be transformed
into the Arabic script for display.
6. SYSTEM
The end-to-end system runs on a standard Pocket PC de-
vice. We have tested it on a number of different machines,
including various HP (Compaq) iPaq machines (38xx 39xx)
and Dell Axims. It can run on 32M machines, but runs best
on a 64M machine with about 40M made available for pro-
gram space. Time from the end of spoken input to start of
translated speech is around 2-4 seconds depending on the
length of the sentence and the actual processor. We have
found StrongARM 206MHz processors, found on the older
Pocket PCs, slightly faster than XScale 400MHz, though no
optimization for the newer processors has been attempted.
Upon startup, the user is presented with the screen as
shown in Figure 1. A push-to-talk button is used and the
speaker speaks in his language. The recognized utterance
is first displayed, with the translation following, and the ut-
terance is then spoken in the target language. Buttons are
provided for replaying the output and for switching the in-
put to the other language.
7. DISCUSSION
The current demonstration is designed for the medical inter-
view domain, with the doctor speaking English and the pa-
tient speaking Arabic. At this point in the project no formal
evaluation has taken place. However, informally, in office-
like acoustic environments, accuracy within domain is well
over 80%.
Arabic input Screen
Speechalator snapshot
8. REFERENCES
[1] Sarich, A., ?Phraselator, one-way speech translation
system,? http://www.sarich.com/translator/, 2001.
[2] T. Schultz and A. Waibel, ?The globalphone project:
Multilingual lvcsr with janus-3,? in Multilingual Infor-
mation Retrieval Dialogs: 2nd SQEL Workshop, Plzen,
Czech Republic, 1997, pp. 20?27.
[3] A. Lavie, et al ?A multi-perspective evaluation of
the NESPOLE! speech-to-speech translation system,?
in Proceedings of ACL 2002 workshop on Speech-to-
speech Translation: Algorithms and Systems, Philadel-
phia, PA., 2002.
[4] Linguistic Data Consortium, ?Callhome egyptian ara-
bic speech,? 1997.
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 962?971, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Entropy-based Pruning for Phrase-based Machine Translation
Wang Ling, Joa?o Grac?a, Isabel Trancoso, Alan Black
L2F Spoken Systems Lab, INESC-ID, Lisboa, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
{wang.ling,joao.graca,isabel.trancoso}@inesc-id.pt
awb@cs.cmu.edu
Abstract
Phrase-based machine translation models
have shown to yield better translations than
Word-based models, since phrase pairs en-
code the contextual information that is needed
for a more accurate translation. However,
many phrase pairs do not encode any rele-
vant context, which means that the transla-
tion event encoded in that phrase pair is led
by smaller translation events that are indepen-
dent from each other, and can be found on
smaller phrase pairs, with little or no loss in
translation accuracy. In this work, we pro-
pose a relative entropy model for translation
models, that measures how likely a phrase pair
encodes a translation event that is derivable
using smaller translation events with similar
probabilities. This model is then applied to
phrase table pruning. Tests show that con-
siderable amounts of phrase pairs can be ex-
cluded, without much impact on the transla-
tion quality. In fact, we show that better trans-
lations can be obtained using our pruned mod-
els, due to the compression of the search space
during decoding.
1 Introduction
Phrase-based Machine Translation Models (Koehn
et al 2003) model n-to-m translations of n source
words to m target words, which are encoded in
phrase pairs and stored in the translation model.
This approach has an advantage over Word-based
Translation Models (Brown et al 1993), since trans-
lating multiple source words allows the context for
each source word to be considered during trans-
lation. For instance, the translation of the En-
glish word ?in? by itself to Portuguese is not ob-
vious, since we do not have any context for the
word. This word can be translated in the con-
text of ?in (the box)? to ?dentro?, or in the con-
text of ?in (China)? as ?na?. In fact, the lexical
entry for ?in? has more than 10 good translations
in Portuguese. Consequently, the lexical translation
entry for Word-based models splits the probabilis-
tic mass between different translations, leaving the
choice based on context to the language model. On
the other hand, in Phrase-based Models, we would
have a phrase pair p(in the box, dentro da caixa)
and p(in china, na china), where the words ?in the
box? and ?in China? can be translated together to
?dentro da caixa? and ?na China?, which substan-
tially reduces the ambiguity. In this case, both the
translation and language models contribute to find
the best translation based on the local context, which
generally leads to better translations.
However, not all words add the same amount of
contextual information. Using the same example for
?in?, if we add the context ?(hid the key) in?, it is
still not possible to accurately identify the best trans-
lation for the word ?in?. The phrase extraction algo-
rithm (Ling et al 2010) does not discriminate which
phrases pairs encode contextual information, and ex-
tracts all phrase pairs with consistent alignments.
Hence, phrases that add no contextual information,
such as, p(hid the key in, escondeu a chave na)
and p(hid the key in, escondeu a chave dentro)
are extracted. This is undesirable because we are
populating translation models with redundant phrase
pairs, whose translations can be obtained using com-
962
binations of other phrases with the same probabil-
ities, namely p(hid the key, escondeu a chave),
p(in, dentro) and p(in, na). This is a problem
that is also found in language modeling, where
large amounts of redundant higher-order n-grams
can make the model needlessly large. For backoff
language models, multiple pruning strategies based
on relative entropy have been proposed (Seymore
and Rosenfeld, 1996) (Stolcke, 1998), where the ob-
jective is to prune n-grams in a way to minimize the
relative entropy between the model before and after
pruning.
While the concept of using relative entropy for
pruning is not new and frequently used in backoff
language models, there are no such models for ma-
chine translation. Thus, the main contribution of
our work is to propose a relative entropy pruning
model for translation models used in Phrase-based
Machine Translation. It is shown that our pruning
algorithm can eliminate phrase pairs with little or
no impact in the predictions made in our translation
model. In fact, by reducing the search space, less
search errors are made during decoding, which leads
to improvements in translation quality.
This paper is organized as follows. We describe
and contrast the state of the art pruning algorithms
in section 2. In section 3, we describe our relative-
entropy model for machine translation. Afterwards,
in section 4, we apply our model for pruning in
Phrase-based Machine Translation systems. We per-
form experiments with our pruning algorithm based
on phrase pair independence and analyse the results
in section 5. Finally, we conclude in section 6.
2 Phrase Table Pruning
Phrase table pruning algorithms are important in
translation, since they efficiently reduce the size of
the translation model, without having a large nega-
tive impact in the translation quality. This is espe-
cially relevant in environments where memory con-
straints are imposed, such as translation systems for
small devices like cellphones, and also when time
constraints for the translation are defined, such as
online Speech-to-Speech systems.
2.1 Significance Pruning
A relevant reference in phrase table pruning is the
work of (Johnson and Martin, 2007), where it is
shown that a significant portion of the phrase ta-
ble can be discarded without a considerable negative
impact on translation quality, or even positive one.
This work computes the probability, named p-value,
that the joint occurrence event of the source phrase
s and target phrase t occurring in same sentence pair
happens by chance, and are actually statistically in-
dependent. Phrase pairs that have a high p-value,
are more likely to be spurious and more prone to
be pruned. This work is followed in (Tomeh et al
2009), where phrase pairs are treated discriminately
based on their complexity. Significance-based prun-
ing has also been successfully applied in language
modeling in (Moore and Quirk, 2009).
Our work has a similar objective, but instead
of trying to predict the independence between the
source and target phrases in each phrase pair, we at-
tempt to predict the independence between a phrase
pair and other phrase pairs in the model.
2.2 Relevance Pruning
Another proposed approach (Matthias Eck and
Waibel, 2007) consists at collecting usage statistics
for phrase pairs. This algorithm decodes the train-
ing corpora and extracts the number of times each
phrase pair is used in the 1-best translation hypoth-
esis. Thus, phrase pairs that are rarely used during
decoding are excluded first during pruning.
This method considers the relationship between
phrase pairs in the model, since it tests whether
the decoder is more prone to use some phrase pairs
than others. However, it leads to some undesirable
pruning choices. Let us consider a source phrase
?the box in China? and 2 translation hypotheses,
where the first hypothesis uses the phrase transla-
tion p(the key in China, a chave na China) with
probability 70%, and the second hypothesis uses
two phrase translations p(the key, a chave) and
p(in China, na China) with probability 65%. This
approach will lean towards pruning the phrase pairs
in the second hypothesis, since the decoder will use
the first hypothesis. This is generally not desired,
since the 2 smaller phrase pairs can be used to trans-
late the same source sentence with a small probabil-
963
ity loss (5%), even if the longer phrase is pruned.
On the other hand, if the smaller phrases are pruned,
the longer phrase can not be used to translate smaller
chunks, such as ?the key in Portugal?. This matter is
aggravated due to the fact that the training corpora is
used to decode, so longer phrase pairs will be used
more frequently than when translating unseen sen-
tences, which will make the model more biased into
pruning shorter phrase pairs.
3 Relative Entropy Model For
Phrase-based Translation Models
In this section, we shall define our entropy model
for phrase pairs. We start by introducing some no-
tation to distinguish different types of phrase pairs
and show why some phrase pairs are more redun-
dant than others. Afterwards, we illustrate our no-
tion of relative entropy between phrase pairs. Then,
we describe our entropy model, its computation and
its application to phrase table pruning.
3.1 Atomic and Composite Phrase Pairs
We discriminate between 2 types of phrase pairs:
atomic phrase pairs and composite phrase pairs.
Atomic phrase pairs define the smallest transla-
tion units, such that given an atomic phrase pair that
translates from s to t, the same translation cannot
be obtained using any combination of other phrase
pairs. Removing these phrase pairs reduces the
range of translations that our model is capable of
translating and also the possible translations.
Composite phrase pairs define translations of a
given sequence of words that can also be obtained
using atomic or other smaller composite phrase
pairs. Each combination is called a derivation or
translation hypothesis. Removing these phrase pairs
does not change the amount of sentences that the
model can translate, since all translations encoded
in these phrases can still be translated using other
phrases, but these will lead to different translation
probabilities.
Considering table 1, we can see that atomic
phrases encode one elementary translation event,
while composite phrases encode joint events that are
encoded in atomic phrase pairs. If we look at the
source phrase ?in?, there is a multitude of possible
translations for this word in most target languages.
Taking Portuguese as the target language, the proba-
bility that ?in? is translated to ?em? is relatively low,
since it can also be translated to ?no?, ?na?, ?den-
tro?, ?dentro de? and many others.
However, if we add another word such as ?Por-
tugal? forming ?in Portugal?, it is more likely that
?in? is translated to ?em?. Thus, we define the
joint event of ?in? translating to ?em? (A1) and
?Portugal? to ?Portugal? (B1), denoted as A1 ? B1,
in the phrase pair p(in Portugal, em Portugal).
Without this phrase pair it is assumed that these
are independent events with probability given by
P (A1)P (B1)1, which would be 10%, leading to a
60% reduction. In this case, it would be more likely,
that in Portugal is translated to no Portugal or
na Portugal, which would be incorrect.
Some words, such as ?John?, forming ?John in?,
do not influence the translations for the word ?in?,
since it can still be translated to ?em?, ?no?, ?na?,
?dentro? or ?dentro de? depending on the word that
follows. By definition, if the presence of phrase
p(John, John) does not influence the translation of
p(in, em) and viceversa, we can say that probability
of the joint event P (A1?C1) is equal to the product
of the probabilities of the events P (A1)P (C1).
If we were given a choice of pruning either the
composite phrase pairs p(John in, John em) or
p(in Portugal, em Portugal), the obvious choice
would be the former, since the probability of the
event encoded in that phrase pair is composed by 2
independent events, in which case the decoder will
inherently consider the hypothesis that ?John in? is
translated to ?John em? with the same probability. In
another words, the model?s predictions even, with-
out this phrase pair will remain the same.
The example above shows an extreme case,
where the event encoded in the phrase pair
p(John in, John em) is decomposed into indepen-
dent events, and can be removed without chang-
ing the model?s prediction. However, finding and
pruning phrase pairs that are independent, based on
smaller events is impractical, since most translation
events are not strictly independent. However, many
phrase pairs can be replaced with derivations using
smaller phrases with a small loss in the model?s pre-
1For simplicity, we assume at this stage that no reordering
model is used
964
Phrase Pair Prob Event
Atomic Phrase Pairs
in? em 10% A1
in? na 20% A2
in? no 20% A3
in? dentro 5% A4
in? dentro de 5% A5
Portugal? Portugal 100% B1
John? John 100% C1
Composite Phrase Pairs
in Portugal? em Portugal 70% A1 ?B1
John in? John em 10% C1 ?A1
John in? John na 20% C1 ?A2
John in? John no 20% C1 ?A3
John in? John dentro 5% C1 ?A4
John in? John dentro de 5% C1 ?A5
Table 1: Phrase Translation Table with associated events
dictions.
Hence, we would like to define a metric for phrase
pairs that allows us evaluate how discarding each
phrase pair will affect the pruned model?s predic-
tions. By removing phrase pairs that can be derived
using smaller phrase pairs with similar probability,
it is possible to discard a significant portion of the
translation model, while minimizing the impact on
the model?s predictions.
3.2 Relative Entropy Model for Machine
Translation
For each phrase pair pa, we define the supporting
set SP (pa(s, t)) = S1, ..., Sk, where each element
Si = pi, ..., pj is a distinct derivation of pa(s, t) that
translates s to t, with probability P (Si) = P (pi) ?
...?P (pj). A phrase pair can have multiple elements
in its supporting set. For instance, the phrase pair
p(John in Portugal, John em Portugal), has 3
elements in the support set:
? S1 = {p(John, John), p(in, em), p(Portugal, Portugal)}
? S2 = {p(John, John), p(in Portugal, em Portugal)}
? S3 = {p(John in, John em), p(Portugal, Portugal)}
S1, S2 and S3 encode 3 different assumptions
about the event of translating ?John in Portugal?
to ?John em Portugal?. S1 assumes that the event
is composed by 3 independent events A1, B1 and
C1, S2 assumes that A1 and B1 are dependent, and
groups them into a single composite event A1 ?B1,
which is independent from C1, and S3 groups A1
and C1 independently from B1. As expected, the
event encoded in the phrase pair p itself isA1?B1?
C1, which assumes thatA1,B1 andC1 are all depen-
dent. We can see that if any of the events S1, S2 or
S3 has a ?similar probability? as the event coded in
the phrase pair, we can remove this phrase pair with
a minimal impact in the phrase prediction.
To formalize our notion of ?similar probabil-
ity?, we apply the relative entropy or the Kullback-
Leibler divergence, and define the divergence be-
tween a pruned translation model Pp(s, t) and the
unpruned model P (s, t) as:
D(Pp||P ) = ?
?
s,t
P (s, t)log
Pp(t|s)
P (t|s)
(1)
Where Pp(t|s)P (t|s) , measures the deviation from the
probability emission from the pruned model and the
original probability from the unpruned model, for
each source-target pair s, t. This is weighted by
the frequency that the pair s, t is observed, given by
P (s, t).
Our objective is to minimize D(Pp||P ), which
can be done locally by removing phrase pairs p(s, t)
with the lowest values for ?P (s, t)logPp(t|s)P (t|s) . Ide-
ally, we would want to minimize the relative entropy
for all possible source and target sentences, rather
than all phrases in our model. However, minimiz-
ing such an objective function would be intractable
due to reordering, since the probability assigned to a
phrase pair in a sentence pair by each model would
depend on the positioning of all other phrase pairs
used in the sentence. Because of these dependen-
cies, we would not be able to reduce this problem to
a local minimization problem. Thus, we assume that
all phrase pairs have the same probability regardless
of their context in a sentence.
Thus, our pruning algorithm takes a threshold ?
and prunes all phrase pairs that fail to meet the fol-
lowing criteria:
?P (s, t)log
Pp(t|s)
P (t|s)
> ? (2)
The main components of this function is the ratio
between the emission from the pruned model and
965
unpruned models given by Pp(t|s)P (t|s) , and the weight
given to each s, t pair given by P (s, t). In the re-
mainder of this section, we will focus on how to
model each of these components in equation 2.
3.3 Computing P (s, t)
The term P (s, t) can be seen as a weighting function
for each s, t pair. There is no obvious optimal dis-
tribution to model P (s, t). In this work, we apply 2
different distributions for P (s, t). First, an uniform
distribution, where all phrases are weighted equally.
Secondly, a multinomial function defined as:
P (s, t) =
N(s, t)
N
(3)
whereN is the number of sentence pairs in the paral-
lel data, and N(s, t) is the number of sentence pairs
where s was observed in the source sentence and t
was observed in the target sentence. Using this dis-
tribution, the model is more biased in pruning phrase
pairs with s, t pairs that do not occur frequently.
3.4 Computing Pp(t|s)P (t|s)
The computation of Pp(t|s)P (t|s) depends on how the de-
coder adapts when a phrase pair is pruned from the
model. In the case of back-off language models,
this can be solved by calculating the difference of
the logs between the n-gram estimate and the back-
off estimate. However, a translation decoder gen-
erally functions differently. In our work, we will
assume that the decoding will be performed using
a Viterbi decoder, such as MOSES (Koehn et al
2007), where the translation with the highest score
is chosen.
In the example above, where s=?John in Portu-
gal? and t=?John em Portugal?, the decoder would
choose the derivation with the highest probability
from s to t. Using the unpruned model, the possi-
ble derivations are either using phrase p(s, t) or one
element of its support set S1, S2 or S3. On the other
hand, on the pruned model where p(s, t) does not
exist, only S1, S2 and S3 can be used. Thus, given
a s, t pair one of three situations may occur. First, if
the probability of the phrase pair p(s, t) is lower than
the highest probability element in SP (p(s, t)), then
both the models will choose that element, in which
case, Pp(t|s)P (t|s) = 1. This can happen, if we define
features that penalize longer phrase pairs, such as
lexical weighting, or if we apply smoothing (Foster
et al 2006). Secondly, if the probability of p(s, t)
is equal to the most likely element in SP (p(s, t)),
regardless of whether the unpruned model choses to
use p(s, t) or that element, the probability emissions
of the pruned and unpruned model will be identi-
cal. Thus, for this case Pp(t|s)P (t|s) = 1. Finally, if the
probability of p(s, t) is higher than other possible
derivations, the unpruned model will choose to emit
the probability of p(s, t), while the pruned model
will emit the most likely element in SP (p(s, t)).
Hence, the probability loss between the 2 models,
will be the ratio between the probability of p(s, t)
and the probability of the most likely element in
SP (p(s, t)).
From the example above, we can generalize the
function for Pp(t|s)P (t|s) as:
?
p??argmax(SP (p(s,t))) P (p
?)
P (p(s, t))
(4)
Where P (p(s, t)) denotes the probability of
p(s, t) and
?
p??argmax(SP (p(s,t))) P (p
?) the most
likely sequence of phrasal translations that translates
s to t, with the probability equal to the product of all
phrase translation probabilities in that sequence.
Replacing in equation 2, our final condition that
must be satisfied for keeping a phrase pair is:
?P (s, t)log
?
p??argmax(SP (p(s,t))) P (p
?)
P (p(s, t))
> ? (5)
4 Application for Phrase-based Machine
Translation
We will now show how we apply our entropy prun-
ing model in the state-of-the-art phrase-based trans-
lation system MOSES and describe the problems
that need to be addressed during the implementation
of this model.
4.1 Translation Model
The translation model in Moses is composed by
a phrase translation model and a phrase reorder-
ing model. The first one models, for each phrase
pair p(s, t), the probability of translating the s to
t by combining multiple features ?i, weighted by
966
wTi , as PT (p) =
?n
i=1 ?i(p)
wTi . The reordering
model is similar, but models the local reordering be-
tween p, given the previous and next phrase accord-
ing to the target side, pP and pN , or more formally,
PR(p|pP , pN ) =
?m
i=1 ?i(p|pP , pP )
wRi
4.2 Building the Support Set
Essentially, implementing our model is equiva-
lent to calculating the components described in
equation 5. These are P (s, t), P (p(s|t)) and
argmax(SP (p(s, t))). Calculating the uniform dis-
tribution and multinomial distributions for P (s, t)
is simple, the uniform distribution just assumes the
same value for all s and t, and the multinomial dis-
tribution can be modeled by extracting counts from
the parallel corpora.
Calculating P (s|t) is also trivial, since it only en-
volves calculating PT (p(s, t)), which can be done
by retrieving the translation features of p and apply-
ing the weights for each feature.
The most challenging task is to calculate
argmax(SP (p(s, t))), which is similar to the de-
coding task in machine translation, where we need to
find the best translation t? for a sentence s, that is, t? =
argmaxtP (s|t)P (t). In practice, we are not search-
ing in the space of possible translations, but in the
space of possible derivations, which are sequences
of phrase translations p1(s1, t1), ..., pn(sn, tn) that
can be applied to s to generate an output t with the
score given by P (t)
?n
i=1 P (si, ti).
Our algorithm to determine SP (p(s, t)) can be
described as an adaptation to the decoding algorithm
in Moses, where we restrict the search space to the
subspace SP (p(s, t)), that is, our search space is
only composed by derivations that output t, with-
out using p itself. This can be done using the forced
decoding algorithm proposed in (Schwartz, 2008).
Secondly, the score of a given translation hypothesis
does not depend on the language model probability
P (t), since all derivations in this search space have
the same t, thus we discard this probability from
the score function. Finally, rather than using beam
search, we exhaustively search all the search space,
to reduce the hypothesis of incurring a search error
at this stage. This is possible, since phrase pairs are
generally smaller than text (less than 8 words), and
because we are constraining the search space to t,
which is an order of magnitude smaller than the reg-
ular search space with all possible translations.
4.3 Pruning Algorithm
The algorithm to generate a pruned translation
model is shown in 1. We iterate over all phrase pairs
p1(s1, t1), ..., pn(sn, tn), decode using our forced
decoding algorithm from si to ti, to obtain the best
path S. If no path is found then it means that the pi
is atomic. Then, we prune pi based on condition 5.
Algorithm 1 Independence Pruning
Require: pruning threshold ?,
unpruned model {p1(s1, t1), ..., pn(sn, tn)}
for pi(si, ti) ? {p1(s1, t1), ..., pn(sn, tn)} do
S := argmax(SP (pi)) \ pi
score :=?
if S 6= {} then
score := ?P (s, t)log
?
p?(s?,t?)?S P (s
?|t?)
P (s|t)
end if
if score ? ? then
prune(pi)
end if
end for
return pruned model
The main bottle neck in this algorithm is find-
ing argmax(SP (pi)). While this appears relatively
simple and similar to a document decoding task, the
size of our task is on a different order of magni-
tude, since we need to decode every phrase pair in
the translation model, which might not be tractable
for large models with millions of phrase pairs. We
address this problem in section 5.3.
Another problem with this algorithm is that the
decision to prune each phrase pair is made assuming
that all other phrase pairs will remain in the model.
Thus, there is a chance a phrase pair p1 is pruned
because of a derivation using p2 and p3 that leads to
the same translation. However, if p3 also happens to
be pruned, such a derivation will no longer be pos-
sible. One possible solution to address this problem
is to perform pruning iteratively, from the smallest
phrase pairs (number of words) and increase the size
at each iteration. However, we find this undesirable,
since the model will be biased into removing smaller
phrase pairs, which are generally more useful, since
they can be used in multiple derivation to replace
larger phrase pairs. In the example above, the model
967
would eliminate p3 and keep p1, yet the best deci-
sion could be to keep p3 and remove p1, if p3 is also
frequently used in derivations of other phrase pairs.
Thus, we leave the problem of finding the best set of
phrases to prune as future work.
5 Experiments
We tested the performance of our system under two
different environments. The first is the small scale
DIALOG translation task for IWSLT 2010 evalua-
tion (Paul et al 2010) using a small corpora for
the Chinese-English language pair (henceforth re-
ferred to as ?IWSLT?). The second one is a large
scale test using the complete EUROPARL (Koehn,
2005) corpora for the Portuguese-English language
pair, which we will denote by ?EUROPARL?.
5.1 Corpus
The IWSLT model was trained with 30K training
sentences. The development corpus and test corpus
were taken from the evaluation dataset in IWSLT
2006 (489 tuning and 500 test sentences with 7 ref-
erences). The EUROPARL model was trained using
the EUROPARL corpora with approximately 1.3M
sentence pairs, leaving out 1K sentences for tuning
and another 1K sentences for tests.
5.2 Setup
In the IWSLT experiment, word alignments were
generated using an HMM model (Vogel et al 1996),
with symmetric posterior constraints (V. Grac?a et
al., 2010), using the Geppetto toolkit2. This setup
was used in the official evaluation in (Ling et al
2010). For the EUROPARL experiment the word
alignments were generated using IBM model 4. In
both experiments, the translation model was built
using the phrase extraction algorithm (Paul et al
2010), with commonly used features in Moses (Ex:
probability, lexical weighting, lexicalized reordering
model). The optimization of the translation model
weights was done using MERT tuning (Och, 2003)
and the results were evaluated using BLEU-4.
5.3 Pruning Setup
Our pruning algorithm is applied after the translation
model weight optimization with MERT. We gener-
2http://code.google.com/p/geppetto/
ate multiple translation models by setting different
values for ?, so that translation models of different
sizes are generated at intervals of 5%. We also run
the significance pruning (Johnson and Martin, 2007)
algorithm in these conditions.
While the IWSLT translation model has only
88,424 phrase pairs, for the EUROPARL exper-
iment, the translation model was composed by
48,762,372 phrase pairs, which had to be decoded.
The average time to decode each phrase pair us-
ing the full translation model is 4 seconds per sen-
tence, since the table must be read from disk due to
its size. This would make translating 48M phrase
pairs unfeasible. To address this problem, we di-
vide the phrase pairs in the translation model into
blocks of K phrase pairs, that are processed sepa-
rately. For each block, we resort to the approach
used in MERT tuning, where the model is filtered to
only include the phrase pairs that are used for trans-
lating tuning sentences. We filter each block with
phrase pairs fromK to 2K with the source sentences
sK , ..., s2K . Furthermore, since we are force de-
coding using the target sentences, we also filter the
remaining translation models using the target sen-
tences tK , ..., t2K . We used blocks of 10,000 phrase
pairs and each filtered table was reduced to less than
1% of the translation table on average, reducing the
average decoding time to 0.03 seconds per sentence.
Furthermore, each block can be processed in parallel
allowing multiple processes to be used for the task,
depending on the resources that are available.
5.4 Results
Figure 1 shows the BLEU results for different sizes
of the translation model for the IWSLT experiment
using the uniform and multinomial distributions for
P (s, t). We observe that there is a range of values
from 65% to 95% where we actually observe im-
provements caused by our pruning algorithm, with
the peak at 85% for the uniform distribution, where
we improve from 15.68 to 15.82 (0.9% improve-
ment). Between 26% and 65%, the BLEU score is
lower than the baseline at 100%, with the minimum
at 26% with 15.54, where only atomic phrase pairs
remain and both the multinomial and uniform distri-
bution have the same performance, obviously. This
is a considerable reduction in phrase table size by
sacrificing 0.14 BLEU points. Regarding the com-
968
15.5	 ?
15.55	 ?
15.6	 ?
15.65	 ?
15.7	 ?
15.75	 ?
15.8	 ?
15.85	 ?
25
%	 ?
30
%	 ?
35
%	 ?
40
%	 ?
45
%	 ?
50
%	 ?
55
%	 ?
60
%	 ?
65
%	 ?
70
%	 ?
75
%	 ?
80
%	 ?
85
%	 ?
90
%	 ?
95
%	 ?
10
0%
	 ?
IWSLT	 ?Results	 ?
Uniform	 ?
Mul?nomial	 ?
Figure 1: Results for the IWSLT experiment. The x-
axis shows the percentage of the phrase table used. The
BLEU scores are shown in the y-axis. Two distributions
for P (s, t) were tested Uniform and Multinomial.
parison between the uniform and multinomial distri-
bution, we can see that both distributions yield sim-
ilar results, specially when a low number of phrase
pairs is pruned. In theory, the multinomial distri-
bution should yield better results, since the pruning
model will prefer to prune phrase pairs that are more
likely to be observed. However, longer phrase pairs,
which tend compete with other long phrase pairs on
which get pruned first. These phrase pairs gener-
ally occur only once or twice, so the multinomial
model will act similarly to the uniform model re-
garding longer phrase pairs. On the other hand, as
the model size reduces, we can see that using multi-
nomial distribution seems to start to improve over
the uniform distribution.
The comparison between our pruning model and
pruning based on significance is shown in table 2.
These models are hard to compare, since not all
phrase table sizes can be obtained using both met-
rics. For instance, the significance metric can ei-
ther keep or remove all phrase pairs that only appear
once, leaving a large gap of phrase table sizes that
cannot be attained. In the EUROPARL experiment
the sizes of the table suddenly drops from 60% to
8%. The same happens with our metric that cannot
distinguish atomic phrase pairs. In the EUROPARL
experiment, we cannot generate phrase tables with
sizes smaller than 15%. Thus, we only show re-
sults at points where both algorithms can produce
a phrase table.
Significant improvements are observed in the
Table size Significance Entropy (u) Entropy (m)
Pruning Pruning Pruning
IWSLT
57K (65%) 14.82 15.77 15.78
71K (80%) 15.14 15.76 15.77
80K (90%) 15.31 15.73 15.72
88K (100%) 15.68 15.68 15.68
EUROPARL
29M (60%) 28.64 28.82 28.91
34M (70%) 28.84 28.94 28.99
39M (80%) 28.86 28.99 28.99
44M (90%) 28.91 29.00 29.02
49M (100%) 29.18 29.18 29.18
Table 2: Comparison between Significance Pruning (Sig-
nificance Pruning) and Entropy-based pruning using the
uniform (Entropy (u) Pruning) and multinomial distribu-
tions (Entropy (m) Pruning).
IWSLT experiment, where significance pruning
does not perform as well. On the other hand, on the
EUROPARL experiment, our model only achieves
slightly higher results. We believe that this is re-
lated by the fact the EUROPARL corpora is gener-
ated from automatically aligning documents, which
means that there are misaligned sentence pairs.
Thus, many spurious phrase pairs are extracted. Sig-
nificance pruning performs well under these condi-
tions, since the measure is designed for this purpose.
In our metric, we do not have any means for detect-
ing spurious phrase pairs, in fact, spurious phrase
pairs are probably kept in the phrase table, since
each distinct spurious phrase pair is only extracted
once, and thus, they have very few derivations in
its support set. This suggests, that the significance
score can be integrated in our model to improve our
model, which we leave as future work.
John married Portugal
married 
in
in 
Portugal
married 
married 
in
John 
in 
Portugal
Portugal
a)
b)
Figure 2: Translation order in for different reordering
starting from left to right.
We believe that in language pairs such as Chinese-
969
English with large distance reorderings between
phrases are more prone to search errors and benefit
more from our pruning algorithm. To illustrate this,
let us consider the source sentence ?John married
in Portugal?, and translating either using the blocks
?John?, ?married? and ?in Portugal? or the blocks
?John?, ?married in?, ?Portugal?, the first hypoth-
esis would be much more viable, since the word
?Portugal? is more relevant as the context for the
word ?in?. Thus, the key choice for the decoder is
to decide whether to translate using ?married? with
or without ?in?, and it is only able to predict that
it is better to translate ?married? by itself until it
finds that ?in? is better translated with ?Portugal?.
Thus, a search error occurs if the hypothesis where
?married? is translated by itself is removed. In fig-
ure 2, we can see the order that blocks are consid-
ered for different reorderings, starting from left to
right. In a), we illustrate the case for a monotonous
translation. We observe that the correct decision be-
tween translating ?married in? or just ?married? is
found immediately, since the blocks ?Portugal? and
?in Portugal? are considered right afterwards. In this
case, it is unlikely that the hypothesis using ?mar-
ried? is removed. However, if we consider that due
to reordering, ?John? is translated after ?married?
and before ?Portugal?, which is shown in b). Then,
the correct decision can only be found after consid-
ering ?John?. In this case, ?John? does not have
many translations, so the likelihood of eliminating
the correct hypothesis. However, if there were many
translations for John, it is highly likely that the cor-
rect partial hypothesis is eliminated. Furthermore,
the more words exist between ?married? and ?Portu-
gal?, the more likely will the correct hypothesis not
exist when we reach ?Portugal?. By pruning the hy-
pothesis ?married in? a priori, we contribute in pre-
venting such search errors.
We observe that some categories of phrase pairs
that are systematically pruned, but these cannot
be generalized in rules, since there are many ex-
ceptions. The most obvious type of phrase pairs
are phrases with punctuations, such as ???.? to
?thanks .? and ?. ??? to ?thanks .?, since ?.?
is translated independently from most contextual
words. However, this rule should not be general-
ized, since in some cases ?.? is a relevant contextual
marker. For instance, the word ?please? is translated
to ??? in the sentence ?open the door, please.? and
translated to ????? in ?please my advisors?. An-
other example are sequences of numbers, which are
generally translated literally. For instance, ??(8)
?(3)?(8)? is translated to ?eight three eight? (Ex:
?room eight three eight?). Thus, phrase pairs for
number sequences can be removed, since those num-
bers can be translated one by one. However, for se-
quences such as ??(1)?(8)?, we need a phrase pair
to represent this specifically. This is because ??(1)?
can be translated to ?one?, but also to ?a?, ?an?, ?sin-
gle?. Other exceptions include ??(1)?(1)?, which
tends to be translated as ?eleven?, and which tends to
be translated to ?o?, rather than ?zero? in sequences
(?room eleven o five?).
6 Conclusions
We present a pruning algorithm for Machine Trans-
lation based on relative entropy, where we assess
whether the translation event encoded in a phrase
pair can be decomposed into combinations of events
encoded in other phrase pairs. We show that such
phrase pairs can be removed from the translation
model with little negative impact or even a positive
one in the overall translation quality. Tests show that
our method yields comparable or better results with
state of the art pruning algorithms.
As future work, we would like to combine our
approach with significance pruning, since both ap-
proaches are orthogonal and address different issues.
We also plan to improve the pruning step of our algo-
rithm to find the optimal set of phrase pairs to prune
given the pruning threshold.
The code used in this work will be made available.
7 Acknowledgements
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors also wish
to thank the anonymous reviewers for many helpful
comments.
970
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263?311, June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?06, pages 53?61, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
J Howard Johnson and Joel Martin. 2007. Improv-
ing translation quality by discarding most of the
phrasetable. In In Proceedings of EMNLP-CoNLL?07,
pages 967?975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
Wang Ling, Tiago Lu??s, Joa?o Grac?a, Lu??sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ?10:
International Workshop on Spoken Language Transla-
tion, pages 313?320, Paris, France.
Stephen Vogal Matthias Eck and Alex Waibel. 2007. Es-
timating phrase pair relevance for translation model
pruning. MTSummit XI.
Robert C. Moore and Chris Quirk. 2009. Less is more:
significance-based n-gram selection for smaller, bet-
ter language models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 746?755, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Michael Paul, Marcello Federico, and Sebastian Stu?ker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ?10: International Workshop on Spo-
ken Language Translation, pages 3?27.
Lane Schwartz. 2008. Multi-source translation methods.
In Proceedings of AMTA, pages 279?288.
Kristie Seymore and Ronald Rosenfeld. 1996. Scalable
backoff language models. In In Proceedings of ICSLP,
pages 232?235.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270?274.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. MTSummit XII, Aug.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481?504.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841. Association for
Computational Linguistics.
971
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73?84,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Paraphrasing 4 Microblog Normalization
Wang Ling Chris Dyer Alan W Black Isabel Trancoso
L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
Compared to the edited genres that have
played a central role in NLP research, mi-
croblog texts use a more informal register with
nonstandard lexical items, abbreviations, and
free orthographic variation. When confronted
with such input, conventional text analysis
tools often perform poorly. Normalization
? replacing orthographically or lexically id-
iosyncratic forms with more standard variants
? can improve performance. We propose a
method for learning normalization rules from
machine translations of a parallel corpus of
microblog messages. To validate the utility of
our approach, we evaluate extrinsically, show-
ing that normalizing English tweets and then
translating improves translation quality (com-
pared to translating unnormalized text) using
three standard web translation services as well
as a phrase-based translation system trained
on parallel microblog data.
1 Introduction
Microblogs such as Twitter, Sina Weibo (a popular
Chinese microblog service) and Facebook have re-
ceived increasing attention in diverse research com-
munities (Han and Baldwin, 2011; Hawn, 2009, in-
ter alia). In contrast to traditional text domains that
use carefully controlled, standardized language, mi-
croblog content is often informal, with less adher-
ence to conventions regarding punctuation, spelling,
and style, and with a higher proportion of dialect
or pronouciation-derived orthography. While this
diversity itself is an important resource for study-
ing, e.g., sociolinguistic variation (Eisenstein et al,
2011; Eisenstein, 2013), it poses challenges to NLP
applications developed for more formal domains. If
retaining variation due to sociolinguistic or phono-
logical factors is not crucial, text normalization can
improve performance on downstream tasks (?2).
This paper introduces a data-driven approach to
learning normalization rules by conceiving of nor-
malization as a kind of paraphrasing and taking
inspiration from the bilingual pivot approach to
paraphrase detection (Bannard and Callison-Burch,
2005) and the observation that translation is an
inherently ?simplifying? process (Laviosa, 1998;
Volansky et al, 2013). Starting from a parallel cor-
pus of microblog messages consisting of English
paired with several other languages (Ling et al,
2013), we use standard web machine translation sys-
tems to re-translate the non-English segment, pro-
ducing ?English original,English MT? pairs (?3).
These are our normalization examples, with MT out-
put playing the role of normalized English. Sev-
eral techniques for identifying high-precision nor-
malization rules are proposed, and we introduce a
character-based normalization model to account for
predictable character-level processes, like repetition
and substitution (?4). We then describe our decod-
ing procedure (?5) and show that our normaliza-
tion model improve translation quality for English?
Chinese microblog translation (?6).1
2 Why Normalize?
Consider the English tweet shown in the first row of
Table 1 which contains several elements that NLP
1The datasets used in this paper are available from http:
//www.cs.cmu.edu/?lingwang/microtopia.
73
Table 1: Translations of an English microblog message
into Mandarin, using three web translation services.
orig. To DanielVeuleman yea iknw imma work on that
MT1 ?iknw DanielVeuleman?????
MT2 DanielVeuleman?iknw???????
MT3 ?DanielVeuleman??iknw imma??????
systems trained on edited domains may not handle
well. First, it contains several nonstandard abbre-
viations, such as, yea, iknw and imma (abbrevia-
tions of yes, I know and I am going to). Second,
there is no punctuation in the text although stan-
dard convention would dictate that it should be used.
To illustrate the effect this can have, consider now
the translations produced by Google Translate,2 Mi-
crosoft Bing,3 and Youdao,4 shown in rows 2?4.
Even with no knowledge of Chinese, it is not hard
to see that all engines have produced poor transla-
tions: the abbreviation iknw is left translated by all
engines, and imma is variously deleted, left untrans-
lated, or transliterated into the meaningless sequence
?? (pronounced y?? ma?).
While normalization to a form like To Daniel
Veuleman: Yes, I know. I am going to work on that.
does indeed lose some information (information im-
portant for an analysis of sociolinguistic or phono-
logical variation clearly goes missing), it expresses
the propositional content of the original in a form
that is more amenable to processing by traditional
tools. Translating the normalized form with Google
Translate produces ????Veuleman?????
???????????, which is a substantial
improvement over all translations in Table 1.
3 Obtaining Normalization Examples
We want to treat normalization as a supervised learn-
ing problem akin to machine translation, and to do
so, we need to obtain pairs of microblog posts and
their normalized forms. While it would be possible
to ask annotators to create such a corpus, it would
be quite expensive to obtain large numbers of ex-
amples. In this section, we propose a method for
creating normalization examples without any human
2http://translate.google.com/
3http://www.bing.com/translator
4http://fanyi.youdao.com/
Table 2: Translations of Chinese original post to English
using web-based service.
orig. To DanielVeuleman yea iknw imma work on that
orig. ?DanielVeuleman?????????
?????????
MT1 Right DanielVeuleman say, yes, I know, I?m
Xiangna efforts
MT2 DanielVeuleman said, Yes, I know, I?m that hard
MT3 Said to DanielVeuleman, yes, I know, I?m to
that effort
annotation, by leveraging existing tools and data re-
sources.
The English example sentence in Table 1 was se-
lected from the ?topia parallel corpus (Ling et
al., 2013), which consists of self-translated mes-
sages from Twitter and Sina Weibo (i.e., each mes-
sage contains a translation of itself). Row 2 of
Table 2 shows the Mandarin self-translation from
the corpus. The key observation is what happens
when we automatically translate the Mandarin ver-
sion back into English. Rows 3?5 shows automatic
translations from three standard web MT engines.
While not perfect, the translations contain several
correctly normalized subphrases. We will use such
re-translations as a source of (noisy) normalization
examples. Since such self-translations are relatively
numerous on microblogs, this technique can provide
a large amount of data.
Of course, to motivate this paper, we argued that
NLP tools ? like the very translation systems we
propose to use ? often fail on unnormalized input.
Is this a problem? We argue that it is not for the
following two reasons.
Normalization in translation. Work in transla-
tion studies has observed that translation tends to
be a generalizing process that ?smooths out? author-
and work-specific idiosyncrasies (Laviosa, 1998;
Volansky et al, 2013). Assuming this observa-
tion is robust, we expect that dialectal variant forms
found in microblogs to be normalized in translation.
Therefore, if the parallel segments in our microblog
parallel corpus did indeed originate through a trans-
lation process (rather than, e.g., being generated as
two independent utterances from a bilingual), we
may then state the following assumption about the
distribution of variant forms in a parallel segment
74
?e, f?: if e contains nonstandard lexical variants,
then f is likely to be a normalized translation using
with fewer nonstandard lexical variants (and vice-
versa).
Uncorrelated orthographic variants. Any writ-
ten language has the potential to make creative use
of orthography: alphabetic scripts can render ap-
proximations of pronunciation variants; logographic
scripts can use homophonic substitutions. However,
the kinds of innovations used in particular languages
will be language specific (depending on details of
the phonology, lexicon, and orthography of the lan-
guage). However, for language pairs that differ sub-
stantially in these dimensions, it may not always
be possible (or at least easy) to preserve particular
kinds of nonstandard orthographic forms in trans-
lation. Consider the (relatively common) pronoun-
verb compounds like iknw and imma from our mo-
tivating example: since Chinese uses a logographic
script without spaces, there is no obvious equivalent.
3.1 Variant?Normalized Parallel Corpus
For the two reasons outlined above, we argue that
we will be able to translate back into English us-
ing MT, even when the underlying English part of
the parallel corpus has a great deal of nonstandard
content. We leverage this fact to build the normal-
ization corpus, where the original English tweet is
treated as the variant form, and the automatic trans-
lation obtained from another language is considered
a potential normalization.5
Our process is as follows. The microblog cor-
pus of Ling et al (2013) contains sentence pairs ex-
tracted from Twitter and Sina Weibo, for multiple
language pairs. We use all corpora that include En-
glish as one of the languages in the pair. The respec-
tive non-English side is translated into English using
different translation engines. The different sets we
used and the engines we used to translate are shown
in Table 3. Thus, for each original English post o,
we obtain n paraphrases {pi}
n
i=1, from n different
translation engines.
5We additionally assume that the translation engines are
trained to output more standardized data, so there will be addi-
tional normalizing effect from the machine translation system.
Table 3: Corpora Used for Paraphrasing.
Lang. Pair Source Segs. MT Engines
ZH-EN Weibo 800K Google, Bing, Youdao
ZH-EN Twitter 113K Google, Bing, Youdao
AR-EN Twitter 114K Google, Bing
RU-EN Twitter 119K Google, Bing
KO-EN Twitter 78K Google, Bing
JA-EN Twitter 75K Google, Bing
3.2 Alignment and Filtering
Our parallel microblog corpus was crawled automat-
ically and contains many misaligned sentences. To
improve precision, we attempt to find the similar-
ity between the (unnormalized) original and each
of the normalizations using an alignment based on
the one used in METEOR (Denkowski and Lavie,
2011), which computes the best alignment between
the original tweet and each of the normalizations
but modified to permit domain-specific approximate
matches. To address lexical variants, we allow fuzzy
word matching, that is, we allow lexically similar,
such as yea and yes to be aligned (similarity is de-
termined by the Levenshtein distance). We also per-
form phrasal matchings, such as ikwn to i know. To
do so, we extend the alignment algorithm from word
to phrasal alignments. More precisely, given the
original post o and a candidate normalization n, we
wish to find the optimal segmentation producing a
good alignment. A segmentation s = ?s1, . . . , s|s|?
is a sequence of segments that aligns as a block to a
source word. For instance, for the sentence yea iknw
imma work on that, one possible segmentation could
be s1 =yea ikwn, s2 =imma and s3 =work on that.
Model. We define the score of an alignment a and
segmentation s in using a model that makes semi-
Markov independence assumptions, similar to the
work in (Bansal et al, 2011), u(a, s | o,n) =
|s|?
i=1
[
ue(si, ai | n)? ut(ai | ai?1)? u`(|si|)
]
In this model, the maximal scoring segmentation
and alignment can be found using a polynomial time
dynamic programming algorithm. Each segment
can be aligned to any word or segment in o. The
aligned segment for sk is defined as ak. For the
75
score of a segment correspondence ue(s, a | n), we
assume that this can be estimated using the lexical
similarity between segments, which we define to be
1? L(sk,ak)max{|sk|,|ak|} , where L(x, y) denotes the Leven-
shtein distance between strings x and y, normalized
by the highest possible distance between those seg-
ments.
For the alignment score ut, we assume that the
relative order of the two sequences will be mostly
monotonous. Thus, we approximate ut with the fol-
lowing density poss(ak) ? pose(ak?1) ? N (1, 1),
where the poss is the index of the first word in the
segment and pose the one of the last word.
After finding the Viterbi alignments, we compute
the similarity measure ? = |A||A|+|U | , used in (Resnik
and Smith, 2003), where |A| and |U | are the number
of words that were aligned and unaligned, respec-
tively. In this work, we extract the pair if ? > 0.2.
4 Normalization Model
From the normalization corpus, we learn a nor-
malization model that generalizes the normalization
process. That is, from the data we observe that To
DanielVeuleman yea iknw imma work on that is nor-
malized to To Daniel Veuleman: yes, I know. I
am going to work on that. However, this is not
useful, since the chances of the exact sentence To
DanielVeuleman yea iknw imma work on that occur-
ring in the data is low. We wish to learn a process to
convert the original tweet into the normalized form.
There are two mechanisms that we use in our
model. The first (?4.1) learns word?word and
phrase?phrase mappings. That is, we wish to find
that DanielVeuleman is normalized to Daniel Veule-
man, that iknw is normalized to I know and that
imma is normalized to I am going. These mappings
are more useful, since whenever iknw occurs in the
data, we have the option to normalize it to I know.
The second (?4.2) learns character sequence map-
pings. If we look at the normalization DanielVeule-
man to Daniel Veuleman, we can see that it is only
applicable when the exact word DanielVeuleman oc-
curs. However, we wish to learn that it is uncom-
mon for the letters l and v to occur in the same word
sequentially, so that be can add missing spaces in
words that contain the lv character sequence, such as
normalizing phenomenalvoter to phenomenal voter.
I wanna go 4 pizza 2day
I want go for pizza todayto
Figure 1: Variant?normalized alignment with the variant
form above and the normalized form below; solid lines
show potential normalizations, while dashed lines repre-
sent identical translations.
However, there are also cases where this is not true,
for instance, in the word velvet, we do not wish to
separate the letters l and v. Thus, we shall describe
the process we use to decide when to apply these
transformations.
4.1 From Sentences To Phrases
The process to find phrases from sentences has been
throughly studied in Machine Translation. This is
generally done in two steps, Word Alignments and
Phrase Extraction.
Alignment. The first step is to find the word-level
alignments between the original post and its nor-
malization. This is a well studied problem in MT,
referred as Word Alignment (Brown et al, 1993).
Many alignment models have been proposed, such
as, the HMM-based word alignment models (Vo-
gel et al, 1996) and the IBM models (Och and
Ney, 2003). Generally, a symmetrization step is per-
formed, where the bidirectional alignments are com-
bined heuristically. In our work, we use the fast
aligner proposed in (Dyer et al, 2013) to obtain the
word alignments. Figure 1 shows an example of an
word aligned pair of a tweet and its normalization.
Phrase Extraction. The phrasal extraction
step (Ling et al, 2010), uses the word aligned
sentences and extracts phrasal mappings between
the original tweet and its normalization, named
phrase pairs. For instance, in Figure 1, we would
like to extract the phrasal mapping from go 4 to go
for, so that we learn that the word 4 in the context of
go is normalized to the proposition for. To do this,
the most common approach is to use the template
proposed in (Och and Ney, 2004), which allows
phrase pairs to be extracted, if there is at least one
word alignment within the pair, and there are no
76
Table 4: Fragment of the phrase normalization model
built, for each original phrase o, we present the top-3 nor-
malized forms ranked by f(n | o).
Original (o) Normalization (n) f(n | o)
wanna want to 0.4679
wanna will 0.0274
wanna going to 0.0114
4 4 0.5641
4 for 0.01795
go 4 go for 1.0000
words inside the pair that are aligned to words not
in the pair. For instance, in the example above, the
phrase pair that normalizes wanna to want to would
be extracted, but the phrase pair normalizing wanna
to want to go would not, because the word go in the
normalization is aligned to a word not in the pair.
Phrasal Features. After extracting the phrase
pairs, a model is produced with features derived
from phrase pair occurrences during extraction. This
model is equivalent to phrasal translation model in
MT, but we shall refer to it as the normalization
model. For a phrase pair ?o,n?, where o is the origi-
nal phrase, and n is the normalized phrase, we com-
pute the normalization relative frequency f(n | o) =
C(n,o)
C(o) , where C(n, o) denotes the number of times
o was normalized to n and C(o) denotes the number
of times o was seen in the extracted phrase pairs. Ta-
ble 4 gives a fragment of the normalization model.
The columns represent the original phrase, its nor-
malization and the probability, respectively.
In Table 4, we observe that the abbreviation
wanna is normalized to want to with a relatively
high probability, but it can also be normalized to
other equivalent expressions, such as will and go-
ing to. The word 4 by itself has a low probability
to be normalized to the preposition for. This is ex-
pected, since this decision cannot be made without
context. However, we see that the phrase go 4 is
normalized to go for with a high probability, which
specifies that within the context of go, 4 is generally
used as a preposition.
4.2 From Phrases to Characters
While we can learn lexical variants that are in the
corpora using the phrase model, we can only address
word forms that have been observed in the corpora.
Table 5: Fragment of the character normalization model
where examples representative of the lexical variant gen-
eration process are encoded in the model.
Original (o) Normalization (n) f(n | o)
o o o o o 0.0223
o o o o 0.0439
s c 0.0331
z s 0.0741
s h c h 0.019
2 t o 0.014
4 f o r 0.0013
0 o 0.0657
i n g f o r i n g <space> f o r 0.4545
g f g <space> f 0.01028
This is quite limited, since we cannot expect all the
word forms to be present, such as all the possible
orthographic errors for the word cat, such as catt,
kat and caaaat. Thus, we will build a character-
based model that learns the process lexical variants
are generated at the subword level.
Our character-based model is similar to the
phrase-based model, except that, rather than learn-
ing word-based mappings from the original tweet
and the normalization sentences, we learn character-
based mappings from the original phrases to the nor-
malizations of those phrases. Thus, we extract the
phrase pairs in the phrasal normalization model, and
use them as a training corpora. To do this, for each
phrase pair, we add a start token, <start>, and a
end token, <end>, at the beginning and ending of
the phrase pair. Afterwards, we separate all charac-
ters by space and add a space token <space> where
spaces were originally. For instance, the phrase
pair normalizing DanielVeuleman to Daniel Veule-
man would be converted to <start> d a n i e l v e u
l e m a n <end> and <start> d a n i e l <space> v
e u l e m a n <end>.
Character-based Normalization Model - To
build the character-based model, we proceed using
the same approach as in the phrasal normalization
model. We first align characters using Word Align-
ment Models, and then we perform phrase extrac-
tion to retrieve the phrasal character segments, and
build the character-based model by collecting statis-
tics. Once again, we provide examples of entries in
the model in Table 5.
77
We observe that many of the normalizations dealt
with in the previous model by memorizing phrases
are captured with string transformations. For in-
stance, from phrase pairs such as tooo to too and
sooo to so, we learn that sequences of o?s can be
reduced to 2 or 1 o. Other examples include or-
thographic substitutions, such as 2 for to and 4
for for (as found in 2gether, 2morrow, 4ever and
4get). Moreover, orthographic errors can be gener-
ated from mistaking characters with similar phonetic
properties, such as, s to c, z to s and sh to ch, gener-
ating lexical variants such as reprecenting. Finally,
we learn that the number 0 that resembles the letter
o, can be used as a replacement, as in g00d. Finally,
we can see that the rule ingfor to ing for attempts to
find segmentation errors, such as goingfor, where a
space between going and for was omitted.6
5 Normalization Decoder
In section 4, we built two models to learn the process
of normalization, the phrase-based model and the
character-based model. In this section, we describe
the decoder we used to normalize the sentences.
The advantage of the phrase-based model is that it
can make decisions for normalization based on con-
text. That is, it contains phrasal units, such as, go
4, that determine, when the word 4 should be nor-
malized to the preposition for and when to leave it
as a number. However, it cannot address words that
are unseen in the corpora. For instance, if the word
form 4ever is not seen in the training corpora, it is
not be able to normalize it, even if it has seen the
word 4get normalized to forget. On the other hand,
the character-based model learns subword normal-
izations, for instance, if we see the word nnnnno
normalized to no, we can learn that repetitions of
the letter n are generally shorted to n, which al-
lows it to generate new word forms. This model
has strong generalization potential, but the weak-
ness of the character-based model is that it fails to
6Note that this captures the context in which such transfor-
mations are likely to occur: there are not many words that con-
tain the sequence ingfor, so the probability that these should be
normalized by inserting a space is high. On the other hand, we
cannot assume that if we observe the sequence gf, we can safely
separate these with a space. This is because, there are many
words that contain this sequence, such as the abbreviation of
gf (girlfriend), dogfight, and bigfoot.
consider the context of the normalization that the
phrase-based model uses to make normalization de-
cisions. Thus, our goal in this section is describe a
decoder that uses both models to improve the quality
of the normalizations.
5.1 Phrasal Decoder
We use Moses, an off-the-shelf phrase-based MT
system (Koehn et al, 2007), to ?translate? the orig-
inal tweet its normalized form using the phrasal
model (?4.1). Aside form the normalization prob-
ability, we also use the common features used in
MT. These are the reverse normalization probabil-
ity, the lexical and reverse lexical probabilities and
the phrase penalty. We also use the MSD reorder-
ing model proposed in (Koehn et al, 2005), which
adds reordering features.7 The final score of each
phrase pair is given as a sum of weighted log fea-
tures. The weights for these features are optimized
using MERT (Och, 2003). In our work, we sampled
150 tweets randomly from Twitter and normalized
them manually, and used these samples as devel-
opment data for MERT. As for the character-based
model features, we simply rank the training phrase
pairs by their relative frequency the f(n | o), and use
the top-1000 phrase pairs as development set. Fi-
nally, a language model is required during decoding
as a prior, since it defines the type of language that
is produced by the output. We wish to normalized
to formal language, which is generally better pro-
cessed by NLP tools. Thus, for the phrase model,
we use the English NIST dataset composed of 8M
sentences in English from the news domain to build
a 5-gram Kneser-Ney smoothed language model.
5.2 Character and Phrasal Decoder
We now turn to how to apply the character-based
(?4.2), together with the phrasal model. For this
model, we again use Moses, treating each charac-
ter as a ?word?. The simplest way to combine both
methods is first to decode the input o sentence with
the character-based decoder, normalizing each word
independently and then normalizing the resulting
output using the phrase-based decoder, which en-
ables the phrase model to score the outputs of the
character model in context.
7Reordering helps find lexical variants that are generated by
transposing characters, such as, mabye to maybe.
78
0 1 2 3 4 5 6
I
wanna
want to meeeeet
meet
met
DanielVeuleman
Daniel Veuleman
Figure 2: Example output lattice of the character-based decoder, for the sentence I wanna meeeeet DanielVeuleman.
Our process is as follows. Given the input sen-
tence o, with the words o1, . . . , om, where m is
the number of words in the input, we generate for
each word oi a list of n-best normalization candi-
dates z1oi , . . . , z
n
oi . We further filter the candidates
using two criteria. We start by filtering each can-
didate zjoi that occurs less frequently than the orig-
inal word oi. This is motivated by our observation
that lexical variants occur far less than the respec-
tive standard form. Second, we build a corpus of
English language Twitter consisting of 70M tweets,
extract the unigram counts, and perform Brown clus-
tering (Brown et al, 1992) with k = 3000 clusters.
Next, we calculate the cluster similarity between oi
and each surviving candidate, zjoi . We filter the can-
didate if the similarity is less than 0.8. The similar-
ity between two clusters represented as bit strings,
S[c(oi), c(z
j
oi)], calculated as:
S(x, y) =
2 ? |lpm{x, y)}|
|x|+ |y|
,
where lpm computes the longest common prefix of
the contexts and |x| is the length of the bit string.8
If a candidate contains more than one word (because
a space was inserted), we set its count as the mini-
mum count among its words. To find the cluster for
multiple word units, we concatenate the words to-
gether, and find the cluster with the resulting word if
it exists. This is motivated by the fact that it is com-
mon for missing spaces to exist in microblog cor-
pora, generating new word forms, such as wantto,
goingfor, and given a large enough corpora as the
one we used, these errors occur frequently enough to
be placed in the correct cluster. In fact, the variants
such as wanna and tmi, occur in the same clusters as
the words wantto and toomuchinformation.
Remaining candidates are combined into a word
lattice, enabling us to perform lattice-based decod-
8Brown clusters are organized such that more words with
more similar distributions share common prefixes.
ing with the phrasal model (Dyer et al, 2008). Fig-
ure 2, provides an example of such a lattice for the
variant sentence I wanna meeeet DanielVeuleman.
5.3 Learning Variants from Monolingual Data
Until now, we learned normalizations from pairs of
original tweets and their normalizations. We shall
now describe a process to leverage monolingual doc-
uments to learn new normalizations, since the mono-
lingual data is far easier to obtain than parallel data.
This process is similar to the work in (Han et al,
2012), where confusion sets of contextually simi-
lar words are built initially as potential normaliza-
tion candidates. We again use the k = 3000 Brown
clusters,9 and this time consider the contents of each
cluster as a set of possible normalization variants.
For instance, we find that the cluster that includes the
word never, also includes the variant forms neverrrr,
neva and nevahhh. However, the cluster also con-
tains non-variant forms, such as gladly and glady.
Thus, we want to find that neverrrr maps to never,
while glady maps to gladly in the same cluster. Our
work differs from previous work in that, rather than
defining features manually, we use our character-
based decoder to find the mappings between lexical
variants and their normalizations.
For every word type wi in cluster c(wi) =
{w1, . . . , wn}, we generate a set of possible candi-
dates for each word w1i , . . . , w
m
i . Then, we build
a directed acyclic graph (DAG), where every word.
We add an edge between wi and wj , if wi can be
decoded into wj using the character model from the
previous section, and also if wi occurs less than wj ;
the second condition guarantees that the graph will
be acyclic. Sample graphs are shown in Figure 3.
Afterwards, we find the number of paths between
all nodes in the graph (this can be computed effi-
ciently in O(|V | + |E|) time). Then, for each word
9The Brown clustering algorithm groups words together
based on contextual similarity.
79
neverr
neva neve
nevar
never
glady
gladly
cladly
Figure 3: Example DAGs, built from the cluster contain-
ing the words never and gladly.
wi, we find the wj to which it has the highest num-
ber of paths to and extract the normalization of wi
to wj . In case of a tie, we choose the word wj that
occurs more often in the monolingual corpora. This
is motivated by the fact that normalizations are tran-
sitive. Thus, even if neva cannot be decoded directly
to never, we can use nevar as an intermediate step to
find the correct normalization. This is performed for
all the clusters, and the resulting dictionary of lexi-
cal variants mapped to their standard forms is added
to the training data of the character-based model.
6 Experiments
We evaluate our normalization model intrinsically
by testing whether our normalizations more closely
resemble standardized data, and then extrinsically
by testing whether we can improve the translation
quality of in-house as well as online Machine Trans-
lation systems by normalizing the input.
6.1 Setup
We use the gold standard by Ling et al (2013), com-
posed by 2581 English-Mandarin microblog sen-
tence pairs. From this set, we randomly select 1290
pairs for development and 1291 pairs for testing.
The normalizer model is trained on the corpora
extracted and filtered in section 3, in total, there
were 1.3M normalization pairs used during training.
The test sentences are normalized using four differ-
ent setups. The first setup leaves the input sentence
unchanged, which we call No Norm. The second
uses the phrase-based model to normalize the input
sentence, which we will denote Norm+phrase. The
third uses the character-based model to output lat-
tices, and then decodes with the phrase based model,
which we will denote Norm+phrase+char. Finally,
we test the same model after adding the training data
extracted using monolingual documents, which we
will refer as Norm+phrase+char+mono.
To test the normalizations themselves, we used
Google Translate to translate the Mandarin side of
the 1291 test sentence pairs back to English and use
the original English tweet. While, this is by itself
does not guarantee that the normalizations are cor-
rect, since the normalizations could be syntactically
and semantically incorrect, it will allow us to check
whether the normalizations are closer to those pro-
duced by systems trained on news data. This exper-
iment will be called Norm.
As an application and extrinsic evaluation for our
normalizer, we test if we can obtain gains on the
MT task on microblog data by using our normalizer
prior to translation. We build two MT systems us-
ing Moses. Firstly, we build a out-of-domain model
using the full 2012 NIST Chinese-English dataset
(approximately 8M sentence pairs), which is dataset
from the news domain, and we will denote this sys-
tem as Inhouse+News. Secondly, we build a in-
domain model using the 800K sentence pairs from
?topia corpora (Ling et al, 2013). We also add
the NIST dataset to improve coverage. We call this
system Inhouse+News+Weibo. To train these sys-
tems, we use the Moses phrase-based MT system
with standard features (Koehn et al, 2003). For re-
ordering, we use the MSD reordering model (Axel-
rod et al, 2005). As the language model, we train
a 5-gram model with Kneser-ney smoothing using a
10M tweets from twitter. Finally, the weights were
tuned using MERT (Och, 2003). As for online sys-
tems, we consider the systems used to generate the
paraphrase corpora in section 3, which we will de-
note as Online A, Online B and Online C10
The normalization and MT results are evaluated
with BLEU-4 (Papineni et al, 2002) comparing the
produced translations or normalizations with the ap-
propriate reference.
6.2 Results
Results are shown in Table 6. In terms of the normal-
izations, we observe a much better match between
10The names of the systems are hidden to not violate the pri-
vacy issues in the terms and conditions of these online systems.
80
Table 6: Normalization and MT Results. Rows denote different normalizations, and columns different translation
systems, except the first column (Norm), which denotes the normalization experiment. Cells display the BLEU score
of that experiment.
Moses Moses
Condition Norm (News) (News+Weibo) Online A Online B Online C
baseline 19.90 15.10 24.37 20.09 17.89 18.79
norm+phrase 21.96 15.69 24.29 20.50 18.13 18.93
norm+phrase+char 22.39 15.87 24.40 20.61 18.22 19.08
norm+phrase+char+mono 22.91 15.94 24.46 20.78 18.37 19.21
the normalized text with the reference, than the orig-
inal tweets. In most cases, adding character-based
models improves the quality of the normalizations.
We observe that better normalizations tend to lead
to better translations. The relative improvements
are most significant, when moving from No Norm
to norm+phrase normalization. This is because,
we are normalizing words that are not seen in gen-
eral MT system?s training data, but occur frequently
in microblog data, such as wanna to want to, u to
you and im to i?m. The only exception is in the In-
house+News+Weibo system, where the normaliza-
tion deteriorates the results. This is to be expected,
since this system is trained on the same microblog
data used to learn the normalizations. However, we
can observe on norm+phrase+char that if we add
the character-based model, we can observe improve-
ments for this system as well as for all other ones.
This is because the model is actually learning nor-
malizations that are unseen in the data. Some ex-
amples of these normalization include, normalizing
lookin to looking, nutz to nuts and maimi to miami
but also separating peaceof to peace of. The fact
that these improvements are obtained for all sys-
tems is strong evidence that we are actually produc-
ing good normalizations, and not overfitting to one
of the systems that we used to generate our data.
The gains are much smaller from norm+phrase
to norm+phrase+char, since the improvements we
obtain come from normalizing less frequent words.
Finally, we can obtain another small improvement
by adding monolingual data to the character-based
model in norm+phrase+char+mono.
7 Related Work
Most of the work in microblog normalization is fo-
cused on finding the standard forms of lexical vari-
ants (Yang and Eisenstein, 2013; Han et al, 2013;
Han et al, 2012; Kaufmann, 2010; Han and Bald-
win, 2011; Gouws et al, 2011; Aw et al, 2006). A
lexical variant is a variation of a standard word in
a different lexical form. This ranges from minor or
major spelling errors, such as jst, juxt and jus that
are lexical variants of just, to abbreviations, such as
tmi and wanna, which stand for too much informa-
tion and want to, respectively. Jargon can also be
treated as variants, for instance cday is a slang word
for birthday, in some groups.
There are many rules that govern the process lex-
ical variants are generated. Some variants are gener-
ated from orthographic errors, caused by some mis-
take from the user when writing. For instance, the
variants representin, representting, or reprecenting
can be generated by a spurious letter swap, insertion
or substitution by the user. One way to normalize
these types of errors is to attempt to insert, remove
and swap words in a lexical variant until a word in
a dictionary of standard words is found (Kaufmann,
2010). Contextual features are another way to find
lexical variants, since variants generally occur in the
same context as their standard form. This includes
orthographic errors, abbreviations and slang. How-
ever, this is generally not enough to detect lexical
variants, as many words share similar contexts, such
as already, recently and normally. Consequently,
contextual features are generally used to generate a
confusion set of possible normalizations of a lexical
variant, and then more features are used to find the
correct normalization (Han et al, 2012). One simple
approach is to compute the Levenshtein distance to
find lexical similarities between words, which would
effectively capture the mappings between represent-
ting, reprecenting and representin to representing.
However, a pronunciation model (Tang et al, 2012)
81
would be needed to find the mapping between g8,
2day and 4ever to great, today and forever, respec-
tively. Moreover, visual character similarity features
would be required to find the mapping between g00d
and? to good and i.
Clearly, learning this process is a challenging
task, and addressing each different case individually
would require vast amounts of resources. Further-
more, once we change the language to normalize
to another language, the types of rules that generate
lexical variants would radically change and a new set
of features would have to be engineered. We believe
that to be successful in normalizing microblogs,
the process to learn new lexical variants should be
learned from data, making as few assumptions as
possible. We learn our models without using any
type of predefined features, such as phonetic fea-
tures or lexical features. In fact, we will not assume
that most words and characters map to themselves,
as it is assumed in methods using the Levenshtein
distance (Kaufmann, 2010; Han et al, 2012; Wang
and Ng, 2013). All these mappings are learned from
our data. Furthermore, in the work above, the dictio-
naries built using these methods assume that lexical
variants are mapped to standard forms in a word-to-
word mapping. Thus, variants such as wanna, gonna
and imma are not normalizable, since they are nor-
malized to multiple words want to, going to and I
am gonna. Moreover, there are segmentation errors
that occur from missing spaces, such as sortof and
goingfor, which also map to more than one word to
sort of and going for. These cases shall also be ad-
dressed in our work.
Wang and Ng (2013) argue that microblog nor-
malization is not simply to map lexical variants into
standard forms, but that other tasks, such as punctua-
tion correction and missing word recovery should be
performed. Consider the example tweet you free?,
while there are no lexical variants in this message,
the authors consider that it is the normalizer should
recover the missing article are and normalize this
tweet to are you free?. To do this, the authors train a
series of models to detect and correct specific errors.
While effective for narrow domains, training models
to address each specific type of normalization is not
scalable over all types of normalizations that need to
be performed within the language, and the fact that a
set of new models must be implemented for another
language limits the applicability of this work.
Another strong point of the work above is that
a decoder is presented, while the work on build-
ing dictionaries only normalize out of vocabu-
lary (OOV) words. The work on (Han et al, 2012)
trains a classifier to decide whether to normalize a
word or not, but is still preconditioned on the fact
that the word in question is OOV. Thus, lexical vari-
ants, such as, 4 and u, with the standard forms for
and you, are left untreated, since they occur in other
contexts, such as u in u s a. Inspired by the work
above, we also propose a decoder based on the exist-
ing off-the-self decoder Moses (Koehn et al, 2007).
Finally, the work in (Xu et al, 2013) obtains para-
phrases from Twitter, by finding tweets that contain
common entities, such as Obama, that occur during
the same period by matching temporal expressions.
The resulting paraphrase corpora can also be used to
train a normalizer.
8 Conclusion
We introduced a data-driven approach to microblog
normalization based on paraphrasing. We build a
corpora of tweets and their normalizations using par-
allel corpora from microblogs using MT techniques.
Then, we build two models that learn generalizations
of the normalization process, one the phrase level
and on the character level. Then, we build a de-
coder that combines both models during decoding.
Improvements on multiple MT systems support the
validity of our method.
In future work, we shall attempt to build normal-
izations for other languages. We shall also attempt
to learn an unsupervised normalization model with
only monolingual data, similar to the work for MT
in (Ravi and Knight, 2011).
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT ?
Fundac?a?o para a Cie?ncia e a Tecnologia, under project
SFRH/BD/51157/2010. This work was supported by na-
tional funds through FCT ? Fundac?a?o para a Cie?ncia e a
Tecnologia, under project PEst-OE/EEI/LA0021/2013.
The authors also wish to express their gratitude to the
anonymous reviewers for their comments and insight.
82
References
[Aw et al2006] AiTi Aw, Min Zhang, Juan Xiao, and
Jian Su. 2006. A phrase-based statistical model for
SMS text normalization. In Proceedings of the ACL,
COLING-ACL ?06, pages 33?40, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David Tal-
bot. 2005. Edinburgh system description for the 2005
iwslt speech translation evaluation. In In Proc. Inter-
national Workshop on Spoken Language Translation
(IWSLT.
[Bannard and Callison-Burch2005] Colin Bannard and
Chris Callison-Burch. 2005. Paraphrasing with bilin-
gual parallel corpora. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 597?604, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
[Bansal et al2011] Mohit Bansal, Chris Quirk, and
Robert C. Moore. 2011. Gappy phrasal alignment by
agreement. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ?11,
pages 1308?1317, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Brown et al1992] Peter F Brown, Peter V Desouza,
Robert L Mercer, Vincent J Della Pietra, and Jenifer C
Lai. 1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Linguist.,
19:263?311, June.
[Denkowski and Lavie2011] Michael Denkowski and
Alon Lavie. 2011. Meteor 1.3: Automatic metric
for reliable optimization and evaluation of machine
translation systems. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
85?91, Edinburgh, Scotland, July. Association for
Computational Linguistics.
[Dyer et al2008] Chris Dyer, Smaranda Muresan, and
Philip Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of HLT-ACL.
[Dyer et al2013] Chris Dyer, Victor Chahuneau, and
Noah A Smith. 2013. A simple, fast, and effective
reparameterization of ibm model 2. In Proceedings of
NAACL-HLT, pages 644?648.
[Eisenstein et al2011] Jacob Eisenstein, Noah A. Smith,
and Eric P. Xing. 2011. Discovering sociolinguis-
tic associations with structured sparsity. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1365?1374,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
[Eisenstein2013] Jacob Eisenstein. 2013. What to do
about bad language on the internet. In Proceedings
of NAACL-HLT, pages 359?369.
[Gouws et al2011] Stephan Gouws, Dirk Hovy, and Don-
ald Metzler. 2011. Unsupervised mining of lexical
variants from noisy text. In Proceedings of the First
Workshop on Unsupervised Learning in NLP, EMNLP
?11, pages 82?90, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Han and Baldwin2011] Bo Han and Timothy Baldwin.
2011. Lexical normalisation of short text messages:
makn sens a #twitter. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 368?378, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Han et al2012] Bo Han, Paul Cook, and Timothy Bald-
win. 2012. Automatically constructing a normalisa-
tion dictionary for microblogs. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ?12, pages 421?
432, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
[Han et al2013] Bo Han, Paul Cook, and Timothy Bald-
win. 2013. Lexical normalization for social media
text. ACM Transactions on Intelligent Systems and
Technology (TIST), 4(1):5.
[Hawn2009] Carleen Hawn. 2009. Take two aspirin and
tweet me in the morning: how twitter, facebook, and
other social media are reshaping health care. Health
affairs, 28(2):361?368.
[Kaufmann2010] M. Kaufmann. 2010. Syntactic Nor-
malization of Twitter Messages. studies, 2.
[Koehn et al2003] Philipp Koehn, Franz Josef Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, NAACL ?03, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
[Koehn et al2005] Philipp Koehn, Amittai Axelrod,
Alexandra Birch Mayne, Chris Callison-Burch, Miles
Osborne, David Talbot, and Michael White. 2005.
Edinburgh system description for the 2005 nist mt
evaluation. In Proceedings of Machine Translation
Evaluation Workshop 2005.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-burch, Richard Zens, Rwth
83
Aachen, Alexandra Constantin, Marcello Federico,
Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade
Shen, Christine Moran, and Ondrej Bojar. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Laviosa1998] Sara Laviosa. 1998. Core patterns of
lexical use in a comparable corpus of English lexical
prose. Meta, 43(4):557?570.
[Ling et al2010] Wang Ling, Tiago Lu??s, Joa?o Grac?a,
Lu??sa Coheur, and Isabel Trancoso. 2010. Towards a
general and extensible phrase-extraction algorithm. In
IWSLT ?10: International Workshop on Spoken Lan-
guage Translation, pages 313?320, Paris, France.
[Ling et al2013] Wang Ling, Guang Xiang, Chris Dyer,
Alan Black, and Isabel Trancoso. 2013. Microblogs
as parallel corpora. In Proceedings of the 51st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?13. Association for Computational Lin-
guistics.
[Och and Ney2003] Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statis-
tical alignment models. Computational linguistics,
29(1):19?51.
[Och and Ney2004] Franz Josef Och and Hermann Ney.
2004. The alignment template approach to statistical
machine translation. Comput. Linguist., 30(4):417?
449, December.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ?02,
pages 311?318, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Ravi and Knight2011] Sujith Ravi and Kevin Knight.
2011. Deciphering foreign language. In ACL, pages
12?21.
[Resnik and Smith2003] Philip Resnik and Noah A
Smith. 2003. The web as a parallel corpus. Com-
putational Linguistics, 29(3):349?380.
[Tang et al2012] Hao Tang, Joseph Keshet, and Karen
Livescu. 2012. Discriminative pronunciation mod-
eling: A large-margin, feature-rich approach. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 194?203. Association for Computational Lin-
guistics.
[Vogel et al1996] S. Vogel, H. Ney, and C. Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 2, pages 836?841. Asso-
ciation for Computational Linguistics.
[Volansky et al2013] Vered Volansky, Noam Ordan, and
Shuly Wintner. 2013. On the features of transla-
tionese. Literary and Linguistic Computing.
[Wang and Ng2013] Pidong Wang and Hwee Ng. 2013.
A beam-search decoder for normalization of social
media text with application to machine translation. In
Proceedings of NAACL-HLT 2013, NAACL ?13. As-
sociation for Computational Linguistics.
[Xu et al2013] Wei Xu, Alan Ritter, and Ralph Grish-
man. 2013. Gathering and generating paraphrases
from twitter with application to normalization. In Pro-
ceedings of the Sixth Workshop on Building and Us-
ing Comparable Corpora, pages 121?128, Sofia, Bul-
garia, August. Association for Computational Linguis-
tics.
[Yang and Eisenstein2013] Yi Yang and Jacob Eisenstein.
2013. A log-linear model for unsupervised text nor-
malization. In Proc. of EMNLP.
84
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 176?186,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Microblogs as Parallel Corpora
Wang Ling123 Guang Xiang2 Chris Dyer2 Alan Black2 Isabel Trancoso 13
(1)L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,guangx,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
In the ever-expanding sea of microblog data, there
is a surprising amount of naturally occurring par-
allel text: some users create post multilingual mes-
sages targeting international audiences while oth-
ers ?retweet? translations. We present an efficient
method for detecting these messages and extract-
ing parallel segments from them. We have been
able to extract over 1M Chinese-English parallel
segments from Sina Weibo (the Chinese counter-
part of Twitter) using only their public APIs. As a
supplement to existing parallel training data, our
automatically extracted parallel data yields sub-
stantial translation quality improvements in trans-
lating microblog text and modest improvements
in translating edited news commentary. The re-
sources in described in this paper are available at
http://www.cs.cmu.edu/?lingwang/utopia.
1 Introduction
Microblogs such as Twitter and Facebook have
gained tremendous popularity in the past 10 years.
In addition to being an important form of commu-
nication for many people, they often contain ex-
tremely current, even breaking, information about
world events. However, the writing style of mi-
croblogs tends to be quite colloquial, with fre-
quent orthographic innovation (R U still with me
or what?) and nonstandard abbreviations (idk!
shm)?quite unlike the style found in more tra-
ditional, edited genres. This poses considerable
problems for traditional NLP tools, which were
developed with other domains in mind, which of-
ten make strong assumptions about orthographic
uniformity (i.e., there is just one way to spell you).
One approach to cope with this problem is to an-
notate in-domain data (Gimpel et al, 2011).
Machine translation suffers acutely from the
domain-mismatch problem caused by microblog
text. On one hand, standard models are probably
suboptimal since they (like many models) assume
orthographic uniformity in the input. However,
more acutely, the data used to develop these sys-
tems and train their models is drawn from formal
and carefully edited domains, such as parallel web
pages and translated legal documents. MT training
data seldom looks anything like microblog text.
This paper introduces a method for finding nat-
urally occurring parallel microblog text, which
helps address the domain-mismatch problem.
Our method is inspired by the perhaps surpris-
ing observation that a reasonable number of mi-
croblog users tweet ?in parallel? in two or more
languages. For instance, the American entertainer
Snoop Dogg regularly posts parallel messages on
Sina Weibo (Mainland China?s equivalent of Twit-
ter), for example, watup Kenny Mayne!! - Kenny
Mayne?????????, where an English
message and its Chinese translation are in the
same post, separated by a dash. Our method is able
to identify and extract such translations. Briefly,
this requires determining if a tweet contains more
than one language, if these multilingual utterances
contain translated material (or are due to some-
thing else, such as code switching), and what the
translated spans are.
The paper is organized as follows. Section 2
describes the related work in parallel data extrac-
tion. Section 3 presents our model to extract par-
allel data within the same document. Section 4
describes our extraction pipeline. Section 5 de-
scribes the data we gathered from both Sina Weibo
(Chinese-English) and Twitter (Chinese-English
and Arabic-English). We then present experiments
showing that our harvested data not only substan-
tially improves translations of microblog text with
176
existing (and arguably inappropriate) translation
models, but that it improves the translation of
more traditional MT genres, like newswire. We
conclude in Section 6.
2 Related Work
Automatic collection of parallel data is a well-
studied problem. Approaches to finding par-
allel web documents automatically have been
particularly important (Resnik and Smith, 2003;
Fukushima et al, 2006; Li and Liu, 2008; Uszko-
reit et al, 2010; Ture and Lin, 2012). These
broadly work by identifying promising candidates
using simple features, such as URL similarity or
?gist translations? and then identifying truly par-
allel segments with more expensive classifiers.
More specialized resources were developed using
manual procedures to leverage special features of
very large collections, such as Europarl (Koehn,
2005).
Mining parallel or comparable messages from
microblogs has mainly relied on Cross-Lingual In-
formation Retrieval techniques (CLIR). Jelh et al
(2012) attempt to find pairs of tweets in Twitter us-
ing Arabic tweets as search queries in a CLIR sys-
tem. Afterwards, the model described in (Xu et al,
2001) is applied to retrieve a set of ranked trans-
lation candidates for each Arabic tweet, which are
then used as parallel candidates.
The work on mining parenthetical transla-
tions (Lin et al, 2008), which attempts to find
translations within the same document, has some
similarities with our work, since parenthetical
translations are within the same document. How-
ever, parenthetical translations are generally used
to translate names or terms, which is more lim-
ited than our work which extracts whole sentence
translations.
Finally, crowd-sourcing techniques to obtain
translations have been previously studied and ap-
plied to build datasets for casual domains (Zbib
et al, 2012; Post et al, 2012). These approaches
require remunerated workers to translate the mes-
sages, and the amount of messages translated per
day is limited. We aim to propose a method that
acquires large amounts of parallel data for free.
The drawback is that there is a margin of error in
the parallel segment identification and alignment.
However, our system can be tuned for precision or
for recall.
3 Parallel Segment Retrieval
We will first abstract from the domain of Mi-
croblogs and focus on the task of retrieving par-
allel segments from single documents. Prior work
on finding parallel data attempts to reason about
the probability that pairs of documents (x, y) are
parallel. In contrast, we only consider one doc-
ument at a time, defined by x = x1, x2, . . . , xn,
and consisting of n tokens, and need to deter-
mine whether there is parallel data in x, and if
so, where are the parallel segments and their lan-
guages. For simplicity, we assume that there are
at most 2 continuous segments that are parallel.
As representation for the parallel seg-
ments within the document, we use the tuple
([p, q], l, [u, v], r, a). The word indexes [p, q] and
[u, v] are used to identify the left segment (from
p to q) and right segment (from u to v), which
are parallel. We shall refer [p, q] and [u, v] as the
spans of the left and right segments. To avoid
overlaps, we set the constraint p ? q < u ? v.
Then, we use l and r to identify the language of
the left and right segments, respectively. Finally, a
represents the word alignment between the words
in the left and the right segments.
The main problem we address is to find the
parallel data when the boundaries of the parallel
segments are not defined explicitly. If we knew
the indexes [p, q] and [u, v], we could simply run
a language detector for these segments to find l
and r. Then, we would use an word alignment
model (Brown et al, 1993; Vogel et al, 1996),
with source s = xp, . . . , xq, target t = xu, . . . , xv
and lexical table ?l,r to calculate the Viterbi align-
ment a. Finally, from the probability of the word
alignments, we can determine whether the seg-
ments are parallel.
Thus, our model will attempt to find the opti-
mal values for the segments [p, q][u, v], languages
l, r and word alignments a jointly. However, there
are two problems with this approach. Firstly, word
alignment models generally attribute higher prob-
abilities to smaller segments, since these are the
result of a smaller product chain of probabilities.
In fact, because our model can freely choose the
segments to align, choosing only one word as the
left segment that is well aligned to a word in the
right segment would be the best choice. This
is obviously not our goal, since we would not
obtain any useful sentence pairs. Secondly, in-
ference must be performed over the combination
of all latent variables, which is intractable using
177
a brute force algorithm. We shall describe our
model to solve the first problem in 3.1 and our
dynamic programming approach to make the in-
ference tractable in 3.2.
3.1 Model
We propose a simple (non-probabilistic) three-
factor model that models the spans of the parallel
segments, their languages, and word alignments
jointly. This model is defined as follows:
S([u, v], r, [p, q],l, a | x) =
S?S ([p, q], [u, v] | x)?
S?L(l, r | [p, q], [u, v], x)?
S?T (a | [p, q], l, [u, v], r, x)
Each of the components is weighted by the pa-
rameters ?, ? and ?. We set these values empiri-
cally ? = 0.3, ? = 0.3 and ? = 0.4, and leave the
optimization of these parameters as future work.
We discuss the components of this model in turn.
Span score SS . We define the score of hypothe-
sized pair of spans [p, q], [u, v] as:
SS([p, q], [u, v] | x) =
(q ? p+ 1) + (v ? u+ 1)?
0<p??q?<u??v??n(q? ? p? + 1) + (v? ? u? + 1)
?
?([p, q], [u, v], x)
The first factor is a distribution over all spans that
assigns higher probability to segmentations that
cover more words in the document. It is highest
for segmentations that cover all the words in the
document (this is desirable since there are many
sentence pairs that can be extracted but we want
to find the largest sentence pair in the document).
The function ? takes on values of 0 or 1 depend-
ing on whether certain constraints are violated,
these include: parenthetical constraints that en-
force that spans must not break text within par-
enthetical characters and language constraints that
ensure that we do break a sequence of Mandarin
characters, Arabic words or Latin words.
Language score SL. The language score
SL(l, r | [p, q], [u, v], x) indicates whether the lan-
guage labels l, r are appropriate to the document
contents:
SL(l, r | [p, q], [u, v], x) =?q
i=p L(l, xi) +
?v
i=u L(r, xi)
n
where L(l, x) is a language detection function that
yields 1 if the word xi is in language l, and 0 oth-
erwise. We build the function simply by consid-
ering all words that are composed of Latin char-
acters as English, Arabic characters as Arabic and
Han characters as Mandarin. This approach is not
perfect, but it is simple and works reasonably well
for our purposes.
Translation score ST . The translation score
ST (a | [p, q], l, [u, v], r) indicates whether [p, q]
is a reasonable translation of [u, v] with the align-
ment a. We rely on IBM Model 1 probabilities for
this score:
ST (a | [p, q], l, [u, v], r, x) =
1
(q ? p+ 1)v?u+2
v?
i=u
PM1(xi | xai).
The lexical tables PM1 for the various language
pairs are trained a priori using available parallel
corpora. While IBM Model 1 produces worse
alignments than other models, in our problem, we
need to efficiently consider all possible spans, lan-
guage pairs and word alignments, which makes
the problem intractable. We will show that dy-
namic programing can be used to make this prob-
lem tractable, using Model 1. Furthermore, IBM
Model 1 has shown good performance for sen-
tence alignment systems previously (Xu et al,
2005; Braune and Fraser, 2010).
3.2 Inference
Our goal is to find the spans, language pair and
alignments such that:
argmax
[p,q],l,[u,v],r,a
S([p, q], l, [u, v], r, a | x) (1)
A high score indicates that the predicted bispan is
likely to correspond to a valid parallel span, so we
set a constant threshold ? to determine whether a
document has parallel data, i.e., the value of z:
z? = max
[u,v],r,[p,q],l,a
S([u, v], r, [p, q], l, a | x) > ?
Naively maximizing Eq. 1 would require
O(|x|6) operations, which is too inefficient to be
practical on large datasets. To process millions
of documents, this process would need to be op-
timized.
The main bottleneck of the naive algorithm is
finding new Viterbi Model 1 word alignments ev-
ery time we change the spans. Thus, we propose
178
an iterative approach to compute the Viterbi word
alignments for IBM Model 1 using dynamic pro-
gramming.
Dynamic programming search. The insight we
use to improve the runtime is that the Viterbi
word alignment of a bispan can be reused to cal-
culate the Viterbi word alignments of larger bis-
pans. The algorithm operates on a 4-dimensional
chart of bispans. It starts with the minimal valid
span (i.e., [0, 0], [1, 1]) and progressively builds
larger spans from smaller ones. Let Ap,q,u,v rep-
resent the Viterbi alignment (under ST ) of the bis-
pan [p, q], [u, v]. The algorithm uses the follow-
ing recursions defined in terms of four operations
?{+v,+u,+p,+q} that manipulate a single dimension
of the bispan to construct larger spans:
? Ap,q,u,v+1 = ?+v(Ap,q,u,v) adds one token to
the end of the right span with index v + 1 and
find the viterbi alignment for that token. This
requires iterating over all the tokens in the left
span, [p, q] and possibly updating their align-
ments. See Fig. 1 for an illustration.
? Ap,q,u+1,v = ?+u(Ap,q,u,v) removes the first to-
ken of the right span with index u, so we only
need to remove the alignment from u, which can
be done in time O(1).
? Ap,q+1,u,v = ?+q(Ap,q,u,v) adds one token to
the end of the left span with index q + 1, we
need to check for each word in the right span, if
aligning to the word in index q+1 yields a better
translation probability. This update requires n?
q + 1 operations.
? Ap+1,q,u,v = ?+p(Ap,q,u,v) removes the first
token of the left span with index p. After re-
moving the token, we need to find new align-
ments for all tokens that were aligned to p.
Thus, the number of operations for this update
is K ? (q ? p + 1), where K is the number of
words that were aligned to p. In the best case, no
words are aligned to the token in p, and we can
simply remove it. In the worst case, if all target
words were aligned to p, this update will result
in the recalculation of all Viterbi Alignments.
The algorithm proceeds until all valid cells have
been computed. One important aspect is that the
update functions differ in complexity, so the se-
quence of updates we apply will impact the per-
formance of the system. Most spans are reach-
able using any of the four update functions. For
instance, the span A2,3,4,5 can be reached us-
ing ?+v(A2,3,4,4), ?+u(A2,3,3,5), ?+q(A2,2,4,5) or
?+p(A1,3,4,5). However, we want to use ?+u
a b - A B
a
b
-
A
B
a b - A B
p
qu v
p
qu v?+v
Figure 1: Illustration of the ?+v operator. The
light gray boxes show the parallel span and the
dark boxes show the span?s Viterbi alignment.
In this example, the parallel message contains a
?translation? of a b to A B.
whenever possible, since it only requires one op-
eration, although that is not always possible. For
instance, the state A2,2,2,4 cannot be reached us-
ing ?+u, since the state A2,2,1,4 is not valid, be-
cause the spans overlap. If this happens, incre-
mentally more expensive updates need to be used,
such as ?+v, then ?+q, which are in the same order
of complexity. Finally, we want to minimize the
use of ?+p, which is quadratic in the worst case.
Thus, we use the following recursive formulation
that guarantees the optimal outcome:
Ap,q,u,v =
?
????
????
?+u(Ap,q,u?1,v) if u > q + 1
?+v(Ap,q,u,v?1) else if v > q + 1
?+p(Ap?1,q,u,v) else if q = p+ 1
?+q(Ap,q?1,u,v) otherwise
This transition function applies the cheapest
possible update to reach state Ap,q,u,v.
Complexity analysis. We can see that ?+u
is only needed in the following the cases
[0, 1][2, 2], [1, 2][3, 3], ? ? ? , [n ? 2, n ? 1][n, n].
Since, this update is quadratic in the worst
case, the complexity of this operations is
O(n3). The update ?+q, is applied to the cases
[?, 1][2, 2], [?, 2][3, 3], ? ? ? , [?, n?1], [n, n], where
? denotes any number within the span constraints
but not present in previous updates. Since, the
update is linear and we need to iterate through
all tokens twice, this update takes O(n3) opera-
tions. The update ?+v is applied for the cases
[?, 1][2, ?], [?, 2][3, ?], ? ? ? , [?, n? 1], [n, ?]. Thus,
with three degrees of freedom and a linear update,
it runs in O(n4) time. Finally, update ?+u runs in
constant time, but is run for all remaining cases,
which constitute O(n4) space. By summing the
179
executions of all updates, we observe that the or-
der of magnitude of our exact inference process is
O(n4). Note that for exact inference, it is not pos-
sible to get a lower order of magnitude, since we
need to at least iterate through all possible span
values once, which takes O(n4) time.
4 Parallel Data Extraction
We will now describe our method to extract par-
allel data from Microblogs. The target domains
in this work are Twitter and Sina Weibo, and
the main language pair is Chinese-English. Fur-
thermore, we also run the system for the Arabic-
English language pair using the Twitter data.
For the Twitter domain, we use a previously
crawled dataset from the years 2008 to 2013,
where one million tweets are crawled every day.
In total, we processed 1.6 billion tweets.
Regarding Sina Weibo, we built a crawler that
continuously collects tweets from Weibo. We start
from one seed user and collect his posts, and then
we find the users he follows that we have not con-
sidered, and repeat. Due to the rate limiting es-
tablished by the Weibo API1, we are restricted in
terms of number of requests every hour, which
greatly limits the amount of messages we can col-
lect. Furthermore, each request can only fetch up
to 100 posts from a user, and subsequent pages of
100 posts require additional API calls. Thus, to
optimize the number of parallel posts we can col-
lect per request, we only crawl all messages from
users that have at least 10 parallel tweets in their
first 100 posts. The number of parallel messages
is estimated by running our alignment model, and
checking if ? > ?, where ? was set empirically
initially, and optimized after obtaining annotated
data, which will be detailed in 5.1. Using this
process, we crawled 65 million tweets from Sina
Weibo within 4 months.
In both cases, we first filter the collection of
tweets for messages containing at least one trigram
in each language of the target language pair, deter-
mined by their Unicode ranges. This means that
for the Chinese-English language pair, we only
keep tweets with more than 3 Mandarin charac-
ters and 3 latin words. Furthermore, based on the
work in (Jelh et al, 2012), if a tweet A is iden-
tified as a retweet, meaning that it references an-
other tweetB, we also consider the hypothesis that
these tweets may be mutual translations. Thus, if
A and B contain trigrams in different languages,
1http://open.weibo.com/wiki/API??/en
these are also considered for the extraction of par-
allel data. This is done by concatenating tweets A
and B, and adding the constraint that [p, q] must
be within A and [u, v] must be within B. Finally,
identical duplicate tweets are removed.
After filtering, we obtained 1124k ZH-EN
tweets from Sina Weibo, 868k ZH-EN and 136k
AR-EN tweets from Twitter. These language pairs
are not definite, since we simply check if there is
a trigram in each language.
Finally, we run our alignment model described
in section 3, and obtain the parallel segments and
their scores, which measure how likely those seg-
ments are parallel. In this process, lexical tables
for EN-ZH language pair used by Model 1 were
built using the FBIS dataset (LDC2003E14) for
both directions, a corpus of 300K sentence pairs
from the news domain. Likewise, for the EN-
AR language pair, we use a fraction of the NIST
dataset, by removing the data originated from UN,
which leads to approximately 1M sentence pairs.
5 Experiments
We evaluate our method in two ways. First, intrin-
sically, by observing how well our method identi-
fies tweets containing parallel data, the language
pair and what their spans are. Second, extrinsi-
cally, by looking at how well the data improves
a translation task. This methodology is similar to
that of Smith et al (2010).
5.1 Parallel Data Extraction
Data. Our method needs to determine if a given
tweet contains parallel data, and if so, what is
the language pair of the data, and what segments
are parallel. Thus, we had a native Mandarin
speaker, also fluent in English, to annotate 2000
tweets sampled from crawled Weibo tweets. One
important question of answer is what portion of
the Microblogs contains parallel data. Thus, we
also use the random sample Twitter and annotated
1200 samples, identifying whether each sample
contains parallel data, for the EN-ZH and AR-EN
filtered tweets.
Metrics. To test the accuracy of the score S, we
ordered all 2000 samples by score. Then, we cal-
culate the precision, recall and accuracy at increas-
ing intervals of 10% of the top samples. We count
as a true positive (tp) if we correctly identify a par-
allel tweet, and as a false positive (fp) spuriously
detect a parallel tweet. Finally, a true negative (tn)
occurs when we correctly detect a non-parallel
180
tweet, and a false negative (fn) if we miss a par-
allel tweet. Then, we set the precision as tptp+fp ,
recall as tptp+fn and accuracy as tp+tntp+fp+tn+fn . Forlanguage identification, we calculate the accuracy
based on the number of instances that were iden-
tified with the correct language pair. Finally, to
evaluate the segment alignment, we use the Word
Error Rate (WER) metric, without substitutions,
where we compare the left and right spans of our
system and the respective spans of the reference.
We count an insertion error (I) for each word in
our system?s spans that is not present in the refer-
ence span and a deletion error (D) for each word
in the reference span that is not present in our sys-
tem?s spans. Thus, we set WER = D+IN , where
N is the number of tokens in the tweet. To com-
pute this score for the whole test set, we compute
the average of the WER for each sample.
Results. The precision, recall and accuracy
curves are shown in Figure 2. The quality of the
parallel sentence detection did not vary signifi-
cantly with different setups, so we will only show
the results for the best setup, which is the baseline
model with span constraints.
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
0.9	 ?
1	 ?
10%	 ? 20%	 ? 30%	 ? 40%	 ? 50%	 ? 60%	 ? 70%	 ? 80%	 ? 90%	 ? 100%	 ?
Precision	 ?
Recall	 ?
Accuracy	 ?
Figure 2: Precision, recall and accuracy curves
for parallel data detection. The y-axis denotes the
scores for each metric, and the x-axis denotes the
percentage of the highest scoring sentence pairs
that are kept.
From the precision and recall curves, we ob-
serve that most of the parallel data can be found
at the top 30% of the filtered tweets, where 5 in 6
tweets are detected correctly as parallel, and only
1 in every 6 parallel sentences is lost. We will de-
note the score threshold at this point as ?, which is
a good threshold to estimate on whether the tweet
is parallel. However, this parameter can be tuned
for precision or recall. We also see that in total,
30% of the filtered tweets are parallel. If we gen-
eralize this ratio for the complete set with 1124k
tweets, we can expect approximately 337k paral-
lel sentences. Finally, since 65 million tweets were
extracted to generate the 337k tweets, we estimate
that approximately 1 parallel tweet can be found
for every 200 tweets we process using our tar-
geted approach. On the other hand, from the 1200
tweets from Twitter, we found that 27 had parallel
data in the ZH-EN pair, if we extrapolate for the
whole 868k filtered tweets, we expect that we can
find 19530. 19530 parallel sentences from 1.6 bil-
lion tweets crawled randomly, represents 0.001%
of the total corpora. For AR-EN, a similar re-
sult was obtained where we expect 12407 tweets
out of the 1.6 billion to be parallel. This shows
that targeted approaches can substantially reduce
the crawling effort required to find parallel tweets.
Still, considering that billions of tweets are posted
daily, this is a substantial source of parallel data.
The remainder of the tests will be performed on
the Weibo dataset, which contains more parallel
data. Tests on the Twitter data will be conducted
as future work, when we process Twitter data on a
larger scale to obtain more parallel sentences.
For the language identification task, we had an
accuracy of 99.9%, since distinguishing English
and Mandarin is trivial. The small percentage of
errors originated from other latin languages (Ex:
French) due to our naive language detector.
As for the segment alignment task. Our base-
line system with no constraints obtains a WER of
12.86%, and this can be improved to 11.66% by
adding constraints to possible spans. This shows
that, on average, approximately 1 in 9 words on
the parallel segments is incorrect. However, trans-
lation models are generally robust to such kinds of
errors and can learn good translations even in the
presence of imperfect sentence pairs.
Among the 578 tweets that are parallel, 496
were extracted within the same tweet and 82 were
extracted from retweets. Thus, we see that the ma-
jority of the parallel data comes from within the
same tweet.
Topic analysis. To give an intuition about the
contents of the parallel data we found, we looked
at the distribution over topics of the parallel
dataset inferred by LDA (Blei et al, 2003). Thus,
we grouped the Weibo filtered tweets by users,
and ran LDA over the predicted English segments,
with 12 topics. The 7 most interpretable topics are
shown in Table 1. We see that the data contains a
181
# Topic Most probable words in topic
1 (Dating) love time girl live mv back word night rt wanna
2 (Entertainment) news video follow pong image text great day today fans
3 (Music) cr day tour cn url amazon music full concert alive
4 (Religion) man god good love life heart would give make lord
5 (Nightlife) cn url beijing shanqi party adj club dj beijiner vt
6 (Chinese News) china chinese year people world beijing years passion country government
7 (Fashion) street fashion fall style photo men model vogue spring magazine
Table 1: Most probable words inferred using LDA in several topics from the parallel data extracted from
Weibo. Topic labels (in parentheses) were assigned manually for illustration purposes.
variety of topics, both formal (Chinese news, reli-
gion) and informal (entertainment, music).
Example sentence pairs. To gain some perspec-
tive on the type of sentence pairs we are extract-
ing, we will illustrate some sentence pairs we
crawled and aligned automatically. Table 2 con-
tains 5 English-Mandarin and 4 English-Arabic
sentence pairs that were extracted automatically.
These were chosen, since they contain some as-
pects that are characteristic of the text present in
Microblogs and Social Media. These are:
? Abbreviations - In most sentence pairs exam-
ples, we can witness the use of abbreviated
forms of English words, such as wanna, TMI,
4 and imma. These can be normalized as want
to, too much information, for and I am going
to, respectively. In sentence 5, we observe that
this phenomena also occurs in Mandarin. We
find that TMD is a popular way to write???
whose Pinyin rendering is ta? ma? de. The mean-
ing of this expression depends on the context it
is used, and can convey a similar connotation
as adding the intensifier the hell to an English
sentence.
? Jargon - Another common phenomena is the
appearance of words that are only used in sub-
communities. For instance, in sentence pair 4,
we the jargon word cday is used, which is a col-
loquial variant for birthday.
? Emoticons - In sentence 8, we observe the pres-
ence of the emoticon :), which is frequently
used in this media. We found that emoticons are
either translated as they are or simply removed,
in most cases.
? Syntax errors - In the domain of microblogs, it
is also common that users do not write strictly
syntactic sentences, for instance, in sentence
pair 7, the sentence onni this gift only 4 u, is
clearly not syntactically correct. Firstly, onni
is a named entity, yet it is not capitalized. Sec-
ondly, a comma should follow onni. Thirdly, the
verb is should be used after gift. Having exam-
ples of these sentences in the training set, with
common mistakes (intentional or not), might
become a key factor in training MT systems that
can be robust to such errors.
? Dialects - We can observe a much broader range
of dialects in our data, since there are no di-
alect standards in microblogs. For instance, in
sentence pair 6, we observe an arabic word (in
bold) used in the spoken Arabic dialect used in
some countries along the shores of the Persian
Gulf, which means means the next. In standard
Arabic, a significantly different form is used.
We can also see in sentence pair 9 that our
aligner does not alway make the correct choice
when determining spans. In this case, the segment
RT @MARYAMALKHAWAJA: was included in the
English segment spuriously, since it does not cor-
respond to anything in the Arabic counterpart.
5.2 Machine Translation Experiments
We report on machine translation experiments us-
ing our harvested data in two domains: edited
news and microblogs.
News translation. For the news test, we cre-
ated a new test set from a crawl of the Chinese-
English documents on the Project Syndicate web-
site2, which contains news commentary articles.
We chose to use this data set, rather than more
standard NIST test sets to ensure that we had re-
cent documents in the test set (the most recent
NIST test sets contain documents published in
2007, well before our microblog data was created).
We extracted 1386 parallel sentences for tuning
and another 1386 sentences for testing, from the
manually aligned segments. For this test set, we
used 8 million sentences from the full NIST par-
allel dataset as the language model training data.
We shall call this test set Syndicate.
2http://www.project-syndicate.org/
182
ENGLISH MANDARIN
1 i wanna live in a wes anderson world ??????Wes Anderson????
2 Chicken soup, corn never truly digests. TMI. ??????????????????.??
3 To DanielVeuleman yea iknw imma work on that ?DanielVeuleman?????????????????
4 msg 4 Warren G his cday is today 1 yr older. ????Warren G????????????????
5 Where the hell have you been all these years? ????TMD????
ENGLISH ARABIC
6 It?s gonna be a warm week! Qk ?


AJ
? @ ? ?J.?B@
7 onni this gift only 4 u ?? ?? 	? ?K
Y?? @ ? 	Y? ?

	
G?

@
8 sunset in aqaba :) (: ?J. ???@ ?

	
? ?? ??@ H. ?Q
	
?
9 RT @MARYAMALKHAWAJA: there is a call @Y 	? ??A 	J? ?Y? ?


	
? H@Q?A 	??? Z @Y 	K ?A 	J?for widespread protests in #bahrain tmrw
Table 2: Examples of English-Mandarin and English-Arabic sentence pairs. The English-Mandarin
sentences were extracted from Sina Weibo and the English-Arabic sentences were extracted from Twitter.
Some messages have been shorted to fit into the table. Some interesting aspects of these sentence pairs
are marked in bold.
Microblog translation. To carry out the mi-
croblog translation experiments, we need a high
quality parallel test set. Since we are not aware
of such a test set, we created one by manually se-
lecting parallel messages from Weibo. Our proce-
dure was as follows. We selected 2000 candidate
Weibo posts from users who have a high num-
ber of parallel tweets according to our automatic
method (at least 2 in every 5 tweets). To these, we
added another 2000 messages from our targeted
Weibo crawl, but these had no requirement on the
proportion of parallel tweets they had produced.
We identified 2374 parallel segments, of which we
used 1187 for development and 1187 for testing.
We refer to this test set as Weibo.3
Obviously, we removed the development and
test sets from our training data. Furthermore, to
ensure that our training data was not too similar to
the test set in the Weibo translation task, we fil-
tered the training data to remove near duplicates
by computing edit distance between each paral-
lel sentence in the heldout set and each training
instance. If either the source or the target sides
of the a training instance had an edit distance of
less than 10%, we removed it.4 As for the lan-
guage models, we collected a further 10M tweets
from Twitter for the English language model and
another 10M tweets from Weibo for the Chinese
language model.
3We acknowledge that self-translated messages are prob-
ably not a typically representative sample of all microblog
messages. However, we do not have the resources to produce
a carefully curated test set with a more broadly representative
distribution. Still, we believe these results are informative as
long as this is kept in mind.
4Approximately 150,000 training instances removed.
Syndicate Weibo
ZH-EN EN-ZH ZH-EN EN-ZH
FBIS 9.4 18.6 10.4 12.3
NIST 11.5 21.2 11.4 13.9
Weibo 8.75 15.9 15.7 17.2
FBIS+Weibo 11.7 19.2 16.5 17.8
NIST+Weibo 13.3 21.5 16.9 17.9
Table 3: BLEU scores for different datasets in dif-
ferent translation directions (left to right), broken
with different training corpora (top to bottom).
Baselines. We report results on these test sets us-
ing different training data. First, we use the FBIS
dataset which contains 300K high quality sentence
pairs, mostly in the broadcast news domain. Sec-
ond, we use the full 2012 NIST Chinese-English
dataset (approximately 8M sentence pairs, includ-
ing FBIS). Finally, we use our crawled data (re-
ferred as Weibo) by itself and also combined with
the two previous training sets.
Setup. We use the Moses phrase-based MT sys-
tem with standard features (Koehn et al, 2003).
For reordering, we use the MSD reordering
model (Axelrod et al, 2005). As the language
model, we use a 5-gram model with Kneser-
Ney smoothing. The weights were tuned using
MERT (Och, 2003). Results are presented with
BLEU-4 (Papineni et al, 2002).
Results. The BLEU scores for the different par-
allel corpora are shown in Table 3 and the top 10
out-of-vocabulary (OOV) words for each dataset
are shown in Table 4. We observe that for the
Syndicate test set, the NIST and FBIS datasets
183
Syndicate (test) Weibo (test)
FBIS NIST Weibo FBIS NIST Weibo
obama (83) barack (59) democracies (15) 2012 (24) showstudio (9) submissions (4)
barack (59) namo (6) imbalances (13) alanis (13) crue (9) ivillage (4)
princeton (40) mitt (6) mahmoud (12) crue (9) overexposed (8) scola (3)
ecb (8) guant (6) millennium (9) showstudio (9) tweetmeian (5) rbst (3)
bernanke (8) fairtrade (6) regimes (8) overexposed (8) tvd (5) curitiba (3)
romney (7) hollande (5) wolfowitz (7) itunes (8) iheartradio (5) zeman (2)
gaddafi (7) wikileaks (4) revolutions (7) havoc (8) xoxo (4) @yaptv (2)
merkel (7) wilders (3) qaddafi (7) sammy (6) snoop (4) witnessing (2)
fats (7) rant (3) geopolitical (7) obama (6) shinoda (4) whoohooo (2)
dialogue (7) esm (3) genome (7) lol (6) scrapbook (4) wbr (2)
Table 4: The most frequent out-of-vocabulary (OOV) words and their counts for the two English-source
test sets with three different training sets.
perform better than our extracted parallel data.
This is to be expected, since our dataset was ex-
tracted from an extremely different domain. How-
ever, by combining the Weibo parallel data with
this standard data, improvements in BLEU are ob-
tained. Error analysis indicates that one major fac-
tor is that names from current events, such as Rom-
ney and Wikileaks do not occur in the older NIST
and FBIS datasets, but they are represented in the
Weibo dataset. Furthermore, we also note that the
system built on the Weibo dataset does not per-
form substantially worse than the one trained on
the FBIS dataset, a further indication that harvest-
ing parallel microblog data yields a diverse collec-
tion of translated material.
For the Weibo test set, a significant improve-
ment over the news datasets can be achieved us-
ing our crawled parallel data. Once again newer
terms, such as iTunes, are one of the reasons older
datasets perform less well. However, in this case,
the top OOV words of the news domain datasets
are not the most accurate representation of cov-
erage problems in this domain. This is because
many frequent words in microblogs, e.g., nonstan-
dard abbreviations, like u and 4 are found in the
news domain as words, albeit with different mean-
ings. Thus, the OOV table gives an incomplete
picture of the translation problems when using
the news domain corpora to translate microblogs.
Also, some structural errors occur when training
with the news domain datasets, one such example
is shown in table 5, where the character ? is in-
correctly translated to said. This occurs because
this type of constructions is infrequent in news
datasets. Furthermore, we can see that compound
expressions, such as the translation from ???
? to party time are also learned.
Finally, we observe that combining the datasets
Source ?sam farrar??????
Reference to sam farrar , party time
FBIS farrar to sam said , in time
NIST to sam farrar said , the moment
WEIBO to sam farrar , party time
Table 5: Translation Examples using different
training sets.
yields another gain over individual datasets, both
in the Syndicate and in the Weibo test sets.
6 Conclusion
We presented a framework to crawl parallel data
from microblogs. We find parallel data from sin-
gle posts, with translations of the same sentence
in two languages. We show that a considerable
amount of parallel sentence pairs can be crawled
from microblogs and these can be used to improve
Machine Translation by updating our translation
tables with translations of newer terms. Further-
more, the in-domain data can substantially im-
prove the translation quality on microblog data.
The resources described in this paper and fur-
ther developments are available to the general pub-
lic at http://www.cs.cmu.edu/?lingwang/utopia.
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors wish
to express their gratitude to thank William Cohen,
Noah Smith, Waleed Ammar, and the anonymous
reviewers for their insight and comments. We are
also extremely grateful to Brendan O?Connor for
providing the Twitter data and to Philipp Koehn
and Barry Haddow for providing the Project Syn-
dicate data.
184
References
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT.
[Blei et al2003] David M. Blei, Andrew Y. Ng, and
Michael I. Jordan. 2003. Latent dirichlet alocation.
J. Mach. Learn. Res., 3:993?1022, March.
[Braune and Fraser2010] Fabienne Braune and Alexan-
der Fraser. 2010. Improved unsupervised sentence
alignment for symmetrical and asymmetrical paral-
lel corpora. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 81?89, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Lin-
guist., 19:263?311, June.
[Fukushima et al2006] Ken?ichi Fukushima, Kenjiro
Taura, and Takashi Chikayama. 2006. A fast and
accurate method for detecting English-Japanese par-
allel texts. In Proceedings of the Workshop on Mul-
tilingual Language Resources and Interoperability,
pages 60?67, Sydney, Australia, July. Association
for Computational Linguistics.
[Gimpel et al2011] Kevin Gimpel, Nathan Schneider,
Brendan O?Connor, Dipanjan Das, Daniel Mills, Ja-
cob Eisenstein, Michael Heilman, Dani Yogatama,
Jeffrey Flanigan, and Noah A. Smith. 2011. Part-
of-speech tagging for twitter: annotation, features,
and experiments. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 42?47, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
[Jelh et al2012] Laura Jelh, Felix Hiebel, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montre?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
[Koehn2005] Philipp Koehn. 2005. Europarl: A Par-
allel Corpus for Statistical Machine Translation. In
Proceedings of the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
[Li and Liu2008] Bo Li and Juan Liu. 2008. Mining
Chinese-English parallel corpora from the web. In
Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing (IJCNLP).
[Lin et al2008] Dekang Lin, Shaojun Zhao, Benjamin
Van Durme, and Marius Pas?ca. 2008. Mining par-
enthetical translations from the web by word align-
ment. In Proceedings of ACL-08: HLT, pages 994?
1002, Columbus, Ohio, June. Association for Com-
putational Linguistics.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Smith et al2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Proceedings of the 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
[Ture and Lin2012] Ferhan Ture and Jimmy Lin. 2012.
Why not grab a free lunch? mining large corpora for
parallel sentences to improve translation modeling.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 626?630, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
[Uszkoreit et al2010] Jakob Uszkoreit, Jay Ponte,
Ashok C. Popat, and Moshe Dubiner. 2010. Large
scale parallel document mining for machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 1101?
1109.
[Vogel et al1996] Stephan Vogel, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based word align-
ment in statistical translation. In Proceedings of the
16th conference on Computational linguistics - Vol-
ume 2, COLING ?96, pages 836?841, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
[Xu et al2001] Jinxi Xu, Ralph Weischedel, and Chanh
Nguyen. 2001. Evaluating a probabilistic model
185
for cross-lingual information retrieval. In Proceed-
ings of the 24th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?01, pages 105?110, New
York, NY, USA. ACM.
[Xu et al2005] Jia Xu, Richard Zens, and Hermann
Ney. 2005. Sentence segmentation using ibm word
alignment model 1. In Proceedings of EAMT 2005
(10th Annual Conference of the European Associa-
tion for Machine Translation, pages 280?287.
[Zbib et al2012] Rabih Zbib, Erika Malchiodi, Jacob
Devlin, David Stallard, Spyros Matsoukas, Richard
Schwarz, John Makhoul, Omar F. Zaidan, and Chris
Callison-Burch. 2012. Machine translation of Ara-
bic dialects. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies.
186
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 48?53
Manchester, August 2008
Speech Translation for Triage of Emergency Phonecalls in Minority 
Languages 
Udhyakumar Nallasamy, Alan W Black, Tanja 
Schultz, Robert Frederking 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213  USA 
udhay@cmu.edu, 
{awb,ref,tanja}@cs.cmu.edu 
Jerry Weltman 
Louisiana State University 
Baton Rouge,  
Louisiana 70802  USA 
jweltm2@lsu.edu 
 
Abstract 
We describe Ayudame, a system de-
signed to recognize and translate Spanish 
emergency calls for better dispatching. 
We analyze the research challenges in 
adapting speech translation technology to 
9-1-1 domain. We report our initial re-
search in 9-1-1 translation system design, 
ASR experiments, and utterance classifi-
cation for translation.  
1 Introduction 
In the development of real-world-applicable lan-
guage technologies, it is good to find an applica-
tion with a significant need, and with a complex-
ity that appears to be within the capabilities of 
current existing technology.  Based on our ex-
perience in building speech-to-speech translation, 
we believe that some important potential uses of 
the technology do not require a full, complete 
speech-to-speech translation system; something 
much more lightweight can be sufficient to aid 
the end users (Gao et al 2006). 
A particular task of this kind is dealing with 
emergency call dispatch for police, ambulance, 
fire and other emergency services (in the US the 
emergency number is 9-1-1).  A dispatcher must 
answer a large variety of calls and, due to the 
multilingual nature of American society, they 
may receive non-English calls and be unable to 
service them due to lack of knowledge of the 
caller language. 
                                               
 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
    Figure 1. Ayudame system architecture 
 
As a part of a pilot study into the feasibility of 
dealing with non-English calls by a mono-lingual 
English-speaking dispatcher, we have designed a 
translation system that will aid the dispatcher in 
communicating without understanding the 
caller?s language. 
48
The fundamental idea is to use utterance clas-
sification of the non-English input.  The non-
English is first recognized by a speech recogni-
tion system; then the output is classified into a 
small number of domain-specific classes called 
Domain Acts (DAs) that can indicate directly to 
the dispatcher the general intended meaning of 
the spoken phrase.  Each DA may have a few 
important parameters to be translated, such as 
street addresses (Levin et al 2003; Langley 
2003). The dispatcher can then select from a lim-
ited number of canned responses to this through 
a simple menu system.  We believe the reduction 
in complexity of such a system compared to a 
full speech-to-speech translation will be advanta-
geous because it should be much cheaper to con-
struct, easier to port to new languages, and, im-
portantly, sufficient to do the job of processing 
emergency calls.  
In the ?NineOneOne? project, we have de-
signed an initial prototype system, which we call 
?Ayudame? (Spanish word for ?Help me?).  Fig-
ure 1 gives an overview of the system architec-
ture. 
2 The NineOneOne Domain 
Our initial interest in this domain was due to con-
tact from the Cape Coral Police Department 
(CCPD) in Florida.  They were interested in in-
vestigating how speech-to-speech translations 
could be used in emergency 9-1-1 dispatch sys-
tems.  Most current emergency dispatching cen-
ters use some proprietary human translation ser-
vice, such as Language Line (Language Line 
Services).  Although this service provides human 
translation services for some 180 languages, it is 
far from ideal.  Once the dispatcher notes that the 
caller cannot speak/understand English, they 
must initiate the call to Language Line, including 
identifying themselves to the Language Line op-
erator, before the call can actually continue.  This 
delay can be up to a minute, which is not ideal in 
an emergency situation.    
After consulting with CCPD, and collecting a 
number of example calls, it was clear that full 
speech-to-speech translation was not necessary 
and that a limited form of translation through 
utterance classification (Lavie et al 2001) might 
be sufficient to provide a rapid response to non-
English calls.  The language for our study is 
Spanish.  Cape Coral is on the Gulf Coast of  
Florida and has fewer Spanish speakers than e.g. 
the Miami area, but still sufficient that a number 
of calls are made to their emergency service in 
Spanish, yet many of their operators are not 
sufficiently fluent in Spanish to deal with the 
calls. 
There are a number of key pieces of 
information that a dispatcher tries to collect 
before passing on the information to the 
appropriate emergency service.  This includes 
things like location, type of emergency, urgency, 
if anyone is hurt, if the situation is dangerous, 
etc.  In fact many dispaching organizations have 
existing, well-defined  policies on what 
information they should collect for different 
types of emergencies.  
3 Initial system design 
Based on the domain's characteristics, in addition 
to avoiding full-blown translation, we are follow-
ing a highly asymmetrical design for the system 
(Frederking et al 2000).  The dispatcher is al-
ready seated at a workstation, and we intend to 
keep them ?in the loop?, for both technical and 
social reasons.  So in the dispatcher-to-caller di-
rection, we can work with text and menus, sim-
plifying the technology and avoiding some cog-
nitive complexity for the operator.  So in the dis-
patcher-to-caller direction we require  
 no English ASR, 
 no true English-to-Spanish MT, and 
 simple, domain-limited, Spanish speech 
synthesis. 
The caller-to-dispatcher direction is much more 
interesting. In this direction we require 
 Spanish ASR that can handle emotional 
spontaneous telephone speech in mixed 
dialects, 
 Spanish-to-English MT, but 
 no English Speech Synthesis. 
We have begun to consider the user interfaces 
for Ayudame as well.  For ease of integration 
with pre-existing dispatcher workstations, we 
have chosen to use a web-based graphical inter-
face.  For initial testing of the prototype, we plan 
to run in ?shadow? mode, in parallel with live 
dispatching using the traditional approach.  Thus 
Ayudame will have a listen-only connection to 
the telephone line, and will run a web server to 
interact with the dispatcher.  Figure 2 shows an 
initial design of the web-based interface.  There 
are sections for a transcript, the current caller 
utterance, the current dispatcher response 
choices, and a button to transfer the interaction to 
a human translator as a fall-back option.  For 
each utterance, the DA classification is displayed 
in addition to the actual utterance (in case the 
dispatcher knows some Spanish). 
49
 Figure 2. Example of initial GUI design 
 
4 Automatic Speech Recognition 
An important requirement for such a system is 
the ability to be able to recognize the incoming 
non-English speech with a word error rate suffi-
ciently low for utterance classification and pa-
rameter translation to be possible.  The issues in 
speech recognition for this particular domain in-
clude: telephone speech (which is through a lim-
ited bandwidth channel); background noise (the 
calls are often from outside or in noisy places); 
various dialects of Spanish, and potential stressed 
speech.  Although initially we expected a sub-
stantial issue with recognizing stressed speakers, 
as one might expect in emergency situations, in 
the calls we have collected so far, although it is 
not a negligible issue, it is far less important that 
we first expected. 
The Spanish ASR system is built using the 
Janus Recognition Toolkit (JRTk) (Finke et al 
1997) featuring the HMM-based IBIS decoder 
(Soltau et al 2001). Our speech corpus consists 
of 75 transcribed 9-1-1 calls, with average call 
duration of 6.73 minutes (min: 2.31 minutes, 
max: 13.47 minutes). The average duration of 
Spanish speech (between interpreter and caller) 
amounts to 4.8 minutes per call. Each call has 
anywhere from 46 to 182 speaker turns with an 
average of 113 speaker turns per call. The turns 
that have significant overlap between speakers 
are omitted from the training and test set. The 
acoustic models are trained on 50 Spanish 9-1-1 
calls, which amount to 4 hours of speech data.  
 
 
The system uses three-state, left-to-right, sub-
phonetically tied acoustic models with 400 con-
text-dependent distributions with the same num-
ber of codebooks. Each codebook has 32 gaus-
sians per state. The front-end feature extraction 
uses standard 39 dimensional Mel-scale cepstral 
coefficients and applies Linear Discriminant 
Analysis (LDA) calculated from the training 
data. The acoustic models are seeded with initial 
alignments from GlobalPhone Spanish acoustic 
models trained on 20 hours of speech recorded 
from native Spanish speakers (Schultz et al 
1997). The vocabulary size is 65K words. The 
language model consists of a trigram model 
trained on the manual transcriptions of 40 calls 
and interpolated with a background model 
trained on GlobalPhone Spanish text data con-
sisting of 1.5 million words (Schultz et al 1997). 
The interpolation weights are determined using 
the transcriptions of 10 calls (development set). 
The test data consists of 15 telephone calls from 
different speakers, which amounts to a total of 1 
hour. Both development and test set calls con-
sisted of manually segmented and transcribed 
speaker turns that do not have a significant over-
lap with other speakers. The perplexity of the test 
set according to the language model is 96.7. 
The accuracy of the Spanish ASR on the test 
set is 76.5%.  This is a good result for spontane-
ous telephone-quality speech by multiple un-
known speakers, and compares favourably to the 
ASR accuracy of other spoken dialog systems.  
We had initially planned to investigate novel 
ASR techniques designed for stressed speech and 
multiple dialects, but to our surprise these do not 
50
seem to be required for this application.  Note 
that critical information such as addresses will be 
synthesized back to the caller for confirmation in 
the full system. So, for the time-being we will 
concentrate on the accuracy of the DA classifica-
tion until we can show that improving ASR accu-
racy would significantly help. 
5 Utterance Classification 
As mentioned above, the translation approach we 
are using is based on utterance classification. The 
Spanish to English translation in the Ayudame 
system is a two-step process. The ASR 
hypothesis is first classified into domain-specific 
Domain Acts (DA). Each DA has a 
predetermined set of parameters. These 
parameters are identified and translated using a 
rule-based framework.  For this approach to be 
accomplished with reasonable effort levels, the 
total number of types of parameters and their 
complexity must be fairly limited in the domain, 
such as addresses and injury types. This section 
explains our DA tagset and classification 
experiments. 
5.1 Initial classification and results 
The initial evaluation (Nallasamy et al 2008) 
included a total of 845 manually labeled turns in 
our 9-1-1 corpus. We used a set of 10 tags to an-
notate the dialog turns. The distribution of the 
tags are listed below 
 
Tag (Representation) Frequency 
Giving Name 80 
Giving Address 118 
Giving Phone number 29 
Requesting Ambulance 8 
Requesting Fire Service 11 
Requesting Police 24 
Reporting Injury/Urgency 61 
Yes 119 
No 24 
Others 371 
Table 1. Distribution of first-pass tags in the 
corpus. 
 
We extracted bag-of-word features and trained a 
Support Vector Machine (SVM) classifier (Bur-
ges, 1998) using the above dataset. A 10-fold 
stratified cross-validation has produced an aver-
age accuracy of 60.12%. The accuracies of indi-
vidual tags are listed below. 
 
Tag Accuracy  
  (%) 
Giving Name 57.50 
Giving Address 38.98 
Giving Phone number 48.28 
Req. Ambulance 62.50 
Req. Fire Service 54.55 
Req. Police 41.67 
Reporting Injury/Urgency 39.34 
Yes 52.94 
No 54.17 
Others 75.74 
Table 2. Classification accuracies of first-pass 
tags. 
5.2 Tag-set improvements 
We improved both the DA tagset and the 
classification framework in our second-pass 
classification, compared to our initial  
experiment. We had identified several issues in 
our first-pass classification:  
 We had forced each dialog turn to have a 
single tag. However, the tags and the 
dialog turns don?t conform to this 
assumption. For example, the dialog 
?Yes, my husband has breathing prob-
lem. We are at two sixty-one Oak 
Street?1 should get 3 tags: ?Yes?, ?Giv-
ing-Address?, ?Requesting-Ambulance?.  
 Our analysis of the dataset alo showed 
that the initial set of tags are not 
exhaustive enough to cover the whole 
range of dialogs required to be translated 
and conveyed to the dispatcher.  
We made several iterations over the tagset to 
ensure that it is both compact and achieves  
requisite coverage. The final tag set consists of 
67 entries. We manually annotated 59 calls with 
our new tagset using a web interface. The 
distribution of the top 20 tags is listed below. 
The whole list of tags can be found in the 
NineOneOne project webpage: 
http://www.cs.cmu.edu/~911/ 
 
                                               
1
 The dialog is English Translation of  ?s?, mi esposo le falta 
el aire. es ac? en el dos sesenta y uno Oak Street?. It is 
extracted from the transcription of a CCPD 9-1-1 
emergency call, with address modified to protect privacy 
51
Tag (Representation) Frequency 
Yes 227 
Giving-Address 133 
Giving-Location 113 
Giving-Name 107 
No 106 
Other 94 
OK 81 
Thank-You 51 
Reporting-Conflict 43 
Describing-Vehicle 42 
Giving-Telephone-Number 40 
Hello 36 
Reporting-Urgency-Or-Injury 34 
Describing-Residence 28 
Dont-Know 19 
Dont-Understand 16 
Giving-Age 15 
Goodbye 15 
Giving-Medical-Symptoms 14 
Requesting-Police 12 
Table 3. Distribution of top 20 second-pass 
tags 
 
The new tagset is hierarchical, which allows 
us to evaluate the classifier at different levels of 
the hierarchy, and eventually select the best 
trade-off between the number of tags and 
classification accuracy. For example, the first 
level of tags for reporting incidents includes the 
five most common incidents, viz, Reporting-
Conflict, Reporting-Robbery, Reporting-Traffic-
accident, Reporting-Urgency-or-Injury and 
Reporting-Fire. The second level of tags are used 
to convey more detailed information about the 
above incidents (eg. Reporting-Weapons in the 
case of conflict) or rare incidents (eg. Reporting-
Animal-Problem). 
5.3 Second-pass classification and Results 
We also improved our classification 
framework to allow multiple tags for a single 
turn and to easily accomodate any new tags in 
the future. Our earlier DA classification used a 
multi-class classifier, as each turn was restricted 
to have a single tag. To accomodate multiple tags 
for a single turn, we trained binary classifiers for 
each tag. All the utterances of the corresponding 
tag are marked positive examples and the rest are 
marked as negative examples. Our new data set 
has 1140 dialog turns and 1331 annotations. Note 
that the number of annotations is more than the 
number of labelled turns as each turn may have 
multiple tags. We report classification accuracies 
in the following table for each tag based on 10-
fold cross-validation: 
 
Tag (Representation) Accuracy   
   (%) 
Yes 87.32 
Giving-Address 42.71 
Giving-Location 87.32 
Giving-Name 42.71 
No 37.63 
Other 54.98 
OK 72.5 
Thank-You 41.14 
Reporting-Conflict 79.33 
Describing-Vehicle 96.82 
Giving-Telephone-Number 39.37 
Hello 38.79 
Reporting-Urgency-Or-Injury 49.8 
Describing-Residence 92.75 
Dont-Know 41.67 
Dont-Understand 36.03 
Giving-Age 64.95 
Goodbye 87.27 
Giving-Medical-Symptoms 47.44 
Requesting-Police 79.94 
Table 4. Classification accuracies of 
individual second-pass tags 
 
The average accuracy of the 20 tags is 
58.42%. Although multiple classifiers increase 
the computational complexity during run-time, 
they are independent of each other, so we can run 
them in parallel. To ensure the consistency and 
clarity of the new tag set, we had a second 
annotator label 39 calls. The inter-coder 
agreement (Kappa coefficient) between the two 
annotators is 0.67. This is considered substantial 
agreement between the annotators, and confirms 
the consistency of the tag set. 
6 Conclusion 
The work reported here demonstrates that we can 
produce Spanish ASR for Spanish emergency 
calls with reasonable accuracy (76.5%), and clas-
sify manual transcriptions of these calls with rea-
sonable accuracy (60.12% on the original tagset, 
52
58.42% on the new, improved tagset).  We be-
lieve these results are good enough to justify the 
next phase of research, in which we will develop, 
user-test, and evaluate a full pilot system. We are 
also investigating a number of additional tech-
niques to improve the DA classification accura-
cies.  Further we believe that we can design the 
overall dialog system to ameliorate the inevitable 
remaining misclassifications, based in part on the 
confusion matrix of actual errors (Nallasamy et 
al, 2008).  But only actual user tests of a pilot 
system will allow us to know whether an even-
tual deployable system is really feasible. 
Acknowledgements 
This project is funded by NSF Grant No: IIS-
0627957 ?NineOneOne: Exploratory Research 
on Recognizing Non-English Speech for Emer-
gency Triage in Disaster Response?. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the 
authors and do not necessarily reflect the views 
of sponsors. 
References 
Burges C J C, A tutorial on support vector machines 
for pattern recognition, In Proc. Data Mining and 
Knowledge Discovery, pp 2(2):955-974, USA, 
1998 
Finke M, Geutner P, Hild H, Kemp T, Ries K and 
Westphal M, The Karlsruhe-Verbmobil Speech 
Recognition Engine, In Proc. IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP), pp. 83-86, Germany, 1997 
Frederking R, Rudnicky A, Hogan C and Lenzo K, 
Interactive Speech Translation in the Diplomat 
Project, Machine Translation Journal 15(1-2), 
Special issue on Spoken Language Translation, pp. 
61-66, USA, 2000 
Gao Y, Zhou B, Sarikaya R, Afify M, Kuo H, Zhu W, 
Deng Y, Prosser C, Zhang W and Besacier L, IBM 
MASTOR SYSTEM: Multilingual Automatic 
Speech-to-Speech Translator, In Proc. First Inter-
national Workshop on Medical Speech Translation, 
pp. 53-56, USA, 2006 
Langley C, Domain Action Classification and Argu-
ment Parsing for Interlingua-based Spoken Lan-
guage Translation. PhD thesis, Carnegie Mellon 
University, Pittsburgh, PA, 2003 
Language Line Services http://www.languageline.com 
Lavie A, Balducci F, Coletti P, Langley C, Lazzari G, 
Pianesi F, Taddei L and Waibel A, Architecture 
and Design Considerations in NESPOLE!: a 
Speech Translation System for E-Commerce Ap-
plications,. In Proc. Human Language Technolo-
gies (HLT), pp 31-34, USA, 2001 
Levin L, Langley C, Lavie A, Gates D, Wallace D and 
Peterson K, Domain Specific Speech Acts for Spo-
ken Language Translation, In Proc. 4th SIGdial 
Workshop on Discourse and Dialogue, pp. 208-
217, Japan, 2003 
Nallasamy U, Black A, Schultz T and Frederking R, 
NineOneOne: Recognizing and Classifying Speech 
for Handling Minority Language Emergency Calls, 
In Proc. 6th International conference on Language 
Resources and Evaluation (LREC), Morocco, 2008 
NineOneOne project webpage 
[www.cs.cmu.edu/~911] 
Schultz T, Westphal M and Waibel A, The 
GlobalPhone Project: Multilingual LVCSR with 
JANUS-3, In Proc. Multilingual Information Re-
trieval Dialogs: 2nd SQEL Workshop, pp. 20-27, 
Czech Republic, 1997 
Soltau H, Metze F, F?ugen C and Waibel A, A One 
Pass-Decoder Based on Polymorphic Linguistic 
Context Assignment, In Proc. IEEE workshop on 
Automatic Speech Recognition and Understanding 
(ASRU), Italy, 2001 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
53
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 91?94,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards Improving the Naturalness of
Social Conversations with Dialogue Systems
Matthew Marge, Joa?o Miranda, Alan W Black, Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{mrmarge,jmiranda,awb,air}@cs.cmu.edu
Abstract
We describe an approach to improving
the naturalness of a social dialogue sys-
tem, Talkie, by adding disfluencies and
other content-independent enhancements
to synthesized conversations. We investi-
gated whether listeners perceive conversa-
tions with these improvements as natural
(i.e., human-like) as human-human con-
versations. We also assessed their ability
to correctly identify these conversations as
between humans or computers. We find
that these enhancements can improve the
perceived naturalness of conversations for
observers ?overhearing? the dialogues.
1 Introduction
An enduring problem in spoken dialogue systems
research is how to make conversations between
humans and computers approach the naturalness
of human-human conversations. Although this
has been addressed in several goal-oriented dia-
logue systems (e.g., for tutoring, question answer-
ing, etc.), social dialogue systems (i.e., non-task-
oriented) have not significantly advanced beyond
so-called ?chatbots?. Proper social dialogue sys-
tems (Bickmore and Cassell, 2004; Higuchi et
al., 2002) would be able to conduct open con-
versations, without being restricted to particular
domains. Such systems would find use in many
environments (e.g., human-robot interaction, en-
tertainment technology).
This paper presents an approach to improving a
social dialogue system capable of chatting about
the news by adding content-independent enhance-
ments to speech. We hypothesize that enhance-
ments such as explicit acknowledgments (e.g.,
right, so, well) and disfluencies can make human-
computer conversations sound indistinguishable
from those between two humans.
Enhancements to synthesized speech have been
found to influence perception of a synthetic
voice?s hesitation (Carlson et al, 2006) and per-
sonality (Nass and Lee, 2001). Andersson et
al. (2010) used machine learning techniques to
determine where to include conversational phe-
nomena to improve synthesized speech. Adell et
al. (2007) developed methods for inserting filled
pauses into synthesized speech that listeners found
more natural. In these studies, human judges com-
pared utterances in isolation with and without im-
provements. In our study, we focus on a holistic
evaluation of naturalness in dialogues and ask ob-
servers to directly assess the naturalness of con-
versations that they ?overhear?.
2 The Talkie System
Talkie is a spoken dialogue system capable of hav-
ing open conversations about recent topics in the
news. This system was developed for a dialogue
systems course (Lim et al, 2009). Interaction
is intended to be unstructured and free-flowing,
much like social conversations. Talkie initiates a
conversation by mentioning a recent news head-
line and invites the user to comment on it.
The system uses a database of news topics and
human-written comments from the ?most blogged
about articles? of the New York Times (NYT)1.
Comments are divided into single sentences to ap-
proximate the length of a spoken response. Given
a user?s utterance (e.g., keywords related to the
topic), Talkie responds with the comment that
most closely resembles that utterance. Talkie may
access any comment related to the topic under dis-
cussion (without repetition). The user may choose
to switch to a different topic at any time (at which
point Talkie will propose a different topic from its
set).
1http://www.nytimes.com/gst/mostblogged.html
Follow links to each article?s comment section.
91
3 Study
We performed a study to determine if the per-
ceived naturalness of conversations could be im-
proved by using heuristic enhancements to speech
output. Participants ?overheard? conversations
(similar to Walker et al (2004)). Originally typed
interactions, the conversations were later synthe-
sized into speech using the Flite speech synthesis
engine (Black and Lenzo, 2001). For distinctive-
ness, conversations were between one male voice
(rms) and one female voice (slt). The voices were
generated using the CLUSTERGEN statistical para-
metric synthesizer (Black, 2006). All conversa-
tions began with the female voice.
3.1 Dialogue Content
We considered four different conversation types:
(1 & 2) between a human and Talkie (human-
computer and computer-human depending on the
first speaker), (3) between two humans on a
topic in Talkie?s database (human-human), and
(4) between two instances of Talkie (computer-
computer). The human-computer and computer-
human conditions differed from each other by
one utterance; that is, one was a shifted version
of the other by one dialogue turn. The human-
computer conversations were collected from two
people (one native English speaker, one native
Portuguese speaker) interacting with Talkie on
separate occasions. For human-human conversa-
tions, Talkie proposed a topic for discussion. Each
conversation contained ten turns of dialogue. To
remove any potential effects from the start and end
content of the conversations, we selected the mid-
dle three turns for synthesis. Each conversation
type had five conversations, each about one of five
recent headlines (as of May 2010).
3.2 Heuristic Enhancements
We defined a set of rules that added phenomena
observed in human-human spoken conversations.
These included filled pauses, word repetitions, si-
lences, and explicit acknowledgments. Conversa-
tions in this study were enhanced manually by fol-
lowing the set of rules described in Figure 1; an
example is shown in Figure 2.
3.3 Participants and Task
Eighty participants were recruited from Ama-
zon?s Mechanical Turk2 (MTurk) for this between-
2http://www.mturk.com
Category I - Explicit Acknolwedgements
? inserted sparingly at the beginning of sentences
when grammatical (e.g., well, so, you know,
right).
Category II - Filled pauses / repetitions
? no more than three per dialogue
? no more than two in any one sentence
? placed repetitions in places where speaker may
have hesitation (e.g., after ?I think...?)
? utterances may have a filled pause or a repetition
followed by a filled pause.
Category III - Silences
? added sparingly after filled pauses.
Figure 1: Heuristics used for adding enhance-
ments to synthesized dialogue.
subjects study. Only workers with a 95% HIT (hu-
man intelligence task) approval rating could par-
ticipate. There were three experimental dimen-
sions that we counterbalanced across participants:
the four conversation ?types? described in Section
3.1, the order of the five topics, and the presence
or absence of heuristic enhancements to the syn-
thesized speech that participants heard. Each par-
ticipant listened to every topic such that exactly
five participants heard each condition.
Participants made two judgments per conversa-
tion: rating the ?naturalness? of a conversation
based on a four-point Likert scale and ranking
the conversant identities (i.e., as two humans, two
computers, or a computer and a human) from most
to least confident. We used the non-parametric
Kruskal-Wallis one-way analysis of variance to
determine statistically significant differences in
naturalness responses across groups (naturalness
being the dependent measure).
4 Results
We analyzed two measures: (1) naturalness rat-
ings of conversations, and (2) frequencies from
highest-rank to lowest-rank of who the participant
believed to be the speakers in each conversation.
We found no statistically significant ordering ef-
fects or biases due to the topic of conversation.
4.1 Speaker Type Condition Results
Results for naturalness scores are summarized in
Figure 3. As expected, mean naturalness was
highest for the human-human condition and lowest
for the computer-computer condition. We found
92
Human-computer conversation (original, enhancements)
h: Well, that sounds ridiculous, you get taxed proportionally to your paycheck.
c: So, let them retire on social security payments, then you will see changes with <uh>with an overabundance of funds.
h: I don?t see how this would be possible.
c: You know, you don?t have the money yet, and you have just increased your debt by 2.5 trillion dollars.
h: Exactly why should we be raising taxes on the rich?
c: With our current politics of division, I <umm> worry we have the will or determination to address these issues.
Figure 2: Example conversation with heuristic enhancements marked in bold.
1.9
2.5 2.3
2.7
1.5
2.0
2.5
3.0
1.0 cc ch hc hh
Figure 3: Naturalness across the speaker type con-
dition.
no statistically significant difference in naturalness
ratings for the computer-human condition com-
pared to the human-computer condition (H(1) =
2.94; p = 0.09). Also, the computer-computer
condition was significantly different from all other
conditions, suggesting that conversation flow is an
important factor in determining the naturalness of
a conversation (H(3) = 42.49, p < 0.05).
People rated conversations involving a com-
puter and a human similarly to human-human con-
versations (without enhancements). There were
no statistically significant differences between the
three conditions cc, ch, and hc (H(2) = 5.36, p =
0.06). However, a trend indicated that hc natural-
ness ratings differed from those of the ch and hh
conditions. Conversations from the hc condition
had much lower (18%) mean naturalness ratings
compared to their ch counterparts, even though
they were nearly equivalent in content.
4.2 Heuristic Enhancements Results
There were significant differences in naturalness
ratings when heuristic enhancements were present
(H(1) = 17.49, p < 0.05). Figure 4 shows that
the perceived naturalness was on average higher
with heuristic enhancements. Overall, mean natu-
ralness improved by 20%. This result agrees with
findings from Andersson et al (2010).
Computer-computer conversations had the
highest relative improvement (42%) in mean nat-
uralness. Naturalness ratings were significantly
different when comparing these conversations
with and without enhancements (H(1) = 11.77, p
< 0.05). Content-free conversational phenomena
appear to compensate for the lack of logical flow
in these conversations. According to Figure 5,
after enhancements people are no better than
chance at correctly determining the speakers in
a computer-computer conversation. Thus the
heuristic enhancements clearly affect naturalness
judgments.
Even the naturalness of conversations with good
logical flow can improve with heuristic adjust-
ments; there was a 26% relative improvement in
the mean naturalness of human-human conver-
sations. Participant ratings of naturalness were
again significantly different (H(1) = 12.45, p <
0.05). Note that these conversations were origi-
nally typed dialogue. As such, they did not capture
turn-taking properties present in conversational
speech. When enhanced with conversational phe-
nomena, they more closely resembled natural spo-
ken conversations. As shown in Figure 5, people
are more likely than chance to correctly identify
two humans as being the participants in the di-
alogue after these enhancements were applied to
speech.
Conversations with one computer and one hu-
man also benefited from heuristic enhancements.
Improvements in naturalness were marginal, how-
ever. Naturalness scores in the hc condition im-
proved by 16%, but this improvement was only
a trend (H(1) = 3.66, p = 0.06). Improvement
was negligible in the ch condition. Participants
selected the correct speakers in human-computer
dialogues no better than random. We note that
participants tended to avoid ranking conversations
as ?human & computer? with confidence (i.e., the
highest rank). A significant majority (267 out of
400) of second-rank selections were ?human &
computer.? Participants tended to order conditions
93
1.5
2.5 2.1 2.42.2
2.5 2.4
3.0
1.5
2.0
2.5
3.0
3.5
1.0 cc ch hc hh
no_enhance all_enhance
Figure 4: Mean naturalness across enhancement
conditions.
66.0%
34.0%
16.0%
30.0%30.0%
16.0%
44.0% 56.0%
10%20%
30%40%
50%60%
70%
0% cc ch hc hh
no_enhance all_enhance
Figure 5: Percentage of participants? selections of
members of the conversation that were correct.
from all human to all computer or vice-versa.
5 Conclusions
We have shown that content-independent heuris-
tics can be used to improve the perceived natural-
ness of conversations. Our conversations sampled
a variety of interactions using Talkie, a social di-
alogue system that converses about recent news
headlines. An experiment examined the factors
that could influence how external judges rate the
naturalness of these conversations.
We found that without enhancements, people
rated conversations involving a human and a com-
puter similarly to conversations involving two hu-
mans. Adding heuristic enhancements produced
different results, depending on the conversation
type: computer-computer and human-human con-
versations had the best gain in naturalness scores.
Though it remains to be seen if people are always
influenced by such enhancements, they are clearly
useful for improving the naturalness of human-
computer dialogues.
Future work will involve developing methods to
automatically inject enhancements into the synthe-
sized speech output produced by Talkie, as well
as determining whether other types of systems can
benefit from these techniques.
Acknowledgments
We would like to thank Aasish Pappu, Jose-Pablo
Gonzales Brenes, Long Qin, and Daniel Lim for
developing the Talkie dialogue system.
References
J. Adell, A. Bonafonte, and D. Escudero. Filled pauses
in speech synthesis: Towards conversational speech.
In TSD?07, Pilsen, Czech Republic, 2007.
S. Andersson, K. Georgila, D. Traum, M. Aylett, and
R.A.J. Clark. Prediction and realisation of con-
versational characteristics by utilising spontaneous
speech for unit selection. In the 5th International
Conference on Speech Prosody, Chicago, Illinois,
USA, 2010.
T. Bickmore and J. Cassell. Social Dialogue with Em-
bodied Conversational Agents. J. van Kuppevelt, L.
Dybkjaer, and N. Bernsen (eds.), Natural, Intelligent
and Effective Interaction with Multimodal Dialogue
Systems. New York: Kluwer Academic.
A. Black. CLUSTERGEN: A Statistical Parametric
Synthesizer using Trajectory Modeling. In Inter-
speech?06 - ICSLP, Pittsburgh, PA, 2006.
A. Black and K. Lenzo. Flite: a small fast run-time
synthesis engine. In ISCA 4th Speech Synthesis
Workshop, Scotland, 2001.
R. Carlson and K. Gustafson and E. Strangert. Cues for
Hesitation in Speech Synthesis. In Interspeech?06 -
ICSLP, Pittsburgh, PA, 2006.
S. Higuchi, R. Rzepka, and K. Araki. A casual conver-
sation system using modality and word associations
retrieved from the web. In EMNLP?08. Honolulu,
Hawaii, 2008.
D. Lim, A. Pappu, J. Gonzales-Brenes, and L. Qin.
The Talkie Spoken Dialogue System. Unpublished
manuscript, Carnegie Mellon Univeristy, 2009.
C. Nass and K. M. Lee. Does computer-synthesized
speech manifest personality? Experimental tests of
recognition, similarity-attraction, and consistency-
attraction. Journal of Experimental Psychology:
Applied 7 (2001) 171-181.
M. A. Walker, S. J. Whittaker, A. Stent, P. Maloor, J.
Moore, M. Johnston, G. Vasireddy. Generation and
evaluation of user tailored responses in multimodal
dialogue. Cognitive Sci. 28 (2004) 811-840.
94
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialog Challenge 2010: 
 Comparison of Live and Control Test Results 
Alan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  Oliver 
Lemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, Jason 
D. Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi1 
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA 
2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK 
3Engineering Department, Cambridge University, Cambridge, UK 
4AT&T Labs ? Research, Florham Park, NJ, USA 
awb@cs.cmu.edu 
 
 
Abstract 
The Spoken Dialog Challenge 2010 was an 
exercise to investigate how different spo-
ken dialog systems perform on the same 
task.  The existing Let?s Go Pittsburgh Bus 
Information System was used as a task and 
four teams provided systems that were first 
tested in controlled conditions with speech 
researchers as users. The three most stable 
systems were then deployed to real callers.  
This paper presents the results of the live 
tests, and compares them with the control 
test results. Results show considerable 
variation both between systems and be-
tween the control and live tests.  Interest-
ingly, relatively high task completion for 
controlled tests did not always predict 
relatively high task completion for live 
tests.  Moreover, even though the systems 
were quite different in their designs, we 
saw very similar correlations between word 
error rate and task completion for all the 
systems.  The dialog data collected is 
available to the research community. 
1 Background 
The goal of the Spoken Dialog Challenge (SDC) is 
to investigate how different dialog systems per-
form on a similar task.  It is designed as a regularly 
recurring challenge. The first one took place in 
2010. SDC participants were to provide one or 
more of three things: a system; a simulated user, 
and/or an evaluation metric.   The task chosen for 
the first SDC was one that already had a large 
number of real callers. This had several advan-
tages. First, there was a system that had been used 
by many callers. Second, there was a substantial 
dataset that participants could use to train their sys-
tems.  Finally, there were real callers, rather than 
only lab testers.  Past work has found systems 
which appear to perform well in lab tests do not 
always perform well when deployed to real callers, 
in part because real callers behave differently than 
lab testers, and usage conditions can be considera-
bly different [Raux et al2005, Ai et al2008].  De-
ploying systems to real users is an important trait 
of the Spoken Dialog Challenge. 
The CMU Let?s Go Bus Information system 
[Raux et al2006] provides bus schedule informa-
tion for the general population of Pittsburgh.  It is 
directly connected to the local Port Authority, 
whose evening calls for bus information are redi-
rected to the automated system.  The system has 
been running since March 2005 and has served 
over 130K calls. 
The software and the previous years of dialog 
data were released to participants of the challenge 
to allow them to construct their own systems.  A 
number of sites started the challenge, and four sites 
successfully built systems, including the original 
CMU system. 
An important aspect of the challenge is that 
the quality of service to the end users (people in 
Pittsburgh) had to be maintained and thus an initial 
robustness and quality test was carried out on con-
tributed systems.  This control test provided sce-
narios over a web interface and required 
researchers from the participating sites to call each 
of the systems.  The results of this control test were 
published in [Black et al 2010] and by the individ-
ual participants [Williams et al 2010, Thomson et 
al. 2010, Hastie et al 2010] and they are repro-
2
duced below to give the reader a comparison with 
the later live tests. 
Important distinctions between the control 
test callers and the live test callers were that the 
control test callers were primarily spoken dialog 
researchers from around the world.  Although they 
were usually calling from more controlled acoustic 
conditions, most were not knowledgeable about 
Pittsburgh geography.     
As mentioned above, four systems took part 
in the SDC.  Following the practice of other chal-
lenges, we will not explicitly identify the sites 
where these systems were developed. We simply 
refer to them as SYS1-4 in the results.  We will, 
however, state that one of the systems is the system 
that has been running for this task for several 
years. The architectures of the systems cover a 
number of different techniques for building spoken 
dialog systems, including agenda based systems, 
VoiceXML and statistical techniques. 
2 Conditions of Control and Live tests 
For this task, the caller needs to provide the depar-
ture stop, the arrival stop and the time of departure 
or arrival in order for the system to be able to per-
form a lookup in the schedule database. The route 
number can also be provided and used in the 
lookup, but it is not necessary. The present live 
system covers the East End of Pittsburgh.  Al-
though the Port Authority message states that other 
areas are not covered, callers may still ask for 
routes that are not in the East End; in this case, the 
live system must say it doesn?t have information 
available.  Some events that affect the length of the 
dialog include whether the system uses implicit or 
explicit confirmation or some combination of both, 
whether the system has an open-ended first turn or 
a directed one, and whether it deals with requests 
for the previous and/or following bus (this latter 
should have been present in all of the systems). 
Just before the SDC started, the Port Author-
ity had removed some of its bus routes. The sys-
tems were required to be capable of informing the 
caller that the route had been canceled, and then 
giving them a suitable alternative. 
SDC systems answer live calls when the Port 
Authority call center is closed in the evening and 
early morning.  There are quite different types and 
volumes of calls over the different days of the 
week.  Weekend days typically have more calls, in 
part because the call center is open fewer hours on 
weekends.  Figure 1 shows a histogram of average 
calls per hour for the evening and the early morn-
ing of each day of the week. 
 
calls per weekday / ave per hour
0
1
2
3
4
5
6
7
8
9
10
Fr-
19-
0
Sa-
0-8
Sa-
16
-
0
Su-
0-8
Su-
16
-
0
Mo
-
0-7
Mo
-
19
-
0
Tu
-
0-7
Tu
-
19-
0
We
-
0-7
We
-
19
-
0
Th
-
0-7
Th
-
19-
0
Fr-
0-7
 
Figure 1: average number of calls per hour on weekends 
(dark bars) and weekdays. Listed are names of days and 
times before and after midnight when callers called the 
system. 
 
The control tests were set up through a simple 
web interface that presented 8 different scenarios 
to callers. Callers were given a phone number to 
call; each caller spoke to each of the 4 different 
systems twice.  A typical scenario was presented 
with few words, mainly relying on graphics in or-
der to avoid influencing the caller?s choice of vo-
cabulary.  An example is shown in Figure 2. 
 
 
 
Figure 2: Typical scenario for the control tests.  This 
example requests that the user find a bus from the cor-
ner of Forbes and Morewood (near CMU) to the airport, 
using bus route 28X, arriving by 10:45 AM. 
 
3
3 Control Test Results 
The logs from the four systems were labeled for 
task success by hand.  A call is successful if any of 
the following outputs are correctly issued: 
 
? Bus schedule for the requested departure and 
arrival stops for the stated bus number (if giv-
en). 
? A statement that there is no bus available for 
that route. 
? A statement that there is no scheduled bus at 
that time. 
 
We additionally allowed the following boundary 
cases: 
 
? A departure/arrival stop within 15 minutes 
walk. 
? Departure/arrival times within one hour of re-
quested time. 
? An alternate bus number that serves the re-
quested route. 
 
In the control tests, SYS2 had system connection 
issues that caused a number of calls to fail to con-
nect, as well as a poorer task completion.  It was 
not included in the live tests.  It should be pointed 
out that SYS2 was developed by a single graduate 
student as a class project while the other systems 
were developed by teams of researchers.  The re-
sults of the Control Tests are shown in Table 1 and 
are discussed further below. 
 
Table 1. Results of hand analysis of the four systems in 
the control test 
 The three major classes of system response 
are as follows.  no_info: this occurs when the sys-
tem gives neither a specific time nor a valid excuse 
(bus not covered, or none at that time).  no_info 
calls can be treated as errors (even though there 
maybe be valid reasons such as the caller hangs up 
because the bus they are waiting for arrives).  
donthave: identifies calls that state the requested 
bus is not covered by the system or that there is no 
bus at the requested time. pos_out: identifies calls 
where a specific time schedule is given.  Both 
donthave and pos_out calls may be correct or er-
roneous (e.g the given information is not for the 
requested bus,  the departure stop is wrong, etc). 
4 Live Tests Results 
In the live tests the actual Pittsburgh callers had 
access to three systems: SYS1, SYS3, and SYS4.  
Although engineering issues may not always be 
seen to be as relevant as scientific results, it is im-
portant to acknowledge several issues that had to 
be overcome in order to run the live tests. 
Since the Pittsburgh Bus Information System 
is a real system, it is regularly updated with new 
schedules from the Port Authority. This happens 
about every three months and sometimes includes 
changes in bus routes as well as times and stops. 
The SDC participants were given these updates 
and were allowed the time to make the changes to 
their systems. Making things more difficult is the 
fact that the Port Authority often only releases the 
schedules a few days ahead of the change. Another 
concern was that the live tests be run within one 
schedule period so that the change in schedule 
would not affect the results.   
The second engineering issue concerned 
telephony connectivity. There had to be a way to 
transfer calls from the Port Authority to the par-
ticipating systems (that were run at the participat-
ing sites, not at CMU) without slowing down or 
perturbing service to the callers.  This was 
achieved by an elaborate set of call-forwarding 
mechanisms that performed very reliably.  How-
ever, since one system was in Europe, connections 
to it were sometimes not as reliable as to the US-
based systems.  
 
 SYS1 SYS3 SYS4 
Total Calls 678 451 742 
Non-empty calls 633 430 670 
no_ info 18.5% 14.0% 11.0% 
donthave 26.4% 30.0% 17.6% 
donthave_corr 47.3% 40.3% 37.3% 
donthave_incorr 52.7% 59.7% 62.7% 
pos_out 55.1% 56.0% 71.3% 
pos_out_corr 86.8% 93.8% 91.6% 
pos_out_incorr 13.2% 6.2% 8.4% 
 
Table 2. Results of hand analysis of the three systems in 
the live tests.  Row labels are the same as in Table 1. 
 SYS1 SYS2 SYS3 SYS4 
Total Calls 91 61 75 83 
no_ info 3.3% 37.7% 1.3% 9.6% 
donthave 17.6% 24.6% 14.7% 9.6% 
donthave_corr 68.8% 33.3% 100.0% 100.0% 
donthave_incorr 31.3% 66.7% 0.0% 0.0% 
pos_out 79.1% 37.7% 84.0% 80.7% 
pos_out_corr 66.7% 78.3% 88.9% 80.6% 
pos_out_incorr 33.3% 21.7% 11.1% 19.4% 
4
We ran each of the three systems for multiple two 
day periods over July and August 2010.  This de-
sign gave each system an equal distribution of 
weekdays and weekends, and also ensured that 
repeat-callers within the same day experienced the 
same system. 
One of the participating systems (SYS4) 
could support simultaneous calls, but the other two 
could not and the caller would receive a busy sig-
nal if the system was already in use.  This, how-
ever, did not happen very often. 
Results of hand analysis of real calls are 
shown in Table 4 alongside the results for the Con-
trol Test for easy comparison.  In the live tests we 
had an additional category of call types ? empty 
calls (0-turn calls) ? which are calls where there 
are no user turns, for example because the caller 
hung up or was disconnected before saying any-
thing.  Each system had 14 days of calls and exter-
nal daily factors may change the number of calls. 
We do suspect that telephony issues may have pre-
vented some calls from getting through to SYS3 on 
some occasions.   
Table 3 provides call duration information for 
each of the systems in both the control and live 
tests. 
 
 
 Length (s) Turns/call Words/turn 
SYS1 control 155 18.29 2.87 (2.84) 
SYS1 live 111 16.24 2.15 (1.03) 
SYS2 control 147 17.57 1.63 (1.62) 
SYS3 control 96 10.28 2.73 (1.94) 
SYS3 live 80 9.56 2.22 (1.14) 
SYS4 control 154 14.70 2.25 (1.78) 
SYS4 live 126 11.00 1.63 (0.77) 
 
Table 3: For live tests, average length of each call, aver-
age number of turns per call, and average number of 
words per turn (numbers in brackets are standard devia-
tions). 
 
Each of the systems used a different speech 
recognizer.  In order to understand the impact of 
word error rate on the results, all the data were 
hand transcribed to provide orthographic transcrip-
tions of each user turn.   Summary word error sta-
tistics are shown in Table 4.   However, summary 
statistics do not show the correlation between word 
error rate and dialogue success.  To achieve this, 
following Thomson et al(2010), we computed a 
logistic regression of success against word error 
rate (WER) for each of the systems. Figure 3 
shows the regressions for the Control Tests and 
Figure 4 for the Live Tests.  
 
 SYS1 SYS3 SYS4 
Control 38.4 27.9 27.5 
Live 43.8 42.5 35.7 
 
Table 4: Average dialogue word error rate (WER). 
 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
es
s 
Ra
te
Sys4
Sys3
Sys1
 
Figure 3: Logistic regression of control test success vs 
WER for the three fully tested systems 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
e
ss
Sys1
Sys3
Sys4
 
Figure 4: Logistic regression of live success vs WER for 
the three fully tested systems 
 
5
In order to compare the control and live tests, 
we can calculate task completion as the percentage 
of calls that gave a correct result.  We include only 
non-empty calls (excluding 0-turn calls), and treat 
all no_info calls as being incorrect, even though 
some may be due to extraneous reasons such as the 
bus turning up (Table 5). 
 
 SYS1 SYS3 SYS4 
Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%) 
Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%) 
 
Table 5: Live and control test task completion (std. err).  
 
5 Discussion 
All systems had lower WER and higher task com-
pletion in the controlled test vs. the live test.  This 
agrees with past work [Raux et al2005, Ai et al
2008], and underscores the challenges of deploying 
real-world systems. 
For all systems, dialogs with controlled sub-
jects were longer than with live callers ? both in 
terms of length and number of turns.  In addition, 
for all systems, live callers used shorter utterances 
than controlled subjects.  Controlled subjects may 
be more patient than live callers, or perhaps live 
callers were more likely to abandon calls in the 
face of higher recognition error rates.   
Some interesting differences between the sys-
tems are evident in the live tests.  Looking at dia-
log durations, SYS3 used confirmations least often, 
and yielded the fastest dialogs (80s/call).  SYS1 
made extensive use of confirmations, yielding the 
most turns of any system and slightly longer dia-
logs (111s/call).  SYS4 was the most system-
directed, always collecting information one ele-
ment at a time.  As a result it was the slowest of the 
systems (126s/call), but because it often used im-
plicit confirmation instead of explicit confirmation, 
it had fewer turns/call than SYS1.   
For task completion, SYS3 performed best in 
the controlled trials, with SYS1 worst and SYS4 in 
between.  However in the live test, SYS4 per-
formed best, with SYS3 and SYS1 similar and 
worse.  It was surprising that task completion for 
SYS3 was the highest for the controlled tests yet 
among the lowest for the live tests.  Investigating 
this, we found that much of the variability in task 
completion for the live tests appears to be due to 
WER.  In the control tests SYS3 and SYS4 had 
similar error rates but the success rate of SYS3 was 
higher.  The regression in Figure 3 shows this 
clearly.   In the live tests SYS3 had a significantly 
higher word error rate and average success rate 
was much lower than in SYS4.   
It is interesting to speculate on why the rec-
ognition rates for SYS3 and SYS4 were different 
in the live tests, but were comparable in the control 
tests.  In a spoken dialogue system the architecture 
has a considerable impact on the measured word 
error rate.  Not only will the language model and 
use of dialogue context be different, but the dia-
logue design and form of system prompts will in-
fluence the form and content of user inputs.   Thus, 
word error rates do not just depend on the quality 
of the acoustic models ? they depend on the whole 
system design.  As noted above, SYS4 was more 
system-directed than SYS3 and this probably con-
tributed to the comparatively better ASR perform-
ance with live users.   In the control tests, the 
behavior of users (research lab workers) may have 
been less dependent on the manner in which users 
were prompted for information by the system.  
Overall, of course, it is user satisfaction and task 
success which matter. 
6 Corpus Availability and Evaluation 
The SDC2010 database of all logs from all systems 
including audio plus hand transcribed utterances, 
and hand defined success values is released 
through CMU?s Dialog Research Center 
(http://dialrc.org). 
One of the core goals of the Spoken Dialog 
Challenge is to not only create an opportunity for 
researchers to test their systems on a common plat-
form with real users, but also create common data 
sets for testing evaluation metrics.  Although some 
work has been done on this for the control test data 
(e.g. [Zhu et al2010]), we expect further evalua-
tion techniques will be applied to these data. 
One particular issue which arose during this 
evaluation concerned the difficulty of defining pre-
cisely what constitutes task success.  A precise de-
finition is important to developers, especially if 
reinforcement style learning is being used to opti-
mize the success.  In an information seeking task 
of the type described here, task success is straight-
forward when the user?s requirements can be satis-
fied but more difficult if some form of constraint 
relaxation is required.   For example, if the user 
6
asks if there is a bus from the current location to 
the airport ? the answer ?No.? may be strictly cor-
rect but not necessarily helpful.  Should this dia-
logue be scored as successful or not?  The answer 
?No, but there is a stop two blocks away where 
you can take the number 28X bus direct to the air-
port.? is clearly more useful to the user.  Should 
success therefore be a numeric measure rather than 
a binary decision?  And if a measure, how can it be 
precisely defined?  A second and related issue is 
the need for evaluation algorithms which deter-
mine task success automatically.   Without these, 
system optimization will remain an art rather than 
a science. 
7 Conclusions 
This paper has described the first attempt at an ex-
ercise to investigate how different spoken dialog 
systems perform on the same task.  The existing 
Let?s Go Pittsburgh Bus Information System was 
used as a task and four teams provided systems 
that were first tested in controlled conditions with 
speech researchers as users. The three most stable 
systems were then deployed ?live? with real call-
ers. Results show considerable variation both be-
tween systems and between the control and live 
tests.  Interestingly, relatively high task completion 
for controlled tests did not always predict rela-
tively high task completion for live tests.  This 
confirms the importance of testing on live callers, 
not just usability subjects. 
 The general organization and framework 
of the evaluation worked well.  The ability to route 
audio telephone calls to anywhere in the world us-
ing voice over IP protocols was critical to the suc-
cess of the challenge since it provides a way for 
individual research labs to test their in-house sys-
tems without the need to port them to a central co-
ordinating site. 
 Finally, the critical role of precise evalua-
tion metrics was noted and the need for automatic 
tools to compute them.  Developers need these at 
an early stage in the cycle to ensure that when sys-
tems are subsequently evaluated, the results and 
system behaviors can be properly compared.  
Acknowledgments 
Thanks to AT&T Research for providing telephony 
support for transporting telephone calls during the 
live tests.  This work was in part supported by the 
US National Science foundation under the project 
?Dialogue Research Center?.   
References  
Ai, H., Raux, A., Bohus, D., Eskenzai, M., and Litman, 
D.  (2008)  ?Comparing spoken dialog corpora col-
lected with recruited subjects versus real users?, Proc 
SIGDial, Columbus, Ohio, USA.  
Black, A., Burger, S., Langner, B., Parent, G., and Es-
kenazi, M. (2010) ?Spoken Dialog Challenge 2010?, 
SLT 2010, Berkeley, CA.  
Hastie, H., Merigaud, N., Liu, X and Oliver Lemon. 
(2010) ? ?Let?s Go Dude?, Using The Spoken Dia-
logue Challenge to Teach Spoken Dialogue Devel-
opment?, SLT 2010, Berkeley, CA. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M.  (2005)  ?Let?s go public! Taking a spoken dialog 
system to the real world?, Interspeech 2005, Lisbon, 
Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. (2006) ?Doing Research on a Deployed 
Spoken Dialogue System: One Year of Let's Go! Ex-
perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.  
Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,  
Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-
tem for the Let?s Go Spoken Dialogue Challenge?, 
SLT 2010, Berkeley, CA. 
Williams, J., Arizmendi, I., and Conkie, A. ?Demonstra-
tion of AT&T ?Let?s Go?: A Production-Grade Statis-
tical Spoken Dialog System.? SLT 2010, Berkeley, 
CA. 
Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., and 
King, I. (2010) ?Using Finite State Machines for 
Evaluating Spoken Dialog Systems?, SLT 2010, 
Berkeley, CA. 
7
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 20?29,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
?Love ya, jerkface?: using Sparse Log-Linear Models to Build
Positive (and Impolite) Relationships with Teens
William Yang Wang, Samantha Finkelstein, Amy Ogan, Alan W Black, Justine Cassell
School of Computer Science, Carnegie Mellon University
{yww, slfink, aeo, awb, justine}@cs.cmu.edu
Abstract
One challenge of implementing spoken di-
alogue systems for long-term interaction is
how to adapt the dialogue as user and sys-
tem become more familiar. We believe this
challenge includes evoking and signaling as-
pects of long-term relationships such as rap-
port. For tutoring systems, this may addi-
tionally require knowing how relationships are
signaled among non-adult users. We therefore
investigate conversational strategies used by
teenagers in peer tutoring dialogues, and how
these strategies function differently among
friends or strangers. In particular, we use an-
notated and automatically extracted linguis-
tic devices to predict impoliteness and posi-
tivity in the next turn. To take into account
the sparse nature of these features in real data
we use models including Lasso, ridge estima-
tor, and elastic net. We evaluate the predictive
power of our models under various settings,
and compare our sparse models with stan-
dard non-sparse solutions. Our experiments
demonstrate that our models are more ac-
curate than non-sparse models quantitatively,
and that teens use unexpected kinds of lan-
guage to do relationship work such as signal-
ing rapport, but friends and strangers, tutors
and tutees, carry out this work in quite differ-
ent ways from one another.
1 Introduction and Related Work
Rapport, the harmonious synchrony between in-
terlocutors, has numerous benefits for a range of
dialogue types, including direction giving (Cas-
sell et al, 2007) or contributing to patient recov-
ery (Vowles and Thompson, 2012). In peer tutor-
ing, an educational paradigm in which students of
similar ability tutor one another, friendship among
tutors and tutees leads to better learning (Gartner et
al., 1971). With the burgeoning use of spoken dia-
logue systems in education, understanding the pro-
cess by which two humans build and signal rapport
during learning becomes a vital step for implement-
ing spoken dialogue systems (SDSs) that can initi-
ate (and, as importantly, maintain) a successful re-
lationship with students over time. However, im-
plementing a tutorial dialogue system that appropri-
ately challenges students in the way that peers do
so well (Sharpley et al, 1983), while still demon-
strating the rapport that peers can also provide, calls
for understanding the differences in communication
between peer tutors just meeting and those who are
already friends.
The Tickle-Degnen and Rosenthal (1990) model
provides a starting point by outlining the compo-
nents of rapport, including the finding that positiv-
ity decreases over the course of a relationship. The
popularity of this model, however, has not dimin-
ished the disproportionate attention that positivity
and politeness receive in analyses of rapport (Brown
and Levinson, 1978), including in the vast majority
of computational approaches to rapport-building in
dialogue (Stronks et al, 2002; Johnson and Rizzo,
2004; Bickmore and Picard, 2005; Gratch et al,
2006; McLaren et al, 2007; Cassell et al, 2007;
Baker et al, 2008; Bickmore et al, 2011). The
creation and expression of rapport is complex, and
can also be signaled through negative, or impolite,
exchanges (Straehle, 1993; Watts, 2003; Spencer-
Oatey, 2008) that communicate affection and re-
lationship security among intimates who can flout
common social norms (Culpeper, 2011; Kienpoint-
ner, 1997).
However, it is an open question as to whether such
rudeness is likely to impress a new student on the
first day of class. We must better understand how
and when impoliteness and other negative dialogue
moves can contribute to the development and ex-
pression of the rapport that is so important in educa-
tional relationships. In this analysis, then, we begin
with a corpus of tutoring chat data annotated with
a set of affectively-charged linguistic devices (e.g.
complaining, emoticons), and then differentiate be-
tween the linguistic devices that friend and stranger
interlocutors employ (with friendship standing as a
proxy for pre-existent rapport) and the resulting so-
cial effects or functions of those devices on the part-
ners.
Since our ultimate goal is to build an SDS that
can adapt to the user?s language in real time, we
also automatically extract lexical and syntactic fea-
tures from the conversations. And, in order to deter-
mine what the system should say to evoke particular
20
responses, we predict social effects in partner two
from the use of the linguistic devices in partner one.
Since we want to understand how the system can
deal with newly met peers as well as peers who
have become friends, we develop and evaluate our
model on dyads of friends and then evaluate the
same model with dyads of strangers, to examine
whether dyads with less a priori rapport react dif-
ferently to the same linguistic devices.
Of course, in addition to understanding the phe-
nomenon of rapport in all of its complexity, a major
challenge for building rapport-signaling SDS is to
construct a compact feature space that capture only
reliable rapport signals and generalizes well across
different speakers. Of course phenomena such as in-
sults, complaints and pet names, no matter how im-
portant, appear relatively rarely in data of this sort.
Training discriminative models with maximum like-
lihood estimators (MLE) on such datasets usually re-
sults in assigning too much weight on less frequent
signals. This standard MLE training method not
only produces dense models, but may also overes-
timates lower frequency features that might be unre-
liable signals and overfit to a particular set of speak-
ers. In recent studies on speaker state prediction that
use lexical features, it has been shown that MLE
estimators demonstrate large performance gaps be-
tween non-overlapping speaker datasets (Jeon et al,
2010; Wang et al, 2012a).
On the other hand, recent studies on `1/`2
based group penalty for evaluating dialogue systems
(Gonza?lez-Brenes and Mostow, 2011), structured
sparsity for linguistic structure prediction (Mar-
tins et al, 2011), and discovering historical legal
opinions with a sparse mixed-effects latent vari-
able model (Wang et al, 2012b) have all shown
concrete benefits of modeling sparsity in language-
related predictive tasks. We therefore apply sparsity-
sensitive models that can prevent less frequent
features from overfitting. We start with the `1-
regularized Lasso (Tibshirani, 1994) model, since,
compared to other covariance matrix based sparse
models, such as sparse Principal Component Anal-
ysis (PCA) and sparse Canonical Correlation Anal-
ysis (CCA), the Lasso model is straightforward and
requires fewer computing resources when the fea-
ture dimension is high. Hence, we compare the con-
tributions of both automated features and annotated
features using the proposed Lasso model to predict
impoliteness and positivity.
In addition to Lasso and a logistic regression base-
line, we introduce two alternative penalty models:
the non-sparse ridge (le Cessie and van Houwelin-
gen, 1992) estimator, and an elastic net model (Zou
and Hastie, 2005). The ridge estimator applies a
quadratic penalty for feature selection, resulting in
a smooth objective function and a non-sparse fea-
ture space, which can be seen as a strong non-sparse
penalty model. We investigate the elastic net model,
because it balances the pros and cons of Lasso and
ridge estimators, and enforces composite penalty. In
addition to the model comparisons, by varying the
different sizes of feature windows (number of turns
in the dialogue history), we empirically show that
our proposed sparse log-linear model is flexible, en-
abling the model to capture long-range dependency.
This approach also allows us to extend previous
work on speaker state prediction. Although speaker
state prediction has attracted much attention in the
dialogue research community, most studies have fo-
cused on the analysis of anger, frustration, and other
classic emotions (Litman and Forbes-Riley, 2004;
Liscombe et al, 2005; Devillers and Vidrascu, 2006;
Ai et al, 2006; Grimm et al, 2007; Gupta and Ni-
tendra., 2007; Metallinou et al, 2011). Recently,
Wang and Hirschberg (2011) proposed a hierarchi-
cal model that detects level of interest of speakers
in dialogue, using a multistream prediction feedback
technique. However, to the best of our knowledge,
we are among the first to study the problem of auto-
matic impoliteness and positivity prediction in dia-
logue. Because our ultimate goal is to build an SDS
that responds to users? language use over time, the
features from the user?s target turn that the model is
aiming to predict are not observable, which renders
the task more difficult than previous speaker state
detection tasks.
Our main contributions are three-fold: (1) analy-
sis of linguistic devices that function to signal rap-
port among friends - and their effects on non-friend
dyads; (2) detailed analyses of language behavior
features that predict these rapport behaviors - both
impoliteness and positivity - in the next turn of
teenagers? peer tutoring sessions; (3) an evaluation
of non-sparse and sparse log-linear models for pre-
dicting impoliteness and positivity.
By understanding the signals of rapport that a per-
son is likely to display in response to various lin-
guistic devices, we can begin to build an SDS that
can anticipate the social response and adapt to the
rapport-signaling efforts of its partner, both as a
newly introduced technology, and, over time, as a
system with whom the user has a rapport.
2 The Corpus
We use the data from a previous study evaluating the
impact of a peer tutoring intervention that monitored
students? collaboration and in some cases provided
adaptive support (Walker et al, 2011). In the inter-
vention, peer tutors observed the work of their tutee
21
and supported them through a chat interface as they
completed algebra problems. The system logged all
chat and other information about the problem steps.
Participants were 130 high school students (81 fe-
male) in grades 7-12 from one American high school
with some prior knowledge of the algebra material.
Participants were asked to sign up for the study with
a friend. Those who were interested but were un-
able to participate with a friend, were matched with
another unmatched participant. In an after-school
session, participants first took a 20-minute pre-test
on the math concepts, and then spent 20 minutes
working alone with the computer to prepare for tu-
toring. One student in each dyad was then randomly
assigned the role of tutor, while the other was given
the role of tutee, regardless of relative ability. They
spent the next 60 minutes engaging in tutoring. Fi-
nally, students were given a domain posttest isomor-
phic to the pretest.
54 dyads signed up as friends and 6 were un-
matched strangers. To compare behavior between
friends and strangers in the face of very different
data set sizes we use 48 friend dyads for training,
and select 6 friend and 6 stranger dyads as two sep-
arate test sets. The total number of utterances in the
friend training set, friend test set, and stranger test
set are 4538, 468 and 402. To perform turn-based
prediction experiments, we concatenate the text in
the utterances by the same speaker into a single turn,
and perform an ?OR? operation1 on features (See
Section 3 for details) in multiple utterances of the
same speaker to generate the turn-based binary fea-
tures.
3 Feature Engineering
In this section, we describe both the annotated and
automatically extracted features analyzed.
3.1 Annotated Features and Labels2
To understand what linguistic devices participated in
positivity and impoliteness during tutoring, we an-
notated all 60 dyads for surface-level language be-
haviors such as complaints, challenges (Culpeper,
1996) and praise. We also automatically identi-
fied chat features that socially color the communi-
cation, such as excessive punctuation[P] or capital-
ization[Ca]. Utterances could receive more than one
code, and inter-rater reliability ranged from K=.71
to K=1.
Because these linguistic behaviors may serve a
range of different functions in context, such as rude
1If any of the utterances within one turn has this feature
turned on, then we say that we have observed this feature in
this turn.
2We thank Erin Walker for data collection and annotation.
language serving to cement a relationship (Arding-
ton, 2006), or teasing to increase rapport (Straehle,
1993), we also annotate the social functionality
of each utterance in context, in terms of positivity
(K=.79)3 and impoliteness (K=.76), which are seen
as holding down opposite kinds of social functional-
ity (Terkourafi, 2008). Details of annotation can be
found in our recent work (Ogan et al, 2012).
Language Behavior Features
Language behavior features were annotated by
two raters, based on previous work on impo-
liteness (Culpeper, 1996), positivity (Boyer et
al., 2008), and computer-mediated communica-
tion (Herring and Zelenkauskaite, 2009), as fol-
lows:.
? Insults[Di] (?=1): Personalized negative voca-
tives or references. eg. ?you are so weird.?
? Challenges[Ch] (? =.91): Directly questioning
partner?s decision or ability. eg. Partner 1:
?see I am helping?, Partner 2: ?barely.?
? Condescensions / brags[C] (?=1): Asserting
authority or partner?s inferiority. eg. Tutee:
?nothing you have done has affected me what
so ever.?
? Message enforcer[Ef] (?=.85): Emphasizing
text or attracting partner?s attention. eg. ?Earth
to Erin.?
? Dismissal / Silencer / Curse[Cu] (? =.76): As-
serting unimportance of contribution/partner.
eg. ?shuttttt up computer.?
? Pet name[Pe] (? = .9): Vocatives that may or
may not be insulting. eg. ?whats up homie??
? Criticisms / exclusive complaints[EC] (?=.8):
Negative evaluation of partner. eg. ?You are so
bad at this dude.?
? Inclusive complaints[I] (?=.78): Complaints
directed outside the partner, such as at the task,
computers, or study. eg. ?This is really dumb,
ya think??
? Laughter[L] (?=1): eg. ?haha?, ?lol?
? Off-task[O] (?=.71): Doesn?t pertain to or ad-
vance tutorial dialogue. eg. ?Coming over after
this??
Impoliteness and Positivity Labels
While the surface-level features were coded based
on a single utterance, context determined the labels
for impoliteness and positivity, including the recent
tone of the dialogue and the partner?s response to
the utterance. Utterances were coded as positivity
(?=.79) when they included goals that directly added
positive affect into the exchange through praise, em-
pathy, reassurance, cooperative talk (McLaren et al,
3We use Cohen?s kappa in this study.
22
2011), task enthusiasm, and making or responding
to jokes. Impoliteness (?=.76) included both coop-
eratively rude utterances such as teasing (typical eg.
?hahah you?re the worst tutor ever?) and uncooper-
atively rude utterances that may cause offense (typ-
ical eg. ?um why don?t you try actually explainin
urself..?) (Kienpointner, 1997).
3.2 Automated Features
To compare the performance between what could be
automatically extracted from dialogue and hand an-
notation, we extracted 2,872 unigram and 12,016 bi-
gram features from the text corpus. Using the Stan-
ford PoS tagger4 with its attached model, we also
extracted 46 common part-of-speech tags from the
text. In addition to the above lexical and syntac-
tic features, we automatically extracted the capital-
ization features[Ca] that have at least one full word
(eg. ?CALM DOWN?) (Chovanec, 2009). Since
a recent text prediction task (Wang and McKeown,
2010) observed benefits from modeling punctua-
tion features[P], we extracted the expressive punc-
tuation that included at least one exclamation point
or more than one question-mark (eg. ?I don?t get
it?!??!?) (Crystal, 2001). We used a smiley dictio-
nary5 to extract the emoticons[E] that convey emo-
tional states (Sa?nchez et al, 2006) from text.
4 Sparse Log-Linear Models
We formulate our impoliteness and positivity predic-
tion problems as binary classifications. To do this,
we estimate the label y?t ? Bernoulli(??). First, we
introduce a standard log-linear parametrization6 to
our predictive tasks:
??~yt =
exp
?
i ~wi ~fi(~yt)
1 + exp
?
i ~wi ~fi(~yt)
, (1)
where ~f(~yt) is a set of feature functions computed
on the observation vector ~yt. The term ~wi puts a
weight on feature i for predicting impoliteness, and
our estimation problem is now to set these weights.
The log-likelihood and the gradient are:
` =
?
t
yt log ??~yt + (1? yt) log(1? ??~yt) (2)
?`
? ~w =
?
t
(
???~yt
? ~w
)(
yt
??~yt
? 1? yt
1? ??~yt
)
(3)
???~yt
? ~w =
(
??~yt ? (??~yt)2
)
~f(~yt), (4)
4http://nlp.stanford.edu/software/tagger.shtml
5http://www.techdictionary.com/emoticon.html
6We thank Jacob Eisenstein for the formulation of logistic
regression model.
so the parameters can be set using gradient as-
cent. To control the overall complexity, we can ap-
ply regularized models on the elements of ~w. A
sparsity-inducing model, such as the Lasso (Tibshi-
rani, 1994) or elastic net (Zou and Hastie, 2005)
model, will drive many of these weights to zero, re-
vealing important interactions between the impolite-
ness/positivity label and other features. Instead of
maximizing the log-likelihood, we can minimize the
following Lasso model that consists of the negative
log-likelihood loss function:
min
(
? `+
?
i
?1||~wi||
)
(5)
Since the Lasso penalty can introduce discontinu-
ities to the original convex function, we can also
consider an alternative non-sparse ridge estima-
tor (le Cessie and van Houwelingen, 1992) that has
the convex property:
min
(
? `+
?
i
?2||~wi||2
)
(6)
In addition to the Lasso and ridge estimators, the
composite penalty based elastic net model balances
the sparsity and smoothness properties of both Lasso
and ridge estimators:
min
(
? `+
?
i
?1||wi||+
?
i
?2||wi||2
)
(7)
Our log-linear model is quite flexible; by compar-
ing various restrictions, we can test different features
when modeling impoliteness and positivity. In addi-
tion, the model can incorporate features from previ-
ous time windows, which requires much less compu-
tational complexity compared to standard high order
Markov models. We use the L-BFGS method (Liu
and Nocedal, 1989) for the numerical optimization.
5 Empirical Experiments
We predict impoliteness vs. non-impoliteness and
positivity vs. non-positivity of an interlocutor in the
immediate future turn, given only information from
current/previous turns. Because accuracy, precision,
recall and F-measure are threshold-based point esti-
mation metrics that might prevent one from observ-
ing the big picture of system performance, we con-
sider the Receiver Operating Characteristic (ROC)
metric to evaluate the dynamics of the true posi-
tive rate vs. the false positive rate (Hanley and Mc-
Neil, 1982) in our system. We mainly use Area Un-
der Curve (AUC) as a metric to compare classifiers,
since it maps the ROC metric to a single scalar value
representing expected performance. A random clas-
sifier will have an AUC of 0.5 (Fawcett, 2006).
23
Models P Ca E L O Ef Pe Di C EC Ch Cu I
Impoliteness Prediction
Tr-Te .44 -1.10 .62 .72 .09 .64 .09 1.29 .96 .89 .69 .77 -0.19
Te-Tr -2.48 .54 -0.26 0.15 .59 1.62 .24 .22 .89 .72 .75 .04 -0.18
Positivity Prediction
Tr-Te -0.87 .19 .36 .55 1.06 -0.62 .69 -1.63 -1.57 .16 -0.41 1.22 .86
Te-Tr -1.39 -0.46 .70 .48 .46 .33 .62 -0.71 .70 -0.65 -0.47 -0.54 .78
Table 1: Comparing the Learned Weights of Different Features when Predicting the Partner?s Impoliteness in a Non-
Sparse Log-Linear Model. Tr-Te: predict tutee turn with tutor turn. Te-Tr: predict tutor turn with tutee turn. For full
name of features, see Section 3.
5.1 Comparing the Learned Weights of
Different Features
In our previous analysis of these data (Ogan et al,
2012), a PCA method allowed us to group linguistic
behaviors in order to address the issue of data spar-
sity. With the use of log-linear models, we are able
to investigate the contributions of individual lan-
guage behaviors in one student?s turn to the predic-
tion of social functions in their partner?s next turn. In
this experiment, we evaluate the weights of various
linguistic devices in a standard logistic regression
model. We found that behaviors commonly asso-
ciated with impoliteness were predictors of partner
impoliteness in the next turn, while positive behav-
iors such as laughter were predictors of upcoming
positivity. SDSs can leverage this knowledge to take
the partners lead during a tutoring session, using the
partners positivity or impoliteness to determine the
affect of the systems upcoming move. As we intend
to develop a system that acts as a tutee, however, we
further divided the analysis by tutoring role, inves-
tigating how partners in different roles employ lan-
guage features differently, such that the system can
act in accordance with its given role. Table 1 shows
the results.
Similarly to the collapsed factors in our previous
work, we found here that tutors and tutees do in
fact use language behaviors differently, and to ac-
complish different social functions. Effectively, this
means that certain language behaviors may instigate
impoliteness when said by one partner, but lead to
positivity when expressed by the other. For exam-
ple, tutee bragging predicts a response of positiv-
ity on behalf of the tutor (~w(TE)C = .7), perhaps be-cause the tutor wants to be supportive of a prote?ge??s
self-efficacy and success. Conversely, when the tu-
tor brags during a peer tutoring dialogue, the tu-
tee, who may feel threatened by the tutors bravado,
is extremely likely to respond with impoliteness (
~w(TR)C = .96). In a peer tutoring paradigm, whenthe more powerful partner (the tutor) expresses dom-
inance through self-inflation, the subordinate part-
ner may use impoliteness to regain some social con-
trol. On the other hand, some language behaviors
actively work to tear down this power imbalance,
such as inclusive complaining, where the partners
take an us against the task approach, building sol-
idarity through complaining about the experiment.
These utterances predict positivity whether used by
the tutor ( ~w(TR)I = .86) or tutee ( ~w(TE)I = .78).Other comparisons between weighted features by
role demonstrate similarly theoretically-motivated
findings that shed light on how language is used to
achieve social functions.
5.2 Comparing the Contributions of Different
Features on Friend and Stranger Datasets
A previous study (Ogan et al, 2012) on these same
data seemed to indicate that negative conversational
strategies composed of linguistic devices such as
complaining and insults were correlated with learn-
ing in the friend dyads and negatively correlated
with learning in strangers. However the small num-
ber of stranger dyads prevented them from draw-
ing conclusions about particular linguistic devices
from the data. Here, we empirically show the pre-
dictive performance of different feature sets on both
friend and stranger test sets in Table 2 , using a
sparse Lasso model with features from only the
current turn. In the impoliteness prediction task,
when predicting on the test set that consists of only
friends, we observe statistically significant improve-
ment over a random baseline, using surface-level
language behavior features, lexical, lexical + syn-
tactic, all automatic, and all features. When com-
bining all features, the best AUC is .621. The auto-
matic features, mainly including n-grams and part-
of-speech tags, have emerged as a useful automated
feature space. On the other hand, we do not observe
any significant results on the stranger datasets, sug-
gesting that strangers do not respond with impolite-
ness in the same way that friends do. When pre-
dicting positivity on the friend dataset, we see that
24
the performance of surface-level language behavior
features has dropped from the first task, and the sta-
tistical t-test is non-significant when comparing to
a random baseline. This is not surprising, because
we have shown in the previous section that surface-
level language behavior features are strong indica-
tors of impoliteness, but might not have advantages
in predicting positivity for friends. Interestingly, the
automated features outperform the combination of
all features, indicating a promising future for the ac-
tual deployment of an SDS that can interact using
appropriate positivity and impoliteness.
When predicting positivity in the stranger dataset,
we find the opposite trend. In contrast to the impo-
liteness prediction task, the overall performance on
the stranger dataset improved, and the lexical, lexi-
cal+syntactic, and all feature combination have sig-
nificantly outperformed the chance baseline. These
results suggest that positivity is a predictable behav-
ior among strangers, who may all express uniform
positivity across all dyads, while it is the impolite-
ness that is predictable among friends. Perhaps it
is that through the development of a rapport with a
partner, the particular ways in which positivity is ex-
pressed becomes personalized to the dyad, and can
no longer be applied to other groups who have their
own expressions of positivity. In other words, un-
like in Tolstoy?s world, here unhappy families are all
alike; every happy family is happy in its own way.
We must look to the easily-predictable impoliteness
among friends instead, arguing strongly for the in-
clusion of impoliteness in a model of rapport.
5.3 Comparing Logistic Regression, Lasso,
Ridge, and Elastic Net
While our previous work (Ogan et al, 2012) demon-
strated that PCA is a useful feature selection method
when there are only a dozen features, in this experi-
ment, the dimension of our feature space is substan-
tially higher, which aligns to the size of vocabulary.
Thus, covariance-based feature selection methods,
such as PCA, might be too slow. Here we compare
the performances of standard MLE trained logistic
regression, Lasso, non-sparse ridge, and elastic net
models. In particular, we demonstrate the predic-
tive power of Lasso and elastic net models, varying
distinct levels of sparsity. In the Figure 1, we show
the comparison of three different models in the im-
politeness prediction task. The horizontal axis rep-
resents different values of regularization coefficient
?. For the Lasso model and the elastic net model,
increasing the value ? will result in a sparser feature
space, and we set the ? = ?1 = ?2 in the elastic net
model to promote same level of sparsity and smooth-
ness. The result at ? = 0 represents the standard
Feature Sets F-AUC p S-AUC p
Impoliteness Prediction
Random .500 - .500 -
Behavior .596 .017 .505 .473
Lex .599 .014 .435 .819
Lex + POS .605 .009 .425 .857
All Auto .591 .022 .451 .751
All Features .621 .003 .427 .850
Positivity Prediction
Random .500 - .500 -
Behavior .549 .141 .527 .302
Lex .623 .003 .601 .025
Lex + POS .646 .001 .587 .047
All Auto .651 .001 .577 .070
All Features .641 .001 .608 .019
Table 2: Comparing contributions of different feature
streams on both friend and stranger testsets with Lasso
model when predicting impoliteness and positivity of the
next turn using only features from the current turn. ( F-:
the friend test set. S: the stranger test set. p: one-tailed
p-value by comparing to a random classifier. Behavior:
detailed surface-level language behavior features defined
in Section 3. Lex: unigram and bigram. POS: part-of-
speech features. All Auto: all automatically extracted
features (Lex + POS + punctuation + caps + emoti-
cons).)
non-sparse logistic regression model, which obtains
an AUC of .563. When introducing penalty for large
weights in this standard model, .4 to .5 significant
improvements (p = .003 for Lasso, p = .007 for
ridge, and p = .004 for elastic net) of AUC are
observed from Lasso, ridge and elastic net models
when ? = 1. The elastic net model that balances
sparsity and smoothness, has obtain the best result
in this experiment. The best result of elastic net
model is .63 when ? = 7. This experiment shows
that all three penalty models have outperformed the
non-sparse logistic regression model. The elastic net
model, which balances sparisty and smoothness, ob-
tains the best results when predicting impoliteness.
Figure 2 shows the comparison of three models on
the friend dataset in the positivity prediction task.
When ? = 0, the standard logistic regression model
has an AUC of .638. When increasing the ? to 1,
both Lasso and elastic net models have shown sig-
nificant improvements (both p < .001) in AUC, but
not the non-sparse ridge estimator. The Lasso model
is found to be the best model in this task: we obtain
better results when the model gets sparser until the
model is too sparse when ? = 6. In contrast to the
experiment in Figure 1, we see that both the ridge
and elastic net models do not very strong advantages
25
in this positivity prediction task. We hypothesize
that the reason why Lasso works better in the pos-
itivity task is that the frequency of positivity labels
is substantially higher than the impoliteness labels in
our corpus, so that a Lasso model that enforces full
`1 penalty fits better in this task. In contrast, since
the impoliteness label is less frequent, a denser elas-
tic net composite penalty model that preserve critical
features, works the best in the impoliteness predic-
tion task. In general, we can see that sparse log-
linear models outperform standard log-linear mod-
els as well as non-sparse ridge estimators in the two
tasks.
Figure 1: Comparing Impacts of Different Levels of Spar-
sity on the Friend Dataset When Predicting Impoliteness
with Lasso, Ridge, and Elastic Net Models
Figure 2: Comparing Impacts of Different Levels of Spar-
sity on the Friend Dataset When Predicting Positivity
with Lasso, Ridge, and Elastic Net Models
5.4 Comparing Impacts of Different Feature
Window Sizes
A practical problem for parameter estimation in both
generative and discriminative models for dialogue
processing is to evaluate how much history the sys-
tem should take into account, so that it can have
enough information to make correct predictions. In
this experiment, we investigate the impact of using
different feature window sizes using the elastic net
model. We compare the two-tailed student t-test be-
tween the baseline that only uses features from the
current turn and models that use current + previous
n turn(s). For the friend dataset, when only using
the features from the current turn to predict the im-
politeness in the immediate next turn, we observe
an AUC of .619. The best result is obtained when
we combine the previous two turns together with the
current feature turn: an AUC of .635, significantly
better (p = .03) than only using the current turn win-
dow. The patterns on the non-friend dataset are less
clear, while the model obtains the best result when
window size is +3 previous turns, the improvement
is not significant (p = .962). In the positivity task,
we also observe benefits to incorporating larger fea-
ture windows. The AUC on the friend test set starts
at .638, when only using the current feature window
in the elastic net model. After incorporating larger
feature windows, we obtain the best result of .675 at
the +4 window (p = .04). Similarly, the AUC on
non-friend test set initializes at .618, but climbs to
.632 at the +4 window.
6 Error Analysis and Discussion
We performed an error analysis to understand the
contexts under which our model failed to accurately
predict a students? social response, and discuss the
implications of these examples based on a theoret-
ical understanding of the roles of tutors and tutees
as well as friends and strangers. The following is
an example error produced when looking only at the
previous turn to predict the current turn:
? Tutee (impolite): ?dude thats def wrong i gotta
subract 16m not just 16? (the current turn)
? Tutor (non-impolite): ?16m is what has to be
subtracted from both sides? (the next turn, pre-
dicted incorrectly)
In the segment above the tutee challenges the tutor
by pointing out a ?def? mistake; the tutor responds
with a task-oriented contribution that moves the di-
alogue forward, but does not escalate the face threat
(Ogan et al, 2012). And, in fact, if we look one
more turn back in the history, the tutor once again
uses calm language: ?wait it says youre wrong i dont
know why ust wait?. The increased window size
is implicitly evoking the differential conversational
strategies of tutors vs. tutees. And while the current
data set is too small to build separate models for tu-
tors and tutees, in this case (and based on the prior
work in Ogan et al, 2012), accounting for role dis-
tinctions that differentiate strategies taken by tutors
and tutees is the likely reason behind the improve-
ment due to window size.
Conversely to the friend data set, the false nega-
tives that occur when predicting impoliteness in the
stranger data set are not improved by increasing the
26
window size, as is demonstrated in the following ex-
change:
? Tutor (non-impolite): ?subtract ym from both
sides.?
? Tutee (non-impolite): ?first step? first Step??
? Tutor (non-impolite): ?subtract hb from both
sides? (the current turn)
? Tutee (impolite): ?first step? FIRST
STEP??????????? (the next turn, predicted in-
correctly)
The impolite tutee utterance at turn 4 is predicted
to be non-impolite when analysis is limited to the
previous turn, as is also shown in the first example
in this section. However, unlike the previous ex-
ample which improved with an expanding window
size, looking back to turns 1 and 2 does not improve
the model. While we do not have enough stranger
dyads to completely explore this phenomenon, it
seems clear that strangers? responses do not follow
the same patterns as friends. The current unpre-
dictability of strangers can be due to a number of
social phenomena, such as less affect (both posi-
tive and negative) overall, which results in a differ-
ent conversational flow. Less overall affect means
that there is less likely to be useful information in
the previous utterances. This is an important dis-
tinction between designing models for dyads with
rapport and those without, which is a primary con-
cern in the development of social SDSs. Among
strangers, other techniques may need to be used to
increase model accuracy, such as looking at the con-
tent of the utterances to determine whether or not a
speaker had been repeating themselves, as is shown
in this example, which could likely be an indicator
of rudeness.
As a final example of how the error analysis
can reveal important phenomena for future study,
when examining the prediction of positivity on the
stranger test set, we first observe that emoticons
are useful indicators of positivity. However, some-
times emoticons serve quite different social func-
tions, which leads to false positives:
? Tutor (non-positivity): ?Simplify ! :)? (the cur-
rent turn)
? Tutee (non-positivity): ?y didnt it chang? (the
next turn, predicted incorrectly)
Here, the smiley face is used by the tutor primarily
to mitigate the face threat of an impolite command.
However, since the experiment reported in Section
6.1 shows that our model attributes more weight to
emoticons when predicting positivity, the model errs
on this utterance. Here the error analysis suggests
that in fact we might need to investigate more com-
plicated latent variable models to capture the subtle
social functionality of some language use in context.
7 Conclusion
Long-term relationships involve the expression of
both positive and negative sentiments and, paradox-
ically, both can serve to increase closeness. In this
paper, we have addressed the novel task of predict-
ing impoliteness and positivity in teenagers? peer tu-
toring conversations, and our results shed light on
what kinds of behaviors evoke these social functions
for friends and for strangers, and for tutors and tu-
tees. Our investigation has successfully predicted
impoliteness and positivity on the basis of both an-
notated and automatically extracted features, sug-
gesting that a dialogue system will one day be able to
employ analyses such as these to signal relationships
with users. And while social features such as those
we annotated are naturally quite rare in dialogue, our
quantitative experiments have demonstrated the ca-
pabilities of modeling sparsity in log-linear models:
elastic net and Lasso models outperformed standard
logistic regression model and the non-sparse ridge
penalty model.
We found that positivity is much more predictable
for strangers than is impoliteness, while the oppo-
site was true for friends. This could lend support for
the importance of positivity as a rapport-signaling
function in the early stages of a relationship (as
in (Tickle-Degnen and Rosenthal, 1990)), and indi-
cating the need for further research on the increasing
importance of impoliteness as a rapport signal over
the course of relationship development.
We also found that performance on the prediction
tasks increased with larger feature window sizes,
particularly for impoliteness among friends and pos-
itivity among strangers. From our error analysis,
we see that this improvement may arise because dif-
ferent behaviors predict impoliteness and positivity
based on the social role of the speaker. Thus tu-
tee bragging predicts positivity in tutors, while tu-
tor bragging negatively predicts positivity among tu-
tees. The power differential between the two may
lead tutees to want to take tutors ?down a peg? while
tutors struggle to maintain the position of power in
the dyad.
While results such as these may seem specific to
teenage peer tutors, the general conclusion remains,
that linguistic devices have different social functions
in different contexts, and dialogue systems that in-
tend to spend a lifetime on the job will do well to
adapt their language to the stage of relationship with
a user, and the social role they play.
27
References
Hua Ai, Diane J. Litman, Kate Forbes-Riley, Mihai Ro-
taru, Joel Tetreault, and Amruta Purandare. 2006.
Using system and user performance features to im-
prove emotion detection in spoken tutoring dialogs. In
Proceedings of the Ninth International Conference on
Spoken Language Processing (Interspeech 2006).
Angela M. Ardington. 2006. Playfully negotiated activ-
ity in girls talk. Journal of Pragmatics, 38(1):73 ? 95.
Rachel E. Baker, Alastair J. Gill, and Justine Cassell.
2008. Reactive redundancy and listener comprehen-
sion in direction-giving. In Proceedings of the 9th
SIGdial Workshop on Discourse and Dialogue.
Timothy W. Bickmore and Rosalind W. Picard. 2005.
Establishing and maintaining long-term human-
computer relationships. ACM Transactions on
Computer-Human Interaction.
Timothy Bickmore, Laura Pfeifer, and Daniel Schulman.
2011. Relational agents improve engagement and
learning in science museum visitors. In Proceedings
of the 10th international conference on Intelligent vir-
tual agents, IVA?11.
Kristy Elizabeth Boyer, Robert Phillips, Michael Wallis,
Mladen Vouk, and James Lester. 2008. Balancing
cognitive and motivational scaffolding in tutorial di-
alogue. In Proceedings of the 9th international con-
ference on Intelligent Tutoring Systems, ITS ?08.
Penelope Brown and Stephen Levinson. 1978. Uni-
versals in language usage: Politeness phenomena. In
Questions and politeness: Strategies in social interac-
tion.
Justine Cassell, Alastair J. Gill, and Paul A. Tepper.
2007. Coordination in conversation and rapport. In
Proceedings of the Workshop on Embodied Language
Processing, EmbodiedNLP ?07, pages 41?50, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jan Chovanec. 2009. Simulation of spoken interaction in
written online media texts. Brno Studies in English.
David Crystal. 2001. Language and the internet. Cam-
bridge University Press.
Jonathan Culpeper. 1996. Towards an anatomy of impo-
liteness. In Journal of Pragmatics.
Jonathan Culpeper. 2011. Impoliteness: Using language
to cause offence.
Laurence Devillers and Laurence Vidrascu. 2006. Real-
life emotions detection with lexical and paralinguistic
cues on human-human call center dialogs. In Proceed-
ings of the Ninth International Conference on Spoken
Language Processing (Interspeech 2006).
A Gartner, M Kohler, and F Riessman. 1971. Children
teach children: Learning by teaching. In New York and
London: Harper and Row.
Jose? Gonza?lez-Brenes and Jack Mostow. 2011. Which
system differences matter? using l1/l2 regulariza-
tion to compare dialogue systems. In Proceedings of
the SIGDIAL 2011 Conference, pages 8?17, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Jonathan Gratch, Anna Okhmatovskaia, Francois
Lamothe, Stacy Marsella, Mathieu Morales, Rick J.
van der Werf, and Louis-Philippe Morency. 2006.
Virtual rapport. In Proceedings of the International
Conference on Intelligent Virtual Agents (IVA 2006).
M. Grimm, E. Mower K. Kroschel, and S. Narayanan.
2007. Primitives-based evaluation and estimation of
emotions in speech. In Speech Communication.
P. Gupta and R. Nitendra. 2007. Two-stream emo-
tion recognition for call center monitoring. In Pro-
ceedings of the 8th Annual Conference of the Inter-
national Speech Communication Association (Inter-
speech 2007).
Susan C. Herring and Asta Zelenkauskaite. 2009. Sym-
bolic capital in a virtual heterosexual market. In Writ-
ten Communication.
Je Hun Jeon, Rui Xia, and Yang Liu. 2010. Level of in-
terest sensing in spoken dialog using multi-level fusion
of acoustic and lexical evidence. In Proceedings of the
11th Annual Conference of the International Speech
Communication Association (Interspeech 2010), IN-
TERSPEECH 2010.
W. Lewis Johnson and Paola Rizzo. 2004. Politeness in
tutoring dialogs: run the factory, thats what id do. In
Intelligent Tutoring Systems, Lecture Notes in Com-
puter Science.
Manfred Kienpointner. 1997. Varieties of rudeness:
types and functions of impolite utterances. In Func-
tions of Language.
S. le Cessie and J.C. van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191?201.
Jackson Liscombe, Julia Hirschberg, and Jennifer J. Ven-
ditti. 2005. Detecting certainness in spoken tutorial
dialogues. In Proceedings of the 6th Annual Confer-
ence of the International Speech Communication As-
sociation (Interspeech 2005).
D. Litman and K. Forbes-Riley. 2004. Predicting stu-
dent emotions in computer-human tutoring dialogues.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2004).
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimization.
Mathematical Programming, 45:503?528.
Andre Martins, Noah Smith, Mario Figueiredo, and Pe-
dro Aguiar. 2011. Structured sparsity in structured
prediction. In Proceedings of the 2011 Conference on
28
Empirical Methods in Natural Language Processing,
pages 1500?1511, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Bruce M. McLaren, Sung-Joo Lim, David Yaron, and
Ken Koedinger. 2007. Can a polite intelligent tutor-
ing system lead to improved learning outside of the
lab? In Proceedings of the 2007 conference on Arti-
ficial Intelligence in Education: Building Technology
Rich Learning Contexts That Work.
Bruce McLaren, DeLeeuwm Krista E., and Richard E.
Mayer. 2011. Polite web-based intelligent tutors: Can
they im-prove learning in classrooms? In Computers
and Education.
Angeliki Metallinou, Martin Wollmer, Athanasios
Katsamanis, Florian Eyben, Bjorn Schuller, and
Shrikanth S. Narayanan. 2011. Context-sensitive
learning for enhanced audiovisual emotion classifica-
tion. IEEE Transactions on Affective Computing.
Amy Ogan, Samantha Finkelstein, Erin Walker, Ryan
Carlson, and Justine Cassell. 2012. Rudeness and
rapport: Insults and learning gains in peer tutoring. In
Proceedings of the 11 International Conference on In-
telligence Tutoring Systems (ITS 2012).
J. Alfredo Sa?nchez, Norma P. Herna?ndez, Julio C. Pena-
gos, and Yulia Ostro?vskaya. 2006. Conveying mood
and emotion in instant messaging by using a two-
dimensional model for affective states. In Proceedings
of VII Brazilian symposium on Human factors in com-
puting systems, IHC ?06, pages 66?72, New York, NY,
USA. ACM.
A. Sharpley, J. Irvine, and C. Sharpley. 1983. An exami-
nation of the effectiveness of a cross-age tutoring pro-
gram in mathematics for elementary school children.
In American Educational Research Journal.
Helen Spencer-Oatey. 2008. Face (im)politeness and
rapport. In Culturally Speaking: Culture, Communi-
cation and Politeness Theory.
Carolyn A. Straehle. 1993. ?samuel?? ?yes dear?? teas-
ing and conversatrion rapport. In Framing in Dis-
course.
Bas Stronks, Anton Nijholt, Paul van Der Vet, Dirk
Heylen, and Aaron Machado. 2002. Designing for
friendship: Becoming friends with your eca. In Pro-
ceedings of Embodied conversational agents - let?s
specify and evaluate (AAMAS).
Marina Terkourafi. 2008. Toward a unified theory of po-
liteness, impoliteness, and rudeness. Impoliteness in
language: studies on its interplay with power in the-
ory and practice.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267?288.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
nature of rapport and its nonverbal correlates. In Psy-
chological Inquiry.
Kevin E. Vowles and Miles Thompson. 2012. The
patient-provider relationship in chronic pain. In Psy-
chiatric Management of Pain.
Erin Walker, Nikol Rummel, and Kenneth R. Koedinger.
2011. Is it feedback relevance or increased account-
ability that matters? In Proceedings of the 10th Inter-
national Conference on Computer-Supported Collab-
orative Learning (CSCL 2011).
William Yang Wang and Julia Hirschberg. 2011. Detect-
ing levels of interest from spoken dialog with multi-
stream prediction feedback and similarity based hier-
archical fusion learning. In Proceedings of the 12th
annual SIGdial Meeting on Discourse and Dialogue
(SIGDIAL 2011), Portland, OR., USA, June. ACL.
William Yang Wang and Kathleen McKeown. 2010. ?got
you!?: Automatic vandalism detection in wikipedia
with web-based shallow syntactic-semantic modeling.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
1146?1154, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
William Yang Wang, Fadi Biadsy, Andrew Rosenberg,
and Julia Hirschberg. 2012a. Automatic detection
of speaker state: Lexical, prosodic, and phonetic ap-
proaches to level-of-interest and intoxication classifi-
cation. Computer Speech & Language.
William Yang Wang, Elijah Mayfield, Suresh Naidu, and
Jeremiah Dittmar. 2012b. Historical analysis of le-
gal opinions with a sparse mixed-effects latent vari-
able model. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012).
Richard J. Watts. 2003. Politeness. Cambridge Univer-
sity Press.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
29
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 19?20,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
!"#"$%&'($%)#(*+,&(+&-.*/%+&'(01*2&-3,#%4,5&&
6&7*44"+(#3&*8&9*,,(:(1(#(%,&
!
610+&;&<10)/&0+=&>0?(+%&@,/%+0A(&&&
"#$%&#%'!(')*$+,+%-'.!/$.0-0&0'1!!
2#3$'%-'!4',,+$!5$-6'3.-071!8-00.9&3%*1!8:1!5;:!
!"#$%&"'()*+,*&-,./-0
!
!
6:,#$0)#&
:!.<+='$!>-#,+%!.7.0'?!)+$.-.0.!+@!#!$&?A
9'3!+@!$+$A03-6-#,,7!-$0'3#)0-$%!)+?<+$'$0.B!
/$!+3>'3!0+!#,,+C!$'C!.0&>'$0.1!3'.'#3)*'3.!
#$>! >'6',+<'3.! 0+! ?'#$-$%@&,,7! #$>! 3',#A
0-6',7! 3#<->,7! '$0'3! 0*'! @-',>! -0! -.! )3-0-)#,!
0*#01!>'.<-0'!0*'-3!)+?<,'D-071!0*'!3'.+&3)'.!
9'! #))'..-9,'! #$>! '#.7! 0+! &.'B! E6'37+$'!
.*+&,>! 9'! #9,'! 0+! .0#30! 9&-,>-$%! $'C! 0')*A
$+,+%-'.! C-0*+&0! .<'$>-$%! #! .-%$-@-)#$0!
#?+&$0! +@! 0-?'! 3'A-$6'$0-$%! 0*'! C*'',B!!
(*'3'!#3'!@+&3!,'6',.!+@!.&<<+30!0*#0!C'!9'A
,-'6'!$'C!'$03#$0.!.*+&,>!*#6'B!!FG!:!@,'D-A
9,'!+<'$! .+&3)'! .7.0'?! 0*#0! 3&$.!+$!?#$7!
>-@@'3'$0! +<'3#0-$%! .7.0'?.1! -.! C',,! >+)&A
?'$0'>!#$>!.&<<+30.!9+0*!.-?<,'!#$>!)+?A
<,'D! >-#,+%! .7.0'?.B! ! HG! "+%.! #$>! .<'')*!
@-,'.! @3+?! #! ,#3%'! $&?9'3! +@! >-#,+%.! 0*#0!
'$#9,'! #$#,7.-.! #$>! 03#-$-$%! +@! $'C! .7.A
0'?.! #$>! 0')*$-I&'.B! JG! :$! #)0&#,! .'0! +@!
3'#,! &.'3.! 0*#0! .<'#=! 0+! 0*'! .7.0'?! +$! #!
3'%&,#3! 9#.-.B! KG!(*'! #9-,-07! 0+! 3&$! .0&>-'.!
+$!)+?<,'0'!3'#,!&.'3!<,#0@+3?.B!
B! <0)/2$*"+=&
(*'! %+#,!+@! 0*'!L-#,+%!M'.'#3)*!2'$0'3! NL-#,M2G!
*#.!9''$! 0+!<3+6->'! 0*'! .<+='$!>-#,+%!)+??&$-07!
C-0*!0*3''!,'6',.!+@!.&<<+30!-$!0*'!@+3?!+@!0++,.!#$>!
>#0#! @+3! .<+='$! >-#,+%! .7.0'?.O! +<'$! .+&3)'! .+@0A
C#3'P! ,+%.! #$>! .<'')*! >#0#! @3+?! 3'#,! >-#,+%.1! #!
)+??&$-07!+@!3'#,!&.'3.!0*#0!&.'!#!.7.0'?!3'%&,#3,7!
+$!3'#,1!&.'@&,!<,#0@+3?.!+$!C*-)*!3'.'#3)*'3.!)#$!
3&$! .0&>-'.B! /$! 0*-.! .*+30! <#<'3! C'! >'.)3-9'! 0*'.'!
@+&3! ','?'$0.! 0*#0! +&3! 2'$0'3! *#.! '$>'#6+3'>! 0+!
<3+6->'B!"++=-$%!0+!0*'!@&0&3'!C'! ,++=!0+!0*'!.<+A
='$! >-#,+%! )+??&$-07! 0+! )+$03-9&0'! +0*'3! <,#0A
@+3?.!0+!+&3.!0+!%-6'!$'C)+?'3.!0+!0*'!@-',>!#!3-)*!
.'0!+@!'D<'3-?'$0#,!<,#0@+3?.!+$!C*-)*!0+!,'#3$!0*'!
3+<'.B!!
!
!.%+&,*"$)%&,.*/%+&=(01*2&,*8#C0$%O!Q'!#,3'#>7!
<3+6->'1! #.! R<'$! ;+&3)'! .+@0C#3'1! 0*'! 245!
R,7?<&.! ;<+='$!L-#,+%! ;7.0'?! 0*#0! +@@'3.!:;M1!
((;1!#!L-#,+%!4#$#%'3!#$>!+0*'3!)+?<+$'$0.!0*#0!
#,,+C!>'6',+<'3.!0+!9&-,>!9+0*!.-?<,'!#$>!)+?<,'D!
>-#,+%! .7.0'?.B! !Q*-,'! 0*-.! #3)*-0')0&3'! *#.! 9''$!
&.'>! -$! ?#$7! .7.0'?.! N.+?'! +@! 0*'?! #3'O! ('#?A
(#,=! ST#33-.! '0! #,! HUUKV1! M#6'$2#,'$>#3! S;0'$A
)*-=+6#!'0!#,B!HUUWV1!2+$X&'.0!SY+*&.!'0!#,B!HUUWV!
#$>! "'0Z.! [+! SM#&D! '0! #,B! HUU\VG1! -0! $''>.! 0+! 9'!
#))+?<#$-'>!97!?+3'!.&<<+30! -$! 0*'!@+3?!+@!9+0*!
>+)&?'$0#0-+$!#$>!@,'D-9-,-07B!/0!.*+&,>!#,.+!$+0!9'!
0*'! +$,7! <,#0@+3?! 0*#0! -.! #6#-,#9,'! 0+! 0*'! )+??&A
$-07! 0+! 3&$! .0&>-'.B! (*'3'! #3'! $'C! .0&>'$0.1!
3'.'#3)*'3.!#$>!>'6',+<'3.!C*+!C#$0!0+!*+$'!0*'-3!
.=-,,.!97!#>#<0-$%!#!>-#,+%!#3)*-0')0&3'!#$>!3&$$-$%!
-0!+$!#!3'#,!&.'3!<,#0@+3?B!/$!+3>'3!0+!?#='!-0!'#.-'3!
@+3!0*'.'!$'C)+?'3.!0+!9&-,>!>-#,+%!.7.0'?.!-$!0*'!
@+3?! +@! .*+30! *+?'C+3=! #..-%$?'$0.! N<'3*#<.! -$!
FAH!C''=.G1! @+3! #! 3'%&,#3! ),#..1!R,7?<&.!?&.0! 9'!
?+3'!@,'D-9,'1!#$>!'#.-'3!0+!&$>'3.0#$>!#$>!?#.0'3B!
Q-0*! 0*'!+<'$!.+&3)'!)+3'!.7.0'?!0*#0!*#.!
#,3'#>7! 9''$! 3','#.'>1!C'! <,#$! 0+! #>>! 6-30&#,!?#A
)*-$'.! 0*#0! *#6'! #,,! +@! 0*'! )+?<+$'$0.! <3'A
-$.0#,,'>1!#.!>+$'!-$!#$+0*'3!#3'#!97!S(+=&>#!'0!#,!
HUFHVB! (*-.! C-,,! ?#='! -0! '#.-'3! @+3! $'C)+?'3.! 0+!
.0#30!C3-0-$%! #$>!?+>-@7-$%!>-#,+%! .7.0'?.! -??'A
>-#0',7! 3#0*'3! 0*#$! .<'$>-$%! 0-?'! -$.0#,,-$%! 9,#)=!
9+D! .+@0C#3'B! (*-.! -?<,-'.! 0*#0! +&3! 'D-.0-$%!Q-$A
>+C.!.&<<+30!?&.0!9'!'D0'$>'>!0+!#,.+!)+6'3!"-$&D!
#$>!4#)!R;]B!!!
!
D*2& =0#0& 8$*4& =(01*2,O! ;+?'! +@! 0*'! .-%$-@-)#$01!
'D)-0-$%! #>6#$)'.! 0*#0! *#6'! 3')'$0,7! 9''$! .''$! -$!
0*'! 3'#,?! +@! .<+='$! >-#,+%! .7.0'?.! &.'! .0#0-.0-)#,!
?+>',-$%B! (*-.! -?<,-'.! 0*'! #)&0'! $''>! @+3! >#0#1!
#9+6'!#,,! !."10/"2"1! 0+! 03#-$! 0*'!?+>',.B!(*'!<,#0A
@+3?.!0*#0!<3+6->'!0*#0!>#0#!0+!#!)+??&$-07!.*+&,>!
@+,,+C!#!.0#$>#3>-^'>!@+3?#0! -$! 0*'!.#?'!C#7!0*#0!
.<'')*!@-,'.!*#6'!9')+?'!.0#$>#3>-^'>B! !"+%!L#0#!
)#$! 9'! &.'>! @+3! +@@,-$'! #$#,7.-.! 0*#01! -$! 0&3$1! )#$!
19
#@@+3>! >''<'3! @-3.0! *#$>! -$.-%*0! -$0+! *+C! .<+='$!
>-#,+%!.7.0'?.B!
!
6&)*44"+(#3&*8&$%01&",%$,&0+=&$%01&.10#8*$4,&#*&
$"+&,#"=(%,O!!C'!*#6'!.''$!S_+&$%1!HUFUV!0*#0!-0!-.!
$+! ,+$%'3! 3'#.+$#9,'! 0+! 0'.0! #! *7<+0*'.-.! #9+&0!
.<+='$!>-#,+%!C-0*!#!.?#,,!$&?9'3!+@!<#->!<#30-)-A
<#$0.B!!E$>!&.'3.!?&.0!9'!3'#,O!0*'7!*#6'!.+?'!-$A
0'3'.0! -$! 0*'!+&0)+?'! +@! 0*'! 0#.=! #0! *#$>!#$>! 0*'7!
#3'!$+0!&.-$%! 0*'! .7.0'?! `&.0! 9')#&.'! 0*'7a3'!<#->!
#$>b+3!)+,,')0-$%!'6#,&#0-+$!>#0#B!!(*-.!%+#,!-.!>-@A
@-)&,0B! T+C'6'31! C'! #0! L-#,M2!C#$0! 0+! <3+6->'! #!
)'$03#,-^'>!?')*#$-.?!0+!%-6'!0*'!$'C)+?'3.!N#$>!
#,3'#>7!'.0#9,-.*'>!3'.'#3)*'3.!#.!C',,G!#))'..!0+!#!
%3+&<!+@!<,#0@+3?.!C-0*! 3'#,!&.'3.B!(*'3'!?&.0!9'!
0*'!<+..-9-,-07!+@!+90#-$-$%!#!0#$%-9,'!9'$'@-0!@3+?!
0*'.'! 3'#,! <,#0@+3?.! #$>! 0*'! 3'.'#3)*! )+??&$-07!
.*+&,>!9'!C-,,-$%!0+!+<'$!0*'-3!.7.0'?.!0+!+0*'3.!.+!
0*#0! 0*'7!)#$! 0'.0! 0*'-3! ->'#.! -$! #! 3'#,-.0-)! )+$0'D01!
C-0*!#!.-%$-@-)#$0!$&?9'3!+@!3'#,!)#,,'3.B!!!
!
R&3! )&33'$0! '@@+30.! *#6'! +@0'$! )'$0'3'>! +$! ),#..-)!
0','<*+$'A9#.'>! -$@+3?#0-+$! %-6-$%! .7.0'?.B! [+A
-$%!@+3C#3>!-0!-.!-?<+30#$0!0+!0#='!#!C->'3!6-'C!+@!
0*'! 07<'.! +@! .<+='$! >-#,+%! 3'.'#3)*! C'! )#$! #>A
>3'..B! !(*&.!C'! #3'! #,.+! -$0'3'.0'>! -$! .&<<+30-$%O!
?&,0-?+>#,! -$0'3#)0-+$1! *&?#$A3+9+0! -$0'3#)0-+$1!
?&,0-A<#307!)+??&$-)#0-+$!#$>!'6'$!0#.=.!C-0*!$+!
),'#3!>'@-$-0-+$!+@! 0#.=! )+?<,'0-+$! N'B%B! )+$6'3.#A
0-+$#,!9#$0'3GB!!Q*#0!-.!-?<+30#$0!-.!$+0!<3+?+0-$%!
+$'!07<'!+@!3'.'#3)*!?+3'!0*#$!#$+0*'3B!/0!-.!?#=A
-$%!?#$7!>-@@'3'$0!3'#,A&.'3!<,#0@+3?.!#6#-,#9,'!0+!
0*'!)+??&$-07!#0!,#3%'B!
245Z.!L-#,M2!<3+<+.'.! 0+!#)0!#.!#!),'#3A
-$%! *+&.'! @+3! .+@0C#3'! N+&3! +C$! #$>! 0*#0! +@! #$7!
+0*'3.G1! >#0#! N9+0*! .<'')*! #$>! ,+%@-,'.G1! #$>! 3&$A
0-?'!3'#,!#<<,-)#0-+$b3'#,!&.'3!<,#0@+3?.! 0*#0!%-6'.!
0*'! )+??&$-07! #! )'$03#,! <,#)'! 0+! @-$>! #! <,#0@+3?!
0*#0!)+33'.<+$>.!0+!0*'-3!$''>.1!0+!)+$$')0!0*'?!0+!
0*'!>'6',+<'3.!+@!0*#0!<,#0@+3?1!#$>!0+!*',<!>-.03-9A
&0'! 0*'! >#0#! N.<'')*! #$>! ,+%@-,'.G! )+?-$%! @3+?!
0*'-3!&.'!+@!-0B!!!
(*'.'!0*3''!#)0-+$.!C-,,!9&-,>!)+??&$-0-'.!
+@!$'C!3'.'#3)*'3.!#$>!>'6',+<'3.!C*+1!@3+?!0*'-3!
&.'! +@! 0*-.! <,'0*+3#! +@! <,#0@+3?.! C-,,! '$3-)*! 0*'!
,#00'3!C-0*!C*#0! 0*'7!*#6'! ,'#3$'>!#$>!C-,,! '$3-)*!
+&3!)+??&$-07!C-0*!0*'-3!<3'.'$)'B!!
Q'! '$6-.#%'! 0*'! @+,,+C-$%! .)'$#3-+B! :!
.0&>'$0! *#.! #! .*+30! #..-%$?'$0! 0+! ?#='! .+?'!
)*#$%'! 0+! +$'! +@! 0*'! 9#.-)! #3)*-0')0&3'.! 0*#0! *#.!
9''$!?#>'!#6#-,#9,'!97!L-#,M2B!Q*'$!0*'!#..-%$A
?'$0! -.! @-$-.*'>1! 0*'7! ,-$=! 0*'-3! .7.0'?! 0+! #! <,#0A
@+3?! 0*#0! 0*'7! @+&$>! 0*3+&%*! +&3! )'$03#,! ,-.0-$%B!
M'#,! &.'3.! )#,,! 0*#0! <,#0@+3?! N*'3'1! +&3! .0&>'$0Z.!
.7.0'?G!C*'$!0*'7!$''>!C*#0!-.!9'-$%!+@@'3'>!N-$A
@+3?#0-+$!+$!#!%++>!6'%'0#3-#$!3'.0#&3#$0!-$!2#?A
93->%'1!C*'$! 0*'!$'D0!9&.! 0+! 0*'!#-3<+30! -.!)+?-$%!
-$!8-00.9&3%*1!#!>-.)&..-+$!+@!$'C!0*-$%.!0+!.''!-$!#!
?&.'&?1!'0)GB!(*'!.0&>'$0!)#$!0*'$!#))'..!0*'!3'#,!
&.'3! >#0#! 0*#0! *#.! 9''$! )+,,')0'>! C*-,'! 0*'-3! 6'3A
.-+$! +@! 0*'! .7.0'?!C#.! 3&$$-$%B! 8'3*#<.! -$! #! @+,A
,+C-$%! #..-%$?'$01! -@! 0*'7! <3+6->'>! 0C+! 6'3.-+$.!
+@! 0*'! .7.0'?1! 0*'7! )#$! @-$>! +&01! @3+?! #$#,7^-$%!
0*'!>#0#1!C*-)*!)+$>-0-+$!C+3='>!9'.0B!/@!0*'7!<3+A
6->'>! +$'! )+$>-0-+$1! #$>! +0*'3! .0&>'$0.! <3+6->'>!
+0*'3! +$'.1! 0*'$! 0*'7! )#$! )+?<#3'! 0*'-3! 3'.&,0.! 0+!
0*+.'!+@!0*'!+0*'3!.0&>'$0.B!!
:! )+$.0#$0,7! #6#-,#9,'! 3'.+&3)'1! 0*'! )+?A
<'0-0-+$! 9'0C''$! 6'3.-+$.! +@! #! .7.0'?! >+'.! $+0!
*#6'!0+!9'!*',>!#0!#!0-?'!0*#0!?#7!9'!-$)+$6'$-'$0!
@+3!.+?'B!/0!)#$!9'!#$!+$%+-$%!'6'$0!0*#0!3'.'#3)*A
'3.! )#$! <#30-)-<#0'! -$! C*'$! -0! -.! )+$6'$-'$0! @+3!
0*'?!0+!>+!.+B!!
&
E%8%$%+)%,&
!
T#33-.1!(B1!Y#$'3`''1!;B1!M&>$-)=71!:B1!;-.+$1!cB1!Y+>-$'1!
dB1! #$>! Y,#)=1! :B! NHUUKG! e:!M'.'#3)*! 8,#0@+3?! @+3!
4&,0-A:%'$0! L-#,+%&'! L7$#?-).f1! /EEE! Q+3=.*+<!
+$!M+9+0-).!#$>!T&?#$!/$0'3#)0-6'!2+??&$-)#0-+$B!!
;0'$)*-=+6#1! ;B1! 4&)*#1! YB1! T+@@?#$1! ;B1! ;0'$01! :B!
NHUUWG1!!fM#6'$2#,'$>#3O!:!4&,0-?+>#,!L-#,+%!;7.A
0'?! @+3! 4#$#%-$%! #! 8'3.+$#,! 2#,'$>#3fB! T"(A
g::2"!HUUWB!
Y+*&.1!YB1![3#&1!;1!T&%%-$.AL#$'.1!LB1!d'3-1!hB1!:$&A
?#)*-<#,,-1! [B1! d&?#31! MB1! M#&D1! :B! i! (+?=+1! ;B!
NHUUWG! f2+$X&'.0O! #$! R<'$A;+&3)'! L-#,+%! ;7.0'?!
@+3!2+$@'3'$)'.f1!T"(Ag::2"!HUUWB!!
M#&D1!:B1!!"#$%$'31!YB1!Y+*&.1!LB1!Y,#)=1!:B1!#$>!ED='A
$#^-1!4B! NHUUjG! e"'0Z.![+!8&9,-)k!(#=-$%!#!;<+='$!
L-#,+%!;7.0'?!0+!0*'!M'#,!Q+3,>Bf!/$0'3.<'')*!HUUjB!!
(+=&>#1!d1!"''1!:1!_#?#%-.*-1!cB!'0!#,B!NHUFHG!e;<+='$!
L-#,+%! .7.0'?!l3#?'C+3=! 9#.'>! +$!5.'3!['$'3#0'>!
2+$0'$0f1!g#%+7#! /$.0-0&0'! +@!(')*$+,+%71! #$>!5$-A
6'3.-07!+@!E>-$9&3%*1!@&$>'>!&$>'3!c;(!2ME;(!83+A
%3#?B!!!
_+&$%1! ;B! e;0-,,! (#,=-$%! 0+! 4#)*-$'.! N2+%$-0-6',7!
;<'#=-$%G1! d'7$+0'! /$0'3.<'')*! HUFU1!4#=&*#3-! c#A
<#$B!
20
Proceedings of the SIGDIAL 2013 Conference, pages 51?60,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Automatic Prediction of Friendship via Multi-model Dyadic Features 
Zhou Yu, David Gerritsen, Amy Ogan, Alan W Black, Justine Cassell 
School of Computer Science, Carnegie Mellon University 
{zhouyu, dgerrits, aeo, awb, justine }@cs.cmu.edu 
 
Abstract 
In this paper we focus on modeling 
friendships between humans as a way of 
working towards technology that can initiate 
and sustain a lifelong relationship with users. 
We do this by predicting friendship status in a 
dyad using a set of automatically harvested 
verbal and nonverbal features from videos of 
the interaction of students in a peer tutoring 
study. We propose a new computational 
model used to model friendship status in our 
data, based on a group sparse model (GSM) 
with L2,1 norm which is designed to 
accommodate the sparse and noisy properties 
of the multi-channel features. Our GSM model 
achieved the best overall performance 
compared to a non-sparse linear model (NLM) 
and a regular sparse linear model (SLM), as 
well as outperforming human raters. Dyadic 
features, such as number and length of 
conversational turns and mutual gaze, in 
addition to low level features such as F0 and 
gaze at task, were found to be good predictors 
of friendship status. 
1 Introduction and Related Work 
While significant advances have been made in 
detecting the speech and nonverbal social signals 
emitted by individuals (see Vinciarelli, Pantic & 
Bourlard, 2009, for a review), and research has 
addressed the social roles and states of 
individuals in groups (see Gatica-Perez, 2009, 
for a review), considerably less computational 
work has focused on the automatic detection of 
speech or nonverbal correlates of specifically 
dyadic states, such as rapport. And yet rapport 
has been shown to have important effects on 
interactions as diverse as survey interviewing 
(Berg, 1989), sales (Brooks, 1989), and health 
(Harrigan et al, 1985).  If we are to build 
interactive systems that are successful, then, we 
believe that the ability to build rapport with a 
human user will be essential. 
Rapport can be instantaneous and can also 
build over time. Granovetter (1973) describes the 
strength of an interpersonal ?tie? as a function of 
the time, emotional intensity, and reciprocity that 
accumulates between people. These ties mediate 
effects in myriad domains such as learning 
(Azmitia & Montgomery, 1993) and healthcare 
(Harrigan & Rosenthal, 1983).  
Accordingly, analysis of initial exchanges and 
those after many years of interaction suggests 
that the behavioral signals that indicate rapport 
change over time. For example, in Tickle-
Degnen and Rosenthal?s highly cited model 
(1990), rapport consists of mutual attention, 
positivity, and coordination. High levels of 
positivity between conversational partners are 
common in the initial phases of a relationship, 
but positivity has been shown to decline, without 
a loss in rapport, as the number of interactions 
increases. In fact, Ogan et al (2012) gave 
evidence that the use of playful rudeness 
between friends during peer tutoring correlates to 
greater learning. This leads to an associated 
challenge of spoken dialogue system 
development: creating systems that can develop 
social ties, and increase rapport with the user 
over repeated interactions to maximize beneficial 
outcomes. 
While little work has addressed automatic 
detection, some prior work has addressed the 
problem of emitting signals to build rapport in 
dialogue and agent systems (Stronks et al, 2002; 
Bickmore & Picard, 2005; Gratch et al, 2006; 
Cassell et al, 2007; Bickmore et al, 2011), and 
we turn to this research for what cues might be 
important in rapport. The majority of this prior 
work, however, has addressed harmony ? or 
instant rapport ? rather than rapport over time. 
For those systems that have addressed friendship 
or the growth of rapport, most commonly the 
number of interactions has been used as a meter 
of relationship progression, instigating changes 
in the dialogue system as the social odometer 
scrolls onward (Cassell & Bickmore, 2003; 
Vardoulakis et al, 2012). Counting the times a 
dyad has interacted is a crude approximation of a 
relationship state, however; being able to detect 
the behavioral signals that people actually use to 
indicate relationship status would be superior. 
In our own prior work (Cassell et al,2007) we 
looked at particular hand-annotated nonverbal 
signals (such as nodding and mutual gaze) as 
operationalizations of rapport, and found that 
friends and non-friends indeed show differing 
distributions of each signal as a function of 
relationship state. In the current study, we move 
to the next step and automatically harvest a set of 
multimodal dyadic and time contingent features 
to identify those features that play a significant 
role in predicting friendship state. A major 
51
challenge for predicting relational states such as 
these is to construct a compact feature space that 
captures only reliable rapport signals and also 
generalizes across different users. To provide 
strength to our model (as well as to fit the 
multimodal nature of embodied conversational 
agents), we look at both acoustic and visual 
features. Such an approach takes advantage of 
the fact that multimodal aspects of 
communication are not redundant, but often 
complementary (Cassell, 2000).  
    However, dyadic behaviors such as 
conversational turns, mutual/non-mutual smile, 
mutual/non-mutual gaze, and mutual/non-mutual 
lean forward provide an additional challenge in 
modeling; no matter how important, they appear 
relatively rarely in conversational data. Thus 
standard non-sparse linear models, normally 
trained on high frequency factors, might assign 
too much weight to low frequency (i.e., sparse) 
features. In order to address issues of this sort 
Yuan and Lin (2007) introduced the group 
lasso.   To address the sparse nature of our 
features in real-world data and the noise that 
occurs from different production sources, we 
propose an extension to this genre of technique 
in the form of a Group Sparse Model (GSM) 
which enforces sparsity with a L2,1 norm instead 
of the group lasso penalty (Chen, et  al., 2011), 
due to the relatively efficient optimization 
process of L2,1 norms (Liu, et al, 2009). Unlike 
a straightforward sparse linear model (SLM) 
(Yang et al, 2010), which treats each feature 
independently, GSMs group features which share 
the same production source in the optimization 
process. In the GSM linear model, the removal of 
the assumption of independence between 
features means that the penalty is on group rather 
than individual features. Thus the model has 
general robustness to noise, since grouping 
features from the same production source can 
increase the overall confidence of the feature 
group. 
Our contributions in this work, then, are three-
fold: we (1) designed and implemented a method 
for automatic dyadic feature extraction which is 
based on low level features, and which yields 
strong predictive power of friendship status, (2) 
propose a new Group Sparse Model (GSM) with 
L2,1 norm, that deals with the noisy and sparse 
nature of the feature sets, and (3) illuminate, 
from this model, the nature of verbal and 
nonverbal behavior between friends and non-
friends in a peer tutoring setting. 
The remainder of the paper is organized as 
follows. We first describe the data set and 
introduce the features used in our experiments. 
We then describe the performance of the three 
computational models we evaluated. Finally, we 
discuss the contributions of different features to 
friendship prediction and provide an error 
analysis of our proposed model.  
2 The Data Set 
  
Figure 1: Camera View 1 and Camera View 2 
We collected data from dyads of students 
engaged in a reciprocal peer tutoring task. We 
chose peer tutoring as it is a domain in which 
friendship has been shown to have a positive 
effect on student learning (see e.g. Ogan et al 
2012). In addition, tutoring systems that rely on 
dialogue are common, and peer tutoring dialogue 
systems are increasingly common. Thus, being 
able to assess friendship state in this domain is a 
useful step on the path to creating a peer tutoring 
agent that can use rapport to increase learning 
gains.  
    Each dyad consisted of two American English 
speakers with a mean age of 13.3 years (range = 
12 ? 15). We collected data from 12 dyads, of 
which 6 dyads were already friends. Dyads were 
either both girls or both boys, and each condition 
contained 3 boy dyads and 3 girl dyads.  
Each dyad came to the lab for 3 sessions, with 
an average interval between visits of 4.6 days 
(SD = 3.1), totaling 36 sessions across all dyads. 
Each session consisted of about 90 minutes of 
interaction recorded from three camera views (a 
frontal view of each participant and a side view 
of the two participants). With close talk 
microphones, we also recorded the participants? 
speech in separate audio channels for the purpose 
of automatic dyadic acoustic feature extraction. 
The setting is shown in Figure 1. 
Each session began with a short period of time 
for participants to become acquainted. After that, 
using a standard reciprocal tutoring procedure 
(see Fantuzzo et al, 1989), participants tutored 
each other on procedural and conceptual aspects 
of an algebra topic in which both participants 
were relatively novice. Order of seating and 
assignment of tutoring roles (tutor or tutee) was 
determined in the first session by alphabetical 
order of participant name. Tutoring roles 
alternated from that point on, such that both 
participants had the opportunity to take on the 
role of ?expert? during each session. After a 
period of individual study time to familiarize 
52
themselves with the material, the first tutoring 
period began and lasted approximately 25 
minutes. This was followed by a 5 minute break, 
after which students? tutoring roles were reversed 
for a second tutoring period of 25 minutes. 
Finally, each student answered a survey about 
the interaction.  
The current study examines only the tutoring 
sections of each session, which were divided into 
30-second clips or ?thin slices? (Ambady et al, 
2006). In total, the data points used for modeling 
comprise 2259 clips from the 12 dyads. 
3 Multimodal Information  
In our analyses, low-level audio and visual 
features were automatically extracted using three 
off-the-shelf toolkits. Dyadic features, which are 
a second order derivative of the low level 
features, and which capture the interaction of two 
participants, are also automatically produced. 
Taken together, analysis of these features allows 
us to determine if the verbal and nonverbal 
behaviors of the participants index their 
friendship status in any significant way.  
3.1 Low Level Audio Features (LA)  
Type # of Features 
Prosodic Features 
  F0 72 
  Energy 38 
  Duration 154 
Voice Quality Features 
  Jitter 68 
  Shimmer 34 
  Voicing 38 
Spectral Features 
  MFCC 570 
Total 974 
 
Table 1: Acoustic Feature Groups 
 
For acoustic feature extraction, a large set of 
acoustic low-level descriptors (LLD) and 
derivatives of LLDs combined with appropriate 
statistical functionals, i.e., maxPos (the absolute 
position of the maximum value in frames), 
minPos (the absolute position of the minimum 
value in frames), amean (The arithmetic mean of 
the contour), etc., were extracted for each of the 
split channel recordings. The ?INTERSPEECH 
2010 Paralinguistic Challenge Feature Set? in the 
openSMILE toolkit (Schuller et al, 2012) was 
used as our basic acoustic feature set. For 
spectral features, Mel Spectrum and LSP were 
excluded due to the possible overlap with 
MFCC. The set contained 974 features which 
resulted from a base of 32 low-level descriptors 
(LLD) with 32 corresponding delta coefficients, 
and 21 functionals applied to each of these 68 
LLD contours. In addition, 19 functionals were 
applied to the 4 pitch-based LLD and their four 
delta coefficient contours. Finally the number of 
pitch onsets (pseudo syllables) and the total 
duration of the input were included. The 
dimension of each feature group is shown in 
Table 1. 
3.2 Low Level Vision Features (LV) 
Type # of Features 
Face Position Feature 10 
38 Face Interest Points 114 
Gaze Features 3 
Face Direction  Features 4 
Mouth and Eye Openness 6 
Smile Intensity 1 
Discretized Smile 1 
Total 139 
 
Table 2: Vision Feature Groups 
 
Since participants were facing the camera 
directly most of the time, as seen in Fig 1, 
current technology for facial tracking can 
efficiently be applied to our dataset. OMRON?s 
OKAO Vision System was used in face 
detection, facial feature extraction, and basic face 
related features extrapolation. For each frame, 
the vision software returns a smile intensity (0-
100) and the gaze direction, using both 
horizontal and vertical angles expressed in 
degrees. Apart from gaze direction, the software 
also provides information about head orientation: 
horizontal, vertical, and roll (in or out). 38 
additional face interest points, position and 
confidence, were also extracted. These were 
normalized to pixel coordinates, which turned 
out to lead to quite noisy data, and hence to 
diminished utility of these 38 points (in the 
future we will consider normalizing to face 
coordinates). We also calculated the openness of 
the left eye, right eye, mouth, and the location of 
the face. Details are shown in Table 2. Similar to 
our audio feature extraction method, one static 
feature vector per 30 second video clip was 
produced. All the features were computed at the 
same rate as the original videos: 30 Hz. 
Altogether, 139 dimensions were extracted in 
each frame from each camera view. 
3.3 Dyadic Features (DF) 
All of the features discussed above are low-level 
acoustic and visual features, extracted with 
53
respect to individual participants. While 
individual behavior may index friendship state, 
we posit that patterns of interaction will be more 
effective. For example, prior research (Baker et 
al., 2008) suggests that the number and length of 
conversational turns (Cassell et al, 2007), 
presence of mutual smiles and non-mutual smiles 
(Prepin et al, 2012), mutual gaze and non-
mutual gaze (Nakano et al, 2010), as well as 
posture shifting (Cassell, et al, 2001; Tickle-
Degnen & Rosenthal, 1990), are important 
features to investigate in dyadic data. While 
other features such as gestures and mutual pitch 
shift may also play a role in indexing relationship 
state, these are not yet a part of the dyadic 
features we address here.  
3.3.1 Number and Average Length of 
Conversational Turns   
We recorded individual audio channels for each 
participant, which makes the automatic 
extraction of conversational turns possible. First, 
we extracted intervals of silence with toolbox 
SoX which produced speech chunks, and then 
identified the speaker by comparing the speech 
energy (loudness) in each audio channel, as 
speech from each speaker is carried by the 
other?s microphone. After that we combined the 
speech chunks and speaker ID to approximate 
conversational turns. The approximation quality 
is not perfect, given the variability of the audio 
recording, but noise can be mediated during 
model building. 
3.3.2 Mutual Smile and Non Mutual Smile  
Prepin et al (2012) describe the role of mutual 
smiles (smiles that occur during the same time 
period) in ?stance alignment? and make the point 
that interactional alignment of this behavior 
reflects synchronization of internal states. Such 
synchrony predicts mutual understanding and 
increased quality of interaction, and as such is a 
fundamental quality in the formation of 
adolescent friendships (Youniss, 1982). Cappella 
& Pelachaud (2002) likewise describe 
?mutuality? as the precondition for how smiles 
function in contingent ways in a dyad. Smiles are 
clearly therefore important to assess in data such 
as ours. We defined a maximum window of 500 
milliseconds between the end of one participant?s 
smile and the beginning of the next for smiles to 
be considered mutual.  
3.3.3 Mutual Gaze and Non-mutual Gaze 
Nakano & Ishii (2010) describe eye gaze as a 
clue to engagement, and integrate mutual gaze 
into their conversational agents. There is no 
feature for direct gaze at partner provided in the 
OKAO vision toolkit. Mutual gaze was therefore 
approximated by annotating a gaze ?in front,? 
achieved by combining the information from 
three directions of gaze: vertical, horizontal, and 
depth. Gaze ?in front?, or at the partner, was 
recorded only if the participant gaze had less 
than a 15 degree angle from straight forward in 
all of these three directions. A maximum window 
of 500 milliseconds for gaze to be considered 
mutual was also employed here.  
3.3.4 Mutual Lean Forward and Non-Mutual 
Lean Forward 
Forward leaning has been shown to be a 
significant predictor of the ability to establish 
rapport in a dyad (Harrigan et al, 1985). In fact, 
friends who lean in are seen as more socially 
competent, while strangers are seen as less 
socially competent when they lean in (Burgoon 
& Hale, 1988). For our study, lean forward was 
approximated by detecting the smooth trend of 
face enlargement within the video frame. In 
order to improve precision of the feature, the 
segments with high confidence in face detection 
were processed. Furthermore, posture shifting, 
i.e., forward leaning, is not as quickly executed 
as changes in gaze or smile. We therefore used a 
1 second sample window for lean forward, rather 
than a 500 millisecond window.  
3.3.5 Mutual Gaze followed by Mutual Smile 
Mutual gaze followed by mutual smile is also 
approximated using a similar approach as above. 
It is a relatively dense feature compared to all the 
other possible combinations of nonverbal 
behaviors, thus it is the only combination that is 
included in the feature set in this paper. The 
window within which mutual gaze is considered 
to be followed by mutual smile is set to be within 
2 seconds. 
4 Computational Model  
We formulate friendship prediction as a set of 
binary classifications. In order to have the least 
variance and make sure no participant appeared 
in both the training and testing set, a leave-one-
out cross-validation setting was adopted in all of 
our experiments. Each session had approximately 
180 30-second video clips, totaling 2259 data 
points. Z-score normalization by dyad was used 
to scale all the features into the same range. 
Early fusion, which is simple concatenation of 
feature vectors, was adopted throughout our 
experiments to combine different features. We 
evaluated our group sparse model (GSM), along 
with a non-sparse linear model (NLM) and 
sparse linear model (SLM). 
54
4.1 Non-sparse Linear Model (NLM) 
We began with a standard non-sparse linear 
model (NLM), which is a Support Vector 
Machine (SVM) (Cortes & Vapnik, 1995) with a 
linear kernel. The libsvm (Fan et al, 2008) 
package was used in our experiment, and the 
parameter, the slack value of SVM that controls 
the scale of the soft margin, was obtained by 
cross validation.  
4.2 Sparse Linear Model (SLM)  
In order to prevent over-fitting on rare dyadic 
features, a sparse sensitive model SLM was 
introduced. As well as preventing over-fitting, 
through weight shrinkage the sparse model can 
also exclude redundant features. In our 
experiment, an L2,1 norm sparse model with 
linear kernel (Yang et al, 2012) was selected as 
our baseline sparse model. 
4.3 Group Sparse Model (GSM) 
Based on the SLM, we propose a group-sparse 
model (GSM) with the novel use of an L2,1 
norm. Instead of assuming every feature is 
uncorrelated to other features, the GSM groups 
some of the features together and utilizes their 
correlated information to mediate the noise of the 
data. For an arbitrary matrix        , its 
          is defined as  
         ? ??    
  
   
 
     
Suppose that we have n training data indicated 
by            and sampled from c classes. In 
our setting, c = 2, friends or non-friends.     
{   }          is the corresponding label. 
The total scatter matrix    and between class 
scatter matrix    are defined as follows.  
         ?             
           
         ?               
              
where ? is the mean of all samples,    is the 
mean of samples in the i-th class.    is the 
number of samples in the i-th class,   
            . 
           
              
G is the scaled label matrix. A well-known 
method to utilize discriminate information is to 
find a low dimensional subspace in which     is 
maximized while    is minimized (Fukunaga et 
al., 1990). So the object function could be easily 
written as follows  
    ( 
 (    
  ) )            
           
The optimization of the above object function 
was introduced in Yang et al (2012). It is an 
adaptation of iterative singular value 
decomposition. In GSM, a block-wise constraint 
is imposed on the diagonal matrix (D) which is 
the intermediate result of the iterative single 
value decomposition. 
      (
 
       
     
 
       
  ) 
W in the equation is the weight function,    is 
the ith feature group in W, and there are a total 
number of G sub diagonal matrices 
corresponding to G groups of features. 
     For acoustic features, Steidl et al, (2012) 
designed a grouping schema which consists of 
Prosodic Features, Voice Quality Features and 
Spectral features which we adopted. For visual 
features, based on our observation of the highly 
unstable performance of the 38 feature points of 
the face, we introduced group bondage for the 
entire group to prevent single face features over-
fitting the classifier. Detailed group information 
is shown in Table 1 and Table 2. 
5 Human Baseline 
 
Figure 2: Boxplot of human rating accuracy with 
respect to gender. 
In order to establish a baseline of the difficulty 
of predicting friendship, we conducted an 
experiment with humans, rating whether two 
people in a video were friends or not, after 
watching a 30-second video/audio clip taken 
from the first session of tutoring (in which the 
behaviors of strangers are most likely to be 
distinct from friends). We recruited 14 people 
and screened out participants with prior 
theoretical knowledge of nonverbal behavior, 
gesture, friendship, and rapport, or who rated all 
12 clips in under 8 minutes, leaving 10 
participants, half male, with an average age of 23 
(SD 4.8). Each participant was asked to watch 
one 30-second clip per dyad, taken from 3 
minutes after tutoring began. The mean accuracy 
of their friendship prediction was 0.717 (SD 
0.119), which is significantly lower than our best 
GSM model (trained on all three sessions) 
applied to those same 12 clips, with a 
55
performance of 0.837 (t(11) = -2.1381 p.<.05). 
When we split the ratings by gender, we found 
females on average were more accurate than 
males (see Figure 2). According to Hall et al, 
(1979) females are generally better decoders of 
nonverbal behaviors, which may lead to better 
judgment of friendship. 
6 Results: Models  
 
 Human NLM SLM GSM 
LV  0.743 0.768 0.792* 
LA  0.674 0.664 0.682* 
LV+DF  0.752 0.769 0.801* 
LA+DF  0.679 0.681 0.683 
LV+LA  0.744 0.780 0.803* 
LV+LA+DF  0.717 0.749 0.782 0.814# 
 
Table 3: The classification accuracy of the three 
algorithms on different features sets. Feature sets 
were combined with early fusion (+). Values marked 
* are significantly better (p<.05, pairwise t-test) than 
other results in the same row. Values marked # are 
significantly better (p<.001, pairwise t-test) than other 
results in the same column. 
Our group sparse model (GSM) along with the 
non-sparse linear model (NLM) and sparse linear 
model (SLM) were evaluated on different 
combinations of three sets of features: low-level 
vision features (LV), low-level audio features 
(LA) and dyadic features (DF), and their 
performance is presented in Table 3. We did not 
evaluate dyadic features (DF) alone due to their 
sparse nature. 
     In particular, we found that adding the 
automatically extracted DF to LV and LA with 
early fusion improved the performance (t(2258)= 
-3.12,p<.001) of the GSM model. When using 
fewer modalities, our newly proposed GSM 
outperformed NLM and SLM (t(2258)=-1.65, 
p<0.05). However, when the number of feature 
sets increased, there was no statistical difference 
in performance between GSM and the other two 
models. We suspect that when features are 
abundant, the information that the features 
provide reaches a ceiling. The advantage of the 
GSM was gained by mediating the noise and 
sparseness of the data, which resulted in better 
weight assignment for each feature. Alternatively, 
when features are abundant, even NLM can have 
a comparative weight assignment by performing 
a greedy high dimensional feature space search. 
Thus there is limited room for further 
improvement by better weight assignment among 
the group features which GSM assumes. 
    When we looked at the top features selected 
by NLM using the vision modality alone, two 
(out of 38) face features, which had an unstable 
nature, appeared high in rank, which suggests the 
possibility of NLM over-fitting the noise of these 
features. Surprisingly, when more modalities are 
added, NLM stops picking single face features as 
top informative features. In GSM, none of the 38 
face features are listed in the top ranked features 
for any of the modalities, which demonstrates its 
ability in noise mediation. 
In real world applications, data sets which 
produce ideal, abundant, and accurate features 
are rarely encountered. We often end up with 
data that are poor in video quality, e.g. with no 
split channels for each participant or no frontal 
face view. Our newly proposed GSM may 
therefore be more robust when features are noisy 
or certain modalities are not available.  
7 Results: Contributions of Features 
Feature Name Weight 
Number of Conversational Turns & 
Average Length of Turns 
0.041 
Gaze Down -0.036 
Mutual Gaze 0.014 
F0 0.013 
Non-mutual Gaze -0.013 
Voicing 0.014 
MFCC -0.007 
Non-mutual Smile 0.004 
Non-mutual Lean Forward 0.004 
Mutual Gaze followed by Mutual 
Smile 
0.001 
 
Table 4:  The top 10 informative features and their 
weights as trained by GSM. Positive weight is 
associated with friends while negative weight is 
associated with non-friends. 
After building the model and ranking the 
features, we looked into the weights learned for 
each feature. This weight comprises not only the 
magnitude, which tells us if the feature is 
important, but also the polarity. A detailed list of 
the most informative features and their weights is 
shown in Table 4.  
The strongest feature is number and length of 
conversational turns which is grouped in the 
table and should be interpreted as meaning that 
friends have more and shorter conversational 
turns. This is consistent with previous research 
on direction giving (Cassell et al, 2007), and 
mirrors the fact that friends are more likely to 
interrupt one another. 
We expected that unfamiliar participants, 
seated about two feet across from one another, 
would maintain a low level of eye contact 
(Argyle & Dean, 1965). In fact we found that 
non-friends tend to gaze down more often. In 
this context, non-friends spend more time 
56
looking down at their study materials. In turn, 
mutual gaze is higher among friends. 
Among the audio features, F0, which captures 
pitch related information such as range and 
mean, has been shown to differ between 
conversational and non-conversational speech 
(Bolinger, 1986). Here, friends show that more 
conversational style in their speech, despite the 
tutoring nature of the interaction.  
In order to further examine the lessons to be 
learned from this GSM model about verbal and 
nonverbal behavior in friends and strangers, we 
also ran a repeated measures ANOVA, including 
both gender and friendship status as factors. 
There were no significant effects for gender, 
however, and so that factor was collapsed for 
further analysis. The four features described 
above were all significantly different between 
friends and strangers (although gaze down was 
simply a trend, at p<.08). 
The following features were also important to 
the model, but did not show significance in the 
ANOVA, perhaps because of their sparse nature 
in our data. MFCC (Mel-Frequency Cepstral 
Coefficients) was associated with strangers and 
the similar audio feature of voicing was 
associated with friends. Both of these features 
have been described as approximating speech 
style ? voicing, for example, may indicate more 
backchannels, such as ?uh huh? and ?hmm? 
(Ward, 2006). 
     In Nakano et al (2003), listener gaze at the 
speaker is interpreted as evidence of non-
understanding. We found similar results whereby 
non-friends were more likely to engage in non-
mutual gaze ? looking at one another when the 
other person was not looking back.  Mutual smile 
did not distinguish between friends and non-
friends, while non-mutual smile, on the other 
hand, provided indicative strength, in spite of its 
sparse nature, for friendship. This may relate to 
our prior work (Ogan et al, 2012) which found 
significant teasing and other behavior whereby 
friends appear comfortable enjoying themselves 
at the expense of the other.  
    Mutual lean forward lacked predictive power 
in our model, while non-mutual lean forward 
was more salient between friends. We often 
found, for example, that friends maintained very 
different postures, with a tutor leaning back 
much of the time, leaning forward only to answer 
a direct question from the tutee. Non-friends, on 
the other hand, tended to remain fixed on the 
study material. This may have been a display of 
formality, where a casual attitude would have 
been perceived as either impolite or 
inappropriate. In either relationship state, the 
tutee tended to sit hunched over the worksheet, 
and since we did not enter tutor state into the 
model, this may have washed out some tutor-
specific results.  
     For the time contingent feature, mutual gaze 
followed by mutual smile is informative and 
predictive of friends. 
8 Error Analysis and Discussion 
Dyad  
ID 
LA+DF LV+DF LA+LV+DF 
     1 0.732 0.809 0.819 
     2* 0.703 0.793 0.804 
     3* 0.574 0.771 0.778 
     4* 0.713 0.708 0.762 
     5 0.653 0.879 0.880 
     6 0.728 0.827 0.835 
     7 0.624 0.873 0.882 
     8* 0.712 0.861 0.852 
     9* 0.698 0.820 0.830 
    10 0.606 0.834 0.854 
    11* 0.700 0.682 0.743 
    12 0.749 0.780 0.785 
 
Table 5: The average accuracy of classification in 
each dyad using the group sparse model (GSM) with 
different combination of feature sets. Dyads marked 
with * are friends 
We performed an error analysis to understand the 
contexts under which our model failed to 
accurately predict friendship states, and here we 
discuss the implications of these examples for 
our work. Table 5 shows the average accuracy of 
each dyad using audio, visual, and dyadic 
features to predict friendship. Dyads 2, 3, 4, 8, 9 
and 11 are friend dyads, and the rest are 
strangers.  
Dyad 3 (friends) showed very low accuracy in 
audio and dyadic features alone, which might be 
explained by the fact that in one early session for 
this dyad, most of the 30-second clips contain 
very sparse numbers of low-level audio features 
(LA). An examination of the audio recording 
reveals that one of the participants was more 
aggressive than in the other sessions. The student 
told his friend, ?Just be quiet?I am trying to 
work,? and ?Shh, you don?t understand, so I 
basically have to teach you how to work that, but 
I'm trying to work.? At this point in the 
interaction, his partner stopped participating in 
the task and said virtually nothing for the rest of 
the session. This lack of speech led to a lower 
number of turns ? a pattern with a closer 
resemblance to strangers than friends. 
It seems that such rude behavior would be 
more likely between friends than strangers, 
meaning that ultimately our model will need to 
57
be sensitive to this kind of variance. With more 
pairs of friends, different styles of friendship can 
be further distinguished. However, this specific 
phenomenon signals that in the future, lexical 
information which could be obtained by 
automatic speech recognition could further 
improve performance. 
Dyad 11 also showed low relative accuracy in 
predication, particularly when the model used 
vision features. We found that one of the 
participants often tilted her head, which partially 
blocked the frontal camera view of the other 
participant, thus resulting in less confidence in 
automatically extracted visual features. In the 
future we will set our cameras in a better position 
in order to reach higher feature extraction 
accuracy.  
When we combined all our features, the 
prediction accuracy of Dyad 3 and 11 improved, 
further demonstrating that multimodal 
information improves friendship modeling. 
9 Conclusion and Future Work 
As a first step towards predicting the state of 
friendship between two interlocutors, we 
analyzed a set of automatically harvested low-
level and dyadic features from dialogues in a 
peer-tutoring task. Both low level features and 
dyadic features were shown to be useful in 
discriminating between those who are friends 
and those who are not.  
     To perform the analysis, we introduced a new 
computational group sparse model (GSM) in 
order to accommodate the sparse and noisy 
properties of multi-channel features. GSM 
outperformed a baseline of human raters who 
make these types of social judgments in 
everyday interactions. GSM also statistically 
outperformed a non-sparse linear model (NLM) 
and a sparse linear model (SLM) when the 
analysis used only a single set of low level 
features or single set of low level features 
combined with dyadic features. When all 
features were used, the distinctions between 
models decreased, since in a huge multimodal 
feature space, even a na?ve model could greedy 
search for a good weight assignment. Thus our 
newly proposed model did not significantly 
outperform the others in this scenario. And in 
general, more features produced more accurate 
prediction. 
    Based on the outcomes of the GSM model, we 
investigated differences between verbal and 
nonverbal behavior cues as a function of 
different friendship states. While much research 
on rapport detection and building in ECAs has 
focused on low level features, we found that 
dyadic features provided some of the most 
distinguishing differences between friends and 
non-friends. For example, mutual gaze and non-
mutual gaze were both indicative, as friends are 
comfortable looking directly at one another while 
non-friends may have used direct gaze only to 
signal non-understanding. This comfort between 
friends was also notable in other salient dyadic 
features; i.e., while non-friends often work in 
concert looking down at the task, friends were 
relaxed such that one partner could lean back, 
interrupt to take more conversational turns, and 
smile at the other without needing to reciprocate 
the smile each time. 
In future work we will look at temporal 
contingency more closely, examining whether 
participants? actions are contingent on the 
behavior of their partner. We will also examine 
whether the behavior of friends and strangers 
changes over multiple sessions. In this context, 
we include one suggestive graph, which shows 
that strangers increase their mutual gaze over 
sessions but friends decrease it. We are currently 
collecting further sessions for each dyad so as to 
be able to further analyze the nature of these 
relationships over time. 
 
Figure 3: Weight of the mutual gaze in each 
session, by friendship status 
 
To date we have found that the inclusion of 
automatically extracted dyadic features can lead 
to better prediction of friendship state. Both 
verbal and nonverbal behaviors were discovered 
that distinguish between different friendship 
status and that suggest how to design embodied 
dialogue systems that intend to spend a lifetime 
on the job. 
Acknowledgements 
Thanks to Angela Ng, Rachel Marino and Marissa 
Cross for data collection, Giota Stratou for visual 
feature extraction, Yi Yang, Louis-Philippe Morency, 
Shoou-I Yu, William Wang, and Eric Xing for 
valuable discussions, and the NSF IIS for generous 
funding. 
References  
Ambady, N., Krabbenhoft, M. A. & Hogan, D. 
(2006). The 30-sec sale: Using thin-slice 
0
2
4
6
1 2 3
Session Number 
Mutual gaze per clip 
strangers
friends
58
judgments to evaluate sales effectiveness. 
Journal of Consumer Psychology, 16(1), 4?13. 
Argyle, M. & Dean, J. (1965). Eye-contact, distance 
and affiliation. Sociometry, 28(3), 289?304. 
Azmitia, M. & Montgomery, R. (1993). Friendship, 
transactive dialogues, and the development of 
scientific reasoning. Social Development, 2(3), 
202?221.  
Baker, R. E., Gill, A. J., & Cassell, J. (2008). Reactive 
redundancy and listener comprehension in 
direction-giving. In Proceedings of the 9th 
SIGdial Workshop on Discourse and Dialogue 
(pp. 37?45). 
Brooks, M. (1989). Instant rapport (p. 205). New 
York: Warner Books. 
Berg, B. L. (1989). Qualitative research methods for 
the social sciences. Boston: Allyn and Bacon. 
Bernieri, F. J. (1988). Coordinated movement and 
rapport in teacher-student interactions. Journal 
of Nonverbal Behavior, 12(2), 120?138. 
Bickmore, T. W. & Picard, R. W. (2005). Establishing 
and maintaining long-term human-computer 
relationships. ACM Transactions on Computer-
Human Interaction, 12(2), 293?327. 
Bickmore, T. W., Pfeifer, L., & Schuman, D. (2011). 
Relational agents improve engagement and 
learning in science museum visitors. In 
Intelligent Virtual Agents (pp. 55?67). 
Reykjavik. 
Bolinger, D. (1986). Intonation and its parts: Melody 
in spoken English. Stanford, CA: Stanford 
University Press. 
Burgoon, J. K. & Hale, J. L. (1988). Nonverbal 
expectancy violations: Model elaboration and 
application to immediacy behaviors. 
Communications Monographs, (May 2013), 37?
41. 
Cappella, J. N.  & Pelachaud, C. (2002). Rules for 
responsive robots: Using human interactions to 
build virtual interactions. In Reis, Firzpatrick, & 
Vangelisti (Eds.), Stability and change in 
relationships. New York, NY: Cambridge 
University Press. 
Cassell, J. (2000). Nudge nudge wink wink: Elements 
of face-to-face conversation for embodied 
conversational agents. In J. Cassell, J. Sullivan, 
S. Prevost, & E. Churchill (Eds.), Embodied 
conversational agents (pp. 1?27). MIT Press. 
Cassell, J., Gill, A. J., & Tepper, P. A. (2007). 
Coordination in conversation and rapport. 
Proceedings of the ACL Workshop on Embodied 
Natural Language, 40 ?50. 
Cassell, J., Bickmore, T. W., Campbell, L., 
Vilhj?lmsson, H. H., & Yan, H. (2001). More 
than just a pretty face: Conversational protocols 
and the affordances of embodiment. Knowledge-
Based Systems, 14(1-2), 55?64. 
Cassell, J. & Bickmore, T. W. (2003). Negotiated 
collusion: Modeling social language and its 
relationship effects in intelligent agents. User 
Modeling and User-Adapted Interaction, 13(1), 
89?132. 
Chen, X., Lin, Q., Kim, S., Carbonell, J. G., & Xing, 
E. P. (2012). Smoothing proximal gradient 
method for general structured sparse regression. 
The Annals of Applied Statistics, 6(2), 719?752. 
Cortes, C. & Vapnik, V. (1995). Support-vector 
networks. Machine Learning, 20(3), 273?297. 
Fan, R., Chang, K., Hsieh, C., Wang, X., & Lin, C. 
(2008). LIBLINEAR: A library for large linear 
classification. The Journal of Machine Learning 
Research, 9, 1871?1874. 
Fantuzzo, J., Riggio, R., Connelly, S., & Dimeff, L. 
(1989). Effects of reciprocal peer tutoring on 
academic achievement and psychological 
adjustment: A component analysis. Journal of 
Educational Psychology, 81(2), 173-177. 
Fukunaga, K. (1990). Introduction to Statistical 
Pattern Recognition, Second Edition (2nd ed., p. 
592). San Diego, CA: Academic Press. 
Gatica-Perez, D. (2009). Automatic nonverbal 
analysis of social interaction in small groups: A 
review. Image and Vision Computing, 27(12), 
1775?1787. 
Granovetter, M. S. (1973). The strength of weak ties. 
American Journal of Sociology, 78(6), 1360?
1380.  
Gratch, J., Okhmatovskaia, A., Lamothe, F., Marsella, 
S.,  Morales, M., van der Werf, R. J., & 
Morency, L.-P. (2006). Virtual rapport. In 
Intelligent Virtual Agents (pp. 14?27). Springer 
Berlin/Heidelberg. 
Hall, J. A., DiMatteo, M. R., Rogers, P. L., & Archer, 
D. (1979). Sensitivity to nonverbal 
communication: The PONS test. Baltimore, MD: 
Johns Hopkins University Press. 
Harrigan, J. A., Oxman, T. E., & Rosenthal, R. 
(1985). Rapport expressed through nonverbal 
behavior. Journal of Nonverbal Behavior, 9, 
95?110. 
Harrigan, J. A. & Rosenthal, R. (1983). Physicians? 
head and body positions as determinants of 
perceived rapport. Applied Social Psychology, 
13(6), 496?509. 
Liu, J., Ji, S., & Ye, J. (2009). Multi-task feature 
learning via efficient l2, 1-norm minimization. 
In Proceedings of the Twenty-Fifth Conference 
on Uncertainty in Artificial Intelligence (pp. 
339?348). AUAI Press. 
Nakano, Y. I., Reinstein, G., Stocky, T., & Cassell, J. 
(2003). Towards a model of face-to-face 
grounding. In Proceedings of the 41st Annual 
Meeting on Association for Computational 
Linguistics. ACL?03 (Vol. 1, pp. 553?561). 
Sapporo: Association for Computational 
Linguistics. 
Nakano, Y. I. & Ishii, R. (2010). Estimating user?s 
engagement from eye-gaze behaviors in human-
agent conversations. In Proceedings of the 15th 
international conference on Intelligent user 
59
interfaces. IUI?10 (pp. 139?148). Hong Kong: 
ACM Press. 
Ogan, A., Finkelstein, S., Walker, E., Carlson, R., & 
Cassell, J. (2012). Rudeness and rapport: Insults 
and learning gains in peer tutoring. In 
Proceedings of the 11 International Conference 
on Intelligence Tutoring Systems (ITS 2012). 
Prepin, K., Ochs, M., & Pelachaud, C. (2012). Mutual 
stance building in dyad of virtual agents: Smile 
alignment and synchronisation. In Privacy, 
Security, Risk and Trust (PASSAT), 2012 
International Conference on Social Computing 
(SocialCom) (pp. 938?943). 
Schuller, B., Steidl, S., Batliner, A., N?th, E., 
Vinciarelli, A., Burkhardt, F., ? Weiss, B. 
(2012). The INTERSPEECH 2012 speaker trait 
challenge. In Proceedings of the 13th Annual 
Conference of the International Speech 
Communication Association (INTERSPEECH 
2012). Portland, OR: ISCA. 
Steidl, S., Polzehl, T., Bunnell, H. T., Dou, Y., 
Muthukumar, P. K., Perry, ? Metze, F. (2012). 
Emotion identification for evaluation of 
synthesized emotional speech. In Proceedings of 
the 6th International Conference on Speech 
Prosody 2012 (pp. 4?7). Shanghai: Tongji 
University Press. 
Stronks, B., Nijholt, A., van Der Vet, P., Heylen, D., 
& Machado, A. (2002). Designing for 
friendship: Becoming friends with your ECA. In 
A. Marriott, C. Pelachaud, T. Rist, Z. M. 
Ruttkay, & H. Vilhjalmsson (Eds.), Workshop 
on Embodied Conversational Agents - Let?s 
specify and evaluate them!, AMAAS 2002 (pp. 
91?96). Bologna: AMAAS. 
Tickle-Degnen, L. & Rosenthal, R. (1990). The nature 
of rapport and its nonverbal correlates. 
Psychological Inquiry, 1(4), 285?293. 
Vardoulakis, L. P., Ring, L., Barry, B., Sidner, C. L., 
& Bickmore, T. W. (2012). Designing relational 
agents as long term social companions for older 
adults. In Y. Nakano, M. Neff, A. Paiva, & M. 
Walker (Eds.), Intelligent Virtual Agents (pp. 
289?302). Santa Cruz, CA: Springer Berlin 
Heidelberg. 
Vinciarelli, A., Pantic, A., Bourlard, H. (2009) Social 
signal processing: Survey of an emerging 
domain. Image and Vision Computing, (27)12, 
1743-1759. 
Ward, N. (2006). Non-Lexical Conversational Sounds 
in American English. Pragmatics and 
Cognition, (14)1, 113-184. 
Wang, W. Y., Finkelstein, S., Ogan, A., Black, A. W., 
& Cassell, J. (2012). ?Love ya, jerkface?: Using 
sparse log-linear models to build positive (and 
impolite) relationships with teens. In 
Proceedings of the 13th Annual SIGDIAL 
Meeting on Discourse and Dialogue (pp. 20?
29). Seoul, South Korea. 
Yang, Y., Shen, H. T., Ma, Z., Huang, Z., & Zhou, X. 
(2010). l2,1-regularized discriminative feature 
selection for unsupervised learning. In 
Proceedings of the Twenty-Second International 
Joint Conference on Artificial Intelligence (pp. 
1589?1594). AAAI Press.  
Youniss, J. (1982). Parents and peers in social 
development: A Sullivan-Piaget perspective. 
University of Chicago Press. 
Yuan, M. & Lin, Y. (2007), Model selection and 
estimation in regression with grouped variables. 
Journal of the Royal Statistical Society, Series B 
68(1), 49-67. 
 
60
Proceedings of the SIGDIAL 2013 Conference, pages 404?413,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
The Dialog State Tracking Challenge
Jason Williams1, Antoine Raux2?, Deepak Ramachandran3?, and Alan Black4
1Microsoft Research, Redmond, WA, USA 2Lenovo Corporation, Santa Clara, CA, USA
3Nuance Communications, Mountain View, CA, USA 4Carnegie Mellon University, Pittsburgh, PA, USA
jason.williams@microsoft.com araux@lenovo.com deepak.ramachandran@nuance.com awb@cmu.edu
Abstract
In a spoken dialog system, dialog state
tracking deduces information about the
user?s goal as the dialog progresses, syn-
thesizing evidence such as dialog acts over
multiple turns with external data sources.
Recent approaches have been shown to
overcome ASR and SLU errors in some
applications. However, there are currently
no common testbeds or evaluation mea-
sures for this task, hampering progress.
The dialog state tracking challenge seeks
to address this by providing a heteroge-
neous corpus of 15K human-computer di-
alogs in a standard format, along with a
suite of 11 evaluation metrics. The chal-
lenge received a total of 27 entries from 9
research groups. The results show that the
suite of performance metrics cluster into 4
natural groups. Moreover, the dialog sys-
tems that benefit most from dialog state
tracking are those with less discriminative
speech recognition confidence scores. Fi-
nally, generalization is a key problem: in
2 of the 4 test sets, fewer than half of the
entries out-performed simple baselines.
1 Overview and motivation
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog
in a process called dialog state tracking (DST).
For example, in a bus schedule information sys-
tem, the dialog state might indicate the user?s de-
sired bus route, origin, and destination. Dialog
state tracking is difficult because automatic speech
?Most of the work for the challenge was performed when
the second and third authors were with Honda Research In-
stitute, Mountain View, CA, USA
recognition (ASR) and spoken language under-
standing (SLU) errors are common, and can cause
the system to misunderstand the user?s needs. At
the same time, state tracking is crucial because
the system relies on the estimated dialog state to
choose actions ? for example, which bus schedule
information to present to the user.
Most commercial systems use hand-crafted
heuristics for state tracking, selecting the SLU re-
sult with the highest confidence score, and dis-
carding alternatives. In contrast, statistical ap-
proaches compute scores for many hypotheses for
the dialog state (Figure 1). By exploiting correla-
tions between turns and information from external
data sources ? such as maps, bus timetables, or
models of past dialogs ? statistical approaches can
overcome some SLU errors.
Numerous techniques for dialog state tracking
have been proposed, including heuristic scores
(Higashinaka et al, 2003), Bayesian networks
(Paek and Horvitz, 2000; Williams and Young,
2007), kernel density estimators (Ma et al, 2012),
and discriminative models (Bohus and Rudnicky,
2006). Techniques have been fielded which scale
to realistically sized dialog problems and operate
in real time (Young et al, 2010; Thomson and
Young, 2010; Williams, 2010; Mehta et al, 2010).
In end-to-end dialog systems, dialog state tracking
has been shown to improve overall system perfor-
mance (Young et al, 2010; Thomson and Young,
2010).
Despite this progress, direct comparisons be-
tween methods have not been possible because
past studies use different domains and system
components, for speech recognition, spoken lan-
guage understanding, dialog control, etc. More-
over, there is little agreement on how to evaluate
dialog state tracking. Together these issues limit
progress in this research area.
The Dialog State Tracking Challenge (DSTC)
provides a first common testbed and evaluation
404
Figure 1: Overview of dialog state tracking. In this example, the dialog state contains the user?s desired
bus route. At each turn t, the system produces a spoken output. The user?s spoken response is processed
to extract a set of spoken language understanding (SLU) results, each with a local confidence score. A
set of Nt dialog state hypotheses is formed by considering all SLU results observed so far, including the
current turn and all previous turns. Here, N1 = 3 and N2 = 5. The dialog state tracker uses features of
the dialog context to produce a distribution over all Nt hypotheses and the meta-hypothesis that none of
them are correct.
suite for dialog state tracking. The DSTC orga-
nizers made available a public, heterogeneous cor-
pus of over 15K transcribed and labeled human-
computer dialogs. Nine teams entered the chal-
lenge, anonymously submitting a total of 27 dialog
state trackers.
This paper serves two roles. First, sections 2
and 3 provide an overview of the challenge, data,
and evaluation metrics, all of which will remain
publicly available to the community (DST, 2013).
Second, this paper summarizes the results of the
challenge, with an emphasis on gaining new in-
sights into the dialog state tracking problem, in
Section 4. Section 5 briefly concludes.
2 Challenge overview
2.1 Problem statement
First, we define the dialog state tracking problem.
A dialog state tracker takes as input all of the ob-
servable elements up to time t in a dialog, includ-
ing all of the results from the automatic speech
recognition (ASR) and spoken language under-
standing (SLU) components, and external knowl-
edge sources such as bus timetable databases and
models of past dialogs. It also takes as input a
set of Nt possible dialog state hypotheses, where
a hypothesis is an assignment of values to slots in
the system. The tracker outputs a probability dis-
tribution over the set of Nt hypotheses, and the
meta-hypothesis REST which indicates that none
of them are correct. The goal is to assign probabil-
ity 1.0 to the correct state, and 0.0 to other states.
Note that the set of dialog states is given. Also
note that Nt varies with t ? typically as the dia-
log progresses and more concepts are discussed,
the number of candidate hypotheses increases. An
example is given in Figure 1.
In this challenge, dialog states are generated in
the usual way, by enumerating all slots values that
have appeared in the SLU N-best lists or system
output up until the current turn. While this ap-
proach precludes a tracker assigning a score to an
405
SLU value that has not been observed, the cardi-
nality of the slots is generally large, so the likeli-
hood of a tracker correctly guessing a slot value
which hasn?t been observed anywhere in the input
or output is vanishingly small.
2.2 Challenge design
The dialog state tracking challenge studies this
problem as a corpus-based task ? i.e., dialog state
trackers are trained and tested on a static corpus
of dialogs, recorded from systems using a variety
of state tracking models and dialog managers. The
challenge task is to re-run state tracking on these
dialogs ? i.e., to take as input the runtime system
logs including the SLU results and system output,
and to output scores for dialog states formed from
the runtime SLU results. This corpus-based de-
sign was chosen because it allows different track-
ers to be evaluated on the same data, and because a
corpus-based task has a much lower barrier to en-
try for research groups than building an end-to-end
dialog system.
In practice of course, a state tracker will be used
in an end-to-end dialog system, and will drive ac-
tion selection, thereby affecting the distribution of
the dialog data the tracker experiences. In other
words, it is known in advance that the distribu-
tion in the training data and live data will be mis-
matched, although the nature and extent of the
mis-match are not known. Hence, unlike much
of supervised learning research, drawing train and
test data from the same distribution in offline ex-
periments may overstate performance. So in the
DSTC, train/test mis-match was explicitly created
by choosing test data to be from different dialog
systems.
2.3 Source data and challenge corpora
The DSTC uses data from the public deployment
of several systems in the Spoken Dialog Challenge
(SDC) (Black et al, 2010), provided by the Dialog
Research Center at Carnegie Mellon University. In
the SDC, telephone calls from real passengers of
the Port Authority of Allegheny County, who runs
city buses in Pittsburgh, were forwarded to dialog
systems built by different research groups. The
goal was to provide bus riders with bus timetable
information. For example, a caller might want
to find out the time of the next bus leaving from
Downtown to the airport.
The SDC received dialog systems from three
different research groups, here called Groups A,
B, and C. Each group used its own ASR, SLU,
and dialog manager. The dialog strategies across
groups varied considerably: for example, Groups
A and C used a mixed-initiative design, where the
system could recognize any concept at any turn,
but Group B used a directed design, where the
system asked for concepts sequentially and could
only recognize the concept being queried. Groups
trialled different system variants over a period of
almost 3 years. These variants differed in acoustic
and language models, confidence scoring model,
state tracking method and parameters, number of
supported bus routes, user population, and pres-
ence of minor bugs. Example dialogs from each
group are shown in the Appendix.
The dialog data was partitioned into 5 train-
ing corpora and 4 testing corpora (Table 1).
The partioning was intended to explore different
types of mis-match between the training and test
data. Specifically, the dialog system in TRAIN1A,
TRAIN1B, TRAIN1C, TRAIN2, and TEST1 are all
very similar, so TEST1 tests the case where there
is a large amount of similar data. TEST2 uses the
same ASR and SLU but a different dialog con-
troller, so tests the case where there is a large
amount of somewhat similar data. TEST3 is very
similar to TRAIN3 and tests the case where there
is a small amount of similar data. TEST4 uses a
completely different dialog system to any of the
training data.
2.4 Data preparation
The dialog system log data from all three groups
was converted to a common format, which
described SLU results and system output using
a uniform set of dialog acts. For example, the
system speech East Pittsburgh Bus Schedules.
Say a bus route, like 28X, or say I?m not sure.
was represented as hello(), request(route), exam-
ple(route=28x), example(route=dontknow). The
user ASR hypothesis the next 61c from oakland to
mckeesport transportation center was represented
as inform(time.rel=next), inform(route=61c),
inform(from.neighborhood=oakland), in-
form(to.desc=?mckeesport transportation
center?). In this domain there were a total
of 9 slots: the bus route, date, time, and three
components each for the origin and destination,
corresponding to streets, neighborhoods, and
points-of-interest like universities. For complete
details see (Williams et al, 2012).
406
TRAIN TEST
1A 1B 1C 2 3 1 2 3 4
Group A A A A B A A B C
Year(s) 2009 2009 2009 2010 2010 2011 2012 2011-2 2010
Dialogs 1013 1117 9502 643 688 715 750 1020 438
Turns/Dialog 14.7 13.3 14.5 14.5 12.6 14.1 14.5 13.0 10.9
Sys acts/turn 4.0 3.8 3.8 4.0 8.4 2.8 3.2 8.2 4.6
Av N-best len 21.7 22.3 21.9 22.4 2.9 21.2 20.5 5.0 3.2
Acts/N-best hyp 2.2 2.2 2.2 2.3 1.0 2.1 2.0 1.0 1.6
Slots/turn 44.0 46.5 45.6 49.0 2.1 41.4 36.9 4.3 3.5
Transcribed? yes yes yes yes yes yes yes yes yes
Labelled? yes no no yes yes yes yes yes yes
1-best WER 42.9% 41.1% 42.1% 58.2% 40.5% 57.9% 62.1% 48.1% 55.6%
1-best SLU Prec. 0.356 - - 0.303 0.560 0.252 0.275 0.470 0.334
1-best SLU Recall 0.522 - - 0.388 0.650 0.362 0.393 0.515 0.376
N-best SLU Recall 0.577 - - 0.485 0.738 0.456 0.492 0.634 0.413
Table 1: Summary of the datasets. One turn includes a system output and a user response. Slots are
named entity types such as bus route, origin neighborhood, date, time, etc. N-best SLU Recall indicates
the fraction of concepts which appear anywhere on the SLU N-best list.
Group B and C systems produced N-best lists
of ASR and SLU output, which were included in
the log files. Group A systems produced only 1-
best lists, so for Group A systems, recognition was
re-run with the Pocketsphinx speech recognizer
(Huggins-Daines et al, 2006) with N-best output
enabled, and the results were included in the log
files.
Some information in the raw system logs was
specific to a group. For example, Group B?s logs
included information about word confusion net-
works, but other groups did not. All of this infor-
mation was included in a ?system specific? sec-
tion of the log files. Group A logs contained about
40 system-specific name/value pairs per turn, and
Group B about 600 system-specific name/value
pairs per turn. Group C logs contained no system
specific data.
3 Labeling and evaluation design
The output of a dialog state tracker is a proba-
bility distribution over a set of given dialog state
hypotheses, plus the REST meta-hypothesis. To
evaluate this output, a label is needed for each di-
alog state hypothesis indicating its correctness.
In this task-oriented domain, we note that the
user enters the call with a specific goal in mind.
Further, when goal changes do occur, they are
usually explicitly marked: since all of the sys-
tems first collect slot values, and then provide bus
timetables, if the user wishes to change their goal,
they need to start over from the beginning. These
?start over? transitions are obvious in the logs.
This structure allows the correctness of each di-
alog state to be equated to the correctness of the
SLU items it contains. As a result, in the DSTC
we labeled the correctness of SLU hypotheses in
each turn, and then assumed these labels remain
valid until either the call ends, or until a ?start
over? event. Thus to produce the labels, the la-
beling task followed was to assign a correctness
value to every SLU hypothesis on the N-best list,
given a transcript of the words actually spoken in
the dialog up to the current turn.
To accomplish this, first all user speech was
transcribed. The TRAIN1 datasets had been tran-
scribed using crowd-sourcing in a prior project
(Parent and Eskenazi, 2010); the remainder were
transcribed by professionals. Then each SLU hy-
pothesis was labled as correct or incorrect. When a
transcription exactly and unambiguously matched
a recognized slot value, such as the bus route
?sixty one c?, labels were assigned automati-
cally. The remainder were assigned using crowd-
sourcing, where three workers were shown the true
words spoken and the recognized concept, and
asked to indicate if the recognized concept was
correct ? even if it did not match the recognized
words exactly. Workers were also shown dialog
407
history, which helps decipher the user?s meaning
when their speech was ambiguous. If the 3 work-
ers were not unanimous in their labels (about 4%
of all turns), the item was labeled manually by the
organizers. The REST meta-hypothesis was not
explicitly labeled; rather, it was deemed to be cor-
rect if none of the prior SLU results were labeled
as correct.
In this challenge, state tracking performance
was measured on each of the 9 slots separately,
and also on a joint dialog state consisting of all the
slots. So at each turn in the dialog, a tracker output
10 scored lists: one for each slot, plus a 10th list
where each dialog state contains values from all
slots. Scores were constrained to be in the range
[0, 1] and to sum to 1.
To evaluate tracker output, at each turn, each hy-
pothesis (including REST) on each of the 10 lists
was labeled as correct or incorrect by looking up
its corresponding SLU label(s). The scores and la-
bels over all of the dialogs were then compiled to
compute 11 metrics. Accuracy measures the per-
cent of turns where the top-ranked hypothesis is
correct. This indicates the correctness of the item
with the maximum score. L2 measures the L2 dis-
tance between the vector of scores, and a vector of
zeros with 1 in the position of the correct hypoth-
esis. This indicates the quality of all scores, when
the scores as viewed as probabilities.
AvgP measures the mean score of the first cor-
rect hypothesis. This indicates the quality of the
score assigned to the correct hypothesis, ignoring
the distribution of scores to incorrect hypotheses.
MRR measures the mean reciprocal rank of the
first correct hypothesis. This indicates the quality
of the ordering the scores produces (without nec-
essarily treating the scores as probabilities).
The remaining measures relate to receiver-
operating characteristic (ROC) curves, which
measure the discrimination of the score for the
highest-ranked state hypothesis. Two versions
of ROC are computed ? V1 and V2. V1 com-
putes correct-accepts (CA), false-accepts (FA),
and false-rejects (FR) as fractions of all utter-
ances, so for example
CA.V 1(s) = #CA(s)N (1)
where #CA(s) indicates the number of correctly
accepted states when only those states with score
? s are accepted, and N is the total number
of states in the sample. The V1 metrics are a
20%
30%
40%
50%
60%
70%
80%
90%
100%
sche
dule
2 ac
cura
cy fo
r all
 slot
s
Trackers Oracle Baseline0 Baseline1
train293% test175% test289% test348%train382% test438%
Figure 2: Schedule2 accuracy averaged over slots
for every tracker on every dataset. Percentages un-
der the datasets indicate the percent of the track-
ers which exceeded the performance of both base-
lines.
useful indication of overall performance because
they combine discrimination and overall accuracy
? i.e., the maximum CA.V 1(s) value is equal to
accuracy computed above.
V2 considers fractions of correctly classified ut-
terances, so for example
CA.V 2(s) = #CA(s)#CA(0) . (2)
The V2 metrics are useful because they measure
the discrimination of the scoring independently of
accuracy ? i.e., the maximum value of CA.V 2(s)
is always 1, regardless of accuracy.
From these ROC statistics, several met-
rics are computed. ROC.V1.EER computes
FA.V 1(s) where FA.V 1(s) = FR.V 1(s).
The metrics ROC.V1.CA05, ROC.V1.CA10,
and ROC.V1.CA20 compute CA.V 1(s) when
FA.V 1(s) = 0.05, 0.10, and 0.20 respec-
tively. ROC.V2.CA05, ROC.V2.CA10, and
ROC.V2.CA20 do the same using the V2 ver-
sions.
Apart from what to measure, there is currently
no standard that specifies when to measure ? i.e.,
which turns to include when computing each met-
ric. So for this challenge, a set of 3 schedules were
used. schedule1 includes every turn. schedule2
include turns where the target slot is either present
on the SLU N-best list, or where the target slot
is included in a system confirmation action ? i.e.,
where there is some observable new information
408
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0% 20% 40% 60% 80% 100%
True 
posit
ive ra
te
 
False positive rate  
test4test3test2test1
Figure 3: Receiver operating characteristc (ROC)
curve for SLU confidence scores of the 1-best hy-
pothesis in the test datasets. The SLU confidence
score in TEST3 is most discriminative; TEST1 and
TEST2 are the least discriminative.
about the target slot. schedule3 includes only the
last turn of a dialog.
In sum, for each tracker, one measurement is re-
ported for each test set (4), schedule (3), and met-
ric (11) for each of the 9 slots, the ?joint? slot, and
a weighted average of the individual slots (11), for
a total of 4 ? 3 ? 11 ? 11 = 1452 measurements per
tracker. In addition, each tracker reported average
latency per turn ? this ranged from 10ms to 1s.
3.1 Baseline trackers
For comparisons, two simple baselines were im-
plemented. The first (Baseline0) is a majority
class baseline that always guesses REST with
score 1. The second (Baseline1) follows simple
rules which are commonly used in spoken dialog
systems. It maintains a single hypothesis for each
slot. Its value is the SLU 1-best with the highest
confidence score observed so far, with score equal
to that SLU item?s confidence score.
4 Results and discussion
Logistically, the training data and labels, bus
timetable database, scoring scripts, and baseline
system were publicly released in late December
2012. The test data (without labels) was released
on 22 March 2013, and teams were given a week to
run their trackers and send results back to the orga-
nizers for evaluation. After the evaluation, the test
labels were published. Each team could enter up
to 5 trackers. For the evaluation, teams were asked
to process the test dialogs online ? i.e., to make a
1
3
5
7
9
11
13
15
17
19
accuracy l2 roc.v1_eer roc.v2_ca05
Average
 rank
 in tes
t datase
ts
Metric - schedule2 - weighted average over all slots
T3.E2T5.E1T5.E2T5.E5T6.E2T6.E3T6.E4T6.E5T9.E1
Figure 4: Average rank of top-performing trackers
for the four metrics identified in Figure 6. Rank-
ing was done using the given metric, schedule2,
and the weighted average of all slots. Tn.Em in-
dicates team n, entry m.
single pass over the data, as if the tracker were be-
ing run in deployment. Participation was open to
researchers at any institution, including the orga-
nizers and advisory board. To encourage partici-
pation, the organizers agreed not to identify par-
ticipants in publications, and there was no require-
ment to disclose how trackers were implemented.
9 teams entered the DSTC, submitting a total of
27 trackers. The raw output and all 1452 measure-
ments for each tracker (and the 2 baselines) are
available from the DSTC homepage (DST, 2013).
4.1 Analysis of trackers and datasets
We begin by looking at one illustrative metric,
schedule2 accuracy averaged over slots, which
measures the accuracy of the top dialog hypothe-
sis for every slot when it either appears on the SLU
N-best list or is confirmed by the system.1 Results
in Figure 2 show two key trends. First, relative
to the baselines, performance on the test data is
markedly lower than the training data. Comparing
TRAIN2 to TEST1/TEST2 and TRAIN3 to TEST3,
the relative gain over the baselines is much lower
on test data. Moreover, only 38% of trackers per-
formed better than a simple majority-class base-
line on TEST4, for which there was no matched
training data. These findings suggests that gen-
eralization is an important open issues for dialog
state trackers.
Second, Figure 2 indicates that the gains made
1Results using the joint dialog state are broadly similar,
and are omitted for space.
409
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
Trackers Baseline0 Baseline1
(a) TEST1
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(b) TEST2
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(c) TEST3
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(d) TEST4
Figure 5: Percent of highest-scored dialog state hypotheses which did not appear in the top-ranked SLU
position vs. schedule2 accuracy over all slots. Trackers ? including those with the highest accuracy ?
for TEST1 and TEST2 rarely assigned the highest score to an SLU hypothesis other than the top. All
trackers for TEST3 and TEST4 assigned the highest score to an SLU hypothesis other than the top in a
non-trivial percent of turns.
by the trackers over the baselines are larger
for Group A systems (TEST1 and TEST2) than
for Group B (TEST3) and C (TEST4) systems.
Whereas the baselines consider only the top SLU
hypothesis, statistical trackers can make use of
the entire N-best list, increasing recall ? compare
the 1-best and N-best SLU recall rates in Table 1.
However, Group A trackers almost never assigned
the highest score to an item below the top position
in the SLU N-best list. Rather, the larger gains for
Group A systems seem due to the relatively poor
discrimination of Group A?s SLU confidence score
(Figure 3): whereas the trackers use a multitude
of features to assign scores, the baselines rely en-
tirely on the SLU confidence for their scores, so
undiscriminative SLU confidence measures ham-
per baseline performance.
4.2 Analysis of metrics
This challenge makes it possible to study the em-
pirical differences among the evaluation metrics.
Intuitively, if the purpose of a metric is to order
a set of trackers from best to worst, then 2 met-
rics are similar if they yield a similar ordering over
trackers. Specifically, for every metricm, we have
a value x(m, d, s, t) where d is the dataset, and
s is the evaluation schedule, and t is the tracker.
We define r(m, d, s, t) as the rank of tracker t
when ordered using metric m, dataset d and eval-
uation schedule s. Using these ranks, we compute
Kendall?s Tau for every d, s, and pair of metrics
m1 and m2 (Kendall, 1938). We then compute the
average Kendall?s Tau for m1 and m2 by averag-
ing over all d and s.2
Results are in Figure 6. Here we see 4 natu-
ral clusters emerge: a cluster for correctness with
Accuracy, MRR, and the ROC.V1.CA measures; a
cluster for probability quality with L2 and Aver-
age score; and two clusters for score discrimina-
tion ? one with ROC.V1.EER and the other with
the three ROC.V2 metrics. This finding suggest
2A similar analysis over schedules showed that the differ-
ences in ranking for different schedules were smaller than for
metrics.
410
accuracy
mrr
roc_v1.ca05
roc_v1.ca10
roc_v1.ca20
roc.v1_eer
avgp
l2
roc.v2_ca05
roc.v2_ca05
roc.v2_ca05
Figure 6: Average divergence between rank orderings produced by different metrics. The size of a circle
at (x, y) is given by 1?? , where ? is the average Kendall?s Tau computed on the rank orderings produced
by methods x and y. Larger circles indicate dissimilar rankings; smaller circles indicate similar rankings;
missing circles indicate identical rankings. The red boxes indicate groups of metrics that yield similar
rankings.
that measuring one metric from each cluster will
contain nearly the same information as all 9 met-
rics. For example, one might report only Accu-
racy, L2, ROC.V1.EER, and ROC.V2.CA5.
Using these 4 metrics, we rank-ordered each
tracker, using schedule2 and a weighted average
of all slots. We then computed the average rank
across the 4 test sets. Finally we selected the set
of trackers with the top three average ranks for
each metric. Results in Figure 4 emphasize that
different trackers are tuned for different perfor-
mance measures, and the optimal tracking algo-
rithm depends crucially on the target performance
measure.
5 Conclusion
The dialog state tracking challenge has provided
the first common testbed for this task. The data,
evaluation tools, and baselines will continue to be
freely available to the research community (DST,
2013). The details of the trackers themselves will
be published at SIGDIAL 2013.
The results of the challenge show that the
suite of performance metrics cluster into 4 natural
groups. We also find that larger gains over conven-
tional rule-based baselines are present in dialog
systems where the speech recognition confidence
score has poor discrimination. Finally, we observe
substantial limitations on generalization: in mis-
matched conditions, around half of the trackers en-
tered did not exceed the performance of two sim-
ple baselines.
In future work, it should be verified that im-
provements in dialog state tracking lead to im-
provements in end-to-end dialog performance
(e.g., task completion, user satisfaction, etc.). In
addition, it would be interesting to study dialogs
where goal changes are more common.
Acknowledgements
The organizers thank the advisory board for their
valuable input on the design of the challenge:
Daniel Boies, Paul Crook, Maxine Eskenazi, Mil-
ica Gasic, Dilek Hakkani-Tur, Helen Hastie, Kee-
Eung Kim, Ian Lane, Sungjin Lee, Teruhisa Misu,
Olivier Pietquin, Joelle Pineau, Blaise Thomson,
David Traum, and Luke Zettlemoyer. The orga-
nizers also thank Ian Lane for his support for tran-
scription, and Microsoft and Honda Research In-
stitute USA for funding the challenge. Finally,
we thank the participants for making the challenge
successful.
411
References
AW Black, S Burger, B Langner, G Parent, and M Es-
kenazi. 2010. Spoken dialog challenge 2010. In
Proc SLT, Berkeley.
D Bohus and AI Rudnicky. 2006. A ?K hypotheses +
other? belief updating model. In Proc AAAI Work-
shop on Statistical and Empirical Approaches for
Spoken Dialogue Systems, Boston.
2013. Dialog State Tracking Challenge Home-
page. http://research.microsoft.com/
events/dstc/.
H Higashinaka, M Nakano, and K Aikawa. 2003.
Corpus-based discourse understanding in spoken di-
alogue systems. In Proc ACL, Sapporo.
D Huggins-Daines, M Kumar, A Chan, A W Black,
M Ravishankar, and A I Rudnicky. 2006. Pock-
etSphinx: A Free, Real-Time Continuous Speech
Recognition System for Hand-Held Devices. In
Proc ICASSP, Toulouse.
M Kendall. 1938. A new measure of rank correlation.
Biometrika, 30(1-2):81?89.
Y Ma, A Raux, D Ramachandran, and R Gupta. 2012.
Landmark-based location belief tracking in a spoken
dialog system. In Proc SigDial, Seoul.
N Mehta, R Gupta, A Raux, D Ramachandran, and
S Krawczyk. 2010. Probabilistic ontology trees for
belief tracking in dialog systems. In Proc SigDial,
Tokyo.
T Paek and E Horvitz. 2000. Conversation as ac-
tion under uncertainty. In Proc UAI, Stanford, pages
455?464.
G Parent and M Eskenazi. 2010. Toward Bet-
ter Crowdsourced Transcription: Transcription of a
Year of the Let?s Go Bus Information System Data.
In Proc SLT, Berkeley.
B Thomson and SJ Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog sys-
tems. Computer Speech and Language, 21(2):393?
422.
JD Williams, A Raux, D Ramachandran, and
AW Black. 2012. Dialog state tracking challenge
handbook. Technical report, Microsoft Research.
JD Williams. 2010. Incremental partition recombina-
tion for efficient tracking of multiple dialogue states.
In Proc. of ICASSP.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The hid-
den information state model: a practical framework
for POMDP-based spoken dialogue management.
Computer Speech and Language, 24(2):150?174.
Appendix 1: Sample dialog excerpts
Below are transcripts of calls in the DSTC corpus.
The user?s side shows utterances as transcribed by
human annotators.
Group A
SYS: Welcome to the CMU Let?s Go bus informa-
tion system. To get help at any time, just say Help
or press zero. What can I do for you?
USR: 61c evening hours
SYS: For example, you can say, when is the next
28X from DOWNTOWN to THE AIRPORT? or
I?d like to go from MCKEESPORT to HOME-
STEAD tomorrow at 10 a.m.
USR: i?d like to go [noise]
SYS: Where are you leaving from?
USR: from Kennywood tomorrow at 6:30 PM
SYS: Leaving from MCKEE. Is this correct?
USR: yes
SYS: Okay What is your destination?
USR: downtown
SYS: Going to DOWNTOWN. Is this correct?
USR: yes
SYS: Right When do you want to travel?
USR: 6:30 PM
SYS: Leaving at 6 30 p.m.. Is this correct?
USR: [noise] yes
SYS: Alright Hold on. I?ll look that up. There is
a 71D leaving FIFTH AVENUE AT DARRAGH
MCKEE at 6 40 p.m.. It will arrive at FIFTH AV-
ENUE AT MARKET at 6 58 p.m.. To get more
information about buses related to this trip, you
can say, when is the next bus, or, when is the pre-
vious bus. To ask about a different trip, you can
say, start a new query. If you are finished, you can
say goodbye.
USR: start a new query
SYS: Okay, let?s start from the beginning.
USR: ...
Group B
SYS: East Pittsburgh Bus Schedules. Say a bus
route, like 28X, or say I?m not sure.
USR: 61A
SYS: Okay, 61A. To change, say go back. Where
are you leaving from?
USR: Downtown
SYS: Okay, downtown. You can always say go
back. And where are you going to?
USR: East Pittsburgh East Pittsburgh
SYS: Pittsburgh. Is that right?
412
USR: No
SYS: Oh, sorry, where are you going to?
USR: East Pittsburgh
SYS: Okay, East Pittsburgh. Do you want times
for the next few buses? Say yes or no.
USR: Yes
SYS: Ok, the next few buses. To change, say go
back. At 7:45 PM today, there is a 61 A, from
Fifth Avenue And Market Street, Downtown, ar-
riving Dynamo Alley And Beech Avenue, In East
Pittsburgh, at 8:42 PM. Say repeat, next, previous,
or start over.
Group C
SYS: Welcome to the Lets Go! bus information
system for Pittsburgh. Where would you like to
leave from?
USR: [noise]
SYS: Where do you wanna leave from?
USR: [noise] um I?m leaving from Forbes avenue
and Murray avenue
SYS: I?m sorry but I do not know any stops in the
moon neighborhood. The routes I currently cover
are the following: 28X, 54C, 56U, 61A, 61B, 61C,
61D and 64.
USR: um I need the scheduling information for the
61D
SYS: Where would you like to leave from?
USR: Forbes and Murray
SYS: Where do you wanna go?
USR: Greenfield
SYS: When are you going to take that bus?
USR: 61D
SYS: When are you going to take that bus?
USR: 61D
SYS: When would you like to travel? Say next bus
for the the next bus .
USR: next bus
SYS: The next bus from forbes avenue at murray
to greenfield is the 61D, leaving at 8 43 p.m. . It
arrives at murray avenue at loretta at 8 52 p.m. .
You may now say. next bus, previous bus, restart,
or goodbye.
USR: [noise] good-bye
SYS: Thank you goodbye
Appendix 2: Example log fragment
Following is a log from a call in TRAIN3. Someelements have been omitted for space. Logs areformatted in JavaScript Object Notation (JSON).
{
"session-id": "dt-201007221854-4808D",
"turns": [ {
"input": {
"asr-hyps": [
{ "asr-hyp": "sixty one b",
"score": 0.834314,
},
...
],
"slu-hyps": [
{ "slu-hyp": [
{"act": "inform",
"slots": [["route", "61b"]]},
],
"score": 0.834314,
},
...
],
"audio-file": "002.raw",
"start-time": 7.504
},
"output": {
"start-time": 0.078,
"transcript": "East Pittsburgh Bus
Schedules. Say a bus route, like
28X, or say I?m not sure.",
"dialog-acts": [
{ "act": "hello",
"slots": [] },
{ "act": "request",
"slots": [["route",null]] },
{ "act": "example",
"slots": [["route","28x"]] },
{ "act": "example",
"slots": [["route","dontknow"]] }
],
}
"system-specific": {
"wcn": {
"best_path_score": 0.9965,
"best_path": {
"word": ["sixty","one","b"],
"prob": [ 1.0,1.0,0.9965 ],
}
"network": [ ... ]
},
"nbest": [
{
"normSpeechLhood": -152.654,
"lastSpeechFrame": 266,
"numFrames": 354,
"udelta": -3.0280,
"speechLikelihood": -15876.0,
},
...
],
...
},
},
...
]
}
413
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 426?436,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Crowdsourcing High-Quality Parallel Data Extraction from Twitter
?
Wang Ling
123
Lu?s Marujo
123
Chris Dyer
2
Alan Black
2
Isabel Trancoso
13
(1)L
2
F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior T?cnico, Lisbon, Portugal
{lingwang,lmarujo,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
High-quality parallel data is crucial for a
range of multilingual applications, from
tuning and evaluating machine translation
systems to cross-lingual annotation pro-
jection. Unfortunately, automatically ob-
tained parallel data (which is available
in relative abundance) tends to be quite
noisy. To obtain high-quality parallel data,
we introduce a crowdsourcing paradigm
in which workers with only basic bilin-
gual proficiency identify translations from
an automatically extracted corpus of par-
allel microblog messages. For less than
$350, we obtained over 5000 parallel seg-
ments in five language pairs. Evaluated
against expert annotations, the quality of
the crowdsourced corpus is significantly
better than existing automatic methods:
it obtains an performance comparable to
expert annotations when used in MERT
tuning of a microblog MT system; and
training a parallel sentence classifier with
it leads also to improved results. The
crowdsourced corpora will be made avail-
able in http://www.cs.cmu.edu/
~lingwang/microtopia/.
1 Introduction
High-quality parallel data is essential for tun-
ing and evaluating statistical MT systems, and
it plays a role in a wide range of multilingual
NLP applications, such as word sense disambigua-
tion (Gale et al., 1992; Ng et al., 2003; Specia
et al., 2005), paraphrasing (Bannard and Callison-
burch, 2005; Ganitkevitch et al., 2012), annota-
tion projection (Das and Petrov, 2011), and other
language-specific applications (Schwarck et al.,
?
A sample of the crowdsourced corpora and the inter-
faces used are available as supplementary material.
2010; Liu et al., 2011). While large amounts
of parallel data can be easily obtained by mining
the web (Resnik and Smith, 2003), comparable
corpora (Munteanu and Marcu, 2005), and even
social media sites (Ling et al., 2013), automati-
cally extracted parallel tends to be noisy, and, as a
result, ?evaluation-quality? parallel corpora have
generally been produced at considerable expense
by targeted translation efforts (Bojar et al., 2013,
inter alia). Unfortunately, in some domains such
as microblogs, the only corpora that are available
are automatically extracted and noisy.
While phrase-based translation models can ef-
fectively learn translation rules from noisy parallel
data (Goutte et al., 2012), having a subset of high-
quality parallel segments is nevertheless crucial.
Firstly, the automatic parallel data extraction sys-
tem?s parameters can be tuned by optimizing on
the gold standard data. Secondly, even though the
parallel data used to train MT systems can contain
a considerable amount of noise, it is conventional
to use human annotated parallel data to tune and
evaluate the system. Finally, other NLP applica-
tions may not be as noise-robust as MT.
We introduce a new crowdsourcing protocol for
obtaining high-quality parallel data from noisy,
automatically extracted parallel data (?3), focus-
ing on the challenging case of identifying par-
allel data in microblog messages (Ling et al.,
2013). In contrast to previous attempts to use
crowdsourcing to obtain parallel data, in which
workers performed translation (Ambati and Vo-
gel, 2010; Zaidan and Callison-Burch, 2011; Post
et al., 2012; Ambati et al., 2012), our approach
only requires that they identify whether a candi-
date message contains a translation, and if so, what
the spans of the translated segments are. This is
a much simpler task than translation, and one that
can often be completed by workers with only a ba-
sic proficiency in the source and target languages.
For evaluation (?4), we use our protocol to build
426
parallel datasets on a Chinese-English corpus orig-
inally extracted from Sina Weibo and for which we
have expert annotations. This lets us quantify the
effectiveness of our method under different task
variations. We also show that the crowdsourced
corpus performs as well as expert annotation (and
better than the automatically extracted corpus) for
tuning an MT system with MERT. We next apply
our method on a corpus of five language pairs (en-
ar, en-ja, en-ko, en-ru, en-zh) extracted from Twit-
ter (?5), for which we have no gold-standard data.
Using this data in a cross-validation setup, we train
and evaluate a maxent classifier for detecting par-
allel data (?6), and then we conclude (?7).
2 Related Work
Our work crosses crowdsourcing techniques and
automatic parallel data extraction from mi-
croblogs. In this section, we shall provide back-
ground information and analysis of the work per-
formed in these two fields.
2.1 Parallel Data Extraction from Microblogs
Many sources of parallel data exist on the
web. The most popular choice are parallel web
pages (Resnik and Smith, 2003), while other
work have looked at specific domains with large
amounts of data, such as Wikipedia (Smith et
al., 2010). Microblogs, such as Twitter and Sina
Weibo, represent a subdomain of the Web. Some
of its characteristics is the informal language used
and the short nature of the messages that are
posted. Due to its large size and growing pop-
ularity, work has been done on parallel data ex-
traction from this domain. Ling et al. (2013) at-
tempt to find naturally occurring parallel data from
Sina Weibo and Twitter. Some examples of what
is found are illustrated in Figure 1. The extrac-
tion process starts by finding the parallel segments
within the same message and the word alignments
between those segments that maximize a hand-
tuned model score.
Another method (Jehl et al., 2012) leverages
CLIR (Cross Lingual Information Retrieval) tech-
niques to find pairs of tweets that are translations.
The main challenge in this approach is the large
amount of pairs of tweets that must be considered,
which raises some scalability issues when process-
ing billions of tweets.
Our crowdsourcing method can be applied to
annotate data from any naturally occurring source.
In this paper, we will use the corpus developed
by Ling et al. (2013), since it is publicly available
and has parallel data for 6 languages from Twitter,
and for 10 languages from Sina Weibo.
2.2 Parallel Data using Crowdsourcing
Most of the work done in building parallel data
using crowdsourcing (Ambati and Vogel, 2010;
Zaidan and Callison-Burch, 2011; Post et al.,
2012; Ambati et al., 2012) relies on using crowd-
sourcing workers to translate. These methods
must address the fact that workers may produce
poor and sometimes incorrect translations. Thus,
in order to find good translations, subsequent
postediting and/or ranking is generally necessary.
In contrast, in our work, crowdsourcing is used
for data extraction rather than translation, a sub-
stantially simpler task than translation (in particu-
lar, translation of informal text) that requires less
expertise in the language pair (basic proficiency in
the two languages is generally sufficient to suc-
cessfully complete the task). Furthermore, assess-
ing whether a worker performed the task correctly
and combining the outputs of different workers is
simpler. The time spent per item is also reduced:
our annotation interface only requires the worker
to make a few clicks on the tweet to complete
each annotation, meaning that tasks are completed
faster and with less effort, allowing us to obtain
translations at lower cost. On the other hand,
the main drawback of our method is that it can
only obtain parallel data from translations that ex-
ist, which corresponds to the amount of posts that
have been translated and posted. This limits the
potential coverage of our method. Furthermore,
the resulting datasets may not be fully representa-
tive of the Twitter domain, since not all types of
content are translated and follow the same distri-
bution as the data in Twitter.
3 Proposed Crowdsourcing Protocol
As discussed above, automatically extracted par-
allel is often noisy. The sources of error range
from language detection errors, to errors determin-
ing if material is actually translation, and errors in
extracting the appropriate spans of the translated
material. Consider the fragment of the microblog
parallel corpus mined by Ling et al. (2013), which
is shown in Figure 1. In the Korean-English mes-
sage, the system may incorrectly added the un-
translated word Hahah in the English segment,
427
and missed the translated word Weather. At a high
level, the task faced by annotators will be to iden-
tify and resolve such errors.
3.1 Overview
We separate the tasks of identifying the parallel
posts, which we shall denote by identification,
and of locating the parallel segments, which we
will call location. The justification for this is that
the majority of the tweets are not parallel, as re-
ported by Ling et al. (2013), and the location of
the parallel data is only applicable if the tweet
actually contains parallel data. This is also de-
sirable because the identification task is simpler
than the location task. Firstly, identifying whether
a tweet contains translations requires much less
proficiency in the respective languages than locat-
ing the parallel segments, since it only requires
the worker to understand parts of the message.
This means we can have more potential workers
capable of performing this task. Secondly, the
first task is a binary decision, and each annota-
tion can be completed with only one action, which
means that the average required time for this task
is much lower than the second task and the pay-
ment required for each hit will naturally be lower
as well. Finally, combining worker results for a
binary decision is simpler than combining transla-
tions, since the space of possible answers is sev-
eral orders of magnitude lower.
As crowdsourcing platform, we use Amazon?s
Mechanical Turk. In this platform, the requesters
can submit tasks, where one can define the num-
ber of workers n that will complete each task and
what is the payment p for each task submission,
henceforth denoted as job. In our work, we had to
consider the following components:
? Interface - To submit a task, an interface
must be provided, which workers will be us-
ing to complete the job.
? Worker Quality Prediction - After submit-
ting a job, the requester can accept and pay
the agreed fee or reject the task. It is cru-
cial to have a method to automatically pre-
dict whether workers have performed the job
properly, and reject them otherwise.
? Result Combination - It is common for mul-
tiple workers to complete the same task with
different results. Thus, a method must be im-
plemented to combine multiple responses for
correctly predicting the desired response.
We structured each of our tasks as a series of q
questions, which include a small number of refer-
ences r, for which we know the answers. Thus,
the amount of answers we obtain for each dollar is
given by
q?r
np
, where n is the number of workers
per task and p is the payment for each task. In or-
der to maximize this quotient, we can either reduce
the number of reference question r, the number of
workers per task n, or the payment p. However,
reducing r will also limit our capability of esti-
mating the quality of the worker results, since we
will have less data to make such prediction. For
the same reason, reducing n will limit our abil-
ity to combine results properly. As for the pay-
ment p, while there is no direct effect on our task,
it has been noted that workers will perform the
task faster for higher payments (Post et al., 2012).
In our work, we will propose methods to predict
quality and combine results that will minimize the
requirements for n and r, while maximizing the
quality of the final results.
3.2 Parallel Post Identification
In the identification task, for each question, we
will show a post, and solicit the worker to detect if
it contains translations in a given language pair.
Interface The interface for this task is straight-
forward. We present to the worker each tweet in-
dividually, together with a checkbox to be checked
in case the tweet contains parallel data. The navi-
gation between tweets is done by adding next and
previous buttons, allowing the user to go back and
review previous answers. Finally, the worker can
only submit the HIT after traversing all 25 ques-
tions. Unlike the work in crowdsourcing transla-
tion (Zaidan and Callison-Burch, 2011), where au-
tomatic translation systems are discouraged, since
it produces poor output, we allow its usage as long
as this leads to correct annotations. In fact, we add
a button to automatically translate the tweet into
English from the non-English language.
Worker Quality Prediction We accept the job
if it answers enough reference questions correctly.
We consider two different approaches to select ref-
erences. A random sampler that selects tweets
randomly and a balanced sampler that selects
the same number of positive and negative sam-
ples. As notation, we will denote as acceptor
428
Figure 1: Parallel microblog posts in 5 language pairs. Shaded backgrounds mark the parallel segments
(annotated manually), non shaded parts do not have translations.
accept(rand, c, r) a setup where the worker?s job
is accepted if c out of r randomly sampled refer-
ences are correctly answered. Likewise, acceptor
accept(bal, c, r) denotes the same setup using bal-
anced reference questions.
Result Combination Given n jobs with answers
for a question that can be either positive or nega-
tive, we calculate the weighted ratio of positive an-
swers, given by
?
i=1..n
?
p
(i)w(i)
?
i=1..n
w(i)
, where ?
p
is one if
answer i is positive and 0 otherwise, and w(i) is
the weight of the worker. w(i) is defined as the
ratio of correct answers from job i in the reference
set. If the weighted ratio is higher than 0.5, we la-
bel the tweet as positive and otherwise as negative.
3.3 Parallel Data Location
In the location task, we also present one tweet per
question, where the worker will be asked to iden-
tify the parallel segments. The worker can also
define that there are no translations in the tweet.
Interface The interface for this task presents the
user with one tweet at a time, and allows the user
to break the tweet into segments, by clicking be-
tween characters. Each segment can then be clas-
sified as English, the non-English language (Ex:
Mandarin), or non-parallel, which is the default
option. To understand the concept of non-parallel
segments, notice that when we are locating par-
allel data in tweets, we are essentially breaking
the tweet into the structure ?N
left
P
left
N
middle
P
right
N
right
", where P
left
and P
right
are the par-
allel segments and N
left
, N
middle
and N
right
are
textual segments that are non-parallel. These may
not exist, for instance, the Arabic tweet in Fig-
ure 1 (line 1) does not contain any non-parallel text
and does not require any non-parallel segments
to delineate the parallel data. The Korean tweet
(line 2), on the other hand, has an N
middle
corre-
sponding to????????????????????????????* and an
N
right
corresponding to Hahah and requires two
non-parallel segments to locate the parallel data.
Thus, if the worker does not commit any errors,
each question can be answered with at most four
clicks, when all five segments exist, and two op-
tion choices for identifying the parallel segments.
In the easiest case, when only the parallel seg-
ments exist, only one click and two option choices
are needed. If there are no translations, the button
no translations can be clicked.
For instance, to annotate the Korean tweet in
Figure 1, the worker must click immediately be-
fore????, then before Weather and finally before
Hahah. Then on the drop-down box of the first
and and third segments, the worker must choose
Korean and English, respectively. The interface
after these operations is show in Figure 2.
Work Quality Prediction To score the worker?s
jobs, we use the scoring function devised in (Ling
et al., 2013), which measures the word overlap
between the reference parallel segments segments
and the predicted segments. However, setting the
score threshold to accept a job is a challenge, since
scores are bound to change for different language-
pairs and domains. Moreover, some tweets are
harder to annotate than others. Learning this
threshold automatically requires annotated data,
which we do not have for all language pairs and
domains. Thus, we propose a method to generate
thresholds specifically for each sample.
We consider a ?smart but lazy" pseudo worker,
who will complete the same jobs automatically
and generate scores that the real worker?s jobs
must beat to be accepted. We say he is ?smart",
429
Figure 2: Location Interface (After the annotation is performed)
since he knows the reference annotation, and
?lazy" because he will only define a new non-
parallel segment if it is significant, otherwise it
will just be left in the parallel segments. By sig-
nificant, we will define whether it is at least 20%
larger (in number of characters) than the parallel
segments. For instance, in the Korean example in
Figure 1, Hahah would be left in the English par-
allel segment, while ???????????????????????
??? ??* would not be in the Korean segment. We
will accept a job if the average of the scores in the
reference set is higher or equal than the pseudo
worker?s scores. This acceptor shall be denoted as
accept(lazy, a), where a is the number of refer-
ences used.
Another option is to use the automatic system?s
output as a baseline that workers must improve to
be accepted. We will also test this option and call
this acceptor accept(auto, a).
Result Combination Unlike the identification
task, where the result is binary and combining
multiple decisions is straightforward, the range of
results from this task is larger and combining them
is a challenge. Thus, we score each job based on
the WER on the reference set and use annotations
of the highest scoring job.
4 Experiments
To obtain results on the effectiveness of the meth-
ods described in Section 3, we will first perform
experiments using pre-annotated data. We use the
annotated dataset with tweets in Mandarin-English
from Sina Weibo created in (Ling et al., 2013).
It consists of approximately 4000 tweets crawled
from Sina Weibo that were annotated on whether
they contained parallel data and the location of the
parallel segments. In our experiment, we sample
1000 tweets from this dataset, where 602 tweets
were parallel and 398 were not.
1
We will not submit the same tasks using differ-
ent setups, since we would have to pay the cost of
the tasks multiple times. Furthermore, we know
the answers for all the questions in this controlled
experiment, the quality of a job can be evalu-
ated precisely by using all questions as references.
Thus, we will perform the task once, with a larger
number of workers and accepting and rejecting
jobs based on their real quality. Then, we will use
the resulting datasets and simulate the conditions
using different setups.
430
Acceptor avg(a) avg(r) d
accept(rand, 2, 2) 0.44 0.00 0.44
accept(rand, 3, 4) 0.44 0.00 0.44
accept(rand, 4, 4) 0.55 0.04 0.51
accept(bal, 2, 2) 0.69 0.09 0.60
accept(bal, 3, 4) 0.64 0.03 0.61
accept(bal, 4, 4) 0.76 0.15 0.61
Table 1: Agreement with the expert annotations
for different acceptors.
4.1 Identification Task
The 1000 tweets were distributed into 40 tasks
with 25 questions each (q = 25). Each task is
to be performed by 5 workers (n = 5) and upon
acceptance, a worker would be rewarded with 6
cents (p = 0.06). As we know the answers for
all the questions in this case, we will calculate the
Cohen?s Kappa between the responses of each job
and the expert annotator, and accept a job if it is
higher than 0.5. We decided to use Cohen?s kappa
to evaluate a job, rather than accuracy, since each
set of 25 questions does not contain the same num-
ber of positive and negative samples. For instance,
in a set of 20 negative samples, a worker would
achieve an accuracy of 80% if he simply answers
negatively to all questions, which is not an ade-
quate assessment of the job?s quality. On the other
hand, the Cohen?s Kappa balances the positive and
negative question in each task by using their prior
probabilities. In total, there were 566 jobs, where
200 where accepted and 366 were rejected.
Next, we pretended that we only have access to
4 references, which will be used for quality es-
timation and simulate the acceptances and rejec-
tions for each strategy. Table 1 shows the aver-
ages of the real Kappa values of accepted (col-
umn avg(a)) and rejected jobs (column avg(r))
using different acceptors. Our goal is to maximize
the number of acceptances with high Kappa val-
ues and minimize those that have low Kappa val-
ues. Thus, we define d as the difference between
avg(a) and avg(r). From the results, we observe
that using a balanced reference yields a much bet-
ter estimation of the jobs quality using our metric
d. Similar conclusions can be reached by compar-
ing accept(rand, 3, 4) with accept(bal, 3, 4) and
accept(rand, 4, 4) with accept(bal, 4, 4). Quality
predictors that use balanced reference sets achieve
1
We wished to annotate a sample where the number of
parallel posts is high, so that we would have enough samples
to perform the location task.
Acceptor prec recall F1 acc ?
Automatic 0.87 0.69 0.77 0.75 0.51
All jobs 0.75 0.84 0.8 0.74 0.44
accept(rand, 2, 2) 0.85 0.92 0.88 0.86 0.69
accept(rand, 3, 4) 0.84 0.93 0.88 0.85 0.68
accept(rand, 4, 4) 0.91 0.95 0.93 0.92 0.82
accept(bal, 2, 2) 0.94 0.94 0.94 0.92 0.84
accept(bal, 3, 4) 0.93 0.95 0.94 0.93 0.85
accept(bal, 4, 4) 0.94 0.93 0.93 0.92 0.84
Table 2: Parallel post prediction scores using dif-
ferent acceptors.
approximately the same results for d. However,
the setup accept(bal, 3, 4) has a lower Kappas for
both avg(a) and avg(r), which means that it is
less likely to reject good jobs at the cost of accept-
ing more bad jobs. This is desirable from an ethi-
cal perspective, since workers are not responsible
for errors in our quality prediction. Furthermore,
rejecting good jobs has a negative impact on the
progress of the task, since good workers may be
discouraged to perform more tasks.
Results on the identification task, obtained for
n = 3, are shown in Table 2. Naturally, us-
ing a balanced reference set yields better results,
since these have a higher d value. We can also
see the importance of quality prediction, since not
performing quality estimation (row All jobs) will
yield worse results than the automatic system.
Next, we will compare results using different
numbers of workers. We fix the quality predic-
tion methodology to accept(bal, 3, 4) and results
are shown in Table 3. We observe that in gen-
eral, using more workers will generate better re-
sults, but score gains from adding another worker
becomes lower as n increases. One problem for
n = 2 is the fact that there are many cases where
two workers with the same weight chose a posi-
tive and a negative answer, in which case, no de-
cision can be made, and we simply choose false
by default. This explains the high recall and low
precision values. However, this problem seems to
occur much less with higher values of n.
4.2 Location Task
For the location task, we used the predicted par-
allel posts the identification task with the setup
accept(bal, 3, 4) and n = 5. We preferred to use
this rather than using the expert annotations, since
it would not contain false positives, which does not
simulate a real situation. Then, we used 500 out of
431
# workers prec recall F1 acc ?
Automatic 0.87 0.69 0.77 0.75 0.51
1 0.86 0.85 0.85 0.82 0.64
2 0.85 0.95 0.90 0.87 0.72
3 0.93 0.95 0.94 0.93 0.85
4 0.94 0.96 0.95 0.94 0.87
5 0.96 0.96 0.96 0.95 0.90
Table 3: Identification scores for different n.
the 607 identified positive samples. This makes
20 tasks in total, with 25 questions (q = 25), and
each task would be run until 5 jobs are accepted
(n = 5). For this task, we set a payment of 30
cents (p = 0.3), since it is a more complex task.
Again, since we have the expert annotations for all
questions, we calculated the average WER on all
answers and rejected jobs scoring less than 0.6
2
.
This task is mainly focused on the quality pre-
diction of the workers, as the result combination
is done by finding the job with the highest score
in the reference set. This means, for an arbitrary
large n, all quality estimation methods will pro-
duce the same result, since we will find the best
job on the references eventually. However, bet-
ter quality estimation will allow us to find the best
jobs with lower n, which makes the task less ex-
pensive. Table 4 shows results using different se-
tups. In these results, we set aside 4 questions to
be used as references. We can see that for low n
(1 or 2), if we simply accept all jobs, the quality
of the results will be lower than the automatic sys-
tem. For n = 4, this approach can achieve a WER
score of 0.06. However, if we use the automatic
system as a baseline that jobs must surpass, we can
achieve this WER score with only two jobs, which
reduces the cost of this task by half. Yet, this is
strongly dependent on the automatic system, as a
worse system will be easier to match for the work-
ers. On the other hand, using the smart but lazy
pseudo worker, where we degrade the reference
annotations slightly, we can see that we can obtain
the 0.06 WER score using only the first worker. At
n = 2, we can see that the WER improves to 0.05,
which is lost for n = 3. This is because the pre-
diction of the quality of the job using the workers
is not always precise.
4.3 Machine Translation Results
Finally, we will perform an extrinsic test to see
how the improvements obtained by using crowd-
2
Determined empirically
Number of jobs 1 2 3 4 5
Automatic 0.16 0.16 0.16 0.16 0.16
All Jobs 0.23 0.21 0.07 0.06 0.06
accept(auto, 4) 0.09 0.06 0.06 0.06 0.06
accept(lazy, 4) 0.06 0.05 0.06 0.06 0.06
Table 4: Parallel data location scores for different
acceptors (rows) and different numbers of work-
ers. Each cell denotes the WER for that setup.
Auto (Pos) Crowd Expert Auto (All)
Size 483 479 483 908
EN-ZH 10.21 10.49 10.51 10.71
ZH-EN 7.59 7.87 7.82 8.02
Table 5: BLEU score comparison using different
corpora for MERT tuning. The Size row denotes
the number of sentences of each corpus, and the
EN-ZH and ZH-EN rows denote the BLEU scores
of the respective language pair and tuning dataset.
sourcing map to Machine Translations. We will
build an out of domain MT system using the FBIS
dataset (LDC2003E14), a corpus of 300K sen-
tence pairs from the news domain in the Chinese-
English pair using the Moses (Koehn et al., 2007)
pipeline. Due to the small size of our crowd-
sourced corpus, we will use it in the MERT tun-
ing (Och, 2003), and test its effects compared to
automatically extracted parallel data and the ex-
perts judgements. As the test set, we will use
1,500 sentence pairs from the Weibo gold standard
from Ling et al. (2013), that were not used in our
crowdsourcing experiment to prevent data over-
lap. For reordering, we use the MSD reordering
model (Axelrod et al., 2005) and as the language
model, we use a 5-gram model with Kneser-Ney
smoothing (Heafield, 2011). Finally, results are
presented with BLEU-4 (Papineni et al., 2002).
We build 3 tuning corpora, the automatically ex-
tracted corpus (denoted Auto), the crowdsourced
corpus (denoted Crowd) and the corpus annotated
by the expert (denoted Expert). This is done by
taking the 1000 tweets used in this experiment, se-
lect those that were identified as parallel accord-
ing to each criteria. For the automatic extraction,
the authors in (Ling et al., 2013) simply use all
tweets as parallel, which may influence the tun-
ing results. Thus, we test two versions of this cor-
pus, one where we take all samples as parallel (de-
noted Auto (All)), and one where we use the ex-
pert?s decision for the identification task only (de-
432
Pair Parallel Avg(en) cost(I) cost(L) total
en-ar 1512 8.3 $35.7 $43.2 $76.2
en-zh 1302 8.7 $35.7 $37.2 $70.2
en-ja 1155 7.9 $35.7 $33.0 $68.7
en-ko 1008 7.1 $35.7 $28.8 $64.5
en-ru 798 6.3 $35.7 $22.8 $58.5
all 5775 ? $178.5 $165.0 $343.5
Table 7: AMT costs for crowdsourced corpora
from Twitter.
noted Auto (Pos)). In the crowdsourcing case, we
use the accept(bal, 3, 4) setup, with n = 5, for the
identification task and the accept(lazy, 4) setup,
with n = 2, for the location task. From the re-
sulting parallel tweets, we also remove all tweets
that were used as reference in the accept(lazy, 4)
quality estimator, as this would give an unfair ad-
vantage to the crowdsourced corpora.
Results are shown in Table 5, where each cell
contains the average BLEU score in 5 MERT runs,
using a different tuning dataset. Surprisingly, us-
ing the whole set of automatically extracted cor-
pora actually achieves better results than using
carefully selected data that are parallel. We be-
lieve that is because many non-parallel segments
actually contain comparable information that can
be used to improve the weights during MERT tun-
ing. However, this does not mean that the qual-
ity of the automatically crawled corpus is better
than the crowdsourced and expert annotated cor-
pus. When using a similar number of parallel sen-
tences, we observe that using the crowdsourced
corpus yields better scores than the automatically
extracted corpora, comparable to experts annota-
tions. While results are not significantly better
than automatically extracted corpora, this suggests
that the crowdsourced corpora has a better overall
quality than automatically extracted corpora.
5 Five Language Twitter Parallel Corpus
Now that we have established the effectiveness of
our technique for extracting high-quality parallel
data in a scenario where we have gold standard
annotations, we apply it to creating parallel cor-
pora in five languages on Twitter, for which we
have no gold-standard parallel data: Arabic, Man-
darin, Japanese, Korean and Russian. Once again,
we use the extracted automatically Twitter cor-
pus from Ling et al. (2013) and deploy the task
in Mechanical Turk. We use the setup that ob-
tained the best results in Section 4. For the identi-
fication task, we used the accept(bal, 3, 4) setup,
with n = 5. The payment for each task was
0.06 dollars. Thus, for this task, each dollar spent
yields 70 annotated tweets. For the location task,
we used the accept(lazy, 4) setup, with n = 2
and each task was rewarded with 0.3 dollars. To
obtain the tweet sample, we filtered the corpora
in Ling et al. (2013) for tweets with alignment
scores higher than 0.1. Then, we uniformly ex-
tracted 2500 tweets for each language. To gener-
ate gold standard references, the authors manually
annotated 40 samples for each pair.
Table 7 contains information about the result-
ing corpora. The number of parallel sentences ex-
tracted from the 2500 tweets in each language pair
is shown in column Parallel and we can see that
this differs given the language pair. We can also
see in column Avg(en) that the average number of
English words is much smaller than what is seen
in more formal domains. Finally, Arabic parallel
data seems more predominant from our samples
followed by Mandarin, while Russian parallel data
seem scarcer.
6 Discriminative Parallel Data Detection
While the work in (Ling et al., 2013) used a linear
combination of three models, the alignment, lan-
guage and segment features, these weights were
determined manually. However, using the crowd-
sourced corpus (in Section 5), we will apply previ-
ously proposed methods that learn a classifier with
machine learning techniques as in related work
on finding parallel data (Resnik and Smith, 2003;
Munteanu and Marcu, 2005). In our work, we use
a max entropy classifier model, similar to that pre-
sented by Munteanu and Marcu (2005) to detect
parallel data in tweets. Our features are:
? Alignment feature - The baseline feature is
the alignment score from the work in (Ling et
al., 2013), and measures how well the paral-
lel segments align, which is derived from the
content-based matching methods for detect-
ing parallel data (Resnik and Smith, 2003).
? User features - An observation in (Ling et
al., 2013) is that a user that frequently posts
in parallel is likely to post more parallel mes-
sages. Based on this, we added the aver-
age alignment score from all messages of the
same user and the ratio of messages that are
predicted to be parallel as features.
433
Weibo (en-zh) Twitter (en-zh) Twitter (en-ar) Twitter (en-ru) Twitter (en-ko) Twitter (en-ja)
Alignment 0.781 0.599 0.721 0.692 0.635 0.570
+User 0.814 0.598 0.721 0.705 0.650 0.566
+Length 0.839 0.603 0.725 0.706 0.650 0.569
+Repetition 0.849 0.652 0.763 0.729 0.655 0.579
+Language 0.849 0.668 0.782 0.737 0.747 0.584
Table 6: Classification Results using a 10-fold cross validation over different datasets. Each cell contains
the F-measure using a given dataset and an incremental set of features.
? Repetition features - There are many words
that are not translated, such as hashtags, at
mentions, numbers and named entities. So, if
we see these repeated twice in the same post,
it can be used as a strong cue that this was
the result of a translation. Hence, we define
features for each of these cases, that trigger if
either of these occur in multiples of two times
in the same post. Named Entities were iden-
tified using a naive approach by considering
words with capital letters.
? Length feature - It is known that the length
differences between parallel sentences can
be modelled by a normal distribution (Gale
and Church, 1991). Hence, we used parallel
data in the respective language to determine
(??, ??
2
), which lets us calculate the likelihood
of two hypothesized segments being parallel.
Since we did not have annotated parallel data
for this domain, we used the top 2000 scoring
parallel sentences from the respective Twitter
dataset in (Ling et al., 2013).
? Language feature - It is common for non-
English words to be found in English seg-
ments, such as names of foreign celebri-
ties, numbers and hashtags. However, when
this happens to the majority of the words in
a segment that is supposed to be English,
it may indicated that there was an error in
the language detection. The same happens
with non-English segments. We used the
same naive approach to detect languages as
in (Ling et al., 2013), where we calculate the
ratio of number of words in the English seg-
ment and the total number of words from the
segment detected as English and the ratio of
the number of Foreign words and the total
number of words in the Foreign segment ,de-
tected by their unicode ranges. This was also
included in the work in (Ling et al., 2013).
Results using a 10 fold cross-validation are
shown in Table 6. In general, we can see that the
classifier performs worse in Twitter datasets com-
pared to the Weibo dataset. We believe that this is
because parallel sentences extracted from Twitter
are smaller, due to the 140 character limit, which
does not hold in Sina Weibo. Each parallel En-
glish segment from the Sina Weibo parallel data
contains 15.4 words on average. On other hand,
we see in Table 7 that this number is smaller in
the parallel data from Twitter. This means that the
aligner will have a much smaller range of words to
align when detecting parallel data, which makes it
more difficult to find parallel segments.
As for the features, we observe that by defin-
ing these simple features, we can get a signifi-
cant improvement over previous baselines. For
the User feature, we see that the improvements
in the Weibo dataset are much larger than in
the Twitter datasets. This is because the Twitter
dataset was crawled uniformly, whereas the Weibo
dataset was focused on users that post parallel
data frequently. Thus, in the Weibo dataset there
more posts that were posted by the same user,
which does not happen as frequently in the Twitter
dataset. As for the Length feature, we can see that
it yields a small but consistent improvement over
all datasets. Repetition based features also lead to
improvements across all datasets, and produces a
5% improvement in the English-Mandarin Twitter
dataset. Finally, language based features also add
another improvement over previous results.
7 Conclusions
We presented a crowdsourcing approach to extract
parallel data from tweets. As opposed to meth-
ods to crowdsource translations, our tasks do not
require workers to translate sentences, but to find
them in tweets. Our method is divided into two
tasks. First, we identify which tweets contain
translations, and we show that multiple worker?s
jobs can be combined to obtain results compara-
434
ble to those of expert annotators. Secondly, tweets
that are found to contain translations are given
to other workers to locate the parallel segments,
where we can also obtain high quality results.
Then, we use our method to extract high quality
parallel data from Twitter in 5 language pairs. Fi-
nally, we improve the automatic identification of
tweets with translations by using a max entropy
classifier trained on the crowdsourced data.
We are currently extracting more data and the
crowdsourced parallel data from Twitter will made
be available to the public.
References
[Ambati and Vogel2010] Vamshi Ambati and Stephan
Vogel. 2010. Can crowds build parallel corpora
for machine translation systems? In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, Stroudsburg, PA, USA. Association for
Computational Linguistics.
[Ambati et al.2012] Vamshi Ambati, Stephan Vogel,
and Jaime Carbonell. 2012. Collaborative workflow
for crowdsourcing translation. In Proceedings of the
ACM 2012 Conference on Computer Supported Co-
operative Work, CSCW ?12, pages 1191?1194, New
York, NY, USA. ACM.
[Axelrod et al.2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings International Workshop on Spoken Lan-
guage Translation (IWSLT.
[Bannard and Callison-burch2005] Colin Bannard and
Chris Callison-burch. 2005. Paraphrasing with
bilingual parallel corpora. In In ACL-2005, pages
597?604.
[Bojar et al.2013] Ond
?
rej Bojar, Christian Buck, Chris
Callison-Burch, Christian Federmann, Barry Had-
dow, Philipp Koehn, Christof Monz, Matt Post,
Radu Soricut, and Lucia Specia. 2013. Find-
ings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 1?
44, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
[Das and Petrov2011] Dipanjan Das and Slav Petrov.
2011. Unsupervised part-of-speech tagging with
bilingual graph-based projections. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 600?609,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Gale and Church1991] William A. Gale and Ken-
neth W. Church. 1991. A program for aligning
sentences in bilingual corpora. In Proceedings of
the 29th Annual Meeting on Association for Com-
putational Linguistics, ACL ?91, pages 177?184,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Gale et al.1992] William A. Gale, Kenneth W. Church,
and David Yarowsky. 1992. Using bilingual materi-
als to develop word sense disambiguation methods.
[Ganitkevitch et al.2012] Juri Ganitkevitch, Yuan Cao,
Jonathan Weese, Matt Post, and Chris Callison-
Burch. 2012. Joshua 4.0: Packing, PRO, and para-
phrases. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 283?291,
Montr?al, Canada, June. Association for Computa-
tional Linguistics.
[Goutte et al.2012] Cyril Goutte, Marine Carpuat, and
George Foster. 2012. The impact of sentence
alignment errors on phrase-based machine transla-
tion performance. In Proc. of AMTA.
[Heafield2011] Kenneth Heafield. 2011. KenLM:
faster and smaller language model queries. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
[Jehl et al.2012] Laura Jehl, Felix Hieber, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montr?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-burch, Richard
Zens, Rwth Aachen, Alexandra Constantin, Mar-
cello Federico, Nicola Bertoldi, Chris Dyer, Brooke
Cowan, Wade Shen, Christine Moran, and Ondrej
Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June.
Association for Computational Linguistics.
[Ling et al.2013] Wang Ling, Guang Xiang, Chris Dyer,
Alan Black, and Isabel Trancoso. 2013. Microblogs
as parallel corpora. In Proceedings of the 51st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?13. Association for Computational
Linguistics.
[Liu et al.2011] Feifan Liu, Fei Liu, and Yang Liu.
2011. Learning from chinese-english parallel data
for chinese tense prediction. In IJCNLP, pages
1116?1124.
[Munteanu and Marcu2005] Dragos Munteanu and
Daniel Marcu. 2005. Improving machine transla-
tion performance by exploiting comparable corpora.
Computational Linguistics, 31(4):477?504.
435
[Ng et al.2003] Hwee Tou Ng, Bin Wang, and Yee Seng
Chan. 2003. Exploiting parallel texts for word sense
disambiguation: An empirical study. In Proceedings
of ACL03, pages 455?462.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al.2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montr?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Schwarck et al.2010] Florian Schwarck, Alexander
Fraser, and Hinrich Sch?tze. 2010. Bitext-based
resolution of german subject-object ambiguities. In
Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 737?740, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Smith et al.2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 403?411, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Specia et al.2005] Lucia Specia, Maria Das Gra?as,
Volpe Nunes, and Mark Stevenson. 2005. Exploit-
ing parallel texts to produce a multilingual sense
tagged corpus for word sense disambiguation. In
Proceedings of RANLP-05, Borovets, pages 525?
531.
[Zaidan and Callison-Burch2011] Omar F. Zaidan and
Chris Callison-Burch. 2011. Crowdsourcing trans-
lation: professional quality from non-professionals.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
1220?1229, Stroudsburg, PA, USA. Association for
Computational Linguistics.
436
