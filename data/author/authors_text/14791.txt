Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 127?137,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Translation via Targeted Paraphrasing
Philip Resnik
Linguistics and UMIACS
University of Maryland
resnik@umd.edu
Olivia Buzek
Linguistics and Computer Science
University of Maryland
olivia.buzek@gmail.com
Chang Hu
Computer Science
University of Maryland
changhu@cs.umd.edu
Yakov Kronrod
Linguistics and UMIACS
University of Maryland
yakov@umd.edu
Alex Quinn
Computer Science
University of Maryland
aq@cs.umd.edu
Benjamin B. Bederson
Computer Science and UMIACS
University of Maryland
bederson@cs.umd.edu
Abstract
Targeted paraphrasing is a new approach to the
problem of obtaining cost-effective, reasonable
quality translation that makes use of simple and
inexpensive human computations by monolin-
gual speakers in combination with machine
translation. The key insight behind the process
is that it is possible to spot likely translation
errors with only monolingual knowledge of the
target language, and it is possible to generate al-
ternative ways to say the same thing (i.e. para-
phrases) with only monolingual knowledge
of the source language. Evaluations demon-
strate that this approach can yield substantial
improvements in translation quality.
1 Introduction
For most of the world?s languages, the availability of
translation is limited to two possibilities: high qual-
ity at high cost, via professional translators, and low
quality at low cost, via machine translation (MT). The
spectrum between these two extremes is very poorly
populated, and at any point on the spectrum the ready
availability of translation is limited to only a small
fraction of the world?s languages. There is, of course,
a long history of technological assistance to transla-
tors, improving cost effectiveness using translation
memory (Laurian, 1984; Bowker and Barlow, 2004)
or other interactive tools to assist translators (Esteban
et al, 2004; Khadivi et al, 2006). And there is a
recent and rapidly growing interest in crowdsourc-
ing with non-professional translators, which can be
remarkably effective (Munro, 2010). However, all
these alternatives face a central availability bottle-
neck: they require the participation of humans with
bilingual expertise.
In this paper, we report on a new exploration of
the middle ground, taking advantage of a virtually
unutilized resource: speakers of the source and tar-
get language who are effectively monolingual, i.e.
who each only know one of the two languages rel-
evant for the translation task. The solution we are
proposing has the potential to provide a more cost
effective approach to translation in scenarios where
machine translation would be considered acceptable
to use, if only it were generally of high enough qual-
ity. This would clearly exclude tasks like transla-
tion of medical reports, business contracts, or literary
works, where the validation of a qualified bilingual
translator is absolutely necessary. However, it does
include a great many real-world scenarios, such as
following news reports in another country, reading in-
ternational comments about a product, or generating
a decent first draft translation of a Wikipedia page
for Wikipedia editors to improve.
The use of monolingual participants in a human-
machine translation process is not entirely new.
Callison-Burch et al (2004) pioneered the explo-
ration of monolingual post-editing within the MT
community, an approach extended more recently to
provide richer information to the user by Albrecht et
al. (2009) and Koehn (2009). There have also been at
least two independently developed human-machine
translation frameworks that employ an iterative pro-
tocol involving monolinguals on both the source and
target side. Morita and Ishida (2009) describe a sys-
tem in which target and source language speakers
perform editing of MT output to improve fluency
and adequacy, respectively; they utilize source-side
paraphrasing at a course grain level, although their ap-
proach is limited to requests to paraphrase the entire
sentence when the translation cannot be understood.
127
Bederson et al (2010) describe a similar protocol in
which cross-language communication is enhanced by
metalinguistic communication in the user interface.
Shahaf and Horvitz (2010) use machine translation
as a specific instance of a general game-based frame-
work for combining a range of machine and human
capabilities.
We call the technique used here targeted para-
phrasing. In a nutshell, target-language monolin-
guals identify parts of an initial machine translation
that don?t appear to be right, and source-language
monolinguals provide the MT system with alterna-
tive phrasings that might lead to better translations;
these are then passed through MT again and the best
scoring hypothesis is selected as the final translation.
This technique can be viewed as compatible with
the richer protocol- and game-based approaches, but
it is considerably simpler; in Sections 2 through 4
we describe the method and present evaluation re-
sults on Chinese-English translation. Unlike other
approaches, the technique also offers clear opportu-
nities to replace human participation with machine
components if the latter are up to the task; we discuss
this in Section 5 before wrapping up in Section 6
with conclusions and directions for future work.
2 Targeted Paraphrasing
The starting point for our approach is an observa-
tion: the source sentence provided as input to an MT
system is just one of many ways in which the mean-
ing could have been expressed, and for any given
MT system, some forms of expression are easier to
translate than others. The same basic observation
has been applied quite fruitfully over the past several
years to deal with statistical MT challenges involv-
ing segmentation, morphological analysis, and more
recently, source language word order (Dyer, 2007;
Dyer et al, 2008; Dyer and Resnik, 2010). Here we
apply it to the surface expression of meaning.
For example, consider the following real example
of translation from English to French by an automatic
MT system:
? Source: Polls indicate Brown, a state senator,
and Coakley, Massachusetts? Attorney General,
are locked in a virtual tie to fill the late Sen. Ted
Kennedy?s Senate seat.
? System: Les sondages indiquent Brown,
un se?nateur d?e?tat, et Coakley,
Massachusetts? Procureur ge?ne?ral, sont en-
ferme?s dans une cravate virtuel a` remplir le
regrette? se?nateur Ted Kennedy?s sie`ge au Se?nat.
A French speaker can look at this automatic transla-
tion and see immediately that the underlined parts
are wrong, even without knowing the intended source
meaning. We can identify the spans in the source En-
glish sentence that are responsible for these badly
translated French spans, and change them to alterna-
tive expressions with the same meaning (e.g. chang-
ing Massachusetts? Attorney General to the Attorney
General of Massachusetts); if we do so and then use
the same MT system again, we obtain a translation
that is still imperfect (e.g. cravate means necktie),
but is more acceptable:
? System: Les sondages indiquent que Brown, un
se?nateur d?e?tat, et Coakley, le procureur ge?ne?ral
du Massachusetts, sont enferme?s dans une cra-
vate virtuel pourvoir le sie?ge au Se?nat de Sen.
Ted Kennedy, qui est de?ce?de? re?cemment.
Operationally, then, translation with targeted para-
phrasing includes the following steps.
Initial machine translation. For this paper, we
use the Google Translate Research API, which,
among other advantages, provides word-level align-
ments between the source text and its output. In
principle, however, any automatic translation system
can be used in this role, potentially at some cost
to quality, by performing post hoc target-to-source
alignment.
Identification of mistranslated spans. This step
identifies parts of the source sentence that lead to
ungrammatical, nonsensical, or apparently incorrect
translations on the target side. In the experiments
of Sections 3 and 4, this step is performed by hav-
ing monolingual target speakers identify likely error
spans on the target side, as in the French example
above, and projecting those spans back to the source
spans that generated them using word alignments
as the bridge (Hwa et al, 2005; Yarowsky et al,
2001). In Section 5, we describe a heuristic but effec-
tive method for performing this fully automatically.
Du et al (2010), in this proceedings, explore the
128
use of source paraphrases without targeting appar-
ent mistranslations, using lattice translation (Dyer
et al, 2008) to efficiently represent and decode the
resulting very large space of paraphrase alternatives.
Source paraphrase generation. This step gener-
ates alternative expressions for the source spans iden-
tified in the previous step. In this paper, it is per-
formed by monolingual source speakers who perform
the paraphrase task: the speaker is given a sentence
with a phrase span marked, and is asked to replace the
marked text with a different way of saying the same
thing, so that the resulting sentence still makes sense
and means the same thing as the original sentence.
To illustrate in English, someone seeing John and
Mary took a European vacation this summer might
supply the paraphrase Mary went on a European, ver-
ifying that the resulting John and Mary went on a
European vacation this summer preserves the origi-
nal meaning. This step can also be fully automated
(Max, 2009) by taking advantage of bilingual phrase-
table pivoting (Bannard and Callison-Burch, 2005);
see Max (2010), in these proceedings, for a related
approach in which the paraphrases of a source phrase
are used to refine the estimated probability distribu-
tion over its possible target phrases.
Generating sentential source paraphrases. For
each sentence, there may be multiple paraphrased
spans. These are multiplied out to provide full-
sentence paraphrases. For example, if two non-
overlapping source spans are each paraphrased in
three ways, we generate 9 sentential source para-
phrases, each of which represents an alternative way
of expressing the original sentence.
Machine translation of alternative sentences.
The alternative source sentences, produced via para-
phrase, are sent through the same MT system, and
a single-best translation hypothesis is selected, e.g.
on the basis of the translation system?s model score.
In principle, one could also combine the alternatives
into a lattice representation and decode to find the
best path using lattice translation (Dyer et al, 2008);
cf. Du et al (2010). One could also present trans-
lation alternatives to a target speaker for selection,
similarly to Callison-Burch et al (2004).
Notice that with the exception of the initial trans-
lation, each remaining step in this pipeline can in-
volve either human participation or fully automatic
processing. The targeted paraphrasing framework
therefore defines a rich set of intermediate points on
the spectrum between fully automatic and fully hu-
man translation, of which we explore only a few in
this paper.
3 Pilot Study
In order to assess the potential of our approach,
we conducted a small pilot study, using eleven
sentences in simplified Chinese selected from
the article on ?Water? in Chinese Wikipedia
(http://zh.wikipedia.org/zh-cn/%E6%B0%B4). This
article was chosen because its topic is well known
in both English-speaking and Chinese-speaking pop-
ulations. The first five sentences were taken from
the first paragraph of the article. The other six sen-
tences were taken from a randomly-chosen paragraph
in the article. As a preprocessing step, we removed
any parenthetical items from the input sentences, e.g.
?(H20)?. The shortest sentence in this set has 12 Chi-
nese characters, the longest has 54.1
Human participation in this task was accomplished
using Amazon Mechanical Turk, an online market-
place that enables human performance of small ?hu-
man intelligence tasks? (HITs) in return for micropay-
ments. For each sentence, after we translated it au-
tomatically (using Google Translate), three English-
speaking Mechanical Turk workers (?Turkers?) on
the target side performed identification of mistrans-
lated spans. Each span identified was projected back
to its corresponding source span, and three Chinese-
speaking Turkers were asked to provide paraphrases
of each source span. These tasks were easy to per-
form (no more than around 30 seconds to complete
on average) and inexpensive (less than $1 for the
entire pilot study).2 The Chinese source span para-
phrases were then used to construct full-sentence
paraphrases, which were retranslated, once again by
Google Translate, to produce the output of the tar-
geted paraphrasing translation process.
1Note that this page is not a translation of the corresponding
English Wikipedia page or vice versa.
2The four English-speaking Turkers were recruited through
the normal Mechanical Turk mechanism. The three Chinese-
speaking Turkers were recruited offline by the authors in order to
quickly obtain results, although they participated as full-fledged
Turkers.
129
The initial translation outputs from Google Trans-
late (GT) and the results of the targeted paraphrasing
translation process (TP) were evaluated according
to widely used critera of fluency and adequacy. Flu-
ency ratings were obtained on a 5-point scale from
three native English speakers without knowledge of
Chinese. Translation adequacy ratings were obtained
from three native Chinese speakers who are also flu-
ent in English; they assessed adequacy of English
sentences by comparing the communicated meaning
to the Chinese source sentences.
Fluency was rated on the following scale:
1. Unintelligible: nothing or almost nothing of the sen-
tence is comprehensible.
2. Barely intelligible: only a part of the sentence (less
than 50%) is understandable.
3. Fairly intelligible: the major part of the sentence
passes.
4. Intelligible: all the content of the sentence is com-
prehensible, but there are errors of style and/or of
spelling, or certain words are missing.
5. Very intelligible: all the content of the sentence is
comprehensible. There are no mistakes.
Adequacy was rated on the following scale:
1. None of the meaning expressed in the reference sen-
tence is expressed in the sentence.
2. Little of the reference sentence meaning is expressed
in the sentence.
3. Much of the reference sentence meaning is expressed
in the sentence.
4. Most of the reference sentence meaning is expressed
in the sentence.
5. All meaning expressed in the reference sentence ap-
pears in the sentence.
For each GT output, we averaged across the ratings
of the alternative TP to produce average TP fluency
and adequacy scores. The average GT output rat-
ings, measuring the pure machine translation base-
line, were 2.36 for fluency and 2.91 for adequacy.
Averaging across the TP outputs, these rose to 3.32
and 3.49, respectively.
One could argue that a more sensible evaluation
is not to average across alternative TP outputs, but
rather to simulate the behavior of a target-language
speaker who simply chooses the one translation
among the alternatives that seems most fluent. If
we select the most fluent TP output for each source
sentence according to the English-speakers? average
fluency ratings, we obtain average test set ratings of
3.58 for fluency and 3.73 for adequacy. Those are
respective gains of 0.82 and 1.21 over the baseline
initial MT output, each on a 5-point scale.
Figure 1 shows a selection of outputs: we present
the two cases where the most fluent TP alternative
shows the greatest gain in average fluency rating (best
gain +2.67); two cases near the median gain in av-
erage fluency (median +1); and the worst two cases
with respect to effect on average fluency rating (worst
-0.33). The table accurately conveys a qualitative im-
pression corresponding to the quantitative results: the
overall quality of translations appears to be improved
by our process consistently, despite the absence of
any bilingual input in the improvements.
4 Chinese-English Evaluation
As a followup to our pilot study, we conducted an
evaluation using Chinese-English test data taken from
the NIST MT?08 machine translation evaluation, in
order to obtain fully automatic translation evaluation
scores. We report on results for 49 sentences of the
1,357 in this data set. These underwent the same
targeted paraphrasing process as in the pilot study,
with the addition of a basic step to filter out cheaters:
we disregarded as invalid any responses consisting
purely of ASCII characters (signifying a non-Chinese
response) or responses that were identical to the orig-
inal source text.
Target English speakers identified 115 potential
mistranslation spans, or 2.3 spans per sentence, that
yielded at least one source paraphrase on the source
Chinese side. Chinese speakers provided 138 valid
paraphrases. The entire cost for the human tasks in
this experiment was $5.06, or a bit under $0.11 per
sentence on average.3
Table 1 reports on the results, evaluating in stan-
dard fashion using BLEU with the four English
MT?08 references for each Chinese sentence. Since
the targeted paraphrasing translation process (TP)
produces multiple hypotheses ? one automatic trans-
lation output per sentential paraphrases ? we se-
lected the single best output for each sentence by
3Invalid paraphrase responses were rejected, i.e. zero-cost.
130
Condition Fluency Adequacy Sentence
GT 1.33 2.33 Water play life evolve into important to use.
TP 4.00 4.33 Water in the evolution of life played an important role.
GT 1.33 2.67 Human civilization from the source of the majority of large rivers
in the domain.
TP 3.33 4.67 Most of the origin of human civilization in river basin.
GT 2.33 3.00 In human daily life, the water in drinking, cleaning, washing and
other side to make use of an indispensable.
TP 3.67 3.33 In human daily life, water for drinking, cleaning, washing and other
essential role.
GT 2.00 2.33 Eastern and Western ancient Pak prime material view of both the
water regarded as a kind of basic groups into the elements, water is
the Chinese ancient five rows of a; the West ancient four elements
that also have water.
TP 3.00 3.33 East and West in ancient concept of simple substances regarded wa-
ter as a basic component elements. Among them, the five elements
of water is one of ancient China; Western ancient four elements
that also have water.
GT 4.00 4.00 Early cities will generally be in the water side of the establishment,
in order to solve irrigation, drinking and sewage problems.
TP 4.67 4.33 Early cities are generally built near the water to solve the irrigation,
drinking and sewage problems.
GT 3.0 3.33 Human very early on began to produce a water awareness.
TP 2.67 3.00 Man long ago began to understand the water produced.
Figure 1: Original Google Translate output (GT) for the pilot study in Section 3, together with translations produced by
the targeted paraphrase translation process (TP), selected to show a range from strong to weak improvements in fluency.
131
Condition BLEU
GT (baseline) 28.33
GT n-best oracle 28.47
TP one-best 30.01
TP oracle 30.79
Human upper bound 49.41
Table 1: Results on a 49-sentence subset of the NIST
MT?08 Chinese-English test set
selecting the highest scoring English translation, ac-
cording to the translation score delivered with each
output by the Google Translate Research API. (The
original translation was, of course, included among
the candidates for selection.) This yielded an im-
provement of 1.68 BLEU points on the 49-sentence
test set (TP one-best).
One could argue that this result is simply a result of
having more hypotheses to choose from, not a result
of the targeted paraphrasing process itself. In order
to rule out this possibility, we generated (n+ 1)-best
Google translations, setting n for each sentence to
match the number of alternative translations gener-
ated via targeted paraphrasing. We then chose the
best translation for each sentence, among the (n+1)-
best Google hypotheses, via oracle selection, using
the TERp metric (Snover et al, 2009) to evaluate
each hypothesis against the reference translations.4
The resulting BLEU score for the full set showed
negligible improvement (GT n-best oracle).
We did a similar oracle-best calculation using
TERp for targeted paraphrasing (TP oracle). The
result shows a potential gain of 2.46 BLEU points
over the baseline, if the best scoring alternative from
the targeted paraphrasing process were always cho-
sen.
In addition to aggregate scoring using BLEU, we
also looked at oracle results on a per-sentence ba-
sis using TERp (since BLEU more appropriate to
use at the document level, not the sentence level).
Identifying the best sentential paraphrase alternative
using TERp as an oracle, we find that the TERp
score would improve for 32 of the 49 test sentences,
4An ?oracle? telling us which variant is best is not available
in the real world, of course, but in situations like this one, oracle
studies are often used to establish the magnitude of the potential
gain (Och et al, 2004).
65.3%. For those 32 sentences, the average gain is
8.36 TERp points.5 A fairer measure is the average
obtained when scoring zero gain for the 17 sentences
where no improvement was obtained; taking these
into account, i.e. assuming an oracle who chooses the
original translation if none of the paraphrase-based
alternatives are better, the average improvement over
the entire set of 49 sentences is 5.46 TERp points.
Although we have obtained results on only a small
subset of the full NIST MT?08 test set, our automatic
evaluation confirms the qualitative impressions in
Figure 1 and the subjective ratings results obtained
in our pilot study in Section 3. The TP oracle results
establish that by taking advantage of monolingual
human speakers, it is possible to obtain quite sub-
stantial gains in translation quality. The TP one-best
results demonstrate that the majority of that oracle
gain is obtained in automatic hypothesis selection,
simply by selecting the paraphrase-based alternative
translation with the highest translation score.
The last line in Table 1 shows a human upper
bound computed using the reference translations via
cross validation; that is, for each of the four reference
translations, we evaluate it as a hypothesized transla-
tion using the other three references as ground truth;
these four scores were then averaged. The value of
this upper bound is quite consistent with the bound
computed similarly by Callison-Burch (2009).
5 English-Chinese Evaluation
As we noted in Section 2, the targeted paraphrasing
translation process defines a set of human-machine
combinations that do not require bilingual expertise.
The previous section described human identification
of mistranslated spans on the target side, human gen-
eration of paraphrases for problematic sub-sentential
spans on the source side, and both automatic hypothe-
sis selection and human selection (via fluency ratings,
in Section 3).
In this section, we take a step toward more au-
tomated processing, replacing human identification
of mistranslated spans with an a fully automatic
method.6 The idea behind our automatic error iden-
tification is straightforward: if the source sentence
5?Gains? refer to a lower score: since TERp is an error
measure, lower is better.
6This section contains material we originally reported in
Buzek et al (2010).
132
GT: WTO chief negotiator on behalf of the United States to propose substantial reduction of
agricultural subsidies, Kai Fa countries substantially reduce industrial products import tariffs to Dapo
?? Doha Round of negotiations deadlock.
TP: World Trade Organization negotiator suggested the United States today, a substantial reduction
of agricultural subsidies, developing countries substantially reduce industrial products?? Import
tariffs, in order to break the deadlock in the Doha Round of trade negotiations.
REF: the main delegates at the world trade organization talks today suggested that the us make major
cuts in its agricultural subsidies and that developing countries significantly reduce import duties on
industrial products in order to break the deadlock in the doha round of trade talks .
GT: Emergency session of the Palestinian prime minister Salam Fayyad state will set a new Govern-
ment
TP: Emergency session of the Palestinian Prime Minister Salam Fayyad will set the new government
REF: state of emergency period ends ; palestinian prime minister fayyad to form new government
GT: Indian territory from south to north, one week before the start after another wet season, the
provincial residents hold long drought every rain in the mood to meet the heavy rain, but did not
expect rain came unexpectedly fierce, a rain disaster, roads become rivers, low-lying areas housing to
make Mo in the water, transport almost paralyzed, Zhi Jin statistics about You nearly 500 people due
to floods were killed.
TP: Indian territory from south to north, one week before the start have entered into the rainy season,
provincial residents hold long drought to hope rain in the mood to meet the heavy rain, but did not
feed rain came unexpectedly fierce, a rain disaster, roads change the river, low-lying areas housing
do not water, traffic almost to a standstill, since statistics are nearly 500 people due to floods killed.
REF: the whole of india , from south to north , started to progressively enter the monsoon season a
week ago . the residents of each state all greeted the heavy rains as relief at the end of a long drought
, but didn?t expect that the rain would come with unexpected violence , a real deluge . highways have
become rivers ; houses in low-lying areas have been surbmerged in the water ; the transport system is
nearly paralyzed . to date , figures show that nearly 500 people have unfortunately lost their lives to
the floods .
GT: But the Taliban said in the meantime, the other a German hostages kidnapped in very poor
health, began to fall into a coma and lost consciousness.
TP: But the Taliban said in the meantime, another German hostages kidnapped a very weak body
fell into a coma and began to lose consciousness.
REF: but at the same time the taliban said that another german hostage who had been kidnapped
was in extremely poor health , and had started to become comatose and to lose consciousness .
GT: Taliban spokesman Ahmadi told AFP in an unknown location telephone interview, said: We,
through tribal elders, representatives of direct contact with South Korea.
TP: Taliban spokesman Ahmadi told AFP in an unknown location telephone interview, said: We are
through tribal elders, directly with the South Korean leadership, business
REF: taliban spokesperson ahmadi said in a telephone interview by afp at an undisclosed location :
we have established direct contact with the south korean delegation through tribal elders .
Figure 2: Random sample of 5 items from study in Section 4: original Google translation (GT), results of targeted
paraphrasing translation process (TP), and a human reference translation.
133
is translated to the target and then back-translated, a
comparison of the result with the original is likely to
identify places where the translation process encoun-
tered difficulty.7 Briefly, we automatically translate
source F to target E, then back-translate to produce F?
in the source language. We compare F and F? using
TERp ? which, in addition to its use as an evaluation
metric, is a form of string-edit distance that identifies
various categories of differences between two sen-
tences. When at least two consecutive edits are found,
we flag their smallest containing syntactic constituent
as a potential source of translation difficulty.8
In more detail, we posit that if an area of backtrans-
lation F? has many edits relative to original sentence
F, then that area probably comes from parts of the
target translation that did not represent the desired
meaning in F very well. We only consider consec-
utive edits in certain of the TERp edit categories,
specifically, deletions (D), insertions (I), and shifts
(S); the two remaining categories, matches (M) and
paraphrases (P), indicate that the words are identical
or that the original meaning was preserved. Further-
more, we assume that while a single D, S, or I edit
might be fairly meaningless, a string of at least two of
those types of edits is likely to represent a substantive
problem in the translation.
In order to identify reasonably meaningful para-
phrase units based on potential errors, we rely on a
source language constituency parser. Using the parse,
we find the smallest constituent of the sentence con-
taining all of the tokens in a particular error string. At
times, these constituents can be quite large, even the
entire sentence. To weed out these cases, we restrict
constituent length to no more than 7 tokens.
For example, given
F The most recent probe to visit Jupiter was the
Pluto-bound New Horizons spacecraft in late Febru-
ary 2007.
E La investigacio?n ma?s reciente fue la visita de Ju?piter
a Pluto?n de la envolvente sonda New Horizons a
fines de febrero de 2007.
7Exactly the same insight is behind the ?source-side pseudo-
referencebased feature? employed by Soricut and Echihabi
(2010) in their system for predicting the trustworthiness of trans-
lations.
8It is possible that the difficulty so identified involves back-
translation only, not translation in the original direction. If that
is the case, then more paraphrasing will be done than necessary,
but the quality of the TP process?s output should not suffer.
F? The latest research visit Jupiter was the Pluto-bound
New Horizons spacecraft in late February 2007.
spans in the the bolded phrase in F would be iden-
tified, based on the TERp alignment and smallest
containing constituent as shown in Figure 3.
In order to evaluate this approach, we again use
NIST MT08 data, this time going in the English-
to-Chinese direction since we are assuming source
language resources not currently available for Chi-
nese.9 We used English reference 0 as the source
sentence, and the original Chinese sentence as the
target.10
The data set comprises 1,357 sentence pairs. Us-
ing the above described algorithm to automatically
identify possible problem areas in the translation,
with the Google Translate API providing both the
translation and back-translation, we generated 1,780
potential error spans in 1,006 of the sentences, and,
continuing the targeted paraphrasing process, we ob-
tained up to three source paraphrases per span, for
the problemantic spans in 1,000 of those sentences.
(For six sentences, no paraphrases weres suggested
for any of the problematic spans.) These yielded
full-sentence paraphrase alternatives for the 1,000
sentences, which we again evaluated via an oracle
study.
For this study we used the TER metric (Snover
et al, 2006) rather than TERp. Comparing with the
GT output, we find that TP yields a better-translated
paraphrase sentence is available in 313 of the 1000
cases, or 31.3%, and for those 313 cases, TER for the
oracle-best paraphrase alternative improves on the
TER for the original sentence by 12.16 TER points.
Also taking into account the cases where there is
no improvement over the baseline, the average TER
score improves by 3.8 points. The cost for human
tasks in this study ? just paraphrases, since identi-
fying problematic spans was done automatically ?
was $117.48, or a bit under $0.12 per sentence.
9The Stanford parser (Klein and Manning, 2002), which
we use to identify source syntactic constituents, exists for both
English and Chinese, but TERp uses English resources such as
WordNet in order to capture acceptable variants of expression
for the same meaning. Matt Snover (personal communication) is
working on extension of TERp to other languages.
10We chose reference 0 because on inspection these references
seemed most reflective of native English grammar and usage.
134
NP PP 
NP 
Figure 3: TERp alignment of a source sentence and its back-translation in order to identify a problematic source span.
6 Conclusions and Future Work
In this paper we have focused on a relatively less-
explored space on the spectrum between high quality
and low cost translation: sharing the burden of the
translation task among a fully automatic system and
monolingual human participants, without requiring
human bilingual expertise. The monolingual par-
ticipants in this framework perform straightforward
tasks: they identify parts of sentences in their lan-
guage that seem to have errors, they provide sub-
sentential paraphrases in context, and they judge the
fluency of sentences they are presented with (or, in a
variant still to be explored, they simply select which
target sentence they like the best). Unlike other pro-
posals for exploiting monolingual speakers in human-
machine collaborative translation, the human steps
here are amenable to automation, and in addition
to evaluating a mostly-human variant of our targeted
paraphrasing translation framework, we also assessed
a version in which the identification of mistranslated
spans (to be paraphrased) is done automatically.
Our experimentation yielded a consistent pattern
of results, supporting the conclusion that targeted
paraphrasing can lead to significant improvements
in translation, via several different measures. First,
a very small pilot study for Chinese-English trans-
lation in Wikipedia provided preliminary validation
that translation fluency and accuracy can be improved
quite significantly for a set of fairly chosen test sen-
tences, according to human ratings. Second, a small
experiment in Chinese-English translation using stan-
dard NIST test sentences suggested the potential for
dramatic gains using the BLEU and TERp scores,
with oracle improvements of 2.46 points and 5.46
points, respectively. In addition, a non-oracle experi-
ment, selecting the best hypothesis according to the
MT system?s model score, yielded a gain of nearly 1.7
BLEU points. And third, in a large scale evaluation
of the approach using English-Chinese translation
of 1,000 sentences, this time automating the step of
identifying potentially mistranslated parts of source
sentences, the oracle results demonstrated that a gain
of nearly 4 TER points is available.
These initial studies leave considerable room for
future work. One important step will be to better char-
acterize the relationship between cost and quality in
quantitative terms: how much does it cost to obtain
135
how much quality improvement, and how does that
compare with typical professional translation costs of
$0.25 per word? This question is closely connected
with the dynamics of crowdsourcing platforms such
as Mechanical Turk ? the cost per sentence in these
experiments works out to be around $0.12, but trans-
lation on a large scale will involve a complicated
ecosystem of workers and cheaters, tasks and motiva-
tions and incentives (Quinn and Bederson, 2009). A
related crowdsourcing issue requiring further study
is the availability of monolingual human participants
for a range of language pairs, in order to validate
the argument that drawing on monolingual human
participation will significantly reduce the severity of
the availability bottleneck. And, of course, in the
upper bound in Table 1 makes quite clear the cru-
cial value added by bilingual translators, when they
are available; we hope to explore whether the tar-
geted paraphrasing translation pipeline can improve
the productivity of post-editing by bilinguals, mak-
ing it easier to move toward the upper bound in a
cost-effective way.
Another set of issues concerns the underlying trans-
lation technology. A reviewer correctly notes that the
value of the approach taken here is likely to vary
depending upon the quality of the underlying trans-
lation system, and the approach may break down at
the extrema, when the baseline translation is either
already very good or completely awful. We chose
to use Google Translate for its wide availability and
the fact that it represents a state of the art baseline to
beat; however, in future work we plan to substitute
our own statistical MT systems, which will permit us
to experiment across a range of translation model and
language model LM training set sizes, and therefore
to vary quality while keeping other system details
constant. More directly connected to research in ma-
chine translation, this framework provides a variety
of opportunities for improving fully automatic sta-
tistical MT systems. We plan to implement a fully
automatic targeted paraphrasing translation pipeline,
using the automated methods discussed when intro-
ducing the pipeline in Section 2, including transla-
tion of targeted paraphrase lattices (cf. (Max, 2010;
Du et al, 2010)). Finally, we intend to explore the
application of our approach in scenarios involving
less-common languages, by using a more common
language as a pivot or bridge (Habash and Hu, 2009).
Acknowledgments
This work has been supported in part by the National
Science Foundation under awards BCS0941455 and
IIS0838801. The authors would like to thank three
anonymous reviewers for their helpful comments,
and Chris Callison-Burch and Chris Dyer for their
helpful comments and discussion.
References
Joshua S. Albrecht, Rebecca Hwa, and G. Elisabeta Marai.
2009. Correcting automatic translations through collab-
orations between mt and monolingual target-language
users. In EACL ?09: Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 60?68, Morristown,
NJ, USA. Association for Computational Linguistics.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Benjamin B. Bederson, Chang Hu, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Graphics Interface (GI) confer-
ence.
Lynne Bowker and Michael Barlow. 2004. Bilingual
concordancers and translation memories: a comparative
evaluation. In LRTWRT ?04: Proceedings of the Second
International Workshop on Language Resources for
Translation Work, Research and Training, pages 70?79,
Morristown, NJ, USA. Association for Computational
Linguistics.
Olivia Buzek, Philip Resnik, and Ben Bederson. 2010. Er-
ror driven paraphrase annotation using mechanical turk.
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 217?221, Los Angeles, June.
Association for Computational Linguistics.
Chris Callison-Burch, Colin Bannard, , and Josh
Schroeder. 2004. Improving statistical translation
through editing. In Workshop of the European Associa-
tion for Machine Translation.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Mechan-
ical Turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 286?295, Singapore, August. Association for
Computational Linguistics.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
translation using source language paraphrase lattices.
136
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Chris Dyer and Philip Resnik. 2010. Forest translation.
In NAACL?10.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proceedings of HLT-ACL,
Columbus, OH.
C. Dyer. 2007. Noisier channel translation: translation
from morphologically complex languages. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, Prague, June.
Jose? Esteban, Jose? Lorenzo, Antonio S. Valderra?banos,
and Guy Lapalme. 2004. Transtype2 - an innovative
computer-assisted translation system. In The Compan-
ion Volume to the Proceedings of 42st Annual Meeting
of the Association for Computational Linguistics, pages
94?97, Barcelona, Spain, jul. Association for Computa-
tional Linguistics. TT2.
Nizar Habash and Jun Hu. 2009. Improving arabic-
chinese statistical machine translation using english
as pivot language. In StatMT ?09: Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 173?181, Morristown, NJ, USA. Association for
Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325.
Shahram Khadivi, Richard Zens, and Hermann Ney. 2006.
Integration of speech to computer-assisted translation
using finite-state automata. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
467?474, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural lan-
guage parsing. In Suzanna Becker, Sebastian Thrun,
and Klaus Obermayer, editors, Advances in Neural
Information Processing Systems 15 - Neural Informa-
tion Processing Systems, NIPS 2002, pages 3?10. MIT
Press.
Philipp Koehn. 2009. A web-based interactive computer
aided translation tool. In Proceedings of the ACL-
IJCNLP 2009 Software Demonstrations, pages 17?20,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Anne-Marie Laurian. 1984. Machine translation : What
type of post-editing on what type of documents for
what type of users. In 10th International Conference on
Computational Linguistics and 22nd Annual Meeting
of the Association for Computational Linguistics.
Aure?lien Max. 2009. Sub-sentencial paraphrasing by con-
textual pivot translation. In Proceedings of the 2009
Workshop on Applied Textual Inference, pages 18?26,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Aure?lien Max. 2010. Example-based paraphrasing for
improved phrase-based statistical machine translation.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Daisuke Morita and Toru Ishida. 2009. Designing pro-
tocols for collaborative translation. In PRIMA ?09:
Proceedings of the 12th International Conference on
Principles of Practice in Multi-Agent Systems, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Robert Munro. 2010. Haiti emergency response: the
power of crowdsourcing and SMS. Relief 2.0 in Haiti,
Stanford, CA.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alexander Fraser,
Shankar Kumar, Libin Shen, David Smith, Katherine
Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev.
2004. A smorgasbord of features for statistical ma-
chine translation. In HLT-NAACL, pages 161?168.
Alex Quinn and Benjamin B. Bederson. 2009. A tax-
onomy of distributed human computation. Technical
Report HCIL-2009-23, University of Maryland, Octo-
ber.
D. Shahaf and E. Horvitz. 2010. Generalized task markets
for human and machine computation. In AAAI 2010,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2009. TER-Plus: Paraphrases, Semantic,
and Alignment Enhancements to Translation Edit Rate.
Machine Translation.
Radu Soricut and Abdessamad Echihabi. 2010. Trustrank:
Inducing trust in automatic translations via ranking. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 612?621,
Uppsala, Sweden, July. Association for Computational
Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In HLT ?01:
Proceedings of the first international conference on
Human language technology research, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
137
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 217?221,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Error Driven Paraphrase Annotation using Mechanical Turk
Olivia Buzek
Computer Science and Linguistics
University of Maryland
College Park, MD 20742, USA
olivia.buzek@gmail.com
Philip Resnik
Linguistics and UMIACS
University of Maryland
College Park, MD 20742, USA
resnik@umd.edu
Benjamin B. Bederson
Computer Science and HCIL
University of Maryland
College Park, MD 20742, USA
bederson@cs.umd.edu
Abstract
The source text provided to a machine translation
system is typically only one of many ways the input
sentence could have been expressed, and alternative
forms of expression can often produce a better trans-
lation. We introduce here error driven paraphras-
ing of source sentences: instead of paraphrasing a
source sentence exhaustively, we obtain paraphrases
for only the parts that are predicted to be problematic
for the translation system. We report on an Amazon
Mechanical Turk study that explores this idea, and
establishes via an oracle evaluation that it holds the
potential to substantially improve translation quality.
1 Introduction
The source text provided to a translation system is typ-
ically only one of many ways the input sentence could
have been expressed, and alternative forms of expression
can often produce better translation. This observation is
familiar to most statistical MT researchers in the form of
preprocessing choices ? for example, one segmentation
of a Chinese sentence might yield better translations than
another.1 Over the past several years, MT frameworks
have been developed that permit all the alternatives to be
used as input, represented efficiently as a confusion net-
work, lattice, or forest, rather than forcing selection of
a single input representation. This has improved perfor-
mance when applied to phenomena including segmenta-
tion, morphological analysis, and more recently source
langage word order (Dyer, 2007; Dyer et al, 2008; Dyer
and Resnik, to appear).
We have begun to explore the application of the same
key idea beyond low-level processing phenomena such
as segmentation, instead looking at alternative expres-
sions of meaning. For example, consider translating The
1Chinese is written without spaces, so most MT systems need to
segment the input into words as a preprocessing step.
Democratic candidates stepped up their attacks during
the debate. The same basic meaning could have been ex-
pressed in many different ways, e.g.:
? During the debate the Democratic candidates
stepped up their attacks.
? The Democratic contenders ratcheted up their at-
tacks during the debate.
? The Democratic candidates attacked more aggres-
sively during the debate.
? The candidates in the Democratic debate attacked
more vigorously.
These examples illustrate lexical variation, as well as syn-
tactic differences, e.g. whether the attacking or the in-
creasing serves as the main verb. We hypothesize that
variation of this kind holds a potential advantage for
translation systems, namely that some variations may be
more easily translated than others depending on the train-
ing data that was given to the system, and we can im-
prove translation quality by allowing a system to take best
advantage of the variations it knows about, at the sub-
sentential level, just as the systems described above can
take advantage of alternative segmentations.
Paraphrase lattices provide a way to make this hypoth-
esis operational. This idea is a variation on the uses of
paraphrase in translation introduced by Callison-Burch
and explored by others, as well (Callison-Burch et al,
2006; Madnani et al, 2007; Callison-Burch, 2008; Mar-
ton et al, 2009). These authors have shown that perfor-
mance improvements can be gained by exploiting para-
phrases using phrase pivoting. We have investigated us-
ing pivoting to create exhaustive paraphrase lattices, and
we have also investigated defining upper bounds by elic-
iting human sub-sentential paraphrases using Mechani-
cal Turk. Unfortunately, in both cases, we have found
the size of the paraphrase lattice prohibitive: there are
217
too many spans to paraphrase to make using Turk cost-
effective, and automatically generated paraphrase lattices
turn out to be too noisy to produce improved translations.
A potential solution to this problem comes from a dif-
ferent line of work we are pursuing, in which translation
is viewed as a collaborative process involving people and
machines (Bederson et al, 2010). Here, the idea is that
in translating from a source to a target language, source-
and target-language speakers who are not bilingual can
collaborate to improve the quality of automatic transla-
tion, via an iterative protocol involving translation, back
translation, and the use of a very rich user interface. For
example, consider the following translation from English
to French by an automatic MT system:
? Source: Polls indicate Brown, a state senator,
and Coakley, Massachusetts? Attorney General, are
locked in a virtual tie to fill the late Sen. Ted
Kennedy?s Senate seat.
? System: Les sondages indiquent Brown,
un se?nateur d?e?tat, et Coakley,
Massachusetts? Procureur ge?ne?ral, sont enferme?s
dans une cravate virtuel a` remplir le regrette?
se?nateur Ted Kennedy?s sie`ge au Se?nat.
Someone with only a semester of college French (one of
the authors) can look at this automatic translation, and
see that the underlined parts are probably wrong. Chang-
ing the source sentence to rephrase the underlined pieces
(e.g. changing Massachusetts? Attorney General to the
Attorney General of Massachusetts), we obtain a transla-
tion that is still imperfect but is more acceptable:
? System: Les sondages indiquent que Brown, un
se?nateur d?e?tat, et Coakley, le procureur ge?ne?ral
du Massachusetts, sont enferme?s dans une cravate
virtuel pourvoir le sige au Se?nat de Sen. Ted
Kennedy, qui est de?ce?de? re?cemment.
One could imagine (and, indeed, we are building) a vi-
sual interface that allows a human participant on the tar-
get side to communicate back to a source-side collabora-
tor, in effect saying, ?These underlined pieces look like
they were translated poorly; can you rephrase the rele-
vant parts of your sentence, and perhaps that will lead to
a better translation??2
Putting these ideas together ? source paraphrase and
identification of difficult regions of input for translation
? we arrive at the idea of error driven paraphrasing of
source sentences: instead of paraphrasing to introduce as
much variation as possible everywhere in the sentence,
we suggest that instead it makes sense to paraphrase only
2Communicating which parts of the sentence are relevant across lan-
guages is being done via projection across languages using word align-
ments; cf. (Hwa et al, 2001).
the parts of a source sentence that are problematic for the
translation system. In Section 2 we give a first-pass algo-
rithm for error driven paraphrasing, in Section 3 we de-
scribe how this was realized using MTurk, and Sections 4
and 5 provide an oracle evaluation, discussion, and con-
clusions.
2 Identifying source spans with errors
In error driven paraphrasing, the key idea is to focus on
source spans that are likely to be problematic for trans-
lation. Although in principle one could use human feed-
back from the target side to identify relevant spans, in
this paper we begin with an automatic approach, auto-
matically identifying that are likely to be incorrect via
a novel algorithm. Briefly, we automatically translate
source F to target E, then back-translate to produce F? in
the source language. We compare F and F? using TERp
(Snover et al, 2009), a form of string-edit distance that
identifies various categories of differences between two
sentences, and when at least two consecutive non-P (non-
paraphrase) edits are found, we flag their smallest con-
taining syntactic constituent.
In more detail, we posit that areas of F? where there
were many edits from F will correspond to areas in where
the target translation did not match the English very well.
Specifically, deletions (D), insertions (I), and shifts (S)
are likely to represent errors, while matches (M) and
paraphrases (P) probably represent a fairly accurate trans-
lation. Furthermore, we assume that while a single D, S,
or I edit might be fairly meaningless, a string of at least 2
of those types of edits is likely to represent a substantive
problem in the translation.
In order to identify reasonably meaningful paraphrase
units based on potential errors, we rely on a source lan-
guage constituency parser. Using the parse, we find the
smallest constituent of the sentence containing all of the
tokens in a particular error string. At times, these con-
stituents can be quite large, even the entire sentence. To
weed out these cases, we restrict constituent length to no
more than 7 tokens.
For example, given
F The most recent probe to visit Jupiter was the Pluto-
bound New Horizons spacecraft in late February 2007.
E La investigacio?n ma?s reciente fue la visita de Ju?piter a
Pluto?n de la envolvente sonda New Horizons a fines de
febrero de 2007.
F? The latest research visit Jupiter was the Pluto-bound New
Horizons spacecraft in late February 2007.
spans in the the bolded phrase in F would be identified,
based on the TERp alignment and smallest containing
constituent as shown in Figure 1.
218
NP PP 
NP 
Figure 1: TERp alignment of a source sentence and its back-translation in order to identify a problematic source span.
3 Error driven paraphrasing on MTurk
We chose to use translation from English to Chinese in
this first foray into Mechanical Turk for error driven para-
phrase. This made sense for a number of reasons: first,
because we expected to have a much easier time finding
Turkers; second, because we could make use of a high
quality English parser (in this case the Stanford parser);
and, third, because it meant that we as researchers could
easily read and judge the quality of Turkers? paraphrases.
To create an English-to-Chinese data set, we used the
Chinese-to-English data from the MT08 NIST machine
translation evaluation. We used English reference 0 as
the source sentence, and the original Chinese sentence as
the target. We chose reference 0 because on inspection
these references seemed most reflective of native English
grammar and usage. The data set comprises 1357 sen-
tence pairs. Using the the above described algorithm to
identify possible problem areas in the translation, with
the Google Translate API providing both the translation
and back-translation, we generated 1780 potential error
regions in 1006 of the sentences. Then we created HITs
both to obtain paraphrases, and to validate the quality of
paraphrase responses. Costs were $117.48 for obtaining
multiple paraphrases, and $44.06 for verification.
3.1 Obtaining paraphrases
Based on the phrases marked as problematic by our algo-
rithm, we created HITs asking for paraphrases within 5
sentences, as illustrated in Figure 2. Workers were given
60 minutes to come up with a single paraphrase for each
of the five indicated problematic regions, for a reward of
$0.10. If a worker felt they could not come up with an
alternate phrasing for the marked phrase, they had the
option of marking an ?Unable to paraphrase? checkbox.
We assigned each task to 3 workers, resulting in 3 para-
phrases for every marked phrase. From the 1780 errors,
we got 5340 responses. Of these, 4821 contained actual
paraphrase data, while the rest of the responses indicated
an inability to paraphrase, via the checkbox response. All
paraphrases were passed on to the verification phase.
3.2 Paraphrase Verification
In the verification phase, we generated alternative full
sentences based on the 4821 paraphrases. Workers were
shown an original sentence F and asked to compare it to at
most 5 alternatives, with a maximum of 20 comparisons
made in a HIT. (Recall that although F is the conven-
tional notation for source sentences in machine transla-
tion, in this study the F sentences are in English.) Re-
sponses were given in the form of radio buttons, mark-
ing ?Yes? for an alternate sentence if workers felt it was
grammatical and accurately reflected the content of the
219
original sentence, or ?No? if it did not meet both of those
criteria. Workers were given 30 minutes to make their
decisions, for a reward of $0.05. This task was also as-
signed to 3 workers, resulting in 3 judgments for every
paraphrase.
4 Evaluating Results
Using the paraphrase results from Mechanical Turk, we
constructed rephrased full sentences for every combina-
tion of paraphrase alternatives. For example, if a sentence
had 2 sub-spans paraphrased, and the two sub-spans had 2
and 3 unique paraphrasings, respectively, we would con-
struct 2 ? 3 = 6 alternative full sentences. From the
1780 predicted problematic phrases (within the 1006 au-
tomatically identified sentences with possible translation
errors), we generated 14,934 rephrased sentences. Each
rephrased English sentence was translated into a Chinese
sentence, again via the Google Translate API. We then
evaluated results for translation of the original sentences,
and of all their paraphrase alternatives, via the TER met-
ric, using the MT08 original Chinese sentence as the
target-language reference translation. The evaluation set
includes the 1000 sentence where at least one paraphrase
was provided.3
Our evaluation takes the form of an oracle study: if
we knew with perfect accuracy which variant of a sen-
tence to translate, i.e. among the original and all its para-
phrases, based on knowledge of the reference translation,
how well could we do? An ?oracle? telling us which vari-
ant is best is not available in the real world, of course, but
in situations like this one, oracle studies are often used
to establish the magnitude of the potential gain (Och et
al., 2004). In this case, the baseline is the average TER
score for the 1000 original sentences, 84.4. If an ora-
cle were permitted to choose which variant was the best
to translate, the average TER score would drop to 80.6.4
Drilling down a bit further, we find that a better-translated
paraphrase sentence is available in 313 of the 1000 cases,
or31.3%, and for those 313 cases, TER for the best para-
phrase alternative improves on the TER for the original
sentence by 12.16 TER points.
5 Conclusions
This annotation effort has produced gold standard sub-
sentential paraphrases and paraphrase quality ratings for
spans in a large number of sentences, where the choice
of spans to paraphrase is specifically focused on regions
of the sentence that are difficult to translate. In addi-
3For the other 6 sentences, all problematic spans were marked ?Un-
able to paraphrase? by all 3 MTurkers.
4TER measures errors, so lower is better. A reduction in TER of 3.8
for an MT evaluation dataset would be considered quite substantial; a
reduction of 1 point would typically be a publishable result.
tion, we have performed an initial analysis, using human-
generated paraphrases to provide an oracle evaluation of
how much could be gained in translation by translating
paraphrases of problematic regions in the source sen-
tence. The results suggest if paraphrasing is automati-
cally targeted to problematic source spans using a back-
translation comparison, good paraphrases of the problem-
atic spans could improve translation performance quite
substantially.
In future work, we will use a translation system sup-
porting lattice input (Dyer et al, 2008), rather than the
Google Translation API, in order to take advantage of
fully automatic error-driven paraphrasing, using pivot-
based approaches (e.g. (Callison-Burch et al, 2006)) to
complete the automation of the error-driven paraphrase
process. We will also investigate the use of human rather
than machine identification of likely translation prob-
lems, in the context of collaborative translation (Beder-
son et al, 2010).
References
Benjamin B. Bederson, Chang Hu, and Philip Resnik. 2010. Trans-
lation by iterative collaboration between monolingual users. In
Graphics Interface (GI) conference.
Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006.
Improved statistical machine translation using paraphrases. In
Robert C. Moore, Jeff A. Bilmes, Jennifer Chu-Carroll, and Mark
Sanderson, editors, HLT-NAACL. The Association for Computa-
tional Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on paraphrases ex-
tracted from parallel corpora. In EMNLP, pages 196?205. ACL.
Chris Dyer and Philip Resnik. to appear. Forest translation. In
NAACL?10.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing word lattice
translation. In Proceedings of HLT-ACL, Columbus, OH.
C. Dyer. 2007. Noisier channel translation: translation from morpho-
logically complex languages. In Proceedings of the Second Work-
shop on Statistical Machine Translation, Prague, June.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2001.
Evaluating translational correspondence using annotation projection.
In ACL ?02: Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 392?399, Morristown, NJ,
USA. Association for Computational Linguistics.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie Dorr.
2007. Using paraphrases for parameter tuning in statistical ma-
chine translation. In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 120?127, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Im-
proved statistical machine translation using monolingually-derived
paraphrases. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 381?390, Singa-
pore, August. Association for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar,
Kenji Yamada, Alexander Fraser, Shankar Kumar, Libin Shen, David
Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev.
2004. A smorgasbord of features for statistical machine translation.
In HLT-NAACL, pages 161?168.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz.
2009. TER-Plus: Paraphrases, Semantic, and Alignment Enhance-
ments to Translation Edit Rate. Machine Translation.
220
Figure 2: HIT format 1: Obtaining sub-sentential paraphrases. Note that as the MTurker types a paraphrase into the box,what
is typed appears immediately (character by character) in the full-sentence context under ?New sentence?, so that they can see
immediately how the entire sentence looks with their paraphrase.
the press trust of india quoted
the government minister for relief and rehabilitation kadam
kadam, the governments relief and rehabilitation minister (2/3)
the government minister concerned with relief and rehabiliation kadam (1/3)
as revealing today that in the last week, the monsoon has started in
all of indias states one
every one of indias state, one (3/3)
each of Indias states one (2/3)
all states of india one (1/3)
after another, and that the financial losses and casualties have been serious in all areas. just in maharashtra, the state which
includes
mumbai, indias largest city,
india?s largest city, mumbai (3/3)
the largest city in India, Mumbai, (3/3)
mumbai, the largest city of india, (3/3)
the number of people
known to have died
who died (3/3)
identified to have died (2/3)
known to have passed away (2/3)
has now reached 358.
Figure 3: Example of error-driven paraphrases produced via HIT format 1, above, for a single sentence. The paraphrase spans
(indented) are shown with the number of MTurkers, out of 3, who labeled that paraphrase in context as acceptable using a ?vali-
dation? HIT.
221
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 399?404,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Value of Monolingual Crowdsourcing in a Real-World Translation
Scenario: Simulation using Haitian Creole Emergency SMS Messages
Chang Hu?, Philip Resnik??, Yakov Kronrod?
Vladimir Eidelman?, Olivia Buzek??, Benjamin B. Bederson?
?UMIACS and Department of Linguistics
?UMIACS and Department of Computer Science
University of Maryland, College Park
{changhu,bederson}@cs.umd.edu
{resnik,vlad,buzek}@umiacs.umd.edu
yakov@umd.edu
Abstract
MonoTrans2 is a translation system that com-
bines machine translation (MT) with human
computation using two crowds of monolin-
gual source (Haitian Creole) and target (En-
glish) speakers. We report on its use in the
WMT 2011 Haitian Creole to English trans-
lation task, showing that MonoTrans2 trans-
lated 38% of the sentences well compared to
Google Translate?s 25%.
1 Introduction
One of the most remarkable success stories to come
out of the January 2010 earthquake in Haiti in-
volved translation (Munro, 2010). While other
forms of emergency response and communication
channels were failing, text messages were still get-
ting through, so a number of people came together to
create a free phone number for emergency text mes-
sages, which allowed earthquake victims to report
those who were trapped or in need of medical atten-
tion. The problem, of course, was that most people
were texting in Haitian Creole (Kreyol), a language
not many of the emergency responders understood,
and few, if any, professional translators were avail-
able. The availability of usable translations literally
became a matter of life and death.
In response to this need, Stanford University grad-
uate student Rob Munro coordinated the rapid cre-
ation of a crowdsourcing framework, which allowed
volunteers ? including, for example, Haitian expa-
triates and French speakers ? to translate messages,
providing responders with usable information in as
little as ten minutes. Translations may not have been
perfect, but to a woman in labor, it had to have made
a big difference for English-speaking responders to
see Undergoing children delivery Delmas 31 instead
of Fanm gen tranche pou fe` yon pitit nan Delmas 31.
What about a scenario, though, in which even am-
ateur bilingual volunteers are hard to find, or too
few in number? What about a scenario, e.g. the
March 2011 earthquake and tsunami in Japan, in
which there are many people worldwide who wish
to help but are not fluent in both the source and tar-
get languages?
For the last few years, we have been exploring the
idea of monolingual crowdsourcing for translation
? that is, technology-assisted collaborative transla-
tion involving crowds of participants who know only
the source or target language (Buzek et al, 2010;
Hu, 2009; Hu et al, 2010; Hu et al, 2011; Resnik
et al, 2010). Our MonoTrans2 framework has pre-
viously shown very promising results on children?s
books: on a test set where Google Translate pro-
duced correct translations for only 10% of the input
sentences, monolingual German and Spanish speak-
ers using our framework produced translations that
were fully correct (as judged by two independent
bilinguals) nearly 70% of the time (Hu et al, 2011).
We used the same framework in the WMT 2011
Haitian-English translation task. For this experi-
ment, we hired Haitian Creole speakers located in
Haiti, and recruited English speakers located in the
U.S., to serve as the monolingual crowds.
2 System
MonoTrans2 is a translation system that combines
machine translation (MT) with human computation
(Quinn et al, 2011) using two ?crowds? of mono-
lingual source (Haitian Creole) and target (English)
399
speakers.1 We summarize its operation here; see Hu
et al (2011) for details.
The Haitian Creole sentence is first automatically
translated into English and presented to the English
speakers. The English speakers then can take any of
the following actions for candidate translations:
? Mark a phrase in the candidate as an error
? Suggest a new translation candidate
? Vote candidates up or down
Identifying likely errors and voting for candidates
are things monolinguals can do reasonably well:
even without knowing the intended interpretation,
you can often identify when some part of a sentence
doesn?t make sense, or when one sentence seems
more fluent or plausible than another. Sometimes
rather than identifying errors, it is easier to suggest
an entirely new translation candidate based on the
information available on the target side, a variant
of monolingual post-editing (Callison-Burch et al,
2004).
Any new translation candidates are then back-
translated into Haitian Creole, and any spans marked
as translation errors are projected back to identify
the corresponding spans in the source sentence, us-
ing word alignments as the bridge (cf. Hwa et al
(2002), Yarowsky et al (2001)).2 The Haitian Cre-
ole speakers can then:
? Rephrase the entire source sentence (cf.
(Morita and Ishida, 2009))
? ?Explain? spans marked as errors
? Vote candidates up or down (based on the back-
translation)
Source speakers can ?explain? error spans by of-
fering a different way of phrasing that piece of the
source sentence (Resnik et al, 2010), in order to
produce a new source sentence, or by annotating the
spans with images (e.g. via Google image search)
or Web links (e.g. to Wikipedia). The protocol then
continues: new source sentences created via partial-
1For the work reported here, we used Google Translate as
the MT component via the Google Translate Research API.
2The Google Translate Research API provides alignments
with its hypotheses.
or full-sentence paraphrase pass back through MT
to the English side, and any explanatory annota-
tions are projected back to the corresponding spans
in the English candidate translations (where the er-
ror spans had been identified). The process is asyn-
chronous: participants on the Haitian Creole and
English sides can work independently on whatever
is available to them at any time. At any point, the
voting-based scores can be used to extract a 1-best
translation.
In summary, the MonoTrans2 framework uses
noisy MT to cross the language barrier, and supports
monolingual participants in doing small tasks that
gain leverage from redundant information, the hu-
man capacity for linguistic and real-world inference,
and the wisdom of the crowd.
3 Experiment
We recruited 26 English speakers and 4 Haitian Cre-
ole speakers. The Haitian Creole speakers were re-
cruited from Haiti and do not speak English. Five of
the 26 English speakers were paid UMD undergrad-
uates; the other 21 were volunteer researchers, grad-
uate students, and staff unrelated to this research. 3
Over a 13 day period, Haitian Creole and English
speaker efforts totaled 15 and 29 hours, respectively.
4 Data Sets
Our original goal of fully processing the entire SMS
clean test and devtest sets could not be realized in the
available time, owing to unanticipated reshuffling of
the data by the shared task organizers and logistical
challenges working with participants in Haiti. Ta-
ble 1 summarizes the data set sizes before and after
reshuffling. We put 1,224 sentences from the pre-
before after
test 1,224 1,274
devtest 925 900
Table 1: SMS clean data sets before and after reshuffling
reshuffling test set, interspersed with 123 of the 925
sentences from the pre-reshuffling devtest set, into
the system ? 1,347 sentences in total. We report
3These, obviously, did not include any of the authors.
400
results on the union of pre- and post-reshuffling de-
vtest sentences (Set A, |A| = 1516), and the post-
reshuffling test set (Set B, |B| = 1274 ).
5 Evaluation
Of the 1,347 sentences available for processing in
MonoTrans2, we define three subsets:
? Touched: Sentences that were processed by at
least one person (657 sentences)
? Each-side: Sentences that were processed by at
least one English speaker followed by at least
one Haitian Creole speaker (431 sentences)
? Full: Sentences that have at least three trans-
lation candidates, of which the most voted-for
one received at least three votes (207 sentences)
We intersect these three sets with sets A and B in or-
der to evaluate MonoTrans2 output against the pro-
vided references (Table 2).4
Set S |S| |S ?A| |S ?B|
Touched 657 162 168
Each-side 431 127 97
Full 207 76 60
Table 2: Data sets for evaluation and their sizes
Tables 3 and 4 report two automatic scoring met-
rics, uncased BLEU and TER, comparing Mono-
Trans2 (M2) against Google Translate (GT) as a
baseline.
Set Condition BLEU TER
Touched ?A
GT 21.75 56.99
M2 23.25 57.27
Each-side ?A
GT 21.44 57.51
M2 21.47 58.98
Full ?A
GT 25.05 54.15
M2 27.59 52.78
Table 3: BLEU and TER results for different levels of com-
pletion on the devtest set A
Since the number of sentences in each evaluated
set is different (Table 2), we cannot directly compare
4Note that according to these definitions, Touched contains
both Each-side and Full, but Each-side does not contain Full.
Set Condition BLEU TER
Touched ?B
GT 19.78 59.88
M2 24.09 58.15
Each-side ?B
GT 21.15 56.88
M2 23.80 57.19
Full ?B
GT 22.51 54.51
M2 28.90 52.22
Table 4: BLEU and TER results for different levels of com-
pletion on the test set B
scores between the sets. However, Table 4 shows
that when the MonoTrans2 process is run on test
items ?to completion?, in the sense defined by ?Full?
(i.e. Full?B), we see a dramatic BLEU gain of 6.39,
and a drop in TER of 2.29 points. Moreover, even
when only target-side or only source-side monolin-
gual participation is available we see a gain of 4.31
BLEU and a drop of 1.73 TER points (Touched?B).
By contrast, the results on the devtest data are en-
couraging, but arguably mixed (Table 3). In order to
step away from the vagaries of single-reference au-
tomatic evaluations, therefore, we also conducted an
evaluation based on human judgments. Two native
English speakers unfamiliar with the project were
recruited and paid for fluency and adequacy judg-
ments: for each target translation paired with its cor-
responding reference, each evaluator rated the tar-
get sentence?s fluency and adequacy on a 5-point
scale, where fluency of 5 indicates complete fluency
and adequacy of 5 indicates complete preservation
of meaning (Dabbadie et al, 2002).5
Sentences N Google MonoTrans2
Full ?A 76 18 (24%) 30 (39%)
Full ?B 60 15 (25%) 23 (38%)
Table 5: Number of sentences with maximum possible
adequacy (5) in Full ?A and Full ?B, respectively.
Similar to Hu et al (2011), we adopt the very con-
servative criterion that a translation output is consid-
ered correct only if both evaluators independently
give it a rating of 5. Unlike Hu et al (2011), for
whom children?s book translation requires both flu-
ency and adequacy, we make this a requirement only
5Presentation order was randomized.
401
for adequacy, since in this scenario what matters to
aid organizations is not whether a translation is fully
fluent, but whether it is correct. On this criterion,
the Google Translate baseline of around 25% cor-
rect improves to around 40% for Monotrans, con-
sistently for both the devtest and test data (Table 5).
Nonetheless, Figures 1 and 2 make it clear that the
improvements in fluency are if anything more strik-
ing.
5.1 Statistical Analysis
Variable Adequacy Fluency
Positive
mostSingleCandidateVote ** ***
candidateCount ** **
numOfAnswers * NS
Negative
roundTrips *** ***
voteCount * .
Table 6: Effects of independent variables in linear regres-
sion for 330 touched sentences
(Signif. codes: ?***? 0.001, ?**? 0.01, ?*? 0.05, ?.? 0.1)
In addition to the main evaluation, we investi-
gated the relationship between tasks performed in
the MonoTrans2 system and human judgments us-
ing linear regression and an analysis of variance.
We evaluate the set of all 330 touched sentences in
Touched?A and Touched?B in order to under-
stand which properties of the MonoTrans2 process
correlate with better translation outcomes.
Our analysis focused on improvement over the
Google Translate baseline, looking specifically at
the improvement based on the human evaluators? av-
eraged fluency and adequacy scores.
Table 6 summarizes the positive and negative
effects for five of six variables we considered that
came out significant for at least one of the measures.
6
The positive results were as expected. Having
more votes for the winning candidate (mostSingle-
CandidateVote) made it more successful, since this
means that more people felt it was a good represen-
tative translation. Having more candidates to choose
6A sixth, numOfVoters, was not significant in the linear re-
gression for either adequacy or fluency.
from (candidateCount) meant that more people had
taken the time to generate alternatives, reflecting at-
tention paid to the sentence. Also, the amount of
attention paid to target speakers? requests for clarifi-
cation (numOfAnswers) is as expected related to the
adequacy of the final translation, and perhaps as ex-
pected does not correlate with fluency of the output
since it helps with meaning and not actual target-side
wording.
We were, however, confused at first by the neg-
ative influence of the roundTrips measure and vote-
Count measures. We conjecture that the first effect
arises due to a correlation between roundTrips and
translation difficulty; much harder sentences would
have led to many more paraphrase requests, and
hence to more round trips. We attempted to inves-
tigate this hypothesis by testing correlation with a
naive measure of sentence difficulty, length, but this
was not fruitful. We suspect that inspecting use of
abbreviations, proper nouns, source-side mistakes,
and syntactic complexity would give us more insight
into this issue.
As for voteCount, the negative correlation is un-
derstandable when considered side by side with
the other vote-based measure, mostSingleCandidat-
eVote. Having a higher number of votes for the win-
ning candidate leads to improvement (strongly sig-
nificant for both adequacy and fluency), so a higher
general vote count means that people were also vot-
ing more times for other candidates. Hence, once the
positive winning vote count is taken into account,
the remaining votes actually represent disagreement
on the candidates, hence correlating negatively with
overall improvement over baseline.
It is important to note that when these measures
are all considered together, they show that there is a
clear correlation between the MonoTrans2 system?s
human processing and the eventual increase in both
quality and fluency of the sentences. As people give
more attention to sentences, these sentences show
better performance, as judged by increase over base-
line.
6 Discussion
Our experiment did not address acquisition of, and
incentives for, monolingual participants. In fact, get-
ting time from Haitian Creole speakers, even for pay,
402
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?s
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(a) Fluency Distribution
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?S
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(b) Adequacy Distribution
Figure 1: Human judgments for fluency and adequacy in fully processed devtest items (Full ?A)
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?s
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(a) Fluency Distribution
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?S
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(b) Adequacy Distribution
Figure 2: Human judgments for fluency and adequacy in fully processed test items (Full ?B)
created a large number of logistical challenges, and
was a contributing factor as to why we did not obtain
translations for the entire test set. However, avail-
ability of monolingual participants is not the issue
being addressed in this experiment: we are confi-
dent that in a real-world scenario like the Haitian
or Japanese earthquakes, large numbers of monolin-
gual volunteers would be eager to help, certainly in
larger total numbers than bilingual volunteers. What
matters here, therefore, is not how much of the test
set was translated in total, but how much the trans-
lations improved for the sentences where monolin-
gual crowdsourcing was involved, compared to the
MT baseline, and what throughput might be like in
a real-world scenario.
We also were interested in throughput, particu-
larly in comparison to bilingual translators. In previ-
ous experimentation (Hu et al, 2011), throughput in
MonoTrans2 extrapolated to roughly 800 words per
day, a factor of 2.5 slower than professional trans-
lators? typical speed of 2000 words per day. In
this experiment, overall translation speed averaged
about 300 words per day, a factor of more than 6
times slower. However, this is an extremely pes-
simistic estimate, for several reasons. First, our pre-
vious experiment had more than 20 users per side,
while here our Haitian crowd consisted of only four
people. Second, we discovered after beginning the
experiment that the translation of our instructions
into Haitian Creole had been done somewhat slop-
pily. And, third, we encountered a range of tech-
nical and logistical problems with our Haitian par-
ticipants, ranging from finding a location with In-
ternet access to do the work (ultimately an Internet
Cafe? turned out to be the best option), to slow and
sporadic connections (even in an Internet Cafe?), to
relative lack of motivation for part-time rather than
full-time work. It is fair to assume that in a real-
world scenario, some unanticipated problems like
these might crop up, but it also seems fair to assume
that many would not; for example, most people from
the Haitian Creole and French-speaking communi-
ties who volunteered using Munro et al?s system
in January 2010 were not themselves located in the
403
third world.
Finally, regarding quality, the results here are
promising, albeit not as striking as those Hu et al
(2011) obtained for Spanish-German translation of
children?s books. The nature of SMS messages
themselves may have been a contributing factor to
the lower translation adequacy: even in clean form,
these are sometimes written using shorthand (e.g.
?SVP?), and are sometimes not syntactically correct.
The text messages are seldom related to each other,
unlike sentences in larger bodies of text where even
partially translated sentences can be related to each
other to provide context, as is the case for children?s
books. One should also keep in mind that the under-
lying machine translation engine, Google Translate
between Haitian Creole and English, is still in an al-
pha phase.
Those considerations notwithstanding, it is en-
couraging to see a set of machine translations get
better without the use of any human bilingual exper-
tise. We are optimistic that with further refinements
and research, monolingual translation crowdsourc-
ing will make it possible to harness the vast num-
ber of technologically connected people who want
to help in some way when disaster strikes.
7 Acknowledgments
This research is supported by NSF contract
#BCS0941455 and by a Google Research Award.
References
Olivia Buzek, Philip Resnik, and Benjamin B. Bederson.
2010. Error driven paraphrase annotation using me-
chanical turk. In NAACL 2010 Workshop on Creating
Speech and Text Language Data With Amazon?s Me-
chanical Turk.
Chris Callison-Burch, Colin Bannard, , and Josh
Schroeder. 2004. Improving statistical translation
through editing. In Workshop of the European Asso-
ciation for Machine Translation.
Marianne Dabbadie, Anthony Hartley, Margaret King,
Keith J. Miller, Widad Mustafa El Hadi, Andrei
Popescu-Belis, Florence Reeder, and Michelle Vanni.
2002. A hands-on study of the reliability and coher-
ence of evaluation metrics. In Workshop at the LREC
2002 Conference, page 8. Citeseer.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of Graphics Inter-
face 2010 on Proceedings of Graphics Interface 2010,
pages 39?46, Ottawa, Ontario, Canada. Canadian In-
formation Processing Society.
Chang Hu, Ben Bederson, Philip Resnik, and Yakov Kro-
nrod. 2011. Monotrans2: A new human computation
system to support monolingual translation. In Human
Factors in Computing Systems (CHI 2011), Vancou-
ver, Canada, May. ACM, ACM.
Chang Hu. 2009. Collaborative translation by monolin-
gual users. In Proceedings of the 27th international
conference extended abstracts on Human factors in
computing systems, pages 3105?3108, Boston, MA,
USA. ACM.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspon-
dence using annotation projection. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 392?399, Philadelphia, Penn-
sylvania. Association for Computational Linguistics.
Daisuke Morita and Toru Ishida. 2009. Designing pro-
tocols for collaborative translation. In PRIMA ?09:
Proceedings of the 12th International Conference on
Principles of Practice in Multi-Agent Systems, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Robert Munro. 2010. Crowdsourced translation for
emergency response in haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation. Keynote.
Alexander J. Quinn, Bederson, and Benjamin B. Beder-
son. 2011. Human computation: A survey and tax-
onomy of a growing field. In Human Factors in Com-
puting Systems (CHI 2011), Vancouver, Canada, May.
ACM, ACM.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alexander J. Quinn, and Benjamin B. Bederson. 2010.
Improving translation via targeted paraphrasing. In
EMNLP.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via
robust projection across aligned corpora. In HLT
?01: Proceedings of the first international conference
on Human language technology research, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
404
