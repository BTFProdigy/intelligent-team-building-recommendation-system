Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 97?102,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Parsing Syntactic and Semantic Dependencies for Multiple Languages 
with A Pipeline Approach 
 
Han Ren, Donghong Ji Jing Wan, Mingyao Zhang 
School of Computer Science Center for Study of Language & Information 
Wuhan University Wuhan University 
Wuhan 430079, China Wuhan 430079, China 
cslotus@mail.whu.edu.cn 
donghong_ji@yahoo.com 
{jennifer.wanj, my.zhang}@gmail.com
 
 
 
Abstract 
This paper describes a pipelined approach for 
CoNLL-09 shared task on joint learning of 
syntactic and semantic dependencies. In the 
system, we handle syntactic dependency pars-
ing with a transition-based approach and util-
ize MaltParser as the base model. For SRL, 
we utilize a Maximum Entropy model to iden-
tify predicate senses and classify arguments. 
Experimental results show that the average 
performance of our system for all languages 
achieves 67.81% of macro F1 Score, 78.01% 
of syntactic accuracy, 56.69% of semantic la-
beled F1, 71.66% of macro precision and 
64.66% of micro recall. 
1 Introduction 
Given a sentence with corresponding part-of-
speech for each word, the task of syntactic and se-
mantic dependency parsing contains two folds: (1) 
identifying the syntactic head of each word and 
assigning the dependency relationship between the 
word and its head; (2) identifying predicates with 
proper senses and labeling semantic dependencies 
for them. 
For data-driven syntactic dependency parsing, 
many approaches are based on supervised learning 
using treebank or annotated datasets. Currently, 
graph-based and transition-based algorithms are 
two dominating approaches that are employed by 
many researchers, especially in previous CoNLL 
shared tasks. Graph-based algorithms (Eisner, 
1996; McDonald et al, 2005) assume a series of 
dependency tree candidates for a sentence and the 
goal is to find the dependency tree with highest 
score. Transition-based algorithms (Yamada and 
Matsumoto, 2003; Nivre et al, 2004) utilize transi-
tion histories learned from dependencies within 
sentences to predict next state transition and build 
the optimal transition sequence. Although different 
strategies were considered, two approaches yielded 
comparable results at previous tasks. 
Semantic role labeling contains two problems: 
identification and labeling. Identification is a bi-
nary classification problem, and the goal is to iden-
tify annotated units in a sentence; while labeling is 
a multi-class classification problem, which is to 
assign arguments with appropriate semantic roles. 
Hacioglu (2004) utilized predicate-argument struc-
ture and map dependency relations to semantic 
roles. Liu et al (2005) combined two problems 
into a classification one, avoiding some annotated 
units being excluded due to some incorrect identi-
fication results. In addition, various features are 
also selected to improve accuracy of SRL. 
In this paper, we propose a pipelined approach 
for CoNLL-09 shared task on joint learning of syn-
tactic and semantic dependencies, and describe our 
system that can handle multiple languages. In the 
system, we handle syntactic dependency parsing 
with a transition-based approach. For SRL, we util-
ize Maximum Entropy model to identify predicate 
senses and classify arguments. 
The remain of the paper is organized as follows. 
In Section 2, we discuss the processing mechanism 
containing syntactic and semantic dependency 
parsing of our system in detail. In Section 3, we 
give the evaluation results and analysis. Finally, 
the conclusion and future work are given in Sec-
tion 4. 
97
2 System Description  
The system, which is a two-stage pipeline, proc-
esses syntactic and semantic dependencies respec-
tively. To reduce the difficulties in SRL, predicates 
of each sentence in all training and evaluation data 
are labeled, thus predicate identification can be 
ignored. 
 
Figure 1. System Architectures 
 
For syntactic dependencies, we employ a state-
of-the-art dependency parser and basic plus ex-
tended features for parsing. For semantic depend-
encies, a Maximum Entropy Model is used both in 
predicate sense identification and semantic role 
labeling. Following subsections will show compo-
nents of our system in detail. 
2.1 Syntactic Dependency Parsing 
In the system, MaltParser1 is employed for syntac-
tic dependency parsing. MaltParser is a data-driven 
deterministic dependency parser, based on a Sup-
port Vector Machine classifier. An extensive re-
search (Nivre, 2007) parsing with 9 different 
languages shows that the parser is language-
independent and yields good results. 
MaltParser supports two kinds of parsing algo-
rithms: Nivre?s algorithms and Covington?s incre-
mental algorithms. Nivre?s algorithms, which are 
deterministic algorithms consisting of a series of 
shift-reduce procedures, defines four operations: 
?Right. For a given triple <t|S, n|I, A>, S 
represents STACK and I represents INPUT. If 
dependency relation t ? n exists, it will be 
                                                          
1 http://w3.msi.vxu.se/~jha/maltparser/ 
pendency relation t?n exists, it will be appended 
into A and t will be removed from S. 
?Left. For a given triple <t|S, n|I, A>, if de-
pendency relation n?t exists, it will be appended 
into A and n will be pushed into S. 
?Reduce. If dependency relation n?t does not 
exist, and the parent node of t exists left to it, t will 
be removed from S. 
?Shift. If none of the above satisfies, n will be 
pushed into S. 
The deterministic algorithm simplifies determi-
nation for Reduce operation. As a matter of fact, 
some languages, such as Chinese, have more flexi-
ble word order, and some words have a long dis-
tance with their children. In this case, t should not 
be removed from S, but be handled with Shift op-
eration. Otherwise, dependency relations between t 
and its children will never be identified, thus se-
quential errors of dependency relations may occur 
after the Reduce operation. 
For syntactic dependencies with long distance, 
an improved Reduce strategy is: if the dependency 
relation between n and t does not exist, and the 
parent node of t exists left to it and the dependency 
relation between the parent node and n, t will be 
removed from S. The Reduce operation is projec-
tive, since it doesn?t influence the following pars-
ing procedures. The Improved algorithm is 
described as follows: 
(1) one of the four operations is performed ac-
cording to the dependency relation between t and n 
until EOS; if only one token remains in S, go to (3). 
(2) continue to select operations for remaining 
tokens in S; when Shift procedure is performed, 
push t to S; if only one token remains in S and I 
contains more tokens than only EOS, goto (1). 
(3) label all odd tokens in S as ROOT, pointing 
to EOS. 
We also utilize history-based feature models 
implemented in the parser to predict the next action 
in the deterministic derivation of a dependency 
structure. The parser provides some default fea-
tures that is general for most languages: (1) part-
of-speech features of TOP and NEXT and follow-
ing 3 tokens; (2) dependency features of TOP con-
taining leftmost and rightmost dependents, and of 
NEXT containing leftmost dependents; (3) Lexical 
98
features of TOP, head of TOP, NEXT and follow-
ing one token. We also extend features for multiple 
languages: (1) count of part-of-speech features of 
following tokens extend to 5; (2) part-of-speech 
and dependent features of head of TOP. 
2.2 Semantic Dependency Parsing 
Each defacto predicate in training and evaluation 
data of CoNLL09 is labeled with a sign ?Y?, which 
simplifies the work of semantic dependency pars-
ing. In our system, semantic dependency parsing is 
a pipeline that contains two parts: predicate sense 
identification and semantic role labeling. For 
predicate sense identification, each predicate is 
assigned a certain sense number. For semantic role 
labeling, local and global features are selected. 
Features of each part are trained by a classification 
algorithm.  Both parts employ a Maximum Entropy 
Tool MaxEnt in a free package OpenNLP 2 as a 
classifier. 
2.2.1  Predicate Sense Identification 
The goal of predicate sense identification is to de-
cide the correct frame for a predicate. According to 
PropBank (Palmer, et al, 2005), predicates contain 
one or more rolesets corresponding to different 
senses. In our system, a classifier is employed to 
identify each predicate?s sense.  
Suppose { }01, 02, , LC = ? N
s t
                                                          
 is the sense set 
(NL is the count of categories corresponding to the 
language L, eg., in Chinese training set NL = 10 
since predicates have at most 10 senses in the set), 
and ti is the ith sense of word w in sentence s. The 
model is implemented to assign each predicate to 
the most probatilistic sense. 
( | , )i C it P w?=argmax                (1) 
Features for predicate sense identification are 
listed as follows: 
?WORD, LEMMA, DEPREL: The lexical 
form and lemma of the predicate; the dependency 
relation between the predicate and its head; for 
Chinese and Japanese, WORD is ignored. 
? HEAD_WORD, HEAD_POS: The lexical 
form and part-of-speech of the head of the predi-
cate. 
2 http://maxent.sourceforge.net/ 
? CHILD_WORD_SET, CHILD_POS_SET, 
CHILD_DEP_SET: The lexical form, part-of-
speech and dependency relation of dependents of 
the predicate. 
?LSIB_WORD, LSIB_POS, LSIB_DEPREL, 
RSIB_WORD, RSIB_POS, RSIB_DEPREL: The 
lexical form, part-of-speech and dependency rela-
tion of the left and right sibling token of the predi-
cate. Features of sibling tokens are adopted, 
because senses of some predicates can be inferred 
from its left or right sibling. 
For English data set, we handle verbal and 
nominal predicates respectively; for other lan-
guages, we handle all predicates with one classifier. 
If a predicate in the evaluation data does not exist 
in the training data, it is assigned the most frequent 
sense label in the training data. 
2.2.2  Semantic Role Labeling 
Semantic role labeling task contains two parts: ar-
gument identification and argument classification. 
In our system the two parts are combined as one 
classification task. Our reason is that those argu-
ment candidates that potentially become semantic 
roles of corresponding predicates should not be 
pruned by incorrect argument identification. In our 
system, a predicate-argument pair consists of any 
token (except predicates) and any predicate in a 
sentence. However, we find that argument classifi-
cation is a time-consuming procedure in the ex-
periment because the classifier spends much time 
on a great many of invalid predicate-argument 
pairs. To reduce useless computing, we add a sim-
ple pruning method based on heuristic rules to re-
move invalid pairs, such as punctuations and some 
functional words.  
Features used in our system are based on (Ha-
cioglu, 2004) and (Pradhan et al 2005), and de-
scribed as follows:  
?WORD, LEMMA, DEPREL: The same with 
those mentioned in section 2.2.1. 
?VOICE: For verbs, the feature is Active or 
Passive; for nouns, it is null. 
?POSITION: The word?s position correspond-
ing to its predicate: Left, Right or Self. 
?PRED: The lemma plus sense of the word. 
?PRED_POS: The part-of-speech of the predi-
cate. 
99
?LEFTM_WORD, LEFTM_POS, RIGHTM_ 
WORD, RIGHTM_POS: Leftmost and rightmost 
word and their part-of-speech of the word. 
? POS_PATH: All part-of-speech from the 
word to its predicate, including Up, Down, Left 
and Right, eg. ?NN?VV?CC?VV?. 
?DEPREL_PATH: Dependency relations from 
the word to its predicate, eg. ?COMP?RELC?
COMP??. 
?ANC_POS_PATH, ANC_DEPREL_PATH: 
Similar to POS_PATH and DEPREL_PATH, part-
of-speech and dependency relations from the word 
to the common ancestor with its predicate. 
?PATH_LEN: Count of passing words from 
the word to its predicate. 
? FAMILY: Relationship between the word 
and its predicate, including Child, Parent, Descen-
dant, Ancestor, Sibling, Self and Null. 
? PRED_CHD_POS, PRED_CHD_DEPREL: 
Part-of-speech and dependency relations of all 
children of the word?s predicate. 
For different languages, some features men-
tioned above are invalid and should be removed, 
and some extended features could improve the per-
formance of the classifier. In our system we mainly 
focus on Chinese, therefore, WORD and VOICE 
should be removed when processing Chinese data 
set. We also adopt some features proposed by (Xue, 
2008): 
? POS_PATH_BA, POS_PATH_SB, POS_ 
PATH_LB: BA and BEI are functional words that 
impact the order of arguments. In PropBank, BA 
words have the POS tag BA, and BEI words have 
two POS tags: SB (short BEI) and LB (long BEI). 
3 Experimental Results  
Our experiments are based on a PC with a Intel 
Core 2 Duo 2.1G CPU and 2G memory. Training 
and evaluation data (Taul? et al, 2008; Xue et al, 
2008; Haji? et al, 2006; Palmer et al, 2002; Bur-
chardt et al, 2006; Kawahara et al, 2002) have 
been converted to a uniform CoNLL Shared Task 
format. In all experiments, SVM and ME model 
are trained using training data, and tested with 
development data of all languages.  
The system for closed challenge is designed as 
two parts. For syntactic dependency training and 
parsing, we utilize the projective model in Malt-
Parser for data sets. We also follow default settings 
in MaltParser, such as assigned parameters for 
LIBSVM and combined prediction strategy, and 
utilize improved approaches mentioned in section 
2. For semantic dependency training and parsing, 
we choose the count of iteration as 100 and cutoff 
value as 10 for the ME model. Table 1 shows the 
training time for syntactic and semantic depend-
ency of all languages. Parsing time for syntactic is 
not more than 30 minutes, and for semantic is not 
more than 5 minutes of each language. 
 
 syn prd sem 
English 7h 12min 47min 
Chinese 8h 18min 61min 
Japanese 7h 14min 46min 
Czech 13h 46min 77min 
German 6h 16min 54min 
Spanish 6h 15min 55min 
Catalan 6h 15min 50min 
Table 1. Training cost for all languages. syn, prd and 
sem mean training time for syntactic dependency, predi-
cate identification and semantic dependency. 
3.1 Syntactic Dependency Parsing 
We utilize MaltParser with improved algorithms 
mentioned in section 2.1 for syntactic dependency 
parsing, and the results are shown in Table 2. 
 
 LAS UAS label-acc. 
English 87.57 89.98 92.19 
Chinese 79.17 81.22 85.94 
Japanese 91.47 92.57 97.28 
Czech 57.30 75.66 65.39 
German 76.63 80.31 85.97 
Spanish 76.11 84.40 84.69 
Catalan 77.84 86.41 85.78 
Table 2. Performance of syntactic dependency parsing 
 
Table 2 indicates that parsing for Japanese and 
English data sets has a better performance than 
other languages, partly because determinative algo-
rithm and history-based grammar are more suited 
for these two languages. To compare the perform-
ance of our approach of improved deterministic 
algorithm and extended features, we make another 
experiment that utilize original arc-standard algo-
rithm and base features for syntactic experiments. 
Due to time limitation, the experiments are only 
based on Chinese training and evaluation data. The 
results show that LAS and UAS drops about 2.7% 
and 2.2% for arc-standard algorithm, 1.6% and 
1.2% for base features. They indicate that our de-
100
terministic algorithm and the extend features can 
help to improve syntactic dependency parsing. We 
also notice that the results of Czech achieve a 
lower performance than other languages. It mainly 
because the language has more rich morphology, 
usually accompanied by more flexible word order. 
Although using a large training set, linguistic prop-
erties greatly influence the parsing result. In addi-
tion, extended features are not suited for this 
language and the feature model should be opti-
mized individually. 
For all of the experiments we mainly focus on 
the language of Chinese. When parsing Chinese 
data sets we find that the focus words where most 
of the errors occur are almost punctuations, such as 
commas and full stops. Apart from errors of punc-
tuations, most errors occur on prepositions such as 
the Chinese word ?at?. Most of these problems 
come from assigning the incorrect dependencies, 
and the reason is that the parsing algorithm con-
cerns the form rather than the function of these 
words. In addition, the prediction of dependency 
relation ROOT achieves lower precision and recall 
than others, indicating that MaltParser overpredicts 
dependencies to the root. 
3.2 Semantic Dependency Parsing 
MaxEnt is employed as our classifier to train and 
parse semantic dependencies, and the results are 
shown in Table 3, in which all criterions are la-
beled. 
 
 P R F1 
English 76.57 60.45 67.56 
Chinese 75.45 69.92 72.58 
Japanese 91.93 43.15 58.73 
Czech 68.83 57.78 62.82 
German 62.96 47.75 54.31 
Spanish 40.11 39.50 39.80 
Catalan 41.34 40.66 41.00 
Table 3. Performance of semantic dependency parsing 
 
As shown in Table 3, the scores of the latter 
five languages are quite lower than those of the 
former two languages, and the main reason could 
be inferred from the scores of Table 2 that the drop 
of the performance of semantic dependency pars-
ing comes from the low performance of syntactic 
dependency parsing. Another reason is that, mor-
phological features are not be utilized in the classi-
fier. Our post experiments after submission show 
that average performance could improve the per-
formance after adding morphological and some 
combined features. In addition, difference between 
precision and recall indicates that the classification 
procedure works better than the identification 
procedure in semantic role labeling.  
For Chinese, semantic role of some words with 
part-of-speech VE have been mislabeled. It?s 
mainly because that these words in Chinese have 
multiple part-of-speech. The errors of POS and 
PRED greatly influence the system to perform 
these words. Another main problem occurs on the 
pairs NN + A0/A1. Identification of the two pairs 
are much lower than VA/VC/VE/VV + A0/A1 
pairs. The reason is that the identification of nomi-
nal predicates have more errors than that of verbal 
predicates due to the combination of SRL for these 
two kinds of predicates. For further study, verbal 
predicates and nominal predicates should be han-
dled respectively so that the overall performance 
can be improved.  
3.3 Overall Performance 
The average performance of our system for all lan-
guages achieves 67.81% of macro F1 Score, 
78.01% of syntactic accuracy, 56.69% of semantic 
labeled F1, 71.66% of macro precision and 64.66% 
of micro recall. 
4 Conclusion 
In this paper, we propose a pipelined approach for 
CoNLL-09 shared task on joint learning of syntac-
tic and semantic dependencies, and describe our 
system that can handle multiple languages. Our 
system focuses on improving the performance of 
syntactic and semantic dependency respectively. 
Experimental results show that the overall per-
formance can be improved for multiple languages 
by long distance dependency algorithm and ex-
tended history-based features. Besides, the system 
fits for verbal predicates than nominal predicates 
and the classification procedure works better than 
identification procedure in semantic role labeling. 
For further study, respective process should be 
handled between these two kinds of predicates, and 
argument identification should be improved by 
using more discriminative features for a better 
overall performance. 
101
Acknowledgments 
This work is supported by the Natural Science 
Foundation of China under Grant Nos.60773011, 
90820005, and Independent Research Foundation 
of Wuhan University. 
References  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2006). Genoa, Italy. 
Jason M. Eisner. 1996. Three new probabilistic models 
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING), pp.340?345. 
Kadri Hacioglu. 2004. Semantic Role Labeling Using 
Dependency Trees. In Proceedings of the Interna-
tional Conference on Computational Linguistics 
(COLING). 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie Mikulo-
v? and Zden?k ?abokrtsk?. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data 
Consortium, Philadelphia, Pennsylvania, USA. ISBN 
1-58563-370-4. LDC Cat. No. LDC2006T01. URL: 
http://ldc.upenn.edu. 
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL 2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th 
Conference on Computational Natural Language 
Learning (CoNLL-2009). Boulder, Colorado, USA. 
June 4-5. pp.3-22. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. pp.2008-2013. 
Ryan McDonald, Koby Crammer, and Fernando Pereira.  
2005. Online large-margin training of dependency 
parsers. In Proceedings of the 43rd Annual Meeting of 
the Association for Computational Linguistics (ACL), 
pp.91?98. 
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. 
Memory-based dependency parsing. In Proceedings 
of the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL), pp.49?56. 
Joakim Nivre. 2004. Incrementality in Deterministic 
Dependency Parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop 
at ACL-2004, Barcelona, Spain, pp.50-57. 
Joakim Nivre and Johan Hall. 2005. MaltParser: A lan-
guage-independent system for data-driven depend-
ency parsing. In Proceedings of the Fourth Workshop 
on Treebanks and Linguistic Theories (TLT). 
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, 
Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov 
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural language Engineering, Volume 13, Is-
sue 02, pp.95-135. 
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin and Daniel Jurafsky. 
2005. Support Vector Learning for Semantic Argu-
ment classification. Machine Learning Journal, 2005, 
60(3): 11?39. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings of 
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008). 
Mariona Taul?, Maria Ant?nia Mart? and Marta Reca-
sens. 2008. AnCora: Multilevel Annotated Corpora 
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC-2008). Marrakech, Morocco. 
Liu Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and 
Huaijun Liu. 2005. Semantic role labeling system us-
ing maximum entropy classifier. In Proceedings of 
the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL). 
Nianwen Xue. 2008. Labeling Chinese Predicates with 
Semantic roles. Computational Linguistics, 34(2): 
225-255. 
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank.  Natural Language 
Engineering, 15(1):143-172. 
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector ma-
chines. In Proceedings of the 8th International 
Workshop on Parsing Technologies (IWPT), pp.195?
206. 
 
102
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 113?118
Manchester, August 2008
Automatic Chinese Catchword Extraction 
Based on Time Series Analysis 
Han Ren1, Donghong Ji1, Jing Wan2 and Lei Han1
1 School of Computer Science, Wuhan University 430079, China 
2 Center for Study of Language & Information, Wuhan University 430072, China 
cslotus@mail.whu.edu.cn, donghong_ji@yahoo.com, 
jennifer.wanj@gmail.com, hattason@mail.whu.edu.cn 
 
Abstract 
Catchwords refer to those popular words 
or phrases in a time period. In this paper, 
we propose a novel approach for 
automatic extraction of Chinese 
catchwords. By analyzing features of 
catchwords, we define three aspects to 
describe Popular Degree of catchwords. 
Then we use curve fitting in Time Series 
Analysis to build Popular Degree Curves 
of the extracted terms. Finally we give a 
formula that can calculate Popular 
Degree values of catchwords and get a 
ranking list of catchword candidates. 
Experiments show that the method is 
effective. 
1 Introduction 
Generally, a catchword is a term which 
represents a hot social phenomenon or an 
important incident, and is paid attention by 
public society within certain time period. On the 
one hand, catchwords represent the mass value 
orientation for a period. On the other hand, they 
have a high timeliness. Currently, there are quiet 
a few ranking and evaluations of catchwords 
every year in various kinds of media. Only in 
year 2005, tens of Chinese organizations 
published their ranking list of Chinese 
catchwords.  
Catchwords contain a great deal of 
information from any particular area, and such 
words truly and vividly reflect changes of our 
lives and our society. By monitoring and analysis 
of catchwords, we can learn the change of public 
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
attention in time. In addition, we may detect the 
potential changes of some linguistic rules, which 
can help establish and adjust state language 
policies. 
Currently, two kinds of approaches are 
adopted to evaluate catchwords. One is by CTR 
(Click-Through Rate) or retrieval times, but the 
limitation is that it is just based on frequency, 
which is only one feature of catchwords. The 
other is by manual evaluation, but it depends on 
their subjective judgment to a large extent. In this 
paper, we propose a novel approach that can 
automatically analyze and extract Chinese 
catchwords. By analyzing sample catchwords 
and finding out their common features, we 
provide a method to evaluate the popular degree. 
After ranking, terms that have high values are 
picked out as catchword candidates.  
The rest of the paper is organized as follows. 
In Section 2, we discuss about the linguistic basis 
of catchword judgment. In Section 3, we describe 
the extraction method in detail. In Section 4, we 
present the experimental results as well as some 
discussions. Finally, we give the conclusion and 
future work in Section 5. 
2 Linguistic basis 
The popularity of a word or phrase contains two 
factors: time and area, namely how long it lasts 
and how far it spreads. But neither of them have 
definite criterion. 
2.1 Linguistic definition of catchword 
Many researches of catchwords come from pure 
linguistic areas. Wang (1997) proposed that 
catchwords, which include words, phrases, 
sentences or special patterns, are a language form 
in certain times and among certain groups or 
communities. Guo (1999) specified that 
catchwords are popular words, which are widely 
113
used in certain period of time among certain 
groups of people. To sum up, catchwords are a 
language form spreading quickly within certain 
area in certain period of time. 
According to Zipf?s Law (Zipf, 1949), the 
word that has a higher usage frequency is shorter 
than others. Catchwords also follow this 
principle: most catchwords are words and 
phrases instead of sentences and longer language 
units, which are more difficult to extract 
automatically. In the paper, we focus on 
catchwords as words and phrases. 
2.2 Features of catchword 
Some features of catchwords have been proposed, 
but there have been few research to quantify and 
weigh the features. Zhang (1999) proposed a 
method to judge catchwords by weighing 
Circulating Degree of catchwords, which are 
based on Dynamic Circulating Corpus. But the 
corpus construction and the judgment still 
depend on manual efforts. 
By analyzing usage frequency of catchwords, 
we find that being a language phenomenon 
within a period of time, a catchword has two 
features: one is high usage frequency, namely a 
catchword is frequently used in certain period of 
time; the other is timeliness, namely this 
situation will lasts for some time. Our 
quantification method is based on these features. 
3 Extraction Method 
In this section, the extraction method is described 
in detail. After term extraction, the features of 
terms are weighed by time series analysis. The 
algorithm in section 3.4 shows the process to 
extract catchword candidates. 
3.1 Term Extraction 
Catchwords are words or phrases with maximal 
meanings, most of which are multi-character 
words or phrases. Word segmentation has a low 
discrimination for long phrases, while term 
extraction has a better way to extract them. 
Zhang (2006) proposed a new ATE algorithm, 
which is based on the decomposition of prime 
string. The algorithm evaluates the probability of 
a long string to be a term by weighing relation 
degree among sub-strings within the long string. 
The algorithm can raise the precision in 
extracting multi-character words and long 
phrases. In this paper, we use this method to 
extract terms. 
3.2 Popular Degree Curve 
For extracted terms, a time granularity should be 
defined to describe their features. We select 
?day? as the time granularity and get every day?s 
usage frequency for each term in one year. These 
can be described as a time series like below: 
{ }1 2, ,..., ,...,w w w wt wC c c c c= n           (1) 
wC  is the time series of term w.  is the 
usage frequency of term w in the day t. n is the 
number of observation days.  
wtc
As a latent knowledge, two features of 
catchwords mentioned in section 2.2 exist in 
their time series. The effective method to find out 
the latent knowledge in the time series is Time 
Series Analysis, which includes linear analysis 
and nonlinear analysis. As the time series of 
terms belong to nonlinear series, we use 
nonlinear analysis to deal with them. 
After getting usage frequency, we use SMA 
(Simple Moving Average) method to eliminate 
the random fluctuation of series . The 
formula is as follows: 
wC
( )
1
m
w t m j
j
wt
c
c
m
? +
==
?
                   (2) 
wtc  is the smoothed usage frequency of term 
w in the day t and m is the interval. In SMA 
method, a short interval has a little effect, while a 
long one may result in low accuracy. So we 
should specify a proper interval. Through 
experiments we find that an appropriate interval 
is between 10 and 20. Smoothed time series is as 
follows: 
{ }1 2, ,..., ,...,w w w wt wC c c c c= n          (3) 
Smoothed time series of terms can be 
described as curves, in which the coordinate x is 
day t and coordinate y is wtc . Through these 
curves we can see that, catchwords appear in 
certain period of time and its usage frequency 
increases in this period. After reaching the 
highest point, usage frequency of catchwords 
decrease slowly. We call this process Popular 
Degree, which contains three aspects: 
1) Popular Trend: the increasing process of 
usage frequency; the more obviously the popular 
trend changes, the higher the popular degree is. 
2) Peak Value: maximum usage frequency 
within certain period of time; the larger the peak 
value is, the higher the popular degree is. 
114
3) Popular Keeping: the decreasing process of 
usage frequency; the more gently the popular 
keeping changes, the higher the popular degree is. 
Three aspects above determine popular degree 
of catchwords. Figure 1 shows the smoothed 
time series curve of the catchword ???? 2 ?  
evaluated in year 2005: 
 
Figure 1. Smoothed time series curve of 
the catchword ????? 
To the catchword ?????, its Popular Trend 
changes obviously and its Popular Keeping 
changes gently. Meanwhile, its Peak Value is 
relatively higher than those of most catchwords. 
So the catchword ????? has a high Popular 
Degree. 
According to three aspects of Popular Degree, 
smoothed time series curve is separated into two 
parts: one is ascending period, namely Popular 
Trend process; the other is descending period, 
namely Popular Keeping process. We use conic 
fitting to deal with two parts of series. A conic?s 
formula is like below: 
2Y a bt ct= + +                             
According to least square method, a standard 
equation that can deduce three parameters a, b 
and c is as follows: 
2
2
2 2 3
 
 
 
Y na b t c t
tY a t b t c t
t Y a t b t c t
? = + +?? = + +?? = + +??
? ? ?
? ? ? ?
? ? ? ?
3
4
M
              
Assume TS is the starting time, TE is the ending 
time, and TM is the time that time series curve 
reaches the highest point. According to conic 
fitting method we can get curves of ascending 
and descending period. Formulas of two conics 
are as follows: 
2
2
( )           
( )        
S
M E
u a bu cu T t T
v a b v c v T t T
?
?
? = + + ? ??? ? ? ?= + + ? ???
     (4)  
                                                 
2 ??? means Sudan red in English. 
Variable u and v are usage frequency of a term 
in a day, ( )u?  is the formula of ascending curve, 
and ( )v?  is the formula of descending curve. The 
curve described by equation (4) is called Popular 
Degree Curve. Figure 2 shows the Popular 
Degree Curve of the catchword ?????: 
 Figure 2. Popular Degree Curve of 
the catchword ????? 
3.3 Popular Degree Value 
The decision of catchwords is based on three 
aspects of Popular Degree described in section 
3.2. We propose a formula to calculate Popular 
Degree values of terms. After getting the values, 
a ranking list by inverse order is established. The 
Popular Degree of a catchword is in the direct 
ratio to its place in the ranking list. The formula 
is as follows: 
( ) ( ) ( ) ( )PD w PT w PV w PK w= ? ?        (5) 
PD(w) is the Popular Degree value of the 
catchword w. PT(w) is the Popular Trend value 
of w: 
( ) ( )
( )
( )
M S
M
T T
PT w
T
? ?? ?
?= i                  (6) 
? is the adjusting parameter of Popular Trend. 
The formula indicates that PT(w) is related to 
changing process of Popular Degree Curve. 
PV(w) is the Peak Value of w: 
{ }
{ } { }
max
( )
1
max max
wt
wt wt
ww
c
PV w
c c
N
?=
+?i
 (7) 
? is the adjusting parameter of Peak Value. 
The formula indicates that PV(w) is related to the 
maximum usage frequency of w. PK(w) is the 
Popular Keeping value of w: 
( ) ( )
( ) 1
( )
M E
M
T T
PK w
T
? ?? ?
? ??= ?? ?? ?
i            (8) 
? is the adjusting parameter of Popular 
Keeping. The formula indicates that PK(w) is 
related to changing process of Popular Degree 
Curve. Parameter ?, ? and ? control proportion 
of three aspects in Popular Degree value. 
115
All extracted terms are ranked according to 
their Popular Degree values. Terms that have 
high scores are picked out as catchword 
candidates. 
3.4 Algorithm 
The algorithm of automatic catchwords 
extraction is described below: 
 
Algorithm Extracting catchwords 
Input text collections 
Output ranking list of catchword candidates 
Method 
1) use ATE algorithm mentioned in section 3.1 to 
extract terms 
2) filter terms that contains numbers and 
punctuations 
3) foreach term 
4)   calculate its smoothed time series by formula 
(2) 
5)   use conic fitting method in section 3.2 to get 
its Popular Degree Curve like equation (4) 
6)   use formula (5) ~ (8) to calculate its Popular 
Degree value 
7) rank all Popular Degree values from high to 
low 
4 Experimental Results and Analysis 
4.1 Text Collection 
In the experiment, we use 136,191 web pages 
crawled from Sina3?s news reports in year 2005 
including six categories: economy, science, 
current affairs, military, sports and entertainment. 
For the experimental purpose, we extract body 
content in every web page by using Noise 
Reducing algorithm (Shianhua Lin & Janming 
Ho, 2002). Totally, the extracted subset includes 
129,328 documents. 
4.2 Experiment settings 
In the experiment, several parameters should be 
settled to perform the catchwords extraction. 
?n  
A large time granularity may result in low 
accuracy for conic fitting. In this paper, we 
select ?day? as the time granularity. 
?m 
For the interval m in formula (2), a proper 
value should be specified to not only 
eliminate random fluctuation but also keep 
                                                 
3 http://www.sina.com.cn/ 
accuracy of data. In the experiment we find 
that the proper interval is between 10 and 20.  
?TS and TE
Catchwords have a high timeliness, so we 
should specify a time domain. By analysis of 
sample catchwords, we find that popular 
time domain for most of them approximately 
last for not more than 6 months. So we 
specify the time domain is n / 2. Thus the 
relationship among the starting time TS and 
the ending time TE is below: 
2S E
n
T T= ?  
As a proper example, the starting point can 
be 60 days away from the highest point. 
Thus the Popular Trend process and the 
Popular Keeping process both last for nearly 
3 months. So the relationship can be 
described as formulas below: 
4S M
n
T T ? ?= ? ? ?? ?
,  
4E M
n
T T ? ?= + ? ?? ?
                
??, ?, ? 
To keep the Popular Degree values of 
catchwords within [0, 1], three adjusting 
parameters are satisfied to the inequation: 
0 , , 1? ? ?< ? . 
Table 1 shows proper values of parameters as 
schema 1. We also give other schemas, which 
contain different values of parameters, to 
compare with the schema 1. In schema 2 to 
schema 4, default values of parameters are the 
same with schema 1. 
 
parameter Value 
n 365 
t [1, 365] 
m 15 
TS TM ? ? n / 4? 
TE TM + ? n / 4? 
? 1 
? 1 
? 1 
Table 1. parameters in schema 1 
 
schema 2: different m values 
schema 3: different values of TS and TE
schema 4: different values of ?, ? and ? 
4.3 Evaluation Measure 
Currently, there is no unified standard for 
catchword evaluation. In year 2005, NLRMRC 
116
(National Language Resources Monitoring and 
Research Centre, held by MOE of China) had 
published their top 100 Chinese catchwords. We 
use co-occurrence ratio of catchwords for the 
evaluation. The formula of co-occurrence ratio is 
as follows: 
CNr
N
=  
N is the number of ranking catchwords. NC is 
the co-occurrence of catchwords, namely the 
number of catchwords which appear both in our 
approach and NLRMRC in top N. 
4.4 Results 
We use algorithm described in section 3.4 to get 
a ranking list of catchword candidates. 
According to ATE algorithm mentioned in 
section 3.1, we extract 966,532 terms. After 
filtering invalid terms we get 892,184 terms and 
calculate each term?s Popular Degree value. 
Table 2 - 5 shows the co-occurrence ratio with 
schema 1 - 4. 
N=20 N=40 N=60 N=80 N=100
7% 18% 36% 53% 66%
Table 2. Co-occurrence ratio using schema 1 
m N=20 N=40 N=60 N=80 N=100
5 3% 7% 16% 29% 45%
10 4% 11% 25% 44% 59%
20 7% 15% 32% 49% 63%
25 6% 14% 29% 46% 60%
Table 3. Co-occurrence ratio using schema 2 
TM - TS : 
TE - TM
N=20 N=40 N=60 N=80 N=100
1 : 4 0% 3% 8% 15% 22%
2 : 3 4% 14% 30% 49% 64%
3 : 2 5% 15% 33% 51% 63%
4 : 1 2% 5% 12% 21% 26%
Table 4. Co-occurrence ratio using schema 3 
 N=20 N=40 N=60 N=80 N=100
?=0.5 3% 9% 24% 42% 55%
?=0.8 6% 15% 31% 50% 64%
?=0.5 2% 6% 16% 37% 52%
?=0.8 5% 13% 29% 47% 59%
?=0.5 3% 11% 26% 43% 57%
?=0.8 6% 15% 32% 51% 62%
Table 5. Co-occurrence ratio using schema 4 
Table 2 shows the co-occurrence ratio of the 
catchwords extracted by our approach and 
NLRMRC in top N catchwords ranking list. It 
indicates that, when N is 100, co-occurrence of 
the catchwords reaches 66%; when N is lower, 
the ratio is also lower. On the one hand, we can 
see that our approach has a good effect on 
automatically extracting catchwords, closing to 
the result of manual evaluation with the 
increment of N. On the other hand, it proves that 
divergence exists between our approach and 
manual evaluation in high-ranking catchwords. 
Table 3 indicates that, the condition of m = 20 
has a better co-occurrence ratio in contrast with 
others in schema 2. It is because a short interval 
has a little effect, while a long one may result in 
low accuracy in SMA. 
Table 4 indicates that a better performance can 
be made when the proportion of TM - TS and TE - 
TM is close to 1:1. It proves that Popular Trend 
process is just as important as Popular Keeping 
process. Therefore the best time domain of these 
two processes are both n / 4.  
Three parameters can adjust the weights of PD, 
PV and PK in formula (5). Table 5 indicates that 
three factors above are all important for weighing 
a catchword, while ?  is a little more important 
than ? and ?. Therefore, maximum usage 
frequency of a catchword is a little more 
important than two other factors. 
From Table 2 ? 5 we can see that, parameters 
in schema 1 is most appropriate for the 
evaluation. 
Table 6 shows the ranking list of top 10 
catchword candidates according to their Popular 
Degree values: 
candidates4 PD value 
??? 0.251262 
???? 0.220975 
?? 0.213843 
????? 0.196326 
TD-SCDMA 0.185691 
???? 0.166730 
??? 0.154803 
??? 0.137211 
???? 0.121738 
???? 0.120667 
Table 6. Popular Degree values of Top 10 
catchword candidates 
                                                 
4  ???? means a talent show by Hunan Satellite. 
?? means petroleum price 
????? means textile negotiation 
???? means a famous girl called sister lotus 
??? means STS Discovery OV-103 
??? means a billiards player named Junhui Ding 
???? means Six-Party Talks 
???? means swine streptococcus suis 
117
4.5 Analysis 
In our experiment, Popular Values of some 
catchwords by manual evaluation are lower. By 
analyzing their time series curves, we find that 
usage frequencies of these terms are not high. 
We also find that these catchwords mostly have 
other expressions. Such as the catchword ???
???? 5 ? can be also called ????? 6 ?. 
These two synonyms are treated as one term in 
manual evaluation that corresponds to promote 
usage frequency. However, relationship between 
the two synonyms is not concerned in automatic 
extraction. They are treated as separate terms. So 
the Popular Degree Values of these two 
synonyms are not high either. It proves that parts 
of catchwords by manual evaluation are collected 
and generalized. A catchword should be treated 
not only as a separate word or a phrase, but also 
as a part of a word-cluster, which consist of 
synonymous words or phrases. Through word 
clustering method, we can get an increasing 
quantity of the co-occurrence of catchwords 
between our approach and manual evaluations. 
5 Conclusions 
Being as one aspect of dynamic language 
research, catchwords have a far-reaching 
significance for the development of linguistics. 
The paper proposes an approach that can 
automatically detect and extract catchwords. By 
analyzing evaluated catchwords and finding out 
their common feature called popular degree, the 
paper provides a method of popular degree 
quantification and gives a formula to calculate 
term?s popular degree value. After ranking, terms 
that have high values are picked out as 
catchword candidates. The result can be provided 
as a reference for catchword evaluation. 
Experiments show that automatic catchword 
extraction can promote the precision and 
objectivity, and mostly lighten difficulties and 
workload of evaluation. 
In the experiment, we also find that some 
catchwords are not isolated, but have a strong 
relationship and express the same meaning. In 
the future, we can unite all synonymous 
catchwords to a word cluster and calculate the 
cluster?s popular degree value. Thus we would 
be able to achieve a better performance for 
extraction. 
                                                 
5 ?????? means social security system 
6 ???? is the abbreviation of ?????? 
Acknowledgement 
This work is supported by the Natural Science 
Foundation of China under Grant Nos.60773011, 
60703008. 
References 
G.E.P.Box, G.M.Jenkins and G.C.Reinsel. 1994. Time 
Series Analysis, Forecasting and Control. Third 
Edition, Prentice-Hall. 
Richard L. Burden and J.Douglas Faires. 2001. 
Numerical Analysis. Seventh Edition, Brooks/Cole, 
Thomson Learning, Inc., pp. 186-226. 
Xi Guo. 1999. China Society Linguistics. Nanjing : 
Nanjing University Press. 
H. Kantz and T. Schreiber. 1997. Nonlinear Time 
Series Analysis. Cambridge University Press, 1997 
Shianhua Lin, Janming Ho. 2002. Discovering 
informative content blocks from Web documents. In: 
SIGKDD. 
Dechun Wang 1997. Introduction to Linguistics. 
Shanghai: Shanghai Foreign Language Education 
Press. 
George K.Zipf 1949. Human Behavior and Principle 
of Least Effort: an Introduction to Human Ecology. 
Addison Wesley, Cambridge, Massachusetts. 
Pu Zhang 1999. On thinking of language sense and 
Circulating Degree. Beijing: Language Teaching 
and Linguistic Studies, (1). 
Yong Zhang 2006. Automatic Chinese Term 
Extraction Based on Decomposition of Prime 
String. Beijing: Computer Engineering, (23). 
 
118
