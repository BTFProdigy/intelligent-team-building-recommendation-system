Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 45?52,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Second Language Acquisition Model Using  
Example Generalization and Concept Categories 
 
Ari Rappoport Vera Sheinman 
Institute of Computer Science Institute of Computer Science 
The Hebrew University The Hebrew University 
Jerusalem, Israel Jerusalem, Israel 
arir@cs.huji.ac.il vera46@cl.cs.titech.ac.jp 
 
Abstract 
We present a computational model of ac-
quiring a second language from example 
sentences. Our learning algorithms build a 
construction grammar language model, 
and generalize using form-based patterns 
and the learner?s conceptual system. We 
use a unique professional language learn-
ing corpus, and show that substantial reli-
able learning can be achieved even though 
the corpus is very small. The model is ap-
plied to assisting the authoring of Japa-
nese language learning corpora. 
1 Introduction 
Second Language Acquisition (SLA) is a central 
topic in many of the fields of activity related to 
human languages. SLA is studied in cognitive sci-
ence and theoretical linguistics in order to gain a 
better understanding of our general cognitive abili-
ties and of first language acquisition (FLA)1. Gov-
ernments, enterprises and individuals invest 
heavily in foreign language learning due to busi-
ness, cultural, and leisure time considerations. SLA 
is thus vital for both theory and practice and should 
be seriously examined in computational linguistics 
(CL), especially when considering the close rela-
tionship to FLA and the growing attention devoted 
to the latter by the CL community. 
In this paper we present a computational model 
of SLA. As far as we could determine, this is the 
first model that simulates the learning process 
 
1 Note that the F stands here for ?First?, not ?Foreign?.  
computationally. Learning is done from examples, 
with no reliance on explicit rules. The model is 
unique in the usage of a conceptual system by the 
learning algorithms. We use a unique professional 
language learning corpus, showing effective learn-
ing from a very small number of examples. We 
evaluate the model by applying it to assisting the 
authoring of Japanese language learning corpora.   
We focus here on basic linguistic aspects of 
SLA, leaving other aspects to future papers. In par-
ticular, we assume that the learner possesses per-
fect memory and is capable of invoking the 
provided learning algorithms without errors.  
In sections 2 and 3 we provide relevant back-
ground and discuss previous work. Our input, 
learner and language models are presented in sec-
tion 4, and the learning algorithms in section 5. 
Section 6 discusses the authoring application. 
2 Background 
We use the term ?second language acquisition? to 
refer to any situation in which adults learn a new 
language2. A major concept in SLA theory 
[Gass01, Mitchell03] is that of interlanguage:
when learning a new language (L2), at any given 
point in time the learner has a valid partial L2 lan-
guage system that differs from his/her native lan-
guage(s) (L1) and from the L2. The SLA process is 
that of progressive enhancement and refinement of 
interlanguage. The main trigger for interlanguage 
modification is when the learner notices a gap be-
tween interlanguage and L2 forms. In order for this 
to happen, the learner must be provided with com-
 
2 Some SLA texts distinguish between ?second? and ?foreign? 
and between ?acquisition? and ?learning?. We will not make 
those distinctions here.   
45
prehensible input. Our model directly supports all 
of these notions.  
A central, debated issue in language acquisition 
is whether FLA mechanisms [Clark03] are avail-
able in SLA. What is clear is that SL learners al-
ready possess a mature conceptual system and are 
capable of explicit symbolic reasoning and abstrac-
tion. In addition, the amount of input and time 
available for FLA are usually orders of magnitude 
larger than those for SLA. 
The general linguistic framework that we utilize 
in this paper is that of Construction Grammar 
(CG) [Goldberg95, Croft01], in which the building 
blocks of language are words, phrases and phrase 
templates that carry meanings. [Tomasello03] pre-
sents a CG theory of FLA in which children learn 
whole constructions as ?islands? that are gradually 
generalized and merged. Our SLA model is quite 
similar to this process. 
In language education, current classroom meth-
ods use a combination of formal rules and commu-
nicative situations. Radically different is the 
Pimsleur method [Pimsleur05], an audio-based 
self-study method in which rules and explanations 
are kept to a minimum and most learning occurs by 
letting the learner infer L2 constructs from transla-
tions of contextual L1 sentences. Substantial anec-
dotal evidence (as manifested by learner comments 
and our own experience) suggests that the method 
is highly effective. We have used a Pimsleur cor-
pus in our experiments. One of the goals of our 
model is to assist the authoring of such corpora. 
3 Previous Work 
There is almost no previous CL work explicitly 
addressing SLA. The only one of which we are 
aware is [Maritxalar97], which represents interlan-
guage levels using manually defined symbolic 
rules. No language model (in the CL sense) or 
automatic learning are provided.   
Many aspects of SLA are similar to first lan-
guage acquisition. Unsupervised grammar induc-
tion from corpora is a growing CL research area 
([Clark01, Klein05] and references there), mostly 
using statistical learning of model parameters or 
pattern identification by distributional criteria. The 
resulting models are not easily presentable to hu-
mans, and do not utilize semantics.  
[Edelman04] presents an elegant FLA system in 
which constructions and word categories are iden-
tified iteratively using a graph. [Chang04] presents 
an FLA system that truly supports construction 
grammar and is unique in its incorporation of gen-
eral cognitive concepts and embodied semantics.  
SLA is related to machine translation (MT), 
since learning how to translate is a kind of acquisi-
tion of the L2. Most relevant to us here is modern 
example-based machine translation (EBMT) [So-
mers01, Carl03], due to its explicit computation of 
translation templates and to the naturalness of 
learning from a small number of examples 
[Brown00, Cicekli01]. 
The Computer Assisted Language Learning 
(CALL) literature [Levy97, Chapelle01] is rich in 
project descriptions, and there are several commer-
cial CALL software applications. In general, 
CALL applications focus on teacher, environment, 
memory and automatization aspects, and are thus 
complementary to the goals that we address here. 
4 Input, Learner and Language Knowl-
edge Models  
Our ultimate goal is a comprehensive computa-
tional model of SLA that covers all aspects of the 
phenomenon. The present paper is a first step in 
that direction. Our goals here are to:  
 
 Explore what can be learned from exam-
ple-based, small, beginner-level input 
corpora tailored for SLA; 
 Model a learner having a mature concep-
tual system;
 Use an L2 language knowledge model 
that supports sentence enumeration; 
 Identify cognitively plausible and effective 
SL learning algorithms;
 Apply the model in assisting the author-
ing of corpora tailored for SLA.  
In this section we present the first three compo-
nents; the learning algorithms and the application 
are presented in the next two sections. 
4.1 Input Model 
The input potentially available for SL learners is of 
high variability, consisting of meta-linguistic rules, 
usage examples isolated for learning purposes, us-
age examples partially or fully understood in con-
text, dictionary-like word definitions, free-form 
explanations, and more.  
46
One of our major goals is to explore the rela-
tionship between first and second language acqui-
sition. Methodologically, it therefore makes sense 
to first study input that is the most similar linguis-
tically to that available during FLA, usage exam-
ples. As noted in section 2, a fundamental property 
of SLA is that learners are capable of mature un-
derstanding. Input in our model will thus consist of 
an ordered set of comprehensible usage exam-
ples, where an example is a pair of L1, L2 sen-
tences such that the former is a translation of the 
latter in a certain understood context.  
We focus here on modeling beginner-level pro-
ficiency, which is qualitatively different from na-
tive-like fluency [Gass01] and should be studied 
before the latter. 
We are interested in relatively small input cor-
pora (thousands of examples at most), because this 
is an essential part of SLA modeling. In addition, it 
is of great importance, in both theoretical and 
computational linguistics, to explore the limits of 
what can be learned from meager input.  
One of the main goals of SLA modeling is to 
discover which input is most effective for SLA, 
because a substantial part of learners? input can be 
controlled, while their time capacity is small. We 
thus allow our input to be optimized for SLA, by 
containing examples that are sub-parts of other 
examples and whose sole purpose is to facilitate 
learning those (our corpus is also optimized in the 
sense of covering simpler constructs and words 
first, but this issue is orthogonal to our model). We 
utilize two types of such sub-examples. First, we 
require that new words are always presented first 
on their own. This is easy to achieve in controlled 
teaching, and is actually very frequent in FLA as 
well [Clark03]. In the present paper we will as-
sume that this completely solves the task of seg-
menting a sentence into words, which is reasonable 
for a beginner level corpus where the total number 
of words is relatively small. Word boundaries are 
thus explicitly and consistently marked.  
Second, the sub-example mechanism is also use-
ful when learning a construction. For example, if 
the L2 sentence is ?the boy went to school? (where 
the L2 here is English), it could help learning algo-
rithms if it were preceded by ?to school? or ?the 
boy?. Hence we do not require examples to be 
complete sentences.  
In this paper we do not deal with phonetics or 
writing systems, assuming L2 speech has been 
consistently transcribed using a quasi-phonetic 
writing system. Learning L2 phonemes is certainly 
an important task in SLA, but most linguistic and 
cognitive theories view it as separable from the rest 
of language acquisition [Fromkin02, Medin05].  
The input corpus we have used is a transcribed 
Pimsleur Japanese course, which fits the input 
specification above. 
4.2 Learner Model 
A major aspect of SLA is that learners already pos-
sess a mature conceptual system (CS), influenced 
by their life experience (including languages they 
know). Our learning algorithms utilize a CS model. 
We opted for being conservative: the model is only 
allowed to contain concepts that are clearly pos-
sessed by the learner before learning starts. Con-
cepts that are particular to the L2 (e.g., ?noun 
gender? for English speakers learning Spanish) are 
not allowed. Examples for concept classes include 
fruits, colors, human-made objects, physical activi-
ties and emotions, as well as meta-linguistic con-
cepts such as pronouns and prepositions. A single 
concept is simply represented by a prototypical 
English word denoting it (e.g., ?child?, ?school?). A 
concept class is represented by the concepts it con-
tains and is conveniently named using an English 
word or phrase (e.g., ?types of people?, ?buildings?, 
?language names?).  
Our learners can explicitly reason about concept 
inter-relationships. Is-a relationships between 
classes are represented when they are beyond any 
doubt (e.g., ?buildings? and ?people? are both 
?physical things?).  
A basic conceptual system is assumed to exist 
before the SLA process starts. When the input is 
controlled and small, as in our case, it is both 
methodologically valid and practical to prepare the 
CS manually. CS design is discussed in detail in 
section 6.  
In the model described in the present  paper we 
do not automatically modify the CS during the 
learning process; CS evolution will be addressed in 
future models.  
As stated in section 1, in this paper we focus on 
linguistic SLA aspects and do not address issues 
such as human errors, motivation and attention. 
We thus assume that our learner possesses perfect 
memory and can invoke our learning algorithms 
without any mistakes.   
47
4.3 Language Knowledge Model 
We require our model to support a basic capability 
of a grammar: enumeration of language sentences 
(parsing will be reported in other papers). In addi-
tion, we provide a degree of certainty for each. The 
model?s quality is evaluated by its applicability for 
learning corpora authoring assistance (section 6).   
The representation is based on construction 
grammar (CG), explicitly storing a set of construc-
tions and their inter-relationships. CG is ideally 
suited for SLA interlanguage because it enables the 
representation of partial knowledge: every lan-
guage form, from concrete words and sentences to 
the most abstract constructs, counts as a construc-
tion. The generative capacity of language is ob-
tained by allowing constructions to replace 
arguments. For example, (child), (the child goes to 
school), (<x> goes to school), (<x> <v> to school) 
and (X goes Z) are all constructions, where <x>, 
<v> denote word classes and X, Z denote other 
constructions.  
SL learners can make explicit judgments as to 
their level of confidence in the grammaticality of 
utterances. To model this, our learning algorithms 
assign a degree of certainty (DOC) to each con-
struction and to the possibility of it being an argu-
ment of another construction. The certainty of a 
sentence is a function (e.g., sum or maximum) of 
the DOCs present in its derivation path. 
Our representation is equivalent to a graph 
whose nodes are constructions and whose directed, 
labeled arcs denote the possibility of a node filling 
a particular argument of another node. When the 
graph is a-cyclic the resulting language contains a 
finite number of concrete sentences, easily com-
puted by graph traversal. This is similar to [Edel-
man04]; we differ in our partial support for 
semantics through a conceptual system (section 5) 
and in the notion of a degree of certainty.   
5 Learning Algorithms 
Our general SLA scheme is that of incremental 
learning ? examples are given one by one, each 
causing an update to the model. A major goal of 
our model is to identify effective, cognitively plau-
sible learning algorithms. In this section we present 
a concrete set of such algorithms. 
Structured categorization is a major driving 
force in perception and other cognitive processes 
[Medin05]. Our learners are thus driven by the de-
sire to form useful generalizations over the input. 
A generalization of two or more examples is possi-
ble when there is sufficient similarity of form and 
meaning between them. Hence, the basic ingredi-
ent of our learning algorithms is identifying such 
similarities. 
To identify concrete effective learning algo-
rithms, we have followed our own inference proc-
esses when learning a foreign language from an 
example-based corpus (section 6). The set of algo-
rithms described below are the result of this study.  
The basic form similarity algorithm is Single 
Word Difference (SWD). When two examples 
share all but a single word, a construction is 
formed in which that word is replaced by an argu-
ment class containing those words. For example, 
given ?eigo ga wakari mas? and ?nihongo ga wakari 
mas?, the construction (<eigo, nihongo> ga wakari 
mas) (?I understand English/Japanese?), containing 
one argument class, is created. In itself, SWD only 
compresses the input, so its degree of certainty is 
maximal. It does not create new sentences, but it 
organizes knowledge in a form suitable for gener-
alization.  
The basic meaning-based similarity algorithm is 
Extension by Conceptual Categories (ECC). For 
an argument class W in a construction C, ECC at-
tempts to find the smallest concept category U? 
that contains W?, the set of concepts corresponding 
to the words in W. If no such U? exists, C is re-
moved from the model. If U? was found, W is re-
placed by U, which contains the L2 words 
corresponding to the concepts in U?. When the re-
placement occurs, it is possible that not all such 
words have already been taught; when a new word 
is taught, we add it to all such classes U (easily 
implemented using the new word?s translation, 
which is given when it is introduced.) 
In the above example, the words in W are ?eigo? 
and ?nihongo?, with corresponding concepts ?Eng-
lish? and ?Japanese?. Both are contained in W?, the 
?language names? category, so in this case U? 
equals W?. The language names category contains 
concepts for many other language names, includ-
ing Korean, so it suffices to teach our learner the 
Japanese word for Korean (?kankokugo?) at some 
point in the future in order to update the construc-
tion to be (<eigo, nihongo, kankokugo> ga wakari 
mas). This creates a new sentence ?kankokugo ga 
wakari mas? meaning ?I understand Korean?. An 
48
example in which U? does not equal W? is given in 
Table 1 by ?child? and ?car?.  
L2 words might be ambiguous ? several con-
cepts might correspond to a single word. Because 
example semantics are not explicitly represented, 
our system has no way of knowing which concept 
is the correct one for a given construction, so it 
considers all possibilities. For example, the Japa-
nese ?ni? means both ?two? and ?at/in?, so when 
attempting to generalize a construction in which 
?ni? appears in an argument class, ECC would con-
sider both the ?numbers? and ?prepositions? con-
cepts.  
The degree of certainty assigned to the new con-
struction by ECC is a function of the quality of the 
match between W and U?. The more abstract is U, 
the lower the certainty. 
The main form-based induction algorithm is 
Shared Prefix, Generated Suffix (SPGS). Given 
an example ?x y? (x, y are word sequences), if there 
exist (1) an example of the form ?x z?, (2) an ex-
ample ?x?, and (3) a construction K that derives ?z? 
or ?y?, we create the construction (x K) having a 
degree of certainty lower than that of K. A Shared 
Suffix version can be defined similarly. Require-
ment (2) ensures that the cut after the prefix will 
not be arbitrary, and assumes that the lesson author 
presents constituents as partial examples before-
hand (as indeed is the case in our corpus).  
SPGS utilizes the learner?s current generative 
capacity. Assume input ?watashi wa biru o nomi 
mas? (?I drink beer?), previous inputs ?watashi wa 
america jin des? (?I am American?), ?watashi wa? 
(?as to me...?) and an existing construction K = 
(<biru, wain> o nomi mas). SPGS would create the 
construction (watashi wa K), yielding the new sen-
tence ?watashi wa wain o nomi mas? (?I drink 
wine?). 
To enable faster learning of more abstract con-
structions, we use generalized versions of SWD 
and SPGS, which allow the differing or shared 
elements to be a construction rather than a word or 
a word sequence.  
The combined learning algorithm is: given a 
new example, iteratively invoke each of the above 
algorithms at the given order until nothing new can 
be learned. Our system is thus a kind of inductive 
programming system (see [Thompson99] for a sys-
tem using inductive logic programming for seman-
tic parsing).  
Note that the above algorithms treat words as 
atomic units, so they can only learn morphological 
rules if boundaries between morphemes are 
marked in the corpus. They are thus more useful 
for languages such as Japanese than, say, for Ro-
mance or Semitic languages. 
Our algorithms have been motivated by general 
cognitive considerations. It is possible to refine 
them even further, e.g. by assigning a higher cer-
tainty when the focus element is a prefix or a suf-
fix, which are more conspicuous cognitively. 
6 Results and Application to Authoring of 
Learning Corpora 
We have experimented with our model using the 
Pimsleur Japanese I (for English speakers) course, 
which comprises 30 half-hour lessons, 1823 differ-
ent examples, and about 350 words. We developed 
a simple set of tools to assist transcription, using an 
arbitrary, consistent Latin script transliteration 
based on how the Japanese phonemes are pre-
sented in the course, which differs at places from 
common transliterations (e.g., we use ?mas?, not 
?masu?). Word boundaries were marked during 
transliteration, as justified in section 4.  
Example sentences from the corpus are ?nani o 
shi mas kaa ? / what are you going to do??, ?wa-
tashi ta chi wa koko ni i mas / we are here?, ?kyo 
wa kaeri masen / today I am not going back?, 
?demo hitori de kaeri mas / but I am going to return 
alone?, etc. Sentences are relatively short and ap-
propriate for a beginner level learner.  
Evaluating the quality of induced language 
models is notoriously difficult. Current FLA prac-
tice favors comparison of predicted parses with 
ones in human annotated corpora. We have fo-
cused on another basic task of a grammar, sentence 
enumeration, with the goal of showing that our 
model is useful for a real application, assistance for 
authoring of learning corpora. 
The algorithm has learned 113 constructions 
from the 1823 examples, generating 525 new sen-
tences. These numbers do not include construc-
tions that are subsumed by more abstract ones 
(generating a superset of their sentences) or those 
involving number words, which would distort the 
count upwards. The number of potential new sen-
tences is much higher: these numbers are based 
only on the 350 words present, organized in a 
rather flat CS. The constructions contain many 
49
placeholders for concepts whose words would be 
taught in the future, which could increase the num-
ber exponentially.  
In terms of precision, 514 of the 525 sentences 
were judged (by humans) to be syntactically cor-
rect (53 of those were problematic semantically). 
Regarding recall, it is very difficult to assess for-
mally. Our subjective impression is that the learned 
constructions do cover most of what a reasonable 
person would learn from the examples, but this is 
not highly informative ? as indicated, the algo-
rithms were discovered by following our own in-
herence processes. In any case, our algorithms 
have been deliberately designed to be conservative 
to ensure precision, which we consider more im-
portant than recall for our model and application. 
There is no available standard benchmark to 
serve as a baseline, so we used a simpler version of 
our own system as a baseline. We modified ECC to 
not remove C in case of failure of concept match 
(see ECC?s definition in section 5). The number of 
constructions generated after seeing 1300 exam-
ples is 3,954 (yielding 35,429 sentences), almost 
all of which are incorrect.  
The applicative scenario we have in mind is the 
following. The corpus author initially specifies the 
desired target vocabulary and the desired syntacti-
cal constructs, by writing examples (the easiest 
interface for humans). Vocabulary is selected ac-
cording to linguistic or subject  (e.g., tourism, 
sports) considerations. The examples are fed one 
by one into the model (see Table 1). For a single 
word example, its corresponding concepts are first 
manually added to the CS. 
The system now lists the constructions learned. 
For a beginner level and the highest degree of cer-
tainty, the sentences licensed by the model can be 
easily grasped just by looking at the constructions. 
The fact that our model?s representations can be 
easily communicated to people is also an advan-
tage from an SLA theory point of view, where ?fo-
cus on form? is a major topic [Gass01]. For 
advanced levels or lower certainties, viewing the 
sentences themselves (or a sample, when their 
number gets too large) might be necessary.  
The author can now check the learned items for 
errors. There are two basic error types, errors 
stemming from model deficiencies and errors that 
human learners would make too. As an example of 
the former, wrong generalizations may result from 
discrepancies between the modeled conceptual sys-
tem and that of a real person. In this case the au-
thor fixes the modeled CS. Discovering errors of 
the second kind is exactly the point where the 
model is useful. To address those, the author usu-
ally introduces new full or partial examples that 
would enable the learner to induce correct syntax. 
In extreme cases there is no other practical choice 
but to provide explicit linguistic explanations in 
order to clarify examples that are very far from the 
learner?s current knowledge. For example, English 
speakers might be confused by the variability of 
the Japanese counting system, so it might be useful 
to insert an explanation of the sort ?X is usually 
used when counting long and thin objects, but be 
aware that there are exceptions?. In the scenario of 
Table 1, the author might eventually notice that the 
learner is not aware that when speaking of some-
body else?s child a more polite reference is in or-
der, which can be fixed by giving examples 
followed by an explanation. The DOC can be used 
to draw the author?s attention to potential prob-
lems.  
Preparation of the CS is a sensitive issue in our 
model, because it is done manually while it is not 
clear at all what kind of CS people have (WordNet 
is sometimes criticized for being arbitrary, too fine, 
and omitting concepts). We were highly conserva-
tive in that only concepts that are clearly part of the 
conceptual system of English speakers before any 
exposure to Japanese were included. Our task is 
made easier by the fact that it is guided by words 
actually appearing in the corpus, whose number is 
not large, so that it took only about one hour to 
produce a reasonable CS. Example categories are 
names (for languages, places and people), places 
(park, station, toilet, hotel, restaurant, shop, etc), 
people (person, friend, wife, husband, girl, boy), 
food, drink, feelings towards something (like, 
need, want), self motion activities (arrive, come, 
return), judgments of size, numbers, etc. We also 
included language-related categories such as pro-
nouns and prepositions. 
7 Discussion 
We have presented a computational model of sec-
ond language acquisition. SLA is a central subject 
in linguistics theory and practice, and our main 
contribution is in addressing it in computational 
linguistics. The model?s learning algorithms are 
unique in their usage of a conceptual system, and 
50
its generative capacity is unique in its support for 
degrees of certainty. The model was tested on a 
unique corpus. 
The dominant trend in CL in the last years has 
been the usage of ever growing corpora. We have 
shown that meaningful learning can be achieved 
from a small corpus when the corpus has been pre-
pared by a ?good teacher?. Automatic identification 
(and ordering) of corpora subsets from which 
learning is effective should be a fruitful research 
direction for CL. 
We have shown that using a simple conceptual 
system can greatly assist language learning algo-
rithms. Previous FLA algorithms have in effect 
computed a CS simultaneously with the syntax; 
decoupling the two stages could be a promising 
direction for FLA.  
The model presented here is the first computa-
tional SLA model and obviously needs to be ex-
tended to address more SLA phenomena. It is clear 
that the powerful notion of certainty is only used in 
a rudimentary manner. Future research should also 
address constraints (e.g. for morphology and agree-
ment), recursion, explicit semantics (e.g. parsing 
into a semantic representation), word segmenta-
tion, statistics (e.g. collocations), and induction of 
new concept categories that result from the learned 
language itself (e.g. the Japanese counting system). 
An especially important SLA issue is L1 trans-
fer, which refers to the effect that the L1 has on the 
learning process. In this paper the only usage of the 
L1 part of the examples was for accessing a con-
ceptual system. Using the L1 sentences (and the 
existing conceptual system) to address transfer is 
an interesting direction for research, in addition to 
using the L1 sentences for modeling sentence se-
mantics.  
Many additional important SLA issues will be 
addressed in future research, including memory, 
errors, attention, noticing, explicit learning, and 
motivation. We also plan additional applications, 
such as automatic lesson generation. 
 
Acknowledgement. We would like to thank Dan 
Melamed for his comments on a related document.  
References  
Brown Ralf, 2000, Automated Generalization of Trans-
lation Examples, COLING ?00. 
Carl Michael, Way Andy, (eds), 2003, Recent Advances 
in Example Based Machine Translation, Kluwer. 
Chang Nancy, Gurevich Olya, 2004. Context-Driven 
Construction Learning. Proceedings, Cognitive Sci-
ence ?04.  
Chapelle Carol, 2001. Computer Applications in SLA. 
Cambridge University Press. .  
Cicekli Ilyas, Gu?venir Altay, 2001, Learning Transla-
tion Templates from Bilingual Translational Exam-
ples. Applied Intelligence 15:57-76, 2001.  
Clark Alexander, 2001. Unsupervised Language Acqui-
sition: Theory and Practice. PhD thesis, University of 
Sussex. 
Clark Eve Vivienne, 2003. First Language Acquisition. 
Cambridge University Press.   
Croft, William, 2001. Radical Construction Grammar. 
Oxford University Press.   
Edelman Shimon, Solan Zach, Horn David, Ruppin 
Eytan, 2004. Bridging Computational, Formal and 
Psycholinguistic Approaches to Language. Proceed-
ings, Cognitive Science ?04.  
Fromkin Victoria, Rodman Robert, Hyams Nina, 2002. 
An Introduction to Language, 7th ed. Harcourt. 
Gass Susan M, Selinker Larry, 2001. Second Language 
Acquisition: an Introductory Course. 2nd ed. LEA 
Publishing.  
Goldberg Adele, 1995. Constructions: a Construction 
Grammar Approach to Argument Structure. Chicago 
University Press. 
Klein Dan, 2005. The Unsupervised Learning of Natural 
Language Structure. PhD Thesis, Stanford.  
Levy Michael, 1997. Computer-Assisted Language 
Learning. Cambridge University Press.  
Maritxalar Montse, Diaz de Ilarraza Arantza, Oronoz 
Maite, 1997. From Psycholinguistic Modelling of In-
terlanguage in SLA to a Computational Model. 
CoNLL ?97.  
Medin Douglas, Ross Brian, Markman Arthur, 2005. 
Cognitive Psychology, 4th ed. John Wiley & Sons.   
Mitchell Rosamond, Myles Florence, 2003. Second 
Language Learning Theories. 2nd ed. Arnold Publica-
tion. 
Pimsleur 2005. www.simonsays.com, under ?foreign 
language instruction?.  
Somers Harold, 2001. Example-based Machine Transla-
tion. Machine Translation 14:113-158. 
Thompson Cynthia, Califf Mary Elaine, Mooney Ray-
mond, 1999. Active Learning for Natural Language 
Parsing and Information Extraction. ICML ?99.  
Tomasello Michael, 2003. Constructing a Language: a 
Usage Based Theory of Language Acquisition. Har-
vard University Press. 
51
Construction  DOC Source  Comment 
1 anata / you 0 example  
2 watashi / I  0 example  
3 anata no / your 0 example  
4 watashi no / my 0 example  
5 (<anata,watashi> no ) 0 SWD(3,4) The first words of 3 and 4 are different, the 
rest is identical. 
6 (W no), where W is <anata, 
watashi, Japanese word for 
?we?> 
-1 ECC(5)  The concept category W?={I, you, we} was 
found in the CS. We know how to say ?I? and 
?you?, but not ?we?.   
7 watashi ta chi / we  0 example  
8 (W no), where W is  
<anata, watashi, watashi ta 
chi> 
-2 ECC(6,7) We were taught how to say ?we?, and an 
empty slot for it was found in 6.  
Now we can generate a new sentence: ?wa-
tashi ta chi no?, whose meaning (?our?) is 
inferred from the meaning of construction 6. 
9 chiisai / small  0 example  
10 kuruma / car 0 example  
11 chiisai kuruma / a small car 0 example  
12 watashi ta chi no kuruma / our 
car 
0 example  
13 ((W no) kuruma) -3 SSGP (12, 
11, 10, 8) 
Shared Suffix Generated Prefix: 
(0) new example 12 = ?y x? (x: kuruma)  
(1) existing example 11 = ?z x?  
(2) existing example 10 = ?x?  
(3) construction K (#8) deriving ?y?  
learns the new construction (K x) 
Now we can generate a new sentence: ?wa-
tashi no kuruma?, meaning ?my car?.  
14 kodomo / child 0 example  
... ... 0 examples Skipping a few examples... 
20 ((W no) kodomo) -3 ... This construction was learned using the 
skipped examples.  
21 ((W no) <kuruma, kodomo>) -3 SWD (13, 
20) 
Note that the shared element is a construction 
this time, not a sub-sentence.  
22 ((W no) P), where P is the set 
of Japanese words for physi-
cal things (animate or inani-
mate)  
-4 ECC (21) The smallest category that contains the con-
cepts ?car? and ?child? is P?=PhysicalThings.  
Now we can generate many new sen-
tences, meaning ?my X? where X is any 
Japanese word we will learn in the future 
denoting a physical thing.  
Table 1: A learning scenario. For simplicity, the degree of certainty here is computed by adding that of the algorithm 
type to that of the most uncertain construction used. Note that the notation used was designed for succinct presen-
tation and is not the optimal one for authors of learning corpora (for example, it is probably easier to visualize the 
sentences generated by construction #22 if it were shown as ((<watashi, anata, watashi ta chi> no) <kuruma, 
kodomo>).) 
52
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1210?1219,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Creating a manually error-tagged and shallow-parsed learner corpus
Ryo Nagata
Konan University
8-9-1 Okamoto,
Kobe 658-0072 Japan
rnagata @ konan-u.ac.jp.
Edward Whittaker Vera Sheinman
The Japan Institute for
Educational Measurement Inc.
3-2-4 Kita-Aoyama, Tokyo, 107-0061 Japan
 
whittaker,sheinman  @jiem.co.jp
Abstract
The availability of learner corpora, especially
those which have been manually error-tagged
or shallow-parsed, is still limited. This means
that researchers do not have a common devel-
opment and test set for natural language pro-
cessing of learner English such as for gram-
matical error detection. Given this back-
ground, we created a novel learner corpus
that was manually error-tagged and shallow-
parsed. This corpus is available for research
and educational purposes on the web. In
this paper, we describe it in detail together
with its data-collection method and annota-
tion schemes. Another contribution of this
paper is that we take the first step toward
evaluating the performance of existing POS-
tagging/chunking techniques on learner cor-
pora using the created corpus. These contribu-
tions will facilitate further research in related
areas such as grammatical error detection and
automated essay scoring.
1 Introduction
The availability of learner corpora is still somewhat
limited despite the obvious usefulness of such data
in conducting research on natural language process-
ing of learner English in recent years. In particular,
learner corpora tagged with grammatical errors are
rare because of the difficulties inherent in learner
corpus creation as will be described in Sect. 2. As
shown in Table 1, error-tagged learner corpora are
very few among existing learner corpora (see Lea-
cock et al (2010) for a more detailed discussion
of learner corpora). Even if data is error-tagged,
it is often not available to the public or its access
is severely restricted. For example, the Cambridge
Learner Corpus, which is one of the largest error-
tagged learner corpora, can only be used by authors
and writers working for Cambridge University Press
and by members of staff at Cambridge ESOL.
Error-tagged learner corpora are crucial for devel-
oping and evaluating error detection/correction al-
gorithms such as those described in (Rozovskaya
and Roth, 2010b; Chodorow and Leacock, 2000;
Chodorow et al, 2007; Felice and Pulman, 2008;
Han et al, 2004; Han et al, 2006; Izumi et al,
2003b; Lee and Seneff, 2008; Nagata et al, 2004;
Nagata et al, 2005; Nagata et al, 2006; Tetreault et
al., 2010b). This is one of the most active research
areas in natural language processing of learner En-
glish. Because of the restrictions on their availabil-
ity, researchers have used their own learner corpora
to develop and evaluate error detection/correction
methods, which are often not commonly available
to other researchers. This means that the detec-
tion/correction performance of each existing method
is not directly comparable as Rozovskaya and Roth
(2010a) and Tetreault et al (2010a) point out. In
other words, we are not sure which methods achieve
the best performance. Commonly available error-
tagged learner corpora are therefore essential to fur-
ther research in this area.
For similar reasons, to the best of our knowledge,
there exists no such learner corpus that is manually
shallow-parsed and which is also publicly available,
unlike, say, native-speaker corpora such as the Penn
Treebank. Such a comparison brings up another cru-
cial question: ?Do existing POS taggers and chun-
1210
Name Error-tagged Parsed Size (words) Availability
Cambridge Learner Corpus Yes No 30 million No
CLEC Corpus Yes No 1 million Partially
ETLC Corpus Partially No 2 million Not Known
HKUST Corpus Yes No 30 million No
ICLE Corpus (Granger et al, 2009) No No 3.7 million+ Yes
JEFLL Corpus (Tono, 2000) No No 1 million Partially
Longman Learners? Corpus No No 10 million Not Known
NICT JLE Corpus (Izumi et al, 2003a) Partially No 2 million Partially
Polish Learner English Corpus No No 0.5 million No
Janus Pannoius University Learner Corpus No No 0.4 million Not Known
In Availability, Yes denotes that the full texts of the corpus is available to the public. Partially denotes that it is acces-
sible through specially-made interfaces such as a concordancer. The information in this table may not be consistent
because many of the URLs of the corpora give only sparse information about them.
Table 1: Learner corpus list.
kers work on learner English as well as on edited text
such as newspaper articles?? Nobody really knows
the answer to the question. The only exception in the
literature is the work by Tetreault et al (2010b) who
evaluated parsing performance in relation to prepo-
sitions. Nevertheless, a great number of researchers
have used existing POS taggers and chunkers to ana-
lyze the writing of learners of English. For instance,
error detection methods normally use a POS tagger
and/or a chunker in the error detection process. It is
therefore possible that a major cause of false pos-
itives and negatives in error detection may be at-
tributed to errors in POS-tagging and chunking. In
corpus linguistics, researchers (Aarts and Granger,
1998; Granger, 1998; Tono, 2000) use such tools to
extract interesting patterns from learner corpora and
to reveal learners? tendencies. However, poor per-
formance of the tools may result in misleading con-
clusions.
Given this background, we describe in this paper
a manually error-tagged and shallow-parsed learner
corpus that we created. In Sect. 2, we discuss the
difficulties inherent in learner corpus creation. Con-
sidering the difficulties, in Sect. 3, we describe our
method for learner corpus creation, including its
data collection method and annotation schemes. In
Sect. 4, we describe our learner corpus in detail. The
learner corpus is called the Konan-JIEM learner cor-
pus (KJ corpus) and is freely available for research
and educational purposes on the web1. Another
contribution of this paper is that we take the first
step toward answering the question about the per-
formance of existing POS-tagging/chunking tech-
niques on learner data. We report and discuss the
results in Sect. 5.
2 Difficulties in Learner Corpus Creation
In addition to the common difficulties in creating
any corpus, learner corpus creation has its own dif-
ficulties. We classify them into the following four
categories of the difficulty in:
1. collecting texts written by learners;
2. transforming collected texts into a corpus;
3. copyright transfer; and
4. error and POS/parsing annotation.
The first difficulty concerns the problem in col-
lecting texts written by learners. As in the case
of other corpora, it is preferable that the size of a
learner corpus be as large as possible where the size
can be measured in several ways including the total
number of texts, words, sentences, writers, topics,
and texts per writer. However, it is much more diffi-
cult to create a large learner corpus than to create a
1http://www.gsk.or.jp/index_e.html
1211
large native-speaker corpus. In the case of native-
speaker corpora, published texts such as newspa-
per articles or novels can be used as a corpus. By
contrast, in the case of learner corpora, we must
find learners and then let them write since there
are no such published texts written by learners of
English (unless they are part of a learner corpus).
Here, it should be emphasized that learners often
do not spontaneously write but are typically obliged
to write, for example, in class, or during an exam.
Because of this, learners may soon become tired of
writing. This in itself can affect learner corpus cre-
ation much more than one would expect especially
when creating a longitudinal learner corpus. Thus, it
is crucial to keep learners motivated and focused on
the writing assignments.
The second difficulty arises when the collected
texts are transformed into a learner corpus. This
involves several time-consuming and troublesome
tasks. The texts must be archived in electronic
form, which requires typing every single collected
text since learners normally write on paper. Be-
sides, each text must be archived and maintained
with accompanying information such as who wrote
what text when and on what topic. Optionally, a
learner corpus could include other pieces of infor-
mation such as proficiency, first language, and age.
Once the texts have been electronically archived, it
is relatively easy to maintain and access them. How-
ever, this is not the case when the texts are first col-
lected. Thus, it is better to have an efficient method
for managing such information as well as the texts
themselves.
The third difficulty concerning copyright is a
daunting problem. The copyright for each text
must be transferred to the corpus creator so that the
learner corpus can be made available to the public.
Consider the case when a number of learners par-
ticipate in a learner corpus creation project and ev-
eryone has to sign a copyright transfer form. This is-
sue becomes even more complicated when the writer
does not actually have such a right to transfer copy-
right. For instance, under the Japanese law, those
younger than 20 years of age do not have the right;
instead their parents do. Thus, corpus creators have
to ask learners? parents to sign copyright transfer
forms. This is often the case since the writers in
learner corpus creation projects are normally junior
high school, high school, or college students.
The final difficulty is in error and POS/parsing
annotation. For error annotation, several annota-
tion schemes exist (for example, the NICT JLE
scheme (Izumi et al, 2005)). While designing an an-
notation scheme is one issue, annotating errors is yet
another. No matter how well an annotation scheme
is designed, there will always be exceptions. Every
time an exception appears, it becomes necessary to
revise the annotation scheme. Another issue we have
to remember is that there is a trade-off between the
granularity of an annotation scheme and the level of
the difficulty in error annotation. The more detailed
an annotation scheme is, the more information it can
contain and the more difficult identifying errors is,
and vice versa.
For POS/parsing annotation, there are also a num-
ber of annotation schemes including the Brown tag
set, the Claws tag set, and the Penn Treebank tag
set. However, none of them are designed to be used
for learner corpora. In other words, a variety of lin-
guistic phenomena occur in learner corpora which
the existing annotation schemes do not cover. For
instance, spelling errors often appear in texts writ-
ten by learners of English as in sard year, which
should be third year. Grammatical errors prevent us
applying existing annotation schemes, too. For in-
stance, there are at least three possibilities for POS-
tagging the word sing in the sentence everyone sing
together. using the Penn Treebank tag set: sing/VB,
sing/VBP, or sing/VBZ. The following example is
more complicated: I don?t success cooking. Nor-
mally, the word success is not used as a verb but
as a noun. The instance, however, appears in a po-
sition where a verb appears. As a result, there are
at least two possibilities for tagging: success/NN
and success/VB. Errors in mechanics are also prob-
lematic as in Tonight,we and beautifulhouse (miss-
ing spaces)2. One solution is to split them to obtain
the correct strings and then tag them with a normal
scheme. However, this would remove the informa-
tion that spaces were originally missing which we
want to preserve. To handle these and other phe-
nomena which are peculiar to learner corpora, we
need to develop a novel annotation scheme.
2Note that the KJ corpus consists of typed essays.
1212
3 Method
3.1 How to Collect and Maintain Texts Written
by Learners
Our text-collection method is based on writing exer-
cises. In the writing exercises, learners write essays
on a blog system. This very simple idea of using a
blog system naturally solves the problem of archiv-
ing texts in electronic form. In addition, the use of a
blog system enables us to easily register and main-
tain accompanying information including who (user
ID) writes when (uploaded time) and on what topic
(title of blog item). Besides, once registered in the
user profile, the optional pieces of information such
as proficiency, first language, and age are also easy
to maintain and access.
To design the writing exercises, we consulted
with several teachers of English and conducted pre-
experiments. Ten learners participated in the pre-
experiments and were assigned five essay topics on
average. Based on the experimental results, we
designed the procedure of the writing exercise as
shown in Table 2. In the first step, learners are as-
signed an essay topic. In the second step, they are
given time to prepare during which they think about
what to write on the given topic before they start
writing. We found that this enables the students to
write more. In the third step, they actually write an
essay on the blog system. After they have finished
writing, they submit their essay to the blog system
to be registered.
The following steps were considered optional. We
implemented an article error detection method (Na-
gata et al, 2006) in the blog system as a trial at-
tempt to keep the learners motivated since learners
are likely to become tired of doing the same exercise
repeatedly. To reduce this, the blog system high-
lights where article errors exist after the essay has
been submitted. The hope is that this might prompt
the learners to write more accurately and to continue
the exercises. In the pre-experiments, the detection
did indeed seem to interest the learners and to pro-
vide them with additional motivation. Considering
these results, we decided to include the fourth and
fifth steps in the writing exercises when we created
our learner corpus. At the same time, we should of
course be aware that the use of error detection affects
learners? writing. For example, it may change the
Step Min.
1. Learner is assigned an essay topic ?
2. Learner prepares for writing 5
3. Learner writes an essay 35
4. System detects errors in the essay 5
5. Learner rewrites the essay 15
Table 2: Procedure of writing exercise.
distribution of errors. Nagata and Nakatani (2010)
reported the effects in detail.
To solve the problem of copyright transfer, we
took legal professional advice but were informed
that, in Japan at least, the only way to be sure is
to have a copyright transfer form signed every time.
We considered having it signed on the blog system,
but it soon turned out that this did not work since
participating learners may still be too young to have
the legal right to sign the transfer. It is left for our
long-term future work to devise a better solution to
this legal issue.
3.2 Annotation Scheme
This subsection describes the error and
POS/chunking annotation schemes. Note that
errors and POS/chunking are annotated separately,
meaning that there are two files for any given text.
Due to space restrictions we limit ourselves to only
summarizing our annotation schemes in this section.
The full descriptions are available together with the
annotated corpus on the web.
3.2.1 Error Annotation
We based our error annotation scheme on that used
in the NICT JLE corpus (Izumi et al, 2003a), whose
detailed description is readily available, for exam-
ple, in Izumi et al (2005). In that annotation
scheme and accordingly in ours, errors are tagged
using an XML syntax; an error is annotated by tag-
ging a word or phrase that contains it. For in-
stance, a tense error is annotated as follows: I  v tns
crr=?made?  make  /v tns  pies last year.
where v tns denotes a tense error in a verb. It
should be emphasized that the error tags contain the
information on correction together with error anno-
tation. For instance, crr=?made? in the above ex-
ample denotes the correct form of the verb is made.
For missing word errors, error tags are placed where
1213
a word or phrase is missing (e.g., My friends live
 prp crr=?in?  /prp  these places.).
As a pilot study, we applied the NICT JLE annota-
tion scheme to a learner corpus to reveal what mod-
ifications we needed to make. The learner corpus
consisted of 455 essays (39,716 words) written by
junior high and high school students3. The follow-
ing describes the major modifications deemed nec-
essary as a result of the pilot study.
The biggest difference between the NICT JLE
corpus and our targeted corpus is that the former is
spoken data and the latter is written data. This differ-
ence inevitably requires several modifications to the
annotation scheme. In speech data, there are no er-
rors in spelling and mechanics such as punctuation
and capitalization. However, since such errors are
not usually regarded as grammatical errors, we de-
cided simply not to annotate them in our annotation
schemes.
Another major difference is fragment errors.
Fragments that do not form a complete sentence of-
ten appear in the writing of learners (e.g., I have
many books. Because I like reading.). In written
language, fragments can be regarded as a grammat-
ical error. To annotate fragment errors, we added a
new tag  f  (e.g., I have many books.  f  Because
I like reading.  /f  ).
As discussed in Sect. 2, there is a trade-off be-
tween the granularity of an annotation scheme and
the level of the difficulty in annotating errors. In our
annotation scheme, we narrowed down the number
of tags to 22 from 46 in the original NICT JLE tag
set to facilitate the annotation; the 22 tags are shown
in Appendix A. The removed tags are merged into
the tag for other. For instance, there are only three
tags for errors in nouns (number, lexis, and other) in
our tag set whereas there are six in the NICT JLE
corpus (inflection, number, case, countability, com-
plement, and lexis); the other tag (  n o  ) covers
the four removed tags.
3.2.2 POS/Chunking Annotation
We selected the Penn Treebank tag set, which is
one of the most widely used tag sets, for our
3The learner corpus had been created before this reported
work started. Learners wrote their essays on paper. Unfortu-
nately, this learner corpus cannot be made available to the pub-
lic since the copyrights were not transferred to us.
POS/chunking annotation scheme. Similar to the er-
ror annotation scheme, we conducted a pilot study
to determine what modifications we needed to make
to the Penn Treebank scheme. In the pilot study, we
used the same learner corpus as in the pilot study for
the error annotation scheme.
As a result of the pilot study, we found that the
Penn Treebank tag set sufficed in most cases except
for errors which learners made. Considering this, we
determined a basic rule as follows: ?Use the Penn
Treebank tag set and preserve the original texts as
much as possible.? To handle such errors, we made
several modifications and added two new POS tags
(CE and UK) and another two for chunking (XP and
PH), which are described below.
A major modification concerns errors in mechan-
ics such as Tonight,we and beautifulhouse as already
explained in Sect. 2. We use the symbol ?-? to an-
notate such cases. For instance, the above two ex-
amples are annotated as follows: Tonight,we/NN-
,-PRP and beautifulhouse/JJ-NN. Note that each
POS tag is hyphenated. It can also be used
for annotating chunks in the same manner. For
instance, Tonight,we is annotated as [NP-PH-NP
Tonight,we/NN-,-PRP ]. Here, the tag PH stands for
 chunk label and denotes tokens which are not
normally chunked (cf., [NP Tonight/NN ] ,/, [NP
we/PRP ]).
Another major modification was required to han-
dle grammatical errors. Essentially, POS/chunking
tags are assigned according to the surface informa-
tion of the word in question regardless of the ex-
istence of any errors. For example, There is ap-
ples. is annotated as [NP There/EX ] [VP is/VBZ
] [NP apples/NNS ] ./. Additionally, we define the
CE4 tag to annotate errors in which learners use a
word with a POS which is not allowed such as in I
don?t success cooking. The CE tag encodes a POS
which is obtained from the surface information to-
gether with the POS which would have been as-
signed to the word if it were not for the error. For
instance, the above example is tagged as I don?t
success/CE:NN:VB cooking. In this format, the sec-
ond and third POSs are separated by ?:? which de-
notes the POS which is obtained from the surface
information and the POS which would be assigned
4CE stands for cognitive error.
1214
to the word without an error. The user can select
either POS depending on his or her purposes. Note
that the CE tag is compatible with the basic anno-
tation scheme because we can retrieve the basic an-
notation by extracting only the second element (i.e.,
success/NN). If the tag is unknown because of gram-
matical errors or other phenomena, UK and XP5 are
used for POS and chunking, respectively.
For spelling errors, the corresponding POS and
chunking tag are assigned to mistakenly spelled
words if the correct forms can be guessed (e.g., [NP
sird/JJ year/NN ]); otherwise UK and XP are used.
4 The Corpus
We carried out a learner corpus creation project us-
ing the described method. Twenty six Japanese col-
lege students participated in the project. At the be-
ginning, we had the students or their parents sign
a conventional paper-based copyright transfer form.
After that, they did the writing exercise described in
Sect. 3 once or twice a week over three months. Dur-
ing that time, they were assigned ten topics, which
were determined based on a writing textbook (Ok-
ihara, 1985). As described in Sect. 3, they used a
blog system to write, submit, and rewrite their es-
says. Through out the exercises, they did not have
access to the others? essays and their own previous
essays.
As a result, 233 essays were collected; Table 3
shows the statistics on the collected essays. It turned
out that the learners had no difficulties in using the
blog system and seemed to focus on writing. Out of
the 26 participants, 22 completed the 10 assignments
while one student quit before the exercises started.
We annotated the grammatical errors of all 233
essays. Two persons were involved in the annota-
tion. After the annotation, another person checked
the annotation results; differences in error annota-
Number of essays 233
Number of writers 25
Number of sentences 3,199
Number of words 25,537
Table 3: Statistics on the learner corpus.
5UK and XP stand for unknown and X phrase, respectively.
tion were resolved by consulting the first two. The
error annotation scheme was found to work well on
them. The error-annotated essays can be used for
evaluating error detection/correction methods.
For POS/chunking annotation, we chose 170 es-
says out of 233. We annotated them using our
POS/chunking scheme; hereafter, the 170 essays
will be referred to as the shallow-parsed corpus.
5 Using the Corpus and Discussion
5.1 POS Tagging
The 170 essays in the shallow-parsed corpus was
used for evaluating existing POS-tagging techniques
on texts written by learners. It consisted of 2,411
sentences and 22,452 tokens.
HMM-based and CRF-based POS taggers were
tested on the shallow-parsed corpus. The former was
implemented using tri-grams by the author. It was
trained on a corpus consisting of English learning
materials (213,017 tokens). The latter was CRFTag-
ger6, which was trained on the WSJ corpus. Both
use the Penn Treebank POS tag set.
The performance was evaluated using accuracy
defined by
number of tokens correctly POS-tagged
number of tokens  (1)
If the number of tokens in a sentence was differ-
ent in the human annotation and the system out-
put, the sentence was excluded from the calcula-
tion. This discrepancy sometimes occurred because
the tokenization of the system sometimes differed
from that of the human annotators. As a result, 19
and 126 sentences (215 and 1,352 tokens) were ex-
cluded from the evaluation in the HMM-based and
CRF-based POS taggers, respectively.
Table 4 shows the results. The second column
corresponds to accuracies on a native-speaker cor-
pus (sect. 00 of the WSJ corpus). The third column
corresponds to accuracies on the learner corpus.
As shown in Table 4, the CRF-based POS tagger
suffers a decrease in accuracy as expected. Interest-
ingly, the HMM-based POS tagger performed bet-
ter on the learner corpus. This is perhaps because it
6?CRFTagger: CRF English POS Tagger,? Xuan-Hieu Phan,
http://crftagger.sourceforge.net/, 2006.
1215
was trained on a corpus consisting of English learn-
ing materials whose distribution of vocabulary was
expected to be relatively similar to that of the learner
corpus. By contrast, it did not perform well on the
native-speaker corpus because the size of the train-
ing corpus was relatively small and the distribution
of vocabulary was not similar, and thus unknown
words often appeared. This implies that selecting
appropriate texts as a training corpus may improve
the performance.
Table 5 shows the top five POSs mistakenly
tagged as other POSs. An obvious cause of mis-
takes in both taggers is that they inevitably make
errors in the POSs that are not defined in the Penn
Treebank tag set, that is, UK and CE. A closer
look at the tagging results revealed that phenom-
ena which were common to the writing of learners
were major causes of other mistakes. Errors in cap-
italization partly explain why the taggers made so
many mistakes in NN (singular nouns). They often
identified erroneously capitalized common nouns
as proper nouns as in This Summer/NNP Vaca-
tion/NNP. Spelling errors affected the taggers in the
same way. Grammatical errors also caused confu-
sion between POSs. For instance, omission of a cer-
tain word often caused confusion between a verb and
an adjective as in I frightened/VBD. which should
be I (was) frightened/JJ. Another interesting case
is expressions that learners overuse (e.g., and/CC
so/RB on/RB and so/JJ so/JJ). Such phrases are not
erroneous but are relatively infrequent in native-
speaker corpora. Therefore, the taggers tended to
identify their POSs according to the surface infor-
mation on the tokens themselves when such phrases
appeared in the learner corpus (e.g., and/CC so/RB
on/IN and so/RB so/RB). We should be aware that
tokenization is also problematic although failures in
tokenization were excluded from the accuracies.
The influence of the decrease in accuracy on other
NLP tasks is expected to be task and/or method de-
pendent. Methods that directly use or handle se-
Method Native Corpus Learner Corpus
CRF 0.970 0.932
HMM 0.887 0.926
Table 4: POS-tagging accuracy.
HMM CRF
POS Freq. POS Freq.
NN 259 NN 215
VBP 247 RB 166
RB 163 CE 144
CE 150 JJ 140
JJ 108 FW 86
Table 5: Top five POSs mistakenly tagged.
quences of POSs are likely to suffer from it. An
example is the error detection method (Chodorow
and Leacock, 2000), which identifies unnatural se-
quences of POSs as grammatical errors in the writ-
ing of learners. As just discussed above, existing
techniques often fail in sequences of POSs that have
a grammatical error. For instance, an existing POS
tagger likely tags the sentence I frightened. as I/PRP
frightened/VBD ./. as we have just seen, and in turn
the error detection method cannot identify it as an
error because the sequence PRP VBD is not unnatu-
ral; it would correctly detect it if the sentence were
correctly tagged as I/PRP frightened/JJ ./. For the
same reason, the decrease in accuracy may affect the
methods (Aarts and Granger, 1998; Granger, 1998;
Tono, 2000) for extracting interesting sequences of
POSs from learner corpora; for example, BOS7 PRP
JJ is an interesting sequence but is never extracted
unless the phrase is correctly POS-tagged. It re-
quires further investigation to reveal how much im-
pact the decrease has on these methods. By contrast,
error detection/correction methods based on the bag-
of-word features (or feature vectors) are expected to
suffer less from it since mistakenly POS-tagged to-
kens are only one of the features. At the same time,
we should notice that if the target errors are in the
tokens that are mistakenly POS-tagged, the detec-
tion will likely fail (e.g., verbs should be correctly
identified in tense error detection).
In addition to the above evaluation, we at-
tempted to improve the POS taggers using the
transformation-based POS-tagging technique (Brill,
1994). In the technique, transformation rules are
obtained by comparing the output of a POS tagger
and the human annotation so that the differences be-
tween the two are reduced. We used the shallow-
7BOS denotes a beginning of a sentence.
1216
Method Original Improved
CRF 0.932 0.934
HMM 0.926 0.933
Table 6: Improvement obtained by transformation.
parsed corpus as a test corpus and the other man-
ually POS-tagged corpus created in the pilot study
described in Subsect. 3.2.1 as a training corpus. We
used POS-based and word-based transformations as
Brill (1994) described.
Table 6 shows the improvements together with the
original accuracies. Table 6 reveals that even the
simple application of Brill?s technique achieves a
slight improvement in both taggers. Designing the
templates of the transformation for learner corpora
may achieve further improvement.
5.2 Head Noun Identification
In the evaluation of chunking, we focus on head
noun identification. Head noun identification often
plays an important role in error detection/correction.
For example, it is crucial to identify head nouns to
detect errors in article and number.
We again used the shallow-parsed corpus as a test
corpus. The essays contained 3,589 head nouns.
We implemented an HMM-based chunker using 5-
grams whose input is a sequence of POSs, which
was obtained by the HMM-based POS tagger de-
scribed in the previous subsection. The chunker was
trained on the same corpus as the HMM-based POS
tagger. The performance was evaluated by recall and
precision defined by
number of head nouns correctly identified
number of head nouns (2)
and
number of head nouns correctly identified
number of tokens identified as head noun  (3)
respectively.
Table 7 shows the results. To our surprise, the
chunker performed better than we had expected. A
possible reason for this is that sentences written by
learners of English tend to be shorter and simpler in
terms of their structure.
The results in Table 7 also enable us to quanti-
tatively estimate expected improvement in error de-
tection/correction which is achieved by improving
chunking. To see this, let us define the following
symbols:  : Recall of head noun identification, 	 :
recall of error detection without chunking error, 
	
recall of error detection with chunking error. 	 and

	 are interpreted as the true recall of error detection
and its observed value when chunking error exists,
respectively. Here, note that 
	 can be expressed
as 
		 . For instance, according to Han et al
(2006), their method achieves a recall of 0.40 (i.e.,

	

), and thus 	

assuming that chunk-
ing errors exist and recall of head noun identification
is 

 just as in this evaluation. Improving  to

