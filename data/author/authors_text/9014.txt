Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1232?1240,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Accurate Semantic Class Classifier for Coreference Resolution
Zhiheng Huang1, Guangping Zeng1,2, Weiqun Xu3, and Asli Celikyilmaz1
1EECS Department, University of California at Berkeley,
CA 94720, USA
{zhiheng,gpzeng,asli}@eecs.berkeley.edu
2Computer Science Department, School of Information Engineering,
University of Science and Technology Beijing, China
3ThinkIT, Institute of Acoustics, Chinese Academy of Sciences,
Beijing, 100190, China
xuweiqun@hccl.ioa.ac.cn
Abstract
There have been considerable attempts
to incorporate semantic knowledge into
coreference resolution systems: different
knowledge sources such as WordNet and
Wikipedia have been used to boost the per-
formance. In this paper, we propose new
ways to extract WordNet feature. This
feature, along with other features such as
named entity feature, can be used to build
an accurate semantic class (SC) classifier.
In addition, we analyze the SC classifica-
tion errors and propose to use relaxed SC
agreement features. The proposed accu-
rate SC classifier and the relaxation of SC
agreement features on ACE2 coreference
evaluation can boost our baseline system
by 10.4% and 9.7% using MUC score and
anaphor accuracy respectively.
1 Introduction
Coreference resolution is used to determine which
noun phrases (including pronouns, proper names,
and common nouns) refer to the same entities in
documents. Much work on coreference resolution
is based on (Soon et al, 2001), which built a de-
cision tree classifier to label pairs of mentions as
coreferent or not. Recent work aims to improve
the performance from two aspects: new models
and new features. The former cast the pair wise
mention classifications into various forms such as
the best path in a Bell tree (Luo et al, 2004), the
best graph cut (Nicolae and Nicolae, 2006), in-
teger linear programming (Denis and Baldridge,
2007) and graph partition based conditional model
(McCallum and Wellner, 2004). The latter de-
velop and investigate new linguistic features for
the problem. For instance, WordNet (Poesio et al,
2004), Wikipedia (Ponzetto and Strube, 2006), se-
mantic neighbor words (Ng, 2007a), and pattern
based features (Yang and Su, 2007) have been ex-
tensively studied.
Deeper linguistic knowledge is required to en-
able the coreference resolution to reach a higher
level of performance (Kehler et al, 2004). An im-
portant type of semantic knowledge that has been
employed in coreference resolution system is the
semantic class (SC) of an NP, which can be used
to filter out the coreference between semantically
incompatible NPs. However, the difficulty is to
accurately compute the semantic class features. In
this paper, we show that the WordNet may not be
efficiently employed in the traditional way such
as (Soon et al, 2001; Ng, 2007a; Ponzetto and
Strube, 2006) to compute the semantic class fea-
tures. We introduce new ways to use the WordNet
and the experiments show its effectiveness in de-
termining the semantic classes for noun phrases.
In addition, we analyze the classification errors of
the SC classifier and propose to use relaxed SC
agreement features. With these proposed features
and other standard syntactic features (which are
commonly employed in existing coreference sys-
tems), our coreference resolution system can ob-
tain an increase of 10.4% for MUC score and 9.7%
for anaphor accuracy from the baseline in ACE2
evaluation.
2 Related Work
WordNet (Fellbaum, 1998) as an important knowl-
edge source has been widely employed in previ-
ous coreference resolution work. For example,
Harabagiu et al (2001) have used WordNet rela-
tions such as synonym and is-a to mine the pat-
terns of WordNet paths for pairs of antecedents
and anaphors. Due to the nature of the rule based
coreference system (in contrast to machine learn-
ing based), the weights of relations may not be
accurately estimated. Vieira and Poesio (2000)
and Markert and Nissim (2005) have used Word-
Net synonym and hyponym etc. to determine if
an anaphor semantically relates to one previous
NP. Ponzetto and Strube (2006) have used Word-
Net semantic similarity and relatedness scores be-
tween antecedents and candidate anaphors. Their
1232
work is different to this work in the following: 1)
Their work involves various relations such as hy-
ponyms and meronyms while ours only makes use
of hypernyms; and 2) Their work focuses on in-
vestigating if two NPs have particular WordNet re-
lations or not, while ours focuses on using Word-
Net hypernyms for their SC classification and then
testing their SC compatibility. In doing so, we can
directly model the accuracy of semantic class clas-
sification and test its impact on coreference reso-
lution.
While the SC of a proper name is computed
fairly accurately using a named entity (NE) recog-
nizer, many coreference resolution systems sim-
ply assign to a common noun the first (i.e., most
frequent) WordNet synset as its SC (Soon et al,
2001; Markert and Nissim, 2005). This heuris-
tics, apparently, did not lead to good performance.
The best reported ACE2 coreference resolution
system (Ng, 2007a; Ng, 2007b) has proposed an
accurate SC classifier which used heterogeneous
semantic knowledge sources. WordNet is just
one of the several knowledge sources which have
been utilized. However, the WordNet based fea-
tures is not informative compared to other features
such as the semantic neighbor feature. Similarly,
Ponzetto and Strube (2006) have discovered that
the WordNet feature is no more informative than
the community-generated Wikipedia feature. In
this paper, we focus on the investigation of vari-
ous usages of WordNet for the SC classification
task. The work which is directly comparable to
ours would be (Ng, 2007a; Ng, 2007b).
Other similar work includes the mention detec-
tion (MD) task (Florian et al, 2006) and joint
probabilistic model of coreference (Daume? III and
Marcu, 2005). The MD task identifies the bound-
ary of a mention, its mention type (e.g., pronoun,
name), and its semantic type (e.g., person, orga-
nization). Unlike them, we do not perform the
boundary detection, as we make use of the noun
phrases directly from the noun phrase chunker and
NE recognizer. The joint probabilistic model mod-
els the MD and coreference simultaneously, while
our work focuses on them separately.
3 Semantic Class Classification
In this section, we describe how we compile the
training corpus and extract features using Word-
Net. We report our results on the ACE coreference
corpus due to that it has been commonly used and
it was annotated SCs of six types.1 As in (Ng,
1Person, organization, gpe, location and facility are ex-
plicitly annotated. The rest noun phrases are other type.
2007a), we first train a classifier to predict the SC
of an NP. This SC information is used later in the
coreference resolution stage. For example, the au-
dience is classified as SC of person, and it thus
should not be coreferent with the security industry,
which is usually classified as organization. This
task is by no means trivial. First, while the classi-
fication of Tom Hanks being SC of person can be
accurately achieved by an NE recognizer, the as-
sociation of audience and person requires seman-
tic language source such as WordNet. Second, the
same noun phrase can be annotated with different
SCs under different context. For example, the au-
thorities is usually annotated as person, but it is
sometimes as organization. Even worse, the same
noun phrases are sometimes annotated with one of
the five explicitly annotated classes while some-
times are not annotated at all (thus falling into the
other SC). For example, people is annotated as
person SC explicitly 20 times and is not annotated
at all 21 times in the ACE2 testset. This inconsis-
tent annotation adversely affects the performance
of an SC classifier. And this in turn would cause
errors during coreference stage. In section 4.3, we
show how to relax the strict SC agreement feature
to address this.
3.1 Training instance creation
We use ACE Phase 2 Coreference corpus to train
the SC classifier. Each noun phrase which is iden-
tified by the noun phrase chunker or NE recognizer
is used to create a training instance. Each instance
is represented by a set of lexical, syntactic and se-
mantic features, as described below. If the NP un-
der consideration is annotated as one of the five
ACE SCs in the corpus, then the classification of
the associated training instance is the ACE SC of
the NP. Otherwise, the instance is labeled as other.
ACE 2 corpus has a training set and a test set
which comprise of 422 and 97 texts respectively.
We divide the training set into a new training and a
development set: the former consists of 90% ran-
domly generated and stratified original training in-
stances and the latter consists of the rest 10% in-
stances. The test set remains the same as in ACE2
corpus. The size of each dataset and its SC dis-
tributions are shown in Table 1. Note that the
training and development datasets have exactly the
same distributions of SCs due to the stratification
procedure. That is, each class has the same pro-
portion in training and development datasets. We
tune the feature parameters against development
set and report performance on both development
set and test set.
1233
Table 1: Distributions of SCs in ACE2 corpus.
Size PER ORG GPE FAC LOC OTH
Train 55629 20.29 7.30 8.42 0.61 0.55 62.80
Dev 6181 20.29 7.30 8.42 0.61 0.55 62.80
Test 15360 20.48 7.57 6.90 0.85 0.41 63.79
3.2 Lexical features
Each instance is represented as a bag of features
and is fed into a classifier in training stage. We
present four binary lexical feature sets as follows.
Word unigrams and bigrams: An N-gram is
a sub-sequence of N words from a given noun
phrase. Unigram forms the bag of words feature,
and bigram forms the pairs of words feature, and
so forth. We have considered word unigram and
bigram features in our experiments.
First and last words: This feature extracts the
first and last words of an NP. For example, the first
word the and the last word store are extracted from
the NP the main store. This feature does not only
coarsely models the influence of the first word, for
example, a or the, but also models the head word,
since the head word usually is the last word in the
NP.
Head word: We use Collins style rules
(Collins, 1999) to extract the head words for given
NPs. These features should be most informative
if the training corpus is large enough.2 For exam-
ple, the head word company of the NP the com-
pany immediately determines its SC being organi-
zation. However, due to the sparseness of training
data, its potential importance is adversely affected.
3.3 Semantic features
NE feature is extracted from Stanford named en-
tity recognizer (NER) (Finkel et al, 2005). Three
types of named entities: person, location and or-
ganization can be recognized for a given NP. This
feature is primarily useful for SC classification of
proper nouns.
WordNet is a large English lexicon in which se-
mantically related words are connected via cogni-
tive synonyms (synsets). The WordNet is a use-
ful tool for word semantics analysis and has been
widely used in natural language processing appli-
cations. In WordNet, synsets are organized into hi-
erarchies with hypernym/hyponym relationships:
Y is a hypernym of X if every X is a (kind of) Y
(X is called a hyponym of Y in this case).
The WordNet is employed in (Ng, 2007a) as
following to create the WN CLASS feature. For
each keyword w as shown in the right column of
2It, however, is mostly useful for nominal noun phrase and
not for the pronoun and proper noun phrases.
Table 2, if the head noun of a given NP is a hy-
ponym of w in WordNet,3 then the word w be-
comes a feature for such NP. It is explained that
these keywords are correlated with the ACE SCs
and they are obtained via experimentation with
WordNet and the ACE SCs of the NPs in the ACE
training data. However, it is likely that these hand-
crafted keywords have poor coverage for general
cases. As a result, it may not make full use of
WordNet semantic knowledge. This will be shown
in our individual feature contribution experiment
in Section 3.5.
Table 2: List of keywords used in WordNet seman-
tic feature in (Ng, 2007a).
ACE SC Keywords
PER person
ORG social group
FAC establishment, construction, building,
facility, workplace
GPE country, province, government, town, city,
administration, society, island, community
LOC dry land, region, landmass, body of water
geographical area, geological formation
There are other ways of using WordNet for se-
mantic feature extraction. For example, Ponzetto
and Strube (2006) have employed WordNet sim-
ilarity measure for coreference resolution. The
difference is that they created the feature di-
rectly at the coreference resolution stage, ie, us-
ing the WordNet similarity between the antecedent
and anaphor to determine if they are coreferent,
while we focus on using this feature to classify
an NP into a particular SC. For comparison, we
implemented a WordNet similarity based feature
(WN SIM) as follows: for a given NP head word
and a key word as listed in Table 2, the WordNet
similarity package (Seco et al, 2004) models the
length of path traveling from the head word to the
key word over the WordNet network. It then com-
putes the semantic similarity based on the path.
For example, the similarity between company and
social group is 0.77, while the similarity between
company and person is 0.59. The key word which
receives the highest similarity to the head word is
marked as a feature.
The WN CLASS feature may suffer from the
coverage problem and the WN SIM feature is
heavily dependent on the definition of similarity
metric which may turn out to be inappropriate for
coreference resolution task. To make better use of
WordNet knowledge, we attempt to directly intro-
duce hypernyms for the NP head words (we denote
3Only the first synset of the NP is used.
1234
it as WN HYP feature). The most similar work
to ours is (Daume? III and Marcu, 2005), in which
two most common synsets from WordNet for all
words in an NP and their hypernyms are extracted
as features. We avoid augmenting the hypernyms
for non-head words in the NP to prevent introduc-
ing noisy information, which may potentially cor-
rupt the hypernym feature space.
Considering a WordNet hypernym structure as
shown in Fig. 1 for the word company, its first
synset (an institution created to conduct business)
has a unique id of 08058098 and can also be rep-
resented by a set of description words (company
in this case). Its third synset (the state of being
with someone) has an id of 13929588 and descrip-
tion words of company, companionship, fellow-
ship, society. Each synset can be extended by its
hypernym synsets. For example, the direct hyper-
nym of the first synset is the synset of 08053576
which can be described as institution, establish-
ment. The augmentation of hypernyms for NP
head words can introduce useful information, but
can also bring noise if the head word or the synset
of head word are not correctly identified. For an
optimal use of WordNet hypernyms, four ques-
tions shall be addressed: 1) how many depths are
required to tradeoff the generality (thus more in-
formative) and the specificity (thus less noisy)? 2)
which synset of the given word is needed to be
augmented? 3) which representation (synset id or
synset word) is better? and 4) is it helpful to en-
code the hypernym depth into the hypernym fea-
ture?4 These four questions provide the guideline
to search the optimal use of WordNet. We will de-
sign experiments in Section 3.5 to determine the
optimal configuration of WN HYP feature.
state
(08008335)
(13931145)
(13928668)
(08053576)
(07950920)
company
depth 2
depth 3
depth 4
depth 1
social group
institution,establishment
organization,organisation
(00024720)
friendship,friendlyrelationship
fellowship,society
(13929588)(08058098)
company, companionship,
company
relationship
Figure 1: WordNet hypernym hierarchy for the
word company.
4For example, we encode the synset 08053576 as
08053576-1, with the last digit 1 indicating the depth of hy-
pernym with regard to the entry word company.
3.4 Learning algorithm
Maximum entropy (ME) models (Berger et al,
1996; Manning and Klein, 2003), also known as
log-linear and exponential learning models, has
been adopted in the SC classification task. Max-
imum entropy models can integrate features from
many heterogeneous information sources for clas-
sification. Each feature corresponds to a constraint
on the model. Given a training set of (C,D),
where C is a set of class labels and D is a set
of feature represented data points, the maximum
entropy model attempts to maximize the log like-
lihood
logP (C|D,?) =
?
(c,d)?(C,D)
log
exp
?
i
?
i
f
i
(c, d)
?
c
?
exp
?
j
?
j
f
i
(c, d)
(1)
where f
i
(c, d) are feature indicator functions and
?
i
are the parameters to be estimated. We use ME
models for both SC classification and mention pair
classification.
3.5 SC classification evaluation
We design three experiments to test the accuracy
of our classifiers. The first experiment evalu-
ates the individual contribution of different fea-
ture sets to SC classification accuracy. In par-
ticular, a ME model is trained on the 55,629
training instances using the following feature sets
separately: 1) unigram, 2) bigram, 3) first-last
word, 4) head word (HW), 5) named entities
(NE), 6) HW+WN CLASS, 7) HW+WN SIM,
and 8) variants of HW+WN HYP. Note that
HW+WN CLASS is the semantic feature used in
(Ng, 2007a), HW+WN SIM is the semantic fea-
ture using WordNet similarity measure (Seco et
al., 2004), and variants of HW+WN HYP are the
work proposed in this paper. We combine head
word and the semantic features due to the fact that
WordNet features are dependent on head words
and they could be treated as units. In the second
experiment, features are fed into the ME model
incrementally until all features have been used.5
Finally, we perform the feature ablation experi-
ments. That is, we remove one feature at a time
from the entire feature set and test the accuracy
loss. The SC classification performance is mea-
sured by accuracy, i.e., the proportion of the cor-
rectly classified instances among all test instances.
Individual feature contribution Table 3 shows
the SC classification accuracy of all NPs (all)
and non-pronoun NPs (non-PN) on the develop-
ment and test datasets using individual feature
5The optimal of HW+WN HYP configuration is used.
1235
sets. Among all the lexical features, unigram fea-
Table 3: SC classification accuracy of ME using
individual feature sets for development and test
ACE2 datasets.
Feature type dev test
all non-PN all non-PN
unigram 81.3 81.6 72.4 71.9
bigram 32.5 36.4 26.3 28.4
first-last word 80.1 80.2 71.6 71.0
HW 78.2 78.0 68.3 67.1
NE 74.0 82.8 73.1 81.9
HW+WN CLASS 79.5 79.4 70.3 69.5
HW+WN SIM 81.2 81.4 73.8 73.6
HW+WN HYP (1) 82.6 83.1 74.8 74.7
HW+WN HYP (3) 82.8 83.4 75.2 75.2
HW+WN HYP (6) 83.1 83.7 75.6 75.7
HW+WN HYP (9) 83.0 83.6 75.7 75.7
HW+WN HYP (?) 83.1 83.7 75.8 75.9
HW+WN HYP (6) 82.8 83.3 75.6 75.7
word form
HW+WN HYP (6) 82.9 83.5 75.4 75.4
depth encoded
HW+WN HYP (6) 83.0 83.6 76.4 76.6
first synset
ture performs the best (81.3%) for all NPs over the
development dataset. The bigram feature performs
poorly due to the sparsity problem: NPs usually
consist of one to three words. The first-last word
feature effectively models the prefix words (such
as a and the) and the head words and thus obtains a
reasonably high accuracy of 80.1%. As mentioned
before, the head word feature may suffer from the
sparsity and it results in the accuracy of 78.2%.
We also list the accuracies for non-pronoun NP
SC classification, which are slightly different com-
pared to all NP SC classification except for bi-
gram, in which the accuracy has increased 3.9%.
Although Stanford NER performs well on
named entity recognition task, it results in ac-
curacy of 74.0% for all NP SC classification,
due to its inability to deal with pronouns such
as he and common nouns such as the govern-
ment. The removal of pronouns significantly
boosts its accuracy to 82.8%. The introduc-
tion of semantic feature HW+WN CLASS boosts
the performance to 79.5% compared to the head
word alone of 78.2%. This conforms to (Ng,
2007a) that only small gain can be achieved us-
ing WN CLASS feature. The HW+WN SIM
feature outperforms HW+WN CLASS and the
accuracy reaches 81.2%. For the variants of
HW+WN HYP, we first search the optimal depth.
This is performed by using all synsets for NP head
word, encoding the feature using synset id (rather
than synset word), and no hypernym depth is en-
coded in the features. We try various depths of
1, 3, 6, 9 and ?, with ? signifies that no depth
constraint is imposed. The optimal depth of 6 is
obtained with the accuracy of 83.1% over the de-
velopment dataset. We then fix the depth of 6 to try
using synset word as features, using synset id with
depth encoded as features, and using first synset
only. The results show that the optimum is to en-
code the features using hypernym synset id with-
out hypernym depth information and all synsets
are considered for hypernym extraction. This is
slightly different from the previous finding (Soon
et al, 2001; Lin, 1998b) that a coreference res-
olution system employing only the first WordNet
synset performs slightly better than that employ-
ing more than one synset.6 The best result reaches
the accuracy of 83.1%. Although the best seman-
tic feature only outperforms the best lexical fea-
ture by 1.8% on the development dataset, its gain
in the test dataset is more significant (3.2%, from
72.4% to 75.6%).
Incremental feature contribution Once we
use the training and development datasets to find
the optimal configuration of HW+WN HYP se-
mantic feature, we use all lexical features and the
optimal HW+WN HYP feature incrementally to
train an ME model over the combination of train-
ing and development datasets. Table 4 shows
the SC classification accuracy of all NPs (all)
and non-pronoun NPs (non-PN) on the train-
ing+development (we refer it as training hereafter)
and test datasets.
Table 4: SC classification accuracy of ME using
incremental feature sets for training and test ACE2
datasets.
Feature type train test
all non-PN all non-PN
HW 87.8 89.0 68.6 67.6
+WN HYP 87.8 89.0 75.7 75.8
+unigram 91.5 93.3 77.7 78.1
+bigram 93.1 95.2 78.7 79.2
+first-last word 93.2 95.3 78.8 79.3
+NE 93.4 95.6 83.1 84.4
Ng 2007a - 85.0 - 83.3
Note that the significant higher accuracies in
training compared to test are due to the overfit-
ting problem. The interesting evaluation thus re-
mains on the test data. As can be seen, the in-
clusion of more features results in higher perfor-
mance. This is more obvious in the test dataset
than in the training dataset. The inclusion of the
6In fact, the accuracy of the test data supports their claims.
The accuracy using the first synset compared to using all
synsets results in the accuracy increase from 75.6% to 76.4%
for all NPs over the test dataset.
1236
optimized WN HYP feature (ie, using all synsets?
hypernyms up to 6 depth and with synset id encod-
ing) results in 7.1% increase for all NP SC classifi-
cation over test data. This shows the effectiveness
of the WN HYP features to overcome the sparsity
of head word feature. The unigram, bigram and
first-last word features offer reasonable accuracy
gain, and the final inclusion of NE boosts the over-
all performance to 83.1% for all NP and 84.4% for
non pronoun NPs over test data. This result can
be directly compared to the SC classification ac-
curacy as reported in (Ng, 2007a), in which the
highest accuracy is 83.3% for non pronoun NPs.7
The large difference between the highest training
accuracies is due to that our classifier is trained di-
rectly on the ACE2 training dataset, while their SC
classifier was trained on BBN Entity Type Corpus
(Weischedel and Brunstein, 2005), which is five
times larger than the ACE2 corpus used by us. In
addition to WordNet, they have adopted multiple
knowledge sources which include BBN?s Identi-
Finder (this is equivalent to the Stanford NER in
our work), BLLIP corpus and Reuters Corpus,8
and dependency based thesaurus (Lin, 1998a). It
is remarkable that our SC classifier can achieve
even higher accuracy only using WordNet hyper-
nym and NE features. It is worth noting that the
small accuracy gain is indeed hard to achieve con-
sidering that the test data size is large (15360).
Feature ablation experiment We now perform
the feature ablation experiments to further deter-
mine the importance of individual features. We re-
move one feature at a time from the entire feature
set. Table 5 shows the SC classification accuracy
of all NPs (all) and non-pronoun NPs (non-PN) on
the training and test datasets respectively.
Table 5: SC classification accuracy of ME by re-
moving one feature at a time for training and test
ACE2 datasets.
Feature type train test
all non-PN all non-PN
overall 93.4 95.6 83.1 84.4
-HW 93.4 95.5 82.9 84.2
-WN HYP 93.4 95.5 82.6 83.8
-HW+WN HYP 93.4 95.5 82.3 83.5
-unigram 93.4 95.5 82.9 84.2
-bigram 92.5 94.5 82.7 84.0
-first-last word 93.4 95.5 82.9 84.1
-NE 93.2 95.3 78.8 79.3
Again, the significant higher accuracies in train-
ing compared to test are due to overfitting. The re-
7All NP accuracy was not reported as they excluded the
pronouns in creating their training and test data.
8They use these corpus to extract patterns to induce SC of
common nouns.
moval of NE feature results in the largest accuracy
loss of 4.3% (from 83.1% to 78.8%) for all nouns
on test data. It follows WN HYP (0.5% loss) and
the bigram (0.4%). If we treat HW+WN HYP as
one feature, the removal of it results in accuracy
loss of 0.8% for all nouns on test data. The un-
igram, first-last word and head word each results
in the loss of 0.2%. The reason that the removal
of NE results in a much significant loss is due to
the fact that the NE feature is quite different from
other features. Its strength is to distinguish SCs for
proper names, while other features are more sim-
ilar (their targets are common nouns). The pro-
posed use of HW+WN HYP can bring 0.8% gain
on top of other features, higher than other informa-
tive lexical features including unigram and first-
last word.
3.6 Error analysis
A closer look at the errors produced by our SC
classifier reveals that the second probable label is
very likely to be the actual labels if the first proba-
ble one is wrong. In fact, if we allow the classifier
to predict two most probable labels and the clas-
sification is judged to be true if the actual label is
one of the two predictions, then the classification
accuracy increases from 83.1% to 96.4%. This
is because that the same noun phrases are some-
times annotated with one of the five explicitly an-
notated classes while sometimes are not annotated
at all (thus falling into the other SC). Again for
the example of people. It is annotated as person
SC 20 times and is not annotated at all 21 times.
Given the same feature set for this instance, the
best the classifier can do is to classify it to other
semantic class. To address this annotation incon-
sistency issue, we relax the SC agreement feature
from the strict match in designing coreference res-
olution features. For example, if the first probable
SC of an NP matches the second probable SC of
another NP, we still give some partial match credit.
4 Application to Coreference Resolution
We can now incorporate the NP SC classifier into
our ME based coreference resolution system. This
section examines how our WordNet hypernym fea-
tures help improve the coreference resolution per-
formance.
4.1 Experimental setup
We use the ACE-2 (version 1.0) coreference cor-
pus. Each raw text in this corpus was prepro-
cessed automatically by a pipeline of NLP com-
ponents, including sentence boundary detection,
1237
POS-tagging and text chunking. The statistics of
corpus and mention extraction are shown in Table
6, where g-mention is the automatically extracted
mentions which contain the annotated (gold) men-
tions. The recalls of gold mentions are 95.88%
and 95.93% for training and test data respectively.
Table 6: Statistics for corpus and extracted men-
tions.
text# mention# g-mention# gold# recall(%)
train 422 61810 22990 23977 95.88
test 97 15360 5561 5797 95.93
Our coreference system uses Maximum En-
tropy model to determine whether two NPs are
coreferent. As in (Soon et al, 2001; Ponzetto and
Strube, 2006), we generate training instances as
follows: a positive instance is created for each
anaphoric NP, NP
j
, and its closest antecedent,
NP
i
; and a negative instance is created for NP
j
paired with each of the intervening NPs, NP
i+1
,
NP
i+2
, ..., NP
j?1
. Each instance is represented
by syntactic or semantic features described as fol-
lows. All training data are used to train a maxi-
mum entropy model. In the test stage,we select the
closest preceding NP that is classified as corefer-
ent with NP
j
as the antecedent of NP
j
. If no such
NP exists, no antecedent is selected for NP
j
.
Unlike other natural language processing tasks
such as information extraction which have de facto
evaluation metrics, it is an open question which
evaluation is the most suitable one. The evalu-
ation becomes more complicated when automat-
ically extracted mentions (in contrast to the gold
mentions) are used. To facilitate the comparison
with previous work, we report performance us-
ing two different scoring metrics: the commonly-
used MUC scorer (Vilain et al, 1995) and the ac-
curacy of the anaphoric references (Ponzetto and
Strube, 2006). An anaphoric reference is correctly
resolved if it and its closest antecedent are in the
same coreference chain in the resulting partition.
4.2 Baseline features
We briefly review the baseline features used in
this paper as follows. More detailed information
and implementations can be found at (Soon et al,
2001; Versley et al, 2008). For example, the
ALIAS feature takes values of true or false. The
value of true means that the antecedent and the
anaphor refer to the same entity (date, person, or-
ganization or location). The ALIAS feature de-
tection works differently depending on the named
entity type. For date, the day, month, and year
values are extracted and compared. For person,
the last words of the noun phrases are compared.
For organization names, the alias detection checks
for acronym match such as IBM and International
Business Machines Corp.
Lexical features STRING MATCH: true if
NP
i
and NP
j
have the same spelling after remov-
ing article and demonstrative pronouns, false oth-
erwise. ALIAS: true if NP
j
is the alias of NP
i
.
Grammatical features I PRONOUN: true if
NP
i
is a pronoun; J PRONOUN: true if NP
j
is pronoun; J REFL PRONOUN: true if NP
j
is
reflexive pronoun; J PERS PRONOUN: true if
NP
j
is personal pronoun; J POSS PRONOUN:
true if NP
j
is possessive pronoun; J PN: true
if NP
j
is proper noun; J DEF: true if NP
j
starts with the; J DEM: true if NP
j
starts with
this, that, these or those; J DEM NOMINAL:
true if NP
j
is a demonstrative nominal noun;
J DEM PRONOUN: true if NP
j
is a demonstra-
tive pronoun; PROPER NAME: true if both NP
i
and NP
j
are proper names; NUMBER: true if
NP
i
and NP
j
agree in number; GENDER: true
if NP
i
and NP
j
agree in gender; APPOSITIVE:
true if NP
i
and NP
j
are appositions.
Distance feature DISTANCE: how many sen-
tences NP
i
and NP
j
are apart.
Semantic feature SEMCLASS: This feature is
implemented from (Soon et al, 2001). Its possible
values are true, false, or unknown. First the fol-
lowing semantic classes are defined: female, male,
person, organization, location, date, time, money,
percent, and object. Each of these defined seman-
tic classes is then mapped to a WordNet synset.
Then the semantic class determination module de-
termines the semantic class for every NP as the
first synset of the head noun of the NP. If such
synset is a hyponym of defined semantic class,
then such semantic class is assigned to the NP.
Otherwise, unknown class is assigned. Finally, the
agreement of semantic classes of NP
i
and NP
j
is
unknown if either assigned class is unknown; true
if their assigned class are the same, false other-
wise. Notice that the WordNet use in (Ng, 2007a)
and this feature apply in the same principle except
that 1) the former is used in SC classification while
the latter is used directly for coreference resolu-
tion, and 2) they have different semantic class cat-
egories.
4.3 Proposed WordNet agreement features
For each instance which consists of NP
i
and NP
j
,
we apply our SC classifier to label them, say l
i
and
l
j
respectively. We then use these two induced la-
1238
bels to propose the SC agreement feature for NP
i
and NP
j
. In particular, SC STRICT is true if l
i
and l
j
are the same and they are not of other type,
false otherwise; SC COARSE is true if both l
i
and
l
j
are not of other type; In addition, we propose
two other SC agreement features to cope with the
SC classification errors. SC RELAX1 is true if the
first probable of NP
i
, l
i1
, is not other type and is
the same as the second probable of N
j
, l
j2
, or vice
visa. SC RELAX2 is true if the second probable
of NP
i
, l
i2
, is not other type and is the same as the
second probable label of NP
j
, l
j2
. The purpose in
using SC RELAX1 and SC RELAX2 features is
to relax the strict SC agreement feature in the hope
that partial SC match is useful for coreference res-
olution.
4.4 Coreference results
Table 7 shows the MUC score for ACE2 corpus
and its three partitions: bnews, npaper, and nwire
using baseline and the proposed semantic features.
It also shows the accuracy of resolving anaphors
for all nouns in ACE2 corpus. SC STRICT is
the configuration that uses the baseline features
with the SEMCLASS (Soon et al, 2001) replaced
by SC STRICT, and SC COARSE, SC RELAX1,
and SC RELAX2 are incrementally included into
the SC STRICT feature set.
As can be seen, the SC STRICT significantly
boosts the performance: it improves the MUC
F score and anaphor accuracy of baseline from
57.7% to 65.7% and 37.7% to 46.3% respectively.
It is remarkable that the new use of WordNet can
obtain such significant gain in both MUC score
and anaphor accuracy. The large improvement
of the precision from 58.1% to 73.3% for all
NPs shows that the SC STRICT feature can ef-
fectively filter out the semantic incompatible pairs
of antecedents and anaphors. In accordance with
our hypothesis, the relaxation of strict SC agree-
ment by including SC COARSE, SC RELAX1
and SC RELAX2 help improve the performance
further, which is reflected by both MUC score and
anaphor accuracy. For example, compared to the
baseline, the use of all proposed four SC agree-
ment features results in the maximal accuracy gain
of 9.7% (from 37.7% to 47.4%) and the use of
SC STRICT, SC COARSE, and SC RELAX1 re-
sults in the maximal MUC score gain of 10.4%
(from 57.7% to 68.1%).
Our best MUC score is 68.1% which outper-
forms the MUC score of 64.6% as reported in
(Ng, 2007a) by 3.5%, while our best accuracy
of anaphor is 47.4%, which is 4.1% less than
the accuracy of 51.5% in (Ng, 2007a). Note
that, unlike (Ng, 2007a) which performed exten-
sive experiments using different machine learn-
ing algorithms, alternative use of features (either
constraint or normal features), and heterogeneous
knowledge sources, this paper simply uses one
learning classifier (ME model) and only employs
WordNet and Stanford NER semantic sources.
The different MUC and accuracy scores reflect
the non-trivial cases of evaluating coreference sys-
tems. While we leave out the discussion of which
evaluation is more appropriate, we focus on show-
ing that the proposed SC classifier can bring sig-
nificant boost from the baseline using both MUC
and accuracy metrics.
5 Conclusion
We have showed that the traditional use of Word-
Net in coreference resolution may not effectively
exploit the WordNet semantic knowledge. We pro-
posed new ways to extract WordNet feature. This
feature, along with other features such as named
entity feature, can be used to build an accurate se-
mantic class (SC) classifier. In addition, we ana-
lyzed the classification errors of the SC classifier
and relaxed SC agreement features to cope with
part of the classification errors. The proposed ac-
curate SC classifier and the relaxation of SC agree-
ment features can boost our baseline coreference
resolution system by 10.4% and 9.7% using MUC
score and anaphor accuracy respectively.
Acknowledgments
We wish to thank Yannick Versley for his sup-
port with BART coreference resolution system and
the three anonymous reviewers for their invaluable
comments. This research was supported by British
Telecom grant CT1080028046 and BISC Program
of UC Berkeley.
References
A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
M. Collins 1999. Head-driven statistical models for
natural language parsing. PhD thesis, University of
Pennsylvania.
H. Daume? III and D. Marcu. 2005. A large-scale
exploration of effective global features for a joint
entity detection and tracking model. In Proc. of
HLT/EMNLP, pages 97-104.
1239
Table 7: MUC score and accuracy of baseline and proposed SC agreement features for ACE2 dataset.
MUC score Accuracy
All bnews npaper nwire All
System R P F R P F R P F R P F
baseline 57.4 58.1 57.7 56.6 55.4 56.0 59.3 60.4 59.9 56.2 58.6 57.3 37.7
Ng 2007a 59.5 70.6 64.6 - - - - - - - - - 51.5
SC STRICT 59.6 73.3 65.7 61.6 72.8 66.7 60.3 74.9 66.8 56.8 72.1 63.5 46.3
+ SC COARSE 59.2 76.7 66.8 61.0 76.7 67.9 59.8 77.2 67.4 56.6 76.2 64.9 45.9
+ SC RELAX1 59.8 79.0 68.1 61.3 79.8 69.3 60.9 80.3 69.3 57.2 76.7 65.5 47.2
+ SC RELAX2 60.2 77.7 67.8 61.5 78.2 68.9 61.4 78.9 69.1 57.5 75.7 65.4 47.4
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In HLT-NAACL.
C. Fellbaum. 1998. An electronic lexical database.
The MIT press.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
ACL, pages 363-370.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni.
2006. Factorizing complex models: a case study in
mention detection. In Proc. of COLING/ACL, pages
473-480.
S. M. Harabagiu, R. C. Bunescu, and S. J. Maiorano.
2001. Text and knowledge mining for coreference
resolution. In Proc. of NAACL, pages 55-62.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proc. of HLT/NAACL,
pages 289-296.
D. Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proc. of COLING/ACL, pages
768-774.
D. Lin. 1998b. Using collocation statistics in informa-
tion extraction. In Proc. of MUC-7.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S.
Roukos. 2004. A mention synchronous coreference
resolution algorithm based on the Bell tree. In Proc.
of the ACL.
C. Manning and D. Klein. 2003. Optimization,
Maxent Models, and Conditional Estimation with-
out Magic. Tutorial at HLT-NAACL 2003 and ACL
2003.
K. Markert and M. Nissim. 2005. Comparing knowl-
edge sources for nominal anaphora resolution. Com-
putational Linguistics, 31(3):367-401.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proc. of the NIPS.
V. Ng. 2007a. Semantic Class Induction and Corefer-
ence Resolution. In Proc. of the ACL.
V. Ng. 2007b. Shallow Semantics for Coreference
Resolution. In Proc. of the IJCAI.
C. Nicolae and G. Nicolae 2006. BESTCUT: A Graph
Algorithm for Coreference Resolution. In Proc. of
the EMNLP.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004. Learning to resolve bridging references. In
Proc. of the ACL.
S. P. Ponzetto and M. Strube. 2006. Exploiting se-
mantic role labeling, WordNet and Wikipedia for
coreference resolution. In Proc. of the HLT/NAACL,
pages 192-199.
N. Seco, T. Veale, and J. Hayes. 2004. An Intrinsic
Information Content Metric for Semantic Similarity
in WordNet. Proc. of the European Conference of
Artificial Intelligence.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. A machine
learning approach to coreference resolution of noun
phrases. Computation Linguistics, 27(4):521-544.
Y. Versley, S. P. Ponzetto, M. Poesio, V. Eidelman, A.
Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: a modular toolkit for coreference resolution.
ACL 2008 System demo.
R. Vieira and M. Poesio. 2000. An empirically-based
system for processing definite descriptions. Compu-
tational Linguistics, 26(4):539-593.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A model-theoretic coreference
scoreing scheme. In Proc. of MUC-6, pages 45-52.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Linguistic Data
Consortium.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automat-
ically discovered pattens. In Proc. of the ACL.
1240
Bridging the Gap
Between Dialogue Management and Dialogue Models
Weiqun Xu and Bo Xu and Taiyi Huang and Hairong Xia
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
Beijing, 100080, P. R. China
 wqxu, xubo, huang, hrxia@nlpr.ia.ac.cn
Abstract
Why do few working spoken dialogue sys-
tems make use of dialogue models in their
dialogue management? We find out the
causes and propose a generic dialogue
model. It promises to bridge the gap be-
tween practical dialogue management and
(pattern-based) dialogue model through
integrating interaction patterns with the
underling tasks and modeling interaction
patterns via utterance groups using a high
level construct different from dialogue act.
1 Introduction
Due to the rapid progress of speech and language
processing technologies (Cole et al, 1998; Juang
and Furui, 2000), ever-increasing computing power,
and vast quantity of social requirements, spoken di-
alogue systems (SDSs), which promise to provide
natural and ubiquitous access to online information
and service, have become the focus of many research
groups (both academic and industrial) with many
projects sponsored by EU, US (D)ARPA and others
in the past few years (Zue and Glass, 2000; McTear,
2002; Xu, 2001). The last decade saw the emergence
of a great deal of SDSs.
Despite so much progress, some problems still
remain, prominent among which are usability and
reusability (or portability across domains and lan-
guages). Through a survey of typical working spo-
ken (or natural language) dialogue systems in the
nineties (Xu, 2001), we find their central control-
ling component ? dialogue management ? is rela-
tively less well-established than other components.
In most working SDSs, the design of dialogue
management is usually guided by some principles
(den Os et al, 1999), strategies (Souvignier et al,
2000), or objectives (Lamel et al, 2000). In some
even these guidelines are implicit. The problem is
more outstanding in those SDSs developed by the
speech recognition community, in which most work-
ing SDSs come into being. Among many causes,
we think, the most important is that dialogue man-
agement is short of solid theoretical support from
dialogue models (the distinction between dialogue
management and dialogue model will be explicated
in section 2), in addition to the design of SDSs being
a real world problem.
The approach we adopt in building dialogue man-
agement model for SDSs is to study human-human
dialogues solving the same or similar problem.
Though human-computer dialogues may be differ-
ent in some aspects from human-human dialogues,
the design of human-computer dialogue will bene-
fit a lot from the study of human-human dialogues.
It will not be clear whether those that characterize
human-human dialogues are applicable to human-
computer dialogues until they are well studied. Ap-
plicable or not, they are sure to contribute some in-
sights to the design of dialogue management.
In what follows we first inspect main approaches
to dialogue modeling and dialogue management and
find two deep causes behind the gap between them
(section 2). Against the causes we propose a generic
dialogue model which distinguishes five ranks of
discourse units and three levels of dialogue dynam-
     Philadelphia, July 2002, pp. 201-210.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
ics (section 3). Then we apply it to information-
seeking (one of the most common tasks adopted in
the study of SDSs) dialogues and elaborate interac-
tion patterns as utterance groups, which are classi-
fied along two dimensions (initiative and direction
of information flow) into four basic types with some
variations (section 4). We also experiment on seg-
menting utterance groups in our corpus with a sub-
ject and three algorithms.
2 The Gap
Why do most working SDSs make little use of di-
alogue models in their dialogue management? Or,
why is there a gap between dialogue management
and dialogue models?
To make it clear, we first distinguish between di-
alogue models and dialogue management models1,
or equivalently, between dialogue modeling and di-
alogue management modeling.The goal of dialogue
modeling is to develop general theories of (coopera-
tive task-oriented) dialogues and to uncover the uni-
versals in dialogues and, if appropriate, to provide
dialogue management with theoretical support. It
takes an analyzer?s point of view. While the goal
of dialogue management modeling is to integrate di-
alogue model with task model in some specific do-
main to ?develop algorithms and procedures to sup-
port a computer?s participation in a cooperative dia-
logue? (Cohen, 1998, p.204). It takes the viewpoint
of a dialogue system designer.
Next, we briefly overview main approaches to di-
alogue modeling and dialogue management, then
point out the causes behind the gap.
2.1 Dialogue Models
There are mainly two approaches to dialogue mod-
eling: pattern-based and plan-based.2
1The distinction between dialogue models and dialogue
management models is close to what Cohen (1998) makes in
dialogue modeling. He distinguishes ?two related, but at times
conflicting, research goals ... often adopted by researchers of
dialogue?. Roughly speaking, one is theoretical and the other is
practical.
2Cohen (1998) gives a more detailed discussion on dialogue
modeling. Below we draw a lot from there. He mentions ?three
approaches to modeling dialogue ? dialogue grammars, plan-
based models of dialogue, and joint action theories of dialogue?.
We treat joint action theories as further development of original
plan-based approach. So his latter two correspond to our plan-
based approach in general.
Patten-based approach models recurrent interac-
tion patterns or regularities in dialogues at the illo-
cutionary force level of speech acts (Austin, 1962;
Searle, 1969) in terms of dialogue grammar (Sin-
clair and Coulthard, 1975), dialogue/conversational
game (Carlson, 1983; Kowtko et al, 1992; Mann,
2001), or adjacency pairs (Sacks et al, 1974). It
benefits a lot from the insights of discourse analy-
sis (Sinclair and Coulthard, 1975; Coulthard, 1992;
Brown and Yule, 1983) and conversation analysis
(Levinson, 1983).
Plan-based approach relates speech acts per-
formed in utterances to plans and complex mental
states (Cohen and Perrault, 1979; Allen and Perrault,
1980; Lochbaum et al, 2000) and uses AI plan-
ning techniques (Fikes and Nilsson, 1971). Later de-
velopments of plan-based dialogue models include
multilevel plan extension (Litman and Allen, 1987;
Litman and Allen, 1990; Carberry, 1990; Lambert
and Carberry, 1991), theories of joint action (Co-
hen and Levesque, 1991) and SharedPlan (Grosz and
Sidner, 1990; Grosz and Kraus, 1996).
Pattern-based dialogue model describes what hap-
pens in dialogues at the speech act level and cares
little about why. Plan-based dialogue model ex-
plains why agents act in dialogues, but at the ex-
pense of complex representation and reasoning. In
other words, the former is shallow and descrip-
tive and the latter is deep and explanatory. Hul-
stijn (2000) argues for the complementary aspects of
the two approaches and claims that ?dialogue games
are recipes for joint action?.
Since, on the one hand, our target tasks belong to
the class of simple service, like information-seeking
and simple transactions, which are relatively well-
structured and well-defined and not too complex for
pattern-based dialogue models, on the other hand,
there are some significant problems in using plan-
based models in practical SDSs ? those of ?knowl-
edge representation, knowledge engineering, com-
putational complexity, and noisy input? (Allen et
al., 2000), we will choose pattern-based instead of
plan-based dialogue model as our theoretical basis
for practical dialogue management at present.
2.2 Dialogue Management Models
We view dialogue management as an organic com-
bination of dialogue model with task model in some
specific domain. Its basic functionalities include in-
terpretation in context, generation in context, task
management, interaction management, choice of di-
alogue strategies, and context management. All
of them require contextual (linguistic and/or world)
knowledge.
According to how task model and dialogue model
are used, approaches to dialogue management can
be classified into four categories3 in Table 1.
Table 1: Classifying dialogue management models
Task Model
implicit explicit
Dialogue implicit DITI DITE
Model explicit DETI DETE
DITI or graph-based, both dialogue model and task
model are implicit. Dialogue is controlled via
finite state transitions (McTear, 1998). Topic
flow is predetermined. It is neither flexible nor
natural, but simple and efficient. It?s suitable
for simple and well-structured tasks similar to
automated services over ATMs or telephones
with DTMF input.
DITE or frame-based, with no explicit dialogue
model, but task is explicitly represented as a
frame or a form (Goddeau et al, 1996), a task
description table (Lin et al, 1998), a topic for-
est (Wu et al, 2000), or an agenda (Xu and
Rudnicky, 2000), etc. Both system and user
may take the initiative. Topic flow is not prede-
termined. It?s more flexible than that of DITI,
but still far from naturalness and friendliness,
since it makes no explicit use of dialogue mod-
els. Most working SDSs adopt this way of dia-
logue management.
DETI there is no practical dialogue management
3For a more comprehensive discussion on dialogue man-
agement (and SDSs), see (McTear, 2002). He identifies two
aspects of dialogue control (i.e., dialogue management) ? ini-
tiative and flow of dialogue, and three strategies for dialogue
control ? finite-state-based, frame-based, and agent-based. The
first two are similar to DITI and DITE respectively and the third
is a collection of some other approaches which are now hardly
applicable for practical dialogue management, among which is
plan-based. Our classification below is more clear.
using such a combination of task model and di-
alogue model.
DETE both dialogue model and task model are
explicit. This type of dialogue management
shares the advantages of frame-based one. At
the same time it is potential to allow of more
natural interactions according to the dialogue
model used. This is what we are after here.
2.3 The Causes behind the Gap
From the analysis above we can see the surface
gap between (DITE) dialogue management in most
working SDSs and (pattern-based) dialogue models
is mainly due to a deep one, i.e., the one between
dialogue models and the underlying tasks.
There is another important cause ? the interaction
patterns are described at the level of speech act or
dialogue act.4 To link dialogue acts to utterances,
three problems5 must be addressed at the same time:
  Dialogue act classification scheme and its reli-
ability in coding corpus, (Carletta et al, 1997;
Allen and Core, 1997; Traum, 1999);
  Choice of features/cues that can support auto-
matic dialogue act identification, including lex-
ical, syntactic, prosodic, collocational, and dis-
course cues;
  A model that correlates dialogue acts with
those features.
Some of the problems are discussed in (Jurafsky et
al., 1998; Stolcke et al, 2000; Jurafsky and Martin,
2000; Jurafsky, 2002). The empirical work on dia-
logue act classification and recognition did not begin
until some dialogue corpora (like Map Task, Verb-
mobil, TRAINS, and our NLPR-TI) were available.
But how could dialogue act recognition be suc-
cessfully applied to practical dialogue management
remains to be seen. So we choose a higher level
4Following Jurafsky (2002), we will adopt the term dia-
logue act, which captures the illocutionary force or commuca-
tive function of speech act. Though there are some arguments
in (Levinson, 1983) and others against using dialogue act to
model dialogues, and there are indeed some unresolved prob-
lems in linking dialogue acts to utterances, it will be our choice
for the time being.
5We extend Webber?s (2001) idea by splitting feature choice
out.
construct (UT-3, see section 3.1.3) to describe inter-
action patterns instead. We are by no means deny-
ing the important role dialogue act plays in dialogue
modeling, but try to incorporate high level knowl-
edge into dialogue modeling.
3 The Bridge ? GDM
Against the above gap and its causes we propose a
generic dialogue model (GDM) for task-oriented di-
alogues, which consists of five ranks of discourse
units and three levels of dialogue dynamics. It cap-
tures two important aspects of task-oriented dia-
logue ? interaction patterns at the low level and un-
derlying task at the high level.
3.1 Discourse Units
We distinguish five ranks of discourse units in de-
scribing task-oriented dialogues: dialogue, phase,
transaction, utterance group, and utterance.
3.1.1 Dialogue, Phase, and Transaction
The overall organization of a typical task-oriented
dialogue can be divided into three phases, namely,
an opening phase, a closing phase, and between
them a problem-solving phase, which can be subdi-
vided into transactions depending on how the under-
lying task is divided into subtasks. Each subtask cor-
responds to a transaction. If a task is atomic, there
will be only one transaction in the problem-solving
phase, just like the task of tourism information-
seeking.
3.1.2 Utterance Group
In performing a subtask (or task, if atomic), some
interaction patterns will recur. We name the interac-
tion patterns utterance groups (or groups, for short).
It?s also called exchanges or conversational games
(see section 2.1). The unit at this level involves com-
plex grounding process towards common ground or
mutual knowledge (Clark and Schaefer, 1989; Clark,
1996; Traum, 1994).
3.1.3 Utterance
The elementary unit in our model is utterance.
Every utterance either initiates a new group, contin-
ues, or ends an old one. Usually it is what a speaker
utters in his/her turn (for simplification, overlaps
will not be considered here). But there are some
turns with two or more utterances. These multi-
utterance turns usually end an old group with their
first utterance and begin a new one with their last ut-
terance. Similar observations are found in Verbmo-
bil corpus (Alexandersson and Reithinger, 1997).
Each utterance can be analyzed at three levels
and assigned a type correspondingly (utterance type,
UT):
UT-1 sentence type or mood, i.e., declarative, im-
perative, and interrogative (including yes-no
question (ynq), wh-question (whq), alterna-
tive question (atq), disjunctive question (djq),
which can be identified using surface lexical
and prosodic features).
UT-2 dialogue act, see section 2.3.
UT-3 a more general communicative function, rel-
ative to a group, of a small number, including
initiative (I), response/reply (R), feedback (F),
acknowledgement (A) (typical in information-
seeking dialogues), and others. It can be iden-
tified using UT-1 and semantic content (or ut-
terance topic) and preceding UT-3s, It is at this
level that interaction patterns are more obvi-
ous. What?s more, it can be recognized without
UT-2 (dialogue act) but contribute to dialogue
act recognition.
3.2 Dialogue Dynamics
By dialogue dynamics, we mean the dynamic pro-
cess within dialogues, i.e., how dialogues flow from
one partner?s utterance to another?s all the way till
the closing. The dynamic process includes that of
intra-utterance (micro-dynamics) and that of inter-
utterance. Inter-utterance dynamics is further di-
vided into intra-group dynamics (meso-dynamics)
and inter-group dynamics (macro-dynamics).
3.2.1 Micro-dynamics
Micro-dynamics deals with how discourse phe-
nomena (like anaphora, ellipsis, etc.) within one
utterance are decoded (interpretation) or encoded
(generation) in discourse context and how utterance
level intention (dialogue act) is recognized using
lexical, prosodic, and other cues and discourse struc-
ture (see section 2.3). Discourse phenomena contain
much discourse-level context information. It is those
that contribute partly to the naturalness and coher-
ence in human-human dialogues. But it?s very dif-
ficult for computers to make full use of them, either
in interpretation or in generation. They are imple-
mented in few of present SDSs, though much effort
has been put on the study of computational models
of discourse phenomena (see (Webber, 2001) for an
overview and references therein for further details).
3.2.2 Meso-dynamics
Meso-dynamics explains utterance-to-utterance
moves within one group which present recurrent
interaction patterns. Our corpus study shows that
those patterns in information-seeking dialogues are
closely related to two factors ? initiative and direc-
tion of information flow between user and server
(see section 4.1).
3.2.3 Macro-dynamics
Macro-dynamics describes inter-group moves,
which may take place intra-transactionally within
one subtask or inter-transactionally between sub-
tasks. Inter-group moves are subject to the under-
lying task. It?s difficult to give an account like intra-
group moves, because they reflect the process how
a problem is solved.The account depends on how
tasks are represented and reasoned. We may gain
some hints from the study of general problem solv-
ing in AI (Bonet and Geffner, 2001).
3.3 Discussion
GDM as we propose above is a DETE dialogue man-
agement framework with fine-grained patterns. We
discuss related work and its implication for dialogue
management below.
3.3.1 Discourse Unit
Different discourse units are used by different re-
searchers in studying discourse. In (Sinclair and
Coulthard, 1975), five ranks of units are used to ana-
lyze classroom interactions: lesson, transaction, ex-
change, move, and act. The first four roughly cor-
respond to our dialogue, transaction, group, utter-
ance. We add the unit phase and omit act, which
is a sub-utterance unit. In (Alexandersson and Re-
ithinger, 1997), four ranks of units are used to ana-
lyze meeting scheduling dialogues: dialogue, phase,
turn, and dialogue act. Turn is a natural unit that
appears in dialogues, but is it an basic unit? Four
units with conversation acts (Traum and Hinkelman,
1992; Traum, 1994), are used to analyze TRAINS
(freight scheduling) dialogues: multiple discourse
unit (argumentation act), discourse unit (core speech
act), utterance unit (grounding act), sub-utterance
unit (turn-taking act). Theirs differ a lot from ours
partly because they pay more attention to grounding.
3.3.2 Discourse Structure
In GDM the structure of discourse6 is accounted
for from two aspects: local structure is reflected
in utterance groups and shaped by meso-dynamics;
global structure is determined by the underlying task
and shaped by macro-dynamics. This is obvious to
task-oriented dialogues in view of GDM.
3.3.3 Dialogue Strategies
In most working SDSs dialogue strategies are
handcrafted by system developers. Recently there
are some efforts in applying machine learning ap-
proaches to the acquisition of dialogue strategies
(Walker, 2000; Levin et al, 2000). We hope to
find out what strategies are used in human-human
dialogue and how they could be applied to human-
computer dialogue. We first refine the concept of
dialogue strategies. From the view of GDM, the
strategies a dialogue agent may choose can also be
classified into three levels, i.e.,
Micro-level strategies how to realize information
structure, anaphora, ellipsis, and others, in ut-
terances,
Meso-level strategies what to say regarding current
group status, so as to complete ongoing group
more friendly,
Macro-level strategies how to choose discourse
topic regarding current task status, so as to
complete the underlying task more efficiently.
6Grosz and Sidner (1986) proposed a tripartite discourse
model consisting of attentional state, intentional structure, and
linguistic structure. It is influential and covers both dialogue
and text. But their intentional structure fails to capture the dis-
tinction between global level and local level structure. Their
discourse unit ? discourse segment ? is used without noticing
that there are different ranks of discourse unit in dialogues. This
is partly due to that they looked more at the similarities between
dialogue and text and less at the differences between them. Di-
alogue and text, as two types of discourse, share something in
common, but there is also something that makes them different.
3.3.4 The Complexity of Dialogue Management
Since dialogue management is closely related to
dialogue model and underlying task and domain,
the complexity of dialogue management can be de-
composed into three parts, i.e., the complexity of
dialogue model, the complexity of task, and the
complexity of domain. The complexity of dialogue
model is affected by what kind of initiative and dia-
logue phenomena are allowed. The task complexity
is affected by the number of its possible actions and
by whether it is well-structured and well-defined.
The domain complexity is affected by domain en-
tities and their relations and by the volume of in-
formation. The three are not independent but inter-
twined.
4 Utterance Groups in GDM-IS
We now apply GDM to information-seeking dia-
logues (GDM-IS) and search for interaction patterns
in the NLPR-TI corpus. We first try to classify and
segment utterance groups. This is a preliminary step
toward group pattern recognition. Details of the
recognition process and results will be given in (Xu,
2002).
4.1 Group Classification
Group patterns are recurrent, but how many? Or,
is there a limited number? In our NLPR-TI corpus
information-seeking dialogues (see section 4.2.1),
we find four basic groups with some variations.
4.1.1 Basic Groups
The recurrent patterns, according to our observa-
tion, can be classified into one of the four types in
Table 2 along two dimensions ? initiative and the di-
rection of information flow (determined using world
knowledge in the domain).
Table 2: Basic utterance groups
Information Flow
S  U U  S
Group User UISU UIUS
Initiative Server SISU SIUS
Direction of information flow In the dialogues
of information-seeking, there are two directions of
information flow: one from user to server (U  S)
and the other from server to user (S  U). In the
tourism domain, the former includes intended route
(or sight-spot, or a rough area, obligatory), intended
start time, number of tourists (optional); the latter in-
cludes start time, duration, vehicle, price, accommo-
dation, meal, schedule, and more. Server must know
the information about user?s intended route before
providing user with other information.
Initiative7 In GDM, initiative always starts a new
utterance group. It is one of utterance?s general com-
municative functions relative to a group, together
with reply, feedback, acknowledgement, as we men-
tion in section 3.1.3. Regarding one group topic
there are user initiatives (UI) and there are server ini-
tiatives (SI). Group patterns depend heavily on who
initiates the group regarding some specific topic.
This is due to the role asymmetry of the dialogue
partners.
4.1.2 Complex Groups
Though most groups can be covered by the above
basic patterns, there are some exceptions which are
more complex. They are usually embedded ones.
When one partner signals non-understanding or non-
hearing, or a normal group is suspended, one or two
more utterances will be inserted, either to repeat pre-
vious utterance or resume suspended group. The
embedded groups may also be precondition groups.
Precondition groups occur when some obligatory in-
formation is missing before the salient issue could
be addressed. Once the missing is provided, the
outer group will continue. Complex groups can also
occur when one partner lists more than one items or
does some repairing.
4.2 Group Segmentation
Given the above group classification, how to rec-
ognize them? We have to segment and classify
groups, and determine UT-3 of every utterance
within groups. This is a big problem. Only the ex-
periment on group segmentation is reported in this
paper.
7We note that there are task initiative and dialogue initia-
tive (Chu-Carroll and Brown, 1998) and there are local initiative
and global initiative (Rich and Sidner, 1998). Our initiative-in-
group is more task-related and global. For a comprehensive dis-
cussion on mixed initiative interaction, see (Haller and McRoy,
1998, 1999).
To segment a dialogue into groups is first to deter-
mine the beginning of a group, i.e., to determine if
an utterance is an initiative or not. (Multi-utterance
turns are manually segmented beforehand for sim-
plification.)
4.2.1 NLPR-TI Corpus
We use NLPR-TI corpus (Xu et al, 1999) in the
experiment. It consists of 60 spontaneous human-
human dialogues (about 5.5 hours) over telephones
on tourism information-seeking. There are total
2716 turns (1346 by the user and 1370 by the
server). The average length of user?s turns is about
7 Chinese characters and server?s about 9. The first
20 dialogues (transcript) are used for current group
segmentation.
4.2.2 Manual Segmentation
A subject was given the basic ideas about GDM
and utterance groups in GDM-IS and segmented two
dialogues with an expert?s guide before starting the
work.
To test the reliability of group segmentation
within GDM-IS, we calculate the kappa coefficient
()8 (Carletta, 1996; Carletta et al, 1997; Flam-
mia, 1998) to measure pairwise agreement between
the subject and the expert. Two coders segmented
the first 20 dialogues (totally 845 utterances). They
reached       , which shows a high reliabil-
ity. Using the expert?s segmentation as reference, we
also measure the subject?s segmentation using infor-
mation retrieval metrics ? precision (P), recall (R),
and F-measure9 (see Table 3 for the result).
4.2.3 Automated Segmentation
Three simple algorithms in Figure 1 are used to
perform the same task on the 20 dialogues. The in-
put is a semantic tag sequence produced by a statis-
tical parser (Deng et al, 2000)10.
8
             , where   is the
proportion of times that the coders agree and   is the pro-
portion of times that one would expect them to agree by chance.
? From (Carletta, 1996)
9Combined metric          , from
(Jurafsky and Martin, 2000, p.578),    .
10That we adopt such deep features in discourse segmenta-
tion is mainly due to our target application ? dialogue manage-
ment. This makes it different from others using surface features
like (Passonneau and Litman, 1997).
I. Using topic only for segmentation
if topic is new
then UT-3 = initiative
else UT-3 = non-initiative
II. Using UT-1 only for segmentation
if UT-1  interrogatives
then UT-3 = initiative
else UT-3 = non-initiative
III. Using both for segmentation
if topic is new  UT-1  interrogatives
then UT-3 = initiative
else UT-3 = non-initiative
Figure 1: Group segmentation algorithms
Given the semantic tag sequence of an utterance,
we determine its topic11 and UT-1 (what we are most
interested in is interrogatives (ynq, whq, atq, and
djq)). Since the parser performs with an error rate
of , there will be some wrong semantic tags,
which lead to errors in assigning UT-1 and topic.
Then we use the three simple algorithms to seg-
ment groups in the 20 dialogues. Their performance
(also using the expert?s segmentation as reference)
is given in Table 3.
Table 3: Group segmentation results
subject I II III
Precision .88 .59 .67 .83
Recall .92 .82 .62 .56
F-measure .90 .69 .64 .67
4.3 Discussion
Table 3 shows the results of group segmentation,
both manual and automated. Though none of the
three algorithms outperforms the subject, they all
show that topic change and UT-1 as interrogative
are acceptable and also good indicators of utterance
group beginning, esp. when topic and UT-1 are the
11We presume that the topic of an utterance is the last one in
the candidate tags. This seems problematic but is true to most of
the utterances according to our observation. How to determine
the topic of an utterance needs further study.
only information sources and when discourse mark-
ers (Schiffrin, 1987) in spontaneous speech are un-
available in current deep analysis.
There is no obvious performance difference in
segmenting dialogue into groups with the three al-
gorithms. The performance of algorithm I may be
improved if the noises brought by the parser and
our simple topic identification algorithm are cleared.
This implies that topic change is a potentially bet-
ter indicator of the beginning of new groups. The
result using UT-1 only is the worst. This is partly
because not all groups begin with interrogatives and
that interrogatives do not always occur at the begin-
ning of a group. When using both topic and UT-1,
the performance changes little, though seemly more
constraints are used. This possibly is because topic
change and UT-1 as interrogative overlap a lot.
5 Conclusions
After a survey of the main approaches to dialogue
modeling and dialogue management in working
SDSs, we find the causes behind the gap between
practical dialogue management and dialogue models
and propose GDM, which consists of five ranks of
discourse units and three levels of dialogue dynam-
ics. It promises to bridge the gap through integrat-
ing meso-dynamics at the group level with macro-
dynamics at the task level, and modeling interaction
patterns via utterance groups using UT-3.
Then we apply it to information-seeking dia-
logues and elaborate utterance groups (or interaction
patterns) in the model. We also classify and seg-
ment utterance groups in our information-seeking
corpus, which takes a preliminary step toward bet-
ter dialogue modeling for practical dialogue man-
agement with empirical justification. A more chal-
lenging task ? group pattern recognition ? is under
way (Xu, 2002). After that we will investigate how
local discourse structure in terms of utterance group
structure could contribute to the recognition of dia-
logue act (UT-2).
GDM takes a step further toward better dialogue
modeling for practical dialogue management with
empirical justification. It is expected to be used in
practical dialogue management in the near future for
better usability and portability.
Acknowledgments
The work described in this paper was partly sup-
ported by the National Key Fundamental Research
Program (the 973 Program) of China under the grant
G19980300504 and the National Natural Science
Foundation of China under the grant 69835003.
References
Jan Alexandersson and Norbert Reithinger. 1997. Learn-
ing dialogue structures from a corpus. In Proceedings
of the 5th European Conference on Speech Communi-
cation and Technology, volume 4, pages 2231?2234.
James Allen and Mark Core. 1997. Draft of damsl:
Dialog act markup in several layers. Available from
http://www.cs.rochester.edu/research/
cisd/resources/damsl/.
James F. Allen and C. Raymond Perrault. 1980. Ana-
lyzing intention in utterances. Artificial Intelligence,
15(3):143?178.
James Allen, George Ferguson, Bradford W. Miller,
Eric K. Ringger, and Teresa Sikorski Zollo, 2000. Di-
alogue Systems: From Theory to Practice in TRAINS-
96, chapter 14, pages 347?376. In Dale et al (Dale et
al., 2000).
J. L. Austin. 1962. How to do Things with Words.
Clarendon Press, Oxford.
Blai Bonet and He?ctor Geffner. 2001. Planning and Con-
trol in Artificial Intelligence: A Unifying Perspective.
Applied Intelligence, 14(3):237?252.
Gillian Brown and George Yule. 1983. Discourse Anal-
ysis. Cambridge University Press.
Sandra Carberry. 1990. Plan Recognition in Natural
Language Dialogue. ACL-MIT Press Series in Nat-
ural Language Processing. A Bradford book, MIT
Press, Cambridge, Massachusetts.
J. Carletta, A. Isard, S. Isard, J. C. Kowtko, G. Doherty-
Sneddon, and A. H. Anderson. 1997. The reliability
of a dialogue structure coding scheme. Computational
Linguistics, 23(1):13?31.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: The Kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Lari Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. D. Reidel, Dordrecht, Holland.
Jennifer Chu-Carroll and Michael K. Brown. 1998. An
evidential model for tracking initiative in collabora-
tive dialogue interactions. User Modeling and User-
Adapted Interaction, 8(3-4):215?253.
Herbert H. Clark and Edward F. Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13:259?294.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Philip Cohen, Jerry Morgan, and Martha Pollack , editors.
1990. Intentions in Communication. MIT Press.
P. R. Cohen and H. J. Levesque. 1991. Teamwork. Nou?s,
25(4):487?512.
P R. Cohen and C. R. Perrault 1979. Elements of a
plan-based theory of speech acts. Cognitive Science,
3(3):177?212.
Phil Cohen, 1998. Dialogue Modeling, chapter 6.3. In
Cole et al (Cole et al, 1998).
Ronald Cole, Joseph Mariani, Hans Uszkoreit, Giovanni
Varile, Annie Zaenen, Antonio Zampolli, and Victor
Zue, editors. 1998. Survey of the State of the Art in
Human Language Technology. Cambridge University
Press, Cambridge.
Malcolm Coulthard, editor. 1992. Advances in Spoken
Discourse Analysis. Routledge. London.
Robert Dale, Hermann Moisl, and Harold Somers, edi-
tors. 2000. Handbook of Natural Language Process-
ing. Marcel Dekker. New York.
Yunbin Deng, Bo Xu, and Taiyi Huang. 2000. Chi-
nese spoken language understanding across domain.
In Proceedings of the 6th International Conference on
Spoken Language Processing, volume 1, pages 230?
233.
Richard Fikes and Nils J. Nilsson. 1971. STRIPS: A
new approach to the application of theorem proving to
problem solving. Artificial Intelligence, 2(3-4):189?
208.
Giovanni Flammia. 1998. Discourse segmentation of
spoken dialogue: an empirical approach. Ph.D. the-
sis, MIT.
D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and
S. Busayapongchai. 1996. A form-based dialogue
manager for spoken language applications. In Pro-
ceedings of the 4th International Conference on Spo-
ken Language Processing, volume 2, pages 701?704.
Barbara J. Grosz and Sarit Kraus. 1996. Collaborative
plans for complex group action. Artificial Intelligence,
86(2):269?357.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention, and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Barbara J. Grosz and Candace L. Sidner. 1990. Plans for
Discourse. In Cohen et al (Cohen et al, 1990).
Susan Haller and Susan McRoy, editors. 1998, 1999.
User Modeling and User-Adapted Interaction, Special
Issue on Computational Models for Mixed Initiative
Interaction, 8(3-4),9(1-2).
Joris Hulstijn. 2000. Dialogue games are recipes for joint
action. In Proceedings of the Forth Workshop on the
Semantics and Pragmatics of Dialogue (Gotalog?00).
Biing-Hwang Juang and Sadaoki Furui, editors. 2000.
Proceedings of the IEEE , Special Issue on Spoken
Language Processing, 88(8).
Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel
Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,
Andreas Stolcke, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Switchboard discourse language
modeling project report. Technical Report Research
Note No. 30, Center for Speech and Language Pro-
cessing, Johns Hopkins University, Baltimore, MD.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Com-
putational Linguistics. Prentice-Hall.
Daniel Jurafsky, 2002. Pragmatics and Computational
Linguistics. To appear in Laurence R. Horn and Gre-
gory Ward, editors. Handbook of Pragmatics. Black-
well, Oxford.
J. Kowtko, S. Isard, and G. M. Doherty. 1992. Conver-
sational games within dialogue. Research Paper 31,
Human Communication Research Centre, Edinburgh
University, Edinburgh.
Lynn Lambert and Sandra Carberry. 1991. A tripartite
plan-based model of dialogue. In Proceedings of the
29th Annual Meeting of the Association for Computa-
tional Linguistics, pages 47?54, Berkeley, CA.
L. Lamel, S. Rosset, J.L. Gauvain, S. Bennacef,
M. Garnier-Rizet, and B. Prouts. 2000. The LIMSI
ARISE system. Speech Communication, 31(4):339?
354.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine interac-
tion for learning dialog strategies. IEEE Transactions
on Speech and Audio Processing, 8(1):11?24.
Stephen C. Levinson. 1983. Pragmatics. Cambridge
University Press.
Y. Lin, T. Chiang, H. Wang, C. Peng, and C. Chang.
1998. The design of a multi-domain mandarin Chinese
spoken dialogue system. In Proceedings of the 5th In-
ternational Conference on Spoken Language Process-
ing, volume 1, pages 230?233.
Diane J. Litman and James F. Allen. 1987. A plan recog-
nition model for subdialogues in conversation. Cogni-
tive Science, 11(2):163?200.
Diane J. Litman and James F. Allen. 1990. Discourse
Processing and Commonsense Plans. In Cohen et al
(Cohen et al, 1990).
Karen E. Lochbaum, Barbara J. Grosz, and Candace L.
Sidner, 2000. Discourse Structure and Intention
Recognition, chapter 6, pages 123?146. In Dale et al
(Dale et al, 2000).
William C. Mann. 2001. The genre diversity of dia-
logue game theory. Available from http://www-
rcf.usc.edu/ billmann/memos.htm.
Michael F McTear. 1998. Modelling spoken dialogues
with state transition diagrams: experiences with the
CSLU toolkit. In Proceedings of the 5th Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 1223?1226.
Michael F. McTear. 2002. Spoken dialogue technology:
Enabling the conversational user interface. ACM Com-
puting Surveys,34(1):90?169.
Els den Os, Lou Boves, Lori Lamel, and Paolo Baggia.
1999. Overview of the ARISE project. In Proceedings
of the 6th European Conference on Speech Communi-
cation and Technology, volume 4, pages 1527?1530.
Rebecca Passonneau and Diane Litman. 1997. Discourse
segmentation by human and automated means. Com-
putational Linguistics, 23(1):103?140.
Charles Rich and Candace L. Sidner. 1998. Colla-
gen: A collaboration manager for software interface
agents. User Modeling and User-Adapted Interaction,
8(3-4):315?350.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
Deborah Schiffrin. 1987. Discourse Markers. Cam-
bridge University Press.
J. R. Searle. 1969. Speech Acts. Cambridge University
Press.
John M. Sinclair and Malcolm Coulthard. 1975. Towards
an Analysis of Discourse: The English Used by Teach-
ers and Pupils. Oxford University Press.
Bernd Souvignier, Andreas Kellner, Bernhard Rueber,
Hauke Schramm, and Frank Seide. 2000. The
thoughtful elephant: Strategies for spoken dialog sys-
tems. IEEE Transactions on Speech and Audio Pro-
cessing, 8(1):51?62.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?371.
David R. Traum and Elizabeth A. Hinkelman. 1992.
Conversation acts in task-oriented spoken dialogue.
Computational Intelligence, 8(3):575?599.
David R. Traum. 1994. A Computational Theory of
Grounding in Natural Language Conversation. Ph.D.
thesis, University of Rochester.
David R. Traum. 1999. 20 questions for dialogue act tax-
onomies. In Proceedings of the Third Workshop on the
Semantics and Pragmatics of Dialogue (Amstelog?99).
Marilyn A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Artificial
Intelligence Research, 12:387?416.
Bonnie Webber. 2001. Computational Perspectives on
Discourse and Dialogue. In Deborah Schiffrin, Deb-
orah Tannen, and Heidi Hamilton, editors. Handbook
of Discourse Analysis. Blackwell, Oxford.
Xiaojun Wu, Fang Zheng, and Mingxing Xu. 2000.
Topic forest: A plan-based dialog management struc-
ture. In Proceedings of ICASSP, volume 1, pages 617?
620.
Wei Xu and Alexander I. Rudnicky. 2000. Task-based di-
alog management using an agenda. In Proceedings of
ANLP/NAACL 2000 Workshop on Conversational Sys-
tems, pages 42?47.
B. Xu, T.Y. Huang, X. Zhang, and C. Huang. 1999. A
Chinese spoken dialogue database and its application
for travel routine information retrieval. In Proceed-
ings of the Second International Workshop on East-
Asia Language Resources and Evaluation, Taipei.
Weiqun Xu. 2001. Survey of the state of the art in spoken
dialogue systems. Manuscript.
Weiqun Xu. 2002. Grouping utterances in information-
seeking dialogues. In preparation.
Victor Zue and Jim Glass. 2000. Conversational inter-
faces: Advances and challenges. Proceedings of the
IEEE, 88(8):1166?1180.
 Semantic class induction and its application for a Chinese voice 
search system 
Yali Li 
ThinkIT laboratory, 
Institute of 
Acoustics, Chinese 
Academy of Sciences 
liyali@hccl.ioa.ac.cn 
Weiqun Xu 
ThinkIT laboratory, 
Institute of Acoustics, 
Chinese Academy of 
Sciences 
xuweiqun@hccl.ioa.ac.cn
Yonghong Yan 
ThinkIT laboratory, 
Institute of 
Acoustics, Chinese 
Academy of Sciences
yyan@hccl.ioa.ac.cn 
 
Abstract 
In this paper, we propose a novel 
similarity measure based on  
co-occurrence probabilities for inducing 
semantic classes. Clustering with the new 
similarity measure outperformed that 
with the widely used distance measure 
based on Kullback-Leibler divergence in 
precision, recall and F1 evaluation. We 
then use the induced semantic classes and 
structures by the new similarity measure 
to generate in-domain data. At last, we 
use the generated data to do language 
model adaptation and improve the result 
of character recognition from 85.2% to 
91%. 
1 Introduction 
Voice search (e.g. Wang et al, 2008) has 
recently become one of the major foci in spoken 
dialogue system research and development. In 
main stream large vocabulary ASR engines, 
statistical language models (n-grams in 
particular), usually trained with plenty of data, 
are widely used and proved very effective. But 
for a voice search system, we have to deal with 
the case where there is no or very little relevant 
data for language modeling. One of the 
conventional solutions to this problem is to 
collect and use some human-human or 
Wizard-of-Oz (WOZ) dialogue data. Once the 
initial system is up running, the performance can 
be further improved with human-computer data 
in a system-in-the-loop style. Another practical 
approach is to handcraft some grammar rules and 
generate some artificial data. But writing 
grammars manually is tedious and 
time-consuming and requires some linguistic 
expertise. 
In this paper, we introduced a new similarity 
measure to induce semantic classes and 
structures. We then generated a large number of 
data using the induced semantic classes and 
structures to make language model adaptation. 
At the end, we give the conclusion and implied 
the future work. 
2 Semantic Class Induction 
The studies on semantic class induction in spoken 
language (or spoken language acquisition in 
general) have received some attention since the 
middle 90's. One of the earlier works is carried 
out by Gorin (1995), who employed an 
information -theoretic connectionist network 
embedded in a feedback control system to acquire 
spoken language. Later on Arai et al (1999) 
further studied how to acquire grammar 
fragments in fluent speech through clustering 
similar phrases using Kullback-Leibler distance. 
Meng and Siu (2002) proposed to 
semi-automatically induce language structures 
from unannotated corpora for spoken language 
understanding, mainly using Kullback-Liebler 
divergence and mutual information. Pargellis et 
al. (2004) used similar measures (plus three 
others) to induce semantic classes for comparing 
domain concept independence and porting 
concepts across domains. Potamianos (2005, 
2006, 2007) and colleagues conducted a series of 
studies to further improve semantic class 
induction, including combining wide and narrow 
context similarity measures, and adopting a 
soft-clustering algorithm (via a probabilistic 
class-membership function). 
2.1 Clustering 
In general, words and phrases which appear in 
similar context usually share similar semantics. 
E.g., ????(Tsinghua University) and ??
??(Peking University) in the following two 
utterances (literal translations are given in 
brackets) are both names of place or 
organisation. 
 
? ? ???? ?? ? ??? 
Please/look for/Tsinghua University/near//bank 
(Please look for banks near Tsinghua 
University.) 
 
? ? ???? ?? ? ???? 
Please/look for/Peking University/nearby//gym 
(Please look for gyms near Peking University.) 
 
To automatically discover that the above two 
words have similar semantics from unannotated 
corpus, we try unsupervised clustering based on 
some similarity measures to induce semantic 
classes. Further details about similarity measures 
are given in section 2.2.  
Before clustering, the utterances are 
segmented into phrases using a simple maximum 
matching against a lexicon. Clustering are 
conducted on phrases, which may be of a single 
word. 
2.2 Similarity Measures 
For lexical distributional similarity, several 
measures have been proposed and adopted, e.g., 
Meng and Siu (2002), Lin(1998), Dagan et al 
(1999), Weeds et al (2004). 
We use two kinds of similarity measures in 
the experiments. One is similarity measure based 
on distance, and the other is a new similarity 
measure directly using the co-occurrence 
probabilities. 
2.3 Distance based similarity measures 
The relative entropy between two probability 
mass functions )(xp  and )(xq  is defined by 
(Cover and Thomas, 2006) as: 
)(
)(
log
)(
)(
log)()||(
xq
xp
E
xq
xp
xpqpD p
Xx
== ?
?
(1) 
The relative entropy, as an asymmetric 
distance between two distributions, measures the 
inefficiency of assuming that the distribution is 
q  when the true distribution is p . 
It is commonly used as a statistical distance 
and can be symmetry as follows: 
)||()||(),( pqDqpDqpdiv +=      (2) 
For two words in a similar context, e.g., in 
the sequence { ,...,,..., 11 www ? }, 
where w  can be word a  or b , the right 
bigram )||(1
RR baD and )||(1
RR abD  are 
defined as: 
 
??= W b)|p(w
a)|p(w
a)|p(wbaD
w
RR
1 1
1
11 log)||(
(3) 
and  
??= W a)|p(w
b)|p(w
b)|p(wabD
w
RR
1 1
1
11 log)||( (4) 
where W  is the set of words or phrases. 
And the symmetric divergence is 
)||()||(),( 111
RRRRRR abDbaDbadiv += (5) 
The left bigram symmetric divergence can be 
similarly defined. 
Using both left and right symmetric 
divergences, the distance between a  and b  
is 
),(),(),( 111
RRLL badivbadivbad +=     (6) 
So the KL distance becomes: 
?
?
?
?
?
?
?
?
? ?
?
?
? ?
?
?
W a)|p(w
b)|p(w
b)|p(w+
W b)|p(w
a)|p(w
a)|p(w+
W a)|p(w
b)|p(w
b)|p(w+
W b)|p(w
a)|p(w
a)|p(w=
)b,div(a+)b,div(a=b)KL(a,
w
w
w
w
RRLL
1 1
1
1
1 1
1
1
1 1
1
1
1
1
1
1
log
log
log
log
(7) 
This is the widely used distance measure for  
lexical semantic similarity, e.g., Dagan et al 
(1999); Meng and Siu (2002); Pargellis et al
(2004). We can also see the IR distance and L1 
distance below: 
?
?
?
?
? +
? +
? +
? +
? ??
?
?
? ??
?
?
W b)|p(wa)|p(w
b)|p(w
b)|p(w+
W b)|p(wa)|p(w
a)|p(w
a)|p(w+
W b)|p(wa)|p(w
b)|p(w
b)|p(w+
W b)|p(wa)|p(w
a)|p(w
a)|p(w=b)IR(a,
w
w
w
w
1 11
1
1
1 11
1
1
1 11
1
1
1 11
1
1
2
log
2
log
2
log
2
log
(8) 
We can see from the IR metric that it is 
similar to the KL distance. Manhattan-norm (L1) 
distance : 
?
?
?
?
?
?
?
??
W
b)|p(wa)|p(w+
W
b)|p(wa)|p(w=b)(a,L
w
w
1
11
1
11
||
||1
     (9) 
In Pargellis et al (2004), the lexical context 
is further extended from bigrams to trigrams as 
follows. For the sequence: 
,...,,,,..., 2112 wwwww ??  
where w  can be word a  or b , the trigram 
KL between  a  and b is: 
?
?
?
?
?
?
?
?
?? ??
??
??
?? ??
??
??
Ww, a)|wp(w
b)|wp(w
b)|wp(w+
Ww, b)|wp(w
a)|wp(w
a)|wp(w+
Ww, a)|wpw
b)|wp(w
b)|wp(w+
Ww, b)|wp(w
a)|wp(w
a)|wp(w=b)(a,KL
w
w
w 2
w
2
21 21
21
21
21
21
21
21
22 1
12
12
22
12
12
12
log
log
log
log
(10) 
Since more information is taken into account 
in b)(a,KL2 , more constraints are imposed on the 
similarity measure. This is expected to improve 
the precision of clustering but may lead to a lower 
recall. 
2.4 Co-occurrence Probability based 
similarity measures 
After a close investigation of the corpus, we 
came up with an intuitive similarity measure 
directly based on the co-occurrence probability. 
The key idea is that the more common 
neighbouring words or phrases any two words or 
phrases in question share, the more similar they 
are to each other. Therefore, for each left or right 
neighboring word or phrase, we take the lower 
conditional probability into account. 
Thus we have the following similarity 
measures: 
Similarity using the bigram context 
?
?
?
??
??
W
b))|p(wa),|(p(w+
W
b))|p(wa),|(p(w=b)(a,S
w
w
1
11
1
111
min
min
    (11) 
Similarity using the trigram context 
?
?
?
???
????
Ww,
b))|wp(wa),|w(p(w+
Ww
b))|wp(wa),|w(p(w=b)(a,S
w
w
21
2121
12,
12122
min
min
(12) 
Similarity extending b)(a,S1 , taking both left 
and right contexts into account simultaneously  
??? ??Ww, b))|wp(wa),|w(p(w+
S=b)(a,S
w 11
1111
13
min (13) 
After pairs of words or phrases are clustered 
above, those pairs with common members are 
further merged. 
2.5 Comparison of measures 
The KL distances emphasize on the difference of 
two probability but the new measure take the 
probability itself into account. Take the right 
bigram context the similarity measure for 
example: 
)log
log(
1
1
1
1 1
1
1
a)|p(w
b)|p(w
b)|p(w+
W b)|p(w
a)|p(w
a)|p(w=b)(a,KL
w
R ??
 (14) 
seeing  as )|( 1 awP x  and seeing  
as , the equation changed to: 
)|( 1 bwP
y
? )xyy+yx(x=y)(x,KL R loglog  (15) 
and y)(x,SR  becomes to: ?= ),min(),( yxyxS R              (16) 
We can also get the y)(x,IRR  and |1 y)(x,L R  
? ++= )yx yy+yx x(xy)(x,IR R 2log2log   (17) 
and ||1 yxy)(x,L R ?=                   (18) 
We can see the space distribution in Figure.1. 
 
 Figure 1. Space distribution of different metrics 
0=
=
z
yx
                              (19) 
zyx ==                            (20) 
We can see from the four figures (the space 
distribution of four bigram metrics) that four 
curve surface are all symmetric. The curve 
surface of the three distance (KL,IR, L1) all 
contain the curve of (19), and curve surface of 
the minimum similarity contains the curve of 
(20). We say that the KL distances, IR distances 
and L1 distances all emphasize only on the 
distances and don't take the probability itself into 
account.  
We take the right context of two pairs 
),( 11 ba  and ),( 22 ba  for example. If  
9.0)|(,1.0)|( 111 == awpawp  
9.0)|(,1.0)|( 121 == bwpbwp  
0.10.9, 232 =)a|p(w=)a|p(w
'   
1.0)|(,9.0)|( 242
' == bwpbwp  
The calculation is shown as follows: 
0
0.1
0.1
log*0.1
0.1
0.1
log*0.1
log
log
1
1
1
1
1
111
=
+=
)a|p(w
)b|p(w
)b|p(w+
)b|p(w
)a|p(w
)a|p(w=)b,(aKL R
 
0
0.9
0.9
log*0.9
0.9
0.9
log*0.9
log
log
2
2
2
2
2
222
=
+=
)a|p(w
)b|p(w
)b|p(w+
)b|p(w
)a|p(w=),b(aKL
'
'
'
'
'
R
)a|p(w '
 
1.0
)1.0,1.0min(
1111
=
=
R )|()|(min )bwp,aw(p=),b(aS
 
9.0
)9.0,9.0min(
2222
=
=
R )|()|(min )bwp,aw(p=),b(aS
''
 
The KL calculation result of two pairs is the 
same but the new similarity calculated that  
),( 22 ba   is more similar than ),( 11 ba  
because they have more similar context 
probability 0.9. 
3 Experiments and Results 
3.1 Data 
In our experiments, four types of corpora are 
exploited in different stages and different ways.  
z T: A large collection of text corpus is used 
to train a general n-gram language model.  
z H: Some WOZ dialogues were collected 
before the system is built, using a similar 
scenario where users talked in Chinese to a 
service provider (human) via telephone to 
search for local information, or information 
about some local points of interest (POI). 
These dialogues were manually transcribed 
and used for language model training. This 
is the best data we could get before the 
system is built though it is not the real but 
near in-domain data.  
z C: After the initial system was up running, 
some real human-computer dialogues were 
collected and transcribed. These dialogues 
were split into three sets. One (C1) is used 
for semantic class and structure induction. 
One (C2) is used as test data. The other (C3) 
is reserved.  
z A: Domain information (domain entities) is 
used in conjunction with the induced 
semantic classes and structures from C1 to 
generate a large amount of in-domain 
corpus for language model adaptation. In 
Table 1,  we give some statistics in terms 
of the number of utterances(no. u) and 
Chinese characters(no. c) for the above 
corpora. 
corpus no.u no.c 
T  38,636  8,706,340 
H  6,652  151,460 
C1  658  15,434 
C2  1,000  19,284 
C3  411  8,014 
A  14,205  365,576 
Table 1. statistics of different corpus 
3.2 Semantic Clustering 
We conducted clustering with the above 
similarity measures on the data set C1. 
During the clustering, it is required that all the 
probabilities involved in calculating similarity be 
larger than 0. We have no threshold except this 
constraint. 
The outcomes are pairs of phrases. 
It is noticed that most of the clustered words 
and phrases are domain entities. 
In our experiments, we merged the induced 
similar pairs into large clusters. For example, if 
a  is similar to b  and b  is similar to c , then 
( a , b , c ) are merged into one category. In the 
end we use the categories to replace those words 
and phrases in corpus C1 and obtained templates. 
Examples of  the results are given below. 
 
$ask $toponym $near $wh-word $sevice 
[??] $ask $toponym $near ? $sevice ? 
? ? $toponym $ask ??? $poi 
where: 
$ask = ?? | ???| ???? | ... 
$toponym = ???? | ??? | ... 
$sevice = ?? | ??? | ??? |... 
$near = ?? | ?? | ... 
$wh-word = ??? | ??? | ??? | ... 
$poi = ???? | ????? | ... 
 
To evaluate the induction performance, we 
compare the induced word pairs against manual 
annotation. We manually annotated each phrase 
with a tag like $toponym, $poi and so on. If a  
and b  are calculated as a pairs and the 
annotation is the same, we see that they are 
correctly induced which is referred to Pangos 
(2006).  
We compute the metrics of precision P , 
recall R  and f-score 1F  as follows: 
%100?=
M
m
P                        (21) 
where m  is the number of correctly induced 
pairs, and M  is the number of induced pairs. 
%100?=
N
n
R                        (22) 
where n  is the number of correctly induced 
words and phrases, and N  is the number of 
words and phrases in the annotation. 
%100
2
1 ?+
??=
RP
RP
F                 (23) 
which is a harmonic mean of P  and R . 
 
Figure 2. Induction process 
The iterate process we adopted is as in 
Pargellis et al (2004). In the first iteration, we 
calculated the similarity and use the largest 
similarity pairs to generate large classes which 
can be called semantic generalizer. Then we use 
these semantic classes to replace the corpus, and 
obtained new corpus just as the example 
presented above. Then we duplicate this process 
for the second iteration and so on. 
 
 
Figure 3. Precision according to iterations 
induced by KL and S1 similarity measure 
 
Figure 4. Recall according to iterations induced 
by KL and S1 similarity measure 
 
Figure 5. F1 according to iterations induced by 
KL and S1 similarity measure 
 
Figure 6. F1 according to iterations induced by 
all bigram similarity measure 
From figures (Figure 3-6), we can see that 
clustering with our new co-occurrence 
probability based similarity measures 
outperforms that with the widely used relative 
entropy based distance measure consistently for 
both bigram and trigram contexts. This confirms 
the effectiveness of our new and simple measure. 
Regarding the context size, the results from 
using the bigram context outperforms that from 
using the trigram context in precision. But recall 
and 1F  drops a lot.  This is due to that larger 
contexts bring more constraints. The context size 
effect holds for both types of similarity measures. 
And the best performance is achieved with the 
similarity measure 3S . It is based on 1S  and 
takes both left and right contexts into account at 
the same time. 
3.3 Corpus Generation 
Since the number of the domain entities 
(terminals) we can collect from the dialogues is 
very limited, we have to expand those variables 
(non-terminals) in the induced templates with 
domain information from the application 
database and relevant web sites. For example, we 
used all the words and phrases in the toponym  
cluster, e.g., ``????  | ???  | ...'', to 
replace $toponym in the templates above. Then 
we generated a large collection of artificial data 
which has a good coverage in both the utterance 
structures (the way people speak) and the domain 
entities. This resulted in the generated corpus A 
in Table 1. In generation we used the semantic 
classes and structures  induced with 3S  and 
manually corrected some obvious errors. In the 
generated data, there are 14,205 utterances and 
365,576 Chinese characters.: 
3.4 Language Model Adaptation 
There are some language model adaptation 
(LMA) work oriented to the dialogue systems e.g. 
Wang et al2006), Hakkani-T?r et al(2006),  
Bellegarda(2004). So far major effort has been 
spent on adaptation for large vocabulary speech 
recognition or transcription tasks. But recently 
there have been a few studies that are oriented 
toward dialogue systems, e.g. Wang et al2006), 
Hakkani-T?r et al(2006). In our experiments, 
three trigram language models were built, each 
trained separately on the large text collection (T), 
on the WOZ data (H) and on the artificially 
generated data (A). These trigram models were 
then combined through model interpolation as 
follows: We used the linear interpolation to adapt 
language model. The formula is shown as follows. 
T is the out-of-domain data, H is the 
humane-to-humane dialogues, and A is the 
corpus generated by grammars  
)ww|(wP?+
)ww|(wP?+
)ww|(wP?=)ww|P(w
iiiAA
iiiHH
iiiTTiii
21
21
2121
??
??
????
  (24) 
where 1,,0 << AHT ???  and 1=++ AHT ??? . 
The weights were determined empirically on 
the held-out data (C3 in Table 1}). 
All the language models were built with the 
Stolcke(2002)?s {SRILM} toolkit. 
Why we did not use the C corpus directly is that it 
does't have a good covering on the 
domain-entities and other users usually say 
utterances similar to C in structures but different 
domain entities. So we use the good covering 
generated data to make LMA.  
We evaluated the different language models 
with both intrinsic and extrinsic metrics. For 
intrinsic evaluation, we computed the perplexity. 
For extrinsic evaluation, we ran speech 
recognition experiments on the test data C2 and 
calculated the character error rate (CER).  
We can see that corpus A is useful to make 
model adaptation and it is closer to the in-domain 
data than the human-human data for 
human-computer dialogues. By using these 
generated sentences, our domain-specific 
Chinese speech recognition have a growth from 
85.2% to 91.4%. 
 
A
H
T
?
,?
,?
 
1,  
0,  
0  
0.2, 
0.8, 
0  
0.2, 
0, 
0.8  
0.2,
0.4,
0.4 
PP  984  95.4  33.6  23.3 
CER(%)  32.3  14.8  10.7  9.0 
Table 2. perplexity and character error rate 
according to model interpolation 
The optimized weights (0.2,0.4,0.4) is 
obtained from the develop sets C3. From Table 2, 
we can see that language models built using 
additional dialogue related data, either 
human-human/WOZ  dialogues or data 
generated from human-computer dialogues, 
shows significant improvement in both 
perplexity and speech recognition performance 
over the one built with the general text data only. 
For the two dialogue related data, the generated 
data is better than the WOZ data or closer to the 
test data, since perplexity further drops from 
103.5 to 38.1 and CER drops from 14.8 to 10.7. 
This confirms our conjecture that human-human 
WOZ dialogue data is near in-domain and not 
very proper for human-computer dialogues. 
Therefore, to effectively improve language 
modeling for human-computer dialogues, we 
need more in-domain data, even if it is generated 
or artificial. The best language model is obtained 
through interpolation of both language models 
from dialogue related data with the one from 
general text data. This may be because there is 
still some mismatch between data sets C1 (for 
induction and generation) and C2 (for test).  
And some of the missing bits in C1 appeared in 
the WOZ data (corpus A). 
4 Related Works 
The most relevant work to ours is done by Wang 
et al (2006), who generated in-domain data 
through out-of-domain data transformation. First 
some artificial sentences are generated through 
parsing and reconstructing out-of-domain data 
and the illegal ones are filtered out. Then the 
synthetic corpus is sampled to achieve a desired 
probability distribution, based on either 
simulated dialogues or semantic information 
extracted from development data. But we used a 
different approach in producing more in-domain 
data. First semantic classes and structures are 
induced from limited human-computer dialogues. 
Then large amount of artificial in-domain corpus 
is generated with the induced semantic classes 
and patterns augmented with domain entities. 
The main difference between the two works lies 
in how the data is generated and how the 
generated data helped. 
5 Conclusions and Future Work 
In this paper, we described our work on 
generating in-domain corpus using the 
auto-induced semantic classes and structures for 
language model adaptation in a Chinese voice 
search dialogue system. In inducing semantic 
classes we proposed a novel co-occurrence 
probability based similarity measure. Our 
experiments show that the simple co-occurrence 
probability based similarity measure is effective 
for semantic clustering which is used in our 
experiment. For interpolation based language 
model adaptation, the data generated using the 
induced semantic classes and structures 
enhanced with domain entities helped a lot for 
human-computer dialogues. Despite that we 
dealt with the language of Chinese, we believe 
that that approaches we employed are language 
independent and can be applied to other 
languages as well. 
In our experiment we noticed that the 
performance of semantic clustering was affected 
quite a lot by the noises in the data. For future 
work, we would like to investigate how to 
further improve the robustness of semantic 
clustering in noisy spoken language. The 
semantic structures induced above are very 
shallow. We would like to investigate how to 
find deep semantics and relations in the data. 
Acknowledgement 
 
This work is partially supported by The National 
Science & Technology Pillar Program 
(2008BAI50B03), National Natural Science 
Foundation of China (No. 10925419, 90920302, 
10874203, 60875014). 
 
References 
Arai, K. J. H., Wright, G. Riccardi, and Gorin, A. L. 
?Grammar fragment acquisition using syntactic 
and semantic clustering,? Speech Communication, 
vol. 27, iss. 1, pp. 43?62, 1999 
Bellegarda, J. R. Statistical language model 
adaptation: review and perspectives, Speech 
Communication, vol. 42, iss. 1, pp. 93?108, 2004 
Cover, T. M. and Thomas, J. A., Elements of 
Information Theory. Wiley-Interscience, 2006 
Dagan, I., Lee, L. and Pereira, F. C. N. 
?Similarity-Based Models of Word Cooccurrence 
Probabilities,? Machine Learning, 1999 
Gorin, A. L. ?On automated language acquisition,? 
Acoustical Society of America Journal, vol. 97, pp. 
3441?3461, 1995 
Hakkani-T?r, D. Z., Riccardi, G. and Tur, G. An 
active approach to spoken language processing, 
ACM Transactions on Speech and Language 
Processing (TSLP), vol. 3, iss. 3, pp. 1?31, 2006 
Lin, D. ?An information-theoretic definition of 
similarity,? in Proc. ICML ?98: Proceedings of the 
Fifteenth International Conference on Machine 
Learning, 1998 
Meng, H. M. and Siu, K.-C. ?Semiautomatic 
Acquisition of Semantic Structures for 
Understanding Domain-Specific Natural Language 
Queries,? IEEE Trans. Knowl. Data Eng. 2002 
Pargellis, A. N., Fosler-Lussier, E., Fosler-Lussier, 
Lee, C.-H., Potamianos, A. and Tsai, A. 
?Auto-induced semantic classes,? Speech 
Communication, vol. 43, iss. 3, pp. 183?203, 2004 
Pangos, A Combining statistical similarity measures 
for automatic induction of semantic classes, 2005 
Pangos, A., Iosif, E. and Tegos, A. Unsupervised 
combination of metrics for semantic class 
induction, SLT 2006, 2006 
Pangos, A. and Iosif, E., A Soft-Clustering Algorithm 
for Automatic Induction of Semantic Classes, 
interspeech07, 2007 
Stolcke, A. SRILM ? an extensible language 
modeling toolkit, in Proc. ICSLP, 2002 
Wang, C. Chung, G. and Seneff, S. Automatic 
induction of language model data for a spoken 
dialogue system, Language Resources and 
Evaluation, vol. 40, iss. 1, pp. 25?46, 2006 
Wang, Y.-Y. and Dong Yu, E. A., An introduction to 
voice search, Signal Processing Magazine, IEEE, 
vol. 25, iss. 3, pp. 28?38, 2008 
Weeds, J., Weir, D. and McCarthy, D. 
?Characterising measures of lexical distributional 
similarity,? in Proc. in Proc. COLING ?04, 2004, 
 
Proceedings of the SIGDIAL 2013 Conference, pages 457?461,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Dialog State Tracking using Conditional Random Fields
Hang Ren, Weiqun Xu, Yan Zhang,Yonghong Yan
The Key Laboratory of Speech Acoustics and Content Understanding
Institute of Acoustics, Chinese Academy of Sciences
21 North 4th Ring West Road, Beijing, China, 100190
{renhang, xuweiqun, zhangyan, yanyonghong}@hccl.ioa.ac.cn
Abstract
This paper presents our approach to dialog
state tracking for the Dialog State Track-
ing Challenge task. In our approach we
use discriminative general structured con-
ditional random fields, instead of tradi-
tional generative directed graphic models,
to incorporate arbitrary overlapping fea-
tures. Our approach outperforms the sim-
ple 1-best tracking approach.
1 Introduction
Spoken dialog systems have been widely devel-
oped in recent years. However, when dialogs are
conducted in noisy environments or the utterance
itself is noisy, it is difficult for machines to cor-
rectly recognize or understand user utterances. In
this paper we present a novel dialog state track-
ing method, which directly models the joint prob-
ability of hypotheses onN -best lists. Experiments
are then conducted on the DSTC shared corpus,
which provides a common dataset and an evalua-
tion framework
The remainder of this paper is organized as fol-
lows. Section 2 reviews relevant studies in dia-
log state tracking. Section 3 introduces our new
approach and presents the model and features we
used in detail. Section 4 describes experiment set-
tings and gives the result. Section 5 concludes this
paper with a discussion for possible future direc-
tions.
2 Previous Work
For the task of dialog state tracking, previous
research focused on dynamic Bayesian models
(DBN)(Young et al, 2013). User goal, dialog his-
tory and other variables are modeled in a graphi-
cal model. Usually, Markov assumptions are made
and in each turn the dialog state is dependent on
the ASR outputs and the dialog state of the pre-
vious turn. Dependency on other features, such
as system action, dialog history could be assumed
as long as their likelihood is modeled. For a
POMDP-based dialog model, the state update rule
is as follows:
bt+1(st+1) = ?P (ot+1|st+1, at)?
st
P (st+1|st, at)bt(st) (1)
where bt(st) is the belief state at time t, ot+1 is the
observation at time t+ 1, at is the machine action.
Thus the dialog states are estimated incrementally
turn by turn.
Since each node has hundreds, or even thou-
sands, of possible assignments, approximation is
necessary to make efficient computation possible.
In POMDP-based dialog systems, two common
approaches are adopted (Young et al, 2013), i.e.,
N -best approximation and domain factorization.
In theN -best approach, the probability distribu-
tion of user goals are approximated using N -best
list. The hidden information state (HIS) model
(Young et al, 2010) makes a further simplification
that similar user goals are grouped into a single
entity called partition, inside which all user goals
are assigned the same probabilities. The Bayesian
update of dialog state (BUDS) model (Thomson
and Young, 2010) is a representative of the second
approach and adopts a different approximation
strategy, where each node is further divided into
sub-nodes for different domain concepts and in-
dependence assumptions of sub-nodes across con-
cepts are made. Recent studies have suggested
that a discriminative model may yield better per-
formance than a generative one (Bohus and Rud-
nicky, 2006). In a discriminative model, the emis-
sion part of the state update rule is modeled dis-
criminatively. Possible flawed assumptions in a
completely generative models could be mitigated
457
in this way, such as the approximation of obser-
vation probability using SLU scores (Williams,
2012a; Williams, 2012b).
3 Proposed Method
3.1 Discriminative State Tracking Model
Most previous methods model the distribution of
user goals for each turn explicitly, which can lead
to high computation cost. In our work, the joint
probability of all items on the N -best lists from
SLU is modeled directly and the state tracking re-
sult is generated at a post-processing stage. Thus
the state tracking problem is converted into a la-
beling task as is shown in equation 2, which in-
volves modeling the joint probability of the N -
best hypotheses.
bt(st) = P (H1,1, H1,2, ...,Ht,m?1, Ht,m) (2)
where Ht,m is a binary variable indicating the
truthfulness of the m-th hypothesis at turn t.
For each turn, the model takes into account all
the slots on theN -best lists from the first turn up to
the current one, and those slots predicted to be true
are added to the dialog state. The graphical model
is illustrated in figure 1. To predict dialog state at
turn t, the N -best items from turn 1 to t are all
considered. Hypotheses assigned true labels are
included in the dialog state. Compared to the DBN
approach, the dialog states are built ?jointly?. This
approach is reasonable because what the tracker
generates is just some combinations of all N -best
lists in a session, and there is no point guessing be-
yond SLU outputs. We leverage general structured
Conditional Random Fields (CRFs) to model the
probabilities of the N -best items, where factors
are used to strengthen local dependency. Since
CRF is a discriminative model, arbitrary overlap-
ping features can be added, which is commonly
considered as an advantage over generative mod-
els.
3.2 Conditional Random Fields
CRF is first introduced to address the problem
of label bias in sequence prediction (Lafferty et
al., 2001). Linear-chain CRFs are widely used to
solve common sequence labeling problem in nat-
ural language processing. General structured CRF
has also been reported to be successful in various
tasks (Sutton and McCallum, 2012).
In general structured CRF, factor templates are
utilized to specify both model structure and pa-
...
Hyp1
Hyp2
HypN
Turn t
Slot1=...
Slot2=...
...
Turn t-1
Figure 1: Dialog state update using CRFs, where
the 8 rectangles above denote N -best hypothe-
ses for each turn, and the box below represents
the dialog state up to the current turn. Con-
nections between rectangles denote ?Label-Label?
factors. ?Label-Observation? factors are not shown
for simplicity.
rameter tying (Sutton and McCallum, 2012). Fac-
tors are partitioned into a series of templates, and
factors inside each template share the same param-
eters.
p(y|x) = 1Z(x)
?
Cp?C
?
?c?Cp
?c(xc,yc; ?p), (3)
where C is the set of factor templates and x,y are
inputs and labels respectively. Template factors
are written as
?c(xc,yc; ?p) = exp
K(p)?
k=1
?pkfpk (xc,yc) (4)
and Z(x) is the normalizing function
Z(x) =
?
y
?
Cp?C
?
?c?Cp
?c(xc,yc; ?p) (5)
In the experiment we use Factorie1 to define and
train the model.
3.3 Model Structure and Features
In the model, slots in every N -best item up
to the current turn are represented as binary
variables. For simplification of data structure,
each slot in a single N -best item is extracted
and represented using different label vari-
ables, with the same rank indicating their
1Available from https://github.com/
factorie/factorie.
458
original places in the N -best list. For exam-
ple, the item slots: [from: Pittsburgh,
data: Tuesday], score: 0.85, rank: 2,
is converted to two slots: slots: [from:
Pittsburgh], score: 0.85, rank: 2 and
slots: [date: Tuesday], score: 0.85,
rank: 2. Label-label connections are specified
using factor templates between slot pairs, and
Label-observation templates are used to add
slot-wise features. Without label-label connection
the model is reduced to a maximum entropy
model, and with more connections added, the
graph tends to have loopy structures.
Two classes of feature sets (templates) in the ex-
periment are defined as follows.
(1) Label-Label factor templates are used to
strengthen the bond between certain slots.
Pairwise-slots of the same rank This template is
built for pairs of slots in a turn with the same
rank to bind their boolean assignment. To
avoid creating too many loops and make in-
ference efficient, the factors are added in such
an order that the slots involved in a single turn
are linked in a linear way.
Pairwise-slots with identical value Slots with
identical value may appear in the N -best
list for multiple times. Besides, user can
mention the same slot in different turns,
making these slots more reliable. Similar
ordering mechanism is utilized to avoid
redundant loops.
(2) Label-observation templates are used to add
features for the identification of the truthfulness of
slots.
SLU score and rank of slot The score generated
by the ASR and SLU components is a direct
indicator of the correctness degree of slots.
However, a slot?s true reliability is not neces-
sarily linear with its score. The relationship is
quite different for various ASR and SLU al-
gorithms, and scores produced by some ASR
are not valid probabilities. As we adopt a
data-driven approach, we are able to learn
this relationship from data. In addition to the
SLU score, the slot rank is also added to the
feature set.
Dialog history (grounding information) In
most spoken dialog systems, explicit and
implicit groundings are adapted to indicate
the correctness of the system belief. This
information is useful to determine the
correctness of slots. The grounding infor-
mation includes grounding type (implicit
or explicit grounding), user reply (negation
or confirmation) and corresponding SLU
scores.
Count of slots with identical value As previ-
ously mentioned, slots with identical values
can appear several times and slots with more
frequent occurrences are more likely to be
correct.
Domain-specific features Slots for some domain
concepts often have values with specific
forms. For example, in the DSTC data sets,
the route slots are usually filled with values
like ?61d?, ?35b?, and SLU often generates
noisy outputs like ?6d?, ?3d?. Thus the lexi-
cal form is a very useful feature.
Baseline Tracker The simple and fast 1-best
tracking algorithm is used as the baseline
tracker and exhibits a satisfying performance.
Thus the tracking result is added as an addi-
tional feature. This indicates the possibility
of combining tracking outputs from differ-
ent algorithms in this discriminative model,
which may improve the overall tracking per-
formance.
4 Experiment
4.1 Task and Data
The Dialog State Tracking Challenge (DSTC)2
aims at evaluating dialog state tracking algorithms
on shared real-user dialog corpus. In each dia-
log session, ASR and SLU results are annotated,
making it possible to conduct direct comparison
among various algorithms. For further details,
please refer to the DSTC handbook (Williams et
al., 2013b).
4.2 Corpus Preprocessing
The ASR and SLU components can generate many
noisy hypotheses which are completely wrong,
rendering the dialog corpus seriously imbalanced
at the level of slots (there are more wrong slots
than true slots). We use resampling to prevent
2http://research.microsoft.com/en-us/
events/dstc/
459
the model from biasing towards making negative
judgements. Before training, the total number of
correct slots in a set is counted, and equal num-
ber of wrong slots are sampled from the subset of
all the wrong slots. These chosen negative slots
plus all the positive slots together constitute the
effective slot set for model training, with remain-
ing slots fixed to their true value and regarded as
observed variables. To make full use of the dia-
log corpus, this process is repeated for eight times,
producing a bigger and balanced corpus.
4.3 Model Training
In the training phase, the log-likelihood function
is optimized using the LBFGS method with L2-
regularization. Loopy belief propagation is used
as the inference routine. It can be shown that for
factor graphs without loops, the marginal proba-
bilities produced by loopy belief propagation are
exact. Model selection is done according to the
log-likelihood on the development set.
4.4 Predicting and Tracking
For each dialog session, the predicted slot labels
are mapped to tracking results. To produce a N -
best list of tracking results, the top N configura-
tions of slots and corresponding probability scores
are generated. Gibbs sampling is adopted. The
sampling is repeated for 1000 times in each cor-
pus, after each sampling the configuration of slots
is mapped to certain tracking state. More efficient
inference routines, such as M-best belief propaga-
tion (Yanover and Weiss, 2004), could be utilized,
which would be suitable for practical real-time ap-
plication.
4.5 Results
After tracker outputs are generated based on the
sampling results, they are scored using evaluation
tools provided by the DSTC organizers. Several
metrics are evaluated, including precisions, ROC
performance, etc. Individual and joint slots are
scored respectively. And different schedules are
used, which indicats the turns included for evalu-
ation. Partial results are shown in table 1. A sys-
tematic analysis by the organizers is in the DSTC
overview paper (Williams et al, 2013a). The com-
plete challenge results can be found on DSTC
website. On the test sets of test1, test2 and test3,
the CRF-based model achieves better performance
than the simple baseline in most metrics. How-
ever, on test4, the performance degrades seriously
when there is a mismatch between training data
and test data, since test4 is produced by a different
group and does not match the training set. It shows
that the CRF-based model is very flexible and is
able to learn the properties of ASR and SLU, thus
adapting to a specific system. But it has a tendency
of overfitting .
Test1 Test4
Metric CRF BASE CRF BASE
ACC 0.987 0.983 0.960 0.986
L2 0.020 0.021 0.046 0.017
MRR 0.990 0.988 0.980 0.990
CA05 0.987 0.983 0.960 0.986
EER 0.015 0.983 0.021 0.012
Table 1: Results of slot ?Date? on Test1 and Test4
(schedule1 is used). The tracker used on Test4 is
trained on Test3. Details of the metrics can be
found in DSTC handbook(Williams et al, 2013b)
5 Conclusions and Future Directions
We proposed a CRF-based discriminative ap-
proach for dialog state tracking. Preliminary re-
sults show that it achieves better performance than
the 1-best baseline tracker in most metrics when
the training set and testing set match. This indi-
cates the feasibility of our approach which directly
models joint probabilities of the N -best items.
In the future, we will focus on the following
possible directions to improve the performance.
Firstly, we will enrich the feature set and add more
domain-related features. Secondly, interactions of
slots between dialog turns are not well modeled
currently. This problem can be alleviated by tun-
ing graph structures, which deservers further stud-
ies. Moreover, it is challenging to use online train-
ing methods, so that the performance could be im-
proved incrementally when more training samples
are available.
6 Acknowledgments
This work is partially supported by the Na-
tional Natural Science Foundation of China (Nos.
10925419, 90920302, 61072124, 11074275,
11161140319, 91120001), the Strategic Prior-
ity Research Program of the Chinese Academy
of Sciences (Grant Nos. XDA06030100,
XDA06030500), the National 863 Program (No.
2012AA012503) and the CAS Priority Deploy-
ment Project (No. KGZD-EW-103-2).
460
References
Dan Bohus and Alex Rudnicky. 2006. A ?k hypotheses
+ other? belief updating model. In Proceedings of
the 2006 AAAI Workshop on Statistical and Empiri-
cal Approaches for Spoken Dialogue Systems, pages
13?18, Menlo Park, California. The AAAI Press.
John Lafferty, Andrew Mccallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
Charles A. Sutton and Andrew McCallum. 2012. An
introduction to conditional random fields. Founda-
tions and Trends in Machine Learning, 4(4):267?
373.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013a. The dialog state track-
ing challenge. In Proceedings of the 14th SIGdial
workshop on Discourse and Dialogue.
Jason D. Williams, Antoine Raux, Deepak Ra-
machandran, and Alan Black. 2013b. Dia-
log state tracking challenge handbook. Avail-
able from http://research.microsoft.
com/apps/pubs/?id=169024.
Jason D. Williams. 2012a. Challenges and opportu-
nities for state tracking in statistical spoken dialog
systems: Results from two public deployments. Se-
lected Topics in Signal Processing, IEEE Journal of,
6(8):959 ?970.
Jason D. Williams. 2012b. A critical analysis of two
statistical spoken dialog systems in public use. In
SLT, pages 55?60. IEEE.
Chen Yanover and Yair Weiss. 2004. Finding the
m most probable configurations using loopy belief
propagation. Advances in Neural Information Pro-
cessing Systems, 16:289?296.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Gas?ic?, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160?1179.
461
Proceedings of the SIGDIAL 2014 Conference, pages 327?331,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Markovian Discriminative Modeling for Dialog State Tracking
Hang Ren, Weiqun Xu, Yonghong Yan
The Key Laboratory of Speech Acoustics and Content Understanding
Institute of Acoustics, Chinese Academy of Sciences
21 North 4th Ring West Road, Beijing, China, 100190
{renhang, xuweiqun, yanyonghong}@hccl.ioa.ac.cn
Abstract
Discriminative dialog state tracking has
become a hot topic in dialog research com-
munity recently. Compared to genera-
tive approach, it has the advantage of be-
ing able to handle arbitrary dependent fea-
tures, which is very appealing. In this
paper, we present our approach to the
DSTC2 challenge. We propose to use dis-
criminative Markovian models as a natu-
ral enhancement to the stationary discrim-
inative models. The Markovian structure
allows the incorporation of ?transitional?
features, which can lead to more effi-
ciency and flexibility in tracking user goal
changes. Results on the DSTC2 dataset
show considerable improvements over the
baseline, and the effects of the Markovian
dependency is tested empirically.
1 Introduction
Spoken dialog systems (SDS) have become much
more popular these days, but still far from wide
adoption. One of the most outstanding problems
that affect user experience in an SDS is due to
automatic speech recognition (ASR) and spoken
language understanding (SLU) errors. While the
advancement of ASR technology has a positive ef-
fect on SDS, it is possible to improve the SDS user
experience by designing a module which explicitly
handles ASR and SLU errors. With accurately es-
timated dialog state, the dialog manager could se-
lect more effective and flexible dialog actions, re-
sulting in shorter dialogs and higher dialog success
rate. Dialog state tracking is the task of identifying
the correct dialog state (user action, user goal, etc.)
from ASR and SLU outputs in the presence of er-
rors. Commercial dialog systems these days usu-
ally use simple dialog state tracking strategies that
only consider the most probable SLU output. Pre-
vious research shows that several errors in dialog
state tracking can be rectified by considering the
full N-best results from the ASR and SLU compo-
nents (Williams, 2012). Thus it is very important
to develop robust and practical dialog state track-
ing models.
In statistical dialog state tracking, models
can be roughly divided into two major classes,
i.e. generative and discriminative. Generative
(Bayesian) dialog tracking models are prevalent
in early studies due to its close relationship with
the POMDP dialog management model (Young et
al., 2013). Generative models generally use Dy-
namic Bayesian Networks to model the observa-
tion probability P (O
t
|S
t
) and transition probabil-
ity P (S
t
|S
t?1
), where O
t
and S
t
are observations
and dialog state at turn t. In a discriminative
model, the conditional probability P (S
t
|O
t
1
) is
modeled directly, where O
t
1
is all the observations
from turn 1 to t. One problem with the generative
models is that the independent assumptions are al-
ways not realistic. For example, N-best hypothe-
ses are often assumed independent of each other,
which is flawed in realistic scenarios (Williams,
2012). Furthermore, it is intrinsically difficult for
generative models to handle overlapping features,
which prevents model designers from incorporat-
ing arbitrarily large feature set. Discriminative
model does not suffer from the above problems as
there is no need to make any assumptions about
the probabilistic dependencies of the features. As
a result. it is potentially able to handle much larger
feature sets and to make more accurate predic-
tions (Bohus and Rudnicky, 2006). Discrimina-
tive models also tend to be more data-driven, un-
like generative models in which many sub-models
parameters are heuristically tuned.
2 DSTC1 revisited
The first Dialog State Tracking Challenge
(DSTC1) for the first time provided a common
test bed for various state tracking methods, and
327
several participants employed various discrimi-
native models in the challenge. DSTC1 provided
real user dialog corpora in the domain of bus
route service to evaluate performance of various
state tracking methods. In DSTC1 there are 9
teams with 27 submissions, where discriminative,
generative and rule-based models are used in the
challenge. Maximum entropy models (Lee and
Eskenazi, 2013), conditional random fields (Lee,
2013) and neural networks (Henderson et al.,
2013) are the most frequently used discriminative
models, which gave competitive results on several
metrics. It has been empirically analyzed that dis-
criminative methods are especially advantageous
when the ASR/SLU confidence scores are poorly
estimated (Williams et al., 2013).
3 Discriminative modeling in dialog state
tracking
In the design of a slot-filling or task-oriented di-
alog systems, dialog state tracking can be consid-
ered as a classification problem, i.e. assigning pre-
defined values to a fixed number of slots. One
major problem in the formulation is that in com-
plex dialog scenarios the number of classes tends
to be very big, resulting in extremely sparse train-
ing instances for each class. This sparsity affects
the classification performance. A large predic-
tion domain also leads to computation inefficiency
which makes the model less practical. Usually we
could focus only on the on-list hypotheses, which
are the hypotheses appeared in the SLU results,
and all the other values in the slot value set are
grouped into a meta category Other. It is simi-
lar to the partition concept in HIS (Young et al.,
2010), and by doing this we could reduce the num-
ber of classes to a reasonable size. We use Y
t
to
denote the prediction domain at turn t. Although
the number of classes is reduced by focusing on
the dynamically generated Y
t
, some classes will
still suffer from the lack of training instances, and
what is even worse is that a large portion of the
classes will not have any training data, since in
practical SDS deployment it is hard to collect a
large dialog corpus. To handle the data sparseness
problem, parameters are often shared across dif-
ferent slots, or even data sets, and by doing this the
model complexity could be effectively controlled
and the overfitting problem would be alleviated.
Williams proposed to use various techniques from
multi-domain learning to improve model perfor-
Monday: 0.5
Thursday: 0.2
Other: 0.3
Monday: 0.7
Tuesday: 0.1
Thursday: 0.1
Other: 0.1
Observations 
from turn 1 to t
Turn t-1 Turn t
Figure 1: Markovian discriminative model depen-
dency diagram. In this figure the dialog state is
simplified to a single slot variable: date, the do-
main of the slot typically increases as dialog con-
tinues, which includes all the slot values appeared
as SLU results. As indicated by the arrows, S
t
depends on S
t?1
and O
t
1
. In stationary discrimi-
native model, there?s no dependency between ad-
jacent turns indicated by the upper arrow.
mance (Williams, 2013), which could be taken as
another way of parameter sharing.
3.1 Markovian discriminative model
A dialog can be naturally seen as a temporal se-
quence involving a user and an agent, where strong
dependencies exist between adjacent turns. In typ-
ical task-oriented dialogs, users often change their
goals when their original object cannot be satis-
fied. Even when the true user goal stays constant
in a dialog session, the agent?s perception of it will
tend to evolve and be more accurate as the con-
versation proceeds, and thus the dialog state will
often change. The states at adjacent turns are sta-
tistically correlated, and therefore it is important
to leverage this natural temporal relationship in
tracking dialog state. We enhance the stationary
discriminative model in a similar way as described
in (McCallum et al., 2000), by assuming Marko-
vian dependency between adjacent turns.
Thus, the original probability P (S
t
|O
t
1
) can be
factored into the following form:
P (S
t
|O
t
1
) = (1)
?
S
t?1
?S
P (S
t
|O
t
1
, S
t?1
)P (S
t?1
|O
t?1
1
)
The graphical model is shown is figure
1. Unlike stationary discriminative models,
328
we model the conditional transition probability
P (S
t
|O
t
1
, S
t?1
) instead of P (S
t
|O
t
1
) and the dia-
log state is updated according to equation 1 at each
turn. The feature functions in the structured model
can depend on the state of the previous turn, which
we call transitional features.
It is worth noting that stationary discriminative
model can include features built from dialog his-
tory (Metallinou et al., 2013). The major dif-
ference in utilizing this information from our ap-
proach is that by explicitly assuming the Marko-
vian dependency, the structured model is able to
exploit the whole probabilistic dialog state distri-
bution of the previous turn. The previous dialog
state S
t?1
is inferred from previous dialog history
O
t?1
1
, which contains higher level hypotheses than
the raw history features. Apart from that, the struc-
tured model can also use any stationary features
built from O
t
1
, which makes the stationary model
a special case of the structured one.
3.2 Neural network classifier
We use the family of multi-layer neural net-
works to model the transition probability
P (S
t
|O
t?1
1
, S
t?1
). To allow for the use of the
dynamic prediction domain, we utilize a forward
network structure similar to (Henderson et al.,
2013). Feature vectors for each class in Y
t
are
fed into the model and forwarded through several
hidden layers for non-linear transformation in the
hope that deeper layers may form higher abstrac-
tion of the raw inputs. The parameter vectors for
each class are shared. For each feature vector
the model generates a real score. The scores for
all the classes in Y
t
are then normalized using a
softmax function resulting in valid probabilities.
y
i
= W
l?1
? g
l?1
(. . . g
1
(W
1
?X
i
) . . .) (2)
P
Y
= Softmax(y
1
, . . . , y
|Y
t
|
) (3)
where g
1
to g
l?1
are sigmoid functions, W
i
is the
weight matrix for linear transformation at layer i
and X
i
= f(O
t
1
, y
i
) is the feature vector for class
i. We also test maximum entropy models, which
can be seen as a simple neural network without
hidden layers:
P (Y = y|O
t
1
) =
e
??f(O
t
1
,y)
?
y?Y
e
??f(O
t
1
,y)
(4)
4 DSTC2 challenge
DSTC2 is the second round of Dialog State Track-
ing Challenge, and it provides dialog corpora
collected from real human-machine dialogs in a
restaurant domain. The corpora are split into la-
beled training and development sets and unlabeled
test set. Test sets are collected from a SDS dif-
ferent from the training and development set to
reflect the mismatch in real deployment. Unlike
DSTC1, the user goal often changes in DSTC2
when the condition specified by the user cannot
be met. For evaluation DSTC2 defined a number
of metrics among which several featured metrics
are selected. Besides tracking user goals (the val-
ues of each slot), two additional states method and
requested slots are also defined, which track the
method to query and the slots requested by users
respectively. Further details about DSTC2 could
be found in (Henderson et al., 2014).
5 Feature set
We briefly describe the feature set used in our sys-
tem. We only use the live SLU information pro-
vided by the organizers, and no extra external data
is used. The features used can be divided into two
classes.
stationary features which only depend on the
observations and the class (slot value) pre-
dicted at current turn in the form of f(y
t
, O
t
).
transitional features that can also depends on
the predicted class at the previous turn in the
form of f(y
t
, y
t?1
, O
t
).
Stationary features include:
? SLU Scores: confidence scores of the current
prediction binned into boolean values, raw
scores are also added as real features.
? SLU Status: whether the prediction is denied,
informed and confirmed in the current turn.
? Dialog history: whether the prediction has
been denied, informed and confirmed in all
the dialog turns until the current one.
? User/system action: The most probable user
action and the machine action in the current
turn.
The transitional features are as follows:
? Transition1: whether the predictions in the
previous and the current turn are the same.
329
Name Model Class Hidden layers
Entry1 MEMM ?
Entry2 Structured NN [50]
Entry3 Structured NN [50, 30]
MLP Stationary NN [50, 30]
Table 1: Configurations of models. The model
MLP uses the same structure as Entry3, but with-
out the transitional features described in section 5.
Number in brackets denotes the number of units
used in each hidden layers.
? Transition2: joint feature of Transition1 in
conjunction with the machine action in cur-
rent turn, i.e. for each machine cation, Tran-
sision1 is replicated and only the one corre-
sponding to the machine action at current turn
is activated.
Transitional features are specific to Markovian
models while stationary features can be used in
any discriminative models.
6 Model training
Markovian models in various forms are tested to
find the most appropriate structure for the task.
Models for ?method? and ?state? are built sepa-
rately using similar structured models.
When using the maximum entropy model to
build the conditional probability, the Markovian
model is equivalent to the maximum-entropy
Markov model (MEMM) model introduced in
(McCallum et al., 2000). More sophisticated neu-
ral networks with different configurations are used
to fit the model to more complex patterns in the
input features. In tracking the state ?goal?, the
joint distribution of slots is built assuming differ-
ent slots are independent of each other. From the
perspective of practical implementation, one ad-
vantage of the simpler MEMM model is that the
training objective is convex. Thus the optimiza-
tion routine is guaranteed to find the global opti-
mum, while neural networks with hidden layers al-
ways have many local optima which require care-
ful initialization of the parameters. LBFGS (Liu
and Nocedal, 1989) is used in optimizing the batch
log-likelihood objective and L1 and L2 regulariz-
ers are used to penalize the model from overfitting.
We train the model on the training set, the devel-
opment set is used for model selection and models
produced at each training iteration are evaluated.
State Tracker ACC L2 CA05
Goal
Baseline 0.619 0.738 0.000
Entry1 0.707 0.447 0.223
Entry2 0.713 0.437 0.207
Entry3 0.718 0.461 0.100
MLP 0.713 0.448 0.128
Method
Baseline 0.875 0.217 0.000
Entry1 0.865 0.228 0.199
Entry2 0.871 0.211 0.290
Entry3 0.871 0.210 0.287
MLP 0.946 0.092 0.000
Requested
Baseline 0.884 0.196 0.000
Entry1 0.932 0.118 0.057
Entry2 0.947 0.093 0.218
Entry3 0.951 0.085 0.225
MLP 0.863 0.231 0.291
Table 2: Evaluation results on the DSTC2 test set.
ACC stands for accuracy, L2 measures the Eu-
clidean distance between the predicted distribution
and the ground truth vector with only the correct
hypothesis set to 1. CA05 is the correct accep-
tance rate when false acceptance rate is 5%. De-
tails of the metrics can be found in (Henderson et
al., 2014). Except L2, the larger the scores, the
better the performance.
In DSTC2 we submitted 3 trackers, an additional
tracker without the transitional features is trained
afterwards for comparison. Configurations of the
models are described in table 1.
7 Experiments and part of the results
Featured metrics on the test set are shown in ta-
ble 2. By most metrics our models are superior
to the simple baseline. Especially in tracking user
goals which is the most important state to track in
DSTC2, the discriminative trackers show consid-
erable performance gain. Judging from the per-
formance of Entry1 to Entry3, we can conclude
that the more complex 2-layer neural networks
have better performance. Markovian neural net-
works can fit to the training instances with much
more flexibility than the simple MEMM model.
We have also trained a standard multi-layer neural
network (MLP) model by disabling all the transi-
tional features. By comparing the model ?Entry 3?
and ?MLP?, which share the same network struc-
ture, we explicitly test the effect of the Marko-
vian structure. On the state ?goal? and ?requested?,
the Markovian model shows better tracking accu-
330
racies, which means that the Markovian structure
has a positive effect on fitting the target. But in
tracking the state ?method?, the MLP model has
the best performance among all the models com-
pared. Thus although the log-likelihood increases
considerably on the training set by adding the tran-
sitional features, the overfiting to the training set is
more serious in tracking ?method?.
8 Conclusion
We described the models used in the DSTC2 chal-
lenge. We proposed a novel approach to enhanc-
ing the model capability of stationary discrimina-
tive models in dialog state tracking by assuming
Markovian dependencies between adjacent turns.
The results showed better performance than the
simple baseline which uses the most probable hy-
pothesis, and we empirically compared the mod-
els with and without the Markovian dependency.
In future work, more discriminative models in dif-
ferent forms could be compared to evaluate their
capability, and the effects of the Markovian struc-
ture and transitional features needs to be further
studied.
Acknowledgments
We would like to thank the DSTC committee for
their great efforts in organizing the challenge. We
also thank the anonymous reviewers for their con-
structive comments.
This work is partially supported by the Na-
tional Natural Science Foundation of China (Nos.
10925419, 90920302, 61072124, 11074275,
11161140319, 91120001), the Strategic Prior-
ity Research Program of the Chinese Academy
of Sciences (Grant Nos. XDA06030100,
XDA06030500), the National 863 Program (No.
2012AA012503) and the CAS Priority Deploy-
ment Project (No. KGZD-EW-103-2).
References
Dan Bohus and Alex Rudnicky. 2006. A k-
hypotheses+ other belief updating model. In Proc.
of the AAAI Workshop on Statistical and Empirical
Methods in Spoken Dialogue Systems.
Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for
the dialog state tracking challenge. In Proceedings
of the SIGDIAL 2013 Conference, pages 467?471,
Metz, France, August. Association for Computa-
tional Linguistics.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the SIGDIAL 2014
Conference, Baltimore, U.S.A., June.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, pages
414?422, Metz, France, August. Association for
Computational Linguistics.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the
SIGDIAL 2013 Conference, pages 442?451, Metz,
France, August. Association for Computational Lin-
guistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Andrew McCallum, Dayne Freitag, and Fernando C. N.
Pereira. 2000. Maximum entropy markov mod-
els for information extraction and segmentation. In
Pat Langley, editor, ICML, pages 591?598. Morgan
Kaufmann.
Angeliki Metallinou, Dan Bohus, and Jason Williams.
2013. Discriminative state tracking for spoken di-
alog systems. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 466?475, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL 2013
Conference, page 404?413, Metz, France, August.
Association for Computational Linguistics.
Jason Williams. 2012. A critical analysis of two
statistical spoken dialog systems in public use. In
2012 IEEE Spoken Language Technology Workshop
(SLT), pages 55?60.
Jason Williams. 2013. Multi-domain learning and
generalization in dialog state tracking. In Proceed-
ings of the SIGDIAL 2013 Conference, pages 433?
441, Metz, France, August. Association for Compu-
tational Linguistics.
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech & Lan-
guage, 24(2):150?174.
Steve Young, Milica Ga?si?c, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE, 101(5):1160?1179.
331
