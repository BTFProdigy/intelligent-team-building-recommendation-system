Proceedings of the 12th European Workshop on Natural Language Generation, pages 130?137,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Investigating Content Selection for Language Generation using Machine
Learning
Colin Kelly
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge, UK
Ann Copestake
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge, UK
{colin.kelly,ann.copestake,nikiforos.karamanis}@cl.cam.ac.uk
Nikiforos Karamanis
Department of Computer Science
Trinity College Dublin
Dublin 2
Ireland
Abstract
The content selection component of a nat-
ural language generation system decides
which information should be communi-
cated in its output. We use informa-
tion from reports on the game of cricket.
We first describe a simple factoid-to-text
alignment algorithm then treat content se-
lection as a collective classification prob-
lem and demonstrate that simple ?group-
ing? of statistics at various levels of granu-
larity yields substantially improved results
over a probabilistic baseline. We addi-
tionally show that holding back of specific
types of input data, and linking database
structures with commonality further in-
crease performance.
1 Introduction
Content selection is the task executed by a natu-
ral language generation (NLG) system of decid-
ing, given a knowledge-base, which subset of the
information available should be conveyed in the
generated document (Reiter and Dale, 2000).
Consider the task of generating a cricket match
report, given the scorecard for that match. Such
a scorecard would typically contain a large num-
ber of statistics pertaining to the game as a whole
as well as individual players (e.g. see Figure 1).
Our aim is to identify which statistics should be
selected by the NLG system.
Much work has been done in the field of con-
tent selection, in a diverse range of domains e.g.
weather forecasts (Coch, 1998). Approaches are
usually domain specific and predominantly based
on structured tables of well-defined input data.
Duboue and McKeown (2003) attempted a sta-
tistical approach to content selection using a sub-
stantial corpus of biographical summaries paired
with selected content, where they extracted rules
and patterns linking the two. They then used ma-
chine learning to ascertain what was relevant.
Barzilay and Lapata (2005) extended this ap-
proach but applying it to a sports domain (Amer-
ican football), similarly viewing content selection
as a classification task and additionally taking ac-
count of contextual dependencies between data,
and found that this improved results compared to
a content-agnostic baseline. We aim throughout
to extend and improve upon Barzilay and Lapata?s
methods.
We emphasise that content selection through
statistical machine learning is a relatively new area
? approaches prior to Duboue and McKeown?s are,
in principle, much less portable ? and as such there
is not an enormous body of work to build upon.
This work offers a novel algorithm for data-to-
text alignment, presents a new ?grouping? method
for sharing knowledge across similar but distinct
learning instances and shows that holding back
certain data from the machine learner, and rein-
troducing it later on can improve results.
2 Data Acquisition & Alignment
We first must obtain appropriately aligned cricket
data, for the purposes of machine learning.
Our data comes from the online Wisden al-
manack (Cricinfo, 2007), which we used to down-
load 133 match report/scorecard pairs. We em-
ployed an HTML parser to extract the main text
from the match report webpage, and the match
data-tables from the scorecard webpage. An ex-
ample scorecard can be found in Figure 11.
1Cricket is a bat-and-ball sport, contested by two oppos-
ing teams of eleven players. Each side?s objective is to score
more ?runs? than their opponents. An ?innings? refers to the
collective performance of the batting team, and (usually) ends
when all eleven players have batted.
In Figure 1, in the batting section R stands for ?runs made?,
M for ?minutes played on the field?, B for ?number of balls
faced?. 4s and 6s are set numbers of runs awarded for hit-
ting balls that reach the boundary. SR is the number of runs
per 100 balls. In the bowling section, O stands for ?overs
130
Result India won by 63 runs
India innings (50 overs maximum) R M B 4s 6s SR
SC Ganguly? run out (Silva/Sangakarra?) 9 37 19 2 0 47.36
V Sehwag run out (Fernando) 39 61 40 6 0 97.50
D Mongia b Samaraweera 48 91 63 6 0 76.19
SR Tendulkar c Chandana b Vaas 113 141 102 12 1 110.78
. . .
Extras (lb 6, w 12, nb 7) 25
Total (all out; 50 overs; 223 mins) 304
Fall of wickets 1-32 (Ganguly, 6.5 ov), 2-73 (Sehwag, 11.2 ov), 3-172 (Mongia,
27.4 ov), 4-199 (Dravid, 32.1 ov), . . . , 10-304 (Nehra, 49.6 ov)
Bowling O M R W Econ
WPUJC Vaas 10 1 64 1 6.40 (2w)
DNT Zoysa 10 0 66 1 6.60 (6nb, 2w)
. . .
TT Samaraweera 8 0 39 2 4.87 (2w)
Figure 1: Statistics in a typical cricket scorecard.
2.1 Report Alignment
We use a supervised method to train our data, and
thus need to find all ?links? between the scorecard
and match report. We execute this alignment by
first creating tags with tag attributes according to
the common structure of the scorecards, and tag
values according to the data within a particular
scorecard. We then attempt to automatically align
the values of those tags with factoids, single pieces
of information found in the report.
For example, from Figure 1 the fact that Ten-
dulkar was the fourth player to bat on the first team
is captured by constructing a tag with tag attribute
team1 player4, and tag value ?SR Tendulkar?. The
fact he achieved 113 runs is encapsulated by an-
other tag, with tag attribute as team1 player4 R
and tag value as ?113?. Then if the report con-
tained the phrase ?Tendulkar made 113 off 102
balls? we would hope to match the ?Tendulkar?
factoid with our tag value ?SR Tendulkar?, the
?113? factoid with our tag value ?113? and replace
both factoids with their respective tag attributes, in
this case team1 player4 and team1 player4 R re-
spectively. Similar methods for this problem have
been employed by Barzilay and Lapata (2005) and
Duboue and McKeown (2003).
The basic idea behind our 6-step process for
alignment is that we align those factoids we are
bowled?, M for ?maiden overs?, R for ?runs conceded? and W
for ?wickets taken?. Econ is ?economy rate?, or number of
runs per over.
It is important to note that Figure 1 omits the opposing
team?s innings (comprising new instances of the ?Batting?,
?Fall of Wickets? and ?Bowling? sections), and some addi-
tional statistics found at the bottom of the scorecard.
most certain of first. The main obstacle we face
when aligning is the large incidence of repeated
numbers occurring within the scorecard, as this
would imply we have multiple, different tags all
with the same tag values. It is wholly possible
(and quite typical) that single figures will be re-
peated many times within a single scorecard2.
Therefore it would be advantageous for us to
have some means to differentiate amongst tags,
and hopefully select the correct tag when encoun-
tering a factoid which corresponds to repeated tag
values. Our algorithm is as follows:
Preprocessing We began by converting all ver-
balised numbers to their cardinal equivalents, e.g.
?one?, ?two? to ?1?, ?2?, and selected instances of
?a? into ?1?.
Proper Nouns In the first round of tagging we
attempt to match proper names from the scorecard
with strings within the report. Additionally, we
maintain a list of all players referenced thus far.
Player-Relevant Details Using the list of play-
ers we have accumulated, we search the report for
matches on tag values relating to only those play-
ers. This step was based on the assumption that a
factoid about a specific player is unlikely to appear
unless that player has been named.
Non-Player-Relevant Details The next stage
involves attempting to match factoids to tag values
whose attributes don?t refer to a particular player
e.g., more general match information as well as
team statistics.
2For example in Figure 1 we can see the number 6 appear-
ing four times: twice as the number of 4s for two different
players, once as an lb statistic and once as an nb statistic.
131
Anchor-Based Matching We next use sur-
rounding text anchor-based matching: for exam-
ple, if a sentence contains the string ?he bowled
for 3 overs? we will preferentially attempt to match
the factoid ?3? with tag values from tags which we
know refer to overs.
Remaining Matches The final step acts as our
?catch-all? ? we proceed through all remaining
words in the report and try to match each poten-
tial factoid with the first (if any) tag found whose
tag value is the same.
2.2 Evaluation
The output of our program is the original text with
all aligned figures and strings (factoids) replaced
with their corresponding tag attributes. We can see
an extract from an aligned report in Figure 2 where
we show the aligned factoids in bold, and their cor-
responding tag attributes in italics. We also note at
this point that much of commentary shown does
not in fact appear in the scorecard, and therefore
additional knowledge sources would typically be
required to generate a full match report ? this is
beyond the scope of our paper, but Robin (1995)
attempts to deal with this problem in the domain
of basketball using revision-based techniques for
including additional content.
We asked a domain expert to evaluate five of
our aligned match reports ? he did this by creat-
ing his own ?gold standard? for each report, a list
of aligned tags. Compared to our automatically
aligned tags, we obtained 79.0% average preci-
sion, 75.8% average recall and a mean F of 77.0%.
3 Categorization
We are using the methods of Barzilay and Lapata
(henceforth B&L) as our starting point, so we de-
scribe what we did to emulate and extend them.
3.1 Barzilay and Lapata?s Method
B&L?s corpus was composed of a relational
database of football statistics. Within the database
were multiple tables, which we will refer to as
?categories? (actions within a game, e.g. touch-
downs and fumbles). Each category was com-
posed of ?groups? (the rows within a category ta-
ble), with each row referring to a distinct player,
and each column referring to different types of ac-
tion within that category (?attributes?).
B&L?s technique for the purposes of the ma-
chine learning was to assign a ?1? or ?0? to each
NatWest Series (series), match 9 (team1 player1 R)
India v Sri Lanka (matchtitle)
At Bristol (venue town), July 11 (date) (day/night
(daynight)).
India (team1) won by 63 runs (winmethod).
India (team1) 5 (team1 points) pts.
Toss: India (team1).
The highlight of a meaningless match was a sublime in-
nings from Tendulkar (team1 player4), who resumed
his fleeting love affair with Nevil Road to the delight
of a flag-waving crowd. On India (team1)?s only other
visit to Bristol (venue town), for a World Cup game
in 1999 against Kenya, Tendulkar (team1 player4)
had creamed an unbeaten 140, and this time he drove
with elan to make 113 (team1 player4 R) off just 102
(team1 player4 B) balls with 12 (team1 player4 4s)
fours and a (team1 player4 6s) six.
. . .
Figure 2: Aligned match report extract
row, where a row would receive the value ?1? if
one or more of the entries in the row was ver-
balised in the report. In the context of our data
we could apply a similar division, for example, by
constructing a category entitled ?Batting? with at-
tributes (columns) ?Runs?, ?Balls?, ?Minutes?, ?4s?
and ?6s? etc., and rows corresponding to players.
In this case a group within that category would
correspond to one line of the ?Innings? table in Fig-
ure 1.
We note that B&L were selecting content on a
row basis, while we are aiming to select individual
tag attributes (i.e., specific row/column cell refer-
ences) within the categories, a more difficult task.
We discuss this further in Section 6.
The technique above allows the machine learn-
ing algorithm to be aware that different statistics
are semantically related ? i.e., each group within a
category contains the same ?type? of information.
We therefore think this is a logical starting point
for our work, and we aim to expand upon it.
3.2 Classifying Tags
The key step was deciding upon an appropriate
division of our scorecard into various categories
and the groups for each category in the style of
B&L. As can be seen from Figure 1 our input in-
formation is a mixture of structured (e.g. Bowling,
Batting sections), semi-structured (Fall of Wickets
section) and almost unstructured (Result) informa-
tion. This is somewhat unlike B&L?s data, which
was fully structured in database form. We deal
132
Category Attributes Verb
Batting 9 47.0
Bowling 11 10.2
Fall of Wickets 8 46.4
Match Details 11 75.2
Match Result 8 45.1
Officials 8 6.0
Partnerships 11 75.5
Team Statistics 13 46.2
Table 1: Number of attributes per category with
percent verbalised (Verb)
with this by enforcing a stronger structure ? di-
viding the information into eight of our own ?cat-
egories?, based roughly on the formatting of the
webpages. These are outlined in Table 1.
The first three categories in the table are quite
intuitive and implicit from the respective sections
of the scorecard. There is additional information
in a typical scorecard (not shown in Figure 1),
which we must also categorise. The ?Team Statis-
tics? category contains details about the ?extras?3
scored by each team, as well as the number of
points gained by the team towards that particular
series4. We divide the remaining tag attributes as
follows into three categories: ?Officials? ? persons
participating in the match, other than the teams
(e.g. umpires, referees); ?Match Details? ? infor-
mation that would have been known before the
match was played (e.g. venue, date, season); and
?Match Result? ? data that could only be known
once the match was over (e.g. final result, player
of the match).
Finally we have an additional ?Partnerships?5
category which is given explicitly on a separate
webpage referenced from each scorecard, but is
also implicit from information contained in the
?Fall of Wickets? and ?Batting? sections. We an-
ticipate that this category will help us manage the
issue of data sparsity. For instance, in our domain
we could group partnerships (which could con-
tain a multitude of player combinations and there-
3Additional runs awarded to the batting team for specific
actions executed by the bowling team. There are four types:
No Ball, Wide, Bye, Leg Bye.
4Each cricket game is part of a specific ?series? of games.
e.g. India would receive five points for their win within the
NatWest series.
5A ?partnership? refers to a pair of players who bat to-
gether, and usually comprises information such as the num-
ber of runs scored between them, the number of deliveries
faced and so on.
fore distinct tags) with the various possible binary
combinations of players together for shared learn-
ing. We discuss this further in Section 8.3.
Within 5 of the categories described above, we
are further able to divide the data into ?groups? -
the Batting, Bowling, Fall of Wickets and Partner-
ships categories refer to multiple players and thus
have multiple rows. The Team Statistics category
contains two groups, one for each team. The other
categories merely form one-line tables.
4 Machine Learning
Our task is to establish which tag attributes (and
hence tag values) should be included in the final
match report, and is a multi-label classification
problem. We chose to use BoosTexter (Schapire
and Singer, 2000) as it has been shown to be an
effective classifier (Yang, 1999), and it is one of
the few text classification tools which directly sup-
ports multi-label classification. This is also what
B&L used.
Schapire and Singer?s BoosTexter (2000) uses
?decision stumps?, or single level decision trees
to classify its input data. The predicates of these
stumps are defined, for text, by the presence or
absence of a single term, and, for numerical at-
tributes, whether the attribute exceeds a given
threshold, decided dynamically.
4.1 Running BoosTexter
BoosTexter requires two input files to train a hy-
pothesis, ?Names? and ?Data?.
Names The Names file contains, for each pos-
sible tag attribute, t, across all scorecards, the type
of its corresponding tag value. These are contin-
uous for numbers and text for normal text. From
our 133 scorecards we extracted a total of 61,063
tag values, of which 82.2% were continuous, the
remainder being text.
Data The Data file contains, for each scorecard,
a comma-delimited list of all tag values for a par-
ticular scorecard, with a ??? for unknown values,
followed by a list of the verbalised tag attributes.
Testing We can now run BoosTexter with a
user-defined number of rounds, T , which creates
a hypothesis file. Using this hypothesis file and
a test ?data? file (without the list of verbalised tag
attributes), BoosTexter will give its hypothesized
predictions, a value f for each tag attribute t. The
sign of f determines whether the classifier be-
lieves the tag value corresponding to t is relevant
133
to the test scorecard, while |f | is a measure of the
confidence the classifier has in its assertion.
4.2 Data Sparsity
The very nature of the data means that there are
a large number of tag values which do not occur
in every scorecard ? the average scorecard con-
tained 24 values, yet our ?names? file contained
1193 possible tag attributes. A lot of this was due
to partnership tag attributes which formed 43.6%
of the ?names? entries. This large figure is because
a large number of all possible binary combinations
of players existed in the training data across both
teams6. This implies we will be unable to train for
a significant number of tag attributes as many spe-
cific tag values occur very rarely. Indeed we found
that of 158,669 entries, 97,666 (61.55%) were ?un-
known?.
5 Evaluation Baselines
It is not clear what constitutes a suitable baseline
so we considered multiple options. The issue of
ambiguous reference baselines is not specific to
the cricket domain, as there is no standardized
baseline approach across the prior literature. We
employ ten-fold cross validation throughout.
5.1 Majority Baseline
B&L created a ?majority baseline? whereby they
returned those categories (i.e., tables) which were
verbalised more than half of the time in their
aligned reports.
As explained in Section 3.2 we divided our tag
attributes into 8 categories. We emulated B&L?s
baseline method as follows: For each category, if
any of the tag values within a particular ?group?
was tagged as verbalised, we counted that as a
?vote? for that particular category. We then cal-
culated the total number of ?votes? divided by the
total number of ?groups? within each category. All
categories which had a ratio of 50% or greater
in this calculation were considered to be ?major-
ity categories?. Our baseline Bmaj then consisted
of all tag attributes forming part of those majority
categories. As shown in Table 1 there were only
two categories which exceeded the 50% threshold,
?Match Details? and ?Partnerships?.
We can see that this baseline performs
abysmally. The reason for this poor behaviour is
693 of the possible 2
?10
i=1 i = 110 combinations oc-
curred.
Bmaj ? min max ?
Precision 0.0966 0.0333 0.1583 0.0250
Recall 0.4879 0.2727 0.7895 0.0977
F 0.1603 0.0620 0.2568 0.0384
Table 2: Majority Baseline, Bmaj
that since so many tag attributes contribute to the
categories we are including far too many possibil-
ities in our baseline.
5.2 Probabilistic Baseline
This baseline is based on the premise that those
tag attributes which occur with highest frequency
across the training data refer to those tag values
which will often occur in a typical match report.
To create our baseline set of tag attributes Bprob
we extract the a most frequently verbalised tag at-
tributes across all the training data where a is the
average length of the verbalised tag attribute lists
for each report/scorecard pair.
Bprob ? min max ?
Precision 0.5157 0.2174 0.7391 0.1010
Recall 0.5157 0.1389 0.7647 0.0990
F 0.5100 0.1695 0.6939 0.0852
Table 3: Probabilistic Baseline, Bprob
This baseline achieves a mean F score of 51%,
however the tag attributes being returned are very
inconsistent with a typical match report ? they
correspond in the majority to player names but
not one refers to any other tag attributes relevant
to those players. This renders the output mostly
meaningless in terms of our aim to select content
for an NLG system.
5.3 No-Player Probabilistic Baseline
Taking the above into account we create a base-
line which derives its choice of tag attributes from
match statistics only. This baseline is similar to
the Probabilistic Baseline above, with the excep-
tion that when summing the numbers of tag at-
tributes in the sets we do not consider player-name
tag attributes in our counts. Instead, we extract
the a? most frequent tag attributes, where a? is
the average size of the sets excluding player-name
tag attributes. To finally obtain our baseline set
Bnops we merge our a? most frequent tag attributes
134
with any and all corresponding player-name tag at-
tributes7.
Bnops ? min max ?
Precision 0.4923 0.1765 0.6875 0.0922
Recall 0.3529 0.1111 0.5625 0.0842
F 0.4064 0.1538 0.5946 0.0767
Table 4: No-Player Probabilistic Baseline, Bnops
As can be seen from Table 4, this method suffers
an absolute F-score drop of more than 10% from
the previous method. However if we analyse the
output more closely we can see that although the
accuracy has dropped, the returned tag attributes
are more thematically consistent with the training
data. This is our preferred baseline.
6 Evaluation Paradigm
The main difficulty we encountered arose when
we came to assessing the Precision and Recall fig-
ures as we have yet to decide on what level we are
considering the output of our system to be correct.
We see three possibilities for the level:
Category We could simply count the ?votes?
predicted on a per category basis (as described
in sections 3.1 and 5.1), and evaluate categories
based on the number of votes given for each. We
would expect this to generate very good results as
we are effectively overgrouping, once on a group
basis (grouping together all attributes) and once on
a category basis (unifying all groups within a cate-
gory), but the output would be so general and triv-
ial (effectively stating something to the effect that
?a match report should contain information about
batting, bowling and team statistics?) that it would
be of no use in an NLG system.
Groups Here we compare which ?groups? were
verbalised within each category, and which were
predicted to be verbalised (as we did for the Major-
ity Baseline of Section 5.1). Our implicit grouping
means that we do not have to necessarily return the
correct statistic pertaining to a group since each
group acts as a basket for the statistics contained
within it, and is susceptible to ?false positives?.
This method is most similar to B&L?s.
Tags Since we are trying to establish which tag
attributes should be included rather than which
groups are likely to contain verbalised tag at-
tributes, we could say that even the above method
7e.g., if team1 player4 R is in a? then we would also in-
clude team1 player4 in our final set.
is too liberal in its definition of correctness. Thus
we also evaluate our groups on the basis of their
component parts, i.e., if a particular group of tag
attributes is estimated to be verbalised by Boos-
Texter, then we include all attributes from that
group.
7 Initial Results
Our ?categorized? results are derived from present-
ing BoosTexter with each individual category as
described in Section 3.2, then merging the selected
tag attributes together and evaluating based on the
criteria described above. We then show BoosTex-
ter?s performance ?as is?, by running the program
on the full output of our alignment stage with no
categorization/grouping.
7.1 Categorized ? Groups Level
Our ?Categorized Groups? results can be found in
Figure 3 and Table 5. For each of our tests we vary
the value of T (the number of rounds) to see how
it affects our accuracy.
Here we see we have a maximum F score of
0.7039 for T = 25. This is a very high result,
performing far better than all our baselines, how-
ever we feel the ?basketing? mentioned in Section
6 means that the results are not particularly in-
structive ? instead of specific ?interesting? tag at-
tributes, we return a grouped list of tag attributes,
only some of which are likely to be ?interesting?.
Thus we decide to no longer pursue ?grouping?
as a valid evaluation method, and evaluate all our
methods at the ?tag attribute? level.
Best ? ?
Precision 0.7620 0.7473 0.0320
CG Recall 0.6795 0.6680 0.0322
F 0.7039 0.6897 0.0106
Table 5: Categorized Groups with Best value for
T = 25.
7.2 Categorized ? Tags Level
What is notable here is that, for all values of T
which we ran our tests on (ranging from 1 to
3000), we obtained just one set of results for ?Cat-
egorized Tags?, displayed in Table 6.
This behaviour indicates that the boosting is not
helping to improve the results. Rather, it is repeat-
edly producing the same hypotheses, with vary-
ing confidence levels. The low F score is due to
135
 0.4
 0.5
 0.6
 0.7
 0.8
 1  10  100  1000
T
Unassisted Boosting
Categorized Groups
No Players
Enhanced Categorization
Figure 3: All F scores Results
? min max ?
Precision 0.0880 0.0496 0.1933 0.0223
Recall 0.7872 0.5417 1.0000 0.1096
F 0.1575 0.0924 0.3151 0.0361
Table 6: Categorized Tags Results
the very low Precision value. This method is ef-
fectively a direct application of B&L?s method to
our domain, however because of our strict accu-
racy measurement, it does not perform particularly
well. In fact it is even worse thanBmaj, our worst-
performing baseline. We believe this is because
the Majority Baseline is limited in the breadth of
tags returned, whereas this method returns very
large sets of over 200 tag attributes (due to the
many contributing tag attributes of each category)
while the average size of the training sets is 24.
Ideally we want to strike a balance between
the improved granularity of the Categorized Tags
evaluation (without the low accuracy) with the
excellent performance of the Categorized Groups
evaluation (without the too-broad basketing).
7.3 Unassisted Boosting
Our results are in Table 7 (row UB) and Figure 3.
We can see F values are increasing on the whole,
and that we have nearly reached our Probabilis-
tic Baseline. Inspecting the contents of the sets
returned by BoosTexter, we see they are slightly
more in line with a typical training set, but still suf-
fer from an over-emphasis on player names. We
also believe the high number of rounds required
for our best result (T = 2250) is caused by the
sparsity issue described in Section 4.2.
Best ? ?
Precision 0.4965 0.4730 0.0253
UB Recall 0.4961 0.4723 0.0252
F 0.4907 0.4673 0.0249
Precision 0.4128 0.3976 0.0094
NP Recall 0.4759 0.4633 0.0126
F 0.4367 0.4227 0.0091
Precision 0.4440 0.4318 0.0136
EC Recall 0.5127 0.4753 0.0271
F 0.4703 0.4467 0.0194
Table 7: Unassisted Boosting (UB), No Players
(NP) and Enhanced Categorization (EC). Best val-
ues for T = 2250, 250 and 20 respectively.
8 No-Players & Enhanced
Categorization
We now consider alternative, novel methods for
improving our results.
8.1 Player Exclusion
We have thus far ignored coherency in our data
? for example we want to make sure that player
statistics will be accompanied by their correspond-
ing player name.
One problem so far with our approach has been
that we are effectively double-counting the play-
ers. Our methods inspect which player names
should appear at the same time as finding ap-
propriate match statistics, whereas we believe we
should instead be finding relevant statistics in the
first instance, holding back player names, then in-
cluding only those players to whom the statistics
refer. Thus we restate our task in this way.
This is also sensible as in previous incarnations
the learning algorithm had been learning from the
literal strings of the player names. Although a
player could be more likely to be named for vari-
ous reasons, these reasons would not appear in the
scorecard and we feel the strings are best ignored.
Thus we decide to remove all player names
from the machine learning input, reinstating only
relevant ones once BoosTexter has selected its
chosen tag attributes.
8.2 Player Exclusion Results
As can be seen from Table 7 (row NP) and Figure
3, we have a maximum F value of 0.4367 when
T = 250, and have achieved a 3% absolute in-
crease, over ourBnops baseline, a static implemen-
tation of the above ideas.
136
8.3 Enhanced Categorization
Our final method combines the ideas of Section
8.1 above with the benefits of categorization, and
handles data sparsity issues.
The method is identical to that of Section 3.1,
with two important exceptions: The first is that
we reintroduce player names after the learning, as
above. The second is that instead of just a bi-
nary include/don?t-include decision for each tag
attribute, we offer a list of verbalised tag attributes
to the learner, but anonymising them with respect
to the group in which they appear. This enables
the learner to, given any group, predict which tag
attributes should be returned, independent of the
group in question. This means groups with often-
empty tag values are able to leverage the informa-
tion from groups with usually populated tag val-
ues, hence solving our data-sparsity issues. For
example, this will solve the issue, referenced in
Section 4.2 of a lack of training data for particular
player-combination partnerships.
Having held back the group to which the tag at-
tributes belong, we reintroduce them enabling dis-
covery of the original tag attribute. This offers the
benefits of categorization, but with a finer-grained
approach to the returned sets of tag attributes.
8.4 Enhanced Categorization Results
Our results are in Table 7 (row EC) and Figure
3. We achieved our best F score result of 0.4703
for a relatively low value of T = 20, and we can
clearly see that boosting establishes a reasonable
ruleset after a small number of iterations ? we be-
lieve we have resolved the issue of data sparsity.
The fact that this grouping has improved our re-
sults compared to feeding the information in ?flat?
(as in Section 7.3) emphasises that the construc-
tion and make-up of the categories play a key role
in defining performance.
9 Conclusions & Future Work
This paper has presented an exploration of various
methods which could prove useful when select-
ing content given a partially structured database
of statistics and output text to emulate. We be-
gan by acquiring the necessary domain data, in the
form of scorecards and reports, and employed a
six-step process to align scorecard statistics ver-
balised in the reports. We next categorised our
statistics based on the scorecard format. We es-
tablished three baselines ? one ?unthinking? proba-
bilistic baseline, a ?sensible? probabilistic one, and
another using categorization.
We found that unassisted boosting actually per-
formed worse than our comparable probabilistic
baseline, Bprob, but its output was marginally
more in line with the typical training data. We
explored how categorization affected our results,
and showed that by grouping similar sets of tag
attributes together we achieved a 7.4% improve-
ment over the comparable baseline value, Bnops
(Table 4). We further improved this technique in
a novel way by sharing structural information be-
tween learning instances, and by holding back cer-
tain information from the learner. Our final best F-
value marked a relative 15.7% increase on Bnops.
There are multiple avenues still available for ex-
ploration. One possibility would be to further in-
vestigate the effects of categorization from Section
3.2, for example by varying the size and number of
categories. We would also like to apply our meth-
ods to another domain (e.g. rugby games) to es-
tablish the relative generality of our approach.
Acknowledgments
This paper is based on Colin Kelly?s M.Phil. thesis, written
towards his completion of the University of Cambridge Com-
puter Laboratory?s Computer Speech, Text and Internet Tech-
nology course. Grateful thanks go to the EPSRC for funding.
References
Regina Barzilay and Mirella Lapata. 2005. Collective Con-
tent Selection for Concept-To-Text Generation. In HLT
?05, pages 331?338. Association for Computational Lin-
guistics.
Jose Coch. 1998. Multimeteo: multilingual production of
weather forecasts. ELRA Newsletter, 3(2).
Cricinfo. 2007. Wisden Almanack.
http://cricinfo.com/wisdenalmanack. Retrieved 28
April 2007. Registration required.
Pablo A. Duboue and Kathleen R. McKeown. 2003. Statis-
tical Acquisition of Content Selection Rules for Natural
Language Generation. EMNLP ?03, pages 121?128.
Ehud Reiter and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University Press.
Jacques Robin. 1995. Revision-based generation of natu-
ral language summaries providing historical background:
corpus-based analysis, design, implementation and evalu-
ation. Ph.D. thesis, Columbia University.
Robert E. Schapire and Yoram Singer. 2000. BoosTexter:
A boosting-based system for text categorization. Machine
Learning, 39(2/3):135?168.
Yiming Yang. 1999. An evaluation of statistical approaches
to text categorization. Information Retrieval, 1(1/2):69?
90.
137
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 61?69,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Acquiring Human-like Feature-Based
Conceptual Representations from Corpora
Colin Kelly
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
colin.kelly
@cl.cam.ac.uk
Barry Devereux
Centre for Speech,
Language, and the Brain
University of Cambridge
Cambridge, CB2 3EB, UK
barry@csl.psychol.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
anna.korhonen
@cl.cam.ac.uk
Abstract
The automatic acquisition of feature-based
conceptual representations from text corpora
can be challenging, given the unconstrained
nature of human-generated features. We
examine large-scale extraction of concept-
relation-feature triples and the utility of syn-
tactic, semantic, and encyclopedic informa-
tion in guiding this complex task. Meth-
ods traditionally employed do not investi-
gate the full range of triples occurring in
human-generated norms (e.g. flute produce
sound), rather targeting concept-feature pairs
(e.g. flute ? sound) or triples involving specific
relations (e.g. is-a, part-of ). We introduce
a novel method that extracts candidate triples
(e.g. deer have antlers, flute produce sound)
from parsed data and re-ranks them using se-
mantic information. We apply this technique
to Wikipedia and the British National Corpus
and assess its accuracy in a variety of ways.
Our work demonstrates the utility of external
knowledge in guiding feature extraction, and
suggests a number of avenues for future work.
1 Introduction
In the cognitive sciences, theories about how con-
crete concepts such as ELEPHANT are represented in
the mind have often adopted a distributed, feature-
based model of conceptual knowledge (e.g. Ran-
dall et al (2004), Tyler et al (2000)). According
to such accounts, conceptual representations consist
of patterns of activation over sets of interconnected
semantic feature nodes (e.g. has eyes, has ears,
is large). To test these theories empirically, cogni-
tive psychologists require an accurate estimate of the
kinds of knowledge that people are likely to repre-
sent in such a system. To date, the most important
sources of such knowledge are property-norming
studies, where a large number of participants write
down lists of features for concepts. For example,
McRae et al (2005) collected a set of norms list-
ing features for 541 concrete concepts. In that study,
the features listed by different participants were nor-
malised by mapping different feature descriptions
with identical meanings to the same feature label.1
Table 1 gives the ten most frequent normed features
for two concepts in the norms.
elephant banana
Relation Feature Relation Feature
is large is yellow
has a trunk is a fruit
is an animal is edible
is grey is soft
lives in Africa grows on trees
has ears eaten by peeling
has tusks - grows
has legs eaten by monkeys
has four legs is long
has large ears tastes good
Table 1: Sample triples from McRae Norms
However, property norm data have certain weak-
nesses (these have been widely discussed; e.g. Mur-
phy (2002), McRae et al (2005)). One issue is
that participants tend to under-report features that
are present in many of the concepts in a given cat-
egory (McRae et al, 2005; Murphy, 2002). For ex-
ample, for the concept ELEPHANT, participants list
salient features like has trunk, but not less salient
features such as breathes air, even though presum-
ably all McRae et al?s participants knew that ele-
phants breathe air. Although the largest collection
1For example, for CAR, ?used for transportation? and
?people use it for transportation? were mapped to the same
used for transportation feature.
61
of norms lists features for over 500 concepts, the
relatively small size of property norm sets still gives
cause for concern. Larger sets of norms would be
useful to psycholinguists; however, large-scale prop-
erty norming studies are time-consuming and costly.
In NLP, researchers have developed methods for
extracting and classifying generic relationships from
data, e.g. Pantel and Pennacchiotti (2008), Davidov
and Rappoport (2008a, 2008b). In recent years,
researchers have also begun to develop methods
which can automatically extract feature norm-like
representations from corpora, e.g. Almuhareb and
Poesio (2005), Barbu (2008), Baroni et al (2009).
The automatic approach is capable of gathering
large-scale distributional data, and furthermore it is
cost-effective. Corpora contain natural-language in-
stances of words denoting concepts and their fea-
tures, and therefore serve as ideal material for fea-
ture generation tasks. However, current methods
are restricted to specific relations between concepts
and their features, or target concept-feature pairs
only. For example, Almuhareb and Poesio (2005)
proposed a method based on manually developed
lexico-syntactic patterns that extracts information
about attributes and values of concepts. They used
these syntactic patterns and two grammatical rela-
tions to create descriptions of nouns consisting of
vector entries and evaluated their approach based
on how well their vector descriptions clustered con-
cepts. This method performed well, but targeted
is-a and part-of relations only. Barbu (2008) com-
bined manually defined linguistic patterns with a co-
occurrence based method to extract features involv-
ing six classes of relations. He then split learning
for the property classes into two distinct paradigms.
One used a pattern-based approach (four classes)
with a seeded pattern-learning algorithm. The other
measured strength of association between the con-
cept and referring adjectives and verbs (two classes).
His pattern-based approach worked well for proper-
ties in the superordinate class, had reasonable recall
for stuff and location classes, but zero recall for part
class. His approach for the other two classes used
various association measures which he summed to
establish an overall score for potential properties.
The recent Strudel model (Baroni et al, 2009) re-
lies on more general linguistic patterns, ?connector
patterns?, consisting of sequences of part-of-speech
(POS) tags to look for candidate feature terms near
a target concept. The method assumes that ?the va-
riety of patterns connecting a concept and a poten-
tial property is a good indicator of the presence of
a true semantic link?. Thus, properties are scored
based on the count of distinct patterns connecting
them to a concept. When evaluated against the ESS-
LLI dataset (Baroni et al (2008); see section 3.1),
Strudel yields a precision of 23.9% ? this figure is
the best state-of-the-art result for unconstrained ac-
quisition of concept-feature pairs.
It seems unlikely that further development of the
shallow connector patterns will significantly im-
prove accuracy, as these already broadly cover most
POS sequences that are concept-feature connectors.
Because of the difficult nature of the task, we believe
that extraction of more accurate representations ne-
cessitates additional linguistic and world knowl-
edge. Furthermore, the utility of Strudel is limited
because it only produces concept-feature pairs, and
not concept-relation-feature triples similar to those
in human generated norms (although the distribution
of the connector patterns for a extracted pair does of-
fer clues about the broad class of semantic relation
that holds between concept and feature).
In this paper, we explore issues of both method-
ology and evaluation that arise when attempting
unconstrained, large-scale extraction of concept-
relation-feature triples in corpus data. Extracting
such human-like features is difficult, and we do not
anticipate a high level of accuracy in these early ex-
periments. We examine the utility of three types
of external knowledge in guiding feature extrac-
tion: syntactic, semantic and encyclopedic. We
build three automatically parsed corpora, two from
Wikipedia and one from the British National Cor-
pus. We introduce a method that (i) extracts concept-
relation-feature triples from grammatical depen-
dency paths produced by a parser and (ii) uses prob-
abilistic information about semantic classes of fea-
tures and concepts to re-rank the candidate triples
before filtering them. We then assess the accuracy
of our model using several different methods, and
demonstrate that external knowledge can help guide
the extraction of human-like features. Finally, we
highlight issues in both methodology and evaluation
that are important for further progress in this area of
research.
62
2 Extraction Method
2.1 Corpora
We used Wikipedia to investigate the usefulness of
world knowledge for our task. Almost all con-
cepts in the McRae norms have their own Wikipedia
articles, and the articles often include facts simi-
lar to those elicited in norming studies.2 Extrane-
ous data were removed from the articles (e.g. in-
foboxes, bibliographies) to create a plaintext version
of each article. The 1.84 million articles were then
compiled into two subcorpora. The first of these
(Wiki500) consists of the Wikipedia articles corre-
sponding to each of the McRae concepts. It con-
tains c. 500 articles (1.1 million words). The sec-
ond subcorpus is comprised of those articles where
the title is fewer than five words long and contains
one of the McRae concept words.3 This corpus,
called Wiki110K, holds 109,648 plaintext articles
(36.5 million words).
We also employ the 100-million word British Na-
tional Corpus (BNC) (Leech et al, 1994) which con-
tains written (90%) and spoken (10%) English. It
was designed to represent a broad cross-section of
modern British English. This corpus provides an in-
teresting contrast with Wikipedia, since we assume
that any features contained in such a wide-ranging
corpus would be presented in an incidental fashion
rather than explicitly. The BNC may contain use-
ful features which are encoded in everyday speech
and text but not in Wikipedia, perhaps due to their
ambiguity for encyclopedic purposes, or due to their
non-scientific but rather common-sense nature. For
example, eaten by monkeys is listed as a feature of
BANANA in the McRae norms, but the word monkey
does not appear in the Wikipedia banana article.
2.2 Candidate feature extraction
Using a modified, British English version of the
published norms, we recoded them to a uniform
concept-relation-feature representation suitable for
our experiments ? it is triples of this form that we
aim to extract. Our method for extracting concept-
2e.g. The article Elephant describes how elephants are large,
are mammals, and live in Africa.
3This was done in order to avoid articles on very specific
topics which are unlikely to contain basic information about the
target concept.
relation-feature triples consists of two main stages.
In the first stage, we extract large sets of candidate
concept-relation-feature triples for each target con-
cept from parsed corpus data. In the second stage,
we re-rank and filter these triples with the intention
of retaining only those triples which are likely to be
true semantic features.
In the first stage, the corpora are parsed using the
Robust Accurate Statistical Parsing (RASP) system
(Briscoe et al, 2006). For each sentence in the cor-
pora, this yields the most probable analysis returned
by the parser in the form of a set of grammatical
relations (GRs). The GR sets for each sentence con-
taining the target concept noun are then retrieved
from the corpus. These GRs form an undirected
acyclic graph, whose nodes are labelled with words
in the sentence and their POS, and whose edges are
labelled with the GR types linking the nodes to-
gether. Using this graph we generate all possible
paths which are rooted at our target concept node
using a breadth-first search.
We then examine whether any of these paths
match prototypical feature-relation GR structures
according to our manually-generated rules. The
rules were created by first extracting features from
the McRae norms for a small subset of the concepts
and extracting those sentences from the Wiki500
corpus which contained both concept and feature
terms. For each sentence, we then examined each
path through the graph (containing the GRs and POS
tags) linking the concept, the feature, and all inter-
mediate terms, and (providing no other rule already
generated the concept-relation-feature triple) manu-
ally generated a rule based on each path.
For example, the sentence There are also aprons
that will cover the sleeves should yield the triple
apron cover sleeve. We examine the tree structure
of the sentence rooted at the concept (apron):
apron+s:17_NN2
cmod-that cover:34_VV0
L--- dobj sleeve+s:44_NN2
L--- det the:40_AT
L--- aux will:29_VM
cmod-that cover:34_VV0
xcomp be+:8_VBR
L--- ncmod also:12_RR
L--- ncsubj There:2_EX
Here, the relation is relatively simple ? we merely
63
create a rule which requires that the relation is a verb
(i.e. has a V POS tag), the feature has an NN tag and
that there is a dobj GR linking the feature to the
concept. Our rules are effectively a constraint on (a)
which paths should be followed through the tree, and
(b) which items in that path should be noted in our
concept-relation-feature triple. By creating several
such rules and applying them to a large number of
sentences, we extract potential features and relations
for our concepts.
We avoided specifying too many POS tags and
GRs in rules since this could have resulted in too
few matching paths. In the above example, we could
have required also a cmod-that relation linking the
feature and concept ? but this would have excluded
sentences like the apron covered the sleeves. Con-
versely, we avoided making our rules too permis-
sive. For example, eliminating the dobj requirement
would have yielded the triple apron be steel from the
sentence the apron hooks were steel.
The application of this method to a number of
concepts in the Wiki500 corpus yielded 15 rules
which we employed in our experiments. We extract
triples using both singular and plural occurrences of
both the concept term and the feature term. We show
the first three of our rules in Table 2. The first stage
of our method uses the 15 rules to extract a very
large number of candidate triples from corpus data.
Rule: relation of concept has a VVN tag, feature
has a NN tag and they are linked by an xcomp
GR
S: This is an anchor which relies solely on be-
ing a heavy weight.
T: anchor be weight
Rule: relation of concept is a verb, feature is an ad-
jective and they are linked by an xcomp GR
S: Sliced apples turn brown with exposure to
air due to the conversion of natural pheno-
lic substances into melanin upon exposure to
oxygen.
T: apple turn brown
Rule: feature of concept has a VV0 tag, relation is
a verb and they are linked by an aux GR
S: Grassy bottoms may be good holding, but
only if the anchor can penetrate the foliage.
T: anchor can penetrate
Table 2: Three sample rules for a given concept, with
example sentence (S) and corresponding triple (T).
2.3 Re-ranking based on semantic information
The second stage of our method evaluates the quality
of the extracted candidates using semantic informa-
tion, with the aim of filtering out the poor quality
features generated in the first stage. We would ex-
pect the number of times a triple is extracted for a
given concept to be proportional to the likelihood
that the triple represents a true feature of that con-
cept. However, production frequency alone is not a
sufficient indicator of quality, because concept terms
can produce unexpected candidate feature terms.4
One may attempt to address this issue by intro-
ducing semantic categories. In other words, the
probability of a feature being part of a concept?s
representation is dependent on the semantic cate-
gory to which the concept belongs (for example,
used for-cutting would be expected to have low
probability for animal concepts). We analysed the
norms to quantify this type of semantic information
with the aim of identifying higher-order structure in
the distribution of semantic classes for features and
concepts. The overarching goal was to determine
whether this information can indeed improve the ac-
curacy of feature extraction.
In formal terms, we assume that there is a 2-
dimensional probability distribution over concept
and feature classes, P(C,F), where C is a concept
class (e.g. Apparel) and F is a feature class (e.g.
Materials). Knowing this distribution provides us
with a means of assessing how likely it is that a can-
didate feature f is true for a concept c, assuming that
we know that c ? C and f ? F . The McRae norms
may be considered to be a sample drawn from this
distribution, if the concept and feature terms appear-
ing in the norms can be assigned to suitable concept
and feature classes. These classes were identified
by way of clustering. The reranking step employed
the McRae norms so we could establish an upper
bound for the semantic analysis, although we could
also use other knowledge resources, e.g. the Open
Mind Common Sense database (Singh et al, 2002).
2.3.1 Clustering
We utilised Lin?s similarity measure (1998) for
our similarity metric, employing WordNet (Fell-
4For example, one of the extracted triples for TIGER is tiger
have squadron because of the RAF squadron called the Tigers.
64
k-means
banjo biscuit blackbird
bat cup ox
beehive kettle peacock
birch sailboat prawn
bookcase shoe prune
NMF
ashtray bouquet eel
bayonet cabinet grapefruit
cape card guppy
cat cellar moose
catfish chandelier otter
Hierarchical
Fruit/Veg Apparel Instruments
apple apron accordion
avocado armour bagpipes
banana belt banjo
beehive blouse cello
blueberry boot clarinet
Table 3: First five elements alphabetically from three
sample clusters for the three clustering methods.
baum, 1998) as the basis for calculating similarity.
This metric is suitable for our task as we would
like to generate appropriate superordinate classes for
which we can calculate distributional statistics. We
could merely cluster on the most frequent sense of
concept and feature words in WordNet, but the most
frequent sense in WordNet may not correspond to
the intended sense in our feature norm data.5 So we
consider also other senses of words in WordNet by
employing a manually-annotated list to choose the
correct sense in WordNet. This is only possible for
concept clustering since we don?t possess a manual
WordNet sense annotation for the 7000 McRae fea-
tures; for the feature clustering, we simply use the
most frequent sense in WordNet.
The concepts and feature-head terms appearing
in the recoded norms were each clustered indepen-
dently into 50 clusters using three methods: hi-
erarchical clustering, k-means clustering and non-
negative matrix factorization (NMF). We show the
first five alphabetical elements from three of the
clusters produced by our clustering methods in Table
3. The hierarchical clustering seems to be producing
5e.g. the first and second most frequent definitions of kite
refer to a slang meaning for the word cheque ? only the third
most frequent meaning refers to kite as a toy, which most people
would understand to be its predominant sense.
Hierarchical Clustering
Plant Parts Materials Activities
berry cotton annoying
bush fibre listening
core nylon music
plant silk showing
seed spandex looking
Table 4: Example members of feature clusters for hierar-
chical clustering.
Fruit/Veg Apparel Instruments
Plant Parts 0.144 0.037 0.008
Materials 0.006 0.148 0.008
Activities 0.009 0.074 0.161
Table 5: P(F |C) for C ? {Fruit/Veg, Apparel, Instru-
ments} and F ? {Plant Parts, Materials, Activities}
the most intuitive clusters.
We calculated the conditional probability P(F |C)
of a feature cluster given a concept cluster using the
data in the McRae norms. Table 5 gives the condi-
tional probability for each of the three feature clus-
ters given each of the three concept clusters that
were presented in Tables 3 and 4 for hierarchical
clustering. For example, P(Materials|Apparel) is
higher than P(Materials|Fruit/Veg): given a concept
in the Apparel cluster the probability of a Materials
feature is relatively high whereas given a concept in
the Fruit/Veg cluster the probability of a Materials
feature is low. The cluster analysis therefore sup-
ports our hypothesis that the likelihood of a partic-
ular feature for a particular concept is dependent on
the semantic categories that both belong to.
2.3.2 Reranking
We investigated whether this distributional semantic
information could be used to improve the quality of
the candidate triples, by using the conditional prob-
abilities of the appropriate feature cluster given the
concept cluster as a weighting factor. To obtain the
probabilities for a triple, we first find the clusters that
the concept and feature-head words belong to. If the
feature-head word of the extracted triple appears in
the norms, its cluster membership is drawn directly
from there; if not, we assign the feature-head to the
feature cluster with which it has the highest average
similarity.6 Having determined the concept and fea-
6We use average-linkage for hiearchical and k-means clus-
tering, and mean cosine similarity for NMF.
65
ture clusters for the triple, we reweight its raw cor-
pus occurrence frequency by multiplying it by the
conditional probability. In this way, incorrect triples
that occur frequently in the data are downgraded and
more plausible triples have their ranking boosted.
2.3.3 Baseline model
We also implemented as a baseline a co-occurrence-
based model, based on the ?SVD? model de-
scribed by Baroni and colleagues (Baroni and Lenci,
2008; Baroni et al, 2009) ? it is a simple, word-
association method, not tailored to extracting fea-
tures. A context-word-by-target-word frequency co-
occurrence matrix was constructed for both corpora,
with a sentence-sized window. Context words and
target words were defined to be the 5,000 and 10,000
most frequent content words in the corpus respec-
tively. The target words were supplemented with
the concept words from the recoded norms. The
co-occurrence matrix was reduced to 150 dimen-
sions by singular value decomposition, and cosine
similarity between pairs of target words was calcu-
lated. The 200 most similar target words to each
concept acted as the feature-head terms extracted by
this model.
3 Experimental Evaluation
3.1 Methods of Evaluation
We considered a number of methods for evaluating
the quality of the extracted feature triples. One pos-
sibility would be to calculate precision and recall
for the extracted triples with respect to the McRae
norms ?gold standard?. However, direct comparison
with the recoded norms is problematic, since there
may be extracted features which are semantically
equivalent to a triple in the norms but possessing a
different lexical form.7
Since semantically identical features can be lex-
ically different, we followed the approach taken in
the ESSLLI 2008 Workshop on semantic models
(Baroni et al, 2008). The gold standard for the ESS-
LLI task was the top 10 features for 44 of the McRae
concepts. For each concept-feature pair an expan-
sion set was generated containing synonyms of the
7For example, avocado have stone appears in the recoded
norms whilst avocado contain pit is extracted by our method;
direct comparison of these two triples results in avocado con-
tain pit being incorrectly marked as an error.
feature terms appearing in the norms. For example,
the feature lives on water was expanded to the set
{aquatic, lake, ocean, river, sea, water}.
We would expect to find in corpus data correct
features that do not appear in our ?gold standard?
(e.g. breathes air is listed for WHALE but for no
other animal). We therefore aim to attain high re-
call when evaluating against the ESSLLI set (since
ideally all features in the norms should be extracted)
but we are somewhat less concerned about achieving
high precision (since extracted features that are not
in the norms may still be correct, e.g. breathes air
for TIGER). To evaluate the ability of our model
to generate such novel features, we also conducted
a manual evaluation of the highest-ranked extracted
features that did not appear in the norms.
Extraction set Corpus Prec. Recall
SVD Baseline
Wiki500 0.0235 0.4712
Wiki110K 0.0140 0.2798
BNC 0.0131 0.2621
Method -
unfiltered
Wiki500 0.0242 0.6515
Wiki110K 0.0039 0.8944
BNC 0.0042 0.8813
Method - top 20
(unweighted)
Wiki500 0.1159 0.2326
Wiki110K 0.0761 0.1523
BNC 0.0841 0.1692
Method - top 20
(hierarchical
clustering)
Wiki500 0.1693 0.3394
Wiki110K 0.1733 0.3553
BNC 0.1943 0.3896
Method - top 20
(k-means
clustering)
Wiki500 0.1159 0.2323
Wiki110K 0.1000 0.2008
BNC 0.1216 0.2442
Method - top 20
(NMF
clustering)
Wiki500 0.1375 0.2755
Wiki110K 0.1409 0.2826
BNC 0.1500 0.3010
Table 6: Results when matching on features only.
3.2 Evaluation
Previous large-scale models of feature extraction
have been evaluated on pairs rather than triples e.g.
Baroni et al (2009). Table 6 presents the results
of our method when we evaluate using the feature-
head term alone (i.e. in calculating precision and re-
call we disregard the relation verb and require only
a match between the feature-head terms in the ex-
tracted triples and the recoded norms). Results for
six sets of extractions are presented. The first set
is the set of features extracted by the SVD baseline.
66
The second set of extracted triples consists of the
full set of triples extracted by our method, prior to
the reweighting stage. ?Top 20 unweighted? gives
the results when all but the top 20 most frequently
extracted triples for each concept are filtered out.
Note that the filtering criteria here is raw extraction
frequency, without reweighting by conditional prob-
abilities. ?Top 20 (clustering type)? are the corre-
sponding results when the features are weighted by
the conditional probability factors (derived from our
three clustering methods) prior to filtering; that is,
using the top 20 reranked features. The effective-
ness of using the semantic class-based analysis data
in our method can be assessed by comparing the fil-
tered results with and without feature weighting.
For the baseline implementation, the results are
better when we use the smaller Wiki500 corpus
compared to the larger Wiki110K corpus. This is
not surprising, since the smaller corpus contains
only those articles which correspond to the concepts
found in the norms. This smaller corpus thus min-
imises noise due to phenomena such as word poly-
semy which are more apparent in the larger corpus.
The results for the baseline model and the unfil-
tered method are quite similar for the Wiki500 cor-
pus, whilst the results for the unfiltered method us-
ing the Wiki110K corpus give the maximum recall
achieved by our method; 89.4% of the features are
extracted, although this figure is closely followed by
that of the BNC at 88.1%. As the unfiltered method
is deliberately greedy, a large number of features are
being extracted and therefore precision is low.
Extraction set Corpus Prec. Recall
Method - top 20
(hierarchical
clustering)
Wiki500 0.1011 0.2028
Wiki110K 0.1102 0.2210
BNC 0.0955 0.1917
Table 7: Results for our best method when matching on
features and relations.
For the results of the filtered method, where all
but the top 20 of features were discarded, we see the
benefit of reranking, with the reranked frequencies
for all three clustering types yielding much higher
precision and recall scores than the unweighted
method. Our best performance is achieved using the
BNC and hierarchical clustering, where we obtain
19.4% precision and 38.9% recall. Thus both gen-
eral and encyclopedic corpus data prove useful for
the task. An interesting question is whether these
two data types offer different, complementary fea-
ture types for the task. We discuss this point further
in section 3.3.
Using exactly the same gold standard, Baroni et
al. (2009) obtained precision of 23.9%. However,
this result is not directly comparable with ours, since
we define precision over the whole set of extracted
features while Baroni et al considered the top 10
extracted features only.
The innovation of our method is that it uses infor-
mation about the GR-graph of the sentence to also
extract the relation which appears in the path link-
ing the concept and feature terms in the sentence,
which is not possible in a purely co-occurrence-
based model. We therefore also evaluated the ex-
tracted triples using the full relation + feature-head
pair (i.e. both the feature and the relation verb have
to be correct). The results for our best method are
shown in Table 7. Unsurprisingly, because this task
is more difficult, precision and recall are reduced.
However, since we enforce no constraints on what
the relation may be and since we do not have ex-
panded synonym sets for our relations (as we do for
our features) it is actually impressive to have both
the exact relation verb and feature matching with the
recoded norms almost one in every five times. To our
knowledge, our work is the first to try to compare ex-
tracted features to the full relation and feature norm
parts of the triple.
3.3 Qualitative analysis
Since a key aim of our work is to learn novel features
in corpus data, we also performed a qualitative eval-
uation of the extracted features and relations. This
analysis revealed that many of the errors were not
true errors but potentially valid triples missing from
the gold standard. Table 8 shows the top 10 features
for two concepts extracted by our best method from
the Wiki500 corpus and the BNC corpus. We la-
bel those features that are correct according to the
norms as Correct (C), those which do not appear in
our norms but we believe to be plausible as Plausi-
ble (P), and those that do not appear in the norms
and are also implausible as Incorrect (I). We can see
that our method has detected several plausible fea-
tures not appearing in the norms (and thus our gold
standard), e.g. swan have chick and screwdriver be
67
swan
Wiki500 BNC
be bird C have number I
be black P have water C
have chick P have lake C
have plumage C be bird C
have feather C be white C
restrict water C have neck C
be mute P be wild P
eat grass P have duck I
turn elisa I have song I
have neck C have pair I
screwdriver
Wiki500 BNC
use handle C have tool C
have blade P have end P
use tool C have blade P
remedy problem P have hand I
have size P be sharp P
have head C have bit P
rotate end P have arm I
have plastic P be large P
achieve goal I be sonic P
have hand I have range P
Table 8: Top 10 returned features and relations for swan
and screwdriver.
sharp. Indeed, it could be argued that some ?incor-
rect? features (e.g. screwdriver achieve goal) could
be considered to be at least broadly accurate. We
recognise that the ideal evaluation for our method
would involve having human participants assess the
extracted features for a diverse cross-section of our
concepts, but this is beyond the scope of this paper.
When considering the top 20 features extracted
using our best method applied to the Wiki500 cor-
pus versus the BNC corpus, the overlap of features
is relatively low at 22.73%. When one also takes the
extracted relations into account, this figure descends
to 6.45%. It is clear that relatively distinct groups of
features are being extracted from the encyclopedic
and general corpus data. Future work could investi-
gate combining these for improved performance e.g.
using the intersection of the best features from the
BNC and Wiki110k corpora to improve precision
and the union to improve recall.
4 Discussion
This paper examined large-scale, unconstrained ac-
quisition of human-like feature norms from corpus
data. Our work was not limited to only a subset
of concepts, relation types or concept-feature pairs.
Rather, we investigated concepts, features and rela-
tions in conjunction, and extracted property norm-
like concept-relation-feature triples.
Our investigation shows that external knowledge
is highly useful in guiding this challenging task. En-
cyclopedic information proved useful for feature ex-
traction: although our Wikipedia corpora are consid-
erably smaller than the BNC, they performed almost
equally well. We also demonstrated the benefits of
employing syntactic information in feature extrac-
tion: our base extraction method operating on parsed
data outperforms the co-occurrence-based baseline
and permits us to extract relation verbs. This un-
derscores the usefulness of parsing for semantically
meaningful feature extraction. This is consistent
with recent work in the field of computational lex-
ical semantics, although GR data has not previously
been successfully applied to feature extraction.
We showed that semantic information about co-
occurring concept and feature clusters can be used
to enhance feature acquisition. We employed the
McRae norms for our analysis, however we could
also employ other knowledge resources and cluster
relation verbs using recent methods, e.g. Sun and
Korhonen (2009), Vlachos et al (2009).
Our paper has also investigated methods of eval-
uation, which is a critical but difficult issue for fea-
ture extraction. Most recent approaches have been
evaluated against the ESSLLI sub-set of the McRae
norms which expands the set of features in the norms
with their synonyms. Yet even expansion sets like
the ESSLLI norms do not facilitate adequate eval-
uation because they are not complete in the sense
that there are true features which are not included
in the norms. Our qualitative analysis shows that
many of the errors against the recoded norms are
in fact correct or plausible features. Future work
can aim for larger-scale qualitative evaluation using
multiple judges as well as investigating other task-
based evaluations. For example, we have demon-
strated that our automatically-acquired feature rep-
resentations can make predictions about fMRI activ-
ity associated with concept stimuli that are as pow-
erful as those produced by a manually-selected set
of features (Devereux et al, 2010).
68
Acknowledgments
This research was supported by EPSRC grant
EP/F030061/1 and the Royal Society University
Research Fellowship, UK. We are grateful to McRae and
colleagues for making their norms publicly available,
and to the anonymous reviewers for their input.
References
Abdulrahman Almuhareb and Massimo Poesio. 2005.
Concept learning and categorization from the web. In
Proceedings of the 27th Annual Meeting of the Cogni-
tive Science Society, pages 103?108.
Eduard Barbu. 2008. Combining methods to learn
feature-norm-like concept descriptions. In Proceed-
ings of the ESSLLI Workshop on Distributional Lexical
Semantics, pages 9?16.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of Lin-
guistics, 20(1):55?88.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. ESSLLI 2008 Workshop on Distributional
Lexical Semantics.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2009. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, pages 1?33.
Edward J. Briscoe, John Carroll, and Rebecca Wat-
son. 2006. The second release of the RASP sys-
tem. In Proceedings of the Interactive Demo Session
of COLING/ACL-06, pages 77?80.
D. Davidov and A. Rappoport. 2008a. Classification of
semantic relationships between nominals using pattern
clusters. ACL.08.
D. Davidov and A. Rappoport. 2008b. Unsupervised
discovery of generic relationships using pattern clus-
ters and its evaluation by automatically generated SAT
analogy questions. ACL.08.
Barry Devereux, Colin Kelly, and Anna Korhonen. 2010.
Using fmri activation to conceptual stimuli to evalu-
ate methods for extracting conceptual representations
from corpora. In Proceedings of the NAACL-HLT
Workshop on Computational Neurolinguistics.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
G. Leech, R. Garside, and M. Bryant. 1994. CLAWS4:
the tagging of the British National Corpus. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 1, pages 622?628.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML?98, pages 296?
304.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37:547?559.
Gregory Murphy. 2002. The big book of concepts. The
MIT Press, Cambridge, MA.
Patrick Pantel and Marco Pennacchiotti. 2008. Automat-
ically harvesting and ontologizing semantic relations.
In Paul Buitelaar and Philipp Cimiano, editors, Ontol-
ogy learning and population. IOS press.
Billi Randall, Helen E. Moss, Jennifer M. Rodd, Mike
Greer, and Lorraine K. Tyler. 2004. Distinctive-
ness and correlation in conceptual structure: Behav-
ioral and computational studies. Journal of Experi-
mental Psychology: Learning, Memory & Cognition,
30(2):393?406.
P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins,
and W. Li Zhu. 2002. Open Mind Common
Sense: Knowledge acquisition from the general pub-
lic. On the Move to Meaningful Internet Systems 2002:
CoopIS, DOA, and ODBASE, pages 1223?1237.
Lin Sun and Anna Korhonen. 2009. Improving Verb
Clustering with Automatically Acquired Selectional
Preferences. Empirical Methods on Natural Language
Processing.
L. K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained dirichlet
process mixture models for verb clustering. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics, pages 74?82, Athens,
Greece.
69
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 70?78,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using fMRI activation to conceptual stimuli to evaluate methods for
extracting conceptual representations from corpora
Barry Devereux
Centre for Speech, Language and the Brain
Department of Experimental Psychology
University of Cambridge
barry@csl.psychol.cam.ac.uk
Colin Kelly & Anna Korhonen
Computer Laboratory
University of Cambridge
{ck329,alk23}@cam.ac.uk
Abstract
We present a series of methods for deriv-
ing conceptual representations from corpora
and investigate the usefulness of the fMRI
data and machine learning methodology of
Mitchell et al (2008) as a basis for evaluat-
ing the different models. Within this frame-
work, the quality of a semantic model is quan-
tified by its ability to predict the fMRI ac-
tivation associated with conceptual stimuli.
Mitchell et al used a manually-acquired set of
verbs as the basis for their semantic model; in
this paper, we also consider automatically ac-
quired feature-norm-like semantic representa-
tions. These models make different assump-
tions about the kinds of information avail-
able in corpora that is relevant to represent-
ing conceptual knowledge. Our results in-
dicate that automatically-acquired representa-
tions can make equally powerful predictions
about the brain activity associated with the
stimuli.
1 Introduction
Mitchell et al (2008) presented a novel approach for
predicting human brain activity associated with con-
ceptual stimuli. This approach represents a useful
development for interdisciplinary researchers inter-
ested in lexical semantics, for several reasons. Most
broadly, it is useful in testing the hypothesis that
distributional properties of words in corpora can re-
veal important information about the meanings of
words. A strong version of this hypothesis (i.e. that
children in part learn the meaning of concrete con-
cept words from co-occurring words in discourse
that they are exposed to) has formed the basis of
one class of probabilistic cognitive models of con-
ceptual representation (Andrews et al, 2005; An-
drews et al, 2009; Steyvers, 2010). Furthermore
this approach is useful for testing hypotheses about
the kind of co-occurring information that is useful
for representing conceptual semantics. In Mitchell
et al?s work (2008), for example, they adopt the po-
sition that the meaning of concrete concepts is en-
coded in the brain with information associated with
basic sensory and motor activities (such as actions
involving changes to spatial relationships and phys-
ical actions performed on objects).
At a more technical level, Mitchell et al?s fMRI
activation data1 give researchers developing feature-
based models of conceptual representation an im-
portant benchmark for evaluation. For these re-
searchers, a key problem is the lack of a reason-
able ?gold standard? against which the quality of the
representations generated by a computational model
may be evaluated. Previous research has adopted
two main approaches to evaluation. Firstly, some
models ? especially those aiming to extract repre-
sentations composed of psychologically meaningful
semantic feature units, such as Baroni et al (2009)
? have been evaluated against features gathered in
large scale property norming studies (e.g. McRae
et al (2005)).2 By comparing the system output
against features elicited by people, this kind of eval-
1fMRI data measures changes in oxygen concentrations in
the brain. These changes are tied to cognitive processes.
2In property norming studies, a group of human subjects are
asked to cite features which come to mind for a given concept.
These features are compiled by frequency (with a minimum fre-
quency cut-off) to generate a list of features for each concept.
70
uation aims to test the psychological validity of com-
putational methods. Furthermore, it allows a fine-
grained analysis of performance, for example by re-
vealing the classes of features (part-of, taxonomic,
etc) which a given model is particularly good at ex-
tracting (Baroni et al, 2008).
However, property norms come with important
caveats. One problem is that they tend to over-
represent informative or salient information about
concepts whilst under-representing other kinds of
features. For example, participants report that
camels have humps, but not that camels have hearts,
even though all participants are likely to have both
pieces of information accessible in their representa-
tion of the concept CAMEL. If a model is successful
in extracting these less salient features, there is no
way of evaluating their correctness using property
norms. A related issue is that participants can only
report verbalizable features, which may not repre-
sent the total sum of their conceptual knowledge
(Murphy, 2002; McRae et al, 2005).
A second problem with using property norms as
the basis of evaluation is that there is often no direct
lexical match between feature terms appearing in the
system output and the norms. Feature norms are typ-
ically normalized such that near-synonymous prop-
erties (e.g. is endangered, is an endangered species,
is almost extinct, etc., for WHALE) given by differ-
ent participants are mapped to the same feature la-
bel (e.g. is endangered). As a consequence, a model
may correctly extract endangered for WHALE, but
other lexical forms of the same feature will not
match any feature in the norms. One solution to this
is to create an expansion set for each feature which
includes its synonyms (Baroni et al, 2008). How-
ever, this is only a partial solution because lexical
variation in features is not limited to synonyms.
A second approach to evaluating semantic mod-
els uses classification or similarity data. For exam-
ple, Andrews et al (2009) evaluated their models by
calculating cosine similarity scores between seman-
tic representations and using these similarity scores
to predict behavioral data which are contingent on
the semantic similarity between pairs of concepts
(e.g. lexical substitution errors, semantic priming
latencies, word-association norms, etc). Although
this approach is psychologically motivated, it evalu-
ates a set of extracted features more indirectly than
comparison with norm data. In computational lin-
guistics, a similarly indirect evaluation method is to
cluster the extracted representations. This approach
avoids the difficulties in evaluating individual fea-
tures; however it only allows consideration along
one dimension of the data, namely the similarity be-
tween pairs of concepts.
fMRI data such as the Mitchell et al (2008)
dataset offers an advancement over both of these
evaluation techniques. Unlike, for example, prop-
erty norming data, fMRI data offers direct insight
into how the brain is functioning in response to given
stimuli. Its multidimensional nature makes it eas-
ier to inspect what aspects of meaning a particular
model is performing strongly or weakly on, and al-
lows for better control of experimental variation. Fi-
nally, it avoids the two major issues associated with
property norms, which we outlined above.
This paper is structured as follows. In the next
section, we briefly describe the models which we
used to extract conceptual representations for the 60
concepts in the Mitchell et al (2008) dataset. In
Section 3, we outline our experimental objectives,
and the framework we adopt for testing our seman-
tic models. In Section 4, we present the results of
our evaluation, which indicate above chance perfor-
mance for each of the models. Finally, we exam-
ine the differences between models by investigating
for which concepts prediction of the fMRI activity is
poorest, and discuss these differences with respect to
the differing assumptions made by the methods.
2 Semantic models
We consider four different semantic models in this
paper, which are described briefly below. These
models were selected as we were interested in the
various kinds of knowledge (part-of-speech, syntac-
tic, and semantic) in corpora available to the extrac-
tion process, and the extent to which the use of these
types of knowledge can affect the quality of the ex-
tracted conceptual representations.
2.1 Mitchell verb-based semantic model
The first semantic model we considered was that
of Mitchell et al (2008). This model assumes that
sensory-motor information is an important aspect of
conceptual representation, and that the information
71
relevant to a target concept?s representation can be
estimated from the concept word?s frequency of co-
occurrence with 25 sensory-motor verbs (eat, ma-
nipulate, push, etc) in a very large corpus. Our reim-
plementation of this method used the co-occurrence
statistics provided by Mitchell et al3 which were
extracted from the Google n-gram corpus consisting
of 1 trillion words of web text.
2.2 SVD model
Secondly, we implemented a co-occurrence-based
Singular Value Decomposition (SVD) model based
on the one described by Baroni and colleagues (Ba-
roni and Lenci, 2008; Baroni et al, 2009). This
model combines aspects of both the HAL (Landauer
et al, 1998) and LSA (Lund and Burgess, 1996)
models in constructing representations for words
based on their co-occurrences in texts. A word-
by-word co-occurrence matrix was constructed for
our corpus, storing how often each target word co-
occurred with each context word. The set of context
words consisted of the 5,000 most frequent content
words (i.e. words not occurring in a stop-list of func-
tion words) appearing in the corpus. The set of target
words consisted of the 60 concept terms appearing
in the fMRI dataset, supplemented with the 10,000
most frequent content words in the corpus (with the
exception of the top 10 most frequent words). For
calculating co-occurrence frequency between target
and context words, the context window was defined
by sentence boundaries: two words were considered
to co-occur if they appeared in the same sentence4.
Following Baroni and Lenci (2008), the dimen-
sionality of the target-word ? context-word co-
occurrence matrix was reduced to 150 columns by
singular value decomposition. That is, the singu-
lar value decomposition of the co-occurrence matrix
was computed and the 150 left singular vectors that
accounted for most of the variance, multiplied by the
corresponding singular values, were used as the 150-
dimensional representation of each target term. Sim-
3http://www.cs.cmu.edu/?tom/science2008/
semanticFeatureVectors.html
4In Baroni et al?s implementation a context window of 5
(Baroni and Lenci, 2008) or 20 (Baroni et al, 2009) words
either side of the target word was used instead; we chose a
sentence-based context window as it is analogous to the context
used in our experimental method (described in the following
section).
ilarity between pairs of target words was calculated
as the cosine between their vectors, and for each of
the 60 concept words in the experimental stimuli we
chose the 200 most similar target words to act as the
feature terms extracted by the model. The corpus
used with this model was the British National Cor-
pus (BNC) (Leech et al, 1994).
2.3 Novel extraction method
Finally we implemented a novel extraction method,
which aims to extract property-norm-like, psy-
chologically meaningful features from corpus data
(Kelly et al, 2010). The method aims to extract se-
mantically unconstrained feature triples of the form
concept-relation-feature , where feature is a feature
(either noun or adjective) of the target concept and
relation is a verb representing the semantic relation-
ship between them. Examples of extracted triples
include: swan be white, swan have neck and screw-
driver be tool. The model uses a corpus parsed for
grammatical relations (GRs) using Robust Accurate
Statistical Parsing (RASP) (Briscoe et al, 2006).
For each sentence containing a target concept, the
set of GRs for that sentence are examined to test
whether they match manually-created rules. These
rules include prototypical feature-relation GR struc-
tures connecting elements of the sentence and rep-
resent dependency patterns which encode potential
semantic relationships between the concept and can-
didate feature terms occurring in the sentence. A
large set of candidate triples are extracted by ap-
plying these rules to each sentence in the corpus
containing a target concept, and the triples for each
concept are ranked by their frequency of extraction.
In the second stage of the method, the extracted
triples are reweighted on the basis of probabilistic
high-level semantic information obtained from hu-
man property norm data. This subsequent stage has
the effect of increasing the weight associated with
more high-quality features and downgrading lower-
quality features. The extraction method is described
more fully in Kelly et al (2010). For this method
we also used the BNC. The top 200 triples ranked
by frequency (i.e. unweighted) and the top 200 fea-
tures after reweighting with the semantic data were
used in our experiments.
72
3 Experiment
As mentioned above, we are primarily interested in
using the fMRI data to evaluate the quality of the
different methods for extracting conceptual repre-
sentations from corpora (rather than being interested
in investigating methods for predicting fMRI activa-
tion). We make no attempt to build on the method
described by Mitchell et al (2008), although there
are likely to be many interesting avenues through
which that method could be extended.5 We therefore
followed the Mitchell et al methodology as closely
as possible, using the same multiple regression train-
ing and leave-two-out cross-validation paradigms as
presented in their paper and supporting online ma-
terial. The only parameter that we varied was the
extraction method (and corpus) that was used to gen-
erate the feature-vectors associated with the 60 con-
cepts that were used during the training phase. The
quality of the predictions generated for the concepts
using each semantic model can therefore be adopted
as an index of model performance.
The Mitchell et al method uses co-occurrence
with a specific set of 25 manually selected verbs
(eat, push, etc) that are the same for each concept.
This results in 25-dimensional feature vectors for in-
put into training. However, for both the SVD model
and our triple extraction models there are no a pri-
ori constraints on the number of unique features that
can be extracted for the concepts. For these mod-
els, we selected the top 200 features associated with
each concept; therefore, across all 60 concepts in the
Mitchell et al dataset, there are thousands of unique
features extracted which are used in the concepts?
representations. To ensure that the linear regres-
sion model for each method would be fitted using
the same number of free parameters during training
(thereby maximizing the comparability of the dif-
ferent methods), we reduced the dimensionality of
the generated feature spaces for the SVD method
and the two triple-extraction methods using Prin-
cipal Components Analysis (PCA). The concept ?
feature extraction frequency matrices for the three
models were submitted to PCA, and the first 25 com-
ponents (i.e. those components which best charac-
5For example, the method currently makes the simplifying
assumption that the activity in neighbouring voxels is indepen-
dent.
Triples (weighted) SVD
PCA1 PCA2 PCA1 PCA2
Highest-valued concepts
horse house coat butterfly
cat apartment skirt cow
cow dog shirt ant
dog igloo pants bee
beetle car dress lettuce
Lowest-valued concepts
knife pants car desk
door coat watch arm
hammer dress horse chair
saw skirt dog knife
chisel shirt fly leg
Table 1: Highest- and lowest-valued concepts for the
first two components for the SVD and weighted triple-
extraction methods.
terized the variance of the original features) for each
model were selected. In the case of the SVD model,
these 25 dimensions explained 77.7% of the vari-
ance in the original 3,061-dimensional vectors. For
our unweighted extraction method, the 25 extracted
components explained 63.0% of the original 5,525
dimensions; for the weighted method the compo-
nents explained 71.5% of the original 6,567 dimen-
sions.
It is interesting to consider the kind of seman-
tic information that is being captured by the resul-
tant PCA components. In particular, the compo-
nents appear to capture meaningful distinctions be-
tween stimuli. For example, the first PCA com-
ponent for our weighted triple extraction method
can be interpreted as the concepts? degree of ?an-
imalness? (animal stimuli have high values on this
component). Table 1 presents the five highest and
lowest-valued concepts for the first two components
for the SVD model and the weighted triple extrac-
tion model. Concepts which overlap with respect
to a specific set of semantic properties tend to have
high or low values on a given dimension, indicating
that that component is capturing a specific cluster of
co-occurring semantic features. For example, PCA1
for SVD can be interpreted as ?has features associ-
ated with clothing?.
Therefore, a key difference between the Michell
73
Method Feature Type POS Syntax Semantics
Mitchell 25 verbs no no no
SVD tuples (content-words) yes no no
triple-extraction method (unweighted) feature-triples yes yes no
triple-extraction method (weighted) feature-triples yes yes yes
Table 2: Comparison of the information available to each model.
et al model and our models is that while Mitchell
et al posit that certain sensory-motor function verbs
can act as important features of concepts, our models
instead place more importance on intrinsic semantic
features.
Finally, Table 2 gives a summary comparison of
the different models, in terms of whether or not each
uses part of speech (POS) data, syntactic informa-
tion (i.e. GRs), and semantic filtering (Section 2.3).
It should be noted that the BNC corpus (used with
the SVD model and our triple-extraction method) is
10,000 times smaller than the corpus from which
the Mitchell et al feature vectors are derived. As
such the semantic representations we extract with
our method need to make better use of the data avail-
able in the corpus if they are to compete with the
verb-based features used by Mitchell et al?s method.
4 Results
The accuracy for each of the four methods was eval-
uated using a leave-two-out validation paradigm.
There are 1,770 possible pairs of concepts that can
be drawn from the set of 60 concept stimuli. Train-
ing was performed separately for each participant
and for each of the 1,770 held-out pairs. Given
a particular participant and held-out pair, for each
voxel v we fit the activation at that voxel to the set
of 58 training items with multiple linear regression,
using as predictor variables the elements of the 25-
dimensional feature vectors associated with each of
the 58 concepts. Training therefore yields a set of
25 ?-coefficients, which can be used to generate a
prediction for the activation yv of voxel v for the
held-out word w using the equation
ypredv =
25?
i=1
?v,ifi,w (1)
where fi,w is the ith element of the feature vector for
word w (see Mitchell et al (2008) for details). Over
all voxels, this method gives a prediction for the ac-
tivation with respect to the held-out word w which
can then be compared to the observed activation for
that stimulus.
Rather than comparing the activity between pre-
dicted and observed images using all voxels, we
compared images using only the 500 most stable
voxels for each participant. For each participant, the
500 most stable voxels were the voxels which gave
the most consistent pattern of activation across the
six presentations of all 60 stimuli (see Mitchell et al
(2008) for details).
The top row of Figure 1 presents the learned co-
efficients for one feature dimension for each of the
four semantic models considered in our experiments
(for these images, all voxels rather then the 500
most stable voxels are used). For the Mitchell et al
method, the coefficients presented correspond to the
verb eat; for the other models the feature is the PCA
component that explained the most variance in the
original representations. We also present the pre-
dicted images for the concepts CELERY and AIR-
PLANE, calculated on the coefficients learned over
the remaining 58 concepts. Importantly, for the
Mitchell et al method (column (a)), the learned co-
efficients for eat and the predicted images for CEL-
ERY and AIRPLANE agree with those reported by
Mitchell et al (2008, Figure 2 & online supplemen-
tary material6).
We calculated similarity between predicted and
observed images using both cosine and Pearson cor-
relation and the 500 most stable voxels; we report
the results using Pearson correlation here as this
measure consistently gave slightly better accuracies
6http://www.cs.cmu.edu/?tom/science2008/
featureSignaturesP1.html
74
(a) ?eat? (b) PCA1/clothes (c) PCA1/clothes (d) PCA1/animals
(a) celery (b) celery (c) celery (d) celery
(a) airplane (b) airplane (c) airplane (d) airplane
Figure 1: Learned coefficients on a selected feature dimension (top row) and predicted activation for CELERY (middle
row) and AIRPLANE (bottom row) for four semantic models: (a) Mitchell et al (2008), (b) SVD (c) triple extraction
method (unweighted), and (d) triple extraction method (weighted). Warmer colours indicate higher values (i.e. larger
?-coefficients for the feature dimensions and higher predicted activation for the concepts). PCA components have been
given intuitive labels indicating the kind of information described by that component (see Table 1). As in Figure 2 of
Mitchell et al (2008), the figure shows just one slice in the horizontal plane (z = -12 in MNI space) for one participant
(P1). The predicted images for CELERY and AIRPLANE were generated from the feature coefficients learned on the
other 58 concepts using each of the four models; the corresponding observed images for CELERY and AIRPLANE can
be found in Mitchell et al (2008) Figure 2 B.
75
Method P1 P2 P3 P4 P5 P6 P7 P8 P9 Mean
Mitchell et al (2008) 0.84 0.83 0.76 0.81 0.79 0.66 0.73 0.64 0.68 0.75
SVD 0.82 0.67 0.79 0.83 0.74 0.64 0.64 0.70 0.75 0.73
Triple-extraction (unweighted) 0.82 0.71 0.79 0.80 0.70 0.69 0.65 0.53 0.78 0.72
Triple-extraction (weighted) 0.82 0.72 0.76 0.83 0.73 0.65 0.68 0.51 0.76 0.72
Table 3: Accuracy results for the four semantic models.
for each of the four models (the results are very simi-
lar using the cosine measure). Following Mitchell et
al. (2008; supplementary material), a match score
for each held out pair w1 and w2 was calculated
as the sum of the similarities between the correctly
aligned predicted and observed images:
a = sim(wpred1 , w
obs
1 ) + sim(w
pred
2 , w
obs
2 ) (2)
Similarly a mismatch score was calculated as
b = sim(wpred1 , w
obs
2 ) + sim(w
pred
2 , w
obs
1 ) (3)
Cases where the match score is greater than the mis-
match score (i.e. a > b) count as successes for the
model (i.e. the model correctly identifies the two
predicted images). Otherwise there is a failure by
the model (i.e. the model identifies the observed im-
age for w1 as being w2 and vice-versa).
Table 3 presents the results of the leave-two-
out cross-validation evaluation, giving the propor-
tion (across all 1,770 pairs) of predicted images for
the held-out pairs that were correctly matched to
the observed images.7 The original Mitchell et al
(2008) model has the best mean performance, al-
though across the nine participants, there is no sig-
nificant difference in accuracy between any of the
models (|t(8)| < 1.49, p > 0.17, for all pairwise
paired t-tests between Mitchell et al (2008), SVD,
and weighted triple extraction).
That there is no difference between the perfor-
mance of the Mitchell et al (2008), SVD and triple
7Our results for the Mitchell et al (2008) method are simi-
lar, though not identical, to those reported in that paper (where
the reported mean accuracy across all participants is 0.77, using
cosine similarity). Our implementation of the method for select-
ing the 500 most stable voxels yields slightly different voxels
from those obtained by Mitchell et al (2008; see supplemen-
tary material). In any case, the same set of 500 voxels for each
participant were used for generating the results of each model
presented here, and so we do not believe that this discrepancy
affects comparison of the different models.
extraction methods is surprising, given the different
kinds of information that are available to the dif-
ferent models. In particular, the models that auto-
matically acquire very general and semantically un-
constrained feature-based representations perform
as well as the model which uses a set of manually-
selected sensory-motor verbs, even though the rep-
resentations generated for these models are derived
from 10,000 times less corpus data.
As mentioned in our introduction, an advantage of
evaluating against the fMRI dataset is that this multi-
dimensional data allows us to investigate strengths
and weaknesses of different models in a way which
is not possible using similarity or clustering-based
evaluation. As a very simple investigation of spe-
cific differences in model performance, we present
in Table 4 the pairs of concepts for which each of the
models performs most poorly on. The Mitchell et al
(2008) method appears to do poorly on pairs of con-
cepts where a constituent word can be ambiguous
with respect to its part-of-speech (e.g. SAW, BEAR).
This is not surprising, given that part-of-speech data
is not available in the Google n-gram corpus used
with this method. The performance of the Mitchell
et al method might therefore be improved signifi-
cantly by applying heuristics to the n-gram data to
make inferences about the correct part-of-speech of
instances of words like SAW and BEAR. For the SVD
and weighted triple extraction methods, which both
use the BNC corpus, there is some evidence that
the models are performing poorly for relatively low
frequency words8 (e.g. CHISEL), words which are
semantically ambiguous as nouns (e.g. ARM), and
pairs which are semantically similar (e.g. SPOON &
KNIFE). This suggests that the SVD and triple ex-
traction methods may perform better with a larger
and more diverse corpus.
8AIRPLANE is relatively low frequency in the BNC; it may
be more sensible to use the word AEROPLANE with a British
corpus.
76
Mitchell et al SVD Triple Extraction (weighted)
Pair Nr. Pair Nr. Pair Nr.
bear saw 0 cup airplane 0 dresser chimney 0
bell carrot 0 cup lettuce 0 airplane chisel 0
bell saw 0 horse beetle 0 airplane hand 0
knife bear 0 chisel arm 0 airplane tomato 0
cup saw 1 hammer arm 1 spoon chisel 0
bear tomato 1 dresser arch 1 spoon knife 0
Table 4: Leave-out pairs for which each model performs least accurately, across the nine participants. Nr. = the number
of participants for which this leave-out pair was correctly matched.
5 Conclusion
The fMRI dataset and training and evaluation
methodology presented by Mitchell et al (2008)
gives researchers an interesting new framework with
which to evaluate the quality of feature-based con-
ceptual representations extracted from corpora. This
framework avoids some of the problems inherent in
evaluating extracted representations against a ?gold
standard? based on participant-generated property
norms. It also provides a rich multi-dimensional
dataset through which the strengths and weaknesses
of extraction methods can be identified.
We have applied this evaluation framework to
four feature extraction methods which use different
sources of information available in corpora to extract
conceptual representations. Surprisingly, in spite of
their major differences, we did not find any signifi-
cant difference in performance between the models.
This finding has interesting theoretical implica-
tions, given that previous research has suggested
that aspects of meaning defined by sensory-motor
verbs may have a somewhat distinctive role to play
in predicting the fMRI activation associated with
conceptual stimuli (Mitchell et al, 2008). Our re-
sults suggest that general feature-based representa-
tions of concepts, which place no a priori distinc-
tion on sensory-motor properties, may be equally
capable of predicting activation to conceptual stim-
uli. This highlights the potential for the Mitchell
et al method to be used to inform both distributed
and sensory-motor accounts of conceptual represen-
tation (e.g. McRae et al (1997), Cree et al (2006),
Tyler et al (2000), Tyler & Moss (2001), Moss et
al. (2007), Martin & Chao (2001)), as well as pro-
viding a benchmark with which to assess semantic
model development. In a similar vein, Murphy et
al. (2009) used a dependency-parsed corpus yielding
verb co-occurrence statistics to predict EEG9 activa-
tion patterns with significant accuracy.
The training and evaluation framework presented
by Mitchell et al (2008) represents just one point
in a large space of possibilities for using computa-
tional modelling to predict human brain activity as-
sociated with conceptual stimuli. In these initial ex-
periments, we have chosen to follow the Mitchell
et al approach as closely as possible, in order to
maximize comparability with their results. In future
work, we aim to investigate other methods for train-
ing and evaluation, other corpora and other sources
of imaging data. Furthermore, we aim to use the
evaluation results from such work to inform the de-
velopment of our extraction method.
Acknowledgments
Our work was funded by the EPSRC grant
EP/F030061/1, and the Royal Society University
Research Fellowship, UK. We thank Mitchell et
al. (2008) and McRae et al (2005) for making their
data publically available.
References
Mark Andrews, G. Vigliocco, and D. Vinson. 2005.
Integrating attributional and distributional informa-
tion in a probabilistic model of meaning representa-
tion. In Timo Honkela et al, editor, Proceedings of
AKRR?05, International and Interdisciplinary Confer-
ence on Adaptive Knowledge Representation and Rea-
9EEG measures voltages induced by neuronal firing across
the human scalp.
77
soning, pages 15?25, Espoo, Finland: Helsinki Uni-
versity of Technology.
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic representations. Psychological Re-
view, 116(3):463?498.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. From context to mean-
ing: Distributional models of the lexicon in linguis-
tics and cognitive science (Special issue of the Italian
Journal of Linguistics), 20(1):55?88.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. ESSLLI 2008 Workshop on Distributional
Lexical Semantics.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2009. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, pages 1?33.
E. Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the Interactive Demo Session of COLING/ACL-
06, pages 77?80.
George S. Cree, Chris McNorgan, and Ken McRae.
2006. Distinctive features hold a privileged status
in the computation of word meaning: Implications
for theories of semantic memory. Journal of Experi-
mental Psychology. Learning, Memory, and Cognition,
32(4):643?58.
Colin Kelly, Barry Devereux, and Anna Korhonen. 2010.
Acquiring human-like feature-based conceptual repre-
sentations from corpora. In Brian Murphy, Kai min
Kevin Chang, and Anna Korhonen, editors, Proceed-
ings of the NAACL-HLT Workshop on Computational
Neurolinguistics, Los Angeles, USA.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
G. Leech, R. Garside, and M. Bryant. 1994. CLAWS4:
the tagging of the British National Corpus. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 1, pages 622?628. Association for
Computational Linguistics.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28(2):203?208.
Alex Martin and Linda L. Chao. 2001. Semantic mem-
ory and the brain: structure and processes. Current
Opinion in Neurobiology, 11(2):194?201.
Ken McRae, Virginia R. de Sa, and Mark S. Seidenberg.
1997. On the nature and scope of featural representa-
tions of word meaning. Journal of Experimental Psy-
chology: General, 126(2):99?130.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37:547?559.
Tom M. Mitchell, Svetlana V. Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L. Malave, Robert A.
Mason, and Marcel A. Just. 2008. Predicting human
brain activity associated with the meanings of nouns.
Science, 320(5880):1191?1195.
Helen E. Moss, Lorraine K. Tyler, and Kirsten I. Taylor.
2007. Conceptual structure. In M. Gareth Gaskell, ed-
itor, The Oxford handbook of psycholinguistics, pages
217?234. Oxford University Press, Oxford, UK.
B. Murphy, M. Baroni, and M. Poesio. 2009. Eeg re-
sponds to conceptual stimuli and corpus semantics.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2009),
pages 619?627, East Stroudsburg, PA.
Gregory Murphy. 2002. The big book of concepts. The
MIT Press, Cambridge, MA.
Mark Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234?243.
Lorraine K. Tyler and Helen E. Moss. 2001. Towards a
distributed account of conceptual knowledge. Trends
in Cognitive Sciences, 5(6):244?252.
L. K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
78
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 11?20,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Semi-supervised learning for automatic conceptual property extraction
Colin Kelly
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
colin.kelly
@cl.cam.ac.uk
Barry Devereux
Centre for Speech,
Language, and the Brain
University of Cambridge
Cambridge, CB2 3EB, UK
barry@csl.psychol.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
Cambridge, CB3 0FD, UK
anna.korhonen
@cl.cam.ac.uk
Abstract
For a given concrete noun concept, humans
are usually able to cite properties (e.g., ele-
phant is animal, car has wheels) of that con-
cept; cognitive psychologists have theorised
that such properties are fundamental to un-
derstanding the abstract mental representation
of concepts in the brain. Consequently, the
ability to automatically extract such properties
would be of enormous benefit to the field of
experimental psychology. This paper investi-
gates the use of semi-supervised learning and
support vector machines to automatically ex-
tract concept-relation-feature triples from two
large corpora (Wikipedia and UKWAC) for
concrete noun concepts. Previous approaches
have relied on manually-generated rules and
hand-crafted resources such as WordNet; our
method requires neither yet achieves bet-
ter performance than these prior approaches,
measured both by comparison with a property
norm-derived gold standard as well as direct
human evaluation. Our technique performs
particularly well on extracting features rele-
vant to a given concept, and suggests a number
of promising areas for future focus.
1 Introduction
The representation of concrete concepts (e.g., car,
banana, spanner) in the human brain has long been
an important area of investigation for cognitive psy-
chologists. Recent theories of this mental repre-
sentation have proposed a componential, property-
based and distributed model of conceptual knowl-
edge (e.g., Farah and McClelland (1991), Randall et
al. (2004), Tyler et al (2000)).
In order to empirically test these cognitive the-
ories, researchers have moved towards employing
real-world knowledge in their experiments. This
knowledge has usually been procured from human-
derived lists of properties taken from property norm-
ing studies (Garrard et al, 2001; McRae et al,
2005). In such studies, human participants are
asked to describe and note properties of a given
concept (e.g., has shell for turtle). Synonymous
responses are grouped together as a single prop-
erty and those meeting a certain minimum response-
frequency threshold are taken as valid properties.
The most wide-ranging study to date was that con-
ducted by McRae et al (2005): some sample prop-
erties from this set are in Table 1.
As others have noted (Murphy, 2002; McRae et
al., 2005), property norming studies are prone to a
number of deficiencies. One such weakness is the
incongruity of shared properties across even highly-
related concepts: human respondents exhibit a lack
of consistency when listing properties that are com-
mon to many similar concepts. For example, while
has legs is listed as a property of crocodile in the
McRae norms, it is absent as a property of alliga-
tor. A related issue is the non-comprehensive nature
of the generated norms ? although they may cover
the most salient properties for a given concept, they
are unlikely to comprise all of a concept?s properties
(e.g., has heart does not appear as a property of any
of the 92 animal concepts).
Our research aims to use NLP techniques to cre-
ate a system able to emulate the output of such
studies, and overcome some of the aforementioned
weaknesses. Our proposed system begins by search-
ing dependency-parsed corpora for those sentences
containing concept and feature terms which are
also found in a McRae norm-derived training set
of properties. For these sentences, the system
generates grammatical relation/part-of-speech struc-
tural attributes and applies support vector machines
(SVMs) to learn sets of attributes likely to indicate
the instantiation of a property in a sentence. These
11
turtle bowl
has a shell 25 is round 19
lays eggs 16 used for eating 12
swims 15 used for soup 11
is green 14 used for food 11
lives in water 14 used for liquids 10
is slow 13 used for eating cereal 10
an animal 11 made of plastic 8
walks 10 used for holding things 7
walks slowly 10 is curved 7
has 4 legs 9 found in kitchens 7
Table 1: Top ten properties from McRae norms with pro-
duction frequencies for turtle and bowl.
learned patterns of salient attributes are finally ap-
plied to a corpus to derive new properties for unseen
concepts.
Our task is a challenging one: the properties we
seek are extremely diverse in their form. They range
from the simple (e.g., banana is yellow) to the com-
plex (e.g., bayonet found at the end of a gun). Al-
though the properties can broadly be divided into
a number of categories (encyclopedic, taxonomic,
functional, etc) there is not a great deal of regular-
ity in the nature of the properties a given noun will
likely possess: it is highly concept-dependent.
Furthermore, we hope to derive these properties
from corpora, with the assumption that these prop-
erties will manifest themselves therein. Indeed, An-
drews et al (2005) discuss a theory of human knowl-
edge which relies on a combination of both dis-
tributional (i.e., derived from spoken and written
language) and experiential data (i.e., that derived
from our interactions with the real world), claiming
that the necessary contribution of each data-type for
a comprehensive human semantic representation is
non-trivial. Finally, there are difficulties associated
with evaluating our system?s output directly against
a set of human-generated property norms: we dis-
cuss these in further detail later.
Given their provenance, the properties found in
property norms are free-form. To simplify our task
we apply a more rigid representation to the proper-
ties we already have and to those we aim to seek. We
delineate each property into a concept relation fea-
ture triple (see Section 2.2) and our task becomes
one of finding valid relation feature pairs given a par-
ticular concept. This recoding renders our task more
well-defined and makes evaluation of our method
reptile1NNS
include2VBP
species0IN
of3IN
turtle5NN
dobj
five1DT
ncmod ncmod
dobj
marine0NNP
ncmod ncsubj
Figure 1: C&C-derived GR-POS graph for the sentence
Marine reptiles include five species of turtle.
more comparable to previous and related work.
Having framed our task in this way, there is an
obvious parallel with relation extraction: both ne-
cessitate the selection/classification of relationships
between individual entities (in our case, between
concept and feature). Hearst (1992) was the first
to propose a pattern-based approach to this task us-
ing lexico-syntactic patterns to automatically extract
hyponyms and this technique has frequently been
used for ontology learning. For example, Pantel and
Pennacchiotti (2008) linked instantiations of a set of
semantic relations into existing semantic ontologies
and Davidov et al (2007) employed seed concepts
from a given semantic class to discover relations
shared by concepts in that class.
Our task is more complex than classic relation ex-
traction for two main reasons: 1) the relations which
we aim to extract are not limited to a small set of
just a few well-defined relations (e.g., is-a and part-
of) nor to the relations of a specific semantic class
(e.g., capital-is for countries). Indeed the relations
can be as many and diverse as the concepts them-
selves (e.g., each concept could possess a unique
and distinguishing relation and feature). 2) We are
attempting to simultaneously extract two pieces of
information: features of the concept and those fea-
tures? defining relationship with the concept, but
only those relations and features which would be
classified as ?common-sense?, something which is
easy for humans to recognise but difficult (if not im-
possible) to describe rigorously or formally.
There has recently been work on the automatic ex-
12
traction of binary relations that scale to a web cor-
pus, for example the ReVerb (Etzioni et al, 2011)
and WOE (Wu and Weld, 2010) systems. These
systems are designed to extract legitimate relations
from a given sentence. In contrast, our aim is to cap-
ture more general relationships which are ?common-
sense?; just because an extracted relation is correct
in a given context does not automatically make it
true in general. Previous reasoned approaches to our
task have taken their lead from Hearst and her suc-
cessors, employing manually-created rulesets to ex-
tract such properties from corpora (e.g., Baroni et al
(2009), Devereux et al (2010), and our comparison
system (Kelly et al, 2010)). Baroni et al extract re-
lational information in the form of ?type-sketches?,
which give an approximate, implicit description of
the relationship whereas we are aiming to extract
explicit relations between the target concept and its
corresponding features. Devereux et al and Kelly
et al have attempted this, but both employ WordNet
(Fellbaum, 1998) to extract semantic relatedness in-
formation.
We use semi-supervised learning as it offers a
flexible technique of harnessing small amounts of
labelled data to derive information from unlabelled
datasets/corpora and allows us to guide the extrac-
tion towards our desired ?common-sense? output.
We chose SVMs as they have been used for a va-
riety of tasks in NLP (e.g., Joachims et al (1998),
Gime?nez and Marquez (2004)). We will demon-
strate that our system?s performance exceeds that of
Kelly et al (2010) and Etzioni et al (2011). It is, as
far as we are aware, the first work to employ semi-
supervised learning for this task.
2 Method
We will use SVMs to learn lexico-syntactic pat-
terns in our corpora corresponding to known prop-
erties in order to find new ones. Training an SVM
requires a labelled training set. To generate this
set we harness our already-known concepts/features
(and their relationships) from the McRae norms to
find instantiations of said relationships within our
corpora. We use parsed sentence information from
our corpora to create a set of attributes describing
each relationship, our learning patterns. In doing
so, we are assuming that across sentences in our
corpora containing a concept/feature pair found in
the McRae norms, there will be a set of consistent
lexico-syntactic patterns which indicate the same re-
lationship as that linking the pair in the norms.
Thus we iterate over our chosen corpora, parsing
each concept-containing sentence to yield grammat-
ical relation (GR) and part-of-speech (POS) infor-
mation from which we can create a GR-POS graph
relating the two. Then for each triple, we find any/all
paths through the graph which link the concept to its
feature and use the corresponding relation to label
this path. We collect descriptive information about
the path in the form of attributes describing it (e.g.,
path nodes, labels, length) to create a training pattern
specific to that concept relation feature triple and
sentence. It is these lists of attributes (and their rela-
tion labels) which we employ as the labelled training
set and as input for our SVM.
2.1 Corpora
We employ two corpora for our experiments:
Wikipedia and the UKWAC corpus (Ferraresi et al,
2008). These are both publicly available and web-
based: the former a source of encyclopedic infor-
mation and the latter a source of general text. Our
Wikipedia corpus is based on a Sep 2009 version
of English-language Wikipedia and contains around
1.84 million articles (>1bn words). Our UKWAC
corpus is an English-language corpus (>2bn words)
obtained by crawling the .uk internet domain.
2.2 Training data
Our experiments use a British-English version of
the McRae norms (see Taylor et al (2011) for de-
tails). We needed to recode the free-form McRae
properties into relation-classes and features which
would be usable for our learning algorithm. As
we will be matching the features from these prop-
erties with individual words in the training corpus
it was essential that the features we generated con-
tained only one lemmatised word. In contrast, the
relations were merely labels for the relationship de-
scribed (they did not need to occur in the sentences
we were training from) and therefore needed only
to be single-string relations. This allowed preposi-
tional verbs as distinct relations, something which
has not been attempted in previous work yet can be
semantically significant (e.g., the relations used-in,
used-for and used-by have dissimilar meanings).
We applied the following sequential multi-step
13
process to our set of free-form properties to distill
them to triples of the form concept relation feature,
where relation can be a multi-word string and feature
is a single word:
1. Translation of implicit properties to their correct re-
lations (e.g., pig an animal ? pig is an animal).
2. Removal of indefinite and definite articles.
3. Behavioural properties become ?does? properties
(e.g., turtle beh eats ? turtle does eats).
4. Negative properties given their own relation classes
(e.g., turkey does cannot fly ? turkey doesnt fly).
5. All numbers are translated to named cardinals (e.g.,
spider has 8 legs ? spider has eight legs).
6. Some of the norms already contained synonymous
terms: these were split into separate triples for each
synonym (e.g., pepper tastes hot/spicy ? pepper
tastes hot and pepper tastes spicy).
7. Prepositional verbs were translated to one-word,
hyphenated strings (e.g., made of ? made-of ).
8. Properties with present participles as the penulti-
mate word were split into one including the verb as
the feature and one including it in the relation (e.g.,
envelope used for sending letters ? envelope used-
for-sending letters and envelope used-for sending).
9. Any remaining multi-word properties were split
with the first term after the concept acting as the
relation (e.g., bull has ring in its nose ? bull has
ring, bull has in, bull has its and bull has nose).
10. All remaining stop-words were removed; properties
ending in stop-words (e.g., bull has in and bull has
its) were removed completely.
This yielded 7,518 property-triples with 254 distinct
relations and an average of 14.7 triples per concept.
2.3 Parsing
We parsed both corpora using the C&C parser
(Clark and Curran, 2007) as we employ both GR
and POS information in our learning method. To ac-
celerate this stage, we process only sentences con-
taining a form (e.g., singular/plural) of one of our
training/testing concepts. We lemmatise each word
using the WordNet NLTK lemmatiser (Bird, 2006).
Parsing our corpora yields around 10Gb and 12Gb
of data for UKWAC and Wikipedia respectively.
The C&C dependency parse output contains, for
a given sentence, a set of GRs forming an acyclic
graph whose nodes correspond to words from the
sentence, with each node also labelled with the POS
of that word. Thus the GR-POS graph interrelates all
lexical, POS and GR information for the entire sen-
tence. It is therefore possible to construct a GR-POS
graph rooted at our target term (the concept in ques-
tion), with POS-labelled words as nodes, and edges
labelled with GRs linking the nodes to one another.
An example graph can be seen in Figure 1.
2.4 Support vector machines
We use SVMs (Cortes and Vapnik, 1995) for our
experiments as they have been widely used in NLP
and their properties are well-understood, showing
good performance on classification tasks (Meyer et
al., 2003). In their canonical form, SVMs are non-
probabilistic binary linear classifiers which take a set
of input data and predict, for each given input, which
of two possible classes it corresponds to.
There are more than two possible relation-labels
to learn for our input patterns, so ours is a multi-class
classification task. For our experiments we use the
SVM Light Multiclass (v. 2.20) software (Joachims,
1999) which applies the fixed-point SVM algorithm
described by Crammer and Singer (2002) to solve
multi-class problem instances. Joachims? software
has been widely used to implement SVMs (Vinok-
ourov et al, 2003; Godbole et al, 2002).
2.5 Attribute selection
Previous techniques for our task have made use of
lexical, syntactic and semantic information. We are
deliberately avoiding the use of manually-created
semantic resources, so we rely only on lexical and
syntactic attributes for our learning stage (i.e., the
GR-POS paths described earlier).
A table of all the categories of attributes we ex-
tract for each GR-POS path are in Table 2.4, together
with attributes from the path linking turtle and reptile
in our example sentence (see Figure 1).
We ran our experiments with two vector-types
which we call our ?verb-augmented? and our ?non-
augmented? vector-types. The sets are identical ex-
cept the verb-augmented vector-type will also con-
tain an additional attribute category containing an
attribute for every instance of a relation verb (i.e.,
a verb which is found in our training set of relations,
e.g., become, cause, taste, use, have and so on) in
the lexical path. We do this to ascertain whether this
additional verb-information might be more informa-
tive to our system when learning relations (which
tend to be composed of verbs).
14
Attribute category Example attribute(s)
GR path-length LEN
lemmatised anchor node LEM=turtle
POS of anchor node POS=NN
GR path labels GR1=dobjR
from anchor GR2=ncmodR
(indexed) GR3=dobjR
GR4=ncsubjN
GR path labels GR1=ncsubjR
from target GR2=dobjN
(indexed) GR3=ncmodN
GR4=dobjN
POS of path nodes POS1=IN
from anchor POS2=NNS
(indexed) POS3=VBP
POS4=NNS
POS of path nodes POS1=NNS
from target POS2=VBP
(indexed) POS3=NNS
POS4=IN
lemmatised path nodes LEM=include
(bag of words) LEM=species
LEM=of
POS of all path nodes POS=IN
(set) POS=NNS
POS=VBP
Relation verbs N/A
GR path labels GR=dobjR
(set) GR=ncmodN
GR=ncsubjN
lemmatised target node LEM=reptile
POS of target node POS=NNS
Table 2: An example vector for an instance of the
relation-label is. The attributes are distinguished from
one another by their attribute category. Relation verbs
only appear in the verb-augmented vector-type and no
such verbs appear in our example sentence, so this cat-
egory of attribute is empty. All attributes in the table will
receive the value 1.0 except the LEN attribute which will
have the value 0.2 (the reciprocal of the path length, 5).
We considered allocating a ?no-rel? relation la-
bel to those sets of attributes corresponding to paths
through the GR-POS graph which did not link the
concept to a feature found in our training data;
however our initial experiments indicated the SVM
model would assign every pattern we tested to the
?no-rel? relation. Therefore we used only positive
instances in our training pattern data.
We cycle through all training concepts/features,
finding sentences containing both. For each such
sentence, our system generates the attributes from
the GR-POS path linking the concept to the fea-
ture (the linking-path) to create a pattern for that
pair, in the form of a relation-labelled vector con-
taining real-valued attributes. The system assigns
1.0 to all attributes occurring in a given path
and the LEN value receives the reciprocal of the
path-length.1 Each linking-path is collected into a
relation-labelled, sparse vector in this manner. In
the larger UKWAC corpus this corresponds to over
29 million unique attributes across all found linking-
paths (this figure corresponds to the dimensionality
of our vectors). We then pass all vectors to the learn-
ing module2 of SVM Light to generate a learned
model across all training concepts.
2.6 Extracting candidate patterns
Having trained our model, we must now find po-
tential features and relations for our test concepts
in our corpora. We again only examine sentences
which contain at least one of our test concepts. Fur-
thermore, to avoid a combinatorial explosion of pos-
sible paths rooted at those concepts we only permit
as candidates those paths whose anchor node is a
singular or plural noun and whose target node is ei-
ther a singular/plural noun or adjective. This filter-
ing corresponds to choosing patterns containing one
of the three most frequent anchor node POS tags
(NN, NNS and NNP) and target node POS tags (NN,
JJ and NNS) found during our training stage. These
candidate patterns constitute 92.6% and 87.7% of
all the vectors, respectively, from our training set
of patterns (on the UKWAC corpus). This pattern
pre-selection allows us to immediately ignore paths
which, despite being rooted at a test concept, are un-
likely to contain property norm-like information.
2.7 Generating and ranking triples
We next classified our test concepts? candidate
patterns using the learned model. SVM Light as-
signs each pattern a relation-class from the training
set and outputs the values of the decision functions
from the learned model when applied to that par-
ticular pattern. The sign of these values indicates
the binary decision function choice, and their mag-
nitude acts as a measure of confidence. We wanted
those vectors which the model was most confident
in across all decision functions, so we took the sum
of the absolute values of the decision values to gen-
erate a pattern score for each vector/relation-label.
1All other possible attributes are assigned the value 0.0.
2Using a regularisation parameter (C) value of 1.0 and de-
fault parameters otherwise.
15
Vector-type Corpus ?LL ?PMI ?SVM Prec. Recall F
Ignoring relation.
Non-augmented
Wikipedia 0.3 0.00 1.00 0.2214 0.3197 0.2564
UKWAC 0.10 0.05 0.60 0.2279 0.3330 0.2664
UKWAC-Wikipedia 0.35 0.00 0.75 0.2422 0.3533 0.2829
Verb-augmented
Wikipedia 0.20 0.00 0.65 0.2217 0.3202 0.2568
UKWAC 0.30 0.00 0.95 0.2326 0.3400 0.2720
UKWAC-Wikipedia 0.40 0.05 1.00 0.2444 0.3577 0.2859
With relation.
Non-augmented
Wikipedia 0.05 0.00 1.00 0.1199 0.1732 0.1394
UKWAC 0.05 0.00 1.00 0.1126 0.1633 0.1312
UKWAC-Wikipedia 0.05 0.00 0.65 0.1241 0.1808 0.1449
Verb-augmented
Wikipedia 0.05 0.00 1.00 0.1215 0.1747 0.1410
UKWAC 0.05 0.00 1.00 0.1190 0.1724 0.1387
UKWAC-Wikipedia 0.05 0.00 0.70 0.1281 0.1860 0.1494
Table 3: Parameter estimation both with and without relation, using our augmented and non-augmented vector-types
and across our two corpora and the combined corpora set.
From these patterns we derived an output set of
triples where the concept and feature of a triple cor-
responded to the anchor and target nodes of its pat-
tern and the relation corresponded to the pattern?s
relation-label. Identical triples from differing pat-
terns had their pattern scores summed to give a final
?SVM score? for that triple.
2.8 Calculating triple scores
A brief qualitative evaluation of our system?s out-
put indicates that although the higher-ranked (by
SVM score) features and relations were, for the most
part, quite sensible, there were some obvious output
errors (e.g., non-dictionary strings or verbs appear-
ing as features). Therefore we restricted our fea-
tures to those which appear as nouns or adjectives in
WordNet and excluded features containing an NLTK
(Bird, 2006) corpus stop-word. Despite these exclu-
sions, some general (and therefore less informative)
relation/feature combinations (e.g., is good, is new)
were still ranking highly. To mitigate this, we ex-
tract both log-likelihood (LL) and pointwise mutual
information (PMI) scores for each concept/feature
pair to assess the relative saliency of each extracted
feature, with a view to downweighting common but
less interesting features. To speed up this and later
stages, we calculate both statistics for the top 1,000
triples extracted for each concept only.
PMI was proposed by Church and Hanks (1990)
to estimate word association. We will use it to mea-
sure the strength of association between a concept
and its feature. We hope that emphasising concept-
feature pairs with high mutual information will ren-
der our triples more relevant/informative.
We also employ the LL measure across our set of
concept-feature pairs. Proposed by Dunning (1993),
LL is a measure of the distribution of linguistic phe-
nomena in texts and has been used to contrast the
relative corpus frequencies of words. Our aim is to
highlight features which are particularly distinctive
for a given concept, and hence likely to be features
of that concept alone.
We calculate an overall score for a triple, t, by a
weighted combination of the triple?s SVM, PMI and
LL scores using the following formula:
score(t) = ?PMI?PMI(t)+?LL?LL(t)+?SVM?SVM(t)
where the PMI, SVM and LL scores are normalised
so they are in the range [0, 1]. The relative ? weights
thus give an estimate of the three measures? impor-
tance relative to one another and allows us to gauge
which combination of these scores is optimal.
2.9 Datasets
We also wanted to ascertain the extent to which
the output from both our corpora could be combined
to improve results, balancing the encyclopedic but
somewhat specific nature of Wikipedia with the gen-
erality and breadth of the UKWAC corpus. We com-
bined the output by summing individual SVM scores
of each triple from both corpora to yield a combined
SVM score. PMI and LL scores were then calcu-
lated as usual from this combined set of triples.
3 Experimental Evaluation
3.1 Evaluation methodology
We employ ten-fold cross-validation to ascertain
optimal SVM, LL and PMI ? parameters for our fi-
nal system. We exclude 44 concepts from our set of
16
Relation Prec. Recall F
Kelly et al Without 0.1943 0.3896 0.2592With 0.1102 0.2210 0.1471
ReVerb Without 0.1142 0.2258 0.1514With 0.0431 0.0864 0.0576
Our method Without 0.2417 0.4847 0.3225With 0.1238 0.2493 0.1654
Table 4: Our best scores on the ESSLLI set compared to
Kelly et al (2010) and the ReVerb system (Etzioni et al,
2011). Our results are from the verb-augmented vector-
type, using the combined UKWAC-Wikipedia corpus and
using the ? parameters highlighted in Table 3.
510 to use in our final system testing and split the
remaining 466 concepts randomly and evenly into
10 folds. We apply the training steps above to nine
of the folds, generating predictions for the single
held-out fold. We repeat this for all ten folds, yield-
ing relations and features with SVM, LL and PMI
scores for our full set of 466 training concepts on
the UKWAC, Wikipedia and combined corpora.
We varied the ? values from our scoring equa-
tion in the range [0,1] (interval 0.05) and com-
pared the top twenty triples for each concept directly
against the held-out training set. The best F-scores
and their corresponding ? values (evaluating on full
triples and concept-feature pairs alone) are in Ta-
ble 3. We can see that our best results employ the
verb-augmented vector-type and the combined cor-
pus, with a best F-score of 0.2859 when ignoring
the relation term and 0.1494 when including it in the
evaluation. The main difference between these two
results is the relative contribution of the reweighting
factors: the SVM score is the most important over-
all, but the LL and PMI scores come into play when
evaluating without the relation. This could be ex-
plained by the fact that the PMI and LL scores do
not use any relation terms in their calculations.
3.2 Quantitative evaluation
The unseen subset of the McRae norms is a set
of human-generated common-sense properties with
which our extracted properties can be compared.
However, an issue with the McRae norms is that
semantically identical properties can be represented
by lexically different triples. This problem was ac-
knowledged by Baroni et al (2008) who created
a synonym-expanded set of properties for 44 con-
cepts (selected evenly across six semantic classes;
the 44 concepts we excluded for testing) to par-
Judge Judge
turtle A B bowl A B
is green c c is large p p
is small c c used for food c c
is species c c used for mixing c c
is marine c c used for storing food c c
used for sea r r used for storing soup r r
is animal c c is ceramic c c
is many p c is small p p
has shell c c used for storing cereal r r
is large c p used for storing spoon r r
is reptile c c used for storing sugar p c
Table 5: Our judges? assessments of the correctness of the
top ten relation/feature pairs for two concepts extracted
from our best system.
tially solve it. This expansion set comprises the con-
cepts? top ten properties from the McRae norms with
semi-automatically generated synonyms for each of
the ten distinct features. For example, the triple
turtle has shell was expanded to also include tur-
tle has shield and turtle has carapace.
We use the two best systems (i.e., including and
excluding the relation; highlighted in Table 3) to
generate two sets of top twenty output triples for
our 44 concepts. We then calculate precision, re-
call and F-scores for each against our synonym-
expanded set.3 Using this expanded set alows us
to compare our work with that of Kelly et al (2010).
We also compare with the top twenty output of the
Reverb system Etzioni et al (2011) using their pub-
licly available relations derived from the ClueWeb09
corpus, employing their normalized triples ranked
by frequency. All sets of results are in Table 4. We
note that even though Kelly et al optimised their
algorithm on the ESSLLI set to yield a theoretical
best-possible score?we are evaluating ?blind??our
performance still shows an advance on theirs: the
improvement on both sets when comparing the pop-
ulation of F-scores across all 44 concepts is statisti-
cally significant at the 0.5% level.4
3.3 Human evaluation
The above does not quite offer the full picture:
unlike the features, the relations are not synonym-
expanded. Furthermore, it is possible that there
3We note that we are incorporating an upper bound for pre-
cision of 0.500 by comparing with only the top ten properties.
4Paired t-tests. ?With relation?: t = 3.524, d.f.= 43, p =
0.0010. ?Without relation?: t = 3.503, d.f.= 43, p = 0.0011.
17
Relation A B ? Agreements
With c / p 146 161 0.7421 261 (87%)
r / w 153 138
Without c / p 226 235 0.5792 255 (85%)
r / w 74 65
Table 6: Inter-annotator agreement for our best system,
both including and excluding the relation.
are correct properties being generated which simply
don?t appear in the ESSLLI evaluation set.
In order to address these concerns, we also per-
formed a human evaluation on 15 of our concepts.5
We asked two native English-speaking judges to de-
cide whether a given triple was correct,6 plausible,7
wrong but related,8 or wrong.9 We executed the
human evaluation on our two best systems (as de-
scribed above). As there were shared triples and
concept-feature pairs across the two output sets,
each triple and pair was evaluated only once. The
judges were aware of the purposes of the study but
were blind to the source sets. Some example judge-
ments are in Table 5.
The agreement results across all 15 concepts to-
gether with their ? coefficients (Cohen, 1960) are
in Table 6. In our evaluation we conflated the cor-
rect/plausible and wrong but related/wrong cate-
gories (see also Kelly et al (2010) and Devereux et
al. (2010)). We did this because of the subjective na-
ture of the judgements, and because we are seeking
properties which are indeed correct or at least plausi-
ble. These results indicate that our system is extract-
ing correct or plausible triples 51.1% of the time (ris-
ing to 76.8% when considering features only). They
also demonstrate a marked discrepancy between the
results for our two evaluations, reflecting the neces-
sity of human evaluation when assessing our partic-
ular task.
4 Discussion
In this paper we have shown that semi-supervised
learning techniques can automatically learn lexico-
5The 44 evaluation concepts had been separated into super-
ordinate categories for unrelated psycholinguistic research and
we selected our 15 proportionally and at random from these su-
perordinate categories.
6A correct, valid, feature.
7A triple which is plausible but only in a specific set of cir-
cumstances or a feature which was correct but very general.
8The triple is incorrect but there existed some sort of rela-
tionship between the concept and relation and/or feature.
9When the triple is simply wrong.
syntactic patterns indicative of property norm-like
relations and features. Using these patterns, our
system can extract relevant and accurate properties
from any parsed corpus and allows for multi-word
relation labels, allowing greater semantic precision.
As already mentioned, the work of Baroni et
al. (2009) is relevant to our own. Their approach
achieves a precision score of 0.239 on the top ten
returned features evaluated against the ESSLLI set:
our best system offers precision of 0.370 on the same
evaluation. Moreover, Baroni et al do not explicitly
derive relation terms. We better the performance of
a comparable system (Kelly et al, 2010), even when
evaluating against an unseen set of concepts, and our
system does not use manually-generated rules or se-
mantic information. Furthermore, human evaluation
shows over half of our extracted properties are cor-
rect/plausible.
For future work, we have already mentioned that
we are ignoring a large amount of potentially in-
structive training data, specifically those GR-POS
paths in our corpus which don?t terminate on one of
our training features, as well as those paths through
sentences containing one of our concepts but none
of our training features. It might therefore be worth-
while investigating the use of this ?negative? infor-
mation. Another potential avenue for exploration
would be the expansion of the learning vector-types.
Although we already use a significant number of
learning attributes (an average of 37.9 per training
pattern), we could include more: there may be addi-
tional information not directly on the GR-POS path
linking a concept and feature (e.g., nodes adjacent
to said path) which might be indicative of their re-
lationship. We would also consider using active-
learning, introducing a feedback loop and human-
annotation to better distinguish between relations
which our algorithm tends to classify incorrectly.
For example, we could supplement input pattern
data with disambiguating POS-GR graphs, drawing
a distinction between valid and non-valid relations.
Finally, our system could also be evaluated in the
context of a psycholinguistic experiment. For exam-
ple, we could use our system output to predict con-
cept similarity by using our extracted triples to cre-
ate vector representations of each concept, calculat-
ing the distance between those vectors and compar-
ing these similarity ratings with human judgements.
18
Acknowledgements
This research was supported by EPSRC grant
EP/F030061/1. We are grateful to McRae and col-
leagues for making their norms publicly available,
and to the anonymous reviewers for their helpful in-
put.
References
M. Andrews, G. Vigliocco, and D. Vinson. 2005.
Integrating attributional and distributional informa-
tion in a probabilistic model of meaning representa-
tion. In Timo Honkela et al, editor, Proceedings of
AKRR?05, International and Interdisciplinary Confer-
ence on Adaptive Knowledge Representation and Rea-
soning, pages 15?25, Espoo, Finland: Helsinki Uni-
versity of Technology.
M. Baroni, S. Evert, and A. Lenci, editors. 2008. ESSLLI
2008 Workshop on Distributional Lexical Semantics.
M. Baroni, B. Murphy, Barbu E., and Poesio M. 2009.
Strudel: A corpus-based semantic model based on
properties and types. Cognitive Science, pages 1?33.
S. Bird. 2006. NLTK: The natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, pages 69?72. Association for Com-
putational Linguistics.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
S. Clark and J.R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4):493?552.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and psychological measurement.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273?297.
K. Crammer and Y. Singer. 2002. On the algorithmic
implementation of multiclass kernel-based vector ma-
chines. The Journal of Machine Learning Research,
2:265?292.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Annual Meeting-Association
For Computational Linguistics, volume 45, page 232.
B. Devereux, N. Pilkington, T. Poibeau, and A. Korho-
nen. 2010. Towards unrestricted, large-scale acquisi-
tion of feature-based conceptual representations from
corpus data. Research on Language & Computation,
pages 1?34.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational linguistics,
19(1):61?74.
O. Etzioni, A. Fader, J. Christensen, S. Soderland, and
M.T. Center. 2011. Open information extraction: The
second generation. In Twenty-Second International
Joint Conference on Artificial Intelligence.
M.J. Farah and J.L. McClelland. 1991. A computational
model of semantic memory impairment: Modality
specificity and emergent category specificity. Journal
of Experimental Psychology: General, 120(4):339?
357.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini.
2008. Introducing and evaluating ukwac, a very large
web-derived corpus of english. In Proceedings of the
4th Web as Corpus Workshop (WAC-4) Can we beat
Google, pages 47?54.
P. Garrard, M.A.L. Ralph, J.R. Hodges, and K. Patterson.
2001. Prototypicality, distinctiveness, and intercorre-
lation: Analyses of the semantic attributes of living
and nonliving concepts. Cognitive Neuropsychology,
18(2):125?174.
J. Gime?nez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector ma-
chines. In In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Citeseer.
S. Godbole, S. Sarawagi, and S. Chakrabarti. 2002. Scal-
ing multi-class support vector machines using inter-
class confusion. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 513?518. ACM.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics-Volume 2,
pages 539?545. Association for Computational Lin-
guistics.
T. Joachims, C. Nedellec, and C. Rouveirol. 1998. Text
categorization with support vector machines: learning
with many relevant. In Machine Learning: ECML-
98 10th European Conference on Machine Learning,
Chemnitz, Germany, pages 137?142. Springer.
T. Joachims. 1999. Svmlight: Support vector machine.
SVM-Light Support Vector Machine http://svmlight.
joachims. org/, University of Dortmund, 19.
C. Kelly, B. Devereux, and A. Korhonen. 2010. Ac-
quiring human-like feature-based conceptual represen-
tations from corpora. In First Workshop on Computa-
tional Neurolinguistics, page 61. Citeseer.
K. McRae, G.S. Cree, M.S. Seidenberg, and C. McNor-
gan. 2005. Semantic feature production norms for
a large set of living and nonliving things. Behav-
ioral Research Methods, Instruments, and Computers,
37:547?559.
19
D. Meyer, F. Leisch, and K. Hornik. 2003. The support
vector machine under test. Neurocomputing, 55(1-
2):169?186.
G. Murphy. 2002. The big book of concepts. The MIT
Press, Cambridge, MA.
P. Pantel and M. Pennacchiotti. 2008. Automatically har-
vesting and ontologizing semantic relations. In Pro-
ceeding of the 2008 conference on Ontology Learning
and Population: Bridging the Gap between Text and
Knowledge, pages 171?195. IOS Press.
B. Randall, H.E. Moss, J.M. Rodd, M. Greer, and L.K.
Tyler. 2004. Distinctiveness and correlation in con-
ceptual structure: Behavioral and computational stud-
ies. Journal of Experimental Psychology Learning
Memory and Cognition, 30(2):393?406.
K.I. Taylor, B.J. Devereux, K. Acres, B. Randall, and
L.K. Tyler. 2011. Contrasting effects of feature-based
statistics on the categorisation and basic-level identifi-
cation of visual objects. Cognition.
L.K. Tyler, H.E. Moss, M.R. Durrant-Peatfield, and J.P.
Levy. 2000. Conceptual structure and the structure of
concepts: A distributed account of category-specific
deficits. Brain and Language, 75(2):195?231.
A. Vinokourov, J. Shawe-Taylor, and N. Cristianini.
2003. Inferring a semantic representation of text via
cross-language correlation analysis. Advances in neu-
ral information processing systems, 15:1473?1480.
F. Wu and D.S. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118?127. Association for Computational
Linguistics.
20
