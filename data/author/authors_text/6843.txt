Overcoming the customization bottleneck using example-based MT
Stephen D. Richardson, William B. Dolan, Arul Menezes, Monica Corston-Oliver?
Microsoft Research ?Butler Hill Group
One Microsoft Way 4610 Wallingford Ave. N.
Redmond, WA 98052 Seattle WA 98103
{steveri, billdol, arulm}@microsoft.com moco@butlerhill.com
Abstract
We describe MSR-MT, a large-scale
hybrid machine translation system
under development for several
language pairs. This system?s ability to
acquire its primary translation
knowledge automatically by parsing a
bilingual corpus of hundreds of
thousands of sentence pairs and
aligning resulting logical forms
demonstrates true promise for
overcoming the so-called MT
customization bottleneck. Trained on
English and Spanish technical prose, a
blind evaluation shows that MSR-MT?s
integration of rule-based parsers,
example based processing, and
statistical techniques produces
translations whose quality exceeds that
of uncustomized commercial MT
systems in this domain.
1 Introduction
Commercially available machine translation
(MT) systems have long been limited in their
cost effectiveness and overall utility by the need
for domain customization. Such customization
typically includes identifying relevant
terminology (esp. multi-word collocations),
entering this terminology into system lexicons,
and making additional tweaks to handle
formatting and even some syntactic
idiosyncrasies. One of the goals of data-driven
MT research has been to overcome this
customization bottleneck through automated or
semi-automated extraction of translation
knowledge from bilingual corpora.
To address this bottleneck, a variety of
example based machine translation (EBMT)
systems have been created and described in the
literature. Some of these employ parsers to
produce dependency structures for the sentence
pairs in aligned bilingual corpora, which are
then aligned to obtain transfer rules or examples
(Meyers et al 2000; Watanabe et al 2000).
Other systems extract and use examples that are
represented as linear patterns of varying
complexity (Brown 1999; Watanabe and Takeda
1998; Turcato et al 1999).
For some EBMT systems, substantial
collections of examples are also manually
crafted or at least reviewed for correctness after
being identified automatically (Watanabe et al
2000; Brown 1999; Franz et al 2000). The
efforts that report accuracy results for fully
automatic example extraction (Meyers et al
2000; Watanabe et al 2000) do so for very
modest amounts of training data (a few thousand
sentence pairs). Previous work in this area thus
raises the possibility that manual review or
crafting is required to obtain example bases of
sufficient coverage and accuracy to be truly
useful.
Other variations of EBMT systems are
hybrids that integrate an EBMT component as
one of multiple sources of transfer knowledge
(in addition to other transfer rule or knowledge
based components) used during translation
(Frederking et al 1994; Takeda et al 1992).
To our knowledge, commercial quality MT
has so far been achieved only through years of
effort in creating hand-coded transfer rules.
Systems whose primary source of translation
knowledge comes from an automatically created
example base have not been shown capable of
matching or exceeding the quality of
commercial systems.
This paper reports on MSR-MT, an MT
system that attempts to break the customization
bottleneck by exploiting example-based (and
some statistical) techniques to automatically
acquire its primary translation knowledge from a
bilingual corpus of several million words. The
system leverages the linguistic generality of
existing rule-based parsers to enable broad
coverage and to overcome some of the
limitations on locality of context characteristic
of data-driven approaches. The ability of MSR-
MT to adapt automatically to a particular
domain, and to produce reasonable translations
for that domain, is validated through a blind
assessment by human evaluators. The quality of
MSR-MT?s output in this one domain is shown
to exceed the output quality of two highly rated
(though not domain-customized) commercially
available MT systems.
We believe that this demonstration is the first
in the literature to show that automatic training
methods can produce a commercially viable
level of translation quality.
2 MSR-MT
MSR-MT is a data-driven hybrid MT system,
combining rule-based analysis and generation
components with example-based transfer. The
automatic alignment procedure used to create
the example base relies on the same parser
employed during analysis and also makes use of
its own small set of rules for determining
permissible alignments. Moderately sized
bilingual dictionaries, containing only word
pairs and their parts of speech, provide
translation candidates for the alignment
procedure and are also used as a backup source
of translations during transfer. Statistical
techniques supply additional translation pair
candidates for alignment and identify certain
multi-word terms for parsing and transfer.
The robust, broad-coverage parsers used by
MSR-MT were created originally for
monolingual applications and have been used in
commercial grammar checkers.1 These parsers
produce a logical form (LF) representation that
is compatible across multiple languages (see
section 3 below). Parsers now exist for seven
languages (English, French, German, Spanish,
Chinese, Japanese, and Korean), and active
development continues to improve their
accuracy and coverage.
1 Parsers for English, Spanish, French, and German
provide linguistic analyses for the grammar checker
in Microsoft Word.
Figure 1. MSR-MT architecture.
Generation components are currently being
developed for English, Spanish, Chinese, and
Japanese. Given the automated learning
techniques used to create MSR-MT transfer
components, it should theoretically be possible,
provided with appropriate aligned bilingual
corpora, to create MT systems for any language
pair for which we have the necessary parsing
and generation components. In practice, we
have thus far created systems that translate into
English from all other languages and that
translate from English to Spanish, Chinese, and
Japanese. We have experimented only
preliminarily with Korean and Chinese to
Japanese.
Results from our Spanish-English and
English-Spanish systems are reported at the end
of this paper. The bilingual corpus used to
produce these systems comes from Microsoft
manuals and help text. The sentence alignment
of this corpus is the result of using a commercial
translation memory (TM) tool during the
translation process.
The architecture of MSR-MT is presented in
Figure 1. During the training phase, source and
target sentences from the aligned bilingual
corpus are parsed to produce corresponding LFs.
The normalized word forms resulting from
parsing are also fed to a statistical word
association learner (described in section 4.1),
which outputs learned single word translation
pairs as well as a special class of multi-word
pairs. The LFs are then aligned with the aid of
translations from a bilingual dictionary and the
learned single word pairs (section 4.2). Transfer
mappings that result from LF alignment, in the
form of linked source and target LF segments,
are stored in a special repository known as
MindNet (section 4.3). Additionally, the learned
multi-word pairs are added to the bilingual
dictionary for possible backup use during
translation and to the main parsing lexicon to
improve parse quality in certain cases.
At runtime, MSR-MT?s analysis parses
source sentences with the same parser used for
source text during the training phase (section
5.1). The resulting LFs then undergo a process
known as MindMeld, which matches them
against the LF transfer mappings stored in
MindNet (section 5.2). MindMeld also links
segments of source LFs with corresponding
target LF segments stored in MindNet. These
target LF segments are stitched together into a
single target LF during transfer, and any
translations for words or phrases not found
during MindMeld are searched for in the
updated bilingual dictionary and inserted in the
target LF (section 5.3). Generation receives the
target LF as input, from which it produces a
target sentence (section 5.4).
3 Logical form
MSR-MT?s broad-coverage parsers produce
conventional phrase structure analyses
augmented with grammatical relations (Heidorn
et al 2000). Syntactic analyses undergo further
processing in order to derive logical forms
(LFs), which are graph structures that describe
labeled dependencies among content words in
the original input. LFs normalize certain
syntactic alternations (e.g. active/passive) and
resolve both intrasentential anaphora and long-
distance dependencies.
MT has proven to be an excellent application
for driving the development of our LF
representation. The code that builds LFs from
syntactic analyses is shared across all seven of
the languages under development. This shared
architecture greatly simplifies the task of
aligning LF segments (section 4.2) from
different languages, since superficially distinct
constructions in two languages frequently
collapse onto similar or identical LF
representations. Even when two aligned
sentences produce divergent LFs, the alignment
and generation components can count on a
consistent interpretation of the representational
machinery used to build the two. Thus the
meaning of the relation Topic, for instance, is
consistent across all seven languages, although
its surface realizations in the various languages
vary dramatically.
4 Training MSR-MT
This section describes the two primary
mechanisms used by MSR-MT to automatically
extract translation mappings from parallel
corpora and the repository in which they are
stored.
4.1 Statistical learning of single word-
and multi-word associations
The software domain that has been our
primary research focus contains many words
and phrases that are not included in our general-
domain lexicons. Identifying translation
correspondences between these unknown words
and phrases across an aligned dataset can
provide crucial lexical anchors for the alignment
algorithm described in section 4.2.
In order to identify these associations, source
and target text are first parsed, and normalized
word forms (lemmas) are extracted. In the
multi-word case, English ?captoid? processing is
exploited to identify sequences of related,
capitalized words. Both single word and multi-
word associations are iteratively hypothesized
and scored by the algorithm under certain
constraints until a reliable set of each is
obtained.
Over the English/Spanish bilingual corpus
used for the present work, 9,563 single word and
4,884 multi-word associations not already
known to our system were identified using this
method.
Moore (2001) describes this technique in
detail, while Pinkham & Corston-Oliver (2001)
describes its integration with MSR-MT and
investigates its effect on translation quality.
4.2 Logical form alignment
As described in section 2, MSR-MT acquires
transfer mappings by aligning pairs of LFs
obtained from parsing sentence pairs in a
bilingual corpus. The LF alignment algorithm
first establishes tentative lexical
correspondences between nodes in the source
and target LFs using translation pairs from a
bilingual lexicon. Our English/Spanish lexicon
presently contains 88,500 translation pairs,
which are then augmented with single word
translations acquired using the statistical method
described in section 4.1. After establishing
possible correspondences, the algorithm uses a
small set of alignment grammar rules to align
LF nodes according to both lexical and
structural considerations and to create LF
transfer mappings. The final step is to filter the
mappings based on the frequency of their source
and target sides. Menezes & Richardson (2001)
provides further details and an evaluation of the
LF alignment algorithm.
The English/Spanish bilingual training
corpus, consisting largely of Microsoft manuals
and help text, averaged 14.1 words per English
sentence. A 2.5 million word sample of English
data contained almost 40K unique word forms.
The data was arbitrarily split in two for use in
our Spanish-English and English-Spanish
systems. The first sub-corpus contains over
208,000 sentence pairs and the second over
183,000 sentence pairs. Only pairs for which
both Spanish and English parsers produce
complete, spanning parses and LFs are currently
used for alignment. Table 1 provides the
number of pairs used and the number of transfer
mappings extracted and used in each case.
Spanish-
English
English-
Spanish
Total sentence pairs 208,730 183,110
Sentence pairs used 161,606 138,280
Transfer mappings
extracted
1,208,828 1,001,078
Unique, filtered
mappings used
58,314 47,136
Table 1. English/Spanish transfer mappings from
LF alignment
4.3 MindNet
The repository into which transfer mappings
from LF alignment are stored is known as
MindNet. Richardson et al (1998) describes
how MindNet began as a lexical knowledge
base containing LF-like structures that were
produced automatically from the definitions and
example sentences in machine-readable
dictionaries. Later, MindNet was generalized,
becoming an architecture for a class of
repositories that can store and access LFs
produced for a variety of expository texts,
including but not limited to dictionaries,
encyclopedias, and technical manuals.
For MSR-MT, MindNet serves as the
optimal example base, specifically designed to
store and retrieve the linked source and target
LF segments comprising the transfer mappings
extracted during LF alignment. As part of daily
regression testing for MSR-MT, all the sentence
pairs in the combined English/Spanish corpus
are parsed, the resulting spanning LFs are
aligned, and a separate MindNet for each of the
two directed language pairs is built from the LF
transfer mappings obtained. These MindNets
are about 7MB each in size and take roughly 6.5
hours each to create on a 550 Mhz PC.
5 Running MSR-MT
MSR-MT translates sentences in four processing
steps, which were illustrated in Figure 1 and
outlined in section 2 above. These steps are
detailed using a simple example in the following
sections.
5.1 Analysis
The input source sentence is parsed with the
same parser used on source text during MSR-
MT?s training. The parser produces an LF for
the sentence, as described in section 3. For the
example LF in Figure 2, the Spanish input
sentence is Haga clic en el bot?n de opci?n. In
English, this is literally Make click in the button
of option. In fluent, translated English, it is
Click the option button.
Figure 2. LF produced for Haga clic en el bot?n
de opci?n.
5.2 MindMeld
The source LF produced by analysis is next
matched by the MindMeld process to the source
LF segments that are part of the transfer
mappings stored in MindNet. Multiple transfer
mappings may match portions of the source LF.
MindMeld attempts to find the best set of
matching transfer mappings by first searching
for LF segments in MindNet that have matching
lemmas, parts of speech, and other feature
information. Larger (more specific) mappings
are preferred to smaller (more general)
mappings. In other words, transfers with context
will be matched preferentially, but the system
will fall back to the smaller transfers when no
matching context is found. Among mappings of
equal size, MindMeld prefers higher-frequency
mappings. Mappings are also allowed to match
overlapping portions of the source LF so long as
they do not conflict in any way.
After an optimal set of matching transfer
mappings is found, MindMeld creates Links on
nodes in the source LF to copies of the
corresponding target LF segments retrieved
from the mappings. Figure 3 shows the source
LF for the example sentence with additional
Links to target LF segments. Note that Links
for multi-word mappings are represented by
linking the root nodes (e.g., hacer and click) of
the corresponding segments, then linking an
asterisk (*) to the other source nodes
participating in the multi-word mapping (e.g.,
usted and clic). Sublinks between
corresponding individual source and target
nodes of such a mapping (not shown in the
figure) are also created for use during transfer.
Figure 3. Linked LF for Haga clic en el bot?n de
opci?n.
5.3 Transfer
The responsibility of transfer is to take a linked
LF from MindMeld and create a target LF that
will be the basis for the target translation. This
is accomplished through a top down traversal of
the linked LF in which the target LF segments
pointed to by Links on the source LF nodes are
stitched together. When stitching together LF
segments from possibly complex multi-word
mappings, the sublinks set by MindMeld
between individual nodes are used to determine
correct attachment points for modifiers, etc.
Default attachment points are used if needed.
Also, a very small set of simple, general, hand-
coded transfer rules (currently four for English
to/from Spanish) may apply to fill current (and
we hope, temporary) gaps in learned transfer
mappings.
In cases where no applicable transfer
mapping was found during MindMeld, the
nodes in the source LF and their relations are
simply copied into the target LF. Default (i.e.,
most commonly occurring) single word
translations may still be found in the MindNet
for these nodes and inserted in the target LF, but
if not, translations are obtained, if possible, from
the same bilingual dictionary used during LF
alignment.
Figure 4 shows the target LF created by
transfer from the linked LF shown in Figure 3.
Figure 4. Target LF for Click the option button.
5.4 Generation
A rule-based generation component maps
from the target LF to the target string (Aikawa
et al 2001). The generation components for the
target languages currently handled by MSR-MT
are application-independent, having been
designed to apply to a range of tasks, including
question answering, grammar checking, and
translation. In its application to translation,
generation has no information about the source
language for a given input LF, working
exclusively with the information passed to it by
the transfer component. It uses this information,
in conjunction with a monolingual (target
language) dictionary to produce its output. One
generic generation component is thus sufficient
for each language.
In some cases, transfer produces an
unmistakably ?non-native? target LF. In order to
correct some of the worst of these anomalies, a
small set of source-language independent rules
is applied prior to generation. The need for such
rules reflects deficiencies in our current data-
driven learning techniques during transfer.
6 Evaluating MSR-MT
In evaluating progress, we have found no
effective alternative to the most obvious
solution: periodic, blind human evaluations
focused on translations of single sentences. The
human raters used for these evaluations work for
an independent agency and played no
development role building the systems they test.
Each language pair under active development is
periodically subjected to the evaluation process
described in this section.
6.1 Evaluation Methodology
For each evaluation, five to seven evaluators
are asked to evaluate the same set of 200 to 250
blind test sentences. For each sentence, raters
are presented with a reference sentence in the
target language, which is a human translation of
the corresponding source sentence. In order to
maintain consistency among raters who may
have different levels of fluency in the source
language, raters are not shown the source
sentence. Instead, they are presented with two
machine-generated target translations presented
in random order: one translation by the system
to be evaluated (the experimental system), and
another translation by a comparison system (the
control system). The order of presentation of
sentences is also randomized for each rater in
order to eliminate any ordering effect.
Raters are asked to make a three-way choice.
For each sentence, raters may choose one of the
two automatically translated sentences as the
better translation of the (unseen) source
sentence, assuming that the reference sentence
represents a perfect translation, or, they may
indicate that neither of the two is better. Raters
are instructed to use their best judgment about
the relative importance of fluency/style and
accuracy/content preservation. We chose to use
this simple three-way scale in order to avoid
making any a priori judgments about the
relative importance of these parameters for
subjective judgments of quality. The three-way
scale also allows sentences to be rated on the
same scale, regardless of whether the
differences between output from system 1 and
system 2 are substantial or negligible.
The scoring system is similarly simple; each
judgment by a rater is represented as 1 (sentence
from experimental system judged better), 0
(neither sentence judged better), or -1 (sentence
from control system judged better). For each
sentence, the score is the mean of all raters?
judgments; for each comparison, the score is the
mean of the scores of all sentences.
6.2 Evaluation results
Although work on MSR-MT encompasses a
number of language pairs, we focus here on the
evaluation of just two, Spanish-English and
English-Spanish. Training data was held
constant for each of these evaluations.
6.2.1 Spanish-English over time
Spanish-English
systems
Mean preference
score (7 raters)
Sample
size
MSR-MT 9/00
vs.
MSR-MT 12/00
0.30 ? 0.09
(at 0.95)
200
sentences
MSR-MT 12/00
vs.
MSR-MT 4/01
0.28 ? 0.07
(at 0.99)
250
sentences
This table summarizes two evaluations
tracking progress in MSR-MT?s Spanish-
English (SE) translation quality over a seven
month development period. The first evaluation,
with seven raters, compared a September 2000
version of the system to a December 2000
version. The second evaluation, carried out by
six raters, examined progress between
December 2000 and April 2001.
A score of -1 would mean that raters
uniformly preferred the control system, while a
score of 1 would indicate that all raters preferred
the comparison system for all sentences. In each
of these evaluations, all raters significantly
preferred the comparison, or newer, version of
MSR-MT, as reflected in the mean preference
scores of 0.30 and 0.28. These numbers confirm
that the system made considerable progress over
a relatively short time span.
6.2.2 Spanish-English vs. alternative system
Spanish-English
systems
Mean preference
score (7 raters)
Sample
size
MSR-MT 9/00 vs.
Babelfish
-0.23 ? 0.12
(at 0.95)
200
sentences
MSR-MT 12/00
vs. Babelfish
0.11 ? 0.10
(at 0.95)
200
sentences
MSR-MT 4/01 vs.
Babelfish
0.32 ? 0.11
(at .99)
250
sentences
This table summarizes our comparison of
MSR-MT?s Spanish-English (SE) output to the
output of Babelfish (http://world.altavista.com/).
Three separate evaluations were performed, in
order to track MSR-MT?s progress over seven
months. The first two evaluations involved
seven raters, while the third involved six.
The shift in the mean preference score from
-0.23 to 0.32 shows clear progress against
Babelfish; by the second evaluation, raters very
slightly preferred MSR-MT in this domain. By
April, all six raters strongly preferred MSR-MT.
6.2.3 English-Spanish vs. alternative system
English-Spanish
systems
Mean preference
score (5 raters)
Sample
size
MSR-MT 2/01
vs. L&H
0.078 ? 0.13
(at 0.95)
250
sentences
MSR-MT 4/01
vs. L&H
0.19 ? 0.14
(at 0.99)
250
sentences
The evaluations summarized in this table
compared February and April 2001 versions of
MSR-MT?s English-Spanish (ES) output to the
output of the Lernout & Hauspie (L&H) ES
system (http://officeupdate.lhsl.com/) for 250
source sentences. Five raters participated in the
first evaluation, and six in the second.
The mean preference scores show that by
April, MSR-MT was strongly preferred over
L&H. Interestingly, though, one rater who
participated in both evaluations maintained a
slight but systematic preference for L&H?s
translations. Determining which aspects of the
translations might have caused this rater to
behave differently from the others is a topic for
future investigation.
6.3 Discussion
These results document steady progress in
the quality of MSR-MT?s output over a
relatively short time. By April 2001, both the SE
and ES versions of the system had surpassed
Babelfish in translation quality for this domain.
While these versions of MSR-MT are the most
fully developed, the other language pairs under
development are also progressing rapidly.
In interpreting our results, it is important to
keep in mind that MSR-MT has been
customized to the test domain, while the
Babelfish and Lernout & Hauspie systems have
not.2 This certainly affects our results, and
2Babelfish was chosen for these comparisons only
after we experimentally compared its output to that
of the related Systran system augmented with its
computer domain dictionary. Surprisingly, the
means that our comparisons have a certain
asymmetry. As our work progresses, we hope to
evaluate MSR-MT against a quality bar that is
perhaps more meaningful: the output of a
commercial system that has been hand-
customized for a specific domain.
The asymmetrical nature of our comparison
cuts both ways, however. Customization
produces better translations, and a system that
can be automatically customized has an inherent
advantage over one that requires laborious
manual customization. Comparing an
automatically-customized version of MSR-MT
to a commercial system which has undergone
years of hand-customization will represent a
comparison that is at least as asymmetrical as
those we have presented here.
We have another, more concrete, purpose in
regularly evaluating our system relative to the
output of systems like Babelfish and L&H: these
commercial systems serve as (nearly) static
benchmarks that allow us to track our own
progress without reference to absolute quality.
7 Conclusions and Future Work
This paper has described MSR-MT, an
EBMT system that produces MT output whose
quality in a specific domain exceeds that of
commercial MT systems, thus attacking head-on
the customization bottleneck. This work
demonstrates that automatic data-driven
methods can provide commercial-quality MT.
In future work we hope to demonstrate that
MSR-MT can be rapidly adapted to very
different semantic domains, and that it can
compete in translation quality even with
commercial systems that have been hand-
customized to a particular domain.
Acknowledgements
We would like to acknowledge the efforts of
the MSR NLP group in carrying out this work.
References
Aikawa, T., M. Melero, L. Schwartz, and A. Wu
2001 ?Multilingual natural language
generation,? Proceedings of 8th European
Workshop on Natural Language Generation,
Toulouse.
generic SE Babelfish engine produced slightly better
translations of our technical data.
Brown, R. 1999. ?Adding linguistic knowledge
to a lexical example-based translation
system,? Proceedings of TMI 99.
Franz, A., K. Horiguchi, L. Duan, D. Ecker, E.
Koontz, and K. Uchida 2000. ?An integrated
architecture for example-based machine
translation,? Proceedings of COLING2000.
Frederking, R., S. Nirenburg, D. Farwell, S.
Helmreich, E. Hovy, K. Knight, S. Beale, C.
Domashnev, D. Attardo, D. Grannes, and R.
Brown 1994. ?Integrating translations from
multiple sources within the Pangloss Mark
III machine translation system,? Proceedings
of AMTA94.
Heidorn, G., K. Jensen, S. Richardson, and A.
Viesse 2000. In R. Dale, H. Moisl and H.
Somers (eds) Handbook of Natural Language
Processing. Marcel Dekker Inc.
Meyers, A., M. Kosaka, and R. Grishman. 2000.
?Chart-based transfer rule application in
machine translation,? Proceedings of
COLING98.
Menezes, A. and S. Richardson 2001. ?A best-
first alignment algorithm for automatic
extraction of transfer mappings from
bilingual corpora,? Proceedings of the
Workshop on Data-Driven Machine
Translation, ACL 2001.
Moore, R. 2001 ?Towards a Simple and
Accurate Statistical Approach to Learning
Translation Relationships Among Words,?
Proceedings of the Workshop on Data-
Driven Machine Translation, ACL 2001.
Pinkham, J and M. Corston-Oliver 2001
?Adding Domain Specificity to an MT
system,? Proceedings of the Workshop on
Data-Driven Machine Translation, ACL
2001.
Richardson, S. D., W. Dolan, and L.
Vanderwende 1998. ?MindNet: Acquiring
and Structuring Semantic Information from
Text,? Proceedings of COLING-ACL ?98,
Montreal.
Takeda, K., N. Uramoto, T. Nasukawa, and T.
Tsutsumi 1992. ?Shalt 2?a symmetric
machine translation system with conceptual
transfer,? Proceedings of COLING92.
Turcato, D., P. McFetridge, F. Popowich, and J.
Toole 1999. ?A unified example-based and
lexicalist approach to machine translation,?
Proceedings of TMI 99.
Watanabe, W. Kurohashi, S. and E. Aramaki
2000. ?Finding structural correspondences
from bilingual parsed corpus for corpus-
based translation,? Proceedings of
COLING2000.
Watanabe, H. and K. Takeda 1998. ?A pattern-
based machine translation system extended
by example-based processing,? Proceedings
of COLING98.
A best-first alignment algorithm for automatic extraction of transfer
mappings from bilingual corpora
Arul Menezes and Stephen D. Richardson
Microsoft Research
One Microsoft Way
Redmond, WA 98008, USA
arulm@microsoft.com
steveri@microsoft.com
Abstract
Translation systems that automatically
extract transfer mappings (rules or
examples) from bilingual corpora have
been hampered by the difficulty of
achieving accurate alignment and
acquiring high quality mappings. We
describe an algorithm that uses a best-
first strategy and a small alignment
grammar to significantly improve the
quality of the transfer mappings
extracted. For each mapping,
frequencies are computed and sufficient
context is retained to distinguish
competing mappings during translation.
Variants of the algorithm are run
against a corpus containing 200K
sentence pairs and evaluated based on
the quality of resulting translations.
1 Introduction
A machine translation system requires a
substantial amount of translation knowledge
typically embodied in bilingual dictionaries,
transfer rules, example bases, or a statistical
model. Over the last decade, research has
focused on the automatic acquisition of this
knowledge from bilingual corpora. Statistical
systems build translation models from this data
without linguistic analysis (Brown, 1993).
Another class of systems, including our own,
parses sentences in parallel sentence-aligned
corpora to extract transfer rules or examples
(Kaji, 1992) (Meyers, 2000) (Watanabe, 2000).
These systems typically obtain a predicate-
argument or dependency structure for source
and target sentences, which are then aligned,
and from the resulting alignment, lexical and
structural translation correspondences are
extracted, which are then represented as a set of
rules or an example-base for translation.
However, before this method of knowledge
acquisition can be fully automated, a number of
issues remain to be addressed. The alignment
and transfer-mapping acquisition procedure
must acquire rules with very high precision. It
must be robust against errors introduced by
parsing and sentence-level alignment, errors
intrinsic to the corpus, as well as errors resulting
from the alignment procedure itself. The
procedure must also produce transfer mappings
that provide sufficient context to enable the
translation system utilizing these mappings to
choose the appropriate translation for a given
context.
In this paper, we describe the alignment and
transfer-acquisition algorithm used in our
machine translation system, which attempts to
address the issues raised above. This system
acquires transfer mappings by aligning pairs of
logical form structures (LFs) similar to those
described by Jensen (1993). These LFs are
obtained by parsing sentence pairs from a
sentence-aligned bilingual corpus. (The problem
of aligning parallel corpora at the sentence level
has been addressed by Meyers (1998b) Chen
(1993) and others and is beyond the scope of
this paper).
We show that alignment using a best-first
strategy in conjunction with a small alignment
grammar improves the alignment and the quality
of the acquired transfer mappings.
hacer
usted
informaci?n
hiperv?nculo hiperv?nculo
clic
direcci?n
click
Hyperlink_Information
you
address
hyperlink
de de
en en
under
Dsub Dobj
Mod
Dsub
Dobj
Figure 1a: Lexical correspondences Figure 1b: Alignment Mappings
hacer
usted
informaci?n
hiperv?nculo hiperv?nculo
clic
direcci?n
click
Hyperlink_Information
you
address
hyperlink
de de
en en
under
Dsub Dobj
Mod
Dsub
Dobj
2 Logical Form
A Logical Form (LF) is an unordered graph
representing the relations among the most
meaningful elements of a sentence. Nodes are
identified by the lemma of a content word and
directed, labeled arcs indicate the underlying
semantic relations. Logical Forms are intended
to be as independent as possible of specific
languages and their grammars. In particular,
Logical Forms from different languages use the
same relation types and provide similar analyses
for similar constructions. The logical form
abstracts away from such language-particular
aspects of a sentence as constituent order,
inflectional morphology, and certain function
words.
Figure 1a illustrates the LFs for the
following Spanish sentence and its
corresponding English translation, which we use
in example below.
En Informaci?n del hiperv?nculo, haga clic en
la direcci?n del hiperv?nculo.
Under Hyperlink Information, click the
hyperlink address.
3 Alignment
We consider an alignment of two logical forms
to be a set of mappings, such that each mapping
is between a node or set of nodes (and the
relations between them) in the source LF and a
node or set of nodes (and the relations between
them) in the target LF, where no node
participates in more than one such mapping. In
other words, we allow one-to-one, one-to-many,
many-to-one and many-to-many mappings but
the mappings do not overlap.
Our alignment algorithm proceeds in two
phases. The first phase establishes tentative
lexical correspondences between nodes in the
source and target LFs. The second phase aligns
nodes based on these lexical correspondences as
well as structural considerations. The algorithm
starts from the nodes with the tightest lexical
correspondence (?best-first?) and works outward
from these anchor points.
We first present the algorithm, and then
illustrate how it applies to the sentence-pair in
Figure-1.
3.1 Finding tentative lexical
correspondences
We use a bilingual lexicon that merges data
from several sources (CUP, 1995), (SoftArt,
1995), (Langenscheidt, 1997), and inverts
target-to-source dictionaries to improve
coverage. Our Spanish-English lexicon contains
88,500 translation pairs. We augment this with
19,762 translation correspondences acquired
using statistical techniques described by Moore
(2001).
Like Watanabe (2000) and Meyers (2000),
we use a lexicon to establish initial tentative
word correspondences. However, we have found
that even a relatively large bilingual dictionary
has only moderately good coverage for our
purposes. Hence, we pursue an aggressive
matching strategy for establishing tentative
word correspondences. Using the bilingual
dictionary together with the derivational
morphology component in our system
(Pentheroudakis, 1993), we find direct
translations, translations of morphological bases
and derivations, and base and derived forms of
translations. Fuzzy string matching is also used
to identify possible correspondences. We have
found that aggressive over-generation of
correspondences at this phase is balanced by the
more conservative second phase and results in
improved overall alignment quality.
We also look for matches between
components of multi-word expressions and
individual words. This allows us to align such
expressions that may have been analyzed as a
single lexicalized entity in one language but as
separate words in the other.
3.2 Aligning nodes
Our alignment procedure uses the tentative
lexical correspondences established above, as
well as structural cues, to create affirmative
node alignments. A set of alignment grammar
rules licenses only linguistically meaningful
alignments. The rules are ordered to create the
most unambiguous alignments (?best?) first and
use these to disambiguate subsequent
alignments. The algorithm and the alignment
grammar rules are intended to be applicable
across multiple languages. The rules were
developed while working primarily with a
Spanish-English corpus, but have also been
applied to other language pairs such as French,
German, and Japanese to/from English.
The algorithm is as follows:
1. Initialize the set of unaligned source and
target nodes to the set of all source and
target nodes respectively.
2. Attempt to apply the alignment rules in the
specified order, to each unaligned node or
set of nodes in source and target. If a rule
fails to apply to any unaligned node or set of
nodes, move to the next rule.
3. If all rules fail to apply to all nodes, exit. No
more alignment is possible. (Note: some
nodes may remain unaligned).
4. When a rule applies, mark the nodes or sets
of nodes to which it applied as aligned to
each other and remove them from the lists
of unaligned source and target nodes
respectively. Go to step 2 and apply rules
again, starting from the first rule.
The alignment grammar currently consists
of 18 rules. Below we provide the specification
for some of the most important rules.
1. Bidirectionally unique translation: A set of
contiguous source nodes S and a set of
contiguous target nodes T such that every
node in S has a lexical correspondence with
every node in T and with no other target
node, and every node in T has a lexical
correspondence with every node in S and
with no other source node. Align S and T to
each other.
2. Translation + Children: A source node S
and a target node T that have a lexical
correspondence, such that each child of S
and T is already aligned to a child of the
other. Align S and T to each other.
3. Translation + Parent: A source node S and
a target node T that have a lexical
correspondence, such that a parent Ps of S
has already been aligned to a parent Pt of T.
Align S and T to each other.
4. Verb+Object to Verb: A verb V1 (from
either source or target), that has child O that
is not a verb, but is already aligned to a verb
V2, and either V2 has no unaligned parents,
or V1 and V2 have children aligned to each
other. Align V1+O to V2.
5. Parent + relationship: A source node S and
a target node T, with the same part-of-
speech, and no unaligned siblings, where a
parent Ps of S is already aligned to a parent
Pt of T, and the relationship between Ps and
S is the same as that between Pt and T.
Align S and T to each other.
6. Child + relationship: Analogous to previous
rule but based on previously aligned
children instead of parents.
Note that rules 4-6 do not exploit lexical
correspondence, relying solely on relationships
between nodes being examined and previously
aligned nodes.
3.3 Alignment Example
In this section, we illustrate the application of
the alignment procedure to the example in
Figure 1. In the first phase, using the bilingual
lexicon, we identify the lexical correspondences
depicted in Figure-1a as dotted lines. Note that
each of the two instances of hiperv?nculo has
two ambiguous correspondences, and that while
the correspondence from Informaci?n to
Hyperlink Information is unique, the reverse is
not. Note also that neither the monolingual nor
bilingual lexicons have been customized for this
domain. For example, there is no entry in either
lexicon for Hyperlink_Information. This unit has
been assembled by general-purpose "Captoid"
grammar rules. Similarly, lexical
correspondences established for this unit are
based on translations found for its individual
components, there being no lexicon entry for the
captoid as a whole.
In the next phase, the alignment rules apply
to create alignment mappings depicted in
Figure-1b as dotted lines.
Rule-1: Bidirectionally unique translation,
applies in three places, creating alignment
mappings between direcci?n and address,
usted and you, and clic and click. These are
the initial ?best? alignments that provide the
anchors from which we will work outwards to
align the rest of the structure.
Rule-3: Translation + Parent, applies next to
align the instance of hiperv?nculo that is the
child of direcci?n to hyperlink, which is the
child of address. We leverage a previously
created alignment (direcci?n to address) and
the structure of the logical form to resolve the
ambiguity present at the lexical level.
Rule-1 now applies (where previously it did not)
to create a many-to-one mapping between
informaci?n and hiperv?nculo to
Hyperlink_Information. The uniqueness
condition in this rule is now met because the
ambiguous alternative was cleared away by
the prior application of Rule-3.
Rule-4: Verb+Object to Verb applies to rollup
hacer with its object clic, since the latter is
already aligned to a verb. This produces the
many-to-one alignment of hacer and clic to
click
4 Acquiring Transfer Mappings
Figure-2 shows the transfer mappings derived
from the alignment example in Figure-1.
informaci?n
hiperv?nculo
de Hyperlink_Information
direcci?n
hiperv?nculo
de
address
hyperlink
Mod
hacer
Pron) clic
enDsub Dobj
(Noun)
hacer
Pron) clic
enDsub Dobj DobjDsub
click
direcci?n address(Pron)
informaci?n
hiperv?nculo
de
Hyperlink_Inform ation
under
(Verb)en
(Verb)
direcci?n address
hiperv?nculo hyperlink
DobjDsub
click
(Noun)(Pron)
Figure-2 : Transfer mappings acquired
4.1 Transfer mappings with context
Each mapping created during alignment forms
the core of a family of mappings emitted by the
transfer mapping acquisition procedure. The
alignment mapping by itself represents a
minimal transfer mapping with no context. In
addition, we emit multiple variants, each one
expanding the core mapping with varying types
and amounts of local context.
We use linguistic constructs such as noun
and verb phrases to provide the boundaries for
the context we include. For example, the
transfer mapping for an adjective is expanded to
include the noun it modifies; the mapping for a
modal verb is expanded to include the main
verb; the mapping for a main verb is expanded
to include its object; mappings for collocations
of nouns are emitted individually and as a
whole. Mappings may include ?wild card? or
under-specified nodes, with a part of speech, but
no lemma, as shown in Figure 2.
4.2 Alignment Post-processing
After we have acquired transfer mappings from
our entire training corpus, we compute
frequencies for all mappings. We use these to
resolve conflicting mappings, i.e. mappings
where the source sides of the mapping are
identical, but the target sides differ. Currently
we resolve the conflict by simply picking the
most frequent mapping. Note that this does not
imply that we are committed to a single
translation for every word across the corpus,
since we emitted each mapping with different
types and amounts of context (see section 4.1).
Ideally at least one of these contexts serves to
disambiguate the translation. The conflicts being
resolved here are those mappings where the
necessary context is not present.
A drawback of this approach is that we are
relying on a priori linguistic heuristics to ensure
that we have the right context. Our future work
plans to address this by iteratively searching for
the context that serves to optimally
disambiguate (across the entire training corpus)
between conflicting mappings.
4.2.1 Frequency Threshold
During post-processing we also apply a
frequency threshold, keeping only mappings
seen at least N times (where N is currently 2).
This frequency threshold greatly improves the
speed of the runtime system, with negligible
impact on translation quality (see section 5.6).
5 Experiments and Results
5.1 Evaluation methodology
In the evaluation process, we found that various
evaluation metrics of alignment in isolation bore
very little relationship to the quality of the
translations produced by a system that used the
results of such alignment. Since it is the overall
translation quality that we care about, we use the
output quality (as judged by humans) of the MT
system incorporating the transfer mappings
produced by an alignment algorithm (keeping all
other aspects of the system constant) as the
metric for that algorithm.
5.2 Translation system
Our translation system (Richardson, 2001)
begins by parsing an input sentence and
obtaining a logical form. We then search the
transfer mappings acquired during alignment,
for mappings that match portions of the input
LF. We prefer larger (more specific) mappings
to smaller (more general) mappings. Among
mappings of equal size, we prefer higher-
frequency mappings. We allow overlapping
mappings that do not conflict. The lemmas in
any portion of the LF not covered by a transfer
mapping are translated using the same bilingual
dictionary employed during alignment, or by a
handful of hard-coded transfer rules (see Section
5.7 for a discussion of the contribution made by
each of these components). Target LF fragments
from matched transfer mappings and default
dictionary translations are stitched together to
form an output LF. From this, a rule-based
generation component produces an output
sentence.
The system provides output for every input
sentence. Sentences for which spanning parses
are not found are translated anyway, albeit with
lower quality.
5.3 Training corpus
We use a sentence-aligned Spanish-English
training corpus consisting of 208,730 sentence
pairs mostly from technical manuals. The data
was already aligned at the sentence-level since it
was taken from sentence-level translation
memories created by human translators using a
commercial translation-memory product. This
data was parsed and aligned at the sub-sentence
level by our system, using the techniques
described in this paper. Our parser produces a
parse in every case, but in each language
roughly 15% of the parses produced are ?fitted?
or non-spanning. Since we have a relatively
large training corpus, we apply a conservative
heuristic and only use in alignment those
sentence-pairs that produced spanning parses in
both languages. In this corpus 161,606 pairs (or
77.4% of the corpus) were used. This is a
substantially larger training corpus than those
used in previous work on learning transfer
mappings from parsed data. Table-1 presents
some data on the mappings extracted from this
corpus using Best-First.
Total Sentence pairs 208,730
Sentence pairs used 161,606
Number of transfer mappings 1,202,828
Transfer mappings per pair 7.48
Num. unique transfer mappings 437,479
Num. unique after elim. conflicts 369,067
Num. unique with frequency > 1 58,314
Time taken to align entire corpus
(on a 800MHz PC)
74 minutes
Alignment speed 35.6 sent/s
Table-1: Best-first alignment of training corpus
5.4 Experiments
In each experiment we used 5 human evaluators
in a blind evaluation, to compare the translations
produced by the test system with those produced
by a comparison system. Evaluators were
presented, for each sentence, with a reference
human translation and with the two machine
translations in random order, but not the original
source language sentence. They were asked to
pick the better overall translation, taking into
account both content and fluency. They were
allowed to choose ?Neither? if they considered
both translations equally good or equally bad.
All the experiments were run with our
Spanish-English system. The test sentences were
randomly chosen from unseen data from the
same domain. Experiment-1 used 200 sentences
and each sentence was evaluated by all raters.
Sentences were rated better for one system or
the other if a majority of the raters agreed.
Experiments 2-4 used 500 sentences each, but
each sentence was rated by a single rater.
In each experiment, the test system was the
system described in section 5.2, loaded with
transfer mappings acquired using the techniques
described in this paper (hereafter ?Best-First?).
5.5 Comparison systems
In the first experiment the comparison system is
a highly rated commercial system, Babelfish
(http://world.altavista.com).
Each of the next three experiments varies
some key aspect of Best-First in order to explore
the properties of the algorithm.
5.5.1 Bottom Up
Experiment-2 compares Best-First to the
previous algorithm we employed, which used a
bottom-up approach, similar in spirit to that used
by Meyers (1998a).
This algorithm follows the procedure
described in section 3.1 to establish tentative
lexical correspondences. However, it does not
use an alignment grammar, and relies on a
bottom-up rather than a best-first strategy. It
starts by aligning the leaf nodes and proceeds
upwards, aligning nodes whose child nodes have
already aligned. Nodes that do not align are
skipped over, and later rolled-up with ancestor
nodes that have successfully aligned.
5.5.2 No Context
Experiment-3 uses a comparison algorithm that
differs from Best First in that it retains no
context (see section 4.1) when emitting transfer
mappings.
5.5.3 No Threshold
The comparison algorithm used in Experiment-4
differs from Best First in that the frequency
threshold (see section 4.2.1) is not applied, i.e.
all transfer mappings are retained.
Comparison
System
Num. sentences
Best-First rated
better
Num. sentences
comparison
system rated better
Num. sentences
neither rated better
Net percentage
improvement
Babelfish 93 (46.5%) 73 (36.5%) 34 (17%) 10.0%
Bottom-Up 224 (44.8%) 111 (22.2%) 165 (33%) 22.6%
No-Context 187 (37.4%) 69 (13.8%) 244 (48.8%) 23.6%
No-Threshold 112 (22.4%) 122 (24.4%) 266 (53.2%) -2.0%
Table-2: Translation Quality
5.6 Discussion
The results of the four experiments are
presented in Table-2.
Experiment-1 establishes that the algorithm
presented in this paper automatically acquires
translation knowledge of sufficient quantity and
quality as to enable translations that exceed the
quality of a highly rated traditional MT system.
Note however that Babelfish/Systran was not
customized to this domain.
Experiment-2 shows that Best-First
produces transfer mappings resulting in
significantly better translations than Bottom-Up.
Using Best-First produced better translations for
a net of 22.6% of the sentences.
Experiment-3 shows that retaining sufficient
context in transfer mappings is crucial to
translation quality, producing better translations
for a net of 23.6% of the sentences.
Experiment-4 shows that the frequency
threshold hurts translation quality slightly (a net
loss of 2%), but as Table-3 shows it results in a
much smaller (approx. 6 times) and faster
(approx 45 times) runtime system.
Num
mappings
Translation speed
(500 sentences)
Best-First 58,314 173s (0.34s/sent)
No-Threshold 359,528 8059s (17s/sent)
Table-3: Translation Speed (500 sentences)
5.7 Transfer mapping coverage
Using end-to-end translation quality as a metric
for alignment leaves open the question of how
much of the translation quality derives from
alignment versus other sources of translation
knowledge in our system, such as the bilingual
dictionary, or the 2 hand-coded transfer rules in
our system. To address this issue we measured
the contribution of each using a 3264-sentence
test set. Table-4 presents the results. The first
column indicates the total number of words in
each category. The next four columns indicate
the percentage translated using each knowledge
source, and the percentage not translated
respectively.
As the table shows, the vast majority of
content words get translated using transfer-
mappings obtained via alignment.
Our alignment algorithm does not explicitly
attempt to learn transfer mappings for pronouns,
but pronouns are sometimes included in transfer
mappings when they form part of the context
that is included with each mapping (see section
4.1). The 31.89% of pronoun translations that
the table indicates as coming from alignment
fall into this category.
Our algorithm does try to learn transfer
mappings for prepositions and conjunctions,
which are represented in the Logical Form as
labels on arcs (see Figure-1). Mappings for
prepositions and conjunctions always include
the nodes on both ends of this arc. These
mappings may translate a preposition in the
source language to a preposition in the target
language, or to an entirely different relationship,
such as direct object, indirect object, modifier
etc.
As the table shows, the system is currently
less successful at learning transfer mappings for
prepositions and conjunctions than it is for
content words.
As a temporary measure we have 2 hand-
coded transfer rules that apply to prepositions,
which account for 8.4% of such transfers. We
intend for these to eventually be replaced by
mappings learned from the data.
Number of
instances
Alignment Dictionary Rules Not
translated
Content words 21,245 93.50% 4.10% 0% 2.4%
Pronouns 2,158 31.89% 68.20% 0% 0%
Prepositions/Conjunctions 6,640 32.00% 59.70% 8.4% 0%
Table-4: Coverage of transfer mappings, dictionary & rules
6 Conclusions and Future Work
We proposed an algorithm for automatically
acquiring high-quality transfer mappings from
sentence-aligned bilingual corpora using an
alignment grammar and a best-first strategy.
We reported the results of applying the
algorithm to a substantially larger training
corpus than that used in previously reported
work on learning transfer mappings from parsed
data.
We showed that this approach produces
transfer mappings that result in translation
quality comparable to a commercial MT system
for this domain.
We also showed that a best-first, alignment-
grammar based approach produced better results
than a bottom-up approach, and that retaining
context in the acquired transfer mappings is
essential to translation quality.
We currently rely on a priori linguistic
heuristics to try to provide the right context for
each transfer mapping. In future work, we plan
to use machine-learning techniques to determine
the extent of the context that optimally
disambiguates between conflicting mappings.
References
Peter Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer, 1993. ?The
mathematics of statistical machine translation?
Computational Linguistics, 19:263-312
Cambridge University Press (1995), McCarthy, M.
ed., Cambridge Word Selector
Stanley F. Chen, 1993. ?Aligning sentences in
bilingual corpora using lexical information?
Proceedings of ACL 1993
Karen Jensen, 1993. ?PEGASUS: Deriving argument
structures after syntax.? In Natural Language
Processing: The PLNLP Approach. Kluwer
Academic Publishers, Boston, MA.
Hiroyuki Kaji, Yuuko Kida, and Yasutsugu
Morimoto, 1992. ?Learning Translation Templates
from Bilingual Text? Proceedings of COLING
1992
Langenscheidt Publishers 1997, The Langenscheidt
Pocket Spanish Dictionary
Adam Meyers, Roman Yangarber, Ralph Grishman,
Catherine Macleod, and Antonio Moreno-
Sandoval, 1998a. ?Deriving transfer rules from
dominance-preserving alignments?, Proceedings
of COLING 1998
Adam Meyers, Michiko Kosaka and Ralph
Grishman, 1998b. ?A multilingual procedure for
dictionary-based sentence alignment? Proceedings
of AMTA 98
Adam Meyers, Michiko Kosaka and Ralph
Grishman, 2000. ?Chart-based transfer rule
application in machine translation? Proceedings of
COLING 2000
Robert C. Moore 2001, ?Towards a Simple and
Accurate Statistical Approach to Learning
Translation Relationships among Words?
Proceedings of the Workshop on Data-Driven
Machine Translation, ACL 2001
Joseph Pentheroudakis and Lucretia Vanderwende
1993, ?Automatically identifying morphological
relations in machine-readable dictionaries? Ninth
Annual conference of the University of Waterloo
Center for the new OED and Text Research
Stephen D. Richardson, William Dolan, Monica
Corston-Oliver, and Arul Menezes 2001,
?Overcoming the customization bottleneck using
example-based MT?, Workshop on Data-Driven
Machine Translation, ACL 2001
SoftArt Inc (1995) Soft-Art translation dictionary.
Version 7
Hideo Watanabe, Sado Kurohashi, and Eiji Aramaki,
2000. ?Finding Structural Correspondences from
Bilingual Parsed Corpus for Corpus-based
Translation? Proceedings of COLING 2000
