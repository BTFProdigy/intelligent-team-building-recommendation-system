Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 89?96, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Maximum Entropy Word Aligner for Arabic-English Machine
Translation
Abraham Ittycheriah and Salim Roukos
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
{abei,roukos}@us.ibm.com
Abstract
This paper presents a maximum entropy
word alignment algorithm for Arabic-
English based on supervised training data.
We demonstrate that it is feasible to cre-
ate training material for problems in ma-
chine translation and that a mixture of su-
pervised and unsupervised methods yields
superior performance. The probabilistic
model used in the alignment directly mod-
els the link decisions. Significant improve-
ment over traditional word alignment tech-
niques is shown as well as improvement on
several machine translation tests. Perfor-
mance of the algorithm is contrasted with
human annotation performance.
1 Introduction
Machine translation takes a source sequence,
S = [s1 s2 . . . sK ]
and generates a target sequence,
T = [t1 t2 . . . tM ]
that renders the meaning of the source sequence into
the target sequence. Typically, algorithms operate
on sentences. In the most general setup, one or more
source words can generate 0, 1 or more target words.
Current state of the art machine translation systems
(Och, 2003) use phrasal (n-gram) features extracted
automatically from parallel corpora. These phrases
are extracted using word alignment algorithms that
are trained on parallel corpora. Phrases, or phrasal
features, represent a mapping of source sequences
into a target sequences which are typically a few
words long.
In this paper, we investigate the feasibility of train-
ing alignment algorithms based on supervised align-
ment data. Although there is a modest cost associ-
ated with annotating data, we show that a reduction
of 40% relative in alignment error (AER) is possible
over the GIZA++ aligner (Och and Ney, 2003).
Although there are a number of other applications
for word alignment, for example in creating bilingual
dictionaries, the primary application continues to be
as a component in a machine translation system. We
test our aligner on several machine translation tests
and show encouraging improvements.
2 Related Work
Most of the prior work on word alignments has been
done on parallel corpora where the alignment at the
sentence level is also done automatically. The IBM
models 1-5 (Brown et al, 1993) produce word align-
ments with increasing algorithmic complexity and
performance. These IBM models and more recent
refinements (Moore, 2004) as well as algorithms that
bootstrap from these models like the HMM algo-
rithm described in (Vogel et al, 1996) are unsuper-
vised algorithms.
The relative success of these automatic techniques
together with the human annotation cost has delayed
the collection of supervised word-aligned corpora for
more than a decade.
(Cherry and Lin, 2003) recently proposed a di-
rect alignment formulation and state that it would
be straightforward to estimate the parameters given
a supervised alignment corpus. In this paper, we ex-
tend their work and show that with a small amount
of annotated data, together with a modeling strat-
egy and search algorithm yield significant gains in
alignment F-measure.
89
show
vAny +pAl#
AlvAnyp
secondWords
WordNet
the
2nd
2d
pointed
+pwvyqAl#+tA$Arw#
wA$Art AlwvyqpWords
Segm.
to
Aly
Aly
Source
Target
papers
document
indicate
point
Figure 1: Alignment example.
3 Algorithm
In order to describe the algorithm, we will need to
first describe the direct link model. Figure 1 shows
two sequences where the top sequence is considered
the source sequence and the bottom sequence the
target sequence. Each sequence can have auxilliary
information such as Arabic segmentation or English
WordNet (Miller, 1990) information as shown. Given
the source and target sequences, there are a number
of different ways to link each target word to a source
word. Each target word has a link li which indi-
cates which source position it links to. The range
of li is from 0 to K and there are M of these links.
The source word position 0 is used to indicate NULL
which we imagine gives rise to unaligned English
words. In this paper, we refer to these words as be-
ing spontaneous. A valid link configuration has M
links. Define L to be the set of all possible valid link
configurations, and L to be a member of that set.
We seek to maximize the alignment probability by
finding the optimum link configuration Lopt,
p(Lopt|S, T ) = argmax
L?L
p(L|S, T )
= p(lMi |tM1 , sK1 )
=
M
?
i=0
p(li|tM1 , sK1 , li?11 ).
We factor this into a transition model and an obser-
vation model,
p(L|S, T ) = 1Z
M
?
i=0
p(li|li?1)?p(li|tM1 , sK1 , li?11 )1??.
where Z is the normalizing constant.
We factor the model as above so that the tran-
sition model computation, which uses information
available on the search hypotheses, is reduced during
the search process. In the aligner presented here, ?
is always set to 0.5. Next we will describe the tran-
sition model, then the observation model and finally
the experiments in alignment and machine transla-
tion.
In the IBM Model 1 aligner, the choice of the lan-
guage to serve as states of the search algorithm is not
prescribed, but practically the choice is important as
it affects performance. To see this, note that in gen-
erative models an input word can only be aligned to
a single state in the search. In our current situa-
tion, we are interested in aligning unsegmented Ara-
bic words and typical words have a few affixes to
indicate for example pronouns, definiteness, prepo-
sitions and conjunctions. In English these are sepa-
rate words, and therefore to maximize performance
the unsegmented Arabic words serve as states in the
search algorithm and we align English words to these
states.
3.1 Transition Model
The transition model tends to keep the alignments
close together and penalizes alignments in which ad-
jacent words in the target language come from very
distant words in the source language. Also, we would
like to penalize many English words coming from the
same Arabic state; we call this the state visit penalty
and will be described later. In this paper, we use a
parametric form for the transition model,
p(li|li?1) =
1
Z(li?1)
[ 1
dist(li, li?1)
+ 1ns(li)
]
(1)
90
where ns(i) represents the state visit penalty for
state i, Z(li?1) is the normalization constant and
dist(li, li?1) = min(|li ? li?1|, |li ? fi|) + a.
Here a is a penalty for a zero distance transition and
is set to 1 in the experiments below. The min op-
erator chooses the lowest cost transition distance ei-
ther from the previous state or the frontier state, fi,
which is the right most state that has been visited
(even though Arabic is normally displayed right to
left, we make our Arabic state graphs from left to
right). This is a language specific criteria and in-
tended to model the adjective noun reversal between
English and Arabic. Once the current noun phrase
is completed, the next word often aligns to the state
just beyond frontier state. As an example, in Fig-
ure 1, the verb ?pointed? aligns to the first Arabic
word ?wA$Art?, and aligning the ?to? to its Arabic
counterpart ?Aly? would incur normally a distance of
3 but with the frontier notion it incurs only a penalty
of 1 on the hypothesis that aligns the word ?second?
to ?AlvAnyp?. In this alignment with the frontier no-
tion, there are only distance 1 transitions, whereas
the traditional shapes would incur a penalty of 2 for
alignment of ?pointed? and a penalty of 3 for the word
?to?.
The state visit penalty, ns(i) is the distance be-
tween the English words aligned to this state times
the number of state visits1. This penalty controls
the fertility of the Arabic words. To determine the
English words that aligned to the Arabic position,
the search path is traced back for each hypothe-
sis and a sufficiently large beam is maintained so
that alignments in the future can correct past align-
ment decisions. This penalty allows English deter-
miners and prepositions to align to the Arabic con-
tent word while penalizing distant words from align-
ing to the state. In terms of alignment F-measure
to be described below, the state visit penalty, if re-
moved makes the performance degrade from F=87.8
to F=84.0 compared to removing the frontier notion
which only degrades performance to F=86.9.
3.2 Observation Model
The observation model measures the linkage of the
source and target using a set of feature functions
defined on the words and their context. In Figure 1,
an event is a single link from an English word to
an Arabic state and the event space is the sentence
pair. We use the maximum entropy formulation (e.g.
(Berger et al, 1996)),
1We are overloading the word ?state? to mean Arabic
word position.
f = ?(li)
h =
[
ti?11 , sK1
]
p(f |h) = 1Z(h) exp
?
i
?i?i(h, f),
where Z(h) is the normalizing constant,
Z(h) =
?
f
exp
?
i
?i?i(h, f).
and ?i(h, f) are binary valued feature functions. The
function ? selects the Arabic word at the position
being linked or in the case of segmentation features,
one of the segmentations of that position. We re-
strict the history context to select from the current
English word and words to the left as well as the
current word?s WordNet (Miller, 1990) synset as re-
quired by the features defined below. As in (Cherry
and Lin, 2003), the above functions simplify the con-
ditioning portion, h by utilizing only the words and
context involved in the link li. Training is done us-
ing the IIS technique (Della Pietra et al, 1995) and
convergence often occurs in 3-10 iterations. The five
types of features which are utilized in the system are
described below.
Phrase to phrase (for example, idiomatic phrases)
alignments are intepreted as each English word com-
ing from each of the Arabic words.
3.2.1 Lexical Features
The lexical features are similar to the translation
matrix of the IBM Model 1. However, there is a sign-
ficant out of vocabulary (OOV) issue in the model
since training data is limited. All words that have
a corpus frequency of 1 are left out of the model
and classed into an unknown word class in order to
explicitly model connecting unknown words. From
the training data we obtain 50K lexical features, and
applying the Arabic segmenter obtain another 17K
lexical features of the form ?(English content word,
Arabic stem).
3.2.2 Arabic Segmentation Features
An Arabic segmenter similar to (Lee et al, 2003)
provides the segmentation features. A small dictio-
nary is used (with 71 rules) to restrict the set of Ara-
bic segments that can align to English stopwords, for
example that ?the? aligns to ?Al#? and that ?for?, ?in?
and ?to? align to ?b#? and ?her? aligns with the suf-
fix ?+hA?. Segmentation features also help align un-
known words, as stems might be seen in the training
corpus with other prefixes or suffixes. Additionally,
the ability to align the prefix and suffix accurately,
tends to ?drag? the unknown stem to its English tar-
get.
91
3.2.3 WordNet Features
WordNet features provide normalization on the
English words. The feature is instantiated for nouns,
adjectives, adverbs and verbs following their defini-
tions in WordNet. If the Arabic word has a seg-
mentation then the feature is ?(WordNet synset id,
Arabic stem), otherwise it is ?(WordNet synset id,
Arabic word). The feature ties together English syn-
onyms and helps improve recall of the aligner.
3.2.4 Spelling Feature
The spelling feature is applied only on unknown
words and is used to measure the string kernel dis-
tance(Lodhi et al, 2000) between romanized Arabic
and English words. The feature is designed primar-
ily to link unknown names. For example, ?Clinton?
is written as ?klyntwn? in one of its romanized Ara-
bic versions. In a sentence, measuring the string ker-
nel distance shows a correlation between these names
even though there is not much overlap between the
characters. The feature has four possible values: no-
match, somematch, goodmatch, and exact.
3.2.5 Dynamic Features
Dynamic features are defined on the lattice of the
search algorithm. These features fire when the pre-
vious source and target word pair are linked. For
example, one such feature is ?b# in? and if on the
hypothesis we have just linked this pair and the next
English word is being aligned to the stem of the Ara-
bic word where this prefix occurs, this feature fires
and boosts the probability that the next words are
aligned. The basic intuition behind this feature is
that words inside prepositional phrases tend to align,
which is similar to the dependency structure feature
of (Cherry and Lin, 2003).
At training time, the lattice reduces to the sin-
gle path provided by the annotation. Since this fea-
ture tends to suffer from the drag of function words,
we insist that the next words that are being linked
have at least one feature that applies. All word pairs
linked in the training data have lexical features as de-
scribed above, and if both source and target words
are unknown they have a single feature for their link.
Applying dynamic features on words that have at
least one other feature prevents words which are com-
pletely unrelated from being linked because of a fea-
ture about the context of the words.
Two types of dynamic features are distinguished:
(a) English word with Arabic prefix/suffix and (b)
English word with Arabic stem.
4 Smoothing the Observation Model
Since the annotated training data for word alignment
is limited and a much larger parallel corpus is avail-
able for other aligners, we smooth the observation
Anno. 1 Anno. 1? Anno. 2
Correction
Anno. 1 96.5 92.4 91.7
Anno. 1? 95.2 ? 93.2
Table 1: F-measure for human performance on word
alignment for Arabic-English.
probability with an IBM Model 1 estimate,
p(li|tM1 , sK1 ) =
1
Z pME(li|t
M
1 , sK1 )?pM1(s|ti)1?? .
where ? is set to 0.9 in the experiments below. In
the equation above, the s represents the Arabic word
that is being linked from the English word ti.
When ? is set to 1.0 there is no smoothing per-
formed and performance degrades to F=84.0 from
the best system performance (F=87.8). When ? is
set to 0, the model uses only the IBM Model 1 distri-
bution and the resulting aligner is similar to an HMM
aligner with the transition shape discussed above and
yields performance of F=73.2.
5 Search Algorithm
A beam search algorithm is utilized with the English
words consumed in sequence and the Arabic word
positions serving as states in the search process. In
order to take advantage of the transition model de-
scribed above, a large beam must be maintained. To
see this, note that English words often repeat in a
sentence and the models will tend to link the word
to all Arabic positions which have the same Ara-
bic content. In traditional algorithms, the Markov
assumption is made and hypothesis are merged if
they have the same history in the previous time step.
However, here we maintain all hypotheses and merge
only if the paths are same for 30 words which is the
average sentence length.
6 Experimental Data
We have word aligned a portion of the Arabic Tree-
bank (4300 sentences) and material from the LDC
news sources (LDC, 2005) to obtain a total of 10.3K
sentence pairs for training. As a test of alignment,
we use the first 50 sentences of the MT03 Evaluation
test set which has 1313 Arabic words and 1528 En-
glish words 2. In terms of annotation guidelines, we
use the following instructions: (a) Align determiners
to their head nouns, (b) Alignments are done word
by word unless the phrase is idiomatic in which case
the entire phrase to phrase alignment was marked,
(c) spontaneous words are marked as being part of a
2The test data is available by contacting the authors.
92
1K 3K 5K 7K 9K 10.3K
# of features 15510 32111 47962 63140 73650 80321
English % OOV 15.9 8.2 5.5 4.4 4.05 3.6
Arabic % OOV 31 19.6 15.6 13.2 10.8 10.3
F-measure 83.2 85.4 86.5 87.4 87.5 87.8
Table 2: Varying Training data size.
phrase wherever possible but left unaligned if there
is no evidence to link the word.
In order to measure alignment performance, we
use the standard AER measure (Och and Ney, 2000)
but consider all links as sure. This measure is then
related to the F-measure which can be defined in
terms of precision and recall as
Precision The number of correct word links over
the total number of proposed links.
Recall The number of correct word links over the
total number of links in the reference.
and the usual definition of the F-measure,
F = 2PR(R+ P )
and define the alignment error as AER = 1 ? F .
In this paper, we report our results in terms of F-
measure over aligned links. Note that links to the
NULL state (unaligned English words) are not in-
cluded in the F-measure. Systems are compared rel-
ative to the reduction in AER.
6.1 Annotator Agreement
We measure intra/inter-annotator agreement on the
test set in order to determine the feasibility of hu-
man annotation of word links. These are shown in
Table 1. In the table, the column for ?Annotator 1
Correction? is the first annotator correcting his own
word alignments after a span of a year. After two
weeks, the annotator (Annotator 1?) was given the
same material with all the links removed and asked
to realign and we see that there is more discrepancy
in resulting alignments. The differences are largely
on the head concept where determiners are attached
and the alignment of spontaneous words. The perfor-
mance with a second annotator is in the same range
as the reannotation by a single annotator.
7 Experiments
In order to evaluate the performance of the algo-
rithm, we investigate the effect due to: (a) increasing
the training data size, (b) additional feature types,
and (c) comparable algorithms.
7.1 Training Data Size
We varied the training data size from 1K sentences to
the complete set in Table 2. Each batch re-estimates
the unknown word class by creating a vocabulary
on the training set. The trend indicates a reasonable
progression of performance and more data is required
to determine the saturation point.
7.2 Feature Types
The results obtained by different feature sets are
shown in Table 3. Each feature type was added incre-
mentally (Add Feature column) to the line above to
determine the effect of the individual feature types
and then removed incrementally from the full sys-
tem (Subtract Feature column) in order to see the
final effect. The results indicate that lexical features
are the most important type of feature; segmenta-
tion features further reduce the AER by 15.8%. The
other features add small gains in performance which,
although are not statistically significant for the align-
ment F-measure, are important in terms of feature
extraction. Segmentation features discussed above
result in both suffix and prefix features as well as
stem features. In the Subtract column, for the seg-
mentation feature, only the suffix and prefix features
were removed. This result indicates that most of the
alignment improvement from the segmentation fea-
ture comes in the form of new lexical features to link
Arabic stems and English words.
7.3 Comparison to other alignment
algorithms
In order to gauge the performance of the algorithm
with respect to other alignment strategies, we pro-
vide results using GIZA++ and an HMM Max Poste-
rior Algorithm (Ge, 2004). These algorithms, as well
as the Model 1 smoothing for the MaxEnt aligner,
are all trained on a corpus of 500K sentence pairs
from the UN parallel corpus and the LDC news cor-
pora released for 2005 (LDC, 2005). Note that these
algorithms are unsupervised by design but we utilize
them to have a baseline for comparing the perfor-
mance of this supervised approach.
7.3.1 HMM Max Posterior Aligner
The maximum-posterior word alignments are ob-
tained by finding the link configuration that maxi-
93
System # of Add Subtract
feats Feature Feature
Word pairs 50070 85.03 76.3
Spelling 4 85.11 87.7
Segmentation 70 87.39 87.5(*)
WordNet 13789 87.54 87.5
Dynamic-Words 1952 87.80 87.1
Dynamic-Segmentation 42 87.84 87.8
Table 3: Alignment performance in terms of the feature types utilized.
F-Measure
GIZA++ 79.5
HMM 76.3
MaxEnt 87.8
Table 4: Alignment performance
mizes the posterior state probability. In contrast, in
performing a Viterbi alignment, we compute the best
state sequence given the observation. The maximum
posterior computes the best state one at a time and
iterates over all possible combinations. Once we find
the maximum in the posterior probability matrix,
we also know the corresponding state and observa-
tion which is nothing but the word pair (sj , ti). We
will then align the pair and continue to find the next
posterior maximum and align the resulting pair. At
each iteration of the process, a word pair is aligned.
The process is repeated until either every word in one
(or both) language is aligned or no more maximum
can be found, whichever happens first.
7.3.2 GIZA Alignment
In order to contrast our algorithm, we ran
GIZA++ in the standard configuration which im-
plies 5 iterations of IBM Model 1, HMM, Model 3
and Model 4. All parameters are left to their default
values.
The results using the three different aligners is
shown in Table 4. The reduction in AER over the
GIZA++ system is 40.5% and over the HMM sys-
tem is 48.5%. The Wilcoxon signed-rank test yields
a probability of 0.39 for rejecting the GIZA++ align-
ment over the HMM alignment, whereas the MaxEnt
algorithm should be rejected with a probability of
1.7e-6 over the HMM algorithm and similarly Max-
Ent should be rejected with a probability of 0.9e-
6 over the GIZA++ algorithm. These significance
tests indicate that the MaxEnt algorithm presented
above is significantly better than either GIZA++ or
HMM.
Figure 2: An alignment showing a split link from an
Arabic word.
8 Phrase Extraction
Once an alignment is obtained, phrases which sat-
isfy the inverse projection constraint are extracted
(although earlier this constraint was called consis-
tent alignments (Och et al, 1999)). This constraint
enforces that a sequence of source words align to a
sequence of target words as defined by the lowest and
highest target index, and when the target words are
projected back to the source language through the
alignment, the original source sequence is retrieved.
Examination of the hand alignment training data
showed that this criteria is often violated for Ara-
bic and English. Prepositional phrases with adjec-
tives often require a split? for example, the align-
ment shown in Figure 2 has ?of its relations? aligned
to a word in Arabic and ?tense? aligned to the next
word. The inverse projection constraint fails in this
case, and in the experiments below, we relax this con-
straint and generate features for single source words
as long as the target phrase has a gap less than 2
English words. This relaxation allows a pair of ad-
jectives to modify the head noun. In future work we
explore the use of features with variables to be filled
at decode time.
9 Translation Experiments
The experiments in machine translation are carried
out on a phrase based decoder similar to the one de-
94
MT03 MT04 MT05
GIZA++ 0.454 ? ?
HMM 0.459 0.419 0.456
MaxEnt 0.468 0.433 0.451
Combined 0.479 0.437 0.465
Significance 0.017 0.020 ?
Table 5: Machine Translation Performance using the
NIST 2005 Bleu scorer
scribed in (Tillmann and Ney, 2003). In order to con-
trast the performance of the extracted features, we
compare the translation performance to (a) a system
built from alignments proposed by an HMM Max
Posterior Aligner, and (b) a system built from GIZA
alignments. All other parameters of the decoder re-
main constant and only the feature set is changed for
these experiments. As training data, we use the UN
parallel corpus and the LDC news corpora released
in 2005. Comparison should therefore be only made
across systems reported here and not to earlier eval-
uations or other systems. The results are shown in
Table 5.
Combination of the phrasal features from the
HMM and MaxEnt alignments results in the ?Com-
bined? system. The Combined system performs bet-
ter in all cases; in MT03 and MT04 the MaxEnt
derived features perform better than the HMM sys-
tem. In MT05, there is a slight degradation which is
not significant and the combination system still re-
sults in an improvement over either system. Since
the MaxEnt aligner has access to a unique resource,
every attempt was made to make that resource avail-
able to the other systems. Although GIZA++ and
HMM can not directly utilize word aligned data, the
training data for MaxEnt was converted to paral-
lel sentences where each sentence has only the pair
of linked words. The resulting numbers make both
HMM and GIZA much closer in performance to the
MaxEnt aligner but the results are better for com-
paring alignment methods.
10 Error Analysis and Discussion
The alignment errors made by the system can be
attributed to
? English words that require multi-word Arabic
states, for example (a) dates which are written
in Arabic in more than one form ?kAnwn Al-
vAny / ynAyr? for ?january?, and (b) compound
words like ?rAm Allh? in English is ?Ramallah?.
? Rare translation of a common Arabic word as
well as a common English word used as the
translation for a rare Arabic word.
? Parallel corpora mismatch: training material for
translation is processed at a document level and
yet systems often operate at a sentence level.
Human translators often use pronouns for ear-
lier mentioned names although in the source lan-
guage the name is repeated. Information which
is sometimes repeated in the source in an ear-
lier sentence is dropped in future sentences of
the document. Document level features are re-
quired to allow the system to have information
to leave these words unaligned.
Figure 3 shows a human alignment on the left and
a machine output on the right. The columns next
to the words indicate whether the alignments are
?good? or ?extra? which indicates that these words
are aligned to the special NULL state. There are two
examples of multi-word Arabic states shown: (a) for
?january?, and (b) the English word ?agenda?. The
system aligns ?the? before committee and it seems
in this case its an annotation error. In this exam-
ple the Arabic words lnAHyp, AltnZym, wAlAEdAd
and Allwjsty are all unknown words in the vocabu-
lary yet the system managed to link 3 out 4 words
correctly.
While significant gains have been made in align-
ment performance, these gains have not directly
translated to machine translation improvements. In
fact, although the GIZA system is better than the
HMM system at alignment, the machine translation
result on MT03 indicates a slight degradation (al-
though it is not statistically significant). The prime
reason for this is that features extracted from the
alignments are aggregated over the training corpus
and this process helps good alignments to have signif-
icantly better counts than errors in alignment. Align-
ing rare words correctly should help performance but
since their count is low it is not reflected in bleu
scores.
11 Conclusion and Future Work
This paper presented a word aligner trained on anno-
tated data. While the performance of the aligner is
shown to be significantly better than other unsuper-
vised algorithms, the utility of these alignments in
machine translation is still an open subject although
gains are shown in two of the test sets. Since features
are extracted from a parallel corpus, most of the in-
formation relating to the specific sentence alignment
is lost in the aggregation of features across sentences.
Improvements in capturing sentence context could
allow the machine translation system to use a rare
but correct link appropriately.
Another significant result is that a small amount
(5K sentences) of word-aligned data is sufficient for
this algorithm since a provision is made to handle
95
Figure 3: An example sentence with human output on the left and system output on the right.
unknown words appropriately.
12 Acknowledgements
This work was partially supported by the Defense
Advanced Research Projects Agency and monitored
by SPAWAR under contract No. N66001-99-2-8916.
The views and findings contained in this material are
those of the authors and do not necessarily reflect
the position or policy of the U.S. government and no
official endorsement should be inferred. This paper
owes much to the collaboration of the Statistical MT
group at IBM.
References
Adam L. Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A probability model
to improve word alignment. In 41st Annual Meeting of
the Association for Computational Linguistics, pages
88?95, Sapporo, Japan.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1995. Inducing features of random fields.
Technical Report, Department of Computer Science,
Carnegie-Mellon University, CMU-CS-95-144, May.
Niyu Ge. 2004. Improvement in Word Alignments. Pre-
sentation given at DARPA/TIDES MT workshop.
LDC. 2005. http://ldc.upenn.edu/projects/tides/
mt2005ar.htm.
Young-Suk Lee, Kishore Papineni, and Salim Roukos.
2003. Language model based arabic word segmenta-
tion. In 41st Annual Meeting of the Association for
Computational Linguistics, pages 399?406, Sapporo,
Japan.
Huma Lodhi, John Shawe-Taylor, Nello Cristianini, and
Christopher J. C. H. Watkins. 2000. Text classification
using string kernels. In NIPS, pages 563?569.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?244.
Robert C. Moore. 2004. Improving IBM Word-Alignment
Model 1. In 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 518?525,
Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In 38th Annual Meeting of
the Association for Computational Linguistics, pages
440?447, Hong Kong, China.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Joint Conf. of Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 20?28, College Park, Maryland.
Franz Josef Och. 2003. Minimum error rate training in
Statistical Machine Translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 160?167, Sapporo, Japan.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for Statistical Machine Translation. 29(1):97?
133.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM BasedWord Alignment in Statistical Ma-
chine Translation. In Proc. of the 16th Int. Conf.
on Computational Linguistics (COLING 1996), pages
836?841, Copenhagen, Denmark, August.
96



Automatic Derivation of Surface Text Patterns for a Maximum Entropy
Based Question Answering System
Deepak Ravichandran
 
USC Information Sciences Institute
4676, Admiralty Way
Marina del Rey, CA, 90292
ravichan@isi.edu
Abraham Ittycheriah and Salim Roukos
IBM TJ Watson Research Center
Yorktown Heights, NY, 10598

abei,roukos  @us.ibm.com
Abstract
In this paper we investigate the use of surface
text patterns for a Maximum Entropy based
Question Answering (QA) system. These text
patterns are collected automatically in an un-
supervised fashion using a collection of trivia
question and answer pairs as seeds. These pat-
terns are used to generate features for a statis-
tical question answering system. We report our
results on the TREC-10 question set.
1 Introduction
Several QA systems have investigated the use of text pat-
terns for QA (Soubbotin and Soubbotin, 2001), (Soub-
botin and Soubbotin, 2002), (Ravichandran and Hovy,
2002). For example, for questions like ?When was
Gandhi born??, typical answers are ?Gandhi was born in
1869? and ?Gandhi (1869-1948)?. These examples sug-
gest that the text patterns such as ?  NAME  was born in
 BIRTHDATE  ? and ?  NAME  (  BIRTHDATE  -
 DEATHYEAR  )? when formulated as regular expres-
sions, can be used to select the answer phrase to ques-
tions. Another approach to a QA system is learning cor-
respondences between question and answer pairs. IBM?s
Statistical QA (Ittycheriah et al, 2001a) system uses a
probabilistic model trainable from Question-Answer sen-
tence pairs. The training is performed under a Maximum
Entropy model, using bag of words, syntactic and name
entity features. This QA system does not employ the use
of patterns. In this paper, we explore the inclusion of
surface text patterns into the framework of a statistical
question answering system.
2 KM Corpus
A corpus of question-answer pairs was obtained from
Knowledge Master (1999). We refer to this corpus as the
1Work done while the author was an intern at IBM TJ Wat-
son Research Center during Summer 2002.
KM database. Each of the pairs in KM represents a trivia
question and its corresponding answer, such as the ones
used in the trivia card game. The question-answer pairs
in KM were filtered to retain only questions that look
similar to the ones presented in the TREC task2. Some
examples of QA pairs in KM:
1. Which country was invaded by the Libyan troops in
1983? - Chad
2. Who led the 1930 Salt March in India? - Mohandas
Gandhi
3 Unsupervised Construction of Training
Set for Pattern Extraction
We use an unsupervised technique that uses the QA in
KM as seeds to learn patterns. This method was first de-
scribed in Ravichandran and Hovy (2002). However, in
this work we have enriched the pattern format by induc-
ing specific semantic types of QTerms, and have learned
many more patterns using the KM.
3.1 Algorithm for sentence construction
1. For every question, we run a Named Entity Tagger
HMMNE 3 and identify chunks of words, that sig-
nify entities. Each such entity obtained from the
Question is defined as a Question term (QTerm).
The Answer Term (ATerm) is the Answer given by
the KM corpus.
2. Each of the question-answer pairs is submitted as
query to a popular Internet search engine4. We use
the top 50 relevant documents after stripping off the
HTML tags. The text is then tokenized to smoothen
white space variations and chopped to individual
sentences.
3. For every sentence obtained from Step (3) apply
2This was done by retaining only those questions that had
10 words or less, and were not multiple choice.
3In these experiments we use HMMNE, a named entity tag-
ger similar to the BBN?s Identifinder HMM Tagger (Bikel et al,
1999).
4Alta Vista http://www.altavista.com
HMMNE and retain only those sentences that con-
tains at least one of the QTerms plus the ATerm.
For example, we obtain the following sentences for the
QA pair ?Which country was invaded by the Libyan
troops in 1983? - Chad?:
1. More than 7,000 Libyan troops entered Chad.
2. An OUA peacekeeping force of 3,500 troops replaced the
Libyan forces in the remainder of Chad.
3. In the summer of 1983, GUNT forces launched an offen-
sive against government positions in northern and eastern
Chad.
The underlined words indicate the QTerms and the
ATerms that helped to select the sentence as a potential
way of answering the Question. The algorithm described
above was applied to each of the 16,228 QA pairs in our
KM database. A total of more than 250K sentences was
obtained.
3.2 Sentence Canonicalization
Every sentence obtained from the sentence construction
algorithm is canonicalized. Canonicalization of a sen-
tence is performed on the basis of the information pro-
vided by HMMNE, the QTerms and the ATerm. Canon-
icalization in this context may be defined as the general-
ization of a sentence based on the following process:
1. Apply HMMNE to each sentence obtained from the
sentence construction algorithm.
2. Identify the QTerms and ATerm in the answer sen-
tence.
3. Replace the ATerm by the tag ?  ANSWER  ?.
4. Replace each identified Named Entity by the class
of entity it represents.
5. If a given Named Entity is also a QTerm, indicate it
by the tag ?QT?.
The following example illustrates canonicalization.
Consider the sentence:
More than 7,000 Libyan troops entered Chad.
The application of HMMNE results in:
More than  NUMEX TYPE=CARDINAL  7,000
 /NUMEX   HUMAN TYPE=PEOPLE  Libyan
 /HUMAN  troops entered  ENAMEX TYPE=
COUNTRY  Chad  /ENAMEX  .
The canonicalization step gives the sentence:
More than  CARDINAL  PEOPLE QT  troops en-
tered  ANSWER  .
3.3 Pattern Extraction
Pattern extraction algorithm.
1. Every sentence obtained from sentence canon-
icalization algorithm is delimited by the tags
?  START  ? and ?  END  ? and then passed
through a Suffix Tree. The Suffix Tree algorithm
obtains the counts of all sub-strings of the sentence.
2. From the Suffix Tree we obtain only those sub-
strings that are at least a trigram, contain both the
?  ANSWER  ? and the ?  QT  ? tag and have at
least a count of 3 occurrences.
Source Number of Questions
Trec8 200
Trec9 500
KM 4200
Table 1: Training source and sizes.
Some examples of patterns obtained from the Suffix Tree
algorithm are as follows:
1. son of  PERSON QT  and  ANSWER 
2. of the  ANSWER  DISEASE QT 
3. of  ANSWER  at  LOCATION QT 
4.  ANSWER  was the  ORDINAL 	 OCCUPATION
QT  to
5.  ANSWER  was elected  OCCUPATION QT  of the
 LOCATION QT 
6.  ANSWER  was a prolific  OCCUPATION QT 
7.  LOCATION QT  ,  ANSWER 
8.  ANSWER  ,  LOCATION QT 
9.  START 
 ANSWER  served as  OCCUPATION
QT  from  DATE 
10.  START  ANSWER  is the  PEOPLE QT 
name for
A set of 22,353 such patterns were obtained by the ap-
plication of the pattern extraction algorithm from more
than 250,000 sentences. Some patterns are very general
and applicable to many questions, such as the ones in ex-
amples (7) and (8) while others are more specific to a
few questions, such as examples (9) and (10). Having
obtained these patterns we now can learn the appropriate
?weights? to use these patterns in a Question Answering
System.
4 Maximum Entropy Training
For these experiments we use the Maximum Entropy for-
mulation (Della Pietra et al, 1995) and model the distri-
bution (Ittycheriah, 2001b),
 ffTIPS: A Translingual Information Processing System
Y. Al-Onaizan, R. Florian, M. Franz, H. Hassan, Y. S. Lee, S. McCarley, K.
Papineni, S. Roukos, J. Sorensen, C. Tillmann, T. Ward, F. Xia
IBM T. J. Watson Research Center
Yorktown Heights
Abstract
Searching online information is
increasingly a daily activity for many
people. The multilinguality of online
content is also increasing (e.g. the
proportion of English web users, which
has been decreasing as a fraction the
increasing population of web users, dipped
below 50% in the summer of 2001). To
improve the ability of an English speaker
to search mutlilingual content, we built a
system that supports cross-lingual search
of an Arabic newswire collection and
provides on demand translation of Arabic
web pages into English. The cross-lingual
search engine supports a fast search
capability (sub-second response for typical
queries) and achieves state-of-the-art
performance in the high precision region
of the result list. The on demand statistical
machine translation uses the Direct
Translation model along with a novel
statistical Arabic Morphological Analyzer
to yield state-of-the-art translation quality.
The on demand SMT uses an efficient
dynamic programming decoder that
achieves reasonable speed for translating
web documents.
Overview
Morphologically rich languages like Arabic
(Beesley, K. 1996) present significant challenges
to many natural language processing applications
as the one described above because a word often
conveys complex meanings decomposable into
several morphemes (i.e. prefix, stem, suffix). By
segmenting words into morphemes, we can
improve the performance of natural language
systems including machine translation (Brown et
al. 1993) and information retrieval (Franz, M.
and McCarley, S. 2002). In this paper, we
present a cross-lingual English-Arabic search
engine combined with an on demand Arabic-
English statistical machine translation system
that relies on source language analysis for both
improved search and translation. We developed
novel statistical learning algorithms for
performing Arabic word segmentation (Lee, Y.
et al2003) into morphemes and morphological
source language (Arabic) analysis (Lee, Y. et al
2003b). These components improve both mono-
lingual (Arabic) search and cross-lingual
(English-Arabic) search and machine
translation. In addition, the system supports
either document translation or convolutional
models for cross-lingual search (Franz, M. and
McCarley, S. 2002).
The overall demonstration has the following
major components:
1. Mono-lingual search: uses Arabic word
segmentation and an okapi-like search
engine for document ranking.
2. Cross-lingual search: uses Arabic word
segmentation and morphological
analysis along with a statistical
morpheme translation matrix in a
convolutional model for document
ranking. The search can also use
document translation into English to
rank the Arabic documents. Both
approaches achieve similar precision in
the high precision region of retrieval.
The English query is also
morphologically analyzed to improve
performance.
3. OnDemand statistical machine
translation: this component uses both
analysis components along with a direct
channel translation model with a fast
dynamic programming decoder
(Tillmann, C. 2003). This system
                                                               Edmonton, May-June 2003
                                                              Demonstrations , pp. 1-2
                                                         Proceedings of HLT-NAACL 2003
achieves state-of-the-art Arabic-English
translation quality.
4. Arabic named entity detection and
translation: we have 31 categories of
Named Entities (Person, Organization,
etc.) that we detect and highlight in
Arabic text and provide the translation
of these entities into English. The
highlighted named entities help the user
to quickly assess the relevance of a
document.
All of the above functionality is available
through a web browser. We indexed the Arabic
AFP corpus about 330k documents for the
demonstration. The resulting search engine
supports sub-second query response. We also
provide an html detagging capability that allows
the translation of Arabic web pages while trying
to preserve the original layout as much as
possible in the on demand SMT component. The
Arabic Name Entity Tagger is currently run as an
offline process but we expect to have it online by
the demonstration time. We aslo include two
screen shots of the demonstration system.
Acknowledgments
This work was partially supported by the
Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position of
policy of the Government and no official
endorsement should be inferred.
References
Beesley, K. 1996. Arabic Finite-State
Morphological Analysis and Generation.
Proceedings of COLING-96, pages 89? 94.
Brown, P., Della Pietra, S., Della Pietra, V., and
Mercer, R. 1993. The mathematics of statistical
machine translation: Parameter Estimation.
Computational Linguistics, 19(2): 263?311.
Franz, M. and McCarley, S. 2002. Arabic
Information Retrieval at IBM. Proceedings
of TREC 2002, pages 402?405.
Lee, Y., Papineni, K., Roukos, S.,
Emam, O., and Hassan, H. 2003. Language
Model Based Arabic Word Segmentation.
Submitted for publication.
Lee, Y., Papineni, K., Roukos, S., Emam,
O., and Hassan, H. 2003b. Automatic
Induction of Morphological Analysis for
Statistical Machine Translation. Manuscript in
preparation.
Tillmann, C., 2003. Word Reordering and a
DP Beam Search Algorithm for Statistical
Machine Translation. Computational
Linguistics, 29(1): 97-133.
A Statistical Model for Multilingual Entity Detection and Tracking
R. Florian, H. Hassan   , A. Ittycheriah, H. Jing
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos
I.B.M. T.J. Watson Research Center
Yorktown Heights, NY 10598
{raduf,abei,hjing,nanda,xiaoluo, nicolas,roukos}@us.ibm.com

hanyh@eg.ibm.com
Abstract
Entity detection and tracking is a relatively new
addition to the repertoire of natural language
tasks. In this paper, we present a statistical
language-independent framework for identify-
ing and tracking named, nominal and pronom-
inal references to entities within unrestricted
text documents, and chaining them into clusters
corresponding to each logical entity present in
the text. Both the mention detection model
and the novel entity tracking model can use
arbitrary feature types, being able to integrate
a wide array of lexical, syntactic and seman-
tic features. In addition, the mention detec-
tion model crucially uses feature streams de-
rived from different named entity classifiers.
The proposed framework is evaluated with sev-
eral experiments run in Arabic, Chinese and
English texts; a system based on the approach
described here and submitted to the latest Au-
tomatic Content Extraction (ACE) evaluation
achieved top-tier results in all three evaluation
languages.
1 Introduction
Detecting entities, whether named, nominal or pronom-
inal, in unrestricted text is a crucial step toward under-
standing the text, as it identifies the important concep-
tual objects in a discourse. It is also a necessary step for
identifying the relations present in the text and populating
a knowledge database. This task has applications in in-
formation extraction and summarization, information re-
trieval (one can get al hits for Washington/person and not
the ones for Washington/state or Washington/city), data
mining and question answering.
The Entity Detection and Tracking task (EDT hence-
forth) has close ties to the named entity recognition
(NER) and coreference resolution tasks, which have been
the focus of attention of much investigation in the recent
past (Bikel et al, 1997; Borthwick et al, 1998; Mikheev
et al, 1999; Miller et al, 1998; Aberdeen et al, 1995;
Ng and Cardie, 2002; Soon et al, 2001), and have been
at the center of several evaluations: MUC-6, MUC-7,
CoNLL?02 and CoNLL?03 shared tasks. Usually, in com-
putational linguistic literature, a named entity represents
an instance of a name, either a location, a person, an or-
ganization, and the NER task consists of identifying each
individual occurrence of such an entity. We will instead
adopt the nomenclature of the Automatic Content Extrac-
tion program1 (NIST, 2003a): we will call the instances
of textual references to objects or abstractions mentions,
which can be either named (e.g. John Mayor), nominal
(e.g. the president) or pronominal (e.g. she, it). An entity
consists of all the mentions (of any level) which refer to
one conceptual entity. For instance, in the sentence
President John Smith said he has no comments.
there are two mentions: John Smith and he (in the order
of appearance, their levels are named and pronominal),
but one entity, formed by the set {John Smith, he}.
In this paper, we present a general statistical frame-
work for entity detection and tracking in unrestricted text.
The framework is not language specific, as proved by ap-
plying it to three radically different languages: Arabic,
Chinese and English. We separate the EDT task into a
mention detection part ? the task of finding all mentions
in the text ? and an entity tracking part ? the task of com-
bining the detected mentions into groups of references to
the same object.
The work presented here is motivated by the ACE eval-
uation framework, which has the more general goal of
building multilingual systems which detect not only enti-
ties, but also relations among them and, more recently,
events in which they participate. The EDT task is ar-
guably harder than traditional named entity recognition,
because of the additional complexity involved in extract-
ing non-named mentions (nominals and pronouns) and
the requirement of grouping mentions into entities.
We present and evaluate empirically statistical mod-
els for both mention detection and entity tracking prob-
lems. For mention detection we use approaches based on
Maximum Entropy (MaxEnt henceforth) (Berger et al,
1996) and Robust Risk Minimization (RRM henceforth)
1For a description of the ACE program see
http://www.nist.gov/speech/tests/ace/.
(Zhang et al, 2002). The task is transformed into a se-
quence classification problem. We investigate a wide ar-
ray of lexical, syntactic and semantic features to perform
the mention detection and classification task including,
for all three languages, features based on pre-existing sta-
tistical semantic taggers, even though these taggers have
been trained on different corpora and use different seman-
tic categories. Moreover, the presented approach implic-
itly learns the correlation between these different seman-
tic types and the desired output types.
We propose a novel MaxEnt-based model for predict-
ing whether a mention should or should not be linked to
an existing entity, and show how this model can be used
to build entity chains. The effectiveness of the approach
is tested by applying it on data from the above mentioned
languages ? Arabic, Chinese, English.
The framework presented in this paper is language-
universal ? the classification method does not make any
assumption about the type of input. Most of the fea-
ture types are shared across the languages, but there are a
small number of useful feature types which are language-
specific, especially for the mention detection task.
The paper is organized as follows: Section 2 describes
the algorithms and feature types used for mention detec-
tion. Section 3 presents our approach to entity tracking.
Section 4 describes the experimental framework and the
systems? results for Arabic, Chinese and English on the
data from the latest ACE evaluation (September 2003), an
investigation of the effect of using different feature types,
as well as a discussion of the results.
2 Mention Detection
The mention detection system identifies the named, nom-
inal and pronominal mentions introduced in the previous
section. Similarly to classical NLP tasks such as base
noun phrase chunking (Ramshaw and Marcus, 1994), text
chunking (Ramshaw and Marcus, 1995) or named entity
recognition (Tjong Kim Sang, 2002), we formulate the
mention detection problem as a classification problem,
by assigning to each token in the text a label, indicating
whether it starts a specific mention, is inside a specific
mention, or is outside any mentions.
2.1 The Statistical Classifiers
Good performance in many natural language process-
ing tasks, such as part-of-speech tagging, shallow pars-
ing and named entity recognition, has been shown to de-
pend heavily on integrating many sources of information
(Zhang et al, 2002; Jing et al, 2003; Ittycheriah et al,
2003). Given the stated focus of integrating many feature
types, we are interested in algorithms that can easily in-
tegrate and make effective use of diverse input types. We
selected two methods which satisfy these criteria: a linear
classifier ? the Robust Risk Minimization classifier ? and
a log-linear classifier ? the Maximum Entropy classifier.
Both methods can integrate arbitrary types of informa-
tion and make a classification decision by aggregating all
information available for a given classification.
Before formally describing the methods2, we introduce
some notations: let
 	



be the set of pre-
dicted classes,  be the example space and 

be the feature space. Each example Proceedings of NAACL HLT 2007, pages 57?64,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Direct Translation Model 2
Abraham Ittycheriah and Salim Roukos
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
{abei,roukos}@us.ibm.com
Abstract
This paper presents a maximum entropy ma-
chine translation system using a minimal set
of translation blocks (phrase-pairs). While
recent phrase-based statistical machine trans-
lation (SMT) systems achieve significant im-
provement over the original source-channel sta-
tistical translation models, they 1) use a large
inventory of blocks which have significant over-
lap and 2) limit the use of training to just a
few parameters (on the order of ten). In con-
trast, we show that our proposed minimalist
system (DTM2) achieves equal or better per-
formance by 1) recasting the translation prob-
lem in the traditional statistical modeling ap-
proach using blocks with no overlap and 2) re-
lying on training most system parameters (on
the order of millions or larger). The new model
is a direct translation model (DTM) formu-
lation which allows easy integration of addi-
tional/alternative views of both source and tar-
get sentences such as segmentation for a source
language such as Arabic, part-of-speech of both
source and target, etc. We show improvements
over a state-of-the-art phrase-based decoder in
Arabic-English translation.
1 Introduction
Statistical machine translation takes a source se-
quence, S = [s1 s2 . . . sK ], and generates a target
sequence, T ? = [t1 t2 . . . tL], by finding the most
likely translation given by:
T ? = argmax
T
p(T |S).
1.1 Block selection
Recent statistical machine translation (SMT) al-
gorithms generate such a translation by incorpo-
rating an inventory of bilingual phrases (Och and
Ney, 2000). A m-n phrase-pair, or block, is a se-
quence of m source words paired with a sequence
of n target words. The inventory of blocks in cur-
rent systems is highly redundant. We illustrate the
redundancy using the example in Table 1 which
lljnp
Almrkzyp
llHzb
Al$ywEy
AlSyny
the
Politburo
of
the
Central
Committee
of
the
Chinese
Communist
Party
Almktb
AlsyAsy
Figure 1: Example of Arabic snipet and alignment
to its English translation.
shows a set of phrases that cover the two-word
Arabic fragment ?lljnp Almrkzyp? whose align-
ment and translation is shown in Figure 1. One
notices the significant overlap between the vari-
ous blocks including the fact the output target se-
quence ?of the central committee? can be pro-
duced in at least two different ways: 1) as 2-4 block
?lljnp Almrkzyp | of the central committee? cov-
ering the two Arabic words, or 2) by using the 1-
3 block ?Almrkzyp | of the central? followed by
covering the first Arabic word with the 1-1 block
?lljnp | committee?. In addition, if one adds one
more word to the Arabic fragment in the third posi-
tion such as the block ?AlSyny | chinese? the over-
lap increases significantly and more alternate possi-
bilities are available to produce an output such as
the ?of the central chinese committee.?
In this work, we propose to only use 1-n blocks and
avoid completely the redundancy obtained by the use
of m-n blocks for m > 1 in current phrase-based sys-
tems. We discuss later how by defining appropriate
features in the translation model, we capture the im-
portant dependencies required for producing n-long
fragments for an m-word input sequence including
the reordering required to produce more fluent out-
put. So in Table 1 only the blocks corresponding to
a single Arabic word are in the block inventory. To
differentiate this work from previous approaches in
57
lljnp Almrkzyp
committee central
of the commission the central
commission of the central
of the committee of central
the committee and the central
of the commission on and central
the commission , central
committee of ?s central
. . . . . .
of the central committee(11)
of the central committee of (11)
the central committee of (8)
central committee(7)
committee central (2)
central committee , (2)
. . .
Table 1: Example Arabic-English blocks showing
possible 1-n and 2-n blocks ranked by frequency.
Block count is given in () for 2-n blocks.
direct modeling for machine translation, we call our
current approach DTM2 (Direct Translation Model
2).
1.2 Statistical modeling for translation
Earlier work in statistical machine translation
(Brown et al, 1993) is based on the ?noisy-channel?
formulation where
T ? = arg max
T
p(T |S) = argmax
T
p(T )p(S|T ) (1)
where the target language model p(T ) is further de-
composed as
p(T ) ?
?
i
p(ti|ti?1, . . . , ti?k+1)
where k is the order of the language model and the
translation model p(S|T ) has been modeled by a
sequence of five models with increasing complexity
(Brown et al, 1993). The parameters of each of the
two components are estimated using Maximum Like-
lihood Estimation (MLE). The LM is estimated by
counting n-grams and using smoothing techniques.
The translation model is estimated via the EM algo-
rithm or approximations that are bootstrapped from
the previous model in the sequence as introduced in
(Brown et al, 1993). As is well known, improved
results are achieved by modifying the Bayes factor-
ization in Equation 1 above by weighing each distri-
bution differently as in:
p(T |S) ? p?(T )p1??(S|T ) (2)
This is the simplest MaxEnt1 model that uses two
feature functions. The parameter ? is tuned on a
development set (usually to improve an error met-
ric instead of MLE). This model is a special case
of the Direct Translation Model proposed in (Pap-
ineni et al, 1997; Papineni et al, 1998) for language
understanding; (Foster, 2000) demostrated perplex-
ity reductions by using direct models; and (Och and
Ney, 2002) employed it very successfully for language
translation by using about ten feature functions:
p(T |S) = 1Z exp
?
i
?i?i(S, T )
Many of the feature functions used for translation are
MLE models (or smoothed variants). For example,
if one uses ?1 = log(p(T )) and ?2 = log(p(S|T )) we
get the model described in Equation 2. Most phrase-
based systems, including the baseline decoder used
in this work use feature functions:
? a target word n-gram model (e.g., n = 5),
? a target part-of-speech n-gram model (n ? 5),
? various translation models such as a block in-
ventory with the following three varieties: 1) the
unigram block count, 2) a model 1 score p(si|ti)
on the phrase-pair, and 3)a model 1 score for
the other direction p(ti|si),
? a target word count penalty feature |T |,
? a phrase count feature,
? a distortion model (Al-Onaizan and Papineni,
2006).
The weight vector ? is estimated by tuning on a
rather small (as compared to the training set used to
define the feature functions) development set using
the BLEU metric (or other translation error met-
rics). Unlike MaxEnt training, the method (Och,
2003) used for estimating the weight vector for BLEU
maximization are not computationally scalable for a
large number of feature functions.
2 Related Work
Most recent state-of-the-art machine translation de-
coders have the following aspects that we improve
upon in this work: 1) block style, and 2) model pa-
rameterization and parameter estimation. We dis-
cuss each item next.
1The subfields of log-linear models, exponential fam-
ily, and MaxEnt describe the equivalent techniques from
different perspectives.
58
2.1 Block style
In order to extract phrases from alignments available
in one or both directions, most SMT approaches use
a heuristic such as union, intersection, inverse pro-
jection constraint, etc. As discussed earlier, these
approaches result in a large overlap between the ex-
tracted blocks (longer blocks overlap with all the
shorter subcomponents blocks). Also, slightly re-
stating the advantages of phrase-pairs identified in
(Quirk and Menezes, 2006), these blocks are effec-
tive at capturing context including the encoding of
non-compositional phrase pairs, and capturing local
reordering, but they lack variables (e.g. embedding
between ne . . . pas in French), have sparsity prob-
lems, and lack a strategy for global reordering. More
recently, (Chiang, 2005) extended phrase-pairs (or
blocks) to hierarchical phrase-pairs where a grammar
with a single non-terminal allows the embedding of
phrases-pairs, to allow for arbitrary embedding and
capture global reordering though this approach still
has the high overlap problem. However, in (Quirk
and Menezes, 2006), the authors investigate mini-
mum translation units (MTU) which is a refinement
over a similar approach by (Banchs et al, 2005)
to eliminate the overlap issue. The MTU approach
picks all the minimal blocks subject to the condition
that no word alignment link crosses distinct blocks.
They do not have the notion of a block with a vari-
able (a special case of the hierarchical phrase-pairs)
that we employ in this work. They also have a weak-
ness in the parameter estimation method; they rely
on an n-gram language model on blocks which inher-
ently requires a large bilingual training data set.
2.2 Estimating Model Parameters
Most recent SMT systems use blocks (i.e. phrase-
pairs) with a few real valued ?informative? features
which can be viewed as an indicator of how proba-
ble the current translation is. As discussed in Sec-
tion 1.2, these features are typically MLE models
(e.g. block translation, Model 1, language model,
etc.) whose scores are log-linearly combined using
a weight vector, ?f where f is a particular feature.
The ?f are trained using a held-out corpus using
maximum BLEU training (Och, 2003). This method
is only practical for a small number of features; typ-
ically, the number of features is on the order of 10 to
20.
Recently, there have been several discriminative
approaches at training large parameter sets includ-
ing (Tillmann and Zhang, 2006) and (Liang et al,
2006). In (Tillmann and Zhang, 2006) the model
is optimized to produce a block orientation and the
target sentence is used only for computing a sentence
level BLEU. (Liang et al, 2006) demonstrates a dis-
criminatively trained system for machine translation
that has the following characteristics: 1) requires a
varying update strategy (local vs. bold) depending
on whether the reference sentence is ?reachable? or
not, 2) uses sentence level BLEU as a criterion for se-
lecting which output to update towards, and 3) only
trains on limited length (5-15 words) sentences.
So both methods fundamentally rely on a prior
decoder to produce an ?N-best? list that is used to
find a target (using max BLEU) for the training al-
gorithm. The methods to produce an ?N-best? list
tend to be not very effective since most alternative
translations are minor differences from the highest
scoring translation and do not typically include the
reference translation (particularly when the system
makes a large error).
In this paper, the algorithm trains on all sentences
in the test-specific corpus and crucially, the algo-
rithm directly uses the target translation to update
the model parameters. This latter point is a critical
difference that contrasts to the major weakness of the
work of (Liang et al, 2006) which uses a top-N list of
translations to select the maximum BLEU sentence
as a target for training (so called local update).
3 A Categorization of Block Styles
In (Brown et al, 1993), multi-word ?cepts? (which
are realized in our block concept) are discussed and
the authors state that when a target sequence is
sufficiently different from a word by word transla-
tion, only then should the target sequence should
be promoted to a cept. This is in direct opposition
to phrase-based decoders which utilize all possible
phrase-pairs and limit the number of phrases only
due to practical considerations. Following the per-
spective of (Brown et al, 1993), a minimal set of
phrase blocks with lengths (m, n) where either m or
n must be greater than zero results in the following
types of blocks:
1. n = 0, source word producing nothing in the
target language (deletion block),
2. m = 0, spontaneous target word (insertion
block),
3. m = 1 and n ? 1, a source word producing n
target words including the possibility of a vari-
able (denoted by X) which is to be filled with
other blocks from the sentence (the latter case
called a discontiguous block)
4. m ? 1 and n = 1, a sequence of source words
producing a single target words including the
possibility of a variable on the source side (as in
the French ne...pas translating into not, called
multi-word singletons) in the source sequence
59
5. m > 1 and n > 1, a non-compositional phrase
translation
In this paper, we restrict the blocks to Types 1 and 3.
From the example in Figure 1, the following blocks
are extracted:
? lljnp ? of the X Committee
? Almrkzyp ? Central
? llHzb ? of the X Party
? Al$ywEy ? Communist
? AlSyny ? Chinese.
These blocks can now be considered more ?general?
and can be used to generate more phrases compared
to the blocks shown in Table 1. These blocks when
utilized independently of the remainder of the model
perform very poorly as all the advantages of blocks
are absent. These advantages are obtained using the
features to be described below. Also, we store with a
block additional information such as: (a) alignment
information, and (b) source and target analysis. The
target analysis includes part of speech and for each
target string a list of part of speech sequences are
stored along with their corpus frequencies.
The first alignment shown in Figure 1 is an exam-
ple of a Type 5 non-compositional block; although
this is not currently addressed by the decoder, we
plan to handle such blocks in the future.
4 Algorithm
A classification problem can be considered as a map-
ping from a set of histories, S, into a set of futures,
T . Traditional classification problems deal with a
small finite set of futures usually no more than a few
thousands of classes.
Machine translation can be cast into the same
framework with a much larger future space. In con-
trast to the current global models, we decompose the
process into a sequence of steps. The process begins
at the left edge of a sentence and for practical rea-
sons considers a window of source words that could
be translated. The first action is to jump a distance,
j to a source position and to produce a target string,
t corresponding to the source word at that position.
The process then marks the source position as hav-
ing been visited and iterates till all source words have
been visited. The only wrinkle in this relatively sim-
ple process is the presence of a variable in the tar-
get sequence. In the case of a variable, the source
position is marked as having been partially visited.
When a partially visited source position is visited
again, the target string to the right of the variable is
output and the process is iterated. The distortion or
jump from the previously translated source word, j
in training can vary widely due to automatic sentence
alignment that is used to create the parallel corpus.
To limit the sparseness created by these longer jumps
we cap the jump to a window of source words (-5 to 5
words) around the last translated source word; jumps
outside the window are treated as being to the edge
of the window.
We combine the above translation model with a
n-gram language model as in
p(T, j|S) =
?
i
p(ti, j|si)
?
?
i
?LMp(ti|ti?1, . . . , ti?n)+
?TMp(ti, j|si)
This mixing allows the use of language model built
from a very large monolingual corpus to be used with
a translation model which is built from a smaller
parallel corpus. In the rest of this paper, we are
concerned only with the translation model.
The minimum requirements for the algorithm are
(a) parallel corpus of source and target languages
and (b) word-alignments. While one can use the
EM algorithm to train this hidden alignment model
(the jump step), we use Viterbi training, i.e. we use
the most likely alignment between target and source
words in the training corpus to estimate this model.
We assume that each sentence pair in the training
corpus is word-aligned (e.g. using a MaxEnt aligner
(Ittycheriah and Roukos, 2005) or an HMM aligner
(Ge, 2004)). The algorithm performs the following
steps in order to train the maximum entropy model:
(a) block extraction, (b) feature extraction, and (c)
parameter estimation. Each of the first two steps
requires a pass over the training data and param-
eter estimation requires typically 5-10 passes over
the data. (Della Pietra et al, 1995) documents the
Improved Iterative Scaling (IIS) algorithm for train-
ing maximum entropy models. When the system is
restricted to 1-N type blocks, the future space in-
cludes all the source word positions that are within
the skip window and all their corresponding blocks.
The training algorithm at the parameter estimation
step can be concisely stated as:
1. For each sentence pair in the parallel corpus,
walk the alignment in source word order.
2. At each source word, the alignment identifies the
?true? block.
3. Form a window of source words and allow all
blocks at source words to generate at this gen-
eration point.
60
4. Apply the features relevant to each block and
compute the probability of each block.
5. Form the MaxEnt polynomials(Della Pietra et
al., 1995) and solve to find the update for each
feature.
We will next discuss the prior distribution used in
the maximum entropy model, the block extraction
method and the feature generation method and dis-
cuss differences with a standard phrase based de-
coder.
4.1 Prior Distribution
Maximum entropy models are of the form,
p(t, j|s) = p0(t, j|s)Z exp
?
i
?i?i(t, j, s)
where p0 is a prior distribution, Z is a normalizing
term, and ?i(t, j, s) are the features of the model.
The prior distribution can contain any information
we know about our future and in this work we utilize
the normalized phrase count as our prior. Strictly,
the prior has to be uniform on the set of futures to
be a ?maximum? entropy algorithm and choices of
other priors result in minimum divergence models.
We refer to both as a maximum entropy models.
The practical benefit of using normalized phrase
count as the prior distribution is for rare transla-
tions of a common source words. Such a translation
block may not have a feature due to restrictions in
the number of features in the model. Utilizing the
normalized phrase count prior, the model is still able
to penalize such translations. In the best case, a fea-
ture is present in the model and the model has the
freedom to either boost the translation probability
or to further reduce the prior.
4.2 Block Extraction
Similar to phrase decoders, a single pass is made
through the parallel corpus and for each source word,
the target sequence derived from the alignments
is extracted. The ?Inverse Projection Constraint?,
which requires that the target sequence be aligned
only to the source word or phrase in question, is then
checked to ensure that the phrase pair is consistent.
A slight relaxation is made to the traditional target
sequence in that variables are allowed if the length of
their span is 3 words or less. The length restriction is
imposed to reduce the effect of alignment errors. An
example of blocks extracted for the romanized ara-
bic words ?lljnp? and ?Almrkzyp? are shown Figure 2,
where on the left side are shown the unsegmented
Arabic words, the segmented Arabic stream and the
corresponding Arabic part-of-speech. On the right,
the target sequences are shown with the most fre-
quently occuring part-of-speech and the corpus count
of this block.
The extracted blocks are pruned in order to min-
imize alignment problems as well as optimize the
speed during decoding. Blocks are pruned if their
corpus count is a factor of 30 times smaller than the
most frequent target sequence for the same source
word. This results in about 1.6 million blocks from
an original size of 3.2 million blocks (note this is
much smaller than the 50 million blocks or so that
are derived in current phrase-based systems).
4.3 Features
The features investigated in this work are binary
questions about the lexical context both in the source
and target streams. These features can be classi-
fied into the following categories: (a) block internal
features, and (b) block context features. Features
can be designed that are specific to a block. Such
features are modeling the unigram phrase count of
the block, which is information already present in
the prior distribution as discussed above. Features
which are less specific are tied across many transla-
tions of the word. For example in Figure 2, the pri-
mary translation for ?lljnp? is ?committee? and occurs
920 times across all blocks extracted from the corpus;
the final block shown which is ?of the X committee?
occurs only 37 times but employs a lexical feature
?lljnp committee? which fires 920 times.
4.3.1 Lexical Features
Lexical features are block internal features which
examine a source word, a target word and the jump
from the previously translated source word. As dis-
cussed above, these are shared across blocks.
4.3.2 Lexical Context Features
Context features encode the context surrounding
a block by examining the previous and next source
word and the previous two target words. Unlike a
traditional phrase pair, which encodes all the infor-
mation lexically, in this approach we define in Ta-
ble 2, individual feature types to examine a por-
tion of the context. One or more of these features
may apply in each instance where a block is relevant.
The previous source word is defined as the previously
translated source word, but the next source word is
always the next word in the source string. At train-
ing time, the previously translated source word is
found by finding the previous target word and utiliz-
ing the alignment to find the previous source word.
If the previous target word is unaligned, no context
feature is applied.
61
committee/NN (613)
of the commission/IN DT NN (169)
the committee/DT NN (136)
commission/NN (135)
of the committee/IN DT NN (134)
the commission/DT NN (106)
of the HOLE committee/IN DT -1 NN(37)
central/NNP (731)
the central/DT JJ (504)
of the central/IN DT NNP(64)
the cia/DT NNP (58)
Almrkzyp
Al# mrkzy +p
DET ADJ NSUFF_FEM_SG
lljnp
l# ljn +p
PREP NOUN NSUFF_FEM_SG
Figure 2: Extracted blocks for ?lljnp? and ?Almrkzyp?.
Feature Name Feature variables
SRC LEFT source left, source word,
target word
SRC RIGHT source right, source word,
target word
SRC TGT LEFT source left, target left,
source word, target word
SRC TGT LEFT 2 source left, target left,
target left 2, source word,
target word
Table 2: Context Feature Types
4.3.3 Arabic Segmentation Features
An Arabic segmenter produces morphemes; in
Arabic, prefixes and suffixes are used as prepositions,
pronouns, gender and case markers. This produces a
segmentation view of the arabic source words (Lee et
al., 2003). The features used in the model are formed
from the Cartesian product of all segmentation to-
kens with the English target sequence produced by
this source word or words. However, prefixes and
suffixes which are specific in translation are limited
to their English translations. For example the pre-
fix ?Al#? is only allowed to participate in a feature
with the English word ?the? and similarly ?the? is not
allowed to participate in a feature with the stem of
the Arabic word. These restrictions limit the num-
ber of features and also reduce the over fitting by the
model.
4.3.4 Part-of-speech Features
Part-of-speech taggers were run on each language:
the English part of speech tagger is a MaxEnt tag-
ger built on the WSJ corpus and on the WSJ test
set achieves an accuracy of 96.8%; the Arabic part
of speech tagger is a similar tagger built on the Ara-
bic tree bank and achieves an accuracy of 95.7% on
automatically segmented data. The part of speech
feature type examines the source and target as well
as the previous target and the corresponding previ-
ous source part of speech. A separate feature type
examines the part of speech of the next source word
when the target sequence has a variable.
4.3.5 Coverage Features
These features examine the coverage status of the
source word to the left and the source word to the
right. During training, the coverage is determined
by examining the alignments; the source word to the
left is uncovered if its target sequence is to the right
of the current target sequence. Since the model em-
ploys binary questions and predominantly the source
word to the left is already covered and the right
source word is uncovered, these features fire only if
the left is open or if the right is closed in order to
minimize the number of features in the model.
5 Translation Decoder
A beam search decoder similar to phrase-based sys-
tems (Tillmann and Ney, 2003) is used to translate
the Arabic sentence into English. These decoders
have two parameters that control their search strat-
egy: (a) the skip length (how many positions are al-
lowed to be untranslated) and (b) the window width,
which controls how many words are allowed to be
considered for translation. Since the majority of the
blocks employed in this work do not encode local re-
ordering explicitly, the current DTM2 decoder uses
a large skip (4 source words for Arabic) and tries
all possible reorderings. The primary difference be-
tween a DTM2 decoder and standard phrase based
decoders is that the maximum entropy model pro-
vides a cost estimate of producing this translation
using the features described in previous sections. An-
other difference is that the DTM2 decoder handles
blocks with variables. When such a block is pro-
posed, the initial target sequence is first output and
the source word position is marked as being partially
visited and an index into which segment was gener-
ated is kept for completing the visit at a later time.
Subsequent extensions of this path can either com-
plete this visit or visit other source words. On a
search path, we make a further assumption that only
62
one source position can be in a partially visited state
at any point. This greatly reduces the search task
and suffices to handle the type of blocks encountered
in Arabic to English translation.
6 Experiments
The UN parallel corpus and the LDC news corpora
released as training data for the NIST MT06 eval-
uation are used for all evaluations presented in this
paper. A variety of test corpora are now available
and we use MT03 as development test data, and
test results are presented on MT05. Results obtained
on MT06 are from a blind evaluation. For Arabic-
English, the NIST MT06 training data contains 3.7M
sentence pairs from the UN from 1993-2002 and 100K
sentences pairs from news sources. This represents
the universe of training data, but for each test set
we sample this corpus to train efficiently while also
observing slight gains in performance. The training
universe is time sorted and the most recent corpora
are sampled first. Then for a given test set, we obtain
the first 20 instances of n-grams from the test that
occur in the training universe and the resulting sam-
pled sentences then form the training sample. The
contribution of the sampling technique is to produce
a smaller training corpus which reduces the compu-
tational load; however, the sampling of the universe
of sentences can be viewed as test set domain adapta-
tion which improves performance and is not strictly
done due to computational limitations2. The 5-gram
language model is trained from the English Gigaword
corpus and the English portion of the parallel corpus
used in the translation model training.
The baseline decoder is a phrase-based decoder
that employs n-m blocks and uses the same test set
specific training corpus described above.
6.1 Feature Type Experiments
There are 15 individual feature types utilized in the
system, but in order to be brief we present the re-
sults by feature groups (see Table 3): (a) lexical, (b)
lexical context, (c) segmentation, (d) part-of-speech,
and (e) coverage features. The results show im-
provements with the addition of each feature set, but
the part-of-speech features and coverage features are
not statistically significant improvements. The more
complex features based on Arabic segmentation and
English part-of-speech yield a small improvement of
0.5 BLEU points over the model with only lexical
context.
2Recent results indicate that test set adaptation by
test set sampling of the training corpus achieves a cased
Bleu of 53.26 on MT03 whereas a general system trained
on all data achieves only 51.02
Verb Placement 3
Missing Word 5
Extra Word 5
Word Choice 26
Word Order 3
Other error 1
Total 43
Table 4: Errors on last 25 sentences of MT-03.
7 Error Analysis and Discussion
We analyzed the errors in the last 25 sentences of the
MT-03 development data using the broad categories
shown in Table 4. These error types are not indepen-
dent of each other; indeed, incorrect verb placement
is just a special case of the word order error type
but for this error analysis for each error we take the
first category available in this list. Word choice er-
rors can be a result of (a) rare words with few, or
incorrect, or no translation blocks (4 times) or (b)
model weakness3 (22 times). In order to address the
model weakness type of errors, we plan on investigat-
ing feature selection using a language model prior.
As an example, consider an arabic word which pro-
duces both ?the? (due to alignment errors) and ?the
conduct?. An n-gram LM has very low cost for the
word ?the? but a rather high cost for content words
such as ?conduct?. Incorporating the LM model as a
prior should help the maximum entropy model focus
its weighting on the content word to overcome the
prior information.
8 Conclusion and Future Work
We have presented a complete direct translation
model with training of millions of parameters based
on a set of minimalist blocks and demonstrated the
ability to retain good performance relative to phrase
based decoders. Tied features minimize the num-
ber of parameters and help avoid the sparsity prob-
lems associated with phrase based decoders. Uti-
lizing language analysis of both the source and tar-
get languages adds 0.8 BLEU points on MT-03, and
0.4 BLEU points on MT-05. The DTM2 decoder
achieved a 1.7 BLEU point improvement over the
phrase based decoder on MT-06. In this work, we
have restricted the block types to only single source
word blocks. Many city names and dates in Ara-
bic can not be handled by such blocks and in future
work we intend to investigate the utilization of more
complex blocks as necessary. Also, the DTM2 de-
coder utilized the LM component independently of
3The word occurred with the correct translation in
the phrase library with a count more than 10 and yet the
system used an incorrect translation.
63
Feature Types # of feats MT-03 MT-05 MT-06
(MT03)
Training Size
Num. of Sentences 197K 267K 279K
Phrase-based Decoder 51.20 49.06 36.92
DTM2 Decoder
Lex Feats a 439,582 49.70 48.37
+Lex Context b 2,455,394 50.45 49.61
+Seg Feats c 2,563,338 50.97 49.96
+POS Feats d 2,608,352 51.27 49.93
+Cov Feats e 2,783,813 51.19 50.00 38.61
Table 3: Bleu scores on MT03-MT06.
the translation model; however, in future work we
intend to investigate feature selection using the lan-
guage model as a prior which should result in much
smaller systems.
9 Acknowledgements
This work was partially supported by the Department of
the Interior, National Business Center under contract No.
NBCH2030001 and Defense Advanced Research Projects
Agency under contract No. HR0011-06-2-0001. The
views and findings contained in this material are those
of the authors and do not necessarily reflect the position
or policy of the U.S. government and no official endorse-
ment should be inferred. This paper owes much to the
collaboration of the Statistical MT group at IBM.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion
models for statistical machine translation. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
ACL, pages 529?536, Sydney, Australia.
Rafael Banchs, Josep M. Crego, Adria` de Gispert, Pa-
trik Lambert, and Jose? B. Marino. 2005. Statistical
machine translation of euparl data by using bilingual
n-grams. In Proc. of the ACL Workshop on Building
and Using Parallel Texts, pages 133?136, Ann Arbor,
Michigan, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, Michigan, June.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1995. Inducing features of random fields.
Technical Report, Department of Computer Science,
Carnegie-Mellon University, CMU-CS-95-144.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In 38th Annual Meeting
of the ACL, pages 45?52, Hong Kong.
Niyu Ge. 2004. Improvement in Word Alignments. Pre-
sentation given at DARPA/TIDES MT workshop.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT ?05: Proceedings of the HLT and
EMNLP, pages 89?96.
Young-Suk Lee, Kishore Papineni, and Salim Roukos.
2003. Language model based arabic word segmenta-
tion. In 41st Annual Meeting of the ACL, pages 399?
406, Sapporo, Japan.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the ACL, pages
761?768, Sydney, Australia.
Franz Josef Och and Hermann Ney. 2000. Statistical
machine translation. In EAMT Workshop, pages 39?
46, Ljubljana, Slovenia.
Franz-Josef Och and Hermann Ney. 2002. Discriminative
Training and Maximum Entropy Models for Statistical
Machine Translations. In 40th Annual Meeting of the
ACL, pages 295?302, Philadelphia, PA, July.
Franz Josef Och. 2003. Minimum error rate training in
Statistical Machine Translation. In 41st Annual Meet-
ing of the ACL, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, and R. T. Ward.
1997. Feature-based language understanding. In EU-
ROSPEECH, pages 1435?1438, Rhodes,Greece.
Kishore Papineni, Salim Roukos, and R. T. Ward. 1998.
Maximum likelihood and discriminative training of di-
rect translation models. In International Conf. on
Acoustics, Speech and Signal Processing, pages 189?
192, Seattle, WA.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
pages 9?16, New York, NY, USA.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for Statistical Machine Translation. 29(1):97?
133.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical mt. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the ACL, pages 721?728, Sydney, Australia.
64
Active Learning for Statistical Natural Language Parsing
Min Tang
Spoken Language Systems Group
MIT Laboratory for Computer Science
Cambridge, Massachusetts 02139, USA
  mtang@sls.lcs.mit.edu 
Xiaoqiang Luo Salim Roukos
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
  xiaoluo,roukos@us.ibm.com 
Abstract
It is necessary to have a (large) annotated cor-
pus to build a statistical parser. Acquisition of
such a corpus is costly and time-consuming.
This paper presents a method to reduce this
demand using active learning, which selects
what samples to annotate, instead of annotating
blindly the whole training corpus.
Sample selection for annotation is based upon
?representativeness? and ?usefulness?. A
model-based distance is proposed to measure
the difference of two sentences and their most
likely parse trees. Based on this distance, the
active learning process analyzes the sample dis-
tribution by clustering and calculates the den-
sity of each sample to quantify its representa-
tiveness. Further more, a sentence is deemed as
useful if the existing model is highly uncertain
about its parses, where uncertainty is measured
by various entropy-based scores.
Experiments are carried out in the shallow se-
mantic parser of an air travel dialog system.
Our result shows that for about the same pars-
ing accuracy, we only need to annotate a third
of the samples as compared to the usual random
selection method.
1 Introduction
A prerequisite for building statistical parsers (Jelinek et
al., 1994; Collins, 1996; Ratnaparkhi, 1997; Charniak,
1997) is the availability of a (large) corpus of parsed sen-
tences. Acquiring such a corpus is expensive and time-
consuming and is often the bottleneck to build a parser
for a new application or domain. The goal of this study is
to reduce the amount of annotated sentences (and hence
the development time) required for a statistical parser to
achieve a satisfactory performance using active learning.
Active learning has been studied in the context of many
natural language processing (NLP) applications such as
information extraction(Thompson et al, 1999), text clas-
sication(McCallum and Nigam, 1998) and natural lan-
guage parsing(Thompson et al, 1999; Hwa, 2000), to
name a few. The basic idea is to couple tightly knowl-
edge acquisition, e.g., annotating sentences for parsing,
with model-training, as opposed to treating them sepa-
rately. In our setup, we assume that a small amount of
annotated sentences is initially available, which is used
to build a statistical parser. We also assume that there is
a large corpus of unannotated sentences at our disposal ?
this corpus is called active training set. A batch of sam-
ples1 is selected using algorithms developed here, and are
annotated by human beings and are then added to training
data to rebuild the model. The procedure is iterated until
the model reaches a certain accuracy level.
Our efforts are devoted to two aspects: rst, we be-
lieve that the selected samples should reect the underly-
ing distribution of the training corpus. In other words, the
selected samples need to be representative. To this end,
a model-based structural distance is dened to quantify
how ?far? two sentences are apart, and with the help of
this distance, the active training set is clustered so that
we can dene and compute the ?density? of a sample;
second, we propose and test several entropy-based mea-
sures to quantify the uncertainty of a sample in the active
training set using an existing model, as it makes sense
to ask human beings to annotate the portion of data for
which the existing model is not doing well. Samples are
selected from the clusters based on uncertainty scores.
The rest of the paper is organized as follows. In Sec-
tion 2, a structural distance is rst dened based on the se-
quential representation of a parse tree. It is then straight-
forward to employ a k-means algorithm to cluster sen-
tences in the active training set. Section 3 is devoted to
condence measures, where three uncertainty measures
are proposed. Active learning results on the shallow se-
mantic parser of an air travel dialog system are presented
1A sample means a sentence in this paper.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 120-127.
                         Proceedings of the 40th Annual Meeting of the Association for
in Section 4. A summary of related work is given in
Section 5. The paper closes with conclusions and future
work.
2 Sentence Distance and Clustering
To characterize the ?representativeness? of a sentence, we
need to know how far two sentences are apart so that we
can measure roughly how many similar sentences there
are in the active training set. For our purpose, the dis-
tance ought to have the property that two sentences with
similar structures have a small distance, even if they are
lexically different. This leads us to dene the distance be-
tween two sentences based on their parse trees, which are
obtained by applying an existing model to the active train-
ing set. However, computing the distance of two parse
trees requires a digression of how they are represented in
our parser.
2.1 Event Representation of Parse Trees
A statistical parser computes  	
 , the probability of a
parse  given a sentence 	 . Since the space of the entire
parses is too large and cannot be modeled directly, a parse
tree  is decomposed as a series of individual actions
ffBLEU: a Method for Automatic Evaluation of Machine Translation
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598, USA
{papineni,roukos,toddward,weijing}@us.ibm.com
Abstract
Human evaluations of machine translation
are extensive but expensive. Human eval-
uations can take months to finish and in-
volve human labor that can not be reused.
We propose a method of automatic ma-
chine translation evaluation that is quick,
inexpensive, and language-independent,
that correlates highly with human evalu-
ation, and that has little marginal cost per
run. We present this method as an auto-
mated understudy to skilled human judges
which substitutes for them when there is
need for quick or frequent evaluations.1
1 Introduction
1.1 Rationale
Human evaluations of machine translation (MT)
weigh many aspects of translation, including ade-
quacy, fidelity , and fluency of the translation (Hovy,
1999; White and O?Connell, 1994). A compre-
hensive catalog of MT evaluation techniques and
their rich literature is given by Reeder (2001). For
the most part, these various human evaluation ap-
proaches are quite expensive (Hovy, 1999). More-
over, they can take weeks or months to finish. This is
a big problem because developers of machine trans-
lation systems need to monitor the effect of daily
changes to their systems in order to weed out bad
ideas from good ideas. We believe that MT progress
stems from evaluation and that there is a logjam of
fruitful research ideas waiting to be released from
1So we call our method the bilingual evaluation understudy,
BLEU.
the evaluation bottleneck. Developers would bene-
fit from an inexpensive automatic evaluation that is
quick, language-independent, and correlates highly
with human evaluation. We propose such an evalua-
tion method in this paper.
1.2 Viewpoint
How does one measure translation performance?
The closer a machine translation is to a professional
human translation, the better it is. This is the cen-
tral idea behind our proposal. To judge the quality
of a machine translation, one measures its closeness
to one or more reference human translations accord-
ing to a numerical metric. Thus, our MT evaluation
system requires two ingredients:
1. a numerical ?translation closeness? metric
2. a corpus of good quality human reference trans-
lations
We fashion our closeness metric after the highly suc-
cessful word error rate metric used by the speech
recognition community, appropriately modified for
multiple reference translations and allowing for le-
gitimate differences in word choice and word or-
der. The main idea is to use a weighted average of
variable length phrase matches against the reference
translations. This view gives rise to a family of met-
rics using various weighting schemes. We have se-
lected a promising baseline metric from this family.
In Section 2, we describe the baseline metric in
detail. In Section 3, we evaluate the performance of
BLEU. In Section 4, we describe a human evaluation
experiment. In Section 5, we compare our baseline
metric performance with human evaluations.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 311-318.
                         Proceedings of the 40th Annual Meeting of the Association for
2 The Baseline BLEU Metric
Typically, there are many ?perfect? translations of a
given source sentence. These translations may vary
in word choice or in word order even when they use
the same words. And yet humans can clearly dis-
tinguish a good translation from a bad one. For ex-
ample, consider these two candidate translations of
a Chinese source sentence:
Example 1.
Candidate 1: It is a guide to action which
ensures that the military always obeys
the commands of the party.
Candidate 2: It is to insure the troops
forever hearing the activity guidebook
that party direct.
Although they appear to be on the same subject, they
differ markedly in quality. For comparison, we pro-
vide three reference human translations of the same
sentence below.
Reference 1: It is a guide to action that
ensures that the military will forever
heed Party commands.
Reference 2: It is the guiding principle
which guarantees the military forces
always being under the command of the
Party.
Reference 3: It is the practical guide for
the army always to heed the directions
of the party.
It is clear that the good translation, Candidate 1,
shares many words and phrases with these three ref-
erence translations, while Candidate 2 does not. We
will shortly quantify this notion of sharing in Sec-
tion 2.1. But first observe that Candidate 1 shares
"It is a guide to action" with Reference 1,
"which" with Reference 2, "ensures that the
military" with Reference 1, "always" with Ref-
erences 2 and 3, "commands" with Reference 1, and
finally "of the party" with Reference 2 (all ig-
noring capitalization). In contrast, Candidate 2 ex-
hibits far fewer matches, and their extent is less.
It is clear that a program can rank Candidate 1
higher than Candidate 2 simply by comparing n-
gram matches between each candidate translation
and the reference translations. Experiments over
large collections of translations presented in Section
5 show that this ranking ability is a general phe-
nomenon, and not an artifact of a few toy examples.
The primary programming task for a BLEU imple-
mentor is to compare n-grams of the candidate with
the n-grams of the reference translation and count
the number of matches. These matches are position-
independent. The more the matches, the better the
candidate translation is. For simplicity, we first fo-
cus on computing unigram matches.
2.1 Modified n-gram precision
The cornerstone of our metric is the familiar pre-
cision measure. To compute precision, one simply
counts up the number of candidate translation words
(unigrams) which occur in any reference translation
and then divides by the total number of words in
the candidate translation. Unfortunately, MT sys-
tems can overgenerate ?reasonable? words, result-
ing in improbable, but high-precision, translations
like that of example 2 below. Intuitively the prob-
lem is clear: a reference word should be considered
exhausted after a matching candidate word is iden-
tified. We formalize this intuition as the modified
unigram precision. To compute this, one first counts
the maximum number of times a word occurs in any
single reference translation. Next, one clips the to-
tal count of each candidate word by its maximum
reference count,2adds these clipped counts up, and
divides by the total (unclipped) number of candidate
words.
Example 2.
Candidate: the the the the the the the.
Reference 1: The cat is on the mat.
Reference 2: There is a cat on the mat.
Modified Unigram Precision = 2/7.3
In Example 1, Candidate 1 achieves a modified
unigram precision of 17/18; whereas Candidate
2 achieves a modified unigram precision of 8/14.
Similarly, the modified unigram precision in Exam-
ple 2 is 2/7, even though its standard unigram pre-
cision is 7/7.
2Countclip = min(Count,Max Re f Count). In other words,
one truncates each word?s count, if necessary, to not exceed the
largest count observed in any single reference for that word.
3As a guide to the eye, we have underlined the important
words for computing modified precision.
Modified n-gram precision is computed similarly
for any n: all candidate n-gram counts and their
corresponding maximum reference counts are col-
lected. The candidate counts are clipped by their
corresponding reference maximum value, summed,
and divided by the total number of candidate n-
grams. In Example 1, Candidate 1 achieves a mod-
ified bigram precision of 10/17, whereas the lower
quality Candidate 2 achieves a modified bigram pre-
cision of 1/13. In Example 2, the (implausible) can-
didate achieves a modified bigram precision of 0.
This sort of modified n-gram precision scoring cap-
tures two aspects of translation: adequacy and flu-
ency. A translation using the same words (1-grams)
as in the references tends to satisfy adequacy. The
longer n-gram matches account for fluency. 4
2.1.1 Modified n-gram precision on blocks of
text
How do we compute modified n-gram precision
on a multi-sentence test set? Although one typically
evaluates MT systems on a corpus of entire docu-
ments, our basic unit of evaluation is the sentence.
A source sentence may translate to many target sen-
tences, in which case we abuse terminology and re-
fer to the corresponding target sentences as a ?sen-
tence.? We first compute the n-gram matches sen-
tence by sentence. Next, we add the clipped n-gram
counts for all the candidate sentences and divide by
the number of candidate n-grams in the test corpus
to compute a modified precision score, pn, for the
entire test corpus.
pn =
?
C?{Candidates}
?
n-gram?C
Countclip(n-gram)
?
C ??{Candidates}
?
n-gram??C ?
Count(n-gram?) .
4BLEU only needs to match human judgment when averaged
over a test corpus; scores on individual sentences will often vary
from human judgments. For example, a system which produces
the fluent phrase ?East Asian economy? is penalized heavily on
the longer n-gram precisions if all the references happen to read
?economy of East Asia.? The key to BLEU?s success is that
all systems are treated similarly and multiple human translators
with different styles are used, so this effect cancels out in com-
parisons between systems.
2.1.2 Ranking systems using only modified
n-gram precision
To verify that modified n-gram precision distin-
guishes between very good translations and bad
translations, we computed the modified precision
numbers on the output of a (good) human transla-
tor and a standard (poor) machine translation system
using 4 reference translations for each of 127 source
sentences. The average precision results are shown
in Figure 1.
Figure 1: Distinguishing Human from Machine







	



   
tRuEcasIng
Lucian Vlad Lita ?
Carnegie Mellon
llita@cs.cmu.edu
Abe Ittycheriah
IBM T.J. Watson
abei@us.ibm.com
Salim Roukos
IBM T.J. Watson
roukos@us.ibm.com
Nanda Kambhatla
IBM T.J. Watson
nanda@us.ibm.com
Abstract
Truecasing is the process of restoring
case information to badly-cased or non-
cased text. This paper explores truecas-
ing issues and proposes a statistical, lan-
guage modeling based truecaser which
achieves an accuracy of ?98% on news
articles. Task based evaluation shows a
26% F-measure improvement in named
entity recognition when using truecasing.
In the context of automatic content ex-
traction, mention detection on automatic
speech recognition text is also improved
by a factor of 8. Truecasing also en-
hances machine translation output legibil-
ity and yields a BLEU score improvement
of 80.2%. This paper argues for the use of
truecasing as a valuable component in text
processing applications.
1 Introduction
While it is true that large, high quality text corpora
are becoming a reality, it is also true that the digital
world is flooded with enormous collections of low
quality natural language text. Transcripts from var-
ious audio sources, automatic speech recognition,
optical character recognition, online messaging and
gaming, email, and the web are just a few exam-
ples of raw text sources with content often produced
in a hurry, containing misspellings, insertions, dele-
tions, grammatical errors, neologisms, jargon terms
? Work done at IBM TJ Watson Research Center
etc. We want to enhance the quality of such sources
in order to produce better rule-based systems and
sharper statistical models.
This paper focuses on truecasing, which is the
process of restoring case information to raw text.
Besides text rEaDaBILiTY, truecasing enhances the
quality of case-carrying data, brings into the pic-
ture new corpora originally considered too noisy for
various NLP tasks, and performs case normalization
across styles, sources, and genres.
Consider the following mildly ambiguous sen-
tence ?us rep. james pond showed up riding an it
and going to a now meeting?. The case-carrying al-
ternative ?US Rep. James Pond showed up riding an
IT and going to a NOW meeting? is arguably better
fit to be subjected to further processing.
Broadcast news transcripts contain casing errors
which reduce the performance of tasks such as
named entity tagging. Automatic speech recognition
produces non-cased text. Headlines, teasers, section
headers - which carry high information content - are
not properly cased for tasks such as question answer-
ing. Truecasing is an essential step in transforming
these types of data into cleaner sources to be used by
NLP applications.
?the president? and ?the President? are two viable
surface forms that correctly convey the same infor-
mation in the same context. Such discrepancies are
usually due to differences in news source, authors,
and stylistic choices. Truecasing can be used as a
normalization tool across corpora in order to pro-
duce consistent, context sensitive, case information;
it consistently reduces expressions to their statistical
canonical form.
In this paper, we attempt to show the benefits of
truecasing in general as a valuable building block
for NLP applications rather than promoting a spe-
cific implementation. We explore several truecasing
issues and propose a statistical, language modeling
based truecaser, showing its performance on news
articles. Then, we present a straight forward appli-
cation of truecasing on machine translation output.
Finally, we demonstrate the considerable benefits of
truecasing through task based evaluations on named
entity tagging and automatic content extraction.
1.1 Related Work
Truecasing can be viewed in a lexical ambiguity res-
olution framework (Yarowsky, 1994) as discriminat-
ing among several versions of a word, which hap-
pen to have different surface forms (casings). Word-
sense disambiguation is a broad scope problem that
has been tackled with fairly good results generally
due to the fact that context is a very good pre-
dictor when choosing the sense of a word. (Gale
et al, 1994) mention good results on limited case
restoration experiments on toy problems with 100
words. They also observe that real world problems
generally exhibit around 90% case restoration accu-
racy. (Mikheev, 1999) also approaches casing dis-
ambiguation but models only instances when capi-
talization is expected: first word in a sentence, after
a period, and after quotes. (Chieu and Ng, 2002)
attempted to extract named entities from non-cased
text by using a weaker classifier but without focus-
ing on regular text or case restoration.
Accents can be viewed as additional surface forms
or alternate word casings. From this perspective, ei-
ther accent identification can be extended to truecas-
ing or truecasing can be extended to incorporate ac-
cent restoration. (Yarowsky, 1994) reports good re-
sults with statistical methods for Spanish and French
accent restoration.
Truecasing is also a specialized method for
spelling correction by relaxing the notion of casing
to spelling variations. There is a vast literature on
spelling correction (Jones and Martin, 1997; Gold-
ing and Roth, 1996) using both linguistic and statis-
tical approaches. Also, (Brill and Moore, 2000) ap-
ply a noisy channel model, based on generic string
to string edits, to spelling correction.
2 Approach
In this paper we take a statistical approach to true-
casing. First we present the baseline: a simple,
straight forward unigram model which performs rea-
sonably well in most cases. Then, we propose a bet-
ter, more flexible statistical truecaser based on lan-
guage modeling.
From a truecasing perspective we observe four
general classes of words: all lowercase (LC), first
letter uppercase (UC), all letters uppercase (CA), and
mixed case word MC). The MC class could be fur-
ther refined into meaningful subclasses but for the
purpose of this paper it is sufficient to correctly iden-
tify specific true MC forms for each MC instance.
We are interested in correctly assigning case la-
bels to words (tokens) in natural language text. This
represents the ability to discriminate between class
labels for the same lexical item, taking into account
the surrounding words. We are interested in casing
word combinations observed during training as well
as new phrases. The model requires the ability to
generalize in order to recognize that even though the
possibly misspelled token ?lenon? has never been
seen before, words in the same context usually take
the UC form.
2.1 Baseline: The Unigram Model
The goal of this paper is to show the benefits of true-
casing in general. The unigram baseline (presented
below) is introduced in order to put task based eval-
uations in perspective and not to be used as a straw-
man baseline.
The vast majority of vocabulary items have only
one surface form. Hence, it is only natural to adopt
the unigram model as a baseline for truecasing. In
most situations, the unigram model is a simple and
efficient model for surface form restoration. This
method associates with each surface form a score
based on the frequency of occurrence. The decoding
is very simple: the true case of a token is predicted
by the most likely case of that token.
The unigram model?s upper bound on truecasing
performance is given by the percentage of tokens
that occur during decoding under their most frequent
case. Approximately 12% of the vocabulary items
have been observed under more than one surface
form. Hence it is inevitable for the unigram model
to fail on tokens such as ?new?. Due to the over-
whelming frequency of its LC form, ?new? will take
this particular form regardless of what token follows
it. For both ?information? and ?york? as subsequent
words, ?new? will be labeled as LC. For the latter
case, ?new? occurs under one of its less frequent sur-
face forms.
2.2 Truecaser
The truecasing strategy that we are proposing seeks
to capture local context and bootstrap it across a
sentence. The case of a token will depend on the
most likely meaning of the sentence - where local
meaning is approximated by n-grams observed dur-
ing training. However, the local context of a few
words alone is not enough for case disambiguation.
Our proposed method employs sentence level con-
text as well.
We capture local context through a trigram lan-
guage model, but the case label is decided at a sen-
tence level. A reasonable improvement over the un-
igram model would have been to decide the word
casing given the previous two lexical items and their
corresponding case content. However, this greedy
approach still disregards global cues. Our goal is
to maximize the probability of a larger text segment
(i.e. a sentence) occurring under a certain surface
form. Towards this goal, we first build a language
model that can provide local context statistics.
2.2.1 Building a Language Model
Language modeling provides features for a label-
ing scheme. These features are based on the prob-
ability of a lexical item and a case content condi-
tioned on the history of previous two lexical items
and their corresponding case content:
Pmodel(w3|w2, w1) = ?trigramP (w3|w2, w1)
+ ?bigramP (w3|w2)
+ ?unigramP (w3)
+ ?uniformP0 (1)
where trigram, bigram, unigram, and uniform prob-
abilities are scaled by individual ?is which are
learned by observing training examples. wi repre-
sents a word with a case tag treated as a unit for
probability estimation.
2.2.2 Sentence Level Decoding
Using the language model probabilities we de-
code the case information at a sentence level. We
construct a trellis (figure 1) which incorporates all
the sentence surface forms as well as the features
computed during training. A node in this trellis con-
sists of a lexical item, a position in the sentence, a
possible casing, as well as a history of the previous
two lexical items and their corresponding case con-
tent. Hence, for each token, all surface forms will
appear as nodes carrying additional context infor-
mation. In the trellis, thicker arrows indicate higher
transition probabilities.
Figure 1: Given individual histories, the decodings
delay and DeLay, are most probable - perhaps in the
context of ?time delay? and respectively ?Senator
Tom DeLay?
The trellis can be viewed as a Hidden Markov
Model (HMM) computing the state sequence
which best explains the observations. The states
(q1, q2, ? ? ? , qn) of the HMM are combinations of
case and context information, the transition proba-
bilities are the language model (?) based features,
and the observations (O1O2 ? ? ?Ot) are lexical items.
During decoding, the Viterbi algorithm (Rabiner,
1989) is used to compute the highest probability
state sequence (q?? at sentence level) that yields the
desired case information:
q?? = argmaxqi1qi2???qitP (qi1qi2 ? ? ? qit|O1O2 ? ? ?Ot, ?)
(2)
where P (qi1qi2 ? ? ? qit|O1O2 ? ? ?Ot, ?) is the proba-
bility of a given sequence conditioned on the obser-
vation sequence and the model parameters. A more
sophisticated approach could be envisioned, where
either the observations or the states are more expres-
sive. These alternate design choices are not explored
in this paper.
Testing speed depends on the width and length of
the trellis and the overall decoding complexity is:
Cdecoding = O(SMH+1) where S is the sentence
size, M is the number of surface forms we are will-
ing to consider for each word, and H is the history
size (H = 3 in the trigram case).
2.3 Unknown Words
In order for truecasing to be generalizable it must
deal with unknown words ? words not seen during
training. For large training sets, an extreme assump-
tion is that most words and corresponding casings
possible in a language have been observed during
training. Hence, most new tokens seen during de-
coding are going to be either proper nouns or mis-
spellings. The simplest strategy is to consider all
unknown words as being of the UC form (i.e. peo-
ple?s names, places, organizations).
Another approach is to replace the less frequent
vocabulary items with case-carrying special tokens.
During training, the word mispeling is replaced with
by UNKNOWN LC and the word Lenon with UN-
KNOWN UC. This transformation is based on the
observation that similar types of infrequent words
will occur during decoding. This transformation cre-
ates the precedent of unknown words of a particular
format being observed in a certain context. When a
truly unknown word will be seen in the same con-
text, the most appropriate casing will be applied.
This was the method used in our experiments. A
similar method is to apply the case-carrying special
token transformation only to a small random sam-
ple of all tokens, thus capturing context regardless
of frequency of occurrence.
2.4 Mixed Casing
A reasonable truecasing strategy is to focus on to-
ken classification into three categories: LC, UC, and
CA. In most text corpora mixed case tokens such as
McCartney, CoOl, and TheBeatles occur with mod-
erate frequency. Some NLP tasks might prefer map-
ping MC tokens starting with an uppercase letter into
the UC surface form. This technique will reduce the
feature space and allow for sharper models. How-
ever, the decoding process can be generalized to in-
clude mixed cases in order to find a closer fit to the
true sentence. In a clean version of the AQUAINT
(ARDA) news stories corpus, ? 90% of the tokens
occurred under the most frequent surface form (fig-
ure 2).
Figure 2: News domain casing distribution
The expensive brute force approach will consider
all possible casings of a word. Even with the full
casing space covered, some mixed cases will not be
seen during training and the language model prob-
abilities for n-grams containing certain words will
back off to an unknown word strategy. A more fea-
sible method is to account only for the mixed case
items observed during training, relying on a large
enough training corpus. A variable beam decod-
ing will assign non-zero probabilities to all known
casings of each word. An n-best approximation is
somewhat faster and easier to implement and is the
approach employed in our experiments. During the
sentence-level decoding only the n-most-frequent
mixed casings seen during training are considered.
If the true capitalization is not among these n-best
versions, the decoding is not correct. Additional lex-
ical and morphological features might be needed if
identifying MC instances is critical.
2.5 First Word in the Sentence
The first word in a sentence is generally under the
UC form. This sentence-begin indicator is some-
times ambiguous even when paired with sentence-
end indicators such as the period. While sentence
splitting is not within the scope of this paper, we
want to emphasize the fact that many NLP tasks
would benefit from knowing the true case of the first
word in the sentence, thus avoiding having to learn
the fact that beginning of sentences are artificially
important. Since it is uneventful to convert the first
letter of a sentence to uppercase, a more interest-
ing problem from a truecasing perspective is to learn
how to predict the correct case of the first word in a
sentence (i.e. not always UC).
If the language model is built on clean sentences
accounting for sentence boundaries, the decoding
will most likely uppercase the first letter of any sen-
tence. On the other hand, if the language model
is trained on clean sentences disregarding sentence
boundaries, the model will be less accurate since dif-
ferent casings will be presented for the same context
and artificial n-grams will be seen when transition-
ing between sentences. One way to obtain the de-
sired effect is to discard the first n tokens in the train-
ing sentences in order to escape the sentence-begin
effect. The language model is then built on smoother
context. A similar effect can be obtained by initial-
izing the decoding with n-gram state probabilities so
that the boundary information is masked.
3 Evaluation
Both the unigram model and the language model
based truecaser were trained on the AQUAINT
(ARDA) and TREC (NIST) corpora, each consist-
ing of 500M token news stories from various news
agencies. The truecaser was built using IBM?s
ViaVoiceTMlanguage modeling tools. These tools
implement trigram language models using deleted
interpolation for backing off if the trigram is not
found in the training data. The resulting model?s
perplexity is 108.
Since there is no absolute truth when truecasing a
sentence, the experiments need to be built with some
reference in mind. Our assumption is that profes-
sionally written news articles are very close to an
intangible absolute truth in terms of casing. Fur-
thermore, we ignore the impact of diverging stylistic
forms, assuming the differences are minor.
Based on the above assumptions we judge the
truecasing methods on four different test sets. The
first test set (APR) consists of the August 25,
2002 ? top 20 news stories from Associated Press
and Reuters excluding titles, headlines, and sec-
tion headers which together form the second test set
(APR+). The third test set (ACE) consists of ear-
?Randomly chosen test date
Figure 3: LM truecaser vs. unigram baseline.
lier news stories from AP and New York Times be-
longing to the ACE dataset. The last test set (MT)
includes a set of machine translation references (i.e.
human translations) of news articles from the Xin-
hua agency. The sizes of the data sets are as follows:
APR - 12k tokens, ACE - 90k tokens, and MT - 63k
tokens. For both truecasing methods, we computed
the agreement with the original news story consid-
ered to be the ground truth.
3.1 Results
The language model based truecaser consistently
displayed a significant error reduction in case
restoration over the unigram model (figure 3). On
current news stories, the truecaser agreement with
the original articles is ? 98%.
Titles and headlines usually have a higher con-
centration of named entities than normal text. This
also means that they need a more complex model to
assign case information more accurately. The LM
based truecaser performs better in this environment
while the unigram model misses named entity com-
ponents which happen to have a less frequent surface
form.
3.2 Qualitative Analysis
The original reference articles are assumed to have
the absolute true form. However, differences from
these original articles and the truecased articles are
not always casing errors. The truecaser tends to
modify the first word in a quotation if it is not
proper name: ?There has been? becomes ?there has
been?. It also makes changes which could be con-
sidered a correction of the original article: ?Xinhua
BLEU Breakdown
System BLEU 1gr Precision 2gr Precision 3gr Precision 4gr Precision
all lowercase 0.1306 0.6016 0.2294 0.1040 0.0528
rule based 0.1466 0.6176 0.2479 0.1169 0.0627
1gr truecasing 0.2206 0.6948 0.3328 0.1722 0.0988
1gr truecasing+ 0.2261 0.6963 0.3372 0.1734 0.0997
lm truecasing 0.2596 0.7102 0.3635 0.2066 0.1303
lm truecasing+ 0.2642 0.7107 0.3667 0.2066 0.1302
Table 1: BLEU score for several truecasing strategies. (truecasing+ methods additionally employ the ?first
sentence letter uppercased? rule adjustment).
Baseline With Truecasing
Class Recall Precision F Recall Precision F
ENAMEX 48.46 36.04 41.34 59.02 52.65 55.66 (+34.64%)
NUMEX 64.61 72.02 68.11 70.37 79.51 74.66 (+9.62%)
TIMEX 47.68 52.26 49.87 61.98 75.99 68.27 (+36.90%)
Overall 52.50 44.84 48.37 62.01 60.42 61.20 (+26.52%)
Table 2: Named Entity Recognition performance with truecasing and without (baseline).
news agency? becomes ?Xinhua News Agency? and
?northern alliance? is truecased as ?Northern Al-
liance?. In more ambiguous cases both the original
version and the truecased fragment represent differ-
ent stylistic forms: ?prime minister Hekmatyar? be-
comes ?Prime Minister Hekmatyar?.
There are also cases where the truecaser described
in this paper makes errors. New movie names are
sometimes miss-cased: ?my big fat greek wedding?
or ?signs?. In conducive contexts, person names
are correctly cased: ?DeLay said in?. However, in
ambiguous, adverse contexts they are considered to
be common nouns: ?pond? or ?to delay that?. Un-
seen organization names which make perfectly nor-
mal phrases are erroneously cased as well: ?interna-
tional security assistance force?.
3.3 Application: Machine Translation
Post-Processing
We have applied truecasing as a post-processing step
to a state of the art machine translation system in or-
der to improve readability. For translation between
Chinese and English, or Japanese and English, there
is no transfer of case information. In these situations
the translation output has no case information and it
is beneficial to apply truecasing as a post-processing
step. This makes the output more legible and the
system performance increases if case information is
required.
We have applied truecasing to Chinese-to-English
translation output. The data source consists of news
stories (2500 sentences) from the Xinhua News
Agency. The news stories are first translated, then
subjected to truecasing. The translation output is
evaluated with BLEU (Papineni et al, 2001), which
is a robust, language independent automatic ma-
chine translation evaluation method. BLEU scores
are highly correlated to human judges scores, pro-
viding a way to perform frequent and accurate au-
tomated evaluations. BLEU uses a modified n-gram
precision metric and a weighting scheme that places
more emphasis on longer n-grams.
In table 1, both truecasing methods are applied to
machine translation output with and without upper-
casing the first letter in each sentence. The truecas-
ing methods are compared against the all letters low-
ercased version of the articles as well as against an
existing rule-based system which is aware of a lim-
ited number of entity casings such as dates, cities,
and countries. The LM based truecaser is very ef-
fective in increasing the readability of articles and
captures an important aspect that the BLEU score is
sensitive to. Truecasig the translation output yields
Baseline With Truecasing
Source Recall Precision F Recall Precision F
BNEWS ASR 23 3 5 56 39 46 (+820.00%)
BNEWS HUMAN 77 66 71 77 68 72 (+1.41%)
XINHUA 76 71 73 79 72 75 (+2.74%)
Table 3: Results of ACE mention detection with and without truecasing.
an improvement ? of 80.2% in BLEU score over the
existing rule base system.
3.4 Task Based Evaluation
Case restoration and normalization can be employed
for more complex tasks. We have successfully lever-
aged truecasing in improving named entity recogni-
tion and automatic content extraction.
3.4.1 Named Entity Tagging
In order to evaluate the effect of truecasing on ex-
tracting named entity labels, we tested an existing
named entity system on a test set that has signif-
icant case mismatch to the training of the system.
The base system is an HMM based tagger, similar
to (Bikel et al, 1997). The system has 31 semantic
categories which are extensions on the MUC cate-
gories. The tagger creates a lattice of decisions cor-
responding to tokenized words in the input stream.
When tagging a word wi in a sentence of words
w0...wN , two possibilities. If a tag begins:
p(tN1 |wN1 )i = p(ti|ti?1, wi?1)p?(wi|ti, wi?1)
If a tag continues:
p(tN1 |wN1 )i = p(wi|ti, wi?1)
The ? indicates that the distribution is formed from
words that are the first words of entities. The p? dis-
tribution predicts the probability of seeing that word
given the tag and the previous word instead of the
tag and previous tag. Each word has a set of fea-
tures, some of which indicate the casing and embed-
ded punctuation. These models have several levels
of back-off when the exact trigram has not been seen
in training. A trellis spanning the 31 futures is built
for each word in a sentence and the best path is de-
rived using the Viterbi algorithm.
?Truecasing improves legibility, not the translation itself
The performance of the system shown in table 2
indicate an overall 26.52% F-measure improvement
when using truecasing. The alternative to truecas-
ing text is to destroy case information in the train-
ing material 	 SNORIFY procedure in (Bikel et al,
1997). Case is an important feature in detecting
most named entities but particularly so for the title
of a work, an organization, or an ambiguous word
with two frequent cases. Truecasing the sentence is
essential in detecting that ?To Kill a Mockingbird? is
the name of a book, especially if the quotation marks
are left off.
3.4.2 Automatic Content Extraction
Automatic Content Extraction (ACE) is task fo-
cusing on the extraction of mentions of entities and
relations between them from textual data. The tex-
tual documents are from newswire, broadcast news
with text derived from automatic speech recognition
(ASR), and newspaper with text derived from optical
character recognition (OCR) sources. The mention
detection task (ace, 2001) comprises the extraction
of named (e.g. ?Mr. Isaac Asimov?), nominal (e.g.
?the complete author?), and pronominal (e.g. ?him?)
mentions of Persons, Organizations, Locations, Fa-
cilities, and Geo-Political Entities.
The automatically transcribed (using ASR) broad-
cast news documents and the translated Xinhua
News Agency (XINHUA) documents in the ACE
corpus do not contain any case information, while
human transcribed broadcast news documents con-
tain casing errors (e.g. ?George bush?). This prob-
lem occurs especially when the data source is noisy
or the articles are poorly written.
For all documents from broadcast news (human
transcribed and automatically transcribed) and XIN-
HUA sources, we extracted mentions before and af-
ter applying truecasing. The ASR transcribed broad-
cast news data comprised 86 documents containing
a total of 15,535 words, the human transcribed ver-
sion contained 15,131 words. There were only two
XINHUA documents in the ACE test set containing
a total of 601 words. None of this data or any ACE
data was used for training the truecasing models.
Table 3 shows the result of running our ACE par-
ticipating maximum entropy mention detection sys-
tem on the raw text, as well as on truecased text. For
ASR transcribed documents, we obtained an eight
fold improvement in mention detection from 5% F-
measure to 46% F-measure. The low baseline score
is mostly due to the fact that our system has been
trained on newswire stories available from previous
ACE evaluations, while the latest test data included
ASR output. It is very likely that the improvement
due to truecasing will be more modest for the next
ACE evaluation when our system will be trained on
ASR output as well.
4 Possible Improvements & Future Work
Although the statistical model we have considered
performs very well, further improvements must go
beyond language modeling, enhancing how expres-
sive the model is. Additional features are needed
during decoding to capture context outside of the
current lexical item, medium range context, as well
as discontinuous context. Another potentially help-
ful feature to consider would provide a distribu-
tion over similar lexical items, perhaps using an
edit/phonetic distance.
Truecasing can be extended to cover a more gen-
eral notion surface form to include accents. De-
pending on the context, words might take different
surface forms. Since punctuation is a notion exten-
sion to surface form, shallow punctuation restora-
tion (e.g. word followed by comma) can also be ad-
dressed through truecasing.
5 Conclusions
We have discussed truecasing, the process of restor-
ing case information to badly-cased or non-cased
text, and we have proposed a statistical, language
modeling based truecaser which has an agreement
of ?98% with professionally written news articles.
Although its most direct impact is improving legibil-
ity, truecasing is useful in case normalization across
styles, genres, and sources. Truecasing is a valu-
able component in further natural language process-
ing. Task based evaluation shows a 26% F-measure
improvement in named entity recognition when us-
ing truecasing. In the context of automatic content
extraction, mention detection on automatic speech
recognition text is improved by a factor of 8. True-
casing also enhances machine translation output leg-
ibility and yields a BLEU score improvement of
80.2% over the original system.
References
2001. Entity detection and tracking. ACE Pilot Study
Task Definition.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: A high-performance learning name
finder. pages 194?201.
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. ACL.
H.L. Chieu and H.T. Ng. 2002. Teaching a weaker clas-
sifier: Named entity recognition on upper case text.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1994. Discrimination decisions for
100,000-dimensional spaces. Current Issues in Com-
putational Linguistics, pages 429?450.
Andrew R. Golding and Dan Roth. 1996. Applying win-
now to context-sensitive spelling correction. ICML.
M. P. Jones and J. H. Martin. 1997. Contextual spelling
correction using latent semantic analysis. ANLP.
A. Mikheev. 1999. A knowledge-free method for capi-
talized word disambiguation.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. IBM Research Re-
port.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Read-
ings in Speech Recognition, pages 267?295.
David Yarowsky. 1994. Decision lists for ambiguity res-
olution: Application to accent restoration in spanish
and french. ACL, pages 88?95.
Language Model Based Arabic Word Segmentation 
 
Young-Suk Lee     Kishore Papineni      Salim Roukos 
IBM T. J. Watson Research Center 
Yorktown Heights, NY 10598 
 
Ossama Emam    Hany Hassan 
IBM Cairo Technology Development Center 
P.O.Box 166, El-Ahram, Giza, Egypt
  
Abstract 
 
We approximate Arabic?s rich 
morphology by a model that a word 
consists of a sequence of morphemes in 
the pattern prefix*-stem-suffix* (* 
denotes zero or more occurrences of a 
morpheme). Our method is seeded by a 
small manually segmented Arabic corpus 
and uses it to bootstrap an unsupervised 
algorithm to build the Arabic word 
segmenter from a large unsegmented 
Arabic corpus. The algorithm uses a 
trigram language model to determine the 
most probable morpheme sequence for a 
given input. The language model is 
initially estimated from a small manually 
segmented corpus of about 110,000 
words. To improve the segmentation 
accuracy, we use an unsupervised 
algorithm for automatically acquiring 
new stems from a 155 million word 
unsegmented corpus, and re-estimate the 
model parameters with the expanded 
vocabulary and training corpus. The 
resulting Arabic word segmentation 
system achieves around 97% exact match 
accuracy on a test corpus containing 
28,449 word tokens. We believe this is a 
state-of-the-art performance and the 
algorithm can be used for many highly 
inflected languages provided that one can 
create a small manually segmented 
corpus of the language of interest.  
 
 
 
1   Introduction 
 
Morphologically rich languages like       
Arabic present significant challenges to many 
natural language processing applications 
because a word often conveys complex 
meanings decomposable into several 
morphemes (i.e. prefix, stem, suffix).   By 
segmenting words into morphemes, we can 
improve the performance of natural language 
systems including machine translation (Brown 
et al 1993) and information retrieval (Franz, 
M. and McCarley, S. 2002). In this paper, we 
present a general word segmentation algorithm 
for handling inflectional morphology capable 
of segmenting a word into a prefix*-stem-
suffix* sequence, using a small manually 
segmented corpus and a table of 
prefixes/suffixes of the language. We do not 
address Arabic infix morphology where many 
stems correspond to the same root with various 
infix variations; we treat all the stems of a 
common root as separate atomic units. The use 
of a stem as a morpheme (unit of meaning) is 
better suited than the use of a root for the 
applications we are considering in information 
retrieval and machine translation (e.g. different 
stems of the same root translate into different 
English words.) Examples of Arabic words and 
their segmentation into prefix*-stem-suffix* are 
given in Table 1, where '#' indicates a 
morpheme being a prefix, and '+' a suffix.1 As  
                                                          
1 Arabic is presented in both native and Buckwalter 
transliterated Arabic whenever possible. All native 
Arabic is to be read from right-to-left, and transliterated 
Arabic is to be read from left-to-right. The convention of 
shown in Table 1, a word may include multiple 
prefixes, as in   ???? (l: for, Al: the),  or multiple 
suffixes, as in   ????? (t: feminine singular, h: his).  
A word may also consist only of a stem, as in 
 ?????  (AlY, to/towards). 
  The algorithm implementation involves (i) 
language model training on a morpheme-
segmented corpus, (ii) segmentation of input 
text into a sequence of morphemes using the 
language model parameters, and (iii) 
unsupervised acquisition of new stems from a 
large unsegmented corpus. The only linguistic 
resources required include  a small manually 
segmented corpus ranging from 20,000 words 
to 100,000 words, a table of prefixes and 
suffixes of the language and  a large 
unsegmented corpus.   
  In Section 2, we discuss related work. In 
Section 3, we describe the segmentation 
algorithm.  In Section 4, we discuss the  
unsupervised algorithm for new stem 
acquisition. In Section 5, we present 
experimental results. In Section 6, we 
summarize the paper. 
 
2   Related Work 
 
Our work adopts major components of the 
algorithm from (Luo & Roukos 1996): 
language model (LM) parameter estimation 
from a segmented corpus and input 
segmentation on the basis of LM probabilities.  
However, our work diverges from their work 
in two crucial respects: (i) new technique of 
computing all possible segmentations of a 
word into prefix*-stem-suffix* for decoding, 
and  (ii) unsupervised algorithm for new stem 
acquisition based on a stem candidate's 
similarity to stems occurring in the training 
corpus. 
  (Darwish 2002) presents a  supervised 
technique which identifies the root of an 
Arabic word by stripping away the prefix and 
the suffix of the word on the basis of manually 
acquired dictionary of word-root pairs and the 
likelihood that a prefix and a suffix would 
occur with the template from which the root is 
derived. He reports 92.7% segmentation 
accuracy on a 9,606 word evaluation corpus.  
His technique pre-supposes at most one prefix 
and one suffix per stem regardless of the actual 
number and meanings of prefixes/suffixes 
associated with the stem.  (Beesley 1996)  
presents a finite-state morphological analyzer 
for Arabic, which displays the root, pattern, 
and prefixes/suffixes. The analyses are based 
on manually acquired lexicons and rules.  
Although his analyzer is comprehensive in the 
types of knowledge it presents, it has been 
criticized for their extensive development time 
and lack of robustness, cf. (Darwish 2002). 
                                                                                    
marking a prefix with '#" and a suffix with '+' will be 
adopted throughout the paper. 
  (Yarowsky and Wicentowsky 2000) 
presents a minimally supervised morphological 
analysis with a  performance of over 99.2% 
accuracy for the 3,888 past-tense test cases in 
English. The core algorithm lies in the 
estimation of a probabilistic alignment 
between inflected forms and root forms. The 
probability estimation is based on the lemma 
alignment by frequency ratio similarity among 
different inflectional forms derived from the 
same lemma, given a table of inflectional 
parts-of-speech, a list of the canonical suffixes 
for each part of speech, and a list of the 
candidate noun, verb and adjective roots of the 
language.  Their algorithm does not handle 
multiple affixes per word. 
  (Goldsmith 2000) presents an unsupervised 
technique based on the expectation-
maximization algorithm and minimum 
description length to segment exactly one 
suffix per word, resulting in an F-score of 81.8 
for suffix identification in English according to 
(Schone and Jurafsky 2001). (Schone and 
Jurafsky 2001) proposes an unsupervised 
algorithm capable of automatically inducing 
the morphology of inflectional languages using 
only text corpora. Their algorithm combines 
cues from orthography, semantics, and 
contextual information to induce 
morphological relationships in German, Dutch, 
and English, among others. They report F-
scores between 85 and 93 for suffix analyses 
and between 78 and 85 for circumfix analyses 
in these languages. Although their algorithm 
captures prefix-suffix combinations or 
circumfixes, it does not handle the multiple 
affixes per word we observe in Arabic.
 2
                Words            Prefixes                 Stems             Suffixes 
    Arabic    Translit.   Arabic  Translit.    Arabic    Translit.   Arabic   Translit. 
 ????????????? ?   AlwlAyAt  #??    Al#       ????       wlAy      ?? +    +At 
      ???????????    HyAth           ??????     HyA  ?  +? +    +t +h 
 ?????????????    llHSwl  #?#  ??     l# Al#    ??????     HSwl   
         ?????        AlY           ?????      AlY   
 Table 1  Segmentation of Arabic Words into Prefix*-Stem-Suffix* 
 
3  Morpheme Segmentation 
 
3.1 Trigram Language Model 
 
Given an Arabic sentence, we use a trigram 
language model on morphemes to segment it 
into a sequence of morphemes {m1, m2, ?,mn}. 
The input to the morpheme segmenter is a 
sequence of Arabic tokens ? we use a 
tokenizer that looks only at white space and 
other punctuation, e.g. quotation marks, 
parentheses, period, comma, etc.  A sample of 
a manually segmented corpus is given below2. 
Here multiple occurrences of prefixes and 
suffixes per word are marked with an 
underline. 
 
???? # ??? ??????? ???? ?? ?? ??# ?
??? # ???? ?? # ? ??+??? ?? ???? # ??
? ?????? ??? ?+???? ??? ???? # ?? #
 ??? ???+? +? ???? +???? ?? ???  #
??? #?# ??? # ????? ?# ?????? ?? ?? 
?? ??+???? # ? ??????# ??? ???? ? #
 ? ??? ?? ???? ???? ?????? +????? .
????? ?? ?????? #  ?? ???? ??#?# ?# ?
??????? ??????? ????? ???? # ??
??? # ???? ??? ??# ??????? ?? ??
 ?? ?+?? + ??? ???? ??? #? # ????? 
 ?? ?????????+???? ???? 
 
w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# 
Awl fy jA}z +p Al# nmsA Al# EAm Al# 
mADy Ely syAr +p fyrAry $Er b# AlAm fy 
bTn +h ADTr +t +h Aly Al# AnsHAb mn Al#  
tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# 
fHwS +At Al# Drwry +p Hsb mA A$Ar fryq  
 
                                                          
2 A manually segmented Arabic corpus containing about 
140K word tokens has been provided by LDC 
(http://www.ldc.upenn.edu). We divided this corpus into 
training and the development test sets as described in 
Section 5. 
 
 
jAgwAr. w# s# y# Hl sA}q Al# tjArb fy 
jAgwAr Al# brAzyly lwsyAnw bwrty mkAn 
AyrfAyn fy Al# sbAq gdA Al# AHd Al*y s# 
y# kwn Awly xTw +At +h fy EAlm sbAq +At 
AlfwrmwlA 
 
Many instances of prefixes and suffixes in 
Arabic are meaning bearing and correspond to 
a word in English such as pronouns and 
prepositions.  Therefore, we choose a 
segmentation into multiple prefixes and 
suffixes. Segmentation into one prefix  and one 
suffix per word, cf. (Darwish 2002), is not very 
useful for applications like statistical machine 
translation, (Brown et al 1993), for which an 
accurate word-to-word alignment between the 
source and the target languages is critical for 
high quality translations. 
  The trigram language model probabilities 
of morpheme sequences, p(mi|mi-1, mi-2), are 
estimated from the morpheme-segmented 
corpus. At token boundaries, the morphemes 
from previous tokens constitute the histories of 
the current morpheme in the trigram language 
model.  The trigram model is smoothed using 
deleted interpolation with the bigram and 
unigram models, (Jelinek 1997), as in (1): 
 
(1) p(m3 | m1 ,m2) =  ?3 p(m3 |m1 ,m2) + ?2 
p(m3 |m2) + ?3 p(m3), where ?1+?2 +?3 = 1. 
 
  A small morpheme-segmented corpus 
results in a relatively high out of vocabulary 
rate for the stems. We describe below an 
unsupervised acquisition of new stems from a 
large unsegmented Arabic corpus.  However, 
we first describe the segmentation algorithm.   
 
3.2  Decoder for Morpheme Segmentation 
 
 3
We take the unit of decoding to be a sentence 
that has been tokenized using white space and 
punctuation.  The task of a decoder is to find 
the morpheme sequence which maximizes the 
trigram probability of the input sentence, as in 
(2): 
 
(2)  SEGMENTATIONbest = Argmax IIi=1, N 
p(mi|mi-1mi-2), N = number of morphemes in 
the input. 
 
Search algorithm for (2) is informally 
described for each word token as follows: 
 
Step 1: Compute all possible segmentations of 
the token  (to be elaborated in 3.2.1). 
Step 2: Compute the trigram language model 
score of each segmentation.  For some 
segmentations of a token, the stem may be an 
out of vocabulary item. In that case, we use an 
?UNKNOWN? class in the trigram language 
model with the model probability given by 
p(UNKNOWN|mi-1, mi-2) * UNK_Fraction, where 
UNK_Fraction is 1e-9 determined on empirical 
grounds. This allows us to segment new words 
with a high accuracy even with a relatively 
high number of unknown stems in the 
language model vocabulary, cf. experimental 
results in Tables 5 & 6. 
Step 3: Keep the top N highest scored 
segmentations. 
 
3.2.1  Possible Segmentations of  a Word 
 
Possible segmentations of a word token are 
restricted to those derivable from a table of 
prefixes and suffixes of the language for 
decoder speed-up and improved accuracy.   
  Table 2 shows examples of atomic (e.g. ??, 
??) and multi-component (e.g.  ??????     ,???????) 
prefixes and suffixes, along with their 
component morphemes in native Arabic.3 
 
                                                          
3 We have acquired the prefix/suffix table from a 110K 
word manually segmented LDC corpus (51 prefixes & 72 
suffixes) and from IBM-Egypt (additional 14 prefixes & 
122 suffixes). The performance improvement by the 
additional prefix/suffix list ranges from 0.07% to 0.54% 
according to the manually segmented training corpus 
size. The smaller the manually segmented corpus size is, 
the bigger the performance improvement by adding 
additional prefix/suffix list is. 
         Prefixes          Suffixes 
      ??          ??       ??# ??+ 
    ??????        ?#  ??# ?????+   ???     ??+ 
 ???????     ?#  ?#   ??# ?????+?? + ??  
          Table 2  Prefix/Suffix Table 
 
Each token is assumed to have the structure 
prefix*-stem-suffix*, and is compared against 
the prefix/suffix table for segmentation. Given 
a word token, (i) identify all of the matching 
prefixes and suffixes from the table, (ii) further 
segment each matching prefix/suffix at each 
character position, and (iii) enumerate all 
prefix*-stem-suffix* sequences derivable from 
(i) and (ii).  
  Table 3 shows all of its possible 
segmentations of the token ???????  
(wAkrrhA; 'and I repeat it'),4 where ? indicates 
the null prefix/suffix and the Seg Score is the 
language model probabilities of each 
segmentation S1 ... S12. For this token, there 
are two matching prefixes #?(w#) and 
#??(wA#) from the prefix table, and two 
matching suffixes ?+(+A) and ??+(+hA)  
from the suffix table. S1, S2, & S3 are the 
segmentations given the null prefix ? and 
suffixes ?, +A, +hA. S4, S5, & S6 are the 
segmentations given the prefix w# and suffixes 
?, +A, +hA. S7, S8, & S9 are the 
segmentations given the prefix wA# and 
suffixes ?, +A, +hA. S10, S11, & S12 are the 
segmentations given the prefix sequence w# 
A# derived from the prefix wA# and  suffixes 
?, +A, +hA. As illustrated by S12, derivation 
of sub-segmentations of the matching 
prefixes/suffixes enables the system to identify 
possible segmentations which would have been 
missed otherwise. In this case, segmentation 
including the derived prefix sequence               
??+??? # ?# ? (w# A# krr +hA) happens to 
be the correct one.  
 
3.2.2. Prefix-Suffix Filter 
 
While the number of possible segmentations is 
maximized by sub-segmenting matching 
                                                          
4 A sentence in which the token occurs is as follows:  ?????
??????? ???????? ???? ?? ????? ????? ????? ?? ???????? ??????? 
(qlthA wAkrrhA fAlm$klp lyst fy AlfnT AlxAm wAnmA fy 
Alm$tqAt AlnfTyp.) 
 4
prefixes and suffixes, some of illegitimate sub-
segmentations are filtered out on the basis of 
the knowledge specific to the manually 
segmented corpus. For instance, sub-
segmentation of the suffix hA into +h +A is 
ruled out because there is no suffix sequence 
+h +A in the training corpus. Likewise, sub-
segmentation of the prefix Al into A# l# is 
filtered out. Filtering out improbable 
prefix/suffix sequences improves the 
segmentation accuracy, as shown in Table 5. 
 
 Prefix Stem Suffix Seg Scores 
S1 ? wAkrrhA ? 2.6071e-05 
S2 ? wAkrrh +A 1.36561e-06 
S3 ? wAkrr +hA 9.45933e-07 
S4 w# AkrrhA ? 2.72648e-06 
S5 w# Akrrh +A 5.64843e-07 
S6 w# Akrr +hA 4.52229e-05 
S7 wA# krrhA ? 7.58256e-10 
S8 wA# krrh +A 5.09988e-11 
S9 wA# krr +hA 1.91774e-08 
S10 w# A# krrhA ? 7.69038e-07 
S11 w# A# krrh +A 1.82663e-07 
S12 w# A# krr +hA 0.000944511 
Table 3 Possible Segmentations of  
??????? (wAkrrhA) 
 
4  Unsupervised Acquisition  of  New  
Stems 
 
Once the seed segmenter is developed on the 
basis of a manually segmented corpus,  the 
performance may be improved by iteratively 
expanding the stem vocabulary  and retraining 
the language model on a large automatically 
segmented Arabic corpus.  
  Given a small manually segmented corpus 
and a large unsegmented corpus, segmenter 
development proceeds as follows. 
 
Initialization: Develop the seed segmenter 
Segmenter0 trained on the manually segmented 
corpus Corpus0, using the language model 
vocabulary, Vocab0, acquired from Corpus0.  
Iteration: For i = 1 to N, N = the number of 
partitions of the unsegmented corpus 
 i. Use Segmenteri-1 to segment Corpusi. 
 ii.  Acquire new stems from the newly 
segmented Corpusi. Add the new stems to 
Vocabi-1, creating an expanded vocabulary 
Vocabi.  
 iii. Develop Segmenteri trained on Corpus0 
through Corpusi with Vocabi.   
Optimal Performance Identification:  
Identify the Corpusi and Vocabi, which result 
in the best performance, i.e. system training 
with Corpusi+1 and Vocabi+1 does not improve 
the performance any more. 
  Unsupervised acquisition of new stems 
from an automatically segmented new corpus 
is a three-step process: (i)  select new stem 
candidates on the basis of a frequency 
threshold, (ii) filter out new stem candidates  
containing a sub-string with a high likelihood 
of being a prefix, suffix, or prefix-suffix. The 
likelihood of a sub-string being a prefix, suffix, 
and prefix-suffix of a token is computed as in  
(5) to (7), (iii) further filter out new stem 
candidates on the basis of contextual 
information, as in (8). 
 
(5)  Pscore = number of tokens with prefix P / 
number of tokens starting with sub-string P 
(6)  Sscore = number of tokens with suffix S / 
number of tokens ending with sub-string S 
(7)  PSscore = number of tokens with prefix P 
and suffix S / number of tokens starting with 
sub-string P and ending with  sub-string S 
 
Stem candidates containing a sub-string with a 
high prefix, suffix, or prefix-suffix likelihood 
are filtered out. Example sub-strings with the 
prefix, suffix, prefix-suffix likelihood 0.85 or 
higher in a 110K word manually segmented 
corpus are given in Table 4. If a token starts 
with the sub-string ???  (sn), and end with  ???  
(hA), the sub-string's likelihood of being the 
prefix-suffix of the token is 1.  If a token starts 
with the sub-string  ????  (ll), the sub-string's 
likelihood of being the prefix of the token is 
0.945, etc. 
 
        Arabic Transliteration      Score 
 ??? +  stem # ???     sn# stem+hA      1.0 
     ?+ stem # ?????  Al# stem+p      0.984        
         stem # ????   ll# stem      0.945 
  ??+  stem         stem+At      0.889 
    Table 4 Prefix/Suffix Likelihood Score 
 
 5
(8) Contextual Filter: (i) Filter out stems co-
occurring with prefixes/suffixes not present in 
the training corpus. (ii) Filter out stems whose 
prefix/suffix distributions are highly 
disproportionate to those seen in the training 
corpus.  
   According to (8), if a stem is followed by 
a potential suffix +m, not present in the 
training corpus, then it is filtered out as an 
illegitimate stem. In addition, if a stem is 
preceded by a prefix and/or followed by a 
suffix with a significantly higher proportion 
than that observed in the training corpus, it is 
filtered out. For instance, the probability for 
the suffix +A to follow a stem is less than 50% 
in the training corpus regardless of the stem 
properties, and therefore, if a candidate stem is 
followed by +A with the probability of over 
70%, e.g. mAnyl +A, then it is filtered out as 
an illegitimate stem. 
 
5  Performance Evaluations 
 
We present experimental results illustrating the 
impact of three factors on segmentation error 
rate: (i) the base algorithm, i.e. language model 
training and decoding, (ii) language model 
vocabulary and training corpus size, and (iii) 
manually segmented training corpus size.  
Segmentation error rate is defined in (9). 
 
(9)  (number of incorrectly segmented tokens /  
       total number of tokens)  x  100 
 
  Evaluations have been performed on a 
development test corpus containing 28,449 
word tokens.  The test set is extracted from 
20001115_AFP_ARB.0060.xml.txt through 
20001115_AFP_ARB.0236.xml.txt of the 
LDC Arabic Treebank: Part 1 v 2.0 Corpus. 
Impact of the core algorithm and the 
unsupervised stem acquisition has been 
measured on segmenters developed from 4 
different sizes of manually segmented seed 
corpora: 10K, 20K, 40K, and 110K words.    
  The experimental results are shown in 
Table 5. The baseline performances are 
obtained by assigning each token the most 
frequently occurring segmentation in the 
manually segmented training corpus. The 
column headed by '3-gram LM' indicates the 
impact of the segmenter using only trigram 
language model probabilities for decoding. 
Regardless of the manually segmented training 
corpus size, use of  trigram language model 
probabilities reduces the word error rate of the 
corresponding baseline by approximately 50%. 
The column headed by '3-gram LM + PS 
Filter' indicates the impact of the core 
algorithm plus Prefix-Suffix Filter discussed in 
Section 3.2.2. Prefix-Suffix Filter reduces the 
word error rate ranging from 7.4% for the 
smallest (10K word) manually segmented 
corpus to 21.8% for the largest (110K word) 
manually segmented corpus ?- around 1% 
absolute reduction for all segmenters. The 
column headed by '3-gram LM + PS Filter + 
New Stems' shows the impact of unsupervised 
stem acquisition from a 155 million word 
Arabic corpus.  Word error rate reduction due 
to the unsupervised stem acquisition is 38% for 
the segmenter developed from the 10K word 
manually segmented corpus and 32% for the 
segmenter developed from 110K word 
manually segmented corpus. 
  Language model vocabulary size (LM VOC 
Size) and the unknown stem ratio (OOV ratio) 
of various segmenters is given in Table 6. For 
unsupervised stem acquisition, we have set the 
frequency threshold at 10 for every 10-15 
million word corpus, i.e. any new morphemes 
occurring more than 10 times in a 10-15 
million word corpus are considered to be new 
stem candidates. Prefix, suffix, prefix-suffix 
likelihood score to further filter out illegitimate 
stem candidates was set at 0.5 for the 
segmenters developed from 10K, 20K, and 
40K manually segmented corpora, whereas it 
was set at 0.85 for the segmenters developed 
from a 110K manually segmented corpus.  
Both the frequency threshold and the optimal 
prefix, suffix, prefix-suffix likelihood scores 
were determined on empirical grounds. 
Contextual Filter stated in (8) has been applied 
only to the segmenter developed from 110K 
manually segmented training corpus.5 
Comparison of Tables 5 and 6 indicates a high 
correlation between the segmentation error rate 
and the unknown stem ratio.  
                                                          
5 Without the Contextual Filter, the  error rate of the 
same segmenter is 3.1%. 
 6
   
 
Manually Segmented 
Training Corpus Size 
      Baseline  3-gram LM  3-gram LM +  
PS Filter 
3-gram LM + PS 
Filter + New Stems 
        10K Words    26.0%        14.7%            13.6%          8.5% 
        20K Words       19.7%        9.1%            8.0%          5.9% 
        40K Words        14.3%        7.6%            6.5%          5.1% 
      110K Words        11.0%        5.5%            4.3%           2.9% 
Table 5 Impact of Core Algorithm and LM Vocabulary Size on Segmentation Error Rate 
 
                       3-gram LM  3-gram LM + PS Filter + New Stems Manually Segmented 
Training Corpus Size     LM VOC Size      OOV Ratio    LM VOC Size      OOV Ratio 
         10K Words           2,496          20.4%          22,964           7.8% 
         20K Words           4,111          11.4%          25,237           5.3% 
         40K Words           5,531            9.0%          21,156           4.7% 
       110K Words           8,196            5.8%          25,306           1.9% 
             Table 6 Language Model Vocabulary Size and Out of Vocabulary Ratio 
  
                                  3-gram LM + PS Filter + New Stems Manually Segmented 
Training Corpus Size   Unknown Stem          Alywm     Other Errors  Total # of Errors 
         10 K Words    1,844  (76.9%)        98 (4.1%)     455 (19.0%)          2,397 
         20 K Words    1,174  (71.1%)        82 (5.0%)     395 (23.9%)          1,651 
         40 K Words    1,005  (69.9%)        81 (5.6%)     351 (24.4%)          1,437 
       110 K Words       333  (39.6%)        82 (9.8%)     426 (50.7%)             841 
Table 7 Segmentation Error Analyses
  
Table 7 gives the error analyses of four 
segmenters according to three factors: (i) 
errors due to unknown stems, (ii) errors 
involving  ?????????? (Alywm), and (iii) errors due to 
other factors. Interestingly, the segmenter 
developed from a 110K manually segmented 
corpus has the lowest percentage of ?unknown 
stem? errors at 39.6% indicating that our 
unsupervised acquisition of new stems is 
working well, as well as suggesting to use a 
larger unsegmented corpus for unsupervised 
stem acquisition.  
    ?????????? (Alywm) should be segmented 
differently depending on its part-of-speech to 
capture the semantic ambiguities. If it is an 
adverb or a proper noun, it is segmented as 
 ?????????? 'today/Al-Youm', whereas if it is a noun, 
it is segmented as ?? # ??????   'the day.'  Proper 
segmentation of   ?????????? primarily requires its 
part-of-speech information, and cannot be 
easily handled by morpheme trigram models 
alone. 
  Other errors include over-segmentation of  
foreign words such as  ???????????????  (bwtyn) as  ?# 
 ??????????  and  ?????????????  (lytr)  'litre' as ? # ?# ????? .  
These errors are attributed to the segmentation 
ambiguities of these tokens:  ??????????????? is 
ambiguous between ' ??????????????? (Putin)' and '?# 
 ?????????? (by aorta)'.   ?????????????  is ambiguous 
between ' ????????????? (litre)' and ' ? # ?# ?????  (for him 
to harm)'. These errors may also be corrected 
by incorporating part-of-speech information 
for disambiguation. 
  To address the segmentation ambiguity 
problem, as illustrated by ' ??????????????? (Putin)' vs. 
' ? # ??????????  (by aorta)', we have developed a 
joint model for segmentation and part-of-
speech tagging for which the best 
segmentation of an input sentence is obtained 
according to the formula (10), where ti is the 
part-of-speech of morpheme mi, and N is the 
number of morphemes in the input sentence. 
 
(10) SEGMENTATIONbest = Argmax ?i=1,N  
p(mi|mi-1 mi-2) p(ti|ti-1 ti-2) p(mi|ti) 
 
By using the joint model, the segmentation 
word error rate of the best performing 
segmenter has been reduced by about 10% 
 7
from 2.9% (cf. the last column of Table 5) to 
2.6%. 
   
5  Summary and Future Work 
 
We have presented a robust word segmentation 
algorithm which segments a word into a 
prefix*-stem-suffix* sequence, along with 
experimental results. Our Arabic word 
segmentation system implementing the 
algorithm achieves around 97% segmentation 
accuracy on a development test corpus 
containing 28,449 word tokens. Since the 
algorithm can identify any number of prefixes 
and suffixes of a given token, it is generally 
applicable to various language families 
including agglutinative languages (Korean, 
Turkish, Finnish), highly inflected languages 
(Russian, Czech) as well as semitic languages 
(Arabic, Hebrew). 
   Our future work includes (i) application 
of the current technique to other highly 
inflected languages, (ii) application of the 
unsupervised stem acquisition technique on 
about 1 billion word unsegmented Arabic 
corpus, and (iii) adoption of a novel 
morphological analysis technique to handle 
irregular morphology, as realized in Arabic 
broken plurals  ????????? (ktAb) 'book' vs.  ???????? 
(ktb) 'books'. 
 
Acknowledgment 
 
This work was partially supported by the 
Defense Advanced Research Projects Agency 
and monitored by SPAWAR under contract No. 
N66001-99-2-8916. The views and findings 
contained in this material are those of the 
authors and do not necessarily reflect the 
position of policy of the Government and no 
official endorsement should be inferred. We 
would like to thank Martin Franz for discussions 
on language model building, and his help with 
the use of ViaVoice language model toolkit. 
 
References 
 
Beesley, K. 1996. Arabic Finite-State 
 Morphological Analysis and Generation. 
 Proceedings of COLING-96, pages 89?  94. 
Brown, P., Della Pietra, S., Della Pietra, V., 
 and Mercer, R. 1993. The mathematics  of 
 statistical machine translation:  Parameter 
 Estimation. Computational  Linguistics, 
 19(2): 263?311. 
Darwish, K. 2002. Building a Shallow  Arabic 
 Morphological Analyzer in  One  Day. 
 Proceedings of the  Workshop on 
 Computational  Approaches to Semitic 
 Languages,  pages 47?54.  
Franz, M. and McCarley, S. 2002. Arabic 
 Information Retrieval at IBM.  Proceedings 
 of TREC 2002, pages 402? 405. 
Goldsmith, J. 2000. Unsupervised  learning 
 of  the morphology of a natural  language.   
 Computational Linguistics, 27(1). 
Jelinek, F. 1997. Statistical Methods for 
 Speech Recognition. The MIT Press. 
Luo, X. and Roukos, S. 1996. An Iterative 
 Algorithm to Build Chinese Language 
 Models. Proceedings of ACL-96, pages 
 139?143. 
Schone, P. and Jurafsky, D. 2001. 
 Knowledge-Free Induction of  Inflectional 
 Morphologies. Proceedings  of  North 
 American Chapter of  Association for 
 Computational  Linguistics. 
Yarowsky, D. and Wicentowski, R. 2000. 
 Minimally supervised morphological 
 analysis by multimodal alignment. 
 Proceedings of ACL-2000, pages 207? 216. 
Yarowsky, D, Ngai G. and Wicentowski, R. 
 2001. Inducting Multilingual Text  Analysis 
 Tools via Robust Projection  across Aligned 
 Corpora. Proceedings of  HLT 2001, pages 
 161?168. 
 
 
 
 8
A Mention-Synchronous Coreference Resolution Algorithm Based on the
Bell Tree
Xiaoqiang Luo and Abe Ittycheriah
Hongyan Jing and Nanda Kambhatla and Salim Roukos
1101 Kitchawan Road
Yorktown Heights, NY 10598, U.S.A.
{xiaoluo,abei,hjing,nanda,roukos}@us.ibm.com
Abstract
This paper proposes a new approach for
coreference resolution which uses the Bell
tree to represent the search space and casts
the coreference resolution problem as finding
the best path from the root of the Bell tree to
the leaf nodes. A Maximum Entropy model
is used to rank these paths. The coreference
performance on the 2002 and 2003 Auto-
matic Content Extraction (ACE) data will be
reported. We also train a coreference system
using the MUC6 data and competitive results
are obtained.
1 Introduction
In this paper, we will adopt the terminologies used in
the Automatic Content Extraction (ACE) task (NIST,
2003). Coreference resolution in this context is defined
as partitioning mentions into entities. A mention is an
instance of reference to an object, and the collection
of mentions referring to the same object in a document
form an entity. For example, in the following sentence,
mentions are underlined:
?The American Medical Association voted
yesterday to install the heir apparent as its
president-elect, rejecting a strong, upstart
challenge by a District doctor who argued
that the nation?s largest physicians? group
needs stronger ethics and new leadership.?
?American Medical Association?, ?its? and ?group?
belong to the same entity as they refer to the same ob-
ject.
Early work of anaphora resolution focuses on find-
ing antecedents of pronouns (Hobbs, 1976; Ge et al,
1998; Mitkov, 1998), while recent advances (Soon et
al., 2001; Yang et al, 2003; Ng and Cardie, 2002; Itty-
cheriah et al, 2003) employ statistical machine learn-
ing methods and try to resolve reference among all
kinds of noun phrases (NP), be it a name, nominal, or
pronominal phrase ? which is the scope of this paper
as well. One common strategy shared by (Soon et al,
2001; Ng and Cardie, 2002; Ittycheriah et al, 2003) is
that a statistical model is trained to measure how likely
a pair of mentions corefer; then a greedy procedure is
followed to group mentions into entities. While this ap-
proach has yielded encouraging results, the way men-
tions are linked is arguably suboptimal in that an instant
decision is made when considering whether two men-
tions are linked or not.
In this paper, we propose to use the Bell tree to rep-
resent the process of forming entities from mentions.
The Bell tree represents the search space of the coref-
erence resolution problem ? each leaf node corresponds
to a possible coreference outcome. We choose to model
the process from mentions to entities represented in the
Bell tree, and the problem of coreference resolution is
cast as finding the ?best? path from the root node to
leaves. A binary maximum entropy model is trained to
compute the linking probability between a partial entity
and a mention.
The rest of the paper is organized as follows. In
Section 2, we present how the Bell tree can be used
to represent the process of creating entities from men-
tions and the search space. We use a maximum en-
tropy model to rank paths in the Bell tree, which is dis-
cussed in Section 3. After presenting the search strat-
egy in Section 4, we show the experimental results on
the ACE 2002 and 2003 data, and the Message Under-
standing Conference (MUC) (MUC, 1995) data in Sec-
tion 5. We compare our approach with some recent
work in Section 6.
2 Bell Tree: From Mention to Entity
Let us consider traversing mentions in a document from
beginning (left) to end (right). The process of form-
ing entities from mentions can be represented by a tree
structure. The root node is the initial state of the pro-
cess, which consists of a partial entity containing the
first mention of a document. The second mention is
[1][2] 3*
[1][2][3]
[1] [23]
[13][2]
[123]
[12][3]
[1] 2* 3
[1]
[12]
[1]
[2]
(c1)
(c5)
(b1)
(c2)
(c3)
(c4)
(a) [12] 3*
(b2)
Figure 1: Bell tree representation for three mentions:
numbers in [] denote a partial entity. In-focus entities
are marked on the solid arrows, and active mentions
are marked by *. Solid arrows signify that a mention
is linked with an in-focus partial entity while dashed
arrows indicate starting of a new entity.
added in the next step by either linking to the exist-
ing entity, or starting a new entity. A second layer
of nodes are created to represent the two possible out-
comes. Subsequent mentions are added to the tree in
the same manner. The process is mention-synchronous
in that each layer of tree nodes are created by adding
one mention at a time. Since the number of tree leaves
is the number of possible coreference outcomes and it
equals the Bell Number (Bell, 1934), the tree is called
the Bell tree. The Bell Number
 
is the num-
ber of ways of partitioning

distinguishable objects
(i.e., mentions) into non-empty disjoint subsets (i.e.,
entities). The Bell Number has a ?closed? formula
 
	


and it increases rapidly as

in-
creases:
 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1040?1047,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Extracting Social Networks and Biographical Facts From Conversational
Speech Transcripts
Hongyan Jing
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
hjing@us.ibm.com
Nanda Kambhatla
IBM India Research Lab
EGL, Domlur Ring Road
Bangalore - 560071, India
kambhatla@in.ibm.com
Salim Roukos
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
roukos@us.ibm.com
Abstract
We present a general framework for
automatically extracting social networks
and biographical facts from conversational
speech. Our approach relies on fusing
the output produced by multiple informa-
tion extraction modules, including entity
recognition and detection, relation detec-
tion, and event detection modules. We
describe the specific features and algo-
rithmic refinements effective for conver-
sational speech. These cumulatively in-
crease the performance of social network
extraction from 0.06 to 0.30 for the devel-
opment set, and from 0.06 to 0.28 for the
test set, as measured by f-measure on the
ties within a network. The same frame-
work can be applied to other genres of text
? we have built an automatic biography
generation system for general domain text
using the same approach.
1 Introduction
A social network represents social relationships
between individuals or organizations. It consists
of nodes and ties. Nodes are individual actors
within the networks, generally a person or an or-
ganization. Ties are the relationships between the
nodes. Social network analysis has become a key
technique in many disciplines, including modern
sociology and information science.
In this paper, we present our system for au-
tomatically extracting social networks and bio-
graphical facts from conversational speech tran-
scripts by integrating the output of different IE
modules. The IE modules are the building blocks;
the fusing module depicts the ways of assembling
these building blocks. The final output depends on
which fundamental IE modules are used and how
their results are integrated.
The contributions of this work are two fold. We
propose a general framework for extracting social
networks and biographies from text that applies to
conversational speech as well as other genres, in-
cluding general newswire stories. Secondly, we
present specific methods that proved effective for
us for improving the performance of IE systems on
conversational speech transcripts. These improve-
ments include feature engineering and algorithmic
revisions that led to a nearly five-fold performance
increase for both development and test sets.
In the next section, we present our framework
for extracting social networks and other biograph-
ical facts from text. In Section 3, we discuss the
refinements we made to our IE modules in order
to reliably extract information from conversational
speech transcripts. In Section 4, we describe the
experiments, evaluation metrics, and the results of
social network and biography extraction. In Sec-
tion 5, we show the results of applying the frame-
work to other genres of text. Finally, we discuss
related work and conclude with lessons learned
and future work.
2 The General Framework
For extraction of social networks and biographi-
cal facts, our approach relies on three standard IE
modules ? entity detection and recognition, rela-
tion detection, and event detection ? and a fusion
module that integrates the output from the three IE
systems.
2.1 Entity, Relation, and Event Detection
We use the term entity to refer to a person, an or-
ganization, or other real world entities, as adopted
1040
in the Automatic Content Extraction (ACE) Work-
shops (ACE, 2005). A mention is a reference to
a real world entity. It can be named (e.g. ?John
Lennon?), nominal (e.g. ?mother?), or pronomi-
nal (e.g. ?she?).
Entity detection is generally accomplished in
two steps: first, a mention detection module iden-
tifies all the mentions of interest; second, a co-
reference module merges mentions that refer to the
same entity into a single co-reference chain.
A relation detection system identifies (typi-
cally) binary relationships between pairs of men-
tions. For instance, for the sentence ?I?m in New
York?, the following relation exists: locatedAt (I,
New York).
An event detection system identifies events of
interest and the arguments of the event. For ex-
ample, from the sentence ?John married Eva in
1940?, the system should identify the marriage
event, the people who got married and the time
of the event.
The latest ACE evaluations involve all of the
above tasks. However, as shown in the next sec-
tion, our focus is quite different from ACE ?
we are particularly interested in improving perfor-
mance for conversational speech and building on
top of ACE tasks to produce social networks and
biographies.
2.2 Fusion Module
The fusion module merges the output from IE
modules to extract social networks and biographi-
cal facts. For example, if a relation detection sys-
tem has identified the relation motherOf (mother,
my) from the input sentence ?my mother is a
cook?, and if an entity recognition module has
generated entities referenced by the mentions {my,
Josh, me, I, I, ......} and {mother, she, her, her,
Rosa......}, then by replacing my and mother with
the named mentions within the same co-reference
chains, the fusion module produces the follow-
ing nodes and ties in a social network: motherOf
(Rosa, Josh).
We generate the nodes of social networks by se-
lecting all the PERSON entities produced by the
entity recognition system. Typically, we only in-
clude entities that contain at least one named men-
tion. To identify ties between nodes, we retrieve
all relations that indicate social relationships be-
tween a pair of nodes in the network.
We extract biographical profiles by selecting the
events (extracted by the event extraction module)
and corresponding relations (extracted by the rela-
tion extraction module) that involve a given indi-
vidual as an argument. When multiple documents
are used, then we employ a cross-document co-
reference system.
3 Improving Performance for
Conversational Speech Transcripts
Extracting information from conversational
speech transcripts is uniquely challenging. In this
section, we describe the data collection used in
our experiments, and explain specific techniques
we used to improve IE performance on this data.
3.1 Conversational Speech Collection
We use a corpus of videotaped, digitized oral in-
terviews with Holocaust survivors in our experi-
ments. This data was collected by the USC Shoah
Foundation Institute (formerly known as the Vi-
sual History Foundation), and has been used in
many research activities under the Multilingual
Access to Large Spoken Archives (MALACH)
project (Gustman et al, 2002; Oard et al, 2004).
The collection contains oral interviews in 32 lan-
guages from 52,000 survivors, liberators, rescuers
and witnesses of the Holocaust.
This data is very challenging. Besides the usual
characteristics of conversational speech, such as
speaker turns and speech repairs, the interview
transcripts contain a large percentage of ungram-
matical, incoherent, or even incomprehensible
clauses (a sample interview segment is shown in
Figure 1). In addition, each interview covers many
people and places over a long time period, which
makes it even more difficult to extract social net-
works and biographical facts.
speaker2 in on that ninth of Novem-
ber nineteen hundred thirty eight I was
with my parents at home we heard
not through the we heard even through
the windows the crashing of glass the
crashing of and and they are our can?t
Figure 1: Sample interview segment.
3.2 The Importance of Co-reference
Resolution
Our initial attempts at social network extraction
for the above data set resulted in a very poor score
1041
of 0.06 f-measure for finding the relations within
a network (as shown in Table 3 as baseline perfor-
mance).
An error analysis indicated poor co-reference
resolution to be the chief culprit for the low per-
formance. For instance, suppose we have two
clauses: ?his mother?s name is Mary? and ?his
brother Mark went to the army?. Further sup-
pose that ?his? in the first clause refers to a
person named ?John? and ?his? in the second
clause refers to a person named ?Tim?. If the
co-reference system works perfectly, the system
should find a social network involving four peo-
ple: {John, Tim, Mary, Mark}, and the ties: moth-
erOf (Mary, John), and brotherOf (Mark, Tim).
However, if the co-reference system mistakenly
links ?John? to ?his? in the second clause and links
?Tim? to ?his? in the first clause, then we will still
have a network with four people, but the ties will
be: motherOf (Mary, Tim), and brotherOf (Mark,
John), which are completely wrong. This example
shows that co-reference errors involving mentions
that are relation arguments can lead to very bad
performance in social network extraction.
Our existing co-reference module is a state-of-
the-art system that produces very competitive re-
sults compared to other existing systems (Luo et
al., 2004). It traverses the document from left to
right and uses a mention-synchronous approach to
decide whether a mention should be merged with
an existing entity or start a new entity.
However, our existing system has shortcomings
for this data: the system lacks features for han-
dling conversational speech, and the system of-
ten makes mistakes in pronoun resolution. Re-
solving pronominal references is very important
for extracting social networks from conversational
speech, as illustrated in the previous example.
3.3 Improving Co-reference for
Conversational Speech
We developed a new co-reference resolution sys-
tem for conversational speech transcripts. Simi-
lar to many previous works on co-reference (Ng,
2005), we cast the problem as a classification task
and solve it in two steps: (1) train a classifier to
determine whether two mentions are co-referent or
not, and (2) use a clustering algorithm to partition
the mentions into clusters, based on the pairwise
predictions.
We added many features to our model specifi-
cally designed for conversational speech, and sig-
nificantly improved the agglomerative clustering
used for co-reference, including integrating rela-
tions as constraints, and designing better cluster
linkage methods and clustering stopping criteria.
3.3.1 Adding Features for Conversational
Speech
We added many features to our model specifi-
cally designed for conversational speech:
Speaker role identification. In manual tran-
scripts, the speaker turns are given and each
speaker is labeled differently (e.g. ?speaker1?,
?speaker2?), but the identity of the speaker is not
given. An interview typically involves 2 or more
speakers and it is useful to identify the roles of
each speaker (e.g. interviewer, interviewee, etc.).
For instance, ?you? spoken by the interviewer is
likely to be linked with ?I? spoken by the inter-
viewee, but ?you? spoken by the third person in
the interview is more likely to be referring to the
interviewer than to the interviewee.
We developed a program to identify the speaker
roles. The program classifies the speakers into
three categories: interviewer, interviewee, and
others. The algorithm relies on three indicators
? number of turns by each speaker, difference in
number of words spoken by each speaker, and the
ratio of first-person pronouns such as ?I?, ?me?,
and ?we? vs. second-person pronouns such as
?you? and ?your?. This speaker role identifica-
tion program works very well when we checked
the results on the development and test set ? the
interviewers and survivors in all the documents in
the development set were correctly identified.
Speaker turns. Using the results from the
speaker role identification program, we enrich cer-
tain features with speaker turn information. For
example, without this information, the system can-
not distinguish ?I? spoken by an interviewer from
?I? spoken by an interviewee.
Spelling features for speech transcripts. We
add additional spelling features so that mentions
such as ?Cyla C Y L A Lewin? and ?Cyla Lewin?
are considered as exact matches. Names with
spelled-out letters occur frequently in our data col-
lection.
Name Patterns. We add some features that
capture frequent syntactic structures that speakers
use to express names, such as ?her name is Irene?,
?my cousin Mark?, and ?interviewer Ellen?.
Pronoun features. To improve the perfor-
1042
mance on pronouns, we add features such as the
speaker turns of the pronouns, whether the two
pronouns agree in person and number, whether
there exist other mentions between them, etc.
Other miscellaneous features. We also in-
clude other features such as gender, token dis-
tance, sentence distance, and mention distance.
We trained a maximum-entropy classifier using
these features. For each pair of mentions, the clas-
sifier outputs the probability that the two mentions
are co-referent.
We also modified existing features to make
them more applicable to conversational speech.
For instance, we added pronoun-distance features
taking into account the presence of other pronom-
inal references in between (if so, the types of the
pronouns), other mentions in between, etc.
3.3.2 Improving Agglomerative Clustering
We use an agglomerative clustering approach
for partitioning mentions into entities. This is a
bottom-up approach which joins the closest pair
of clusters (i.e., entities) first. Initially, each men-
tion is placed into its own cluster. If we have N
mentions to cluster, we start with N clusters.
The intuition behind choosing the agglomera-
tive method is to merge the most confident pairs
first, and use the properties of existing clusters to
constrain future clustering. This seems to be espe-
cially important for our data collection, since con-
versational speech tends to have a lot of repetitions
or local structures that indicate co-reference. In
such cases, it is beneficial to merge these closely
related mentions first.
Cluster linkage method. In agglomerative
clustering, each cycle merges two clusters into a
single cluster, thus reducing the number of clus-
ters by one. We need to decide upon a method of
measuring the distance between two clusters.
At each cycle, the two mentions with the high-
est co-referent probability are linked first. This re-
sults in the merging of the two clusters that contain
these two mentions.
We improve upon this method by imposingmin-
imal distance criteria between clusters. Two clus-
ters C
1
and C
2
can be combined only if the dis-
tance between all the mentions from C
1
and all
the mentions from C
2
is above the minimal dis-
tance threshold. For instance, suppose C
1
=
{he, father}, and C
2
= {he, brother}, and ?he?
from C
1
and ?he? from C
2
has the highest linkage
probability. The standard single linkage method
will combine these two clusters, despite the fact
that ?father? and ?brother? are very unlikely to
be linked. Imposing minimal distance criteria
can solve this problem and prevent the linkage of
clusters which contain very dissimilar mentions.
In practice, we used multiple minimal distance
thresholds, such as minimal distance between two
named mentions and minimal distance between
two nominal mentions.
We chose not to use complete or average link-
age methods. In our data collection, the narrations
contain a lot of pronouns and the focus tends to
be very local. Whereas the similarity model may
be reasonably good at predicting the distance be-
tween two pronouns that are close to each other, it
is not good at predicting the distance between pro-
nouns that are furthur apart. Therefore, it seems
more reasonable to use single linkage method with
modifications than complete or average linkage
methods.
Using relations to constrain clustering. An-
other novelty of our co-reference system is the
use of relations for constraining co-reference. The
idea is that two clusters should not be merged if
such merging will introduce contradictory rela-
tions. For instance, if we know that person entity
A is the mother of person entity B, and person en-
tity C is the sister of B, then A and C should not
be linked since the resulting entity will be both the
mother and the sister of B.
We construct co-existent relation sets from the
training data. For any two pairs of entities, we col-
lect all the types of relations that exist between
them. These types of relations are labeled as
co-existent. For instance, ?motherOf? and ?par-
entOf? can co-exist, but ?motherOf? and ?sis-
terOf? cannot. By using these relation constraints,
the system refrains from generating contradictory
relations in social networks.
Speed improvement. Suppose the number of
mentions is N , the time complexity of simple link-
age method is O(N2). With the minimal dis-
tance criteria, the complexity is O(N3). However,
N can be dramatically reduced for conversational
transcripts by first linking all the first-person pro-
nouns by each speaker.
4 Experiments
In this section, we describe the experimental setup
and present sample outputs and evaluation results.
1043
Train Dev Test
Words 198k 73k 255k
Mentions 43k 16k 56k
Relations 7K 3k 8k
Table 2: Experimental Data Sets.
4.1 Data Annotation
The data used in our experiments consist of partial
or complete English interviews of Holocaust sur-
vivors. The input to our system is transcripts of
interviews.
We manually annotated manual transcripts with
entities, relations, and event categories, specifi-
cally designed for this task and the results of care-
ful data analysis. The annotation was performed
by a single annotator over a few months. The an-
notation categories for entities, events, and rela-
tions are shown in Table 1. Please note that the
event and relation definitions are slightly different
than the definitions in ACE.
4.2 Training and Test Sets
We divided the data into training, development,
and test data sets. Table 2 shows the size of each
data set. The training set includes transcripts of
partial interviews. The development set consists
of 5 complete interviews, and the test set con-
sists of 15 complete interviews. The reason that
the training set contains only partial interviews is
due to the high cost of transcription and annota-
tion. Since those partial interviews had already
been transcribed for speech recognition purpose,
we decided to reuse them in our annotation. In ad-
dition, we transcribed and annotated 20 complete
interviews (each interview is about 2 hours) for
building the development and test sets, in order
to give a more accurate assessment of extraction
performance.
4.3 Implementation
We developed the initial entity detection, rela-
tion detection, and event detection systems using
the same techniques as our submission systems to
ACE (Florian et al, 2004). Our submission sys-
tems use statistical approaches, and have ranked
in the top tier in ACE evaluations. We easily built
the models for our application by retraining exist-
ing systems with our training set.
The entity detection task is accomplished in two
steps: mention detection and co-reference resolu-
tion. The mention detection is formulated as a la-
Figure 2: Social network extracted by the system.
beling problem, and a maximum-entropy classifier
is trained to identify all the mentions.
Similarly, relation detection is also cast as a
classification problem ? for each pair of men-
tions, the system decides which type of relation
exists between them. It uses a maximum-entropy
classifier and various lexical, contextual, and syn-
tactic features for such predications.
Event detection is accomplished in two steps:
first, identifying the event anchor words using an
approach similar to mention detection; then, iden-
tifying event arguments using an approach similar
to relation detection.
The co-reference resolution system for conver-
sational speech and the fusion module were devel-
oped anew.
4.4 The Output
The system aims to extract the following types of
information:
? The social network of the survivor.
? Important biographical facts about each per-
son in the social network.
? Track the movements of the survivor and
other individuals in the social network.
Figure 2 shows a sample social network ex-
tracted by the system (only partial of the network
is shown). Figure 3 shows sample biographical
facts and movement summaries extracted by the
system. In general, we focus more on higher pre-
cision than recall.
4.5 Evaluation
In this paper, we focus only on the evaluation
of social network extraction. We first describe
the metrics for social network evaluation and then
present the results of the system.
1044
Entity (12) Event (8) Relation (34)
Social Rels (12) Event Args (8) Bio Facts (14)
AGE CUSTODY aidgiverOf affectedBy bornAt
COUNTRY DEATH auntOf agentOf bornOn
DATE HIDING cousinOf participantIn citizenOf
DATEREF LIBERATION fatherOf timeOf diedAt
DURATION MARRIAGE friendOf travelArranger diedOn
GHETTOORCAMP MIGRATION grandparentOf travelFrom employeeOf
OCCUPATION SURVIVAL motherOf travelPerson hasProperty
ORGANIZATION VIOLENCE otherRelativeOf travelTo locatedAt
OTHERLOC parentOf managerOf
PEOPLE siblingOf memberOf
PERSON spouseOf near
SALUTATION uncleOf partOf
partOfMany
resideIn
Table 1: Annotation Categories for Entities, Events, and Relations.
Sidonia Lax:
date of birth: June the eighth nineteen twenty
seven
Movements:
Moved To: Auschwitz
Moved To: United States
... ...
Figure 3: Biographical facts and movement sum-
maries extracted by the system.
To compare two social networks, we first need
to match the nodes and ties between the networks.
Two nodes (i.e., entities) are matched if they have
the same canonical name. Two ties (i.e., edges or
relations) are matched if these three criteria are
met: they contain the same type of relations, the
arguments of the relation are the same, and the or-
der of the arguments are the same if the relation is
unsymmetrical.
We define the the following measurements for
social network evaluation: the precision for nodes
(or ties) is the ratio of common nodes (or ties) in
the two networks to the total number of nodes (or
ties) in the system output, the recall for nodes (or
ties) is the ratio of common nodes (or ties) in the
two networks to the total number of nodes/ties in
the reference output, and the f-measure for nodes
(or ties) is the harmonic mean of precision and re-
call for nodes (or ties). The f-measure for ties in-
dicates the overall performance of social network
extraction.
F-mea Dev Test
Baseline New Baseline New
Nodes 0.59 0.64 0.62 0.66
Ties 0.06 0.30 0.06 0.28
Table 3: Performance of social network extraction.
Table 3 shows the results of social network ex-
traction. The new co-reference approach improves
the performance for f-measure on ties by five-fold
on development set and by nearly five-fold for test
set.
We also tested the system using automatic tran-
scripts by our speech recognition system. Not sur-
prisingly, the result is much worse: the nodes f-
measure is 0.11 for the test set, and the system
did not find any relations. A few factors are ac-
countable for this low performance: (1) Speech
recognition is very challenging for this data set,
since the testimonies contained elderly, emotional,
accented speech. Given that the speech recogni-
tion system fails to recognize most of the person
names, extraction of social networks is difficult.
(2) The extraction systems perform worse on au-
tomatic transcripts, due to the quality of the auto-
matic transcript, and the discrepancy between the
training and test data. (3) Our measurements are
very strict, and no partial credit is given to partially
correct entities or relations.
We decided not to present the evaluation results
of the individual components since the perfor-
mance of individual components are not at all in-
dicative of the overall performance. For instance,
a single pronoun co-reference error might slighlty
1045
change the co-reference score, but can introduce a
serious error in the social network, as shown in the
example in Section 3.2.
5 Biography Generation from General
Domain Text
We have applied the same framework to biogra-
phy generation from general news articles. This
general system also contains three fundamental IE
systems and a fusion module, similar to the work
presented in the paper. The difference is that the IE
systems are trained on general news text using dif-
ferent categories of entities, relations, and events.
A sample biography output extracted from
TDT5 English documents is shown in Figure 4.
The numbers in brackets indicate the corpus count
of the facts.
Saddam Hussein:
Basic Information:
citizenship: Iraq [203]
occupation: president [4412], leader [1792],
dictator [664],...
relative: odai [89], qusay [65], uday [65],...
Life Events:
places been to: bagdad [403], iraq [270],
palaces [149]...
Organizations associated with: manager of
baath party [1000], ...
Custody Events: Saddam was arrested [52],
Communication Events: Saddam said [3587]
... ...
Figure 4: Sample biography output.
6 Related Work
While there has been previous work on extracting
social networks from emails and the web (Culotta
et al, 2004), we believe this is the first paper to
present a full-fledged system for extracting social
networks from conversational speech transcripts.
Similarly, most of the work on co-reference res-
olution has not focused on conversational speech.
(Ji et al, 2005) uses semantic relations to refine
co-reference decisions, but in a approach different
from ours.
7 Conclusions and Future Work
We have described a novel approach for extracting
social networks, biographical facts, and movement
summaries from transcripts of oral interviews with
Holocaust survivors. We have improved the per-
formance of social network extraction five-fold,
compared to a baseline system that already uses
state-of-the-art technology. In particular, we im-
proved the performance of co-reference resolution
for conversational speech, by feature engineering
and improving the clustering algorithm.
Although our application data consists of con-
versational speech transcripts in this paper, the
same extraction approach can be applied to
general-domain text as well. Extracting general,
rich social networks is very important in many ap-
plications, since it provides the knowledge of who
is connected to whom and how they are connected.
There are many interesting issues involved in
biography generation from a large data collection,
such as how to resolve contradictions. The counts
from the corpus certainly help to filter out false
information which would otherwise be difficult to
filter. But better technology at detecting and re-
solving contradictions will definitely be beneficial.
Acknowledgment
We would like to thank Martin Franz and Bhuvana
Ramabhadran for their help during this project.
This project is funded by NSF under the Infor-
mation Technology Research (ITR) program, NSF
IIS Award No. 0122466. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the NSF.
References
2005. Automatic content extraction.
http://www.nist.gov/speech/tests/ace/.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and con-
tact information from email and the web. In CEAS,
Mountain View, CA.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In Proceedings of. HLT-NAACL 2004.
Samuel Gustman, Dagobert Soergeland Douglas Oard,
William Byrne, Michael Picheny, Bhuvana Ramab-
hadran, and Douglas Greenberg. 2002. Support-
ing access to large digital oral history archives. In
Proceedings of the Joint Conference on Digital Li-
braries, pages 18?27.
1046
Heng Ji, DavidWestbrook, and Ralph Grishman. 2005.
Using semantic relations to refine coreference deci-
sions. In Proceedings of HLT/EMNLP?05, Vancou-
ver, B.C., Canada.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL2004), pages 135?142, Barcelona,
Spain.
Vincent Ng. 2005. Machine learning for coreference
resolution: From local classification to global rank-
ing. In Proceedings of ACL?04.
D. Oard, D. Soergel, D. Doermann, X. Huang, G.C.
Murray, J. Wang, B. Ramabhadran, M. Franz,
S. Gustman, J. Mayfield, L. Kharevych, and
S. Strassel. 2004. Building an information re-
trieval test collection for spontaneous conversational
speech. In Proceedings of SIGIR?04, Sheffield, U.K.
1047
A Flexible Framework for Developing  
Mixed-Initiative Dialog Systems 
 
Judith HOCHBERG, Nanda KAMBHATLA, Salim ROUKOS 
IBM T.J. Watson Research Center 
Yorktown Heights, NY 10598, USA 
{judyhoch, nanda, roukos}@us.ibm.com 
 
Abstract  
We present a new framework for rapid 
development of mixed-initiative dialog 
systems. Using this framework, a developer 
can author sophisticated dialog systems for 
multiple channels of interaction by 
specifying an interaction modality, a rich 
task hierarchy and task parameters, and 
domain-specific modules.  The framework 
includes a dialog history that tracks input, 
output, and results. We present the 
framework and preliminary results in two 
application domains. 
1 Introduction 
Developing a mixed-initiative dialog system is a 
complex task.  The developer must model the 
user?s goals, the ?results? (domain objects) 
retrieved, and the state of the dialog, and 
generate the system response at each turn of the 
dialog. In mixed-initiative systems, as opposed 
to directed dialog systems, users can influence 
the dialog flow, and are not restricted to 
answering system questions in a prescribed 
format (e.g. Walker 1990, Chu-Carroll 2000). 
Compounding these challenges, dialog 
applications have evolved from simple look-up 
tasks to complex transactional systems like 
telephony banking and stock trading (Zadrozny 
et al 1998), and air travel information systems. 
These systems increasingly cater to multiple 
channels of user interaction (telephone, PDA, 
web, etc.), each with its own set of modalities. 
To simplify the development of such systems, 
researchers have created frameworks that 
embody core dialog functionalities. 
In MIT?s framework, a developer creates a 
dialog system by specifying a dialog control 
table comprising actions and their triggering 
events. The developer has great freedom in 
designing this table, but must specify basic 
actions such as prompting for missing 
information.  As a result, these tables can 
become quite complex ? the travel system 
control table contains over 200 ordered rules.  
MIT has applied this framework to both weather 
and travel (Zue et al 2000, Seneff and Polifroni 
2000). 
In IBM?s form-based dialog manager, or 
FDM (Papineni et al 1998), a developer defines 
a set of forms that correspond to separate tasks 
in the application, such as finding a flight leg.  
The forms have powerful built-in capabilities, 
including mechanisms that trigger various types 
of prompts, and allow the user to specify 
inheritance and other relationships between 
tasks. Just as in the MIT framework, domain-
specific modules perform database queries and 
other backend processes; the forms call 
additional developer-defined modules that affect 
the dialog state and flow. FDM has supported 
dialog systems for air travel (Papineni et al 
1999, Axelrod 2000) and financial services 
(IBM 2001, IBM 2002). The University of 
Colorado framework also has a form-based 
architecture (Pellom et al 2001), while CMU 
and Bell Labs? frameworks allow the 
specification of deep task hierarchies (Wei and 
Rudnicky 2000, Potamianos et al 2000). 
Our goal is to design a framework that is 
both powerful, embodying much dialog 
functionality, and flexible, accommodating a 
variety of dialog domains, modalities, and styles.  
Our new framework goes beyond FDM in 
building more core functionality into its task 
model, yet provides a variety of software tools, 
such as API calls and overwritable functions, for 
customizing tasks.  The framework allows 
developers to specify a wide range of 
relationships among tasks, and provides a focus 
model that respects these relationships.  To 
support the task framework we introduce a 
       Philadelphia, July 2002, pp. 60-63.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
dialog history component that remembers input, 
output, and cumulative task results.  Section 2 of 
this paper describes the framework, and section 
3 some applications.  In section 4 we discuss 
future plans and implications. 
2 The HOT framework 
Our framework? s moniker is HOT, which stands 
for its three components: dialog History, domain 
Objects, and Task hierarchy.  It is implemented 
as a Java library. In this section, we describe the 
HOT framework. We assume the existence of an 
application specific natural language parser that 
brackets and labels chunks of text corresponding 
to domain specific attributes, and a natural 
language generation module for generating 
prompts from abstract specifications. 
2.1 Task hierarchy 
A task defines a unit of work for a dialog 
system. The HOT framework enables the 
specification of tasks that are organized as a 
hierarchy (e.g. Fig. 1).  The terminal tasks in the 
hierarchy  (UserID, Fund, and Shares) derive 
canonical values of domain attributes (such as 
fund symbol) from parsed portions of user input. 
The RootTask specifies methods for managing 
the dialog, e.g. for disambiguating among 
different sub-tasks in case of ambiguous user 
input. All other tasks perform scripted actions 
using the output produced by other non-terminal 
or terminal tasks: generate a user profile, a share 
transaction, or a price quote. 
The task hierarchy constitutes a plan for the 
dialog.  It remains to be seen whether it can also 
be used for planning domains in which task 
input can come either from a user or from an 
external process such as an environmental 
monitor, as in [Allen at al. 2001]. 
The framework allows developers to easily 
specify five different relationships among tasks 
in a hierarchy.  Many of these will be 
exemplified in Section 3. 
1. Subtasking: UserID is a subtask of Login 
because Login needs the user? s ID to log the 
user in. 
2. Ordering: Login precedes all other tasks, but 
Buy, Sell, and Price are unordered. 
3. Cardinality: Login is executed only once per 
session, and UserId, Fund, and Shares are 
executed only once per parent task. 
However, Buy, Sell, and Price can be 
executed multiple times. 
4. Inheritance: Buy and Sell can potentially 
inherit a fund name from Price and vice 
versa. 
5. Subdialog: The user can carry out certain 
subdialogs, such as a Price query within a 
Buy task. 
2.2 Focus model 
At each turn of the dialog, we automatically 
score the user? s input to infer the task that the 
user wants to work on.  Only a non-terminal task 
can receive focus.  As in FDM, scoring is 
primarily based on the number of matches 
between attributes in the parsed user input, 
different task attributes, and the last system 
prompt.  The developer can specify the 
appropriate system behavior if the inferred user 
focus conflicts with task relationships, e.g. if a 
user wants to Buy but has not yet Logged in.  In 
the absence of such conflicts, the framework 
triggers execution of the inferred task.  If the 
task completes without ending a turn, the focus 
model returns focus to a previously started task 
if possible, or else defaults to the developer? s 
preference for what to do next. 
2.3 Task functionality 
Within RootTask, a developer can specify the 
modalities of interaction and the specific 
backends used, create an initial task layout, and 
set some dialog parameters. Developers must 
specify how they want RootTask to respond to 
various focus situations.  For example, if no 
tasks are eligible for focus, this may represent an 
error condition in one application, but the 
expected end of a dialog in another application. 
For all other tasks, task functionality can be 
divided into operations that happen before and 
after the task calls its backend process 
Root
UserID Fund Name
Login
Number 
of Shares
Buy PriceSell
Fund 
Name
Number 
of Shares
Fund 
Name
 
Figure 1: A task hierarchy for a simple 
mutual fund application. 
(accessing a database, the Internet, or other 
information channel) to create a result.  Pre-
backend functionality involves assessing, and 
possibly confirming with the user, the 
parameters to be sent to the backend.  Post-
backend functionality acts on different backend 
outcomes: for example, informing the user of a 
result, confirming a result, or requesting further 
constraints.  Because the framework already 
defines these functionalites, the developer? s role 
is to define the backend and its result, and to 
choose the pre-defined functionalities that apply. 
As tasks execute, they post communicative 
intentions ? dialog acts (e.g., ?Inform?, 
?Confirm?) and the domain objects they concern 
(e.g., flights) ? to the dialog history.  A separate 
NLG module generates the text of the system 
response based on these communicative 
intentions and the specific modalities in use. 
2.4 Dialog History 
The dialog history provides developers with an 
organized way to store system data regardless of 
the application domain. We store the user input 
(attribute-value pairs), the system response 
(communicative intentions), and the cumulative 
results for each dialog turn.  The developer can 
additionally store the user input at various stages 
of processing. Results can be generic objects 
(e.g., a String representing a fund name) or 
complex, domain-specific objects.  The results 
are tagged with a core set of status indicators 
that developers can add to. The dialog history 
also serves as a task stack, since the current 
result from each task indicates the task? s status. 
The dialog history is reminiscent of BBN? s 
state manager (Stallard 2000), but the latter also 
includes many dialog management 
responsibilities that we reserve to tasks. 
3 Applications 
We are currently implementing the HOT 
framework described above. In this section, we 
describe two sample applications built using this 
framework as implemented so far. 
3.1 Mutual Funds 
We built a mutual funds application, using the  
framework to create a task hierarchy and to add 
task relationships such as ordering and 
inheritance, as described in section 2.1.  We 
customized the tasks by writing backend 
methods and specifying confirmation strategies, 
the domain attributes of interest to each task, and 
domain-specific result objects (UserProfile and 
ShareTransaction).  Figure 2 shows an actual 
dialog with this application. 
The dialog flows through a combination of 
developer specifications and the HOT 
framework, with the framework doing most of 
the work.  In turn 1, only Login is specified as 
active, so the framework generates an initial 
prompt for it.  In turn 2, the framework scores 
the input and executes both UserID and Signin.  
It informs the user of Login? s success (as 
specified), then issues a general prompt because 
Login enabled all other tasks when it finished 
(as specified).  In turn 3, the framework issues 
an error message because of the conflict between 
Login? s cardinality of 1 and the user? s input.  In 
turn 4, the fund name lookup finds an 
ambiguous result; the framework looks to the 
developer to specify what type of prompt to 
issue in response.  In turn 5, the framework 
correctly interprets the user? s selection, and 
requests fund name confirmation (as specified).  
In turn 6, Sell runs its Number of Shares 
subtask.  In turn 7, the user performs an 
authorized Price subdialog from within Sell.  
Price inherits the fund name from Sell, executes 
the task, and returns focus to Sell.  In turn 8, the 
framework requests confirmation prior to 
 User System 
1  Welcome to HOT-DM. 
User ID? 
2 1677 Greetings Judy Hochberg! 
How may I help you? 
3 I want to sign 
in again. 
Sorry, you can only sign in once. 
How may I help you? 
4 I want to sell 
Watson fund 
Watson Yorktown fund or 
Watson Hawthorne fund? 
5 the first one Is Watson Yorktown the fund 
you want to sell? 
6 yes How many shares? 
7 can you tell 
me what the 
price is 
The price of Watson Yorktown 
is $100.2 per share 
How many shares? 
8 I want to sell 
30 shares 
Do you want to sell 30 shares of 
Watson Yorktown fund? 
9 yes i do Transaction #1: sold 30 shares of 
Watson Yorktown fund at 
$100.2 per share on 2/18/2002 
How may I help you? 
Figure 2: A dialog with a mutual funds application 
built using the HOT framework. 
executing the sale (as specified).  In turn 9, note 
that the Sell backend accesses the dialog history 
to determine the correct transaction number. 
3.2 Travel 
We have developed a travel information dialog 
system using the HOT framework. For this 
application, we reused several components of 
the IBM DARPA Communicator system. The 
system accepts either standard text input or text 
from instant messaging from a mobile platform.  
Figure 3 shows a screen shot of a dialog on a 
? Palm?  (PDA) interface to the travel system. 
4 Discussion 
We have presented a new framework for 
developing mixed-initiative dialog systems. This 
framework, dubbed HOT, enables developers to 
rapidly develop dialog systems by specifying 
tasks, their relationships, and relevant domain 
objects. We are currently implementing this 
framework as a toolkit and have developed two 
sample applications in two different modalities. 
The new framework departs from other 
frameworks in the range of functionality that it 
covers.  Its task model triggers not only 
informational prompts and confirmations, but 
also customizable responses to task problems of 
different sorts, such as underspecification.  The 
task relationships modeled are likewise quite 
rich, including subdialog and inheritance.  
Finally, the dialog history provides a generic 
specification of output semantics, a way to track 
task status, and uniform access to dialog results 
of varying complexity.  Our future goal is 
continue to build functionality, especially in 
NLG, without sacrificing flexibility. 
References  
J. Allen, G Ferguson, and Amanda Stent (2001) An 
architecture for more realistic conversational 
systems.  Proc. Intelligent User Interfaces. 
S. Axelrod, (2000) Natural Language Generation in 
the IBM Flight Information System.  Proc. ANLP-
NAACL Workshop on Conversational Systems. 
J. Chu-Carroll (2000) MIMIC: An Adaptive Mixed 
Initiative Spoken Dialogue System for Information 
Queries.  Proc ANLP. 
IBM (2001) http://www-3.ibm.com/software/speech/ 
news/20010609trp.html 
IBM (2002) http://www-3.ibm.com/software/speech/ 
enterprise/dcenter/demo_2.html 
K. Papineni, S. Roukos, and T. Ward (1999) Free-
Flow Dialog Management Using Forms.  Proc. 
Eurospeech, pp. 1411-1414. 
B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. 
Zhang, X. Yu, and S. Pradhan (2001) University of 
Colorado Dialog Systems for Travel and 
Navigation.  Proc. HLT. 
A. Potamianos, E. Ammicht, and H-K. Kuo (2000) 
Dialogue Management in the Bell Labs 
Communicator System.  Proc. ICSLP. 
S. Seneff and J. Polifroni (2000) Dialogue 
Management in the Mercury Flight Reservation 
System.  Proc. Satellite Dialogue Workshop, 
ANLP-NAACL. 
D. Stallard (2000) Talk?N?Travel: A Conversational 
System for Air Travel Planning.  Proc ANLP. 
M. Walker (1990)  Mixed Initiative in Dialogue: An 
Investigation into Discourse Segmentation. Proc. 
ACL90, pp. 70-78. 
X. Wei and A. Rudnicky (2000) Task-based dialog 
management using an agenda.  Proc 
ANLP/NAACL Workshop on Conversational 
Systems, pp. 42-47. 
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. 
Hazen, and L. Hetherington (2000) JUPITER: A 
Telephone-Based conversational Interface for 
Weather Information.  IEEE Trans. Speech and 
Audio Proc., 20/Y, pp. 100-112.  
W. Zadrozny, C. Wolf, N. Kambhatla, and Y. Ye,  
1998. Conversation Machines for Transaction 
Processing. PROC AAAI/IAAI, pp. 1160-1166. Figure 3: A dialog in a ? Palm?  interface to 
an air travel dialog system. 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 546?554,
Beijing, August 2010
Learning to Predict Readability using Diverse Linguistic Features
Rohit J. Kate1 Xiaoqiang Luo2 Siddharth Patwardhan2 Martin Franz2
Radu Florian2 Raymond J. Mooney1 Salim Roukos2 Chris Welty2
1Department of Computer Science
The University of Texas at Austin
{rjkate,mooney}@cs.utexas.edu
2IBM Watson Research Center
{xiaoluo,spatward,franzm,raduf,roukos,welty}@us.ibm.com
Abstract
In this paper we consider the problem of
building a system to predict readability
of natural-language documents. Our sys-
tem is trained using diverse features based
on syntax and language models which are
generally indicative of readability. The
experimental results on a dataset of docu-
ments from a mix of genres show that the
predictions of the learned system are more
accurate than the predictions of naive hu-
man judges when compared against the
predictions of linguistically-trained expert
human judges. The experiments also com-
pare the performances of different learn-
ing algorithms and different types of fea-
ture sets when used for predicting read-
ability.
1 Introduction
An important aspect of a document is whether it
is easily processed and understood by a human
reader as intended by its writer, this is termed
as the document?s readability. Readability in-
volves many aspects including grammaticality,
conciseness, clarity, and lack of ambiguity. Teach-
ers, journalists, editors, and other professionals
routinely make judgements on the readability of
documents. We explore the task of learning to
automatically judge the readability of natural-
language documents.
In a variety of applications it would be useful to
be able to automate readability judgements. For
example, the results of a web-search can be or-
dered taking into account the readability of the
retrieved documents thus improving user satisfac-
tion. Readability judgements can also be used
for automatically grading essays, selecting in-
structional reading materials, etc. If documents
are generated by machines, such as summariza-
tion or machine translation systems, then they are
prone to be less readable. In such cases, a read-
ability measure can be used to automatically fil-
ter out documents which have poor readability.
Even when the intended consumers of text are
machines, for example, information extraction or
knowledge extraction systems, a readability mea-
sure can be used to filter out documents of poor
readability so that the machine readers will not ex-
tract incorrect information because of ambiguity
or lack of clarity in the documents.
As part of the DARPA Machine Reading Pro-
gram (MRP), an evaluation was designed and con-
ducted for the task of rating documents for read-
ability. In this evaluation, 540 documents were
rated for readability by both experts and novice
human subjects. Systems were evaluated based on
whether they were able to match expert readabil-
ity ratings better than novice raters. Our system
learns to match expert readability ratings by em-
ploying regression over a set of diverse linguistic
features that were deemed potentially relevant to
readability. Our results demonstrate that a rich
combination of features from syntactic parsers,
language models, as well as lexical statistics all
contribute to accurately predicting expert human
readability judgements. We have also considered
the effect of different genres in predicting read-
ability and how the genre-specific language mod-
els can be exploited to improve the readability pre-
dictions.
546
2 Related Work
There is a significant amount of published work
on a related problem: predicting the reading diffi-
culty of documents, typically, as the school grade-
level of the reader from grade 1 to 12. Some early
methods measure simple characteristics of docu-
ments like average sentence length, average num-
ber of syllables per word, etc. and combine them
using a linear formula to predict the grade level of
a document, for example FOG (Gunning, 1952),
SMOG (McLaughlin, 1969) and Flesh-Kincaid
(Kincaid et al, 1975) metrics. These methods
do not take into account the content of the doc-
uments. Some later methods use pre-determined
lists of words to determine the grade level of a
document, for example the Lexile measure (Sten-
ner et al, 1988), the Fry Short Passage measure
(Fry, 1990) and the Revised Dale-Chall formula
(Chall and Dale, 1995). The word lists these
methods use may be thought of as very simple
language models. More recently, language mod-
els have been used for predicting the grade level
of documents. Si and Callan (2001) and Collins-
Thompson and Callan (2004) train unigram lan-
guage models to predict grade levels of docu-
ments. In addition to language models, Heilman
et al (2007) and Schwarm and Ostendorf (2005)
also use some syntactic features to estimate the
grade level of texts.
Pitler and Nenkova (2008) consider a differ-
ent task of predicting text quality for an educated
adult audience. Their system predicts readabil-
ity of texts from Wall Street Journal using lex-
ical, syntactic and discourse features. Kanungo
and Orr (2009) consider the task of predicting
readability of web summary snippets produced by
search engines. Using simple surface level fea-
tures like the number of characters and syllables
per word, capitalization, punctuation, ellipses etc.
they train a regression model to predict readability
values.
Our work differs from this previous research in
several ways. Firstly, the task we have consid-
ered is different, we predict the readability of gen-
eral documents, not their grade level. The doc-
uments in our data are also not from any single
domain, genre or reader group, which makes our
task more general. The data includes human writ-
ten as well as machine generated documents. The
task and the data has been set this way because it
is aimed at filtering out documents of poor quality
for later processing, like for extracting machine-
processable knowledge from them. Extracting
knowledge from openly found text, such as from
the internet, is becoming popular but the quality
of text found ?in the wild?, like found through
searching the internet, vary considerably in qual-
ity and genre. If the text is of poor readability then
it is likely to lead to extraction errors and more
problems downstream. If the readers are going
to be humans instead of machines, then also it is
best to filter out poorly written documents. Hence
identifying readability of general text documents
coming from various sources and genres is an im-
portant task. We are not aware of any other work
which has considered such a task.
Secondly, we note that all of the above ap-
proaches that use language models train a lan-
guage model for each difficulty level using the
training data for that level. However, since the
amount of training data annotated with levels
is limited, they can not train higher-order lan-
guage models, and most just use unigram models.
In contrast, we employ more powerful language
models trained on large quantities of generic text
(which is not from the training data for readabil-
ity) and use various features obtained from these
language models to predict readability. Thirdly,
we use a more sophisticated combination of lin-
guistic features derived from various syntactic
parsers and language models than any previous
work. We also present ablation results for differ-
ent sets of features. Fourthly, given that the doc-
uments in our data are not from a particular genre
but from a mix of genres, we also train genre-
specific language models and show that including
these as features improves readability predictions.
Finally, we also show comparison between var-
ious machine learning algorithms for predicting
readability, none of the previous work compared
learning algorithms.
3 Readability Data
The readability data was collected and re-
leased by LDC. The documents were collected
547
from the following diverse sources or genres:
newswire/newspaper text, weblogs, newsgroup
posts, manual transcripts, machine translation out-
put, closed-caption transcripts and Wikipedia arti-
cles. Documents for newswire, machine transla-
tion and closed captioned genres were collected
automatically by first forming a candidate pool
from a single collection stream and then randomly
selecting documents. Documents for weblogs,
newsgroups and manual transcripts were also col-
lected in the same way but were then reviewed
by humans to make sure they were not simply
spam articles or something objectionable. The
Wikipedia articles were collected manually, by
searching through a data archive or the live web,
using keyword and other search techniques. Note
that the information about genres of the docu-
ments is not available during testing and hence
was not used when training our readability model.
A total of 540 documents were collected in this
way which were uniformly distributed across the
seven genres. Each document was then judged
for its readability by eight expert human judges.
These expert judges are native English speakers
who are language professionals and who have
specialized training in linguistic analysis and an-
notation, including the machine translation post-
editing task. Each document was also judged for
its readability by six to ten naive human judges.
These non-expert (naive) judges are native En-
glish speakers who are not language professionals
(e.g. editors, writers, English teachers, linguistic
annotators, etc.) and have no specialized language
analysis or linguistic annotation training. Both ex-
pert and naive judges provided readability judg-
ments using a customized web interface and gave
a rating on a 5-point scale to indicate how readable
the passage is (where 1 is lowest and 5 is highest
readability) where readability is defined as a sub-
jective judgment of how easily a reader can extract
the information the writer or speaker intended to
convey.
4 Readability Model
We want to answer the question whether a
machine can accurately estimate readability as
judged by a human. Therefore, we built a
machine-learning system that predicts the read-
ability of documents by training on expert hu-
man judgements of readability. The evaluation
was then designed to compare how well machine
and naive human judges predict expert human
judgements. In order to make the machine?s pre-
dicted score comparable to a human judge?s score
(details about our evaluation metrics are in Sec-
tion 6.1), we also restricted the machine scores to
integers. Hence, the task is to predict an integer
score from 1 to 5 that measures the readability of
the document.
This task could be modeled as a multi-class
classification problem treating each integer score
as a separate class, as done in some of the previ-
ous work (Si and Callan, 2001; Collins-Thompson
and Callan, 2004). However, since the classes
are numerical and not unrelated (for example, the
score 2 is in between scores 1 and 3), we de-
cided to model the task as a regression problem
and then round the predicted score to obtain the
closest integer value. Preliminary results verified
that regression performed better than classifica-
tion. Heilman et al (2008) also found that it
is better to treat the readability scores as ordinal
than as nominal. We take the average of the ex-
pert judge scores for each document as its gold-
standard score. Regression was also used by Ka-
nungo and Orr (2009), although their evaluation
did not constrain machine scores to be integers.
We tested several regression algorithms avail-
able in the Weka1 machine learning package, and
in Section 6.2 we report results for several which
performed best. The next section describes the
numerically-valued features that we used as input
for regression.
5 Features for Predicting Readability
Good input features are critical to the success of
any regression algorithm. We used three main cat-
egories of features to predict readability: syntac-
tic features, language-model features, and lexical
features, as described below.
5.1 Features Based on Syntax
Many times, a document is found to be unreadable
due to unusual linguistic constructs or ungram-
1http://www.cs.waikato.ac.nz/ml/weka/
548
matical language that tend to manifest themselves
in the syntactic properties of the text. There-
fore, syntactic features have been previously used
(Bernth, 1997) to gauge the ?clarity? of written
text, with the goal of helping writers improve their
writing skills. Here too, we use several features
based on syntactic analyses. Syntactic analyses
are obtained from the Sundance shallow parser
(Riloff and Phillips, 2004) and from the English
Slot Grammar (ESG) (McCord, 1989).
Sundance features: The Sundance system is a
rule-based system that performs a shallow syntac-
tic analysis of text. We expect that this analysis
over readable text would be ?well-formed?, adher-
ing to grammatical rules of the English language.
Deviations from these rules can be indications of
unreadable text. We attempt to capture such de-
viations from grammatical rules through the fol-
lowing Sundance features computed for each text
document: proportion of sentences with no verb
phrases, average number of clauses per sentence,
average sentence length in tokens, average num-
ber of noun phrases per sentence, average number
of verb phrases per sentence, average number of
prepositional phrases per sentence, average num-
ber of phrases (all types) per sentence and average
number of phrases (all types) per clause.
ESG features: ESG uses slot grammar rules to
perform a deeper linguistic analysis of sentences
than the Sundance system. ESG may consider
several different interpretations of a sentence, be-
fore deciding to choose one over the other inter-
pretations. Sometimes ESG?s grammar rules fail
to produce a single complete interpretation of a
sentence, in which case it generates partial parses.
This typically happens in cases when sentences
are ungrammatical, and possibly, less readable.
Thus, we use the proportion of such incomplete
parses within a document as a readability feature.
In case of extremely short documents, this propor-
tion of incomplete parses can be misleading. To
account for such short documents, we introduce
a variation of the above incomplete parse feature,
by weighting it with a log factor as was done in
(Riloff, 1996; Thelen and Riloff, 2002).
We also experimented with some other syn-
tactic features such as average sentence parse
scores from Stanford parser and an in-house maxi-
mum entropy statistical parer, average constituent
scores etc., however, they slightly degraded the
performance in combination with the rest of the
features and hence we did not include them in
the final set. One possible explanation could be
that averaging diminishes the effect of low scores
caused by ungrammaticality.
5.2 Features Based on Language Models
A probabilistic language model provides a predic-
tion of how likely a given sentence was generated
by the same underlying process that generated a
corpus of training documents. In addition to a
general n-gram language model trained on a large
body of text, we also exploit language models
trained to recognize specific ?genres? of text. If a
document is translated by a machine, or casually
produced by humans for a weblog or newsgroup,
it exhibits a character that is distinct from docu-
ments that go through a dedicated editing process
(e.g., newswire and Wikipedia articles). Below
we describe features based on generic as well as
genre-specific language models.
Normalized document probability: One obvi-
ous proxy for readability is the score assigned to
a document by a generic language model (LM).
Since the language model is trained on well-
written English text, it penalizes documents de-
viating from the statistics collected from the LM
training documents. Due to variable document
lengths, we normalize the document-level LM
score by the number of words and compute the
normalized document probability NP (D) for a
document D as follows:
NP (D) =
(
P (D|M)
) 1
|D| , (1)
where M is a general-purpose language model
trained on clean English text, and |D| is the num-
ber of words in the document D.
Perplexities from genre-specific language mod-
els: The usefulness of LM-based features in
categorizing text (McCallum and Nigam, 1998;
Yang and Liu, 1999) and evaluating readability
(Collins-Thompson and Callan, 2004; Heilman
et al, 2007) has been investigated in previous
work. In our experiments, however, since doc-
uments were acquired through several different
channels, such as machine translation or web logs,
549
we also build models that try to predict the genre
of a document. Since the genre information for
many English documents is readily available, we
trained a series of genre-specific 5-gram LMs us-
ing the modified Kneser-Ney smoothing (Kneser
and Ney, 1995; Stanley and Goodman, 1996). Ta-
ble 1 contains a list of a base LM and genre-
specific LMs.
Given a document D consisting of tokenized
word sequence {wi : i = 1, 2, ? ? ? , |D|}, its per-
plexity L(D|Mj) with respect to a LM Mj is
computed as:
L(D|Mj) = e
(
? 1|D|
P|D|
i=1 logP (wi|hi;Mj)
)
, (2)
where |D| is the number of words in D and hi are
the history words for wi, and P (wi|hi;Mj) is the
probability Mj assigns to wi, when it follows the
history words hi.
Posterior perplexities from genre-specific lan-
guagemodels: While perplexities computed from
genre-specific LMs reflect the absolute probabil-
ity that a document was generated by a specific
model, a model?s relative probability compared to
other models may be a more useful feature. To this
end, we also compute the posterior perplexity de-
fined as follows. Let D be a document, {Mi}Gi=1
be G genre-specific LMs, and L(D|Mi) be the
perplexity of the document D with respect to Mi,
then the posterior perplexity, R(Mi|D), is de-
fined as:
R(Mi|D) =
L(D|Mi)?G
j=1 L(D|Mj)
. (3)
We use the term ?posterior? because if a uni-
form prior is adopted for {Mi}Gi=1,R(Mi|D) can
be interpreted as the posterior probability of the
genre LM Mi given the document D.
5.3 Lexical Features
The final set of features involve various lexical
statistics as described below.
Out-of-vocabulary (OOV) rates: We conjecture
that documents containing typographical errors
(e.g., for closed-caption and web log documents)
may receive low readability ratings. Therefore,
we compute the OOV rates of a document with re-
spect to the various LMs shown in Table 1. Since
modern LMs often have a very large vocabulary,
to get meaningful OOV rates, we truncate the vo-
cabularies to the top (i.e., most frequent) 3000
words. For the purpose of OOV computation, a
document D is treated as a sequence of tokenized
words {wi : i = 1, 2, ? ? ? , |D|}. Its OOV rate
with respect to a (truncated) vocabulary V is then:
OOV (D|V) =
?D
i=1 I(wi /? V)
|D| , (4)
where I(wi /? V) is an indicator function taking
value 1 if wi is not in V , and 0 otherwise.
Ratio of function words: A characteristic of doc-
uments generated by foreign speakers and ma-
chine translation is a failure to produce certain
function words, such as ?the,? or ?of.? So we pre-
define a small set of function words (mainly En-
glish articles and frequent prepositions) and com-
pute the ratio of function words over the total
number words in a document:
RF (D) =
?D
i=1 I(wi ? F)
|D| , (5)
where I(wi ? F) is 1 ifwi is in the set of function
words F , and 0 otherwise.
Ratio of pronouns: Many foreign languages that
are source languages of machine-translated docu-
ments are pronoun-drop languages, such as Ara-
bic, Chinese, and romance languages. We conjec-
ture that the pronoun ratio may be a good indica-
tor whether a document is translated by machine
or produced by humans, and for each document,
we first run a POS tagger, and then compute the
ratio of pronouns over the number of words in the
document:
RP (D) =
?D
i=1 I(POS(wi) ? P)
|D| , (6)
where I(POS(wi) ? F) is 1 if the POS tag of wi
is in the set of pronouns, P , and 0 otherwise.
Fraction of known words: This feature measures
the fraction of words in a document that occur
either in an English dictionary or a gazetteer of
names of people and locations.
6 Experiments
This section describes the evaluation methodol-
ogy and metrics and presents and discusses our
550
Genre Training Size(M tokens) Data Sources
base 5136.8 mostly LDC?s GigaWord set
NW 143.2 newswire subset of base
NG 218.6 newsgroup subset of base
WL 18.5 weblog subset of base
BC 1.6 broadcast conversation subset of base
BN 1.1 broadcast news subset of base
wikipedia 2264.6 Wikipedia text
CC 0.1 closed caption
ZhEn 79.6 output of Chinese to English Machine Translation
ArEn 126.8 output of Arabic to English Machine Translation
Table 1: Genre-specific LMs: the second column contains the number of tokens in LM training data (in million tokens).
experimental results. The results of the official
evaluation task are also reported.
6.1 Evaluation Metric
The evaluation process for the DARPAMRP read-
ability test was designed by the evaluation team
led by SAIC. In order to compare a machine?s
predicted readability score to those assigned by
the expert judges, the Pearson correlation coef-
ficient was computed. The mean of the expert-
judge scores was taken as the gold-standard score
for a document.
To determine whether the machine predicts
scores closer to the expert judges? scores than
what an average naive judge would predict, a
sampling distribution representing the underlying
novice performance was computed. This was ob-
tained by choosing a random naive judge for every
document, calculating the Pearson correlation co-
efficient with the expert gold-standard scores and
then repeating this procedure a sufficient number
of times (5000). The upper critical value was set
at 97.5% confidence, meaning that if the machine
performs better than the upper critical value then
we reject the null hypothesis that machine scores
and naive scores come from the same distribution
and conclude that the machine performs signifi-
cantly better than naive judges in matching the ex-
pert judges.
6.2 Results and Discussion
We evaluated our readability system on the dataset
of 390 documents which was released earlier dur-
ing the training phase of the evaluation task. We
Algorithm Correlation
Bagged Decision Trees 0.8173
Decision Trees 0.7260
Linear Regression 0.7984
SVM Regression 0.7915
Gaussian Process Regression 0.7562
Naive Judges
Upper Critical Value 0.7015
Distribution Mean 0.6517
Baselines
Uniform Random 0.0157
Proportional Random -0.0834
Table 2: Comparing different algorithms on the readability
task using 13-fold cross-validation on the 390 documents us-
ing all the features. Exceeding the upper critical value of the
naive judges? distribution indicates statistically significantly
better predictions than the naive judges.
used stratified 13-fold cross-validation in which
the documents from various genres in each fold
was distributed in roughly the same proportion as
in the overall dataset. We first conducted experi-
ments to test different regression algorithms using
all the available features. Next, we ablated various
feature sets to determine how much each feature
set was contributing to making accurate readabil-
ity judgements. These experiments are described
in the following subsections.
6.2.1 Regression Algorithms
We used several regression algorithms available
in theWeka machine learning package and Table 2
shows the results obtained. The default values
551
Feature Set Correlation
Lexical 0.5760
Syntactic 0.7010
Lexical + Syntactic 0.7274
Language Model based 0.7864
All 0.8173
Table 3: Comparison of different linguistic feature sets.
in Weka were used for all parameters, changing
these values did not show any improvement. We
used decision tree (reduced error pruning (Quin-
lan, 1987)) regression, decision tree regression
with bagging (Breiman, 1996), support vector re-
gression (Smola and Scholkopf, 1998) using poly-
nomial kernel of degree two,2 linear regression
and Gaussian process regression (Rasmussen and
Williams, 2006). The distribution mean and the
upper critical values of the correlation coefficient
distribution for the naive judges are also shown in
the table.
Since they are above the upper critical value, all
algorithms predicted expert readability scores sig-
nificantly more accurately than the naive judges.
Bagged decision trees performed slightly better
than other methods. As shown in the following
section, ablating features affects predictive accu-
racy much more than changing the regression al-
gorithm. Therefore, on this task, the choice of re-
gression algorithm was not very critical once good
readability features are used. We also tested two
simple baseline strategies: predicting a score uni-
formly at random, and predicting a score propor-
tional to its frequency in the training data. As
shown in the last two rows of Table 2, these base-
lines perform very poorly, verifying that predict-
ing readability on this dataset as evaluated by our
evaluation metric is not trivial.
6.2.2 Ablations with Feature Sets
We evaluated the contributions of different fea-
ture sets through ablation experiments. Bagged
decision-tree was used as the regression algorithm
in all of these experiments. First we compared
syntactic, lexical and language-model based fea-
tures as described in Section 5, and Table 3 shows
2Polynomial kernels with other degrees and RBF kernel
performed worse.
the results. The language-model feature set per-
forms the best, but performance improves when it
is combined with the remaining features. The lex-
ical feature set by itself performs the worst, even
below the naive distribution mean (shown in Ta-
ble 2); however, when combined with syntactic
features it performs well.
In our second ablation experiment, we com-
pared the performance of genre-independent and
genre-based features. Since the genre-based fea-
tures exploit knowledge of the genres of text used
in the MRP readability corpus, their utility is
somewhat tailored to this specific corpus. There-
fore, it is useful to evaluate the performance of the
system when genre information is not exploited.
Of the lexical features described in subsection 5.3,
the ratio of function words, ratio of pronoun words
and all of the out-of-vocabulary rates except for
the base language model are genre-based features.
Out of the language model features described in
the Subsection 5.2, all of the perplexities except
for the base language model and all of the poste-
rior perplexities3 are genre-based features. All of
the remaining features are genre-independent. Ta-
ble 4 shows the results comparing these two fea-
ture sets. The genre-based features do well by
themselves but the rest of the features help fur-
ther improve the performance. While the genre-
independent features by themselves do not exceed
the upper critical value of the naive judges? dis-
tribution, they are very close to it and still out-
perform its mean value. These results show that
for a dataset like ours, which is composed of a mix
of genres that themselves are indicative of read-
ability, features that help identify the genre of a
text improve performance significantly.4 For ap-
plications mentioned in the introduction and re-
lated work sections, such as filtering less readable
documents from web-search, many of the input
documents could come from some of the common
genres considered in our dataset.
In our final ablation experiment, we evaluated
3Base model for posterior perplexities is computed using
other genre-based LMs (equation 3) hence it can not be con-
sidered genre-independent.
4We note that none of the genre-based features were
trained on supervised readability data, but were trained on
readily-available large unannotated corpora as shown in Ta-
ble 1.
552
Feature Set Correlation
Genre-independent 0.6978
Genre-based 0.7749
All 0.8173
Table 4: Comparison of genre-independent and genre-
based feature sets.
Feature Set By itself Ablated
from All
Sundance features 0.5417 0.7993
ESG features 0.5841 0.8118
Perplexities 0.7092 0.8081
Posterior perplexities 0.7832 0.7439
Out-of-vocabulary rates 0.3574 0.8125
All 0.8173 -
Table 5: Ablations with some individual feature sets.
the contribution of various individual feature sets.
Table 5 shows that posterior perplexities perform
the strongest on their own, but without them, the
remaining features also do well. When used by
themselves, some feature sets perform below the
naive judges? distribution mean, however, remov-
ing them from the rest of the feature sets de-
grades the performance. This shows that no indi-
vidual feature set is critical for good performance
but each further improves the performance when
added to the rest of the feature sets.
6.3 Official Evaluation Results
An official evaluation was conducted by the eval-
uation team SAIC on behalf of DARPA in which
three teams participated including ours. The eval-
uation task required predicting the readability of
150 test documents using the 390 training docu-
ments. Besides the correlation metric, two addi-
tional metrics were used. One of them computed
for a document the difference between the aver-
age absolute difference of the naive judge scores
from the mean expert score and the absolute dif-
ference of the machine?s score from the mean ex-
pert score. This was then averaged over all the
documents. The other one was ?target hits? which
measured if the predicted score for a document
fell within the width of the lowest and the highest
expert scores for that document, and if so, com-
System Correl. Avg. Diff. Target Hits
Our (A) 0.8127 0.4844 0.4619
System B 0.6904 0.3916 0.4530
System C 0.8501 0.5177 0.4641
Upper CV 0.7423 0.0960 0.3713
Table 6: Results of the systems that participated in the
DARPA?s readability evaluation task. The three metrics used
were correlation, average absolute difference and target hits
measured against the expert readability scores. The upper
critical values are for the score distributions of naive judges.
puted a score inversely proportional to that width.
The final target hits score was then computed by
averaging it across all the documents. The upper
critical values for these metrics were computed in
a way analogous to that for the correlation met-
ric which was described before. Higher score is
better for all the three metrics. Table 6 shows the
results of the evaluation. Our system performed
favorably and always scored better than the up-
per critical value on each of the metrics. Its per-
formance was in between the performance of the
other two systems. The performances of the sys-
tems show that the correlation metric was the most
difficult of the three metrics.
7 Conclusions
Using regression over a diverse combination of
syntactic, lexical and language-model based fea-
tures, we built a system for predicting the read-
ability of natural-language documents. The sys-
tem accurately predicts readability as judged by
linguistically-trained expert human judges and
exceeds the accuracy of naive human judges.
Language-model based features were found to be
most useful for this task, but syntactic and lexical
features were also helpful. We also found that for
a corpus consisting of documents from a diverse
mix of genres, using features that are indicative
of the genre significantly improve the accuracy of
readability predictions. Such a system could be
used to filter out less readable documents for ma-
chine or human processing.
Acknowledgment
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
553
References
Bernth, Arendse. 1997. Easyenglish: A tool for improv-
ing document quality. In Proceedings of the fifth con-
ference on Applied Natural Language Processing, pages
159?165, Washington DC, April.
Breiman, Leo. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
Chall, J.S. and E. Dale. 1995. Readability Revisited: The
New Dale-Chall Readability Formula. Brookline Books,
Cambridge, MA.
Collins-Thompson, Kevyn and James P. Callan. 2004. A
language modeling approach to predicting reading diffi-
culty. In Proc. of HLT-NAACL 2004, pages 193?200.
Fry, E. 1990. A readability formula for short passages. Jour-
nal of Reading, 33(8):594?597.
Gunning, R. 1952. The Technique of Clear Writing.
McGraw-Hill, Cambridge, MA.
Heilman, Michael, Kevyn Collins-Thompson, Jamie Callan,
and Maxine Eskenazi. 2007. Combining lexical and
grammatical features to improve readability measures for
first and second language texts. In Proc. of NAACL-HLT
2007, pages 460?467, Rochester, New York, April.
Heilman, Michael, Kevyn Collins-Thompson, and Maxine
Eskenazi. 2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. In Proceedings of
the Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 71?79, Columbus,
Ohio, June. Association for Computational Linguistics.
Kanungo, Tapas and David Orr. 2009. Predicting the read-
ability of short web summaries. In Proc. of WSDM 2009,
pages 202?211, Barcelona, Spain, February.
Kincaid, J. P., R. P. Fishburne, R. L. Rogers, and B.S.
Chissom. 1975. Derivation of new readability formulas
for navy enlisted personnel. Technical Report Research
Branch Report 8-75, Millington, TN: Naval Air Station.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc. of
ICASSP-95, pages 181?184.
McCallum, Andrew and Kamal Nigam. 1998. A comparison
of event models for naive Bayes text classification. In Pa-
pers from the AAAI-98 Workshop on Text Categorization,
pages 41?48, Madison, WI, July.
McCord, Michael C. 1989. Slot grammar: A system for
simpler construction of practical natural language gram-
mars. In Proceedings of the International Symposium on
Natural Language and Logic, pages 118?145, May.
McLaughlin, G. H. 1969. Smog: Grading: A new readabil-
ity formula. Journal of Reading, 12:639?646.
Pitler, Emily and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proc. of EMNLP 2008, pages 186?195,
Waikiki,Honolulu,Hawaii, October.
Quinlan, J. R. 1987. Simplifying decision trees. Interna-
tional Journal of Man-Machine Studies, 27:221?234.
Rasmussen, Carl and Christopher Williams. 2006. Gaussian
Processes for Machine Leanring. MIT Press, Cambridge,
MA.
Riloff, E. and W. Phillips. 2004. An introduction to the Sun-
dance and Autoslog systems. Technical Report UUCS-
04-015, University of Utah School of Computing.
Riloff, Ellen. 1996. Automatically generating extraction
patterns from untagged text. In Proc. of 13th Natl. Conf.
on Artificial Intelligence (AAAI-96), pages 1044?1049,
Portland, OR.
Schwarm, Sarah E. andMari Ostendorf. 2005. Reading level
assessment using support vector machines and statistical
language models. In Proc. of ACL 2005, pages 523?530,
Ann Arbor, Michigan.
Si, Luo and James P. Callan. 2001. A statistical model for
scientific readability. In Proc. of CIKM 2001, pages 574?
576.
Smola, Alex J. and Bernhard Scholkopf. 1998. A tutorial
on support vector regression. Technical Report NC2-TR-
1998-030, NeuroCOLT2.
Stanley, Chen and Joshua Goodman. 1996. An empirical
study of smoothing techniques for language modeling. In
Proc. of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL-96), pages 310?318.
Stenner, A. J., I. Horabin, D. R. Smith, and M. Smith. 1988.
The Lexile Framework. Durham, NC: MetaMetrics.
Thelen, M. and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proc. of EMNLP 2002, Philadelphia, PA, July.
Yang, Yiming and Xin Liu. 1999. A re-examination of text
cateogrization methods. In Proc. of 22nd Intl. ACM SI-
GIR Conf. on Research and Development in Information
Retrieval, pages 42?48, Berkeley, CA.
554
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 335?345,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Mention Detection Robustness to Noisy Input
Radu Florian, John F. Pitrelli, Salim Roukos and Imed Zitouni
IBM T.J. Watson Research Center
Yorktown Heights, NY, U.S.A.
{raduf,pitrelli,roukos,izitouni}us.ibm.com
Abstract
Information-extraction (IE) research typically
focuses on clean-text inputs. However, an IE
engine serving real applications yields many
false alarms due to less-well-formed input.
For example, IE in a multilingual broadcast
processing system has to deal with inaccu-
rate automatic transcription and translation.
The resulting presence of non-target-language
text in this case, and non-language mate-
rial interspersed in data from other applica-
tions, raise the research problem of making
IE robust to such noisy input text. We ad-
dress one such IE task: entity-mention de-
tection. We describe augmenting a statistical
mention-detection system in order to reduce
false alarms from spurious passages. The di-
verse nature of input noise leads us to pursue
a multi-faceted approach to robustness. For
our English-language system, at various miss
rates we eliminate 97% of false alarms on in-
puts from other Latin-alphabet languages. In
another experiment, representing scenarios in
which genre-specific training is infeasible, we
process real financial-transactions text con-
taining mixed languages and data-set codes.
On these data, because we do not train on data
like it, we achieve a smaller but significant im-
provement. These gains come with virtually
no loss in accuracy on clean English text.
1 Introduction
Information-extraction (IE) research is typically per-
formed on clean text in a predetermined language.
Lately, IE has improved to the point of being usable
for some real-world tasks whose accuracy require-
ments are reachable with current technology. These
uses include media monitoring, topic alerts, sum-
marization, population of databases for advanced
search, etc. These uses often combine IE with tech-
nologies such as speech recognition, machine trans-
lation, topic clustering, and information retrieval.
The propagation of IE technology from isolated
use to aggregates with such other technologies, from
NLP experts to other types of computer scientists,
and from researchers to users, feeds back to the IE
research community the need for additional inves-
tigation which we loosely refer to as ?information-
extraction robustness? research. For example:
1. Broadcast monitoring demands that IE handle
as input not only clean text, but also the tran-
scripts output by speech recognizers.
2. Multilingual applications, and the imperfection
of translation technology, require IE to contend
with non-target-language text input (Pitrelli et
al., 2008).
3. Naive users at times input to IE other material
which deviates from clean text, such as a PDF
file that ?looks? like plain text.
4. Search applications require IE to deal with
databases which not only possess clean text but
at times exhibit other complications like mark-
up codes particular to narrow, application-
specific data-format standards, for example, the
excerpt from a financial-transactions data set
shown in Figure 1.
Legacy industry-specific standards, such as il-
lustrated in this example, are part of long-
established processes which are cumbersome
to convert to a more-modern database format.
Transaction data sets typically build up over a
period of years, and as seen here, can exhibit
335
:54D://121000358
BANK OF BOSTON
:55D:/0148280005
NEVADA DEPT.OF VET.94C RECOV.FD
-5:MAC:E19DECA8CHK:641EB09B8968
USING OF FIELD 59: ONLY /INS/ WHEN
FOLLOWED BY BCC CODE IN CASE
OF QUESTIONS DONT HESITATE TO
CONTACT US QUOTING REFERENCE
NON-STC CHARGES OR VIA E-MAIL:
YOVANKA(UL)BRATASOVA(AT)BOA.CZ.
BEST REGARDS
BANKA OBCHODNIKA, A.S. PRAGUE, CZ
:58E::ADTX//++ ADDITIONAL
INFORMATION ++ PLEASE BE
INFORMED THAT AS A RESULT OF
THE PURCHASE OFFER ENDED ON 23
MAR 2008 CALDRADE LTD. IS
POSSESSING WITH MORE THEN 90
PER CENT VOTING RIGHT OF SLICE.
THEREFOR CALDRADE LTD. IS
EXERCISING PURCHASE RIGHTS
FOR ALL SLICE SHARES WHICH ARE
CURRENTLY NOT INHIS OWN.
PURCHASE PRICE: HUF 1.940 PER
SHARE. PLEASE :58E::ADTX//NOTE
THAT THOSE SHARES WHICH WILL
NOT BE PRESENTED TO THE OFFER
WILL BE CANCELLED AND INVALID.
:58:SIE SELBST
TRN/REF:515220 035
:78:RUECKGABE DES BETRAGES LT.
ANZBA43 M ZWECKS RUECKGABE IN
AUD. URSPR. ZU UNSEREM ZA MIT
REF. 0170252313279065 UND IHRE
RUECKG. :42:/BNF/UNSERE REF:
Figure 1: Example application-specific text, in this
case from financial transactions.
peculiar mark-up interspersed with meaning-
ful text. They also suffer complications arising
from limited-size entry fields and a diversity
of data-entry personnel, leading to effects like
haphazard abbreviation and improper spacing,
as shown. These issues greatly complicate the
IE problem, particularly considering that adapt-
ing IE to such formats is hampered by the exis-
tence of a multitude of such ?standards? and by
lack of sufficient annotated data in each one.
A typical state-of-the-art statistical IE engine will
happily process such ?noisy? inputs, and will typ-
ically provide garbage-in/garbage-out performance,
embarrassingly reporting spurious ?information? no
human would ever mistake. Yet it is also inappro-
priate to discard such documents wholesale: even
poor-quality inputs may have relevant information
interspersed. This information can include accurate
speech-recognition output, names which are recog-
nizable even in wrong-language material, and clean
target-language passages interleaved with the mark-
up. Thus, here we address methods to make IE ro-
bust to such varied-quality inputs. Specifically, our
overall goals are
? to skip processing non-language material such
as standard or database-specific mark-up,
? to process all non-target-language text cau-
tiously, catching interspersed target-language
text as well as text which is compatible with
the target language, e.g. person names which
are the same in the target- and non-target lan-
guage, and
? to degrade gracefully when processing anoma-
lous target-language material,
while minimizing any disruption of the processing
of clean, target-language text, and avoiding any ne-
cessity for explicit pre-classification of the genre of
material being input to the system. Such explicit
classification would be impractical in the presence
of the interleaving and the unconstrained data for-
mats from unpredetermined sources.
We begin our robustness work by addressing an
important and basic IE task: mention detection
(MD). MD is the task of identifying and classifying
textual references to entities in open-domain texts.
Mentions may be of type ?named? (e.g. John, Las
Vegas), ?nominal? (e.g. engineer, dentist)
or ?pronominal? (e.g. they, he). A mention also
336
has a specific class which describes the type of en-
tity it refers to. For instance, consider the following
sentence:
Julia Gillard, prime
minister of Australia,
declared she will enhance
the country?s economy.
Here we see three mentions of one person en-
tity: Julia Gillard, prime minister, and
she; these mentions are of type named, nominal,
and pronominal, respectively. Australia and
country are mentions of type named and nominal,
respectively, of a single geopolitical entity. Thus, the
MD task is a more general and complex task than
named-entity recognition, which aims at identifying
and classifying only named mentions.
Our approach to IE has been to use language-
independent algorithms, in order to facilitate reuse
across languages, but we train them with language-
specific data, for the sake of accuracy. Therefore, in-
put is expected to be predominantly in a target lan-
guage. However, real-world data genres inevitably
include some mixed-language/non-linguistic input.
Genre-specific training is typically infeasible due
to such application-specific data sets being unanno-
tated, motivating this line of research. Therefore, the
goal of this study is to investigate schemes to make a
language-specific MD engine robust to the types of
interspersed non-target material described above. In
these initial experiments, we work with English as
the target language, though we aim to make our ap-
proach to robustness as target-language-independent
as possible.
While our ultimate goal is a language-
independent approach to robustness, in these
initial experiments, English is the target language.
However, we process mixed-language material
including real-world data with its own peculiar
mark-up, text conventions including abbreviations,
and mix of languages, with the goal of English MD.
We approach robust MD using a multi-stage strat-
egy. First, non-target-character-set passages (here,
non-Latin-alphabet) are identified and marked for
non-processing. Then, following word-tokenization,
we apply a language classifier to a sliding variable-
length set of windows in order to generate fea-
tures for each word indicative of how much the text
around that word resembles good English, primar-
ily in comparison to other Latin-alphabet languages.
These features are used in a separate maximum-
entropy classifier whose output is a single feature to
add to the MD classifier. Additional features, pri-
marily to distinguish English from non-language in-
put, are added to MD as well. An example is the
minimum of the number of letters and the number of
digits in the ?word?, which when greater than zero
often indicates database detritus. Then we run the
MD classifier enhanced with these new robustness-
oriented features. We evaluate using a detection-
error-trade-off (DET) (Martin et al, 1997) anal-
ysis, in addition to traditional precision/recall/F -
measure.
This paper is organized as follows. Section 2 dis-
cusses previous work. Section 3 describes the base-
line maximum-entropy-based MD system. Section 4
introduces enhancements to the system to achieve
robustness. Section 5 describes databases used for
experiments, which are discussed in Section 6, and
Section 7 draws conclusions and plots future work.
2 Previous work on mention detection
The MD task has close ties to named-entity recog-
nition, which has been the focus of much recent re-
search (Bikel et al, 1997; Borthwick et al, 1998;
Tjong Kim Sang, 2002; Florian et al, 2003; Bena-
jiba et al, 2009), and has been at the center of sev-
eral evaluations: MUC-6, MUC-7, CoNLL?02 and
CoNLL?03 shared tasks. Usually, in computational-
linguistics literature, a named entity represents an
instance of either a location, a person, an organi-
zation, and the named-entity-recognition task con-
sists of identifying each individual occurrence of
names of such an entity appearing in the text. As
stated earlier, in this paper we are interested in
identification and classification of textual references
to object/abstraction mentions, which can be either
named, nominal or pronominal. This task has been
a focus of interest in ACE since 2003. The recent
ACE evaluation campaign was in 2008.
Effort to handle noisy data is still limited, espe-
cially for scenarios in which the system at decoding
time does not have prior knowledge of the input data
source. Previous work dealing with unstructured
data assumes the knowledge of the input data source.
As an example, E. Minkov et al (Minkov et al,
2005) assume that the input data is text from e-mails,
and define special features to enhance the detection
of named entities. Miller et al (Miller et al, 2000)
assume that the input data is the output of a speech
or optical character recognition system, and hence
extract new features for better named-entity recog-
nition. In a different research problem, L. Yi et al
eliminate the noisy text from the document before
337
performing data mining (Yi et al, 2003). Hence,
they do not try to process noisy data; instead, they
remove it. The approach we propose in this paper
does not assume prior knowledge of the data source.
Also we do not want to eliminate the noisy data, but
rather attempt to detect the appropriate mentions, if
any, that appear in that portion of the data.
3 Mention-detection algorithm
Similarly to classical NLP tasks such as base phrase
chunking (Ramshaw and Marcus, 1999) and named-
entity recognition (Tjong Kim Sang, 2002), we for-
mulate the MD task as a sequence-classification
problem, by assigning to each word token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is out-
side any mentions. We also assign to every non-
outside label a class to specify entity type e.g. per-
son, organization, location, etc. We are interested
in a statistical approach that can easily be adapted
for several languages and that has the ability to
integrate easily and make effective use of diverse
sources of information to achieve high system per-
formance. This is because, similar to many NLP
tasks, good performance has been shown to depend
heavily on integrating many sources of informa-
tion (Florian et al, 2004). We choose a Maximum
Entropy Markov Model (MEMM) as described pre-
viously (Florian et al, 2004; Zitouni and Florian,
2009). The maximum-entropy model is trained us-
ing the sequential conditional generalized iterative
scaling (SCGIS) technique (Goodman, 2002), and it
uses a Gaussian prior for regularization (Chen and
Rosenfeld, 2000)1.
3.1 Mention detection: standard features
The featues used by our mention detection systems
can be divided into the following categories:
1. Lexical Features Lexical features are imple-
mented as token n-grams spanning the current
token, both preceding and following it. For a
token xi, token n-gram features will contain the
previous n?1 tokens (xi?n+1, . . . xi?1) and the
following n? 1 tokens (xi+1, . . . xi+n?1). Set-
ting n equal to 3 turned out to be a good choice.
2. Gazetteer-based Features The gazetteer-
based features we use are computed on tokens.
1Note that the resulting model cannot really be called a
maximum-entropy model, as it does not yield the model which
has the maximum entropy (the second term in the product), but
rather is a maximum-a-posteriori model.
The gazetteers consist of several class of
dictionaries: including person names, country
names, company names, etc. Dictionar-
ies contain single names such as John or
Boston, and also phrases such as Barack
Obama, New York City, or The United
States. During both training and decoding,
when we encounter in the text a token or a
sequence of tokens that completely matches an
entry in a dictionary, we fire its corresponding
class.
The use of this framework to build MD systems
for clean English text has given very competitive re-
sults at ACE evaluations (Florian et al, 2006). Try-
ing other classifiers is always a good experiment,
which we didn?t pursue here for two reasons: first,
the MEMM system used here is state-of-the-art, as
proven in evaluations and competitions ? while it is
entirely possible that another system might get better
results, we don?t think the difference would be large.
Second, we are interested in ways of improving per-
formance on noisy data, and we expect any system
to observe similar degradation in performance when
presented with unexpected input ? showing results
for multiple classifier types might very well dilute
the message, so we stuck to one classifier type.
4 Enhancements for robustness
As stated above, our goal is to skip spans of charac-
ters which do not lend themselves to target-language
MD, while minimizing impact on MD for target-
language text, with English as the initial target lan-
guage for our experiments. More specifically, our
task is to process data automatically in any unprede-
termined format from any source, during which we
strive to avoid outputting spurious mentions on:
? non-language material, such as mark-up tags
and other data-set detritus, as well as non-text
data such as code or binaries likely mistakenly
submitted to the MD system,
? non-target-character-set material, here, non-
Latin-alphabet material, such as Arabic and
Chinese in their native character sets, and
? target-character-set material not in the target
language, here, Latin-alphabet languages other
than English.
It is important to note that this is not merely
a document-classification problem; this non-target
data is often interspersed with valid input text.
338
Mark-up is the obvious example of interspersing;
however, other categories of non-target data can also
interleave tightly with valid input. A few examples:
? English text is sometimes infixed right in a Chi-
nese sentence, such as
? some translation algorithms will leave un-
changed an untranslatable word, or will
transliterate it into the target language using a
character convention which may not be a stan-
dard known to the MD engine, and
? some target-alphabet-but-non-target-language
material will be compatible with the target
language, particularly people?s names. An
example with English as the target lan-
guage is Barack Obama in the Spanish
text ...presidente de Estados
Unidos, Barack Obama, dijo el
da 24 que ....
Therefore, to minimize needless loss of process-
able material, a robustness algorithm ideally does a
sliding analysis, in which, character-by-character or
word-by-word, material may be deemed to be suit-
able to process. Furthermore, a variety of strategies
will be needed to contend with the diverse nature of
non-target material and the patterns in which it will
appear among valid input.
Accordingly, the following is a summary of algo-
rithmic enhancements to MD:
1. detection of standard file formats, such as
SGML, and associated detagging,
2. segmentation of the file into target- vs. non-
target-character-set passages, such that the lat-
ter not be processed further,
3. tokenization to determine word and sentence
units, and
4. MD, augmented as follows:
? Sentence-level categorization of likeli-
hood of good English.
? If ?clean? English was detected, run the
same clean baseline model as described in
Section 3.
? If the text is determined to be a
bad fit to English, run an alternate
maximum-entropy model that is heavily
based on gazetteers, using only context-
independent (e.g. primarily gazetteer-
based) features, to catch isolated ob-
vious English/English-compatible names
embedded in otherwise-foreign text.
? If in between ?clean? and ?bad?, use
a ?mixed? maximum-entropy MD model
whose training data and feature set are
augmented to handle interleaving of En-
glish with mark-up and other languages.
These MD-algorithm enhancements will be de-
scribed in the following subsections.
4.1 Detection and detagging for standard file
formats
Some types of mark-up are well-known standards,
such as SGML (Warmer and van Egmond, 1989).
Clearly the optimal way of dealing with them is to
apply detectors of these specific formats, and associ-
ated detaggers, as done previously (Yi et al, 2003).
For this reason, standard mark-up is not a subject of
the current study; rather, our concern is with mark-
up peculiar to specific data sets, as described above,
and so while this step is part of our overall strategy,
it is not employed in the present experiments.
4.2 Character-set segmentation
Some entity mentions may be recognizable in a non-
target language which shares the target-language?s
character set, for example, a person?s name recog-
nizable by English speakers in an otherwise-not-
understandable Spanish sentence. However, non-
target character sets, such as Arabic and Chinese
when processing English, represent pure noise for
an IE system. Therefore, deterministic character-
set segmentation is applied, to mark non-target-
character-set passages for non-processing by the re-
mainder of the system, or, in a multilingual system,
to be diverted to a subsystem suited to process that
character set. Characters which can be ambiguous
with regard to character set, such as some punctua-
tion marks, are attached to target-character-set pas-
sages when possible, but are not considered to break
non-target-character-set passages surrounding them
on both sides.
4.3 Tokenization
Subsequent processing is based on determination of
the language of target-alphabet text. The fundamen-
tal unit of such processing is target-alphabet word,
necessitating tokenization at this point into word-
level units. This step includes punctuation sepa-
339
ration as well as the detction of sentence bound-
ary (Zimmerman et al, 2006).
4.4 Robust mention detection
After preprocessing steps presented earlier, we de-
tect mentions using a cascaded approach that com-
bines several MD classifiers. Our goal is to select
among maximum-entropy MD classifiers trained
separately to represent different degrees of ?nois-
iness? occurring in many genres of data, includ-
ing machine-translation output, informal communi-
cations, mixed-language material, varied forms of
non-standard database mark-up, etc. We somewhat-
arbitrarily choose to employ three classifiers as de-
scribed below. We select a classifier based on a
sentence-level determination of the material?s fit to
the target language. First, we build an n-gram lan-
guage model on clean target-language training text.
This language model is used to compute the perplex-
ity (PP ) of each sentence during decoding. The
PP indicates the quality of the text in the target-
language (i.e. English) (Brown et al, 1992); the
lower the PP , the cleaner the text. A sentence
with a PP lower than a threshold ?1 is considered
?clean? and hence the ?clean? baseline MD model
described in Section 3 is used to detect mentions
of this sentence. The clean MD model has access
to standard features described in Section 3.1. In
the case where a sentence looks particularly badly
matched to the target language, defined as PP > ?2,
we use a ?gazetteer-based? model based on a dic-
tionary look-up to detect mentions; we retreat to
seeking known mentions in a context-independent
manner reflecting that most of the context consists
of out-of-vocabulary words. The gazetteer-based
MD model has access only to gazetteer information
and does not look to lexical context during decod-
ing, reflecting the likelihood that in this poor ma-
terial, words surrounding any recognizable mention
are foreign and therefore unusable. In the case of an
in-between determination, that is, a sentence with
?1 < PP < ?2, we use a ?mixed? MD model, based
on augmenting the training data set and the feature
set as described in the next section. The values of ?1
and ?2 are estimated empirically on a separate devel-
opment data set that is also used to tune the Gaussian
prior (Chen and Rosenfeld, 2000). This set contains
a mix of clean English and Latin-alphabet-but-non-
English text that is not used for traning and evalua-
tion.
The advantage of this combination strategy is that
we do not need pre-defined knowledge of the text
source in order to apply an appropriate model. The
selection of the appropriate model to use for de-
coding is done automatically based on PP value of
the sentence. We will show in the experiments sec-
tion how this combination strategy is effective not
only in maintaining good performance on a clean
English text but also in improving performance on
non-English data when compared to other source-
specific MD models.
4.5 Mixed mention detection model
The mixed MD model is designed to process ?sen-
tences? mixing English with non-English, whether
foreign-language or non-language material. Our
approach is to augment model training compared
to the clean baseline by adding non-English,
mixed-language, and non-language material, and
to augment the model?s feature set with language-
identification features more localized than the
sentence-level perplexity described above, as well as
other features designed primarily to distinguish non-
language material such as mark-up codes.
4.5.1 Language-identification features
We apply an n-gram-based language classi-
fier (Prager, 1999) to variable-length sliding win-
dows as follows. For each word, we run 1- through
6-preceding-word windows through the classifier,
and 1- through 6-word windows beginning with the
word, for a total of 12 windows, yielding for each
window a result like:
0.235 Swedish
0.148 English
0.134 French
...
For each of the 12 results, we extract three fea-
tures: the identity of the top-scoring language, here,
Swedish; the confidence score in the top-scoring
language, here, 0.235; and the score difference be-
tween the target language (English for these ex-
periments) and the top-scoring non-target language,
here, 0.148 ? 0.235 = ?0.087. Thus we have
a 36-feature vector for each word. We bin these
and use them as input to a maximum-entropy clas-
sifier (separate from the MD classifier) which out-
puts ?English? or ?Non-English?, and a confidence
score. These scores in turn are binned into six cate-
gories to serve as a ?how-English-is-it? feature in the
augmented MD model. The language-identification
classifier and the maximum-entropy ?how-English?
classifier are each trained on text data separate from
340
each other and from the training and test sets for
MD.
4.5.2 Additional features
The following features are designed to capture
evidence of whether a ?word? is in fact linguistic
material or not: number of alphabetic characters,
number of characters, maximum consecutive rep-
etitions of a character, numbers of non-alphabetic
and non-alphanumeric characters, fraction of char-
acters which are alphabetic, fraction alphanumeric,
and number of vowels. These features are part of the
augmentation of the mixed MD model relative to the
clean MD model.
5 Data sets
Four data sets are used for our initial experiments.
One, ?English?, consists of 367 documents total-
ing 170,000 words, drawn from web news stories
from various sources and detagged to be plain text.
This set is divided into 340 documents as a train-
ing set and 27 for testing, annotated as described in
more detail elsewhere (Han, 2010). These data av-
erage approximately 21 annotated mentions per 100
words.
The second set, ?Latin?, consists of 23 detagged
web news articles from 11 non-English Latin-
alphabet languages totaling 31,000 words. Of these
articles, 12 articles containing 19,000 words are
used as a training set, with the remaining used for
testing, and each set containing all 11 languages.
They are annotated using the same annotation con-
ventions as ?English?, and from the perspective of
English; that is, only mentions which would be clear
to an English speaker are labeled, such as Barack
Obama in the Spanish example in Section 4. For
this reason, these data average only approximately 5
mentions per 100 words.
The third, ?Transactions?, consists of approxi-
mately 60,000 words drawn from a text data set
logging real financial transactions. Figure 1 shows
example passages from this database, anonymized
while preserving the character of the content.
This data set logs transactions by a staff of
customer-service representatives. English is the pri-
mary language, but owing to international clientele,
occasionally representatives communicate in other
languages, such as the German here, or in English
but mentioning institutions in other countries, here, a
Czech bank. Interspersed among text are codes spe-
cific to this application which delineate and identify
various information fields and punctuate long pas-
sages. The application also places constraints on
legal characters, leading to the unusual representa-
tion of underline and the ?at? sign as shown, mak-
ing for an e-mail address which is human-readable
but likely not obvious to a machine. Abbreviations
represent terms particularly common in this appli-
cation area, though they may not be obvious with-
out adapting to the application; these include stan-
dards like HUF, a currency code which stands for
Hungarian forint, and financial-transaction peculiar-
ities like BNF for ?beneficiary? as seen in Figure 1.
In short, good English is interspersed with non-
language content, foreign-language text, and rough
English like data-entry errors and haphazard abbre-
viations. These data average 4 mentions per 100
words.
Data sets with peculiarities analogous to those in
this Transactions set are commonplace in a variety
of settings. Training specific to data sets like this is
often infeasible due to lack of labeled data, insuffi-
cient data for training, and the multitude of such data
formats. For this reason, we do not train on Transac-
tions, letting our testing on this data set serve as an
example of testing on such data formats unseen.
6 Experiments
MD systems were trained to recognize the 116
entity-mention types shown in Table 1, annotated as
described previously (Han, 2010). The clean-data
classifier was trained on the English training data us-
ing the feature set described in Section 3.1. The clas-
sifier for ?mixed?-quality data and the ?gazetteer?
model were each trained on that set plus the ?Latin?
training set and the supplemental set. In addition,
?mixed? training included the additional features de-
scribed in Section 4.5. The framework used to build
the baseline MD system is similar to the one we used
in the ACE evaluation2. This system has achieved
competitive results with an F -measure of 82.7 when
trained on the seven main types of ACE data with
access to wordnet and part-of-speech-tag informa-
tion as well as output of other MD and named-entity
recognizers (Zitouni and Florian, 2008).
It is instructive to evaluate on the individual com-
ponent systems as well as the combination, despite
the fact that the individual components are not well-
suited to all the data sets, for example, the mixed
and gazetteer systems being a poorer fit to the En-
glish task than the baseline, and vice versa for the
2NIST?s ACE evaluation plan:
http://www.nist.gov/speech/tests/ace/index.htm
341
age event-custody facility people date
animal event-demonstration food percent duration
award event-disaster geological-object person e-mail-address
cardinal event-legal geo-political product measure
disease event-meeting law substance money
event event-performance location title-of-a-work phone-number
event-award event-personnel ordinal vehicle ticker-symbol
event-communication event-sports organ weapon time
event-crime event-violence organization web-address
Table 1: Entity-type categories used in these experiments. The eight in the right-most column are not
further distinguished by mention type, while the remaining 36 are further classified as named, nominal or
pronominal, for a total of 36 ? 3 + 8 = 116 mention labels.
English Latin Transactions
P R F P R F P R F
Clean 78.7 73.6 76.1 16.0 40.0 22.9 19.5 32.2 24.3
Mixed 77.9 69.7 73.6 78.5 55.9 65.3 37.1 47.8 41.7
Gazetteer 76.9 66.2 71.1 77.8 55.5 64.8 36.5 47.5 41.3
Combination 78.1 73.2 75.6 80.4 56.0 66.0 38.5 49.1 43.2
Table 2: Performance of clean, mixed, and gazetteer-based mention detection systems as well as their com-
bination. Performance is presented in terms of Precision (P), Recall (R), and F -measure (F).
non-target data sets. Precision/recall/F -measure re-
sults are shown in Table 2. Not surprisingly, the
baseline system, intended for clean data, performs
poorly on noisy data. The mixed and gazetteer sys-
tems, having a variety of noisy data in their train-
ing set, perform much better on the noisy conditions,
particularly on Latin-alphabet-non-English data be-
cause that is one of the conditions included in its
training, while Transactions remains a condition not
covered in the training set and so shows less im-
provement. However, because the mixed classifier,
and moreso the gazetteer classifier, are oriented to
noisy data, on clean data they suffer in performance
by 2.5 and 5 F -measure points, respectively. But
system combination serves us well: it recovers all
but 0.5 F -measure point of this loss, while also ac-
tually performing better on the noisy data sets than
the two classifiers specifically targeted toward them,
as can be seen in Table 2. It is important to note
that the major advantage of using the combination
model is the fact that we do not have to know the
data source in order to select the appropriate MD
model to use. We assume that the data source is
unknown, which is our claim in this work, and we
show that we obtain better performance than using
source-specific MD models. This reflects the fact
that a noisy data set will in fact have portions with
varying degrees of ?noise?, so the combination out-
performs any single model targeted to a single par-
ticular level of noise, enabling the system to con-
tend with such variability without the need for pre-
segregating sub-types of data for noise level. The
obtained improvement from the system combination
over all other models is statistically significant based
on the stratified bootstrap re-sampling significance
test (Noreen, 1989). We consider results statistically
significant when p < 0.05, which is the case in this
paper. This approach was used in the named-entity-
recognition shared task of CoNNL-20023.
It should be noted that some completely-non-
target types of data, such as non-target-character set
data, have been omitted from analysis here. In-
cluding them would make our system look compar-
atively stronger, as they would have only spurious
mentions and so generate false alarms but no correct
mentions in the baseline system, while our system
deterministically removes them.
As mentioned above, we view MD robustness pri-
marily as an effort to eliminate, relative to a base-
line system, large volumes of spurious ?mentions?
detected in non-target input content, while minimiz-
3http://www.cnts.ua.ac.be/conll2002/ner/
342
(a) DET plot for clean (baseline), mixed, gazetteer,
and combination MD systems on the Latin-alphabet-
non-English text. The clean system (upper curve)
performs far worse than the other three systems de-
signed to provide robustness; these systems in turn
perform nearly indistinguishably.
(b) DET plot for clean (baseline), mixed, gazetteer,
and combination MD systems on the Transactions
data set. The clean system (upper/longer curve)
reaches far higher false-alarm rates, while never ap-
proaching the lower miss rates achievable by any of
the other three systems, which in turn perform com-
parably to each other.
Figure 2: DET plots for Latin-alphabet-non-English and Transactions data sets
ing disruption of detection in target input. A sec-
ondary goal is recall in the event of occasional valid
mentions in such non-target material. Thus, as in-
put material degrades, precision increases in impor-
tance relative to recall. As such, we view precision
and recall asymmetrically on this task, and so rather
than evaluating purely in terms of F -measure, we
perform a detection-error-trade-off (DET) (Martin
et al, 1997) analysis, in which we plot a curve of
miss rate on valid mentions vs. false-alarm rate, with
the curve traced by varying a confidence threshold
across its range. We measure false-alarm and miss
rates relative to the number of actual mentions anno-
tated in the data set:
FA rate = # false alarms# annotated mentions (1)
Miss rate = # misses# annotated mentions (2)
where false alarms are ?mentions? output by the sys-
tem but not appearing in annotation, while misses
are mentions which are annotated but do not ap-
pear in the system output. Each mention is treated
equally in this analysis, so frequently-recurring en-
tity/mention types weigh on the results accordingly.
Figure 2a shows a DET plot for the clean, mixed,
gazetteer, and combination systems on the ?Latin?
data set, while Figure 2b shows the analogous plot
for the ?Transactions? data set. The drastic gains
made over the baseline system by the three experi-
mental systems are evident in the plots. For exam-
ple, on Latin, choosing an operating point of a miss
rate of 0.6 (nearly the best achievable by the clean
system), we find that the robustness-oriented sys-
tems eliminate 97% of the false alarms of the clean
baseline system, as the plot shows false-alarm rates
near 0.07 compared to the baseline?s of 2.08. Gains
on Transaction data are more modest, owing to this
case representing a data genre not included in train-
ing. It should be noted that the jaggedness of the
Transaction curves traces to the repetitive nature of
some of the terms in this data set.
In making a system more oriented toward robust-
ness in the face of non-target inputs, it is important
to quantify the effect of these systems being less-
oriented toward clean, target-language text. Figure 3
shows the analogous DET plot for the English test
set, showing that achieving robustness through the
combination system comes at a small cost to accu-
racy on the text the original system is trained to pro-
cess.
7 Conclusions
For information-extraction systems to be useful,
their performance must degrade gracefully when
confronted with inputs which deviate from ideal
and/or derive from unknown sources in unknown
formats. Imperfectly-translated, mixed-language,
marked-up text and non-language material must not
343
Figure 3: DET plot for clean (baseline), mixed,
gazetteer, and combination MD systems on clean English
text, verifying that performance by the clean system (low-
est curve) is very closely approximated by the combina-
tion system (second-lowest curve), while the mixed sys-
tem performs somewhat worse and the gazetteer system
(top curve), worse still, reflecting that these systems are
increasingly oriented toward noisy inputs.
be processed in a garbage-in-garbage-out fashion
merely because the system was designed only to
handle clean text in one language. Thus we have em-
barked on information-extraction-robustness work,
to improve performance on imperfect inputs while
minimizing disruption of processing of clean text.
We have demonstrated that for one IE task, mention
detection, a multi-faceted approach, motivated by
the diversity of input data imperfections, can elimi-
nate a large proportion of the spurious outputs com-
pared to a system trained on the target input, at a
relatively small cost of accuracy on that target input.
This outcome is achieved by a system-combination
approach in which a perplexity-based measure of
how well the input matches the target language is
used to select among models designed to deal with
such varying levels of noise. Rather than relying on
explicit recognition of genre of source data, the ex-
perimental system merely does its own assessment
of how much each sentence-sized chunk matches the
target language, an important feature in the case of
unknown text sources.
Chief among directions for further work is to con-
tinue to improve performance on noisy data, and to
strengthen our findings via larger data sets. Addi-
tionally, we look forward to expanding analysis to
different types of imperfect input, such as machine-
translation output, different types of mark-up, and
different genres of real data. Further work should
also explore the degree to which the approach to
achieving robustness must vary according to the tar-
get language. Finally, robustness work should be ex-
panded to other information-extraction tasks.
Acknowledgements
The authors thank Ben Han, Anuska Renta,
Veronique Baloup-Kovalenko and Owais Akhtar for
their help with annotation. This work was supported
in part by DARPA under contract HR0011-08-C-
0110.
References
Y. Benajiba, M. Diab, and P. Rosso. 2009. Arabic named
entity recognition: A feature-driven study. In the spe-
cial issue on Processing Morphologically Rich Lan-
guages of the IEEE Transaction on Audio, Speech and
Language.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of ANLP-97, pages 194?201.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, J. C.
Lai, and R. L. Mercer. 1992. An estimate of an up-
per bound for the entropy of English. Computational
Linguistics, 18(1), March.
S. Chen and R. Rosenfeld. 2000. A survey of smooth-
ing techniques for ME models. IEEE Transaction on
Speech and Audio Processing.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In Conference on Computational Natural Lan-
guage Learning - CoNLL-2003, Edmonton, Canada,
May.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proceedings of HLT-NAACL 2004, pages
1?8.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in men-
tion detection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 473?480, Sydney, Australia,
July. Association for Computational Linguistics.
J. Goodman. 2002. Sequential conditional generalized
iterative scaling. In Proceedings of ACL?02.
D. B. Han. 2010. Klue annotation guidelines - version
2.0. Technical Report RC25042, IBM Research, Au-
gust.
344
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki. 1997. The DET curve in assessment
of detection task performance. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), pages 1895?1898. Rhodes,
Greece.
D. Miller, S. Boisen, R. Schwartz, R. Stone, and
R. Weischedel. 2000. Named entity extraction from
noisy input: speech and OCR. In Proceedings of the
sixth conference on Applied natural language process-
ing, pages 316?324, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from email: Applying named
entity recognition to informal text. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 443?450, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley Sons.
J. F. Pitrelli, B. L. Lewis, E. A. Epstein, M. Franz,
D. Kiecza, J. L. Quinn, G. Ramaswamy, A. Srivas-
tava, and P. Virga. 2008. Aggregating Distributed
STT, MT, and Information Extraction Engines: The
GALE Interoperability-Demo System. In Interspeech.
Brisbane, NSW, Australia.
J. M. Prager. 1999. Linguini: Language identification for
multilingual documents. In Journal of Management
Information Systems, pages 1?11.
L. Ramshaw and M. Marcus. 1999. Text chunking using
transformation-based learning. In S. Armstrong, K.W.
Church, P. Isabelle, S. Manzi, E. Tzoukermann, and
D. Yarowsky, editors, Natural Language Processing
Using Very Large Corpora, pages 157?176. Kluwer.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independentnamed entity
recognition. In Proceedings of CoNLL-2002, pages
155?158. Taipei, Taiwan.
J. Warmer and S. van Egmond. 1989. The implementa-
tion of the Amsterdam SGML parser. Electron. Publ.
Origin. Dissem. Des., 2(2):65?90.
L. Yi, B. Liu, and X. Li. 2003. Eliminating noisy in-
formation in web pages for data mining. In KDD ?03:
Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 296?305, New York, NY, USA. ACM.
M. Zimmerman, D. Hakkani-Tur, J. Fung, N. Mirghafori,
L. Gottlieb, E. Shriberg, and Y. Liu. 2006. The
ICSI+ multilingual sentence segmentation system. In
Interspeech, pages 117?120, Pittsburgh, Pennsylvania,
September.
I. Zitouni and R. Florian. 2008. Mention detection
crossing the language barrier. In Proceedings of
EMNLP?08, Honolulu, Hawaii, October.
I. Zitouni and R. Florian. 2009. Cross-language informa-
tion propagation for Arabic mention detection. ACM
Transactions on Asian Language Information Process-
ing (TALIP), 8(4):1?21.
345
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 889?898,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Correction Model for Word Alignments
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos, Bing Xiang, Jian-ming Xu
IBM T.J. Watson Research Center
1101 Kitchawan Road, Rt. 134
Yorktown Heights, NY 10598
{jsmc,abei,roukos,bxiang,jianxu}@us.ibm.com
Abstract
Models of word alignment built as sequences
of links have limited expressive power, but are
easy to decode. Word aligners that model the
alignment matrix can express arbitrary align-
ments, but are difficult to decode. We pro-
pose an alignment matrix model as a cor-
rection algorithm to an underlying sequence-
based aligner. Then a greedy decoding al-
gorithm enables the full expressive power of
the alignment matrix formulation. Improved
alignment performance is shown for all nine
language pairs tested. The improved align-
ments also improved translation quality from
Chinese to English and English to Italian.
1 Introduction
Word-level alignments of parallel text are crucial for
enabling machine learning algorithms to fully uti-
lize parallel corpora as training data. Word align-
ments appear as hidden variables in IBM Models 1-
5 (Brown et al, 1993) in order to bridge a gap be-
tween the sentence-level granularity that is explicit
in the training data, and the implicit word-level cor-
respondence that is needed to statistically model lex-
ical ambiguity and word order rearrangements that
are inherent in the translation process. Other no-
table applications of word alignments include cross-
language projection of linguistic analyzers (such as
POS taggers and named entity detectors,) a subject
which continues to be of interest. (Yarowsky et al,
2001), (Benajiba and Zitouni, 2010)
The structure of the alignment model is tightly
linked to the task of finding the optimal alignment.
Many alignment models are factorized in order to
use dynamic programming and beam search for ef-
ficient marginalization and search. Such a factoriza-
tion encourages - but does not require - a sequential
(often left-to-right) decoding order. If left-to-right
decoding is adopted (and exact dynamic program-
ming is intractable) important right context may ex-
ist beyond the search window. For example, the link-
age of an English determiner may be considered be-
fore the linkage of a distant head noun.
An alignment model that jointly models all of the
links in the entire sentence does not motivate a par-
ticular decoding order. It simply assigns comparable
scores to the alignment of the entire sentence, and
may be used to rescore the top-N hypotheses of an-
other aligner, or to decide whether heuristic pertur-
bations to the output of an existing aligner constitute
an improvement. Both the training and decoding of
full-sentence models have presented difficulties in
the past, and approximations are necessary.
In this paper, we will show that by using an ex-
isting alignment as a starting point, we can make a
significant improvement to the alignment by propos-
ing a series of heuristic perturbations. In effect, we
train a model to fix the errors of the existing aligner.
From any initial alignment configuration, these per-
turbations define a multitude of paths to the refer-
ence (gold) alignment. Our model learns alignment
moves that modify an initial alignment into the ref-
erence alignment. Furthermore, the resulting model
assigns a score to the alignment and thus could be
used in numerous rescoring algorithms, such as top-
N rescorers.
In particular, we use the maximum entropy frame-
889
work to choose alignment moves. The model is sym-
metric: source and target languages are interchange-
able. The alignment moves are sufficiently rich to
reach arbitrary phrase to phrase alignments. Since
most of the features in the model are not language-
specific, we are able to test the correction model
easily on nine language pairs; our corrections im-
proved the alignment quality compared to the input
alignments in all nine. We also tested the impact on
translation and found a 0.48 BLEU improvement on
Chinese to English and a 1.26 BLEU improvement
on English to Italian translation.
2 Alignment sequence models
Sequence models are the traditional workhorse for
word alignment, appearing, for instance, in IBM
Models 1-5. This type of alignment model is not
symmetric; interchanging source and target lan-
guages results in a different aligner. This parameter-
ization does not allow a target word to be linked to
more than one source word, so some phrasal align-
ments are simply not considered. Often the choice of
directionality is motivated by this restriction, and the
choice of tokenization style may be designed (Lee,
2004) to reduce this problem. Nevertheless, aligners
that use this parameterization internally often incor-
porate various heuristics in order to augment their
output with the disallowed alignments - for example,
swapping source and target languages to obtain a
second alignment (Koehn et al, 2007) with different
limitations. Training both directions jointly (Liang
et al, 2006) and using posterior probabilities dur-
ing alignment prediction even allows the model to
see limited right context. Another alignment combi-
nation strategy (Deng and Zhou, 2009) directly op-
timizes the size of the phrase table of a target MT
system.
Generative models (such as Models 1-5, and the
HMM model (Vogel et al, 1996)) motivate a narra-
tive where alignments are selected left-to-right and
target words are then generated conditioned upon
the alignment and the source words. Generative
models are typically trained unsupervised, from par-
allel corpora without manually annotated word-level
alignments.
Discriminative models of alignment incorporate
source and target words, as well as more linguisti-
cally motivated features into the prediction of align-
ment. These models are trained from annotated
word alignments. Examples include the maximum
entropy model of (Ittycheriah and Roukos, 2005) or
the conditional random field jointly normalized over
the entire sequence of alignments of (Blunsom and
Cohn, 2006).
3 Joint Models
An alternate parameterization of alignment is the
alignment matrix (Niehues and Vogel, 2008). For a
source sentence F consisting of words f1...fm, and
a target sentence E = e1...el, the alignment matrix
A = {?ij} is an l ? m matrix of binary variables.
If ?ij = 1, then ei is said to be linked to fj . If ei
is unlinked then ?ij = 0 for all j. There is no con-
straint limiting the number of source tokens to which
a target word is linked either; thus the binary ma-
trix allows some alignments that cannot be modeled
by the sequence parameterization. All 2lm binary
matrices are potentially allowed in alignment matrix
models. For typical l and m, 2lm  (m + 1)l, the
number of alignments described by a comparable se-
quence model. This parameterization is symmetric -
if source and target are interchanged, then the align-
ment matrix is transposed.
A straightforward approach to the alignment ma-
trix is to build a log linear model (Liu et al, 2005)
for the probability of the alignment A. (We continue
to refer to ?source? and ?target? words only for con-
sistency of notation - alignment models such as this
are indifferent to the actual direction of translation.)
The log linear model for the alignment (Liu et al,
2005) is
p(A|E,F ) = exp (
?
i ?i?i(A,E, F ))
Z(E,F ) (1)
where the partition function (normalization) is given
by
Z(E,F ) =
?
A
exp
(?
i
?i?i(A,E, F )
)
. (2)
Here the ?i(A,E, F ) are feature functions. The
model is parameterized by a set of weights ?i, one
for each feature function. Feature functions are often
binary, but are not required to be. Feature functions
890
may depend upon any number of components ?ij of
the alignment matrix A.
The sum over all alignments of a sentence pair
(2lm terms) in the partition function is computa-
tionally impractical except for very short sentences,
and is rarely amenable to dynamic programming.
Thus the partition function is replaced by an ap-
proximation. For example, the sum over all align-
ments may be restricted to a sum over the n-best
list from other aligners (Liu et al, 2005). This ap-
proximation was found to be inconsistent for small
n unless the merged results of several aligners were
used. Alternately, loopy belief propagation tech-
niques were used in (Niehues and Vogel, 2008).
Loopy belief propagation is not guaranteed to con-
verge, and feature design is influenced by consider-
ation of the loops created by the features. Outside
of the maximum entropy framework, similar models
have been trained using maximum weighted bipar-
tite graph matching (Taskar et al, 2005), averaged
perceptron (Moore, 2005), (Moore et al, 2006), and
transformation-based learning (Ayan et al, 2005).
4 Alignment Correction Model
In this section we describe a novel approach to word
alignment, in which we train a log linear (maximum
entropy) model of alignment by viewing it as correc-
tion model that fixes the errors of an existing aligner.
We assume a priori that the aligner will start from
an existing alignment of reasonable quality, and will
attempt to apply a series of small changes to that
alignment in order to correct it. The aligner naturally
consists of a move generator and a move selector.
The move generator perturbs an existing align-
ment A in order to create a set of candidate align-
mentsMt(A), all of which are nearby to A in the
space of alignments. We index the set of moves by
the decoding step t to indicate that we generate en-
tirely different (even non-overlapping) sets of moves
at different steps t of the alignment prediction. Typ-
ically the moves affect linkages local to a particular
word, e.g. the t?th source word.
The move selector then chooses one of the align-
ments At+1 ? Mt(At), and proceeds iteratively:
At+2 ? Mt+1(At+1), etc. until suitable termina-
tion criteria are reached. Pseudocode is depicted in
Fig. (1.) In practice, one move for each source and
Input: sentence pair E1 .. El, F1 .. Fm
Input: alignment A
Output: improved alignment Afinal
for t = 1? l do
generate moves:Mt(At)
select move:
At+1 ? argmaxA?Mt(At)p(A|At, E, F )
Afinal ? Al+1
{repeat for source words}
Figure 1: pseudocode for alignment correction
target word is sufficient.
4.1 Move generation
Many different types of alignment perturbations are
possible. Here we restrict ourselves to a very sim-
ple move generator that changes the linkage of ex-
actly one source word at a time, or exactly one target
word at a time. Many of our corrections are simi-
lar to those of (Setiawan et al, 2010), although our
motivation is perhaps closer to (Brown et al, 1993),
who used similar perturbations to approximate in-
tractable sums that arise when estimating the param-
eters of the generative models Models 3-5, and ap-
proach refined in (Och and Ney, 2003). We note that
our corrections are designed to improve even a high-
quality starting alignment; in contrast the model of
(Fossum et al, 2008) considers deletion of links
from an initial alignment (union of aligners) that is
likely to overproduce links.
From the point of view of the alignment ma-
trix, we consider changes to one row or one col-
umn (generically, one slice) of the alignment matrix.
At each step t, the move setMt(At) is formed by
choosing a slice of the current alignment matrix At,
and generating all possible alignments from a few
families of moves. Then the move generator picks
another slice and repeats. The m + l slices are cy-
cled in a fixed order: the first m slices correspond to
source words (ordered according to a heuristic top-
down traversal of the dependency parse tree if avail-
able), and the remaining l slices correspond to target
words, similarly parse-ordered. For each slice we
consider the following families of moves, illustrated
by rows.
? add link to row i - for one j such that ?ij = 0,
891
make ?ij = 1 (shown here for row i = 1.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? remove one or more links from row i - for some
j such that ?ij = 1, make ?ij = 0 (shown here
for i = 3.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? move a link in row i - for one j and one j? such
that ?ij = 1 and ?ij? = 0, make ?ij = 0 and
?ij? = 1 (shown here for i = 1.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? leave row i unchanged
Similar families of moves apply to column slices
(source words.) In practice, perturbations are re-
stricted by a window (typically ?5 from existing
links.) If the given source word is unlinked, we
consider adding a link to each target word in a win-
dow (?5 from nearby links.) The window size re-
strictions mean that some reference alignments are
not reachable from the starting point. However, this
is unlikely to limit performance - an oracle aligner
achieves 97.6%F -measure on the Arabic-English
training set.
4.2 Move selection
A log linear model for the selection of the candidate
alignment at t+1 from the set of alignmentsMt(At)
generated by the move generator at step t takes the
form:
p(At+1|E,F,Mt(At)) =
e
P
i ?i?i(At+1,E,F )
Z(E,F,Mt(At))
(3)
where the partition function is now given by
Z(E,F,M) =
?
A?M
e
P
i ?i?i(A,E,F ) (4)
and At+1 ? Mt(At) is required for correct normal-
ization. This equation is notationally very similar
to equation (1), except that the predictions of the
model are restricted to a small set of nearby align-
ments. For the move generator considered in this pa-
per, the summation in Eq.(4) is similarly restricted,
and hence training the model is tractable. The set
of candidate alignmentsMt(At) typically does not
contain the reference (gold) alignment; we model
the best alignment among a finite set of alternatives,
rather than the correct alignment from among all
possible alignments. This is a key difference be-
tween our model and (Liu et al, 2005).
Note that if we extended our definition of pertur-
bation to the limiting case that the alignment set in-
cluded all possible alignments then we would clearly
recover the standard log linear model of alignment.
4.3 Training
Since the model is designed to predict perturbation
to an alignment, it is trained from a collection of
errorful alignments and corresponding reference se-
quences of aligner moves that reach the reference
(gold) alignment. We construct a training set from
a collection of sentence pairs and reference align-
ments for training (A?n, En, Fn)Nn=1, as well as col-
lections of corresponding ?first pass? alignments An1
produced by another aligner. For each n, we form a
number of candidate alignment sets Mt(Ant ), one
for each source and target word. For training pur-
poses, the true alignment from the set is taken to be
the one identical withA?n in the slice targeted by the
move generator at the current step. (A small number
of move sets do not have an exact match and are dis-
carded.) Then we form an objective function from
the log likelihood of reference alignment, smoothed
with a gaussian prior
L =
?
n
Ln +
?
i
(?i/?)2 (5)
892
where the likelihood of each training sample is
Ln =
?
?
log p1(A0n|E,Fn;M(f?, A0n, E, Fn))
+
?
?
log p1(A0n|E,Fn;M(e?, A0n, E, Fn)) (6)
The likelihood has a term for each sentence pair
and for each decoder step. The model is trained
by gradient ascent using the l-BFGS method (Liu
and Nocedal, 1989), which has been successfully
used for training log linear models (Blunsom and
Cohn, 2006) in many natural language tasks, includ-
ing alignment.
5 Features
A wide variety of features were used in the model.
We group the features in three broad categories:
link-based, geometrical, and parse-based.
Link-based features are those which decompose
into a (linear) sum of alignment matrix elements ?ij .
An example link-based feature is one that fires if a
source language noun is linked to a target language
determiner. Note that this feature may fire more than
once in a given sentence pair: as with most fea-
tures in our model, it is an integer-valued feature
that counts the number of times a structure appears
in a sentence pair. These features do not capture any
correlation between different ?ij . Among the link-
based features are those based on Model 1 transla-
tion matrix parameters ?(ei|fj) and ?(fj |ei). We
bin the model 1 parameters, and form integer-valued
features for each bin that count the number of links
with ?0 < ?(ei|fj) < ?1.
Geometrical features are those which capture cor-
relation between different ?ij based on adjacency or
nearness. They capture the idea that nearby words
in one language link to nearby words in the other
language - the motivation of HMM-based models
of alignment. An example is a feature that counts
the number of times that the next word in the source
language is linked to the next word in the target lan-
guage:
?(A,E, F ) =
?
ij
?ij?i+1,j+1 (7)
Parse-based features are those which capture cor-
relation between different ?ij , but use parsing to de-
termine links which are correlated - for example, if a
determiner links to the same word as its head noun.
As an example, if ei is the headword of ei? , and fj is
the headword of fj? , then
?(A,E, F ) =
?
ij
?ij?i?j? (8)
counts the number of times that a dependency rela-
tion in one language is preserved by alignment in the
other language. This feature can also be decorated,
either lexically, or with part-of-speech tags (as many
features in all three categories are.)
5.1 Unsupervised Adaptation
We constructed a heuristic phrase dictionary for un-
supervised adapatation. After aligning a large unan-
notated parallel corpus with our aligner, we enumer-
ate fully lexicalized geometrical features that can be
extracted from the resulting alignments - these are
entries in a phrase dictionary. These features are
tied, and treated as a single real-valued feature that
fires during training and decoding phases if a set of
hypothesized links matches the geometrical feature
extracted from the unannotated data. The value of
this real-valued feature is the log of the number of
occurrences of the identical (lexicalized) geometri-
cal feature in the aligned unannotated corpus.
6 Results
We design our experiments to validate that a cor-
rection model using simple features, mostly non-
language-specific, can improve the alignment accu-
racy of a variety of existing aligners for a variety of
language pairs; we do not attempt to exactly match
features between comparison aligners - this is un-
likely to lead to a robust correction model.
6.1 Arabic-English alignment results
We trained the Arabic-English alignment system
on 5125 sentences from Arabic-English treebanks
(LDC2008E61, LDC2008E22) that had been an-
notated for word alignment. Reference parses
were used during the training. Results are mea-
sured on a 500 sentence test set, sampled from
a wide variety of parallel corpora, including vari-
ous genres. During alignment, only automatically-
generated parses (based on the parser of (Rat-
naparkhi, 1999)) were available. Alignments on
893
initial align correction model R (%) P (%) F (%) ?F
GIZA++ 76 76 76
corr(GIZA++) 86 94 90 14?
corr(ME-seq) 88 92 90 14?
HMM 73 73 73
corr(HMM) 87 92 89 16?
corr(ME-seq) 87 93 90 17?
ME-seq 82 84 83
corr(HMM) 88 92 90 7?
corr(GIZA++) 87 94 91 8?
corr(ME-seq) 89 94 91 8?
Table 1: Alignment accuracy for Arabic-English systems in percentage recall (R), precision(P), and F -measure. ?
denotes statistical significance (see text.)
lang method R (%) P(%) F (%) ?F
ZH?EN GIZA++ 55 67 61
ME-seq 66 72 69
corr(ME-seq) 74 76 75 6?
Table 2: Alignment accuracy for Chinese(ZH)-English(EN) systems. ? denotes statistical significance
lang aligner R(%) P(%) F (%) ?F
IT? EN ME-seq 74 87 80
corr(ME-seq) 84 92 88 8?
EN?IT ME-seq 75 86 80
corr(ME-seq) 84 92 88 8?
PT?EN ME-seq 77 83 80
corr(ME-seq) 87 91 89 9?
EN?PT ME-seq 79 87 83
corr(ME-seq) 88 90 89 6?
JA?EN ME-seq 72 78 75
corr(ME-seq) 77 83 80 5?
RU?EN ME-seq 81 85 83
corr(ME-seq) 82 92 87 4?
DE?EN ME-seq 77 82 79
corr(ME-seq) 78 87 82 3?
ES?EN ME-seq 93 86 90
corr(ME-seq) 92 88 90 0.6
FR?EN ME-seq 89 91 90
corr(ME-seq) 88 92 90 0.1
Table 3: Alignment accuracy for additional languages. ? denotes statistical significance; ? statistical significance not
available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French
894
the training and test sets were decoded with three
other aligners, so that the robustness of the cor-
rection model to different input alignments could
be validated. The three aligners were GIZA++
(Och and Ney, 2003) (with the MOSES (Koehn
et al, 2007) postprocessing option -alignment
grow-diag-final-and) the posterior HMM
aligner of (Ge, 2004), a maximum entropy sequen-
tial model (ME-seq) (Ittycheriah and Roukos, 2005).
ME-seq is our primary point of comparison: it is
discriminatively trained (on the same training data,)
uses a rich set of features, and provides the best
alignments of the three. Three correction models
were trained: corr(GIZA++) is trained to correct
the alignments produced by GIZA++, corr(HMM)
is trained to correct the alignments produced by the
HMM aligner, and corr(ME-seq) is trained to correct
the alignments produced by the ME-seq model.
In Table (1) we show results for our system cor-
recting each of the aligners as measured in the usual
recall, precision, and F -measure.1 The resulting
improvements in F -measure of the alignments pro-
duced by our models over their corresponding base-
lines is statistically significant (p < 10?4, indicated
by a ?.) Statistical significance is tested by a Monte
Carlo bootstrap (Efron and Tibshirani, 1986) - sam-
pling with replacement the difference in F -measure
of the two system?s alignments of the same sentence
pair. Both recall and precision are improved, but the
improvement in precision is somewhat larger. We
also show cross-condition results in which a correc-
tion model trained to correct HMM alignments is ap-
plied to correct ME-seq alignments. These results
show that our correction model is robust to different
starting aligners.
6.2 Chinese-English alignment results
Table (2) presents results for Chinese-English word
alignments. The training set for the corr(ME-
seq) model consisted of approximately 8000 hand-
aligned sentences sampled from LDC2006E93 and
LDC2008E57. The model was trained to correct
the output of the ME-seq aligner, and tested on
the same condition. For this language pair, refer-
ence parses were not available in our training set, so
1We do not distinguish sure and possible links in our anno-
tations - under this circumstance, alignment error rate(Och and
Ney, 2003) is 1? F .
automatically-generated parses were used for both
training and test sets. Results are measured on a 512
sentence test set, sampled from a wide variety of par-
allel corpora of various genres. We compare perfor-
mance with GIZA++, and with the ME-seq aligner.
Again the resulting improvement over the ME-seq
aligner is statistically significant. However, here the
improvement in recall is somewhat larger than the
improvement in precision.
6.3 Additional language pairs
Table (3) presents alignment results for seven other
language pairs. Separate alignment corrector mod-
els were trained for both directions of Italian ?
English and Portuguese ? English. The training
and test data vary by language, and are sampled
uniformly from a diverse set of corpora of various
genres, including newswire, and technical manuals.
Manual alignments for training and test data were
annotated. We compare performance with the ME-
seq aligner trained on the same training data. As
with the Chinese results above, customization and
feature development for the language pairs was min-
imal. In general, machine parses were always avail-
able for the English half of the pair. Machine parses
were also available for French and Spanish. Ma-
chine part of speech tags were available for all lan-
guage (although character-based heuristic was sub-
stituted for Japanese.) Large amounts (up to 10 mil-
lion sentence pairs) of unaligned parallel text was
available for model 1 type features. Our model ob-
tained improved alignment F -measure in all lan-
guage pairs, although the improvements were small
for ES?EN and FR?EN, the language pairs for
which the baseline accuracy was the highest.
6.4 Analysis
Some of the improvement can be attributed to ?look-
ahead? during the decoding. For example, the
English word ?the?, which (during Arabic-English
alignment) should often be aligned to the same Ara-
bic words to which its headword is linked. The num-
ber of errors associated with ?the? dropped from 383
(186 false alarms, 197 misses) in the ME-seq model
to 137 (60 false alarms and 77 misses) in the current
model.
In table 5, we show contributions to performance
resulting from various classes of features. The
895
Zh-En Ar-En
method correct miss fa correct miss fa
hmm 147 256 300
GIZA++ 139 677 396 132 271 370
ME-seq 71 745 133 127 276 191
corr(ME-seq) 358 458 231 264 139 114
Table 4: Analysis of 2?1 alignments errors (misses and false alarms) for Zh-En and Ar-En aligners
largest contribution is noted by removing features
based on the Model 1 translation matrices. These
features contain a wealth of lexical information
learned from approximately 7 ? 106 parallel sen-
tences - information that cannot be learned from
a relatively small amount of word-aligned train-
ing data. Geometrical features contribute more
than parse-based features, but the contribution from
parse-based features is important, and these are
more difficult to incorporate into sequential mod-
els. We note that all of the comparison aligners had
equivalent lexical information.
We show a small improvement from the unsuper-
vised adaptation - learning phrases from the parallel
corpus that are not captured by the lexical features
based on model 1. The final row in the table shows
the result of running the correction model on its own
output. The improvement is not statistically signif-
icant, but it is important to note the performance is
stable - a further indication that the model is robust
to a wide variety of input alignments, and that our
decoding scheme is a reasonable approach to find-
ing the best alignment.
In table 4, we characterize the errors based on the
fertility of the source and target words. We focus
on the case that exactly one target word is linked to
exactly two source words. These are the links that
feature R(%) P(%) F (%) Nexact
base 89 94 91 136
base-M1 82 88 85 89
base-geometric 83 90 86 92
base-parse 87 93 90 116
base+un.adapt 89 94 92 141
+iter2 90 94 92 141
Table 5: Importance of feature classes - ablation experi-
ments
corpus-level p90
alignment TER BLEU TER BLEU
ME-seq 56.06 32.65 64.20 21.31
corr(Me-seq) 56.25 33.10 63.47 22.02
both 56.07 33.13 63.41 22.14
Table 6: Translation results, Zh to En. BLEU=BLEUr4n4
alignment TER BLEUr1n4
ME-seq 35.02 69.94
corr(Me-seq ) 33.10 71.20
Table 7: Translation results, En to It
are poorly suited for the HMM and ME-seq mod-
els used in this comparison because of the chosen
directionality: the source (Arabic, Chinese) words
are the states and the target (English) words are the
observation. The HMM is able to produce these
links only by the use of posterior probabilities, rather
than viterbi decoding. The ME-seq model only pro-
duces these links because of language-specific post-
processing. GIZA++ has an underlying sequential
model, but uses both directionalities. The correc-
tion model improved performance across all three of
these links structures. The single exception is that
the number of 2?1 false alarms increased (Zh-En
alignments) but in this case, the first pass ME-seq
alignment produced few false alarms because it sim-
ply proposed few links of this form. It is also notable
that 1?2 links are more numerous than 2?1 links,
in both language pairs. This is consequence of the
choice of directionality and tokenization style.
6.5 Translation Impact
We tested the impact of improved alignments on
the performance of a phrase-based translation sys-
tem (Ittycheriah and Roukos, 2007) for three lan-
896
guage pairs. Our alignment did not improve the
performance of a mature Arabic to English trans-
lation system, but two notable successes were ob-
tained: Chinese to English, and English to Italian.
It is well known that improved alignment perfor-
mance does not always improve translation perfor-
mance (Fraser and Marcu, 2007). A mature machine
translation system may incorporate alignments ob-
tained from multiple aligners, or from both direc-
tions of an asymmetric aligner. Furthermore, with
large amounts of training data (the Gale Phase 4
Arabic English corpus consisting of 8 ? 106 sen-
tences,) a machine translation system is subject to
a saturation effect: correcting an alignment may
not yield a significant improvement because the the
phrases learned from the correct alignment have al-
ready been acquired in other contexts.
For the Chinese to English translation system (ta-
ble 6) the training corpus consisted of 11? 106 sen-
tence pairs, subsampled to 106. The test set was
NIST MT08 Newswire, consisting of 691 sentences
and 4 reference translations. Corpus-level perfor-
mance (columns 2 and 3) improved when measured
by BLEU, but not by TER. Performance on the
most difficult sentences (near the 90th percentile,
columns 4 and 5) improved on both BLEU and TER
(Snover et al, 2006), and the improvement in BLEU
was larger for the more difficult sentences than it
was overall. Translation performance further im-
proved, by a smaller amount, using bothME-seq and
corr(ME-seq) alignments during the training.
The improved alignments impacted the transla-
tion performance of the English to Italian transla-
tion system (table 7) even more strongly. Here the
training corpus consisted of 9.4?106 sentence pairs,
subsampled to 387000 pairs. The test set consisted
of 7899 sentences. Overall performance improved
as measured by both TER and BLEU (1.26 points.)
7 Conclusions
A log linear model for the alignment matrix is used
to guide systematic improvements to an existing
aligner. Our system models arbitrary alignment ma-
trices and allows features that incorporate such in-
formation as correlations based on parse trees in
both languages. We train models to correct the er-
rors of several existing aligners; we find the resulting
models are robust to using different aligners as start-
ing points. Improvements in alignment F -measure,
often significant improvements, show that our model
successfully corrects input alignments from existing
models in all nine language pairs tested. The result-
ing Chinese-English and English-Italian word align-
ments also improved translation performance, espe-
cially on the English-Italian test, and notably on the
particularly difficult subset of the Chinese sentences.
Future work will assess its impact on translation for
the other language pairs, as well as its impact on
other tasks, such as named entity projection.
8 Acknowledgements
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.
References
Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.
2005. Alignment link projection using transformation-
based learning. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 185?
192, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yassine Benajiba and Imed Zitouni. 2010. Enhanc-
ing mention detection using projection via aligned
corpora. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 993?1001. Association for Com-
putational Linguistics.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In In
Proc. of ACL-2006, pages 65?72.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the ACL-IJCNLP 2009 Conference
897
Short Papers, ACLShort ?09, pages 229?232, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
B. Efron and R. Tibshirani. 1986. Bootstrap meth-
ods for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1(1):pp. 54?75.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, StatMT ?08, pages 44?52. Association for Com-
putational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33(3):293?303.
Niyu Ge. 2004. Improvement in word alignments. In
DARPA/TIDES MT workshop.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT-EMNLP, pages 89?96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Human Language Technolo-
gies 2007: The Conference of the NA-ACL, pages 57?
64, Rochester, New York, April. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004: Short Papers on XX, HLT-NAACL ?04,
pages 57?60. Association for Computational Linguis-
tics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 104?111. Associa-
tion for Computational Linguistics.
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimization.
Mathematical Programming, 45:503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In ACL ?05: Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 459?466. Association for
Computational Linguistics.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006.
Improved discriminative bilingual word alignment. In
ACL-44: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 513?520. Association for Computa-
tional Linguistics.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In In Proceedings of HLT-
EMNLP, pages 81?88.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18?25, Columbus, Ohio,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Mach.
Learn., 34:151?175, February.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 534?544. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of Association for Machine Translation in
the Americas.
Ben Taskar, Simon Lacoste-julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In In Proceedings of HLT-EMNLP, pages 73?
80.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836?841.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of the first international conference on Human lan-
guage technology research, HLT ?01, pages 1?8. As-
sociation for Computational Linguistics.
898
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), page 1,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
IBM Cognitive Computing - An NLP Renaissance!
Salim Roukos
Senior Manager of Multi-Lingual NLP and CTO for Translation Technologies
IBM T. J. Watson Research Center
roukos@us.ibm.com
Abstract
Electronically available multi-modal data
(primarily text and meta-data) is unprece-
dented in terms of its volume, variety, ve-
locity, (and veracity). The increased in-
terest and investment in cognitive comput-
ing for building systems and solutions that
enable and support richer human-machine
interactions presents a unique opportunity
for novel statistical models for natural lan-
guage processing.
In this talk, I will describe a journey at
IBM during the past three decades in de-
veloping novel statistical models for NLP
covering statistical parsing, machine trans-
lation, and question-answering systems.
Along with a discussion of some of the re-
cent successes, I will discuss some diffi-
cult challenges that need to be addressed to
achieve more effective cognitive systems
and applications.
About the Speaker
Salim Roukos is Senior Manager of Multi-Lingual
NLP and CTO for Translation Technologies at
IBM T. J. Watson Research Center. Dr. Roukos
received his B.E. from the American University
of Beirut, in 1976, his M.Sc. and Ph.D. from
the University of Florida, in 1978 and 1980, re-
spectively. He joined Bolt Beranek and Newman
from 1980 through 1989, where he was a Senior
Scientist in charge of projects in speech compres-
sion, time scale modification, speaker identifica-
tion, word spotting, and spoken language under-
standing. He was an Adjunct Professor at Boston
University in 1988 before joining IBM in 1989.
Dr. Roukos has served as Chair of the IEEE Digi-
tal Signal Processing Committee in 1988.
Salim Roukos currently leads a group at IBM
T.J. Watson research Center that focuses on vari-
ous problems using machine learning techniques
for natural language processing. The group pi-
oneered many of the statistical methods for NLP
from statistical parsing, to natural language under-
standing, to statistical machine translation and ma-
chine translation evaluation metrics (BLEU met-
ric). Roukos has over a 150 publications in the
speech and language areas and over two dozen
patents. Roukos was the lead of the group which
introduced the first commercial statistical lan-
guage understanding system for conversational
telephony systems (IBM ViaVoice Telephony) in
2000 and the first statistical machine translation
product for Arabic-English translation in 2003.
He has recently lead the effort to create IBM?s
offering of IBM Real-Time Translation Services
(RTTS) a platform for enabling real-time transla-
tion applications such as multilingual chat and on-
demand document translation.
1
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861?870,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Adaptive HTER Estimation for Document-Specific MT Post-Editing
Fei Huang
?
Facebook Inc.
Menlo Park, CA
feihuang@fb.com
Jian-Ming Xu Abraham Ittycheriah
IBM T.J. Watson Research Center
Yorktown Heights, NY
{jianxu, abei, roukos}@us.ibm.com
Salim Roukos
Abstract
We present an adaptive translation qual-
ity estimation (QE) method to predict
the human-targeted translation error rate
(HTER) for a document-specific machine
translation model. We first introduce fea-
tures derived internal to the translation de-
coding process as well as externally from
the source sentence analysis. We show
the effectiveness of such features in both
classification and regression of MT qual-
ity. By dynamically training the QE model
for the document-specific MT model, we
are able to achieve consistency and pre-
diction quality across multiple documents,
demonstrated by the higher correlation co-
efficient and F-scores in finding Good sen-
tences. Additionally, the proposed method
is applied to IBM English-to-Japanese MT
post editing field study and we observe
strong correlation with human preference,
with a 10% increase in human translators?
productivity.
1 Introduction
Machine translation (MT) systems suffer from an
inconsistent and unstable translation quality. De-
pending on the difficulty of the input sentences
(sentence length, OOV words, complex sentence
structures and the coverage of the MT system?s
training data), some translation outputs can be per-
fect, while others are ungrammatical, missing im-
portant words or even totally garbled. As a result,
users do not know whether they can trust the trans-
lation output unless they spend time to analyze
?
This work was done when the author was with IBM Re-
search.
the MT output. This shortcoming is one of the
main obstacles for the adoption of MT systems,
especially in machine assisted human translation:
MT post-editing, where human translators have
an option to edit MT proposals or translate from
scratch. It has been observed that human trans-
lators often discard MT proposals even if some
are very accurate. If MT proposals are used prop-
erly, post-editing can increase translators produc-
tivity and lead to significant cost savings. There-
fore, it is beneficial to provide MT confidence es-
timation, to help the translators to decide whether
to accept MT proposals, making minor modifica-
tions on MT proposals when the quality is high
or translating from scratching when the quality is
low. This will save the time of reading and parsing
low quality MT and improve user experience.
In this paper we propose an adaptive qual-
ity estimation that predicts sentence-level human-
targeted translation error rate (HTER) (Snover et
al., 2006) for a document-specific MT post-editing
system. HTER is an ideal quality measurement
for MT post editing since the reference is ob-
tained from human correction of the MT output.
Document-specific MT model is an MT model that
is specifically built for the given input document.
It is demonstrated in (Roukos et al, 2012) that
document-specific MT models significantly im-
prove the translation quality. However, this raises
two issues for quality estimation. First, existing
approaches to MT quality estimation rely on lex-
ical and syntactical features defined over parallel
sentence pairs, which includes source sentences,
MT outputs and references, and translation models
(Blatz et al, 2004; Ueffing and Ney, 2007; Spe-
cia et al, 2009a; Xiong et al, 2010; Soricut and
Echihabi, 2010a; Bach et al, 2011). Therefore,
when the MT quality estimation model is trained,
861
it can not be adapted to provide accurate estimates
on the outputs of document-specific MT models.
Second, the MT quality estimation might be in-
consistent across different document-specific MT
models, thus the confidence score is unreliable and
not very helpful to users.
In contrast to traditional static MT quality es-
timation methods, our approach not only trains
the MT quality estimator dynamically for each
document-specific MT model to obtain higher pre-
diction accuracy, but also achieves consistency
over different document-specific MT models. The
experiments show that our MT quality estima-
tion is highly correlated with human judgment
and helps translators to increase the MT proposal
adoption rate in post-editing.
We will review related work on MT quality es-
timation in section 2. In section 3 we will intro-
duce the document-specific MT system built for
post-editing. We describe the static quality estima-
tion method in section 4, and propose the adaptive
quality estimation method in section 5. In section
6 we demonstrate the improvement of MT quality
estimation with our method, followed by discus-
sion and conclusion in section 7.
2 Related Work
There has been a long history of study in con-
fidence estimation of machine translation. The
work of (Blatz et al, 2004) is among the best
known study of sentence and word level features
for translation error prediction. Along this line of
research, improvements can be obtained by incor-
porating more features as shown in (Quirk, 2004;
Sanchis et al, 2007; Raybaud et al, 2009; Specia
et al, 2009b). Soricut and Echihabi (2010b) pro-
posed various regression models to predict the ex-
pected BLEU score of a given sentence translation
hypothesis. Ueffing and Hey (2007) introduced
word posterior probabilities (WPP) features and
applied them in the n-best list reranking. Target
part-of-speech and null dependency link are ex-
ploited in a MaxEnt classifier to improve the MT
quality estimation (Xiong et al, 2010).
Quality estimation focusing on MT post-editing
has been an active research topic, especially after
the WMT 2012 (Callison-Burch et al, 2012) and
WMT2013 (Bojar et al, 2013) workshops with
the ?Quality Estimation? shared task. Bic?ici et
al. (2013) proposes a number of features mea-
suring the similarity of the source sentence to the
source side of the MT training corpus, which,
combined with features from translation output,
achieved significantly superior performance in the
MT QE evaluation. Felice and Specia (2012) in-
vestigates the impact of a large set of linguisti-
cally inspired features on quality estimation accu-
racy, which are not able to outperform the shal-
lower features based on word statistics. Gonz?alez-
Rubio et al (2013) proposed a principled method
for performing regression for quality estimation
using dimensionality reduction techniques based
on partial least squares regression. Given the fea-
ture redundancy in MT QE, their approach is able
to improve prediction accuracy while significantly
reducing the size of the feature sets.
3 Document-specific MT System
In our MT post-editing setup, we are given docu-
ments in the domain of software manuals, techni-
cal outlook or customer support materials. Each
translation request comes as a document with sev-
eral thousand sentences, focusing on a specific
topic, such as the user manual of some software.
The input documents are automatically seg-
mented into sentences, which are also called seg-
ments. Thus in the rest of the paper we will use
sentences and segments interchangeably. Our par-
allel corpora includes tens of millions of sentence
pairs covering a wide range of topics. Building
a general MT system using all the parallel data
not only produces a huge translation model (unless
with very aggressive pruning), the performance on
the given input document is suboptimal due to the
unwanted dominance of out-of-domain data. Past
research suggests using weighted sentences or cor-
pora for domain adaptation (Lu et al, 2007; Mat-
soukas et al, 2009; Foster et al, 2010). Here
we adopt the same strategy, building a document-
specific translation model for each input docu-
ment.
The document-specific system is built based on
sub-sampling: from the parallel corpora we se-
lect sentence pairs that are the most similar to
the sentences from the input document, then build
the MT system with the sub-sampled sentence
pairs. The similarity is defined as the number of
n-grams that appear in both source sentences, di-
vided by the input sentence?s length, with higher
weights assigned to longer n-grams. From the
extracted sentence pairs, we utilize the standard
pipeline in SMT system building: word align-
862
Figure 1: Adaptive QE for document-specific MT system.
ment (HMM (Vogel et al, 1996) and MaxEnt (It-
tycheriah and Roukos, 2005) alignment models,
phrase pair extraction, MT model training (Itty-
cheriah and Roukos, 2007) and LM model train-
ing. The top region within the dashed line in Fig-
ure 1 shows the overall system built pipeline.
3.1 MT Decoder
The MT decoder (Ittycheriah and Roukos, 2007)
employed in our study extracts various features
(source words, morphemes and POS tags, target
words and POS tags, etc.) with their weights
trained in a maximum entropy framework. These
features are combined with other features used in
a typical phrase-based translation system. Alto-
gether the decoder incorporates 17 features with
weights estimated by PRO (Hopkins and May,
2011) in the decoding process, and achieves
state-of-the-art translation performance in vari-
ous Arabic-English translation evaluations (NIST
MT2008, GALE and BOLT projects).
4 Static MT Quality Estimation
MT quality estimation is typically formulated as
a prediction problem: estimating the confidence
score or translation error rate of the translated sen-
tences or documents based on a set of features. In
this work, we adopt HTER in (Snover et al, 2006)
as our prediction output. HTER measures the per-
centage of insertions, deletions, substitutions and
shifts needed to correct the MT outputs. In the
rest of the paper, we use TER and HTER inter-
changably.
In this section we will first introduce the set of
features, and then discuss MT QE problem from
classification and regression point of views.
4.1 Features for MT QE
The features for quality estimation should reflect
the complexity of the source sentence and the de-
coding process. Therefore we conduct syntactic
analysis on the source sentences, extract features
from the decoding process and select the follow-
ing 26 features:
? 17 decoding features, including phrase
translation probabilities (source-to-target and
target-to-source), word translation probabil-
ities (also in both directions), maxent prob-
abilities
1
, word count, phrase count, distor-
1
The maxent probability is the translation probability
863
tion probabilities, as well as a set of language
model scores.
? Sentence length, i.e., the number of words in
the source sentence.
? Source sentence syntactic features, including
the number of noun phrases, verb phrases,
adjective phrases, adverb phrases, as in-
spired by (Green et al, 2013).
? The length of verb phrases, because verbs are
typically the roots in dependency structure
and they have more varieties during transla-
tion.
? The maximum length of source phrases in
the final translation, since longer matching
source phrase indicates better coverage of the
input sentence with possibly better transla-
tions.
? The number of phrase pairs with high fuzzy
match (FM) score. The high FM phrases are
selected from sentence pairs which are clos-
est in terms of n-gram overlap to the input
sentence. These sentences are often found in
previous translations of the software manual,
and thus are very helpful for translating the
current sentence.
? The average translation probability of the
phrase translation pairs in the final transla-
tion, which provides the overall translation
quality on the phrase level.
The first 17 features come from the decod-
ing process, which are called ?decoding features?.
The remaining 9 features not related to the de-
coder are called ?external features?. To evaluate
the effectiveness of the proposed features, we train
various classifiers with different feature configura-
tions to predict whether a translation output is use-
ful (with lower TER) as described in the following
section.
4.2 MT QE as Classification
Predicting TER with various input features can
be treated as a regression problem. However for
the post-editing task, we argue that it could also
be cast as a classification problem: MT system
derived from a Maximum Entropy translation model (Itty-
cheriah and Roukos, 2005).
Configuration Training set Test set
Baseline (All negative) 80% 77%
17 decoding features only 89% 79%
9 external features only 85% 81%
total 26 features 92% 83%
Table 1: QE classification accuracy with different
feature configurations
users (including the translators) are often inter-
ested to know whether a given translation is rea-
sonably good or not. If useful, they can quickly
look through the translation and make minor mod-
ifications. On the other hand, they will just skip
reading and parsing the bad translation, and prefer
to translate by themselves from scratch. Therefore
we also develop algorithms that classify the trans-
lation at different levels, depending on whether the
TER is less than a given threshold. In our experi-
ments, we set TER=0.1 as the threshold.
We randomly select one input document with
2067 sentences for the experiment. We build
a document-specific MT system to translate this
document, then ask human translator to correct
the translation output. We compute TER for each
sentence using the human correction as the refer-
ence. The TER of the whole document is 0.31,
which means about 30% errors should be cor-
rected. In the classification task, our goal is to pre-
dict whether a sentence is a Good translation (with
TER ? 0.1), and label them for human correction.
We adopt a decision tree-based classifier, experi-
menting with different feature configurations. We
select the top 1867 sentences for training and the
bottom 200 sentences for test. In the test set, there
are 46 sentences with TER ? 0.1. Table 1 shows
the classification accuracy.
First we can see that as the overall TER is
around 0.3, predicting all the sentences being neg-
ative already has a strong baseline: 77%. How-
ever this is not helpful for the human translators,
because that means they have to translate every
sentence from scratch, and consequently there is
no productivity gain from MT post-editing. If we
only use the 17 decoding features, it improves the
classification accuracy by 9% on the training set,
but only 2% on the test set. This is probably due to
the overfitting when training the decision tree clas-
sifier. While using the 7 external features, the gain
on training set is less but the gain on the test set
864
is greater (4% improvement), because the trans-
lation output is generated based on the log-linear
combination of these decoding features, which are
biased towards the final translations. The exter-
nal features capture the syntactic structure of the
source sentence, as well as the coverage of the
training data with regard to the input sentence,
which are good indicators of the translation qual-
ity. Combining both the decoding features and the
external features, we observed the best accuracy
on both the training and test set. We will use the
combined 26 features in the following work.
4.3 MT QE as Regression
For the QE regression task, we predict the TER for
each sentence translation using the above 26 fea-
tures. We experiment with several classifiers: lin-
ear regression model, decision tree based regres-
sion model and SVM model. With the same train-
ing and test data set up, we predict the TER for
each sentence in the test set, and compute the cor-
relation coefficient (r) and root mean square error
(RMSE). Our experiments show that the decision
tree-based regression model obtains the highest
correlation coefficients (0.53) and lowest RMSE
(0.23) in both the training and test sets. We will
use this model for the adaptive MT QE in the fol-
lowing work.
5 Adaptive MT Quality Estimation
The above QE regression model is trained on a
portion of the sentences from the input document,
and evaluated on the remaining sentences from the
same document. One would like to know whether
the trained model can achieve consistent TER pre-
diction accuracy on other documents. When we
use the cross-document models for prediction, the
correlation is significantly worse (the details are
discussed in section 6.1). Therefore it is neces-
sary to build a QE regression model that?s robust
to different document-specific translation models.
To deal with this problem, we propose this adap-
tive MT QE method described below.
Our proposed method is as follows: we select a
fixed set of sentence pairs (S
q
, R
q
) to train the QE
model. The source side of the QE training data
S
q
is combined with the input document S
d
for
MT system training data subsampling. Once the
document-specific MT system is trained, we use it
to translate both the input document and the source
QE training data, obtaining the translation T
d
and
Figure 2: Correlation coefficient r between pre-
dicted TER (x-axis) and true TER (y-axis) for QE
models trained from the same document (top fig-
ure) or different document (bottom figure).
T
q
. We compute the TER of T
q
using R
q
as the
reference, and train a QE regression model with
the 26 features proposed in section 4.1. Then we
use this document-specific QE model to predict the
TER of the document translation T
d
. As the QE
model is adaptively re-trained for each document-
specific MT system, its prediction is more accurate
and consistent. Figure 1 shows the flow of our MT
system with the adaptive QE training integrated as
part of the built.
6 Experiments
In this section, we first discuss experiments that
compare adaptive QE method and static QE
method on a few documents, and then present
results we obtained after deploying the adaptive
QE method in an English-to-Japanese MT Post-
Editing project. As mentioned before, the main
motivation for us to develop MT QE classification
scheme is that translators often discard good MT
proposals and translate the segments from scratch.
We would like to provide translators with some
guidance on reasonably good MT proposals?the
sentences with low TERs?to help them increase
the leverage on MT proposals to achieve improved
productivity.
865
6.1 Evaluation on Test Set
Our experiment and evaluation is conducted over
three documents, each with about 2000 segments.
We first build document-specific MT model for
each document, then ask human translators to cor-
rect the MT outputs and obtain the reference trans-
lation. In a typical MT QE scenario, the QE model
is pre-trained and applied to various MT outputs,
even though the QE training data and MT out-
puts are generated from different translation mod-
els. To evaluate whether such model mismatch
matters, we compare the cross-model QE with the
same-model QE, where the QE training data and
the MT outputs are generated from the same MT
model.
We select one document LZA with 2067 sen-
tences. We use the first 1867 sentences to train the
static QE model and the remaining 200 sentences
are used as test set for TER prediction. We com-
pute the correlation coefficient (r) between each
predicted TER and true TER, as shown in Figure
2. We find that the TER predictions are reason-
ably correct when the training and test sentences
are from the same MT model (the top figure), with
correlation coefficients around 0.5. For the cross-
model QE, we train a static QE model with 1867
sentences from another document RTW, and use it
to predict the TER of the same 200 sentences from
document LZA (the bottom figure). We observe
significant degradation of correlation coefficient,
dropping from 0.5 to 0.1. This degradation and
unstable nature is the prime motivation to develop
a more robust MT quality estimation model.
We select 1700 sentences from multiple pre-
viously translated documents as the QE training
data, which are independent of the test documents.
We train the static QE model with this training set,
including the source sentences, references and MT
outputs (from multiple translation models). To
train the adaptive QE model for each test docu-
ment, we build a translation model whose subsam-
pling data includes source sentences from both the
test document and the QE training data. We trans-
late the QE source sentences with this newly built
MT model, and the translation output is used to
train the QE model specific to each test document.
We compare these two QE models on three doc-
uments, LZA, RTW and WC7, measuring r and
RMSE for each QE model. The result is shown
in Table 2. We find that the adaptive QE model
demonstrates higher r and lower RMSE than the
static QE model for all the test documents.
Besides the general correlation with human
judgment, we particularly focus on those reason-
ably good translations, i.e., the sentences with low
TERs which can help improve the translator?s pro-
ductivity most. Here we report the precision, re-
call and F-score of finding such ?Good? sentences
(with TER ? 0.1) on the three documents in Ta-
ble 3. Again, the adaptive QE model produces
higher recall, mostly higher precision, and signif-
icantly improved F-score. The overall F-score of
the adaptive QE model is 0.28
2
. Compared with
the static QE model?s 0.17 F-score, this is rela-
tively 64% improvement.
In the adaptive QE model, the source side QE
training data is included in the subsampling pro-
cess to build the document-specific MT model. It
would be interesting to know whether this process
will negatively affect the MT quality. We evaluate
the TER of MT outputs with and without the adap-
tive QE training on the same three documents. As
seen in Table 4, we do not notice translation qual-
ity degradation. Instead, we observe slightly im-
provement on two document, with TERs reduction
by 0.1-0.4 pt. As our MT model training data in-
clude proprietary data, the MT performance is sig-
nificantly better than publicly available MT soft-
ware.
6.2 Impact on Human Translators
We apply the proposed adaptive QE model to
large scale English-to-Japanese MT Post-Editing
project on 36 documents with 562K words. Each
English sentence can be categorized into 3 classes:
? Exact Match (EM): the source sentence is
completely covered in the bilingual training
corpora thus the corresponding target sen-
tence is returned as the translation;
? Fuzzy Match (FM): the source sentence is
similar to some sentence in the training data
(similarity measured by string editing dis-
tance), the corresponding fuzzy match target
sentence (FM proposal) as well as the MT
translation output (MT proposal) are returned
for human translators to select and correct;
? No Proposal (NP): there is no close match
source sentences in the training data (the FM
2
The adaptive QE model obtains much higher F-score
(80%) on the rest of the sentences (with TER > 0.1).
866
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
r ? RMSE ? r ? RMSE ? r ? RMSE ?
Static QE 0.10 0.38 0.40 0.32 0.13 0.36
Adaptive QE 0.58 0.23 0.61 0.22 0.47 0.20
Table 2: QE regression with static and adaptive models
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
P/R/F-score P/R/F-score P/R/F-score
Static QE 0.73/0.08/0.14 0.69/ 0.11/ 0.19 0.74/ 0.10/ 0.18
Adaptive QE 0.69/0.14/0.24 0.84/ 0.16/ 0.26 0.80/ 0.23/ 0.35
Table 3: Performance on predicting Good sentences with static and adaptive models
similarity score of 70% is used as the thresh-
old), therefore only the MT output is re-
turned.
EM sentences are excluded from the study be-
cause in general they do not require editing. We
focus on the FM and NP sentences
3
. In Table 5
we present the precision, recall and F-score of the
?Good? sentences in the FM and NP categories,
similar to those shown in Table 3. We consistently
observe higher performance on the FM sentences,
in terms of precision, recall and F-score. This is
expected because these sentences are well covered
in the training data. The overall F-score is in line
with the test set results shown in Table 3.
We are also interested to know whether the pro-
posed adaptive QE method is helpful to human
translators in the MT post-editing task. Based on
the TERs predicted by the adaptive QE model, we
assign each MT proposal with a confidence label:
High (0 ? TER ? 0.2), Medium (0.2 < TER ?
0.3), or Low (TER > 0.3). We present the MT pro-
posals with confidence labels to human translators,
then measure the percentage of sentences whose
MT proposals are used. From Table 6 and 7,
we can see that sentences with High and Medium
confidence labels are more frequently used by the
translators than those with Low labels, for both the
FM and NP categories. The MT usage for the FM
category is less than that for the NP category be-
cause translators can choose FM proposals instead
of the MT proposals for correction.
We also measure the translator?s productivity
gain for MT proposals with different confidence
3
The word count distribution of EM, FM and NP is 21%,
38% and 41%, respectively.
Document LZA RTW WC7
TER-Baseline 30.81 30.74 29.96
TER-with Adaptive QE 30.69 30.78 29.56
Table 4: MT Quality with and without Adaptive
QE measured by TER
labels. The productivity of a translator is defined
as the number of source words translated per unit
time. The post editing tool, IBM TranslationMan-
ager, records the time that a translator spends on
a segment and computes the number of characters
that a translator types on the segment so that we
can compute how many words the translator has
finished in a given time.
We choose the overall productivity of NP0 as
the base unit 1, where there is no proposal presents
and the translator has to translate the segments
from scratch. Measured with this unit, for exam-
ple, the overall productivity of FM0 being 1.14
implies a relative gain of 14% over that of NP0,
which demonstrates the effectiveness of FM pro-
posals.
Table 6 and 7 also show the productivity gain
on sentences with High, Medium and Low labels
from FM and NP categories. Again, the produc-
tivity gain is consistent with the confidence labels
from the adaptive QE model?s prediction. The
overall productivity gain with confidence-labeled
MT proposals is about 10% (comparing FM1 vs.
FM0 and NP1 vs. NP0). These results clearly
demonstrate the effectiveness of the adaptive QE
model in aiding the translators to make use of MT
proposals and improve productivity.
867
Category Class FM usage MT usage Productivity
High 33% 34% 1.35
FM1 Medium 47% 18% 1.21
Low 60% 8% 1.20
Overall 45% 21% 1.26
High 53% - 1.12
FM0 Medium 64% - 1.14
Low 67% - 1.16
Overall 59% - 1.14
Table 6: MT proposal usage and productivity gain in FM category.
In FM1, both Fuzzy Match and MT proposals present. In control class FM0, only Fuzzy Match proposals
present, and therefore, MT usage is not available for FM0. Strong correlation is observed between
predicted ?High? , ?Medium? and ?Low? sentences with MT usage and post editing productivity.
Category Class MT usage Productivity
High 50% 1.25
NP1 Medium 42% 1.08
Low 27% 1.00
Overall 38% 1.09
High - 1.08
NP0 Medium - 1.00
Low - 0.96
Overall - 1.00
Table 7: MT proposal usage and productivity gain in NP category.
In NP1, MT is the only proposal available, while in control NP0, there presents no proposal at all and
the translator has to translate from scratch. Strong correlation is observed between predicted ?High? ,
?Medium? and ?Low? sentences with MT usage and post editing productivity
868
Type Precision Recall F-score
FM 0.71 0.23 0.35
NP 0.67 0.18 0.29
Overall 0.69 0.21 0.32
Table 5: Performance on predicting Good sen-
tences (TER ? 0.1) by adaptive QE model
7 Discussion and Conclusion
In this paper we proposed a method to adaptively
train a quality estimation model for document-
specific MT post editing. With the 26 pro-
posed features derived from decoding process and
source sentence syntactic analysis, the proposed
QE model achieved better TER prediction, higher
correlation with human correction of MT output
and higher F-score in finding good translations.
The proposed adaptive QE model is deployed to
a large scale English-to-Japanese MT post edit-
ing project, showing strong correlation with hu-
man preference and leading to about 10% gain in
human translator productivity.
The training data for QE model can be selected
independent of the input document. With such
fixed QE training data, it is possible to measure the
consistency of the trained QE models, and to al-
low the sanity check of the document-specific MT
models. However, adding such data in the sub-
sampling process extracts more bilingual data for
building the MT models, which slightly increase
the model building time but increased the transla-
tion quality. Another option is to select the sen-
tence pairs from the MT system subsampled train-
ing data, which is more similar to the input docu-
ment thus the trained QE model could be a better
match to the input document. However, the QE
model training data is no longer constant. The
model consistency is no longer guaranteed, and
the QE training data must be removed from the
MT system training data to avoid data contamina-
tion.
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A method for measuring machine
translation confidence. In ACL, pages 211?219.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the 20th international conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut,
and Lucia Specia. 2013. Findings of the 2013
Workshop on Statistical Machine Translation. In
Eighth Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada.
Mariano Felice and Lucia Specia. 2012. Linguistic
features for quality estimation. In Seventh Workshop
on Statistical Machine Translation, pages 96?103,
Montr?eal, Canada.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 451?459, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jes?us Gonz?alez-Rubio, Jose Ram?on Navarro-Cerd?an,
and Francisco Casacuberta. 2013. Dimensionality
reduction methods for machine translation quality
estimation. Machine Translation, 27(3-4):281?301.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ?13, pages 439?448, New York, NY,
USA. ACM.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In In Proceedings of HLT-
EMNLP, pages 89?96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In In HLT-NAACL 2007: Main
Conference, pages 57?64.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by
869
training data selection and optimization. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 343?350, Prague, Czech Republic,
June. Association for Computational Linguistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 708?717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence measure. In In Pro-
ceedings of LREC.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Sma??li. 2009. New confidence mea-
sures for statistical machine translation. CoRR,
abs/0902.1033.
Salim Roukos, Abraham Ittycheriah, and Jian-Ming
Xu. 2012. Document-specific statistical machine
translation for improving human translation produc-
tivity. In Proceedings of the 13th international con-
ference on Computational Linguistics and Intelli-
gent Text Processing - Volume Part II, CICLing?12,
pages 25?39, Berlin, Heidelberg. Springer-Verlag.
Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-
partament De Sistemes Informtics. 2007. Estima-
tion of confidence measures for machine translation.
In In Procedings of Machine Translation Summit XI.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Radu Soricut and Abdessamad Echihabi. 2010a.
Trustrank: inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?10, pages 612?621, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Soricut and Abdessamad Echihabi. 2010b.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621. Association for Computa-
tional Linguistics.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-taylor. 2009a. Improving
the confidence of machine translation quality esti-
mates. In In Proceedings of MT Summit XII.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009b. Improv-
ing the confidence of machine translation quality es-
timates.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33(1):9?40.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2, COLING
?96, pages 836?841, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 604?611, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
870
