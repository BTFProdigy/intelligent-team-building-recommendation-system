A Relational Syntax-Semantics Interface Based on Dependency Grammar
Ralph Debusmann Denys Duchier? Alexander Koller Marco Kuhlmann Gert Smolka Stefan Thater
Saarland University, Saarbr?cken, Germany ?LORIA, Nancy, France
{rade|kuhlmann|smolka}@ps.uni-sb.de, duchier@loria.fr, {koller|stth}@coli.uni-sb.de
Abstract
We propose a syntax-semantics interface that
realises the mapping between syntax and se-
mantics as a relation and does not make func-
tionality assumptions in either direction. This
interface is stated in terms of Extensible De-
pendency Grammar (XDG), a grammar formal-
ism we newly specify. XDG?s constraint-based
parser supports the concurrent flow of informa-
tion between any two levels of linguistic rep-
resentation, even when only partial analyses are
available. This generalises the concept of under-
specification.
1 Introduction
A key assumption of traditional syntax-semantics
interfaces, starting with (Montague, 1974), is that
the mapping from syntax to semantics is functional,
i. e. that once we know the syntactic structure of a
sentence, we can deterministically compute its se-
mantics.
Unfortunately, this assumption is typically not
justified. Ambiguities such as of quantifier scope
or pronominal reference are genuine semantic am-
biguities; that is, even a syntactically unambigu-
ous sentence can have multiple semantic readings.
Conversely, a common situation in natural language
generation is that one semantic representation can
be verbalised in multiple ways. This means that the
relation between syntax and semantics is not func-
tional at all, but rather a true m-to-n relation.
There is a variety of approaches in the litera-
ture on syntax-semantics interfaces for coping with
this situation, but none of them is completely sat-
isfactory. One way is to recast semantic ambiguity
as syntactic ambiguity by compiling semantic dis-
tinctions into the syntax (Montague, 1974; Steed-
man, 1999; Moortgat, 2002). This restores function-
ality, but comes at the price of an artificial blow-
up of syntactic ambiguity. A second approach is to
assume a non-deterministic mapping from syntax
to semantics as in generative grammar (Chomsky,
1965), but it is not always obvious how to reverse
the relation, e. g. for generation. For LFG, the oper-
ation of functional uncertaintainty allows for a re-
stricted form of relationality (Kaplan and Maxwell
III, 1988). Finally, underspecification (Egg et al,
2001; Gupta and Lamping, 1998; Copestake et al,
2004) introduces a new level of representation,
which can be computed functionally from a syntac-
tic analysis and encapsulates semantic ambiguity in
a way that supports the enumeration of all semantic
readings by need.
In this paper, we introduce a completely rela-
tional syntax-semantics interface, building upon the
underspecification approach. We assume a set of
linguistic dimensions, such as (syntactic) immedi-
ate dominance and predicate-argument structure; a
grammatical analysis is a tuple with one component
for each dimension, and a grammar describes a set
of such tuples. While we make no a priori function-
ality assumptions about the relation of the linguistic
dimensions, functional mappings can be obtained
as a special case. We formalise our syntax-seman-
tics interface using Extensible Dependency Gram-
mar (XDG), a new grammar formalism which gen-
eralises earlier work on Topological Dependency
Grammar (Duchier and Debusmann, 2001).
The relational syntax-semantics interface is sup-
ported by a parser for XDG based on constraint pro-
gramming. The crucial feature of this parser is that
it supports the concurrent flow of possibly partial in-
formation between any two dimensions: once addi-
tional information becomes available on one dimen-
sion, it can be propagated to any other dimension.
Grammaticality conditions and preferences (e. g. se-
lectional restrictions) can be specified on their nat-
ural level of representation, and inferences on each
dimension can help reduce ambiguity on the oth-
ers. This generalises the idea of underspecifica-
tion, which aims to represent and reduce ambiguity
through inferences on a single dimension only.
The structure of this paper is as follows: in Sec-
tion 2, we give the general ideas behind XDG, its
formal definition, and an overview of the constraint-
based parser. In Section 3, we present the relational
syntax-semantics interface, and go through exam-
ples that illustrate its operation. Section 4 shows
how the semantics side of our syntax-semantics in-
terface can be precisely related to mainstream se-
mantics research. We summarise our results and
point to further work in Section 5.
2 Extensible Dependency Grammar
This section presents Extensible Dependency
Grammar (XDG), a description-based formalism
for dependency grammar. XDG generalizes previ-
ous work on Topological Dependency Grammar
(Duchier and Debusmann, 2001), which focussed
on word order phenomena in German.
2.1 XDG in a Nutshell
XDG is a description language over finite labelled
graphs. It is able to talk about two kinds of con-
straints on these structures: The lexicon of an XDG
grammar describes properties local to individual
nodes, such as valency. The grammar?s principles
express constraints global to the graph as a whole,
such as treeness. Well-formed analyses are graphs
that satisfy all constraints.
An XDG grammar allows the characterisation
of linguistic structure along several dimensions of
description. Each dimension contains a separate
graph, but all these graphs share the same set of
nodes. Lexicon entries synchronise dimensions by
specifying the properties of a node on all dimen-
sions at once. Principles can either apply to a single
dimension (one-dimensional), or constrain the rela-
tion of several dimensions (multi-dimensional).
Consider the example in Fig. 1, which shows an
analysis for a sentence of English along two dimen-
sions of description, immediate dominance (ID) and
linear precedence (LP). The principles of the under-
lying grammar require both dimensions to be trees,
and the LP tree to be a ?flattened? version of the ID
tree, in the sense that whenever a node v is a tran-
sitive successor of a node u in the LP tree, it must
also be a transitive successor of u in the ID tree. The
given lexicon specifies the potential incoming and
required outgoing edges for each word on both di-
mensions. The word does, for example, accepts no
incoming edges on either dimension and must there-
fore be at the root of both the ID and the LP tree. It is
required to have outgoing edges to a subject (subj)
and a verb base form (vbse) in the ID tree, needs
fillers for a subject (sf) and a verb complement field
(vcf) in the LP tree, and offers an optional field for
topicalised material (tf). All these constraints are
satisfied by the analysis, which is thus well-formed.
2.2 Formalisation
Formally, an XDG grammar is built up of dimen-
sions, principles, and a lexicon, and characterises a
set of well-formed analyses.
A dimension is a tuple D = (Lab,Fea,Val,Pri) of
a set Lab of edge labels, a set Fea of features, a set
Val of feature values, and a set of one-dimensional
s
u
b
j
v
b
s
e
o
b
j
what does John eat
s
f
v
c
f
what does John eat
t
f
word inID outID inLP outLP
what {obj?} {} {tf?} {}
does {} {subj,vbse} {} {tf?,sf,vcf}
John {subj?,obj?} {} {sf?,of?} {}
eat {vbse?} {obj} {vcf?} {}
Figure 1: XDG analysis of ?what does John eat?
principles Pri. A lexicon for the dimension D is a
set Lex ? Fea ? Val of total feature assignments (or
lexical entries). A D-structure, representing an anal-
ysis on dimension D, is a triple (V,E,F) of a set V
of nodes, a set E ?V ?V ?Lab of directed labelled
edges, and an assignment F : V ? (Fea ? Val) of
lexical entries to nodes. V and E form a graph. We
write StrD for the set of all possible D-structures.
The principles characterise subsets of StrD that have
further dimension-specific properties, such as being
a tree, satisfying assigned valencies, etc. We assume
that the elements of Pri are finite representations of
such subsets, but do not go into details here; some
examples are shown in Section 3.2.
An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,
Lex) consists of n dimensions, multi-dimensional
principles Pri, and a lexicon Lex. An XDG analysis
(V,Ei,Fi)ni=1 is an element of Ana = Str1??? ??Strn
where all dimensions share the same set of nodes V .
Multi-dimensional principles work just like one-
dimensional principles, except that they specify
subsets of Ana, i. e. couplings between dimensions
(e. g. the flattening principle between ID and LP in
Section 2.1). The lexicon Lex ? Lex1 ? ?? ? ? Lexn
constrains all dimensions at once. An XDG analysis
is licenced by Lex iff (F1(w), . . . ,Fn(w)) ? Lex for
every node w ?V .
In order to compute analyses for a given input, we
model it as a set of input constraints (Inp), which
again specify a subset of Ana. The parsing prob-
lem for XDG is then to find elements of Ana that
are licenced by Lex and consistent with Inp and
Pri. Note that the term ?parsing problem? is tradi-
tionally used only for inputs that are sequences of
words, but we can easily represent surface realisa-
tion as a ?parsing? problem in which Inp specifies a
semantic dimension; in this case, a ?parser? would
compute analyses that contain syntactic dimensions
from which we can read off a surface sentence.
2.3 Constraint Solver
The parsing problem of XDG has a natural read-
ing as a constraint satisfaction problem (CSP) (Apt,
2003) on finite sets of integers; well-formed anal-
yses correspond to the solutions of this problem.
The transformation, whose details we omit due to
lack of space, closely follows previous work on ax-
iomatising dependency parsing (Duchier, 2003) and
includes the use of the selection constraint to effi-
ciently handle lexical ambiguity.
We have implemented a constraint solver for
this CSP using the Mozart/Oz programming system
(Smolka, 1995; Mozart Consortium, 2004). This
solver does a search for a satisfying variable assign-
ment. After each case distinction (distribution), it
performs simple inferences that restrict the ranges
of the finite set variables and thus reduce the size
of the search tree (propagation). The successful
leaves of the search tree correspond to XDG anal-
yses, whereas the inner nodes correspond to partial
analyses. In these cases, the current constraints are
too weak to specify a complete analysis, but they
already express that some edges or feature values
must be present, and that others are excluded. Partial
analyses will play an important role in Section 3.3.
Because propagation operates on all dimensions
concurrently, the constraint solver can frequently
infer information about one dimension from infor-
mation on another, if there is a multi-dimensional
principle linking the two dimensions. These infer-
ences take place while the constraint problem is be-
ing solved, and they can often be drawn before the
solver commits to any single solution.
Because XDG allows us to write grammars with
completely free word order, XDG solving is an NP-
complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the
solver is exponential, but the average-case complex-
ity for the hand-crafted grammars we experimented
with is often better than this result suggests. We
hope there are useful fragments of XDG that would
guarantee polynomial worst-case complexity.
3 A Relational Syntax-Semantics Interface
Now that we have the formal and processing frame-
works in place, we can define a relational syntax-
semantics interface for XDG. We will first show
how we encode semantics within the XDG frame-
work. Then we will present an example grammar
(including some principle definitions), and finally
go through an example that shows how the rela-
tionality of the interface, combined with the con-
currency of the constraint solver, supports the flow
of information between different dimensions.
3.1 Representing Meaning
We represent meaning within XDG on two dimen-
sions: one for predicate-argument structure (PA),
every student reads a book
s
u
b
j
d
e
t
o
b
j
d
e
t
every student reads a book
a
g
a
r
g
p
a
t
a
r
g
i. ID-tree ii. PA-structure
s
every student reads a book
s
r
r
s
every student reads a book
s
r
r
iii. scope trees
Figure 2: Two analyses for the sentence ?every stu-
dent reads a book.?
and one for scope (SC). The function of the PA di-
mension is to abstract over syntactic idiosyncrasies
such as active-passive alternations or dative shifts,
and to make certain semantic dependencies e. g. in
control constructions explicit; it deals with concepts
such as agent and patient, rather than subject and ob-
ject. The purpose of the SC dimension is to reflect
the structure of a logical formula that would repre-
sent the semantics, in terms of scope and restriction.
We will make this connection explicit in Section 4.
In addition, we assume an ID dimension as above.
We do not include an LP dimension only for ease of
presentation; it could be added completely orthogo-
nally to the three dimensions we consider here.
While one ID structure will typically correspond
to one PA structure, each PA structure will typically
be consistent with multiple SC structures, because
of scope ambiguities. For instance, Fig. 2 shows the
unique ID and PA structures for the sentence ?Ev-
ery student reads a book.? These structures (and the
input sentence) are consistent with the two possi-
ble SC-structures shown in (iii). Assuming a David-
sonian event semantics, the two SC trees (together
with the PA-structure) represent the two readings of
the sentence:
? ?e.?x.student(x) ??y.book(y)? read(e,x,y)
? ?e.?y.book(y)??x.student(x) ? read(e,x,y)
3.2 A Grammar for a Fragment of English
The lexicon for an XDG grammar for a small frag-
ment of English using the ID, PA, and SC dimensions
is shown in Fig. 3. Each row in the table specifies a
(unique) lexical entry for each part of speech (deter-
miner, common noun, proper noun, transitive verb
and preposition); there is no lexical ambiguity in
this grammar. Each column specifies a feature. The
meaning of the features will be explained together
inID outID inPA outPA inSC outSC
DET {subj?,obj?,pcomp?} {det!} {ag?,pat?,arg?} {quant!} {r?,s?,a?} {r!,s!}
CN {det?} {prep?} {quant?} {mod?} {r?,s?,a?} {}
PN {subj?,obj?,pcomp?} {prep?} {ag?,pat?,arg?} {mod?} {r?,s?,a?} {r?,s!}
TV {} {subj!,obj!,prep?} {} {ag!,pat!, instr?} {r?,s?,a?} {}
PREP {prep?} {pcomp!} {mod?, instr?} {arg!} {r?,s?,a?} {a!}
link codom contradom
DET {quant 7? {det}} {quant 7? {r}} {}
CN,PN {mod 7? {prep}} {} {mod 7? {a}}
TV {ag 7? {subj},pat 7? {obj}, instr 7? {prep}} {} {ag 7? {s},pat 7? {s}, instr 7? {a}}
PREP {arg 7? {pcomp}} {} {arg 7? {s}}
Figure 3: The example grammar fragment
with the principles that use them.
The ID dimension uses the edge labels LabID =
{det,subj,obj,prep,pcomp} resp. for determined
common noun,1 subject, object, preposition, and
complement of a preposition. The PA dimension
uses LabPA = {ag,pat,arg,quant,mod, instr}, resp.
for agent, patient, argument of a modifier, common
noun pertaining to a quantifier, modifier, and instru-
ment; and SC uses LabSC = {r,s,a} resp. for restric-
tion and scope of a quantifier, and for an argument.
The grammar also contains three one-dimen-
sional principles (tree, dag, and valency), and
three multi-dimensional principles (linking, co-
dominance, and contra-dominance).
Tree and dag principles. The tree principle re-
stricts ID and SC structures to be trees, and the
dag principle restricts PA structures to be directed
acyclic graphs.
Valency principle. The valency principle, which
we use on all dimensions, states that the incom-
ing and outgoing edges of each node must obey the
specifications of the in and out features. The possi-
ble values for each feature ind and outd are subsets
of Labd ? {!,?,?}. `! specifies a mandatory edge
with label `, `? an optional one, and `? zero or more.
Linking principle. The linking principle for di-
mensions d1,d2 constrains how dependents on d1
may be realised on d2. It assumes a feature linkd1,d2
whose values are functions that map labels from
Labd1 to sets of labels from Labd2 , and is specified
by the following implication:
v
l
?d1 v
? ? ?l? ? linkd1,d2(v)(l) : v
l?
?d2 v
?
Our grammar uses this principle with the link fea-
ture to constrain the realisations of PA-dependents in
the ID dimension. In Fig. 2, the agent (ag) of reads
must be realised as the subject (subj), i. e.
1We assume on all dimensions that determiners are the
heads of common nouns. This makes for a simpler relationship
between the syntactic and semantic dimensions.
reads ag?PA every ? reads
subj
? ID every
Similarly for the patient and the object. There
is no instrument dependent in the example, so this
part of the link feature is not used. An ergative verb
would use a link feature where the subject realises
the patient; Control and raising phenomena can also
be modelled, but we cannot present this here.
Co-dominance principle. The co-dominance
principle for d1,d2 relates edges in d1 to dominance
relations in the same direction in d2. It assumes a
feature codomd1,d2 mapping labels in Labd1 to sets
of labels in Labd2 and is specified as
v
l
?d1 v
? ? ?l? ? codomd1,d2(v)(l) : v
l?
???d2v
?
Our grammar uses the co-dominance principle on
dimension PA and SC to express, e. g., that the
propositional contribution of a noun must end up in
the restriction of its determiner. For example, for the
determiner every of Fig. 2 we have:
every quant? PA student ? every
r
???SCstudent
Contra-dominance principle. The contra-domi-
nance principle is symmetric to the co-dominance
principle, and relates edges in d1 to dominance
edges into the opposite direction in d2. It assumes
a feature contradomd1,d2 mapping labels of Labd1 to
sets of labels from Labd2 and is specified as
v
l
?d1 v
? ?
?l? ? contradomd1,d2(v)(l) : v?
l?
???d2v
Our grammar uses the contra-dominance principle
on dimensions PA and SC to express, e. g., that pred-
icates must end up in the scope of the quantifiers
whose variables they refer to. Thus, for the transi-
tive verb reads of Fig. 2, we have:
reads ag?PA every ? every
s
???SCreads
reads pat?PA a ? a
s
???SCreads
Mary saw a student with a book
a
g
p
a
t
q
u
a
n
t
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
p
r
e
p
Mary saw a student with a book
a
g
p
a
t
q
u
a
n
t
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
i
n
s
t
r
a
s
Mary saw a student with a book
a
g
p
a
t
a
r
g
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
m
o
d
p
r
e
p
a
i. Partial analysis
ii. verb attachment
iii. noun attachment
ID
PA
SC
Figure 4: Partial description (left) and two solutions (right) for ?Mary saw a student with a book.?
3.3 Syntax-Semantics Interaction
It is important to note at this point that the syntax-
semantics interface we have defined is indeed re-
lational. Each principle declaratively specifies a set
of admissible analyses, i. e. a relation between the
structures for the different dimensions, and the anal-
yses that the complete grammar judges grammatical
are simply those that satisfy all principles. The role
of the lexicon is to provide the feature values which
parameterise the principles defined above.
The constraint solver complements this relation-
ality by supporting the use of the principles to move
information between any two dimensions. If, say,
the left-hand side of the linking principle is found to
be satisfied for dimension d1, a propagator will infer
the right-hand side and add it to dimension d2. Con-
versely, if the solver finds that the right-hand side
must be false for d2, the negation of the left-hand
side is inferred for d1. By letting principles interact
concurrently, we can make some very powerful in-
ferences, as we will demonstrate with the example
sentence ?Mary saw a student with a book,? some
partial analyses for which are shown in Fig. 4.
Column (i) in the figure shows the state after the
constraint solver finishes its initial propagation, at
the root of the search tree. Even at this point, the va-
lency and treeness principles have conspired to es-
tablish an almost complete ID-structure. By the link-
ing principle, the PA-structure has been determined
similarly closely. The SC-structure is still mostly un-
determined, but by the co- and contra-dominance
principles, the solver has already established that
some nodes must dominate others: A dotted edge
with label s in the picture means that the solver
knows there must be a path between these two nodes
which starts with an s-edge. In other words, the
solver has computed a large amount of semantic in-
formation from an incomplete syntactic analysis.
Now imagine some external source tells us that
with is a mod-child of student on PA, i. e. the anal-
ysis in (iii). This information could come e. g. from
a statistical model of selectional preferences, which
will judge this edge much more probable than an
instr-edge from the verb to the preposition (ii).
Adding this edge will trigger additional inferences
through the linking principle, which can now infer
that with is a prep-child of student on ID. In the other
direction, the solver will infer more dominances on
SC. This means that semantic information can be
used to disambiguate syntactic ambiguities, and se-
mantic information such as selectional preferences
can be stated on their natural level of representation,
rather than be forced into the ID dimension directly.
Similarly, the introduction of new edges on SC
could trigger a similar reasoning process which
would infer new PA-edges, and thus indirectly also
new ID-edges. Such new edges on SC could come
from inferences with world or discourse knowledge
(Koller and Niehren, 2000), scope preferences, or
interactions with information structure (Duchier and
Kruijff, 2003).
4 Traditional Semantics
Our syntax-semantics interface represents seman-
tic information as graphs on the PA and SC dimen-
sions. While this looks like a radical departure from
traditional semantic formalisms, we consider these
graphs simply an alternative way of presenting more
traditional representations. We devote the rest of the
paper to demonstrating that a pair of a PA and a SC
structure can be interpreted as a Montague-style for-
mula, and that a partial analysis on these two di-
mensions can be seen as an underspecified semantic
description.
4.1 Montague-style Interpretation
In order to extract a standard type-theoretic expres-
sion from an XDG analysis, we assign each node v
two semantic values: a lexical value L(v) represent-
ing the semantics of v itself, and a phrasal value
P(v) representing the semantics of the entire SC-
subtree rooted at v. We use the SC-structure to de-
termine functor-argument relationships, and the PA-
structure to establish variable binding.
We assume that nodes for determiners and proper
names introduce unique individual variables (?in-
dices?). Below we will write ??v?? to refer to the in-
dex of the node v, and we write ?` to refer to the
node which is the `-child of the current node in the
appropriate dimension (PA or SC). The semantic lex-
icon is defined as follows; ?L(w)? should be read as
?L(v), where v is a node for the word w?.
L(a) = ?P?Q?e.?x(P(x)?Q(x)(e))
L(book) = book?
L(with) = ?P?x.(with?(???arg??)(x)?P(x))
L(reads) = read?(???pat??)(???ag??)
Lexical values for other determiners, common
nouns, and proper names are defined analogously.
Note that we do not formally distinguish event
variables from individual variables. In particular,
L(with) can be applied to either nouns or verbs,
which both have type ?e, t?.
We assume that no node in the SC-tree has more
than one child with the same edge label (which our
grammar guarantees), and write n(`1, . . . , `k) to in-
dicate that the node n has SC-children over the edge
labels `1, . . . , `k. The phrasal value for n is defined
(in the most complex case) as follows:
P(n(r,s)) = L(n)(P(?r))(? ??n??.P(?s))
This rule implements Montague?s rule of quan-
tification (Montague, 1974); note that ? ??n?? is a
binder for the variable ??n??. Nodes that have no
s-children are simply functionally applied to the
phrasal semantics of their children (if any).
By way of example, consider the left-hand SC-
structure in Fig. 2. If we identify each node by the
word it stands for, we get the following phrasal
@
@
every
?
@
@
a
?
@
@
read var
var
student
book
r
s
s
r
every student reads a book
r
r
s
s
every student reads a book
a
g
a
r
g
p
a
t
a
r
g
Figure 5: A partial SC-structure and its correspond-
ing CLLS description.
value for the root of the tree:
L(a)(L(book))(?x.L(every)(L(student)
(?y.read?(y)(x)))),
where we write x for ??a?? and y for ??every??. The
arguments of read? are x and y because every and
a are the arg and pat children of reads on the PA-
structure. After replacing the lexical values by their
definitions and beta-reduction, we obtain the fa-
miliar representation for this semantic reading, as
shown in Section 3.1.
4.2 Underspecification
It is straightforward to extend this extraction of
type-theoretic formulas from fully specified XDG
analyses to an extraction of underspecified seman-
tic descriptions from partial XDG analyses. We will
briefly demonstrate this here for descriptions in the
CLLS framework (Egg et al, 2001), which sup-
ports this most easily. Other underspecification for-
malisms could be used too.
Consider the partial SC-structure in Fig. 5, which
could be derived by the constraint solver for the
sentence from Fig. 2. We can obtain a CLLS con-
straint from it by first assigning to each node of
the SC-structure a lexical value, which is now a part
of the CLLS constraint (indicated by the dotted el-
lipses). Because student and book are known to be r-
daughters of every and a on SC, we plug their CLLS
constraints into the r-holes of their mothers? con-
straints. Because we know that reads must be dom-
inated by the s-children of the determiners, we add
the two (dotted) dominance edges to the constraint.
Finally, variable binding is represented by the bind-
ing constraints drawn as dashed arrows, and can be
derived from PA exactly as above.
5 Conclusion
In this paper, we have shown how to build a fully re-
lational syntax-semantics interface based on XDG.
This new grammar formalism offers the grammar
developer the possibility to represent different kinds
of linguistic information on separate dimensions
that can be represented as graphs. Any two dimen-
sions can be linked by multi-dimensional principles,
which mutually constrain the graphs on the two di-
mensions. We have shown that a parser based on
concurrent constraint programming is capable of in-
ferences that restrict ambiguity on one dimension
based on newly available information on another.
Because the interface we have presented makes
no assumption that any dimension is more ?basic?
than another, there is no conceptual difference be-
tween parsing and generation. If the input is the sur-
face sentence, the solver will use this information
to compute the semantic dimensions; if the input is
the semantics, the solver will compute the syntactic
dimensions, and therefore a surface sentence. This
means that we get bidirectional grammars for free.
While the solver is reasonably efficient for many
(hand-crafted) grammars, it is an important goal
for the future to ensure that it can handle large-
scale grammars imported from e.g. XTAG (XTAG
Research Group, 2001) or induced from treebanks.
One way in which we hope to achieve this is to iden-
tify fragments of XDG with provably polynomial
parsing algorithms, and which contain most useful
grammars. Such grammars would probably have to
specify word orders that are not completely free,
and we would have to control the combinatorics
of the different dimensions (Maxwell and Kaplan,
1993). One interesting question is also whether dif-
ferent dimensions can be compiled into a single di-
mension, which might improve efficiency in some
cases, and also sidestep the monostratal vs. multi-
stratal distinction.
The crucial ingredient of XDG that make rela-
tional syntax-semantics processing possible are the
declaratively specified principles. So far, we have
only given some examples for principle specifi-
cations; while they could all be written as Horn
clauses, we have not committed to any particular
representation formalism. The development of such
a representation formalism will of course be ex-
tremely important once we have experimented with
more powerful grammars and have a stable intuition
about what principles are needed.
At that point, it would also be highly interest-
ing to define a (logic) formalism that generalises
both XDG and dominance constraints, a fragment of
CLLS. Such a formalism would make it possible to
take over the interface presented here, but use dom-
inance constraints directly on the semantics dimen-
sions, rather than via the encoding into PA and SC
dimensions. The extraction process of Section 4.2
could then be recast as a principle.
Acknowledgements
We thank Markus Egg for many fruitful discussions
about this paper.
References
K. Apt. 2003. Principles of Constraint Programming.
Cambridge University Press.
N. Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press, Cambridge, MA.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2004. Minimal recursion semantics. an introduction.
Journal of Language and Computation. To appear.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In ACL 2001, Toulouse.
D. Duchier and G.-J. M. Kruijff. 2003. Information
structure in topological dependency grammar. In
EACL 2003.
D. Duchier. 2003. Configuration of labeled trees un-
der lexicalized constraints and principles. Research
on Language and Computation, 1(3?4):307?336.
M. Egg, A. Koller, and J. Niehren. 2001. The Constraint
Language for Lambda Structures. Logic, Language,
and Information, 10:457?485.
V. Gupta and J. Lamping. 1998. Efficient linear logic
meaning assembly. In COLING/ACL 1998.
R. M. Kaplan and J. T. Maxwell III. 1988. An algorithm
for functional uncertainty. In COLING 1988, pages
297?302, Budapest/HUN.
A. Koller and J. Niehren. 2000. On underspecified
processing of dynamic semantics. In Proceedings of
COLING-2000, Saarbr?cken.
A. Koller and K. Striegnitz. 2002. Generation as depen-
dency parsing. In ACL 2002, Philadelphia/USA.
J. T. Maxwell and R. M. Kaplan. 1993. The interface
between phrasal and functional constraints. Compu-
tational Linguistics, 19(4):571?590.
R. Montague. 1974. The proper treatment of quantifica-
tion in ordinary english. In Richard Thomason, editor,
Formal Philosophy. Selected Papers of Richard Mon-
tague, pages 247?271. Yale University Press, New
Haven and London.
M. Moortgat. 2002. Categorial grammar and formal se-
mantics. In Encyclopedia of Cognitive Science. Na-
ture Publishing Group, MacMillan. To appear.
Mozart Consortium. 2004. The Mozart-Oz website.
http://www.mozart-oz.org/.
G. Smolka. 1995. The Oz Programming Model. In
Computer Science Today, Lecture Notes in Computer
Science, vol. 1000, pages 324?343. Springer-Verlag.
M. Steedman. 1999. Alternating quantifier scope in
CCG. In Proc. 37th ACL, pages 301?308.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for english. Technical Report IRCS-01-
03, IRCS, University of Pennsylvania.
195
196
197
198
199
200
201
202
Generating with a Grammar Based on Tree Descriptions: a
Constraint-Based Approach
Claire Gardent
CNRS
LORIA, BP 239 Campus Scientifique
54506 Vandoeuvre-les-Nancy, France
claire.gardent@loria.fr
Stefan Thater
Computational Linguistics
Universita?t des Saarlandes
Saarbru?cken, Germany
stth@coli.uni-sb.de
Abstract
While the generative view of language
processing builds bigger units out of
smaller ones by means of rewriting
steps, the axiomatic view eliminates in-
valid linguistic structures out of a set of
possible structures by means of well-
formedness principles. We present a
generator based on the axiomatic view
and argue that when combined with a
TAG-like grammar and a flat seman-
tics, this axiomatic view permits avoid-
ing drawbacks known to hold either of
top-down or of bottom-up generators.
1 Introduction
We take the axiomatic view of language and show
that it yields an interestingly new perspective on
the tactical generation task i.e. the task of produc-
ing from a given semantics
 
a string with seman-
tics
 
.
As (Cornell and Rogers, To appear) clearly
shows, there has recently been a surge of interest
in logic based grammars for natural language. In
this branch of research sometimes referred to as
?Model Theoretic Syntax?, a grammar is viewed
as a set of axioms defining the well-formed struc-
tures of natural language.
The motivation for model theoretic grammars
is initially theoretical: the use of logic should sup-
port both a more precise formulation of grammars
and a different perspective on the mathematical
and computational properties of natural language.
But eventually the question must also be ad-
dressed of how such grammars could be put to
work. One obvious answer is to use a model gen-
erator. Given a logical formula
 
, a model genera-
tor is a program which builds some of the models
satisfying this formula. Thus for parsing, a model
generator can be used to enumerate the (minimal)
model(s), that is, the parse trees, satisfying the
conjunction of the lexical categories selected on
the basis of the input string plus any additional
constraints which might be encoded in the gram-
mar. And similarly for generation, a model gener-
ator can be used to enumerate the models satisfy-
ing the bag of lexical items selected by the lexical
look up phase on the basis of the input semantics.
How can we design model generators which
work efficiently on natural language input i.e. on
the type of information delivered by logic based
grammars? (Duchier and Gardent, 1999) shows
that constraint programming can be used to im-
plement a model generator for tree logic (Back-
ofen et al, 1995). Further, (Duchier and Thater,
1999) shows that this model generator can be used
to parse with descriptions based grammars (Ram-
bow et al, 1995; Kallmeyer, 1999) that is, on
logic based grammars where lexical entries are
descriptions of trees expressed in some tree logic.
In this paper, we build on (Duchier and Thater,
1999) and show that modulo some minor modi-
fications, the same model generator can be used
to generate with description based grammars.
We describe the workings of the algorithm and
compare it with standard existing top-down and
bottom-up generation algorithms. In specific, we
argue that the change of perspective offered by
the constraint-based, axiomatic approach to pro-
cessing presents some interesting differences with
the more traditional generative approach usually
pursued in tactical generation and further, that the
combination of this static view with a TAG-like
grammar and a flat semantics results in a system
which combines the positive aspects of both top-
down and bottom-up generators.
The paper is structured as follows. Sec-
tion 2 presents the grammars we are working
with namely, Description Grammars (DG), Sec-
tion 3 summarises the parsing model presented in
(Duchier and Thater, 1999) and Section 4 shows
that this model can be extended to generate with
DGs. In Section 5, we compare our generator
with top-down and bottom-up generators, Section
6 reports on a proof-of-concept implementation
and Section 7 concludes with pointers for further
research.
2 Description Grammars
There is a range of grammar formalisms which
depart from Tree Adjoining Grammar (TAG) by
taking as basic building blocks tree descriptions
rather than trees. D-Tree Grammar (DTG) is pro-
posed in (Rambow et al, 1995) to remedy some
empirical and theoretical shortcomings of TAG;
Tree Description Grammar (TDG) is introduced
in (Kallmeyer, 1999) to support syntactic and se-
mantic underspecification and Interaction Gram-
mar is presented in (Perrier, 2000) as an alterna-
tive way of formulating linear logic grammars.
Like all these frameworks, DG uses tree de-
scriptions and thereby benefits first, from the ex-
tended domain of locality which makes TAG par-
ticularly suitable for generation (cf. (Joshi, 1987))
and second, from the monotonicity which differ-
entiates descriptions from trees with respect to ad-
junction (cf. (Vijay-Shanker, 1992)).
DG differs from DTG and TDG however in
that it adopts an axiomatic rather than a genera-
tive view of grammar: whereas in DTG and TDG,
derived trees are constructed through a sequence
of rewriting steps, in DG derived trees are mod-
els satisfying a conjunction of elementary tree de-
scriptions. Moreover, DG differs from Interaction
Grammars in that it uses a flat rather than a Mon-
tague style recursive semantics thereby permitting
a simple syntax/semantics interface (see below).
A Description Grammar is a set of lexical en-
tries of the form 	 where  is a tree descrip-
tion and  is the semantic representation associ-
ated with  .
Tree descriptions. A tree description is a con-
junction of literals that specify either the label
of a node or the position of a node relative to

  NP: 
John

Bridging the Gap Between Underspecification Formalisms:
Minimal Recursion Semantics as Dominance Constraints
Joachim Niehren
Programming Systems Lab
Universita?t des Saarlandes
niehren@ps.uni-sb.de
Stefan Thater
Computational Linguistics
Universita?t des Saarlandes
stth@coli.uni-sb.de
Abstract
Minimal Recursion Semantics (MRS) is
the standard formalism used in large-scale
HPSG grammars to model underspecified
semantics. We present the first provably
efficient algorithm to enumerate the read-
ings of MRS structures, by translating
them into normal dominance constraints.
1 Introduction
In the past few years there has been considerable
activity in the development of formalisms for un-
derspecified semantics (Alshawi and Crouch, 1992;
Reyle, 1993; Bos, 1996; Copestake et al, 1999; Egg
et al, 2001). The common idea is to delay the enu-
meration of all readings for as long as possible. In-
stead, they work with a compact underspecified rep-
resentation; readings are enumerated from this rep-
resentation by need.
Minimal Recursion Semantics (MRS) (Copes-
take et al, 1999) is the standard formalism for se-
mantic underspecification used in large-scale HPSG
grammars (Pollard and Sag, 1994; Copestake and
Flickinger, ). Despite this clear relevance, the most
obvious questions about MRS are still open:
1. Is it possible to enumerate the readings of
MRS structures efficiently? No algorithm has
been published so far. Existing implementa-
tions seem to be practical, even though the
problem whether an MRS has a reading is NP-
complete (Althaus et al, 2003, Theorem 10.1).
2. What is the precise relationship to other un-
derspecification formalism? Are all of them the
same, or else, what are the differences?
We distinguish the sublanguages of MRS nets
and normal dominance nets, and show that they
can be intertranslated. This translation answers the
first question: existing constraint solvers for normal
dominance constraints can be used to enumerate the
readings of MRS nets in low polynomial time.
The translation also answers the second ques-
tion restricted to pure scope underspecification. It
shows the equivalence of a large fragment of MRSs
and a corresponding fragment of normal dominance
constraints, which in turn is equivalent to a large
fragment of Hole Semantics (Bos, 1996) as proven
in (Koller et al, 2003). Additional underspecified
treatments of ellipsis or reinterpretation, however,
are available for extensions of dominance constraint
only (CLLS, the constraint language for lambda
structures (Egg et al, 2001)).
Our results are subject to a new proof tech-
nique which reduces reasoning about MRS struc-
tures to reasoning about weakly normal dominance
constraints (Bodirsky et al, 2003). The previous
proof techniques for normal dominance constraints
(Koller et al, 2003) do not apply.
2 Minimal Recursion Semantics
We define a simplified version of Minimal Recur-
sion Semantics and discuss differences to the origi-
nal definitions presented in (Copestake et al, 1999).
MRS is a description language for formulas of
first order object languages with generalized quanti-
fiers. Underspecified representations in MRS consist
of elementary predications and handle constraints.
Roughly, elementary predications are object lan-
guage formulas with ?holes? into which other for-
mulas can be plugged; handle constraints restrict the
way these formulas can be plugged into each other.
More formally, MRSs are formulas over the follow-
ing vocabulary:
1. Variables. An infinite set of variables ranged
over by h. Variables are also called handles.
2. Constants. An infinite set of constants ranged
over by x,y,z. Constants are the individual vari-
ables of the object language.
3. Function symbols.
(a) A set of function symbols written as P.
(b) A set of quantifier symbols ranged over
by Q (such as every and some). Pairs Qx
are further function symbols (the variable
binders of x in the object language).
4. The symbol ? for the outscopes relation.
Formulas of MRS have three kinds of literals, the
first two are called elementary predications (EPs)
and the third handle constraints:
1. h :P(x1, . . . ,xn,h1, . . . ,hm) where n,m ? 0
2. h :Qx(h1,h2)
3. h1 ? h2
Label positions are to the left of colons ?:? and argu-
ment positions to the right. Let M be a set of literals.
The label set lab(M) contains those handles of M
that occur in label but not in argument position. The
argument handle set arg(M) contains the handles of
M that occur in argument but not in label position.
Definition 1 (MRS). An MRS is finite set M of
MRS-literals such that:
M1 Every handle occurs at most once in label and
at most once in argument position in M.
M2 Handle constraints h1 ? h2 in M always relate
argument handles h1 to labels h2 of M.
M3 For every constant (individual variable) x in ar-
gument position in M there is a unique literal of
the form h :Qx(h1,h2) in M.
We call an MRS compact if it additionally satisfies:
M4 Every handle of M occurs exactly once in an
elementary predication of M.
We say that a handle h immediately outscopes a
handle h? in an MRS M iff there is an EP E in M such
that h occurs in label and h? in argument position of
E . The outscopes relation is the reflexive, transitive
closure of the immediate outscopes relation.
everyx
studentx
readx,y
somey
booky
{h1 : everyx(h2,h4),h3 : student(x),h5 : somey(h6,h8),
h7 : book(y),h9 : read(x,y),h2 ? h3,h6 ? h7}
Figure 1: MRS for ?Every student reads a book?.
An example MRS for the scopally ambiguous
sentence ?Every student reads a book? is given in
Fig. 1. We often represent MRSs by directed graphs
whose nodes are the handles of the MRS. Elemen-
tary predications are represented by solid edges and
handle constraints by dotted lines. Note that we
make the relation between bound variables and their
binders explicit by dotted lines (as from everyx to
readx,y); redundant ?binding-edges? that are sub-
sumed by sequences of other edges are omitted how-
ever (from everyx to studentx for instance).
A solution for an underspecified MRS is called a
configuration, or scope-resolved MRS.
Definition 2 (Configuration). An MRS M is a con-
figuration if it satisfies the following conditions.
C1 The graph of M is a tree of solid edges: handles
don?t properly outscope themselves or occur in
different argument positions and all handles are
pairwise connected by elementary predications.
C2 If two EPs h :P(. . . ,x, . . .) and h0 :Qx(h1,h2)
belong to M, then h0 outscopes h in M (so that
the binding edge from h0 to h is redundant).
We call M a configuration for another MRS M? if
there exists some substitution ? : arg(M?) 7? lab(M?)
which states how to identify argument handles of M?
with labels of M?, so that:
C3 M = {?(E) | E is EP in M?}, and
C4 ?(h1) outscopes h2 in M, for all h1 ? h2 ? M?.
The value ?(E) is obtained by substituting all ar-
gument handles in E , leaving all others unchanged.
The MRS in Fig. 1 has precisely two configura-
tions displayed in Fig. 2 which correspond to the two
readings of the sentence. In this paper, we present
an algorithm that enumerates the configurations of
MRSs efficiently.
everyx
studentx somey
booky readx,y
somey
booky everyx
studentx readx,y
Figure 2: Graphs of Configurations.
Differences to Standard MRS. Our version de-
parts from standard MRS in some respects. First,
we assume that different EPs must be labeled with
different handles, and that labels cannot be identi-
fied. In standard MRS, however, conjunctions are
encoded by labeling different EPs with the same
handle. These EP-conjunctions can be replaced in
a preprocessing step introducing additional EPs that
make conjunctions explicit.
Second, our outscope constraints are slightly less
restrictive than the original ?qeq-constraints.? A
handle h is qeq to a handle h? in an MRS M, h =q h?,
if either h = h? or a quantifier h :Qx(h1,h2) occurs
in M and h2 is qeq to h? in M. Thus, h =q h? im-
plies h ? h?, but not the other way round. We believe
that the additional strength of qeq-constraints is not
needed in practice for modeling scope. Recent work
in semantic construction for HPSG (Copestake et
al., 2001) supports our conjecture: the examples dis-
cussed there are compatible with our simplification.
Third, we depart in some minor details: we
use sets instead of multi-sets and omit top-handles
which are useful only during semantics construction.
3 Dominance Constraints
Dominance constraints are a general framework for
describing trees, and thus syntax trees of logical for-
mulas. Dominance constraints are the core language
underlying CLLS (Egg et al, 2001) which adds par-
allelism and binding constraints.
3.1 Syntax and Semantics
We assume a possibly infinite signature ? of func-
tion symbols with fixed arities and an infinite set Var
of variables ranged over by X ,Y,Z. We write f ,g for
function symbols and ar( f ) for the arity of f .
A dominance constraint ? is a conjunction of
dominance, inequality, and labeling literals of the
following forms where ar( f ) = n:
? ::= X/?Y | X 6= Y | X : f (X1, . . . ,Xn) | ????
Dominance constraints are interpreted over finite
constructor trees, i.e. ground terms constructed from
the function symbols in ?. We identify ground terms
with trees that are rooted, ranked, edge-ordered and
labeled. A solution for a dominance constraint con-
sists of a tree ? and a variable assignment ? that
maps variables to nodes of ? such that all constraints
are satisfied: a labeling literal X : f (X1, . . . ,Xn) is sat-
isfied iff the node ?(X) is labeled with f and has
daughters ?(X1), . . . ,?(Xn) in this order; a domi-
nance literal X/?Y is satisfied iff ?(X) is an ancestor
of ?(Y ) in ?; and an inequality literal X 6=Y is satis-
fied iff ?(X) and ?(Y ) are distinct nodes.
Note that solutions may contain additional mate-
rial. The tree f (a,b), for instance, satisfies the con-
straint Y :a?Z :b.
3.2 Normality and Weak Normality
The satisfiability problem of arbitrary dominance
constraints is NP-complete (Koller et al, 2001) in
general. However, Althaus et al (2003) identify a
natural fragment of so called normal dominance
constraints, which have a polynomial time satisfia-
bility problem. Bodirsky et al (2003) generalize this
notion to weakly normal dominance constraints.
We call a variable a hole of ? if it occurs in argu-
ment position in ? and a root of ? otherwise.
Definition 3. A dominance constraint ? is normal
(and compact) if it satisfies the following conditions.
N1 (a) each variable of ? occurs at most once in the
labeling literals of ?.
(b) each variable of ? occurs at least once in the
labeling literals of ?.
N2 for distinct roots X and Y of ?, X 6=Y is in ?.
N3 (a) if X C? Y occurs in ?, Y is a root in ?.
(b) if X C? Y occurs in ?, X is a hole in ?.
A dominance constraint is weakly normal if it satis-
fies all above properties except for N1(b) and N3(b).
The idea behind (weak) normality is that the con-
straint graph (see below) of a dominance constraint
consists of solid fragments which are connected
by dominance constraints; these fragments may not
properly overlap in solutions.
Note that Definition 3 always imposes compact-
ness, meaning that the heigth of solid fragments is at
most one. As for MRS, this is not a serious restric-
tion, since more general weakly normal dominance
constraints can be compactified, provided that dom-
inance links relate either roots or holes with roots.
Dominance Graphs. We often represent domi-
nance constraints as graphs. A dominance graph is
the directed graph (V,/?unionmulti/). The graph of a weakly
normal constraint ? is defined as follows: The nodes
of the graph of ? are the variables of ?. A labeling
literal X : f (X1, . . . ,Xn) of ? contributes tree edges
(X ,Xi) ? / for 1 ? i ? n that we draw as X Xi;
we freely omit the label f and the edge order in the
graph. A dominance literal X/?Y contributes a dom-
inance edge (X ,Y ) ? /? that we draw as X Y .
Inequality literals in ? are also omitted in the graph.
f
a
gFor example, the constraint graph
on the right represents the dominance
constraint X : f (X ?)?Y :g(Y ?)?X ?/?Z?
Y ?/?Z?Z :a?X 6=Y ?X 6=Z?Y 6=Z.
A dominance graph is weakly normal or a wnd-
graph if it does not contain any forbidden subgraphs:
Dominance graphs of a weakly normal dominance
constraints are clearly weakly normal.
Solved Forms and Configurations. The main dif-
ference between MRS and dominance constraints
lies in their notion of interpretation: solutions versus
configurations.
Every satisfiable dominance constraint has in-
finitely many solutions. Algorithms for dominance
constraints therefore do not enumerate solutions but
solved forms. We say that a dominance constraint is
in solved form iff its graph is in solved form. A wnd-
graph ? is in solved form iff ? is a forest. The solved
forms of ? are solved forms ?? that are more spe-
cific than ?, i.e. ? and ?? differ only in their dom-
inance edges and the reachability relation of ? ex-
tends the reachability of ??. A minimal solved form
of ? is a solved form of ? that is minimal with re-
spect to specificity.
The notion of configurations from MRS applies
to dominance constraints as well. Here, a configu-
ration is a dominance constraint whose graph is a
tree without dominance edges. A configuration of a
constraint ? is a configuration that solves ? in the
obvious sense. Simple solved forms are tree-shaped
solved forms where every hole has exactly one out-
going dominance edge.
L1
L2
L3 L4
L2
L1
L4L3
Figure 3: A dominance constraint (left) with a mini-
mal solved form (right) that has no configuration.
Lemma 1. Simple solved forms and configurations
correspond: Every simple solved form has exactly
one configuration, and for every configuration there
is exactly one solved form that it configures.
Unfortunately, Lemma 1 does not extend to min-
imal as opposed to simple solved forms: there are
minimal solved forms without configurations. The
constraint on the right of Fig. 3, for instance, has no
configuration: the hole of L1 would have to be filled
twice while the right hole of L2 cannot be filled.
4 Representing MRSs
We next map (compact) MRSs to weakly normal
dominance constraints so that configurations are
preserved. Note that this translation is based on a
non-standard semantics for dominance constraints,
namely configurations. We address this problem in
the following sections.
The translation of an MRS M to a dominance con-
straint ?M is quite trivial. The variables of ?M are the
handles of M and its literal set is:
{h : Px1,...,xn(h1, . . .) | h :P(x1, . . . ,xn,h1, . . .) ? M}
?{h : Qx(h1,h2) | h :Qx(h1,h2) ? M}
?{h1/?h2 | h1 ? h2 ? M}
?{h/?h0 | h :Qx(h1,h2),h0 :P(. . . ,x, . . .) ? M}
?{h6=h? | h,h? in distinct label positions of M}
Compact MRSs M are clearly translated into (com-
pact) weakly normal dominance constraints. Labels
of M become roots in ?M while argument handles
become holes. Weak root-to-root dominance literals
are needed to encode variable binding condition C2
of MRS. It could be formulated equivalently through
lambda binding constraints of CLLS (but this is not
necessary here in the absence of parallelism).
Proposition 1. The translation of a compact MRS
M into a weakly normal dominance constraint ?M
preserves configurations.
This weak correctness property follows straight-
forwardly from the analogy in the definitions.
5 Constraint Solving
We recall an algorithm from (Bodirsky et al, 2003)
that efficiently enumerates all minimal solved forms
of wnd-graphs or constraints. All results of this sec-
tion are proved there.
The algorithm can be used to enumerate config-
urations for a large subclass of MRSs, as we will
see in Section 6. But equally importantly, this algo-
rithm provides a powerful proof method for reason-
ing about solved forms and configurations on which
all our results rely.
5.1 Weak Connectedness
Two nodes X and Y of a wnd-graph ? = (V,E) are
weakly connected if there is an undirected path from
X to Y in (V,E). We call ? weakly connected if all
its nodes are weakly connected. A weakly connected
component (wcc) of ? is a maximal weakly con-
nected subgraph of ?. The wccs of ? = (V,E) form
proper partitions of V and E .
Proposition 2. The graph of a solved form of a
weakly connected wnd-graph is a tree.
5.2 Freeness
The enumeration algorithm is based on the notion of
freeness.
Definition 4. A node X of a wnd-graph ? is called
free in ? if there exists a solved form of ? whose
graph is a tree with root X .
A weakly connected wnd-graph without free
nodes is unsolvable. Otherwise, it has a solved form
whose graph is a tree (Prop. 2) and the root of this
tree is free in ?.
Given a set of nodes V ? ?V , we write ?|V ? for the
restriction of ? to nodes in V ? and edges in V ??V ?.
The following lemma characterizes freeness:
Lemma 2. A wnd-graph ? with free node X satis-
fies the freeness conditions:
F1 node X has indegree zero in graph ?, and
F2 no distinct children Y and Y ? of X in ? that are
linked to X by immediate dominance edges are
weakly connected in the remainder ?|V\{X}.
5.3 Algorithm
The algorithm for enumerating the minimal solved
forms of a wnd-graph (or equivalently constraint) is
given in Fig. 4. We illustrate the algorithm for the
problematic wnd-graph ? in Fig. 3. The graph of ?
is weakly connected, so that we can call solve(?).
This procedure guesses topmost fragments in solved
forms of ? (which always exist by Prop. 2).
The only candidates are L1 or L2 since L3 and
L4 have incoming dominance edges, which violates
F1. Let us choose the fragment L2 to be topmost.
The graph which remains when removing L2 is still
weakly connected. It has a single minimal solved
form computed by a recursive call of the solver,
where L1 dominates L3 and L4. The solved form of
the restricted graph is then put below the left hole of
L2, since it is connected to this hole. As a result, we
obtain the solved form on the right of Fig. 3.
Theorem 1. The function solved-form(?) com-
putes all minimal solved forms of a weakly normal
dominance graph ?; it runs in quadratic time per
solved form.
6 Full Translation
Next, we explain how to encode a large class of
MRSs into wnd-constraints such that configurations
correspond precisely to minimal solved forms. The
result of the translation will indeed be normal.
6.1 Problems and Examples
The naive representation of MRSs as weakly nor-
mal dominance constraints is only correct in a weak
sense. The encoding fails in that some MRSs which
have no configurations are mapped to solvable wnd-
constraints. For instance, this holds for the MRS on
the right in Fig 3.
We cannot even hope to translate arbitrary MRSs
correctly into wnd-constraints: the configurability
problem of MRSs is NP-complete, while satisfia-
bility of wnd-constraints can be solved in polyno-
mial time. Instead, we introduce the sublanguages
of MRS-nets and equivalent wnd-nets, and show that
they can be intertranslated in quadratic time.
solved-form(?) ?
Let ?1, . . . ,?k be the wccs of ? = (V,E)
Let (Vi,Ei) be the result of solve(?i)
return (V,?ki=1Ei)
solve(?) ?
precond: ? = (V,/unionmulti/?) is weakly connected
choose a node X satisfying (F1) and (F2) in ? else fail
Let Y1, . . . ,Yn be all nodes s.t. X /Yi
Let ?1, . . . ,?k be the weakly connected components of ?|V?{X ,Y1,...,Yn}
Let (Wj,E j) be the result of solve(? j), and X j ?Wj its root
return (V,?kj=1E j ?/?/?1?/?2) where
/?1 = {(Yi,X j) | ?X ? : (Yi,X ?) ? /??X ? ?Wj},
/?2 = {(X ,X j) | ??X ? : (Yi,X ?) ? /??X ? ?Wj}
Figure 4: Enumerating the minimal solved-forms of a wnd-graph.
...
...
(a) strong
.
...
...
(b) weak
.
 ...
...
(c) island
Figure 5: Fragment Schemas of Nets
6.2 Dominance and MRS-Nets
A hypernormal path (Althaus et al, 2003) in a wnd-
graph is a sequence of adjacent edges that does
not traverse two outgoing dominance edges of some
hole X in sequence, i.e. a wnd-graph without situa-
tions Y1 X Y2.
A dominance net ? is a weakly normal domi-
nance constraint whose fragments all satisfy one of
the three schemas in Fig. 5. MRS-nets can be de-
fined analogously. This means that all roots of ? are
labeled in ?, and that all fragments X : f (X1, . . . ,Xn)
of ? satisfy one of the following three conditions:
strong. n ? 0 and for all Y ? {X1, . . . ,Xn} there ex-
ists a unique Z such that Y C? Z in ?, and there exists
no Z such that X C? Z in ?.
weak. n ? 1 and for all Y ? {X1, . . . ,Xn?1,X} there
exists a unique Z such that Y C? Z in ?, and there
exists no Z such that Xn C? Z in ?.
island. n = 1 and all variables in {Y | X1 C? Y} are
connected by a hypernormal path in the graph of the
restricted constraint ?|V?{X1}, and there exists no Z
such that X C? Z in ?.
The requirement of hypernormal connections in
islands replaces the notion of chain-connectedness
in (Koller et al, 2003), which fails to apply to dom-
inance constraints with weak dominance edges.
For ease of presentation, we restrict ourselves to
a simple version of island fragments. In general, we
should allow for island fragments with n > 1.
6.3 Normalizing Dominance Nets
Dominance nets are wnd-constraints. We next trans-
late dominance nets ? to normal dominance con-
straints ?? so that ? has a configuration iff ?? is sat-
isfiable. The trick is to normalize weak dominance
edges. The normalization norm(?) of a weakly nor-
mal dominance constraint ? is obtained by convert-
ing all root-to-root dominance literals X C? Y as fol-
lows:
X C? Y ? Xn C? Y
if X roots a fragment of ? that satisfies schema
weak of net fragments. If ? is a dominance net then
norm(?) is indeed a normal dominance net.
Theorem 2. The configurations of a weakly con-
nected dominance net ? correspond bijectively
to the minimal solved forms of its normalization
norm(?).
For illustration, consider the problematic wnd-
constraint ? on the left of Fig. 3. ? has two minimal
solved forms with top-most fragments L1 and L2 re-
spectively. The former can be configured, in contrast
to the later which is drawn on the right of Fig. 3.
Normalizing ? has an interesting consequence:
norm(?) has (in contrast to ?) a single minimal
solved form with L1 on top. Indeed, norm(?) cannot
be satisfied while placing L2 topmost. Our algorithm
detects this correctly: the normalization of fragment
L2 is not free in norm(?) since it violates property
F2.
The proof of Theorem 2 captures the rest of this
section. We show in a first step (Prop. 3) that the con-
figurations are preserved when normalizing weakly
connected and satisfiable nets. In the second step,
we show that minimal solved forms of normalized
nets, and thus of norm(?), can always be configured
(Prop. 4).
Corollary 1. Configurability of weakly connected
MRS-nets can be decided in polynomial time; con-
figurations of weakly connected MRS-nets can be
enumerated in quadratic time per configuration.
6.4 Correctness Proof
Most importantly, nets can be recursively decom-
posed into nets as long as they have configurations:
Lemma 3. If a dominance net ? has a configuration
whose top-most fragment is X : f (X1, . . . ,Xn), then
the restriction ?|V?{X ,X1,...,Xn} is a dominance net.
Note that the restriction of the problematic net ?
by L2 on the left in Fig. 3 is not a net. This does not
contradict the lemma, as ? does not have a configu-
ration with top-most fragment L2.
Proof. First note that as X is free in ? it cannot have
incoming edges (condition F1). This means that the
restriction deletes only dominance edges that depart
from nodes in {X ,X1, . . . ,Xn}. Other fragments thus
only lose ingoing dominance edges by normality
condition N3. Such deletions preserve the validity
of the schemas weak and strong.
The island schema is more problematic. We have
to show that the hypernormal connections in this
schema can never be cut. So suppose that Y : f (Y1) is
an island fragment with outgoing dominance edges
Y1 C? Z1 and Y1 C? Z2, so that Z1 and Z2 are con-
nected by some hypernormal path traversing the
deleted fragment X : f (X1, . . . ,Xn). We distinguish
the three possible schemata for this fragment:
...
(a) strong
.
...
(b) weak
.
 ...
(c) island
Figure 6: Traversals through fragments of free roots
strong: since X does not have incoming dominance
edges, there is only a single non-trival kind of traver-
sal, drawn in Fig. 6(a). But such traversals contradict
the freeness of X according to F2.
weak: there is one other way of traversing weak
fragments, shown in Fig. 6(b). Let X C? Y be the
weak dominance edge. The traversal proves that Y
belongs to the weakly connected components of one
of the Xi, so the ??Xn C? Y is unsatisfiable. This
shows that the hole Xn cannot be identified with any
root, i.e. ? does not have any configuration in con-
trast to our assumption.
island: free island fragments permit one single non-
trivial form of traversals, depicted in Fig. 6(c). But
such traversals are not hypernormal.
Proposition 3. A configuration of a weakly con-
nected dominance net ? configures its normalization
norm(?), and vice versa of course.
Proof. Let C be a configuration of ?. We show that
it also configures norm(?). Let S be the simple
solved form of ? that is configured by C (Lemma 1),
and S? be a minimal solved form of ? which is more
general than S.
Let X : f (Y1, . . . ,Yn) be the top-most fragment of
the tree S. This fragment must also be the top-most
fragment of S?, which is a tree since ? is assumed to
be weakly connected (Prop. 2). S? is constructed by
our algorithm (Theorem 1), so that the evaluation of
solve(?) must choose X as free root in ?.
Since ? is a net, some literal X : f (Y1, . . . ,Yn) must
belong to ?. Let ?? = ?|{X ,Y1,...,Yn} be the restriction
of ? to the lower fragments. The weakly connected
components of all Y1, . . ., Yn?1 must be pairwise dis-
joint by F2 (which holds by Lemma 2 since X is free
in ?). The X -fragment of net ? must satisfy one of
three possible schemata of net fragments:
weak fragments: there exists a unique weak domi-
nance edge X C? Z in ? and a unique hole Yn without
outgoing dominance edges. The variable Z must be a
root in ? and thus be labeled. If Z is equal to X then
? is unsatisfiable by normality condition N2, which
is impossible. Hence, Z occurs in the restriction ??
but not in the weakly connected components of any
Y1, . . ., Yn?1. Otherwise, the minimal solved form S?
could not be configured since the hole Yn could not
be identified with any root. Furthermore, the root of
the Z-component must be identified with Yn in any
configuration of ? with root X . Hence, C satisfies
Yn C? Z which is add by normalization.
The restriction ?? must be a dominance net by
Lemma 3, and hence, all its weakly connected com-
ponents are nets. For all 1 ? i ? n? 1, the compo-
nent of Yi in ?? is configured by the subtree of C at
node Yi, while the subtree of C at node Yn configures
the component of Z in ??. The induction hypothesis
yields that the normalizations of all these compo-
nents are configured by the respective subconfigura-
tions of C. Hence, norm(?) is configured by C.
strong or island fragments are not altered by nor-
malization, so we can recurse to the lower fragments
(if there exist any).
Proposition 4. Minimal solved forms of normal,
weakly connected dominance nets have configura-
tions.
Proof. By induction over the construction of min-
imal solved forms, we can show that all holes of
minimal solved forms have a unique outgoing dom-
inance edge at each hole. Furthermore, all minimal
solved forms are trees since we assumed connect-
edness (Prop.2). Thus, all minimal solved forms are
simple, so they have configurations (Lemma 1).
7 Conclusion
We have related two underspecification formalism,
MRS and normal dominance constraints. We have
distinguished the sublanguages of MRS-nets and
normal dominance nets that are sufficient to model
scope underspecification, and proved their equiva-
lence. Thereby, we have obtained the first provably
efficient algorithm to enumerate the readings of un-
derspecified semantic representations in MRS.
Our encoding has the advantage that researchers
interested in dominance constraints can benefit from
the large grammar resources of MRS. This requires
further work in order to deal with unrestricted ver-
sions of MRS used in practice. Conversely, one can
now lift the additional modeling power of CLLS to
MRS.
References
H. Alshawi and R. Crouch. 1992. Monotonic semantic
interpretation. In Proc. 30th ACL, pages 32?39.
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn,
J. Niehren, and S. Thiel. 2003. An efficient graph
algorithm for dominance constraints. Journal of Algo-
rithms. In press.
Manuel Bodirsky, Denys Duchier, Joachim Niehren, and
Sebastian Miele. 2003. An efficient algorithm for
weakly normal dominance constraints. Available at
www.ps.uni-sb.de/Papers.
Johan Bos. 1996. Predicate logic unplugged. In Amster-
dam Colloquium, pages 133?143.
Ann Copestake and Dan Flickinger. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Confer-
ence on Language Resources and Evaluation.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-
lard. 1999. Minimal Recursion Semantics: An Intro-
duction. Manuscript, Stanford University.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of the
39th ACL, pages 132?139, Toulouse, France.
Markus Egg, Alexander Koller, and Joachim Niehren.
2001. The Constraint Language for Lambda Struc-
tures. Logic, Language, and Information, 10:457?485.
Alexander Koller, Joachim Niehren, and Ralf Treinen.
2001. Dominance constraints: Algorithms and com-
plexity. In LACL?98, volume 2014 of LNAI, pages
106?125.
Alexander Koller, Joachim Niehren, and Stefan Thater.
2003. Bridging the gap between underspecification
formalisms: Hole semantics as dominance constraints.
In EACL?03, April. In press.
Carl Pollard and Ivan Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago Press.
Uwe Reyle. 1993. Dealing with ambiguities by under-
specification: Construction, representation and deduc-
tion. Journal of Semantics, 10(1).
Minimal Recursion Semantics as Dominance Constraints:
Translation, Evaluation, and Analysis
Ruth Fuchss,1 Alexander Koller,1 Joachim Niehren,2 and Stefan Thater1
1 Dept. of Computational Linguistics, Saarland University, Saarbr?cken, Germany ?
2 INRIA Futurs, Lille, France
{fuchss,koller,stth}@coli.uni-sb.de
Abstract
We show that a practical translation of MRS de-
scriptions into normal dominance constraints is fea-
sible. We start from a recent theoretical translation
and verify its assumptions on the outputs of the En-
glish Resource Grammar (ERG) on the Redwoods
corpus. The main assumption of the translation?
that all relevant underspecified descriptions are
nets?is validated for a large majority of cases; all
non-nets computed by the ERG seem to be system-
atically incomplete.
1 Introduction
Underspecification is the standard approach to deal-
ing with scope ambiguity (Alshawi and Crouch,
1992; Pinkal, 1996). The readings of underspecified
expressions are represented by compact and concise
descriptions, instead of being enumerated explic-
itly. Underspecified descriptions are easier to de-
rive in syntax-semantics interfaces (Egg et al, 2001;
Copestake et al, 2001), useful in applications such
as machine translation (Copestake et al, 1995), and
can be resolved by need.
Two important underspecification formalisms in
the recent literature are Minimal Recursion Seman-
tics (MRS) (Copestake et al, 2004) and dominance
constraints (Egg et al, 2001). MRS is the under-
specification language which is used in large-scale
HPSG grammars, such as the English Resource
Grammar (ERG) (Copestake and Flickinger, 2000).
The main advantage of dominance constraints is
that they can be solved very efficiently (Althaus et
al., 2003; Bodirsky et al, 2004).
Niehren and Thater (2003) defined, in a theo-
retical paper, a translation from MRS into normal
dominance constraints. This translation clarified the
precise relationship between these two related for-
malisms, and made the powerful meta-theory of
dominance constraints accessible to MRS. Their
goal was to also make the large grammars for MRS
? Supported by the CHORUS project of the SFB 378 of the
DFG.
and the efficient constraint solvers for dominance
constraints available to the other formalism.
However, Niehren and Thater made three techni-
cal assumptions:
1. that EP-conjunction can be resolved in a pre-
processing step;
2. that the qeq relation in MRS is simply domi-
nance;
3. and (most importantly) that all linguistically
correct and relevant MRS expressions belong
to a certain class of constraints called nets.
This means that it is not obvious whether their
result can be immediately applied to the output of
practical grammars like the ERG.
In this paper, we evaluate the truth of these as-
sumptions on the MRS expressions which the ERG
computes for the sentences in the Redwoods Tree-
bank (Oepen et al, 2002). The main result of our
evaluation is that 83% of the Redwoods sentences
are indeed nets, and 17% aren?t. A closer analysis
of the non-nets reveals that they seem to be sys-
tematically incomplete, i. e. they predict more read-
ings than the sentence actually has. This supports
the claim that all linguistically correct MRS expres-
sions are indeed nets. We also verify the other two
assumptions, one empirically and one by proof.
Our results are practically relevant because dom-
inance constraint solvers are much faster and have
more predictable runtimes when solving nets than
the LKB solver for MRS (Copestake, 2002), as we
also show here. In addition, nets might be useful as
a debugging tool to identify potentially problematic
semantic outputs when designing a grammar.
Plan of the Paper. We first recall the definitions
of MRS (?2) and dominance constraints (?3). We
present the translation from MRS-nets to domi-
nance constraints (?4) and prove that it can be ex-
tended to MRS-nets with EP-conjunction (?5). Fi-
nally we evaluate the net hypothesis and the qeq
assumption on the Redwoods corpus, and compare
runtimes (?6).
2 Minimal Recursion Semantics
This section presents a definition of Minimal Re-
cursion Semantics (MRS) (Copestake et al, 2004)
including EP-conjunctions with a merging seman-
tics. Full MRS with qeq-semantics, top handles, and
event variables will be discussed in the last para-
graph.
MRS Syntax. MRS constraints are conjunctive
formulas over the following vocabulary:
1. An infinite set of variables ranged over by h.
Variables are also called handles.
2. An infinite set of constants x,y,z denoting in-
divual variables of the object language.
3. A set of function symbols ranged over by P,
and a set of quantifier symbols ranged over by
Q. Pairs Qx are further function symbols.
4. The binary predicate symbol ?=q?.
MRS constraints have three kinds of literals, two
kinds of elementary predications (EPs) in the first
two lines and handle constraints in the third line:
1. h : P(x1, . . . ,xn,h1, . . . ,hm), where n,m ? 0
2. h : Qx(h1,h2)
3. h1 =q h2
In EPs, label positions are on the left of ?:? and argu-
ment positions on the right. Let M be a set of literals.
The label set lab(M) contains all handles of M that
occur in label but not in argument position, and the
argument handle set arg(M) contains all handles of
M that occur in argument but not in label position.
Definition 1 (MRS constraints). An MRS con-
straint (MRS for short) is a finite set M of MRS-
literals such that:
M1 every handle occurs at most once in argument
position in M,
M2 handle constraints h =q h? always relate argu-
ment handles h to labels h?, and
M3 for every constant (individual variable) x in ar-
gument position in M there is a unique literal
of the form h : Qx(h1,h2) in M.
We say that an MRS M is compact if every han-
dle h in M is either a label or an argument handle.
Compactness simplifies the following proofs, but it
is no serious restriction in practice.
We usually represent MRSs as directed graphs:
the nodes of the graph are the handles of the MRS,
EPs are represented as solid lines, and handle con-
straints are represented as dotted lines. For instance,
the following MRS is represented by the graph on
the left of Fig. 1.
{h5 : somey(h6,h8),h7 : book(y),h1 : everyx(h2,h4),
h3 : student(x),h9 : read(x,y),h2 =q h3,h6 =q h7}
everyx somey
studentx booky
readx,y
everyx
somey
studentx
booky
readx,y
everyx
someystudentx
booky readx,y
Figure 1: An MRS and its two configurations.
Note that the relation between bound variables
and their binders is made explicit by binding edges
drawn as dotted lines (cf. C2 below); transitively re-
dundand binding edges (e. g., from somey to booky)
however are omited.
MRS Semantics. Readings of underspecified rep-
resentations correspond to configurations of MRS
constraints. Intuitively, a configuration is an MRS
where all handle constraints have been resolved by
plugging the ?tree fragments? into each other.
Let M be an MRS and h,h? be handles in M. We
say that h immediately outscopes h? in M if there
is an EP in M with label h and argument handle h?,
and we say that h outscopes h? in M if the pair (h,h?)
belongs to the reflexive transitive closure of the im-
mediate outscope relation of M.
Definition 2 (MRS configurations). An MRS M is
a configuration if it satisfies conditions C1 and C2:
C1 The graph of M is a tree of solid edges: (i) all
handles are labels i. e., arg(M) = /0 and M con-
tains no handle constraints, (ii) handles don?t
properly outscope themselve, and (iii) all han-
dles are pairwise connected by EPs in M.
C2 If h : Qx(h1,h2) and h? : P(. . . ,x, . . .) belong to
M, then h outscopes h? in M i. e., binding edges
in the graph of M are transitively redundant.
We say that a configuration M is configuration of
an MRS M? if there exists a partial substitution ? :
lab(M?) arg(M?) that states how to identify labels
with argument handles of M? so that:
C3 M = {?(E) | E is an EP in M?}, and
C4 for all h =q h? in M?, h outscopes ?(h?) in M.
The value ?(E) is obtained by substituting all la-
bels in dom(?) in E while leaving all other handels
unchanged.
The MRS on the left of Fig. 1, for instance, has
two configurations given to the right.
EP-conjunctions. Definitions 1 and 2 generalize
the idealized definition of MRS of Niehren and
Thater (2003) by EP-conjunctions with a merging
semantics. An MRS M contains an EP-conjunction
if it contains different EPs with the same label h.The
intuition is that EP-conjunctions are interpreted by
object language conjunctions.
P1, P2
P3
{h1 : P1(h2),h1 : P2(h3),h4 : P3
h2 =q h4,h3 =q h4}
Figure 2: An unsolvable MRS with EP-conjunction
P1
P3P2
P1
P2, P3
configures
Figure 3: A solvable MRS without merging-free
configaration
Fig. 2 shows an MRSwith an EP-conjunction and
its graph. The function symbols of both EPs are con-
joined and their arguments are merged into a set.
The MRS does not have configurations since the ar-
gument handles of the merged EPs cannot jointly
outscope the node P4.
We call a configuration merging if it contains EP-
conjunctions, and merging-free otherwise. Merging
configurations are needed to solve EP-conjuctions
such as {h : P1, h : P2}. Unfortunately, they can also
solve MRSs without EP-conjunctions, such as the
MRS in Fig. 3. The unique configuration of this
MRS is a merging configuration: the labels of P1
and P2 must be identified with the only available ar-
gument handle. The admission of merging configu-
rations may thus have important consequences for
the solution space of arbitrary MRSs.
Standard MRS. Standard MRS requires three
further extensions: (i) qeq-semantics, (ii) top-
handles, and (iii) event variables. These extensions
are less relevant for our comparision.
The qeq-semantics restricts the interpretation of
handle constraints beyond dominance. Let M be an
MRS with handles h,h?. We say that h is qeq h? in M
if either h = h?, or there is an EP h : Qx(h0,h1) in M
and h1 is qeq h? in M. Every qeq-configuration is a
configuration as defined above, but not necessarily
vice versa. The qeq-restriction is relevant in theory
but will turn out unproblematic in practice (see ?6).
Standard MRS requires the existence of top
handles in all MRS constraints. This condition
doesn?t matter for MRSs with connected graphs (see
(Bodirsky et al, 2004) for the proof idea). MRSs
with unconnected graphs clearly do not play any
role in practical underspecified semantics.
Finally, MRSs permit events variables e,e? as a
second form of constants. They are treated equally
to individual variables except that they cannot be
bound by quantifiers.
3 Dominance Constraints
Dominance constraints are a general framework for
describing trees. For scope underspecification, they
are used to describe the syntax trees of object lan-
guage formulas. Dominance constraints are the core
language underlying CLLS (Egg et al, 2001) which
adds parallelism and binding constraints.
Syntax and semantics. We assume a possibly in-
finite signature ? = { f ,g, . . .} of function symbols
with fixed arities (written ar( f )) and an infinite set
of variables ranged over by X ,Y,Z.
A dominance constraint ? is a conjunction of
dominance, inequality, and labeling literals of the
following form, where ar( f ) = n:
? ::= X ? Y | X = Y | X : f (X1, . . . ,Xn) | ????
Dominance constraints are interpreted over fi-
nite constructor trees i. e., ground terms constructed
from the function symbols in ?. We identify ground
terms with trees that are rooted, ranked, edge-
ordered and labeled. A solution for a dominance
constraint ? consists of a tree ? and an assign-
ment ? that maps the variables in ? to nodes of ?
such that all constraints are satisfied: labeling lit-
erals X : f (X1, . . . ,Xn) are satisfied iff ?(X) is la-
beled with f and its daughters are ?(X1), . . . ,?(Xn)
in this order; dominance literals X ? Y are satisfied
iff ?(X) dominates ?(Y ) in ?; and inequality literals
X = Y are satisfied iff ?(X) and ?(Y ) are distinct
nodes.
Solved forms. Satisfiable dominance constraints
have infinitely many solutions. Constraint solvers
for dominance constraints therefore do not enumer-
ate solutions but solved forms i. e., ?tree shaped?
constraints. To this end, we consider (weakly) nor-
mal dominance constraints (Bodirsky et al, 2004).
We call a variable a hole of ? if it occurs in argu-
ment position in ? and a root of ? otherwise.
Definition 3. A dominance constraint ? is normal
if it satisfies the following conditions.
N1 (a) each variable of ? occurs at most once in
the labeling literals of ?.
(b) each variable of ? occurs at least once in
the labeling literals of ?.
N2 for distinct roots X and Y of ?, X =Y is in ?.
N3 (a) if X ? Y occurs in ?, Y is a root in ?.
(b) if X ? Y occurs in ?, X is a hole in ?.
We call ? weakly normal if it satisfies the above
properties except for N1 (b) and N3 (b).
Note that Definition 3 imposes compactness: the
height of tree fragments is always one. This is not
everyx somey
studentx booky
readx,y
everyx
someystudentx
booky readx,y
everyx
somey
studentx
booky
readx,y
Figure 4: A normal dominance constraint (left) and
its two solved forms (right).
a serious restriction, as weakly normal dominance
constraints can be compactified, provided that dom-
inance links relate either roots or holes with roots.
Weakly normal dominance constraints ? can be
represented by dominance graphs. The dominance
graph of ? is a directed graph G = (V,ET unionmultiED) de-
fined as follows. The nodes of G are the variables of
?. Labeling literals X : f (X1, . . . ,Xk) are represented
by tree edges (X ,Xi) ? ET , for 1? i? k, and domi-
nance literals X ? X ? are represented by dominance
edges (X ,X ?) ? ED. Inequality literals are not repre-
sented in the graph. In pictures, labeling literals are
drawn with solid lines and dominance edges with
dotted lines.
We say that a constraint ? is in solved form if its
graph is in solved form. A graph G is in solved form
iff it is a forest. The solved forms of G are solved
forms G? which are more specific than G i. e., they
differ only in their dominance edges and the reacha-
bility relation of G extends the reachability of G?. A
minimal solved form is a solved form which is min-
imal with respect to specificity. Simple solved forms
are solved forms where every hole has exactly one
outgoing dominance edge. Fig. 4 shows as a con-
crete example the translation of the MRS descrip-
tion in Fig. 1 together with its two minimal solved
forms. Both solved forms are simple.
4 Translating Merging-Free MRS-Nets
This section defines MRS-nets without EP-
conjunctions, and sketches their translation to
normal dominance constraints. We define nets
equally for MRSs and dominance constraints. The
key semantic property of nets is that different
notions of solutions coincide. In this section, we
show that merging-free configurations coincides
to minimal solved forms. ?5 generalizes the trans-
lation by adding EP-conjunctions and permitting
merging semantics.
Pre-translation. An MRS constraint M can be
represented as a corresponding dominance con-
straint ?M as follows: The variables of ?M are the
handles of M, and the literals of ?M correspond
... ... ...
... ...
(a) strong (b) weak (c) island
Figure 5: Fragment Schemata of Nets
those of M in the following sence:
h : P(x1, . . . ,xn,h1, . . . ,hk) 	? h : Px1,...,xn(h1, . . . ,hk)
h : Qx(h1,h2) 	? h : Qx(h1,h2)
h =q h
?
	? h ? h?
Additionally, dominance literals h ? h? are added to
?M for all h,h? s. t. h :Qx(h1,h2) and h? :P(. . . ,x, . . .)
belong to M (cf. C2), and literals h = h? are added
to ?M for all h,h? in distinct label position in M.
Lemma 1. If a compact MRS M does not contain
EP-conjunctions then ?M is weakly normal, and the
graph of M is the transitive reduction of the graph
of ?M.
Nets. A hypernormal path (Althaus et al, 2003)
in a constraint graph is a path in the undirected
graph that contains for every leaf X at most one in-
cident dominance edge.
Let ? be a weakly normal dominance constraint
and let G be the constraint graph of ?. We say that
? is a dominance net if the transitive reduction G?
of G is a net. G? is a net if every tree fragment F
of G? satisfies one of the following three conditions,
illustrated in Fig. 5:
Strong. Every hole of F has exactly one outgoing
dominance edge, and there is no weak root-to-root
dominance edge.
Weak. Every hole except for the last one has ex-
actly one outgoing dominance edge; the last hole
has no outgoing dominance edge, and there is ex-
actly one weak root-to-root dominance edge.
Island. The fragment has one hole X , and all vari-
ables which are connected to X by dominance edges
are connected by a hypernormal path in the graph
where F has been removed.
We say that an MRS M is an MRS-net if the pre-
translation of its literals results in a dominance net
?M. We say that an MRS-net M is connected if ?M
is connected; ?M is connected if the graph of ?M is
connected.
Note that this notion of MRS-nets implies that
MRS-nets cannot contain EP-conjunctions as other-
wise the resulting dominance constraint would not
be weakly normal. ?5 shows that EP-conjunctions
can be resolved i. e., MRSs with EP-conjunctions
can be mapped to corresponding MRSs without EP-
conjunctions.
If M is an MRS-net (without EP-conjunctions),
then M can be translated into a corresponding dom-
inance constraint ? by first pre-translating M into
a ?M and then normalizing ?M by replacing weak
root-to-root dominance edges in weak fragments by
dominance edges which start from the open last
hole.
Theorem 1 (Niehren and Thater, 2003). Let M be
an MRS and ?M be the translation of M. If M is a
connected MRS-net, then the merging-free configu-
rations of M bijectively correspond to the minimal
solved forms of the ?M.
The following section generalizes this result to
MRS-nets with a merging semantics.
5 Merging and EP-Conjunctions
We now show that if an MRS is a net, then all its
configurations are merging-free, which in particular
means that the translation can be applied to the more
general version of MRS with a merging semantics.
Lemma 2 (Niehren and Thater, 2003). All mini-
mal solved forms of a connected dominance net are
simple.
Lemma 3. If all solved forms of a normal domi-
nance constraint are simple, then all of its solved
forms are minimal.
Theorem 2. The configurations of an MRS-net M
are merging-free.
Proof. Let M? be a configuration of M and let ? be
the underlying substitution. We construct a solved
form ?M? as follows: the labeling literals of ?M? are
the pre-translations of the EPs in M, and ?M? has a
dominance literal h? ? h iff (h,h?) ? ?, and inequal-
ity literals X = Y for all distinct roots in ?M? .
By condition C1 in Def. 2, the graph of M? is a
tree, hence the graph of ?M? must also be a tree i. e.,
?M? is a solved form. ?M? must also be more spe-
cific than the graph of ?M because the graph of M?
satisfies all dominance requirements of the handle
constraints in M, hence ?M? is a solved form of ?M.
M clearly solved ?M? . By Lemmata 2 and 3, ?M?
must be simple and minimal because ?M is a net.
But then M? cannot contain EP-conjunctions i. e.,M?
is merging-free.
The merging semantics of MRS is needed to
solve EP-conjunctions. As we have seen, the merg-
ing semantics is not relevant for MRS constraints
which are nets. This also verifies Niehren and
Thater?s (2003) assumption that EP-conjunctions
are ?syntactic sugar? which can be resolved in a pre-
processing step: EP-conjunctions can be resolved
by exhaustively applying the following rule which
adds new literals to make the implicit conjunction
explicit:
h : E1(h1, . . . ,hn),h : E2(h
?
1, . . . ,h
?
m)?
h : ?E1&E2?(h1, . . . ,hn,h
?
1, . . . ,h
?
m),
where E(h1, . . . ,hn) stands for an EP with argument
handles h1, . . . ,hn, and where ?E1&E2? is a complex
function symbol. If this rule is applied exhaustively
to an MRS M, we obtain an MRS M? without EP-
conjunctions. It should be intuitively clear that the
configurations of M and M? correspond; Therefore,
the configurations of M also correspond to the min-
imal solved forms of the translation of M?.
6 Evaluation
The two remaining assumptions underlying the
translation are the ?net-hypothesis? that all lin-
guistically relevant MRS expressions are nets, and
the ?qeq-hypothesis? that handle constraints can be
given a dominance semantics practice. In this sec-
tion, we empirically show that both assumptions are
met in practice.
As an interesting side effect, we also compare the
run-times of the constraint-solvers we used, and we
find that the dominance constraint solver typically
outperforms the MRS solver, often by significant
margins.
Grammar and Resources. We use the English
Resource Grammar (ERG), a large-scale HPSG
grammar, in connection with the LKB system, a
grammar development environment for typed fea-
ture grammars (Copestake and Flickinger, 2000).
We use the system to parse sentences and output
MRS constraints which we then translate into domi-
nance constraints. As a test corpus, we use the Red-
woods Treebank (Oepen et al, 2002) which con-
tains 6612 sentences. We exclude the sentences that
cannot be parsed due to memory capacities or words
and grammatical structures that are not included in
the ERG, or which produce ill-formed MRS expres-
sions (typically violating M1) and thus base our
evaluation on a corpus containing 6242 sentences.
In case of syntactic ambiguity, we only use the first
reading output by the LKB system.
To enumerate the solutions of MRS constraints
and their translations, we use the MRS solver built
into the LKB system and a solver for weakly nor-
mal dominance constraints (Bodirsky et al, 2004),
...
(a) open hole (b) ill-formed island
Figure 6: Two classes of non-nets
which is implemented in C++ and uses LEDA, a
class library for efficient data types and algorithms
(Mehlhorn and N?her, 1999).
6.1 Relevant Constraints are Nets
We check for 6242 constraints whether they consti-
tute nets. It turns out that 5200 (83.31%) constitute
nets while 1042 (16.69%) violate one or more net-
conditions.
Non-nets. The evaluation shows that the hypoth-
esis that all relevant constraints are nets seems to
be falsified: there are constraints that are not nets.
However, a closer analysis suggests that these con-
straints are incomplete and predict more readings
than the sentence actually has. This can also be il-
lustrated with the average number of solutions: For
the Redwoods corpus in combination with the ERG,
nets have 1836 solutions on average, while non-nets
have 14039 solutions, which is a factor of 7.7. The
large number of solutions for non-nets is due to the
?structural weakness? of non-nets; often, non-nets
have only merging configurations.
Non-nets can be classified into two categories
(see Fig. 6): The first class are violated ?strong?
fragments which have holes without outgoing dom-
inance edge and without a corresponding root-to-
root dominance edge. The second class are violated
?island? fragments where several outgoing domi-
nance edges from one hole lead to nodes which
are not hypernormally connected. There are two
more possibilities for violated ?weak? fragments?
having more than one weak dominance edge or hav-
ing a weak dominance edge without empty hole?,
but they occur infrequently (4.4%). If those weak
fragments were normalized, they would constitute
violated island fragments, so we count them as such.
124 (11.9%) of the non-nets contain empty holes,
762 (73.13%) contain violated island fragments,
and 156 (14.97%) contain both. Those constraints
that contain only empty holes and no violated is-
land fragments cannot be configured, as in configu-
rations, all holes must be filled.
Fragments with open holes occur frequently, but
not in all contexts, for constraints representing for
example time specifications (e. g., ?from nine to
twelve? or ?a three o?clock flight?) or intensional
expressions (e. g., ?Is it?? or ?I suppose?). Ill-
availablee, ax
aycafeteriax
saunay ande,x,y
prop
ax
aycafeteriax
saunay, 
ande,x,y
availablee
prop
ax ay
cafeteriax saunay
ande,x,y
availablee
prop
?1 ?2
Figure 7: An MRS for ?A sauna and a cafeteria are
available? (top) and two of sixteen merging config-
urations (below).
ax ay
cafeteriax saunay
ande,x,y
availablee
prop
Figure 8: The ?repaired? MRS from Fig. 7
formed island fragments are often triggered by some
kind of coordination, like ?a restaurant and/or a
sauna? or ?a hundred and thirty Marks?, also im-
plicit ones like ?one hour thirty minutes? or ?one
thirty?. Constraints with both kinds of violated frag-
ments emerge when there is some input that yields
an open hole and another part of the input yields a
violated island fragment (for example in construc-
tions like ?from nine to eleven thirty? or ?the ten
o?clock flight Friday or Thursday?, but not neces-
sarily as obviously as in those examples).
The constraint on the left in Fig. 7 gives a con-
crete example for violated island fragments. The
topmost fragment has outgoing dominance edges
to otherwise unconnected subconstraints ?1 and ?2.
Under the merging-free semantics of the MRS di-
alect used in (Niehren and Thater, 2003) where ev-
ery hole has to be filled exactly once, this constraint
cannot be configured: there is no hole into which
?available? could be plugged. However, standard
MRS has merging configuration where holes can be
filled more than once. For the constraint in Fig. 7
this means that ?available? can be merged in almost
everywhere, only restricted by the ?qeq-semantics?
which forbids for instance ?available? to be merged
with ?sauna.? In fact, the MRS constraint solver de-
rives sixteen configurations for the constraint, two
of which are given in Fig. 7, although the sentence
has only two scope readings.
We conjecture that non-nets are semantically ?in-
complete? in the sense that certain constraints are
missing. For instance, an alternative analysis for the
above constraint is given in Fig. 8. The constraint
adds an additional argument handle to ?and? and
places a dominance edge from this handle to ?avail-
able.? In fact, the constraint is a net; it has exactly
two readings.
6.2 Qeq is dominance
For all nets, the dominance constraint solver cal-
culates the same number of solutions as the MRS
solver does, with 3 exceptions that hint at problems
in the syntax-semantics interface. As every config-
uration that satisfies proper qeq-constraints is also
a configuration if handle constraints are interpreted
under the weaker notion of dominance, the solutions
computed by the dominance constraint solver and
the MRS solver must be identical for every con-
straint. This means that the additional expressivity
of proper qeq-constraints is not used in practice,
which in turn means that in practice, the translation
is sound and correct even for the standard MRS no-
tion of solution, given the constraint is a net.
6.3 Comparison of Runtimes
The availability of a large body of underspecified
descriptions both in MRS and in dominance con-
straint format makes it possible to compare the
solvers for the two underspecification formalisms.
We measured the runtimes on all nets using a Pen-
tium III CPU at 1.3 GHz. The tests were run in a
multi-user environment, but as the MRS and domi-
nance measurements were conducted pairwise, con-
ditions were equal for every MRS constraint and
corresponding dominance constraint.
The measurements for all MRS-nets with less
than thirty dominance edges are plotted in Fig. 9.
Inputs are grouped according to the constraint size.
The filled circles indicate average runtimes within
each size group for enumerating all solutions us-
ing the dominance solver, and the empty circles in-
dicate the same for the LKB solver. The brackets
around each point indicate maximum and minimum
runtimes in that group. Note that the vertical axis is
logarithmic.
We excluded cases in which one or both of the
solvers did not return any results: There were 173
sentences (3.33% of all nets) on which the LKB
solver ran out of memory, and 1 sentence (0.02%)
that took the dominance solver more than two min-
utes to solve.
The graph shows that the dominance constraint
solver is generally much faster than the LKB solver:
The average runtime is less by a factor of 50 for
constraints of size 10, and this grows to a factor
of 500 for constraints of size 25. Our experiments
show that the dominance solver outperforms the
LKB solver on 98% the cases. In addition, its run-
times are much more predictable, as the brackets in
the graph are also shorter by two or three orders
of magnitude, and the standard deviation is much
smaller (not shown).
7 Conclusion
We developed Niehren and Thater?s (2003) theoret-
ical translation into a practical system for translat-
ing MRS into dominance constraints, applied it sys-
tematically to MRSs produced by English Resource
Grammar for the Redwoods treebank, and evaluated
the results. We showed that:
1. most ?real life? MRS expressions are MRS-
nets, which means that the translation is correct
in these cases;
2. for nets, merging is not necessary (or even pos-
sible);
3. the practical translation works perfectly for all
MRS-nets from the corpus; in particular, the
=q relation can be taken as synonymous with
dominance in practice.
Because the translation works so well in practice,
we were able to compare the runtimes of MRS and
dominance constraint solvers on the same inputs.
This evaluation shows that the dominance constraint
solver outperforms the MRS solver and displays
more predictable runtimes. A researcher working
with MRS can now solve MRS nets using the ef-
ficient dominance constraint solvers.
A small but significant number of the MRS con-
straints derived by the ERG are not nets. We have
argued that these constraints seem to be systemati-
cally incomplete, and their correct completions are
indeed nets. A more detailed evaluation is an impor-
tant task for future research, but if our ?net hypoth-
esis? is true, a system that tests whether all outputs
of a grammar are nets (or a formal ?safety criterion?
that would prove this theoretically) could be a use-
ful tool for developing and debugging grammars.
From a more abstract point of view, our evalua-
tion contributes to the fundamental question of what
expressive power an underspecification formalism
needs. It turned out that the distinction between qeq
 1
 10
 100
 1000
 10000
 100000
 1e+06
 0  5  10  15  20  25  30
T
i
m
e
 
(
m
s
)
Size (number of dominance edges)
DC solver (LEDA)
MRS solver
Figure 9: Comparison of runtimes for the MRS and dominance constraint solvers.
and dominance hardly plays a role in practice. If the
net hypothesis is true, it also follows that merging is
not necessary because EP-conjunctions can be con-
verted into ordinary conjunctions. More research
along these lines could help unify different under-
specification formalisms and the resources that are
available for them.
Acknowledgments We are grateful to Ann
Copestake for many fruitful discussions, and to our
reviewers for helpful comments.
References
H. Alshawi and R. Crouch. 1992. Monotonic se-
mantic interpretation. In Proc. 30th ACL, pages
32?39.
Ernst Althaus, Denys Duchier, Alexander Koller,
Kurt Mehlhorn, Joachim Niehren, and Sven
Thiel. 2003. An efficient graph algorithm for
dominance constraints. Journal of Algorithms,
48:194?219.
Manuel Bodirsky, Denys Duchier, Joachim Niehren,
and Sebastian Miele. 2004. An efficient algo-
rithm for weakly normal dominance constraints.
In ACM-SIAM Symposium on Discrete Algo-
rithms. The ACM Press.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage english grammar using
HPSG. In Conference on Language Resources
and Evaluation.
Ann Copestake, Dan Flickinger, Rob Malouf, Su-
sanne Riehemann, and Ivan Sag. 1995. Transla-
tion using Minimal Recursion Semantics. Leu-
ven.
Ann Copestake, Alex Lascarides, and Dan
Flickinger. 2001. An algebra for semantic
construction in constraint-based grammars. In
Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics, pages
132?139, Toulouse, France.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan Sag. 2004. Minimal recursion semantics:
An introduction. Journal of Language and Com-
putation. To appear.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stan-
ford, CA.
Markus Egg, Alexander Koller, and Joachim
Niehren. 2001. The Constraint Language for
Lambda Structures. Logic, Language, and Infor-
mation, 10:457?485.
K. Mehlhorn and S. N?her. 1999. The LEDA Plat-
form of Combinatorial and Geometric Comput-
ing. Cambridge University Press, Cambridge.
See also http://www.mpi-sb.mpg.de/LEDA/.
Joachim Niehren and Stefan Thater. 2003. Bridg-
ing the gap between underspecification for-
malisms: Minimal recursion semantics as dom-
inance constraints. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and
Thorsten Brants. 2002. The LinGO Redwoods
treebank: Motivation and preliminary applica-
tions. In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING?02), pages 1253?1257.
Manfred Pinkal. 1996. Radical underspecification.
In 10th Amsterdam Colloquium, pages 587?606.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 9?12, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Efficient solving and exploration of scope ambiguities
Alexander Koller and Stefan Thater
Department of Computational Linguistics
Saarland University, Saarbr?cken, Germany
{koller,stth}@coli.uni-sb.de
Abstract
We present the currently most efficient
solver for scope underspecification; it also
converts between different underspecifica-
tion formalisms and counts readings. Our
tool makes the practical use of large-scale
grammars with (underspecified) semantic
output more feasible, and can be used in
grammar debugging.
1 Introduction
One of the most exciting recent developments in
computational linguistics is that large-scale gram-
mars which compute semantic representations are
becoming available. Examples for such grammars
are the HPSG English Resource Grammar (ERG)
(Copestake and Flickinger, 2000) and the LFG Par-
Gram grammars (Butt et al, 2002); a similar re-
source is being developed for the XTAG grammar
(Kallmeyer and Romero, 2004).
But with the advent of such grammars, a phe-
nomenon that is sometimes considered a some-
what artificial toy problem of theoretical semanti-
cists becomes a very practical challenge: the pres-
ence of scope ambiguities. Because grammars of-
ten uniformly treat noun phrases as quantifiers, even
harmless-looking sentences can have surprisingly
many readings. The median number of scope read-
ings for the sentences in the Rondane Treebank (dis-
tributed with the ERG) is 55, but the treebank also
contains extreme cases such as (1) below, which ac-
cording to the ERG has about 2.4 trillion (1012) read-
ings:
(1) Myrdal is the mountain terminus of the Fl?m
rail line (or Fl?msbana) which makes its way
down the lovely Fl?m Valley (Fl?msdalen) to
its sea-level terminus at Fl?m. (Rondane 650)
In order to control such an explosion of readings
(and also to simplify the grammar design process),
the developers of large-scale grammars typically use
methods of packing or underspecification to spec-
ify the syntax-semantics interface. The general idea
is that the parser doesn?t compute all the individual
scope readings, but only a compact underspecified
description, from which the individual readings can
then be extracted at a later stage of processing ? but
the underspecified description could also be used as
a platform for the integration of lexical and context
information, so as to restrict the set of possible read-
ings without enumerating the wrong ones.
Such an approach is only feasible if we have ac-
cess to efficient tools that support the most impor-
tant operations on underspecified descriptions. We
present utool, the Swiss Army Knife of Underspec-
ification, which sets out to do exactly this. It sup-
ports the following operations:
1. enumerate all scope readings represented by an
underspecified description;
2. check whether a description has any readings,
and compute how many readings it has without
explicitly enumerating them;
3. convert underspecified descriptions between
different underspecification formalisms (at this
point, Minimal Recursion Semantics (Copes-
take et al, 2003), Hole Semantics (Bos, 1996),
and dominance constraints/graphs (Egg et al,
2001; Althaus et al, 2003)).
9
Our system is the fastest solver for underspecifi-
cied description available today; that is, it is fastest
at solving Task 1 above (about 100.000 readings per
second on a modern PC). It achieves this by im-
plementing an efficient algorithm for solving dom-
inance graphs (Bodirsky et al, 2004) and caching
intermediate results in a chart data structure. To our
knowledge, it is the only system that can do Tasks
2 and 3. It is only because utool can compute the
number of readings without enumerating them that
we even know that (1) has trillions of readings; even
utool would take about a year to enumerate and
count the readings individually.
utool is implemented in C++, efficient and
portable, open source, and freely downloadable from
http://utool.sourceforge.net.
2 Technical Description
2.1 Solving dominance graphs
At the core of utool is a solver for dominance
graphs (Bodirsky et al, 2004) ? graph represen-
tations of weakly normal dominance constraints,
which constitute one of the main formalisms used
in scope underspecification (Egg et al, 2001; Al-
thaus et al, 2003). Dominance graphs are directed
graphs with two kinds of edges, tree edges and dom-
inance edges. They can be used to describe the set
of all trees into which their tree edges can be embed-
ded, in such a way that every dominance edge in the
graph is realised as reachability in the tree. Domi-
nance graphs are used as underspecified descriptions
by describing sets of trees that are encodings of the
formulas of some language of semantic representa-
tions, such as predicate logic.
Fig. 1 shows an example of a constraint graph for
the sentence ?every student reads a book.? It con-
sists of five tree fragments ? sets of nodes that are
connected by (solid) tree edges ? which are con-
nected by dominance edges (dotted lines). Two of
the fragments have two holes each, into which other
fragments can be ?plugged?. The graph can be em-
bedded into the two trees shown in the middle of
Fig. 1, which correspond to the two readings of the
sentence. By contrast, the graph cannot be embed-
ded into the tree shown on the right: a dominance
edge stipulates that ?readx,y? must be reachable from
?somey?, but it is not reachable from ?somey? in the
tree. We call the two trees into which the graph can
be embedded its solutions.
The Bodirsky et al algorithm enumerates the so-
lutions of a dominance graph (technically, its solved
forms) by computing the set of its free fragments,
which are the fragments that can occur at the root of
some solution. Then it picks one of these fragments
as the root and removes it from the graph. This splits
the graph into several connected subgraphs, which
are then solved recursively.
This algorithm can call itself for the same sub-
graph several times, which can waste a lot of time
because the set of all solutions was already com-
puted for the subgraph on the first recursive call.
For this reason, our implementation caches interme-
diate results in a chart-like data structure. This data
structure maps each subgraph G to a set of splits,
each of which records which fragment of G should
be placed at the root of the solution, what the sub-
graphs after removal of this fragment are, and how
their solutions should be plugged into the holes of
the fragment. In the worst case, the chart can have
exponential size; but in practice, it is much smaller
than the set of all solutions. For example, the chart
for (1) contains 74.960 splits, which is a tiny num-
ber compared to the 2.4 trillion readings, and can be
computed in a few seconds.
Now solving becomes a two-phase process. In the
first phase, the chart data structure is filled by a run
of the algorithm. In the second phase, the complete
solutions are extracted from the chart. Although the
first phase is conceptually much more complex than
the second one because it involves interesting graph
algorithms whose correctness isn?t trivial to prove,
it takes only a small fraction of the entire runtime in
practice.
Instead of enumerating all readings from the
chart, we can also compute the number of solutions
represented by the chart. For each split, we compute
the numbers of solutions of the fragment sets in the
split. Then we multiply these numbers (choices for
the children can be combined freely). Finally, we
obtain the number of solutions for a subgraph by
adding the numbers of solutions of all its splits. This
computation takes linear time in the size of the chart.
10
every
x
some
y
book
y
student
x
read
x,y
every
x
some
y
book
y
student
x
read
x,y
every
x
some
y
book
y
student
x
read
x,y
every
x
some
y
book
y
student
x
read
x,y
? ? ?
Figure 1: A dominance graph (left), two solutions (middle) and a non-solution (right).
2.2 Translating between formalisms
One of the most significant obstacles in the develop-
ment of tools and resources for scope underspecifi-
cation is that different resources (such as grammars
and solvers) are built for different underspecification
formalisms. To help alleviate this problem, utool
can read and write underspecified descriptions and
write out solutions in a variety of different formats:
? dominance graphs;
? descriptions of Minimal Recursion Semantics;
? descriptions of Hole Semantics.
The input and output functionality is provided
by codecs, which translate between descriptions in
one of these formalisms and the internal dominance
graph format. The codecs for MRS and Hole Se-
mantics are based on the (non-trivial) translations
in (Koller et al, 2003; Niehren and Thater, 2003)
and are only defined on nets, i.e. constraints whose
graphs satisfy certain structural restrictions. This is
not a very limiting restriction in practice (Flickinger
et al, 2005). utool also allows the user to test effi-
ciently whether a description is a net.
In practice, utool can be used to convert de-
scriptions between the three underspecification for-
malisms. Because the codecs work with concrete
syntaxes that are used in existing systems, utool
can be used as a drop-in replacement e.g. in the
LKB grammar development system (Copestake and
Flickinger, 2000).
2.3 Runtime comparison
To illustrate utool?s performance, we compare its
runtimes for the enumeration task with the (already
quite efficient) MRS constraint solver of the LKB
system (Copestake and Flickinger, 2000). Our data
set consists of the 850 MRS-nets extracted from the
 0
 10
 20
 30
 40
 50
 60
 70
 0  5  10  15  20  25  30  35  40
"utool"
"MRS"
Figure 2: Distribution of constraints in Rondane over
different sizes. The solid line shows the constraints
in the data set, and the dashed line shows the con-
straints that the LKB solver could solve.
Rondane treebank which have less than one million
solutions (see Fig. 2). Fig. 3 displays the runtimes
for enumerating all solutions, divided by the num-
ber of solutions, for both solvers. The horizontal axis
shows the description sizes (number of tree frag-
ments), and the (logarithmic!) vertical axis shows
the average runtime per solution for descriptions of
this size.
Due to memory limitations, the LKB solver could
only solve descriptions with up to 21 tree fragments,
which account for 80% of the test data. utool solved
all descriptions in the test set. The evaluation was
done using a 1.2 GHz PC with 2 GB of memory.
The figure shows that utool is generally faster
than the LKB solver, up to a factor of approx. 1000.
We should note that the LKB solver displays a dra-
matically higher variation in runtimes for constraints
of the same size. Note that for small constraints, the
runtimes tend to be too small to measure them accu-
rately.
11
 0.01
 0.1
 1
 10
 100
 1000
 0  5  10  15  20  25  30  35  40
"utool"
"MRS"
Figure 3: Runtimes per solution (in ms) for the MRS
nets in the Rondane treebank for LKB and utool.
3 Conclusion
We have presented utool, a tool that supports a va-
riety of operations related to scope underspecifica-
tion. It is the most efficient solver for underspecifi-
cation available today, and provides functionality for
counting readings, testing whether a description is a
net, and converting between different underspecifi-
cation formalisms. It collects the results of several
years of formal and computational research on dom-
inance graphs into one convenient system.
The most obvious use of utool is the enumeration
of readings of underspecified descriptions produced
by large-scale grammars. This means that a user can
realistically map the semantic output of these gram-
mars into actual semantic representations. However,
the tool is also useful for developers of such gram-
mars. It can be used to count and explore the read-
ings of the underspecified descriptions the grammar
computes, and has already been used in the debug-
ging of the syntax-semantics interface of the ERG
(Flickinger et al, 2005).
From a more general perspective, the real ap-
peal of underspecification is that it could allow us
to eliminate readings that contradict the context or
our world knowledge, without having to enumerate
these readings first. Such inferences could already
take place on the level of the underspecified descrip-
tion (Koller and Niehren, 2000). But the new chart
data structure that utool computes is a more explicit
packed representation of the possible readings, and
still relatively small in practice. Thus it could open
up avenues for more theoretical future research as
well.
References
Ernst Althaus, Denys Duchier, Alexander Koller, Kurt
Mehlhorn, Joachim Niehren, and Sven Thiel. 2003.
An efficient graph algorithm for dominance con-
straints. Journal of Algorithms, 48:194?219.
Manuel Bodirsky, Denys Duchier, Joachim Niehren, and
Sebastian Miele. 2004. An efficient algorithm for
weakly normal dominance constraints. In ACM-SIAM
Symposium on Discrete Algorithms. The ACM Press.
Johan Bos. 1996. Predicate logic unplugged. In Pro-
ceedings of the Tenth Amsterdam Colloquium, pages
133?143.
Miriam Butt, Helge Dyvik, Tracey Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The par-
allel grammar project. In Proceedings of the COLING
2002 Workshop on Grammar engeneering and evalua-
tion.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Confer-
ence on Language Resources and Evaluation.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2003. Minimal recursion semantics: An in-
troduction. Available at http://lingo.stanford.
edu/sag/papers/copestake.pdf.
Markus Egg, Alexander Koller, and Joachim Niehren.
2001. The Constraint Language for Lambda Struc-
tures. Logic, Language, and Information, 10:457?485.
Dan Flickinger, Alexander Koller, and Stefan Thater.
2005. A new well-formedness criterion for semantics
debugging. In Proceedings of the 12th HPSG Confer-
ence, Lisbon.
Laura Kallmeyer and Maribel Romero. 2004. LTAG se-
mantics with semantic unification. In Proceedings of
the TAG+7 Workshop, Vancouver.
Alexander Koller and Joachim Niehren. 2000. On un-
derspecified processing of dynamic semantics. In Pro-
ceedings of COLING-2000, Saarbr?cken.
Alexander Koller, Joachim Niehren, and Stefan Thater.
2003. Bridging the gap between underspecification
formalisms: Hole semantics as dominance constraints.
In Proceedings of the 10th EACL, Budapest.
Joachim Niehren and Stefan Thater. 2003. Bridging the
gap between underspecification formalisms: Minimal
recursion semantics as dominance constraints. In Pro-
ceedings of the 41st Annual Meeting of the Association
for Computational Linguistics.
12
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 409?416,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Improved Redundancy Elimination Algorithm
for Underspecified Representations
Alexander Koller and Stefan Thater
Dept. of Computational Linguistics
Universit?t des Saarlandes, Saarbr?cken, Germany
{koller,stth}@coli.uni-sb.de
Abstract
We present an efficient algorithm for the
redundancy elimination problem: Given
an underspecified semantic representation
(USR) of a scope ambiguity, compute an
USR with fewer mutually equivalent read-
ings. The algorithm operates on underspec-
ified chart representations which are de-
rived from dominance graphs; it can be ap-
plied to the USRs computed by large-scale
grammars. We evaluate the algorithm on
a corpus, and show that it reduces the de-
gree of ambiguity significantly while tak-
ing negligible runtime.
1 Introduction
Underspecification is nowadays the standard ap-
proach to dealing with scope ambiguities in com-
putational semantics (van Deemter and Peters,
1996; Copestake et al, 2004; Egg et al, 2001;
Blackburn and Bos, 2005). The basic idea be-
hind it is to not enumerate all possible semantic
representations for each syntactic analysis, but to
derive a single compact underspecified represen-
tation (USR). This simplifies semantics construc-
tion, and current algorithms support the efficient
enumeration of the individual semantic representa-
tions from an USR (Koller and Thater, 2005b).
A major promise of underspecification is that it
makes it possible, in principle, to rule out entire
subsets of readings that we are not interested in
wholesale, without even enumerating them. For in-
stance, real-world sentences with scope ambigui-
ties often have many readings that are semantically
equivalent. Subsequent modules (e.g. for doing in-
ference) will typically only be interested in one
reading from each equivalence class, and all oth-
ers could be deleted. This situation is illustrated
by the following two (out of many) sentences from
the Rondane treebank, which is distributed with
the English Resource Grammar (ERG; Flickinger
(2002)), a large-scale HPSG grammar of English.
(1) For travellers going to Finnmark there is a
bus service from Oslo to Alta through Swe-
den. (Rondane 1262)
(2) We quickly put up the tents in the lee of a
small hillside and cook for the first time in
the open. (Rondane 892)
For the annotated syntactic analysis of (1), the
ERG derives an USR with eight scope bearing op-
erators, which results in a total of 3960 readings.
These readings are all semantically equivalent to
each other. On the other hand, the USR for (2) has
480 readings, which fall into two classes of mutu-
ally equivalent readings, characterised by the rela-
tive scope of ?the lee of? and ?a small hillside.?
In this paper, we present an algorithm for the
redundancy elimination problem: Given an USR,
compute an USR which has fewer readings, but
still describes at least one representative of each
equivalence class ? without enumerating any read-
ings. This algorithm makes it possible to compute
the one or two representatives of the semantic
equivalence classes in the examples, so subsequent
modules don?t have to deal with all the other equiv-
alent readings. It also closes the gap between the
large number of readings predicted by the gram-
mar and the intuitively perceived much lower de-
gree of ambiguity of these sentences. Finally, it
can be helpful for a grammar designer because it
is much more feasible to check whether two read-
ings are linguistically reasonable than 480. Our al-
gorithm is applicable to arbitrary USRs (not just
those computed by the ERG). While its effect is
particularly significant on the ERG, which uni-
formly treats all kinds of noun phrases, including
proper names and pronouns, as generalised quanti-
fiers, it will generally help deal with spurious ambi-
guities (such as scope ambiguities between indef-
409
inites), which have been a ubiquitous problem in
most theories of scope since Montague Grammar.
We model equivalence in terms of rewrite rules
that permute quantifiers without changing the se-
mantics of the readings. The particular USRs we
work with are underspecified chart representations,
which can be computed from dominance graphs
(or USRs in some other underspecification for-
malisms) efficiently (Koller and Thater, 2005b).
We evaluate the performance of the algorithm on
the Rondane treebank and show that it reduces the
median number of readings from 56 to 4, by up
to a factor of 666.240 for individual USRs, while
running in negligible time.
To our knowledge, our algorithm and its less
powerful predecessor (Koller and Thater, 2006)
are the first redundancy elimination algorithms in
the literature that operate on the level of USRs.
There has been previous research on enumerating
only some representatives of each equivalence
class (Vestre, 1991; Chaves, 2003), but these
approaches don?t maintain underspecification:
After running their algorithms, they are left with
a set of readings rather than an underspecified
representation, i.e. we could no longer run other
algorithms on an USR.
The paper is structured as follows. We will first de-
fine dominance graphs and review the necessary
background theory in Section 2. We will then intro-
duce our notion of equivalence in Section 3, and
present the redundancy elimination algorithm in
Section 4. In Section 5, we describe the evaluation
of the algorithm on the Rondane corpus. Finally,
Section 6 concludes and points to further work.
2 Dominance graphs
The basic underspecification formalism we as-
sume here is that of (labelled) dominance graphs
(Althaus et al, 2003). Dominance graphs are
equivalent to leaf-labelled normal dominance con-
straints (Egg et al, 2001), which have been dis-
cussed extensively in previous literature.
Definition 1. A (compact) dominance graph is a
directed graph (V,E unionmultiD) with two kinds of edges,
tree edges E and dominance edges D, such that:
1. The graph (V,E) defines a collection of node
disjoint trees of height 0 or 1. We call the
trees in (V,E) the fragments of the graph.
2. If (v,v?) is a dominance edge in D, then v is
a hole and v? is a root. A node v is a root if v
does not have incoming tree edges; otherwise,
v is a hole.
A labelled dominance graph over a ranked sig-
nature ? is a triple G = (V,E unionmultiD,L) such that
(V,E unionmultiD) is a dominance graph and L : V  ?
is a partial labelling function which assigns a node
v a label with arity n iff v is a root with n outgoing
tree edges. Nodes without labels (i.e. holes) must
have outgoing dominance edges.
We will write R(F) for the root of the fragment
F , and we will typically just say ?graph? instead
of ?labelled dominance graph?.
An example of a labelled dominance graph is
shown to the left of Fig. 1. Tree edges are drawn
as solid lines, and dominance edges as dotted lines,
directed from top to bottom. This graph can serve
as an USR for the sentence ?a representative of
a company saw a sample? if we demand that the
holes are ?plugged? by roots while realising the
dominance edges as dominance, as in the two con-
figurations (of five) shown to the right. These con-
figurations are trees that encode semantic represen-
tations of the sentence. We will freely read config-
urations as ground terms over the signature ?.
2.1 Hypernormally connected graphs
Throughout this paper, we will only consider hy-
pernormally connected (hnc) dominance graphs.
Hnc graphs are equivalent to chain-connected
dominance constraints (Koller et al, 2003), and
are closely related to dominance nets (Niehren and
Thater, 2003). Fuchss et al (2004) have presented
a corpus study that strongly suggests that all dom-
inance graphs that are generated by current large-
scale grammars are (or should be) hnc.
Technically, a graph G is hypernormally con-
nected iff each pair of nodes is connected by a sim-
ple hypernormal path in G. A hypernormal path
(Althaus et al, 2003) in G is a path in the undi-
rected version Gu of G that does not use two dom-
inance edges that are incident to the same hole.
Hnc graphs have a number of very useful struc-
tural properties on which this paper rests. One
which is particularly relevant here is that we can
predict in which way different fragments can dom-
inate each other.
Definition 2. Let G be a hnc dominance graph. A
fragment F1 in G is called a possible dominator
of another fragment F2 in G iff it has exactly one
hole h which is connected to R(F2) by a simple hy-
410
ay
sample
y
see
x,y
a
x
repr-of
x,z
a
z
comp
z
1 2 3
4 5 6
7
a
y
a
x
a
z
1
2
3
sample
y
see
x,y
repr-of
x,z
comp
z
a
y
a
x
sample
y
see
x,y
repr-of
x,z
a
z
comp
z
1
2
3
Figure 1: A dominance graph that represents the five readings of the sentence ?a representative of a
company saw a sample? (left) and two of its five configurations.
{1,2,3,4,5,6,7} :?1,h1 7? {4},h2 7? {2,3,5,6,7}?
?2,h3 7? {1,4,5},h4 7? {3,6,7}?
?3,h5 7? {5},h6 7? {1,2,4,5,7}?
{2,3,5,6,7} :?2,h3 7? {5},h4 7? {3,6,7}?
?3,h5 7? {6},h6 7? {2,5,7}?
{3,6,7} :?3,h5 7? {6},h6 7? {7}?
{2,5,7} :?2,h3 7? {5},h4 7? {7}?
{1,4,5} :?1,h1 7? {4},h2 7? {5}?
{1,2,4,5,7} :?1,h1 7? {4},h2 7? {2,5,7}?
?2,h3 7? {1,4,5},h4 7? {7}?
Figure 2: The chart for the graph in Fig. 1.
pernormal path which doesn?t use R(F1). We write
ch(F1,F2) for this unique h.
Lemma 1 (Koller and Thater (2006)). Let F1, F2
be fragments in a hnc dominance graph G. If there
is a configurationC ofG in which R(F1) dominates
R(F2), then F1 is a possible dominator of F2, and
in particular ch(F1,F2) dominates R(F2) inC.
By applying this rather abstract result, we can
derive a number of interesting facts about the ex-
ample graph in Fig. 1. The fragments 1, 2, and 3
are possible dominators of all other fragments (and
of each other), while the fragments 4 through 7
aren?t possible dominators of anything (they have
no holes); so 4 through 7 must be leaves in any con-
figuration of the graph. In addition, if fragment 2
dominates fragment 3 in any configuration, then in
particular the right hole of 2 will dominate the root
of 3; and so on.
2.2 Dominance charts
Below we will not work with dominance graphs
directly. Rather, we will use dominance charts
(Koller and Thater, 2005b) as our USRs: they are
more explicit USRs, which support a more fine-
grained deletion of reading sets than graphs.
A dominance chart for the graph G is a mapping
of weakly connected subgraphs of G to sets of
splits (see Fig. 2), which describe possible ways
of constructing configurations of the subgraph.
A subgraph G? is assigned one split for each
fragment F in G? which can be at the root of a
configuration of G?. If the graph is hnc, removing
F from the graph splits G? into a set of weakly
connected components (wccs), each of which is
connected to exactly one hole of F . We also record
the wccs, and the hole to which each wcc belongs,
in the split. In order to compute all configurations
represented by a split, we can first compute
recursively the configurations of each component;
then we plug each combination of these sub-
configurations into the appropriate holes of the
root fragment. We define the configurations asso-
ciated with a subgraph as the union over its splits,
and those of the entire chart as the configurations
associated with the complete graph.
Fig. 2 shows the dominance chart correspond-
ing to the graph in Fig. 1. The chart represents
exactly the configuration set of the graph, and is
minimal in the sense that every subgraph and ev-
ery split in the chart can be used in constructing
some configuration. Such charts can be computed
efficiently (Koller and Thater, 2005b) from a dom-
inance graph, and can also be used to compute the
configurations of a graph efficiently.
The example chart expresses that three frag-
ments can be at the root of a configuration of the
complete graph: 1, 2, and 3. The entry for the split
with root fragment 2 tells us that removing 2 splits
the graph into the subgraphs {1,4,5} and {3,6,7}
(see Fig. 3). If we configure these two subgraphs
recursively, we obtain the configurations shown in
the third column of Fig. 3; we can then plug these
sub-configurations into the appropriate holes of 2
and obtain a configuration for the entire graph.
Notice that charts can be exponentially larger
than the original graph, but they are still expo-
nentially smaller than the entire set of readings
because common subgraphs (such as the graph
{2,5,7} in the example) are represented only once,
411
1 2 3
4 5 6 7
h
2
h
1
h
4
h
3
h
6
h
5
1 3
4 5 6 7
h
2
h
1
h
6
h
5
? ?
1 3
4 5 6 7
2
1 3
4 5 6 7
?
Figure 3: Extracting a configuration from a chart.
and are small in practice (see (Koller and Thater,
2005b) for an analysis). Thus the chart can still
serve as an underspecified representation.
3 Equivalence
Now let?s define equivalence of readings more
precisely. Equivalence of semantic representations
is traditionally defined as the relation between
formulas (say, of first-order logic) which have
the same interpretation. However, even first-order
equivalence is an undecidable problem, and broad-
coverage semantic representations such as those
computed by the ERG usually have no well-
defined model-theoretic semantics and therefore
no concept of semantic equivalence.
On the other hand, we do not need to solve
the full semantic equivalence problem, as we only
want to compare formulas that are readings of the
same sentence, i.e. different configurations of the
same USR. Such formulas only differ in the way
that the fragments are combined. We can therefore
approximate equivalence by using a rewrite system
that permutes fragments and defining equivalence
of configurations as mutual rewritability as usual.
By way of example, consider again the two con-
figurations shown in Fig. 1. We can obtain the sec-
ond configuration from the (semantically equiva-
lent) first one by applying the following rewrite
rule, which rotates the fragments 1 and 2:
ax(az(P,Q),R)? az(P,ax(Q,R)) (3)
Thus we take these two configurations to be
equivalent with respect to the rewrite rule. (We
could also have argued that the second configura-
tion can be rewritten into the first by using the in-
verted rule.)
We formalise this rewriting-based notion of
equivalence as follows. The definition uses the ab-
breviation x[1,k) for the sequence x1, . . . ,xk?1, and
x(k,n] for xk+1, . . . ,xn.
Definition 3. A permutation system R is a system
of rewrite rules over the signature ? of the follow-
ing form:
f1(x[1,i), f2(y[1,k),z,y(k,m]),x(i,n])?
f2(y[1,k), f1(x[1,i),z,x(i,n]),y(k,m])
The permutability relation P(R) is the binary rela-
tion P(R) ? (??N)2 which contains exactly the
tuples (( f1, i),( f2,k)) and (( f2,k),( f1, i)) for each
such rewrite rule. Two terms are equivalent with re-
spect to R, s?R t, iff there is a sequence of rewrite
steps and inverse rewrite steps that rewrite s into t.
If G is a graph over ? and R a permutation sys-
tem, then we write SCR(G) for the set of equiva-
lence classes Conf(G)/?R, where Conf(G) is the
set of configurations of G.
The rewrite rule (3) above is an instance of this
schema, as are the other three permutations of ex-
istential quantifiers. These rules approximate clas-
sical semantic equivalence of first-order logic, as
they rewrite formulas into classically equivalent
ones. Indeed, all five configurations of the graph
in Fig. 1 are rewriting-equivalent to each other.
In the case of the semantic representations gen-
erated by the ERG, we don?t have access to an
underlying interpretation. But we can capture lin-
guistic intuitions about the equivalence of readings
in permutation rules. For instance, proper names
and pronouns (which the ERG analyses as scope-
bearers, although they can be reduced to constants
without scope) can be permuted with anything. In-
definites and definites permute with each other if
they occur in each other?s scope, but not if they
occur in each other?s restriction; and so on.
4 Redundancy elimination
Given a permutation system, we can now try to get
rid of readings that are equivalent to other readings.
One way to formalise this is to enumerate exactly
one representative of each equivalence class. How-
ever, after such a step we would be left with a col-
lection of semantic representations rather than an
USR, and could not use the USR for ruling out
further readings. Besides, a naive algorithm which
412
first enumerates all configurations would be pro-
hibitively slow.
We will instead tackle the following underspec-
ified redundancy elimination problem: Given an
USR G, compute an USR G? with Conf(G?) ?
Conf(G) and SCR(G) = SCR(G?). We want
Conf(G?) to be as small as possible. Ideally, it
would contain no two equivalent readings, but in
practice we won?t always achieve this kind of com-
pleteness. Our redundancy elimination algorithm
will operate on a dominance chart and successively
delete splits and subgraphs from the chart.
4.1 Permutable fragments
Because the algorithm must operate on USRs
rather than configurations, it needs a way to pre-
dict from the USR alone which fragments can be
permuted in configurations. This is not generally
possible in unrestricted graphs, but for hnc graphs
it is captured by the following criterion.
Definition 4. Let R be a permutation system. Two
fragments F1 and F2 with root labels f1 and f2
in a hnc graph G are called R-permutable iff
they are possible dominators of each other and
(( f1,ch(F1,F2)),( f2,ch(F2,F1))) ? P(R).
For example, in Fig. 1, the fragments 1 and 2
are permutable, and indeed they can be permuted
in any configuration in which one is the parent of
the other. This is true more generally:
Lemma 2 (Koller and Thater (2006)). Let G be a
hnc graph, F1 and F2 be R-permutable fragments
with root labels f1 and f2, and C1 any config-
uration of G of the form C( f1(. . . , f2(. . .), . . .))
(where C is the context of the subterm). Then
C1 can be R-rewritten into a tree C2 of the form
C( f2(. . . , f1(. . .), . . .)) which is also a configura-
tion of G.
The proof uses the hn connectedness ofG in two
ways: in order to ensure that C2 is still a configu-
ration of G, and to make sure that F2 is plugged
into the correct hole of F1 for a rule application
(cf. Lemma 1). Note thatC2 ?R C1 by definition.
4.2 The redundancy elimination algorithm
Now we can use permutability of fragments to
define eliminable splits. Intuitively, a split of a
subgraph G is eliminable if each of its configura-
tions is equivalent to a configuration of some other
split of G. Removing such a split from the chart
will rule out some configurations; but it does not
change the set of equivalence classes.
Definition 5. Let R be a permutation system. A
split S= (F, . . . ,hi 7?Gi, . . .) of a graph G is called
eliminable in a chartCh if some Gi contains a frag-
ment F ? such that (a) Ch contains a split S? of G
with root fragment F ?, and (b) F ? is R-permutable
with F and all possible dominators of F ? in Gi.
In Fig. 1, each of the three splits is eliminable.
For example, the split with root fragment 1 is elim-
inable because the fragment 3 permutes both with
2 (which is the only possible dominator of 3 in the
same wcc) and with 1 itself.
Proposition 3. Let Ch be a dominance chart, and
let S be an eliminable split of a hnc subgraph. Then
SC(Ch) = SC(Ch?S).
Proof. Let C be an arbitrary configuration of S =
(F,h1 7? G1, . . . ,hn 7? Gn), and let F ? ? Gi be the
root fragment of the assumed second split S?.
Let F1, . . . ,Fn be those fragments in C that are
properly dominated by F and properly dominate
F ?. All of these fragments must be possible domi-
nators of F ?, and all of them must be in Gi as well,
so F ? is permutable with each of them. F ? must
also be permutable with F . This means that we can
apply Lemma 2 repeatedly to move F ? to the root
of the configuration, obtaining a configuration of
S? which is equivalent toC.
Notice that we didn?t require that Ch must be
the complete chart of a dominance graph. This
means we can remove eliminable splits from a
chart repeatedly, i.e. we can apply the following
redundancy elimination algorithm:
REDUNDANCY-ELIMINATION(Ch,R)
1 for each split S inCh
2 do if S is eliminable with respect to R
3 then remove S fromCh
Prop. 3 shows that the algorithm is a correct
algorithm for the underspecified redundancy
elimination problem. The particular order in
which eliminable splits are removed doesn?t
affect the correctness of the algorithm, but it may
change the number of remaining configurations.
The algorithm generalises an earlier elimination
algorithm (Koller and Thater, 2006) in that the
earlier algorithm required the existence of a single
split which could be used to establish eliminability
of all other splits of the same subgraph.
We can further optimise this algorithm by keep-
ing track of how often each subgraph is referenced
413
every
z
D
x,y,z
a
y
a
x
1 2 3
A
x
B
y
C
z
4 5 6
7
Figure 4: A graph for which the algorithm is not
complete.
by the splits in the chart. Once a reference count
drops to zero, we can remove the entry for this
subgraph and all of its splits from the chart. This
doesn?t change the set of configurations of the
chart, but may further reduce the chart size. The
overall runtime for the algorithm is O(n2S), where
S is the number of splits in Ch and n is the num-
ber of nodes in the graph. This is asymptotically
not much slower than the runtime O((n+m)S) it
takes to compute the chart in the first place (where
m is the number of edges in the graph).
4.3 Examples and discussion
Let?s look at a run of the algorithm on the chart
in Fig. 2. The algorithm can first delete the elim-
inable split with root 1 for the entire graphG. After
this deletion, the splits for G with root fragments
2 and 3 are still eliminable; so we can e.g. delete
the split for 3. At this point, only one split is left
for G. The last split for a subgraph can never be
eliminable, so we are finished with the splits for
G. This reduces the reference count of some sub-
graphs (e.g. {2,3,5,6,7}) to 0, so we can remove
these subgraphs too. The output of the algorithm is
the chart shown below, which represents a single
configuration (the one shown in Fig. 3).
{1,2,3,4,5,6,7} :?2,h2 7? {1,4},h4 7? {3,6,7}?
{1,4} :?1,h1 7? {4}?
{3,6,7} :?3,h5 7? {6},h6 7? {7}?
In this case, the algorithm achieves complete re-
duction, in the sense that the final chart has no two
equivalent configurations. It remains complete for
all variations of the graph in Fig. 1 in which some
or all existential quantifiers are replaces by univer-
sal quantifiers. This is an improvement over our
earlier algorithm (Koller and Thater, 2006), which
computed a chart with four configurations for the
graph in which 1 and 2 are existential and 3 is uni-
versal, as opposed to the three equivalence classes
of this graph?s configurations.
However, the present algorithm still doesn?t
achieve complete reduction for all USRs. One ex-
ample is shown in Fig. 4. This graph has six config-
urations in four equivalence classes, but no split of
the whole graph is eliminable. The algorithm will
delete a split for the subgraph {1,2,4,5,7}, but the
final chart will still have five, rather than four, con-
figurations. A complete algorithm would have to
recognise that {1,3,4,6,7} and {2,3,5,6,7} have
splits (for 1 and 2, respectively) that lead to equiv-
alent configurations and delete one of them. But
it is far from obvious how such a non-local deci-
sion could be made efficiently, and we leave this
for future work.
5 Evaluation
In this final section, we evaluate the the effective-
ness and efficiency of the elimination algorithm:
We run it on USRs from a treebank and measure
how many readings are redundant, to what extent
the algorithm eliminates this redundancy, and how
much time it takes to do this.
Resources. The experiments are based on the
Rondane corpus, a Redwoods (Oepen et al, 2002)
style corpus which is distributed with the English
Resource Grammar (Flickinger, 2002). The cor-
pus contains analyses for 1076 sentences from the
tourism domain, which are associated with USRs
based upon Minimal Recursion Semantics (MRS).
The MRS representations are translated into dom-
inance graphs using the open-source utool tool
(Koller and Thater, 2005a), which is restricted to
MRS representations whose translations are hnc.
By restricting ourselves to such MRSs, we end up
with a data set of 999 dominance graphs. The aver-
age number of scope bearing operators in the data
set is 6.5, and the median number of readings is 56.
We then defined a (rather conservative) rewrite
system RERG for capturing the permutability rela-
tion of the quantifiers in the ERG. This amounted
to 34 rule schemata, which are automatically ex-
panded to 494 rewrite rules.
Experiment: Reduction. We first analysed the
extent to which our algorithm eliminated the re-
dundancy of the USRs in the corpus. We com-
puted dominance charts for all USRs, ran the al-
gorithm on them, and counted the number of con-
figurations of the reduced charts. We then com-
pared these numbers against a baseline and an up-
per bound. The upper bound is the true number of
414
110
100
1000
10000
100000
0 1 2 3 4 5 6 7 8 9 10 11 12 13
log(#configurations)
F
a
c
t
o
r
Algorithm Baseline Classes
Figure 5: Mean reduction factor on Rondane.
equivalence classes with respect to RERG; for effi-
ciency reasons we could only compute this num-
ber for USRs with up to 500.000 configurations
(95% of the data set). The baseline is given by
the number of readings that remain if we replace
proper names and pronouns by constants and vari-
ables, respectively. This simple heuristic is easy to
compute, and still achieves nontrivial redundancy
elimination because proper names and pronouns
are quite frequent (28% of the noun phrase occur-
rences in the data set). It also shows the degree of
non-trivial scope ambiguity in the corpus.
For each measurement, we sorted the USRs ac-
cording to the number N of configurations, and
grouped USRs according to the natural logarithm
of N (rounded down) to obtain a logarithmic scale.
First, we measured the mean reduction factor
for each log(N) class, i.e. the ratio of the num-
ber of all configurations to the number of remain-
ing configurations after redundancy elimination
(Fig. 5). The upper-bound line in the figure shows
that there is a great deal of redundancy in the USRs
in the data set. The average performance of our
algorithm is close to the upper bound and much
0%
20%
40%
60%
80%
100%
0 1 2 3 4 5 6 7 8 9 10 11 12 13
log(#configurations)
Algorithm Baseline
Figure 6: Percentage of USRs for which the algo-
rithm and the baseline achieve complete reduction.
0
1
10
100
1000
10000
0 1 2 3 4 5 6 7 8 9 10 11 12 13
log(#configurations)
t
i
m
e
 
(
m
s
)
Full Chart Reduced Chart Enumeration
Figure 7: Mean runtimes.
better than the baseline. For USRs with fewer than
e8 = 2980 configurations (83% of the data set), the
mean reduction factor of our algorithm is above
86% of the upper bound. The median number
of configurations for the USRs in the whole data
set is 56, and the median number of equivalence
classes is 3; again, the median number of config-
urations of the reduced charts is very close to the
upper bound, at 4 (baseline: 8). The highest reduc-
tion factor for an individual USR is 666.240.
We also measured the ratio of USRs for which
the algorithm achieves complete reduction (Fig. 6):
The algorithm is complete for 56% of the USRs
in the data set. It is complete for 78% of the USRs
with fewer than e5 = 148 configurations (64% of
the data set), and still complete for 66% of the
USRs with fewer than e8 configurations.
Experiment: Efficiency. Finally, we measured
the runtime of the elimination algorithm. The run-
time of the elimination algorithm is generally com-
parable to the runtime for computing the chart in
the first place. However, in our experiments we
used an optimised version of the elimination algo-
rithm, which computes the reduced chart directly
from a dominance graph by checking each split
for eliminability before it is added to the chart.
We compare the performance of this algorithm to
the baseline of computing the complete chart. For
comparison, we have also added the time it takes
to enumerate all configurations of the graph, as a
lower bound for any algorithm that computes the
equivalence classes based on the full set of config-
urations. Fig. 7 shows the mean runtimes for each
log(N) class, on the USRs with less than one mil-
lion configurations (958 USRs).
As the figure shows, the asymptotic runtimes
for computing the complete chart and the reduced
chart are about the same, whereas the time for
415
enumerating all configurations grows much faster.
(Note that the runtime is reported on a logarithmic
scale.) For USRs with many configurations, com-
puting the reduced chart actually takes less time
on average than computing the complete chart
because the chart-filling algorithm is called on
fewer subgraphs. While the reduced-chart algo-
rithm seems to be slower than the complete-chart
one for USRs with less than e5 configurations,
these runtimes remain below 20 milliseconds on
average, and the measurements are thus quite un-
reliable. In summary, we can say that there is no
overhead for redundancy elimination in practice.
6 Conclusion
We presented an algorithm for redundancy elimina-
tion on underspecified chart representations. This
algorithm successively deletes eliminable splits
from the chart, which reduces the set of described
readings while making sure that at least one rep-
resentative of each original equivalence class re-
mains. Equivalence is defined with respect to a cer-
tain class of rewriting systems; this definition ap-
proximates semantic equivalence of the described
formulas and fits well with the underspecification
setting. The algorithm runs in polynomial time in
the size of the chart.
We then evaluated the algorithm on the Ron-
dane corpus and showed that it is useful in practice:
the median number of readings drops from 56 to
4, and the maximum individual reduction factor is
666.240. The algorithm achieves complete reduc-
tion for 56% of all sentences. It does this in neg-
ligible runtime; even the most difficult sentences
in the corpus are reduced in a matter of seconds,
whereas the enumeration of all readings would
take about a year. This is the first corpus evalua-
tion of a redundancy elimination in the literature.
The algorithm improves upon previous work
(Koller and Thater, 2006) in that it eliminates more
splits from the chart. It is an improvement over ear-
lier algorithms for enumerating irredundant read-
ings (Vestre, 1991; Chaves, 2003) in that it main-
tains underspecifiedness; note that these earlier pa-
pers never made any claims with respect to, or eval-
uated, completeness.
There are a number of directions in which the
present algorithm could be improved. We are cur-
rently pursuing some ideas on how to improve the
completeness of the algorithm further. It would
also be worthwhile to explore heuristics for the or-
der in which splits of the same subgraph are elim-
inated. The present work could be extended to al-
low equivalence with respect to arbitrary rewrite
systems. Most generally, we hope that the methods
developed here will be useful for defining other
elimination algorithms, which take e.g. full world
knowledge into account.
References
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn, J. Niehren,
and S. Thiel. 2003. An efficient graph algorithm for dom-
inance constraints. Journal of Algorithms, 48:194?219.
P. Blackburn and J. Bos. 2005. Representation and Inference
for Natural Language. A First Course in Computational
Semantics. CSLI Publications.
R. P. Chaves. 2003. Non-redundant scope disambiguation
in underspecified semantics. In Proc. 8th ESSLLI Student
Session.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag. 2004.
Minimal recursion semantics: An introduction. Journal of
Language and Computation. To appear.
M. Egg, A. Koller, and J. Niehren. 2001. The Constraint
Language for Lambda Structures. Logic, Language, and
Information, 10.
D. Flickinger. 2002. On building a more efficient grammar
by exploiting types. In J. Tsujii S. Oepen, D. Flickinger
and H. Uszkoreit, editors, Collaborative Language Engi-
neering. CSLI Publications, Stanford.
R. Fuchss, A. Koller, J. Niehren, and S. Thater. 2004. Mini-
mal recursion semantics as dominance constraints: Trans-
lation, evaluation, and analysis. In Proc. of the 42nd ACL.
A. Koller and S. Thater. 2005a. Efficient solving and ex-
ploration of scope ambiguities. In ACL-05 Demonstration
Notes, Ann Arbor.
A. Koller and S. Thater. 2005b. The evolution of dominance
constraint solvers. In Proceedings of the ACL-05 Work-
shop on Software, Ann Arbor.
A. Koller and S. Thater. 2006. Towards a redundancy elimi-
nation algorithm for underspecified descriptions. In Proc.
5th Intl. Workshop on Inference in Computational Seman-
tics (ICoS-5).
A. Koller, J. Niehren, and S. Thater. 2003. Bridging the gap
between underspecification formalisms: Hole semantics as
dominance constraints. In Proc. 10th EACL.
J. Niehren and S. Thater. 2003. Bridging the gap between
underspecification formalisms: Minimal recursion seman-
tics as dominance constraints. In Proc. of the 41st ACL.
S. Oepen, K. Toutanova, S. Shieber, C. Manning,
D. Flickinger, and T. Brants. 2002. The LinGO Red-
woods treebank: Motivation and preliminary applications.
In Proceedings of COLING?02.
K. van Deemter and S. Peters. 1996. Semantic Ambiguity
and Underspecification. CSLI, Stanford.
E. Vestre. 1991. An algorithm for generating non-redundant
quantifier scopings. In Proc. of the Fifth EACL, Berlin.
416
Proceedings of ACL-08: HLT, pages 218?226,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Regular tree grammars as a formalism for scope underspecification
Alexander Koller?
a.koller@ed.ac.uk
? University of Edinburgh
Michaela Regneri? ?
regneri@coli.uni-sb.de
? University of Groningen
Stefan Thater?
stth@coli.uni-sb.de
? Saarland University
Abstract
We propose the use of regular tree grammars
(RTGs) as a formalism for the underspecified
processing of scope ambiguities. By applying
standard results on RTGs, we obtain a novel
algorithm for eliminating equivalent readings
and the first efficient algorithm for computing
the best reading of a scope ambiguity. We also
show how to derive RTGs from more tradi-
tional underspecified descriptions.
1 Introduction
Underspecification (Reyle, 1993; Copestake et al,
2005; Bos, 1996; Egg et al, 2001) has become the
standard approach to dealing with scope ambiguity
in large-scale hand-written grammars (see e.g. Cope-
stake and Flickinger (2000)). The key idea behind
underspecification is that the parser avoids comput-
ing all scope readings. Instead, it computes a single
compact underspecified description for each parse.
One can then strengthen the underspecified descrip-
tion to efficiently eliminate subsets of readings that
were not intended in the given context (Koller and
Niehren, 2000; Koller and Thater, 2006); so when
the individual readings are eventually computed, the
number of remaining readings is much smaller and
much closer to the actual perceived ambiguity of the
sentence.
In the past few years, a ?standard model? of scope
underspecification has emerged: A range of for-
malisms from Underspecified DRT (Reyle, 1993)
to dominance graphs (Althaus et al, 2003) have
offered mechanisms to specify the ?semantic mate-
rial? of which the semantic representations are built
up, plus dominance or outscoping relations between
these building blocks. This has been a very suc-
cessful approach, but recent algorithms for elimi-
nating subsets of readings have pushed the expres-
sive power of these formalisms to their limits; for
instance, Koller and Thater (2006) speculate that
further improvements over their (incomplete) redun-
dancy elimination algorithm require a more expres-
sive formalism than dominance graphs. On the theo-
retical side, Ebert (2005) has shown that none of
the major underspecification formalisms are expres-
sively complete, i.e. supports the description of an
arbitrary subset of readings. Furthermore, the some-
what implicit nature of dominance-based descrip-
tions makes it difficult to systematically associate
readings with probabilities or costs and then com-
pute a best reading.
In this paper, we address both of these shortcom-
ings by proposing regular tree grammars (RTGs)
as a novel underspecification formalism. Regular
tree grammars (Comon et al, 2007) are a standard
approach for specifying sets of trees in theoretical
computer science, and are closely related to regu-
lar tree transducers as used e.g. in recent work on
statistical MT (Knight and Graehl, 2005) and gram-
mar formalisms (Shieber, 2006). We show that the
?dominance charts? proposed by Koller and Thater
(2005b) can be naturally seen as regular tree gram-
mars; using their algorithm, classical underspecified
descriptions (dominance graphs) can be translated
into RTGs that describe the same sets of readings.
However, RTGs are trivially expressively complete
because every finite tree language is also regular. We
exploit this increase in expressive power in present-
ing a novel redundancy elimination algorithm that is
simpler and more powerful than the one by Koller
and Thater (2006); in our algorithm, redundancy
elimination amounts to intersection of regular tree
languages. Furthermore, we show how to define a
PCFG-style cost model on RTGs and compute best
readings of deterministic RTGs efficiently, and illus-
trate this model on a machine learning based model
218
of scope preferences (Higgins and Sadock, 2003).
To our knowledge, this is the first efficient algorithm
for computing best readings of a scope ambiguity in
the literature.
The paper is structured as follows. In Section 2,
we will first sketch the existing standard approach
to underspecification. We will then define regular
tree grammars and show how to see them as an un-
derspecification formalism in Section 3. We will
present the new redundancy elimination algorithm,
based on language intersection, in Section 4, and
show how to equip RTGs with weights and compute
best readings in Section 5. We conclude in Section 6.
2 Underspecification
The key idea behind scope underspecification is to
describe all readings of an ambiguous expression
with a single, compact underspecified representation
(USR). This simplifies semantics construction, and
current algorithms (Koller and Thater, 2005a) sup-
port the efficient enumeration of readings from an
USR when it is necessary. Furthermore, it is possible
to perform certain semantic processing tasks such
as eliminating redundant readings (see Section 4) di-
rectly on the level of underspecified representations
without explicitly enumerating individual readings.
Under the ?standard model? of scope underspeci-
fication, readings are considered as formulas or trees.
USRs specify the ?semantic material? common to
all readings, plus dominance or outscopes relations
between these building blocks. In this paper, we con-
sider dominance graphs (Egg et al, 2001; Althaus
et al, 2003) as one representative of this class. An
example dominance graph is shown on the left of
Fig. 1. It represents the five readings of the sentence
?a representative of a company saw every sample.?
The (directed, labelled) graph consists of seven sub-
trees, or fragments, plus dominance edges relating
nodes of these fragments. Each reading is encoded
as one configuration of the dominance graph, which
can be obtained by ?plugging? the tree fragments
into each other, in a way that respects the dominance
edges: The source node of each dominance edge
must dominate (i.e., be an ancestor of) the target
node in each configuration. The trees in Fig. 1a?e
are the five configurations of the example graph.
An important class of dominance graphs are hy-
pernormally connected dominance graphs, or dom-
inance nets (Niehren and Thater, 2003). The pre-
cise definition of dominance nets is not important
here, but note that virtually all underspecified de-
scriptions that are produced by current grammars are
nets (Flickinger et al, 2005). For the rest of the pa-
per, we restrict ourselves to dominance graphs that
are hypernormally connected.
3 Regular tree grammars
We will now recall the definition of regular tree
grammars and show how they can be used as an un-
derspecification formalism.
3.1 Definition
Let ? be an alphabet, or signature, of tree construc-
tors { f ,g,a, . . .}, each of which is equipped with an
arity ar( f )? 0. A finite constructor tree t is a finite
tree in which each node is labelled with a symbol of
?, and the number of children of the node is exactly
the arity of this symbol. For instance, the configura-
tions in Fig. 1a-e are finite constructor trees over the
signature {ax|2,ay|2,compz|0, . . .}. Finite construc-
tor trees can be seen as ground terms over ? that
respect the arities. We write T (?) for the finite con-
structor trees over ?.
A regular tree grammar (RTG) is a 4-tuple G =
(S,N,?,R) consisting of a nonterminal alphabet N,
a terminal alphabet ?, a start symbol S ? N, and a
finite set of production rules R of the form A? ? ,
where A ? N and ? ? T (??N); the nonterminals
count as zero-place constructors. Two finite con-
structor trees t, t ? ? T (? ? N) stand in the deriva-
tion relation, t ?G t ?, if t ? can be built from t by
replacing an occurrence of some nonterminal A by
the tree on the right-hand side of some production
for A. The language generated by G, L(G), is the set
{t ? T (?) | S??G t}, i.e. all terms of terminal sym-
bols that can be derived from the start symbol by a
sequence of rule applications. Note that L(G) is a
possibly infinite language of finite trees. As usual,
we write A? t1 | . . . | tn as shorthand for the n pro-
duction rules A? ti (1 ? i ? n). See Comon et al
(2007) for more details.
The languages that can be accepted by regular tree
grammars are called regular tree languages (RTLs),
and regular tree grammars are equivalent to regular
219
every
y
sample
y
see
x,y
a
x
repr-of
x,z
a
z
comp
z
12 3
4 5 6
7
every
y
a
x
sample
y
see
x,y
repr-of
x,z
a
z
comp
z
(a)
every
y
a
z
a
x
sample
y
see
x,y
comp
z
repr-of
x,z
(c)
every
y
a
z
a
x
sample
y
see
x,y
comp
z
repr-of
x,z
(d)(b)
every
y
sample
y
see
x,y
a
x
repr-of
x,z
a
z
comp
z
(e)
every
y
sample
y
a
x
repr-of
x,z
see
x,y
a
z
comp
z
Figure 1: A dominance graph (left) and its five configurations.
tree automata, which are defined essentially like the
well-known regular string automata, except that they
assign states to the nodes in a tree rather than the po-
sitions in a string. Tree automata are related to tree
transducers as used e.g. in statistical machine trans-
lation (Knight and Graehl, 2005) exactly like finite-
state string automata are related to finite-state string
transducers, i.e. they use identical mechanisms to ac-
cept rather than transduce languages. Many theoreti-
cal results carry over from regular string languages
to regular tree languages; for instance, membership
of a tree in a RTL can be decided in linear time,
RTLs are closed under intersection, union, and com-
plement, and so forth.
3.2 Regular tree grammars in
underspecification
We can now use regular tree grammars in underspeci-
fication by representing the semantic representations
as trees and taking an RTG G as an underspecified
description of the trees in L(G). For example, the
five configurations in Fig. 1 can be represented as
the tree language accepted by the following gram-
mar with start symbol S.
S ? ax(A1,A2) | az(B1,A3) | everyy(B3,A4)
A1 ? az(B1,B2)
A2 ? everyy(B3,B4)
A3 ? ax(B2,A2) | everyy(B3,A5)
A4 ? ax(A1,B4) | az(B1,A5)
A5 ? ax(B2,B4)
B1 ? compz B2 ? repr-ofx,z
B3 ? sampley B4 ? seex,y
More generally, every finite set of trees can be
written as the tree language accepted by a non-
recursive regular tree grammar such as this. This
grammar can be much smaller than the set of trees,
because nonterminal symbols (which stand for sets
of possibly many subtrees) can be used on the right-
hand sides of multiple rules. Thus an RTG is a com-
pact representation of a set of trees in the same way
that a parse chart is a compact representation of the
set of parse trees of a context-free string grammar.
Note that each tree can be enumerated from the RTG
in linear time.
3.3 From dominance graphs to tree grammars
Furthermore, regular tree grammars can be system-
atically computed from more traditional underspeci-
fied descriptions. Koller and Thater (2005b) demon-
strate how to compute a dominance chart from a
dominance graph D by tabulating how a subgraph
can be decomposed into smaller subgraphs by re-
moving what they call a ?free fragment?. If D is
hypernormally connected, this chart can be read as
a regular tree grammar whose nonterminal symbols
are subgraphs of the dominance graph, and whose
terminal symbols are names of fragments. For the
example graph in Fig. 1, it looks as follows.
{1,2,3,4,5,6,7} ? 1({2,4,5},{3,6,7})
{1,2,3,4,5,6,7} ? 2({4},{1,3,5,6,7})
{1,2,3,4,5,6,7} ? 3({6},{1,2,4,5,7})
{1,3,5,6,7} ? 1({5},{3,6,7}) | 3({6},{1,5,7})
{1,2,4,5,7} ? 1({2,4,5},{7}) | 2({4},{1,5,7})
{1,5,7} ? 1({5},{7})
{2,4,5} ? 2({4},{5}) {4} ? 4 {6}? 6
{3,6,7} ? 3({6},{7}) {5} ? 5 {7}? 7
This grammar accepts, again, five different trees,
whose labels are the node names of the dominance
graph, for instance 1(2(4,5),3(6,7)). If f : ?? ??
is a relabelling function from one terminal alpha-
bet to another, we can write f (G) for the grammar
(S,N,??,R?), where R? = {A ? f (a)(B1, . . . ,Bn) |
A? a(B1, . . . ,Bn) ? R}. Now if we choose f to be
the labelling function of D (which maps node names
to node labels) and G is the chart of D, then L( f (G))
will be the set of configurations of D. The grammar
in Section 3.2 is simply f (G) for the chart above (up
to consistent renaming of nonterminals).
In the worst case, the dominance chart of a dom-
inance graph with n fragments has O(2n) produc-
tion rules (Koller and Thater, 2005b), i.e. charts may
be exponential in size; but note that this is still an
220
1,0E+00
1,0E+04
1,0E+08
1,0E+12
1,0E+16
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33
#fragments
#
c
o
n
f
i
g
u
r
a
t
i
o
n
s
/
r
u
l
e
s
0
10
20
30
40
50
60
70
80
#
s
e
n
t
e
n
c
e
s
#sentences
#production rules in chart
#configurations
Figure 2: Chart sizes in the Rondane corpus.
improvement over the n! configurations that these
worst-case examples have. In practice, RTGs that
are computed by converting the USR computed by a
grammar remain compact: Fig. 2 compares the aver-
age number of configurations and the average num-
ber of RTG production rules for USRs of increasing
sizes in the Rondane treebank (see Sect. 4.3); the
bars represent the number of sentences for USRs of a
certain size. Even for the most ambiguous sentence,
which has about 4.5?1012 scope readings, the domi-
nance chart has only about 75 000 rules, and it takes
only 15 seconds on a modern consumer PC (Intel
Core 2 Duo at 2 GHz) to compute the grammar from
the graph. Computing the charts for all 999 MRS-
nets in the treebank takes about 45 seconds.
4 Expressive completeness and
redundancy elimination
Because every finite tree language is regular, RTGs
constitute an expressively complete underspecifica-
tion formalism in the sense of Ebert (2005): They
can represent arbitrary subsets of the original set of
readings. Ebert shows that the classical dominance-
based underspecification formalisms, such as MRS,
Hole Semantics, and dominance graphs, are all
expressively incomplete, which Koller and Thater
(2006) speculate might be a practical problem for al-
gorithms that strengthen USRs to remove unwanted
readings. We will now show how both the expres-
sive completeness and the availability of standard
constructions for RTGs can be exploited to get an
improved redundancy elimination algorithm.
4.1 Redundancy elimination
Redundancy elimination (Vestre, 1991; Chaves,
2003; Koller and Thater, 2006) is the problem of de-
riving from an USR U another USR U ?, such that
the readings of U ? are a proper subset of the read-
ings of U , but every reading in U is semantically
equivalent to some reading in U ?. For instance, the
following sentence from the Rondane treebank is an-
alyzed as having six quantifiers and 480 readings by
the ERG grammar; these readings fall into just two
semantic equivalence classes, characterized by the
relative scope of ?the lee of? and ?a small hillside?.
A redundancy elimination would therefore ideally re-
duce the underspecified description to one that has
only two readings (one for each class).
(1) We quickly put up the tents in the lee of a
small hillside and cook for the first time in the
open. (Rondane 892)
Koller and Thater (2006) define semantic equiva-
lence in terms of a rewrite system that specifies un-
der what conditions two quantifiers may exchange
their positions without changing the meaning of the
semantic representation. For example, if we assume
the following rewrite system (with just a single rule),
the five configurations in Fig. 1a-e fall into three
equivalence classes ? indicated by the dotted boxes
around the names a-e ? because two pairs of read-
ings can be rewritten into each other.
(2) ax(az(P,Q),R)? az(P,ax(Q,R))
Based on this definition, Koller and Thater (2006)
present an algorithm (henceforth, KT06) that deletes
rules from a dominance chart and thus removes sub-
sets of readings from the USR. The KT06 algorithm
is fast and quite effective in practice. However, it es-
sentially predicts for each production rule of a dom-
inance chart whether each configuration that can be
built with this rule is equivalent to a configuration
that can be built with some other production for the
same subgraph, and is therefore rather complex.
4.2 Redundancy elimination as language
intersection
We now define a new algorithm for redundancy elim-
ination. It is based on the intersection of regular tree
languages, and will be much simpler and more pow-
erful than KT06.
Let G = (S,N,?,R) be an RTG with a linear or-
der on the terminals ?; for ease of presentation, we
assume ? ? N. Furthermore, let f : ?? ?? be a re-
labelling function into the signature ?? of the rewrite
221
system. For example, G could be the dominance
chart of some dominance graph D, and f could be
the labelling function of D.
We can then define a tree language LF as follows:
LF contains all trees over ? that do not contain a sub-
tree of the form q1(x1, . . . ,xi?1,q2(. . .),xi+1, . . . ,xk)
where q1 > q2 and the rewrite system contains a rule
that has f (q1)(X1, . . . ,Xi?1, f (q2)(. . .),Xi+1, . . . ,Xk)
on the left or right hand side. LF is a regular tree lan-
guage, and can be accepted by a regular tree gram-
mar GF with O(n) nonterminals and O(n2) rules,
where n = |??|. A filter grammar for Fig. 1 looks
as follows:
S ? 1(S,S) | 2(S,Q1) | 3(S,S) | 4 | . . . | 7
Q1 ? 2(S,Q1) | 3(S,S) | 4 | . . . | 7
This grammar accepts all trees over ? except ones
in which a node with label 2 is the parent of a node
with label 1, because such trees correspond to config-
urations in which a node with label az is the parent of
a node with label ax, az and ax are permutable, and
2 > 1. In particular, it will accept the configurations
(b), (c), and (e) in Fig. 1, but not (a) or (d).
Since regular tree languages are closed under in-
tersection, we can compute a grammar G? such that
L(G?) = L(G)?LF . This grammar has O(nk) nonter-
minals and O(n2k) productions, where k is the num-
ber of production rules in G, and can be computed
in time O(n2k). The relabelled grammar f (G?) ac-
cepts all trees in which adjacent occurrences of per-
mutable quantifiers are in a canonical order (sorted
from lowest to highest node name). For example, the
grammar G? for the example looks as follows; note
that the nonterminal alphabet of G? is the product of
the nonterminal alphabets of G and GF .
{1,2,3,4,5,6,7}S ? 1({2,4,5}S,{3,6,7}S)
{1,2,3,4,5,6,7}S ? 2({4}S,{1,3,5,6,7}Q1)
{1,2,3,4,5,6,7}S ? 3({6}S,{1,2,4,5,7}S)
{1,3,5,6,7}Q1 ? 3({6}S,{1,5,7}S)
{1,2,4,5,7}S ? 1({2,4,5}S,{7}S)
{1,2,4,5,7}S ? 2({4}S,{1,5,7}Q1)
{2,4,5}S ? 2({4}S,{5}Q1) {4}S ? 4
{3,6,7}S ? 3({6}S,{7}S) {5}S ? 5
{1,5,7}S ? 1({5}S,{7}S) {5}Q1 ? 5
{6}S ? 6 {7}S ? 7
Significantly, the grammar contains no produc-
tions for {1,3,5,6,7}Q1 with terminal symbol 1, and
no production for {1,5,7}Q1 . This reduces the tree
language accepted by f (G?) to just the configura-
tions (b), (c), and (e) in Fig. 1, i.e. exactly one
representative of every equivalence class. Notice
that there are two different nonterminals, {5}Q1 and
{5}S, corresponding to the subgraph {5}, so the in-
tersected RTG is not a dominance chart any more.
As we will see below, this increased expressivity in-
creases the power of the redundancy elimination al-
gorithm.
4.3 Evaluation
The algorithm presented here is not only more trans-
parent than KT06, but also more powerful; for exam-
ple, it will reduce the graph in Fig. 4 of Koller and
Thater (2006) completely, whereas KT06 won?t.
To measure the extent to which the new algo-
rithm improves upon KT06, we compare both algo-
rithms on the USRs in the Rondane treebank (ver-
sion of January 2006). The Rondane treebank is a
?Redwoods style? treebank (Oepen et al, 2002) con-
taining MRS-based underspecified representations
for sentences from the tourism domain, and is dis-
tributed together with the English Resource Gram-
mar (ERG) (Copestake and Flickinger, 2000).
The treebank contains 999 MRS-nets, which we
translate automatically into dominance graphs and
further into RTGs; the median number of scope read-
ings per sentence is 56. For our experiment, we con-
sider all 950 MRS-nets with less than 650 000 con-
figurations. We use a slightly weaker version of the
rewrite system that Koller and Thater (2006) used in
their evaluation.
It turns out that the median number of equivalence
classes, computed by pairwise comparison of all con-
figurations, is 8. The median number of configu-
rations that remain after running our algorithm is
also 8. By contrast, the median number after run-
ning KT06 is 11. For a more fine-grained compari-
son, Fig. 3 shows the percentage of USRs for which
the two algorithms achieve complete reduction, i.e.
retain only one reading per equivalence class. In the
diagram, we have grouped USRs according to the
natural logarithm of their numbers of configurations,
and report the percentage of USRs in this group on
which the algorithms were complete. The new algo-
rithm dramatically outperforms KT06: In total, it re-
duces 96% of all USRs completely, whereas KT06
was complete only for 40%. This increase in com-
pleteness is partially due to the new algorithm?s abil-
ity to use non-chart RTGs: For 28% of the sentences,
222
0%
20%
40%
60%
80%
100%
1 3 5 7 9 11 13
KT06 RTG
Figure 3: Percentage of USRs in Rondane for which the
algorithms achieve complete reduction.
it computes RTGs that are not dominance charts.
KT06 was only able to reduce 5 of these 263 graphs
completely.
The algorithm needs 25 seconds to run for the
entire corpus (old algorithm: 17 seconds), and it
would take 50 (38) more seconds to run on the 49
large USRs that we exclude from the experiment.
By contrast, it takes about 7 hours to compute the
equivalence classes by pairwise comparison, and it
would take an estimated several billion years to com-
pute the equivalence classes of the excluded USRs.
In short, the redundancy elimination algorithm pre-
sented here achieves nearly complete reduction at a
tiny fraction of the runtime, and makes a useful task
that was completely infeasible before possible.
4.4 Compactness
Finally, let us briefly consider the ramifications of
expressive completeness on efficiency. Ebert (2005)
proves that no expressively complete underspecifi-
cation formalism can be compact, i.e. in the worst
case, the USR of a set of readings become exponen-
tially large in the number of scope-bearing operators.
In the case of RTGs, this worst case is achieved by
grammars of the form S? t1 | . . . | tn, where t1, . . . , tn
are the trees we want to describe. This grammar is as
big as the number of readings, i.e. worst-case expo-
nential in the number n of scope-bearing operators,
and essentially amounts to a meta-level disjunction
over the readings.
Ebert takes the incompatibility between compact-
ness and expressive completeness as a fundamental
problem for underspecification. We don?t see things
quite as bleakly. Expressions of natural language it-
self are (extremely underspecified) descriptions of
sets of semantic representations, and so Ebert?s ar-
gument applies to NL expressions as well. This
means that describing a given set of readings may
require an exponentially long discourse. Ebert?s def-
inition of compactness may be too harsh: An USR,
although exponential-size in the number of quanti-
fiers, may still be polynomial-size in the length of
the discourse in the worst case.
Nevertheless, the tradeoff between compactness
and expressive power is important for the design
of underspecification formalisms, and RTGs offer a
unique answer. They are expressively complete; but
as we have seen in Fig. 2, the RTGs that are derived
by semantic construction are compact, and even in-
tersecting them with filter grammars for redundancy
elimination only blows up their sizes by a factor of
O(n2). As we add more and more information to
an RTG to reduce the set of readings, ultimately to
those readings that were meant in the actual context
of the utterance, the grammar will become less and
less compact; but this trend is counterbalanced by
the overall reduction in the number of readings. For
the USRs in Rondane, the intersected RTGs are, on
average, 6% smaller than the original charts. Only
30% are larger than the charts, by a maximal factor
of 3.66. Therefore we believe that the theoretical
non-compactness should not be a major problem in
a well-designed practical system.
5 Computing best configurations
A second advantage of using RTGs as an under-
specification formalism is that we can apply exist-
ing algorithms for computing the best derivations
of weighted regular tree grammars to compute best
(that is, cheapest or most probable) configurations.
This gives us the first efficient algorithm for comput-
ing the preferred reading of a scope ambiguity.
We define weighted dominance graphs and
weighted tree grammars, show how to translate the
former into the latter and discuss an example.
5.1 Weighted dominance graphs
A weighted dominance graph D = (V,ET unionmulti ED unionmulti
WDunionmultiWI) is a dominance graph with two new types
of edges ? soft dominance edges, WD, and soft dis-
jointness edges, WI ?, each of which is equipped
with a numeric weight. Soft dominance and dis-
jointness edges provide a mechanism for assigning
weights to configurations; a soft dominance edge ex-
223
every
y
sample
y
see
x,y
a
x
repr-of
x,z
a
z
comp
z
1
2
3
4 5 6
7
9
8
Figure 4: The graph of Fig. 1 with soft constraints
presses a preference that two nodes dominate each
other in a configuration, whereas a soft disjointness
edge expresses a preference that two nodes are dis-
joint, i.e. neither dominates the other.
We take the hard backbone of D to be the ordinary
dominance graph B(D) = (V,ET unionmultiED) obtained by
removing all soft edges. The set of configurations
of a weighted graph D is the set of configurations
of its hard backbone. For each configuration t of
D, we define the weight c(t) to be the product of
the weights of all soft dominance and disjointness
edges that are satisfied in t. We can then ask for
configurations of maximal weight.
Weighted dominance graphs can be used to en-
code the standard models of scope preferences
(Pafel, 1997; Higgins and Sadock, 2003). For exam-
ple, Higgins and Sadock (2003) present a machine
learning approach for determining pairwise prefer-
ences as to whether a quantifier Q1 dominates an-
other quantifier Q2, Q2 dominates Q1, or neither (i.e.
they are disjoint). We can represent these numbers
as the weights of soft dominance and disjointness
edges. An example (with artificial weights) is shown
in Fig. 4; we draw the soft dominance edges as
curved dotted arrows and the soft disjointness edges
as as angled double-headed arrows. Each soft edge
is annotated with its weight. The hard backbone
of this dominance graph is our example graph from
Fig. 1, so it has the same five configurations. The
weighted graph assigns a weight of 8 to configura-
tion (a), a weight of 1 to (d), and a weight of 9 to (e);
this is also the configuration of maximum weight.
5.2 Weighted tree grammars
In order to compute the maximal-weight configura-
tion of a weighted dominance graph, we will first
translate it into a weighted regular tree grammar. A
weighted regular tree grammar (wRTG) (Graehl and
Knight, 2004) is a 5-tuple G = (S,N,?,R,c) such
that G? = (S,N,?,R) is a regular tree grammar and
c : R? R is a function that assigns each production
rule a weight. G accepts the same language of trees
as G?. It assigns each derivation a cost equal to the
product of the costs of the production rules used in
this derivation, and it assigns each tree in the lan-
guage a cost equal to the sum of the costs of its
derivations. Thus wRTGs define weights in a way
that is extremely similar to PCFGs, except that we
don?t require any weights to sum to one.
Given a weighted, hypernormally connected dom-
inance graph D, we can extend the chart of B(D) to
a wRTG by assigning rule weights as follows: The
weight of a rule D0 ? i(D1, . . . ,Dn) is the product
over the weights of all soft dominance and disjoint-
ness edges that are established by this rule. We say
that a rule establishes a soft dominance edge from
u to v if u = i and v is in one of the subgraphs
D1, . . . ,Dn; we say that it establishes a soft disjoint-
ness edge between u and v if u and v are in different
subgraphs D j and Dk ( j 6= k). It can be shown that
the weight this grammar assigns to each derivation
is equal to the weight that the original dominance
graph assigns to the corresponding configuration.
If we apply this construction to the example graph
in Fig. 4, we obtain the following wRTG:
{1, ...,7} ? ax({2,4,5},{3,6,7}) [9]
{1, ...,7} ? az({4},{1,3,5,6,7}) [1]
{1, ...,7} ? everyy({6},{1,2,4,5,7}) [8]
{2,4,5} ? az({4},{5}) [1]
{3,6,7} ? everyy({6},{7}) [1]
{1,3,5,6,7} ? ax({5},{3,6,7}) [1]
{1,3,5,6,7} ? everyy({6},{1,5,7}) [8]
{1,2,4,5,7} ? ax({2,4,5},{7}) [1]
{1,2,4,5,7} ? az({4},{1,5,7}) [1]
{1,5,7} ? ax({5},{7}) [1]
{4} ? compz [1] {5} ? repr?o f x,z [1]
{6} ? sampley [1] {7} ? seex,y [1]
For example, picking ?az? as the root of a con-
figuration (Fig. 1 (c), (d)) of the entire graph has
a weight of 1, because this rule establishes no soft
edges. On the other hand, choosing ?ax? as the root
has a weight of 9, because this establishes the soft
disjointness edge (and in fact, leads to the derivation
of the maximum-weight configuration in Fig. 1 (e)).
5.3 Computing the best configuration
The problem of computing the best configuration of
a weighted dominance graph ? or equivalently, the
224
best derivation of a weighted tree grammar ? can
now be solved by standard algorithms for wRTGs.
For example, Knight and Graehl (2005) present an
algorithm to extract the best derivation of a wRTG in
time O(t + n logn) where n is the number of nonter-
minals and t is the number of rules. In practice, we
can extract the best reading of the most ambiguous
sentence in the Rondane treebank (4.5? 1012 read-
ings, 75 000 grammar rules) with random soft edges
in about a second.
However, notice that this is not the same problem
as computing the best tree in the language accepted
by a wRTG, as trees may have multiple deriva-
tions. The problem of computing the best tree is NP-
complete (Sima?an, 1996). However, if the weighted
regular tree automaton corresponding to the wRTG
is deterministic, every tree has only one derivation,
and thus computing best trees becomes easy again.
The tree automata for dominance charts are always
deterministic, and the automata for RTGs as in Sec-
tion 3.2 (whose terminals correspond to the graph?s
node labels) are also typically deterministic if the
variable names are part of the quantifier node labels.
Furthermore, there are algorithms for determinizing
weighted tree automata (Borchardt and Vogler, 2003;
May and Knight, 2006), which could be applied as
preprocessing steps for wRTGs.
6 Conclusion
In this paper, we have shown how regular tree gram-
mars can be used as a formalism for scope under-
specification, and have exploited the power of this
view in a novel, simpler, and more complete algo-
rithm for redundancy elimination and the first effi-
cient algorithm for computing the best reading of a
scope ambiguity. In both cases, we have adapted
standard algorithms for RTGs, which illustrates the
usefulness of using such a well-understood formal-
ism. In the worst case, the RTG for a scope ambigu-
ity is exponential in the number of scope bearers in
the sentence; this is a necessary consequence of their
expressive completeness. However, those RTGs that
are computed by semantic construction and redun-
dancy elimination remain compact.
Rather than showing how to do semantic construc-
tion for RTGs, we have presented an algorithm that
computes RTGs from more standard underspecifica-
tion formalisms. We see RTGs as an ?underspecifi-
cation assembly language? ? they support efficient
and useful algorithms, but direct semantic construc-
tion may be inconvenient, and RTGs will rather be
obtained by ?compiling? higher-level underspecified
representations such as dominance graphs or MRS.
This perspective also allows us to establish a
connection to approaches to semantic construc-
tion which use chart-based packing methods rather
than dominance-based underspecification to manage
scope ambiguities. For instance, both Combinatory
Categorial Grammars (Steedman, 2000) and syn-
chronous grammars (Nesson and Shieber, 2006) rep-
resent syntactic and semantic ambiguity as part of
the same parse chart. These parse charts can be
seen as regular tree grammars that accept the lan-
guage of parse trees, and conceivably an RTG that
describes only the semantic and not the syntactic
ambiguity could be automatically extracted. We
could thus reconcile these completely separate ap-
proaches to semantic construction within the same
formal framework, and RTG-based algorithms (e.g.,
for redundancy elimination) would apply equally to
dominance-based and chart-based approaches. In-
deed, for one particular grammar formalism it has
even been shown that the parse chart contains an
isomorphic image of a dominance chart (Koller and
Rambow, 2007).
Finally, we have only scratched the surface of
what can be be done with the computation of best
configurations in Section 5. The algorithms gen-
eralize easily to weights that are taken from an ar-
bitrary ordered semiring (Golan, 1999; Borchardt
and Vogler, 2003) and to computing minimal-weight
rather than maximal-weight configurations. It is also
useful in applications beyond semantic construction,
e.g. in discourse parsing (Regneri et al, 2008).
Acknowledgments. We have benefited greatly
from fruitful discussions on weighted tree grammars
with Kevin Knight and Jonathan Graehl, and on dis-
course underspecification with Markus Egg. We
also thank Christian Ebert, Marco Kuhlmann, Alex
Lascarides, and the reviewers for their comments on
the paper. Finally, we are deeply grateful to our for-
mer colleague Joachim Niehren, who was a great fan
of tree automata before we even knew what they are.
225
References
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn,
J. Niehren, and S. Thiel. 2003. An efficient graph
algorithm for dominance constraints. J. Algorithms,
48:194?219.
B. Borchardt and H. Vogler. 2003. Determinization of
finite state weighted tree automata. Journal of Au-
tomata, Languages and Combinatorics, 8(3):417?463.
J. Bos. 1996. Predicate logic unplugged. In Proceedings
of the Tenth Amsterdam Colloquium, pages 133?143.
R. P. Chaves. 2003. Non-redundant scope disambigua-
tion in underspecified semantics. In Proceedings of
the 8th ESSLLI Student Session, pages 47?58, Vienna.
H. Comon, M. Dauchet, R. Gilleron, C. Lo?ding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tommasi.
2007. Tree automata techniques and applications.
Available on: http://www.grappa.univ-lille3.fr/tata.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Confer-
ence on Language Resources and Evaluation.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag. 2005.
Minimal recursion semantics: An introduction. Re-
search on Language and Computation, 3:281?332.
C. Ebert. 2005. Formal investigations of underspecified
representations. Ph.D. thesis, King?s College, Lon-
don.
M. Egg, A. Koller, and J. Niehren. 2001. The Constraint
Language for Lambda Structures. Logic, Language,
and Information, 10:457?485.
D. Flickinger, A. Koller, and S. Thater. 2005. A new
well-formedness criterion for semantics debugging. In
Proceedings of the 12th HPSG Conference, Lisbon.
J. S. Golan. 1999. Semirings and their applications.
Kluwer, Dordrecht.
J. Graehl and K. Knight. 2004. Training tree transducers.
In HLT-NAACL 2004, Boston.
D. Higgins and J. Sadock. 2003. A machine learning ap-
proach to modeling scope preferences. Computational
Linguistics, 29(1).
K. Knight and J. Graehl. 2005. An overview of proba-
bilistic tree transducers for natural language process-
ing. In Computational linguistics and intelligent text
processing, pages 1?24. Springer.
A. Koller and J. Niehren. 2000. On underspecified
processing of dynamic semantics. In Proceedings of
COLING-2000, Saarbru?cken.
A. Koller and O. Rambow. 2007. Relating dominance
formalisms. In Proceedings of the 12th Conference on
Formal Grammar, Dublin.
A. Koller and S. Thater. 2005a. Efficient solving and
exploration of scope ambiguities. Proceedings of the
ACL-05 Demo Session.
A. Koller and S. Thater. 2005b. The evolution of dom-
inance constraint solvers. In Proceedings of the ACL-
05 Workshop on Software.
A. Koller and S. Thater. 2006. An improved redundancy
elimination algorithm for underspecified descriptions.
In Proceedings of COLING/ACL-2006, Sydney.
J. May and K. Knight. 2006. A better n-best list: Prac-
tical determinization of weighted finite tree automata.
In Proceedings of HLT-NAACL.
R. Nesson and S. Shieber. 2006. Simpler TAG semantics
through synchronization. In Proceedings of the 11th
Conference on Formal Grammar.
J. Niehren and S. Thater. 2003. Bridging the gap be-
tween underspecification formalisms: Minimal recur-
sion semantics as dominance constraints. In Proceed-
ings of ACL 2003.
S. Oepen, K. Toutanova, S. Shieber, C. Manning,
D. Flickinger, and T. Brants. 2002. The LinGO Red-
woods treebank: Motivation and preliminary applica-
tions. In Proceedings of the 19th International Con-
ference on Computational Linguistics (COLING?02),
pages 1253?1257.
J. Pafel. 1997. Skopus und logische Struktur: Studien
zum Quantorenskopus im Deutschen. Habilitationss-
chrift, Eberhard-Karls-Universita?t Tu?bingen.
M. Regneri, M. Egg, and A. Koller. 2008. Efficient pro-
cessing of underspecified discourse representations. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT) ? Short Papers,
Columbus, Ohio.
U. Reyle. 1993. Dealing with ambiguities by underspec-
ification: Construction, representation and deduction.
Journal of Semantics, 10(1).
S. Shieber. 2006. Unifying synchronous tree-adjoining
grammars and tree transducers via bimorphisms. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), Trento, Italy.
K. Sima?an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree-grammars. In
Proceedings of the 16th conference on Computational
linguistics, pages 1175?1180, Morristown, NJ, USA.
Association for Computational Linguistics.
M. Steedman. 2000. The syntactic process. MIT Press.
E. Vestre. 1991. An algorithm for generating non-
redundant quantifier scopings. In Proc. of EACL,
pages 251?256, Berlin.
226
The evolution of dominance constraint solvers
Alexander Koller and Stefan Thater
Dept. of Computational Linguistics
Saarland University, Saarbr?cken, Germany
{koller,stth}@coli.uni-sb.de
Abstract
We describe the evolution of solvers
for dominance constraints, a formalism
used in underspecified semantics, and
present a new graph-based solver using
charts. An evaluation on real-world data
shows that each solver (including the
new one) is significantly faster than its
predecessors. We believe that our strat-
egy of successively tailoring a powerful
formalism to the actual inputs is more
generally applicable.
1 Introduction
In many areas of computational linguistics, there is
a tension between a need for powerful formalisms
and the desire for efficient processing. Expressive
formalisms are useful because they allow us to
specify linguistic facts at the right level of abstrac-
tion, and in a way that supports the creation and
maintenance of large language resources. On the
other hand, by choosing a more powerful formal-
ism, we typically run the risk that our processing
tasks (say, parsing or inference) can no longer be
performed efficiently.
One way to address this tension is to switch to
simpler formalisms. This makes processing more
efficient, but sacrifices the benefits of expressive
formalisms in terms of modelling. Another com-
mon strategy is to simply use the powerful for-
malisms anyway. This sometimes works pretty
well in practice, but a system built in this way can-
not give any runtime guarantees, and may become
slow for certain inputs unpredictably.
In this paper, we advocate a third option: Use a
general, powerful formalism, analyse what makes
it complex and what inputs actually occur in prac-
tice, and then find a restricted fragment of the for-
malism that supports all practical inputs and can
be processed efficiently. We demonstrate this ap-
proach by describing the evolution of solvers for
dominance constraints (Egg et al, 2001), a certain
formalism used for the underspecified descrip-
tion of scope ambiguities in computational seman-
tics. General dominance constraints have an NP-
complete satisfiability problem, but normal dom-
inance constraints, which subsume all constraints
that are used in practice, have linear-time satisfia-
bility and can be solved extremely efficiently.
We describe a sequence of four solvers, rang-
ing from a purely logic-based saturation algorithm
(Koller et al, 1998) over a solver based on con-
straint programming (Duchier and Niehren, 2000)
to efficient solvers based on graph algorithms
(Bodirsky et al, 2004). The first three solvers have
been described in the literature before, but we also
present a new variant of the graph solver that uses
caching to obtain a considerable speedup. Finally
we present a new evaluation that compares all four
solvers with each other and with a different under-
specification solver from the LKB grammar devel-
opment system (Copestake and Flickinger, 2000).
The paper is structured as follows. We will first
sketch the problem that our algorithms solve (Sec-
tion 2). Then we present the solvers (Section 3)
and conclude with the evaluation (Section 4).
2 The Problem
The problem we use to illustrate the progress to-
wards efficient solvers is that of enumerating all
readings of an underspecified description. Under-
specification is a technique for dealing with the
combinatorial problems associated with quantifier
scope ambiguities, certain semantic ambiguities
that occur in sentences such as the following:
(1) Every student reads a book.
This sentence has two different readings. Read-
ing (2) expresses that each student reads a possibly
different book, while reading (3) claims that there
is a single book which is read by every student.
(2) ?x.student(x)? (?y.book(y)? read(x,y))
(3) ?y.book(y)? (?x.student(x)? read(x,y))
The number of readings can grow exponen-
tially in the number of quantifiers and other
scope-bearing operators occuring in the sentence.
A particularly extreme example is the follow-
ing sentence from the Rondane Treebank, which
the English Resource Grammar (Copestake and
Flickinger, 2000) claims to have about 2.4 trillion
readings.
(4) Myrdal is the mountain terminus of the Fl?m
rail line (or Fl?msbana) which makes its way
down the lovely Fl?m Valley (Fl?msdalen) to
its sea-level terminus at Fl?m.
(Rondane 650)
Of course, this huge number of readings results
not only from genuine meaning differences, but
from the (quite reasonable) decision of the ERG
developers to uniformly treat all noun phrases, in-
cluding proper names and definites, as quantifiers.
But a system that builds upon such a grammar still
has to deal with these readings in some way.
The key idea of underspecification is now to not
enumerate all these semantic readings from a syn-
tactic analysis during or after parsing, but to derive
from the syntactic analysis a single, compact un-
derspecified description. The individual readings
can be enumerated from the description if they are
needed, and this enumeration process should be
efficient; but it is also possible to eliminate read-
ings that are infelicitous given knowledge about
the world or the context on the level of underspec-
ified descriptions.
?x
?
stud
x
?y
?
book
y yx
read
?x
?y
?
stud
x
?
book
y
yx
read
Figure 1: Trees for the readings (2) and (3).
?x
?y
?
stud
x
?
book
y
yx
read
X1 : ?x(X2) ?
X2 :?(X3,X4) ?
X5 : stud(X6) ?
X6 : x ?
. . .
X4 /? X7 ?
X7 : read(X8,X9) ?
X8 : x ?X9 : y
Figure 2: A dominance constraint (right) and its
graphical representation (left); the solutions of the
constraint are the two trees in Fig. 1.
Dominance constraints. The particular under-
specification formalism whose enumeration prob-
lem we consider in this paper is the formalism
of dominance constraints (Egg et al, 2001). The
basic idea behind using dominance constraints in
underspecification is that the semantic representa-
tions (2) and (3) can be considered as trees (see
Fig. 1). Then a set of semantic representations can
be characterised as the set of models of a formula
in the following language:
? ::= X : f (X1, . . . ,Xn) | X /? Y | X 6= Y | ???
The labelling atom X : f (X1, . . . ,Xn) expresses
that the node in the tree which is denoted by the
variable X has the label f , and its children are de-
noted by the variables X1 to Xn. Dominance atoms
X /? Y say that there is a path (of length 0 or more)
from the node denoted by X to the node denoted
by Y ; and inequality atoms X 6= Y require that X
and Y denote different nodes.
Dominance constraints ? can be drawn infor-
mally as graphs, as shown in Fig. 2. Each node
of the graph stands for a variable; node labels and
solid edges stand for labelling atoms; and the dot-
ted edges represent dominance atoms. The con-
straint represented by the drawing in Fig. 2 is sat-
isfied by both trees shown in Fig. 1. Thus we can
use it as an underspecified description represent-
ing these two readings.
The two obvious processing problems con-
nected to dominance constraints are satisfiability
(is there a model that satisfies the constraint?)
and enumeration (compute all models of a con-
straint). Because every satisfiable dominance con-
straint technically has an infinite number of mod-
els, the algorithms below solve the enumeration
problem by computing solved forms of the con-
straint, which are finite characterisations of infinite
model sets.
3 The Solvers
We present four different solvers for dominance
constraints. As we go along, we analyse what
makes dominance constraint solving hard, and
what characterises the constraints that occur in
practice.
3.1 A saturation algorithm
The first dominance constraint solver (Koller et al,
1998; Duchier and Niehren, 2000) is an algorithm
that operates directly on the constraint as a logical
formula. It is a saturation algorithm, which suc-
cessively enriches the constraint using saturation
rules. The algorithm terminates if it either derives
a contradiction (marked by the special atom false),
or if no rule can contribute any new atoms. In the
first case, it claims that the constraint is unsatisfi-
able; in the second case, it reports the end result of
the computation as a solved form and claims that
it is satisfiable.
The saturation rules in the solver try to match
their preconditions to the constraint, and if they
do match, add their conclusions to the constraint.
For example, the following rules express that dom-
inance is a transitive relation, and that trees have
no cycles:
X /? Y ?Y /? Z ? X /? Z
X : f (. . . ,Y, . . .)?Y /? X ? false
Some rules have disjunctive right-hand sides; if
they are applicable, they perform a case distinction
and add one of the disjuncts. One example is the
Choice Rule, which looks as follows:
X /? Z?Y /? Z ? X /? Y ?Y /? X
This rule checks for the presence of two variables
X and Y that are known to both dominate the same
variable Z. Because models must be trees, this
means that X and Y must dominate each other in
some order; but we can?t know yet whether it is X
or Y that dominates the other one. Hence the solver
tries both choices. This makes it possible to derive
multiple solved forms (one for each reading of the
sentence), such as the two different trees in Fig. 1.
It can be shown that a dominance constraint is
satisfiable iff it is not possible to derive false from
it using the rules in the algorithm. In addition, ev-
ery model of the original constraint satisfies ex-
actly one solved form. So the saturation algorithm
can indeed be used to solve dominance constraints.
However, even checking satisfiability takes nonde-
terministic polynomial time. Because all choices
in the distribution rule applications have to be
checked, a deterministic program will take expo-
nential time to check satisfiability in the worst
case.
Indeed, satisfiability of dominance constraints
is an NP-complete problem (Koller et al, 1998),
and hence it is likely that any solver for dominance
constraints will take exponential worst-case run-
time. At first sight, it seems that we have fallen
into the expressivity trap: We have a formalism
that allows us to model scope underspecification
very cleanly, but actually computing with this for-
malism is expensive.
3.2 Reduction to Set Constraints
In reaction to this NP-completeness result,
Duchier and Niehren (2000) applied techniques
from constraint programming to the problem in or-
der to get a more efficient solver. Constraint pro-
gramming (Apt, 2003) is a standard approach to
solving NP-complete combinatorial problems. In
this paradigm, a problem is modelled as a for-
mula in a logical constraint language. The pro-
gram searches for values for the variables in the
formula that satisfy the formula. In order to reduce
the size of the search space, it performs cheap de-
terministic inferences that exclude some values of
the variables (propagation), and only after prop-
agation can supply no further information it per-
forms a non-deterministic case distinction (distri-
bution).
Side
x
Eq
x
Up
x
Down
x
Figure 3: The four node sets
Duchier and Niehren solved dominance con-
straints by encoding them as finite set constraints.
Finite set constraints (M?ller and M?ller, 1997)
are formulas that talk about relations between
(terms that denote) finite sets of integers, such
as inclusion X ? Y or equality X = Y . Efficient
solvers for set constraints are available, e.g. as part
of the Mozart/Oz programming system (Oz Devel-
opment Team, 2004).
Reduction to set constraints. The basic idea
underlying the reduction is that a tree can be rep-
resented by specifying for each node v of this tree
which nodes are dominated by v, which ones dom-
inate v, which ones are equal to v (i.e. just v it-
self), and which ones are ?disjoint? from v (Fig. 3).
These four node sets are a partition of the nodes in
the tree.
Now the solver introduces for each variable X
in a dominance constraint ? four variables EqX ,
UpX , DownX , SideX for the sets of node variables
that denote nodes in the respective region of the
tree, relative to X . The atoms in ? are translated
into constraints on these variables. For instance, a
dominance atom X /? Y is translated into
UpX ? UpY ?DownY ? DownX ?SideX ? SideY
This constraint encodes that all variables whose
denotation dominates the denotation of X (UpX )
must also dominate the denotation of Y (UpY ), and
the analogous statements for the dominated and
disjoint variables.
In addition, the constraint program contains var-
ious redundant constraints that improve propaga-
tion. Now the search for solutions consists in find-
ing satisfying assignments to the set variables. The
result is a search tree as shown in Fig. 4: The
blue circles represent case distinctions, whereas
each green diamond represents a solution of the
set constraint (and therefore, a solved form of
Figure 4: Search tree for constraint 42 from the
Rondane Treebank.
the dominance constraint). Interestingly, all leaves
of the search tree in Fig. 4 are solution nodes;
the search never runs into inconsistent constraints.
This seems to happen systematically when solving
any constraints that come from underspecification.
3.3 A graph-based solver
This behaviour of the set-constraint solver is ex-
tremely surprising: The key characteristic of an
NP-complete problem is that the search tree must
necessarily contain failed nodes on some inputs.
The fact that the solver never runs into failure is a
strong indication that there is a fragment of domi-
nance constraints that contains all constraints that
are used in practice, and that the solver automat-
ically exploits this fragment. This begs the ques-
tion: What is this fragment, and can we develop
even faster solvers that are specialised to it?
One such fragment is the fragment of normal
dominance constraints (Althaus et al, 2003). The
most important restriction that a normal domi-
nance constraint ? must satisfy is that it is overlap-
free: Whenever ? contains two labelling atoms
X : f (. . .) and Y :g(. . .) (where f and g may be
equal), it must also contain an inequality atom
X 6= Y . As a consequence, no two labelled vari-
ables in a normal constraint may be mapped to the
same node. This is acceptable or even desirable in
underspecification: We are not interested in solu-
tions of the constraint in Fig. 2 in which the quan-
tifier representations overlap. On the other hand,
the NP-completeness proof in (Koller et al, 1998)
is no longer applicable to overlap-free constraints.
Hence normal dominance constraints are a frag-
ment that is sufficient from a modelling perspec-
tive, and possibly admits polynomial-time solvers.
Indeed, it can be shown that the satisfiability
problem of normal dominance constraints can be
gf
a b
g
a b
g
f
a b
Figure 5: An example computation of the graph
solver.
decided in linear time (Thiel, 2004), and the lin-
ear algorithm can be used to enumerate N solved
forms of a constraint of size n in time O(n2N). We
now present the simpler O(n2N) enumeration al-
gorithm by Bodirsky et al (2004).1 Note that N
may still be exponential in n.
Dominance Graphs. The crucial insight under-
lying the fast solvers for normal dominance con-
straints is that such constraints can be seen as dom-
inance graphs, and can be processed using graph
algorithms. Dominance graphs are directed graphs
with two kinds of edges: tree edges and dominance
edges. The graph without the dominance edges
must be a forest; the trees of this forest are called
the fragments of the graph. In addition, the dom-
inance edges must go from holes (i.e., unlabelled
leaves) of fragments to roots of other fragments.
For instance, we can view the graph in Fig. 2,
which we introduced as an informal notation for
a dominance constraint, directly as a dominance
graph with three fragments and two (dotted) dom-
inance edges.
A dominance graph G which is a forest is called
in solved form. We say that G? is a solved form of
a graph G iff G? is in solved form, G and G? con-
tain the same tree edges, and the reachability rela-
tion of G? extends that of G. Using this definition,
it is possible to define a mapping between normal
dominance constraints and dominance graphs such
that the solved forms of the graph can serve as
solved forms of the constraint ? i.e., we can reduce
constraint solving to graph solving.
By way of example, consider Fig. 5. The dom-
inance graph on the left is not in solved form, be-
cause it contains nodes with more than one incom-
1The original paper defines the algorithm for weakly nor-
mal dominance constraints, a slight generalisation.
GRAPH-SOLVER(G?)
1 if G? is already in solved form
2 then return G?
3 free? FREE-FRAGMENTS(G?)
4 if free = /0
5 then fail
6 choose F ? free
7 G1, . . . ,Gk?WCCS(G??F)
8 for each Gi ? G1, . . . ,Gk
9 do Si? GRAPH-SOLVER(Gi)
10 S? Attach S1, . . . ,Sk under F
11 return S
Figure 6: The graph solver.
ing dominance edge. By contrast, the other two
dominance graphs are in solved form. Because the
graph on the right has the same tree edges as the
one on the left and extends its reachability relation,
it is also a solved form of the left-hand graph.
The algorithm. The graph-based enumeration
algorithm is a recursive procedure that succes-
sively splits a dominance graph into smaller parts,
solves them recursively, and combines them into
complete solved forms. In each step, the algo-
rithm identifies the free fragments of the domi-
nance (sub-)graph. A fragment is free if it has no
incoming dominance edges, and all of its holes are
in different biconnected components of the undi-
rected version of the dominance graph. It can be
shown (Bodirsky et al, 2004) that if a graph G has
any solved form and F is a free fragment of G,
then G has a solved form in which F is at the root.
The exact algorithm is shown in Fig. 6. It com-
putes the free fragments of a sub-dominance graph
G? in line 3. Then it chooses one of the free frag-
ments, removes it from the graph, and calls itself
recursively on the weakly connected components
G1, . . . ,Gk of the resulting graph. Each recursive
call will compute a solved form Si of the con-
nected component Gi. Now for each Gi there is
exactly one hole hi of F that is connected to some
node in Gi by a dominance edge. We can obtain a
solved form for G? by combining F and all the Si
with dominance edges from hi to the root of Si for
each i.
gf
a b
Figure 7: An unsolvable dominance graph.
The algorithm is written as a nondeterministic
procedure which makes a nondeterministic choice
in line 6, and can fail in line 5. We can turn it into a
deterministic algorithm by considering the nonde-
terministic choices as case distinctions in a search
tree, as in Fig. 4. However, if the input graph G
is solvable, we know that every single leaf of the
search tree must correspond to a (different) solved
form, because for every free fragment that can be
chosen in line 6, there is a solved form that has this
fragment as its root. Conversely, if G is unsolv-
able, every single branch of the search tree will
run into failure, because it would claim the exis-
tence of a solved form otherwise. So the algorithm
decides solvability in polynomial time.
An example computation of GRAPH-SOLVER
is shown in Fig. 5. The input graph is shown on
the left. It contains exactly one free fragment F ;
this is the fragment whose root is labelled with
f . (The single-node fragments both have incom-
ing dominance edges, and the two holes of the
fragment with label g are in the same biconnected
component.) So the algorithm removes F from the
graph, resulting in the graph in the middle. This
graph is in solved form (it is a tree), so we are fin-
ished. Finally the algorithm builds a solved form
for the whole graph by plugging the solved form
in the middle into the single hole of F ; the result is
shown on the right. By contrast, the graph in Fig. 7
has no solved forms. The solver will recognise this
immediately, because none of the fragments is free
(they either have incoming dominance edges, or
their holes are biconnected).
3.4 A graph solver with charts
The graph solver is a great step forward towards
efficient constraint solving, and towards an under-
standing of why (normal) dominance constraints
can be solved efficiently. But it wastes time when
it is called multiple times for the same subgraph,
f
1
f
2
f
3
f
4
a
5
a
6
a
7
h
1
h
21
h
22
h
31
h
32
h
4
1 2 3 4
5 6 7
Figure 8: The chain of length 4.
{1,2,3,4,5,6,7} : ?1,h1 7? {2,3,4,5,6,7}?
?2,h21 7? {1,5},h22 7? {3,4,6,7}?
?3,h31 7? {1,2,5,6},h32 7? {4,7}?
?4,h4 7? {1,2,3,5,6,7}?
{2,3,4,5,6,7} : ?2,h21 7? {5},h22 7? {3,4,6,7}?
?3,h31 7? {2,5,6},h32 7? {4,7}?
?4,h4 7? {2,3,5,6,7}?
{1,2,3,5,6,7} : ?1,h1 7? {2,3,5,6,7}?
?2,h21 7? {1,5},h22 7? {3,6,7}?
?3,h31 7? {1,2,5,6},h32 7? {7}?
{2,3,5,6,7} : ?2,h21 7? {5},h22 7? {3,6,7}?
?3,h31 7? {2,5,6},h32 7? {7}?
. . . . . .
Figure 9: A part of the chart computed for the con-
straint in Fig. 8.
because it will solve it anew each time. In solv-
ing, for instance, the graph shown in Fig. 8, it
will solve the subgraph consisting of the fragments
{2,3,5,6,7} twice, because it can pick the frag-
ments 1 and 4 in either order.
We will now present a previously unpublished
optimisation for the solver that uses caching to al-
leviate this problem. The data structure we use for
caching (we call it ?chart? below because of its
obvious parallels to charts in parsing) assigns each
subgraph of the original graph a set of splits. Splits
encode the splittings of the graph into weakly con-
nected components that take place when a free
fragment is removed. Formally, a split for the sub-
graph G? consists of a reference to a fragment F
that is free in G? and a partial function that maps
some nodes of F to subgraphs of G?. A split is de-
termined uniquely by G? and F .
Consider, by way of example, Fig. 9, which dis-
plays a part of the chart that we want to compute
for the constraint in Fig. 8. In the entire graph G
(represented by the set {1, . . . ,7} of fragments),
the fragments 1, 2, 3, and 4 are free. As a conse-
quence, the chart contains a split for each of these
four fragments. If we remove fragment 1 from G,
we end up with a weakly connected graph G1 con-
taining the fragments {2, . . . ,7}. There is a dom-
GRAPH-SOLVER-CHART(G?)
1 if there is an entry for G? in the chart
2 then return true
3 free? FREE-FRAGMENTS(G?)
4 if free = /0
5 then return false
6 if G? contains only one fragment
7 then return true
8
9 for each F ? free
10 do split? SPLIT(G?,F)
11 for each S ?WCCS(G??F)
12 do if GRAPH-SOLVER-CHART(S) = false
13 then return false
14 add (G?,split) to the chart
15 return true
Figure 10: The graph solver with charts
inance edge from the hole h1 into G1, so once
we have a solved form of G1, we will have to
plug it into h1 to get a solved form of G; there-
fore G1 is assigned to h1 in the split. On the other
hand, if we remove fragment 2 from G, G is split
into two weakly connected components {1,5} and
{3,4,6,7}, whose solved forms must be plugged
into h21 and h22 respectively.
We can compute a chart like this using the algo-
rithm shown in Fig. 10. This recursive algorithm
gets some subgraph G? of the original graph G as
its first argument. It returns true if G? is solvable,
and false if it isn?t. If an entry for its argument G?
was already computed and recorded in the chart,
the procedure returns immediately. Otherwise, it
computes the free fragments of G?. If there are no
free fragments, G was unsolvable, and thus the al-
gorithm returns false; on the other hand, if G? only
contains one fragment, it is solved and we can im-
mediately return true.
If none of these special cases apply, the algo-
rithm iterates over all free fragments F of G? and
computes the (unique) split that places F at the
root of the solved forms. If all weakly connected
components represented in the split are solvable, it
records the split as valid for G?, and returns true.
If the algorithm returns with value true, the
chart will be filled with splits for all subgraphs of
G that the GRAPH-SOLVER algorithm would have
visited. It is also guaranteed that every split in the
chart is used in a solved form of the graph. Ex-
tracting the actual solved forms from the chart is
straightforward, and can be done essentially like
for parse charts of context-free grammar.
Runtime analysis. The chart computed by the
chart solver for a dominance graph with n
nodes and m edges can grow to at most O(n ?
wcsg(G)) entries, where wcsg(G) is the number of
weakly connected subgraphs of G: All subgraphs
for which GRAPH-SOLVER-CHART is called are
weakly connected, and for each such subgraph
there can be at most n different splits. Because a
recursive call returns immediately if its argument
is already present in the chart, this means that at
most O(n ?wcsg(G)) calls spend more than the ex-
pected constant time that it takes to look up G? in
the chart. Each of these calls needs time O(m+n),
the cost of computing the free fragments.
As a consequence, the total time that GRAPH-
SOLVER-CHART takes to fill the chart is O(n(n+
m)wcsg(G)). Applied to a dominance constraint
with k atoms, the runtime is O(k2wcsg(G)). On
the other hand, if G has N solved forms, it takes
time O(N) to extract these solved forms from the
chart. This is a significant improvement over the
O(n(n + m)N) time that GRAPH-SOLVER takes
to enumerate all solved forms. A particularly dra-
matic case is that of chains ? graphs with a zig-zag
shape of n upper and n? 1 lower fragments such
as in Fig. 8, which occur frequently as part of un-
derspecified descriptions. A chain has only O(n2)
weakly connected subgraphs and O(n) edges, so
the chart can be filled in time O(n4), despite the
fact that the chain has 1
n+1
(2n
n
)
solved forms (this is
the n-th Catalan number, which grows faster than
n!). The worst case for the chart size is shown in
Fig. 11. If such a graph has n upper fragments,
it has O(2n) weakly connected subgraphs, so the
chart-filling phase takes time O(n22n). But this is
still dominated by the N = n! solved forms that
this graph has.
4 Evaluation
We conclude this paper with a comparative run-
time evaluation of the presented dominance con-
gf h
a
i
Figure 11: A worst-case graph for the chart solver.
constraints max. solved forms
Rondane 961 ?
Nets 879 2.4 ?1012
Nets < 106 solved forms 852 997920
Solver solvable max. solved forms
Saturation (?3.1) 757 10030
Set constraints (?3.2) 841 557472
Graph (?3.3) 850 768254
Chart (?3.4) 852 997920
LKB 682 17760
All 682 7742
Figure 12: Sizes of the data sets.
straint solvers. To put the results into context, we
also compare the runtimes with a solver for Min-
imal Recursion Semantics (MRS) (Copestake et
al., 2004), a different formalism for scope under-
specification.
Resources. As our test set we use constraints ex-
tracted from the Rondane treebank, which is dis-
tributed as part of the English Resource Grammar
(Copestake and Flickinger, 2000). The treebank
contains syntactic annotations for sentences from
the tourism domain such as (4) above, together
with corresponding semantic representations.
The semantics is represented using MRS de-
scriptions, which we convert into normal domi-
nance constraints using the translation specified by
Niehren and Thater (2003). The translation is re-
stricted to MRS constraints having certain struc-
tural properties (called nets). The treebank con-
tains 961 MRS constrains, 879 of which are nets.
For the runtime evaluation, we restricted the
test set to the 852 nets with less than one mil-
lion solved forms. The distribution of these con-
straints over the different constraint sizes (i.e.
number of fragments) is shown in Fig. 15. We
solved them using implementations of the pre-
sented dominance constraint solvers, as well as
with the MRS solver in the LKB system (Copes-
take and Flickinger, 2000).
Runtimes. As Fig. 12 shows, the chart solver
is the only solver that could solve all constraints
in the test set; all other solvers ran into memory
limitations on some inputs.2 The increased com-
plexity of constraints that each solver can handle
(given as the maximum number of solved forms of
a solvable constraint) is a first indication that the
repeated analysis and improvement of dominance
constraint solvers described earlier was successful.
Fig. 13 displays the result of the runtime com-
parison, taking into account only those 682 con-
straints that all solvers could solve. For each con-
straint size (counted in number of fragments), the
graph shows the mean quotient of the time to enu-
merate all solved forms by the number of solved
forms, averaged over all constraints of this size.
Note that the vertical axis is logarithmic, and that
the runtimes of the LKB and the chart solver for
constraints up to size 6 are too small for accurate
measurement.
The figure shows that each new generation of
dominance constraint solvers improves the perfor-
mance by an order of magnitude. Another differ-
ence is in the slopes of the graphs. While the sat-
uration solver takes increasingly more time per
solved form as the constraint grows, the set con-
straint and graph solvers remain mostly constant
for larger constraints, and the line for the chart
solver even goes down. This demonstrates an im-
proved management of the combinatorial explo-
sion. It is also interesting that the line of the set-
constraint solver is almost parallel to that of the
graph solver, which means that the solver really
does exploit a polynomial fragment on real-world
data.
The LKB solver performs very well for smaller
constraints (which make up about half of the data
set): Except for the chart algorithm introduced in
this paper, it outperforms all other solvers. For
larger constraints, however, the LKB solver gets
very slow. What isn?t visible in this graph is that
the LKB solver also exhibits a dramatically higher
variation in runtimes for constraints of the same
size, compared to the dominance solvers. We be-
lieve this is because the LKB solver has been op-
timised by hand to deal with certain classes of in-
2On a 1.2 GHz PC with 2 GB memory.
puts, but at its core is still an uncontrolled expo-
nential algorithm.
We should note that the chart-based solver is
implemented in C++, while the other dominance
solvers are implemented in Oz, and the MRS
solver is implemented in Common Lisp. This ac-
counts for some constant factor in the runtime, but
shouldn?t affect the differences in slope and vari-
ability.
Effect of the chart. Because the chart solver is
especially efficient if the chart remains small, we
have compared how the number of solved forms
and the chart size (i.e. number of splits) grow with
the constraint size (Fig. 14). The graph shows that
the chart size grows much more slowly than the
number of solved forms, which supports our intu-
ition that the runtime of the chart solver is asymp-
totically less than that of the graph solver by a sig-
nificant margin. The chart for the most ambigu-
ous sentence in the treebank (sentence (4) above)
contains 74.960 splits. It can be computed in less
than ten seconds. By comparison, enumerating all
solved forms of the constraint would take about a
year on a modern PC. Even determining the num-
ber of solved forms of this constraint is only pos-
sible based on the chart.
5 Conclusion
In this paper we described the evolution of solvers
for dominance constraints, a logical formalism
used for the underspecified processing of scope
ambiguities. We also presented a new solver,
which caches the intermediate results of a graph
solver in a chart. An empirical evaluation shows
that each solver is significantly faster than the pre-
vious one, and that the new chart-based solver
is the fastest underspecification solver available
today. It is available online at http://utool.
sourceforge.net.
Each new solver was based on an analysis of the
main sources of inefficiency in the previous solver,
as well as an increasingly good understanding of
the input data. The main breakthrough was the re-
alisation that normal dominance constraints have
polynomial satisfiability and can be solved using
graph algorithms. We believe that this strategy of
starting with a clean, powerful formalism and then
successively searching for a fragment that con-
tains all practically relevant inputs and excludes
the pathologically hard cases is applicable to other
problems in computational linguistics as well.
However, it is clear that the concept of ?all prac-
tically relevant inputs? is a moving target. In this
paper, we have equated it with ?all inputs that can
be generated by a specific large-scale grammar?,
but new grammars or different linguistic theories
may generate underspecified descriptions that no
longer fall into the efficient fragments. In our case,
it is hard to imagine what dominance constraint
used in scope underspecification wouldn?t be nor-
mal, and we have strong intuitions that all use-
ful constraints must be nets, but it is definitely an
interesting question how our algorithms could be
adapted to, say, the alternative scope theory advo-
cated by Joshi et al (2003).
An immediate line of future research is to ex-
plore uses of the chart data structure that go be-
yond pure caching. The general aim of underspec-
ification is not to simply enumerate all readings
of a sentence, but to use the underspecified de-
scription as a platform on which readings that are
theoretically possible, but infelicitous in the actual
context, can be eliminated. The chart may prove
to be an interesting platform for such operations,
which combines advantages of the underspecified
description (size) and the readings themselves (ex-
plicitness).
Acknowledgements. The work has been funded
by the DFG in the Collaborative Research Cen-
tre 378 Ressource-Adaptive Cognitive Processes,
project MI 2 (CHORUS).
We would like to thank Joachim Niehren and
Denys Duchier for the extremely fruitful col-
laboration on dominance constraint solving, Ann
Copestake and Dan Flickinger for helpful discus-
sions about the ERG and the LKB solver, and our
reviewers for their comments. The primary imple-
mentors of the various earlier constraint solvers
were Katrin Erk and Sebastian Pad? (?3.1), Denys
Duchier (?3.2), and Sebastian Miele (?3.3).
References
Ernst Althaus, Denys Duchier, Alexander Koller, Kurt
Mehlhorn, Joachim Niehren, and Sven Thiel. 2003.
An efficient graph algorithm for dominance con-
straints. Journal of Algorithms, 48:194?219.
Krzysztof R. Apt. 2003. Principles of Constraint Pro-
gramming. Cambridge University Press.
Manuel Bodirsky, Denys Duchier, Joachim Niehren,
and Sebastian Miele. 2004. An efficient algorithm
for weakly normal dominance constraints. In ACM-
SIAM Symposium on Discrete Algorithms. The ACM
Press.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage english grammar using HPSG.
In Conference on Language Resources and Evalua-
tion. The LKB system is available at http://www.
delph-in.net/lkb/.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2004. Minimal recursion semantics: An intro-
duction. Journal of Language and Computation. To
appear.
Denys Duchier and Joachim Niehren. 2000. Domi-
nance constraints with set operators. In Proceed-
ings of the First International Conference on Com-
putational Logic, number 1861 in Lecture Notes in
Computer Science, pages 326?341. Springer-Verlag,
Berlin.
Markus Egg, Alexander Koller, and Joachim Niehren.
2001. The Constraint Language for Lambda Struc-
tures. Logic, Language, and Information, 10:457?
485.
Aravind Joshi, Laura Kallmeyer, and Maribel Romero.
2003. Flexible composition in LTAG, quantifier
scope and inverse linking. In Harry Bunt, Ielka
van der Sluis, and Roser Morante, editors, Proceed-
ings of the Fifth International Workshop on Compu-
tational Semantics, pages 179?194, Tilburg.
Alexander Koller, Joachim Niehren, and Ralf Treinen.
1998. Dominance constraints: Algorithms and com-
plexity. In Proceedings of LACL, pages 106?
125. Appeared in 2001 as volume 2014 of LNAI,
Springer Verlag.
Tobias M?ller and Martin M?ller. 1997. Finite set con-
straints in Oz. In Fran?ois Bry, Burkhard Freitag,
and Dietmar Seipel, editors, 13. Workshop Logische
Programmierung, pages 104?115, Technische Uni-
versit?t M?nchen.
Joachim Niehren and Stefan Thater. 2003. Bridg-
ing the gap between underspecification formalisms:
Minimal recursion semantics as dominance con-
straints. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics.
Oz Development Team. 2004. The Mozart Pro-
gramming System. Web pages. http://www.
mozart-oz.org.
Sven Thiel. 2004. Efficient Algorithms for Con-
straint Propagation and for Processing Tree De-
scriptions. Ph.D. thesis, Department of Computer
Science, Saarland University.
 0.01
 0.1
 1
 10
 100
 1000
 0  2  4  6  8  10  12  14  16  18  20  22
R
u
n
t
i
m
e
 
p
e
r
 
s
o
l
u
t
i
o
n
 
(
i
n
 
m
s
)
Size of the constraint
"Section-3.1"
"Section-3.2"
"Section-3.3"
"Section-3.4"
"MRS"
Figure 13: Average runtimes per solved form, for each constraint size (number of fragments).
 1
 10
 100
 1000
 10000
 0  2  4  6  8  10  12  14  16  18  20  22
Nu
m
be
r
 
o
f
 
s
o
l
u
t
i
o
n
s
/S
i
z
e
 
o
f
 
t
h
e
 
c
h
a
r
t
Size of the constraint
"chart-size"
"solved-forms"
Figure 14: Average size of the chart compared to the average number of solved forms, for each constraint
size. Notice that the measurements are based upon the same set of constraints as in Fig. 13, which
contains very few constraints of size 20 or more.
 0
 10
 20
 30
 40
 50
 60
 70
 0  5  10  15  20  25  30  35  40
"rondane-nets"
"rondane-test-set"
Figure 15: Distribution of the constraints in Rondane over the different constraint sizes. The solid line in-
dicates the 852 nets with less than one million solved forms; the dashed line indicates the 682 constraints
that all solvers could solve.
Towards a redundancy elimination algorithm
for underspecified descriptions
Alexander Koller and Stefan Thater
Department of Computational Linguistics
Universit?t des Saarlandes, Saarbr?cken, Germany
{koller,stth}@coli.uni-sb.de
Abstract
This paper proposes an efficient algorithm for the redundancy elimination problem: Given
an underspecified semantic representation (USR), compute an USR which has fewer read-
ings, but still describes at least one representative of each semantic equivalence class of the
original readings. The algorithm operates on underspecified chart representations which
are derived from dominance graphs; it can be applied to the USRs computed by large-
scale grammars. To our knowledge, it is the first redundancy elimination algorithm which
maintains underspecification, rather than just enumerating non-redundant readings.
1 Introduction
Underspecification is the standard approach to dealing with scope ambiguities in
computational semantics [12,6,7,2]. The basic idea is to not enumerate all possible
semantic representations for each syntactic analysis, but to derive a single compact
underspecified representation (USR). This simplifies semantics construction, and
current algorithms support the efficient enumeration of readings from an USR [10].
In addition, underspecification has the potential for eliminating incorrect or re-
dundant readings by inferences based on context or world knowledge, without even
enumerating them. For instance, sentences with scope ambiguities often have read-
ings which are semantically equivalent. In this case, we typically need to retain
only one reading from each equivalence class. This situation is illustrated by the
following two sentences from the Rondane treebank, which is distributed with the
English Resource Grammar (ERG; [5]), a broad-coverage HPSG grammar.
(1) For travellers going to Finnmark there is a bus service from Oslo to Alta
through Sweden. (Rondane 1262)
(2) We quickly put up the tents in the lee of a small hillside and cook for the first
time in the open. (Rondane 892)
For the two example sentences, the ERG (Version 01-2006) derives USRs with
seven and six quantifiers, respectively, that correspond to various types of noun
phrases (including proper names and pronouns). The USR for (1) describes 3960
readings, which are all semantically equivalent to each other. On the other hand, the
USR for (2) has 480 readings, which fall into two classes of mutually equivalent
readings, characterised by the relative scope of ?the lee of? and ?a small hillside.?
This paper presents an algorithm for the redundancy elimination problem: Given
an USR, compute an USR which has fewer readings, but still describes at least one
representative of each equivalence class ? without enumerating any readings. This
algorithm computes the one or two representatives of the semantic equivalence
classes in the above examples, so subsequent modules don?t have to deal with all
the other equivalent readings. It also closes the gap between the large number of
readings predicted by the grammar and the intuitively perceived much lower degree
of ambiguity of these sentences. Finally, it can be helpful for a grammar designer
because it is much more feasible to check whether two readings are linguistically
reasonable than 480.
We model equivalence in terms of rewrite rules that permute quantifiers without
changing the semantics of the readings. The particular USRs we work with are un-
derspecified chart representations, which can be computed from dominance graphs
(or USRs in some other underspecification formalisms) efficiently [10]. The algo-
rithm can deal with many interesting cases, but is incomplete in the sense that the
resulting USR may still describe multiple equivalent readings.
To our knowledge, this is the first algorithm in the literature for redundancy
elimination on the level of USRs. There has been previous research on enumerating
only some representatives of each equivalence class [13,4], but these approaches
don?t maintain underspecification: After running their algorithms, we have a set of
readings rather than an underspecified representation.
Plan of the paper. We will first define dominance graphs and review the necessary
background theory in Section 2. We will then give a formal definition of equiva-
lence and derive some first results in Section 3. Section 4 presents the redundancy
elimination algorithm. Finally, Section 5 concludes and points to further work.
2 Dominance Graphs
The basic underspecification formalism we assume here are labelled dominance
graphs [1]. Dominance graphs are equivalent to leaf-labelled normal dominance
constraints [7], which have been discussed extensively in previous literature.
Definition 2.1 A (compact) dominance graph is a directed graph (V,E unionmultiD) with
two kinds of edges, tree edges E and dominance edges D, such that:
(i) the graph (V,E) defines a collection of node disjoint trees of height 0 or 1. We
call the trees in (V,E) the fragments of the graph.
(ii) if (v,v?) is a dominance edge in D, then v is a hole and v? is a root in G. A node
v is a root (in G) if v does not have incoming tree edges; otherwise, v is a hole.
A labelled dominance graph over a ranked signature ? is a triple G = (V,E unionmultiD,L)
ay
sample
y
see
x,y
a
x
repr-of
x,z
a
z
comp
z
1 2 3
4 5 6
7
a
y
a
x
a
z
1
2
3
sample
y
see
x,y
repr-of
x,z
comp
z
a
y
a
x
sample
y
see
x,y
repr-of
x,z
a
z
comp
z
1
2
3
Fig. 1. A dominance graph that represents the five readings of the sentence ?a representative
of a company saw a sample? (left) and two (of five) configurations.
1 2 3
4 5 6
7
h
2
h
1
h
4
h
3
h
6
h
5
1 3
4 5 6
7
h
2
h
1
h
6
h
5
? ?
h
2
h
1
h
4
h
3
h
6
h
5
2
1 3
4 5 6 7
Fig. 2. An example computation of a solved form.
such that (V,E unionmultiD) is a dominance graph and L : V  ? is a partial labelling
function which assigns a node v a label with arity n iff v is a root with n outgoing
tree edges. Nodes without labels (i.e., holes) must have outgoing dominance edges.
We will write v: f (v1, . . . ,vk) for a fragment whose root v is labelled with f and
whose holes are v1, . . . ,vk. We will write R(F) for the root of the fragment F , and
we will typically just say graph instead of labelled dominance graph.
An example of a labelled dominance graph is shown to the left of Fig. 1. Tree
edges are drawn as solid lines, and dominance edges are drawn as dotted lines, di-
rected from top to bottom. This graph can serve as an USR for the sentence ?a repre-
sentative of a company saw a sample? if we demand that the holes are ?plugged? by
roots while realising the dominance edges as dominance, as in the two (of five) con-
figurations shown to the right [7]. Configurations encode semantic representations
of the sentence, and we freely read configurations as ground terms over ?.
2.1 Solving dominance graphs
Algorithms for solving a dominance graph in order to compute the readings it de-
scribes typically compute its minimal solved forms [1,3]. In this paper, we restrict
ourselves to hypernormally connected graphs (defined below), for which one can
show that all solved forms are minimal and bijectively correspond to configurations.
Let G,G? be dominance graphs. We say that G is in solved form iff it is a forest,
and G is a solved form of G? if G is in solved form and more specific than G? i.e., G
and G? have the same labels and tree fragments, and the reachability relation of G
extends that of G?. G? is solvable if it has a solved form G. If G? is hypernormally
connected, then each hole in G has exactly one outgoing dominance edge, and G
can be mapped to a configuration by identifying the two ends of each dominance
edge; conversely, we can find a unique solved form for each configuration. The
graph to the left of Fig. 2 shows one of the (minimal) solved forms of the example
graph, which corresponds to the configuration in the middle of Fig. 1.
Compute-Chart(G)
1 if there is an entry for G in the chart
2 then return true
3 free? Free-Fragments(G)
4 if free = /0
5 then return false
6 if G contains only one fragment
7 then return true
8 for each F ? free
9 do split? Split(G,F)
10 for each S ?Wccs(G?F)
11 do if Compute-Chart(S) = false
12 then return false
13 add (G,split) to the chart
14 return true
{1,2,3,4,5,6,7} :?1,h1 7? {4},h2 7? {2,3,5,6,7}?
?2,h3 7? {1,4,5},h4 7? {3,6,7}?
?3,h5 7? {5},h6 7? {1,2,4,5,7}?
{2,3,5,6,7} :?2,h3 7? {5},h4 7? {3,6,7}?
?3,h5 7? {6},h6 7? {2,5,7}?
{3,6,7} :?3,h5 7? {6},h6 7? {7}?
{2,5,7} :?2,h3 7? {5},h4 7? {7}?
{1,4,5} :?1,h1 7? {4},h2 7? {5}?
{1,2,4,5,7} :?1,h1 7? {4},h2 7? {2,5,7}?
?2,h3 7? {1,4,5},h4 7? {7}?
Fig. 3. The chart solver and an example chart computed for the dominance graph in Fig. 2.
The key concept of the solver we build upon is that of a free fragment [3]. A
fragment F in a solvable graph G is free iff there is a solved form in which F is at
the root. It can be shown that a fragment is free iff it has no incoming dominance
edges and its holes are in different biconnected components of the graph i.e., they
are disconnected if the root of the fragment is removed from the graph [3]. Remov-
ing a free fragment from a graph splits the graph into different weakly connected
components (wccs) ? one for each hole. Thus each free fragment F induces a split
of G, which consists of a reference to F and a mapping of the other fragments to the
hole to which they are connected. For instance, the example graph has three free
fragments: 1, 2, and 3. By removing fragment 2, the graph is decomposed into two
wccs, which are connected to the holes h3 and h4, respectively (see Fig. 2).
The solver [10] is shown in Fig. 3. It computes a chart-like data structure which
assigns sets of splits to subgraphs. For each subgraph it is called on, the solver
computes the free fragments, the splits they induce, and calls itself recursively on
the wccs of each split. It records subgraphs and splits in the chart, and will not
repeat work for a subgraph it has encountered before. The algorithm returns true iff
the original graph was solvable. The chart tells us how to build the minimal solved
forms of the graph: For each subgraphs, pick any split, compute a solved form for
each wcc recursively, and plug them into the given hole of the split?s root fragment.
As an example, the chart for the graph in Fig. 1 is shown to the right of Fig. 3.
Notice that the chart which the solver computes, while possibly exponentially
larger than the original graph, is still exponentially smaller than the entire set of
readings because common subgraphs (such as {2,5,7} in the example) are repre-
sented only once. Thus the chart can still serve as an underspecified representation.
2.2 Hypernormally connected dominance graphs
A hypernormal path [1] in a graph G is a path in the undirected version Gu of G that
does not use two dominance edges that are incident to the same hole. We say that
G is hypernormally connected (hnc) iff each pair of nodes is connected by a simple
hypernormal path in G. Hnc graphs are equivalent to chain-connected dominance
constraints [9], and are closely related to dominance nets [11]. The results in this
paper are restricted to hnc graphs, but this does not limit the applicability of our
results: an empirical study suggests that all dominance graphs that are generated by
current large-scale grammars are (or should be) hnc [8].
The key property of hnc dominance graphs is that their solved forms correspond
to configurations, and we will freely switch between solved forms and their corre-
sponding configurations. Another important property of hnc graphs which we will
use extensively in the proofs below is that it is possible to predict which holes of
fragments can dominate other fragments in a solved form.
Lemma 2.2 Let G be a hnc graph with free fragment F. Then all weakly connected
components of G?F are hnc.
Proposition 2.3 Let F1,F2 be fragments in a hnc dominance graph G. If there is a
solved form S of G in which R(F1) dominates R(F2), then there is exactly one hole
h of F1 which is connected to R(F2) by a simple hypernormal path which doesn?t
use R(F1). In particular, h dominates R(F2) in S.
Proof. Let?s say that F1 dominates F2 in some solved form S. There is a run of
the solver which computes S. This run chooses F1 as a free fragment before it
chooses F2. Let?s call the subgraph in which the split for F1 is chosen, G?. G? is hnc
(Lemma 2.2), so in particular there is a simple hypernormal path from the hole h
of F1 which is in the same wcc as F2 to R(F2); this path doesn?t use R(F1). On the
other hand, assume there were another hole h? of F1 which is connected to R(F2) by
a path that doesn?t use R(F1). Then the path via R(F2) would connect h and h? even
if R(F1) were removed, so h and h? would be in the same biconnected component
of G, in contradiction to the assumption that F1 is free in G?.
For the second result, note that F2 is assigned to the hole h in the split for F1.2
The following definition captures the complex condition in Prop. 2.3:
Definition 2.4 Let G be a hnc dominance graph. A fragment F1 in G is called a
possible dominator of another fragment F2 in G iff it has exactly one hole h which
is connected to R(F2) by a simple hypernormal path which doesn?t use R(F1). We
write ch(F1,F2) for this unique h.
3 Equivalence
Equivalence is traditionally defined as the relation between formulas which have
the same interpretation. However, even first-order equivalence is an undecidable
problem, thus an algorithm which checks for semantic equivalence of different con-
figurations of a graph can?t possibly be efficient. On the other hand, we do not need
to solve the full semantic equivalence problem, as we only want to compare formu-
las that are readings of the same sentence i.e., different configurations of the same
USR. Such formulas only differ in the way that the fragments are combined. We
can therefore approximate equivalence by using a rewrite system that permutes frag-
ments and defining equivalence of configurations as mutual rewritability as usual.
By way of example, consider again the two (equivalent) configurations shown
in Fig. 1. We can obtain the second configuration from the first one by applying the
following rewrite rule, which rotates the nodes 1 and 2:
ax(az(P,Q),R)? az(P,ax(Q,R)) (3)
The formulas on both sides of the arrow are semantically equivalent in first-order
logic for any choice of the subformulas P, Q, and R. Thus the equivalence of the
two configurations with respect to our one-rule rewrite system implies that they are
also semantically equivalent.
While we will require that the rewriting approximation is sound i.e., rewrites
formulas into equivalent formulas, we cannot usually hope to achieve completeness
i.e., there will be semantic equivalences that are not modelled by the rewriting
equivalence. However, we believe that the rewriting-based system will still prove
to be useful in practical applications, as the permutation of quantifiers is exactly the
kind of variability that an underspecified description allows.
We formalise this rewriting-based notion of equivalence as follows. The defini-
tion uses the abbreviation x[1,k) for x1, . . . ,xk?1, and x(k,n] for xk+1, . . . ,xn.
Definition 3.1 A permutation system R is a system of rewrite rules over a signature
? of the following form:
f1(x[1,i), f2(y[1,k),z,y(k,m]),x(i,n]) ? f2(y[1,k), f1(x[1,i),z,x(i,n]),y(k,m])
The permutability relation P(R) is the binary relation P(R)? (??N)2 which con-
tains exactly the pairs (( f1, i),( f2,k)) and (( f2,k),( f1, i)) for each such rewrite rule.
As usual, we say that two terms are equivalent with respect to R, s?R t, iff there
is a sequence of rewrite steps and inverse rewrite steps that rewrite s into t. We say
that R is sound with respect to a semantic notion of equivalence ? if ?R ??. If G
is a graph over ? and R a permutation system, then we write SCR(G) for the set of
equivalence classes Conf(G)/?R, where Conf(G) is the set of configurations of G.
A rewrite system (let?s call it Rfol) which is sound for the standard equivalence
relation of first-order logic could use rule (3) and the three other permutations of
two existential quantifiers, plus the following rule for universal quantifiers:
everyx(X ,everyy(Y,Z))? everyy(Y,everyx(X ,Z))
The other three permutations of universal quantifiers, as well as the permutations
of universal and existential quantifiers, are not sound.
It is possible to compute SCR(G) by solving G and using a theorem prover for
equational reasoning to compute the equivalence classes of the configurations, but
this is very inefficient. To replace this by a computation on the USR, we must be
able to recognise whether two fragments of a graph can be permuted in all config-
urations of the graph. This is not possible in general: If we don?t know in advance
xi+1
x
n
x
1
x
i-1
y
1
y
k-1
y
k+1
y
m
y
1
y
k-1
y
k+1
y
m
z
F
2
F
1
? ?
? ?
v
k
v = u
i
u
F
2
F
1
x
1
x
i-1
x
i+1
x
n
? ?
?
?
z
v
u
i
v
k
 = u
(a)
F
2
W
F
1
u
i
?
v
j
v
k
w
?
r
?
u
v
(b)
Fig. 4. Diagrams for the proof of Lemma 3.3
which hole of one fragment the other fragment can plug, we can?t know whether the
two fragments can be permuted. However, in a hnc graph, the hole of a fragment
which another fragment can plug is determined uniquely (because of Lemma 2.3),
and can be recognised without solving the graph.
Definition 3.2 Let R be a permutation system. Two fragments F1 and F2 with root
labels f1 and f2 in a graph G are called R-permutable iff they are possible domina-
tors of each other and (( f1,ch(F1,F2)),( f2,ch(F2,F1))) ? P(R).
Lemma 3.3 Let R be a permutation system, let F1 = u: f1(u1, . . . ,un) and F2 =
v: f2(v1, . . . ,vm) be R-permutable fragments in the hnc graph G, such that F2 is free,
and let C1 be a configuration of G in which u is the father of v. Then:
(a) It is possible to apply a R-rewrite step or an inverse R-rewrite step to C1 at u;
call the resulting tree C2.
(b) C2 is also a configuration of G.
(c) C2 ?R C1.
Proof. Let i = ch(F1,F2) and k = ch(F2,F1); we know that (( f1, i),( f2,k)) ? P(R).
(a) F1 is a possible dominator of F2, so ui is plugged with v in C1 (Lemma 2.3).
Thus the (possibly inverse) rule which justified the tuple (( f1, i),( f2,k)) is applica-
ble at u.
(b) We must verify that every dominance edge in G is realised byC2. As Fig. 4a
shows, all dominance edges that do not go out of a hole of F1 are still trivially
realised byC2. Now let?s consider dominances out of the holes of F1.
? Dominance edges out of any u j with j 6= i are still satisfied (see the figure).
? Dominance edges from ui to a node in z are still satisfied (see the figure).
? Dominance edges from ui to v: Such edges cannot exist in G as F2 is free.
? Dominance edges from ui to a node w in some y j with j 6= k: Such edges cannot
exist either. F2 is a possible dominator of the fragment W whose root w is, so
there is a simple hypernormal path piw from ch(F2,W ) to w which doesn?t use v;
ch(F2,W ) = v j because v j dominates w in C1 (Lemma 2.3). On the other hand,
F2 is a possible dominator of F1, so there is a simple hypernormal path piu from
vk to ui which doesn?t use v. Now if there were a dominance edge from ui to w
in G, then v j and vk would be in the same biconnected component (they would
be connected via piu ? (ui,w) ? pi?1w if v were removed), which contradicts the
freeness of F2 (see Fig. 4b).
4 Underspecified redundancy elimination
Now we can finally consider the problem of strengthening an USR in order to
remove redundant readings which are equivalent to other readings. We will define
an algorithm which gets as its input a graph G, a chart as computed by COMPUTE-
CHART, and a permutability relation P(R). It will then remove splits from the chart,
to the effect that the chart represents fewer solved forms of the original graph, but at
least one representative from each class in SCR(G) remains. The subgraph sharing
of the original chart will be retained, so the computed chart is still an USR.
The key concept in the redundancy elimination algorithm is that of a permutable
split. Intuitively, a split of G is called permutable if its root fragment F is per-
mutable with all other fragments in G which could end up above F . Because of
Lemma 3.3, we can then always pull F to the root by a sequence of rewrite steps.
This means that for any configuration of G, there is an equivalent configuration
whose root is F ? i.e., by choosing the split for F , we lose no equivalence classes.
Definition 4.1 Let R be a permutation system. A split S of a graph G is called R-
permutable iff the root fragment F of S is R-permutable with all other fragments in
G which are possible dominators of F in G.
In the graph of Fig. 1, all three splits are Rfol-permutable: For each of the upper
fragments, the other two upper fragments are possible dominators, but as all three
fragments are labelled with existential quantifiers and Rfol contains all permutations
of existential quantifiers, the fragments are permutable with each other. And indeed,
we can pick any of the three fragments as the root fragment, and the resulting split
will describe a representative of the single equivalence class of the graph.
Proposition 4.2 Let G be a hnc graph, and let S be a permutable split of G. Then
SC(S) = SC(G).
Proof. If G is unsolvable, the claim is trivially true. Otherwise, let C be an arbi-
trary configuration of G; we must show that S = (F,h1 7? G1, . . . ,hn 7? Gn) has a
configurationC? which is equivalent toC.
Let?s say that the fragments which properly dominate F in C are F1, . . . ,Fn
(n ? 0), ordered in such a way that Fi dominates Fj in C for all i < j. Each Fi is
a possible dominator of F , by Prop. 2.3. Because S is permutable, this means that
each Fi is permutable with F in G. By applying Lemma 3.3 n times (first to F and
Fn, then to F and Fn?1, and so on), we can compute a configuration C? of G in
which F is at the root and such that C? ?R C. But C is a configuration of S, which
proves the theorem. 2
This suggests the following redundancy elimination algorithm:
Redundancy-Elimination(Ch,G,R)
1 for each subgraph G? inCh
2 do if G? has an R-permutable split S
3 then remove all splits for G? except for S fromCh
Because of Prop. 4.2, the algorithm is correct in that for each configurationC of
G, the reduced chart still has a configurationC? withC?R C?. The particular choice
of S doesn?t affect the correctness of the algorithm (but may change the number
of remaining configurations). However, the algorithm is not complete in the sense
that the reduced chart can have no two equivalent configurations. We will illustrate
this below. We can further optimize the algorithm by deleting subgraphs (and their
splits) that are not referenced anymore by using reference counters. This doesn?t
change the set of solved forms of the chart, but may further reduce the chart size.
In the running example, we would run REDUNDANCY-ELIMINATION on the
chart in Fig. 3. As we have seen, all three splits of the entire graph are permutable,
so we can pick any of them e.g., the split with root fragment 2, and delete the splits
with root fragments 1 and 3. This reduces the reference count of some subgraphs
(e.g. {2,3,5,6,7}) to 0, so we can remove these subgraphs too. The resulting chart
is shown below, which represents a single solved form (the one shown in Fig. 2).
{1,2,3,4,5,6,7} : ?2,h2 7? {1,4},h4 7? {3,6,7}?
{1,4} : ?1,h1 7? {4}?
{3,6,7} : ?3,h5 7? {6},h6 7? {7}?
Now consider variations of the graph in Fig. 1 in which the quantifier labels are
different; these variant graphs have exactly the same chart, but fewer fragment pairs
will be permutable. If all three quantifiers are universal, then the configurations fall
into two equivalence classes which are distinguished by the relative scope of the
fragments 1 and 2. The algorithm will recognise that the split with root fragment 3
is permutable and delete the splits for 1 and 2. The resulting chart has two solved
forms. Thus the algorithm is still complete in this case. If, however, the fragments
1 and 2 are existential quantifiers and the fragment 3 is universal, there are three
equivalence classes, but the chart computed by the algorithm will have four solved
forms. The problem stems from the fact that neither of the existential quantifiers is
permutable as long as the universal quantifier is still in the same subgraph; but the
two configurations in which 2 dominates 3 are equivalent.
Runtime analysis. Given a graph G with n nodes and m edges, we can compute a
table which specifies for each pair u,v of root nodes whether there is a unique hole
of u from which v can be reached via a simple hypernormal path which doesn?t use
u, and which hole this is. A naive algorithm for doing this iterates over all u and v
and then performs a depth-first search through G, which takes time O(n2(n+m)),
which is a negligible runtime in practice.
Given this table, we can determine the possible dominators of each fragment
in time O(n) (because there are at most O(n) possible dominators). Thus it takes
time O(n) to decide whether a split is permutable, and time O(n ?S), where S is the
number of splits in the chart, to run the entire elimination algorithm. The reference
counting optimisation adds nothing to this asymptotic runtime, as each split may
trigger at most one reference count update for each hole of the split?s root fragment.
5 Conclusion
We have presented an algorithm for redundancy elimination on underspecified chart
representations. It checks for each subgraph in the chart whether it has a permutable
split; if yes, it removes all other splits for this subgraph. This reduces the set of
described readings, while making sure that at least one representative of each orig-
inal equivalence class remains while maintaining underspecification. Equivalence
is defined with respect to a certain class of rewriting systems which approximates
semantic equivalence of the described formulas and fits well with the underspecifi-
cation setting. The algorithm runs in polynomial time in the size of the chart.
The algorithm is useful in practice: it reduces the USRs for (1) and (2) from the
introduction to one and two solved forms, respectively. In fact, initial experiments
with the Rondane treebank suggest that it reduces the number of readings of a
typical sentence by an order of magnitude. It does this efficiently: Even on USRs
with billions of readings, for which the enumeration of readings would take about
a year, it finishes after a few seconds. However, the algorithm is not complete in
the sense that the computed chart has no more equivalent readings. We have some
ideas for achieving this kind of completeness, which we will explore in future work.
Another line in which the present work could be extended is to allow equivalence
with respect to arbitrary rewrite systems.
References
[1] Althaus, E., D. Duchier, A. Koller, K. Mehlhorn, J. Niehren and S. Thiel, An efficient graph
algorithm for dominance constraints, Journal of Algorithms 48 (2003), pp. 194?219.
[2] Blackburn, P. and J. Bos, ?Representation and Inference for Natural Language. A First Course
in Computational Semantics,? CSLI Publications, 2005.
[3] Bodirsky, M., D. Duchier, J. Niehren and S. Miele, An efficient algorithm for weakly normal
dominance constraints, in: ACM-SIAM Symposium on Discrete Algorithms (2004).
[4] Chaves, R. P., Non-redundant scope disambiguation in underspecified semantics, in:
Proceedings of the 8th ESSLLI Student Session, Vienna, 2003, pp. 47?58.
[5] Copestake, A. and D. Flickinger, An open-source grammar development environment and
broad-coverage english grammar using HPSG, in: Proc. of LREC, 2000.
[6] Copestake, A., D. Flickinger, C. Pollard and I. Sag, Minimal recursion semantics: An
introduction., Journal of Language and Computation (2004), to appear.
[7] Egg, M., A. Koller and J. Niehren, The Constraint Language for Lambda Structures, Logic,
Language, and Information 10 (2001), pp. 457?485.
[8] Fuchss, R., A. Koller, J. Niehren and S. Thater, Minimal recursion semantics as dominance
constraints: Translation, evaluation, and analysis, in: Proc. of ACL, Barcelona, 2004.
[9] Koller, A., J. Niehren and S. Thater, Bridging the gap between underspecification formalisms:
Hole semantics as dominance constraints, in: Proc. of EACL-03, 2003.
[10] Koller, A. and S. Thater, The evolution of dominance constraint solvers, in: Proc. of ACL-05
Workshop on Software, Ann Arbor, 2005.
[11] Niehren, J. and S. Thater, Bridging the gap between underspecification formalisms: Minimal
recursion semantics as dominance constraints, in: Proc. of ACL-03, 2003.
[12] van Deemter, K. and S. Peters, ?Semantic Ambiguity and Underspecification,? CSLI, 1996.
[13] Vestre, E., An algorithm for generating non-redundant quantifier scopings, in: Proc. of EACL,
Berlin, 1991, pp. 251?256.
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 10?15,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Semantic Approach To Textual Entailment:
System Evaluation and Task Analysis
Aljoscha Burchardt, Nils Reiter, Stefan Thater
Dept. of Computational Linguistics
Saarland University
Saarbr?cken, Germany
 
		Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 44?47,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Ranking Paraphrases in Context
Stefan Thater
Universit?t des Saarlandes
stth@coli.uni-sb.de
Georgiana Dinu
Universit?t des Saarlandes
dinu@coli.uni-sb.de
Manfred Pinkal
Universit?t des Saarlandes
pinkal@coli.uni-sb.de
Abstract
We present a vector space model that sup-
ports the computation of appropriate vec-
tor representations for words in context,
and apply it to a paraphrase ranking task.
An evaluation on the SemEval 2007 lexical
substitution task data shows promising re-
sults: the model significantly outperforms
a current state of the art model, and our
treatment of context is effective.
1 Introduction
Knowledge about paraphrases is of central impor-
tance to textual inference modeling. Systems which
support automatic extraction of large repositories
of paraphrase or inference rules like Lin and Pantel
(2001) or Szpektor et al (2004) thus form first-class
candidate resources to be leveraged for NLP tasks
like question answering, information extraction, or
summarization, and the meta-task of recognizing
textual entailment.
Existing knowledge bases still suffer a number
of limitations, making their use in applications
challenging. One of the most serious problems
is insensitivity to context. Natural-language infer-
ence is highly context-sensitive, the applicability
of inference rules depending on word sense and
even finer grained contextual distinctions in us-
age (Szpektor et al, 2007). Application of a rule
like ?X shed Y ? X throw Y? is appropriate in a
sentence like ?a mouse study sheds light on the
mixed results,? but not in sentences like ?the econ-
omy seems to be shedding fewer jobs? or ?cats
do not shed the virus to other cats.? Systems like
the above-mentioned ones base the extraction of
inference rules on distributional similarity of words
rather than word senses, and apply unconditionally
whenever one side of the rule matches on the word
level, which may lead to considerable precision
problems (Geffet and Dagan, 2005) .
Some approaches address the problem of con-
text sensitivity by deriving inference rules whose
argument slots bear selectional preference infor-
mation (Pantel et al, 2007; Basili et al, 2007). A
different line of accounting for contextual variation
has been taken by Mitchell and Lapata (2008), who
propose a compositional approach, ?contextualiz-
ing? the vector-space meaning representation of
predicates by combining the distributional proper-
ties of the predicate with those of its arguments.
A related approach has been proposed by Erk and
Pad? (2008), who integrate selectional preferences
into the compositional picture. In this paper, we
propose a context-sensitive vector-space approach
which draws some important ideas from Erk and
Pado?s paper (?E&P? in the following), but imple-
ments them in a different, more effective way: An
evaluation on the SemEval 2007 lexical substitu-
tion task data shows that our model significantly
outperforms E&P in terms of average precision.
Plan of the paper. Section 2 presents our model
and briefly relates it to previous work. Section 3
describes the evaluation of our model on the lexical
substitution task data. Section 4 concludes.
2 A model for meaning in context
We propose a dependency-based model whose di-
mensions reflect dependency relations, and distin-
guish two kinds or layers of lexical meaning: ar-
gument meaning and predicate meaning. The argu-
ment meaning of a word w is a vector representing
frequencies of all pairs (w
?
,r
?
) of predicate expres-
sions w
?
and dependency relations r
?
such that w
?
stands in relation r
?
to w. Intuitively, argument
meaning is similar to E&P?s ?inverse selectional
preferences.? Argument meanings are used for two
purposes in our model: (i) to construct predicate
meanings, and (ii) to contextually constrain them.
For technical convenience, we will use a defini-
tional variant of argument meaning, by indexing
it with an ?incoming? relation, which allows pred-
icate and argument meaning to be treated techni-
cally as vectors of the same type. Assuming a set
44
R of role labels and a set W of words, we represent
both predicate and argument meaning as vectors
in a vector space V with a basis {e
i
}
i?R?R?W
, i.e.,
a vector space whose dimensions correspond to
triples of two role labels and a word. The argument
meaning v
r
(w) of a word w is defined as follows:
v
r
(w) =
?
w
?
?W,r
?
?R
f (w
?
,r
?
,w) ? e
(r,r
?
,w
?
)
, (1)
where r is the ?incoming? relation, and f (w
?
,r
?
,w)
denotes the frequency of w occurring in relation r
?
to w
?
in a collection of dependency trees. To obtain
predicate meaning v
P
(w), we count the occurrences
of argument words w
?
standing in relation r to w,
and compute the predicate meaning as the sum of
the argument meanings v
r
(w
?
), weighted by these
co-occurrence frequencies:
v
P
(w) =
?
r?R,w
?
?W
f (w,r,w
?
) ? v
r
(w
?
) (2)
That is, the meaning of a predicate is modelled by a
vector representing ?second order? co-coccurrence
frequencies with other predicates.
In general, words have both a ?downward look-
ing? predicate meaning and an ?upward looking?
argument meaning. In our study, only one of them
will be relevant, since we will restrict ourselves
to local predicate-argument structures with verbal
heads and nominal arguments.
Computing meaning in context. Vectors repre-
senting predicate meaning are derived by collecting
co-occurrence frequencies for all uses of the pred-
icate, possibly resulting in vector representations
in which different meanings of the predicate are
combined. Given an instance of a predicate w that
has arguments w
1
, . . . ,w
k
, we can now contextually
constrain the predicate meaning of w by the argu-
ment meanings of its arguments. Here, we propose
to simple ?restrict? the predicate meaning to those
dimensions that have a non-zero value in at least
one of its argument meanings. More formally, we
write v
|v
?
to denote a vector that is identical to v
for all components that have a non-zero value in v
?
,
zero otherwise. We compute predicate meaning in
context as follows:
v
P
(w)
|
?
1?i?k
v
r
i
(w
i
)
, (3)
where r
i
is the argument position filled by w
i
.
Parameters. To reduce the effect of noise and
provide a more fine-grained control over the ef-
fect of context, we can choose different thresholds
target subject object paraphrases
shed study light throw 3, reveal 2, shine 1
shed cat virus spread 2, pass 2, emit 1, transmit 2
shed you blood lose 3, spill 1, give 1
Table 1: Lexical substitution task data set
for function f in the computation of predicate and
argument meaning. In Section 3, we obtain best
results if we consider only dependency relations
that occur at least 6 times in the British National
Corpus (BNC) for the computation of predicate
meaning, and relations occurring at least 15 times
for the computation of argument meanings when
predicate meaning is contextually constrained.
Related work. Our model is similar to the struc-
tured vector space model proposed by Erk and Pad?
(2008) in that the representation of predicate mean-
ing is based on dependency relations, and that ?in-
verse selectional preferences? play an important
role. However, inverse selectional preferences are
used in E&P?s model mainly to compute mean-
ing in context, while they are directly ?built into?
the vectors representing predicate meaning in our
model.
3 Evaluation
We evaluate our model on a paraphrase ranking
task on a subset of the SemEval 2007 lexical substi-
tution task (McCarthy and Navigli, 2007) data, and
compare it to a random baseline and E&P?s state
of the art model.
Dataset. The lexical substitution task dataset con-
tains 10 instances for 44 target verbs in different
sentential contexts. Systems that participated in
the task had to generate paraphrases for each of
these instances, which are evaluated against a gold
standard containing up to 9 possible paraphrases
for individual instances. Following Erk and Pad?
(2008), we use the data in a different fashion: we
pool paraphrases for all instances of a verb in all
contexts, and use the models to rank these para-
phrase candidates in specific contexts.
Table 1 shows three instances of the target verb
shed together with its paraphrases in the gold stan-
dard as an expample. The paraphrases are attached
with weights, which correspond to the number of
times they have been given by different annotators.
To allow for a comparision with E&P?s model,
we follow Erk and Pad? (2008) and extract only
sentences from the dataset containing target verbs
45
with overtly realized subject and object, and re-
move instances from the dataset for which the tar-
get verb or one of its arguments is not in the BNC.
We obtain a set of 162 instances for 34 different
verbs. We also remove paraphrases that are not
in the BNC. On average, target verbs have 20.5
paraphrase candidates, 3.9 of which are correct in
specific contexts.
Experimental setup. We parse the BNC using
MiniPar (Lin, 1993) and extract co-occurrence fre-
quencies, considering only dependency relations
for the most frequent 2000 verbs. We don?t use raw
frequency counts directly but reweight the vectors
by pointwise mutual information.
To rank paraphrases in context, we compute con-
textually constrained vectors for the verb in the
input sentence and all its paraphrase candidates
by taking the corresponding predicate vectors and
restricting them to the argument meanings of the
argument head nouns in the input sentence. The
restricted vectors for the paraphrase candidates are
then ranked by comparing them to the restricted
vector of the input verb using cosine similarity.
In order to compare our model with state of the
art, we reimplement E&P?s structured vector space
model. We filter stop words, and compute lexical
vectors in a ?syntactic? space using the most fre-
quent 2000 words from the BNC as basis. We also
consider a variant in which the basis corresponds
to words indexed by their grammatical roles. We
choose parameters that Erk and Pad? (2009) report
to perform best, and use the method described in
Erk and Pad? (2009) to compute vectors in context.
Evaluation metrics. As scoring methods, we
use both ?precision out of ten? (P
oot
), which was
originally used in the lexical substitution task and
also used by E&P, and generalized average preci-
sion (Kishida, 2005), a variant of average precision
which is frequently used in information extraction
tasks and has also been used in the PASCAL RTE
challenges (Dagan et al, 2006).
P
oot
can be defined as follows:
P
oot
=
?
s?M
?
G
f (s)
?
s?G
f (s)
,
where M is the list of 10 paraphrase candidates
top-ranked by the model, G is the corresponding
annotated gold data, and f (s) is the weight of the
individual paraphrases. Here, P
oot
is computed for
each target instance separately; below, we report
the average over all instances.
Model P
oot
GAP
Random baseline 54.25 26.03
E&P (target only) 64.61 (63.31) 29.95 (32.02)
E&P (add, object only) 66.20 (62.90) 29.93 (31.54)
E&P (min, both) 64.86 (59.62) 32.22 (31.28)
TDP 63.32 36.54
TDP (target only) 62.60 33.04
Table 2: Results
Generalized average precision (GAP) is a more
precise measure than P
oot
: Applied to a ranking
task with about 20 candidates, P
oot
just gives the
percentage of good candidates found in the upper
half of the proposed ranking. Average precision
is sensitive to the relative position of correct and
incorrect candidates in the ranking, GAP moreover
rewards the correct order of positive cases w.r.t.
their gold standard weight.
We define average precision first:
AP =
?
n
i=1
x
i
p
i
R
p
i
=
?
i
k=1
x
k
i
where x
i
is a binary variable indicating whether
the ith item as ranked by the model is in the gold
standard or not, R is the size of the gold standard,
and n the number of paraphrase candidates to be
ranked. If we take x
i
to be the gold standard weight
of the ith item or zero if it is not in the gold standard,
we can define generalized average precision as
follows:
GAP =
?
n
i=1
I(x
i
) p
i
R
?
R
?
= ?
R
i=1
I(y
i
)y
i
where I(x
i
) = 1 if x
i
is larger than zero, zero oth-
erwise, and y
i
is the average weight of the ideal
ranked list y
1
, . . . ,y
i
of paraphrases in the gold stan-
dard.
Results and discussion. Table 2 shows the re-
sults of our experiments for two variants of our
model (?TDP?), and compares them to a random
baseline and three instantiations (in two variants) of
E&P?s model. The ?target only? models don?t use
context information, i.e., paraphrases are ranked by
cosine similarity of predicate meaning only. The
other models take context into account. The ?min?
E&P model takes the component-wise minimum to
combine a lexical vector with context vectors and
considers both subject and object as context; it is
the best performing model in Erk and Pad? (2009).
The ?add? model uses vector addition and consid-
ers only objects as context; it is the best-performing
46
020
40
60
80
100
1 2 3 4 5 6 7 8 9 10
p
r
e
s
i
s
i
o
n
 
o
u
t
 
o
f
 
n
E&P (add, object only) 
present paper
baseline
upper bound
Figure 1: ?Precision out of n? for 1? n? 10.
model (in terms of P
oot
) for our dataset. The num-
bers in brackets refer to variants of the E&P models
in which the basis corresponds to words indexed
by their syntactic roles. Note that the results for the
E&P models are better than the results published
in Erk and Pad? (2009), which might be due to
slightly different datasets or lists of stop-words.
As can be seen, our model performs > 10% bet-
ter than the random baseline. It performs > 4%
better than the ?min? E&P model and > 6% better
then the ?add? model in terms of GAP if we use a
vectors space with words as basis. For the variants
of the E&P models in which the basis corresponds
to words indexed by their syntactic role, we ob-
tain different results, but our model is still > 4%
better than these variants. We can also see that
our treatment of context is effective, leading to a
> 3% increase of GAP. A stratified shuffling-based
randomization test (Yeh, 2000) shows that the dif-
ferences are statistically significant (p < 0.05).
In terms of P
oot
, the ?add? E&P model performs
better than our model, which might look surprising,
given its low GAP score. Fig. 1 gives a more fine-
grained comparison between the two models. It
displays the ?precision out of n? of the two models
for varying n. As can be seen, our model performs
better for all n< 10, and much better than the base-
line and E&P for n? 4.
4 Conclusion
In this paper, we have proposed a dependency-
based context-sensitive vector-space approach that
supports the computation of adequate vector-based
representations of predicate meaning in context.
An evaluation on a paraphrase ranking task using
a subset of the SemEval 2007 lexical substitution
task data shows promising results: our model per-
forms significantly better than a current state of the
art system (Erk and Pad?, 2008), and our treatment
of context is effective.
Since the dataset we used for the evaluation is
relatively small, there is a potential danger for over-
fitting, and it remains to be seen whether the results
carry over to larger datasets. First experiments
indicate that this is actually the case.
We expect that our approach can be generalized
to arrive at a general compositional model, which
would allow to compute contextually appropriate
meaning representations for complex relational ex-
pressions rather than single lexical predicates.
Acknowledgements. We thank Katrin Erk and
Sebastian Pad? for help and critical comments.
References
R. Basili, D. De Cao, P. Marocco, and M. Pennacchiotti. 2007.
Learning selectional preferences for entailment or para-
phrasing rules. In Proc. of RANLP 2007.
I. Dagan, O. Glickman, and B. Magnini. 2006. The PASCAL
Recognising Textual Entailment Challenge. In Machine
Learning Challenges, volume 3944. Springer.
K. Erk and S. Pad?. 2008. A structured vector space model
for word meaning in context. In Proc. of EMNLP.
K. Erk and S. Pad?. 2009. Paraphrase assessment in struc-
tured vector space: Exploring parameters and datasets. In
Proc. of the Workshop on Geometrical Models of Natural
Language Semantics, Athens.
M. Geffet and I. Dagan. 2005. The distributional inclusion
hypotheses and lexical entailment. In Proc. of the ACL.
K. Kishida. 2005. Property of average precision and its
generalization: An examination of evaluation indicator for
information retrieval experiments. NII Technical Report.
D. Lin and P. Pantel. 2001. DIRT ? Discovery of Inference
Rules from Text. In Proc. of the ACM Conference on
Knowledge Discovery and Data Mining, San Francisco.
D. Lin. 1993. Principle-based parsing without overgeneration.
In Proc. of ACL, Columbus.
D. McCarthy and R. Navigli. 2007. SemEval-2007 Task 10:
English Lexical Substitution Task. In Proc. of SemEval,
Prague.
J. Mitchell and M. Lapata. 2008. Vector-based models of se-
mantic composition. In Proc. of ACL-08: HLT, Columbus.
P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and E. Hovy.
2007. ISP: Learning inferential selectional preferences. In
Human Language Technologies 2007, Rochester.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scal-
ing web-based acquisition of entailment relations. In Proc.
of EMNLP, Barcellona.
I. Szpektor, E. Shnarch, and I. Dagan. 2007. Instance-based
evaluation of entailment rule acquisition. In Proc. of ACL.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proc. of COLING.
47
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782?792,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Robust Disambiguation of Named Entities in Text
Johannes Hoffart1, Mohamed Amir Yosef1, Ilaria Bordino2, Hagen Fu?rstenau3,
Manfred Pinkal3, Marc Spaniol1, Bilyana Taneva1, Stefan Thater3, Gerhard Weikum1
1 Max Planck Institute for Informatics, Saarbru?cken, Germany
2 Yahoo! Research Lab, Barcelona, Spain
3 Saarland University, Saarbru?cken, Germany
{jhoffart,mamir,mspaniol,btaneva,weikum}@mpi-inf.mpg.de
bordino@yahoo-inc.com {hagenf,pinkal,stth}@coli.uni-sb.de
Abstract
Disambiguating named entities in natural-
language text maps mentions of ambiguous
names onto canonical entities like people or
places, registered in a knowledge base such as
DBpedia or YAGO. This paper presents a ro-
bust method for collective disambiguation, by
harnessing context from knowledge bases and
using a new form of coherence graph. It unifies
prior approaches into a comprehensive frame-
work that combines three measures: the prior
probability of an entity being mentioned, the
similarity between the contexts of a mention
and a candidate entity, as well as the coherence
among candidate entities for all mentions to-
gether. The method builds a weighted graph of
mentions and candidate entities, and computes
a dense subgraph that approximates the best
joint mention-entity mapping. Experiments
show that the new method significantly outper-
forms prior methods in terms of accuracy, with
robust behavior across a variety of inputs.
1 Introduction
1.1 Motivation
Web pages, news articles, blog postings, and other
Internet data contain mentions of named entities such
as people, places, organizations, etc. Names are often
ambiguous: the same name can have many different
meanings. For example, given a text like ?They per-
formed Kashmir, written by Page and Plant. Page
played unusual chords on his Gibson.?, how can we
tell that ?Kashmir? denotes a song by Led Zeppelin
and not the Himalaya region (and that Page refers
to guitarist Jimmy Page and not to Google founder
Larry Page, and that Gibson is a guitar model rather
than the actor Mel Gibson)?
Establishing these mappings between the mentions
and the actual entities is the problem of named-entity
disambiguation (NED).
If the possible meanings of a name are known up-
front - e.g., by using comprehensive gazetteers such
as GeoNames (www.geonames.org) or knowledge
bases such as DBpedia (Auer07), Freebase (www.
freebase.com), or YAGO (Suchanek07), which
have harvested Wikipedia redirects and disambigua-
tion pages - then the simplest heuristics for name res-
olution is to choose the most prominent entity for a
given name. This could be the entity with the longest
Wikipedia article or the largest number of incoming
links in Wikipedia; or the place with the most inhab-
itants (for cities) or largest area, etc. Alternatively,
one could choose the entity that uses the mention
most frequently as a hyperlink anchor text. For the
example sentence given above, all these techniques
would incorrectly map the mention ?Kashmir? to the
Himalaya region. We refer to this suite of methods
as a popularity-based (mention-entity) prior.
Key to improving the above approaches is to con-
sider the context of the mention to be mapped, and
compare it - by some similarity measure - to contex-
tual information about the potential target entities.
For the example sentence, the mention ?Kashmir?
has context words like ?performed? and ?chords? so
that we can compare a bag-of-words model against
characteristic words in the Wikipedia articles of the
different candidate entities (by measures such as co-
sine similarity, weighted Jaccard distance, KL diver-
gence, etc.). The candidate entity with the highest
similarity is chosen. Alternatively, labeled training
data can be harnessed to learn a multi-way classifier,
and additional features like entire phrases, part-of-
speech tags, dependency-parsing paths, or nearby
782
hyperlinks can be leveraged as well. These methods
work well for sufficiently long and relatively clean
input texts such as predicting the link target of a Wi-
kipedia anchor text (Milne08). However, for short or
more demanding inputs like news, blogs, or arbitrary
Web pages, relying solely on context similarity can-
not achieve near-human quality. Similarity measures
based on syntactically-informed distributional mod-
els require minimal context only. They have been
developed for common nouns and verbs (Thater10),
but not applied to named entities.
The key to further improvements is to jointly con-
sider multiple mentions in an input and aim for a col-
lective assignment onto entities (Kulkarni09). This
approach should consider the coherence of the re-
sulting entities, in the sense of semantic relatedness,
and it should combine such measures with the con-
text similarity scores of each mention-entity pair. In
our example, one should treat ?Page?, ?Plant? and
?Gibson? also as named-entity mentions and aim to
disambiguate them together with ?Kashmir?.
Collective disambiguation works very well when a
text contains mentions of a sufficiently large number
of entities within a thematically homogeneous con-
text. If the text is very short or is about multiple, un-
related or weakly related topics, collective mapping
tends to produce errors by directing some mentions
towards entities that fit into a single coherent topic
but do not capture the given text. For example, a text
about a football game between ?Manchester? and
?Barcelona? that takes place in ?Madrid? may end up
mapping either all three of these mentions onto foot-
ball clubs (i.e., Manchester United, FC Barcelona,
Real Madrid) or all three of them onto cities. The
conclusion here is that none of the prior methods
for named-entity disambiguation is robust enough to
cope with such difficult inputs.
1.2 Contribution
Our approach leverages recently developed knowl-
edge bases like YAGO as an entity catalog and a
rich source of entity types and semantic relationships
among entities. These are factored into new measures
for the similarity and coherence parts of collectively
disambiguating all mentions in an input text. For
similarity, we also explore an approach that lever-
ages co-occurrence information obtained from large,
syntactically parsed corpora (Thater10).
We cast the joint mapping into the following graph
problem: mentions from the input text and candidate
entities define the node set, and we consider weighted
edges between mentions and entities, capturing con-
text similarities, and weighted edges among entities,
capturing coherence. The goal on this combined
graph is to identify a dense subgraph that contains
exactly one mention-entity edge for each mention,
yielding the most likely disambiguation. Such graph
problems are NP-hard, as they generalize the well-
studied Steiner-tree problem. We develop a greedy
algorithm that provides high-quality approximations,
and is customized to the properties of our mention-
entity graph model.
In addition to improving the above assets for the
overall disambiguation task, our approach gains in
robustness by using components selectively in a self-
adapting manner. To this end, we have devised the
following multi-stage procedure.
? For each mention, we compute popularity priors
and context similarities for all entity candidates
as input for our tests.
? We use a threshold test on the prior to decide
whether popularity should be used (for mentions
with a very high prior) or disregarded (for men-
tions with several reasonable candidates).
? When both the entity priors and the context simi-
larities are reasonably similar in distribution for
all the entity candidates, we keep the best candi-
date and remove all others, fixing this mention
before running the coherence graph algorithm.
We then run the coherence graph algorithm on all
the mentions and their remaining entity candidates.
This way, we restrict the coherence graph algorithm
to the critical mentions, in situations where the goal
of coherence may be misleading or would entail high
risk of degradation.
The paper makes the following novel contribu-
tions: 1) a framework for combining popularity pri-
ors, similarity measures, and coherence into a robust
disambiguation method; 2) new measures for defin-
ing mention-entity similarity; 3) a new algorithm
for computing dense subgraphs in a mention-entity
graph, which produces high-quality mention-entity
mappings; 4) an empirical evaluation on a demand-
ing corpus (based on additional annotations for the
dataset of the CoNLL 2003 NER task), with signifi-
783
cant improvements over state-of-the-art opponents.
2 State of the Art
Recognizing named entities (NER tagging) in natural-
language text has been extensively addressed in NLP
research. The output is labeled noun phrases. How-
ever, these are not yet canonical entities, explicitly
and uniquely denoted in a knowledge repository.
Approaches that use Wikipedia for explicit disam-
biguation date back to (Bunescu06) and have been
further pursued by (Cucerzan07; Han09; Milne08;
Nguyen08; Mihalcea07). (Bunescu06) defined a sim-
ilarity measure that compared the context of a men-
tion to the Wikipedia categories of an entity candi-
date. (Cucerzan07; Milne08; Nguyen08) extended
this framework by using richer features for the simi-
larity comparison. (Milne08) additionally introduced
a supervised classifier for mapping mentions to en-
tities, with learned feature weights rather than using
the similarity function directly. (Milne08) introduced
a notion of semantic relatedness between a mention?s
candidate entities and the unambiguous mentions in
the textual context. The relatedness values are de-
rived from the overlap of incoming links in Wikipedia
articles. (Han09) considered another feature: the re-
latedness of common noun phrases in a mention?s
context, matched against Wikipedia article names.
While these features point towards semantic coher-
ence, the approaches are still limited to mapping each
mention separately. Nonetheless, this line of feature-
rich similarity-driven methods achieved very good
results in experiments, especially for the task of pre-
dicting Wikipedia link targets for a given href anchor
text. On broader input classes such as news articles
(called ?wikification in the wild? in (Milne08)), the
precision was reported to be about 75 percent.
The first work with an explicit collective-learning
model for joint mapping of all mentions has been
(Kulkarni09). This method starts with a supervised
learner for a similarity prior, and models the pair-
wise coherence of entity candidates for two different
mentions as a probabilistic factor graph with all pairs
as factors. The MAP (maximum a posteriori) es-
timator for the joint probability distribution of all
mappings is shown to be an NP-hard optimization
problem, so that (Kulkarni09) resorts to approxima-
tions and heuristics like relaxing an integer linear
program (ILP) into an LP with subsequent round-
ing or hill-climbing techniques. The experiments in
(Kulkarni09) show that this method is superior to the
best prior approaches, most notably (Milne08). How-
ever, even approximate solving of the optimization
model has high computational costs.
Coreference resolution is the task of mapping
mentions like pronouns or short phrases to a pre-
ceding, more explicit, mention. Recently, interest
has arisen in cross-document coreference resolution
(Mayfield09), which comes closer to NED, but does
not aim at mapping names onto entities in a knowl-
edge base. Word sense disambiguation (McCarthy09;
Navigli09) is the more general task of mapping con-
tent words to a predefined inventory of word senses.
While the NED problem is similar, it faces the chal-
lenges that the ambiguity of entity names tends to be
much higher (e.g., mentions of common lastnames
or firstname-only).
Projects on automatically building knowledge
bases (Doan08) from natural-language text include
KnowItAll (Banko07), YAGO and its tool SOFIE
(Suchanek09; Nakashole11), StatSnowball (Zhu09),
ReadTheWeb (Carlson10), and the factor-graph work
by (Wick09). Only SOFIE maps names onto canon-
ical entities; the other projects produce output with
ambiguous names. SOFIE folds the NED into its
MaxSat-based reasoning for fact extraction. This ap-
proach is computationally expensive and not intended
for online disambiguation of entire texts.
3 Framework
Mentions and Ambiguity: We consider an input
text (Web page, news article, blog posting, etc.) with
mentions (i.e., surface forms) of named entities (peo-
ple, music bands, songs, universities, etc.) and aim
to map them to their proper entries in a knowledge
base, thus giving a disambiguated meaning to entity
mentions in the text. We first identify noun phrases
that potentially denote named entities. We use the
Stanford NER Tagger (Finkel05) to discover these
and segment the text accordingly.
Entity Candidates: For possible entities (with
unique canonical names) that a mention could denote,
we harness existing knowledge bases like DBpedia
or YAGO. For each entity they provide a set of short
names (e.g., ?Apple? for Apple Inc. and para-
784
phrases (e.g., ?Big Apple? for New York City).
In YAGO, these are available by the means relation,
which in turn is harvested from Wikipedia disam-
biguation pages, redirects, and links.
Popularity Prior for Entities: Prominence or
popularity of entities can be seen as a probabilistic
prior for mapping a name to an entity. The most com-
mon way of estimating this are the Wikipedia-based
frequencies of particular names in link anchor texts
referring to specific entities, or number of inlinks.
Context Similarity of Mentions and Entities:
The key for mapping mentions onto entities are the
contexts on both sides of the mapping. We consider
two different approaches. First, for each mention,
we construct a context from all words in the entire
input text. This way, we can represent a mention
as a set of (weighted) words or phrases that it co-
occurs with. Second, we alternatively consider simi-
larity scores based on syntactically-parsed contexts,
based on (Thater10). On the entity side of the map-
ping, we associate each entity with characteristic
keyphrases or salient words, precomputed from Wi-
kipedia articles and similar sources. For example,
Larry Page would have keyphrases like ?Stan-
ford?, ?search engine?, etc., whereas Jimmy Page
may have keyphrases ?Gibson guitar?, ?hard rock?,
etc. Now we can define and compute similarity mea-
sures between a mention and an entity candidate,
e.g., the weighted word overlap, the KL divergence,
n-gram-based measures, etc. In addition, we may
use syntactic contextualization techniques, based on
dependency trees, that suggest phrases that are typi-
cally used with the same verb that appears with the
mention in the input text (Thater10).
Coherence among Entities: On the entity side,
each entity has a context in the underlying knowl-
edge base(s): other entities that are connected via
semantic relationships (e.g., memberOf) or have the
same semantic type (e.g., rock musician). An
asset that knowledge bases like DBpedia and YAGO
provide us with is the same-as cross-referencing to
Wikipedia. This way, we can quantify the coherence
between two entities by the number of incoming links
that their Wikipedia articles share. When we consider
candidate entities for different mentions, we can now
define and compute a notion of coherence among the
corresponding entities, e.g., by the overlap among
their related entities or some form of type distance.
Coherence is a key asset because most texts deal with
a single or a few semantically related topics such as
rock music or Internet technology or global warming,
but not everything together.
Overall Objective Function: To aim for the best
disambiguation mappings, our framework combines
prior, similarity, and coherence measures into a
combined objective function: for each mention mi,
i = 1..k, select entity candidates eji , one per men-
tion, such that
? ?
?
i=1..k
prior(mi, eji)+
? ?
?
i=1..k
sim(cxt(mi), cxt(eji))+
? ? coh(ej1 ? cnd(m1) . . . ejk ? cnd(mk)) = max!
where ? + ? + ? = 1, cnd(mi) is the set of pos-
sible meanings of mi, cxt( ) denotes the context of
mentions and entities, respectively, and coh( ) is the
coherence function for a set of entities.
Section 4 gives details on each of these three com-
ponents. For robustness, our solution selectively en-
ables or disables the three components, based on tests
on the mentions of the input text; see Section 5.
4 Features and Measures
4.1 Popularity Prior
As mentioned above, our framework supports multi-
ple forms of popularity-based priors, but we found a
model based on Wikipedia link anchors to be most
effective: For each surface form that constitutes an
anchor text, we count how often it refers to a partic-
ular entity. For each name, these counts provide us
with an estimate for a probability distribution over
candidate entities. For example, ?Kashmir? refers to
Kashmir (the region) in 90.91% of all occurrences
and in 5.45% to Kashmir (Song).
4.2 Mention-Entity Similarity
Keyphrase-based Similarity: On the mention side,
we use all tokens in the document (except stopwords
and the mention itself) as context. We experimented
with a distance discount to discount the weight of
tokens that are further away, but this did not improve
the results for our test data.
On the entity side, the knowledge base knows au-
thoritative sources for each entity, for example, the
785
corresponding Wikipedia article or an organizational
or individual homepage. These are the inputs for
an offline data-mining step to determine character-
istic keyphrases for each entity and their statistical
weights. We describe this only for Wikipedia as in-
put corpus, the approach extends to other inputs. As
keyphrase candidates for an entity we consider its
corresponding Wikipedia article?s link anchors texts,
including category names, citation titles, and external
references. We extended this further by considering
also the titles of articles linking to the entity?s article.
All these phrases form the keyphrase set of an entity:
KP (e).
For each word w that occurs in a keyphrase, we
compute a specificity weight with regard to the given
entity: the MI (mutual information) between the en-
tity e and the keyword w, calculating the joint proba-
bilities for MI as follows:
p(e, w) =
??w ?
(
KP (e) ??e??INe KP (e?)
)??
N
reflecting if w is contained in the keyphrase set of e
or any of the keyphrase sets of an entity linking to e,
IN(e), with N denoting the total number of entities.
The joint probabilities for the cases p(e, w?), p(e?, w),
p(e?, w?) are calculated accordingly.
Keyphrases may occur only partially in an input
text. For example, the phrase ?Grammy Award win-
ner? associated with entity Jimmy Page may oc-
cur only in the form ?Grammy winner? near some
mention ?Page?. Therefore, our algorithm for the
similarity of mention m with regard to entity e com-
putes partial matches of e?s keyphrases in the text.
This is done by matching individual words and re-
warding their proximity in an appropriate score. To
this end we compute, for each keyphrase, the shortest
window of words that contains a maximal number
of words of the keyphrase. We refer to this window
as the phrase?s cover (cf. (Taneva11)). For example,
matching the text ?winner of many prizes including
the Grammy? results in a cover length of 7 for the
keyphrase ?Grammy award winner?. By this ratio-
nale, the score of partially matching phrase q in a text
is set to:
score(q) = z
(?
w?cover weight(w)?
w?q weight(w)
)2
where z = # matching wordslength of cover(q) andweight(w) is eitherthe MI weight (defined above) or the collection-wide
IDF weight of the keyphrase word w. Note that the
second factor is squared, so that there is a superlinear
reduction of the score for each word that is missing
in the cover.
For the similarity of a mention m to candidate
entity e, this score is aggregated over all keyphrases
of e and all their partial matches in the text, leading
to the similarity score
simscore(m, e) =
?
q?KP (e)
score(q)
Syntax-based Similarity: In addition to surface
features of words and phrases, we leverage informa-
tion about the immediate syntactic context in which
an entity mention occurs. For example, in the sen-
tence ?Page played unusual chords?, we can extract
the fact that the mention ?Page? is the subject of the
verb ?play?. Using a large text corpus for training,
we collect statistics about what kinds of entities tend
to occur as subjects of ?play?, and then rank the can-
didate entities according to their compatibility with
the verb.
Specifically, we employ the framework of
(Thater10), which allows us to derive vector represen-
tations of words in syntactic contexts (such as being
the subject of a particular verb). We do not directly
apply this model to derive contextualized representa-
tions of entity mentions, as information about specific
proper names is very sparse in corpora like GigaWord
or Wikipedia. Instead, we consider a set of substi-
tutes for each possible entity e, which we take as its
context cxt(e). For this, we use the WordNet synsets
associated with the entity?s YAGO types and all their
hypernyms. For each substitute, we compute a stan-
dard distributional vector and a contextualized vector
according to (Thater10). Syntax-based similarity be-
tween cxt(e) and the context cxt(m) of the mention
is then defined as the sum of the scalar-product simi-
larity between these two vectors for each substitute.
This results in high similarity if the syntactic contex-
tualization only leads to small changes of the vectors,
reflecting the compatibility of the entity?s substitutes.
In our example, we compute a vector for ?gui-
tarist? as subject of ?play?, and another one for ?en-
trepreneur? in the same context. The former is more
786
compatible with the given context than the latter, lead-
ing to higher similarity for the entity Jimmy Page.
4.3 Entity-Entity Coherence
As all entities of interest are registered in a knowl-
edge base (like YAGO), we can utilize the semantic
type system, which is usually a DAG of classes. The
simples measure is the distance between two entities
in terms of type and subclassOf edges.
The knowledge bases also provide same-as cross-
referencing to Wikipedia, amd we quantify the coher-
ence between two entities by the number of incom-
ing links that their Wikipedia articles share. This
approach has been refined by Milne and Witten
(Milne08), taking into account the total number N of
entities in the (Wikipedia) collection:
mw coh(e1, e2) =
1? log (max(|INe1 |, |INe2 |))? log(|INe1 ? INe2 |)log(|N |)? log (min(|INe1 |, |INe2 |))
if > 0 and else set to 0.
5 Graph Model and Algorithms
5.1 Mention-Entity Graph
From the popularity, similarity, and coherence mea-
sures discussed in Section 4, we construct a weighted,
undirected graph with mentions and candidate enti-
ties as nodes. As shown in the example of Figure 1,
the graph has two kinds of edges:
? A mention-entity edge is weighted with a similar-
ity measure or a combination of popularity and
similarity measure. Our experiments will use a
linear combination with coefficients learned from
withheld training data.
? An entity-entity edge is weighted based on
Wikipedia-link overlap, or type distance, or some
combination along these lines.
Our experiments will focus on anchor-based pop-
ularity, keyphrase-based and/or syntactic similarity,
and link-based coherence (mw coh). The mention-
entity graph is dense on the entities side and often has
hundreds or thousands of nodes, as the YAGO knowl-
edge base offers many candidate entities for common
mentions (e.g., country names that could also denote
sports teams, common lastnames, firstnames, etc.).
5.2 Graph Algorithm
Given a mention-entity graph, our goal is to com-
pute a dense subgraph that would ideally contain all
mention nodes and exactly one mention-entity edge
for each mention, thus disambiguating all mentions.
We face two main challenges here. The first is how
to specify a notion of density that is best suited for
capturing the coherence of the resulting entity nodes.
The seemingly most natural approach would be to
measure the density of a subgraph in terms of its total
edge weight. Unfortunately, this will not work ro-
bustly for the disambiguation problem. The solution
could be dominated by a few entity nodes with very
high weights of incident edges, so the approach could
work for prominent targets, but it would not achieve
high accuracy also for the long tail of less prominent
and more sparsely connected entities. We need to
capture the weak links in the collective entity set of
the desired subgraph. For this purpose, we define
the weighted degree of a node in the graph to be the
total weight of its incident edges. We then define the
density of a subgraph to be equal to the minimum
weighted degree among its nodes. Our goal is to
compute a subgraph with maximum density, while
observing constraints on the subgraph structure.
The second critical challenge that we need to face
is the computational complexity. Dense-subgraph
problems are almost inevitably NP-hard as they gen-
eralize the Steiner-tree problem. Hence, exact algo-
rithms on large input graphs are infeasible.
To address this problem, we adopt and extend an
approximation algorithm of (Sozio10) for the prob-
lem of finding strongly interconnected, size-limited
groups in social networks. The algorithm starts from
the full mention-entity graph and iteratively removes
the entity node with the smallest weighted degree.
Among the subgraphs obtained in the various steps,
the one maximizing the minimum weighted degree
will be returned as output. To guarantee that we
arrive at a coherent mention-entity mapping for all
mentions, we enforce each mention node to remain
connected to at least one entity. However, this con-
straint may lead to very suboptimal results.
For this reason, we apply a pre-processing phase to
prune the entities that are only remotely related to the
mention nodes. For each entity node, we compute the
distance from the set of all mention nodes in terms
787
They performed 
Kashmir,  
written by  
Page    
and Plant.   
Page played  
unusual chords  
on his Gibson. 
?? Led Zeppelin 
?? Hard rock 
?? Electric guitar 
?? Session guitarist 
?? Led Zeppelin 
?? Gibson 
?? Jimmy Page 
signature model 
?? Hard rock 
Kashmir (song) 
Kashmir (region) 
Larry Page 
Jimmy Page 
Page, Arizona 
Robert Plant 
Gibson Les Paul 
Gibson, Missouri 
Figure 1: Mention-Entity Graph Example
of the sum of the corresponding squared shortest-
path distances. We then restrict the input graph to
the entity nodes that are closest to the mentions. An
experimentally determined good choice for the size
of this set is five times the number of the mention
nodes. Then the iterative greedy method is run on
this smaller subgraph. Algorithm 1 summarizes this
procedure, where an entity is taboo if it is the
last candidate for a mention it is connected to.
Algorithm 1: Graph Disambiguation Algorithm
Input: weighted graph of mentions and entities
Output: result graph with one edge per mention
begin
pre?processing phase;
foreach entity do
calculate distance to all mentions;
keep the closest (5? mentions count)
entities, drop the others;
main loop;
while graph has non-taboo entity do
determine non-taboo entity node
with lowest weighted degree, remove it
and all its incident edges;
if minimum weighted degree increased
then
set solution to current graph;
post?processing phase;
process solution by local search or full
enumeration for best configuration;
The output of the main loop would often be close
to the desired result, but may still have more than one
mention-entity edge for one or more mentions. At
this point, however, the subgraph is small enough to
consider an exhaustive enumeration and assessment
of all possible solutions. This is one of the options
that we have implemented as post-processing step.
Alternatively, we can perform a faster local-search
algorithm. Candidate entities are randomly selected
with probabilities proportional to their weighted de-
grees. This step is repeated for a prespecified number
of iterations, and the best configuration with the high-
est total edge-weight is used as final solution.
5.3 Robustness Tests
The graph algorithm generally performs well. How-
ever, it may be misled in specific situations, namely,
if the input text is very short, or if it is thematically
heterogeneous. To overcome these problems, we in-
troduce two robustness tests for individual mentions
and, depending on the tests? outcomes, use only a
subset of our framework?s features and techniques.
Prior test: Our first test ensures that the popularity
prior does not unduly dominate the outcome if the
true entities are dominated by false alternatives. We
check, for each mention, whether the popularity prior
for the most likely candidate entity is above some
threshold ?, e. g. above 90% probability. If this is not
the case, then the prior is completely disregarded for
computing the mention-entity edge weights. Other-
wise, the prior is combined with the context-based
similarity computation to determine edge weights.
788
We never rely solely on the prior.
Coherence test: As a test for whether the coher-
ence part of our framework makes sense or not,
we compare the popularity prior and the similarity-
only measure, on a per-mention basis. For each
mention, we compute the L1 distance between the
popularity-based vector of candidate probabilities
and the similarity-only-based vector of candidate
probabilities:
?
i=1..k
|prior(m, ei)? simscore(m, ei)|
This difference is always between 0 and 2. If it ex-
ceeds a specified threshold ? (e.g., 1), the disagree-
ment between popularity and similarity-only indi-
cates that there is a situation that coherence may be
able to fix. If, on the other hand, there is hardly any
disagreement, using coherence as an additional as-
pect would be risky for thematically heterogeneous
texts and should better be disabled. In that case, we
choose an entity for the mention at hand, using the
combination of prior and similarity. Only the win-
ning entity is included in the mention-entity graph, all
other candidates are omitted for the graph algorithm.
The robustness tests and the resulting adaptation of
our method are fully automated.
6 Experiments
6.1 Setup
System: All described methods are implemented in
a prototype system called AIDA (Accurate Online
Disambiguation of Named Entities). We use the Stan-
ford NER tagger (Finkel05) to identify mentions in
input texts, the YAGO2 knowledge base (Hoffart11)
as a repository of entities, and the English Wikipe-
dia edition (as of 2010-08-17) as a source of mining
keyphrases and various forms of weights. The graph
algorithm makes use of Webgraph (Boldi04).
Datasets: There is no established benchmark for
NED. The best prior work (Kulkarni09)) compiled
its own hand-annotated dataset, sampled from online
news. Unfortunately, this data set is fairly small (102
short news articles, about 3,500 proper noun men-
tions). Moreover, its entity annotations refer to an old
version of Wikipedia. To avoid unfair comparisons,
we created our own dataset based on CoNLL 2003
articles 1,393
mentions (total) 34,956
mentions with no entity 7,136
words per article (avg.) 216
mentions per article (avg.) 25
distinct mentions per article (avg.) 17
mentions with candidate in KB (avg.) 21
entities per mention (avg) 73
initial annotator disagreement (%) 21.1
Table 1: CoNLL Dataset Properties
data, extensively used in prior work on NER tagging
(Sang03).
This consists of proper noun annotations for 1393
Reuters newswire articles. We hand-annotated all
these proper nouns with corresponding entities in
YAGO2. Each mention was disambiguated by two
students and resolved by us in case of conflict. This
data set is referred to as CoNLL in the following
and fully available at http://www.mpi-inf.mpg.
de/yago-naga/aida/. Table 1 summarizes prop-
erties of the dataset.
Methods under comparison: Our framework in-
cludes many variants of prior methods from the lit-
erature. We report experimental results for some of
them. AIDA?s parameters were tuned by line-search
on 216 withheld development documents. We found
the following to work best:
? threshold for prior test: ? = 0.9
? weights for popularity, similarity, coherence:
? = 0.43, ? = 0.47, ? = 0.10
? initial number of entites in graph: 5 ? #mentions
? threshold for coherence test: ? = 0.9
We checked the sensitivity of the hyper-parameter
settings and found the influence of variations to be
small, e. g. when varying ? within the range [0.5,1.3],
the changes in precision@1.0 are within 1%.
The baseline for our experiments is the collective-
inference method of (Kulkarni09), which outper-
forms simpler methods (such as (Milne08)). We
refer to this method as Kul CI. Since program code
for this method is not available, we re-implemented
it using the LP solver CPLEX for the optimization
problem with subsequent rounding, as described in
(Kulkarni09). In addition, we compare against (our
re-implementation of) the method of (Cucerzan07),
789
Our Methods Competitors
sim-k prior
sim-k
prior
sim-s
sim-k
sim-s
r-prior
sim-k
r-prior
sim-k
coh
r-prior
sim-k
r-coh
prior Cuc Kul s Kul sp Kul CI
Macro P@1.0 76.53 75.75 71.43 76.40 80.71 80.73 81.91 71.24 43.74 58.06 76.74 76.74
Micro P@1.0 76.09 70.72 66.09 76.13 79.57 81.77 81.82 65.84 51.03 63.42 72.31 72.87
MAP 66.98 83.99 85.97 67.00 85.91 89.05 87.31 86.63 40.06 63.90 86.50 85.44
Table 2: Experimental results on CoNLL (all values in %)
referred to as Cuc. For all methods, weights for
combining components were obtained by training
a SVM classifier on 946 withheld CoNLL training
documents.
Performance measures: The key measures in our
evaluation are precision and recall. We consider
the precision-recall curve, as there is an inherent
trade-off between the two measures. Precision is the
fraction of mention-entity assignments that match
the ground-truth assignment. Recall is the fraction
of the ground-truth assignments that our method(s)
could compute. Both measures can aggregate over of
all mentions (across all texts) or over all input texts
(each with several mentions). The former is called
micro-averaging, the latter macro-averaging.
As we use a knowledge base with millions of enti-
ties, we decided to neglect the situation that a mention
may refer to an unknown entity not registered in the
knowledge base. We consider only mention-entity
pairs where the ground-truth gives a known entity,
and thus ignore roughly 20% of the mentions without
known entity in the ground-truth. This simplifies the
calculation of aggregated precision-recall measures
like (interpolated) MAP (mean average precision):
MAP = 1m
?
i=1..m
precision@ im
where precision@ im is the precision at a specificrecall level. This measure is equivalent to the area
under the precision-recall curve.
For constructing the precision-recall curve, we sort
the mention-entity pairs in descending order of con-
fidence, so that x% recall refers to the x% with the
highest confidence. We use each method?s mention-
entity similarity for the confidence values.
6.2 Results
The results of AIDA vs. the collective-inference
method of (Kulkarni09) and the entity disambigua-
tion method of (Cucerzan07) on 229 test documents
are shown in Table 21. The table includes variants
of our framework, with different choices for the sim-
ilarity and coherence computations. The shorthand
notation for the combinations in the table is as fol-
lows: prior: popularity prior; r-prior: popularity
prior with robustness test; sim-k: keyphrase based
similarity measure; sim-s: syntax-based similarity;
coh: graph coherence; r-coh: graph coherence with
robustness test.
The shorthand names for competitors are: Cuc:
(Cucerzan07) similarity measure; Kul s: (Kulka-
rni09) similarity measure only; Kul sp: Kul s com-
bined with plus popularity prior; Kul CI: Kul sp com-
bined with coherence. All coherence methods use
the Milne-Witten inlink overlap measure mw coh.
The most important measure is macro/micro preci-
son@1.0, which corresponds to the overall correct-
ness of the methods for all mentions that are assigned
to an entity in the ground-truth data. Our sim-k pre-
cision is already very good. Combining it with the
syntax-based similarity improves micro-averaged pre-
cision@1.0, but the macro-averaged results are a bit
worse. Thus, the more advanced configurations of
AIDA did not use syntax-based similarity. Uncondi-
tionally combining prior and sim-k degrades the qual-
ity, but including the prior robustness test (r-prior
sim-k) improves the results significantly. The preci-
sion for our best method, the prior- and coherence-
tested Keyphrase-based mention-entity similarity (r-
prior sim-k r-coh), significantly outperforms all com-
petitors (with a p-value of a paired t-test< 0.01). Our
macro-averaged precision@1.0 is 81.91%, whereas
Kul CI only achieves 76.74%. Even r-prior sim-
k, without any coherence, significantly outperforms
12 of the 231documents in the original test set could not be
processed by Kul CI due to memory limitations. All results are
given for the subset, for the sake of comparability. Results for
the complete set are available on our website.
790
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.7
0.8
0.9
r-prior sim-k r-coh
r-prior sim-k
Kul CI Kul sp
prior
recall
pre
cis
ion
Figure 2: Experimental results on CoNLL: precision-recall curves
Kul CI (with coherence) with a p-value of < 0.01.
In micro-average precision@1.0, the differences are
even higher, showing that we perform better through-
out all documents.
The macro-averaged precision-recall curves in Fig-
ure 2 show that the best AIDA method performs
particularly well in the tail of high recall values. The
MAP underlines the robustness of our best methods.
The high MAP for the prior method is because
we rank by mention-entity edge weight; for prior
this is simply the prior probability. As the prior is
most probably correct for mentions with a very high
prior for their most popular entity (by definition), the
initial ranking of the prior is very good, but drops
more sharply. We believe that the main difficulty in
named entity disambiguation lies exactly in the ?long
tail? of not-so-prominent entities.
We also tried the (Milne08) web service on a sub-
set of our test collection, but this was obviously
geared for Wikipedia linkage and performed poorly.
6.3 Discussion
Our keyphrase-based similarity measure performs
better than the Kul s measure, which is a combina-
tion of 4 different entity contexts (abstract tokens,
full text tokens, inlink anchor tokens, inlink anchor
tokens + surrounding tokens), 3 similarity measures
(Jaccard, dot product, and tf.idf cosine similarity),
and the popularity prior. Adding the prior to our
similarity measure by linear combination degrades
the performance. We found that our measure already
captures a notion of popularity because popular enti-
ties have more keyphrases and can thus accumulate
a higher total score. The popularity should only be
used when one entitiy has a very high probability, and
introducing the robustness test for the prior achieved
this, improving on both our similarity and Kul sp.
Unconditionally adding the notion of coherence
among entities improves the micro-average precision,
but not the macro-average. Investigating potential
problems, we found that the coherence can be led
astray when parts of the document form a coherent
cluster of entities, and other entities are then forced
to be coherent to this cluster. To overcome this is-
sue, we introduced the coherence robustness test,
and the results with r-coh show that it makes sense
to fix an entity for a mention when the prior and
similarity are in reasonable agreement. Adding this
coherence test leads to a signigicant (p-value < 0.05)
improvement over the non-coherence based measures
in both micro- and macro-average precision. Our ex-
periments showed that when adding this coherence
test, around 23 of the mentions are solved using localsimilarity only and are assigned an entity before run-
ning the graph algorithm. In summary, we observed
that the AIDA configuration with r-prior, keyphrase-
based sim-k, and r-coh significantly outperformed all
competitors.
7 Conclusions and Future Work
The AIDA system provides an integrated NED
method using popularity, similarity, and graph-based
coherence, and includes robustness tests for self-
adaptive behavior. AIDA performed significantly bet-
ter than state-of-the-art baselines. The system is fully
implemented and accessible online (http://www.
mpi-inf.mpg.de/yago-naga/aida/). Our fu-
ture work will consider additional semantic proper-
ties between entities (types, memberOf/partOf, etc.)
for further enhancing the coherence algorithm.
Acknowledgements
This work has been partially supported by the German Sci-
ence Foundation (DFG) through the Cluster of Excellence
on ?Multimodal Computing and Interaction? and the Eu-
ropean Union through the 7th Framework IST Integrated
Project ?LivingKnowledge? (no. 231126). We also thank
Mauro Sozio for the discussion on the graph algorithm.
791
References
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, Zachary G. Ives: DB-
pedia: A Nucleus for a Web of Open Data. ISWC 2007
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, Oren Etzioni: Open Information
Extraction from the Web. IJCAI 2007
Paolo Boldi and Sebastiano Vigna. The WebGraph frame-
work I: Compression techniques. WWW 2004, soft-
ware at http://webgraph.dsi.unimi.it/
Razvan C. Bunescu, Marius Pasca: Using Encyclopedic
Knowledge for Named entity Disambiguation. EACL
2006
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., Tom M. Mitchell.
Toward an Architecture for Never-Ending Language
Learning. AAAI 2010
Silviu Cucerzan: Large-Scale Named Entity Disambigua-
tion Based on Wikipedia Data. EMNLP-CoNLL 2007
AnHai Doan, Luis Gravano, Raghu Ramakrishnan, Shiv-
akumar Vaithyanathan. (Eds.). Special issue on infor-
mation extraction. SIGMOD Record, 37(4), 2008.
Jenny Rose Finkel, Trond Grenager, Christopher Man-
ning: Incorporating Non-local Information into Infor-
mation Extraction Systems by Gibbs Sampling. ACL
2005, software at http://nlp.stanford.edu/
software/CRF-NER.shtml
Xianpei Han, Jun Zhao: Named entity disambiguation
by leveraging wikipedia semantic knowledge. CIKM
2009.
Johannes Hoffart, Fabian Suchanek, Klaus Berberich, Ed-
win Lewis-Kelham, Gerard de Melo, Gerhard Weikum:
YAGO2: Exploring and Querying World Knowledge in
Time, Space, Context, and Many Languages. Demo Pa-
per, WWW 2011, data at http://www.mpi-inf.
mpg.de/yago-naga/yago/
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
Soumen Chakrabarti: Collective annotation of Wikipe-
dia entities in web text. KDD 2009
James Mayfield et al: Corss-Document Coreference Res-
olution: A Key Technology for Learning by Reading.
AAAI Spring Symposium on Learning by Reading and
Learning to Read, 2009.
Diane McCarthy. Word Sense Disambiguation: An
Overview. Language and Linguistics Compass 3(2):
537-558, Wiley, 2009
Rada Mihalcea, Andras Csomai: Wikify!: Linking Docu-
ments to Encyclopedic Knowledge. CIKM 2007
David N. Milne, Ian H. Witten: Learning to Link with
Wikipedia. CIKM 2008
Ndapandula Nakashole, Martin Theobald, Gerhard
Weikum: Scalable Knowledge Harvesting with High
Precision and High Recall. WSDM 2011
Roberto Navigli: Word sense disambiguation: A survey.
ACM Comput. Surv., 41(2), 2009
Hien T. Nguyen, Tru H. Cao: Named Entity Disambigua-
tion on an Ontology Enriched by Wikipedia. RIVF
2008
Erik F. Tjong Kim Sang, Fien De Meulder: Introduction to
the CoNLL-2003 Shared Task: Language-Independent
Named Entity Recognition. CoNLL 2003
Mauro Sozio, Aristides Gionis: The Community-search
Problem and How to Plan a Successful Cocktail Party.
KDD 2010
Fabian M. Suchanek, Gjergji Kasneci, Gerhard Weikum:
YAGO: a Core of Semantic Knowledge. WWW 2007
Fabian Suchanek, Mauro Sozio, Gerhard Weikum: SOFIE:
a Self-Organizing Framework for Information Extrac-
tion. WWW 2009
Bilyana Taneva, Mouna Kacimi, and Gerhard Weikum:
Finding Images of Rare and Ambiguous Entities. Tech-
nical Report MPI-I-2011-5-002, Max Planck Institute
for Informatics, 2011.
Stefan Thater, Hagen Fu?rstenau, Manfred Pinkal. Contex-
tualizing Semantic Representations using Syntactically
Enriched Vector Models. ACL 2010
Michael L. Wick, Aron Culotta, Khashayar Rohani-
manesh, Andrew McCallum: An Entity Based Model
for Coreference Resolution. SDM 2009: 365-376
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, Ji-Rong
Wen: StatSnowball: a Statistical Approach to Extract-
ing Entity Relationships. WWW 2009
792
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540?549,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
What Substitutes Tell Us ?
Analysis of an ?All-Words? Lexical Substitution Corpus
Gerhard Kremer
Institute for Computational Linguistics
University of Heidelberg, Germany
kremer@cl.uni-heidelberg.de
Katrin Erk
Dept. of Linguistics
University of Texas, Austin, U.S.A.
katrin.erk@utexas.edu
Sebastian Pad?
Institute for Natural Language Processing
University of Stuttgart, Germany
pado@ims.uni-stuttgart.de
Stefan Thater
Dept. of Computational Linguistics
Saarland University, Saarbr?cken, Germany
stth@coli.uni-sb.de
Abstract
We present the first large-scale English ?all-
words lexical substitution? corpus. The
size of the corpus provides a rich resource
for investigations into word meaning. We
investigate the nature of lexical substitute
sets, comparing them to WordNet synsets.
We find them to be consistent with, but
more fine-grained than, synsets. We also
identify significant differences to results
for paraphrase ranking in context reported
for the SEMEVAL lexical substitution data.
This highlights the influence of corpus con-
struction approaches on evaluation results.
1 Introduction
Many, if not most, words have multiple meanings;
for example, the word ?bank? has a financial and
a geographical sense. One common approach to
deal with this lexical ambiguity is supervised word
sense disambiguation, or WSD (McCarthy, 2008;
Navigli, 2009), which frames the task as a lemma-
level classification problem, to be solved by train-
ing classifiers on samples of lemma instances that
are labelled with their correct senses.
This approach has its problems, however. First,
it assumes a complete and consistent set of labels.
WordNet, used in the majority of studies, does
cover several 10,000 lemmas, but has been criti-
cised for both its coverage and granularity. Second,
WSD requires annotation for each sense and lemma,
leading to an ?annotation bottleneck?. A number
of technical solutions have been suggested regard-
ing the second problem (Ando and Zhang, 2005;
Navigli and Ponzetto, 2012), but not for the first.
In 2009, McCarthy and Navigli address both
problems by proposing a fundamentally different
approach, called Lexical Substitution (McCarthy
and Navigli, 2009) which avoids capturing a word?s
meaning by a single label. Instead, annotators are
asked to list, for each instance of a word, one or
more alternative words or phrases to be substituted
for the target in this particular context. This setup
provides a number of benefits over WSD. It al-
lows characterising word meaning without using
an ontology and can be obtained easily from native
speakers through crowdsourcing. Work on mod-
elling Lexical Substitution data has also assumed a
different focus from WSD. It tends to see the predic-
tion of substitutes along the lines of compositional
lexical semantics, concentrating on explaining how
word meaning is modulated in context (Mitchell
and Lapata, 2010).
There are, however, important shortcomings of
the work in the Lexical Substitution paradigm. All
existing datasets (McCarthy and Navigli, 2009;
Sinha and Mihalcea, 2014; Biemann, 2013; Mc-
Carthy et al., 2013) are either comparatively small,
are ?lexical sample? datasets, or both. ?Lexical
sample? datasets consist of sample sentences for
each target word drawn from large corpora, with
just one target word substituted in each sentence. In
WSD, ?lexical sample? datasets contrast with ?all-
words? annotation, in which all content words in a
text are annotated for sense (Palmer et al., 2001).
540
In this paper, we present the first large ?all-
words? Lexical Substitution dataset for English. It
provides substitutions for more than 30,000 words
of running text from two domains of MASC (Ide et
al., 2008; Ide et al., 2010), a subset of the Ameri-
can National Corpus (http://www.anc.org)
that is freely available and has (partial) manual
annotation. The main advantage of the all-words
setting is that it provides a realistic frequency distri-
bution of target words and their senses. We use this
to empirically investigate (a) the nature of lexical
substitution and (b) the nature of the corpus, seen
through the lens of word meaning in context.
2 Related Work
2.1 Lexical Substitution: Data
The original ?English Lexical Substitution? dataset
(McCarthy and Navigli, 2009) comprises 200 target
content words (balanced numbers of nouns, verbs,
adjectives and adverbs). Targets were explicitly se-
lected to exhibit interesting ambiguities. For each
target, 10 sentences were chosen (mostly at ran-
dom, but in part by hand) from the English Internet
Corpus (Sharoff, 2006) and presented to 5 anno-
tators to collect substitutes. Its total size is 2,000
target instances. Sinha and Mihalcea (2014) pro-
duced a small pilot dataset (500 target instances) for
all-words substitution, asking three annotators to
substitute all content words in presented sentences.
Biemann (2013) first investigated the use of
crowdsourcing, developing a three-task bootstrap-
ping design to control for noise. His study covers
over 50,000 instances, but these correspond only to
397 targets, all of which are high-frequency nouns.
Biemann clusters the resulting substitutes into word
senses. McCarthy et al. (2013) applied lexical sub-
stitution in a cross-lingual setting, annotating 130
of the original McCarthy and Navigli targets with
Spanish substitutions (i. e., translations).
2.2 Lexical Substitution: Models
The LexSub task at SEMEVAL 2007 (McCarthy
and Navigli, 2009) required systems to both de-
termine substitution candidates and choose con-
textual substitutions in each case. Erk and Pad?
(2008) treated the gold substitution candidates as
given and focused on the context-specific ranking
of those candidates. In this form, the task has been
addressed through three types of (mostly unsuper-
vised) approaches. The first group computes a sin-
gle type representation and modifies it according
to sentence context (Erk and Pad?, 2008; Thater et
al., 2010; Thater et al., 2011; Van de Cruys et al.,
2011). The second group of approaches clusters
instance representations (Reisinger and Mooney,
2010; Dinu and Lapata, 2010; Erk and Pad?, 2010;
O?S?aghdha and Korhonen, 2011). The third op-
tion is to use a language model (Moon and Erk,
2013). Recently, supervised models have emerged
(Biemann 2013; Szarvas et al., 2013a,b).
3 COINCO ? The MASC All-Words
Lexical Substitution Corpus
1
Compared to, e. g., WSD, there still is little gold-
annotated data for lexical substitution. With the
exception of the dataset created by Biemann (2013),
all existing lexical substitution datasets are fairly
small, covering at most several thousand instances
and few targets which are manually selected. We
aim to fill this gap, providing a dataset that mirrors
the actual corpus distribution of targets in sentence
context and is sufficiently large to enable a detailed,
lexically specific analysis of substitution patterns.
3.1 Source Corpus Choice
For annotation, we chose a subset of the ?Manually
Annotated Sub-Corpus? MASC (Ide et al., 2008;
Ide et al., 2010) which is ?equally distributed across
19 genres, with manually produced or validated
annotations for several layers of linguistic phenom-
ena?, created with the purpose of being ?free of
usage and redistribution restrictions?. We chose
this corpus because (a) our analyses can profit from
the preexisting annotations and (b) we can release
our annotations as part of MASC.
Since we could not annotate the complete MASC,
we selected (complete) text documents from two
prominent genres: news (18,942 tokens) and fiction
(16,605 tokens). These two genres are both rele-
vant for NLP and provide long, coherent documents
that are appropriate for all-words annotation. We
used the MASC part-of-speech annotation to iden-
tify all content words (verbs, nouns, adjectives, and
adverbs), which resulted in a total of over 15,000
targets for annotation. This method differs from
Navigli and McCarthy?s (2009) in two crucial re-
spects: we annotate all instances of each target, and
include all targets regardless of frequency or level
of lexical ambiguity. We believe that our corpus is
considerably more representative of running text.
1
Available as XML-formatted corpus ?Concepts in Con-
text? (COINCO) from http://goo.gl/5C0jBH. Also
scheduled for release as part of MASC.
541
3.2 Crowdsourcing
We used the Amazon Mechanical Turk (AMT) plat-
form to obtain substitutes by crowdsourcing. Inter-
annotator variability and quality issues due to non-
expert annotators are well-known difficulties (see,
e. g., Fossati et al. (2013)). Our design choices
were shaped by ?best practices in AMT?, including
Mason and Suri (2012) and Biemann (2013).
Defining HITs. An AMT task consists of Human
Intelligence Tasks (HITs), each of which is sup-
posed to represent a minimal, self-contained task.
In our case, potential HITs were annotations of
(all target words in) one sentence, or just one tar-
get word. The two main advantages of annotating
a complete sentence at a time are (a) less over-
head, because the sentence has only to be read
once; (b) higher reliability, since all words within a
sentence will be annotated by the same person.
Unfortunately, presenting individual sentences
as HITs also means that all sentences pay the same
amount irrespective of their length. Since long sen-
tences require more effort, they are likely to receive
less attention. We therefore decided to generally
present two random target words per HIT, and one
word in the case of ?leftover? singleton targets.
In the HITs, AMT workers (?turkers?) saw the
highlighted target word in context. Since one sen-
tence was often insufficient to understand the target
fully, we also showed the preceding and the follow-
ing sentence. The task description asked turkers to
provide (preferably single-word) substitutes for the
target that ?would not change the meaning?. They
were explicitly allowed to use a ?more general term?
in case a substitute was hard to find (e. g., dog for
the target dachshund, cf. basic level effects: Rosch
et al. (1976)). Turkers were encouraged to produce
as many replacements as possible (up to 5). If they
could not find a substitute, they had to check one of
the following radio buttons: ?proper name?, ?part
of a fixed expression?, ?no replacement possible?,
?other problem (with description)?.
Improving Reliability. Another major problem
is reliability. Ideally, the complete dataset should
be annotated by the same group of annotators, but
turkers tend to work only on a few HITs before
switching to other AMT jobs. Following an idea
of Biemann and Nygaard (2010), we introduced a
two-tier system of jobs aimed at boosting turker
loyalty. A tier of ?open tasks? served to identify
reliable turkers by manually checking their given
substitutes for plausibility. Such turkers were then
invited to the second, ?closed task? tier, with a
higher payment. In both tiers, bonus payments
were offered to those completing full HIT sets.
For each target, we asked 6 turkers to provide
substitutions. In total, 847 turkers participated suc-
cessfully. In the open tasks, 839 turkers submitted
12,158 HITs (an average of 14.5 HITs). In the
closed tasks, 25 turkers submitted 42,827 HITs (an
average of 1,713 HITs), indicating the substantial
success of our turker retention scheme.
Cost. In the open task, each HIT was paid for
with $ 0.03, in the closed task the wage was $ 0.05
per HIT. The bonus payment for completing a HIT
set amounted to $ 2 ($ 1) in the open (closed) tasks.
The average cost for annotations was $ 0.22 for one
target word instance and $ 0.02 for one substitute.
The total cost with fees was ~$ 3,400.
3.3 COINCO: Corpus and Paraset Statistics
We POS-tagged and lemmatised targets and substi-
tutes in sentence context with TreeTagger (Schmid,
1994). We manually lemmatised unknown words.
Our annotated dataset comprises a total of 167,336
responses by turkers for 15,629 target instances in
2,474 sentences (7,117 nouns, 4,617 verbs, 2,470
adjectives, and 1,425 adverbs). As outlined above,
targets are roughly balanced across the two gen-
res (news: 8,030 instances in 984 sentences; fic-
tion: 7,599 instances in 1,490 sentences). There are
3,874 unique target lemmas; 1,963 of these occur
more than once. On this subset, there is a mean of
6.99 instances per target lemma. To our knowledge,
our corpus is the largest lexical substitution dataset
in terms of lemma coverage.
Each target instance is associated with a paraset
(i. e., the set of substitutions or paraphrases pro-
duced for a target in its context) with an average
size of 10.71. Turkers produced an average of
1.68 substitutions per target instance.
2
Despite
our instructions to provide single-word substitutes,
11,337 substitutions contain more than one word.
3.4 Inter-Annotator Agreement
McCarthy and Navigli (2009) introduced two inter-
annotator agreement (IAA) measures for their
dataset. The first one is pairwise agreement (PA),
2
Note that a small portion of the corpus was annotated by
more than 6 annotators.
542
dataset # targets PA mode-% PA
m
MN09 1,703 27.7 73.9 50.7
SM13 550 15.5 N/A N/A
COINCO (complete) 15,400 19.3 70.9 44.7
COINCO (subset) 2,828 24.6 76.4 50.9
Table 1: Pairwise turker agreement (mode-%: per-
centage of target instances with a mode)
measuring the overlap of produced substitutions:
PA =
?
t?T
?
?s
t
,s
?
t
? ?C
t
|s
t
? s
?
t
|
|s
t
? s
?
t
|
?
1
|C
t
| ? |T |
where t is a target in our target set T , s
t
is the
paraset provided by one turker for t, and C
t
is the
set comprising all pairs of turker-specific parasets
for t. Only targets with non-empty parasets (i. e.,
not marked by turkers as a problematic target) from
at least two turkers are included. The second one
is mode agreement (PA
m
), the agreement of an-
notators? parasets with the mode (the unique most
frequent substitute) for all targets where one exists:
PA
m
=
?
t?T
m
?
s
t
?S
t
[m ? s
t
] ?
1
|s
t
| ? |T
m
|
where T
m
is the set of all targets with some mode
m and S
t
is the set of all parasets for target t. The
Iverson bracket notation [m ? s
t
] denotes 1 if
mode m is included in s
t
(otherwise 0).
Table 1 compares our dataset to the results by
McCarthy and Navigli (2009, MN09) and Sinha
and Mihalcea (2014, SM13). The scores for
our complete dataset (row 3) are lower than Mc-
Carthy and Navigli?s both for PA (?8 %) and PA
m
(?6 %), but higher than Sinha and Mihalcea?s, who
also note the apparent drop in agreement.
3
We believe that this is a result of differences in
the setup rather than an indicator of low quality:
Note that PA will tend to decrease both in the face
of more annotators and of more substitutes. Both
of these factors are present in our setup. To test this
interpretation, we extracted a subset of our data that
is comparable to McCarthy and Navigli?s regard-
ing these factors. It comprises all target instances
where (a) exactly 6 turkers gave responses (9,521
targets), and (b) every turker produced between one
and three substitutes (5,734 targets). The results for
this subset (row 4) are much more similar to those
of McCarthy and Navigli: the pairwise agreement
3
Please see McCarthy and Navigli (2009) for a possible
explanation of the generally low IAA numbers in this field.
relation all verb noun adj adv
syn 9.4 12.5 7.7 8.0 10.4
direct-hyper 6.6 9.3 7.6 N/A N/A
direct-hypo 7.5 11.6 8.0 N/A N/A
trans-hyper 3.2 2.8 4.7 N/A N/A
trans-hypo 3.0 3.7 3.8 N/A N/A
wn-other 68.9 60.7 66.5 88.5 85.4
not-in-wn 2.1 0.9 2.2 3.4 4.2
Table 2: Target?substitute relations in percentages,
overall (all) and by POS. Note: WordNet contains
no hypo-/hypernyms for adjectives and adverbs.
differs only by 3 %, and the mode agreement is
almost identical. We take these figures as indica-
tion that crowdsourcing can serve as a sufficiently
reliable way to create substitution data; note that
Sinha and Mihalcea?s annotation was carried out
?traditionally? by three annotators.
Investigating IAA numbers by target POS and by
genre, we found only small differences (? 2.6 %)
among the various subsets, and no patterns.
4 Characterising Lexical Substitutions
This section examines the collected lexical substi-
tutions, both quantitatively and qualitatively. We
explore three questions: (a) What lexical relations
hold between targets and their substitutes? (b) Do
parasets resemble word senses? (c) How similar
are the parasets that correspond to the same word
sense of a target? These questions have not been
addressed before, and we would argue that they
could not be addressed before, because previous
corpora were either too small or were sampled in a
way that was not conducive to this analysis.
We use WordNet (Fellbaum, 1998), release 3.1,
as a source for both lexical relations and word
senses. WordNet is the de facto standard in NLP
and is used for both WSD and broader investiga-
tions of word meaning (Navigli and Ponzetto, 2012;
Erk and McCarthy, 2009). Multi-word substitutes
are excluded from all analyses.
4
4.1 Relating Targets and Substitutes
We first look at the most canonical lexical relations
between a target and its substitutes. Table 2 lists the
percentage of substitutes that are synonyms (syn),
direct/transitive (direct-/trans-) hypernyms (hyper)
4
All automatic lexical substitution approaches, including
Section 5, omit multi-word expressions. Also, they can be
expected to have WordNet coverage and normalisation issues,
which would constitute a source of noise for this analysis.
543
sentence substitutes
Now, how can I help the elegantly mannered friend of
my Nepthys and his surprising young charge ?
dependent, person, task, lass, prot?g?, effort, companion
The distinctive whuffle of pleasure rippled through the
betas on the bridge, and Rakal let loose a small growl,
as if to caution his charges against false hope.
dependent, command, accusation, private, companion, follower,
subordinate, prisoner, teammate, ward, junior, underling, enemy,
group, crew, squad, troop, team, kid
Table 3: Context effects below the sense level: target noun ?charge? (wn-other shown in italics)
and hyponyms (hypo) of the target. If a substitute
had multiple relations to the target, the shortest path
from any of its senses to any sense of the target
was chosen. The table also lists the percentage of
substitutes that are elsewhere in WordNet but not
related to the target (wn-other) and substitutes that
are not covered by WordNet (not-in-wn).
We make three main observations. First, Word-
Net shows very high coverage throughout ? there
are very few not-in-wn substitutes. Second, the per-
centages of synonyms, hypernyms and hyponyms
are relatively similar (even though the annotation
guidelines encouraged the annotation of hyponyms
over hypernyms), but relatively small. Finally, and
most surprisingly, the vast majority of substitutes
across all parts of speech are wn-other.
A full analysis of wn-other is beyond the cur-
rent paper. But a manual analysis of wn-other
substitutes for 10 lemmas
5
showed that most of
them were context-specific substitutes that can dif-
fer even when the sense of the target is the same.
This is illustrated in Table 3, which features two
occurrences of the noun ?charge? in the sense of
?person committed to your care?. But because of
the sentence context, the first occurrence got sub-
stitutes like ?prot?g??, while the second one was
paraphrased by words like ?underling?. We also
see evidence of annotator error (e. g., ?command?
and ?accusation? in the second sentence).
6
Dis-
counting such instances still leaves a prominent
role for correct wn-other cases.
But are these indeed contextual modulation ef-
fects below the sense level, or are parasets funda-
mentally different from word senses? We perform
two quantitative analyses to explore this question.
4.2 Comparing Parasets to Synsets
To what extent do parasets follow the boundaries
of WordNet senses? To address this question, we
5
We used the nouns business, charge, place, way and the
verbs call, feel, keep, leave, show, stand.
6
A manual analysis of the same 10 lemmas showed only
38 out of 1,398 (0.027) of the substitutes to be erroneous.
paraset?sense mapping class verb noun adj adv
mappable 90.3 73.5 33.0 49.6
uniquely mappable 63.1 57.5 24.3 41.3
Table 4: Ratios of (uniquely) mappable parasets
establish a mapping between parasets and synsets.
Since gold standard word senses in MASC are lim-
ited to high-frequency lemmas and cover only a
small part of our data, we create a heuristic map-
ping that assigns each paraset to that synset of its
target with which it has the largest intersection. We
use extended WordNet synsets that include direct
hypo- and hypernyms to achieve better matches
with parasets. We call a paraset uniquely mappable
if it has a unique best WordNet match, and map-
pable if one or more best matches exist. Table 4
shows that most parasets are mappable for nouns
and verbs, but not for adjectives or adverbs.
We now focus on mappable parasets for nouns
and verbs. To ensure that this does not lead to a
confounding bias, we performed a small manual
study on the 10 noun and verb targets mentioned
above (247 parasets). We found 25 non-mappable
parasets, which were due to several roughly equally
important reasons: gaps in WordNet, multi-word
expressions, metaphor, problems of sense granular-
ity, and annotator error. We also found 66 parasets
with multiple best matches. The two dominant
sources were target occurrences that evoked more
than one sense and WordNet synset pairs with very
close meanings. We conclude that excluding non-
mappable parasets does not invalidate our analysis.
To test whether parasets tend to map to a single
synset, we use a cluster purity test that compares
a set of clusters C to a set of gold standard classes
C
?
. Purity measures the accuracy of each cluster
with respect to its best matching gold class:
purity(C,C
?
) =
1
N
K
?
k=1
max
k
?
|C
k
? C
?
k
?
|
where N is the total number of data points, K is the
544
measure verbs nouns
cluster purity (%) 75.1 81.2
common core size within sense 1.84 2.21
common core size across senses 0.39 0.41
paraset size 6.89 6.29
Table 5: Comparing uniquely mappable parasets to
senses: overlap with best WordNet match as cluster
purity (top), and intersection size of parasets with
and without the same WordNet match (bottom)
number of clusters, and C
?
k
?
is the gold class that
has the largest overlap with cluster C
k
. In our case,
C is the set of mappable parasets
7
, C
?
the set of
extended WordNet synsets, and we only consider
substitutes that occur in one of the target?s extended
synsets (these are the data points). This makes the
current analysis complementary to the relational
analysis in Table 2.
8
The result, listed in the first row of Table 5,
shows that parasets for both verbs and nouns have
a high purity, that is, substitutes tend to focus on a
single sense. This can be interpreted as saying that
annotators tend to agree on the general sense of a
target. Roughly 20?25 % of substitutes, however,
tend to stem from a synset of the target that is not
the best WordNet match. This result comes with
the caveat that it only applies to substitutes that
are synonyms or direct hypo- and hypernyms of
the target. So in the next section, we perform an
analysis that also includes wn-other substitutes.
4.3 Similarity Between Same-Sense Parasets
We now use the WordNet mappings from the pre-
vious section to ask how (dis-)similar parasets are
that represent the same word sense. We also try to
identify the major sources for dissimilarity.
We quantify paraset similarity as the common
core, that is, the intersection of all parasets for
the same target that map onto the same extended
WordNet synset. Surprisingly, the common core
is mostly non-empty (in 85.6 % of all cases), and
contains on average around two elements, as the
second row in Table 5 shows. For this analysis, we
only use uniquely mappable parasets. In relation
to the average paraset size (see row 4), this means
that one quarter to one third of the substitutes are
7
For non-uniquely mappable parasets, the purity is the
same for all best-matching synsets.
8
Including wn-other substitutes would obscure whether
low purity means substitutes from a mixture of senses (which
we are currently interested in) or simply a large number of
wn-other substitutes (which we have explored above).
set elements
synset \ core feel, perceive, comprehend
synset ? core sense
core \ synset notice
non-core substitutes detect, recall, perceive, experi-
ence, note, realize, discern
Table 6: Target feel.v.03: synset and common core
shared among all instances of the same target?sense
combination. In contrast, the common core for
all parasets of targets that map onto two or more
synsets contains only around 0.4 substitutes (see
row 3) ? that is, it is empty more often than not.
At the same time, if about one quarter to one
third of the substitutes are shared, this means that
there are more non-shared than shared substitutes
even for same-sense parasets. Some of these cases
result from small samples: Even 6 annotators can-
not always exhaust all possible substitutes. For
example, the phrase ?I?m starting to see more busi-
ness transactions? occurs twice in the corpus. The
two parasets for ?business? share the same best
WordNet sense match, but they have only 3 shared
and 7 non-shared substitutes. This is even though
the substitutes are all valid and apply to both in-
stances. Other cases are instances of the context
sensitivity of the Lexical Substitution task as dis-
cussed above. Table 6 illustrates on an example
how the common core of a target sense relates to
the corresponding synset; note the many context-
specific substitutes outside the common core.
5 Ranking Paraphrases
While there are several studies on modelling lexi-
cal substitutes, almost all reported results use Mc-
Carthy and Navigli?s SEMEVAL 2007 dataset. We
now compare the results of three recent computa-
tional models on COINCO (our work) and on the
SEMEVAL 2007 dataset to highlight similarities
and differences between the two datasets.
Models. We consider the paraphrase ranking
models of Erk and Pad? (2008, EP08), Thater et
al. (2010, TFP10) and Thater et al. (2011, TFP11).
These models have been analysed by Dinu et al.
(2012) as instances of the same general framework
and have been shown to deliver state-of-the-art per-
formance on the SEMEVAL 2007 dataset, with best
results for Thater et al. (2011).
The three models share the idea to represent the
meaning of a target word in a specific context by
545
corpus syntactically structured syntactically filtered bag of words random
TFP11 TFP10 EP08 TFP11/EP08 TFP10 TFP11/EP08 TFP10
COINCO
context 47.8 46.0 47.4 47.4 41.9 46.2 40.8
33.0
baseline 46.2 44.6 46.2 45.8 38.8 44.7 37.5
SEMEVAL 2007
context 52.5 48.6 49.4 50.1 44.7 48.0 42.6
30.0
baseline 43.7 42.7 43.7 44.4 38.0 42.7 35.8
COINCO Subset
context 40.3 37.7 39.0 39.2 34.1 37.7 32.5
23.7
baseline 36.7 35.7 36.7 36.4 30.6 35.4 28.0
Table 7: Corpus comparison in terms of paraphrase ranking quality (GAP percentage). SEMEVAL results
from Thater et al. (2011). ?Context?: full models, ?baseline?: uncontextualised target-substitute similarity.
modifying the target?s basic meaning vector with
information from the vectors of the words in the
target?s direct syntactic context. For instance, the
vector of ?coach? in the phrase ?the coach derailed?
is obtained by modifying the basic vector represen-
tation of ?coach? through the vector of ?derail?, so
that the resulting contextualised vector reflects the
train car sense of ?coach?.
We replicate the setup of Thater et al. (2011)
to make our numbers directly comparable. We
consider three versions of each model: (a) syntacti-
cally structured models use vectors which record
co-occurrences based on dependency triples, ex-
plicitly recording syntactic role information within
the vectors; (b) syntactically filtered models also
use dependency-based co-occurrence information,
but the syntactic role is not explicitly represented in
the vector representations; (c) bag-of-words mod-
els use a window of ? 5 words. All co-occur-
rence counts are extracted from the English Giga-
word corpus (http://catalog.ldc.upenn.
edu/LDC2003T05), analysed with Stanford de-
pendencies (de Marneffe et al., 2006).
We apply the models to our dataset as follows:
We first collect all substitutes for all occurrences of
a target word in the corpus. The task of our models
for each target instance is then to rank the candi-
dates so that the actual substitutes are ranked higher
than the rest. We rank candidates according to the
cosine similarity between the contextualised vec-
tor of the target and the vectors of the candidates.
Like most previous approaches, we compare the
resulting ranked list with the gold standard annota-
tion (the paraset of the target instance), using gen-
eralised average precision (Kishida, 2005, GAP),
and using substitution frequency as weights. GAP
scores range between 0 and 1; a score of 1 indicates
a perfect ranking in which all correct substitutes
precede all incorrect ones, and correct high-weight
substitutes precede low-weight substitutes.
Results. The upper part of Table 7 shows results
for our COINCO corpus and the previous stan-
dard dataset, SEMEVAL 2007. ?Context? refers to
the full models, and ?baseline? to global, context-
unaware ranking based on the semantic similarity
between target and substitute. Baselines are model-
specific since they re-use the models? vector repre-
sentations. Note that EP08 and TFP11 are identical
unless syntactically structured vectors are used, and
their baselines are identical.
The behaviour of the baselines on the two cor-
pora is quite similar: random baselines have GAPs
around 0.3, and uncontextualised baselines have
GAPs between 0.35 and 0.46. The order of the
models is also highly parallel: the syntactically
structured TFP11 is the best model, followed by
its syntactically filtered version and syntactically
structured EP08. All differences between these
models are significant (p< 0.01) for both corpora,
as computed with bootstrap resampling (Efron and
Tibshirani, 1993). That is, the model ranking on
SEMEVAL is replicated on COINCO.
There are also substantial differences between
the two corpora, though. Most notably, all models
perform substantially worse on COINCO. This
is true in absolute terms (we observe a loss of 2?
5 % GAP) but even more dramatic expressed as the
gain over the uninformed baselines (almost 9 % for
TFP11 on SEMEVAL but only 1.2 % on COINCO).
All differences between COINCO and SEMEVAL
are again significant (p< 0.01).
We see three major possible reasons for these
differences: variations in (a) the annotation setup
(crowdsourcing, multiple substitutes); (b) the sense
distribution; (c) frequency and POS distributions
between the two corpora. We focus on (c) since it
can be manipulated most easily. SEMEVAL con-
tains exactly 10 instances for all targets, while CO-
INCO reflects the Zipf distribution of ?natural? cor-
pora, with many targets occurring only once. Such
546
corpora are easier to model in terms of absolute
performance, because the paraphrase lists for rare
targets contain less false positives for each instance.
For hapax legomena, the set of substitution candi-
dates is identical to the gold standard, and the only
way to receive a GAP score lower than 1 for such
targets is to rank low-weight substitutes ahead of
high-weight substitutes. Not surprisingly, the mean
GAP score of the syntactically structured TFP11
for hapax legomena is 0.863. At the same time,
such corpora make it harder for full models to out-
perform uncontextualised baselines; the best model
(TFP11) only outperforms the baseline by 1.6 %.
To neutralise this structural bias, we created
?SEMEVAL-like? subsets of COINCO (collectively
referred to as the COINCO Subset) by extracting
all COINCO targets with at least 10 instances (141
nouns, 101 verbs, 50 adjectives, 36 adverbs) and
building 5 random samples by drawing 10 instances
for each target. These samples match SEMEVAL in
the frequency distribution of its targets. To account
for the unequal distribution of POS in the samples,
we compute GAP scores for each POS separately
and calculate these GAP scores? average.
The results for the various models on the CO-
INCO Subset in the bottom part of Table 7 show
that the differences between COINCO and SE-
MEVAL are not primarily due to the differences
in target frequencies and POS distribution ? the
COINCO Subset is actually more different to SE-
MEVAL than the complete COINCO. Strikingly,
the COINCO Subset is very difficult, with a ran-
dom baseline of 24 % and model performances be-
low 37 % (baselines) and up to 40 % (full models),
which indicates that the set of substitutes in CO-
INCO is more varied than in SEMEVAL as an effect
of the annotation setup. Encouragingly, the margin
between full models and baselines is larger than on
the complete COINCO and generally amounts to
2?4 % (3.6 % for TFP11). That is, the full models
are more useful on the COINCO corpus than they
appeared at first glance; however, their effect still
remains much smaller than on SEMEVAL.
6 Conclusion
This paper describes COINCO, the first large-scale
?all-words? lexical substitution corpus for English.
It was constructed through crowdsourcing on the
basis of MASC, a corpus of American English.
The corpus has two major advantages over previ-
ous lexical substitution corpora. First, it covers con-
tiguous documents rather than selected instances.
We believe that analyses on our corpus generalise
better to the application domain of lexical substitu-
tion models, namely random unseen text. In fact,
we find substantial differences between the perfor-
mances of paraphrase ranking models for COINCO
and the original SEMEVAL 2007 LexSub dataset:
the margin of informed methods over the baselines
are much smaller, even when controlling for target
frequencies and POS distribution. We attribute this
divergence at least in part to the partially manual se-
lection strategy of SEMEVAL 2007 (cf. Section 2.1)
which favours a more uniform distribution across
senses, while our whole-document annotation faces
the ?natural? distribution skewed towards predom-
inant senses. This favours the non-contextualised
baseline models, consistent with our observations.
At the very least, our findings demonstrate the sen-
sitivity of evaluation results on corpus properties.
The second benefit of our corpus is that its size
enables more detailed analyses of lexical substi-
tution data than previously possible. We are able
to investigate the nature of the paraset, i. e., the
set of lexical substitutes given for one target in-
stance, finding that lexical substitution sets corre-
spond fairly well to WordNet sense distinctions
(parasets for the same synset show high similarity,
while those for different senses do not). In addition,
however, we observe a striking degree of context-
dependent variation below the sense level: the ma-
jority of lexical substitutions picks up fine-grained,
situation-specific meaning components that do not
qualify as sense distinctions in WordNet.
Avenues for future work include a more detailed
analysis of the substitution data to uncover genre-
and domain-specific patterns and the development
of lexical substitution models that take advantage
of the all-words substitutes for global optimisation.
Acknowledgements
We are grateful to Jan Pawellek for implementing
the AMT task, extracting MASC data, and preparing
HITs. Furthermore, we thank Georgiana Dinu for
her support with the word meaning models.
References
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multiple
tasks and unlabeled data. Journal of Machine Learn-
ing Research, 6:1817?1853.
547
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing WordNet. In Proceedings of the 5th Global
WordNet conference, Mumbai, India.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Language Resources and Evaluation, 47(1):97?122.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454, Genoa, Italy.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP, pages 1162?1172, Cambridge, MA.
Georgiana Dinu, Stefan Thater, and S?ren Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL, pages 611?615,
Montr?al, Canada.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of EMNLP, pages
440?449, Singapore.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk and Sebastian Pad?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL, pages 92?97, Uppsala, Sweden.
Christiane Fellbaum, editor. 1998. WordNet: An
electronic lexical database. MIT Press, Cambridge,
MA.
Marco Fossati, Claudio Giuliano, and Sara Tonelli.
2013. Outsourcing FrameNet to the crowd. In Pro-
ceedings of ACL, pages 742?747, Sofia, Bulgaria.
Nancy Ide, Collin F. Baker, Christiane Fellbaum,
Charles Fillmore, and Rebecca Passonneau. 2008.
MASC: The manually annotated sub-corpus of
American English. In Proceedings of LREC, pages
2455?2461, Marrakech, Morocco.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the
people. In Proceedings of ACL, pages 68?73, Upp-
sala, Sweden.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evalua-
tion indicator for information retrieval experiments.
Technical Report NII-2005-014E, Japanese National
Institute of Informatics.
Winter Mason and Siddharth Suri. 2012. Conducting
behavioral research on Amazon?s Mechanical Turk.
Behavior Research Methods, 44(1):1?23.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Diana McCarthy, Ravi Sinha, and Rada Mihalcea.
2013. The cross-lingual lexical substitution task.
Language Resources and Evaluation, 47(3):607?
638.
Diana McCarthy. 2008. Word sense disambiguation.
In Linguistics and Language Compass. Blackwell.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Taesun Moon and Katrin Erk. 2013. An inference-
based model of word meaning in context as a para-
phrase distribution. ACM Transactions on Intelli-
gent Systems and Technology, 4(3).
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:1?69.
Diarmuid O?S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of EMNLP, pages 1047?1057,
Edinburgh, UK.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. English
tasks: All-words and verb lexical sample. In Pro-
ceedings of the SENSEVAL-2 workshop, pages 21?
24, Toulouse, France.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceeding of NAACL, pages 109?117, Los
Angeles, CA.
Eleanor Rosch, Carolyn B. Mervis, Wayne D. Gray,
David M. Johnson, and Penny Boyes-Braem. 1976.
Basic objects in natural categories. Cognitive Psy-
chology, 8(3):382?439.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
NEMLAP, pages 44?49, Manchester, UK.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal
of Corpus Linguistics, 11(4):435?462.
Ravi Sinha and Rada Mihalcea. 2014. Explorations
in lexical sample and all-words lexical substitution.
Natural Language Engineering, 20(1):99?129.
548
Gy?rgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013a. Supervised all-words lexical substitution
using delexicalized features. In Proceedings of
NAACL-HLT, pages 1131?1141, Atlanta, GA.
Gy?rgy Szarvas, R?bert Busa-Fekete, and Eyke H?ller-
meier. 2013b. Learning to rank lexical substitutions.
In Proceedings of EMLNP, pages 1926?1932, Seat-
tle, WA.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of IJCNLP, pages
1134?1143, Chiang Mai, Thailand.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of EMNLP, pages
1012?1022, Edinburgh, Scotland.
549
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 611?615,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A comparison of models of word meaning in context
Georgiana Dinu
Universit?t des Saarlandes
Saarbr?cken, Germany
dinu@coli.uni-saarland.de
Stefan Thater
Universit?t des Saarlandes
Saarbr?cken, Germany
stth@coli.uni-saarland.de
S?ren Laue
Friedrich-Schiller Universit?t
Jena, Germany
soeren.laue@uni-jena.de
Abstract
This paper compares a number of recently pro-
posed models for computing context sensitive
word similarity. We clarify the connections
between these models, simplify their formula-
tion and evaluate them in a unified setting. We
show that the models are essentially equivalent
if syntactic information is ignored, and that the
substantial performance differences previously
reported disappear to a large extent when these
simplified variants are evaluated under identi-
cal conditions. Furthermore, our reformulation
allows for the design of a straightforward and
fast implementation.
1 Introduction
The computation of semantic similarity scores be-
tween words is an important sub-task for a variety
of NLP applications (Turney and Pantel, 2010). One
standard approach is to exploit the so-called distribu-
tional hypothesis that similar words tend to appear
in similar contexts: Word meaning is represented by
the contexts in which a word occurs, and semantic
similarity is computed by comparing these contexts
in a high-dimensional vector space.
Such distributional models of word meaning are
attractive because they are simple, have wide cover-
age, and can be easily acquired in an unsupervised
way. Ambiguity, however, is a fundamental problem:
when encountering a word in context, we want a dis-
tributional representation which reflects its meaning
in this specific context. For instance, while buy and
acquire are similar when we consider them in iso-
lation, they do not convey the same meaning when
acquire occurs in students acquire knowledge. This
is particularly difficult for vector space models which
compute a single type vector summing up over all
occurrences of a word. This vector mixes all of a
word?s usages and makes no distinctions between
its?potentially very diverse?senses.
Several proposals have been made in the recent
literature to address this problem. Type-based meth-
ods combine the (type) vector of the target with the
vectors of the surrounding context words to obtain
a disambiguated representation. In recent work, this
has been proposed by Mitchell and Lapata (2008),
Erk and Pad? (2008) and Thater et al (2010; 2011),
which differ in the choice of input vector representa-
tion and in the combination operation they propose.
A different approach has been taken by Erk and
Pad? (2010), Reisinger and Mooney (2010) and
Reddy et al (2011), who make use of token vectors
for individual occurrences of a word, rather than us-
ing the already mixed type vectors. Generally speak-
ing, these methods ?select? a set of token vectors
of the target, which are similar to the current con-
text, and use only these to obtain a disambiguated
representation.
Yet another approach has been taken by Dinu and
Lapata (2010), ? S?aghdha and Korhonen (2011)
and Van de Cruys et al (2011), who propose to use
latent variable models. Conceptually, this comes
close to token-based models, however their approach
is more unitary as they attempt to recover a hidden
layer which best explains the observation data.
In this paper, we focus on the first group of ap-
proaches and investigate the precise differences be-
tween the three models of Erk and Pad? and Thater et
al., out of which (Thater et al, 2011) achieves state of
the art results on a standard data set. Despite the fact
that these models exploit similar intuitions, both their
formal presentations and the results obtained vary to
a great extent. The answer given in this paper is sur-
prising: the three models are essentially equivalent if
syntactic information is ignored; in a syntactic space
the three methods implement only slightly different
611
intuitions. We clarify these connections, simplify
the syntactic variants originally proposed and reduce
them to straightforward matrix operations, and evalu-
ate them in a unified experimental setting. We obtain
significantly better results than originally reported in
the literature. Our reformulation also also supports
efficient implementations for these methods.
2 Models for meaning in context
We consider the following problem: we are given
an occurrence of a target word and want to obtain a
vector that reflects its meaning in the given context.
To simplify the presentation, we restrict ourselves to
contexts consisting of a single word, and use acquire
in context knowledge as a running example.
EP08. Erk and Pad? (2008) compute a contextu-
alized vector for acquire by combining its type vec-
tor (~w) with the inverse selectional preference vector
of knowledge (c). This is simply the centroid of the
vectors of all words that take knowledge as direct
object (r):
v(w,r,c) =
(
1
n?w?
f (w?,r,c) ? ~w?
)
?~w (1)
where f (w?,r,c) denotes the co-occurrence associa-
tion between the context word c and words w? related
to c by grammatical relation r in a training corpus;
n is the number of words w? and ? denotes a vector
composition operation. In this paper, we take? to be
point-wise multiplication, which is reported to work
best in many studies in the literature.
TFP10. Thater et al (2010) also compute contex-
tualized vectors by combing the vectors of the target
word and of its context. In contrast to EP08, however,
they use second order vectors as basic representation
for the target word.
~w = ?
r,r?,w??
(
?
w?
f (w,r,w?) ? f (w?,r?,w??)
)
~er,r?,w?? (2)
That is, the vector for a target word w has components
for all combinations of two grammatical roles r,r? and
a context word w?; the inner sum gives the value for
each component.
The contextualized vector for acquire is obtained
through pointwise multiplication with the (1st-order)
vector for knowledge (~c), which has to be ?lifted? first
to make the two vectors comparable:
v(w,r,c) = ~w?Lr(~c) (3)
~c = ?r?,w? f (c,r
?,w?)~e(r?,w?) is a first order vector
for the context word; the ?lifting map" Lr(~c) maps
this vector to ?r?,w? f (c,r
?,w?)~e(r,r?,w?) to make it com-
patible with ~w.
TFP11. Thater et al (2011) take a slightly different
perspective on contextualization. Instead of comb-
ing vector representations for the target word and its
context directly, they propose to re-weight the vector
components of the target word, based on distribu-
tional similarity with the context word:
v(w,r,c) = ?
r?,w?
?(r,c,r?,w?) ? f (w,r?,w?) ?~e(r?,w?) (4)
where ?(r,c,r?,w?) is simply cos(~c,~w?) if r and r?
denote the same grammatical function, else 0.
3 Comparison
The models presented above have a number of things
in common: they all use syntactic information and
?second order? vectors to represent word meaning in
context. Yet, their formal presentations differ substan-
tially. We now show that the models are essentially
equivalent if we ignore syntax: they component-wise
multiply the second order vector of one word (target
or context) with the first order vector of the other
word. Specifically, we obtain the following deriva-
tions, where W = {w1, ...,wn} denotes the vocabu-
lary, and V the symmetric n?n input matrix, where
Vi j = f (wi,w j) gives the co-occurrence association
between words wi and w j:
vEP08(w,c) =
1
n?w?
(
f (w?,c) ? ~w?
)
?~w
=
1
n?w?
(
f (w?,c) ? ? f (w?,w1), . . .?
)
?~w
=
1
n
??
w?
f (w?,c) ? f (w?,w1), . . .??~w
=
1
n
?<~c, ~w1>,. . . ,<~c, ~wn>??~w
=
1
n
~c V ?~w
612
vTFP10(w,c) = ?
w???W
(
?
w??W
f (w,w?) ? f (w?,w??)
)
~ew???~c
= ? ?
w??W
f (w,w?) f (w?,w1), . . .??~c
= ?<~w, ~w1>,...,<~w, ~wn>??~c
= ~w V ?~c
vTFP11(w,c) = ?
w??W
?(c,w?) ? f (w,w?) ?~ew?
= ??(w1,c) ? f (w,w1), . . .?
= ??(w1,c), . . .??~w (*)
= ?<~w1,~c>,. . . ,<~wn,~c>??~w
=~c V ?~w
where <~v,~w> denotes scalar product. In step (*), we
assume that ?(w,c) denotes the scalar product of ~w
and~c, instead of cosine similarity, as TFP11. This is
justified if we assume that all vectors are normalized,
in which case the two are identical.
As it can be observed the syntax-free variants of
EP08 and TFP11 are identical up to the choice in
normalization. TFP10 proposes an identical model to
that of TFP11, however with a different interpretation,
in which the roles of the context word and of the
target word are swapped.
4 Evaluation
We have just shown that EP08, TFP10 and TFP11
are essentially equivalent to each other if syntactic
information is ignored, hence it is a bit surprising that
performance results reported in the literature vary
to such a great extent. In this section we consider
syntactic variants of these methods and we show that
performance differences previously reported can only
partly be explained by the different ways syntactic
information is used: when we simplify these models
and evaluate them under identical conditions, the
differences between them disappear to a large extent.
To evaluate the three models, we reimplemented
them using matrix operations similar to the ones used
in Section 3, where we made few simplifications
to the TFP10 and EP08 models: we follow TFP11
and we use component-wise multiplication to com-
bine the target with one context word, and add the
resulting composed vectors when given more con-
text words1. Furthermore for TFP10, we change the
1Note that some of the parameters in the EP08 method (omit-
Model GAP ? Literature
EP08 46.6 + 14.4 (32.2)?
TFP10 48.3 + 3.9 (44.4)
TFP11 51.8 ?0.0
TFP10+11 52.1 N/A
Table 1: GAP scores LST data.
? The best available GAP score for this model (from Erk and
Pad? (2010)) is reported only on a subset of the data - this subset
is however judged by the authors to be ?easier? than the entire
data; all other methods are tested on the entire dataset.
treatment of syntax in the line of the much simpler
proposal of TFP11. Specifically:
v(w,r,c) = Lr?1(VV
T )w,:?Vc,: (TFP10)
v(w,r,c) = Vw,:?Lr(VV
T )c,: (TFP11)
where V is a I? J syntactic input matrix, i.e. the
columns are (word, relation) pairs. For simplification,
the columns of V are reordered such that syntactic
relations form continuous regions. Lr is a lifting map
similar to that of Equation (3) as it maps I- into J-
dimensional vectors: the resulting vector is equal to
the original one in the column region of relation r,
while everything else is 0. In the above equations we
use the standard Matlab notation, Vw,: denoting a row
vector in matrix V .
We evaluate these models on a paraphrase ranking
task, using the SemEval 2007 Lexical Substitution
Task (LST) dataset: the models are given a target
word in context plus a list of potential synonyms
(substitution candidates) ranging over all senses of
the target word. The models have to decide to what
extent each substitution candidate is a synonym of
the target in the given context. We omit the precise de-
scription of the evaluation setting here, as we follow
the methodology described in Thater et al (2011).
Results are shown in Table 1, where the first col-
umn gives the GAP (Generalized Average Precision)
score of the model and the second column gives
the difference to the result reported in the literature.
TFP10 and EP08 perform much better than the origi-
nal proposals, as we obtain very significant gains of
4 and 14 GAP points.
ted in the brief presentation in Section 2), which are difficult to
tune (Erk and Pad? (2009)), disappear this way.
613
We can observe that the differences between the
three methods, when simplified and tested in an uni-
fied setting, largely disappear. This is to be expected
as all three methods implement very similar, all moti-
vated intuitions: TFP11 reweights the vector of the
target acquire with the second order vector of the
context knowledge, i.e. with the vector of similarities
of knowledge to all other words in the vocabulary.
TFP10 takes a complementary approach: it reweights
the vector of knowledge with the second order vector
of acquire. In both these methods, anything outside
the object (object?1 respectively) region of the space,
is set to 0. The variant of EP08 that we implement is
very similar to TFP11, however it compares knowl-
edge to all other words in the vocabulary only using
occurrences as objects while TFP11 takes all syntac-
tic relations into account.
Note that TFP10 and TFP11 operate on comple-
mentary syntactic regions of the vectors. For this
reason the two models can be trivially combined.
The combined model (TFP10+11) achieves even bet-
ter results: the difference to TFP11 is small, however
statistically significant at level p < 0.05.
Implementation details. Straightforward imple-
mentations of the three models are computationally
expensive, as they all use ?second order? vectors to
implement contextualization of a target word. Our re-
formulation in terms of matrix operations allows for
efficient implementations, which take advantage of
the sparsity of the input matrix V : contextualization
of a target word runs in O(nnz(V )), where nnz is the
number of non-zero entries. Note that ranking not
only a small set of predefined substitution candidates,
as in the experiment above, but also ranking the en-
tire vocabulary runs in O(nnz(V )). On this task, this
overall running time is in fact identical to that of sim-
pler methods such as those of Mitchell and Lapata
(2008).
In our experiments, we use GigaWord to extract
a syntactic input matrix V of size ? 2M?7M. V is
only 4.5?10?06 dense. Note that because of the sim-
ple operations involved, we do not need to compute
or store the entire VV T matrix, which is much denser
than V (we have estimated order of 1010 entries). The
sparsity of V allows for very efficient computations
in practice: the best single model, TFP11, runs in
less than 0.2s/0.4s per LST instance, for ranking the
candidate list/entire vocabulary in a Python imple-
mentation using scipy.sparse, on a standard 1GHz
processor.
5 Conclusions
In this paper, we have compared three related vec-
tor space models of word meaning in context. We
have reformulated the models and showed that they
are in fact very similar. We also showed that the
different performances reported in the literature are
only to some extent due to the differences in the
models: We evaluated simplified variants of these
and obtained results which are (much) better than
previously reported, bringing the three models much
closer together in terms of performance. Aside from
clarifying the precise relationship between the three
models under consideration, our reformulation has
the additional benefit of allowing the design of a
straightforward and efficient implementation.
Finally, our focus on these methods is justified by
their clear advantages over other classes of models:
unlike token-based or latent variable methods, they
are much simpler and require no parameter tuning.
Furthermore, they also obtain state of the art results
on the paraphrase ranking task, outperforming other
simple type-based methods (see (Van de Cruys et
al., 2011) and (? S?aghdha and Korhonen, 2011) for
results of other methods on this data).
Acknowledgments. This work was partially sup-
ported by the Cluster of Excellence ?Multimodal
Computing and Interaction", funded by the German
Excellence Initiative.
References
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
EMNLP 2010, Cambridge, MA.
Katrin Erk and Sebastian Pad?. 2008. A structured vector
space model for word meaning in context. In Proceed-
ings of EMNLP 2008, Honolulu, HI, USA.
Katrin Erk and Sebastian Pad?. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics.
Katrin Erk and Sebastian Pad?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL 2010 Short Papers, Uppsala, Sweden.
614
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, Columbus, OH, USA.
Diarmuid ? S?aghdha and Anna Korhonen. 2011. Prob-
abilistic models of similarity in syntactic context. In
Proceedings of EMNLP 2011.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Proc. of
IJCNLP 2011.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of NAACL 2010, Los Angeles, California.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL 2010, Uppsala, Sweden.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effective
vector model. In Proceedings of IJCNLP 2011.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space modes of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.
2011. Latent vector weighting for word meaning in
context. In Proceedings of EMNLP 2011.
615
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 30?39,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Computing weakest readings
Alexander Koller
Cluster of Excellence
Saarland University
koller@mmci.uni-saarland.de
Stefan Thater
Dept. of Computational Linguistics
Saarland University
stth@coli.uni-saarland.de
Abstract
We present an efficient algorithm for com-
puting the weakest readings of semantically
ambiguous sentences. A corpus-based eval-
uation with a large-scale grammar shows
that our algorithm reduces over 80% of sen-
tences to one or two readings, in negligible
runtime, and thus makes it possible to work
with semantic representations derived by
deep large-scale grammars.
1 Introduction
Over the past few years, there has been consid-
erable progress in the ability of manually created
large-scale grammars, such as the English Resource
Grammar (ERG, Copestake and Flickinger (2000))
or the ParGram grammars (Butt et al, 2002), to
parse wide-coverage text and assign it deep seman-
tic representations. While applications should ben-
efit from these very precise semantic representa-
tions, their usefulness is limited by the presence
of semantic ambiguity: On the Rondane Treebank
(Oepen et al, 2002), the ERG computes an aver-
age of several million semantic representations for
each sentence, even when the syntactic analysis is
fixed. The problem of appropriately selecting one
of them to work with would ideally be solved by
statistical methods (Higgins and Sadock, 2003) or
knowledge-based inferences. However, no such
approach has been worked out in sufficient detail to
support the disambiguation of treebank sentences.
As an alternative, Bos (2008) proposes to com-
pute the weakest reading of each sentence and then
use it instead of the ?true? reading of the sentence.
This is based on the observation that the readings
of a semantically ambiguous sentence are partially
ordered with respect to logical entailment, and the
weakest readings ? the minimal (least informative)
readings with respect to this order ? only express
?safe? information that is common to all other read-
ings as well. However, when a sentence has mil-
lions of readings, finding the weakest reading is a
hard problem. It is of course completely infeasible
to compute all readings and compare all pairs for
entailment; but even the best known algorithm in
the literature (Gabsdil and Striegnitz, 1999) is only
an optimization of this basic strategy, and would
take months to compute the weakest readings for
the sentences in the Rondane Treebank.
In this paper, we propose a new, efficient ap-
proach to the problem of computing weakest read-
ings. We follow an underspecification approach
to managing ambiguity: Rather than deriving all
semantic representations from the syntactic analy-
sis, we work with a single, compact underspecified
semantic representation, from which the semantic
representations can then be extracted by need. We
then approximate entailment with a rewrite sys-
tem that rewrites readings into logically weaker
readings; the weakest readings are exactly those
readings that cannot be rewritten into some other
reading any more (the relative normal forms). We
present an algorithm that computes the relative nor-
mal forms, and evaluate it on the underspecified de-
scriptions that the ERG derives on a 624-sentence
subcorpus of the Rondane Treebank. While the
mean number of scope readings in the subcorpus
is in the millions, our system computes on average
4.5 weakest readings for each sentence, in less than
twenty milliseconds; over 80% of all sentences are
reduced to at most two weakest readings. In other
words, we make it feasible for the first time to build
an application that uses the individual (weakest)
semantic representations computed by the ERG,
both in terms of the remaining ambiguity and in
terms of performance. Our technique is not lim-
ited to the ERG, but should be applicable to other
underspecification-based grammars as well.
Technically, we use underspecified descriptions
that are regular tree grammars derived from dom-
inance graphs (Althaus et al, 2003; Koller et al,
30
2008). We compute the weakest readings by in-
tersecting these grammars with other grammars
representing the rewrite rules. This approach can
be used much more generally than just for the com-
putation of weakest readings; we illustrate this by
showing how a more general version of the redun-
dancy elimination algorithm by Koller et al (2008)
can be seen as a special case of our construction.
Thus our system can serve as a general framework
for removing unintended readings from an under-
specified representation.
The paper is structured as follows. Section 2
starts by reviewing related work. We recall domi-
nance graphs, regular tree grammars, and the basic
ideas of underspecification in Section 3, before we
show how to compute weakest readings (Section 4)
and logical equivalences (Section 5). In Section 6,
we define a weakening rewrite system for the ERG
and evaluate it on the Rondane Treebank. Section 7
concludes and points to future work.
2 Related work
The idea of deriving a single approximative seman-
tic representation for ambiguous sentences goes
back to Hobbs (1983); however, Hobbs only works
his algorithm out for a restricted class of quantifiers,
and his representations can be weaker than our
weakest readings. Rules that weaken one reading
into another were popular in the 1990s underspeci-
fication literature (Reyle, 1995; Monz and de Rijke,
2001; van Deemter, 1996) because they simplify
logical reasoning with underspecified representa-
tions. From a linguistic perspective, Kempson and
Cormack (1981) even go so far as to claim that
the weakest reading should be taken as the ?basic?
reading of a sentence, and the other readings only
seen as pragmatically licensed special cases.
The work presented here is related to other ap-
proaches that reduce the set of readings of an un-
derspecified semantic representation (USR). Koller
and Niehren (2000) showed how to strengthen
a dominance constraint using information about
anaphoric accessibility; later, Koller et al (2008)
presented and evaluated an algorithm for redun-
dancy elimination, which removes readings from
an USR based on logical equivalence. Our system
generalizes the latter approach and applies it to a
new inference problem (weakest readings) which
they could not solve.
This paper builds closely upon Koller and Thater
(2010), which lays the formal groundwork for the
?
x
sample
y
see
x,y
?
y
repr-of
x,z
?
z
comp
z
24 3
5 6 7
8
?
1
Figure 1: A dominance graph describing the five
readings of the sentence ?it is not the case that
every representative of a company saw a sample.?
work presented here. Here we go beyond that paper
by applying a concrete implementation of our RTG
construction for weakest readings to a real-world
grammar, evaluating the system on practical inputs,
and combining weakest readings with redundancy
elimination.
3 Underspecification
This section briefly reviews two formalisms for
specifying sets of trees: dominance graphs and
regular tree grammars. Both of these formalisms
can be used to model scope ambiguities compactly
by regarding the semantic representations of a sen-
tence as trees. Some example trees are shown in
Fig. 2. These trees can be read as simplified for-
mulas of predicate logic, or as formulas involv-
ing generalized quantifiers (Barwise and Cooper,
1981). Formally, we assume a ranked signature
? of tree constructors { f ,g,a, . . .}, each of which
is equipped with an arity ar( f ) ? 0. We take a
(finite constructor) tree t as a finite tree in which
each node is labelled with a symbol of ?, and the
number of children of the node is exactly the arity
of this symbol. For instance, the signature of the
trees in Fig. 1 is {?x|2,?y|2,compz|0, . . .}. Finite
constructor trees can be seen as ground terms over
? that respect the arities. We write T (?) for the
finite constructor trees over ?.
3.1 Dominance graphs
A (labelled) dominance graph D (Althaus et al,
2003) is a directed graph that consists of a col-
lection of trees called fragments, plus dominance
edges relating nodes in different fragments. We dis-
tinguish the roots WD of the fragments from their
holes, which are the unlabelled leaves. We write
LD : WD? ? for the labeling function of D.
The basic idea behind using dominance graphs
to model scope underspecification is to specify
31
(a) (b)
?
y
?
x
repr-of
x,z
comp
z
sample
y
see
x,y
?
repr-of
x,z
comp
z
see
x,y
sample
y
?
z
?
?
y
?
x
?
z
[+]
[-]
[-] [-]
[-] [-]
[-]
[-]
[+]
[+]
[-]
[-]
[-]
[+]
[+] [+]
(c)
comp
z
repr-of
x,z
see
x,y
sample
y
?
?
y
?
x
?
z
[+]
[-]
[-]
[-]
[-]
[-]
[-]
[+]
(e)
sample
y
see
x,y
repr-of
x,z
comp
z
?
?
y
?
x
?
z
[+]
[-]
[+]
[-]
[-][-][+][+]
(d)
comp
z
repr-of
x,z 
see
x,y 
sample
y
?
?
y
?
x
?
z
[+]
[-]
[-]
[-]
[-]
[-]
[-]
[+]
Figure 2: The five configurations of the dominance graph in Fig. 1.
the ?semantic material? common to all readings
as fragments, plus dominance relations between
these fragments. An example dominance graph
D is shown in Fig. 1. It represents the five read-
ings of the sentence ?it is not the case that every
representative of a company saw a sample.?
Each reading is encoded as a (labeled) configura-
tion of the dominance graph, which can be obtained
by ?plugging? the tree fragments into each other,
in a way that respects the dominance edges: The
source node of each dominance edge must dom-
inate (be an ancestor of) the target node in each
configuration. The trees in Fig. 2 are the five la-
beled configurations of the example graph.
3.2 Regular tree grammars
Regular tree grammars (RTGs) are a general gram-
mar formalism for describing languages of trees
(Comon et al, 2007). An RTG is a 4-tuple G =
(S,N,?,P), where N and ? are nonterminal and ter-
minal alphabets, S ? N is the start symbol, and
P is a finite set of production rules. Unlike in
context-free string grammars (which look super-
ficially the same), the terminal symbols are tree
constructors from ?. The production rules are of
the form A? t, where A is a nonterminal and t is a
tree from T (??N); nonterminals count as having
arity zero, i.e. they must label leaves. A derivation
starts with a tree containing a single node labeled
with S. Then in each step of the derivation, some
leaf u which is labelled with a nonterminal A is
expanded with a rule A? t; this results in a new
tree in which u has been replaced by t, and the
derivation proceeds with this new tree. The lan-
guage L(G) generated by the grammar is the set of
all trees in T (?) that can be derived in this way.
Fig. 3 shows an RTG as an example. This gram-
mar uses sets of root names from D as nonterminal
symbols, and generates exactly the five configura-
tions of the graph in Fig. 1.
The languages that can be accepted by regular
tree grammars are called regular tree languages
{1,2,3,4,5,6,7,8}? ?({2,3,4,5,6,7,8})
{2,3,4,5,6,7,8}? ?x({4,5,6},{3,7,8})
{2,3,4,5,6,7,8}? ?y({7},{2,4,5,6,8})
{2,3,4,5,6,7,8}? ?z({5},{2,3,6,7,8})
{2,4,5,6,8}? ?x({4,5,6},{8})
| ?z({5},{2,6,8})
{2,3,6,7,8}? ?x({6},{3,7,8})
| ?y({7},{2,6,8})
{2,6,8}? ?x({6},{8})
{3,7,8}? ?y({7},{8})
{4,5,6}? ?z({5},{6})
{5}? compz {7}? sampley
{6}? repr-ofx,z {8}? seex,y
Figure 3: A regular tree grammar that generates
the five trees in Fig. 2.
(RTLs), and regular tree grammars are equivalent
to finite tree automata, which are defined essen-
tially like the well-known finite string automata,
except that they assign states to the nodes in a tree
rather than the positions in a string. Regular tree
languages enjoy many of the closure properties of
regular string languages. In particular, we will later
exploit that RTLs are closed under intersection and
complement.
3.3 Dominance graphs as RTGs
An important class of dominance graphs are hy-
pernormally connected (hnc) dominance graphs
(Koller et al, 2003). The precise definition of hnc
graphs is not important here, but note that virtually
all underspecified descriptions that are produced
by current grammars are hypernormally connected
(Flickinger et al, 2005), and we will restrict our-
selves to hnc graphs for the rest of the paper.
Every hypernormally connected dominance
graph D can be automatically translated into an
equivalent RTG GD that generates exactly the same
configurations (Koller et al, 2008); the RTG in
Fig. 3 is an example. The nonterminals of GD are
32
always hnc subgraphs of D. In the worst case, GD
can be exponentially bigger than D, but in practice
it turns out that the grammar size remains manage-
able: even the RTG for the most ambiguous sen-
tence in the Rondane Treebank, which has about
4.5? 1012 scope readings, has only about 75 000
rules and can be computed in a few seconds.
4 Computing weakest readings
Now we are ready to talk about computing the
weakest readings of a hypernormally connected
dominance graph. We will first explain how we ap-
proximate logical weakening with rewrite systems.
We will then discuss how weakest readings can be
computed efficiently as the relative normal forms
of these rewrite systems.
4.1 Weakening rewrite systems
The different readings of a sentence with a scope
ambiguity are not a random collection of formulas;
they are partially ordered with respect to logical
entailment, and are structurally related in a way
that allows us to model this entailment relation
with simpler technical means.
To illustrate this, consider the five configurations
in Fig. 2. The formula represented by (d) logically
entails (c); we say that (c) is a weaker reading than
(d) because it is satisfied by more models. Similar
entailment relations hold between (d) and (e), (e)
and (b), and so on (see also Fig. 5). We can define
the weakest readings of the dominance graph as
the minimal elements of the entailment order; in
the example, these are (b) and (c). Weakest read-
ings capture ?safe? information in that whichever
reading of the sentence the speaker had in mind,
any model of this reading also satisfies at least one
weakest reading; in the absence of convincing dis-
ambiguation methods, they can therefore serve as
a practical approximation of the intended meaning
of the sentence.
A naive algorithm for computing weakest read-
ings would explicitly compute the entailment order,
by running a theorem prover on each pair of config-
urations, and then pick out the minimal elements.
But this algorithm is quadratic in the number of
configurations, and therefore impractically slow
for real-life sentences.
Here we develop a fast algorithm for this prob-
lem. The fundamental insight we exploit is that
entailment among the configurations of a domi-
nance graph can be approximated with rewriting
rules (Baader and Nipkow, 1999). Consider the re-
lation between (d) and (c). We can explain that (d)
entails (c) by observing that (c) can be built from
(d) by exchanging the positions of the adjacent
quantifiers ?x and ?y; more precisely, by applying
the following rewrite rule:
[?] ?x(Q,?y(P,R))??y(P,?x(Q,R)) (1)
The body of the rule specifies that an occurrence of
?x which is the direct parent of an occurrence of ?y
may change positions with it; the subformulas P,
Q, and R must be copied appropriately. The annota-
tion [?] specifies that we must only apply the rule
to subformulas in negative logical polarity: If the
quantifiers in (d) were not in the scope of a nega-
tion, then applying the rule would actually make
the formula stronger. We say that the rule (1) is
logically sound because applying it to a subformula
with the correct polarity of some configuration t
always makes the result t ? logically weaker than t.
We formalize these rewrite systems as follows.
We assume a finite annotation alphabet Ann with a
special starting annotation a0 ? Ann; in the exam-
ple, we had Ann = {+,?} and a0 = +. We also
assume an annotator function ann : Ann???N?
Ann. The function ann can be used to traverse a
tree top-down and compute the annotation of each
node from the annotation of its parent: Its first
argument is the annotation and its second argu-
ment the node label of the parent, and the third
argument is the position of the child among the par-
ent?s children. In our example, the annotator ann
models logical polarity by mapping, for instance,
ann(+,?z,1)= ann(+,?z,2)= ann(+,?y,2)=+,
ann(?,?z,1)= ann(?,?z,2)= ann(+,?x,1)=?,
etc. We have labelled each node of the configura-
tions in Fig. 1 with the annotations that are com-
puted in this way.
Now we can define an annotated rewrite system
R to be a finite set of pairs (a,r) where a is an anno-
tation and r is an ordinary rewrite rule. The rule (1)
above is an example of an annotated rewrite rule
with a =?. A rewrite rule (a,r) can be applied at
the node u of a tree t if ann assigns the annotation a
to u and r is applicable at u as usual. The rule then
rewrites t as described above. In other words, an-
notated rewrite systems are rewrite systems where
rule applications are restricted to subtrees with spe-
cific annotations. We write t?R t ? if some rule of
R can be applied at a node of t, and the result of
rewriting is t ?. The rewrite system R is called linear
33
if every variable that occurs on the left-hand side
of a rule occurs on its right-hand side exactly once.
4.2 Relative normal forms
The rewrite steps of a sound weakening rewrite sys-
tem are related to the entailment order: Because ev-
ery rewrite step transforms a reading into a weaker
reading, an actual weakest readings must be such
that there is no other configuration into which it
can be rewritten. The converse is not always true,
i.e. there can be non-rewritable configurations that
are not weakest readings, but we will see in Sec-
tion 6 that this approximation is good enough for
practical use. So one way to solve the problem of
computing weakest readings is to find readings that
cannot be rewritten further.
One class of configurations that ?cannot be
rewritten? with a rewrite system R is the set of nor-
mal forms of R, i.e. those configurations to which
no rule in R can be applied. In our example, (b)
and (c) are indeed normal forms with respect to
a rewrite system that consists only of the rule (1).
However, this is not exactly what we need here.
Consider a rewrite system that also contains the fol-
lowing annotated rewrite rule, which is also sound
for logical entailment:
[+] ?(?z(P,Q))??z(P,?(Q)), (2)
This rule would allow us to rewrite
the configuration (c) into the tree
?z(compz,?(?y(sampley,?x(repr?ofx,z,seex,y)))).
But this is no longer a configuration of the graph.
If we were to equate weakest readings with normal
forms, we would erroneously classify (c) as not
being a weakest reading. The correct concept
for characterizing weakest readings in terms of
rewriting is that of a relative normal form. We
define a configuration t of a dominance graph D to
be a R-relative normal form of (the configurations
of) D iff there is no other configuration t ? of D such
that t?R t ?. These are the configurations that can?t
be weakened further without obtaining a tree that
is no longer a configuration of D. In other words,
if R approximates entailment, then the R-relative
normal forms approximate the weakest readings.
4.3 Computing relative normal forms
We now show how the relative normal forms of a
dominance graph can be computed efficiently. For
lack of space, we only sketch the construction and
omit all proofs. Details can be found in Koller and
Thater (2010).
The key idea of the construction is to repre-
sent the relation ?R in terms of a context tree
transducer M, and characterize the relative nor-
mal forms of a tree language L in terms of the
pre-image of L under M. Like ordinary regular
tree transducers (Comon et al, 2007), context tree
transducers read an input tree, assigning states to
the nodes, while emitting an output tree. But while
ordinary transducers read the input tree symbol by
symbol, a context tree transducer can read multiple
symbols at once. In this way, they are equivalent to
the extended left-hand side transducers of Graehl
et al (2008).
We will now define context tree transducers. Let
? be a ranked signature, and let Xm be a set of m
variables. We write Con(m)(?) for the contexts with
m holes, i.e. those trees in T (??Xm) in which each
element of Xm occurs exactly once, and always
as a leaf. If C ? Con(m)(?), then C[t1, . . . , tm] =
C[t1/x1, . . . , tm/xm], where x1, . . . ,xm are the vari-
ables from left to right.
A (top-down) context tree transducer from ? to ?
is a 5-tuple M =(Q,?,?,q0,? ). ? and ? are ranked
signatures, Q is a finite set of states, and q0 ? Q is
the start state. ? is a finite set of transition rules of
the form q(C[x1, . . . ,xn])?D[q1(xi1), . . . ,qm(xim)],
where C ? Con(n)(?) and D ? Con(m)(?).
If t ? T (????Q), then we say that M derives
t ? in one step from t, t ?M t ?, if t is of the form
C?[q(C[t1, . . . , tn])] for some C? ? Con(1)(?), t ? is
of the form C?[D[q1(ti1), . . . ,qm(tim)]], and there is
a rule q(C[x1, . . . ,xn])? D[q1(xi1), . . . ,qm(xim)] in
? . The derivation relation ??M is the reflexive,
transitive closure of?M. The translation relation
?M of M is
?M = {(t, t ?) | t ?T (?) and t ? ?T (?) and q0(t)?? t ?}.
For each linear annotated rewrite system R, we
can now build a context tree transducer MR such
that t ?R t ? iff (t, t ?) ? ?MR . The idea is that MR
traverses t from the root to the leaves, keeping
track of the current annotation in its state. MR
can nondeterministically choose to either copy the
current symbol to the output tree unchanged, or to
apply a rewrite rule from R. The rules are built in
such a way that in each run, exactly one rewrite
rule must be applied.
We achieve this as follows. MR takes as its
states the set {q?}?{qa | a ? Ann} and as its start
state the state qa0 . If MR reads a node u in state
qa, this means that the annotator assigns annota-
tion a to u and MR will rewrite a subtree at or
34
below u. If MR reads u in state q?, this means
that MR will copy the subtree below u unchanged
because the rewriting has taken place elsewhere.
Thus MR has three types of rewrite rules. First,
for any f ? ?, we have a rule q?( f (x1, . . . ,xn))?
f (q?(x1), . . . , q?(xn)). Second, for any f and
1 ? i ? n, we have a rule qa( f (x1, . . . ,xn)) ?
f (q?(x1), . . . ,qann(a, f ,i)(xi), . . . , q?(xn)), which non-
deterministically chooses under which child the
rewriting should take place, and assigns it the
correct annotation. Finally, we have a rule
qa(C[x1, . . . ,xn])? C?[q?(xi1), . . . , q?(xin)] for every
rewrite rule C[x1, . . . ,xn]?C?[xi1 , . . . ,xin ] with an-
notation a in R.
Now let?s put the different parts together. We
know that for each hnc dominance graph D, there is
a regular tree grammar GD such that L(GD) is the
set of configurations of D. Furthermore, the pre-
image ??1M (L) = {t | exists t ? ? L with (t, t ?) ? ?M}
of a regular tree language L is also regular (Koller
and Thater, 2010) if M is linear, and regular tree
languages are closed under intersection and com-
plement (Comon et al, 2007). So we can compute
another RTG G? such that
L(G?) = L(GD)? ??1MR (L(GD)).
L(G?) consists of the members of L(GD) which
cannot be rewritten by MR into members of L(GD);
that is, L(G?) is exactly the set of R-relative normal
forms of D. In general, the complement construc-
tion requires exponential time in the size of MR and
GD. However, it can be shown that if the rules in
R have at most depth two and GD is deterministic,
then the entire above construction can be computed
in time O(|GD| ? |R|) (Koller and Thater, 2010).
In other words, we have shown how to compute
the weakest readings of a hypernormally connected
dominance graph D, as approximated by a weaken-
ing rewrite system R, in time linear in the size of
GD and linear in the size of R. This is a dramatic im-
provement over the best previous algorithm, which
was quadratic in |conf(D)|.
4.4 An example
Consider an annotated rewrite system that contains
rule (1) plus the following rewrite rule:
[?] ?z(P,?x(Q,R))??x(?z(P,Q),R) (3)
This rewrite system translates into a top-down
context tree transducer MR with the following tran-
sition rules, omitting most rules of the first two
{1,2,3,4,5,6,7,8}F ??({2,3,4,5,6,7,8}F )
{2,3,4,5,6,7,8}F ??y({7}{q?},{2,4,5,6,8}F )
| ?z({5}{q?},{2,3,6,7,8}F )
{2,3,6,7,8}F ??y({7}{q?},?x({6}{q?},{8}{q?}))
{2,4,5,6,8}F ??x({4,5,6}{q?},{8}{q?})
{4,5,6}{q?}??z({5}{q?},{6}{q?})
{5}{q?}? compz {6}{q?}? repr-ofx,z
{7}{q?}? sampley {8}{q?}? seex,y
Figure 4: RTG for the weakest readings of Fig. 1.
types for lack of space.
q?(?x(x1,?y(x2,x3)))??y(q?(x2),?x(q?(x1), q?(x3)))
q?(?y(x1,?x(x2,x3)))??x(?y(q?(x1), q?(x2)), q?(x3))
q?(?(x1))??(q?(x1))
q+(?(x1))??(q
?(x1))
q?(?x(x1,x2))??x(q?(x1), q?(x2))
q+(?x(x1,x2))??x(q?(x1),q
+(x2))
q+(?x(x1,x2))??x(q
?(x1), q?(x2)) . . .
The grammar G? for the relative normal forms
is shown in Fig. 4 (omitting rules that involve un-
productive nonterminals). We obtain it by starting
with the example grammar GD in Fig. 3; then com-
puting a deterministic RTG GR for ??1MR (L(GD));
and then intersecting the complement of GR with
GD. The nonterminals of G? are subgraphs of D,
marked either with a set of states of MR or the sym-
bol F , indicating that GR had no production rule
for a given left-hand side. The start symbol of G?
is marked with F because G? should only gener-
ate trees that GR cannot generate. As expected, G?
generates precisely two trees, namely (b) and (c).
5 Redundancy elimination, revisited
The construction we just carried out ? characterize
the configurations we find interesting as the rela-
tive normal forms of an annotated rewrite system
R, translate it into a transducer MR, and intersect
conf(D) with the complement of the pre-image un-
der MR ? is more generally useful than just for the
computation of weakest readings. We illustrate this
on the problem of redundancy elimination (Vestre,
1991; Chaves, 2003; Koller et al, 2008) by show-
ing how a variant of the algorithm of Koller et al
(2008) falls out of our technique as a special case.
Redundancy elimination is the problem of com-
puting, from a dominance graph D, another domi-
nance graph D? such that conf(D?)? conf(D) and
35
every formula in conf(D) is logically equivalent
to some formula in conf(D?). We can approximate
logical equivalence using a finite system of equa-
tions such as
?y(P,?z(Q,R)) = ?z(Q,?y(P,R)), (4)
indicating that ?y and ?z can be permuted without
changing the models of the formula.
Following the approach of Section 4, we can
solve the redundancy elimination problem by trans-
forming the equation system into a rewrite system
R such that t?R t ? implies that t and t ? are equiv-
alent. To this end, we assume an arbitrary linear
order < on ?, and orient all equations into rewrite
rules that respect this order. If we assume ?y < ?z,
the example rule (4) translates into the annotated
rewrite rules
[a] ?z(P,?y(Q,R))??y(Q,?z(P,R)) (5)
for all annotations a ? Ann; logical equivalence
is not sensitive to the annotation. Finally, we can
compute the relative normal forms of conf(D) un-
der this rewrite system as above. The result will be
an RTG G? describing a subset of conf(D). Every
tree t in conf(D) that is not in L(G?) is equivalent
to some tree t ? in L(G?), because if t could not be
rewritten into such a t ?, then t would be in rela-
tive normal form. That is, the algorithm solves the
redundancy elimination problem. Furthermore, if
the oriented rewrite system is confluent (Baader
and Nipkow, 1999), no two trees in L(G?) will be
equivalent to each other, i.e. we achieve complete
reduction in the sense of Koller et al (2008).
This solution shares much with that of Koller et
al. (2008), in that we perform redundancy elimina-
tion by intersecting tree grammars. However, the
construction we present here is much more general:
The algorithmic foundation for redundancy elim-
ination is now exactly the same as that for weak-
est readings, we only have to use an equivalence-
preserving rewrite system instead of a weakening
one. This new formal clarity also simplifies the
specification of certain equations, as we will see in
Section 6.
In addition, we can now combine the weakening
rules (1), (3), and (5) into a single rewrite system,
and then construct a tree grammar for the relative
normal forms of the combined system. This algo-
rithm performs redundancy elimination and com-
putes weakest readings at the same time, and in our
example retains only a single configuration, namely
(5)
(e) ??
x
(?
z
,?
y
) (a) ??
y
?
z
?
x
(3)(1)
(1)
(b) ??
y
?
x
?
z
(c) ??
z
?
y
?
x
(d) ??
z
?
x
?
y
(3)
Figure 5: Structure of the configuration set of Fig. 1
in terms of rewriting.
(b); the configuration (c) is rejected because it can
be rewritten to (a) with (5). The graph in Fig. 5 il-
lustrates how the equivalence and weakening rules
conspire to exclude all other configurations.
6 Evaluation
In this section, we evaluate the effectiveness and
efficiency of our weakest readings algorithm on
a treebank. We compute RTGs for all sentences
in the treebank and measure how many weakest
readings remain after the intersection, and how
much time this computation takes.
Resources. For our experiment, we use the Ron-
dane treebank (version of January 2006), a ?Red-
woods style? (Oepen et al, 2002) treebank con-
taining underspecified representations (USRs) in
the MRS formalism (Copestake et al, 2005) for
sentences from the tourism domain.
Our implementation of the relative normal forms
algorithm is based on Utool (Koller and Thater,
2005), which (among other things) can translate a
large class of MRS descriptions into hypernormally
connected dominance graphs and further into RTGs
as in Section 3. The implementation exploits cer-
tain properties of RTGs computed from dominance
graphs to maximize efficiency. We will make this
implementation publically available as part of the
next Utool release.
We use Utool to automatically translate the 999
MRS descriptions for which this is possible into
RTGs. To simplify the specification of the rewrite
systems, we restrict ourselves to the subcorpus in
which all scope-taking operators (labels with arity
> 0) occur at least ten times. This subset contains
624 dominance graphs. We refer to this subset as
?RON10.?
Signature and annotations. For each domi-
nance graph D that we obtain by converting an
MRS description, we take GD as a grammar over
the signature ?= { fu | u ?WD, f = LD(u)}. That
is, we distinguish possible different occurrences
of the same symbol in D by marking each occur-
36
rence with the name of the node. This makes GD a
deterministic grammar.
We then specify an annotator over ? that assigns
polarities for the weakening rewrite system. We
distinguish three polarities: + for positive occur-
rences, ? for negative occurrences (as in predicate
logic), and ? for contexts in which a weakening
rule neither weakens or strengthens the entire for-
mula. The starting annotation is +.
Finally, we need to decide upon each scope-
taking operator?s effects on these annotations. To
this end, we build upon Barwise and Cooper?s
(1981) classification of the monotonicity prop-
erties of determiners. A determiner is upward
(downward) monotonic if making the denotation of
the determiner?s argument bigger (smaller) makes
the sentence logically weaker. For instance, ev-
ery is downward monotonic in its first argument
and upward monotonic in its second argument,
i.e. every girl kissed a boy entails every blond
girl kissed someone. Thus ann(everyu,a,1) =?a
and ann(everyu,a,2) = a (where u is a node name
as above). There are also determiners with non-
monotonic argument positions, which assign the
annotation ? to this argument. Negation reverses
positive and negative polarity, and all other non-
quantifiers simply pass on their annotation to the
arguments.
Weakest readings. We use the following weak-
ening rewrite system for our experiment, where
i ? {1,2}:
1. [+] (E/i,D/1), (D/2,D/1)
2. [+] (E/i,P/1), (D/2,P/1)
3. [+] (E/i,A/2), (D/1,A/2)
4. [+] (A/2,N/1)
5. [+] (N/1,E/i), (N/1,D/2)
6. [+] (E/i,M/1), (D/1,M/1)
Here the symbols E, D, etc. stand for classes
of labels in ?, and a rule schema [a] (C/i,C?/k) is
to be read as shorthand for a set of rewrite rules
which rearrange a tree where the i-th child of a
symbol from C is a symbol from C? into a tree
where the symbol from C becomes the k-th child
of the symbol from C?. For example, because we
have allu ? A and notv ? N, Schema 4 licenses the
following annotated rewrite rule:
[+] allu(P,notv(Q))? notv(allu(P,Q)).
We write E and D for existential and definite
determiners. P stands for proper names and pro-
nouns, A stands for universal determiners like all
and each, N for the negation not, and M for modal
operators like can or would. M also includes in-
tensional verbs like have to and want. Notice that
while the reverse rules are applicable in negative
polarities, no rules are applicable in polarity ?.
Rule schema 1 states, for instance, that the spe-
cific (wide-scope) reading of the indefinite in the
president of a company is logically stronger than
the reading in which a company is within the re-
striction of the definite determiner. The schema is
intuitively plausible, and it can also be proved to be
logically sound if we make the standard assumption
that the definite determiner the means ?exactly one?
(Montague, 1974). A similar argument applies to
rule schema 2.
Rule schema 3 encodes the classical entailment
(1). Schema 4 is similar to the rule (2). Notice
that it is not, strictly speaking, logically sound;
however, because strong determiners like all or
every carry a presupposition that their restrictions
have a non-empty denotation (Lasersohn, 1993),
the schema becomes sound for all instances that
can be expressed in natural language. Similar ar-
guments apply to rule schemas 5 and 6, which are
potentially unsound for subtle reasons involving
the logical interpretation of intensional expressions.
However, these cases of unsoundness did not occur
in our test corpus.
Redundancy elimination. In addition, we as-
sume the following equation system for redundancy
elimination for i, j ? {1,2} and k ? N (again writ-
ten in an analogous shorthand as above):
7. E/i = E/ j
8. D/1 = E/i, E/i = D/1
9. D/1 = D/1
10. ?/k = P/2
These rule schemata state that permuting exis-
tential determiners with each other is an equiva-
lence transformation, and so is permuting definite
determiners with existential and definite determin-
ers if one determiner is the second argument (in
the scope) of a definite. Schema 10 states that
proper names and pronouns, which the ERG ana-
lyzes as scope-bearing operators, can permute with
any other label.
We orient these equalities into rewrite rules by
ordering symbols in P before symbols that are not
37
All KRT08 RE RE+WR
#conf = 1 8.5% 23.4% 34.9% 66.7%
#conf? 2 20.5% 40.9% 57.9% 80.6%
avg(#conf) 3.2M 7603.1 119.0 4.5
med(#conf) 25 4 2 1
runtime 8.1s 9.4s 8.7s 9.1s
Figure 6: Analysis of the numbers of configurations
in RON10.
in P, and otherwise ordering a symbol fu before a
symbol gv if u < v by comparison of the (arbitrary)
node names.
Results. We used these rewrite systems to com-
pute, for each USR in RON10, the number of all
configurations, the number of configurations that
remain after redundancy elimination, and the num-
ber of weakest readings (i.e., the relative normal
forms of the combined equivalence and weakening
rewrite systems). The results are summarized in
Fig. 6. By computing weakest readings (WR), we
reduce the ambiguity of over 80% of all sentences
to one or two readings; this is a clear improvement
even over the results of the redundancy elimina-
tion (RE). Computing weakest readings reduces
the mean number of readings from several million
to 4.5, and improves over the RE results by a factor
of 30. Notice that the RE algorithm from Section 5
is itself an improvement over Koller et al?s (2008)
system (?KRT08? in the table), which could not
process the rule schema 10.
Finally, computing the weakest readings takes
only a tiny amount of extra runtime compared to
the RE elimination or even the computation of the
RTGs (reported as the runtime for ?All?).1 This re-
mains true on the entire Rondane corpus (although
the reduction factor is lower because we have no
rules for the rare scope-bearers): RE+WR compu-
tation takes 32 seconds, compared to 30 seconds
for RE. In other words, our algorithm brings the
semantic ambiguity in the Rondane Treebank down
to practically useful levels at a mean runtime in-
vestment of a few milliseconds per sentence.
It is interesting to note how the different rule
schemas contribute to this reduction. While the
instances of Schemata 1 and 2 are applicable in 340
sentences, the other schemas 3?6 together are only
1Runtimes were measured on an Intel Core 2 Duo CPU
at 2.8 GHz, under MacOS X 10.5.6 and Apple Java 1.5.0_16,
after allowing the JVM to just-in-time compile the bytecode.
applicable in 44 sentences. Nevertheless, where
these rules do apply, they have a noticeable effect:
Without them, the mean number of configurations
in RON10 after RE+WR increases to 12.5.
7 Conclusion
In this paper, we have shown how to compute the
weakest readings of a dominance graph, charac-
terized by an annotated rewrite system. Evaluat-
ing our algorithm on a subcorpus of the Rondane
Treebank, we reduced the mean number of config-
urations of a sentence from several million to 4.5,
in negligible runtime. Our algorithm can be ap-
plied to other problems in which an underspecified
representation is to be disambiguated, as long as
the remaining readings can be characterized as the
relative normal forms of a linear annotated rewrite
system. We illustrated this for the case of redun-
dancy elimination.
The algorithm presented here makes it possible,
for the first time, to derive a single meaningful se-
mantic representation from the syntactic analysis
of a deep grammar on a large scale. In the future,
it will be interesting to explore how these semantic
representations can be used in applications. For in-
stance, it seems straightforward to adapt MacCart-
ney and Manning?s (2008) ?natural logic?-based
Textual Entailment system, because our annotator
already computes the polarities needed for their
monotonicity inferences. We could then perform
such inferences on (cleaner) semantic representa-
tions, rather than strings (as they do).
On the other hand, it may be possible to re-
duce the set of readings even further. We retain
more readings than necessary in many treebank sen-
tences because the combined weakening and equiv-
alence rewrite system is not confluent, and there-
fore may not recognize a logical relation between
two configurations. The rewrite system could be
made more powerful by running the Knuth-Bendix
completion algorithm (Knuth and Bendix, 1970).
Exploring the practical tradeoff between the further
reduction in the number of remaining configura-
tions and the increase in complexity of the rewrite
system and the RTG would be worthwhile.
Acknowledgments. We are indebted to Joachim
Niehren, who pointed out a crucial simplification
in the algorithm to us. We also thank our reviewers
for their constructive comments.
38
References
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn,
J. Niehren, and S. Thiel. 2003. An efficient graph
algorithm for dominance constraints. Journal of Al-
gorithms, 48:194?219.
F. Baader and T. Nipkow. 1999. Term rewriting and all
that. Cambridge University Press.
J. Barwise and R. Cooper. 1981. Generalized quanti-
fiers and natural language. Linguistics and Philoso-
phy, 4:159?219.
J. Bos. 2008. Let?s not argue about semantics. In
Proceedings of the 6th international conference on
Language Resources and Evaluation (LREC 2008).
M. Butt, H. Dyvik, T. Holloway King, H. Masuichi,
and C. Rohrer. 2002. The parallel grammar
project. In Proceedings of COLING-2002 Workshop
on Grammar Engineering and Evaluation.
R. P. Chaves. 2003. Non-redundant scope disambigua-
tion in underspecified semantics. In Proceedings of
the 8th ESSLLI Student Session.
H. Comon, M. Dauchet, R. Gilleron, C. L?ding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree automata techniques and appli-
cations. Available on: http://www.grappa.
univ-lille3.fr/tata.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and
broad-coverage english grammar using HPSG. In
Proceedings of the 2nd International Conference on
Language Resources and Evaluation (LREC).
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2005. Minimal recursion semantics: An introduc-
tion. Journal of Language and Computation.
D. Flickinger, A. Koller, and S. Thater. 2005. A new
well-formedness criterion for semantics debugging.
In Proceedings of the 12th International Conference
on HPSG, Lisbon.
M. Gabsdil and K. Striegnitz. 1999. Classifying scope
ambiguities. In Proceedings of the First Intl. Work-
shop on Inference in Computational Semantics.
J. Graehl, K. Knight, and J. May. 2008. Training tree
transducers. Computational Linguistics, 34(3):391?
427.
D. Higgins and J. Sadock. 2003. A machine learning
approach to modeling scope preferences. Computa-
tional Linguistics, 29(1).
J. Hobbs. 1983. An improper treatment of quantifi-
cation in ordinary English. In Proceedings of the
21st Annual Meeting of the Association for Compu-
tational Linguistics (ACL?83).
R. Kempson and A. Cormack. 1981. Ambiguity and
quantification. Linguistics and Philosophy, 4:259?
309.
D. Knuth and P. Bendix. 1970. Simple word problems
in universal algebras. In J. Leech, editor, Computa-
tional Problems in Abstract Algebra, pages 263?297.
Pergamon Press, Oxford.
A. Koller and J. Niehren. 2000. On underspecified
processing of dynamic semantics. In Proceedings of
the 18th International Conference on Computational
Linguistics (COLING-2000).
A. Koller and S. Thater. 2005. Efficient solving and ex-
ploration of scope ambiguities. In ACL-05 Demon-
stration Notes, Ann Arbor.
A. Koller and S. Thater. 2010. Computing relative nor-
mal forms in regular tree languages. In Proceedings
of the 21st International Conference on Rewriting
Techniques and Applications (RTA).
A. Koller, J. Niehren, and S. Thater. 2003. Bridg-
ing the gap between underspecification formalisms:
Hole semantics as dominance constraints. In Pro-
ceedings of the 10th EACL.
A. Koller, M. Regneri, and S. Thater. 2008. Regular
tree grammars as a formalism for scope underspeci-
fication. In Proceedings of ACL-08: HLT.
P. Lasersohn. 1993. Existence presuppositions and
background knowledge. Journal of Semantics,
10:113?122.
B. MacCartney and C. Manning. 2008. Modeling
semantic containment and exclusion in natural lan-
guage inference. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING).
R. Montague. 1974. The proper treatment of quantifi-
cation in ordinary English. In R. Thomason, editor,
Formal Philosophy. Selected Papers of Richard Mon-
tague. Yale University Press, New Haven.
C. Monz and M. de Rijke. 2001. Deductions with
meaning. In Michael Moortgat, editor, Logical As-
pects of Computational Linguistics, Third Interna-
tional Conference (LACL?98), volume 2014 of LNAI.
Springer-Verlag, Berlin/Heidelberg.
S. Oepen, K. Toutanova, S. Shieber, C. Manning,
D. Flickinger, and T. Brants. 2002. The LinGO
Redwoods treebank: Motivation and preliminary
applications. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING).
Uwe Reyle. 1995. On reasoning with ambiguities. In
Proceedings of the 7th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL?95).
K. van Deemter. 1996. Towards a logic of ambiguous
expressions. In Semantic Ambiguity and Underspec-
ification. CSLI Publications, Stanford.
E. Vestre. 1991. An algorithm for generating non-
redundant quantifier scopings. In Proc. of EACL,
Berlin.
39
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948?957,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Contextualizing Semantic Representations
Using Syntactically Enriched Vector Models
Stefan Thater and Hagen F?rstenau and Manfred Pinkal
Department of Computational Linguistics
Saarland University
{stth, hagenf, pinkal}@coli.uni-saarland.de
Abstract
We present a syntactically enriched vec-
tor model that supports the computation
of contextualized semantic representations
in a quasi compositional fashion. It em-
ploys a systematic combination of first- and
second-order context vectors. We apply
our model to two different tasks and show
that (i) it substantially outperforms previ-
ous work on a paraphrase ranking task, and
(ii) achieves promising results on a word-
sense similarity task; to our knowledge, it is
the first time that an unsupervised method
has been applied to this task.
1 Introduction
In the logical paradigm of natural-language seman-
tics originating from Montague (1973), semantic
structure, composition and entailment have been
modelled to an impressive degree of detail and
formal consistency. These approaches, however,
lack coverage and robustness, and their impact
on realistic natural-language applications is lim-
ited: The logical framework suffers from over-
specificity, and is inappropriate to model the per-
vasive vagueness, ambivalence, and uncertainty
of natural-language semantics. Also, the hand-
crafting of resources covering the huge amounts
of content which are required for deep semantic
processing is highly inefficient and expensive.
Co-occurrence-based semantic vector models of-
fer an attractive alternative. In the standard ap-
proach, word meaning is represented by feature
vectors, with large sets of context words as dimen-
sions, and their co-occurrence frequencies as val-
ues. Semantic similarity information can be ac-
quired using unsupervised methods at virtually no
cost, and the information gained is soft and gradual.
Many NLP tasks have been modelled successfully
using vector-based models. Examples include in-
formation retrieval (Manning et al, 2008), word-
sense discrimination (Sch?tze, 1998) and disam-
biguation (McCarthy and Carroll, 2003), to name
but a few.
Standard vector-space models have serious lim-
itations, however: While semantic information is
typically encoded in phrases and sentences, distri-
butional semantics, in sharp contrast to logic-based
semantics, does not offer any natural concept of
compositionality that would allow the semantics
of a complex expression to be computed from the
meaning of its parts. A different, but related prob-
lem is caused by word-sense ambiguity and con-
textual variation of usage. Frequency counts of
context words for a given target word provide in-
variant representations averaging over all different
usages of the target word. There is no obvious way
to distinguish the different senses of e.g. acquire
in different contexts, such as acquire knowledge or
acquire shares.
Several approaches for word-sense disambigua-
tion in the framework of distributional semantics
have been proposed in the literature (Sch?tze, 1998;
McCarthy and Carroll, 2003). In contrast to these
approaches, we present a method to model the mu-
tual contextualization of words in a phrase in a com-
positional way, guided by syntactic structure. To
some extent, our method resembles the approaches
proposed by Mitchell and Lapata (2008) and Erk
and Pad? (2008). We go one step further, however,
in that we employ syntactically enriched vector
models as the basic meaning representations, as-
suming a vector space spanned by combinations
of dependency relations and words (Lin, 1998).
This allows us to model the semantic interaction
between the meaning of a head word and its de-
pendent at the micro-level of relation-specific co-
occurrence frequencies. It turns out that the benefit
to precision is considerable.
Using syntactically enriched vector models
raises problems of different kinds: First, the use
948
of syntax increases dimensionality and thus may
cause data sparseness (Pad? and Lapata, 2007).
Second, the vectors of two syntactically related
words, e.g., a target verb acquire and its direct ob-
ject knowledge, typically have different syntactic
environments, which implies that their vector repre-
sentations encode complementary information and
there is no direct way of combining the information
encoded in the respective vectors.
To solve these problems, we build upon pre-
vious work (Thater et al, 2009) and propose to
use syntactic second-order vector representations.
Second-order vector representations in a bag-of-
words setting were first used by Sch?tze (1998);
in a syntactic setting, they also feature in Dligach
and Palmer (2008). For the problem at hand, the
use of second-order vectors alleviates the sparse-
ness problem, and enables the definition of vector
space transformations that make the distributional
information attached to words in different syntactic
positions compatible. Thus, it allows vectors for
a predicate and its arguments to be combined in a
compositional way.
We conduct two experiments to assess the suit-
ability of our method. Our first experiment is car-
ried out on the SemEval 2007 lexical substitution
task dataset (McCarthy and Navigli, 2007). It will
show that our method significantly outperforms
other unsupervised methods that have been pro-
posed in the literature to rank words with respect
to their semantic similarity in a given linguistic
context. In a second experiment, we apply our
model to the ?word sense similarity task? recently
proposed by Erk and McCarthy (2009), which is
a refined variant of a word-sense disambiguation
task. The results show a substantial positive effect.
Plan of the paper. We will first review related
work in Section 2, before presenting our model in
Section 3. In Sections 4 and 5 we evaluate our
model on the two different tasks. Section 6 con-
cludes.
2 Related Work
Several approaches to contextualize vector repre-
sentations of word meaning have been proposed.
One common approach is to represent the mean-
ing of a word a in context b simply as the sum, or
centroid of a and b (Landauer and Dumais, 1997).
Kintsch (2001) considers a variant of this simple
model. By using vector representations of a predi-
cate p and an argument a, Kintsch identifies words
that are similar to p and a, and takes the centroid
of these words? vectors to be the representation of
the complex expression p(a).
Mitchell and Lapata (2008), henceforth M&L,
propose a general framework in which meaning rep-
resentations for complex expressions are computed
compositionally by combining the vector represen-
tations of the individual words of the complex ex-
pression. They focus on the assessment of different
operations combining the vectors of the subexpres-
sions. An important finding is that component-wise
multiplication outperforms the more common addi-
tion method. Although their composition method
is guided by syntactic structure, the actual instanti-
ations of M&L?s framework are insensitive to syn-
tactic relations and word-order, assigning identical
representation to dog bites man and man bites dog
(see Erk and Pad? (2008) for a discussion). Also,
they use syntax-free bag-of-words-based vectors as
basic representations of word meaning.
Erk and Pad? (2008), henceforth E&P, represent
the meaning of a word w through a collection of
vectors instead of a single vector: They assume
selectional preferences and inverse selectional pref-
erences to be constitutive parts of the meaning in
addition to the meaning proper. The interpretation
of a word p in context a is a combination of p?s
meaning with the (inverse) selectional preference
of a. Thus, a verb meaning does not combine di-
rectly with the meaning of its object noun, as on
the M&L account, but with the centroid of the vec-
tors of the verbs to which the noun can stand in an
object relation. Clearly, their approach is sensitive
to syntactic structure. Their evaluation shows that
their model outperforms the one proposed by M&L
on a lexical substitution task (see Section 4). The
basic vectors, however, are constructed in a word
space similar to the one of the M&L approach.
In Thater et al (2009), henceforth TDP, we took
up the basic idea from E&P of exploiting selec-
tional preference information for contextualization.
Instead of using collections of different vectors,
we incorporated syntactic information by assuming
a richer internal structure of the vector represen-
tations. In a small case study, moderate improve-
ments over E&P on a lexical substitution task could
be shown. In the present paper, we formulate a
general model of syntactically informed contextu-
alization and show how to apply it to a number a
of representative lexical substitution tasks. Eval-
uation shows significant improvements over TDP
949
acquire
VB
purchase
VB
gain
VB
share
NN
knowlege
NN
obj, 5 obj, 3 obj, 6 obj, 7
skill
NN
buy-back
NN
conj, 2 nn, 1
Figure 1: Co-occurrence graph of a small sample
corpus of dependency trees.
and E&P.
3 The model
In this section, we present our method of contex-
tualizing semantic vector representations. We first
give an overview of the main ideas, which is fol-
lowed by a technical description of first-order and
second-order vectors (Section 3.2) and the contex-
tualization operation (Section 3.3).
3.1 Overview
Our model employs vector representations for
words and expressions containing syntax-specific
first and second order co-occurrences information.
The basis for the construction of both kinds of
vector representations are co-occurrence graphs.
Figure 1 shows the co-occurrence graph of a small
sample corpus of dependency trees: Words are
represented as nodes in the graph, possible depen-
dency relations between them are drawn as labeled
edges, with weights corresponding to the observed
frequencies. From this graph, we can directly read
off the first-order vector for every word w: the vec-
tor?s dimensions correspond to pairs (r,w?) of a
grammatical relation and a neighboring word, and
are assigned the frequency count of (w,r,w?).
The noun knowledge, for instance, would be rep-
resented by the following vector:
?5(OBJ?1,gain),2(CONJ?1,skill),3(OBJ?1,acquire), . . .?
This vector talks about the possible dependency
heads of knowledge and thus can be seen as the
(inverse) selectional preference of knowledge (see
Erk and Pad? (2008)).
As soon as we want to compute a meaning rep-
resentation for a phrase like acquire knowledge
from the verb acquire together with its direct ob-
ject knowledge, we are facing the problem that
verbs have different syntactic neighbors than nouns,
hence their first-order vectors are not easily com-
parable. To solve this problem we additionally
introduce another kind of vectors capturing infor-
mations about all words that can be reached with
two steps in the co-occurrence graph. Such a path
is characterized by two dependency relations and
two words, i.e., a quadruple (r,w?,r?,w??), whose
weight is the product of the weights of the two
edges used in the path. To avoid overly sparse vec-
tors we generalize over the ?middle word? w? and
build our second-order vectors on the dimensions
corresponding to triples (r,r?,w??) of two depen-
dency relations and one word at the end of the two-
step path. For instance, the second-order vector for
acquire is
?15(OBJ,OBJ?1,gain),
6(OBJ,CONJ?1,skill),
6(OBJ,OBJ?1,buy-back),
42(OBJ,OBJ?1,purchase), . . .?
In this simple example, the values are the prod-
ucts of the edge weights on each of the paths. The
method of computation is detailed in Section 3.2.
Note that second order vectors in particular con-
tain paths of the form (r,r?1,w?), relating a verb
w to other verbs w? which are possible substitution
candidates.
With first- and second-order vectors we can
now model the interaction of semantic informa-
tion within complex expressions. Given a pair
of words in a particular grammatical relation like
acquire knowledge, we contextualize the second-
order vector of acquire with the first-order vec-
tor of knowledge. We let the first-order vector
with its selectional preference information act as a
kind of weighting filter on the second-order vector,
and thus refine the meaning representation of the
verb. The actual operation we will use is point-
wise multiplication, which turned out to be the
best-performing one for our purpose. Interestingly,
Mitchell and Lapata (2008) came to the same result
in a different setting.
In our example, we obtain a new second-order
vector for acquire in the context of knowledge:
?75(OBJ,OBJ?1,gain),
12(OBJ,CONJ?1,skill),
0(OBJ,OBJ?1,buy-back),
0(OBJ,OBJ?1,purchase), . . .?
Note that all dimensions that are not ?licensed? by
the argument knowledge are filtered out as they are
multiplied with 0. Also, contextualisation of ac-
quire with the argument share instead of knowledge
950
would have led to a very different vector, which
reflects the fact that the two argument nouns induce
different readings of the inherently ambiguous ac-
quire.
3.2 First and second-order vectors
Assuming a set W of words and a set R of depen-
dency relation labels, we consider a Euclidean vec-
tor space V1 spanned by the set of orthonormal
basis vectors {~er,w? | r ? R,w? ?W}, i.e., a vector
space whose dimensions correspond to pairs of a re-
lation and a word. Recall that any vector of V1 can
be represented as a finite sum of the form ?ai~er,w?
with appropriate scalar factors ai. In this vector
space we define the first-order vector [w] of a word
w as follows:
[w] = ?
r?R
w??W
?(w,r,w?) ?~er,w?
where ? is a function that assigns the dependency
triple (w,r,w?) a corresponding weight. In the sim-
plest case, ? would denote the frequency in a cor-
pus of dependency trees of w occurring together
with w? in relation r. In the experiments reported be-
low, we use pointwise mutual information (Church
and Hanks, 1990) instead as it proved superior to
raw frequency counts:
pmi(w,r,w?) = log
p(w,w? | r)
p(w | r)p(w? | r)
We further consider a similarly defined vec-
tor space V2, spanned by an orthonormal basis
{~er,r?,w? | r,r? ? R,w? ?W}. Its dimensions there-
fore correspond to triples of two relations and a
word. Evidently this is a higher dimensional space
than V1, which therefore can be embedded into
V2 by the ?lifting maps? Lr : V1 ?? V2 defined by
Lr(~er?,w?) :=~er,r?,w? (and by linear extension there-
fore on all vectors of V1). Using these lifting maps
we define the second-order vector [[w]] of a word w
as
[[w]] = ?
r?R
w??W
?(w,r,w?) ?Lr
(
[w?]
)
Substituting the definitions of Lr and [w?], this
yields
[[w]] = ?
r,r??R
w???W
(
?
w??W
?(w,r,w?)?(w?,r?,w??)
)
~er,r?,w??
which shows the generalization over w? in form of
the inner sum.
For example, if w is a verb, r = OBJ and r? =
OBJ?1 (i.e., the inverse object relation), then the
coefficients of ~er,r?,w?? in [[w]] would characterize
the distribution of verbs w?? which share objects
with w.
3.3 Composition
Both first and second-order vectors are defined for
lexical expressions only. In order to represent the
meaning of complex expressions we need to com-
bine the vectors for grammatically related words
in a given sentence. Given two words w and w? in
relation r we contextualize the second-order vector
of w with the r-lifted first-order vector of w?:
[[wr:w? ]] = [[w]]?Lr([w
?])
Here ? may denote any operator on V2. The ob-
jective is to incorporate (inverse) selectional pref-
erence information from the context (r,w?) in such
a way as to identify the correct word sense of w.
This suggests that the dimensions of [[w]] should
be filtered so that only those compatible with the
context remain. A more flexible approach than
simple filtering, however, is to re-weight those di-
mensions with context information. This can be
expressed by pointwise vector multiplication (in
terms of the given basis of V2). We therefore take
? to be pointwise multiplication.
To contextualize (the vector of) a word w with
multiple words w1, . . . ,wn and corresponding rela-
tions r1, . . . ,rn, we compute the sum of the results
of the pairwise contextualizations of the target vec-
tor with the vectors of the respective dependents:
[[wr1:w1,...,rn:wn ]] =
n
?
k=1
[[wrk:wk ]]
4 Experiments: Ranking Paraphrases
In this section, we evaluate our model on a para-
phrase ranking task. We consider sentences with
an occurrence of some target word w and a list of
paraphrase candidates w1, . . . ,wk such that each of
the wi is a paraphrase of w for some sense of w.
The task is to decide for each of the paraphrase
candidates wi how appropriate it is as a paraphrase
of w in the given context. For instance, buy, pur-
chase and obtain are all paraphrases of acquire, in
the sense that they can be substituted for acquire in
some contexts, but purchase and buy are not para-
phrases of acquire in the first sentence of Table 1.
951
Sentence Paraphrases
Teacher education students will acquire the knowl-
edge and skills required to [. . . ]
gain 4; amass 1; receive 1; obtain 1
Ontario Inc. will [. . . ] acquire the remaining IXOS
shares [. . . ]
buy 3; purchase 1; gain 1; get 1; procure 2; obtain 1
Table 1: Two examples from the lexical substitution task data set
4.1 Resources
We use a vector model based on dependency trees
obtained from parsing the English Gigaword corpus
(LDC2003T05). The corpus consists of news from
several newswire services, and contains over four
million documents. We parse the corpus using the
Stanford parser1 (de Marneffe et al, 2006) and a
non-lexicalized parser model, and extract over 1.4
billion dependency triples for about 3.9 million
words (lemmas) from the parsed corpus.
To evaluate the performance of our model, we
use various subsets of the SemEval 2007 lexical
substitution task (McCarthy and Navigli, 2007)
dataset. The complete dataset contains 10 instances
for each of 200 target words?nouns, verbs, adjec-
tives and adverbs?in different sentential contexts.
Systems that participated in the task had to generate
paraphrases for every instance, and were evaluated
against a gold standard containing up to 10 possible
paraphrases for each of the individual instances.
There are two natural subtasks in generating
paraphrases: identifying paraphrase candidates and
ranking them according to the context. We follow
E&P and evaluate it only on the second subtask:
we extract paraphrase candidates from the gold
standard by pooling all annotated gold-standard
paraphrases for all instances of a verb in all con-
texts, and use our model to rank these paraphrase
candidates in specific contexts. Table 1 shows two
instances of the target verb acquire together with
its paraphrases in the gold standard as an example.
The paraphrases are attached with weights, which
correspond to the number of times they have been
given by different annotators.
4.2 Evaluation metrics
To evaluate the performance of our method we use
generalized average precision (Kishida, 2005), a
1We use version 1.6 of the parser. We modify the depen-
dency trees by ?folding? prepositions into the edge labels to
make the relation between a head word and the head noun of
a prepositional phrase explicit.
variant of average precision.
Average precision (Buckley and Voorhees, 2000)
is a measure commonly used to evaluate systems
that return ranked lists of results. Generalized aver-
age precision (GAP) additionally rewards the cor-
rect order of positive cases w.r.t. their gold standard
weight. We define average precision first:
AP =
?ni=1xi pi
R
pi =
?ik=1xk
i
where xi is a binary variable indicating whether
the ith item as ranked by the model is in the gold
standard or not, R is the size of the gold standard,
and n is the number of paraphrase candidates to
be ranked. If we take xi to be the gold standard
weight of the ith item or zero if it is not in the
gold standard, we can define generalized average
precision as follows:
GAP =
?ni=1 I(xi) pi
?Ri=1 I(yi)yi
where I(xi) = 1 if xi is larger than zero, zero oth-
erwise, and yi is the average weight of the ideal
ranked list y1, . . . ,yi of gold standard paraphrases.
As a second scoring method, we use precision
out of ten (P10). The measure is less discriminative
than GAP. We use it because we want to compare
our model with E&P. P10 measures the percentage
of gold-standard paraphrases in the top-ten list of
paraphrases as ranked by the system, and can be
defined as follows (McCarthy and Navigli, 2007):
P10 =
?s?M?G f (s)
?s?G f (s)
,
where M is the list of 10 paraphrase candidates top-
ranked by the model, G is the corresponding anno-
tated gold-standard data, and f (s) is the weight of
the individual paraphrases.
4.3 Experiment 1: Verb paraphrases
In our first experiment, we consider verb para-
phrases using the same controlled subset of the
952
lexical substitution task data that had been used by
TDP in an earlier study. We compare our model
to various baselines and the models of TDP and
E&P, and show that our new model substantially
outperforms previous work.
Dataset. The dataset is identical to the one used
by TDP and has been constructed in the same way
as the dataset used by E&P: it contains those gold-
standard instances of verbs that have?according
to the analyses produced by the MiniPar parser
(Lin, 1993)?an overtly realized subject and object.
Gold-standard paraphrases that do not occur in the
parsed British National Corpus are removed.2 In
total, the dataset contains 162 instances for 34 dif-
ferent verbs. On average, target verbs have 20.5
substitution candidates; for individual instances of
a target verb, an average of 3.9 of the substitution
candidates are annotated as correct paraphrases.
Below, we will refer to this dataset as ?LST/SO.?
Experimental procedure. To compute the vec-
tor space, we consider only a subset of the complete
set of dependency triples extracted from the parsed
Gigaword corpus. We experimented with various
strategies, and found that models which consider
all dependency triples exceeding certain pmi- and
frequency thresholds perform best.
Since the dataset is rather small, we use a four-
fold cross-validation method for parameter tuning:
We divide the dataset into four subsets, test vari-
ous parameter settings on one subset and use the
parameters that perform best (in terms of GAP) to
evaluate the model on the three other subsets. We
consider the following parameters: pmi-thresholds
for the dependency triples used in the computa-
tion of the first- and second-order vectors, and
frequency thresholds. The parameters differ only
slightly between the four subsets, and the general
tendency is that good results are obtained if a low
pmi-threshold (? 2) is applied to filter dependency
triples used in the computation of the second-order
vectors, and a relatively high pmi-threshold (? 4)
to filter dependency triples in the computation of
the first-order vectors. Good performing frequency
thresholds are 10 or 15. The threshold values for
context vectors are slightly different: a medium
pmi-threshold between 2 and 4 and a low frequency
threshold of 3.
To rank paraphrases in context, we compute con-
textualized vectors for the verb in the input sen-
2Both TDP and E&P use the British National Corpus.
tence, i.e., a second order vector for the verb that
is contextually constrained by the first order vec-
tors of all its arguments, and compare them to the
unconstrained (second-order) vectors of each para-
phrase candidate, using cosine similarity.3 For the
first sentence in Table 1, for example, we compute
[[acquireSUBJ:student,OBJ:knowledge]] and compare it to
[[gain]], [[amass]], [[buy]], [[purchase]] and so on.
Baselines. We evaluate our model against a ran-
dom baseline and two variants of our model: One
variant (?2nd order uncontexualized?) simply uses
contextually unconstrained second-order vectors
to rank paraphrase candidates. Comparing the full
model to this variant will show how effective our
method of contextualizing vectors is. The sec-
ond variant (?1st order contextualized?) represents
verbs in context by their first order vectors that
specify how often the verb co-occurs with its argu-
ments in the parsed Gigaword corpus. We compare
our model to this baseline to demonstrate the bene-
fit of (contextualized) second-order vectors. As for
the full model, we use pmi values rather than raw
frequency counts as co-occurrence statistics.
Results. For the LST/SO dataset, the generalized
average precision, averaged over all instances in the
dataset, is 45.94%, and the average P10 is 73.11%.
Table 2 compares our model to the random base-
line, the two variants of our model, and previous
work. As can be seen, our model improves about
8% in terms of GAP and almost 7% in terms of
P10 upon the two variants of our model, which in
turn perform 10% above the random baseline. We
conclude that both the use of second-order vectors,
as well as the method used to contextualize them,
are very effective for the task under consideration.
The table also compares our model to the model
of TDP and two different instantiations of E&P?s
model. The results for these three models are cited
from Thater et al (2009). We can observe that
our model improves about 9% in terms of GAP
and about 7% in terms of P10 upon previous work.
Note that the results for the E&P models are based
3Note that the context information is the same for both
words. With our choice of pointwise multiplication for the
composition operator ? we have (~v1?~w) ?~v2 =~v1 ? (~v2?~w).
Therefore the choice of which word is contextualized does not
strongly influence their cosine similarity, and contextualizing
both should not add any useful information. On the contrary
we found that it even lowers performance. Although this
could be repaired by appropriately modifying the operator ?,
for this experiment we stick with the easier solution of only
contextualizing one of the words.
953
Model GAP P10
Random baseline 26.03 54.25
E&P (add, object) 29.93 66.20
E&P (min, subject & object) 32.22 64.86
TDP 36.54 63.32
1st order contextualized 36.09 59.35
2nd order uncontextualized 37.65 66.32
Full model 45.94 73.11
Table 2: Results of Experiment 1
on a reimplementation of E&P?s original model?
the P10-scores reported by Erk and Pad? (2009)
range between 60.2 and 62.3, over a slightly lower
random baseline.
According to a paired t-test the differences are
statistically significant at p < 0.01.
Performance on the complete dataset. To find
out how our model performs on less controlled
datasets, we extracted all instances from the lexical
substitution task dataset with a verb target, exclud-
ing only instances which could not be parsed by
the Stanford parser, or in which the target was mis-
tagged as a non-verb by the parser. The resulting
dataset contains 496 instances. As for the LST/SO
dataset, we ignore all gold-standard paraphrases
that do not occur in the parsed (Gigaword) corpus.
If we use the best-performing parameters from
the first experiment, we obtain a GAP score of
45.17% and a P10-score of 75.43%, compared to
random baselines of 27.42% (GAP) and 58.83%
(P10). The performance on this larger dataset is
thus almost the same compared to our results for
the more controlled dataset. We take this as evi-
dence that our model is quite robust w.r.t. different
realizations of a verb?s subcategorization frame.
4.4 Experiment 2: Non-verb paraphrases
We now apply our model to parts of speech (POS)
other than verbs. The main difference between
verbs on the one hand, and nouns, adjectives, and
adverbs on the other hand, is that verbs typically
come with a rich context?subject, object, and so
on?while non-verbs often have either no depen-
dents at all or only closed class dependents such as
determiners which provide only limited contextual
informations, if any at all. While we can apply the
same method as before also to non-verbs, we might
expect it to work less well due to limited contextual
POS Instances M1 M2 Baseline
Noun 535 46.38 42.54 30.01
Adj 508 39.41 43.21 28.32
Adv 284 48.19 51.43 37.25
Table 3: GAP-scores for non-verb paraphrases us-
ing two different methods.
information.
We therefore propose an alternative method to
rank non-verb paraphrases: We take the second-
order vector of the target?s head and contextually
constrain it by the first order vector of the target.
For instance, if we want to rank the paraphrase
candidates hint and star for the noun lead in the
sentence
(1) Meet for coffee early, swap leads and get per-
mission to contact if possible.
we compute [[swapOBJ:lead]] and compare it to the
lifted first-order vectors of all paraphrase candi-
dates, LOBJ([hint]) and LOBJ([star]), using cosine
similarity.
To evaluate the performance of the two methods,
we extract all instances from the lexical substitution
task dataset with a nominal, adjectival, or adverbial
target, excluding instances with incorrect parse or
no parse at all. As before, we ignore gold-standard
paraphrases that do not occur in the parsed Giga-
word corpus.
The results are shown in Table 3, where ?M1?
refers to the method we used before on verbs, and
?M2? refers to the alternative method described
above. As one can see, M1 achieves better results
than M2 if applied to nouns, while M2 is better
than M1 if applied to adjectives and adverbs. The
second result is unsurprising, as adjectives and ad-
verbs often have no dependents at all.
We can observe that the performance of our
model is similarly strong on non-verbs. GAP scores
on nouns (using M1) and adverbs are even higher
than those on verbs. We take these results to show
that our model can be successfully applied to all
open word classes.
5 Experiment: Ranking Word Senses
In this section, we apply our model to a different
word sense ranking task: Given a word w in context,
the task is to decide to what extent the different
954
WordNet (Fellbaum, 1998) senses of w apply to
this occurrence of w.
Dataset. We use the dataset provided by Erk and
McCarthy (2009). The dataset contains ordinal
judgments of the applicability of WordNet senses
on a 5 point scale, ranging from completely differ-
ent to identical for eight different lemmas in 50
different sentential contexts. In this experiment,
we concentrate on the three verbs in the dataset:
ask, add and win.
Experimental procedure. Similar to Pennac-
chiotti et al (2008), we represent different word
senses by the words in the corresponding synsets.
For each word sense, we compute the centroid of
the second-order vectors of its synset members.
Since synsets tend to be small (they even may con-
tain only the target word itself), we additionally
add the centroid of the sense?s hypernyms, scaled
down by the factor 10 (chosen as a rough heuristic
without any attempt at optimization).
We apply the same method as in Section 4.3:
For each instance in the dataset, we compute the
second-order vector of the target verb, contextually
constrain it by the first-order vectors of the verb?s
arguments, and compare the resulting vector to
the vectors that represent the different WordNet
senses of the verb. The WordNet senses are then
ranked according to the cosine similarity between
their sense vector and the contextually constrained
target verb vector.
To compare the predicted ranking to the gold-
standard ranking, we use Spearman?s ? , a standard
method to compare ranked lists to each other. We
compute ? between the similarity scores averaged
over all three annotators and our model?s predic-
tions. Based on agreement between human judges,
Erk and McCarthy (2009) estimate an upper bound
? of 0.544 for the dataset.
Results. Table 4 shows the results of our exper-
iment. The first column shows the correlation of
our model?s predictions with the human judgments
from the gold-standard, averaged over all instances.
All correlations are significant (p< 0.001) as tested
by approximate randomization (Noreen, 1989).
The second column shows the results of a
frequency-informed baseline, which predicts the
ranking based on the order of the senses in Word-
Net. This (weakly supervised) baseline outper-
forms our unsupervised model for two of the three
verbs. As a final step, we explored the effect of
Word Present paper WN-Freq Combined
ask 0.344 0.369 0.431
add 0.256 0.164 0.270
win 0.236 0.343 0.381
average 0.279 0.291 0.361
Table 4: Correlation of model predictions and hu-
man judgments
combining our rankings with those of the frequency
baseline, by simply computing the average ranks
of those two models. The results are shown in the
third column. Performance is significantly higher
than for both the original model and the frequency-
informed baseline. This shows that our model cap-
tures an additional kind of information, and thus
can be used to improve the frequency-based model.
6 Conclusion
We have presented a novel method for adapting
the vector representations of words according to
their context. In contrast to earlier approaches, our
model incorporates detailed syntactic information.
We solved the problems of data sparseness and
incompatibility of dimensions which are inherent in
this approach by modeling contextualization as an
interplay between first- and second-order vectors.
Evaluating on the SemEval 2007 lexical substitu-
tion task dataset, our model performs substantially
better than all earlier approaches, exceeding the
state of the art by around 9% in terms of general-
ized average precision and around 7% in terms of
precision out of ten. Also, our system is the first un-
supervised method that has been applied to Erk and
McCarthy?s (2009) graded word sense assignment
task, showing a substantial positive correlation with
the gold standard. We further showed that a weakly
supervised heuristic, making use of WordNet sense
ranks, can be significantly improved by incorporat-
ing information from our system.
We studied the effect that context has on target
words in a series of experiments, which vary the
target word and keep the context constant. A natu-
ral objective for further research is the influence of
varying contexts on the meaning of target expres-
sions. This extension might also shed light on the
status of the modelled semantic process, which we
have been referring to in this paper as ?contextu-
alization?. This process can be considered one of
955
mutual disambiguation, which is basically the view
of E&P. Alternatively, one can conceptualize it as
semantic composition: in particular, the head of a
phrase incorporates semantic information from its
dependents, and the final result may to some extent
reflect the meaning of the whole phrase.
Another direction for further study will be the
generalization of our model to larger syntactic con-
texts, including more than only the direct neighbors
in the dependency graph, ultimately incorporating
context information from the whole sentence in a
recursive fashion.
Acknowledgments. We would like to thank Ed-
uard Hovy and Georgiana Dinu for inspiring discus-
sions and helpful comments. This work was sup-
ported by the Cluster of Excellence ?Multimodal
Computing and Interaction?, funded by the Ger-
man Excellence Initiative, and the project SALSA,
funded by DFG (German Science Foundation).
References
Chris Buckley and Ellen M. Voorhees. 2000. Evaluat-
ing evaluation measure stability. In Proceedings of
the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 33?40, Athens, Greece.
Kenneth W. Church and Patrick Hanks. 1990. Word
association, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the fifth international conference on
Language Resources and Evaluation (LREC 2006),
pages 449?454, Genoa, Italy.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
Proceedings of ACL-08: HLT, Short Papers, pages
29?32, Columbus, OH, USA.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 440?449, Singapore.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Honolulu,
HI, USA.
Katrin Erk and Sebastian Pad?. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proc. of the Workshop
on Geometrical Models of Natural Language Seman-
tics, Athens, Greece.
Christiane Fellbaum, editor. 1998. Wordnet: An Elec-
tronic Lexical Database. Bradford Book.
Walter Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1993. Principle-based parsing without
overgeneration. In Proceedings of the 31st Annual
Meeting of the Association for Computational Lin-
guistics, pages 112?120, Columbus, OH, USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Volume 2, pages 768?774.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. In
Proc. of SemEval, Prague, Czech Republic.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, OH,
USA.
Richard Montague. 1973. The proper treatment of
quantification in ordinary English. In Jaakko Hin-
tikka, Julius Moravcsik, and Patrick Suppes, editors,
Approaches to Natural Language, pages 221?242.
Dordrecht.
Eric W. Noreen. 1989. Computer-intensive Methods
for Testing Hypotheses: An Introduction. John Wi-
ley and Sons Inc.
Sebastian Pad? and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of framenet lexical units. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 457?465, Hon-
olulu, HI, USA.
956
Hinrich Sch?tze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97?124.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44?47, Singapore.
957
Transactions of the Association for Computational Linguistics, 1 (2013) 25?36. Action Editor: Hal Daume? III.
Submitted 10/2012; Published 3/2013. c?2013 Association for Computational Linguistics.
Grounding Action Descriptions in Videos
Michaela Regneri ?, Marcus Rohrbach , Dominikus Wetzel ?,
Stefan Thater ?, Bernt Schiele  and Manfred Pinkal ?
? Department of Computational Linguistics, Saarland University, Saarbru?cken, Germany
(regneri|dwetzel|stth|pinkal)@coli.uni-saarland.de
 Max Planck Institute for Informatics, Saarbru?cken, Germany
(rohrbach|schiele)@mpi-inf.mpg.de
Abstract
Recent work has shown that the integration of
visual information into text-based models can
substantially improve model predictions, but
so far only visual information extracted from
static images has been used. In this paper, we
consider the problem of grounding sentences
describing actions in visual information ex-
tracted from videos. We present a general
purpose corpus that aligns high quality videos
with multiple natural language descriptions of
the actions portrayed in the videos, together
with an annotation of how similar the action
descriptions are to each other. Experimental
results demonstrate that a text-based model of
similarity between actions improves substan-
tially when combined with visual information
from videos depicting the described actions.
1 Introduction
The estimation of semantic similarity between
words and phrases is a basic task in computational
semantics. Vector-space models of meaning are one
standard approach. Following the distributional hy-
pothesis, frequencies of context words are recorded
in vectors, and semantic similarity is computed as a
proximity measure in the underlying vector space.
Such distributional models are attractive because
they are conceptually simple, easy to implement and
relevant for various NLP tasks (Turney and Pan-
tel, 2010). At the same time, they provide a sub-
stantially incomplete picture of word meaning, since
they ignore the relation between language and extra-
linguistic information, which is constitutive for lin-
guistic meaning. In the last few years, a growing
amount of work has been devoted to the task of
grounding meaning in visual information, in par-
ticular by extending the distributional approach to
jointly cover texts and images (Feng and Lapata,
2010; Bruni et al, 2011). As a clear result, visual
information improves the quality of distributional
models. Bruni et al (2011) show that visual infor-
mation drawn from images is particularly relevant
for concrete common nouns and adjectives.
A natural next step is to integrate visual infor-
mation from videos into a semantic model of event
and action verbs. Psychological studies have shown
the connection between action semantics and videos
(Glenberg, 2002; Howell et al, 2005), but to our
knowledge, we are the first to provide a suitable data
source and to implement such a model.
The contribution of this paper is three-fold:
? We present a multimodal corpus containing
textual descriptions aligned with high-quality
videos. Starting from the video corpus of
Rohrbach et al (2012b), which contains high-
resolution video recordings of basic cooking
tasks, we collected multiple textual descrip-
tions of each video via Mechanical Turk. We
also provide an accurate sentence-level align-
ment of the descriptions with their respective
videos. We expect the corpus to be a valu-
able resource for computational semantics, and
moreover helpful for a variety of purposes, in-
cluding video understanding and generation of
text from videos.
? We provide a gold-standard dataset for the
evaluation of similarity models for action verbs
and phrases. The dataset has been designed
as analogous to the Usage Similarity dataset of
25
Erk et al (2009) and contains pairs of natural-
language action descriptions plus their associ-
ated video segments. Each of the pairs is an-
notated with a similarity score based on several
manual annotations.
? We report an experiment on similarity model-
ing of action descriptions based on the video
corpus and the gold standard annotation, which
demonstrates the impact of scene information
from videos. Visual similarity models outper-
form text-based models; the performance of
combined models approaches the upper bound
indicated by inter-annotator agreement.
The paper is structured as follows: We first place
ourselves in the landscape of related work (Sec. 2),
then we introduce our corpus (Sec. 3). Sec. 4 re-
ports our action similarity annotation experiment
and Sec. 5 introduces the similarity measures we ap-
ply to the annotated data. We outline the results of
our evaluation in Sec. 6, and conclude the paper with
a summary and directions for future work (Sec. 7).
2 Related Work
A large multimodal resource combining language
and visual information resulted from the ESP game
(von Ahn and Dabbish, 2004). The dataset contains
many images tagged with several one-word labels.
The Microsoft Video Description Corpus (Chen
and Dolan, 2011, MSVD) is a resource providing
textual descriptions of videos. It consists of multiple
crowd-sourced textual descriptions of short video
snippets. The MSVD corpus is much larger than our
corpus, but most of the videos are of relatively low
quality and therefore too challenging for state-of-
the-art video processing to extract relevant informa-
tion. The videos are typically short and summarized
with a single sentence. Our corpus contains coher-
ent textual descriptions of longer video sequences,
where each sentence is associated with a timeframe.
Gupta et al (2009) present another useful re-
source: their model learns the alignment of
predicate-argument structures with videos and uses
the result for action recognition in videos. However,
the corpus contains no natural language texts.
The connection between natural language sen-
tences and videos has so far been mostly explored
by the computer vision community, where dif-
ferent methods for improving action recognition
by exploiting linguistic data have been proposed
(Gupta and Mooney, 2010; Motwani and Mooney,
2012; Cour et al, 2008; Tzoukermann et al, 2011;
Rohrbach et al, 2012b, among others). Our resource
is intended to be used for action recognition as well,
but in this paper, we focus on the inverse effect of
visual data on language processing.
Feng and Lapata (2010) were the first to enrich
topic models for newspaper articles with visual in-
formation, by incorporating features from article il-
lustrations. They achieve better results when in-
corporating the visual information, providing an en-
riched model that pairs a single text with a picture.
Bruni et al (2011) used the ESP game data to cre-
ate a visually grounded semantic model. Their re-
sults outperform purely text-based models using vi-
sual information from pictures for the task of mod-
eling noun similarities. They model single words,
and mostly visual features lead only to moderate im-
provements, which might be due to the mixed qual-
ity and random choice of the images. Dodge et al
(2012) recently investigated which words can actu-
ally be grounded in images at all, producing an au-
tomatic classifier for visual words.
An interesting in-depth study by Mathe et al
(2008) automatically learnt the semantics of motion
verbs as abstract features from videos. The study
captures 4 actions with 8-10 videos for each of the
actions, and would need a perfect object recognition
from a visual classifier to scale up.
Steyvers (2010) and later Silberer and Lapata
(2012) present an alternative approach to incorpo-
rating visual information directly: they use so-called
feature norms, which consist of human associations
for many given words, as a proxy for general percep-
tual information. Because this model is trained and
evaluated on those feature norms, it is not directly
comparable to our approach.
The Restaurant Game by Orkin and Roy (2009)
grounds written chat dialogues in actions carried out
in a computer game. While this work is outstanding
from the social learning perspective, the actions that
ground the dialogues are clicks on a screen rather
than real-world actions. The dataset has successfully
been used to model determiner meaning (Reckman
et al, 2011) in the context of the Restaurant Game,
26
but it is unclear how this approach could scale up to
content words and other domains.
3 The TACOS Corpus
We build our corpus on top of the ?MPII Cook-
ing Composite Activities? video corpus (Rohrbach
et al, 2012b, MPII Composites), which contains
videos of different activities in the cooking domain,
e.g., preparing carrots or separating eggs. We ex-
tend the existing corpus with multiple textual de-
scriptions collected by crowd-sourcing via Amazon
Mechanical Turk1 (MTurk). To facilitate the align-
ment of sentences describing activities with their
proper video segments, we also obtained approxi-
mate timestamps, as described in Sec. 3.2.
MPII Composites comes with timed gold-
standard annotation of low-level activities and par-
ticipating objects (e.g. OPEN [HAND,DRAWER] or
TAKE OUT [HAND,KNIFE,DRAWER]). By adding
textual descriptions (e.g., The person takes a knife
from the drawer) and aligning them on the sentence
level with videos and low-level annotations, we pro-
vide a rich multimodal resource (cf. Fig. 2), the
?Saarbru?cken Corpus of Textually Annotated Cook-
ing Scenes? (TACOS). In particular, the TACOS cor-
pus provides:
? A collection of coherent textual descrip-
tions for video recordings of activities of
medium complexity, as as a basis for empiri-
cal discourse-related research, e.g., the selec-
tion and granularity of action descriptions in
context
? A high-quality alignment of sentences with
video segments, supporting the grounding of
action descriptions in visual information
? Collections of paraphrases describing the same
scene, which result as a by-product from the
text-video alignment and can be useful for text
generation from videos (among other things)
? The alignment of textual activity descriptions
with sequences of low-level activities, which
may be used to study the decomposition of ac-
tion verbs into basic activity predicates
1mturk.com
We expect that our corpus will encourage and en-
able future work on various topics in natural lan-
guage and video processing. In this paper, we will
make use of the second aspect only, demonstrating
the usefulness of the corpus for the grounding task.
After a more detailed description of the basic
video corpus and its annotation (Sec. 3.1) we de-
scribe the collection of textual descriptions with
MTurk (Sec. 3.2), and finally show the assembly and
some benchmarks of the final corpus (Sec. 3.3).
3.1 The video corpus
MPII Composites contains 212 high resolution video
recordings of 1-23 minutes length (4.5 min. on av-
erage). 41 basic cooking tasks such as cutting a cu-
cumber were recorded, each between 4 and 8 times.
The selection of cooking tasks is based on those pro-
posed at ?Jamie?s Home Cooking Skills?.2 The cor-
pus is recorded in a kitchen environment with a total
of 22 subjects. Each video depicts a single task exe-
cuted by an individual subject.
The dataset contains expert annotations of low-
level activity tags. Annotations are provided for seg-
ments containing a semantically meaningful cook-
ing related movement pattern. The action must go
beyond single body part movements (such as move
arm up) and must have the goal of changing the state
or location of an object. 60 different activity labels
are used for annotation (e.g. PEEL, STIR, TRASH).
Each low-level activity tag consists of an activity
label (PEEL), a set of associated objects (CARROT,
DRAWER,...), and the associated timeframe (start-
ing and ending points of the activity). Associated
objects are the participants of an activity, namely
tools (e.g. KNIFE), patient (CARROT) and location
(CUTTING-BOARD). We provide the coarse-grained
role information for patient, location and tool in the
corpus data, but we did not use this information in
our experiments. The dataset contains a total of
8818 annotated segments, on average 42 per video.
3.2 Collecting textual video descriptions
We collected textual descriptions for a subset of the
videos in MPII Composites, restricting collection to
tasks that involve manipulation of cooking ingredi-
ents. We also excluded tasks with fewer than four
2www.jamieshomecookingskills.com
27
video recordings in the corpus, leaving 26 tasks to be
described. We randomly selected five videos from
each task, except the three tasks for which only four
videos are available. This resulted in a total of 127
videos. For each video, we collected 20 different
textual descriptions, leading to 2540 annotation as-
signments. We published these assignments (HITs)
on MTurk, using an adapted version3 of the annota-
tion tool Vatic (Vondrick et al, 2012).
In each assignment, the subject saw one video
specified with the task title (e.g. How to prepare an
onion), and then was asked to enter at least five and
at most 15 complete English sentences to describe
the events in the video. The annotation instructions
contained example annotations from a kitchen task
not contained in our actual dataset.
Annotators were encouraged to watch each video
several times, skipping backward and forward as
they wished. They were also asked to take notes
while watching, and to sketch the annotation before
entering it. Once familiarized with the video, sub-
jects did the final annotation by watching the entire
video from beginning to end, without the possibil-
ity of further non-sequential viewing. Subjects were
asked to enter each sentence as soon as the action de-
scribed by the sentence was completed. The video
playback paused automatically at the beginning of
the sentence input. We recorded pause onset for
each sentence annotation as an approximate ending
timestamp of the described action. The annotators
resumed the video manually.
The tasks required a HIT approval rate of 75%
and were open only to workers in the US, in order
to increase the general language quality of the En-
glish annotations. Each task paid 1.20 USD. Before
paying we randomly inspected the annotations and
manually checked for quality. The total costs of col-
lecting the annotations amounted to 3,353 USD. The
data was obtained within a time frame of 3.5 weeks.
3.3 Putting the TACOS corpus together
Our corpus is a combination of the MTurk data and
MPII Composites, created by filtering out inappro-
priate material and computing a high-quality align-
ment of sentences and video segments. The align-
ment is done by matching the approximate times-
3github.com/marcovzla/vatic/tree/bolt
l1
l2
l3
l4
s1
l5
s3
s2
s1
s3
s2
e
l
e
m
e
n
t
a
r
y
 
t
i
m
e
f
r
a
m
e
s
s
e
n
t
e
n
c
e
s
l1
l2
l3
l4
l5
Figure 1: Aligning action descriptions with the video.
tamps of the MTurk data to the accurate timestamps
in MPII Composites.
We discarded text instances if people did not time
the sentences properly, taking the association of sev-
eral (or even all) sentences to a single timestamp as
an indicator. Whenever we found a timestamp asso-
ciated with two or more sentences, we discarded the
whole instance. Overall, we had to filter out 13%
of the text instances, which left us with 2206 textual
video descriptions.
For the alignment of sentence annotations and
video segments, we assign a precise timeframe to
each sentence in the following way: We take the
timeframes given by the low-level annotation in
MPII Composites as a gold standard micro-event
segmentation of the video, because they mark all
distinct frames that contain activities of interest. We
call them elementary frames. The sequence of el-
ementary frames is not necessarily continuous, be-
cause idle time is not annotated.
The MTurk sentences have end points that con-
stitute a coarse-grained, noisy video segmentation,
assuming that each sentence spans the time between
the end of the previous sentence and its own end-
ing point. We refine those noisy timeframes to gold
frames as shown in Fig. 1: Each elementary frame
(l1-l5) is mapped to a sentence (s1-s3) if its noisy
timeframe covers at least half of the elementary
frame. We define the final gold sentence frame then
as the timespan between the starting point of the first
and the ending point of the last elementary frame.
The alignment of descriptions with low-level ac-
tivities results in a table as given in Fig. 3. Columns
contain the textual descriptions of the videos; rows
28
Top 10
Verbs
cut, take, get, put, wash, place,
rinse, remove, *pan, peel
Top 10
Activities
move, take out, cut, wash, take
apart, add, shake, screw, put in, peel
Figure 4: 10 most frequent verbs and low-level actions in
the TACOS corpus. pan is probably often mis-tagged.
correspond to low-level actions, and each sentence
is aligned with the last of its associated low-level ac-
tions. As a side effect, we also obtain multiple para-
phrases for each sentence, by considering all sen-
tences with the same associated time frame as equiv-
alent realizations of the same action.
The corpus contains 17,334 action descrip-
tions (tokens), realizing 11,796 different sentences
(types). It consists of 146,771 words (tokens),
75,210 of which are content word instances (i.e.
nouns, verbs and adjectives). The verb vocabulary
comprises 28,292 verb tokens, realizing 435 lem-
mas. Since verbs occurring in the corpus typically
describe actions, we can note that the linguistic vari-
ance for the 58 different low-level activities is quite
large. Fig. 4 gives an impression of the action re-
alizations in the corpus, listing the most frequent
verbs from the textual data, and the most frequent
low-level activities.
On average, each description covers 2.7 low-level
activities, which indicates a clear difference in gran-
ularity. 38% of the descriptions correspond to ex-
actly one low-level activity, about a quarter (23%)
covers two of them; 16% have 5 or more low-level
elements, 2% more than 10. The corpus shows how
humans vary the granularity of their descriptions,
measured in time or number of low-level activities,
and it shows how they vary the linguistic realization
of the same action. For example, Fig. 3 contains dice
and chop into small pieces as alternative realizations
of the low-level activity sequence SLICE - SCRATCH
OFF - SLICE.
The descriptions are of varying length (9 words
on average), reaching from two-word phrases to de-
tailed descriptions of 65 words. Most sentences are
short, consisting of a reference to the person in the
video, a participant and an action verb (The person
rinses the carrot, He cuts off the two edges). People
often specified an instrument (from the faucet), or
the resulting state of the action (chop the carrots in
small pieces). Occasionally, we find more complex
constructions (support verbs, coordinations).
As Fig. 3 indicates, the timestamp-based align-
ment is pretty accurate; occasional errors occur like
He starts chopping the carrot... in NL Sequence 3.
The data contains some typos and ungrammatical
sentences (He washed carrot), but for our own ex-
periments, the small number of such errors did not
lead to any processing problems.
4 The Action Similarity Dataset
In this section, we present a gold standard dataset,
as a basis for the evaluation of visually grounded
models of action similarity. We call it the ?Action
Similarity Dataset? (ASim) in analogy to the Usage
Similarity dataset (USim) of Erk et al (2009) and
Erk et al (2012). Similarly to USim, ASim con-
tains a collection of sentence pairs with numerical
similarity scores assigned by human annotators. We
asked the annotators to focus on the similarity of the
activities described rather than on assessing seman-
tic similarity in general. We use sentences from the
TACOS corpus and record their timestamps. Thus
each sentence comes with the video segment which
it describes (these were not shown to the annotators).
4.1 Selecting action description pairs
Random selection of annotated sentences from the
corpus would lead to a large majority of pairs which
are completely dissimilar, or difficult to grade (e.g.,
He opens the drawer ? The person cuts off the ends
of the carrot). We constrained the selection pro-
cess in two ways: First, we consider only sentences
describing activities of manipulating an ingredient.
The low-level annotation of the video corpus helps
us identify candidate descriptions. We exclude rare
and special activities, ending up with CUT, SLICE,
CHOP, PEEL, TAKE APART, and WASH, which oc-
cur reasonably frequently, with a wide distribution
over different scenarios. We restrict the candidate
set to those sentences whose timespan includes one
of these activities. This results in a conceptually
more focussed repertoire of descriptions, and at the
same time admits full linguistic variation (wash an
apple under the faucet ? rinse an apple, slice the
cucumber ? cut the cucumber into slices).
29
 896 -1137 wash      [hand,carrot]
1145 -1212 shake     [hand,carrot]
1330 -1388 close     [hand,drawer]
1431 -1647 take out  [hand,knife,drawer]
1647 -1669 move      [hand,cutting board,counter]
1673 -1705 move      [hand,carrot,bowl,cutting board]
1736 -1818 cut       [knife,carrot,cutting board]
1919 -3395 slice     [knife,carrot,cutting board]
>  890: The man takes out a cutting board.
> 1300: He washes a carrot.
> 1500: He takes out a knife.
> 4000: He slices the carrot.
Videos of basic kitchen tasks
Low level annotations with timestamps, actions and objects
Natural language descriptions 
with ending times of the actions
manual low-level annotation
Mechanical Turk data collection
timestamp-based alignment
Figure 2: Corpus Overview
Sample frame Start End Action Participants NL Sequence 1 NL Sequence 2 NL Sequence 3
743 911 wash hand, carrot He washed carrot The person rinses the
carrot.
He rinses the carrot from
the faucet.
982 1090 cut knife, carrot,
cutting board
He cut off ends of
carrots
The person cuts off
the ends of the carrot.
He cuts off the two edges.
1164 1257 open hand, drawer
1679 1718 close hand, drawer He searches for some-
thing in the drawer, failed
attempt, he throws away
the edges in trash.
1746 1799 trash hand, carrot The person searches
for the trash can, then
throws the ends of
the carrot away.
1854 2011 wash hand, carrot He rinses the carrot again.
2011 2045 shake hand, carrot He washed carrot The person rinses the
carrot again.
He starts chopping the
carrot in small pieces.
2083 2924 slice knife, carrot,
cutting board
2924 2959 scratch
off
hand, carrot,
knife, cutting
board
3000 3696 slice knife, carrot,
cutting board
He diced carrots He finished chopping the
carrots in small pieces.
Figure 3: Excerpt from the corpus for a video on PREPARING A CARROT. Example frames, low-level annotation
(Action and Participants) is shown along with three of the MTurk sequences (NL Sequence 1-3).
30
Second, we required the pairs to share some lexi-
cal material, either the head verb or the manipulated
ingredient (or both).4 More precisely, we composed
the ASim dataset from three different subsets:
Different activity, same object: This subset con-
tains pairs describing different types of actions car-
ried out on the same type of object (e.g. The man
washes the carrot. ? She dices the carrot.). Its fo-
cus is on the central task of modeling the semantic
relation between actions (rather than the objects in-
volved in the activity), since the object head nouns
in the descriptions are the same, and the respective
video segments show the same type of object.
Same activity, same object: Description pairs of
this subset will in many cases, but not always, agree
in their head verbs. The dataset is useful for explor-
ing the degree to which action descriptions are un-
derspecified with respect to the precise manner of
their practical realization. For example, peeling an
onion will mostly be done in a rather uniform way,
while cut applied to carrot can mean that the carrot
is chopped up, or sliced, or cut in halves.
Same activity & verb, different object: Descrip-
tion pairs in this subset share head verb and low-
level activity, but have different objects (e.g. The
man washes the carrot. ? A girl washes an apple un-
der the faucet.). This dataset enables the exploration
of the objects? meaning contribution to the complete
action, established by the variation of equivalent ac-
tions that are done to different objects.
We assembled 900 action description pairs for anno-
tation: 480 pairs share the object; 240 of which have
different activities, and the other 240 pairs share the
same activity. We included paraphrases describing
the same video segment, but we excluded pairs of
identical sentences. 420 additional pairs share their
head verb, but have different objects.
4.2 Manual annotation
Three native speakers of English were asked to judge
the similarity of the action pairs with respect to how
4We refer to the latter with the term object; we don?t require
the ingredient term to be the actual grammatical object in the
action descriptions, we rather use ?object? in its semantic role
sense as the entity affected by an action.
Part of Gold Standard Sim ? ?
DIFF. ACTIVITY, SAME OBJECT 2.20 1.07 0.73
SAME ACTIVITY, SAME OBJECT 4.19 1.04 0.73
ALL WITH SAME OBJECT 3.20 1.44 0.84
SAME VERB, DIFF. OBJECT 3.34 0.69 0.43
COMPLETE DATASET 3.27 1.15 0.73
Figure 5: Average similarity ratings (Sim), their standard
deviation (?)) and annotator agreement (?) for ASim.
they are carried out, rating each sentence pair with
a score from 1 (not similar at all) to 5 (the same or
nearly the same). They did not see the respective
videos, but we noted the relevant kitchen task (i.e.
which vegetable was prepared). We asked the an-
notators explicitly to ignore the actor of the action
(e.g. whether it is a man or a woman) and score
the similarities of the underlying actions rather than
their verbalizations. Each subject rated all 900 pairs,
which were shown to them in completely random or-
der, with a different order for each subject.
We compute inter-annotator agreement (and the
forthcoming evaluation scores) using Spearman?s
rank correlation coefficient (?), a non-parametric
test which is widely used for similar evaluation tasks
(Mitchell and Lapata, 2008; Bruni et al, 2011; Erk
and McCarthy, 2009). Spearman?s ? evaluates how
the samples are ranked relative to each other rather
than the numerical distance between the rankings.
Fig. 5 shows the average similarity ratings in the
different settings and the inter-annotator agreement.
The average inter-rater agreement was ? = 0.73 (av-
eraged over pairwise rater agreements), with pair-
wise results of ? = 0.77, 0.72, and 0.69, respec-
tively, which are all highly significant at p < 0.001.
As expected, pairs with the same activity and ob-
ject are rated very similar (4.19) on average, while
the similarity of different activities on the same ob-
ject is the lowest (2.2). For both subsets, inter-rater
agreement is high (? = 0.73), and even higher for
both SAME OBJECT subsets together (0.84).
Pairs with identical head verbs and different ob-
jects have a small standard deviation, at 0.69. The
inter-annotator agreement on this set is much lower
than for pairs from the SAME OBJECT set. This indi-
cates that similarity assessment for different variants
of the same activity is a hard task even for humans.
31
5 Models of Action Similarity
In the following, we demonstrate that visual infor-
mation contained in videos of the kind provided by
the TACOS corpus (Sec. 3) substantially contributes
to the semantic modeling of action-denoting expres-
sions. In Sec. 6, we evaluate several methods for
predicting action similarity on the task provided by
the ASim dataset. In this section, we describe the
models considered in the evaluation. We use two
different models based on visual information, and in
addition two text based models. We will also explore
the effect of combining linguistic and visual infor-
mation and investigate which mode is most suitable
for which kinds of similarity.
5.1 Text-based models
We use two different models of textual similarity
to predict action similarity: a simple word-overlap
measure (Jaccard coefficient) and a state-of-the-art
model based on ?contextualized? vector representa-
tions of word meaning (Thater et al, 2011).
Jaccard coefficient. The Jaccard coefficient gives
the ratio between the number of (distinct) words
common to two input sentences and the total num-
ber of (distinct) words in the two sentences. Such
simple surface-oriented measures of textual similar-
ity are often used as baselines in related tasks such as
recognizing textual entailment (Dagan et al, 2005)
and are known to deliver relatively strong results.
Vector model. We use the vector model of Thater
et al (2011), which ?contextualizes? vector repre-
sentations for individual words based on the particu-
lar sentence context in which the target word occurs.
The basic intuition behind this approach is that the
words in the syntactic context of the target word in a
given input sentence can be used to refine or disam-
biguate its vector. Intuitively, this allows us to dis-
criminate between different actions that a verb can
refer to, based on the different objects of the action.
We first experimented with a version of this vec-
tor model which predicts action similarity scores of
two input sentences by computing the cosine simi-
larity of the contextualized vectors of the verbs in the
two sentences only. We achieved better performance
with a variant of this model which computes vectors
for the two sentences by summing over the contex-
tualized vectors of all constituent content words.
In the experiments reported below, we only use
the second variant. We use the same experimental
setup as Thater et al (2011), as well as the parameter
settings that are reported to work best in that paper.
5.2 Video-based models
We distinguish two approaches to compute the sim-
ilarity between two video segments. In the first, un-
supervised approach we extract a video descriptor
and compute similarities between these raw features
(Wang et al, 2011). The second approach builds
upon the first by additionally learning higher level
attribute classifiers (Rohrbach et al, 2012b) on a
held out training set. The similarity between two
segments is then computed between the classifier re-
sponses. In the following we detail both approaches:
Raw visual features. We use the state-of-the-art
video descriptor Dense Trajectories (Wang et al,
2011) which extracts visual video features, namely
histograms of oriented gradients, flow, and motion
boundary histograms, around densely sampled and
tracked points.
This approach is especially suited for this data as
it ignores non-moving parts in the video: we are
interested in activities and manipulation of objects,
and this type of feature implicitly uses only infor-
mation in relevant image locations. For our setting
this feature representation has been shown to be su-
perior to human pose-based approaches (Rohrbach
et al, 2012a). Using a bag-of-words representation
we encode the features using a 16,000 dimensional
codebook. Features and codebook are provided with
the publicly available video dataset.
We compute the similarity between two encoded
features by computing the intersection of the two
(normalized) histograms.
Visual classifiers. Visual raw features tend to have
several dimensions in the feature space which pro-
vide unreliable, noisy values and thus degrade the
strength of the similarity measure. Intermediate
level attribute classifiers can learn which feature di-
mensions are distinctive and thus significantly im-
prove performance over raw features. Rohrbach et
al. (2012b) showed that using such an attribute clas-
sifier representation can significantly improve per-
32
MODEL SAME OBJECT SAME VERB OVERALL
TE
XT
JACCARD 0.28 0.25 0.25
TEXTUAL VECTORS 0.30 0.25 0.27
TEXT COMBINED 0.39 0.35 0.36
VI
DE
O VISUAL RAW VECTORS 0.53 -0.08 0.35
VISUAL CLASSIFIER 0.60 0.03 0.44
VIDEO COMBINED 0.61 -0.04 0.44
M
IX ALL UNSUPERVISED 0.58 0.32 0.48
ALL COMBINED 0.67 0.28 0.55
UPPER BOUND 0.84 0.43 0.73
Figure 6: Evaluation results in Spearman?s ?. All values > 0.11 are significant at p < 0.001.
formance for composite activity recognition. The
relevant attributes are all activities and objects an-
notated in the video data (cf. Section 3.1). For the
experiments reported below we use the same setup
as Rohrbach et al (2012b) and use all videos in
MPII Composites and MPII Cooking (Rohrbach et
al., 2012a), excluding the 127 videos used during
evaluation. The real-valued SVM-classifier output
provides a confidence how likely a certain attribute
appeared in a given video segment. This results in a
218-dimensional vector of classifier outputs for each
video segment. To compute the similarity between
two vectors we compute the cosine between them.
6 Evaluation
We evaluate the different similarity models intro-
duced in Sec. 5 by calculating their correlation with
the gold-standard similarity annotations of ASim
(cf. Sec. 4). For all correlations, we use Spear-
man?s ? as a measure. We consider the two textual
measures (JACCARD and TEXTUAL VECTORS) and
their combination, as well as the two visual mod-
els (VISUAL RAW VECTORS and VISUAL CLAS-
SIFIER) and their combination. We also combined
textual and visual features, in two variants: The
first includes all models (ALL COMBINED), the sec-
ond only the unsupervised components, omitting the
visual classifier (ALL UNSUPERVISED). To com-
bine multiple similarity measures, we simply aver-
age their normalized scores (using z-scores).
Figure 6 shows the scores for all of these mea-
sures on the complete ASim dataset (OVERALL),
along with the two subparts, where description pairs
share either the object (SAME OBJECT) or the head
verb (SAME VERB). In addition to the model re-
sults, the table also shows the average human inter-
annotator agreement as UPPER BOUND.
On the complete set, both visual and textual mea-
sures have a highly significant correlation with the
gold standard, whereas the combination of both
clearly leads to the best performance (0.55). The
results on the SAME OBJECT and SAME VERB sub-
sets shed light on the division of labor between the
two information sources. While the textual mea-
sures show a comparable performance over the two
subsets, there is a dramatic difference in the contri-
bution of visual information: On the SAME OBJECT
set, the visual models clearly outperform the textual
ones, whereas the visual information has no positive
effect on the SAME VERB set. This is clear evidence
that the visual model does not capture the similar-
ity of the participating objects but rather genuine ac-
tion similarity, which the visual features (Wang et
al., 2011) we employ were designed for. A direction
for future work is to learn dedicated visual object de-
tectors to recognize and capture similarities between
objects more precisely.
The numbers shown in Figure 7 support this hy-
pothesis, showing the two groups in the SAME OB-
JECT class: For sentence pairs that share the same
activity, the textual models seem to be much more
suitable than the visual ones. In general, visual mod-
els perform better on actions with different activity
types, textual models on closely related activities.
33
MODEL (SAME OBJECT) same action diff. action
TE
XT
JACCARD 0.44 0.14
TEXT VECTORS 0.42 0.05
TEXT COMBINED 0.52 0.14
VI
DE
O VIS. RAW VECTORS 0.21 0.23
VIS. CLASSIFIER 0.21 0.45
VIDEO COMBINED 0.26 0.38
M
IX ALL UNSUPERVISED 0.49 0.24
ALL COMBINED 0.48 0.41
UPPER BOUND 0.73 0.73
Figure 7: Results for sentences with the same object, with
either the same or different low-level activity.
Overall, the supervised classifier contributes a
good part to the final results. However, the supervi-
sion is not strictly necessary to arrive at a significant
correlation; the raw visual features alone are suffi-
cient for the main performance gain seen with the
integration of visual information.
7 Conclusion
We presented the TACOS corpus, which provides
coherent textual descriptions for high-quality video
recordings, plus accurate alignments of text and
video on the sentence level. We expect the corpus
to be beneficial for a variety of research activities in
natural-language and visual processing.
In this paper, we focused on the task of grounding
the meaning of action verbs and phrases. We de-
signed the ASim dataset as a gold standard and eval-
uated several text- and video-based semantic simi-
larity models on the dataset, both individually and
in different combinations.
We are the first to provide semantic models for
action-describing expressions, which are based on
information extracted from videos. Our experimen-
tal results show that these models are of considerable
quality, and that predictions based on a combination
of visual and textual information even approach the
upper bound given by the agreement of human an-
notators.
In this work we used existing similarity models
that had been developed for different applications.
We applied these models without any special train-
ing or optimization for the current task, and we com-
bined them in the most straightforward way. There
is room for improvement by tuning the models to
the task, or by using more sophisticated approaches
to combine modality-specific information (Silberer
and Lapata, 2012).
We built our work on an existing corpus of high-
quality video material, which is restricted to the
cooking domain. As a consequence, the corpus cov-
ers only a limited inventory of activity types and ac-
tion verbs. Note, however, that our models are fully
unsupervised (except the Visual Classifier model),
and thus can be applied without modification to ar-
bitrary domains and action verbs, given that they are
about observable activities. Also, corpora contain-
ing information comparable to the TACOS corpus but
with wider coverage (and perhaps a bit noisier) can
be obtained with a moderate amount of effort. One
needs videos of reasonable quality and some sort of
alignment with action descriptions. In some cases
such alignments even come for free, e.g. via subti-
tles, or descriptions of short video clips that depict
just a single action.
For future work, we will further investigate the
compositionality of action-describing phrases. We
also want to leverage the multimodal information
provided by the TACOS corpus for the improvement
of high-level video understanding, as well as for
generation of natural-language text from videos.
The TACOS corpus and all other data described in
this paper (videos, low-level annotation, aligned tex-
tual descriptions, the ASim-Dataset and visual fea-
tures) are publicly available. 5
Acknowledgements
We?d like to thank Asad Sayeed, Alexis Palmer and
Prashant Rao for their help with the annotations.
We?re indebted to Carl Vondrick and Marco An-
tonio Valenzuela Escrcega for their extensive sup-
port with the video annotation tool. Further we
thank Alexis Palmer and in particular three anony-
mous reviewers for their helpful comments on this
paper. ? This work was funded by the Cluster of Ex-
cellence ?Multimodal Computing and Interaction?
of the German Excellence Initiative and the DFG
project SCHI989/2-2.
5http://www.coli.uni-saarland.de/
projects/smile/page.php?id=tacos
34
References
Luis von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of
SIGCHI 2004.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of GEMS 2011.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of ACL 2011.
Timothee Cour, Chris Jordan, Eleni Miltsakaki, and Ben
Taskar. 2008. Movie/script: Alignment and parsing
of video and text transcription. In Computer Vision
? ECCV 2008, volume 5305 of Lecture Notes in Com-
puter Science, pages 158?171. Springer Berlin Heidel-
berg.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of MLCW 2005.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi,
Yejin Choi, Hal Daume? III, Alexander C. Berg, and
Tamara L. Berg. 2012. Detecting visual text. In HLT-
NAACL, pages 762?772.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of EMNLP 2009.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word usages.
In Proceedings of ACL/AFNLP 2009.
Katrin Erk, Diana McCarthy, and Nick Gaylord. 2012.
Measuring word meaning in context. CL.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings of
HLT-NAACL 2010.
A. M. Glenberg. 2002. Grounding language in action.
Psychonomic Bulletin & Review.
Sonal Gupta and Raymond J. Mooney. 2010. Us-
ing closed captions as supervision for video activ-
ity recognition. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence (AAAI-
2010), pages 1083?1088, Atlanta, GA, July.
Abhinav Gupta, Praveen Srinivasan, Jianbo Shi, and
Larry S. Davis. 2009. Understanding videos, con-
structing plots learning a visually grounded storyline
model from annotated videos. In Proceedings of
CVPR 2009.
Steve R. Howell, Damian Jankowicz, and Suzanna
Becker. 2005. A model of grounded language ac-
quisition: Sensorimotor features improve lexical and
grammatical learning. JML.
S. Mathe, A. Fazly, S. Dickinson, and S. Stevenson.
2008. Learning the abstract motion semantics of verbs
from captioned videos. pages 1?8.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL 2008.
Tanvi S. Motwani and Raymond J. Mooney. 2012. Im-
proving video activity recognition using object recog-
nition and text mining. In Proceedings of the 20th
European Conference on Artificial Intelligence (ECAI-
2012), pages 600?605, August.
Jeff Orkin and Deb Roy. 2009. Automatic learning and
generation of social behavior from collective human
gameplay. In Proceedings of AAMAS 2009.
Hilke Reckman, Jeff Orkin, and Deb Roy. 2011. Ex-
tracting aspects of determiner meaning from dialogue
in a virtual world environment. In Proceedings of CCS
2011, IWCS ?11.
Marcus Rohrbach, Sikandar Amin, Mykhaylo Andriluka,
and Bernt Schiele. 2012a. A database for fine grained
activity detection of cooking activities. In Proceedings
of CVPR 2012.
Marcus Rohrbach, Michaela Regneri, Micha Andriluka,
Sikandar Amin, Manfred Pinkal, and Bernt Schiele.
2012b. Script data for attribute-based recognition of
composite activities. In Proceedings of ECCV 2012.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings of
EMNLP-CoNLL 2012.
Mark Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234 ? 243. ?ce:title?Formal modeling of se-
mantic concepts?/ce:title?.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of IJCNLP 2011.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning. vector space models for semantics.
JAIR.
E. Tzoukermann, J. Neumann, J. Kosecka, C. Fermuller,
I. Perera, F. Ferraro, B. Sapp, R. Chaudhry, and
G. Singh. 2011. Language models for semantic ex-
traction and filtering in video action recognition. In
AAAI Workshop on Language-Action Tools for Cogni-
tive Artificial Agents.
Carl Vondrick, Donald Patterson, and Deva Ramanan.
2012. Efficiently scaling up crowdsourced video an-
notation. IJCV.
Heng Wang, Alexander Kla?ser, Cordelia Schmid, and
Cheng-Lin Liu. 2011. Action Recognition by Dense
Trajectories. In Proceedings of CVPR 2011.
35
36
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 603?607,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Saarland: Vector-based models of semantic textual similarity
Georgiana Dinu
Center of Mind/Brain Sciences
University of Trento
georgiana.dinu@unitn.it
Stefan Thater
Dept. of Computational Linguistics
Universita?t des Saarlandes
stth@coli.uni-saarland.de
Abstract
This paper describes our system for the Se-
meval 2012 Sentence Textual Similarity task.
The system is based on a combination of few
simple vector space-based methods for word
meaning similarity. Evaluation results show
that a simple combination of these unsuper-
vised data-driven methods can be quite suc-
cessful. The simple vector space components
achieve high performance on short sentences;
on longer, more complex sentences, they are
outperformed by a surprisingly competitive
word overlap baseline, but they still bring im-
provements over this baseline when incorpo-
rated into a mixture model.
1 Introduction
Vector space models are widely-used methods for
word meaning similarity which exploit the so-called
distributional hypothesis, stating that semantically
similar words tend to occur in similar contexts. Word
meaning is represented by the contexts in which a
word occurs, and similarity is computed by compar-
ing these contexts in a high-dimensional vector space
(Turney and Pantel, 2010). Distributional models of
word meaning are attractive because they are sim-
ple, have wide coverage, and can be easily acquired
at virtually no cost in an unsupervised way. Fur-
thermore, recent research has shown that, at least
to some extent, these models can be generalized to
capture similarity beyond the (isolated) word level,
either as lexical meaning modulated by context, or
as vectorial meaning representations for phrases and
sentences. In this paper we evaluate the use of some
of these models for the Semantic Textual Similarity
(STS) task, which measures the degree of semantic
equivalence between two sentences.
In recent work Mitchell and Lapata (2008) has
drawn the attention to the question of building vecto-
rial meaning representations for sentences by combin-
ing individual word vectors. They propose a family of
simple ?compositional? models that compute a vector
for a phrase or a sentence by combining vectors of
the constituent words, using different operations such
as vector addition or component-wise multiplication.
More refined models have been proposed recently by
Baroni and Zamparelli (2010) and Grefenstette and
Sadrzadeh (2011).
Thater et al (2011) and others take a slightly dif-
ferent perspective on the problem: Instead of com-
puting a vector representation for a complete phrase
or sentence, they focus on the problem of ?disam-
biguating? the vector representation of a target word
based on distributional information about the words
in the target?s context. While this approach is not
?compositional? in the sense described above, it still
captures some meaning of the complete phrase in
which a target word occurs.
In this paper, we report on the system we used in
the Semeval 2012 Sentence Textual Similarity shared
task and describe an approach that uses a combina-
tion of few simple vector-based components. We
extend the model of Thater et al (2011), which has
been shown to perform well on a closely related para-
phrase ranking task, with an additive composition op-
eration along the lines of Mitchell and Lapata (2008),
and compare it with a simple alignment-based ap-
proach which in turn uses vector-based similarity
scores. Results show that in particular the alignment-
based approach can achieve good performance on
the Microsoft Research Video Description dataset.
On the other datasets, all vector-based components
are outperformed by a surprisingly competitive word
603
overlap baseline, but they still bring improvements
over this baseline when incorporated into a mixture
model. On the test dataset, the mixture model ranks
10th and 13th on the Microsoft Research Paraphrase
and Video Description datasets, respectively, which
we take this to be a quite promising result given that
we use only few relatively simple vector based com-
ponents to compute similarity scores for sentences.
The rest of the paper is structured as follows: Sec-
tion 2 presents the individual vector-based compo-
nents used by our system. In Section 3 we present
detailed evaluation results on the training set, as well
as results for our system on the test set, while Sec-
tion 4 concludes the paper.
2 Systems for Sentence Similarity
Our system is based on four different components:
We use two different vector space models to repre-
sent word meaning?a basic bag-of-words model
and a slightly simplified variant of the contextual-
ization model of Thater et al (2011)?and two dif-
ferent methods to compute similarity scores for sen-
tences based on these two vector space models?one
?compositional? method that computes vectors for
sentences by summing over the vectors of the con-
stituent words, and one alignment-based method that
uses vector-based similarity scores for word pairs to
compute an alignment between the words in the two
sentences.
2.1 Vector Space Models
For the basic vector-space model, we assume a set
W of words, and represent the meaning of a word
w ?W by a vector in the vector space V spanned by
the set of basis vectors {~ew? | w? ?W} as follows:
vbasic(w) = ?
w??W
f (w,w?)~ew?
where f is a function that assigns a co-occurrence
value to the word pair (w,w?). In the experiments
reported below, we use pointwise mutual information
estimated on co-occurrence frequencies for words
within a 5-word window around the target word on
either side.1
1We use a 5-word window here as this setting has been shown
to give best results on a closely related task in the literature
(Mitchell and Lapata, 2008)
This basic ?bag of words? vector space model rep-
resents word meaning by summing over all contexts
in which the target word occurs. Since words are of-
ten ambiguous, this means that context words pertain-
ing to different senses of the target word are mixed
within a single vector representation, which can lead
to ?noisy? similarity scores. The vector for the noun
coach, for instance, contains context words like teach
and tell (person sense) as well as derail and crash
(vehicle sense).
To address this problem, Thater et al (2011) pro-
pose a ?contextualization? model in which the indi-
vidual components of the target word?s vector are re-
weighted, based on distributional information about
the words in the target?s context. Let us assume that
the context consist of a single word c. The vector for
a target w in context c is then defined as:
v(w,c) = ?
w??W
?(c,w?) f (w,w?)~ew?
where ? is some similarity score that quantifies to
what extent the vector dimension that corresponds
to w? is compatible with the observed context c. In
the experiments reported below, we take ? to be the
cosine similarity of c and w?; see Section 3 for details.
In the experiments reported below, we use all
words in the syntactic context of the target word to
contextualize the target:
vctx(w) = ?
c?C(w)
v(w,c)
where C(w) is the context in which w occurs, i.e. all
words related to w by a dependency relation such as
subject or object, including inverse relations.
Remark. The contextualization model presented
above is a slightly simplified version of the original
model of Thater et al (2011): it uses standard bag-of-
words vectors instead of syntax-based vectors. This
simplified version performs better on the training
dataset. Furthermore, the simplified model has been
shown to be equivalent to the models of Erk and
Pado? (2008) and Thater et al (2010) by Dinu and
Thater (2012), so the results reported below carry
over directly to these other models as well.
2.2 Vector Composition and Alignment
The two vector space models sketched above repre-
sent the meaning of words, and thus cannot be applied
604
directly to model similarity of phrases or sentences.
One obvious and straightforward way to extend these
models to the sentence level is to follow Mitchell and
Lapata (2008) and represent sentences by vectors
obtained by summing over the individual vectors of
the constituent words. These ?compositional? mod-
els can then be used to compute similarity scores
between sentence pairs in a straightforward way, sim-
ply by computing the cosine of the angle between
vectors (or some other similarity score) for the two
sentences:
simadd(S,S
?) = cos
(
?
w?S
v(w), ?
w??S?
v(w?)
)
(1)
where v(w) can be instantiated either with basic or
with ctx vectors.
In addition to the compositional models, we also
experimented with an alignment-based approach: In-
stead of computing vectors for complete sentences,
we compute an alignment between the words in the
two sentences. To be more precise, we compute
cosine similarity scores between all possible pairs
of words (tokens) of the two sentences; based on
these similarity scores, we then compute a one-to-one
alignment between the words in the two sentences2,
using a greedy search strategy (see Fig. 1). We assign
a weight to each link in the alignment which is simply
the cosine similarity score of the corresponding word
pair and take the sum of the link weights, normalized
by the maximal length of the two sentences to be the
corresponding similarity score for the two sentences.
The final score is then:
simalign(S,S
?) =
?(w,w?)?ALIGN(S,S?) cos(v(w),v(w
?))
max(|S|, |S?|)
where v(w) is the vector for w, which again can be
either the basic or the contextualized vector.
3 Evaluation
In this section we present our experimental results.
In addition to the models described in Section 2, we
define a baseline model which simply computes the
word overlap between two sentences as:
simoverlap(S,S
?) =
|S?S?|
|S?S?|
(2)
2Note that this can result in some words not being aligned
function ALIGN(S1,S2)
alignment? /0
marked? /0
pairs?{?w,w?? | w ? S1,w? ? S2}
while pairs not empty do
?w,w?? ? highest cosine pair in pairs
if w /? marked and w? /? marked then
alignment? ?w,w?? ? alignment
marked?{w,w?} ?marked
end if
pairs? pairs \ {?w,w??}
end while
return alignment
end function
Figure 1: The alignment algorithm
The score assigned by this method is simply the num-
ber of words that the two sentences have in common
divided by their total number of words. Finally, we
also propose a straightforward mixture model which
combines all of the above methods. We use the train-
ing data to fit a degree two polynomial over these
individual predictors using least squares regression.
We report cross-validation scores.
3.1 Evaluation setup
The vector space used in all experiments is a bag-of-
words space containing word co-occurrence counts.
We use the GigaWord (1.7 billion tokens) as input
corpus and extract word co-occurrences within a
symmetric 5-word context window. Co-occurrence
counts smaller than three are set to 0 and we further
apply (positive) pmi weighting.
3.2 Training results
The training data results are shown in Figure 2. The
best performance on the video dataset is achieved
by the alignment method using a basic vector rep-
resentation to compute word-level similarity. All
vector-space methods perform considerably better
than the simple word overlap baseline on this dataset,
the alignment method achieving almost 20% gain
over this baseline. This indicates that information
about the meaning of the words is very beneficial for
this type of data, consisting of small, well-structured
sentences.
Using the alignment method with contextualized
605
Component MSRvid MSRpar SMTeur
basic/add 70.9 33.3 31.8
ctx/add 65.7 23.0 30.4
basic/align 74.6 40.5 32.1
overlap 56.8 59.5 50.0
mixture 78.1 61.8 54.1
Figure 2: Results on the training set.
vector representations (omitted in the table) does not
bring any improvement and it performs similarly to
the ctx/add method. This suggests that aligning sim-
ilar words in the two sentences does not benefit from
further meaning disambiguation through contextual-
ized vectors and that some level of disambiguation
may be implicitly performed.
On the paraphrase and europarl datasets, the over-
lap baseline outperforms, by a large margin, the vec-
tor space models. This is not surprising, as it is
known that word overlap baselines can be very com-
petitive on Recognizing Textual Entailment datasets,
to which these two datasets bare a large resemblance.
In particular this indicates that the methods proposed
for combining vector representations of words do
not provide, in the current state, accurate models for
modeling the meaning of larger sentences.
We also report 10-fold cross-validation scores ob-
tained with the mixture model. On all datasets, this
outperforms the individual methods, improving by
a margin of 2%-4% the best single methods. In par-
ticular, on the paraphrase and europarl datasets, this
shows that despite the considerably inferior perfor-
mance of the vector-based methods, these can still
help improve the overall performance.
This is also reflected in Table 3, where we evaluate
the performance of the mixture method when, in
turn, one of the individual components is excluded:
with few exceptions, all components contribute to the
performance of the mixtures.
3.3 Test results
We have submitted as our official runs the best sin-
gle vector space model, performing alignment with
basic vector similarity, as well as the mixture meth-
ods. The mixture method uses weights individually
learned for each of the datasets made available during
Component MSRvid MSRpar SMTeur
basic/add ?2.1 ?0.1 ?1.5
ctx/add ?0.6 +1.3 +0.4
basic/align ?4.1 ?1.9 ?2.6
overlap ?0.1 ?17.0 ?23.0
Figure 3: Results on the training set when removing indi-
vidual components from the mixture model.
training. For the two surprise datasets we carry over
the weights of what we have considered to be the
most similar training-available sets: video weights of
ontonotes and paraphrase weights for news.
The test data results are given in 4. We report
the results for the individual datasets as well as the
mean Pearson correlation, weighted by the sizes of
the datasets. The table also shows the performance
of the official task baseline as well as the top three
runs accoring to the overall weighted mean score.
As expected, the mixture method outperforms by
a large margin the alignment model, achieving rank
10 and rank 13 on the video and paraphrase datasets.
Overall the mixture method ranks 43 according to the
weighted mean measure (rank 22 if correcting our of-
ficial submission which contained the wrong output
file for the europarl dataset). The other more con-
troversial measures rank our official, not corrected,
submission at position 13 (RankNrm) and 71 (Rank),
overall. This is an encouraging result, as the individ-
ual components we have used are all unsupervised,
obtained solely from large amounts of unlabeled data,
and with no other additional resources. The training
data made available has only been used to learn a
set of weights for combining these individual compo-
nents.
4 Conclusions
This paper describes an approach that combines few
simple vector space-based components to model sen-
tence similarity. We have extended the state-of-the-
art model for contextualized meaning representations
of Thater et al (2011) with an additive composi-
tion operation along the lines of Mitchell and Lap-
ata (2008). We have combined this with a simple
alignment-based method and a word overlap baseline
into a mixture model.
Our system achieves promising results in particular
606
Dataset basic/align mixture baseline Run1 Run2 Run3
MSRvid 77.1 83.1 30.0 87.3 88.0 85.6
MSRpar 40.4 63.1 43.3 68.3 73.4 64.0
SMTeur 26.8 13.9 (37.1?) 45.4 52.8 47.7 51.5
OnWN 57.2 59.6 58.6 66.4 67.9 71.0
SMTnews 35.0 38.0 39.1 49.3 39.8 48.3
ALL 49.5 45.4 31.1 82.3 81.3 73.3
Rank 65 71 87 1 3 15
ALLNrm 78.7 82.5 67.3 85.7 86.3 85.2
RankNrm 50 13 85 2 1 5
Mean 50.6 56.6 (60.0?) 43.5 67.7 67.5 67.0
RankMean 60 43 (22?) 70 1 2 3
Figure 4: Results on the test set. ? ? corrected score (official results score wrong prediction file we have submitted for
the europarl dataset). Official baseline and top three runs according to the weighted mean measure.
on the Microsoft Research Paraphrase and Video
Description datasets, on which it ranks 13th and 10th,
respectively. We take this to be a promising result,
given that our focus has not been the development
of a highly-competitive complex system, but rather
on investigating what performance can be achieved
when using only vector space methods.
An interesting observation is that the methods for
combining word vector representations (the vector
addition, or the meaning contextualization) can be
beneficial for modeling the similarity of the small,
well-structured sentences of the video dataset, how-
ever they do not perform well on comparing longer,
more complex sentences. In future work we plan to
further investigate methods for composition in vector
space models using the STS datasets, in addition to
the small, controlled datasets that have been typically
used in this line of research.
Acknowledgments. This work was supported by
the Cluster of Excellence ?Multimodal Computing
and Interaction,? funded by the German Excellence
Initiative.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, Cambridge, MA, October.
Association for Computational Linguistics.
Georgiana Dinu and Stefan Thater. 2012. A comparison
of models of word meaning in context. In Proceedings
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. Short paper, to appear.
Katrin Erk and Sebastian Pado?. 2008. A structured vector
space model for word meaning in context. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, Honolulu, HI, USA.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, Columbus, OH, USA.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of the 48th Annual Meeting of the Association for Com-
putational Linguistics, Uppsala, Sweden.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effective
vector model. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
1134?1143, Chiang Mai, Thailand, November. Asian
Federation of Natural Language Processing.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space modes of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
607
