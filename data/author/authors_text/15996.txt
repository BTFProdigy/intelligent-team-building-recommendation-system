2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 548?552,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Ranking-based readability assessment for early primary children?s literature
Yi Ma, Eric Fosler-Lussier
Dept. of Computer Science & Engineering
The Ohio State University
Columbus, OH 43210, USA
may,fosler@cse.ohio-state.edu
Robert Lofthus
Xerox Corporation
Rochester, NY 14604, USA
Robert.Lofthus@xerox.com
Abstract
Determining the reading level of children?s lit-
erature is an important task for providing edu-
cators and parents with an appropriate reading
trajectory through a curriculum. Automating
this process has been a challenge addressed
before in the computational linguistics litera-
ture, with most studies attempting to predict
the particular grade level of a text. However,
guided reading levels developed by educators
operate at a more fine-grained level, with mul-
tiple levels corresponding to each grade. We
find that ranking performs much better than
classification at the fine-grained leveling task,
and that features derived from the visual lay-
out of a book are just as predictive as standard
text features of level; including both sets of
features, we find that we can predict the read-
ing level up to 83% of the time on a small cor-
pus of children?s books.
1 Introduction
Determining the reading level of a text has received
significant attention in the literature, dating back to
simple arithmetic metrics to assess the reading level
based on syllable counts (Flesch, 1948). In the com-
putational linguistics community, several projects
have attempted to determine the grade level of a text
(2nd/3rd/4th/etc). However, the education commu-
nity typically makes finer distinctions in reading lev-
els, with each grade being covered by multiple lev-
els. Moreover, there are multiple scales within the
educational community; for example 1st grade is ap-
proximately covered by levels 3?14 on the Reading
Recovery scale,1 or levels C to H in the Fountas and
Pinnell leveling system.2
For grade-level assessment, classification and
regression approaches have been very promising.
However, it is not clear that an increased number of
classes will allow classification techniques to suc-
ceed with a more fine-grained leveling system. Sim-
ilarly, regression techniques may have problems if
the reading levels are not linearly distributed. In this
work, we investigate a ranking approach to book lev-
eling, and apply this to a fine-grained leveling prob-
lem for Kindergarten through 2nd grade books. The
ranking approach also allows us to be more agnostic
to the particular leveling system: for the vast ma-
jority of pairs of books, different systems will rank
the levels of the books the same way, even if the
exact differences in levels are not the same. Since
most previous work uses classification techniques,
we compare against an SVM multi-class classifier
as well as an SVM regression approach.
What has not received much attention in recent
research is the visual layout of the page. Yet, if one
walks into a bookstore and rummages through the
children?s section, it is very easy to tell the reading
level of a book just by thumbing through the pages.
Visual clues such as the number of text lines per
page, or the area of text boxes relative to the illustra-
tions, or the font size, give instant information to the
reader about the reading level of the book. What is
not clear is if this information is sensitive enough to
deliver a fine-grained assessment of the book. While
1http://www.readingrecovery.org
2http://www.fountasandpinnellleveledbooks.com
548
publishers may have standard guidelines for content
providers on visual layout, these guidelines likely
differ from publisher to publisher and are not avail-
able for the general public. Moreover, in the digi-
tal age teachers are also content providers who do
not have access to these guidelines, so our proposed
ranking system would be very helpful as they cre-
ate reading materials such as worksheets, web pages,
etc.
2 Related Work
Due to the limitations of traditional approaches,
more advanced methods which use statistical lan-
guage processing techniques have been introduced
by recent work in this area (Collins-Thompson and
Callan, 2004; Schwarm and Ostendorf, 2005; Feng
et al, 2010). Collins-Thompson and Callan (2004)
used a smoothed unigram language model to pre-
dict the grade reading levels of web page documents
and short passages. Heilman et al (2007) com-
bined a language modeling approach with grammar-
based features to improve readability assessment for
first and second language texts. Schwarm/Petersen
and Ostendorf (2005; 2009) used a support vector
machine to combine surface features with language
models and parsed features. The datasets used in
these previous related works mostly consist of web
page documents and short passages, or articles from
educational newspapers. Since the datasets used are
text-intensive, many efforts have been made to in-
vestigate text properties at a higher linguistic level,
such as discourse analysis, language modeling, part-
of-speech and parsed-based features. However, to
the best of our knowledge, no prior work attempts to
rank scanned children?s books (in fine-grained read-
ing levels) directly by analyzing the visual layout of
the page.
3 Ranking Book Leveling Algorithm
Our proposed method can be regarded as a modi-
fied version of a standard ranking algorithm, where
we develop a leveling classification by first rank-
ing books, and then assigning the level based on
the ranking output. Given a set of leveled books,
the process to generate a prediction for a new target
book involves the following two steps.
In the first step, we extract features from each
book, and train an off-the-shelf ranking model to
minimize the pairwise error of books. During the
test phase (second step), we rank all of the leveled
training books as well as the new target (test) book
using the trained ranking model. The predicted read-
ing level of the target book then can be inferred from
the reading levels of neighboring leveled books in
the rank-ordered list of books (in our experiment, we
take into account a window of three books above and
below the target book with reading levels weighted
by distance). Intuitively, we can imagine a book-
shelf in which books are sorted by their reading lev-
els. The ranker?s prediction of the reading level of a
target book corresponds to inserting the target book
into the sorted bookshelf.
4 Data Preparation
4.1 Book Selection, Scanning and Markup
We have processed 36 children?s books which range
from reading level A to L (3 books each level). The
golden standard key reading levels of those books
are obtained from Fountas and Pinnell leveled book
list (Fountas and Pinnell, 1996) in which letter A in-
dicates the easiest books to read and letter L iden-
tifies more challenging books; this range covers
roughly Kindergarten through Second Grade. The
set of children?s books covers a large variety of gen-
res, series and publishers.
After seeking permission from the publishers,3
all of the books are scanned and OCRed (Optical
Character Recognized) to create PDF versions of
the book. In order to facilitate the feature extrac-
tion process, we manually annotate each book using
Adobe Acrobat markup drawing tools before con-
verting them into corresponding XML files. The
annotation process consists of two straightforward
steps: first, draw surrounding rectangles around the
location of text content; second, find where the pri-
mary illustration images are and mark them using
rectangle markups. Then the corresponding XML
can be generated directly from Adobe Acrobat with
one click on a customized menu item, which is im-
plemented by using Adobe Acrobat JavaScript API.
3This is perhaps the most time-consuming part of the pro-
cess.
549
# of partitions 1 2 3 4
?1 Accuracy %
SVM Ranker 72.2 69.4 80.6 83.3
SVM Classifier 47.2 61.1 55.6 63.9
SVM Regression 72.2 61.1 58.3 58.3
Flesch-Kincaid 30.6 30.6 30.6 19.4
Spache 27.8 13.9 13.9 11.1
Average leveling error ? standard deviation
SVM Ranker 1.00 ? 0.99 1.03 ? 0.91 0.94 ? 0.83 0.92 ? 0.73
SVM Classifier 2.00 ? 1.60 1.86 ? 1.69 1.78 ? 1.57 1.44 ? 1.23
SVM Regression 1.14 ? 1.13 1.25 ? 1.11 1.33 ? 1.22 1.36 ? 1.22
Flesch-Kincaid 3.03 ? 2.21 3.03 ? 2.29 3.08 ? 2.31 3.31 ? 2.28
Spache 4.06 ? 3.33 4.72 ? 3.27 4.83 ? 3.34 5.19 ? 3.21
Table 1: Per-book (averaged) results for ranking versus classification, reporting accuracy within one level and average
error for different numbers of partitions
4.2 Feature Design
4.2.1 Surface-level Features
We extract a number of purely text-based features
that have typically been used in the education litera-
ture (e.g., (Flesch, 1948)), including:
1. Number of words; 2. Number of letters per
word; 3. Number of sentences; 4. Average sentence
length; 5. Type-token ratio of the text content.
4.2.2 Visually-oriented Features
In this feature set, we include a number of features
that would not be available without looking at the
physical layout of the page; with the annotated PDF
versions of the book we are able to extract:
1. Page count; 2. Number of words per page; 3.
Number of sentences per page; 4. Number of text
lines per page; 5. Number of words per text line;
6. Number of words per annotated text rectangle;
7. Number of text lines per annotated text rectan-
gle; 8. Average ratio of annotated text rectangle area
to page area; 9. Average ratio of annotated image
rectangle area to page area; 10. Average ratio of an-
notated text rectangle area to annotated image rect-
angle area; 11. Average font size.
The OCR process provides some of this informa-
tion automatically; while we have manually anno-
tated rectangles for this study one could theoreti-
cally use the OCR information and vision process-
ing techniques to extract rectangles automatically.
5 Experiments
5.1 Ranking vs. Classification/Regression
In this experiment, we look at whether treating book
leveling as a ranking problem is promising com-
pared to using classification/regression techniques.
Besides taking a whole book as input, we also exper-
iment with partitioning each book uniformly into 2,
3, or 4 parts, treating each sub-book as an indepen-
dent entity. We use a leave-n-out paradigm ? dur-
ing each iteration of the training (iterated through all
books), the system leaves out all n partitions corre-
sponding to one book and then tests on all partitions
corresponding to the held-out book. By averaging
the results for the partitions of the held-out book, we
can obtain its predicted reading level.
For ranking, we use the SVMrank ranker
(Joachims, 2006), which learns a (sparse) weight
vector that minimizes the number of swapped pairs
in the training set. The test book is inserted into the
ordering of the training books by the ranking algo-
rithm, and the level is assigned by averaging the lev-
els of the books above and below the order. To com-
pare the performance of our method with classifiers,
we use both SVMmulticlass classifier (Tsochantaridis
et al, 2004) and SVMlight (with regression learning
option) (Joachims, 1999) to determine the level of
the book directly. All systems are given the same
set of surface text-based and visual-based features
(Sections 4.2.1 and 4.2.2) as input.
550
# of partitions 1 2 3 4
?1 Accuracy %
All Features 72.2 69.4 80.6 83.3
Surface Features 61.1 63.9 58.3 61.1
Visual Features 72.2 72.2 72.2 83.3
Average leveling error ? standard deviation
All Features 1.00 ? 0.99 1.03 ? 0.91 0.94 ? 0.83 0.92 ? 0.73
Surface Features 1.42 ? 1.18 1.28 ? 1.00 1.44 ? 0.91 1.28 ? 1.11
Visual Features 1.03 ? 0.88 0.94 ? 0.86 1.03 ? 0.81 0.89 ? 0.82
Table 2: Per-book (averaged) results for all, surface-only, and visual-only features, reporting accuracy within one level
and average error for different numbers of partitions
We score the systems in two ways: first, we com-
pute the accuracy of the system by claiming it is cor-
rect if the book level is within ?1 of the true level.4
The second scoring method is the absolute error of
number of levels away from the true value, averaged
over all of the books.
As we can observe from Table 1, our ranking
system constantly beats the other two approaches
(the ranker is statistically significantly better than
the classifier at p < 0.05 level ? figures in bold).
One bit of interesting discovery is that SVM regres-
sion needs more data in order to have reliable results,
as the performance is downgraded when the number
of partitions goes up; the ranking approach benefits
from averaging the increasing number of partitions.5
All three methods have the same style of learner
(support vector learning), which suggests that the
performance gain is due to using a ranking crite-
rion in our method. Therefore we believe ranking
is likely a more effective and accurate method than
classification for this task.
One might also wonder how a traditional measure
of reading level (in this case, the Flesch-Kincaid
(Flesch, 1948) and Spache (Spache, 1953) Grade
Level) would hold up for this data. Flesch-Kincaid
and Spache predictions are linearly converted from
calculated grade levels to Fountas-Pinnell levels; all
of the systems utilizing our full feature set outper-
form these two baselines by a significant amount on
both ?1 accuracy and average leveling error.
4Note that this is still rather fine-grained as there are multi-
ple book levels per grade level.
5We only partition the books up to 4 sub-books because the
shortest book we have only contains 4 PDF pages (8 ?book?
pages) and further partitioning the book will lead to sparse data.
5.2 Visual vs. Surface Features
In order to evaluate the benefits of using visual cues
to assess reading levels, we repeat the experiments
using SVMrank based on our proposed ranking book
leveling algorithm with only the visual features or
only surface features.
Table 2 shows that the visual features surprisingly
outperform the surface features (statistically signif-
icant at p < 0.05 level ? figures in bold) and on
some partition levels, visual cues even beat the com-
bination of all features. We note, however, that for
early children?s books, pictures and textual layout
dominate the book content over text. Visual features
can be as useful as traditional surface text-based fea-
tures, but as one moves out of primary literature, we
suspect text features will likely be more effective for
leveling as content becomes more complex.
6 Conclusions
In this paper, we proposed a ranking-based book lev-
eling algorithm to assess reading level for children?s
literature. Our experimental results showed that the
ranking-based approach performs significantly bet-
ter than classification approaches as used in current
literature. The increased number of classes deterio-
rates the performance of classifiers in a fine-grained
leveling system. We also introduced visual features
into readability assessment and have seen consider-
able benefits of using visual cues. Since our target
data are children?s books that contain many illustra-
tions and pictures, it is quite reasonable to utilize vi-
sual content to help predict a more accurate reading
level. Future studies in early childhood readability
need to take visual content into account.
551
References
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT / NAACL 2004, volume 4, pages
193?200, Boston, USA.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A comparison of features for automatic read-
ability assessment. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 276?284, Beijing, China. As-
sociation for Computational Linguistics.
R. Flesch. 1948. A new readability yardstick. Journal of
applied psychology, 32(3):221?233.
I. Fountas and G. Pinnell. 1996. Guided Reading:
Good First Teaching for All Children. Heinemann,
Portsmouth, NH.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammatical
features to improve readability measures for first and
second language texts. In Proceedings of NAACL
HLT, pages 460?467.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vec-
tor Learning, chapter 11, pages 169?184. MIT Press,
Cambridge, MA.
T. Joachims. 2006. Training linear SVMs in linear time.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 217?226. ACM.
S. Petersen and M. Ostendorf. 2009. A machine learn-
ing approach to reading level assessment. Computer
Speech & Language, 23(1):89?106.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 523?530. Association for Computational
Linguistics.
G. Spache. 1953. A new readability formula for primary-
grade reading materials. The Elementary School Jour-
nal, 53(7):410?413.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the twenty-first international conference on Machine
learning, page 104. ACM.
552
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 91?97,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
User Goal Change Model for Spoken Dialog State Tracking
Yi Ma
Department of Computer Science & Engineering
The Ohio State University
Columbus, OH 43210, USA
may@cse.ohio-state.edu
Abstract
In this paper, a Maximum Entropy Markov
Model (MEMM) for dialog state tracking
is proposed to efficiently handle user goal
evolvement in two steps. The system first
predicts the occurrence of a user goal change
based on linguistic features and dialog context
for each dialog turn, and then the proposed
model could utilize this user goal change in-
formation to infer the most probable dialog
state sequence which underlies the evolve-
ment of user goal during the dialog. It is
believed that with the suggested various do-
main independent feature functions, the pro-
posed model could better exploit not only the
intra-dependencies within long ASR N-best
lists but also the inter-dependencies of the ob-
servations across dialog turns, which leads to
more efficient and accurate dialog state infer-
ence.
1 Introduction
The ability to converse with humans is usually con-
sidered the most important characteristic which de-
fines the intelligent nature of a machine. In recent
years, advanced approaches for handling different
components within a spoken dialogue system have
been proposed and studied. Both statistical infer-
ence methods for dialog state tracking and machine
learning techniques (such as reinforcement learning)
for automatic policy optimization are active domains
of research, which implies that there are still many
open challenges in this field that are worth being ex-
plored. One of such challenges is how to better ex-
ploit the ASR (Automatic Speech Recognition) N-
best list when the top ASR hypothesis is incorrect.
Furthermore, reasoning over different ASR N-best
lists is also difficult since it is hard to decide when
to detect commonality (when user repeats) and when
to look for differences (when user changes her or his
mind) among multiple ASR N-best lists. Another
challenge is how to handle more complex user ac-
tions such as negotiating alternative choices or seek-
ing out other potential solutions when interacting
with the system.
This proposal presents a probabilistic framework
for modeling the evolvement of user goal during the
dialog (focusing on the shaded component Dialog
State Tracking in Figure 1 that shows a typical di-
agram for a spoken dialog system), which aims to
endow the system with the ability to model natural
negotiation strategies, in the hope of leading to more
accurate and efficient dialog state tracking perfor-
mance.
Figure 1: a typical spoken dialogue system
2 Unanswered Challenges for Spoken
Dialog Systems
Due to the inevitable erroneous hypotheses made by
the speech recognizer as well as the ubiquitous am-
biguity existing in the natural language understand-
91
ing process, it is impossible for a spoken dialog sys-
tem to observe the true user goal directly. Therefore,
methods to efficiently infer the true hidden dialog
states from noisy observations over multiple dialog
turns become crucial for building a robust spoken
dialog system.
The POMDP (Partially Observable Markov De-
cision Process) framework has been proposed to
maintain multiple dialog state hypotheses under
uncertainty with automated dialog policy learn-
ing (Williams and Young, 2007; Henderson et
al., 2008; Thomson and Young, 2010; Young et
al., 2010). Although the original POMDP frame-
work suffers difficulties of scaling up the model to
handle real-world domains in practice, it provides
a unified statistical framework for existing tech-
niques with global optimization. Partition-based ap-
proaches (Gas?ic? and Young, 2011; Williams, 2010;
Young et al, 2010) attempt to group user goals into
a number of partitions and won?t split a partition un-
less when a distinction is required by observations.
Due to this property, partition-based methods could
have high scalability for more complex practical do-
mains.
Bayesian network based approximate methods
also emerged to tackle the complexity of represent-
ing and tracking multiple dialog states within proba-
bilistic frameworks (Raux and Ma, 2011; Thomson
and Young, 2010). In previous work, we presented
a new probabilistic model ? DPOT (Dynamic Prob-
abilistic Ontology Trees) ? to track dialog state in a
spoken dialog system (Raux and Ma, 2011). DPOT
captures both the user goal and the history of user di-
alog acts (user actions) using a unified Bayesian net-
work. Efficient inference (a form of blocked Gibbs
sampling) is performed to exploit the structure of
the model. Evaluation on a corpus of dialogs from
the CMU Let?s Go system shows that DPOT signif-
icantly outperforms a deterministic baseline by ex-
ploiting long ASR N-best lists without loss of ac-
curacy. At any point in the dialog, the joint distri-
bution over the goal network represents the inferred
dialog state about the user goal.1 The goal network
of DPOT does not expand per time slice for each
turn but the evidence accumulates as the dialog pro-
1In the Let?s Go bus information system, a user goal is de-
composed into three concepts: Bus (the bus number), Orig
(the origin stop) and Dest (the destination stop).
gresses. Therefore the model becomes inefficient
when users change their mind ? user has to repeat
multiple times in order to possibly trigger a goal
change in the inferred dialog state.
Figure 2: Example of user goal change: at the end of the
dialog the user would like to explore alternative flights at
a different time, but the dialog system did not expect such
a user action, leading to a system failure
Current approaches often assume that user would
have a fixed goal in his or her mind before convers-
ing with the system and this single goal remains un-
changed throughout the dialog. However, the key
question we would like to raise here is that whether
the assumption that a user would not change her or
his mind during the dialog is reasonable or not in
the first place.2 Figure 2 shows an example where
user goal evolves as the dialog moves on. In this ex-
ample, the system did not catch the partial change
of user goal and failed to return alternative answers
given a new request from the user ? now the fixed
goal assumption has been challenged. Moreover,
sometimes people do not even have a clear goal in
their minds before they start speaking to the system
(e.g., a user might want a flight from Columbus to
San Francisco during the coming weekend, but the
exact departure date depends on user?s schedule as
well as the price of the ticket.). From the example
dialog shown in Figure 2, clearly it can be noticed
that there are some useful hints or linguistic patterns
? such as How about ...? and ... instead? ? which
could be extracted from the user?s spoken language
2It is true that for some simple domains such as luggage re-
trieval or call routing, users are less likely to change their mind.
92
as predictors for potential user goal change. We can
then further use this predicted information (user goal
changed or not) to better infer the true user goal and
prevent a system failure or start over. In fact, it is
this intuition that forms the basis of the proposed
methods.
However, existing methods heavily rely on the as-
sumption that user won?t change her or his mind
throughout the dialog. In order to keep the compu-
tations tractable in practice, POMDP-based methods
often assume that user goal does not change during
the dialog (Young et al, 2010). Moreover, within
the POMDP framework there is a user action model
which would suppress the weights of conflict ob-
servations for those slots which have already been
filled ? the intuition is that if a value for a certain
slot has already been provided or observed, it is
less likely that a new value will be provided again
(based on the assumption of fixed user goal) and it
is more likely to be a speech recognition error in-
stead (Williams and Young, 2007). Furthermore,
one of the claimed benefit for existing statistical di-
alog state inference methods is the ability to exploit
the information lower down from ASR N-best lists
by aggregating weak information across multiple di-
alog turns ? the intuition is that overlapped consis-
tent weak evidence is sometimes a useful hint for
predicting the underlying true user goal (as illus-
trated in Figure 3) ? again it implies that the user
would repeatedly refine the same goal until the ma-
chine gets it.
Figure 3: Given the fact that user action BOSTON has
been repeatedly observed as DEPARTURE CITY across
the first two turns ? although not at the top position of the
ASR N-best list ? existing statistical dialog state tracking
algorithms would capture this pattern and put a strong
bias on BOSTON as the inferred user goal.
It is true that putting such a constraint ? assum-
ing a fixed user goal during the dialog ? simplifies
the computational complexity, it also sacrifices the
flexibility and usability of a spoken dialog system.
Although one could think of some hand-crafted and
ad-hoc rules such as explicit or implicit confirma-
tion/disconfirmation to deal with sudden user goal
changes during a dialog, it increases the number of
dialog turns and makes the dialog system less natu-
ral and user friendly.
3 Spoken Dialog State Tracking with
Explicit Model of User Goal Change
3.1 BuildByVoice Domain
In fact, there are many situations where frequent
user goal changes would be highly expected (i.e. the
user might try to negotiate with the system). These
domains might include but not limited to finding
nearby restaurants or hotels, searching for movies
to watch, ordering food or online shopping, etc., in
which users are very likely to explore different alter-
natives and their goals would probably change fre-
quently as the dialog progresses.
Figure 4: An experimental web interface prototype for
BuildByVoice ? a spoken dialog system aimed to assist
potential car buyers to customize a car by voice
Considering one typical example among those do-
mains ? a spoken interactive system which could al-
low a user to configure a new car by speech (a pro-
totype web interface of the BuildByVoice system is
shown in Figure 43) ? one could imagine the user
would tend to experiment many possible combina-
tions of different configurations for a car. Indeed
that is the purpose of having such a system so that
users could preview the resulting effect before a real
car is made. A BuildByVoice domain may consist of
3A baseline BuildByVoice system by using DPOT for dialog
state tracking (without user goal change detection) is under im-
plementation. The baseline system will be deployed to Amazon
Mechanical Turk for initial data collection.
93
the following five independent concepts with their
possible values listed as follows:4
Model: Accord Coupe, Accord Sedan,
Accord Plug-In, Civic Coupe,
Civic Sedan, . . . 5
Engine: V4, V4 Turbo, V4 Sport, V6, V6
Turbo, V6 Sport, . . .
Exterior Color: Toffee Brown, Coffee
Brown, Candy Brown, Night Blue,
Moonlight Blue, Midnight Blue, . . .
Interior Color: Black Leather, Black
Vinyl, Gray Leather, Gray Vinyl,
Brown Leather, Brown Vinyl, . . .
Wheels: 17 inches Steel, 17 inches
Alloy, 18 inches Steel, 18 inches
Alloy, 18 inches Polished Alloy,
. . .
In (Ammicht et al, 2007), the semantic represen-
tation of a spoken dialog system is augmented with
a dynamic parameter that determines the evolution
of a concept-value pair over time, which could be
considered as early attempts for coping with user
goal changes. However, the determined dynamic
confidence score is used to make a hard choice
for the candidate semantic values, i.e., determin-
ing the birth and death of the observed concept-
value pairs. Thomson and Young (2010) intro-
duced a new POMDP-based framework for building
spoken dialog systems by using Bayesian updates
of dialog state (BUDS). It accommodates for user
goal changes by using a dynamic Bayesian network,
but BUDS is generative rather than a discriminative
model. Therefore it lacks the flexibility of incor-
porating all kinds of overlapping features ? one of
the advantages discriminative models have. Further-
more, BUDS assumes limited changes in the user
goal in order to gain further efficiency. More re-
cently, Gas?ic? and Young (2011) introduces the ex-
plicit representation of complements in partitions
which enables negotiation-type dialogs when user
4More concepts could also be included such as Accessories
or MPG Level, but only these five concepts are picked for
demonstration purpose.
5Here Honda car models are used as an example.
goal evolves during the dialog. However, the explicit
representation of complements is used to provide ex-
istential and universal quantifiers in the system?s re-
sponse.6 Also a special pruning technique is needed
in their approach to ensure the number of partitions
doesn?t grow exponentially.
Therefore, new approaches for recognizing the
event of user goal change and utilizing the goal
change information to better infer dialog states have
been proposed in the following two subsections 3.2
and 3.3.
3.2 Dialog State Tracking with Detected User
Goal Change
Dialog state tracking is usually considered as the
core component of a spoken dialog system where di-
alog manager uses the inferred dialog states to gen-
erate system responses (normally through a learned
or hand-crafted policy mapping from dialog states to
system actions). A specialized version of Maximum
Entropy Markov Model with user goal change vari-
able is proposed for dialog state tracking.7 The most
probable dialog state sequence as well as the most
likely dialog state value for the latest turn can be in-
ferred given the model. Figure 5 illustrates how the
proposed model could infer dialog states of a sin-
gle concept Exterior Color for a dialog of four user
turns where the user changes her or his mind at the
third dialog turn.8
For traditional dialog state tracking methods with-
out user goal change model, the system would be
quite confused by completely conflicting observed
user actions starting from the third dialog turn. How-
ever, the proposed MEMM with user goal change
detection could notice that the user has already
changed her or his mind. Therefore the proposed
model would not only trust more on the observed
user actions for the current dialog turn, but also fa-
vor those transitions which lead to a different state
value by increasing corresponding transition proba-
bilities.
6E.g., ?Charlie Chan is the only Chinese restaurant in the
center.? or ?All Chinese restaurants are in the center.?
7Methods for detecting user goal change are described in
Section 3.3.
8We assume every concept in the domain is mutually inde-
pendent with each other and we model the user goal change
separately for each concept.
94
Figure 5: MEMM for dialog state tracking with explicit user goal change variable. A single concept Exterior Color
from BuildByVoice domain is tracked by the model. The shaded nodes are observed user actions and the white nodes
are hidden dialog states. The bold text in the observed nodes indicates the true user actions whereas the bold text in
the hidden states shows the true dialog state sequence (in this case it is also the most probable decoded dialog state
path inferred by the model).
A more formal description of the proposed
MEMM is given as follows. The observations ot
(shaded nodes) consist of N-best lists of semantic
speech hypotheses (or dialog acts) with confidence
scores (scale from 0 to 100) for the current dialog
turn hypt and previous turn hypt?1 as well as the
binary goal change variable gct for the current turn
? essentially a context window of speech hypotheses
including history:
ot = {hypt?1, hypt, gct}
Typically the semantic speech hypotheses hypt are
extracted concept-value pairs out of ASR results by
using a semantic tagger (such as an FST (Finite State
Transducer) parser or a segment-based semi-Markov
CRF semantic labeler (Liu et al, 2012)). The hid-
den dialog state qt (white nodes) represents the user
goal for dialog turn t (such as a particular color
Moonlight Blue for Exterior Color at time t).
The individual probability of a transition from a state
qt?1 to a state qt producing an observation ot is in a
form of the following:
P (qt|qt?1, ot) =
exp(
?n
k=1 wkfk(qt?1, qt, ot))
Z(ot, qt?1)
Given labeled sequences of true dialog states (true
user goal) for each turn, the corresponding obser-
vations and designed feature functions, we want to
learn a set of weights wk to optimize the discrimina-
tion among competing state values given the train-
ing data. In other words, the learning procedure in-
volves searching in parameter space to maximize the
following conditional likelihood:
P (Q|O) =
N?
i=1
T?
t=1
exp(
?n
k=1 wkfk(qi,t?1, qit, oit))
Z(oit, qi,t?1)
where N is the number of training dialogs. MEMM
can be trained with methods from the field of convex
optimization and Viterbi decoding algorithm could
be applied to MEMMs for inference (McCallum et
al., 2000).
The proposed feature functions are as follows.
The first feature function (1a) implies that if the user
goal is not changed, the system should look for the
common evidence across dialog turns.
f(qt = v, ot) =
?
?
?
1 if gct=0 &
v?common(hypt?1, hypt)
0 otherwise
(1a)
where common(hypt?1, hypt) will return the over-
lapped values from the two N-best lists of dialog
acts hypt?1 and hypt. The second and third feature
functions ((1b) and (1c)) are basically saying that if a
user goal change has been detected, then we should
expect a different state value, otherwise we should
95
remain the same value from previous dialog turn.
f(qt?1 = u, qt = v, ot) =
{
1 if gct=0 & u=v
0 otherwise
(1b)
f(qt?1 = u, qt = v, ot) =
{
1 if gct=1 & u6=v
0 otherwise
(1c)
The intuition behind the following four feature func-
tions (feature function (1d) to (1g)) is that if the user
changes her or his mind then the model should trust
more on the current observed user actions than those
from previous turn; but if the user does not change
her or his mind, we could then consider the observa-
tions from the past.
f(qt = v, ot) =
{
1 if gct=0 & v?hypt?1
0 otherwise
(1d)
f(qt = v, ot) =
{
1 if gct=1 & v?hypt?1
0 otherwise
(1e)
f(qt = v, ot) =
{
1 if gct=0 & v?hypt
0 otherwise (1f)
f(qt = v, ot) =
{
1 if gct=1 & v?hypt
0 otherwise (1g)
The last two feature functions ((1h) and (1i)) try
to incorporate information from confidence scores
? the higher the confidence score is, the more likely
the hypothesis is to be correct.
f(qt = v, ot) =
?
?
?
1 if v?hypt &
confidencehypt(v)>C
0 otherwise
(1h)
f(qt = v, ot) =
?
???
???
1 if gct=0 &
v?hypt?1 &
confidencehypt?1(v)>C
0 otherwise
(1i)
where confidencehypt(v) returns the confidencescore for value v in the speech hypotheses N-best
list hypt and C is an empirical constant threshold
range between 0 to 100 obtained from the training
corpus.
3.3 User Goal Change Detection with
Linguistic Features and Dialog Context
In previous subsection 3.2, we assume we already
know whether or not user changes her or his mind
at each dialog turn, whereas this subsection we dis-
cuss the possible approaches on how to detect a user
goal change. Detecting user goal changes during a
dialog could be cast as a binary classification prob-
lem where class 0 means no goal change and class 1
indicates user changes her or his mind during a dia-
log turn. Candidate machine learning algorithms in-
cluding MLP (Multi-layer Perceptron), SVM (Sup-
port Vector Machine) or Logistic Regression could
be applied to this binary classification problem in
a supervised manner. The input features might be
extracted from user utterance transcription9 and the
corresponding ASR N-best list for each dialog turn.
As mentioned in Section 2, the language patterns
found in the user utterances as presented in the ex-
ample dialog (shown in Figure 2) forms the intuition
for linguistic features to identify user goal change.
The dialog context such as last system action could
also be included as useful hint for predicting a po-
tential user goal change ? user is likely to change
her or his goal if system returns empty results for a
request. Also other helpful features could include
bag of words model, n-grams, prosodic features
(e.g., a pitch change or initial pause) and parsed fea-
tures (e.g., WH questions). Baseline system such
as key word spotting based approach (i.e. look for
How/What about in a sentence) could also be imple-
mented for performance comparison.10
4 Conclusion
By modeling the user goal change in a probabilistic
framework, the proposed approach should better ex-
ploit the mutual information buried deep in the ASR
N-best lists across dialog turns, which leads to more
robust and accurate dialog state estimation. With
the ability to predict and handle user goal change,
proposed techniques provide a bottom-up solution
for managing negotiation style dialogs and not only
should produce more efficient and natural conver-
sations but also open up new possibilities for auto-
mated negotiation dialog policy learning.
9At test time, this could be approximated by the top hypoth-
esis in the ASR N-best list.
10A detailed list of proposed features is omitted due to space
limit.
96
References
Egbert Ammicht, Eric Fosler-Lussier, and Alexandros
Potamianos. 2007. Information seeking spoken di-
alogue systems?part i: Semantics and pragmatics.
Multimedia, IEEE Transactions on, 9(3):532?549.
M. Gas?ic? and S. Young. 2011. Effective handling
of dialogue state in the hidden information state
pomdp-based dialogue manager. ACM Transactions
on Speech and Language Processing (TSLP), 7(3):4.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid reinforcement/supervised learning of di-
alogue policies from fixed data sets. Computational
Linguistics, 34(4):487?511.
J. Liu, S. Cyphers, P. Pasupat, I. McGraw, and J. Glass.
2012. A conversational movie search system based on
conditional random fields. In INTERSPEECH.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy markov models for information extrac-
tion and segmentation. In Proceedings of the Seven-
teenth International Conference on Machine Learning,
volume 951, pages 591?598.
A. Raux and Y. Ma. 2011. Efficient probabilistic track-
ing of user goal and dialog history for spoken dialog
systems. In Twelfth Annual Conference of the Interna-
tional Speech Communication Association.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A pomdp framework for spo-
ken dialogue systems. Computer Speech & Language,
24(4):562?588.
J.D. Williams and S. Young. 2007. Partially observable
markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393?422.
Jason D Williams. 2010. Incremental partition recombi-
nation for efficient tracking of multiple dialog states.
In Acoustics Speech and Signal Processing (ICASSP),
2010 IEEE International Conference on, pages 5382?
5385. IEEE.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
pomdp-based spoken dialogue management. Com-
puter Speech & Language, 24(2):150?174.
97
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 169?178,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Landmark-based Location Belief Tracking in a Spoken Dialog System
Yi Ma
The Ohio State University
Columbus, OH 43210
may@cse.ohio-state.edu
Antoine Raux, Deepak Ramachandran, Rakesh Gupta
Honda Research Institute, USA
425 National Ave, Mountain View, CA 94043
{araux,dramachandran,
rgupta}@hra.com
Abstract
Many modern spoken dialog systems use
probabilistic graphical models to update their
belief over the concepts under discussion, in-
creasing robustness in the face of noisy input.
However, such models are ill-suited to prob-
abilistic reasoning about spatial relationships
between entities. In particular, a car naviga-
tion system that infers users? intended desti-
nation using nearby landmarks as descriptions
must be able to use distance measures as a fac-
tor in inference. In this paper, we describe
a belief tracking system for a location iden-
tification task that combines a semantic belief
tracker for categorical concepts based on the
DPOT framework (Raux and Ma, 2011) with
a kernel density estimator that incorporates
landmark evidence from multiple turns and
landmark hypotheses, into a posterior proba-
bility over candidate locations. We evaluate
our approach on a corpus of destination set-
ting dialogs and show that it significantly out-
performs a deterministic baseline.
1 Introduction
Mobile devices such as smart phones and in-car in-
fotainment systems have generated demand for a
new generation of location-based services such as
local business search, turn-by-turn navigation, and
social event recommendation. Accessing such ser-
vices in a timely manner through speech is a crucial
requirement, particularly on the go when the user is
unable to resort to other modalities e.g. where safety
regulations prohibit drivers from using buttons or a
touchscreeen while driving.
In such systems, a Point of Interest (POI)
or a destination such as a restaurant, store or a
public place is often specified. For example, a
car navigation system needs the user to input the
destination before giving directions. Similarly, a
photo tagging application must allow its users to
designate the location where a picture was taken.
While postal addresses can be used to unambigously
identify locations, they are often either unknown
or hard for users to remember. A more natural
(though potentially ambiguous) means of speci-
fying locations is to use landmarks such as ?the
Italian restaurant near Red Rock
cafe on Castro Street? or ?the bakery
near that mall with a Subway and
a 7 Eleven?. A location-based dialog system
that understands referring expressions using land-
marks could lead to more succinct dialogs, higher
recognition accuracy and a greater appearance of
intelligence to the user.
We present a system that performs belief track-
ing over multiple turns of user speech input to infer
the most probable target location. The user inter-
acts with the system through speech in order to spec-
ify a target location, and may include references to
one or more landmarks. Such a system must han-
dle two sources of uncertainty. First, ASR is notori-
ously error-prone and modern ASR engines provide
ranked lists of possible interpretations of speech in-
put rather than single hypotheses. Second, the suit-
ability of a particular landmark or its likelihood of
usage by the speaker depends on a number of factors
such as distance, size and prominence of the land-
mark, familiarity of the user and his expectation of
169
common ground for understanding. These factors,
or at least the resulting variability, must be taken into
account when making inferences about target loca-
tions from landmark-based expressions.
The first source of ambiguity (speech understand-
ing) has been the target of research on belief tracking
(Mehta et al, 2010; Raux and Ma, 2011; Thomson
and Young, 2010). In previous work, the concepts
of interest are entities that are ontologically related
(i.e. with is-a or has-a relations), thus discrete prob-
abilistic graphical models such as DBNs have gen-
erally sufficed as representations. But these mod-
els are ill-suited for dense continuous spatial rela-
tions like the distance between any two locations on
a map. In this paper, we introduce a kernel-based
belief tracker as a probabilistic model for inferring
target locations from (uncertain) landmarks. The
kernel-based representation allows a natural way to
weigh the suitability of a landmark and the speech
understanding confidence. The output of this tracker
is combined with that of a Dynamic Probabilistic
Ontology Tree (DPOT) (Raux and Ma, 2011), which
performs ontological reasoning over other features
of the target location, to give a posterior distribu-
tion over the intended location. We evaluate our ap-
proach on a new corpus of location setting dialogs
specially collected for this work and find it to signif-
icantly outperform a deterministic baseline.
2 Related Work
In the context of a location-based dialog system,
Seltzer et al (2007) describes a speech understand-
ing system designed to recognize street intersec-
tions and map them to a database of valid intersec-
tions using information retrieval techniques. Ro-
bustness is achieved by exploiting both words and
phonetic information at retrieval time, allowing a
soft-matching of the ASR result to the canonical in-
tersection name. Their approach is specifically tar-
geted at intersections, to the exclusion of other types
of landmarks. While intersections are frequently
used as landmarks in North America (where their
study was conducted), this is not always the case
in other cultures, such as Japan (Suzuki and Wak-
abayashi, 2005), where points of interests such as
train stations are more commonly used. Also, their
approach, which is framed as speech understanding,
does not exploit information from previous dialog
turns to infer user intention.
Landmarks have been integrated in route direc-
tions (Pierre-emmanuel Michon, 2001; Tversky and
Lee, 1999) with significant use at origin, destination
and decision points. Further, landmarks have been
found to work better than street signs in wayfind-
ing (Tom and Denis, 2003). The multimodal system
described in (Gruenstein and Seneff, 2007) supports
the use of landmarks from a limited set that the user
specifies by pointing at the map and typing landmark
names. While this allows the landmarks (and their
designations) to be of any kind, the burden of defin-
ing them is on the user.
Spatial language, including landmarks, has also
been the focus of research within the context of
human-robot interaction. (Huang et al, 2010;
MacMahon et al, 2006) describe systems that trans-
late natural language directions into motion paths or
physical actions. These works focus on understand-
ing the structure of (potentially complex) spatial lan-
guage and mapping it into a representation of the
environment. Issues such as imperfect spoken lan-
guage understanding have not been investigated in
this context. Similarly, this vein of spatial language
research has traditionally been conducted on small
artificial worlds with a few dozen objects and places
at most, whereas real-world location-based services
deal with thousands or millions of entities.
3 Hybrid Semantic / Location Belief
Tracking
Our belief tracking system consists of two trackers
running in parallel: a DPOT belief tracker (Raux and
Ma, 2011) and a novel kernel-based location tracker.
The final inference of user intentions is produced by
combining information from the two trackers. The
general idea is to rerank the user goals given spatial
information provided by the location tracker.
3.1 Semantic Belief Tracker
We perform belief tracking over non-landmark con-
cepts such as business name and street using a Dy-
namic Probabilistic Ontology Tree (DPOT) (Raux
and Ma, 2011). A DPOT is a Bayesian Network
composed of a tree-shaped subnetwork representing
the (static) user goal (Goal Network), connected to
170
Figure 1: Top view heat map of spatial distribution with landmarks Subway and 7 Eleven over potential target
places in Mountain View, CA
a series of subnetworks representing the evidence
gathered from each successive dialog turn (Evidence
Networks). Details of the model and an efficient in-
ference method for posterior probability computa-
tions can be found in (Raux and Ma, 2011).
In the context of this paper, the purpose of the
semantic tracker is to update a list of the most
likely target locations using attributes of that
location provided by the user (see Figure 2). In
a local business database, such attributes include
Business Name, Street, Category (e.g.
Japanese restaurant or convenience store), etc.
The structure and parameters of the Goal Network
encode probabilistic ontological relations between
the attributes (e.g. a Mcdonalds would be described
as a fast-food restaurant with high probability)
that can be exploited during inference. These can
be derived from expert knowledge, learned from
data, or as is the case in our experimental system,
populated from a database of local businesses (see
section 4). After each user utterance, the DPOT
outputs a ranked list of user goal hypotheses (an ex-
ample goal hypothesis is [Category=italian
restaurant,Street=castro street]).
Each hypothesis is converted into a query to the
backend database, and the posterior probability of
the hypothesis is split equally among all matching
entries. This results in a ranked list of database
entries corresponding to the system?s belief over
potential target locations, with potentially many
entries having the same probability.
3.2 Kernel-based Location Tracker
Landmark concepts extracted by the Natural Lan-
guage Understanding module (NLU) are passed to
the location tracker, which maintains a distribution
over coordinates of potential target locations. Each
such landmark concept is treated as evidence of spa-
tial proximity of the target to the landmark and the
distribution is accordingly updated. Any location in
the database can serve as a landmark observation,
including major POIs such as train stations or pub-
lic facilities. If the name of a generic chain store
with multiple locations such as Subway is used for
the landmark, then an observation corresponding to
each individual location is added to the tracker.
For each observed landmark `, the location
tracker constructs a 2-dimensional Gaussian kernel
with mean equal to the longitude and latitude of the
landmark (?` = (long`, lat`)) and a fixed covari-
171
Figure 2: Overview of the hybrid semantic / location belief tracking approach; the database entry in shade is the
underlying true target place to which the provided landmark is close
ance matrix ?` for each landmark:
 `(t) =
1
2?|?|1/2 exp( 
1
2(t  ?`)
T? 1` (t  ?`))
This kernel density determines the conditional prob-
ability that the target is at coordinates t =
(longt, latt) given the fixed landmark `. The covari-
ance matrix ?` and hence the shape of the kernel
can be adjusted for different landmarks depending
on considerations such as the familiarity, size and
prominence of the landmark (a large historic monu-
ment is likely to be used as a landmark for locations
much further away than a small corner grocery store)
etc.
The probability density of the location t being the
target is then given by a weighted mixture model:
Pr(t|L) =
X
`2L
w` `(t) (1)
where L is the set of candidate landmarks returned
by the NLU (see Section 4.1) up to the current turn
and w` is set to the confidence score of ` from the
NLU. Thus candidate landmarks that have higher
confidence in the NLU will contribute more strongly
to the total likelihood. Since Pr(t|L) is a den-
sity function, it is unnormalized. In Figure 1, we
show the kernel tracker distribution for a dialog state
where Subway and 7 Eleven are provided as
landmarks.
The kernel density estimator is a simple approach
to probabilistic spatial reasoning. It is easy to imple-
ment and requires only a moderate amount of tuning.
It naturally models evidence from multiple speech
hypotheses and multiple provided landmarks, and
it benefits from accumulated evidence across dia-
log turns. It can also potentially be used to model
more general kinds of spatial expressions by using
appropriate kernel functions. For example, ?Along
Castro street? can be modeled by a Gaussian
with an asymmetric covariance matrix such that the
shape of the resulting distribution is elongated and
concentrated on the street. While ?Two blocks
away from ...? could be modeled by adding
an extra ?negative? density kernel that extends from
172
Figure 3: Overview of the Destination Setting System
the center of the landmark to a distance two blocks
away.
3.3 Combining the Two Trackers
At each turn, the updated results from the Seman-
tic and Location tracker are combined to give a
single ranked list of likely target locations. In
Figure 2, this process is illustrated for a dia-
log turn where two possible concepts are identi-
fied   a category attribute [Category:italian
restaurant] and a landmark [Landmark:red
rock coffee company]. These are passed to
the DPOT tracker and the location tracker respec-
tively. The output of the DPOT is used to retrieve
and score matching database entries. The score for
each entry is reweighted by the kernel density esti-
mator measured at the coordinates of the location 1:
Pr(eij) = (
pi
Ni
)? ? Pr(eij |L) (2)
where Ni is the number of matching database en-
tries retrieved from ith goal hypothesis (having joint
probability pi) and eij is the jth such entry (j 2
[1..Ni]). The exponent ? for the posterior term is
introduced to account for scale difference between
the semantic score and the kernel density.
The set of candidate entries can then be reranked
according to Eq 2 and returned as the output of the
combined belief tracker.
Figure 4: Structure of the Goal Network for the experi-
mental system.
4 Evaluation
4.1 Experimental System
The architecture of our experimental system is
shown in Figure 3. The web client, shown in Figure
5, runs in the participant?s web browser and displays
the target location of the current scenario using the
Google Map API. The user?s goal is to convey this
target location to the system through speech only.
The system backend consists of a database of
2902 businesses located in Mountain View, Cali-
fornia with their name, street, street number, busi-
ness category, latitude and longitude provided. The
grammar rules for the NLU and the probability ta-
bles in the DPOT are populated from this database.
The web client captures the user speech and sends
it to our server with a push-to-talk interface based
on the WAMI toolkit (Gruenstein et al, 2008). The
server uses a commercial cloud-based ASR service
with generic acoustic and language models, which
were not adapted to our task. The n-best list of hy-
potheses from the ASR is sent to our robust natural
1The scores are renormalized to between 0 and1.
173
language understanding module for parsing.
Our NLU uses a hybrid approach combining
a weighted finite-state transducer (WFST) with
string matching based rescoring of the output. The
WFST incorporates out-of-grammar word loops
that allow skipping input words at certain points
in the parse2. This parser robustly maps free form
utterances (e.g. ?Okay let?s go to that
Italian place near, uh..., Red
Rock Cafe, on Castro?) to semantic frames
(e.g. [Category=italian restaurant,
Street=castro street, Landmark=red
rock coffee company]).
The NLU confidence score is computed based on
the number of words skipped while parsing, and
how close the important concept words match the
canonical phrases found in the database. For in-
stance, ?Red Rock Cafe? matches the canoni-
cal name ?Red Rock Coffee Company? with
high confidence because rare words (Red, Rock)
are identical, and differing but common words
(Cafe, Coffee, Company) have a low weight
in the score. The string matching score is based
on the term-frequency/inverse document frequency
(TF-IDF) metric commonly used in information re-
trieval. In our case, the weight of different terms
(IDF) is estimated based on their frequency of occur-
rence in different database entries (i.e. how uniquely
they describe a matching entry). We use the sec-
ondstring open-source library (Cohen et al, 2003)
for string matching. For any ASR hypothesis, the
NLU is likely to generate several parses which are
all merged in a global list of candidate parses.
For each candidate parse, the system generates
a set of dialog acts (one per concept in the parse)
which are input to the belief tracker with their confi-
dence score. Following the approach described in
section 3, dialog acts corresponding to the Land-
mark concept are sent to the kernel-based location
belief tracker, while all other concepts are sent to a
Dynamic Probabilistic Ontology Trees (DPOT) se-
mantic belief tracker, whose structure is shown in
Figure 4. We use a two-level tree. The value of
the root node (Id) is never directly observed and
represents the database entry targeted by the user.
2This module is implemented using the OpenFST library
(Allauzen et al, 2007)
The leaf nodes correspond to the relevant attributes
Name, Category, and Street. For any database
entry e, attribute a and value of that attribute va, the
conditional probability P (a = va|Id = e) is set to 1
if the value of a is va for entry e in the database, and
to 0 otherwise. For attributes such as Category,
which allow several possible values for each entry,
the probability is split equally among valid values.
After each user utterance, the network is augmented
with a new Evidence Network capturing the possi-
ble interpretations and their likelihood, as computed
by the NLU. The posterior probability distribution
over user goals is computed and rescored using the
kernel-based location tracker.
Finally, the Response Generator takes the highest
scoring target location from the belief tracker and
sends it back to the web client which displays it on
the map and also indicates what are the values of
the Name, Category, and Street concepts for
the top belief (see Figure 5). If the top belief lo-
cation does not match the goal of the scenario, the
user can speak again to refine or correct the system
belief. After the user has spoken 5 utterances, they
also get the choice of moving on to the next scenario
(in which case the dialog is considered a failure).
4.2 Data collection
To evaluate our approach, we ran a data collection
experiment using the Amazon Mechanical Turk on-
line marketplace. We defined 20 scenarios grouped
into 4 Human Intelligence Tasks (HITs). Figure 5
shows a screen shot of the web interface to the sys-
tem. In each scenario, the worker is given a target
location to describe by referring to nearby landmark
information. The target locations were chosen so as
to cover a variety of business categories and nearby
landmarks. The compensation for completing each
set of 5 scenarios is 1 US dollar. Before their first
scenario, workers are shown a video explaining the
goal of the task and how to use the interface, in
which they are specifically encouraged to use land-
marks in their descriptions.
At the beginning of each scenario, the target
location is displayed on the map with a call-
out containing a short description using either a
generic category (e.g. Italian restaurant,
Convenience store) or the name of a chain
store (e.g. Subway, Mcdonalds). The worker
174
Figure 5: Screen capture of the data collection web interface where the target location is an Italian restaurant (in
green, underlying target place is [Ristorante Don Giovanni]) and after the first turn user input ?Italian
restaurant? with a system belief [Frankie, Johnnie & Luigi, Too] in blue returned without any land-
mark information provided so far
then interacts with the system described in section
4.1 until either the system?s top belief matches the
target location, or they decide to skip the scenario.
4.3 Data Statistics
Overall, 99 workers participated in the data col-
lection, providing 948 dialogs (2,869 utterances, 3
turns per scenario on average), which two of the
authors manually transcribed and annotated for di-
alog acts. 76% of the dialogs (46% of utterances)
contained a reference to a landmark. Other strate-
gies commonly used by workers to uniquely identify
a location include using a category or chain name
and a street, as well as explicitly mentioning the tar-
get business name (although workers were explicitly
discouraged form doing so). Figure 7 in appendix
provides one example dialog from the corpus.
Overall, the workers provided 203 unique land-
marks, of which 143 (70%) are in the database.
Workers were able to set the target destination
within 5 turns in 60.1% of the dialogs, which we
hereafter refer to as task successes. However, based
on the manual transcripts, 19.0% of the dialogs
could not have succeeded with the current system
because the workers used landmark or attributes that
do not appear in the database. Since the focus of this
study is robustness rather than coverage, we base our
evaluation on the remaining 768 dialogs, which we
split between a development set of 74 dialogs and
a test set of 694 dialogs. On this test set, the live
system has a task success rate of 70.6%. By inspect-
ing the log files, we noticed that runtime issues such
as timeouts prevented the system from getting any
belief from the belief tracker in 6.3% of the dialogs.
The mean Word Error Rate (WER) per worker on
the test set is 27.5%. There was significant variabil-
ity across workers, with a standard deviation 20.7%.
Besides the usual factors such as acoustic noise and
non-native accents, many of the errors came from
the misrecognition of business names, due to the fact
that ASR uses an open-ended language model that is
tuned neither to Mountain View, nor to businesses,
nor to the kind of utterances that our set up tends
to yield, which is a realistic situation for large scale
practical applications.
Concept precision of the top scoring NLU hypoth-
esis is 73.0% and recall is 57.7%. However, when
considering the full list of NLU hypotheses and us-
ing an oracle to select the best one for each turn,
precision increases to 89.3% and recall to 66.2%,
underscoring the potential of using multiple input
hypotheses in the belief tracker.
175
42% 
50% 
69% 
83% 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
W/o landmarks 
baseline 
W/o landmarks BT W/ landmarks 
baseline 
W/ landmarks BT 
Tas
k S
ucc
ess
 Ra
te 
Figure 6: Batch evaluation of the proposed (BT) and baseline approaches with and without landmark information.
4.4 Batch Results
To further analyze the performance of our approach,
we conducted a series of batch experiments on the
data collected with the runtime system. We first
tuned the parameters of the belief tracker ? and ?l
(see section 3) on the development set (? = 3 and
?l corresponds to a circular Gaussian with standard
deviation 500 meters).
We compare the tuned proposed belief tracking
system (labeled BT) with three other versions. First,
we define a deterministic baseline system which, at
each turn, updates its belief by overwriting each con-
cept?s value with the value found in the top NLU
hypothesis. Based on this (single) user goal hy-
pothesis, we query the database to retrieve match-
ing entries. If the current goal hypothesis con-
tains a Landmark concept, the baseline system se-
lects the matching entry that is closest to any loca-
tion matching the landmark name, by computing the
pairwise distance between candidate target locations
and landmarks.
We also compute the performance of both the
baseline and our proposed approach without us-
ing landmark information at all. In these versions,
the belief over the attributes (Name, Street, and
Category) is updated according to either the top
NLU hypothesis (baseline) or the DPOT model (BT)
and the first matching database entry is returned, ig-
noring any landmark information.
Figure 6 shows the task success of each of the four
versions on the test set. First, it is clear that land-
mark information is critical to complete the tasks in
this corpus since both systems ignoring landmarks
perform significantly worse than their counterparts.
Second, the belief tracking approach significantly
outperforms the deterministic baseline (83.0% vs
69.3%, p < 0.001 using sign test for matched pairs).
To further analyze the performance of the sys-
tem in different input conditions, we split the di-
alogs based on their measured concept accuracy (ex-
pressed in terms of concept F-measure). All dialogs
with an F-measure higher than the median (70.0%)
are labeled as high-accuracy, while the other half of
the data is labeled as low-accuracy. While both the
proposed approach and the baseline perform simi-
larly well for high-accuracy dialogs (task success of
resp. 96.0% and 92.8%, difference is not statisti-
cally significant), the difference is much larger for
low-accuracy dialogs (70.0% vs 45.8%, p < 0.001)
confirming the robustness of the landmark-based be-
lief tracking approach when confronted with poor
input conditions.
5 Conclusion
In this paper, we have explored the possibilities of
incorporating spatial information into belief tracking
in spoken dialog systems. We proposed a landmark-
based location tracker which can be combined with
a semantic belief tracker to output inferred joint user
goal. Based on the results obtained from our batch
experiments, we conclude that integrating spatial in-
formation into a location-based dialog system could
improve the overall accuracy of belief tracking sig-
nificantly.
176
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Ninth International
Conference on Implementation and Application of Au-
tomata (CIAA) Lecture Notes in Computer Science,
volume 4783, pages 11?23. Springer.
W.W. Cohen, P. Ravikumar, and S.E. Fienberg. 2003.
A comparison of string distance metrics for name-
matching tasks. In Proceedings of the IJCAI-2003
Workshop on Information Integration on the Web
(IIWeb-03), pages 73?78.
Alexander Gruenstein and Stephanie Seneff. 2007. Re-
leasing a multimodal dialogue system into the wild:
User support mechanisms. In In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
pages 111?119, September.
A. Gruenstein, I. McGraw, and I. Badr. 2008. The wami
toolkit for developing, deploying, and evaluating web-
accessible multimodal interfaces. In Proceedings of
the 10th international conference on Multimodal in-
terfaces, pages 141?148. ACM.
Albert Huang, Stefanie Tellex, Abe Bachrach, Thomas
Kollar, Deb Roy, and Nick Roy. 2010. Natural lan-
guage command of an autonomous micro-air vehicle.
In International Conference on Intelligent Robots and
Systems (IROS).
M. MacMahon, B. Stankiewicz, and B. Kuipers. 2006.
Walk the talk: Connecting language, knowledge, and
action in route instructions. In Proceedings of the
National Conference on Artificial Intelligence, vol-
ume 21, page 1475. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999.
N. Mehta, R. Gupta, A. Raux, D. Ramachandran, and
S. Krawczyk. 2010. Probabilistic ontology trees for
belief tracking in dialog systems. In Proceedings of
the 11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 37?46. Association
for Computational Linguistics.
Michel Denis Pierre-emmanuel Michon. 2001. When
and why are visual landmarks used in giving direc-
tions? In D. R. Montello, editor, Spatial Information
Theory, Volume 2205 of Lecture Notes in Computer
Science, pages 292?305. Springer, Berlin.
A. Raux and Y. Ma. 2011. Efficient probabilistic track-
ing of user goal and dialog history for spoken dialog
systems. In Proceedings of Interspeech 2011.
Michael L. Seltzer, Yun-Cheng Ju, Ivan Tashev, and Alex
Acero. 2007. Robust location understanding in spo-
ken dialog systems using intersections. In Proceed-
ings of Interspeech 2007, pages 2813?2816.
K. Suzuki and Y. Wakabayashi. 2005. Cultural dif-
ferences of spatial descriptions in tourist guidebooks.
Spatial Cognition IV. Reasoning, Action, and Interac-
tion, 3343:147?164.
B. Thomson and S. Young. 2010. Bayesian update of di-
alogue state: A pomdp framework for spoken dialogue
systems. Computer Speech & Language, 24(4):562?
588.
Ariane Tom and Michel Denis. 2003. Referring to land-
mark or street information in route directions: What
difference does it make? Spatial Information The-
ory. Foundations of Geoghraphic Information Science,
Lecture Notes in Computer Science, 2825/2003:362?
374.
Barbara Tversky and Paul U. Lee. 1999. Pictorial and
verbal tools for conveying routes. In Proceedings of
the International Conference on Spatial Information
Theory: Cognitive and Computational Foundations
of Geographic Information Science(COSIT). Springer-
Verlag London.
177
User: &Italian&restaurant&near&
ASR: %italian%restaurant%near%
NLU: %Category=Italian%Restaurant%
Category %Italian'Restaurant'
Target !Dominos!Pizza!
Category %Italian'Restaurant'
Target !Dominos!Pizza!
User: &Italian&restaurant&near&Kappo&Nami&Nami&
ASR: %italian%restaurant%near%camp%to%numa%numa%
NLU: %Category=Italian%Restaurant,%Street=Camp%Avenue%
%Category=Italian%Restaurant,%Landmark=Jefunira%Camp%
Category %Italian'Restaurant%%
Street %Camp'Avenue'
Target &No!match!
Category %Italian'Restaurant'
Landmark %Jefunira'Camp'
Target ! !Maldonado?s%
User: &Italian&restaurant&near&Tempta5ons&
ASR: %italian%restaurant%near%temptaAons%
NLU: %Category=Italian%Restaurant,%Landmark=TemptaAons%
Category %Italian'Restaurant'
Street %Camp'Avenue'
Landmark %Tempta5ons'
Target &No!match!
Category %Italian'Restaurant'
Landmark %Jefunira'Camp,'Tempta5ons'
Target &Don!Giovanni!
Baseline' DPOT+Kernels'
Example&Dialog
Figure 7: Comparison between baseline and proposed method on an example dialog whose underlying true target is
an Italian restaurant called Don Giovanni.
178
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 58?64,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Comparing human versus automatic feature extraction for fine-grained
elementary readability assessment
Yi Ma, Ritu Singh, Eric Fosler-Lussier
Dept. of Computer Science & Engineering
The Ohio State University
Columbus, OH 43210, USA
may,singhri,fosler@cse.ohio-state.edu
Robert Lofthus
Xerox Corporation
Rochester, NY 14604, USA
Robert.Lofthus@xerox.com
Abstract
Early primary children?s literature poses some
interesting challenges for automated readabil-
ity assessment: for example, teachers often
use fine-grained reading leveling systems for
determining appropriate books for children to
read (many current systems approach read-
ability assessment at a coarser whole grade
level). In previous work (Ma et al, 2012),
we suggested that the fine-grained assess-
ment task can be approached using a ranking
methodology, and incorporating features that
correspond to the visual layout of the page
improves performance. However, the previ-
ous methodology for using ?found? text (e.g.,
scanning in a book from the library) requires
human annotation of the text regions and cor-
rection of the OCR text. In this work, we ask
whether the annotation process can be auto-
mated, and also experiment with richer syntac-
tic features found in the literature that can be
automatically derived from either the human-
corrected or raw OCR text. We find that auto-
mated visual and text feature extraction work
reasonably well and can allow for scaling to
larger datasets, but that in our particular exper-
iments the use of syntactic features adds little
to the performance of the system, contrary to
previous findings.
1 Introduction
Knowing the reading level of a children?s book
is an important task in the educational setting.
Teachers want to have leveling for books in the
school library; parents are trying to select appro-
priate books for their children; writers need guid-
ance while writing for different literacy needs (e.g.
text simplification)?reading level assessment is re-
quired in a variety of contexts. The history of as-
sessing readability using simple arithmetic metrics
dates back to the 1920s when Thorndike (1921) has
measured difficulty of texts by tabulating words ac-
cording to the frequency of their use in general lit-
erature. Most of the traditional readability formulas
were also based on countable features of text, such
as syllable counts (Flesch, 1948).
More advanced machine learning techniques such
as classification and regression have been applied
to the task of reading level prediction (Collins-
Thompson and Callan, 2004; Schwarm and Osten-
dorf, 2005; Petersen and Ostendorf, 2009; Feng et
al., 2010); such works are described in further de-
tail in the next Section 2. In recent work (Ma et al,
2012), we approached the problem of fine-grained
leveling of books, demonstrating that a ranking ap-
proach to predicting reading level outperforms both
classification and regression approaches in that do-
main. A further finding was that visually-oriented
features that consider the visual layout of the page
(e.g. number of text lines per annotated text region,
text region area compared to the whole page area
and font size etc.) play an important role in predict-
ing the reading levels of children?s books in which
pictures and textual layout dominate the book con-
tent over text.
However, the data preparation process in our pre-
vious study involves human intervention?we ask
human annotators to draw rectangle markups around
text region over pages. Moreover, we only use a
very shallow surface level text-based feature set to
58
compare with the visually-oriented features. Hence
in this paper, we assess the effect of using com-
pletely automated annotation processing within the
same framework. We are interested in exploring
how much performance will change by completely
eliminating manual intervention. At the same time,
we have also extended our previous feature set by in-
troducing a richer set of automatically derived text-
based features, proposed by Feng et al (2010),
which capture deeper syntactic complexities of the
text. Unlike our previous work, the major goal of
this paper is not trying to compare different machine
learning techniques used in readability assessment
task, but rather to compare the performance differ-
ences between with and without human labor in-
volved within our previous proposed system frame-
work.
We begin the paper with the description of re-
lated work in Section 2, followed by detailed ex-
planation regarding data preparation and automatic
annotations in Section 3. The extended features will
be covered in Section 4, followed by experimental
analysis in Section 5, in which we will compare the
results between human annotations and automatic
annotations. We will also report the system per-
formance after incorporating the rich text features
(structural features). Conclusions follow in Section
6.
2 Related Work
Since 1920, approximately 200 readability formulas
have been reported in the literature (DuBay, 2004);
statistical language processing techniques have re-
cently entered into the fray for readability assess-
ment. Si and Callan (2001) and Collins-Thompson
and Callan (2004) have demonstrated the use of lan-
guage models is more robust for web documents
and passages. Heilman et al (2007) studied the
impact of grammar-based features combined with
language modeling approach for readability assess-
ment of first and second language texts. They ar-
gued that grammar-based features are more perti-
nent for second language learners than for the first
language readers. Schwarm and Ostendorf (2005)
and Petersen and Ostendorf (2009) both used a sup-
port vector machine to classify texts based on the
reading level. They combined traditional methods
of readability assessment and the features from lan-
guage models and parsers. Aluisio et al (2010)
have developed a tool for text simplification for the
authoring process which addresses lexical and syn-
tactic phenomena to make text readable but their as-
sessment takes place at more coarse levels of liter-
acy instead of finer-grained levels used for children?s
books.
A detailed analysis of various features for auto-
matic readability assessment has been done by Feng
et al (2010). Most of the previous work has used
web page documents, short passages or articles from
educational newspapers as their datasets; typically
the task is to assess reading level at a whole-grade
level. In contrast, early primary children?s literature
is typically leveled in a more fine-grained manner,
and the research question we pursued in our previ-
ous study was to investigate appropriate methods of
predicting what we suspected was a non-linear read-
ing level scale.
Automating the process of readability assessment
is crucial for eventual widespread acceptance. Pre-
vious studies have looked at documents that were
already found in electronic form, such as web texts.
While e-books are certainly on the rise (and would
help automated processing) it is unlikely that paper
books will be completely eliminated from the pri-
mary school classroom soon. Our previous study re-
quired both manual scanning of the books and man-
ual annotation of the books to extract the location
and content of text within the book ? the necessity
of which we evaluate in this study by examining the
effects of errors from the digitization process.
3 Data Preparation and Book Annotation
Our previous study was based on a corpus of 36
scanned children?s books; in this study we have ex-
panded the set to 97 books which range from lev-
els A to N in Fountas and Pinnell Benchmark As-
sessment System 1 (Fountas and Pinnell, 2010); the
Fountas and Pinnell level serves as our gold stan-
dard. The distribution of number of books per read-
ing level is shown in Table 1. Levels A to N,
in increasing difficulty, corresponds to the primary
grade books from roughly kindergarten through
third grade. The collection of children?s books cov-
ers a large diversity of genres, series and publishers.
59
Reading # of Reading # of
Level Books Level Books
A 6 H 7
B 9 I 6
C 5 J 11
D 8 K 6
E 11 L 3
F 10 M 6
G 7 N 2
Table 1: Distribution of books over Fountas and Pinnell
reading levels
Our agreement with the books? publishers only
allows access to physical copies of books rather
than electronic versions; we scan each book into
a PDF version. This situation would be similar to
that of a contemporary classroom teacher who is se-
lecting books from the classroom or school library
for evaluating a child?s literacy progress.1 We then
use Adobe Acrobat to run OCR (Optical Character
Recognition) on the PDF books. Following our pre-
vious work, we first begin our process of annotat-
ing each book using Adobe Acrobat before convert-
ing them into corresponding XML files. Features
for each book are extracted from their correspond-
ing XMLs which contain all the text information and
book layout contents necessary to calculate the fea-
tures. Each book is manually scanned, and then an-
notated in two different ways: we use human anno-
tators (Section 3.1) and a completely automated pro-
cess (Section 3.2). The job of human annotators is
primarily to eliminate the errors made by OCR soft-
ware, as well as correctly identifying text regions on
each page. We encountered three types of typical
OCR errors for the children?s books in our set:
1. False alarms: some small illustration picture
segments (e.g. flower patterns on a little girl?s
pajama or grass growing in bunches on the
ground) are recognized as text.
2. False negatives: this is more likely to occur for
text on irregular background such as white text
1While it is clear that publishers will be moving toward elec-
tronic books which would avoid the process of scanning (and
likely corresponding OCR problems), it is also clear that phys-
ical books and documents will be present in the classroom for
years to come.
OCR Correct Example
output word
1 I 1 ? I
! I ! ? I
[ f [or ? for
O 0 1OO ? 100
nn rm wann ? warm
rn m horne ? home
IT! m aIT! ? am
1n m tilne ? time
n1. m n1.y ? my
1V W 1Ve ? We
vv w vvhen ? when
Table 2: Some common OCR errors
on black background or text overlapped with
illustrations.
3. OCR could misread the text. These are most
common errors. Some examples of this type of
error are shown in Table 2.
The two different annotation processes are explained
in the following Subsections 3.1 and 3.2.
3.1 Human Annotation
Annotators manually draw a rectangular box over
the text region on each page using Adobe Acrobat
markup drawing tools. The annotators also correct
the type 2 and 3 of OCR errors which are mentioned
above. In human annotation process, the false alarm
(type 1) errors are implicitly prevented since the an-
notators will only annotate the regions where text
truly exists on the page (no matter whether the OCR
recognized or not).
3.2 Automatic Annotation
For automatic annotation, we make use of JavaScript
API provided by Adobe Acrobat. The automatic an-
notation tool is implemented as a JavaScript plugin
menu item within Adobe Acrobat. The JavaScript
API can return the position of every single recog-
nized word on the page. Based on the position cues
of each word, we design a simple algorithm to auto-
matically cluster the words into separate groups ac-
cording to certain spatial distance thresholds.2 In-
2A distance threshold of 22 pixels was used in practice.
60
tuitively, one could imagine the words as small
floating soap bubbles on the page?where smaller
bubbles (individual words) which are close enough
will merge together to form bigger bubbles (text re-
gions) automatically. For each detected text region,
a bounding rectangle box annotation is drawn on
the page automatically. Beyond this point, the rest
of the data preparation process is identical to hu-
man annotation, in which the corresponding XMLs
will be generated from the annotated versions of
the PDF books. However, unlike human annota-
tion, automating the annotation process can intro-
duce noise into the data due to uncorrected OCR er-
rors. In correspondence to the three types of OCR
errors, automatic annotation could also draw extra
bounding rectangle boxes on non-text region (where
OCR thinks there is text there but there is not), fails
to draw bounding rectangle boxes on text region
(where OCR should have recognized text there but
it does not) and accepts many mis-recognized non-
word symbols as text content (where OCR misreads
words).
3.3 Generating XMLs From Annotated PDF
Books
This process is also implemented as another
JavaScript plugin menu item within Adobe Acrobat.
The plugin is run on the annotated PDFs and is de-
signed to be agnostic to the annotation types?it will
work on both human-annotated and auto-annotated
versions of PDFs. Once the XMLs for each chil-
dren?s book are generated, we could proceed to the
feature extraction step. The set of features we use in
the experiments are described in the following Sec-
tion 4.
4 Features
For surface-level features and visual features, we
utilize similar features proposed in our previous
study.3 For completeness? sake, we list these two
sets of features as follows in Section 4.1:
3We discard two visual features in both the human and au-
tomatic annotation that require the annotation of the location
of images on the page, as these were features that the Adobe
Acrobat JavaScript API could not directly access.
4.1 Surface-level Features and
Visually-oriented Features
? Surface-level Features
1. Number of words
2. Number of letters per word
3. Number of sentences
4. Average sentence length
5. Type-token ratio of the text content.
? Visually-oriented Features
1. Page count
2. Number of words per page
3. Number of sentences per page
4. Number of text lines per page
5. Number of words per text line
6. Number of words per annotated text rect-
angle
7. Number of text lines per annotated text
rectangle
8. Average ratio of annotated text rectangle
area to page area
9. Average font size
4.2 Structural Features
Since our previous work only uses surface level of
text features, we are interested in investigating the
contribution of high-level structural features to the
current system. Feng et al (2010) found several
parsing-based features and part-of-speech based fea-
tures to be useful. We utilize the Stanford Parser
(Klein and Manning, 2003) to extract the following
features from the XML files based on those used in
(Feng et al, 2010):
? Parsed Syntactic Features for NPs and VPs
1. Number of the NPs/VPs
2. Number of NPs/VPs per sentence
3. Average NP/VP length measured by num-
ber of words
4. Number of non-terminal nodes per parse
tree
5. Number of non-terminal ancestors per
word in NPs/VPs
? POS-based Features
61
1. Fraction of tokens labeled as
noun/preposition
2. Fraction of types labeled as
noun/preposition
3. Number of noun/preposition tokens per
sentence
4. Number of noun/preposition types per
sentence
5 Experiments
In the experiments, we look at how much the perfor-
mance dropped by switching to zero human inputs.
We also investigate the impact of using a richer set
of text-based features. We apply the ranking-based
book leveling algorithm proposed by our previous
study (Ma et al, 2012) and use the SVMrank ranker
(Joachims, 2006) for our experiments. In this sys-
tem, the ranker learns to sort the training books into
leveled order. The unknown test book is inserted
into the ordering of the training books by the trained
ranking model, and the predicted reading level is
calculated by averaging over the levels of the known
books above and below the test book. Following the
previous study, each book is uniformly partitioned
into 4 parts, treating each sub-book as an individ-
ual entity. A leave-n-out procedure is utilized for
evaluation: during each iteration of the training, the
system leaves out all n partitions (sub-books) cor-
responding to one book. In the testing phase, the
trained ranking model tests on all partitions corre-
sponding to the held-out book. We obtain a single
predicted reading level for the held-out book by av-
eraging the results for all its partitions; averaging
produces a more robust result. Two separate experi-
ments are carried out on human-annotated and auto-
annotated PDF books respectively.
We use two metrics to determine quality: first, the
accuracy of the system is computed by claiming it
is correct if the predicted book level is within ?1 of
the true reading level.4 The second scoring metric is
the absolute error of number of levels away from the
key reading level, averaged over all of the books.
4We follow our previous study to use ?1 accuracy evalu-
ation metric in order to generate consistent results and allow
easy comparison. Another thing to notice is that this is still
rather fine-grained since multiple reading levels correspond to
one single grade level.
We report the experiment results on different
combinations of feature sets: surface level features
plus visually-oriented features, surface level features
only, visually-oriented features only, structural fea-
tures only and finally combining all the features to-
gether.
5.1 Human Annotation vs. Automatic
Annotation
As we can observe from Table 3,5 overall the human
annotation gives higher accuracy than automatic an-
notation across different feature sets. The perfor-
mance difference between human annotation and au-
tomatic annotation could be attributed to the OCR
errors (described in Section 3.2) which are intro-
duced in the automatic annotation process. How-
ever, to our surprise, the best performance of human
annotation is not significantly better than automatic
annotation even at p < 0.1 level (figures in bold).6
Only for the experiment using all features does hu-
man annotation outperform the automatic annota-
tion at p < 0.1 level (still not significantly better
at p < 0.05 level, figures with asterisks). There-
fore, we believe that the extra labor involved in the
annotation step could be replaced by the automatic
process without leading to a significant performance
drop. While the process does still require manual
scanning of each book (which can be time consum-
ing depending on the kind of scanner), the automatic
processing can reduce the labor per book from ap-
proximately twenty minutes per book to just a few
seconds.
5.2 Incorporating Structural Features
Our previous study demonstrated that combin-
ing surface features with visual features produces
promising results. As mentioned above, the sec-
ond aim of this study is to see how much benefit
we can get from incorporating high-level structural
features, such as those used in (Feng et al, 2010)
(described in Section 4.2), with the features in our
previous study.
Table 3 shows that for both human and automatic
5In three of the books, the OCR completely failed; thus only
94 books are available for evaluation of the automatic annota-
tion.
6One-tailed Z-test was used with each book taken as an in-
dependent sample.
62
Annotation type Human Automatic
?1 Accuracy %
Surface+Visual features 76.3 70.2
Surface level features 69.1 64.9
Visual features 63.9 58.5
Structural features 63.9 58.5
All features 76.3? 66.0?
Average leveling error ? standard deviation
Surface+Visual features 0.99 ? 0.87 1.16 ? 0.83
Surface level features 1.24 ? 1.05 1.16 ? 0.97
Visual features 1.24 ? 1.00 1.37 ? 0.89
Structural features 1.30 ? 0.89 1.33 ? 0.91
All features 1.05 ? 0.78 1.15 ? 0.90
Table 3: Results on 97 books using human annotations vs. automatic annotations, reporting accuracy within one level
and average error for 4 partitions per book.
annotation under the ?1 accuracy metric, the vi-
sual features and the structural features have the
same performance, whose accuracy are both slightly
lower than that of surface level features. By combin-
ing the surface level features with the visual features,
the system obtains the best performance. How-
ever, by combining all three feature sets, the sys-
tem performance does not change for human annota-
tion whereas it hurts the performance for automatic
annotation?it is likely that the OCR errors existing
in the automatic annotations give rise to erroneous
structural features (e.g. the parser would produce
less robust parses for sentences which have out of
vocabulary words). Overall, we did not observe bet-
ter performance by incorporating structural features.
Using structural features on their own also did not
produce noteworthy results. Although among the
three kinds of features (surface, visual and struc-
tural), structural features have the highest computa-
tional cost, it exhibits no significant improvement to
system results. In the average leveling error metric,
the best performance is again obtained at the com-
bination of surface level features and visual features
for human annotation, whereas the performance re-
mains almost the same after incorporating structural
features for automatic annotation.
6 Conclusion
In this paper, we explore the possibility of reducing
human involvement in the specific task of predicting
reading levels of scanned children?s books by elimi-
nating the need for human annotation. Clearly there
is a trade off between the amount of human labor
involved and the accuracy of the reading level pre-
dicted. Based on the experimental results, we did
not observe significant performance drop by switch-
ing from human annotation to automatic annotation
in the task of predicting reading levels for scanned
children?s books.
We also study the effect of incorporating struc-
tural features into the proposed ranking system. The
experimental results showed that structural features
exhibit no significant effect to the system perfor-
mance. We conclude for the simply structured, short
text that appears in most children?s books, a deep
level analysis of the text properties may be overkill
for the task and produced unsatisfactory results at a
high computational cost for our task.
In the future, we are interested in investigating the
importance of each individual feature as well as ap-
plying various feature selection methods to further
improve the overall performance of the system?in
the hope that making the ranking system more ro-
bust to OCR errors introduced by automatic annota-
tion processing. Another interesting open question
is that how many scanned book pages are needed to
make a good prediction.7 Such analysis would be
very helpful for practical purposes, since a teacher
7We thank an anonymous reviewer of the paper for this sug-
gestion.
63
could just scan few sample pages instead of a full
book for a reliable prediction.
References
S. Aluisio, L. Specia, C. Gasperin, and C. Scarton. 2010.
Readability assessment for text simplification. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 1?9. Association for Computational
Linguistics.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT / NAACL 2004, volume 4, pages
193?200, Boston, USA.
W.H. DuBay. 2004. The principles of readability. Im-
pact Information, pages 1?76.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A comparison of features for automatic read-
ability assessment. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 276?284, Beijing, China. As-
sociation for Computational Linguistics.
R. Flesch. 1948. A new readability yardstick. Journal of
applied psychology, 32(3):221?233.
I. Fountas and G. Pinnell. 2010. Fountas
and pinnell benchmark assessment system 1.
http://www.heinemann.com/products/E02776.aspx.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammatical
features to improve readability measures for first and
second language texts. In Proceedings of NAACL
HLT, pages 460?467.
T. Joachims. 2006. Training linear SVMs in linear time.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 217?226. ACM.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Meeting of
the Association for Computational Linguistics, pages
423?430.
Y. Ma, E. Fosler-Lussier, and R. Lofthus. 2012.
Ranking-based readability assessment for early pri-
mary children?s literature. In Proceedings of NAACL
HLT.
S. Petersen and M. Ostendorf. 2009. A machine learn-
ing approach to reading level assessment. Computer
Speech & Language, 23(1):89?106.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 523?530. Association for Computational
Linguistics.
L. Si and J. Callan. 2001. A statistical model for scien-
tific readability. In Proceedings of the tenth interna-
tional conference on Information and knowledge man-
agement, pages 574?576. ACM.
E.L. Thorndike. 1921. The teacher?s word book, volume
134. Teachers College, Columbia University New
York.
64
