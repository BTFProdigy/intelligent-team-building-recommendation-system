Generating statistical language models from interpretation grammars in
dialogue systems
Rebecca Jonson
Dept. of Linguistics, Go?teborg University and GSLT
rj@ling.gu.se
Abstract
In this paper, we explore statistical lan-
guage modelling for a speech-enabled
MP3 player application by generating a
corpus from the interpretation grammar
written for the application with the Gram-
matical Framework (GF) (Ranta, 2004).
We create a statistical language model
(SLM) directly from our interpretation
grammar and compare recognition per-
formance of this model against a speech
recognition grammar compiled from the
same GF interpretation grammar. The
results show a relative Word Error Rate
(WER) reduction of 37% for the SLM
derived from the interpretation grammar
while maintaining a low in-grammarWER
comparable to that associated with the
speech recognition grammar. From this
starting point we try to improve our arti-
ficially generated model by interpolating
it with different corpora achieving great
reduction in perplexity and 8% relative
recognition improvement.
1 Introduction
Ideally when building spoken dialogue systems,
we would like to use a corpus of transcribed di-
alogues corresponding to the specific task of the
dialogue system, in order to build a statistical lan-
guage model (SLM). However, it is rarely the case
that such a corpus exists in the early stage of
the development of a dialogue system. Collect-
ing such a corpus and transcribing it is very time-
consuming and delays the building of the actual
dialogue system.
An approach taken both in dialogue systems
and dictation applications is to first write an in-
terpretation grammar and from that generate an
artificial corpus which is used as training corpus
for the SLM (Raux et al 2003; Pakhomov et al
2001; Fosler-Lussier & Kuo, 2001). These mod-
els obtained from grammars are not as good as the
ones built from real data as the estimates are arti-
ficial, lacking a real distribution. However, it is a
quick way to get a dialogue system working with
an SLM. When the system is up and running it
is possible to collect real data that can be used to
improve the model. We will explore this idea by
generating a corpus from an interpretation gram-
mar from one of our applications.
A different approach is to compile the interpre-
tation grammar into a speech recognition gram-
mar as the Gemini and REGULUS compilers do
(Rayner et al 2000; Rayner et al 2003). In this
way it is assured that the linguistic coverage of the
speech recognition and interpretation are kept in
sync. Such an approach enables us to interpret al
that we can recognize and the other way round. In
the European-funded project TALK the Grammat-
ical Framework (Ranta, 2005) has been extended
with such a facility that compiles GF grammars
into speech recognition grammars in Nuance GSL
format (www.nuance.com).
Speech recognition for commercial dialogue
systems has focused on grammar-based ap-
proaches despite the fact that statistical language
models seem to have a better overall performance
(Gorrell et al 2002). This probably depends on
the time-consuming work of collecting corpora for
training SLMs compared with the more rapid and
straightforward development of speech recogni-
tion grammars. However, SLMs are more robust,
can handle out-of-coverage output, perform bet-
ter in difficult conditions and seem to work bet-
57
ter for naive users (see (Knight et al 2001)) while
speech recognition grammars are limited in their
coverage depending on how well grammar writers
succeed in predicting what users may say (Huang
et al 2001).
Nevertheless, as grammars only output phrases
that can be interpreted their output makes the fol-
lowing interpretation task easier than with the un-
predictable output from an SLM (especially if the
speech recognition grammar has been compiled
from the interpretation grammar and these are both
in sync). In addition, the grammar-based approach
in the experiments reported in (Knight et al 2001)
outperforms the SLM approach on semantic error
rate on in-coverage data. This has lead to the idea
of trying to combine both approaches, as shown in
(Rayner & Hockey, 2003). This is also something
that we are aiming for.
Domain adaptation of SLMs is another issue
in dialogue system recognition which involves re-
using a successful language model by adapting it
to a new domain i.e. a new application (Janiszek et
al, 1998). If a large corpus is not available for the
specific domain but there is a corpus for a collec-
tion of topics we could use this corpus and adapt
the resulting SLM to the domain. One may as-
sume that the resulting SLM based on a large cor-
pus with a good mixture of topics should be able to
capture at least a part of general language use that
does not vary from one domain to another. We will
explore this idea by using the Gothenburg Spoken
Language Corpus (GSLC) (Allwood, 1999) and a
newspaper corpus to adapt these to our MP3 do-
main.
We will consider several different SLMs based
on the corpus generated from the GF interpreta-
tion grammar and compare their recognition per-
formance with the baseline: a Speech Recogni-
tion Grammar in Nuance format compiled from
the same interpretation grammar. Hence, what we
could expect from our experiment, by looking at
earlier research, is very low word error rate for
our speech recognition grammar on in-grammar
coverage but a lot worse performance on out-of-
grammar coverage. The SLMs we are consider-
ing should tackle out-of-grammar utterances bet-
ter and it will be interesting to see how well these
models built from the grammar will perform on
in-grammar utterances.
This study is organized as follows. Section 2
introduces the domain for which we are doing
language modelling and the corpora we have at
our disposal. Section 3 will describe the different
SLMs we have generated. Section 4 describes the
evaluation of these and the results. Finally, we re-
view the main conclusions of the work and discuss
future work.
2 Description of Corpora
2.1 The MP3 corpus
The domain that we are considering in this pa-
per is the domain of an MP3 player application.
The talking MP3 player, DJGoDiS, is one of sev-
eral applications that are under development in the
TALK project. It has been built with the TrindiKit
toolkit (Larsson et al 2002) and the GoDiS dia-
logue system (Larsson, 2002) as a GoDiS appli-
cation and works as a voice interface to a graphi-
cal MP3 player. The user can among other things
change settings, choose stations or songs to play
and create playlists. The current version of DJ-
GoDiS works in both English and Swedish.
The interpretation and generation grammars are
written with the GF grammar formalism. GF is
being further developed in the project to adapt
it to the use in spoken dialogue systems. This
adaptation includes the facility of generating Nu-
ance recognition grammars from the interpretation
grammar and the possibility of generating corpora
from the grammars. The interpretation grammar
for the domain, written in GF, translates user utter-
ances to dialogue moves and thereby holds all pos-
sible interpretations of user utterances (Ljunglo?f
et al 2005). We used GF?s facilities to generate a
corpus in Swedish consisting of all possible mean-
ingful utterances generated by the grammar to a
certain depth of the analysis trees in GF?s abstract
syntax as explained in (Weilhammer et al 2006).
As the current grammar is under development it
is not complete and some linguistic structures are
missing. The grammar is written on the phrase
level accepting spoken language utterances such
as e.g. ?next, please?.
The corpus of possible user utterances resulted
in around 320 000 user utterances (about 3 mil-
lion words) corresponding to a vocabulary of only
301 words. The database of songs and artists in
this first version of the application is limited to
60 Swedish songs, 60 Swedish artists, 3 albums
and 3 radio stations. The vocabulary may seem
small if you consider the number of songs and
artists included, but the small size is due to a huge
58
overlap of words in songs and artists as pronouns
(such as Jag (I) and Du (You)) and articles (such as
Det (The)) are very common. This corpus is very
domain specific as it includes many artist names,
songs and radio stations that often consist of rare
words. It is also very repetitive covering all com-
binations of songs and artists in utterances such as
?I want to listen to Mamma Mia with Abba?. All
utterances in the corpus occur exactly once.
2.2 The GSLC corpus
The Gothenburg Spoken Language (GSLC) cor-
pus consists of transcribed Swedish spoken lan-
guage from different social activities such as auc-
tions, phone calls, meetings, lectures and task-
oriented dialogue (Allwood, 1999). To be able
to use the GSLC corpus for language modelling
it was pre-processed to remove annotations and all
non-alphabetic characters. The final GSLC corpus
consisted of a corpus of about 1,300,000 words
with a vocabulary of almost 50,000 words.
2.3 The newspaper corpus
We have also used a corpus consisting of a col-
lection of Swedish newspaper texts of 397 million
words.1 Additionally, we have created a subcor-
pus of the newspaper corpus by extracting only the
sentences including domain related words. With
domain related words we mean typical words for
an MP3 domain such as ?music?, ?mp3-player?,
?song? etc. This domain vocabulary was hand-
crafted. The domain-adapted newspaper corpus,
obtained by selecting sentences where these words
occurred, consisted of about 15 million words i.e.
4% of the larger corpus.
2.4 The Test Corpus
To collect a test set we asked students to describe
how they would address a speech-enabled MP3
player by writing Nuance grammars that would
cover the domain and its functionality. Another
group of students evaluated these grammars by
recording utterances they thought they would say
to an MP3 player. One of the Nuance grammars
was used to create a development test set by gen-
erating a corpus of 1500 utterances from it. The
corpus generated from another grammar written
by some other students was used as evaluation
test set. Added to the evaluation test set were the
transcriptions of the recordings made by the third
1This corpus was made available by Leif Gro?nqvist, Dept.
of Linguistics, Go?teborg University
group of students that evaluated both grammars.
This resulted in a evaluation test set of 1700 utter-
ances.
The recording test set was made up partly of the
students? recordings. Additional recordings were
carried out by letting people at our lab record ran-
domly chosen utterances from the evaluation test
set. We also had a demo running for a short time to
collect user interactions at a demo session. The fi-
nal test set included 500 recorded utterances from
26 persons. This test set has been used to com-
pare recognition performance between the differ-
ent models under consideration.
The recording test set is just an approximation
to the real task and conditions as the students only
capture how they think they would act in an MP3
task. Their actual interaction in a real dialogue
situation may differ considerably so ideally, we
would want more recordings from dialogue sys-
tem interactions which at the moment constitutes
only a fifth of the test set. However, until we can
collect more recordings we will have to rely on
this approximation.
In addition to the recorded evaluation test set,
a second set of recordings was created covering
only in-grammar utterances by randomly generat-
ing a test set of 300 utterances from the GF gram-
mar. These were recorded by 8 persons. This test
set was used to contrast with a comparison of in-
grammar recognition performance.
3 Language modelling
To generate the different trigram language models
we used the SRI language modelling toolkit (Stol-
cke, 2002) with Good-Turing discounting.
The first model was generated directly from the
MP3 corpus we got from the GF grammar. This
simple SLM (named MP3GFLM) has the same vo-
cabulary as the Nuance Grammar and models the
same language as the GF grammar. This model
was chosen to see if we could increase flexibility
and robustness in such a simple way while main-
taining in-grammar performance.
We also created two other simple SLMs: a
class-based one (with the classes Song, Artist
and Radiostation) and a model based on a
variant of the MP3 corpus where the utterances
in which songs and artists co-occur would only
match real artist-song pairs (i.e. including some
music knowledge in the model).
These three SLMs were the three basic MP3
59
models considered although we only report the re-
sults for the MP3GFLM in this article (the class-
based model gave a slightly worse result and the a
other slightly better result).
In addition to this we used our general corpora
to produce three different models: GSLCLM from
the GSLC corpus, NewsLM from the newspaper
corpus and DomNewsLM from the domain adapted
newspaper Corpus.
3.1 Interpolating the GSLC corpus and the
MP3 corpus
A technique used in language modelling to com-
bine different SLMs is linear interpolation (Jelinek
& Mercer, 1980). This is often used when the do-
main corpus is too small and a bigger corpus is
available. There have been many attempts at com-
bining domain corpora with news corpora, as this
has been the biggest type of corpus available and
this has given slightly better models (Janiszek et
al, 1998; Rosenfeld, 2000a). Linear interpolation
has also been used when building state dependent
models by combining the state models with a gen-
eral domain model (Xu & Rudnicky, 2000; Sol-
sona et al 2002).
Rosenfeld (Rosenfeld, 2000a) argues that a lit-
tle more domain corpus is always better than a lot
more training data outside the domain. Many of
these interpolation experiments have been carried
out by adding news text, i.e. written language. In
this experiment we are going to interpolate our do-
main model (MP3GFLM) with a spoken language
corpus, the GSLC, to see if this improves perplex-
ity and recognition rates. As the MP3 corpus is
generated from a grammar without probabilities
this is hopefully a way to obtain better and more
realistic estimates on words and word sequences.
Ideally, what we would like to capture from the
GSLC corpus is language that is invariant from
domain to domain. However, Rosenfeld (Rosen-
feld, 2000b) is quite pessimistic about this, argu-
ing that this is not possible with today?s interpo-
lation methods. The GSLC corpus is also quite
small.
The interpolation was carried out with the
SRILM toolkit2 based on equation 1.
MixGSLCMP 3GF = ? ? MP 3GFLM +(1 ? ?) ? GSLCLM
(1)
The optimal lambda weight was estimated to
0.65 with the SRILM toolkit using the develop-
ment test set.
2http://www.speech.sri.com/projects/srilm, as of 2005.
3.2 Interpolating the newspaper corpus and
the MP3 corpus
We also created two models in the same way as
above by interpolating the two variants of the news
corpus with our simplest model.
MixNewsMP 3GF = ? ? MP 3GFLM + (1 ? ?) ? NewsLM
(2)
MixDomNewsMP 3GF = ??MP 3GFLM+(1??)?DomNewsLM
(3)
In addition to these models we created a model
where we interpolated both the GSLC model
and the domain adapted newspaper model with
MP3GFLM. This model was named TripleLM.
3.2.1 Choice of vocabulary
The resulting mixed models have a huge vocab-
ulary as the GSLC corpus and the newspaper cor-
pus include thousands of words. This is not a con-
venient size for recognition as it will affect accu-
racy and speed. Therefore we tried to find an opti-
mal vocabulary combining the small MP3 vocabu-
lary of around 300 words with a smaller part of the
GSLC vocabulary and the newspaper vocabulary.
We used the the CMU toolkit (Clarkson &
Rosenfeld, 1997) to obtain the most frequent
words of the GSLC corpus and the News Corpus.
We then merged these vocabularies with the small
MP3 vocabulary. It should be noted that the over-
lap between the most frequent GSLC words and
the MP3 vocabulary was quite low (73 words for
the smallest vocabulary) showing the peculiarity
of the MP3 domain. We also added the vocabu-
lary used for extracting domain data to this mixed
vocabulary. This merging of vocabularies resulted
in a vocabulary of 1153 words. The vocabulary
for the MP3GFLM and the MP3NuanceGr is the
small MP3 vocabulary.
4 Evaluation and Results
4.1 Perplexity measures
The 8 SLMs (all using the vocabulary of 1153
words) were evaluated by measuring perplexity
with the tools SRI provides on the evaluation test
set of 1700 utterances.
In Table 1 we can see a dramatic perplexity re-
duction with the mixed models compared to the
simplest of our models the MP3GFLM. Surpris-
ingly, the GSLCLM models the test set better than
60
Table 1: Perplexity for the different SLMs.
LM Perplexity
MP3GFLM 587
GSLCLM 350
NewsLM 386
DomNewsLM 395
MixGSLCMP3GF 65
MixNewsMP3GF 78
MixDomNewsMP3GF 88
TripleLM 64
the MP3GFLMwhich indicates that our MP3 gram-
mar is too restricted and differs considerably from
the students? grammars.
Lower perplexity does not necessarily mean
lower word error rates and the relation between
these two measures is not very clear. One of the
reasons that language model complexity does not
measure the recognition task complexity is that
language models do not take into account acoustic
confusability (Huang et al 2001; Jelinek, 1997).
According to Rosenfeld (Rosenfeld, 2000a), a per-
plexity reduction of 5% is usually practically not
significant, 10-20% is noteworthy and a perplex-
ity reduction of 30% or more is quite significant.
The above results of the mixed models could then
mean an improvement in word error rate over the
baseline model MP3GFLM. This has been tested
and is reported in the next section. In addition, we
want to test if we can reduce word error rate using
our simple SLM opposed to the Nuance grammar
(MP3NuanceGr) which is our recognition base-
line.
4.2 Recognition rates
The 8 SLMs under consideration were converted
with the SRILM toolkit into a format that Nuance
accepts and then compiled into recognition pack-
ages. These were evaluated with Nuance?s batch
recognition program on the recorded evaluation
test set of 500 utterances (26 speakers). Table 2
presents word error rates (WER) and in parenthe-
sis N-Best (N=10)WER for the models under con-
sideration and for the Nuance Grammar.
As seen, our simple SLM, MP3GFLM, im-
proves recognition performance considerably
compared with the Nuance grammar baseline
(MP3NuanceGr) showing a much more robust
behaviour to the data. Remember that these two
models have the same vocabulary and are both de-
Table 2: Word error rates(WER) for the recording
test set
LM WER(NBest)
MP3GFLM 37.11 (29.48)
GSLCLM 83.04 (71.51)
NewsLM 61.62 (49.53)
DomNewsLM 45.03 (31.58)
MixGSLCMP3GF 34.58 (22.68)
MixNewsMP3GF 38.00 (27.37)
MixDomNewsMP3GF 34.07 (22.07)
TripleLM 33.97 (22.02)
MP3NuanceGr 59.37 (53.19)
rived from the same GF interpretation grammar.
However the flexibility of the SLM gives a relative
improvement of 37% over the Nuance grammar.
The models giving the best results are the models
interpolated with the GSLC corpus and the domain
news corpus in different ways which at best gives
a relative reduction in WER of 8% in comparison
with MP3GFLM and 43% compared with the base-
line. It is interesting to see that the simple way we
used to create a domain specific newspaper cor-
pus gives a model that better fits our data than the
original much larger newspaper corpus.
4.3 In-grammar recognition rates
To contrast the word error rate performance with
in-grammar utterances i.e. utterances that the orig-
inal GF interpretation grammar covers, we car-
ried out a second evaluation with the in-grammar
recordings. We also used Nuance?s parsing tool to
extract the utterances that were in-grammar from
the recorded evaluation test set. These few record-
ings (5%) were added to the in-grammar test set.
The results of the second recognition experiment
are reported in Table 3.
Table 3: WER on the in-grammar test set
LM WER (NBest)
MP3GFLM 4.95 (2.04)
GSLCLM 78.07 (64.15)
NewsLM 48.03 (36.64)
DomNewsLM 26.34 (15.25)
MixGSLCMP3GF 14.23 (6,29)
MixNewsMP3GF 18.63 (10.22)
MixDomNewsMP3GF 15.57 (6.13)
TripleLM 15.17 (6.05)
MP3NuanceGr 3.69 (1.49)
61
The in-grammar results reveal an increase in
WER for all the SLMs in comparison to the
baseline MP3NuanceGr. However, the simplest
model (MP3GFLM), modelling the language of the
grammar, do not show any greater reduction in
recognition performance.
4.4 Discussion of results
The word error rates obtained for the best mod-
els show a relative improvement over the Nuance
grammar of 40%. The most interesting result is
that the simplest of our models, modelling the
same language as the Nuance grammar, gives such
an important gain in performance that it lowers
the WER with 22%. We used the Chi-square test
of significance to statistically compare the results
with the results of the Nuance grammar show-
ing that the differences of WER of the models
in comparison with the baseline are all signifi-
cant on the p=0.05 significance level. However,
the Chi-square test points out that the difference
of WER for in-grammar utterances of the Nu-
ance model and the MP3GFLM is significant on the
p=0.05 level. This means that all the statistical lan-
guage models significantly outperform the base-
line i.e. the Nuance Grammar MP3NuanceGr
on the evaluation test set (being mostly out-of-
coverage) and that the MP3GFLM outperforms the
baseline overall as the difference of WER in the
in-grammar test is significant but very small.
However, as the reader may have noticed, the
word error rates are quite high, which is partly
due to a totally independent test set with out-of-
vocabulary words (9% OOV for the MP3GFLM )
indicating that domain language grammar writing
is very subjective. The students have captured
a quite different language for the same domain
and functionality. This shows the risk of a hand-
tailored domain grammar and the difficulty of pre-
dicting what users may say. In addition, a fair test
of the model would be to measure concept error
rate or more specifically dialogue move error rate
(i.e. both ?yes? and ?yeah? correspond to the same
dialogue move answer(yes)). A closer look
at the MP3GFLM results give a hint that in many
cases the transcription reference and the recogni-
tion hypothesis hold the same semantic content in
the domain (e.g. confusing the Swedish preposi-
tions ?i? (into) and ?till? (to) which are both used
when referring to the playlist). It was manually
estimated that 53% of the recognition hypotheses
could be considered as correct in this way opposed
to the 65% Sentence Error Rate (SER) that the
automatic evaluation gave. This implies that the
evaluation carried out is not strictly fair consid-
ering the possible task improvement. However, a
fair automatic evaluation of dialogue move error
rate will be possible only when we have a way to
do semantic decoding that is not entirely depen-
dent on the GF grammar rules.
The N-Best results indicate that it could be
worth putting effort on re-ranking the N-Best lists
as both WER and SER of the N-Best candidates
are considerably lower. This could ideally give us
a reduction in SER of 10% and, considering dia-
logue move error rate, perhaps even more. More
or less advanced post-process methods have been
used to analyze and decide on the best choice from
the N-Best list. Several different re-ranking meth-
ods have been proposed that show how recogni-
tion rates can be improved by letting external pro-
cesses do the top N ranking and not the recognizer
(Chotimongkol & Rudnicky, 2001; van Noord et
al., 1997). However, the way that seems most ap-
pealing is how (Gabsdil & Lemon, 2004) and (Ha-
cioglu & Ward, 2001) re-rank N-Best lists based
on dialogue context achieving a considerable im-
provement in recognition performance. We are
considering basing our re-ranking on the informa-
tion held in the dialogue information state, knowl-
edge of what is going on in the graphical interface
and on dialogue moves in the list that seem appro-
priate to the context. In this way we can take ad-
vantage of what the dialogue system knows about
the current situation.
5 Concluding remarks and future work
A first observation is that the SLMs give us a much
more robust recognition, as expected. Our best
SLMs, i.e. the mixed models, give a 43% rela-
tive improvement over the baseline i.e. the Nu-
ance grammar compiled from the GF interpreta-
tion grammar. However, this also implies a falling
off in in-grammar performance. It is interest-
ing that the SLM that only models the grammar
(MP3GFLM), although being more robust and giv-
ing a significant reduction in WER rate, does not
degrade its in-grammar performance to a great ex-
tent. This simple model seems promising for use
in a first version of the system with the possibil-
ity of improving it when logs from system interac-
tions have been collected. In addition, the vocabu-
62
lary of this model is in sync with our GF interpre-
tation grammar. The results seem comparable with
those obtained by (Bangalore & Johnston, 2004)
using random generation to produce an SLM from
an interpretation grammar.
Although interpolating our MP3 model with the
GSLC corpus and the newspaper corpora gave a
large perplexity reduction it did not have as much
impact on WER as expected even though it gave
a significant improvement. It seems from the tests
that the quality of the data is more important than
the quantity. This makes extraction of domain
data from larger corpora an important issue and
increases the interest of generating artificial cor-
pora.
As the approach of using SLMs in our dia-
logue systems seems promising and could im-
prove recognition performance considerably we
are planning to apply the experiment to other ap-
plications that are under development in TALK
when the corresponding GF application grammars
are finished. In this way we hope to find out if
there is a tendency in the performance gain of
a statistical language model vs its correspondent
speech recognition grammar. If so, we have found
a good way of compromising between the ease of
grammar writing and the robustness of SLMs in
the first stage of dialogue system development. In
this way we can use the knowledge and intuition
we have about the domain and include it in our
first SLM and get a more robust behaviour than
with a grammar. From this starting point we can
then collect more data with our first prototype of
the system to improve our SLM.
We have also started to look at dialogue move
specific statistical language models (DM-SLMs)
by using GF to generate all utterances that are
specific to certain dialogue moves from our in-
terpretation grammar. In this way we can pro-
duce models that are sensitive to the context but
also, by interpolating these more restricted mod-
els with the general GF SLM, do not restrict what
the users can say but take into account that cer-
tain utterances should be more probable in a spe-
cific dialogue context. Context-sensitive models
and specifically grammars for different contexts
have been explored earlier (Baggia et al 1997;
Wright et al 1999; Lemon, 2004) but generating
such language models artificially from an interpre-
tation grammar by choosing which moves to com-
bine seems to be a new direction. Our first ex-
periments seem promising but the dialogue move
specific test sets are too small to draw any conclu-
sions. We hope to report more on this in the near
future.
Acknowledgements
I am grateful to Steve Young, Robin Cooper and
the EACL reviewers for comments on previous
versions of this paper. I would also like to thank
Aarne Ranta, Peter Ljunglo?f, Karl Weilhammer
and David Hjelm for help with GF and data col-
lection and finally Nuance Communications Inc.
for making available the speech recognition soft-
ware used in this work. This work was supported
in part by the TALK project (FP6-IST 507802,
http://www.talk-project.org/).
References
Allwood, J. 1999. The Swedish Spoken Language Cor-
pus at Go?teborg University. In Fonetik 99, Gothen-
burg Papers in Theoretical Linguistics 81. Dept. of
Linguistics, University of Go?teborg.
Baggia P., Danieli M., Gerbino E., Moisa L. M., and
Popovici C. 1997. Contextual Information and Spe-
cific Language Models for Spoken Language Un-
derstanding. In Proceedings of SPECOM?97, Cluj-
Napoca, Romania, pp. 51?56.
Bangalore S. and Johnston M. 2004. Balancing Data-
Driven And Rule-Based Approaches in the Context
of aMultimodal Conversational System. In Proceed-
ings of Human Language Technology conference.
HLT-NAACL 2004.
Chotimongkol A. and Rudnicky A.I. 2001. N-best
Speech Hypotheses Reordering Using Linear Re-
gression. In Proceedings of Eurospeech 2001. Aal-
borg, Denmark, pp. 1829?1832.
Clarkson P.R. and Rosenfeld R. 1997. Statistical
Language Modeling Using the CMU-Cambridge
Toolkit. In Proceedings of Eurospeech.
Fosler-Lussier E. and Kuo H.-K. J. 2001. Using Se-
mantic Class Information for Rapid Development of
Language Models within ASR Dialogue Systems. In
Proceedings of ICASSP-2001, Salt Lake City, Utah.
Gabsdil M. and Lemon O. 2004. Combining Acoustic
and Pragmatic Features to Predict Recognition Per-
formance in Spoken Dialogue Systems. In Proceed-
ings of ACL, Barcelona.
Gorrell G., Lewin I. and Rayner M. 2002. Adding In-
telligent Help to Mixed Initiative Spoken Dialogue
Systems. In Proceedings of ICSLP-2002.
63
Hacioglu K. and Ward W. 2001. Dialog-context de-
pendent language modeling combining n-grams and
stochastic context-free grammars. In Proceedings of
ICASSP-2001, Salt Lake City, Utah.
Huang X., Acero A., Hon H-W. 2001. Spoken Lan-
guage Processing: A guide to theory, algorithm and
system development. Prentice Hall.
Janiszek D., De Mori R., Bechet F. 1998. Data Aug-
mentation And Language Model Adaptation. Uni-
versity of Avignon 84911 Avignon Cedex 9 - France.
Jelinek, F. and Mercer, R. 1980. Interpolated Estima-
tion ofMarkov Source Parameters from Sparse Data.
In Pattern Recognition in Practice. E. S. Gelsema
and L. N. Kanal, North Holland, Amsterdam.
Jelinek, F. 1997. Statistical Methods for Speech Recog-
nition. MIT Press.
Knight S., Gorrell G., Rayner M., Milward D., Koeling
R. and Lewin I. 2001. Comparing Grammar-Based
and Robust Approaches to Speech Understanding:
A Case Study. In Proceedings of Eurospeech 2001.
Larsson S. 2002. Issue-based Dialogue Management.
PhD Thesis, Go?teborg University.
Larsson S., Berman A., Gro?nqvist L., Kronlid, F. 2002.
TRINDIKIT 3.0 Manual. D6.4, Siridus Project,
Go?teborg University.
Lemon O. 2004. Context-sensitive speech recognition
in ISU dialogue systems: results for the grammar
switching approach. In Proceedings of CATALOG,
8th Workshop on the Semantics and Pragmatics of
Dialogue, Barcelona.
Ljunglo?f P., Bringert B., Cooper R., Forslund A-C.,
Hjelm D., Jonson R., Larsson S. and Ranta A. 2005.
The TALK Grammar Library: an Integration of GF
with TrindiKit. Deliverable 1.1, TALK project.
Nuance Communications. http://www.nuance.com, as
of May 2005.
Pakhomov SV., Schonwetter M., Bachenko, J. 2001.
Generating Training Data for Medical Dictations. In
Proceedings NAACL-2001.
Ranta A. 2004. Grammatical Framework. A Type-
Theoretical Grammar Formalism. In The Journal of
Functional Programming., Vol. 14, No. 2, pp. 145?
189.
Ranta A. Grammatical Framework Homepage
http://www.cs.chalmers.se/a?arne/GF, as of May
2005.
Raux A., Langner B., Black A. and Eskenazi M. 2003.
LET?S GO: Improving Spoken Dialog Systems for
the Elderly and Non-natives. In Proceedings of Eu-
rospeech 2003. Geneva, Switzerland.
Rayner M., Hockey B.A., James F., Owen Bratt E.,
Goldwater S., Gawron J.M. 2000. Compiling Lan-
guage Models from a Linguistically Motivated Uni-
fication Grammar. In Proceedings of COLING-2000.
Rayner M., Hockey B.A., Dowding J. 2003. An Open-
Source Environment for Compiling Typed Unifica-
tion Grammars into Speech Recognisers. In Pro-
ceedings of EACL, pp. 223?226.
Rayner M. and Hockey B.A. 2003. Transparent combi-
nation of rule-based and data-driven approaches in
speech understanding. In Proceedings of EACL.
Rosenfeld R. 2000. Two decades of statistical language
modeling: Where do we go from here? In Proceed-
ings of IEEE:88(8).
Rosenfeld R. 2000. Incorporating Linguistic Structure
into Statistical Language Models. In Philosophical
Transactions of the Royal Society of London A, 358.
Solsona R., Fosler-Lussier E., Kuo H.J., Potamianos
A. and Zitouni I. 2002. Adaptive Language Mod-
els for Spoken Dialogue Systems. In Proceedings of
ICASSP-2002, Orlando, Florida, USA.
Stolcke A. 2002. SRILM ? An Extensible Language
Modeling Toolkit. In Proceedings of ICSLP-2002,
Vol. 2, pp. 901?904, Denver.
van Noord G., Bouma G., Koeling R. and Nederhof,
M. 1999. Robust Grammatical Analysis for Spoken
Dialogue Systems. In Journal of Natural Language
Engineering, 5(1), pp. 45?93.
Wright H., Poesio M. and Isard S. 1999. Using high
level dialogue information for dialogue act recogni-
tion using prosodic features. In DIAPRO-1999, pp.
139?143.
Weilhammer K., Jonson R., Ranta A, Young Steve.
2006. SLM generation in the Grammatical Frame-
work. Deliverable 1.3, TALK project.
Xu W. and Rudnicky A. 2000. Language modeling for
dialog system? In Proceedings of ICSLP-2000, Bei-
jing, China. Paper B1-06.
64
Proceedings of SPEECHGRAM 2007, pages 25?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Grammar-based context-specific statistical language modelling
Rebecca Jonson
Department of Linguistics, Go?teborg University & GSLT
Box 200, 40530 Go?teborg, Sweden
rj@ling.gu.se
Abstract
This paper shows how we can combine
the art of grammar writing with the power
of statistics by bootstrapping statistical lan-
guage models (SLMs) for Dialogue Systems
from grammars written using the Grammati-
cal Framework (GF) (Ranta, 2004). Further-
more, to take into account that the probabil-
ity of a user?s dialogue moves is not static
during a dialogue we show how the same
methodology can be used to generate dia-
logue move specific SLMs where certain di-
alogue moves are more probable than others.
These models can be used at different points
of a dialogue depending on contextual con-
straints. By using grammar generated SLMs
we can improve both recognition and un-
derstanding performance considerably over
using the original grammar. With dialogue
move specific SLMs we would be able to
get a further improvement if we had an op-
timal way of predicting the correct language
model.
1 Introduction
Speech recognition (ASR) for dialogue systems is
often caught in the trap of the sparse data problem
which excludes the possibility of using statistical
language models (SLMs). A common approach is
to write a grammar for the domain either as a speech
recognition grammar (SRG) or as an interpreta-
tion grammar which can be compiled into a speech
recognition grammar (SRG) using some grammar
development platform such as Gemini, Regulus or
GF (Rayner et al, 2000; Rayner et al, 2006; Ranta,
2004). The last option will assure that the linguis-
tic coverage of the ASR and interpretation are kept
in sync. ASR for commercial dialogue systems has
mainly focused on grammar-based approaches de-
spite the fact that SLMs seem to have a better over-
all performance (Knight et al, 2001; Bangalore and
Johnston, 2003). This probably depends on the time-
consuming work of collecting corpora for training
SLMs compared with the more rapid and straight-
forward development of SRGs. However, SLMs are
more robust for out-of-coverage input, perform bet-
ter in difficult conditions and seem to work better
for naive users as shown in (Knight et al, 2001).
SRGs on the other hand are limited in their coverage
depending on how well grammar writers succeed in
predicting what users may say.
An approach taken in both dialogue systems and
dictation applications is to write a grammar for the
particular domain and generate an artificial corpus
from the grammar to be used as training corpus for
SLMs (Galescu et al, 1998; Bangalore and John-
ston, 2003; Jonson, 2006). These grammar-based
models are not as accurate as the ones built from
real data as the estimates are artificial, lacking a re-
alistic distribution. However, as has been shown in
(Bangalore and Johnston, 2003; Jonson, 2006) these
grammar-based statistical models seem to have a
much more robust behaviour than their correspond-
ing grammars which leaves us with a much better
starting point in the first development stage in a di-
alogue system. It is a way of compromising be-
tween the ease of grammar writing and the robust-
25
ness of SLMs. With this methodology we can use
the knowledge and intuition we have about the do-
main and include it in our first SLM and get a much
more robust behaviour than with a grammar. From
this starting point we can then collect more data
with our first prototype of the system to improve our
SLM. In this paper the advantage of this method is
shown further by evaluating a different domain in
greater detail.
Context-specific models have shown important
recognition performance gain (Baggia et al, 1997;
Riccardi et al, 1998; Xu and Rudnicky, 2000;
Lemon and Gruenstein, 2004) and have usually been
of two types: created as state-specific grammars or
built from collected data partitioned according to di-
alogue states. Both methods have their disadvan-
tages. In the first case, we constrain the user heavily
which makes them unsuitable for use in a more flex-
ible system such as an information-state based sys-
tem. This can be solved by having a back-off method
but leaves us with extra processing (Lemon and Gru-
enstein, 2004). In the latter case, we have an even
more severe sparse data problem than when creat-
ing a general SLM as we need enough data to get a
good distribution of data over dialogue states. In an
information-state based system where the user is not
restricted to only a few dialogue states this problem
gets even worse. In addition, why we chose to work
with grammar-based SLMs in the first place was be-
cause data is seldom available in the first stage of di-
alogue system development. This leaves us with the
requirement of an SLM that although being context-
specific does not constrain the user and which as-
sures a minimal coverage of expressions for a cer-
tain context. In (Gruenstein et al, 2005) this is ac-
complished by dynamically populating a class-based
SLMs with context-sensitive content words and ut-
terances. In this paper, we will show how we can
use the same methodology as in (Jonson, 2006) to
create context-specific SLMs from grammars based
on dialogue moves that match these criteria.
This study is organized as follows. First, we in-
troduce our methodology for developing SLMs from
grammars. Section 3 describes the data collection of
test utterances and how we have partitioned the data
into different test sets depending on grammar cov-
erage, types of users and types of dialogue moves.
In section 4, we show and discuss the results of the
different models for different test sets and finally we
draw some conclusions from the experiments.
2 Grammar-based SLMs
In (Jonson, 2006) we described how we could gen-
erate an SLM from an interpretation grammar writ-
ten in GF for an MP3 player application and get
a much more robust behaviour than by using the
original grammar for ASR. In this study, we ap-
proach a different domain using a GF grammar writ-
ten for a dialogue system application called Agen-
daTalk (Ericsson et al, 2006). It is one of several
applications that has been developed in the TALK
project (www.talk-project.org) and has been built
with the TrindiKit toolkit and the GoDiS dialogue
system (Larsson, 2002) as a GoDiS application. It
works as a voice interface to a graphical calendar.
Apart from evaluating a different domain in a more
extensive way to see if the tendency we found in
(Jonson, 2006) is consisting over domains, we have
driven the methodology a bit further to be able to
generate context-specific SLMs that favour certain
parts of the grammar, in our case certain dialogue
moves. We call these SLMs ?dialogue move spe-
cific SLMs? (DMSLMs). Both types of models are
obtained by generating all possible utterances from
a GF grammar, building trigram SLMs from the
grammar-based corpus using the SRI language mod-
elling toolkit (Stolcke, 2002) and compiling them
into recognition packages. For comparison we have
also compiled the GF grammar directly into a Nu-
ance speech recognition grammar using the GF com-
piler.
2.1 Building a general SLM from
grammar-based corpora
The GF grammar written for the calendar domain
consists of 500 GF functions (rules) where 220 are
domain-specific and 280 inherited from a domain-
independent grammar. It exists in two equivalent
language versions that share the same GF functions:
English and Swedish. We have used GF?s facili-
ties to generate a corpus from the Swedish version
consisting of all possible meaningful utterances gen-
erated by the grammar to a certain depth of the
analysis trees in GF?s abstract syntax. The gram-
mar is written on the phrase level accepting spoken
26
language utterances such as e.g. ?add a booking
please?. The resulting corpus consists of 1.7 million
utterances and 19 million words with a vocabulary
of only 183 words. All utterances in the corpus oc-
cur exactly once. However, all grammar rules are
not expanded which leaves us with a class-tagged
corpus without e.g. all variants of date expressions
but with the class date. What we get in the end is
therefore a class-based SLM that we compile into a
recognition package together with a rule-based de-
scription of these classes. The SLM has 3 differ-
ent classes: time, date and event and the do-
main vocabulary when including all distinct words
in these classes make up almost 500 words.
Adding real speech corpora
In (Jonson, 2006) we saw that the use of real cor-
pora in interpolation with our artificial corpus was
only valuable as long as the real corpora approxi-
mated the language of use. The big news corpus we
had available did not give any significant improve-
ment but the transcribed Swedish speech corpus we
used was much more helpful. In this study we have
therefore once again used the GLSC corpus to im-
prove our word occurrence estimates by interpolat-
ing it with our grammar-based SLM. The Gothen-
burg Spoken Language (GSLC) corpus consists of
transcribed Swedish spoken language from different
social activities such as auctions, phone calls, meet-
ings, lectures and task-oriented dialogue (Allwood,
1999). The corpus is composed of about 1,300,000
words and is turn-based which gives it long utter-
ances including e.g. transcribed disfluencies. From
this corpus we have built an SLM which we have
interpolated with our grammar-based SLM keeping
our domain vocabulary. This means we are just con-
sidering those n-grams in the GSLC corpus which
match the domain vocabulary to hopefully get a
more realistic probability distribution for these. We
will call this model our Extended SLM.
2.2 Dialogue move specific SLMs
SLMs capture the lexical context statistics in a spe-
cific language use. However, the statistical distribu-
tion in a dialogue is not static but varies by boost-
ing and lowering probabilities for different words
and expressions depending on contextual appropri-
ateness. It is not only words and expressions that
vary their distribution but on a semantic level differ-
ent conceptual messages will be more or less prob-
able as a user utterance at different points of the
dialogue. This means that certain dialogue moves
will have a higher degree of expectancy at a specific
point of the dialogue. To capture this phenomenon,
we want to build models that raise the probability of
certain dialogue moves in certain contexts by giving
a higher probability for utterances expressing these
dialogue moves. These are models where utterances
corresponding to a certain dialogue move are more
salient (e.g. a model where all ways of answering
yes or no are more plausible than other utterances).
Such a model will account for the fact that the expec-
tation of dialogue moves a user will perform varies
in a dialogue and thereby their statistics. We can
obtain this by using a version of the grammar-based
corpus where the dialogue moves for each utterance
are generated which allows us to partition the corpus
in different ways based on dialogue moves. We can
then take out part of the corpus e.g. all utterances
corresponding to a certain dialogue move, create an
SLM and interpolate it with the general grammar-
based SLM. In this way, we get SLMs where certain
dialogue moves are more probable than others and
where minimally all possible expressions for these,
which the grammar describes, are covered. By in-
terpolating with the general SLM we put no hard
constraints on the expected dialogue move so the
user can in fact say anything at any point in the di-
alogue despite the raised expectancy for certain dia-
logue moves. We just boost the expected probability
of certain dialogue moves and their possible expres-
sions. By using contextual constraints in the infor-
mation state we could then predict which model to
use and switch SLMs on the fly so that we obtain a
recognizer that takes account of expected user input.
2.2.1 Partitioning the training data by dialogue
moves
In GoDiS, dialogue moves are activity related and
exist in seven different types: request moves,
answer moves, ask moves (i.e. questions), yes
and no ( yn) answers, greet moves, quit moves
and feedback and sequencing moves which are
called ICM:s (Larsson, 2002). We have chosen to
focus on the first four of these dialogue move types
to build up our DMSLMs. We have used GF to gen-
27
erate a corpus with all possible dialogue moves and
their combinations with their corresponding expres-
sions. From this corpus we have extracted all utter-
ances that can be interpreted as an answer move
or a sequence of answer moves, all expressions for
specification of a request (in GoDiS what type of
action to perform e.g. deleting a booking), all ways
of expressing questions in our grammars (i.e. ask
moves) and all possible yn answers. This leaves us
with four new sets of training data.
The decision to partition the data in this way was
based on the distribution of dialogue moves in our
data where the moves we focus on are the most com-
mon ones and the most critical for achievement of
the dialogue tasks. As these dialogue moves are ab-
stract and domain-independent it would be possible
to use a domain-independent prediction of these di-
alogue moves and thereby the language models al-
though the structure of the SLMs would be different
in different domains.
2.2.2 Building dialogue move specific SLMs
For each set of dialogue move specific training
data we created an SLM that only captures ways of
expressing a specific dialogue move. However, we
are looking for less constrained models which just
alter the probability of certain dialogue moves. By
interpolating the SLMs built on dialogue move spe-
cific corpora with the general grammar-based SLM
we achieve models with contextual probabilities but
that generalize to avoid constraining the user input.
The interpolation of these models was carried out
with the SRILM toolkit based on equation 1. The
optimal lambda weight was estimated to 0.85 for all
models with the SRILM toolkit using held-out data.
Pdmslm(W ) = ?Pmovespec(W ) + (1? ?)Pgeneral(W ) (1)
We ended up with four new SLMs, so called DM-
SLMs, in which either the probability of answer,
ask, request or yn moves were boosted.
3 Test Data
The collection of test data was carried out by hav-
ing people interacting with the AgendaTalk system
using the grammar-based SLM. The test group in-
cluded both naive users with no experience of the
system whatsoever and users that had previous ex-
perience with the system to varying extents. We
have classified the latter group as expert users al-
though the expertise varies considerably. All users
were given a printed copy of a calendar month with
scheduled bookings and some question marks and
were assigned the task of altering the voice-based
calendar so that the graphical calendar would look
the same as the printed copy except for the question
marks which they were to find values for by query-
ing the system. This would mean that they would
have to add, delete and alter bookings as well as find
out information about their schedule e.g. the time of
an event. The tasks could be carried out in any order
and there were many different ways to complete the
schedule.
The data collection gave us a recording test set
of 1000 recorded utterances from 15 persons (all na-
tive, 8 female, 7 male). This unrestricted test set was
used to compare recognition performance between
the different models under consideration. We also
partitioned the test set in various ways to explore
different features. The test set was parsed to get al
in-coverage utterances that the original GF grammar
covers to create an in-coverage test set from these.
In addition, we partitioned the data by users with a
test set with the naive user utterances and another
test set from the expert users. In this way we could
explore how our models performed under different
conditions. Different dialogue system applications
will have a different distribution of users. Some
systems will always have a large number of naive
or less experienced users who will use more out-
of-coverage utterances and more out-of-vocabulary
(OOV) words whereas users of other applications
will have the opportunity to obtain considerable ex-
perience which will allow them to adapt to the sys-
tem, in particular to its grammar and vocabulary.
The recordings for the unrestricted test set have an
OOV rate of 6% when using our domain vocabulary.
The naive test set makes up 529 of these recordings
with an OOV rate of 8% whereas the expert test set
of 471 recordings has a lower OOV rate of 4%. The
in-coverage test set consists of 626 utterances leav-
ing us with an in-coverage rate of 62.6% for the un-
restricted test set. This shows the need for a more ro-
bust way of recognition and interpretation if we ex-
pect to expose the system to less experienced users.
For the evaluation of the DMSLMs we have par-
titioned the test data by dialogue moves. The utter-
28
ances corresponding with the four dialogue moves
chosen for our DMSLMs were divided into four test
sets. The utterances left were used to create a fifth
test set where none of our four DMSLMs would
apply but where we would need to use the gen-
eral model. If we look at the distribution of the
test data considering dialogue moves we find that
75.4% of the test data falls into our four dialogue
move categories and that only 24.6% of the data
would require the general model. This part of the
test data includes dialogue moves such as greetings,
quit moves and dialogue move sequences with com-
binations of different moves. The most common di-
alogue move in our data is an answer move or a
sequence of answer moves resulting in common
utterances such as: ?a meeting on friday? as answer
to system questions such as ?what booking do you
want to add??.
4 Experimental Results
To evaluate the recognition performance of our dif-
ferent types of models we ran several experiments
on the different test sets. We report results on word
error rate (WER), sentence error rate (SER) and also
on a semantic level by reporting what we call dia-
logue move error rate (DMER). The dialogue move
error rate was obtained by parsing the recognized ut-
terances and comparing these to a parsed version of
the transcriptions, calculating the rate of correctly
parsed dialogue moves. The calculation was done
in the same way as calculation of concept error rate
(CER) proposed by (Boros et al, 1996) where the
degree of correctly recognized concepts is consid-
ered. In our case this means the degree of correctly
recognized dialogue moves. For parsing we have
used a phrase-spotting grammar written in Prolog
that pattern matches phrases to dialogue moves. Us-
ing the original GF interpretation grammar for pars-
ing would have restricted us to the coverage of the
grammar which is not an optimal choice together
with SLMs. Ideally, we would like to use a robust
version of GF to be able to use the original GF gram-
mar both for parsing and SLM generation and by
that assure the same linguistic coverage. Attempts
to do this have been carried out in the TALK project
for theMP3 domain by training a dialogue move tag-
ger on the same type of corpus that was used for
the DMSLMs where dialogue moves occur together
with their corresponding utterances. Other meth-
ods of relaxing the constraints of the GF parser are
also under consideration. Meanwhile, we are using a
simple robust phrase spotting parser. We have inves-
tigated both how our grammar-based SLMs perform
in comparison to our grammar under different condi-
tions to see how recognition and understanding per-
formance varies as well as how our DMSLMs per-
form in comparison to the general grammar-based
SLM. The results are reported in the following sec-
tions. All models have the same domain vocabulary
and the OOV figures presented earlier thereby apply
for all of them.
4.1 Grammar-based SLMs vs. grammars
Table 1 shows the results for our different language
models on our unrestricted test set of 1000 utter-
ances as well as for the part of this test set which is
in-coverage. As expected they all performmuch bet-
ter on the in-coverage test set with the lowest WER
obtained with our grammar. On the unrestricted test
set we can see an important reduction of both WER
(26% and 38% relative improvement) and DMER
(24% and 40% relative improvement) for the SLMs
in comparison to the grammar which indicates the
robustness of these to new user input.
In table 2 we can see how the performance of all
our models are better for the expert users with a rel-
ative word error rate reduction from 25% to 32% in
comparison to the results for the naive test set. The
same pattern is seen on the semantic level with im-
portant reduction in DMER. The result is expected
as the expert users have greater knowledge of the
language of the system. This is consistent with the
results reported in (Knight et al, 2001). It is also
reflected in the OOV figures discussed earlier where
the naive users seem to have used many more un-
known words than the expert users.
This shows that the models perform very dif-
ferently depending on the types of users and how
much they hold to the coverage of the grammar.
Our grammar-based SLM gives us a much more ro-
bust behaviour which is good when we expect less
experienced users. However, we can see that we
get a degradation in in-coverage performance which
would be critical if we are to use the model in a sys-
tem where we expect that the users will achieve cer-
29
Table 1: Results on unrestricted vs in-coverage test set
Model Unrestricted In-coverage
WER SER DMER WER SER DMER
Grammar 39.0% 47.6% 43.2% 10.7% 16.3% 10.3%
Grammar-based SLM 29.0% 39.7% 33.0% 14.8% 18.3% 13.7%
Extended SLM 24.0% 35.2% 25.8% 11.5% 15.8% 10.4%
Table 2: Results on naive vs expert users
Model Naive users Expert users
WER SER DMER WER SER DMER
Grammar 46.6% 50.3% 54.7% 31.7% 44.4% 33.2%
Grammar-based SLM 34.4% 42.9% 41.3% 23.8% 35.9% 25.8%
Extended SLM 27.6% 38.2% 29.5% 20.7% 31.8% 22.7%
tain proficiency. The Extended SLM seem to per-
form well in all situations and if we look at DMER
there is no significant difference in performance be-
tween this model and our grammar when it comes
to in-coverage input. In most systems we will prob-
ably have a range of users with different amounts
of experience and even experienced users will fail
to follow the grammar in spontaneous speech. This
points towards the advisability of using an SLM as
it is more robust and if it does not degrade too much
on in-coverage user input like the Extended SLM
it would be an optimal choice.
From the results it seems that we have found a cor-
relation between the DMER and WER in our system
which indicates that if we manage to lower WER we
will also achieve better understanding performance
with our simple robust parser. This is good news as it
means that we will not only capture more words with
our SLMs but also more of the message the user is
trying to convey in the sense of capturing more dia-
logue moves. This will definitely result into a better
dialogue system performance overall. Interestingly,
we have been able to obtain this just by converting
our grammar into an SLM.
4.2 Dialogue move specific SLMs vs General
SLMs
We have evaluated our DMSLMs on test sets for
each model which include only utterances that corre-
spond to the dialogue moves in the model. It should
be mentioned that the test sets may include utter-
ances not covered by the original GF grammar e.g. a
different wording for the same move. The results for
each DMSLM on its specific test set and the perfor-
mance of the grammar-based SLM and the Extended
SLM are reported in tables 3, 4, 5 and 6.
Table 3: Ask Move SLM
Model WER SER DMER
Grammar-based SLM 39.2% 68.4% 51.8%
Ask DMSLM 31.8% 68.9% 48.7%
Extended SLM 30.1% 58.0% 44.6%
Table 4: Answer Move SLM
Model WER SER DMER
Grammar-based SLM 17.3% 22.0% 16.3%
Answer DMSLM 15.7% 20.1% 14.1%
Extended SLM 18.2% 22.0% 16.7%
Table 5: Request Move SLM
Model WER SER DMER
Grammar-based SLM 29.1% 44.3% 27.0%
Request DMSLM 17.0% 36.1% 14.7%
Extended SLM 26.3% 42.6% 22.1%
Apart from these four dialogue moves our test
data includes a lot of different dialogue moves and
dialogue move combinations that we have not con-
sidered. As we have no specific model for these we
would need to use a general model in these cases.
This means that apart from predicting the four di-
alogue moves we have considered we would also
need to predict when none of these are expected and
use the general model for these situations. In table
7 we can see how our general models perform on
the rest of the test set. This shows that they seem to
handle this part of the test data quite well.
30
Table 6: YN Move SLM
Model WER SER DMER
Grammar-based SLM 37.3% 27.3% 22.7%
YN DMSLM 21.5% 16.5% 11.9%
Extended SLM 25.0% 18.2% 12.5%
Table 7: General SLM on rest of test data
Model WER SER DMER
Grammar-based SLM 22.2% 42.7% 31.7%
Extended SLM 19.6% 39.8% 26.0%
We can see that the gain we get in recognition
performance varies for the different models and that
relative improvement in WER goes from 9% for the
answer model to 42% for our DMSLMs on appro-
priate test sets. We can see that our models have
most problems with ask moves and yn answers.
In the case of ask moves this seems to be because
our GF grammar is missing a lot of syntactic con-
structions of question expressions. This would then
explain why the Extended SLM gets a much better
figure here. The GSLC corpus does capture more
of this expressive variation of questions. In other
words we seem to have failed to capture and predict
the linguistic usage with our hand-tailored grammar.
In the case of yn answers the result reveals that our
grammar-based SLM does not have a realistic distri-
bution of these expressions at all. This seems to be
something the GSLC corpus contribute, considering
the good results for the Extended SLM. However,
we can see that we can achieve the same effect by
boosting the probability of yes and no answers in
our DMSLM.
If we look at the overall achievement in recog-
nition performance, using our DMSLMs when ap-
propriate and in other cases the general SLM, the
average WER of 22% (27% DMER) is consider-
ably lower than when using the general model for
the same test data (29% WER, 33% DMER). If we
had an optimal method for predicting what language
model to use we would be able to decrease WER by
24% relative. If we chose to use the Extended
SLM in the cases our DMSLMs do not cover we
could get an even greater reduction.
We have also tested how well our DMSLMs per-
form on the general test set (i.e. all 1000 utter-
ances) to see how bad the performance would be if
we chose the wrong model. In table 8 we can see
that this approach yields an average WER of 30%
which is a minimal degradation in comparison to
the general grammar-based SLM. On the contrary,
some of our models actually perform better than our
general grammar-based SLM or very similarly. This
implies that there is no substantial risk on recogni-
tion performance if our prediction model would fail.
This means that we could obtain very good results
with important recognition improvement even with
an imperfect prediction accuracy. We have a relative
improvement of 24% to gain with only a minimal
loss.
Table 8: DMSLMs on general test set
Model WER SER
Answer DMSLM 34.7% 55.6%
Ask DMSLM 28.2% 46.2%
Request DMSLM 26.5% 43.2%
YN DMSLM 29.8% 44.0%
5 Concluding remarks
Our experimental results show that grammar-based
SLMs give an important reduction in both WER and
DMER in accordance with the results in (Jonson,
2006). We reach a relative improvement of 26%
and a further 17% if we interpolate our grammar-
based SLM with real speech data. The correlation
of the DMER and the WER in our results indicates
that the improved recognition performance will also
propagate to the understanding performance of our
system.
Context-specific language models (statistical and
rule-based) have shown important recognition per-
formance gain in earlier work (Baggia et al, 1997;
Xu and Rudnicky, 2000; Lemon and Gruenstein,
2004; Gruenstein et al, 2005) and this study reaf-
firms that taking into account statistical language
variation during a dialogue will give us more accu-
rate recognition. The method we use here has the ad-
vantage that we can build statistical context-specific
models even when no data is available, assuring a
minimal coverage and by interpolation with a gen-
eral model do not constrain the user input unduly.
The language model switch will be triggered by
changing a variable in our information state: the pre-
dicted dialogue move. However, to be able to choose
31
which language model suits the current information
state best we need a way to predict dialogue moves.
The prediction model could either be rule-based or
data based. Our first experimental tests with ma-
chine learning for dialogue move prediction seems
promising and we hope to report on these soon. Op-
timally, we want a prediction model that we can
use in different GoDiS domains to be able to gen-
erate new DMSLMs from our domain-specific GF
grammar for the dialogue moves we have considered
here.
Our experiments show that we could achieve
an overall reduction in WER of 46% and 40% in
DMER if we were able to choose our best suited
SLM instead of our compiled GF grammar. Natu-
rally, we would have to take into account dialogue
move prediction accuracy to get a more realistic fig-
ure. However, our experiments also show that the
effect on performance if we failed to use the correct
model would not be too harmful. This means we
have much more to gain than to lose even if the di-
alogue move prediction is not perfect. This makes
this approach a very interesting option in dialogue
system development.
Acknowledgment
I would like to thank Nuance Communications Inc.
for making available the speech recognition soft-
ware used in this work.
References
Jens Allwood. 1999. The Swedish spoken language
corpus at Go?teborg University. In Proceedings of
Fonetik?99: The Swedish Phonetics Conference.
Paolo Baggia, Morena Danieli, Elisabetta Gerbino,
Loreta Moisa, and Cosmin Popovici. 1997. Con-
textual information and specific language models for
spoken language understanding. In Proceedings of
SPECOM.
Srinivas Bangalore andMichael Johnston. 2003. Balanc-
ing data-driven and rule-based approaches in the con-
text of a multimodal conversational system. In Pro-
ceedings of the ASRU Conference.
M. Boros, W. Eckert, F. Gallwitz, G. Go?rz, G. Hanrieder,
and H. Niemann. 1996. Towards understanding spon-
taneous speech: Word accuracy vs. concept accuracy.
In Proceedings of ICSLP, Philadelphia, PA.
S. Ericsson, G. Amores, B. Bringert, H. Burden,
A. Forslund, D. Hjelm, R. Jonson, S. Larsson,
P. Ljunglo?f, P. Manchon, D. Milward, G. Perez, and
M. Sandin. 2006. Software illustrating a unified ap-
proach to multimodality and multilinguality in the in-
home domain. Deliverable D1.6, TALK Project.
Lucian Galescu, Eric Ringger, and James Allen. 1998.
Rapid language model development for new task do-
mains. In In Proceedings of the ELRA First Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
Alexander Gruenstein, Chao Wang, and Stephanie Sen-
eff. 2005. Context-sensitive statistical language mod-
eling. In Proceedings of Interspeech.
Rebecca Jonson. 2006. Generating statistical language
models from interpretation grammars in dialogue sys-
tems. In Proceedings of EACL, Trento, Italy.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: A
case study. In Proceedings of Eurospeech.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, Go?teborg University.
Oliver Lemon and Alexander Gruenstein. 2004. Multi-
threaded context for robust conversational interfaces:
Context-sensitive speech recognition and interpreta-
tion of corrective fragments. ACM Trans. Comput.-
Hum. Interact., 11(3):241?267.
Aarne Ranta. 2004. Grammatical framework. a type-
theoretical grammar formalism. The Journal of Func-
tional Programming, 14(2):145?189.
Manny Rayner, Beth Ann Hockey, Frankie James, Eliz-
abeth Owen Bratt, Sharon Goldwater, and Jean Mark
Gawron. 2000. Compiling language models from a
linguistically motivated unification grammar. In Pro-
ceedings of the COLING.
Manny Rayner, Beth Ann Hockey, and Pierrette Bouil-
lon. 2006. Putting Linguistics into Speech Recogni-
tion: The Regulus Grammar Compiler. CSLI Publica-
tions.
Guiseppe Riccardi, Alexandros Potamianos, and
Shrikanth Narayanan. 1998. Language model adapta-
tion for spoken language systems. In Proceedings of
the ICSLP, Australia.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
Denver, Colorado.
Wei Xu and Alex Rudnicky. 2000. Language model-
ing for dialog system. In Proceedings of ICSLP 2000,
Beijing, China.
32
