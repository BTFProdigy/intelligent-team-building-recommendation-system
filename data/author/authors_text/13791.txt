Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 163?168,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SEMILAR: The Semantic Similarity Toolkit 
 
Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal Niraula, and Dan Stefanescu 
Department of Computer Science 
The University of Memphis 
Memphis, TN 38152 
{vrus,rbanjade,mclinten,nbnraula,dstfnscu}@memphis.edu 
 
 
Abstract 
We present in this paper SEMILAR, the SE-
Mantic simILARity toolkit. SEMILAR im-
plements a number of algorithms for assessing 
the semantic similarity between two texts. It is 
available as a Java library and as a Java 
standalone ap-plication offering GUI-based 
access to the implemented semantic similarity 
methods. Furthermore, it offers facilities for 
manual se-mantic similarity annotation by ex-
perts through its component SEMILAT (a 
SEMantic simILarity Annotation Tool). 
1 Introduction 
We present in this paper the design and im-
plementation of SEMILAR, the SEMantic 
simILARity toolkit. SEMILAR 
(www.semanticsimilarity.org) includes im-
plementations of a number of algorithms pro-
posed over the last decade or so to address 
various instances of the general problem of 
text-to-text semantic similarity. Semantic sim-
ilarity is an approach to language understand-
ing that is widely used in real applications. It 
is a practical alternative to the true under-
standing approach, which is intractable as it 
requires world knowledge, a yet to-be-solved 
problem in Artificial Intelligence. 
Text A: York had no problem with MTA?s in-
sisting the decision to shift funds had been within 
its legal rights. 
Text B: York had no problem with MTA?s say-
ing the decision to shift funds was within its 
powers. 
 
Given such two texts, the paraphrase identifi-
cation task is about automatically assessing 
whether Text A is a paraphrase of, i.e. has the 
same meaning as, Text B. The example above is 
a positive instance, meaning that Text A is a par-
aphrase of Text B and vice versa. 
The importance of semantic similarity in Nat-
ural Language Processing (NLP) is highlighted 
by the diversity of datasets and shared task eval-
uation campaigns (STECs) that have been pro-
posed over the last decade (Dolan, Quirk, and 
Brockett, 2004; McCarthy & McNamara, 2008; 
Agirre et al, 2012). These datasets include in-
stances from various applications.  Indeed, there 
is a need to identify and quantify semantic rela-
tions between texts in many applications. For 
instance, paraphrase identification, an instance of 
the semantic similarity problem, is an important 
step in a number of applications including Natu-
ral Language Generation, Question Answering, 
and dialogue-based Intelligent Tutoring Systems. 
In Natural Language Generation, paraphrases are 
a method to increase diversity of generated text 
(Iordanskaja et al 1991). In Question Answer-
ing, multiple answers that are paraphrases of 
each other could be considered as evidence for 
the correctness of the answer (Ibrahim et al 
2003). In Intelligent Tutoring Sys-tems (Rus et 
al., 2009; Lintean et al, 2010; Lintean, 2011), 
paraphrase identification is useful to assess 
whether students? articulated answers to deep 
questions (e.g. conceptual physics questions) are 
similar-to/paraphrases-of ideal answers. 
Generally, the problem of semantic similarity 
between two texts, denoted text A and text B, is 
defined as quantifying and identifying the pres-
ence of semantic relations between the two texts, 
e.g. to what extent text A has the same meaning 
as or is a paraphrase of text B (paraphrase rela-
tion; Dolan, Quirk, and Brockett, 2004). Other 
semantic relations that have been investigated 
systematically in the recent past are entailment, 
i.e. to what extent text A entails or logically in-
fers text B (Dagan, Glickman, & Magnini, 2004), 
and elaboration, i.e. is text B is an elaboration of 
text A? (McCarthy & McNamara, 2008). 
163
Semantic similarity can be broadly construed 
between texts of any size. Depending on the 
granularity of the texts, we can talk about the 
following fundamental text-to-text similarity 
problems: word-to-word similarity, phrase-to-
phrase similarity, sentence-to-sentence similari-
ty, paragraph-to-paragraph similarity, or docu-
ment-to-document similarity. Mixed combina-
tions are also possible such as assessing the simi-
larity of a word to a sentence or a sentence to a 
paragraph. For instance, in summarization it 
might be useful to assess how well a sentence 
summarizes an entire paragraph. 
2 Motivation 
The problem of word-to-word similarity has been 
extensively studied over the past decades and a 
word-to-word similarity library (WordNet Simi-
larity) has been developed by Pedersen and col-
leagues (Pedersen, Patwardhan, & Michelizzi, 
2004). 
Methods to assess the semantic similarity of 
larger texts, in particular sentences, have been 
proposed over the last decade (Corley and 
Mihalcea, 2005; Fernando & Stevenson, 2008; 
Rus, Lintean, Graesser, & McNamara 2009). 
Androutsopoulos & Malakasiotis (2010) com-
piled a survey of methods for paraphrasing and 
entailment semantic relation identification at sen-
tence level. Despite all the proposed methods to 
assess semantic similarity between two texts, no 
semantic similarity library or toolkit, similar to 
the WordNet library for word-to-word similarity, 
exists for larger texts. Given the importance of 
semantic similarity, there is an acute need for 
such a library and toolkit. The developed SEMI-
LAR library and toolkit presented here fulfill this 
need. 
In particular, the development of the semantic 
similarity toolkit SEMILAR has been motivated 
by the need for an integrated environment that 
would provide:  
 
? easy access to implementations of various 
semantic similarity approaches from the 
same user-friendly interface and/or library. 
? easy access to semantic similarity methods 
that work at different levels of text granulari-
ty: word-to-word, sentence-to-sentence, par-
agraph-to-paragraph, document-to-
document, or a combination (SEMILAR in-
tegrates word-to-word similarity measures). 
? authoring methods for semantic similarity. 
? a common environment for that allows sys-
tematic and fair comparison of semantic sim-
ilarity methods. 
? facilities to manually annotate texts with se-
mantic similarity relations using a graphical 
user interface that make such annotations 
easier for experts (this component is called 
SEMILAT component - a SEMantic similari-
ty Annotation Tool). 
 
SEMILAR is thus a one-stop-shop for investi-
gating, annotating, and authoring methods for the 
semantic similarity of texts of any level of granu-
larity. 
3 SEMILAR: The Semantic Similarity 
Toolkit 
The authors of the SEMILAR toolkit (see Figure 
1) have been involved in assessing the semantic 
Figure 1. Snapshot of SEMILAR. The Data View tab is shown. 
164
similarity of texts for more than a decade. During 
this time, they have conducted a careful require-
ments analysis for an integrated software toolkit 
that would integrate various methods for seman-
tic similarity assessment. The result of this effort 
is the prototype presented here. We briefly pre-
sent the components of SEMILAR next and then 
describe in more detail the core component of 
SEMILAR, i.e. the set of semantic similarity 
methods that are currently available. It should be 
noted that we are continuously adding new se-
mantic similarity methods and features to SEMI-
LAR. 
The SEMILAR toolkit includes the following 
components: project management; data view-
browsing-visualization; preprocessing (e.g., col-
location identification, part-of-speech tagging, 
phrase or dependency parsing, etc.), semantic 
similarity methods (word-level and sentence-
level), classification components for qualitative 
decision making with respect to textual semantic 
relations (na?ve Bayes, Decision Trees, Support 
Vector Machines, and Neural Network), kernel-
based methods (sequence kernels, word sequence 
kernels, and tree kernels; as of this writing, we 
are still implementing several other tree kernel 
methods); debugging and testing facilities for 
model selection; and annotation components (al-
lows domain expert to manually annotate texts 
with semantic relations using GUI-based facili-
ties; Rus et al, 2012). For space reasons, we only 
detail next the main algorithms in the core com-
ponent, i.e. the major text-to-text similarity algo-
rithms currently available in SEMILAR. 
4 The Semantic Similarity Methods 
Available in SEMILAR 
The core component of SEMILAR is a set of 
text-to-text semantic similarity methods. We 
have implemented methods that handle both uni-
directional similarity measures as well as bidirec-
tional similarity measures. For instance, the se-
mantic relation of entailment between two texts 
is unidirectional (a text T logically entails a hy-
pothesis text H but H does not entail T) while the 
paraphrase relation is bidirectional (text A has 
same meaning as text B and vice versa). 
Lexical Overlap. Given two texts, the sim-
plest method to assess their semantic similarity is 
to compute lexical overlap, i.e. how many words 
they have in common. There are many lexical 
overlap variations. Indeed, a closer look at lexi-
cal overlap reveals a number of parameters that 
turns the simple lexical overlap problem into a 
large space of possibilities. The parameters in-
clude preprocessing options (collocation detec-
tion, punctuation, stopword removal, etc.), filter-
ing options (all words, content words, etc.), 
weighting schemes (global vs. local weighting, 
binary weighting, etc.), and normalization factors 
(largest text, weighted average, etc.). A total of 
3,456 variants of lexical overlap can be generat-
ed by different parameter settings in SEMILAR. 
Lintean (2011) has shown that performance on 
lexical overlap methods on the tasks of para-
phrase identification and textual entailment tasks 
can vary significantly depending on the selected 
parameters. Some lexical overlap variations lead 
to performance results rivaling more sophisticat-
ed, state-of-the-art methods. 
It should be noted that the overlap category of 
methods can be extended to include N-gram 
overlap methods (see the N-gram overlap meth-
ods proposed by the Machine Translation com-
munity such as BLEU and METEOR). SEMI-
LAR offers bigram and unigram overlap methods 
including the BLEU and METEOR scores. 
A natural approach to text-to-text similarity 
methods is to rely on word-to-word similarity 
measures. Many of the methods presented next 
compute the similarity of larger texts using indi-
vidual word similarities. 
Mihalcea, Corley, & Strappavara (2006; 
MCS) proposed a greedy method based on word-
to-word similarity measures. For each word in 
text A (or B) the maximum similarity score to 
any word in the other text B (or A) is used. An 
idf-weighted average is then computed as shown 
in the equation below. 
 
)
}
2
{
)(
}
2
{
)}(*)
1
,(max{
}
1
{
)(
}
1
{
)}(*)
2
,(max{
(
2
1
)2,1(
?
?
?
?
?
?
?
?
?
?
Tw
widf
Tw
widfTwSim
Tw
widf
Tw
widfTwSim
TTsim
 
 
The word-to-word similarity function sim(w, 
T) in the equation above can be instantiated to 
any word-to-word similarity measure (e.g. 
WordNet similarities or Latent Semantic Analy-
sis). The vast majority of word-to-word similari-
ty measures that rely on WordNet are concept-to-
concept measures and to be able to use them one 
must map words in the input texts onto concepts 
in WordNet, i.e. word sense disambiguation 
(WSD) is needed. As of this writing, SEMILAR 
addresses the issue in two simple ways: (1) se-
165
lecting the most frequent sense for each word, 
which is sense #1 in WordNet, and (2) using all 
the senses for each word and then take the max-
imum (or average) of the relatedness scores for 
each pair of word senses. We label the former 
method as ONE (sense one), whereas the latter is 
labeled as ALL-MAX or ALL-AVG (all senses 
maximum score or all senses average score, re-
spectively). Furthermore, most WordNet-based 
measures only work within a part-of-speech cat-
egory, e.g. only between nouns. 
Other types of word-to-word measures, such 
as those based on Latent Semantic Analysis or 
Latent Dirichlet Allocation, do not have a word-
sense disambiguation challenge.  
Rus and Lintean (2012; Rus-Lintean-
Optimal Matching or ROM) proposed an opti-
mal solution for text-to-text similarity based on 
word-to-word similarity measures. The optimal 
lexical matching is based on the optimal assign-
ment problem, a fundamental combinatorial op-
timization problem which consists of finding a 
maximum weight matching in a weighted bipar-
tite graph.  
Given a weighted complete bipartite graph 
, where edge  has weight 
, the optimal assignment problem is to 
find a matching M from X to Y with maximum 
weight. 
A typical application is about assigning a 
group of workers, e.g. words in text A in our 
case, to a set of jobs (words in text B in our case) 
based on the expertise level, measured by 
, of each worker at each job. By adding 
dummy workers or jobs we may assume that X 
and Y have the same size, n, and can be viewed 
as   and Y = . 
In the semantic similarity case, the weight  
is the word-to-word similarity between a word x 
in text A and a word y in text B.  
The assignment problem can also be stated as 
finding a permutation  of {1, 2, 3, ? , n} for 
which  is maximum. Such an 
assignment is called optimum assignment. The 
Kuhn-Munkres algorithm (Kuhn, 1955) can find 
a solution to the optimum assignment problem in 
polynomial time. 
Rus and colleagues (Rus et al, 2009; Rus & 
Graesser, 2006; Rus-Syntax-Negation or RSN) 
used a lexical overlap component combined with 
syntactic overlap and negation handling to com-
pute an unidirectional subsumption score be-
tween two sentences, T (Text) and H (Hypothe-
sis), in entailment recognition and student input 
assessment in Intelligent Tutoring Systems. Each 
text is regarded as a graph with words as 
nodes/vertices and syntactic dependencies as 
edges. The subsumption score reflects how much 
a text is subsumed or contained by another. The 
equation below provides the overall subsumption 
score, which can be averaged both ways to com-
pute a similarity score, as opposed to just the 
subsumption score, between the two texts.  
 
2
)
_#
)1(1(
)
||
),(max
||
),(max
(),(
relneg
h
E
eHh
E
tEh
Ematch
eTtE
h
V
vHh
V
tVh
Vmatch
vTtV
HTsubsump
??
?
?
?
?
??
?
?
?
??
?
?
 
The lexical component can be used by itself 
(given a weight of 1 with the syntactic compo-
nent given a weight of 0) in which case the simi-
larity between the two texts is just a composi-
tional extension of word-to-word similarity 
measures. The match function in the equation 
can be any word-to-word similarity measure in-
cluding simple word match, WordNet similarity 
measures, LSA, or LDA-based similarity 
measures. 
Fernando and Stevenson (FST; 2008) pro-
posed a method in which similarities among all 
pairs of words are taken into account for compu-
ting the similarity of two texts. Each text is rep-
resented as a binary vector (1 ? the word occurs 
in the text; 0 ? the word does not occur in the 
text). They use a similarity matrix operator W 
that contains word-to-word similarities between 
any two words. 
||||
),( ??
??
?
??
ba
TbWa
basim
 
Each element wij represents the word-level 
semantic similarity between word ai in text A 
and word bj in text B. Any word-to-word seman-
tic similarity measure can be used. 
Lintean and Rus (2010; weighted-LSA or 
wLSA) extensively studied methods for semantic 
similarity based on Latent Semantic Analysis 
(LSA; Landauer et al, 2006). LSA represents 
words as vectors in a 300-500 dimensional LSA 
space. An LSA vector for larger texts can be de-
rived by vector algebra, e.g. by summing up the 
individual words? vectors. The similarity of two 
texts A and B can be computed using the cosine 
(normalized dot product) of their LSA vectors. 
Alternatively, the individual word vectors can be 
166
combined through weighted sums. Lintean and 
Rus (2010) experimented with a combination of 
3 local weights and 3 global weights. All these 
versions of LSA-based text-to-text similarity 
measures are available in SEMILAR. 
SEMILAR also includes a set of similarity 
measures based on the unsupervised method La-
tent Dirichlet Allocation (LDA; Blei, Ng, & 
Jordnan, 2003; Rus, Banjade, & Niraula, 
2013). LDA is a probabilistic generative model 
in which documents are viewed as distributions 
over a set of topics (?d - text d?s distribution over 
topics) and topics are distributions over words (?t 
? topic t?s distribution over words). That is, each 
word in a document is generated from a distribu-
tion over words that is specific to each topic. 
A first LDA-based semantic similarity meas-
ure among words would then be defined as a dot-
product between the corresponding vectors rep-
resenting the contributions of each word to a top-
ic (?t(w) ? represents the probability of word w 
in topic t). It should be noted that the contribu-
tions of each word to the topics does not consti-
tute a distribution, i.e. the sum of contributions is 
not 1. Assuming the number of topics T, then a 
simple word-to-word measure is defined by the 
formula below. 
 
  
 
 
 
More global text-to-text similarity measures could 
be defined in several ways as detailed next.  
Because in LDA a document is a distribution 
over topics, the similarity of two texts needs to 
be computed in terms of similarity of distribu-
tions. The Kullback-Leibler (KL) divergence 
defines a distance, or how dissimilar, two distri-
butions p and q are as in the formula below. 
 
 
 
 
 
If we replace p with ?d (text/document d?s dis-
tribution over topics) and q with ?c 
(text/document c?s distribution over topics) we 
obtain the KL distance between two documents 
(documents d and c in our example). The KL 
distance has two major problems. In case qi is 
zero KL is not defined. Then, KL is not symmet-
ric. The Information Radius measure (IR) solves 
these problems by considering the average of pi 
and qi as below. Also, the IR can be transformed 
into a symmetric similarity measure as in the fol-
lowing (Dagan, Lee, & Pereira, 1997): 
 
 
 
The Hellinger and Manhattan distances be-
tween two distributions are two other options 
that avoid the shortcomings of the KL distance. 
Both are options are implemented in SEMILAR. 
LDA similarity measures between two docu-
ments or texts c and d can also include similarity 
of topics. That is, the text-to-text similarity is 
obtained multiplying the similarities between the 
distribution over topics (?d and ?c) and distribu-
tion over words (?t1 and ?t2). The similarity of 
topics can be computed using the same methods 
illustrated above as the topics are distributions 
over words (for all the details see Rus, Banjade, 
& Niraula, 2013). 
The last semantic similarity method presented 
in this paper is based on the Quadratic Assign-
ment Problem (QAP). The QAP method aims at 
finding an optimal assignment from words in text 
A to words in text B, based on individual word-
to-word similarity measures, while simultaneous-
ly maximizing the match between the syntactic 
dependencies of the words. 
The Koopmans-Beckmann (1957) formulation 
of the QAP problem best fits this purpose. The 
goal of the original QAP formulation, in the do-
main of economic activity, was to minimize the 
objective function QAP shown below where ma-
trix F describes the flow between any two facili-
ties, matrix D indicates the distances between 
locations, and matrix B provides the cost of lo-
cating facilities to specific locations. F, D, and B 
are symmetric and non-negative. 
 
?? ??????
n
i
n
i iib
n
j jidjifBDFQAP 1 1 )(,1 )()(,),,(min ???
 
 
The fi,j term denotes the flow between facili-
ties i and j which are placed at locations ?(i) and 
?(j), respectively. The distance between these 
locations is d?(i)?(j). In our case, F and D describe 
dependencies between words in one sentence 
while B captures the word-to-word similarity 
between words in opposite sentences. Also, we 
have weighted each term in the above formula-
tion and instead of minimizing the sum we are 
maximizing it resulting in the formulation below.  
 
?? ???????
n
i
n
i iib
n
j jidjifBDFQAP 1 1 )(,)1(1 )()(,),,(max ?????
 
 
?
?
?? T
t
tt vwvwwwLDA
1
)()(),(2 ??
),(10),( dcIRqpSIM ???
?
?
? T
i i
i
i q
ppqpKL
1
log),(
167
5 Discussion and Conclusions 
The above methods were experimented with on 
various datasets for paraphrase, entailment, and 
elaboration. For paraphrase identification, the 
QAP method provides best accuracy results 
(=77.6%) on the test subset of the Microsoft Re-
search Paraphrase corpus, one of the largest par-
aphrase datasets. 
 Due to space constraints, we have not de-
scribed all the features available in SEMILAR. 
For a complete list of features, latest news, refer-
ences, and updates of the SEMILAR toolkit 
along with downloadable resources including 
software and data files, the reader can visit this 
link: www.semanticsimilarity.org. 
 
Acknowledgments 
This research was supported in part by Institute 
for Education Sciences under award 
R305A100875. Any opinions, findings, and con-
clusions or recommendations expressed in this 
material are solely the authors? and do not neces-
sarily reflect the views of the sponsoring agency. 
References  
Androutsopoulos, I. & Malakasiotis, P. 2010. A sur-
vey of paraphrasing and textual entailment meth-
ods. Journal of Artificial Intelligence Research, 
38:135-187. 
Agirre, E., Cer, D., Diab, M., & Gonzalez-Agirre, A. 
(2012). SemEval-2012 Task 6: A Pilot on Semantic 
Textual Similarity, First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), Mon-
treal, Canada, June 7-8, 2012. 
Blei, D.M., Ng, A.Y., & Jordan, M.I. 2003. Latent 
dirichlet alocation, The Journal of Machine Learn-
ing Research 3, 993-1022. 
Corley, C., & Mihalcea, R. (2005). Measuring the 
Semantic Similarity of Texts. In Proceedings of the 
ACL Workshop on Empirical Modeling of Seman-
tic Equivalence and Entailment. Ann Arbor, MI. 
Dagan, I., Glickman, O., & Magnini, B. (2004). The 
PASCAL Recognising textual entailment Chal-
lenge. In Quinorero-Candela, J.; Dagan, I.; Magni-
ni, B.; d'Alche-Buc, F. (Eds.), Machine Learning 
Challenges. Lecture Notes in Computer Science, 
Vol. 3944, pp. 177-190, Springer, 2006. 
Dolan, B., Quirk, C., & Brockett, C. (2004). Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics (COLING-2004), Gene-
va, Switzerland. 
Fernando, S. & Stevenson, M. (2008). A semantic 
similarity approach to paraphrase detec-
tion, Computational Linguistics UK (CLUK 2008) 
11th Annual Research Colloquium. 
Lintean, M., Moldovan, C., Rus, V., & McNamara D. 
(2010). The Role of Local and Global Weighting in 
Assessing the Semantic Similarity of Texts Using 
Latent Semantic Analysis. Proceedings of the 23rd 
International Florida Artificial Intelligence Re-
search Society Conference. Daytona Beach, FL. 
Lintean, M. (2011). Measuring Semantic Similarity: 
Representations and Methods, PhD Thesis, De-
partment of Computer Science, The University of 
Memphis, 2011.  
Ibrahim, A., Katz, B., & Lin, J. (2003). Extracting 
structural paraphrases from aligned monolingual 
corpora In Proceedings of the Second International 
Workshop on Paraphrasing, (ACL 2003). 
Iordanskaja, L., Kittredge, R., & Polgere, A. (1991). 
Natural Language Generation in Artificial Intelli-
gence and Computational Linguistics. Lexical se-
lection and paraphrase in a meaning-text genera-
tion model, Kluwer Academic. 
McCarthy, P.M. & McNamara, D.S. (2008). User-
Language Paraphrase Corpus Challenge 
https://umdrive.memphis.edu/pmmccrth/public/Par
aphraseCorpus/Paraphrase site.htm. Retrieved 
2/20/2010 online, 2009. 
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). 
WordNet::Similarity - Measuring the Relatedness 
of Concepts, In the Proceedings of the Nineteenth 
National Conference on Artificial Intelligence 
(AAAI-04), pp. 1024-1025, July 25-29, 2004, San 
Jose, CA (Intelligent Systems Demonstration). 
Rus, V., Lintean M., Graesser, A.C., & McNamara, 
D.S. (2009). Assessing Student Paraphrases Using 
Lexical Semantics and Word Weighting. In Pro-
ceedings of the 14th International Conference on 
Artificial Intelligence in Education, Brighton, UK. 
Rus, V., Lintean, M., Moldovan, C., Baggett, W., 
Niraula, N., Morgan, B. (2012). The SIMILAR 
Corpus: A Resource to Foster the Qualitative Un-
derstanding of Semantic Similarity of Texts, In 
Semantic Relations II: Enhancing Resources and 
Applications, The 8th Language Resources and 
Evaluation Conference (LREC 2012), May 23-25, 
Instanbul, Turkey. 
Rus, V., Banjade, R., & Niraula, N. (2013). Similarity 
Measures based on Latent Dirichlet Allocation, 
The 14th International Conference on Intelligent 
Text Procesing and Computational Linguistics, 
March 24-30, 2013, Samos, Greece. 
 
168
The First Question Generation Shared Task Evaluation 
Challenge 
Vasile Rus1, Brendan Wyse2, Paul Piwek2, Mihai Lintean1, Svetlana Stoyanchev2 
and Cristian Moldovan1
 
1 Department of Computer Science/Institute for Intelligent Systems, The University of 
Memphis, Memphis, TN, 38152, USA 
{vrus,mclinten,cmoldova}@memphis.edu 
2 Centre for Research in Computing, Open University, UK 
bjwyse@gmail.com and {p.piwek, s.stoyanchev}@open.ac.uk 
 
Abstract. The paper briefly describes the First Shared Task Evaluation 
Challenge on Question Generation that took place in Spring 2010. The 
campaign included two tasks: Task A ? Question Generation from Paragraphs 
and Task B ? Question Generation from Sentences. An overview of each of the 
tasks is provided.   
Keywords: question generation, shared task evaluation campaign. 
1   Introduction 
Question Generation is an essential component of learning environments, help 
systems, information seeking systems, multi-modal conversations between virtual 
agents, and a myriad of other applications (Lauer, Peacock, and Graesser, 1992; 
Piwek et al, 2007). 
Question Generation has been recently defined as the task (Rus & Graesser, 2009) 
of automatically generating questions from some form of input. The input could vary 
from information in a database to a deep semantic representation to raw text. 
The first Shared Task Evaluation Challenge on Question Generation (QG-STEC) 
follows a long tradition of STECs in Natural Language Processing (see the annual 
tasks run by the Conference on Natural Language Learning - CoNLL). In particular, 
the idea of a QG-STEC was inspired by the recent activity in the Natural Language 
Generation (NLG) community to offer shared task evaluation campaigns as a 
potential avenue to provide a focus for research in NLG and to increase the visibility 
of NLG in the wider Natural Language Processing (NLP) community (White and 
Dale, 2008). It should be noted that the QG is currently perceived as a discourse 
processing task rather than a traditional NLG task (Rus & Graesser, 2009). 
Two core aspects of a question are the goal of the question and its importance. It is 
difficult to determine whether a particular question is good without knowing the 
context in which it is posed; ideally one would like to have information about what 
counts as important and what the goals are in the current context. This suggests that a 
STEC on QG should be tied to a particular application, e.g. tutoring systems. 
However, an application-specific STEC would limit the pool of potential participants 
to those interested in the target application. Therefore, the challenge was to find a 
framework in which the goal and importance are intrinsic to the source of questions 
and less tied to a particular context/application. One possibility was to have the 
general goal of asking questions about salient items in a source of information, e.g. 
core ideas in a paragraph of text. Our tasks have been defined with this concept in 
mind. Adopting the basic principle of application-independence has the advantage of 
escaping the problem of a limited pool of participants (to those interested in a 
particular application had that application been chosen as the target for a QG STEC). 
Another decision aimed at attracting as many participants as possible and 
promoting a more fair comparison environment was the input for the QG tasks. 
Adopting a specific representation for the input would have favored some participants 
already familiar with such a representation. Therefore, we have adopted as a second 
guiding principle for the first QG-STEC tasks: no representational commitment. That 
is, we wanted to have as generic an input as possible. The input to both task A and B 
in the first QG STEC is raw text. 
The First Workshop on Question Generation (www.questiongeneration.org) has 
identified four categories of QG tasks (Rus & Graesser, 2009): Text-to-Question, 
Tutorial Dialogue,  Assessment, and Query-to-Question. The two tasks in the first QG 
STEC are part of the Text-to-Question category or part of the Text-to-text Natural 
Language Generation task categories (Dale & White, 2007). It is important to say that 
the two tasks offered in the first QG STEC were selected among 5 candidate tasks by 
the members of the QG community. A preference poll was conducted and the most 
preferred tasks, Question Generation from Paragraphs (Task A) and Question 
Generation from Sentences (Task B), were chosen to be offered in the first QG STEC. 
The other three candidate tasks were: Ranking Automatically Generated Questions 
(Michael Heilman and Noah Smith), Concept Identification and Ordering (Rodney 
Nielsen and Lee Becker), and Question Type Identification (Vasile Rus and Arthur 
Graesser). 
There is overlap between Task A and B. This was intentional with the aim of 
encouraging people preferring one task to participate in the other. The overlap 
consists of the specific questions in Task A which are more or less similar with the 
type of questions targeted by Task B. 
Overall, we had 1 submission for Task A and 4 submissions for Task B. We also 
had an additional submission on development data for Task A. 
2   TASK A: Question Generation from Paragraphs 
1.1   Task Definition 
The Question Generation from Paragraphs (QGP) task challenges participants to 
generate a list of 6 questions from a given input paragraph. The six questions should 
be at three scope levels: 1 x broad (entire input paragraph), 2 x medium (multiple 
sentences), and 3 x specific (sentence or less). The scope is defined by the portion of 
the paragraph that answers the question. 
The Question Generation from Paragraphs (QGP) task has been defined such that it 
is application-independent. Application-independent means questions will be judged 
based on content analysis of the input paragraph; questions whose answers span more 
input text are ranked higher. 
We show next an example paragraph together with six interesting, application-
independent questions that could be generated. We will use the paragraph and 
questions to describe the judging criteria. 
Table 1.  Example of input paragraph (from  http://en.wikipedia.org/wiki/Abraham_lincoln).  
Input Paragraph 
Abraham Lincoln (February 12, 1809 ? April 15, 1865), the 16th 
President of the United States, successfully led his country through 
its greatest internal crisis, the American Civil War, preserving the 
Union and ending slavery. As an outspoken opponent of the 
expansion of slavery in the United States, Lincoln won the 
Republican Party nomination in 1860 and was elected president 
later that year. His tenure in office was occupied primarily with the 
defeat of the secessionist Confederate States of America in the 
American Civil War. He introduced measures that resulted in the 
abolition of slavery, issuing his Emancipation Proclamation in 1863 
and promoting the passage of the Thirteenth Amendment to the 
Constitution. As the civil war was drawing to a close, Lincoln 
became the first American president to be assassinated. 
Table 2.  Examples of questions and scores for the paragraph in Table 1.  
Questions Scope 
Who is Abraham Lincoln? General 
What major measures did President Lincoln introduce? Medium 
How did President Lincoln die? Medium 
When was Abraham Lincoln elected president? Specific 
When was President Lincoln assassinated? Specific 
What party did Abraham Lincoln belong to? Specific 
 
 
A set of five scores, one for each criterion (specificity, syntax, semantics, question 
type correctness, diversity), and a composite score will be assigned to each question. 
Each question at each position will be assigned a composite score ranging from 1 
(first/top ranked, best) to 4 (worst rank), 1 meaning the question is at the right level of 
specificity given its rank (e.g. the broadest question that the whole paragraph answers 
will get a score of 1 if in the first position) and also it is syntactically and semantically 
correct as well as unique/diverse from other generated questions in the set. 
Ranking of questions based on scope assures a maximum score for the six 
questions of 1, 2, 2, 3, 3 and 3, respectively. A top-rank score of 1 is assigned to a 
broad scope question that is also syntactically and semantically correct or acceptable, 
i.e. if it is semantically ineligible then a decision about its scope cannot be made and 
thus a worst-rank score of 4 is assigned. A maximum score of 2 is assigned to 
medium-scope questions while a maximum score of 3 is assigned to specific 
questions. The best configuration of scores (1, 2, 2, 3, 3, 3) would only be possible for 
paragraphs that could trigger the required number of questions at each scope level, 
which may not always be the case. 
1.3   Data Sources and Annotation 
The primary source of input paragraphs were: Wikipedia, OpenLearn, 
Yahoo!Answers. We collected 20 paragraphs from each of these three sources. We 
collected both a development data set (65 paragraphs) and a test data set (60 
paragraphs). For the development data set we manually generated and scored 6 
questions per paragraph for a total of 6 x 65 = 390 questions.  
Paragraphs were selected such that they are self-contained (no need for previous 
context to be interpreted, e.g. will have no unresolved pronouns) and contain around 
5-7 sentences for a total of 100-200 tokens (excluding punctuation). In addition, we 
aimed for a diversity of topics of general interest. 
We also provided discourse relations based on HILDA, a freely available 
automatic discourse parser (duVerle & Prendinger, 2009). 
2   TASK B: Question Generation from Sentences 
2.1   Task Definition 
Participants were given a set of inputs, with each input consisting of:  
 
? a single sentence and  
? a specific target question type (e.g., WHO?, WHY?, HOW?, WHEN?; see 
below for the complete list of types used in the challenge).  
 
For each input, the task was to generate 2 questions of the specified target question 
type.  
Input sentences, 60 in total, were selected from OpenLearn, Wikipedia and Yahoo! 
Answers (20 inputs from each source). Extremely short or long sentences were not 
included. Prior to receiving the actual test data, participants were provided with a 
development data set consisting of sentences from the aforementioned sources and, 
for one or more target question types, examples of questions. These questions were 
manually authored and cross-checked by the team organizing Task B.  
The following example is taken from the development data set. Each instance has a 
unique identifier and information on the source it was extracted from. The <text> 
element contains the input sentence and the <question> elements contain possible 
questions. The <question> element has the type attribute for specification of the target 
question type. 
 
<instance id="3">  
 <id>OpenLearn</id>  
 <source>A103_5</source>  
 <text> 
  The poet Rudyard Kipling lost his only son  
  in the trenches in 1915. 
 </text>  
 <question type="who"> 
  Who lost his only son in the trenches in 1915? 
 </question> 
 <question type="when"> 
  When did Rudyard Kipling lose his son? 
 </question> 
 <question type="how many"> 
  How many sons did Rudyard Kipling have? 
 </question> 
</instance> 
 
Note that input sentences were provided as raw text. Annotations were not 
provided. There are a variety of NLP open-source tools available to potential 
participants and the choice of tools and how these tools are used was considered a 
fundamental part of the challenge.  
This task was restricted to the following question types: WHO, WHERE, WHEN, 
WHICH, WHAT, WHY, HOW MANY/LONG, YES/NO. Participants were provided 
with this list and definitions of each of the items in it. 
2.2   Evaluation criteria for System Outputs and Human Judges 
The evaluation criteria fulfilled two roles. Firstly, they were provided to the 
participants as a specification of the kind of questions that their systems should aim to 
generate. Secondly, they also played the role of guidelines for the judges of system 
outputs in the evaluation exercise.   
For this task, five criteria were identified: relevance, question type, syntactic 
correctness and fluency, ambiguity, and variety. All criteria are associated with a 
scale from 1 to N (where N is 2, 3 or 4), with 1 being the best score and N the worst 
score. 
The procedure for applying these criteria is as follows: 
 
? Each of the criteria is applied independently of the other criteria to each of 
the generated questions (except for the stipulation provided below).  
 
We need some specific stipulations for cases where no question is returned in 
response to an input. For each target question type, two questions are expected. 
Consequently, we have the following two possibilities regarding missing questions: 
 
? No question is returned for a particular target question type: for each of 
the missing questions, the worst score is recorded for all criteria. 
? Only one question is returned: For the missing question, the worst score is 
assigned on all criteria. The question that is present is scored following 
the criteria, with the exception of the VARIETY criterion for which the 
lowest possible score is assigned.  
 
We compute the overall score on a specific criterion. We can also compute a score 
which aggregates the overall scores for the criteria. 
Conclusions 
The submissions to the first QG STEC are now being evaluated using peer-review 
mechanism in which participants blindly evaluate their peers questions. At least two 
reviews per submissions are performed with the results to be made public at the 3rd 
Workshop on Question Generation that will take place in June 2010. 
 
Acknowledgments. We are grateful to a number of people who contributed to the 
success of the First Shared Task Evaluation Challenge on Question Generation: 
Rodney Nielsen, Amanda Stent, Arthur Graesser, Jose Otero, and James Lester. Also, 
we would like to thank the National Science Foundation who partially supported this 
work through grants RI-0836259 and RI-0938239 (awarded to Vasile Rus) and the 
Engineering and Physical Sciences Research Council who partially supported the 
effort on Task B through grant EP/G020981/1 (awarded to Paul Piwek). The views 
expressed in this paper are solely the authors?. 
References 
1. Lauer, T., Peacock, E., & Graesser, A. C. (1992) (Eds.). Questions and information systems. 
Hillsdale, NJ: Erlbaum. 
2. Rus, V. and Graesser, A.C. (2009). Workshop Report: The Question Generation Task and 
Evaluation Challenge, Institute for Intelligent Systems, Memphis, TN, ISBN: 978-0-615-
27428-7.  
3. Piwek, P., H. Hernault, H. Prendinger, M. Ishizuka (2007). T2D: Generating Dialogues 
between Virtual Agents Automatically from Text. In: Intelligent Virtual Agents: 
Proceedings of IVA07, LNAI 4722, September 17-19, 2007, Paris, France, (Springer-
Verlag, Berlin Heidelberg) pp.161-174 
4. Dale, R. & M. White (2007) (Eds.). Position Papers of the Workshop on Shared Tasks and 
Comparative Evaluation in Natural Language Generation. 
5. duVerle, D. and Prendinger, H. (2009). A novel discourse parser based on Support Vector 
Machines. Proc 47th Annual Meeting of the Association for Computational Linguistics and 
the 4th Int'l Joint Conf on Natural Language Processing of the Asian Federation of Natural 
Language Processing (ACL-IJCNLP'09), Singapore, Aug 2009 (ACL and AFNLP), pp 665-
673. 
 
 
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 157?162,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Comparison of Greedy and Optimal Assessment of Natural Language 
Student Input Using Word-to-Word Similarity Metrics 
 
 
Vasile Rus Mihai Lintean 
Department of Computer Science Department of Computer Science 
The University of Memphis The University of Memphis 
Memphis, TN 38152 Memphis, TN 38152 
vrus@memphis.edu mclinten@memphis.edu 
 
 
 
 
 
 
Abstract 
We present in this paper a novel, optimal 
semantic similarity approach based on 
word-to-word similarity metrics to solve 
the important task of assessing natural 
language student input in dialogue-based 
intelligent tutoring systems. The optimal 
matching is guaranteed using the sailor 
assignment problem, also known as the job 
assignment problem, a well-known 
combinatorial optimization problem. We 
compare the optimal matching method with 
a greedy method as well as with a baseline 
method on data sets from two intelligent 
tutoring systems, AutoTutor and iSTART. 
Introduction  
We address in this paper the important task of 
assessing natural language student input in 
dialogue-based tutoring systems where the primary 
form of interaction is natural language. Students 
provide their responses to tutor?s requests by 
typing or speaking their responses. Therefore, in 
dialogue-based tutoring systems understanding 
students? natural language input becomes a crucial 
step towards building an accurate student model, 
i.e. assessing the student?s level of understanding, 
which in turn is important for optimum feedback 
and scaffolding and ultimately impacts the 
tutoring?s effectiveness at inducing learning gains 
on the student user. 
We adopt a semantic similarity approach to 
assess students? natural language input in 
intelligent tutoring systems. The semantic 
similarity approach to language understanding 
derives the meaning of a target text, e.g. a student 
sentence, by comparing it with another text whose 
meaning is known. If the target text is semantically 
similar to the known-meaning text then we know 
the target?s meaning as well. 
Semantic similarity is one of the two major 
approaches to language understanding, a central 
topic in Artificial Intelligence. The alternative 
approach is full understanding. The full 
understanding approach is not scalable due to 
prohibitive costs to encode world and domain 
knowledge which are needed for full understanding 
of natural language. 
To illustrate the problem of assessing natural 
language student input in dialogue-based tutoring 
systems using a semantic similarity approach, we 
consider the example below from experiments with 
AutoTutor (Graesser et al, 2005), a dialogue-based 
tutoring system. 
Expert Answer: The force of the earth's gravity, 
being vertically down, has no effect on the object's 
horizontal velocity 
Student Input: The horizontal component of motion 
is not affected by vertical forces 
In this example, the student input, also called 
contribution, is highly similar to the correct expert 
answer, called expectation, allowing us to conclude 
that the student contribution is correct. A correct 
response typically triggers positive feedback from 
the tutor. The expert answer could also be an 
157
anticipated wrong answer, usually called a 
misconception. A student contribution similar to a 
misconception would trigger a misconception 
correction strategy. 
We model the problem of assessing natural 
language student input in tutoring systems as a 
paraphrase identification problem (Dolan et al, 
2004). The student input assessment problem has 
been also modeled as a textual entailment task in 
the past (Rus & Graesser, 2006). 
Our novel method to assess a student 
contribution against an expert-generated answer 
relies on the compositionality principle and the 
sailor assignment algorithm that was proposed to 
solve the assignment problem, a well-known 
combinatorial optimization problem. The sailor 
assignment algorithm optimally assigns sailors to 
ships based on the fitness of the sailors? skills to 
the ships? needs [7, 8]. In our case, we would like 
to optimally match words in the student input (the 
sailors) to words in the expert-generated answer 
(the ships) based on how well the words in student 
input (the sailors) fit the words in the expert 
answer (the ships). The fitness between the words 
is nothing else but their similarity according to 
some metric of word similarity. We use the 
WordNet word-to-word similarity metrics 
(Pedersen et al, 2004) and Latent Semantic 
Analysis (Landauer et al, 2007). 
The methods proposed so far that rely on the 
principle of compositionality to compute the 
semantic similarity of longer texts have been 
primarily greedy methods (Corley & Mihalcea, 
2005; Lintean & Rus, 2012). To the best of our 
knowledge, nobody proposed an optimal solution 
based on the principle of compositionality and 
word-to-word similarity metrics for the student 
input assessment problem. It is important to note 
that the optimal method proposed here is generally 
applicable to compute the similarity of any texts. 
We provide experimental results on two datasets 
provided to us by researchers developing two 
world-class dialogue-based tutoring systems: 
AutoTutor (Graesser et al, 2005) and iSTART 
(McNamara et al, 2004). 
Background  
It is beyond the scope of this work to offer an 
exhaustive overview of methods proposed so far to 
handle the task of assessing natural language 
student input in intelligent tutoring systems. We 
only describe next methods that are most relevant 
to our work. 
Assessing student?s contributions in dialogue-
based tutoring systems has been approached either 
as a paraphrase identification task (Graesser et al, 
2005), i.e. the task was to assess how similar 
student contributions were to expert-generated 
answers, or as an entailment task (Rus & Graesser, 
2006), in which case the task was to assess whether 
student contributions were entailed by expert-
generated answers. The expert answers were 
assumed to be true. If a correct expert answer 
entailed a student contribution then the 
contribution was deemed to be true as well. 
Latent Semantic Analysis (LSA; Landauer et al, 
2007) has been used to evaluate student 
contributions during the dialog between the student 
by Graesser and colleagues (2005). In LSA the 
meaning of a word is represented by a reduced-
dimensionality vector derived by applying an 
algebraic method, called Singular Value 
Decomposition (SVD), to a term-by-document 
matrix built from a large collection of documents. 
A typical dimensionality of an LSA vector is 300-
500 dimensions. To compute the similarity of two 
words the cosine of the word?s corresponding LSA 
vector is computed, i.e. the normalized dot-
product. A typical extension of LSA-based word 
similarity to computing the similarity of two 
sentences (or even larger texts) is to use vector 
algebra to generate a single vector for each of the 
sentences (by adding up the individual words? LSA 
vectors) and then compute the cosine between the 
resulting sentence vectors. Another approach 
proposed so far to compute similarities between 
individual words in the two sentences, greedily 
selects for each word its best match, and then sums 
the individual word-to-word similarities in order to 
compute the overall similarity score for the two 
sentences (Lintean & Rus, 2012). We do report 
results with LSA using the latter approach for 
comparison purposes. Another reason is that only 
the latter approach allows the application of the 
optimum matching method. 
Extending word-to-word similarity measures to 
sentence level and beyond has drawn increasing 
interest in the last decade or so in the Natural 
Language Processing community. The interest has 
been driven primarily by the creation of 
standardized data sets and corresponding shared 
158
task evaluation campaigns (STECs) for the major 
text-to-text semantic relations of entailment (RTE; 
Recognizing Textual Entailment corpus by Dagan, 
Glickman, & Magnini, 2005), paraphrase (MSR; 
Microsoft Research Paraphrase corpus by Dolan, 
Quirk, and Brockett, 2004), and more recently for 
elaboration (ULPC; User Language Paraphrase 
Challenge by McCarthy & McNamara, 2008). 
None of the existing methods for assessing the 
similarity of texts based on the compositional 
principle and word-to-word similarity metrics have 
proposed an optimum method. 
Beyond Word-to-Word Similarity Measures  
Based on the principle of compositionality, which 
states that the meaning of longer texts can be 
composed from the meaning of their individual 
words (which includes collocations in our case 
such as ?free fall?), we can extend the word-to-
word similarity metrics to compute the similarity 
of longer texts, e.g. of sentences. 
In our work, we use a set of WordNet-based 
similarity metrics as well as LSA. We used the 
following similarity measures implemented in the 
WordNet::Similarity package and described in 
(Pedersen et al, 2004): LCH (Leacock and 
Chodorow), RES (Resnik), JCN (Jiang and 
Conrath), LIN (Lin), PATH, and WUP (Wu and 
Palmer). Some measures, e.g. PATH, are path-
based, i.e. use paths of lexico-semantic relations 
between concepts in WordNet, while some others 
are gloss-based, that is, they use the text of the 
gloss or the definition of a concept in WordNet as 
the source of meaning for the underlying concept. 
One challenge with the WordNet word-to-word 
relatedness measures is that they cannot be directly 
applied to larger texts such as sentences. They 
must be extended to larger texts, which we did as 
described later. 
Another challenge with the WordNet word-to-
word similarity metrics is the fact that texts express 
meaning using words and not concepts. To be able 
to use the word-to-word related measures we must 
map words in sentences to concepts in WordNet. 
Thus, we are faced with a word sense 
disambiguation (WSD) problem. It is beyond the 
scope of our investigation to fully solve the WSD 
problem, one of the hardest in the area of Natural 
Language Processing. Instead, we addressed the 
issue in two ways: (1) mapped the words in the 
student contribution and expert answer onto the 
concepts corresponding to their most frequent 
sense, which is sense #1 in WordNet, and (2) map 
the words onto all the concepts corresponding to 
all the senses and then take the maximum of the 
relatedness scores for each pair of senses. Because 
the ALL (all senses) method offered better results 
and because of space constraints we only report 
results with the ALL method in this paper. 
Greedy versus Optimal Semantic Similarity 
Matching  
This section describes the greedy and optimal 
matching methods to assess the similarity of two 
texts based on word-to-word similarity metrics. 
We assume the two texts, T1 and T2, are two 
sentences and regard them as bags of words 
(syntactic information is ignored). 
The Greedy Method. In the greedy method, 
each word in text T1 is paired with every word in 
text T2 and word-to-word similarity scores are 
computed according to some metric. The 
maximum similarity score between words in T1 
and any word in T2 is greedily retained regardless 
of the best matching scores of the other words in 
T1. The greedily-obtained scores are added up 
using a simple or weighted sum which can then be 
normalized in different ways, e.g. by dividing to 
the longest text or to the average length of the two 
texts. The formula we used is given in equation 1. 
As one would notice, this formula is asymmetric, 
i.e. score(T1,T2)?score(T2,T1). The average of 
the two scores provides a symmetric similarity 
score, more suitable for a paraphrase task, as 
shown in Equation 2. In this paper, we do a simple 
non-weighted sum, i.e. all the words are equally-
weighted with a weight of 1. 
The obvious drawback of the greedy method is 
that it does not aim for a global maximum 
similarity score. The optimal method described 
next solves this issue. 
?
?
?
? ? ??
1
1 2
)(
),(max*)()2,1(
Tv
Tv Tw
vweight
wvsimwordvweightTTscore
 
Equation 1. Asymmetric semantic similarity score 
between texts T1 and T2. 
2
)1,2()2,1()2,1( TTscoreTTscoreTTsimScore ??
 Equation 2. Symmetric semantic similarity score 
between texts T1 and T2. 
159
Optimal Matching. The optimal assignment 
problem is one of the fundamental combinatorial 
optimization problems and consists of finding a 
maximum weight matching in a weighted bipartite 
graph.  
Given a weighted complete bipartite graph 
, where edge  has weight 
, find a matching M from X to Y with 
maximum weight. 
An application is about assigning a group of 
workers, e.g. sailors, to a set of jobs (on ships) 
based on the expertise level, measured by , 
of each worker at each job. By adding dummy 
workers or jobs we may assume that X and Y have 
the same size, n, and can viewed as  
 and Y = . In the 
semantic similarity case, the workers and jobs are 
words from the two sentences to be compared and 
the weight  is the word-to-word similarity 
between word x and y in the two sentences, 
respectively.  
The assignment problem can be stated as finding 
a permutation  of {1, 2, 3, ? , n} for which 
 is maximum. Such an assignment 
is called optimum assignment. An algorithm, the 
Kuhn-Munkres method (Kuhn, 1955), has been 
proposed that can find a solution to the optimum 
assignment problem in polynomial time. For space 
reasons, we do not show here the algorithm in 
detail. 
To illustrate the difference between the two 
methods, we use the two sentence fragments 
shown in Figure 1. A greedy method would pair 
motion with motion (similarity score of 1.00) as 
that is the maximum similarity between motion and 
any word in the opposite sentence and acceleration 
is paired with speed (similarity score of 0.69) for a 
total score of 1.69 (before normalization). An 
optimal matching would yield an overall score of 
1.70 by pairing motion in the first sentence with 
speed (similarity of 0.75) and acceleration with 
motion (similarity of 0.95). 
 
 
 
 
 
 
 
Figure 1. Examples of two sentence fragments and 
word-to-word similarity scores for each of the word 
pairs across sentences. The bold arrows show optimal 
pairing. 
Experimental Setup and Results  
We present in this section the datasets we used 
in our experiments and the results obtained. As we 
already mentioned, we use two datasets containing 
real student answers from two dialogue-based 
tutoring systems: AutoTutor (Graesser et al, 2005) 
and iSTART (McNamara et al, 2004). 
The AutoTutor dataset contains 125 student 
contribution ? expert answer pairs and the correct 
paraphrase judgment, TRUE or FALSE, as 
assigned by human experts. The target domain is 
conceptual physics. One expert physicist rated the 
degree to which particular speech acts expressed 
during AutoTutor training matched particular 
expert answers. These judgments were made on a 
sample of 25 physics expectations (i.e., correct 
expert answers) and 5 randomly sampled student 
answers per expectation, yielding a total of 125 
pairs of expressions. The learner answers were 
always responses to the first hint for that 
expectation. The E-S pairs were graded by Physics 
experts on a scale of 1-4 (4 being perfect answer). 
This rubric could be mapped onto a binary TRUE-
FALSE rubric: scores 3 and 4 equal a TRUE 
decision and 1 and 2 equal a FALSE decision. We 
ended up with 36 FALSE and 89 TRUE entailment 
pairs, i.e. a 28.8% versus 71.2% split (as compared 
to the 50-50% split of RTE data). 
The iSTART data set, also known as the User 
Language Paraphrase Corpus (McCarty & 
McNamara, 2008) comprises annotations of 
paraphrase relations between student responses and 
ideal answers. The corpus contains 1998 pairs 
collected from previous student iSTART sessions 
and is divided into training (1499 instances) and 
testing (499 instances) subsets. The training subset 
contains 54% positive instances while testing 
contains 55% positive instances. The iSTART 
texts represent high school students? attempts to 
self-explain biology textbook texts. 
To evaluate the performance of our methods, we 
compare the methods? judgments with the expert 
judgments. The percentage of matching judgments 
provides the accuracy of the run, i.e. the fraction of 
correct responses. We also report kappa statistics 
which indicate agreement between our methods? 
output and the human-expert judgments for each 
1.00 
speed                  motion 
0.95 
0.75 
  motion              acceleration    Sentence A: 
Sentence B: 
0.69 
160
instance while taking into account chance 
agreement. 
Tables 1, 2, and 3 summarize the results on the 
original AutoTutor data (from Rus & Graesser, 
2006; Table 1), the re-annotated AutoTutor data by 
a second rater with inter-annotator agreement of 
0.606 (Table 2), and the ULPC test subset (Table 
3). For the ULPC corpus the methods have been 
trained on the training subset, an optimum 
threshold has been learned (such that scores above 
the threshold mean TRUE paraphrases) which is 
then used on the test data. Since the AutoTutor 
dataset is small, we only report results on it as a 
whole, i.e. only training. We report for each corpus 
a baseline method of guessing all the time the 
dominant class in the dataset (which is TRUE 
paraphrase for all three datasets), a pure greedy 
method (Greedy label in the first column of the 
tables), a greedy method applied to the words 
paired by the optimum method (optGreedy), and 
the results with the optimum matching method 
(Optimum).  
Overall, the optimum method offered better 
performance in terms of accuracy and kappa 
statistics. The greedy method yields results that are 
close. In fact, when analyzed as raw scores instead 
of binary decisions (as is the case when computing 
accuracy) the greedy raw score are on average very 
similar to the optimum scores. For instance, for the 
LSA word-to-word similarity metric which 
provided best accuracy results on the ULPC 
dataset (accuracy=.643 for optimum and .615 for 
greedy), the average raw scores are .563 (using 
optimum matching) and .567 (using greedy 
matching). One reason for why they are so closed 
is that in optimum matching we have one-to-one 
word matches while in the greedy matching many-
to-one matches are possible. That is, two words v 
and w from text T1 can be matched to same word y 
in text T2 in the greedy method. If we enforce that 
only one-to-one matches are possible in the greed 
method as in the optimum method, then we obtain 
the optGreedy method. The optGreedy method 
does work better than the pure greedy method 
(Greedy in the tables). 
Another reason for why the raw scores are close 
for greedy and optimum is the fact that student 
input and expert answers in both the AutoTutor 
and ULPC corpora are sharing many words in 
common (>.50). This is the case because the 
dialogue is highly contextualized around a given, 
e.g. physics, problem. In the answer, both students 
and experts refer to the entities and interactions in 
the problem statement which leads to high 
identical word overlap. Identical words lead to 
perfect word-to-word similarity scores (=1.00) 
increasing the overall similarity score of the two 
sentences in both the greedy and optimum method. 
Conclusions and Future Work 
Overall, the optimum method offers better 
performance in terms of accuracy and kappa 
statistics than greedy and baseline methods. 
The way we modeled the student assessment 
problem in this paper cannot deal with some type 
of responses. For instance, sometimes students? 
responses are mixed. Instead of being TRUE or 
FALSE responses, they contain both a correct part 
and an incorrect part as illustrated in the example 
below (Expert Answer provided for reference). 
Expert Answer: The object continues to have a 
constant horizontal velocity component after it is 
released that is the same as the person horizontal 
velocity at the time of dropping the object. 
Student Input: The horizontal velocity will decrease 
while the vertical velocity increases. 
Such a mixed student input should trigger a 
mixed feedback from the system: ?You are 
partially right! The vertical velocity will increase 
but not the horizontal velocity. Can you explain 
why?? We plan to address this problem in the 
future by proposing a more sophisticated model. 
We also plan to answer the question of how much 
lexical versus world and domain knowledge each 
of these measures can capture. For instance, 
WordNet can be viewed as capturing some world 
knowledge as the concepts? definitions provide 
information about the world. However, it might be 
less rich in capturing domain specific knowledge. 
Indeed, WordNet seems to capture less domain 
knowledge at first sight. For instance, the 
definition of acceleration in WordNet does not 
link it to the concept of force but physics laws do, 
e.g. Newton?s second law of motion. 
Acknowledgments 
This research was supported in part by the Institute 
for Education Sciences (award R305A100875). 
The opinions and findings reported in this paper 
are solely the authors?. 
 
161
 ID RES LCH JCN LSA Path Lin WUP 
Baseline .712 .712 .712 .712 .712 .712 .712 
Greedy .736/.153 .752/.204 .760/.298 .744/.365 .752/.221 .744/.354 .760/.298 
optGreedy .744/187 .752/.221 .760/.298 .744/.306 .752/.309 .752/.204 .784/.349 
Optimal .744/.236 .752/.204 .760/.298 .744/.221 .752/.334 .752/.204 .784*/.409* 
Table 1.  Accuracy/kappa on AutoTutor data (* indicates statistical significance over the baseline method at p<0.005 level). 
ID RES LCH JCN LSA Path Lin WUP 
Baseline .568 .568 .568 .568 .568 .568 .568 
Greedy .616/.137 .608/.117 .624/.214 .632/.256 .624/.161 .608/1.34 .624/.181 
optGreedy .632/.192 .632/.207 .632/.229 .624/.218 .632*/.177* .624/.165 .648*/.235* 
Optimal .624*/.153* .624/.169 .640*/.208* .640/.283 .624/.165 .624*/.148 .624/.173 
Table 2.  Accuracy/kappa on AutoTutor data with user annotations (* indicates statistical significance over the baseline 
method at p<0.005 level). 
ID RES LCH JCN LSA Path Lin WUP 
Baseline .547 .547 .547 .547 .547 .547 .547 
Greedy .619/.196 .619/.201 .629/.208 .615/.183 .635/.221 .629/.214 .621/.201 
optGreedy .621/.195 .615/.201 .629/.208 .643/.237 .623/.197 .619/.196 .613/.190 
Optimal .625/.205 .615/.196 .629/.208 .643/.237 .633/.215 .623/.203 .625/.214 
Table 3.  Accuracy/kappa on ULPC test data (all results are statistically different from the baseline at p<0.005 level). 
 
 
References  
Courtney Corley and Rada Mihalcea. 2005. Measures of 
Text Semantic Similarity. In Proceedings of the ACL 
workshop on Empirical Modeling of Semantic 
Equivalence, Ann Arbor, MI, June 2005. 
Ido Dagan, Oren Glickman, and Bernando Magnini. 
2005. The PASCAL recognizing textual entailment 
challenge. In Proceedings of the PASCAL 
Workshop. 
Bill W. Dolan, Chris Quirk, and Chris Brockett. 2004. 
Unsupervised construction of large paraphrase 
corpora: Exploiting massively parallel news sources. 
In Proceedings of the 20th International Conference 
on Computational Linguistics, Geneva, Switzerland. 
Arthur C. Graesser, Andrew Olney, Brian C. Hayes, and 
Patrick Chipman. 2005. Autotutor: A cognitive 
system that simulates a tutor that facilitates learning 
through mixed-initiative dialogue. In Cognitive 
Systems: Human Cognitive Models in System 
Design. Mahwah: Erlbaum. 
Harold W. Kuhn. 1955. "The Hungarian Method for the 
assignment problem", Naval Research Logistics 
Quarterly, 2:83?97, 1955. Kuhn's original 
publication. 
Thomas K. Landauer, Danielle S. McNamara, Simon 
Dennis, and Walter Kintsch. 2007. Handbook of 
Latent Semantic Analysis. Mahwah, NJ: Erlbaum. 
Mihai Lintean and Vasile Rus. 2012. Measuring 
Semantic Similarity in Short Texts through Greedy 
Pairing and Word Semantics. To be presented at The 
Twenty-Fifth International FLAIRS Conference. 
Marco Island, Florida. 
Philip M. McCarty and Danielle S. McNamara. 2008. 
User-Language Paraphrase Corpus Challenge, online. 
Danielle S. McNamara, Irwin B. Levinstein, and 
Chutima Boonthum. 2004. iSTART: interactive 
strategy training for active reading and thinking. 
Behavioral Research Methods, Instruments, and 
Computers, 36(2). 
Ted Pedersen, Siddharth Patwardhan, and Jason 
Michelizzi. 2004. WordNet::Similarity ? Measuring 
the Relatedness of Concepts. In Proceedings of Fifth 
Annual Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(NAACL-2004). 
Vasile Rus, and Arthur C. Graesser. 2006. Deeper 
Natural Language Processing for Evaluating Student 
Answers in Intelligent Tutoring Systems, 
Proceedings of the Twenty-First National Conference 
on Artificial Intelligence (AAAI-06). 
162
